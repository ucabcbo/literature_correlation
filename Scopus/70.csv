"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"","","","5th IFIP TC 12 International Conference on Intelligence Science, ICIS 2022","2022","IFIP Advances in Information and Communication Technology","659 IFIP","","","","","465","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144527713&partnerID=40&md5=82441a44adb92b612e33dfcf02d2f785","The proceedings contain 49 papers. The special focus in this conference is on Intelligence Science. The topics include: Accelerating Deep Convolutional Neural Network Inference Based on OpenCL; a Simple Approach to the Multiple Source Identification of Information Diffusion; a Directed Search Many Objective Optimization Algorithm Embodied with Kernel Clustering Strategy; a Two-Branch Neural Network Based on Superpixel Segmentation and Auxiliary Samples; augmentation Based Synthetic Sampling and Ensemble Techniques for Imbalanced Data Classification; BA-GAN: Bidirectional Attention Generation Adversarial Network for Text-to-Image Synthesis; personalized Recommendation Using Extreme Individual Guided and Adaptive Strategies; Improved Transformer-Based Implicit Latent GAN with Multi-headed Self-attention for Unconditional Text Generation; learning a Typhoon Bayesian Network Structure from Natural Language Reports; DNM-SNN: Spiking Neural Network Based on Dual Network Model; what Is Information? An Interpretation Based on the Theory of Modern Complexity Science; deep Siamese Network with Contextual Transformer for Remote Sensing Images Change Detection; GSoP Based Siamese Feature Fusion Network for Remote Sensing Image Change Detection; PolSF: PolSAR Image Datasets on San Francisco; RSMatch: Semi-supervised Learning with Adaptive Category-Related Pseudo Labeling for Remote Sensing Scene Classification; visual Question Answering of Remote Sensing Image Based on Attention Mechanism; multi-scale Spatial Aggregation Network for Remote Sensing Image Segmentation; deep Complex Convolutional Neural Networks for Remote Sensing Image Classification; dual Siamese Channel Attention Networks for Visual Object Tracking; motion-Aligned and Hardness-Aware Dynamic Update Network for Weakly-Supervised Vehicle Detection in Satellite Videos; a Memetic Algorithm Based on Adaptive Simulated Annealing for Community Detection; a Multi-level Mixed Perception Network for Hyperspectral Image Classification; A Lightweight SAR Ship Detection Network Based on Superpixel Statistical Modeling; gaussian Balanced Sampling for End-to-End Pedestrian Detector.","","","Conference review","Final","","Scopus","2-s2.0-85144527713"
"Xu J.; Liu W.; Qin Y.; Xu G.","Xu, Jianming (57825321000); Liu, Weichun (57835195800); Qin, Yang (57824901700); Xu, Guangrong (57825155200)","57825321000; 57835195800; 57824901700; 57825155200","Image Super-Resolution Reconstruction Method for Lung Cancer CT-Scanned Images Based on Neural Network","2022","BioMed Research International","2022","","3543531","","","","10.1155/2022/3543531","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135202208&doi=10.1155%2f2022%2f3543531&partnerID=40&md5=8e459600bfbbcca736f32e75179aefb6","The super-resolution (SR) reconstruction of a single image is an important image synthesis task especially for medical applications. This paper is studying the application of image segmentation for lung cancer images. This research work is utilizing the power of deep learning for resolution reconstruction for lung cancer-based images. At present, the neural networks utilized for image segmentation and classification are suffering from the loss of information where information passes through one layer to another deep layer. The commonly used loss functions include content-based reconstruction loss and generative confrontation network. The sparse coding single-image super-resolution reconstruction algorithm can easily lead to the phenomenon of incorrect geometric structure in the reconstructed image. In order to solve the problem of excessive smoothness and blurring of the reconstructed image edges caused by the introduction of this self-similarity constraint, a two-layer reconstruction framework based on a smooth layer and a texture layer is proposed for a medical application of lung cancer. This method uses a global nonzero gradient number constrained reconstruction model to reconstruct the smooth layer. The proposed sparse coding method is used to reconstruct high-resolution texture images. Finally, a global and local optimization models are used to further improve the quality of the reconstructed image. An adaptive multiscale remote sensing image super-division reconstruction network is designed. The selective core network and adaptive gating unit are integrated to extract and fuse features to obtain a preliminary reconstruction. Through the proposed dual-drive module, the feature prior drive loss and task drive loss are transmitted to the super-resolution network. The proposed work not only improves the subjective visual effect but the robustness has also been enhanced with more accurate construction of edges. The statistical evaluators are used to test the viability of the proposed scheme. © 2022 Jianming Xu et al.","Algorithms; Humans; Image Processing, Computer-Assisted; Lung Neoplasms; Neural Networks, Computer; Tomography, X-Ray Computed; Article; computer assisted tomography; deep learning; deep neural network; human; image processing; image reconstruction; image segmentation; lung cancer; reconstruction algorithm; remote sensing; algorithm; diagnostic imaging; image processing; lung tumor; procedures; x-ray computed tomography","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135202208"
"Liu L.; Zou Z.; Shi Z.","Liu, Liqin (57215536317); Zou, Zhengxia (56073977200); Shi, Zhenwei (23398841900)","57215536317; 56073977200; 23398841900","Hyperspectral Remote Sensing Image Synthesis Based on Implicit Neural Spectral Mixing Models","2023","IEEE Transactions on Geoscience and Remote Sensing","61","","5500514","","","","10.1109/TGRS.2022.3232705","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146258064&doi=10.1109%2fTGRS.2022.3232705&partnerID=40&md5=e0354c20ba09f7c4aae126916989d890","Hyperspectral image (HSI) synthesis, as an emerging research topic, is of great value in overcoming sensor limitations and achieving low-cost acquisition of high-resolution remote sensing HSIs. However, the linear spectral mixing model used in recent studies oversimplifies the real-world hyperspectral imaging process, making it difficult to effectively model the imaging noise and multiple reflections of the object spectrum. As a prerequisite for hyperspectral data synthesis, accurate modeling of nonlinear spectral mixtures has long been a challenge. Considering the above difficulties, we propose a novel method for modeling nonlinear spectral mixtures based on implicit neural representations (INRs) in this article. The proposed method learns from INR and adaptively implements different mixture models for each pixel according to their spectral signature and surrounding environment. Based on the above neural mixing model, we also propose a new method for HSI synthesis. Given an RGB image as input, our method can generate an accurate and physically meaningful HSI. As a set of by-products, our method can also generate subpixel-level spectral abundance as well as the solar atmosphere signature. The whole framework is trained end-to-end in a self-supervised manner. We constructed a new dataset for HSI synthesis based on a wide range of Airborne Visible Infrared Imaging Spectrometer (AVIRIS) data. Our method achieves a mean peak signal-to-noise ratio (MPSNR) of 52.36 dB and outperforms other state-of-the-art hyperspectral synthesis methods. Finally, our method shows great benefits to downstream data-driven applications. With the HSIs and abundance directly generated from low-cost RGB images, the proposed method improves the accuracy of HSI classification tasks by a large margin, particularly for those with limited training samples.  © 1980-2012 IEEE.","Hyperspectral imaging; Image enhancement; Independent component analysis; Mixing; Pixels; Remote sensing; Adaptive spectral mixture model; Atmospheric modeling; HyperSpectral; Hyperspectral image synthesis; Images reconstruction; Images synthesis; Implicit neural representation; Mixture modeling; Neural representations; Remote-sensing; Spectral mixture model; AVIRIS; byproduct; data acquisition; pixel; remote sensing; satellite imagery; sensor; Image reconstruction","Adaptive spectral mixture model; hyperspectral image (HSI) synthesis; implicit neural representation (INR); remote sensing","Article","Final","","Scopus","2-s2.0-85146258064"
"Jia F.; Xu J.; Sun X.; Ma Y.; Ni M.","Jia, Fei (57298794000); Xu, Jindong (35176864300); Sun, Xiao (57208776868); Ma, Yongli (57219779729); Ni, Mengying (26325030000)","57298794000; 35176864300; 57208776868; 57219779729; 26325030000","Blind image separation method based on cascade generative adversarial networks","2021","Applied Sciences (Switzerland)","11","20","9416","","","","10.3390/app11209416","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117199296&doi=10.3390%2fapp11209416&partnerID=40&md5=4dcab0101086c46f4edbf9800a6c341c","To solve the challenge of single-channel blind image separation (BIS) caused by unknown prior knowledge during the separation process, we propose a BIS method based on cascaded generative adversarial networks (GANs). To ensure that the proposed method can perform well in different scenarios and to address the problem of an insufficient number of training samples, a synthetic network is added to the separation network. This method is composed of two GANs: a U-shaped GAN (UGAN), which is used to learn image synthesis, and a pixel-to-attention GAN (PAGAN), which is used to learn image separation. The two networks jointly complete the task of image separation. UGAN uses the unpaired mixed image and the unmixed image to learn the mixing style, thereby generating an image with the “true” mixing characteristics which addresses the problem of an insufficient number of training samples for the PAGAN. A self-attention mechanism is added to the PAGAN to quickly extract important features from the image data. The experimental results show that the proposed method achieves good results on both synthetic image datasets and real remote sensing image datasets. Moreover, it can be used for image separation in different scenarios which lack prior knowledge and training samples. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Blind image separation; Generative adversarial networks; Remote sensing images; Visual attention","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117199296"
"Huang H.; Yi W.; Du L.; Cui W.; Zeng X.","Huang, Honglian (56138784000); Yi, Weining (55765345200); Du, Lili (36772849300); Cui, Wenyu (55943343800); Zeng, Xianfang (57192272750)","56138784000; 55765345200; 36772849300; 55943343800; 57192272750","Multi-spectral remote sensing image true color synthesis technique based on artificial target","2016","Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering","45","11","1126002","","","6","10.3788/IRLA201645.1126002","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002873106&doi=10.3788%2fIRLA201645.1126002&partnerID=40&md5=74c0333c00967122d9b59f58140fe1af","Multi-spectral true color synthesized images have the prospects of broad application in the interpretation of remote sensing image, target recognition and information processing etc. The technique of true color synthesis depends on the accuracy of the obtained tristimulus values CIE-XYZ, which is used to establish the right relationship between the system of camera's RGB and human being's visual color. So, a method of true color image synthesis based on the information of artificial target was proposed by laying the man-made targets when the satellite passes. And, the transformation matrix between camera's RGB trichromatic system and human being's visual color system was estimated from the reflectance spectrum of man-made targets. Eventually, the suitable true color correction model was established in the certain atmospheric conditions. Experiment of true color correction was conducted on the multi-spectral images of GF-1 satellite, and the results show that good color correction effects has been exhibited on even every image with different degrees of color's richness. © 2016, Editorial Board of Journal of Infrared and Laser Engineering. All right reserved.","Cameras; Color; Color image processing; Image reconstruction; Linear transformations; Reflection; Remote sensing; Spectroscopy; Artificial targets; Atmospheric conditions; Multispectral images; Reflectance spectrum; Remote sensing images; Spectral reflectances; Transformation matrices; True colors; Image processing","Artificial target; Multi-spectral image; Spectral reflectance; True color synthesis","Article","Final","","Scopus","2-s2.0-85002873106"
"Wang G.; Dong G.; Li H.; Han L.; Tao X.; Ren P.","Wang, Guangxing (57213191777); Dong, Guoshuai (57208247636); Li, Hui (57207879663); Han, Lirong (57203552486); Tao, Xuanwen (57205509060); Ren, Peng (25960361900)","57213191777; 57208247636; 57207879663; 57203552486; 57205509060; 25960361900","Remote Sensing Image Synthesis via Graphical Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898915","10027","10030","3","10.1109/IGARSS.2019.8898915","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077676346&doi=10.1109%2fIGARSS.2019.8898915&partnerID=40&md5=3895a5ecd138121cbdc7d04b7c58f859","We explore the use of graphical generative adversarial networks (Graphical-GAN) for synthesizing remote sensing images. The model is probabilistic graphical based generative adversarial networks (GAN). It pairs a generative network G with a recognition network R. Both of them are adversarially trained with a discriminative network D. Particularly, R is employed to infer the underlying causal relationships among both observed and latent variables from real remote sensing images. The advantages of the Graphical-GAN for synthesizing multiple categories of remote sensing images are two fold. Firstly, it considers the underlying causal relationships and captures the true data distribution of remote sensing images. Secondly, the adversarial learning generates synthetic sensing images that are similar to real ones with slight differences. Our remote sensing image synthesis scheme paves a promising way for remote sensing dataset augmentation, which is an effective means of improving the accuracy of learning models. Experimental results with high Inception Scores (IS) validate the effectiveness of the Graphical-GAN for remote sensing image synthesis. © 2019 IEEE.","Geology; Image enhancement; Adversarial learning; Adversarial networks; Causal relationships; Data distribution; Discriminative networks; Latent variable; Learning models; Remote sensing images; Remote sensing","Graphical Generative Adversarial Networks; Remote Sensing Image Synthesis","Conference paper","Final","","Scopus","2-s2.0-85077676346"
"Zhang Y.; Li M.","Zhang, Yali (57215935694); Li, Mingshi (7405266599)","57215935694; 7405266599","A new method for monitoring start of season (SOS) of forest based on multisource remote sensing","2021","International Journal of Applied Earth Observation and Geoinformation","104","","102556","","","","10.1016/j.jag.2021.102556","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121631593&doi=10.1016%2fj.jag.2021.102556&partnerID=40&md5=f2443faad217e4b424fc01a8b653b56d","As a sensitive indicator of climate change, forest phenology (e.g., the start of season (SOS)) has profound impacts on the global carbon cycle. Traditional phenological observations are based on surface observation networks. Generally, the outcomes of this manner are less representative and hard to implement in wide forested regions. Remote sensing based observation of forest SOS has currently been a popular way. Those sensors with coarse spatial resolution have been widely used to estimate forest SOS, but they create serious estimation errors in areas of high heterogeneity. Medium-resolution sensors, such as Landsat, face significant challenges in SOS monitoring due to the long revisit period. In this study, we aimed to develop a new method to estimate forest SOS from 2013 to 2019. First, we collected all available Landsat and Sentinel-2 images, and then redefined the linear regression coefficients for the bandpass adjustment to weaken the surface reflectance (SR) differences in different sensors. Subsequently, we improved and developed the modified continuous change detection and classification (MCCDC) model to generate daily vegetation index curves. Finally, we adopted the logistic regression model to test the potential of the enhanced vegetation index (EVI), normalized difference vegetation index (NDVI) and land surface water index (LSWI) in evaluating the annual SOS. The reduced root mean square error (RMSE) for all bands after the integration indicated that the adjustment was successful. We visually compared Landsat's synthetic images with the actual acquired images and found that their respective false-colour composites were highly similar. Assessing the SOS from the EVI, NDVI and LSWI showed different estimated results. By comparing the annual SOS derived from the three indices with the field observations, it was found that the SOS based on the EVI maintained a low consistency with the field observations. The SOS accuracy from LSWI was the highest and most forest SOS from LSWI in the study area were mainly concentrated in 80–150 days. These three indices all showed that the SOS always fluctuated by ± 4 days from 2013 to 2019. Facing the lack of clear remote sensing images with medium spatial resolution in cloudy and rainy areas, this study proposes an improved method to generate clear daily vegetation index images at a spatial resolution of 30 m, making annual SOS monitoring promising and feasible. © 2021 The Author(s)","forest; image analysis; Landsat; monitoring system; phenology; regression analysis; remote sensing; Sentinel; time series","Dense time series; Forest SOS; Image synthesis; Multi-sensor","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121631593"
"Sharma A.; Jindal N.; Thakur A.","Sharma, Akanksha (57188975329); Jindal, Neeru (8571819900); Thakur, Abhishek (57055028500)","57188975329; 8571819900; 57055028500","Comparison on Generative Adversarial Networks - A Study","2018","ICSCCC 2018 - 1st International Conference on Secure Cyber Computing and Communications","","","8703267","391","396","5","10.1109/ICSCCC.2018.8703267","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065640687&doi=10.1109%2fICSCCC.2018.8703267&partnerID=40&md5=58155d051aa1abe8791a3d2595bfc668","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks. © 2018 IEEE.","Deep learning; Learning systems; Remote sensing; Adversarial networks; Diverse fields; Image synthesis; Learning models; Video generation; Image processing","Generative Adversarial Networks; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85065640687"
"Gao X.; Wu M.; Gao J.; Han L.; Niu Z.; Chen F.","Gao, Xumiao (57480307000); Wu, Mingquan (55612797500); Gao, Ju (57639027200); Han, Li (57642055800); Niu, Zheng (7101688905); Chen, Fang (57441279900)","57480307000; 55612797500; 57639027200; 57642055800; 7101688905; 57441279900","Modelling Electricity Consumption in Cambodia Based on Remote Sensing Night-Light Images","2022","Applied Sciences (Switzerland)","12","8","3971","","","","10.3390/app12083971","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128874247&doi=10.3390%2fapp12083971&partnerID=40&md5=944c311a55e5b32823f972b629cfbc0c","The accurate estimation of electricity consumption and its spatial distribution are important in electricity infrastructural planning and the achievement of the United Nations Sustainable Development Goal 7 (SDG7). Electricity consumption can be estimated based on its correlation with nighttime lights observed using remote sensing imagery. Since night-light images are easily affected by cloud cover, few previous studies have estimated electricity consumption in cloudy areas. Taking Cambodia as an example, the present study proposes a method for denoising night-light images in cloudy areas and estimating electricity consumption. The results show that an exponential model is superior to linear and power function models for modelling the relationship between total night-light data and electricity consumption in Cambodia. The month-specific substitution method is best for annual night-light image synthesis in cloudy areas. Cambodia’s greatest electricity consumption occurs in its four most economically developed cities. Electricity consumption spreads outwards from these cities along the main transport routes to a large number of unelectrified areas. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","estimated electricity consumption; night-light image processing in cloudy areas; SDG7; spatial patterns of electricity consumption; VIIRS/DNB","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128874247"
"Deng K.; Zhang K.; Yao P.; Cheng S.; He P.","Deng, Kai (57217296176); Zhang, Kun (57214938091); Yao, Ping (57206347584); Cheng, Siyuan (57261961400); He, Peng (57192956908)","57217296176; 57214938091; 57206347584; 57261961400; 57192956908","Skip attention GaN for remote sensing image synthesis","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","2305","2309","4","10.1109/ICASSP39728.2021.9414701","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115066093&doi=10.1109%2fICASSP39728.2021.9414701&partnerID=40&md5=41f0d63d2d07c053c46253fa30d878df","High-quality remote sensing images are difficult to obtain due to limited conditions and high cost for data acquisition. With the development of machine vision and deep learning, some image generation methods (e.g., GANs) are introduced into this field, but it's still hard to generate images with good texture details and structural dependencies. We establish Skip Attention Mechanism to deal with this problem, which learns dependencies between local points on low-resolution feature maps, and then upsample the attention map and combine it with high-resolution feature maps. With this method, long-range dependencies learned from low-resolution are used for generating remote sensing images with more structural details. We name this method as Skip Attention GAN, which is the first method applying cross-scale attention mechanism for unsupervised remote sensing image generation. Experiments show that our method outperforms previous methods under several metrics. Visual and ablation results of attention layers show that Skip Attention has learned long-distance structural dependencies between similar targets. © 2021 IEEE","Data acquisition; Deep learning; Gallium nitride; III-V semiconductors; Signal processing; Textures; Attention mechanisms; High quality; High resolution; Image generations; Long-range dependencies; Low resolution; Remote sensing images; Structural details; Remote sensing","Attention mechanism; Generative adversarial network; Remote sensing image synthesis","Conference paper","Final","","Scopus","2-s2.0-85115066093"
"","","","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","","","178","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060419385&partnerID=40&md5=9c12bd0dc33bbf219d0df980041bcf20","The proceedings contain 22 papers. The topics discussed include: SAR to optical image synthesis for cloud removal with generative adversarial networks; development of a portable high performance mobile mapping system using the robot operating system; data processing and recording using a versatile multi-sensor vehicle; semantic segmentation of aerial imagery via multi-scale shuffling convolutional neural networks with deep supervision; Copernicus sentinel-2 data for the determination of groundwater withdrawal in the maghreb region; calibration study of a trimble ACX4 system for direct georeferencing mapping applications; infrared measurements and estimation of temperature in the restrictive scope of an industrial cement plant; extraction of solar cells from UAV-based thermal image sequences; disparity refinement of building edges using robustly matched straight lines for stereo matching; and pilot study on the retrieval of DBH and diameter distribution of deciduous forest stands using cast shadows in UAV-based orthomosaics.","","","Conference review","Final","","Scopus","2-s2.0-85060419385"
"Xu L.; Wong A.; Clausi D.A.","Xu, Linlin (55921131900); Wong, Alexander (15073608800); Clausi, David A. (7003991297)","55921131900; 15073608800; 7003991297","An Enhanced Probabilistic Posterior Sampling Approach for Synthesizing SAR Imagery with Sea Ice and Oil Spills","2017","IEEE Geoscience and Remote Sensing Letters","14","2","7797259","188","192","4","10.1109/LGRS.2016.2633572","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007110623&doi=10.1109%2fLGRS.2016.2633572&partnerID=40&md5=629bb5bd8b9ca008679764706c03d359","Although the synthesis of the synthetic aperture radar (SAR) imagery with both sea ice and oil spills can significantly benefit in improving the consistency and comprehensiveness of testing and evaluating algorithms that are designed for mapping cold ocean regions, creating such imagery is difficult due to the heterogeneity and complexity of the source images. This letter presents an enhanced region-based probabilistic posterior sampling approach to effectively synthesize SAR imagery with different ocean features. In the proposed approach, instead of relying entirely on the SAR intensity values, the posterior sampling is performed based on a number of quantitative factors, such as intensity, label field, and the prior class probability of sampling candidates, constituting a complete probabilistic framework that addresses key aspects in the synthesis of SAR imagery from heterogeneous sources. The experiments demonstrate that the proposed approach can better address the difficulties caused by the heterogeneity in the source images compared with the existing state-of-the-art ice synthesis method, and it will improve the consistency, comprehensiveness, and fairness of the evaluation of the remote sensing classification and segmentation algorithms. © 2016 IEEE.","Ice; Image processing; Image segmentation; Mapping; Marine pollution; Oil spills; Remote sensing; Sea ice; Synthetic aperture radar; Class probabilities; Evaluating algorithms; Heterogeneous sources; Probabilistic framework; Quantitative factors; Remote sensing classification; Segmentation algorithms; Synthetic Aperture Radar Imagery; Radar imaging","Image synthesis; oil spills; sea ice; Statistical texture modeling","Article","Final","","Scopus","2-s2.0-85007110623"
"Qiao H.; Zhang P.; Li Z.; Liu C.","Qiao, Haiwei (57225882444); Zhang, Ping (57198754127); Li, Zhen (57196398078); Liu, Chang (57191676667)","57225882444; 57198754127; 57196398078; 57191676667","A New Geostationary Satellite-Based Snow Cover Recognition Method for FY-4A AGRI","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","11372","11385","13","10.1109/JSTARS.2021.3125015","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118673143&doi=10.1109%2fJSTARS.2021.3125015&partnerID=40&md5=37c48de8074becf11f94cc8a7f6f04e7","Snow cover is an important component of the cryosphere. Clouds have a large influence on optical remote sensing satellites when recognizing snow cover. Geostationary satellites, due to their high-frequency observations over coverage areas, can effectively compensate for the drawback of snow cover recognition from polar orbit optical satellites under cloud-covered conditions. However, past geostationary satellites have relatively few band settings to produce sensitive factors for snow cover recognition. The FY-4A Advanced Geostationary Radiation Imager (AGRI) satellite has the advantage of high temporal resolution with a wealth of bands, which highlights its potential in reducing the impact of clouds and accurately obtaining snow cover information. Based on the advantages of FY-4A AGRI data and the flow characteristics of clouds, we developed an improved maximum brightness temperature image synthesis algorithm, which can greatly reduce the probability of cloud and snow cover misclassification. Combining the features of FY-4A AGRI data, we reorganized the snow cover recognition factor and developed a new snow cover recognition method. The results show that the proposed method can reduce cloud cover by 57.172% compared with MOD10A1 data. After evaluating the proposed method using meteorological ground observation datasets and MOD10A1 data, we found that the overall accuracy of the proposed method can reach 94.11% and 98.55%, respectively, and the F-score (FS) can reach 73.05% and 85.40%, respectively. © 2008-2012 IEEE.","China; Satellites; Geostationary satellites; Image enhancement; Luminance; Optical sensors; Orbits; Snow; China; Cloud cover; Cryosphere; FY-4a advanced geostationary radiation imager; Imager data; Optical imaging; Radiation imagers; Recognition methods; Remote-sensing; Snow covers; accuracy assessment; algorithm; brightness temperature; FengYun; geostationary satellite; remote sensing; snow cover; Remote sensing","China; Fengyun-4A (FY-4A) advanced geostationary radiation imager (AGRI); geostationary satellite; snow cover","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118673143"
"Hoang N.T.; Koike K.","Hoang, Nguyen Tien (57188550727); Koike, Katsuaki (57219664247)","57188550727; 57219664247","Development of Bayesian-based transformation method of Landsat imagery into pseudo-hyperspectral imagery","2015","Proceedings of SPIE - The International Society for Optical Engineering","9643","","96430J","","","","10.1117/12.2194886","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961664703&doi=10.1117%2f12.2194886&partnerID=40&md5=c439df34b76396d6a52a47df34496147","It has been generally accepted that hyperspectral remote sensing is more effective and provides greater accuracy than multispectral remote sensing in many application fields. EO-1 Hyperion, a representative hyperspectral sensor, has much more spectral bands, while Landsat data has much wider image scene and longer continuous space-based record of Earth's land. This study aims to develop a new method, Pseudo-Hyperspectral Image Synthesis Algorithm (PHISA), to transform Landsat imagery into pseudo hyperspectral imagery using the correlation between Landsat and EO-1 Hyperion data. At first Hyperion scene was precisely pre-processed and co-registered to Landsat scene, and both data were corrected for atmospheric effects. Bayesian model averaging method (BMA) was applied to select the best model from a class of several possible models. Subsequently, this best model is utilized to calculate pseudo-hyperspectral data by R programming. Based on the selection results by BMA, we transform Landsat imagery into 155 bands of pseudo-hyperspectral imagery. Most models have multiple R-squared values higher than 90%, which assures high accuracy of the models. There are no significant differences visually between the pseudo-and original data. Most bands have Pearson's coefficients < 0.95, and only a small fraction has the coefficients < 0.93 like outliers in the data sets. In a similar manner, most Root Mean Square Error values are considerably low, smaller than 0.014. These observations strongly support that the proposed PHISA is valid for transforming Landsat data into pseudo-hyperspectral data from the outlook of statistics. © 2015 SPIE.","Bayesian networks; Data visualization; Earth (planet); Image processing; Mean square error; Metadata; Signal processing; Space optics; Spectroscopy; Data simulation; Hyperion; HyperSpectral; Hyperspectral Data; LANDSAT; PHISA; Spectral reconstruction; Remote sensing","data simulation; Hyperion; Hyperspectral data; Landsat; PHISA; pseudo-hyperspectral im-agery; spectral reconstruction","Conference paper","Final","","Scopus","2-s2.0-84961664703"
"Huang H.; Chen W.; Zhang Y.; Qiao L.; Du Y.","Huang, Huiping (56138765400); Chen, Wei (57218157466); Zhang, Yuan (56422564300); Qiao, Lin (57211690876); Du, Yunyan (7402893687)","56138765400; 57218157466; 56422564300; 57211690876; 7402893687","Analysis of ecological quality in Lhasa Metropolitan Area during 1990–2017 based on remote sensing and Google Earth Engine platform","2021","Journal of Geographical Sciences","31","2","","265","280","15","10.1007/s11442-021-1846-8","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103839990&doi=10.1007%2fs11442-021-1846-8&partnerID=40&md5=2f1bade7ecaa818ef5d2b89ef0c639f5","Based on a total of 519 images, the composite images with the lowest possible cloud cover were generated at pixel level with image synthesis method on Google Earth Engine (GEE) platform. The Remote Sensing Ecological Index (RSEI) was adopted, and calculated in an efficient way with the assistance of parallel cloud computing of the GEE platform. The RSEI was used in this paper to evaluate and monitor the eco-environmental quality of the Lhasa Metropolitan Area. Results show that: (1) The ecological quality is better in the west than in the east of Lhasa Metropolitan Area, with Lhasa as an approximate dividing point. The ecological quality improved and then deteriorated dramatically before 2000, with the mean RSEI value dropping from 0.51 to 0.46; the trend was followed by a gradual increase up until 2017, with the mean RSEI value increased from 0.46 to 0.55. (2) The RSEI is weakly and positively correlated with socioeconomic indicators. This indicates that the population growth and economic development did not negatively influence the ecological quality, but actually boosted it. (3) The GEE can serve as an efficient computing platform for the assessment and monitoring of eco-environmental quality in vast regions. © 2021, Science in China Press.","","ecological index; ecological quality; Google Earth Engine; Lhasa Metropolitan Area; remote sensing","Article","Final","","Scopus","2-s2.0-85103839990"
"Zhan B.; Li F.; Lu M.","Zhan, Bangcheng (57216615206); Li, Feng (57171116800); Lu, Ming (56399795000)","57216615206; 57171116800; 56399795000","HDR Synthesis Technology for Spaceborne CMOS Cameras Based on Virtual Digital TDI","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9128021","3824","3833","9","10.1109/JSTARS.2020.3005667","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089308773&doi=10.1109%2fJSTARS.2020.3005667&partnerID=40&md5=b5b48db3d252c56b7485c78425cf970f","Due to the fact that the traditional high dynamic range (HDR) imaging methods cannot be used for satellites, finding a way to generate HDR remote sensing images from the satellites has long been explored in the field of remote sensing imaging. In this article, a systematic method of synthesizing the HDR remote sensing images based on a new registration algorithm and virtual digital delay integration (TDI) technology is proposed. First, a series of original images are generated by the fast continuous shooting method through a push-broom spaceborne camera. Then, a new registration algorithm with high accuracy and high robustness is proposed in this article, which is used for image registration. Finally, an HDR multiframe image synthesis algorithm is used to generate high-quality and high signal-to-noise-ratio HDR images. This technology greatly improves the image information acquisition capabilities of digital TDI area scan cameras. © 2008-2012 IEEE.","Cameras; CMOS integrated circuits; Remote sensing; Signal to noise ratio; High dynamic range; High signal-to-noise ratio; Image information; Multiframe images; Registration algorithms; Remote sensing images; Remote sensing imaging; Systematic method; algorithm; image analysis; image classification; image resolution; remote sensing; satellite imagery; signal-to-noise ratio; Image enhancement","High-resolution imaging; image processing; image registration algorithms; optical imaging","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85089308773"
"de Lima D.C.; Saqui D.; Mpinda S.A.T.; Saito J.H.","de Lima, Daniel Caio (57209397114); Saqui, Diego (56028559700); Mpinda, Steve Ataky Tsham (56786495100); Saito, José Hiroki (7102105877)","57209397114; 56028559700; 56786495100; 7102105877","Pix2Pix Network to Estimate Agricultural Near Infrared Images from RGB Data; [  Un réseau Pix2Pix pour générer des images dans le proche infrarouge en zones agricoles à partir de données RVB]","2022","Canadian Journal of Remote Sensing","48","2","","299","315","16","10.1080/07038992.2021.2016056","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123681785&doi=10.1080%2f07038992.2021.2016056&partnerID=40&md5=efc295efee187f1a7c7976414a8dfaeb","Remote sensing has been applied to agriculture, making it possible to acquire a large amount of data far away from crops, providing information for decision making by producers that can impact production costs and crops quality. One way of getting the production information is through vegetation indices, arithmetic operations that use spectral bands, especially the Near Infrared (NIR). However, sensors that capture this spectral information are very expensive for small producers to afford it. In a previous article, a pixel-to-pixel image synthesis model to estimate NIR images from RGB data using hyperspectral endmembers (pure hyperspectral signatures) was described. In this work, an image-to-image synthesis model, known as Pix2Pix, is used for estimating NIR images from low-cost RGB camera images. Pix2Pix is a kind of Generative Adversarial Networks (GANs), composed by two neural networks, a generator (G) and a discriminator (D), that compete. G learns to create images from a random noise inputs and D learns to verify if these images are real or fake. The results showed that the presented method generated NIR images quite similar to real ones, reaching a value of 0.912 on M3SIM similarity metric, outperforming results obtained with the previous endmembers method (0.775 on M3SIM). ©, Copyright © CASI.","Costs; Crops; Decision making; Infrared devices; Pixels; Remote sensing; Crop quality; Decisions makings; Endmembers; Images synthesis; Large amounts of data; Learn+; Near- infrared images; Production cost; Remote-sensing; Synthesis models; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85123681785"
"Onaizah A.N.; Xia Y.; zhan Y.; hussain K.; Koondhar I.A.","Onaizah, Ameer N. (57197762972); Xia, Yuanqing (55916672500); zhan, Yufeng (56949825900); hussain, Khurram (57556609100); Koondhar, Iftikhar Ahmed (57204077826)","57197762972; 55916672500; 56949825900; 57556609100; 57204077826","Systematic literature review on approaches of extracting image merits","2022","Optik","271","","170097","","","","10.1016/j.ijleo.2022.170097","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140334759&doi=10.1016%2fj.ijleo.2022.170097&partnerID=40&md5=bb08e567e6a36145e6c87d951f30a027","Texture analysis is gaining popularity among the scientific community. A wide variety of applications use texture analysis method. Texture analysis methods can be used for image segmentation, pattern analysis and pattern classification tasks. The application areas range from remote sensing, biomedical imaging, image synthesis, image inpainting and image processing. However, the preliminary step in all these applications refers to the extraction of intricate features from the given image. As a result, a wide verity of feature extraction methods exists in the literature. All the feature extraction methods have their own advantages and shortfalls. For example, some of the methods are computationally expensive, some are rotation and scale invariant whereas the others are easy to implement. This article provides an insight regarding different texture feature extraction techniques. The article bifurcate these techniques into different techniques according to their working principles. Besides provision of the basic working principle of every technique, the article provides an insight regarding their advantages and shortfalls. Moreover, this article considers deep learning and entropy based methods interesting for texture evaluation. Besides, the article also proposes a thorough study of these methods in texture analysis. © 2022 Elsevier GmbH","Deep learning; Extraction; Feature extraction; Image analysis; Image texture; Medical imaging; Remote sensing; Textures; Extracting image merit; Feature extraction methods; Images segmentations; Pattern analysis; Patterns classification; Scientific community; Systematic literature; Systematic literature review; Texture analysis; Texture analysis method; Image segmentation","Extracting image merits; Systematic Literature; Texture analysis method","Article","Final","","Scopus","2-s2.0-85140334759"
"Wang T.; Wei R.; Wang L.; Zhu L.; Zhou E.; Liu S.; Yang H.; Wang S.","Wang, Tong (57768934500); Wei, Ruizeng (57207034794); Wang, Lei (57225162641); Zhu, Ling (57751539100); Zhou, Enze (57216136941); Liu, Shuqin (57572632000); Yang, Hemeng (36633418700); Wang, Sen (57573209100)","57768934500; 57207034794; 57225162641; 57751539100; 57216136941; 57572632000; 36633418700; 57573209100","Detection of Transmission Towers and Insulators in Remote Sensing Images with Deep Learning","2021","Proceeding - 2021 China Automation Congress, CAC 2021","","","","3298","3303","5","10.1109/CAC53003.2021.9728166","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128012003&doi=10.1109%2fCAC53003.2021.9728166&partnerID=40&md5=5b44893377dbfd4bb45724445b6b4f8d","Intelligent inspections of high-voltage electronic power grids with high-altitude aerial or satellite remote sensing images (RSIs) have attracted more and more attention. The detection of transmission tower and insulator in smart electronic power is of great importance. Traditional image recognition based methods have been proven to be difficult to complete this task effectively and efficiently, lots of deep learning based methods have been adopted due to their promising performance. However, collecting a large number of labeled aerial/satellite imaging data for deep learning requires a lot of manpower/financial costs and the deep models trained on small size of samples are often easy to over-fit. For this reason, it has practical significance to study the automatic detection of towers and insulators in the case of small samples. Aiming at the problem of object detection under small samples, a deep learning framework for simultaneous towers and insulators detection in RSIs based on Faster-RCNN and neural style image synthesis is proposed. Firstly, to alleviate the small sample size problem, a sample generation method based on neural style transfer and alpha channel image fusion techniques is proposed, which randomly combines the foreground towers and background images to expand the training data set. Secondly, upon the expanded training data, an object detection model for towers and insulators based on Faster-RCNN is further trained. Experiments show that the object detection model trained with the extended training data has better generalization performance and can better suppress false alarms. © 2021 IEEE","Antennas; Deep learning; Electric power transmission; Electric power transmission networks; Image fusion; Image recognition; Object detection; Remote sensing; Towers; Electronic power grid; High voltage electronics; Insulator detection; Intelligent inspection; Neural style transfer; Remote sensing images; Small samples; Tower detection; Training data; Transmission tower; Object recognition","insulator detection; neural style transfer; object detection; power grids; tower detection","Conference paper","Final","","Scopus","2-s2.0-85128012003"
"Barka I.; Lukeš P.; Bucha T.; Hlásny T.; Strejček R.; Mlčoušek M.; Křístek S.","Barka, Ivan (6506856440); Lukeš, Petr (7004040173); Bucha, Tomáš (23479581100); Hlásny, Tomáš (24921257400); Strejček, Radim (57205077677); Mlčoušek, Marek (57205078039); Křístek, Štěpán (49663576500)","6506856440; 7004040173; 23479581100; 24921257400; 57205077677; 57205078039; 49663576500","Remote sensing-based forest health monitoring systems-case studies from Czechia and Slovakia","2018","Central European Forestry Journal","64","3-4","","259","275","16","10.1515/forj-2017-0051","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058472951&doi=10.1515%2fforj-2017-0051&partnerID=40&md5=219086836e8a3a8e4653f295735b8ccf","Aim of this paper is to present the remote sensing-based systems of forest health assessment in the Czech Republic and Slovakia, and to analyse both their strengths and weaknesses. Nationwide assessment of forest health in the Czech Republic is based on the interpretation of Sentinel-2 satellite data using novel approaches for cloud-free image synthesis based on all available satellite observations. A predictive statistical model to yield time series of leaf area index (LAI) from satellite observations is developed above extensive in-situ data, including LAI and forest defoliation assessment. Forest health is evaluated for each pixel from yearly changes of forest LAI, while the country-wise assessment of the health status is performed at the cadastral level. Methodology developed for Slovakia is based on a two-phase regression sampling. The first phase of the procedure provides an initial fast estimate of forest damage using only satellite observations (visible and infrared channels from Landsat or Sentinel-2 systems). The second phase refines the result of the first phase using data from a ground damage assessment (site-level defoliation from ICP Forests database). Resulting forest health assessment over the whole forest area is presented in 10 defoliation classes. The Czech Republic shows 1.6% of heavily damaged forests, 12.5% of damaged forests, 79.2% of forests with stable conditions, 6.3% of regenerated forests and 0.4% of strongly regenerated forests. In Slovakia, the total share of damaged stands (i. e. with defoliation higher than 40%) increased from 6-8% in 2003-2011 to 13-15% in 2012-2017. Both methodologies conduct nationwide assessment of forest health status in a fast and automatized way with high accuracy and minimal costs. The weaknesses are, for example, a high computational demands for production cloud free mosaics, inability to identify initial phases of forest health decline, exclusion of stands older than 80 years (in the Czech Republic) and inability to differentiate between harvested and severely damaged stands (in Slovakia). Finally, the paper outlines future development of both methodologies. © 2018 Ivan Barka et al. published by Sciendo.","","Defoliation; Forest health status; Landsat; Satellite scenes; Sentinel-2","Article","Final","","Scopus","2-s2.0-85058472951"
"Ka M.-H.; Shimkin P.E.; Baskakov A.I.; Babokin M.I.","Ka, Min-Ho (15834471000); Shimkin, Pavel E. (57197844463); Baskakov, Aleksandr I. (7006836990); Babokin, Mikhail I. (57197845525)","15834471000; 57197844463; 7006836990; 57197845525","A new single-pass SAR interferometry technique with a single-antenna for terrain height measurements","2019","Remote Sensing","11","9","1070","","","","10.3390/rs11091070","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065744693&doi=10.3390%2frs11091070&partnerID=40&md5=a55603a1bb02880ea2eb0f50375496d4","One of the prospective research topics in radar remote sensing technology is the methodology for designing an optimal radar system for high-precision two-dimensional and three-dimensional image acquisition of the Earth's surface with minimal hardware requirements. In this study, we propose a single-pass interferometric synthetic aperture radar (SAR) imaging technique with only a single antenna for the estimation of the terrain height. This technique enabled us to obtain terrain height information in one flight of the carrier, on which only one receiving antenna was mounted. This single-antenna single-pass interferometry required a squint angle look geometry and additional image synthesis processing. The limiting accuracy of the terrain height measurement was approximately 1.5 times lower than that of the conventional two-pass mode and required a longer baseline than two-pass interferometry to have an equivalent accuracy performance. This imaging method could overcome the temporal decorrelation problem of two-pass interferometry due to a short time gap in the radar echo acquisitions during two sub-aperture intervals. We compared the accuracy performance of the terrain height measurements of our method with the conventional two-pass interferometry. This comparison was carried out at various spectral bandwidths, degrees of surface roughness, and baseline lengths. We validated our idea with numerical simulations of a digital elevation map, and showed real extracted data of the terrain heights in the Astrakhan and Volga regions of the Russian Federation, obtained from airborne SAR with our single-antenna single-pass interferometry technique. © 2019 by the authors.","Imaging techniques; Landforms; Radar antennas; Radar imaging; Receiving antennas; Remote sensing; Surface roughness; Synthetic aperture radar; Interferometric synthetic aperture radars; Limiting accuracy; Single antenna; Single pass; Single-pass interferometry; Single-pass SAR interferometry; Squint angles; Three-dimensional image acquisition; Interferometry","Interferometry; Limiting accuracy; Remote sensing; Single antenna; Single-pass; Squint angle; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85065744693"
"Bejiga M.B.; Hoxha G.; Melgani F.","Bejiga, Mesay Belete (57192697078); Hoxha, Genc (57213198314); Melgani, Farid (35613488300)","57192697078; 57213198314; 35613488300","Retro-Remote Sensing with Doc2Vec Encoding","2020","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium, M2GARSS 2020 - Proceedings","","","9105139","89","92","3","10.1109/M2GARSS47143.2020.9105139","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086743107&doi=10.1109%2fM2GARSS47143.2020.9105139&partnerID=40&md5=922348b5f0dd3c64fbbfe33476e00728","In this work, we attempt to address the issue of developing a sophisticated text encoder for retro-remote sensing application. The encoder converts ancient landscape descriptions into a fixed-size vector that, adequately, represents the available information. This vector is then used as a conditioning data to a Generative adversarial network (GAN) that synthesizes the equivalent image. We propose using a pre-trained Doc2Vec encoder for text encoding and train a Wasserstein GAN (a variant of GAN) to convert landscape descriptions written by travelers and geographers into the equivalent image. Qualitative and quantitative analysis of the generated images signify usefulness of the proposed method. © 2020 IEEE.","Encoding (symbols); Geology; Signal encoding; Adversarial networks; Fixed size; Qualitative and quantitative analysis; Remote sensing applications; Text encoding; Remote sensing","Deep learning; Generative adversarial networks; Retro-remote sensing; Text embedding; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85086743107"
"Li G.; Yang H.; Wang J.; Li Y.; Zhang C.; Xie H.; Feng B.","Li, Gaoyuan (57223049231); Yang, Haitao (57218121328); Wang, Jinyu (57223050320); Li, Yang (57223054201); Zhang, Changgong (57223049318); Xie, Haiping (57223038193); Feng, Bodi (57223052143)","57223049231; 57218121328; 57223050320; 57223054201; 57223049318; 57223038193; 57223052143","PCA-based Wavelet Remote Sensing Image Synthesis Simulation Method","2021","IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","","","9390650","1042","1046","4","10.1109/IAEAC50856.2021.9390650","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104571807&doi=10.1109%2fIAEAC50856.2021.9390650&partnerID=40&md5=6715d01cdc60040dc1c4617181a3bab6","The traditional image mosaic method directly replaces the remote sensing target image into the corresponding area of the background image, but splicing traces are prone to appear. This paper proposes a method of fusion of remote sensing background and target images under wavelet multi-frequency based on principal component analysis. First, the remote sensing background and target image are decomposed by wavelet, and then the fusion weight of the image wavelet low-frequency coefficient is calculated by the principal component analysis method, and the high-frequency coefficient is fused by the absolute maximum method, and finally the wavelet fusion coefficient is inversely transformed to realize the image synthesis simulation. The simulation results show that the fusion image can not only highlight the target features, but also has no stitching traces. © 2021 IEEE.","Image analysis; Image fusion; Background image; High frequency HF; Image synthesis; Multi frequency; Principal component analysis method; Remote sensing images; Target feature; Wavelet fusion; Remote sensing","image fusion; image geometric transformation; principal component analysis; wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85104571807"
"Nechyporuk M.V.; Pavlikov V.V.; Sobkolov A.D.; Tserne E.O.; Volosyuk V.K.; Zhyla S.S.","Nechyporuk, M.V. (57210558126); Pavlikov, V.V. (23397933100); Sobkolov, A.D. (57192199954); Tserne, E.O. (57218704755); Volosyuk, V.K. (7003358846); Zhyla, S.S. (57207914339)","57210558126; 23397933100; 57192199954; 57218704755; 7003358846; 57207914339","Aperture synthesis of surface images using active remote sensing with ultra-wideband stochastic signals","2020","Telecommunications and Radio Engineering (English translation of Elektrosvyaz and Radiotekhnika)","79","15","","1327","1347","20","10.1615/TELECOMRADENG.V79.I15.30","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111105883&doi=10.1615%2fTELECOMRADENG.V79.I15.30&partnerID=40&md5=67b73226744cff1d49d3f52b55aa2702","Mathematical models for the fields of stochastic ultra-wideband signals, which are essential for solving problems of aperture image synthesis using active radar methods, are presented. The expediency of applying V-transforms in such problems, the effectiveness of which has already been substantiated in the mathematical description of ultra-wideband spatio-temporal fields in problems of passive radiolocation and radio astronomy, is justified. The spectral-correlation characteristics of ultra-wideband fields and the complex functions of spatio-temporal coherence in their connection with coherent and incoherent images of the studied objects are determined in accordance with the problems of active aperture synthesis. © 2020 by Begell House, Inc.","Image processing; Mathematical transformations; Radio astronomy; Radio navigation; Remote sensing; Stochastic models; Stochastic systems; Aperture synthesis; Complex functions; Image synthesis; Mathematical descriptions; Passive radiolocation; Spectral correlation; Stochastic signals; Ultra wideband signals; Ultra-wideband (UWB)","Active aperture synthesis; Radar imaging; V-transforms","Article","Final","","Scopus","2-s2.0-85111105883"
"Baier G.; Deschemps A.; Schmitt M.; Yokoya N.","Baier, Gerald (57188720676); Deschemps, Antonin (57221248595); Schmitt, Michael (7401931279); Yokoya, Naoto (36440631200)","57188720676; 57221248595; 7401931279; 36440631200","Synthesizing Optical and SAR Imagery from Land Cover Maps and Auxiliary Raster Data","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3068532","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104617195&doi=10.1109%2fTGRS.2021.3068532&partnerID=40&md5=4f1d774df6d33691b5d314efc9f28290","We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fréchet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth, and we published the newly created high-resolution data set at https://ieee-dataport.org/open-access/geonrw.  © 1980-2012 IEEE.","Data fusion; HTTP; Radar imaging; Rasterization; Remote sensing; Synthetic aperture radar; Water levels; Adversarial networks; Auxiliary information; Digital elevation model; High resolution data; Information contents; Remote sensing images; Segmentation models; Spatially adaptive; land cover; mapping method; optical method; radar imagery; raster; synthetic aperture radar; Image processing","Deep learning; generative adversarial network (GAN); image synthesis; synthetic aperture radar (SAR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104617195"
"Koskowich B.; Starek M.","Koskowich, Bradley (57207878528); Starek, Michael (23494042900)","57207878528; 23494042900","Extracting Camera Pose Using Single Image Super Resolution Networks","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323098","1873","1876","3","10.1109/IGARSS39084.2020.9323098","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101997914&doi=10.1109%2fIGARSS39084.2020.9323098&partnerID=40&md5=0a063efd880e04b4685f656e0f9de40f","This work proposes a mechanism which can be used as a basis for allowing camera POSE information to be maintained reliably during loss or interference with inertial motion unit or positioning system integration. This basis is formed by employing image synthesis networks with atypical data for the network type: inputs are normal down scaled source imagery while outputs are native resolution images composed of the contents of the same scene viewed from a fixed offset position. The goal of this application is to simulate the presence of a binary camera from monocular hardware, which makes feasible certain POSE estimation workflows which would normally require binary cameras on monocular platforms. Being able to rapidly synthesize images of additional camera positions without having to physically navigate to those positions allows for two methods to build off each other. First, knowing that the model should consistently maintain a specific POSE from the source camera allows synthetic images to be used to artificially inflate available data during structure from motion processing with confidence in the accuracy of synthetic points. It also enables the comparison of an image at an actual physical location with the synthetic one later as a measure of POSE accuracy which can be incorporated into a solution for computing POSE of the image source. © 2020 IEEE.","Cameras; Geology; Remote sensing; Camera positions; Inertial motions; Physical locations; Pose information; Positioning system; Resolution images; Structure from motion; Synthetic images; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85101997914"
"Humeau-Heurtier A.","Humeau-Heurtier, Anne (54793024000)","54793024000","Color Texture Analysis: A Survey","2022","IEEE Access","10","","","107993","108003","10","10.1109/ACCESS.2022.3213439","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139873119&doi=10.1109%2fACCESS.2022.3213439&partnerID=40&md5=28cad417009c015368167142c727fbe3","In the field of image processing, texture features and color are fundamental visual cue with complementary roles. They are used in many applications and in a large variety of areas such as quality control, content-based image retrieval, remote sensing, industrial inspection, surface inspection, object recognition, and medical image analysis. For this purpose, a large number of algorithms have been proposed for texture feature extraction. Some of them are dedicated to gray-scale images while others aim at processing both color and texture. It has been shown that, for many cases, the use of color improves the performance of gray level texture classification. This paper provides a comprehensive survey of the texture feature extraction methods that consider both texture and color information. We propose a categorization of these methods into seven classes, two of them being very recent. For each method, we present the concept, the advantages and drawbacks, and we give examples of application.  © 2013 IEEE.","Color; Color image processing; Content based retrieval; Extraction; Feature extraction; Image analysis; Image segmentation; Image texture; Inspection; Luminance; Medical imaging; Object recognition; Remote sensing; Textures; Chrominance; Color textures; Colored noise; Features extraction; Histogram; Image color analysis; Images processing; Images synthesis; Segmentation; Shape from texture; Surveys","Chrominance; classification; color texture; feature extraction; image processing; image synthesis; luminance; segmentation; shape from texture; texture","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139873119"
"","","","7th National Conference on Computer Vision, Pattern Recognition, Image Processing, and Graphics, NCVPRIPG 2019","2020","Communications in Computer and Information Science","1249","","","","","628","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097306523&partnerID=40&md5=5db10814e11e6467512863b6bf65fbfa","The proceedings contain 58 papers. The special focus in this conference is on Computer Vision, Pattern Recognition, Image Processing, and Graphics. The topics include: Deep Learn Bananas: A Transfer Learning for Banana Variety Classification; a Framework for Lane Prediction Based on Vehicle Detection and Tracking; detector-SegMentor Network for Skin Lesion Localization and Segmentation; choroid Disease Classification Using Convolutional Neural Network; hyper Vision Net: Kidney Tumor Segmentation Using Coordinate Convolutional Layer and Attention Unit; texture Classification by Local Rajan Transform Based Descriptor; domain Decomposition Based Preconditioned Solver for Bundle Adjustment; deep Dictionary Learning for Inpainting; A Robust Pose Transformational GAN for Pose Guided Person Image Synthesis; structure Preserving Image Inpainting Using Edge Priors with Contextual Attention; preface; fast Stereo Depth Estimation in Smartphone Devices with Narrow Baseline; exploring Temporal Differences in 3D Convolutional Neural Networks; PoshakNet: Framework for Matching Dresses from Real-Life Photos Using GAN and Siamese Network; unsupervised Domain Adaptation for Remote Sensing Images Using Metric Learning and Correlation Alignment; computationally Efficient Super-Resolution Approach for Real-World Images; Accurate Damage Dimension Estimation in AI Driven Vehicle Inspection System; a Deep Learning Based Framework for Distracted Driver Detection; RECAL: Reuse of Established CNN Classifier Apropos Unsupervised Learning Paradigm; Pose Estimation of UAVs Using Stereovision; U-RME: Underwater Refined Motion Estimation in Hazy, Cluttered and Dynamic Environments; emphasizing Similar Feature Representations to Defend Against Adversarial Attacks; single Storage Semi-Global Matching for Real Time Depth Processing; iSalGAN - An Improvised Saliency GAN; putting Jewellery and Accessories on a 3D Face Model Generated from 2D Image.","","","Conference review","Final","","Scopus","2-s2.0-85097306523"
"Oliveira D.A.B.","Oliveira, Dario A. B. (27567900100)","27567900100","Augmenting Data Using Gaussian Mixture Embedding for Improving Land Cover Segmentation","2020","2020 IEEE Latin American GRSS and ISPRS Remote Sensing Conference, LAGIRS 2020 - Proceedings","","","9165670","333","338","5","10.1109/LAGIRS48042.2020.9165670","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091650824&doi=10.1109%2fLAGIRS48042.2020.9165670&partnerID=40&md5=b3eccc9c3df9acdec6e5af3fd98afa3f","The use of convolutional neural networks improved greatly data synthesis in the last years and have been widely used for data augmentation in scenarios where very imbalanced data is observed, such as land cover segmentation. Balancing the proportion of classes for training segmentation models can be very challenging considering that samples where all classes are reasonably represented might constitute a small portion of a training set, and techniques for augmenting this small amount of data such as rotation, scaling and translation might be not sufficient for efficient training. In this context, this paper proposes a methodology to perform data augmentation from few samples to improve the performance of CNN-based land cover semantic segmentation. First, we estimate the latent data representation of selected training samples by means of a mixture of Gaussians, using an encoder-decoder CNN. Then, we change the latent embedding used to generate the mixture parameters, at random and in training time, to generate new mixture models slightly different from the original. Finally, we compute the displacement maps between the original and the modified mixture models, and use them to elastically deform the original images, creating new realistic samples out of the original ones. Our disentangled approach allows the spatial modification of displacement maps to preserve objects where deformation is undesired, like buildings and cars, where geometry is highly discriminant. With this simple pipeline, we managed to augment samples in training time, and improve the overall performance of two basal semantic segmentation CNN architectures for land cover semantic segmentation.  © 2020 IEEE.","Convolutional neural networks; Embeddings; Historic preservation; Image segmentation; Semantics; Data augmentation; Data representations; Displacement maps; Gaussian mixtures; Mixture of Gaussians; Rotation , scaling and translations; Segmentation models; Semantic segmentation; Remote sensing","Gaussian Mixture Models.; Image Synthesis; Land Cover Segmentation; Latent Data Representation","Conference paper","Final","","Scopus","2-s2.0-85091650824"
"Humeau-Heurtier A.","Humeau-Heurtier, Anne (54793024000)","54793024000","Texture feature extraction methods: A survey","2019","IEEE Access","7","","8600329","8975","9000","25","10.1109/ACCESS.2018.2890743","194","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060738375&doi=10.1109%2fACCESS.2018.2890743&partnerID=40&md5=c7a6c505ca49273c2314db2e733ff847","Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract - from raw images - meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet. © 2013 IEEE.","Affine transforms; Classification (of information); Extraction; Feature extraction; Graphic methods; Image processing; Image segmentation; Medical imaging; Remote sensing; Surveys; Textures; Feature extraction methods; Image synthesis; Learning-based approach; Low computational complexity; Perspective transforms; Shape from texture; Texture classification; Texture feature extraction; Image texture","Classification; feature extraction; image processing; image synthesis; segmentation; shape from texture; texture","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85060738375"
"Bejiga M.B.; Melgani F.; Vascotto A.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300); Vascotto, Antonio (57208124510)","57192697078; 35613488300; 57208124510","Retro-Remote Sensing: Generating Images from Ancient Texts","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","3","8660422","950","960","10","10.1109/JSTARS.2019.2895693","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063894914&doi=10.1109%2fJSTARS.2019.2895693&partnerID=40&md5=300d238b0b3891057325948999c8e038","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area. © 2008-2012 IEEE.","Deep learning; Deep neural networks; Image processing; Neural networks; Adversarial networks; Convolutional neural network; Geographical area; Image synthesis; Multi-modal learning; Multimodal sources; Remote sensing images; Statistical properties; artificial neural network; civilization; geographical variation; image analysis; learning; pixel; remote sensing; Remote sensing","Convolutional neural networks (CNN); deep learning; generative adversarial networks (GAN); multimodal learning; remote sensing; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85063894914"
"Zexing Z.; Qizhi X.; Haibo W.; Wenyong Y.","Zexing, Zhao (57202311139); Qizhi, Xu (50562407300); Haibo, Wang (57681846600); Wenyong, Yu (57202309456)","57202311139; 50562407300; 57681846600; 57202309456","High-reflectivity objects distributed optical satellite image fusion based on NDVI classification","2017","Proceedings of 2017 2nd International Conference on Frontiers of Sensors Technologies, ICFST 2017","2017-January","","","231","235","4","10.1109/ICFST.2017.8210509","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047793500&doi=10.1109%2fICFST.2017.8210509&partnerID=40&md5=eccca03a328a30ceba7bc11aea7a449a","Ratioing method is one type of the most famous fusion methods in remote sensing image fusion domain. Generally, the ratioing method synthesizes a low-resolution panchromatic (Pan) image by adaptive weighted summation of a multispectral (MS) image. Consequently, the accuracy of the weights for low-resolution Pan image synthesis is of great importance. However, in most cases, the optical satellite images contain lots of high-reflectivity objects, such as clouds covered regions, and high-reflectivity buildings. These objects are saturate due to their strong reflectance. The distortion of saturated objects results in the failure of weights calculation, so that causes the color distortion of fused images. To solve the problem, this paper proposes a high-reflectivity objects distributed optical satellite image fusion method based on NDVI classification. First, the NDVI index is employed to classify the pixels of a MS image into high-reflectivity group and normal group, then the pixels in normal group is used to calculate the weighted coefficients, finally the fused image is obtained by ratioing transform. Experimental results on a large number of test images show that the proposed method has good performance on reducing color distortion. © 2017 IEEE.","Adaptive optics; Image classification; Pixels; Reflection; Remote sensing; Satellites; Multispectral images; NDVI index; Optical satellite images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Weighted coefficients; Weights calculation; Image fusion","Image fusion; NDVI index; Pan-sharpening; Remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85047793500"
"Bejiga M.B.; Melgani F.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300)","57192697078; 35613488300","Towards Generating Remote Sensing Images of the Far Past","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899834","9502","9505","3","10.1109/IGARSS.2019.8899834","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077699173&doi=10.1109%2fIGARSS.2019.8899834&partnerID=40&md5=ca6577d1479893c8d5b6ea3d7f491233","Text-to-image synthesis is a research topic that has not yet been addressed by the remote sensing community. It consists in learning a mapping from text description to image pixels. In this paper, we propose to address this topic for the very first time. More specifically, our objective is to convert ancient text descriptions of geographic areas written by past explorers into an equivalent remote sensing image. To this effect, we rely on generative adversarial networks (GANs) to learn the mapping. GANs aim to represent the distribution of a dataset using weights of a deep neural network, which are trained as an adversarial competition between two networks. We collected ancient texts dating back to 7 BC to train our network and obtained interesting results, which form the basis to highlight future research directions to advance this new topic. © 2019 IEEE.","Deep neural networks; Geology; Image processing; Mapping; Adversarial networks; Future research directions; GANs; Geographic areas; Image pixels; Image synthesis; Remote sensing images; Research topics; Remote sensing","GANs; remote sensing; text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85077699173"
"Chen F.; Xing Q.; Fan C.","Chen, Feng (57221088955); Xing, Qinghua (57324650500); Fan, Chengli (55818690500)","57221088955; 57324650500; 55818690500","Multilevel Strong Auxiliary Network for Enhancing Feature Representation to Protect Secret Images","2022","IEEE Transactions on Industrial Informatics","18","7","","4577","4586","9","10.1109/TII.2021.3123233","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118544231&doi=10.1109%2fTII.2021.3123233&partnerID=40&md5=3f6963a2c61ba1f2d9d53c4896ca3be7","Image data play an important role in the network information, however, some images containing sensitive or confidential information are easy to attract the attention of malicious attackers. On the basis of deep learning and data hiding technology, in this article, a novel hiding-revealing network is designed to protect these secret images. The sender uses the hiding network to conceal the secret image into an ordinary cover image and the receiver uses the revealing network to recover the secret image. Symmetrical shortcut connection is designed to improve both the hiding and the revealing performances without adding any parameters. Consider that some secret images may have complex spatial features, a multilevel strong auxiliary module is designed to enhance feature representation and boost the restoration quality of the secret image. Then, a lifeline is proposed to transform the image hiding task into a residual identity mapping, which reduces the difficulty of network learning and obviously improves the hiding performance. In addition, a mixed loss function is designed to further improve the perceptual quality of both the hidden image and the revealed image, which further completely eliminates the secret content in the residual image and ensures the hiding security. Experimental results demonstrate that compared with the state-of-the-art methods, our proposed method achieves the best performance in both hidden image synthesis and secret image restoration.  © 2005-2012 IEEE.","Deep learning; Image enhancement; Image reconstruction; Mapping; Restoration; Steganography; Data hiding technologies; Deep learning; Features extraction; Identity mappings; Image steganography; Multi-level strong auxiliary module; Multilevels; Remote-sensing; Residual identity mapping; Security; Task analysis; Remote sensing","Data hiding technology; deep learning; image steganography; multilevel strong auxiliary (MLSA) module; residual identity mapping","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85118544231"
"Hu S.; Short N.; Gurton K.; Riggan B.","Hu, Shuowen (34881794700); Short, Nathaniel (56940280300); Gurton, Kristan (6603404949); Riggan, Benjamin (56406510300)","34881794700; 56940280300; 6603404949; 56406510300","Overview of polarimetric thermal imaging for biometrics","2018","Proceedings of SPIE - The International Society for Optical Engineering","10655","","1065502","","","","10.1117/12.2299761","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049258818&doi=10.1117%2f12.2299761&partnerID=40&md5=29a19e35e74bf0886eca4600831d1e5e","This paper presents an overview of polarimetric thermal imaging for biometrics, focusing on face recognition, with a short discussion on fingerprints and iris. Face recognition has been and continues to be an active area of biometrics research, with most of the research dedicated to recognition in the visible spectrum. However, face recognition in the visible spectrum is not practical for discrete surveillance in low-light and nighttime scenarios. Polarimetric thermal imaging represents an ideal modality for acquiring the naturally emitted thermal radiation from the human face, providing additional geometric and textural details not available in conventional thermal imagery. One of the main challenges lies in matching the acquired polarimetric thermal facial signature to gallery databases containing only visible facial signature, for interoperability with existing government biometric repositories. This paper discusses approaches and algorithms to exploit polarization information, as represented by the Stokes vectors, through feature extraction and nonlinear regression to enable polarimetric thermal-To-visible face recognition. In addition to cross-spectrum feature based approaches, crossspectrum image synthesis methods are discussed that seek to reconstruct a visible-like image given a polarimetric thermal face image input. Beyond facial biometrics, this paper presents an initial exploration of polarimetric thermal imaging for latent fingerprint acquisition. Latent prints are formed when the oils and sweat from the finger are deposited onto another surface through contact, and are typically collected by first dusting with powder before being imaged and then lifted with adhesive tape. This paper presents polarimetric thermal imagery of latent prints from a nonporous glass surface, acquired without the dusting process. A brief discussion of the utility of polarimetric thermal imaging for iris recognition is also presented. © 2018 SPIE.","Biometrics; Infrared imaging; Petroleum prospecting; Polarimeters; Polarization; Remote sensing; Spectrum analysis; Face; Fingerprint; Initial exploration; Iris; Iris recognition; Latent fingerprint; Non-linear regression; Polarimetric thermal; Face recognition","Biometrics; Face; Fingerprint; Iris; Polarimetric thermal","Conference paper","Final","","Scopus","2-s2.0-85049258818"
"Zhang X.; Yang G.; Yang Y.; Huang J.","Zhang, Xiaohan (57192412934); Yang, Guang (56895369200); Yang, Yongbo (57192414153); Huang, Junhua (57192412813)","57192412934; 56895369200; 57192414153; 57192412813","Band selection method based on spectrum difference in targets of interest in hyperspectral imagery","2016","Proceedings of SPIE - The International Society for Optical Engineering","10156","","101560J","","","","10.1117/12.2244818","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006272247&doi=10.1117%2f12.2244818&partnerID=40&md5=9f216cc034747390a4c3435a30fd38a1","While hyperspectral data shares rich spectrum information, it has numbers of bands with high correlation coefficients, causing great data redundancy. A reasonable band selection is important for subsequent processing. Bands with large amount of information and low correlation should be selected. On this basis, according to the needs of target detection applications, the spectral characteristics of the objects of interest are taken into consideration in this paper, and a new method based on spectrum difference is proposed. Firstly, according to the spectrum differences of targets of interest, a difference matrix which represents the different spectral reflectance of different targets in different bands is structured. By setting a threshold, the bands satisfying the conditions would be left, constituting a subset of bands. Then, the correlation coefficients between bands are calculated and correlation matrix is given. According to the size of the correlation coefficient, the bands can be set into several groups. At last, the conception of normalized variance is used on behalf of the information content of each band. The bands are sorted by the value of its normalized variance. Set needing number of bands, and the optimum band combination solution can be get by these three steps. This method retains the greatest degree of difference between the target of interest and is easy to achieve by computer automatically. Besides, false color image synthesis experiment is carried out using the bands selected by this method as well as other 3 methods to show the performance of method in this paper. © 2016 SPIE.","Correlation methods; Environmental engineering; Environmental technology; Matrix algebra; Remote sensing; Spectroscopy; Bands selections; Hyper-spectral imageries; Normalized variance; Spectrum difference; Targets of interest; Safety testing","Bands selection; Correlation; Hyperspectral imagery; Normalized variance; Spectrum difference; Targets of interest","Conference paper","Final","","Scopus","2-s2.0-85006272247"
"Liu S.-B.; Wang J.-H.; Yuan R.-Y.; Zhao W.-X.; Li L.; Wang Q.-H.","Liu, Shu-Bin (57218270558); Wang, Jin-Hui (57195838481); Yuan, Rong-Ying (57195836879); Zhao, Wu-Xiang (16023511500); Li, Lei (56140809100); Wang, Qiong-Hua (22936217300)","57218270558; 57195838481; 57195836879; 16023511500; 56140809100; 22936217300","Real-time and ultrahigh accuracy image synthesis algorithm for full field of view imaging system","2020","Scientific Reports","10","1","12389","","","","10.1038/s41598-020-69353-9","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088521336&doi=10.1038%2fs41598-020-69353-9&partnerID=40&md5=4d90e1abd9c144fb9cd3d405d8e59cae","In this paper, we propose a real time, ultrahigh accuracy and full-field-of-view (RUF) algorithm for full field of view (FOV) imaging system. The proposed algorithm combines rough matching and precise matching method to stitch multiple images with the whole FOV in short time and high imaging quality. In order to verify real-time imaging effect of RUF algorithm, we also fabricate a multi-camera imaging system which includes 19 independent cameras. And the experiment result practically illustrates that full-FOV system can achieve good performances under a near-limiting FOV of 360° × 240° with low distortion, meanwhile, optical resolution reaches up to 95 megapixels. 100% registration-accuracy RUF algorithm for imaging in one second can be widely applied to any optical imaging engineering field with large FOV, such as remote sensing imaging, microscopy imaging, monitoring system engineering fields and so on. © 2020, The Author(s).","","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088521336"
"Duan P.; Hu S.; Kang X.; Li S.","Duan, Puhong (57188576823); Hu, Shangsong (57881065800); Kang, Xudong (47061561600); Li, Shutao (7409240361)","57188576823; 57881065800; 47061561600; 7409240361","Shadow Removal of Hyperspectral Remote Sensing Images With Multiexposure Fusion","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5537211","","","","10.1109/TGRS.2022.3203808","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137573461&doi=10.1109%2fTGRS.2022.3203808&partnerID=40&md5=6f01960e0d5c8ae83b505dd77cf179d7","Shadow removal is a challenging problem in hyperspectral remote sensing images due to its spatial-variant properties and diverse patterns. In this work, a shadow removal framework with multiexposure fusion is proposed for hyperspectral remote sensing images, which consists of three major steps. First, a color space conversion method is exploited to detect the shadow regions. Second, the principle of the intrinsic decomposition model is utilized to generate a set of differently exposed hyperspectral images (HSIs), i.e., multiexposure images. Third, the generated multiexposure images and the original HSIs are fused together with a two-stage image fusion method so as to remove the shadows in hyperspectral remote sensing images effectively. Experiments performed on three real hyperspectral datasets confirm that the performance of the proposed method outperforms other state-of-the-art shadow removal approaches.  © 1980-2012 IEEE.","Color; Color image processing; Edge detection; Hyperspectral imaging; Image fusion; Remote sensing; Space optics; Spectroscopy; Color space conversion; Exposure fusions; Image color analysis; Image decomposition; Image edge detection; Images synthesis; Intrinsic image decomposition; Intrinsic images; Multi exposure; Multi-exposure fusion; Shadow removal; Two-stage image fusion; decomposition analysis; image analysis; Image reconstruction","Color space conversion; intrinsic image decomposition; multiexposure fusion; shadow removal; two-stage image fusion","Article","Final","","Scopus","2-s2.0-85137573461"
"Willis C.J.; Pilgrim A.; Li E.K.C.","Willis, Chris J. (57196925111); Pilgrim, Alan (23012421900); Li, Emma K.C. (57214094958)","57196925111; 23012421900; 57214094958","SAR image simulation for performance assessment","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","1115519","","","","10.1117/12.2532574","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078180040&doi=10.1117%2f12.2532574&partnerID=40&md5=8e605cb985b3620a04c63f5553bdf370","Performance assessment of image processing systems may be carried out using large volumes of data with known ground truth. Unfortunately such data, collected in sensor trials, can be challenging to source for many problems of interest. In particular, trials collection may require the acquisition of imagery in a range of scenario settings, imaging geometries and environmental conditions. An alternative to trials data collection uses synthetically generated imagery of objects and environments configured into realistic scenarios. For performance assessment of image processing chains, large volumes of synthetic imagery may be required in order to characterise individual algorithmic steps or for complete system assessment. In order to generate sufficiently large volumes for such characterisations the simulation approach must also be fast to execute. This paper presents a process for the generation of simulated Synthetic Aperture Radar (SAR) imagery which is fit-for-purpose for the task of algorithm and systems performance assessment of image processing for Automatic Target Detection, Recognition and Identification (ATDRI) tasks. The approach taken is based on the exploitation of computational geometry primitives. It uses a simplified imaging model and correctly treats both layover effects and shadowed regions on both the target object and within the background region. For speed and simplicity the simulation process synthesises single bounce reflections only. This means that the simulation is effective only up to the intermediate resolutions which are typically used for ATDRI applications. The input models are comprised of three-dimensional triangulations representing the geometric structure of the scene content, with each triangle having a parameterised scattering response based on distributional models often used for SAR imagery. The synthesis process generates a collection of two-dimensional arrays of distributional parameters of the same size as the image to be produced. It is straightforward to use these to generate representations of, for example, mean scattering response, or realistic-looking simulated SAR images with speckle 'noise'. Results are presented for different scene content and sensor configurations, including target aspect and sensor depression angles. © 2019 SPIE.","Computational geometry; Image processing; Radar target recognition; Remote sensing; Synthetic aperture radar; ATDRI; Automatic target detection; Image simulations; Image synthesis; Performance assessment; Radar imaging","ATDRI; Automatic Target Detection; Image simulation; Image synthesis; Performance assessment; Recognition & Identification; SAR; Synthetic Aperture Radar","Conference paper","Final","","Scopus","2-s2.0-85078180040"
"Kuznetcov A.; Svetelkin P.","Kuznetcov, A. (56513917800); Svetelkin, P. (56520415600)","56513917800; 56520415600","Integration of multispectral images from modern earth remote sensing systems","2015","2015 5th International Workshop on Computer Science and Engineering: Information Processing and Control Engineering, WCSE 2015-IPCE","","","","","","","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939529560&partnerID=40&md5=f7e6506f55910c22bdac594382a3d6ff","This paper shows algorithms of color image acquisition by the use of a panchromatic high resolution image and multispectral lower resolution images. The algorithm of artifact elimination from moving and high objects was developed. The algorithm of color image synthesis in true colors was developed on the basis of the NDVI index.","Algorithms; Color; Image acquisition; Earth remote sensing; High resolution image; Lower resolution; Moving objects; Multi-spectral; Multispectral images; Pan-sharpening; True colors; Remote sensing","Moving objects elimination; Pansharpening; True color images","Conference paper","Final","","Scopus","2-s2.0-84939529560"
"Li J.; Chen Z.; Zhao X.; Shao L.","Li, Jingtao (57217013418); Chen, Zhanlong (26322275400); Zhao, Xiaozhen (57217013673); Shao, Lijia (57217014471)","57217013418; 26322275400; 57217013673; 57217014471","MAPGAN: An intelligent generation model for network tile maps","2020","Sensors (Switzerland)","20","11","3119","","","","10.3390/s20113119","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729281&doi=10.3390%2fs20113119&partnerID=40&md5=11c206c75755ee4a1045444f289c8a7b","In recent years, the generative adversarial network (GAN)-based image translation model has achieved great success in image synthesis, image inpainting, image super-resolution, and other tasks. However, the images generated by these models often have problems such as insufficient details and low quality. Especially for the task of map generation, the generated electronic map cannot achieve effects comparable to industrial production in terms of accuracy and aesthetics. This paper proposes a model called Map Generative Adversarial Networks (MapGAN) for generating multitype electronic maps accurately and quickly based on both remote sensing images and render matrices. MapGAN improves the generator architecture of Pix2pixHD and adds a classifier to enhance the model, enabling it to learn the characteristics and style differences of different types of maps. Using the datasets of Google Maps, Baidu maps, and Map World maps, we compare MapGAN with some recent image translation models in the fields of one-to-one map generation and one-to-many domain map generation. The results show that the quality of the electronic maps generated by MapGAN is optimal in terms of both intuitive vision and classic evaluation indicators. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Maps; Remote sensing; Adversarial networks; Evaluation indicators; Image Inpainting; Image super resolutions; Image synthesis; Image translation; Industrial production; Remote sensing images; article; classifier; human; human experiment; remote sensing; vision; Image processing","Deep generation model; Image translation; Map generation; Network tile map","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085729281"
"Bejiga M.B.; Hoxha G.; Melgani F.","Bejiga, Mesay Belete (57192697078); Hoxha, Genc (57213198314); Melgani, Farid (35613488300)","57192697078; 57213198314; 35613488300","Improving Text Encoding for Retro-Remote Sensing","2021","IEEE Geoscience and Remote Sensing Letters","18","4","9066830","622","626","4","10.1109/LGRS.2020.2983851","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103405336&doi=10.1109%2fLGRS.2020.2983851&partnerID=40&md5=191c3564bbabf57658d20c378cee4ded","A recent work on retro-remote sensing (converting ancient text descriptions into images) was proposed using a multilabel encoding scheme in which an input text description is represented by a binary vector indicating the presence or absence of specific objects. However, this kind of encoding disregards information such as object attributes and spatial relationship between multiple objects in a description, resulting in images that do not semantically (fully) conform to the input description. In this letter, we propose an improved text-encoding mechanism that takes into account different levels of information available from an input text. The encoded text is then used as conditional information to guide the image synthesis process using generative adversarial networks (GANs). Besides, we present a modified GAN architecture intending to improve the semantic content of the generated images. Both the qualitative and quantitative results obtained indicate that the proposed method is particularly promising. © 2004-2012 IEEE.","Encoding (symbols); Image coding; Remote sensing; Semantics; Signal encoding; Adversarial networks; Encoding schemes; Image synthesis; Multiple objects; Object attributes; Quantitative result; Semantic content; Spatial relationships; image analysis; remote sensing; satellite imagery; spatial analysis; Image enhancement","Generative adversarial networks (GANs); multimodal learning; retro-remote sensing; text-To-image synthesis","Article","Final","","Scopus","2-s2.0-85103405336"
"Du W.-L.; Zhou Y.; Zhao J.; Tian X.; Yang Z.; Bian F.","Du, Wen-Liang (55265123100); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Tian, Xiaolin (7202380154); Yang, Zhi (57284929100); Bian, Fuqiang (57202867285)","55265123100; 35480110700; 57138970300; 7202380154; 57284929100; 57202867285","Exploring the Potential of Unsupervised Image Synthesis for SAR-Optical Image Matching","2021","IEEE Access","9","","9427486","71022","71033","11","10.1109/ACCESS.2021.3079327","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105853208&doi=10.1109%2fACCESS.2021.3079327&partnerID=40&md5=3ebbd56086511abe8160d0a847102a35","We consider SAR-optical image matching problems, where correspondences are acquired from a pair of SAR and optical images. Recent methods for such a problem typically simplify the SAR-optical image matching to the SAR-SAR or optical-optical image matchings using supervised-image-synthesis methods. However, training supervised-image-synthesis needs plenty of aligned SAR-optical image pairs while gathering sufficient amounts of aligned multi-modal image pairs is challenging in remote sensing. In this work, we investigate the applicability of unsupervised-image-synthesis for SAR-optical image matching such that the unaligned SAR-optical images could be used. To this end, we apply feature matching loss to a well known unsupervised-image-synthesis method, i.e., CycleGAN, to enforce the feature matching consistency. Moreover, we develop a shared-matching-strategy to improve the results of SAR-optical image matching further. Qualitative comparisons against CycleGAN, StarGAN, and DualGAN demonstrate the superiority of our approach. Quantitative results show that, compared with CycleGAN, StarGAN, and DualGAN, our method obtains at least 2.6 times more qualified SAR-optical matchings.  © 2013 IEEE.","Geometrical optics; Image enhancement; Image matching; Remote sensing; Feature matching; Image synthesis; Matching problems; Matchings; Multi-modal image; Optical image; Quantitative result; Radar imaging","generative adversarial networks (GANs); Image matching; synthetic aperture radar (SAR); unsupervised-image-synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105853208"
"Huang H.; Zhang F.; Zhou Y.; Yin Q.; Hu W.","Huang, Henghua (57219442515); Zhang, Fan (56320587700); Zhou, Yongsheng (22959334700); Yin, Qiang (36959885000); Hu, Wei (56316293300)","57219442515; 56320587700; 22959334700; 36959885000; 56316293300","High resolution sar image synthesis with hierarchical generative adversarial networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900494","2782","2785","3","10.1109/IGARSS.2019.8900494","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082673599&doi=10.1109%2fIGARSS.2019.8900494&partnerID=40&md5=a10eaaed3665d0fd8aacfcae65f8bade","Generative adversarial network (GAN) is an artificial neural network based on unsupervised learning method. Due to its powerful model representation capabilities, GAN has been introduced to synthesize synthetic aperture radar (SAR) image data, for the real sample is difficult to acquire. Largescale, high-resolution SAR images play an important role in promoting SAR applications, such as automatic target recognition and image interpretation. However, on account of the difficult training problem of GAN network, especially for SAR images with speckle noise, it is difficult to obtain high-resolution SAR images by simply transfer the net from optical image. Recent studies in other image fields have shown that hierarchical structure is an effective and useful way to decompose a generation task into several smaller subtasks. How to obtain more high-resolution SAR images from limited original samples through GAN is the target of our research. Therefore, in this paper, we introduce a hierarchical GAN network model to generate SAR images, through the multi-stage network, gradually improve the quality of the generated image, and finally obtain highresolution images. The type and aspect of generated images are determined by the input of condition vectors in the last two stages. In addition, we introduce the triple loss, in which the background loss is used to imitating background clutter noise of SAR image, the condition loss is to make the generated images' type and aspect become controllable, and the global loss for getting higher image generation quality. The generated images show high similarity with the real samples.  © 2019 IEEE.","Automatic target recognition; Geometrical optics; Image enhancement; Learning systems; Radar target recognition; Remote sensing; Synthetic aperture radar; Unsupervised learning; Adversarial networks; Hierarchical structures; High resolution image; High-resolution SAR; Image interpretation; Model representation; Synthetic aperture radar (SAR) images; Unsupervised learning method; Radar imaging","Automatic target recognition (ATR); Generative adversarial network(GAN); SAR simulator; Synthetic aperture radar (SAR); Triple loss","Conference paper","Final","","Scopus","2-s2.0-85082673599"
"Zhang Y.; Wang C.; Chen J.; Wang F.","Zhang, Yi (57203829285); Wang, Chengyi (57129401700); Chen, Jingbo (56386393100); Wang, Futao (56415234000)","57203829285; 57129401700; 56386393100; 56415234000","Shape-Constrained Method of Remote Sensing Monitoring of Marine Raft Aquaculture Areas on Multitemporal Synthetic Sentinel-1 Imagery","2022","Remote Sensing","14","5","1249","","","","10.3390/rs14051249","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126285516&doi=10.3390%2frs14051249&partnerID=40&md5=33cc2687117b46356127f867c7e1d0db","Large-scale and periodic remote sensing monitoring of marine raft aquaculture areas is significant for scientific planning of their layout and for promoting sustainable development of marine ecology. Synthetic aperture radar (SAR) is an important tool for stable monitoring of marine raft aquaculture areas since it is all-weather, all-day, and cloud-penetrating. However, the scattering signal of marine raft aquaculture areas is affected by speckle noise and sea state, so their features in SAR images are complex. Thus, it is challenging to extract marine raft aquaculture areas from SAR images. In this paper, we propose a method to extract marine raft aquaculture areas from Sentinel-1 images based on the analysis of the features for marine raft aquaculture areas. First, the data are preprocessed using multitemporal phase synthesis to weaken the noise interference, enhance the signal of marine raft aquaculture areas, and improve the significance of the characteristics of raft aquaculture areas. Second, the geometric features of the marine raft aquaculture area are combined to design the model structure and introduce the shape constraint module, which adds a priori knowledge to guide the model convergence direction during the training process. Experiments verify that the method outperforms the popular semantic segmentation model with an F1 of 84.52%. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Aquaculture; Marine biology; Ocean currents; Radar imaging; Remote sensing; Semantic Segmentation; Semantics; Constrained method; Images synthesis; Large-scales; Monitoring of mariculture; Multi-temporal; Remote sensing monitoring; Scientific planning; Semantic segmentation; Sentinel-1; Synthetic aperture radar images; Synthetic aperture radar","Image synthesis; Monitoring of mariculture; SAR; Semantic segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126285516"
"Hoang N.T.; Koike K.","Hoang, Nguyen Tien (57188550727); Koike, Katsuaki (57219664247)","57188550727; 57219664247","Hyperspectral transformation from EO-1 ali imagery using pseudo-hyperspectral image synthesis algorithm","2016","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41","","","661","665","4","10.5194/isprsarchives-XLI-B7-661-2016","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979527215&doi=10.5194%2fisprsarchives-XLI-B7-661-2016&partnerID=40&md5=9b4297fef6d664f2a275554244a1cb39","Hyperspectral remote sensing is more effective than multispectral remote sensing in many application fields because of having hundreds of observation bands with high spectral resolution. However, hyperspectral remote sensing resources are limited both in temporal and spatial coverage. Therefore, simulation of hyperspectral imagery from multispectral imagery with a small number of bands must be one of innovative topics. Based on this background, we have recently developed a method, Pseudo-Hyperspectral Image Synthesis Algorithm (PHISA), to transform Landsat imagery into hyperspectral imagery using the correlation of reflectance at the corresponding bands between Landsat and EO-1 Hyperion data. This study extends PHISA to simulate pseudo-hyperspectral imagery from EO-1 ALI imagery. The pseudo-hyperspectral imagery has the same number of bands as that of high-quality Hyperion bands and the same swath width as ALI scene. The hyperspectral reflectance data simulated from the ALI data show stronger correlation with the original Hyperion data than the one simulated from Landsat data. This high correlation originates from the concurrent observation by the ALI and Hyperion sensors that are on-board the same satellite. The accuracy of simulation results are verified by a statistical analysis and a surface mineral mapping. With a combination of the advantages of both ALI and Hyperion image types, the pseudo-hyperspectral imagery is proved to be useful for detailed identification of minerals for the areas outside the Hyperion coverage.","Algorithms; Image processing; Reflection; Spectral resolution; Spectroscopy; High spectral resolution; Hyperion; Hyperspectral Data; Hyperspectral reflectance; Hyperspectral remote sensing; Multiple analysis; Multispectral remote sensing; Spectral reconstruction; Remote sensing","ALI; Hyperion; Hyperspectral data; Multiple analysis; Spectral reconstruction","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84979527215"
"Cao Y.; Xu L.; Peng J.","Cao, Yun (57202049913); Xu, Linlin (55921131900); Peng, Junhuan (12785442100)","57202049913; 55921131900; 12785442100","Smsynth: An imagery synthesis system for soil moisture retrieval","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","3","","127","131","4","10.5194/isprs-archives-XLII-3-127-2018","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046956265&doi=10.5194%2fisprs-archives-XLII-3-127-2018&partnerID=40&md5=3104c71d784f2e8acb156cb9b7c737d6","Soil moisture (SM) is a important variable in various research areas, such as weather and climate forecasting, agriculture, drought and flood monitoring and prediction, and human health. An ongoing challenge in estimating SM via synthetic aperture radar (SAR) is the development of the retrieval SM methods, especially the empirical models needs as training samples a lot of measurements of SM and soil roughness parameters which are very difficult to acquire. As such, it is difficult to develop empirical models using realistic SAR imagery and it is necessary to develop methods to synthesis SAR imagery. To tackle this issue, a SAR imagery synthesis system based on the SM named SMSynth is presented, which can simulate radar signals that are realistic as far as possible to the real SAR imagery. In SMSynth, SAR backscatter coefficients for each soil type are simulated via the Oh model under the Bayesian framework, where the spatial correlation is modeled by the Markov random field (MRF) model. The backscattering coefficients simulated based on the designed soil parameters and sensor parameters are added into the Bayesian framework through the data likelihood where the soil parameters and sensor parameters are set as realistic as possible to the circumstances on the ground and in the validity range of the Oh model. In this way, a complete and coherent Bayesian probabilistic framework is established. Experimental results show that SMSynth is capable of generating realistic SAR images that suit the needs of a large amount of training samples of empirical models. © Authors 2018.","Backscattering; Image segmentation; Markov processes; Radar measurement; Remote sensing; Sampling; Search engines; Soil moisture; Structural frames; Synthetic aperture radar; Weather forecasting; Backscattering coefficients; Bayesian frameworks; Bayesian probabilistic frameworks; Image synthesis; Markov Random Field model; Markov random field models; Soil moisture retrievals; Spatial correlations; Radar imaging","Bayesian framework; Image synthesis; Markov random field model; Oh model; Soil moisture","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85046956265"
"Sibler P.; Sica F.; Schmitt M.","Sibler, Philipp (57219793990); Sica, Francescopaolo (56673917100); Schmitt, Michael (7401931279)","57219793990; 56673917100; 7401931279","Deep Learning-Based SAR Interferogram Synthesis from Raster and Land Cover Data","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","5236","5239","3","10.1109/IGARSS46834.2022.9884964","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141603213&doi=10.1109%2fIGARSS46834.2022.9884964&partnerID=40&md5=db5cf8b0d8ff765d97d184f36862ae15","Image-to-image translation between different imaging modalities in Earth observation has become a widely utilized application area of deep learning. However, most of the translation is performed on real-valued data, to some extent neglecting the opportunities of complex-valued SAR data for interferometric methods. In this work, we propose a multi-task deep learning approach for simulating complex-valued InSAR data based on splitting the overall task into multi-modal image-toimage translation sub-tasks. Instead of synthesizing complex-valued SAR data directly, magnitudes, phase values and coherence magnitudes are simulated in parallel and combined to full complex-valued information afterward. With experiments on a Sentinel-1 interferogram, conditioned by DEM and land cover data, we demonstrate the feasibility of the approach. © 2022 IEEE.","Deep learning; Interferometry; Learning systems; Radar imaging; Remote sensing; Auto encoders; Coherence estimation; Complex-valued; Deep learning; GAN; Images synthesis; Land cover; Multi tasks; SAR interferometry; SAR-interferometry; Synthetic aperture radar","autoencoder; CNN; coherence estimation; complex-valued; deep learning; GAN; image synthesis; multi-task; SAR; SAR interferometry","Conference paper","Final","","Scopus","2-s2.0-85141603213"
"De Vieilleville F.; Ristorcelli T.; Delvit J.-M.","De Vieilleville, F. (8948135500); Ristorcelli, T. (55809098300); Delvit, J.-M. (6505969352)","8948135500; 55809098300; 6505969352","Dem reconstruction using light field and bidirectional reflectance function from multi-view high resolution spatial images","2016","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41","","","503","509","6","10.5194/isprsarchives-XLI-B3-503-2016","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978176218&doi=10.5194%2fisprsarchives-XLI-B3-503-2016&partnerID=40&md5=5c5ab0b1f39ed6d6f87f1fd0ecb9f29b","This paper presents a method for dense DSM reconstruction from high resolution, mono sensor, passive imagery, spatial panchromatic image sequence. The interest of our approach is four-fold. Firstly, we extend the core of light field approaches using an explicit BRDF model from the Image Synthesis community which is more realistic than the Lambertian model. The chosen model is the Cook-Torrance BRDF which enables us to model rough surfaces with specular effects using specific material parameters. Secondly, we extend light field approaches for non-pinhole sensors and non-rectilinear motion by using a proper geometric transformation on the image sequence. Thirdly, we produce a 3D volume cost embodying all the tested possible heights and filter it using simple methods such as Volume Cost Filtering or variational optimal methods. We have tested our method on a Pleiades image sequence on various locations with dense urban buildings and report encouraging results with respect to classic multi-label methods such as MIC-MAC, or more recent pipelines such as S2P. Last but not least, our method also produces maps of material parameters on the estimated points, allowing us to simplify building classification or road extraction.","Computer vision; Image reconstruction; Mathematical transformations; Remote sensing; Bidirectional reflectance; BRDF; Dem reconstruction; Geometric transformations; Image sequence; Light fields; Panchromatic images; Rectilinear motion; Image processing","BRDF; DSM reconstruction; Image sequence; Light field","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84978176218"
"Liu L.; Li W.; Shi Z.; Zou Z.","Liu, Liqin (57215536317); Li, Wenyuan (57204784272); Shi, Zhenwei (23398841900); Zou, Zhengxia (56073977200)","57215536317; 57204784272; 23398841900; 56073977200","Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5528215","","","","10.1109/TGRS.2022.3173532","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130678301&doi=10.1109%2fTGRS.2022.3173532&partnerID=40&md5=d0c26ffea5447d5804eb35757065b4df","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as 'Physics-informed Deep Adversarial Spectral Synthesis (PDASS).' On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.  © 1980-2012 IEEE.","Generative adversarial networks; Hyperspectral imaging; Military applications; Optical resolving power; Pixels; Reflection; Remote sensing; Spectroscopy; Adversarial networks; Atmospheric modeling; Generation adversarial network; Images reconstruction; Imaging modeling; Remote-sensing; Spatial resolution; Spectral super-resolution; Superresolution; artificial neural network; image analysis; image resolution; imaging method; physics; pixel; remote sensing; satellite imagery; spectral analysis; Image reconstruction","Generation adversarial networks (GANs); hyperspectral image; imaging model; remote sensing; spectral super-resolution (SSR)","Article","Final","","Scopus","2-s2.0-85130678301"
