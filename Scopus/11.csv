"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Li J.; Zhang Y.; Sheng Q.; Wu Z.; Wang B.; Hu Z.; Shen G.; Schmitt M.; Molinier M.","Li, Jun (57790004000); Zhang, Yuejie (57927825700); Sheng, Qinghong (36562635800); Wu, Zhaocong (15023850900); Wang, Bo (57836499500); Hu, Zhongwen (55630272400); Shen, Guanting (57928295100); Schmitt, Michael (7401931279); Molinier, Matthieu (22234853700)","57790004000; 57927825700; 36562635800; 15023850900; 57836499500; 55630272400; 57928295100; 7401931279; 22234853700","Thin Cloud Removal Fusing Full Spectral and Spatial Features for Sentinel-2 Imagery","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8759","8775","16","10.1109/JSTARS.2022.3211857","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139828521&doi=10.1109%2fJSTARS.2022.3211857&partnerID=40&md5=caab2fe27943e37e465f7432d43cfda4","Multispectral remote sensing images are widely used for monitoring the globe. Although thin clouds can affect all optical bands, the influences of thin clouds differ with band wavelength. When processing multispectral bands at different resolutions, many methods only remove thin clouds in visible/near-infrared bands or rescale multiresolution bands to the same resolution and then process them together. The former cannot make full use of multispectral information, and in the latter, the rescaling process will introduce noise. In this article, a deep-learning-based thin cloud removal method that fuses full spectral and spatial features in original Sentinel-2 bands is proposed, named CR4S2. A multi-input and output architecture is designed for better fusing information in all bands and reconstructing the background at original resolutions. In addition, two parallel downsampling residual blocks are designed to transfer features extracted from different depths to the bottom of the network. Experiments were conducted on a new globally distributed Sentinel-2 thin cloud removal dataset called WHUS2-CRv. The results show that the best averaged peak signal-to-noise ratio, structural similarity index measurement, normalized root-mean-square error, and spectral angle mapper of the proposed method over 12 bands in all 20 testing images were 39.55, 0.9443, 0.0245, and 2.5676°, respectively. Compared with baseline methods, the proposed CR4S2 method can better restore not only the spatial features but also spectral features. This indicates that the proposed method is very promising for removing thin clouds in multispectral remote sensing images at different resolutions.  © 2008-2012 IEEE.","Distributed computer systems; Generative adversarial networks; Infrared devices; Optical remote sensing; Cloud removal; Cloud-computing; Deep learning; Features extraction; Multi-feature fusion; Parallel down-sample residual block; Remote-sensing; Sentinel-2; Spatial resolution; Thin cloud removal; Vegetation mapping; machine learning; multispectral image; satellite imagery; Sentinel; spatial analysis; spectral analysis; Deep learning","Deep learning (DL); multifeature fusion; parallel downsample residual block (PDRB); Sentinel-2; thin cloud removal","Article","Final","","Scopus","2-s2.0-85139828521"
"Xiong Q.; Di L.; Feng Q.; Liu D.; Liu W.; Zan X.; Zhang L.; Zhu D.; Liu Z.; Yao X.; Zhang X.","Xiong, Quan (57057052900); Di, Liping (7006684098); Feng, Quanlong (56417016300); Liu, Diyou (56505709200); Liu, Wei (57215411759); Zan, Xuli (57195976527); Zhang, Lin (55709261100); Zhu, Dehai (8903358300); Liu, Zhe (36666810800); Yao, Xiaochuang (56085211600); Zhang, Xiaodong (57202810216)","57057052900; 7006684098; 56417016300; 56505709200; 57215411759; 57195976527; 55709261100; 8903358300; 36666810800; 56085211600; 57202810216","Deriving non-cloud contaminated sentinel-2 images with rgb and near-infrared bands from sentinel-1 images based on a conditional generative adversarial network","2021","Remote Sensing","13","8","1512","","","","10.3390/rs13081512","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105764684&doi=10.3390%2frs13081512&partnerID=40&md5=b089cc95a036e1d8f0f8c0e4d52bea88","Sentinel-2 images have been widely used in studying land surface phenomena and pro-cesses, but they inevitably suffer from cloud contamination. To solve this critical optical data availability issue, it is ideal to fuse Sentinel-1 and Sentinel-2 images to create fused, cloud-free Sentinel-2-like images for facilitating land surface applications. In this paper, we propose a new data fusion model, the Multi-channels Conditional Generative Adversarial Network (MCcGAN), based on the conditional generative adversarial network, which is able to convert images from Domain A to Domain B. With the model, we were able to generate fused, cloud-free Sentinel-2-like images for a target date by using a pair of reference Sentinel-1/Sentinel-2 images and target-date Sentinel-1 images as inputs. In order to demonstrate the superiority of our method, we also compared it with other state-of-the-art methods using the same data. To make the evaluation more objective and reliable, we calculated the root-mean-square-error (RSME), R2, Kling–Gupta efficiency (KGE), structural similarity index (SSIM), spectral angle mapper (SAM), and peak signal-to-noise ratio (PSNR) of the simulated Sentinel-2 images generated by different methods. The results show that the simulated Sentinel-2 images generated by the MCcGAN have a higher quality and accuracy than those produced via the previous methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data fusion; Infrared devices; Mean square error; Surface measurement; Adversarial networks; Cloud contamination; Near infrared band; Peak signal to noise ratio; Root mean square errors; Spectral angle mappers; State-of-the-art methods; Structural similarity indices (SSIM); Signal to noise ratio","Data fusion; Generative adversarial network; Non-cloud contamination; Sentinel-1; Sentinel-2","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85105764684"
"Singh P.; Komodakis N.","Singh, Praveer (57220983904); Komodakis, Nikos (57210774496)","57220983904; 57210774496","Cloud-GAN: Cloud removal for sentinel-2 imagery using a cyclic consistent generative adversarial networks","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519033","1772","1775","3","10.1109/IGARSS.2018.8519033","98","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064168458&doi=10.1109%2fIGARSS.2018.8519033&partnerID=40&md5=bd33a91f221d44b923417ea9badca61e","Cloud cover is a serious impediment in land surface analysis from Remote Sensing images either causing complete obstruction (thick clouds) with loss of information or blurry effects when being semi-transparent (thin clouds). While thick clouds require complete pixel replacement, thin cloud removal is fairly challenging as the atmospheric and land-cover information is inter-twined. In this paper, we address this problem and propose a Cloud-GAN to learn the mapping between cloudy images and cloud-free images. The adversarial loss in the proposed method constrains the distribution of generated images to be close enough to the underlying distribution of the non-cloudy images. An additional cycle consistency loss is used to further restrain the generator to predict cloud-free images only of the same scene as reflected in the cloudy images. Our method not only rejects the necessity of any paired (cloud/cloud-free) training dataset but also avoids the need of any additional (expensive) spectral source of information such as Synthetic Aperture Radar imagery which is cloud penetrable. Lastly, we demonstrate the efficacy of our technique by training on an openly available and fairly new Sentinel-2 Imagery dataset consisting of real clouds. We also show significant improvement in PSNR values after removing clouds on synthetic images thus validating the competency of our methodology. © 2018 IEEE.","Deep learning; Geology; Radar imaging; Remote sensing; Surface analysis; Synthetic aperture radar; Tracking radar; Adversarial networks; Cloud removal; Land cover informations; Land surface analysis; Remote sensing images; Sentinel-2 imagery; Synthetic Aperture Radar Imagery; Underlying distribution; Image enhancement","Cloud removal; Deep learning; Generative adversarial networks; Sentinel-2 imagery","Conference paper","Final","","Scopus","2-s2.0-85064168458"
"Park N.-W.; Park M.-G.; Kwak G.-H.; Hong S.","Park, No-Wook (7202111787); Park, Min-Gyu (57215432360); Kwak, Geun-Ho (57206203736); Hong, Sungwook (55817600100)","7202111787; 57215432360; 57206203736; 55817600100","Deep Learning-Based Virtual Optical Image Generation and Its Application to Early Crop Mapping","2023","Applied Sciences (Switzerland)","13","3","1766","","","","10.3390/app13031766","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147917899&doi=10.3390%2fapp13031766&partnerID=40&md5=665bb799b92337d2b903139947863e3e","This paper investigates the potential of cloud-free virtual optical imagery generated using synthetic-aperture radar (SAR) images and conditional generative adversarial networks (CGANs) for early crop mapping, which requires cloud-free optical imagery at the optimal date for classification. A two-stage CGAN approach, including representation and generation stages, is presented to generate virtual Sentinel-2 spectral bands using all available information from Sentinel-1 SAR and Sentinel-2 optical images. The dual-polarization-based radar vegetation index and all available multi-spectral bands of Sentinel-2 imagery are particularly considered for feature extraction in the representation stage. A crop classification experiment using Sentinel-1 and -2 images in Illinois, USA, demonstrated that the use of all available scattering and spectral features achieved the best prediction performance for all spectral bands, including visible, near-infrared, red-edge, and shortwave infrared bands, compared with the cases that only used dual-polarization backscattering coefficients and partial input spectral bands. Early crop mapping with an image time series, including the virtual Sentinel-2 image, yielded satisfactory classification accuracy comparable to the case of using an actual time-series image set, regardless of the different combinations of spectral bands. Therefore, the generation of virtual optical images using the proposed model can be effectively applied to early crop mapping when the availability of cloud-free optical images is limited. © 2023 by the authors.","","crop classification; deep learning; generative adversarial networks; virtual image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85147917899"
"Pineda F.; Ayma V.; Aduviri R.; Beltran C.","Pineda, Ferdinand (57216822078); Ayma, Victor (56566776600); Aduviri, Robert (57207467513); Beltran, Cesar (55602499700)","57216822078; 56566776600; 57207467513; 55602499700","Super Resolution Approach Using Generative Adversarial Network Models for Improving Satellite Image Resolution","2020","Communications in Computer and Information Science","1070 CCIS","","","291","298","7","10.1007/978-3-030-46140-9_27","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084819739&doi=10.1007%2f978-3-030-46140-9_27&partnerID=40&md5=9fe165b60f94c8fbbadd58a0c7da0341","Recently, the number of satellite imaging sensors deployed in space has experienced a considerable increase, but most of these sensors provide low spatial resolution images, and only a small proportion contribute with images at higher resolutions. This work proposes an alternative to improve the spatial resolution of Landsat-8 images to the reference of Sentinel-2 images, by applying a Super Resolution (SR) approach based on the use of Generative Adversarial Network (GAN) models for image processing, as an alternative to traditional methods to achieve higher resolution images, hence, remote sensing applications could take advantage of this new information and improve its outcomes. We used two datasets to train and validate our approach, the first composed by images from the DIV2K open access dataset and the second by images from Sentinel-2 satellite. The experimental results are based on the comparison of the similarity between the Landsat-8 images obtained by the super resolution processing by our approach (for both datasets), against its corresponding reference from Sentinel-2 satellite image, computing the Peak Signal-to-Noise Ratio (PSNR) and the Structural Similarity (SSIM) as metrics for this purpose. In addition, we present a visual report in order to compare the performance of each trained model, analysis that shows interesting improvements of the resolution of Landsat-8 satellite images. © Springer Nature Switzerland AG 2020.","Big data; Image resolution; Information management; Optical resolving power; Remote sensing; Signal to noise ratio; Small satellites; Adversarial networks; Higher resolution images; Peak signal to noise ratio; Remote sensing applications; Satellite imaging; Spatial resolution; Spatial resolution images; Structural similarity; Image enhancement","Landsat-8; Sentinel-2; SR-GAN; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85084819739"
"Jozdani S.; Chen D.; Chen W.; Leblanc S.G.; Lovitt J.; He L.; Fraser R.H.; Johnson B.A.","Jozdani, Shahab (57188989068); Chen, Dongmei (55724967900); Chen, Wenjun (55597863700); Leblanc, Sylvain G. (7006985218); Lovitt, Julie (57194797446); He, Liming (37115581900); Fraser, Robert H. (7402280263); Johnson, Brian Alan (55481350500)","57188989068; 55724967900; 55597863700; 7006985218; 57194797446; 37115581900; 7402280263; 55481350500","Evaluating image normalization via GANs for environmental mapping: A case study of lichen mapping using high-resolution satellite imagery","2021","Remote Sensing","13","24","5035","","","","10.3390/rs13245035","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121337074&doi=10.3390%2frs13245035&partnerID=40&md5=9de56052f9230aac0f8421a044dabcdf","Illumination variations in non-atmospherically corrected high-resolution satellite (HRS) images acquired at different dates/times/locations pose a major challenge for large-area environmental mapping and monitoring. This problem is exacerbated in cases where a classification model is trained only on one image (and often limited training data) but applied to other scenes without collecting additional samples from these new images. In this research, by focusing on caribou lichen mapping, we evaluated the potential of using conditional Generative Adversarial Networks (cGANs) for the normalization of WorldView-2 (WV2) images of one area to a source WV2 image of another area on which a lichen detector model was trained. In this regard, we considered an extreme case where the classifier was not fine-tuned on the normalized images. We tested two main scenarios to normalize four target WV2 images to a source 50 cm pansharpened WV2 image: (1) normalizing based only on the WV2 panchromatic band, and (2) normalizing based on the WV2 panchromatic band and Sentinel-2 surface reflectance (SR) imagery. Our experiments showed that normalizing even based only on the WV2 panchromatic band led to a significant lichen-detection accuracy improvement compared to the use of original pansharpened target images. However, we found that conditioning the cGAN on both the WV2 panchromatic band and auxiliary information (in this case, Sentinel-2 SR imagery) further improved normalization and the subsequent classification results due to adding a more invariant source of information. Our experiments showed that, using only the panchromatic band, F1-score values ranged from 54% to 88%, while using the fused panchromatic and SR, F1-score values ranged from 75% to 91%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Deep learning; Fungi; Generative adversarial networks; Image enhancement; Remote sensing; Satellite imagery; Deep learning; Environmental mapping; GAN; Image normalization; Lichen mapping; Normalisation; Panchromatic bands; Remote-sensing; Surface reflectance; Worldview-2; Mapping","Deep learning; Environmental mapping; GANs; Image normalization; Lichen mapping; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121337074"
"Latif H.; Ghuffar S.; Ahmad H.M.","Latif, Hasan (57971676700); Ghuffar, Sajid (14630367300); Ahmad, Hafiz Mughees (57208205069)","57971676700; 14630367300; 57208205069","Super-resolution of Sentinel-2 images using Wasserstein GAN","2022","Remote Sensing Letters","13","12","","1194","1202","8","10.1080/2150704X.2022.2136019","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142262873&doi=10.1080%2f2150704X.2022.2136019&partnerID=40&md5=6ed46ccc6359ea33ed4a46f5cec66add","The Sentinel-2 satellites deliver 13 band multi-spectral imagery with bands having 10 m, 20 m or 60 m spatial resolution. The low-resolution bands can be upsampled to match the high resolution bands to extract valuable information at higher spatial resolution. This paper presents a Wasserstein Generative Adversarial Network (WGAN) based approach named as DSen2-WGAN to super-resolve the low-resolution (i.e., 20 m and 60 m) bands of Sentinel-2 images to a spatial resolution of 10 m. A proposed generator is trained in an adversarial manner using the min-max game to super-resolve the low-resolution bands with the guidance of available high-resolution bands in an image. The performance evaluated using metrics such as Signal Reconstruction Error (SRE) and Root Mean Squared Error (RMSE) shows the effectiveness of the proposed approach as compared to the state-of-the-art method, DSen2 as the DSen2-WGAN reduced RMSE by 14.68% and 7%, while SRE improved by almost 4% and 1.6% for 6 (Formula presented.) and 2 (Formula presented.) super-resolution. Lastly, for further evaluation, we have used trained DSen2-WGAN model to super-resolve the bands of EuroSAT dataset, a satellite image classification dataset based on Sentinel-2 images. The per band classification accuracy of low-resolution bands shows significant improvement after super-resolution using our proposed approach. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Air navigation; Classification (of information); Image resolution; Mean square error; Signal reconstruction; Spectroscopy; High resolution; High spatial resolution; Lower resolution; Multispectral imagery; Network-based approach; Reconstruction error; Root mean squared errors; Signals reconstruction; Spatial resolution; Superresolution; data set; image classification; image resolution; satellite data; Sentinel; spatial resolution; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85142262873"
"Liu Z.; Zhu H.; Chen Z.","Liu, Ziyu (58087753100); Zhu, Han (57221180146); Chen, Zhenzhong (57985265500)","58087753100; 57221180146; 57985265500","Adversarial Spectral Super-Resolution for Multispectral Imagery Using Spatial Spectral Feature Attention Module","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","","","1","14","13","10.1109/JSTARS.2023.3238853","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147300696&doi=10.1109%2fJSTARS.2023.3238853&partnerID=40&md5=9c1fcd62e9714f69543d9e53282a0dbf","Acquiring high-quality hyperspectral imagery with high spatial and spectral resolution plays an important role in remote sensing. Due to the limited capacity of sensors, providing high spatial and spectral resolution is still a challenging issue. Spectral super-resolution (SSR) increases the spectral dimensionality of multispectral images to achieve resolution enhancement. In this paper, we propose a spectral resolution enhancement method based on the generative adversarial network (GAN) framework without introducing additional spectral responses prior. In order to adaptively rescale informative features for capturing interdependencies in the spectral and spatial dimensions, a spatial spectral feature attention module (SSFAM) is introduced. The proposed method jointly exploits spatio-spectral distribution in the hyperspectral manifold to increase spectral resolution while maintaining spatial content consistency. Experiments are conducted on both synthetic Landsat 8 and Sentinel-2 radiance data and real co-registered ALI and Hyperion (MS and HS) images, which indicates the superiority of the proposed method compared to other state-of-the-art methods. Author","Generative adversarial networks; Hyperspectral imaging; Image enhancement; Remote sensing; Spectral resolution; Adversarial learning; Attention mechanisms; Correlation; Hyper-spectral imageries; Images reconstruction; Spatial resolution; Spectral feature; Spectral super-resolution; Superresolution; Image reconstruction","adversarial learning; attention Mechanism; Cameras; Correlation; Hyperspectral imagery; Hyperspectral imaging; Image reconstruction; Sensors; Spatial resolution; spectral super-resolution; Superresolution","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85147300696"
"Jozdani S.; Chen D.; Pouliot D.; Alan Johnson B.","Jozdani, Shahab (57188989068); Chen, Dongmei (57203235632); Pouliot, Darren (8212745300); Alan Johnson, Brian (57493655200)","57188989068; 57203235632; 8212745300; 57493655200","A review and meta-analysis of generative adversarial networks and their applications in remote sensing","2022","International Journal of Applied Earth Observation and Geoinformation","108","","102734","","","","10.1016/j.jag.2022.102734","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126636731&doi=10.1016%2fj.jag.2022.102734&partnerID=40&md5=477a218f5f179f495a2631341aad3e8a","Generative Adversarial Networks (GANs) are one of the most creative advances in Deep Learning (DL) in recent years. The Remote Sensing (RS) community has adopted GANs quickly, and reported successful use in a wide variety of applications. Given a sharp increase in research on GANs in the field of RS, there is a need for an in-depth review of the major technological/methodological advances and new applications. In this regard, we conducted a comprehensive review and meta-analysis of GAN-related RS papers, with the goals of familiarizing the RS community with the potential of GANs and helping researchers further explore RS applications of GANs by untangling challenges common in this field. Our review is based on 231 journal papers that were retrieved and selected through the Web of Science (WoS) database. We reviewed the theories, applications, and challenges of GANs, and highlighted the gaps to explore in future studies. Through the meta-analysis conducted in this study, we observed that image classification (especially urban mapping) has been the most popular application of GANs, potentially due to the wide availability of benchmark datasets. One the other hand, we found that relatively few studies have explored the potential of GANs for analyzing medium spatial-resolution multi-spectral images (e.g., Landsat or Sentinel-2), even though such images are often freely available and useful for a wide range of applications (e.g., urban expansion analysis, vegetation mapping, etc.). In spite of the applications of GANs for different RS processing tasks, there are still several gaps/questions in this field such as: 1) which GAN models/configurations are more suitable for different applications?) 2) to what degree can GANs replace real RS data in different applications? Such gaps/questions can be appropriately addressed by, for example, conducting experimental studies on evaluating different GAN models for various RS applications to provide better insights into how/which GAN models can be best deployed. The meta-analysis results presented in this study could be helpful for RS researchers to know the opportunities of using GANs and understand how GANs contribute to the current challenges in different RS applications. © 2022 The Authors","database; image analysis; machine learning; meta-analysis; remote sensing; spatial resolution","Deep learning; GANs; Generative adversarial networks; Remote sensing","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126636731"
"Efremova N.; Seddik M.E.A.; Erten E.","Efremova, Natalia (42261427800); Seddik, Mohamed El Amine (57376013800); Erten, Esra (23011359600)","42261427800; 57376013800; 23011359600","Soil Moisture Estimation Using Sentinel-1/-2 Imagery Coupled with CycleGAN for Time-Series Gap Filing","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3134127","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121377049&doi=10.1109%2fTGRS.2021.3134127&partnerID=40&md5=f8eaba4f2061d9daf016220a21af0851","Fast soil moisture content (SMC) mapping is necessary to support water resource management and to understand crop growth, quality, and yield. Therefore, earth observation (EO) plays a key role due to its ability of almost real-time monitoring of large areas at a low cost. This study aimed to explore the possibility of taking advantage of freely available Sentinel-1 (S1) and Sentinel-2 (S2) EO data for the simultaneous prediction of SMC with cycle-consistent adversarial network (CycleGAN) for time-series gap filling. The proposed methodology, first, learns latent low-dimensional representation of the satellite images, then learns a simple machine learning (ML) model on top of these representations. To evaluate the methodology, a series of vineyards, located in South Australia 's Eden valley are chosen. Specifically, we presented an efficient framework for extracting latent features from S1 and S2 imagery. We showed how one could use S1 to S2 feature translation based on CycleGAN using S1 and S2 time series when there are missing images acquired over an area of interest. The resulting data in our study is then used to fill gaps in time-series data. We used the resulting latent representations to predict SMC with various ML tools. In the experiments, CycleGAN and the autoencoders were trained with data randomly chosen around the site of interest, so we could augment the existing dataset. The best performance was demonstrated with random forest (RF) algorithm, whereas linear regression model demonstrated significant overfitting. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there are missing optical and synthetic-aperture radar (SAR) images.  © 2021 IEEE.","Decision trees; Generative adversarial networks; Regression analysis; Soil surveys; Synthetic aperture radar; Water management; Domain adaptation; Features extraction; Generative adversarial network; Machine-learning; Predictive models; Sentinel-1; Sentinel-2; Soil measurement; Times series; Unsupervised domain adaptation; image processing; real time; Sentinel; soil moisture; time series analysis; water resource; Soil moisture","Agriculture; generative adversarial networks (GANs); machine learning (ML); Sentinel-1; Sentinel-2; soil moisture (SM); unsupervised domain adaptation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85121377049"
"Jamali A.; Mahdianpari M.; Mohammadimanesh F.; Brisco B.; Salehi B.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939); Mohammadimanesh, Fariba (56541784200); Brisco, Brian (7003505161); Salehi, Bahram (36610817400)","56909712300; 57190371939; 56541784200; 7003505161; 36610817400","A synergic use of sentinel-1 and sentinel-2 imagery for complex wetland classification using generative adversarial network (Gan) scheme","2021","Water (Switzerland)","13","24","3601","","","","10.3390/w13243601","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121375982&doi=10.3390%2fw13243601&partnerID=40&md5=b9a064aa3ff25becdd73fa48f05d4f5c","Due to anthropogenic activities and climate change, many natural ecosystems, especially wetlands, are lost or changing at a rapid pace. For the last decade, there has been increasing attention towards developing new tools and methods for the mapping and classification of wetlands using remote sensing. At the same time, advances in artificial intelligence and machine learning, particularly deep learning models, have provided opportunities to advance wetland classification methods. However, the developed deep and very deep algorithms require a higher number of training samples, which is costly, logistically demanding, and time-consuming. As such, in this study, we propose a Deep Convolutional Neural Network (DCNN) that uses a modified architecture of the well-known DCNN of the AlexNet and a Generative Adversarial Network (GAN) for the generation and classification of Sentinel-1 and Sentinel-2 data. Applying to an area of approximately 370 sq. km in the Avalon Peninsula, Newfoundland, the proposed model with an average accuracy of 92.30% resulted in F-1 scores of 0.82, 0.85, 0.87, 0.89, and 0.95 for the recognition of swamp, fen, marsh, bog, and shallow water, respectively. Moreover, the proposed DCNN model improved the F-1 score of bog, marsh, fen, and swamp wetland classes by 4%, 8%, 11%, and 26%, respectively, compared to the original CNN network of AlexNet. These results reveal that the proposed model is highly capable of the generation and classification of Sentinel-1 and Sentinel-2 wetland samples and can be used for large-extent classification problems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Avalon Peninsula; Canada; Newfoundland; Newfoundland and Labrador; Climate change; Convolution; Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image classification; Remote sensing; Anthropogenic activity; Anthropogenic climate; CNN; Natural ecosystem; Network scheme; Remote-sensing; Sentinel-1; Time advance; Tools and methods; Wetland classification; accuracy assessment; artificial intelligence; satellite imagery; Sentinel; wetland; Wetlands","CNN; Deep Convolutional Neural Network; Generative Adversarial Network; Machine learning; Wetland classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121375982"
"Sener E.; Çolak E.; Erten E.; Taskin G.","Sener, Ecre (57482537300); Çolak, Emre (57201377169); Erten, Esra (23011359600); Taskin, Gülsen (35105306400)","57482537300; 57201377169; 23011359600; 35105306400","THE ADDED VALUE OF CYCLE-GAN FOR AGRICULTURE STUDIES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","7039","7042","3","10.1109/IGARSS47720.2021.9553876","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126041204&doi=10.1109%2fIGARSS47720.2021.9553876&partnerID=40&md5=d9d7e2727ac594969804309acca1762b","It is significant to monitor the phenological stages of agricultural crops with accurate and up-to-date information. In monitoring the phenological phases of some crops, optical remote sensing data offers significant spectral information and outstanding feature identification. However, a continuous time series of optical remote sensing data is difficult to obtain due to the weather dependency of optical acquisitions. In this paper, the feasibility of transfer learning between the features of Sentinel-1 and Sentinel-2 is evaluated to reduce these difficulties. A feature translation based on deep learning (DL) method, namely Cycle-Consistent Generative Adversarial Networks (cycle-GAN), was applied between Sentinel-1 and Sentinel-2 data. In order to evaluate the effect of the cycle-GAN method on crop type mapping and identification, Random Forest classification was applied to four different cases (Real SAR, Fake Optical + Real SAR, Real Optical, and Real Optical + Real SAR). © 2021 IEEE","","Agriculture; Consistent adversarial networks; Cycle-GAN; Random forest classification; Sentinel-1; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85126041204"
"Sanchez E.H.; Serrurier M.; Ortner M.","Sanchez, Eduardo H. (57216825636); Serrurier, Mathieu (55892862500); Ortner, Mathias (57216016719)","57216825636; 55892862500; 57216016719","Learning Disentangled Representations of Satellite Image Time Series","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11908 LNAI","","","306","321","15","10.1007/978-3-030-46133-1_19","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084814845&doi=10.1007%2f978-3-030-46133-1_19&partnerID=40&md5=0332c783d24d96d344843c2257e03cb8","In this paper, we investigate how to learn a suitable representation of satellite image time series in an unsupervised manner by leveraging large amounts of unlabeled data. Additionally, we aim to disentangle the representation of time series into two representations: a shared representation that captures the common information between the images of a time series and an exclusive representation that contains the specific information of each image of the time series. To address these issues, we propose a model that combines a novel component called cross-domain autoencoders with the variational autoencoder (VAE) and generative adversarial network (GAN) methods. In order to learn disentangled representations of time series, our model learns the multimodal image-to-image translation task. We train our model using satellite image time series provided by the Sentinel-2 mission. Several experiments are carried out to evaluate the obtained representations. We show that these disentangled representations can be very useful to perform multiple tasks such as image classification, image retrieval, image segmentation and change detection. © Springer Nature Switzerland AG 2020.","Image retrieval; Image segmentation; Machine learning; Satellites; Time series; Adversarial networks; Change detection; Image translation; Multi-modal image; Novel component; Satellite images; Shared representations; Specific information; Image classification","Disentangled representation; GAN; Image-to-image translation; Satellite image time series; Unsupervised learning; VAE","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85084814845"
"Huang G.-L.; Wu P.-Y.","Huang, Gi-Luen (58075611900); Wu, Pei-Yuan (55413171800)","58075611900; 55413171800","CTGAN: CLOUD TRANSFORMER GENERATIVE ADVERSARIAL NETWORK","2022","Proceedings - International Conference on Image Processing, ICIP","","","","511","515","4","10.1109/ICIP46576.2022.9897229","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146693577&doi=10.1109%2fICIP46576.2022.9897229&partnerID=40&md5=1fd3edd319b5b3b6136aedd14c984e3e","Cloud occlusions obstruct some applications of remote sensing imagery, such as environment monitoring, land cover classification, and poverty prediction. In this paper, we propose the Cloud Transformer Generative Adversarial Network (CTGAN), taking three temporal cloudy images as input and generating a corresponding cloud-free image. Unlike previous work using generative networks, we design the feature extractor to maintain the weight of the cloudless region while reducing the weight of the cloudy region, and we pass the extracted features to a conformer module to find the most critical representations. Meanwhile, to address the lack of datasets, we collected a new dataset named Sen2 MTC from the Sentinel-2 satellite and manually labeled each cloudy and cloud-free image. Finally, we conducted extensive experiments on FS-2, the STGAN dataset, and Sen2 MTC. Our proposed CTGAN demonstrates higher qualitative and quantitative performance than the previous work and achieves state-of-the-art performance on these three datasets. The code is available at https://github.com/come880412/CTGAN. © 2022 IEEE.","Remote sensing; Satellites; Cloud removal; Cloud removal for multi-temporal cloudy image; Conformer; Environment monitoring; Formosat-2; Formosat-2 satellite; Land cover classification; Multi-temporal; Remote sensing imagery; Sentinel-2 satellite; Generative adversarial networks","Cloud removal for multi-temporal cloudy images; conformer; FormoSat-2 satellite; generative adversarial network; Sentinel-2 satellite","Conference paper","Final","","Scopus","2-s2.0-85146693577"
"Christovam L.E.; Shimabukuro M.H.; Galo M.L.B.T.; Honkavaara E.","Christovam, L.E. (57204813121); Shimabukuro, M.H. (7004876502); Galo, M.L.B.T. (57195519405); Honkavaara, E. (55927897800)","57204813121; 7004876502; 57195519405; 55927897800","Evaluation of sar to optical image translation using conditional generative adversarial network for cloud removal in a crop dataset","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2021","","823","828","5","10.5194/isprs-archives-XLIII-B3-2021-823-2021","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115884850&doi=10.5194%2fisprs-archives-XLIII-B3-2021-823-2021&partnerID=40&md5=fef6a726fb4f09f52dfe6f5eb7e09528","Most methods developed to map crop fields with high-quality are based on optical image time-series. However, often accuracy of these approaches is deteriorated due to clouds and cloud shadows, which can decrease the availably of optical data required to represent crop phenological stages. In this sense, the objective of this study was to implement and evaluate the conditional Generative Adversarial Network (cGAN) that has been indicated as a potential tool to address the cloud and cloud shadow removal; we also compared it with the Witthaker Smother (WS), which is a well-known data cleaning algorithm. The dataset used to train and assess the methods was the Luis Eduardo Magalhães benchmark for tropical agricultural remote sensing applications. We selected one MSI/Sentinel-2 and C-SAR/Sentinel-1 image pair taken in days as close as possible. A total of 5000 image pair patches were generated to train the cGAN model, which was used to derive synthetic optical pixels for a testing area. Visual analysis, spectral behaviour comparison, and classification were used to evaluate and compare the pixels generated with the cGAN and WS against the pixel values from the real image. The cGAN provided consistent pixel values for most crop types in comparison to the real pixel values and outperformed the WS significantly. The results indicated that the cGAN has potential to fill cloud and cloud shadow gaps in optical image time-series. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.","Benchmarking; Crops; Geometrical optics; Pixels; Radar imaging; Remote sensing; Synthetic aperture radar; Time series; CGAN; Image translation; Image-to-image; Optical image; Optical-; Pix2pix; Remote-sensing; Sar-to-optical; Sentinel-2; Synthetic images; Generative adversarial networks","CGAN; Image Translation; Image-to-Image; Pix2Pix; Remote Sensing; Sar-to-Optical; Sentinel-2; Synthetic Images","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115884850"
"","","","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","","","178","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060419385&partnerID=40&md5=9c12bd0dc33bbf219d0df980041bcf20","The proceedings contain 22 papers. The topics discussed include: SAR to optical image synthesis for cloud removal with generative adversarial networks; development of a portable high performance mobile mapping system using the robot operating system; data processing and recording using a versatile multi-sensor vehicle; semantic segmentation of aerial imagery via multi-scale shuffling convolutional neural networks with deep supervision; Copernicus sentinel-2 data for the determination of groundwater withdrawal in the maghreb region; calibration study of a trimble ACX4 system for direct georeferencing mapping applications; infrared measurements and estimation of temperature in the restrictive scope of an industrial cement plant; extraction of solar cells from UAV-based thermal image sequences; disparity refinement of building edges using robustly matched straight lines for stereo matching; and pilot study on the retrieval of DBH and diameter distribution of deciduous forest stands using cast shadows in UAV-based orthomosaics.","","","Conference review","Final","","Scopus","2-s2.0-85060419385"
"Li J.; Wu Z.; Sheng Q.; Wang B.; Hu Z.; Zheng S.; Camps-Valls G.; Molinier M.","Li, Jun (57202722259); Wu, Zhaocong (15023850900); Sheng, Qinghong (36562635800); Wang, Bo (57190495006); Hu, Zhongwen (55630272400); Zheng, Shaobo (57852261300); Camps-Valls, Gustau (6603888005); Molinier, Matthieu (22234853700)","57202722259; 15023850900; 36562635800; 57190495006; 55630272400; 57852261300; 6603888005; 22234853700","A hybrid generative adversarial network for weakly-supervised cloud detection in multispectral images","2022","Remote Sensing of Environment","280","","113197","","","","10.1016/j.rse.2022.113197","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136277692&doi=10.1016%2fj.rse.2022.113197&partnerID=40&md5=58da582698f1afcd1e7983df21b2c735","Cloud detection is a crucial step in the optical satellite image processing pipeline for Earth observation. Clouds in optical remote sensing images seriously affect the visibility of the background and greatly reduce the usability of images for land applications. Traditional methods based on thresholding, multi-temporal or multi-spectral information are often specific to a particular satellite sensor. Convolutional Neural Networks for cloud detection often require labeled cloud masks for training that are very time-consuming and expensive to obtain. To overcome these challenges, this paper presents a hybrid cloud detection method based on the synergistic combination of generative adversarial networks (GAN) and a physics-based cloud distortion model (CDM). The proposed weakly-supervised GAN-CDM method (available online https://github.com/Neooolee/GANCDM) only requires patch-level labels for training, and can produce cloud masks at pixel-level in both training and testing stages. GAN-CDM is trained on a new globally distributed Landsat 8 dataset (WHUL8-CDb, available online doi:https://doi.org/10.5281/zenodo.6420027) including image blocks and corresponding block-level labels. Experimental results show that the proposed GAN-CDM method trained on Landsat 8 image blocks achieves much higher cloud detection accuracy than baseline deep learning-based methods, not only in Landsat 8 images (L8 Biome dataset, 90.20% versus 72.09%) but also in Sentinel-2 images (“S2 Cloud Mask Catalogue” dataset, 92.54% versus 77.00%). This suggests that the proposed method provides accurate cloud detection in Landsat images, has good transferability to Sentinel-2 images, and can quickly be adapted for different optical satellite sensors. © 2022 The Authors","Deep learning; HTTP; Image processing; Landsat; Learning systems; Optical data processing; Optical remote sensing; Cloud detection; Cloud distortion model; Cloud masks; Deep learning; Distortion model; Generative adversarial network; LANDSAT; Model method; Remote-sensing; Satellite sensors; artificial neural network; detection method; image processing; Landsat; remote sensing; satellite imagery; Sentinel; spectral analysis; supervised learning; Generative adversarial networks","Cloud detection; Cloud distortion model; Deep learning; Generative adversarial networks (GAN); Remote sensing","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85136277692"
"Xia Y.; Zhang H.; Zhang L.; Fan Z.","Xia, Yu (35319088500); Zhang, Hongyan (54954032600); Zhang, Liangpei (8359720900); Fan, Zhiyu (57204877960)","35319088500; 54954032600; 8359720900; 57204877960","Cloud Removal of Optical Remote Sensing Imagery with Multitemporal Sar-Optical Data Using X-Mtgan","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899105","3396","3399","3","10.1109/IGARSS.2019.8899105","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077693096&doi=10.1109%2fIGARSS.2019.8899105&partnerID=40&md5=2eeb9bcbacb654e079e0c4d5923d94ac","Optical remote sensing images are inevitably corrupted by clouds during the acquisition process. To reconstruct the missing information contaminated by clouds, this paper introduces a new cloud removal method based on X-fork generative adversarial network with multitemporal data, which can be named X-MTGAN. By utilizing the auxiliary differential image between two imaging times, X-MTGAN can be well trained with multitemporal SAR-optical data. Then, the target optical image is synthesized with an end-to-end generator of the X-MTGAN, which has advantages in capturing change information between two temporal images. Finally, the cloud-free image can be subsequently acquired by replacing cloud-contaminated regions with the simulated image. By utilizing Setinel-1 and Sentinel-2 data, experiments are conducted to validate the feasibility of the proposed approach. Compared with the state-of-the-art methods, the results illustrate that X-MTGAN is visually and quantitatively effective in the removal of clouds, which has favorable applicability and competitive performance. © 2019 IEEE.","Geology; Geometrical optics; Image processing; Adversarial networks; Cloud removal; Competitive performance; Optical image; Optical remote sensing; Optical remote-sensing imagery; SAR data; State-of-the-art methods; Remote sensing","cloud removal; GAN; optical image simulation; SAR data","Conference paper","Final","","Scopus","2-s2.0-85077693096"
"Jamali A.; Mahdianpari M.; Mohammadimanesh F.; Homayouni S.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939); Mohammadimanesh, Fariba (56541784200); Homayouni, Saeid (24070293900)","56909712300; 57190371939; 56541784200; 24070293900","A deep learning framework based on generative adversarial networks and vision transformer for complex wetland classification using limited training samples","2022","International Journal of Applied Earth Observation and Geoinformation","115","","103095","","","","10.1016/j.jag.2022.103095","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141519900&doi=10.1016%2fj.jag.2022.103095&partnerID=40&md5=d705bcbc2474023599e7331d161879d0","Wetlands have long been recognized among the most critical ecosystems globally, yet their numbers quickly diminish due to human activities and climate change. Thus, large-scale wetland monitoring is essential to provide efficient spatial and temporal insights for resource management and conservation plans. However, the main challenge is the lack of enough reference data for accurate large-scale wetland mapping. As such, the main objective of this study was to investigate the efficient deep-learning models for generating high-resolution and temporally rich training datasets for wetland mapping. The Sentinel-1 and Sentinel-2 satellites from the European Copernicus program deliver radar and optical data at a high temporal and spatial resolution. These Earth observations provide a unique source of information for more precise wetland mapping from space. The second objective was to investigate the efficiency of vision transformers for complex landscape mapping. As such, we proposed a 3D Generative Adversarial Network (3D GAN) to best achieve these two objectives of synthesizing training data and a Vision Transformer model for large-scale wetland classification. The proposed approach was tested in three different study areas of Saint John, Sussex, and Fredericton, New Brunswick, Canada. The results showed the ability of the 3D GAN to stimulate and increase the number of training data and, as a result, increase the accuracy of wetland classification. The quantitative results also demonstrated the capability of jointly using data augmentation, 3D GAN, and Vision Transformer models with overall accuracy, average accuracy, and Kappa index of 75.61%, 73.4%, and 71.87%, respectively, using a disjoint data sampling strategy. Therefore, the proposed deep learning method opens a new window for large-scale remote sensing wetland classification. © 2022","artificial neural network; classification; climate change; conservation planning; human activity; learning; radar; remote sensing; resource management; Sentinel; wetland","Convolutional neural network; Deep learning; Generative adversarial network; New Brunswick; Vision Transformer (ViT); Wetland classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141519900"
"Luo X.; Tong X.; Hu Z.","Luo, Xin (56316646000); Tong, Xiaohua (55500134600); Hu, Zhongwen (55630272400)","56316646000; 55500134600; 55630272400","Improving Satellite Image Fusion via Generative Adversarial Training","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9212572","6969","6982","13","10.1109/TGRS.2020.3025821","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111155134&doi=10.1109%2fTGRS.2020.3025821&partnerID=40&md5=52efcfa7eaddf11c1f7bf0d58f09c968","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications. © 1980-2012 IEEE.","Geometrical optics; Image fusion; Remote sensing; Satellites; Adversarial networks; Discriminative networks; Multiresolution images; Operational land imager; Remote sensing applications; Remote sensing images; Spatial informations; Spectral information; artificial neural network; optical method; satellite data; Image enhancement","Deep learning; generative adversarial networks (GANs); Landsat 8; remote sensing image fusion; residual dense blocks; Sentinel-2","Article","Final","","Scopus","2-s2.0-85111155134"
"Wang M.; Meng X.; Shao F.; Fu R.","Wang, Mengyao (57210968797); Meng, Xiangchao (56158755000); Shao, Feng (7006717672); Fu, Randi (14821950200)","57210968797; 56158755000; 7006717672; 14821950200","SAR-Assisted Optical Remote Sensing Image Cloud Removal Method Based on Deep Learning; [基于深度学习的SAR辅助下光学遥感图像去云方法]","2021","Guangxue Xuebao/Acta Optica Sinica","41","12","1228002","","","","10.3788/AOS202141.1228002","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113278742&doi=10.3788%2fAOS202141.1228002&partnerID=40&md5=9b89574ba859279c7a4f3fd7c4491b52","The existing deep learning based SAR-assisted cloud removal methods do not take full into account the texture and spectral information of the optical images, which results in blurring and spectral loss. In this paper, we constructed a data set for SAR-assisted cloud removal based on the Sentinel-1 and Sentinel-2 satellite images in Yuhang District of Hangzhou. In addition, we established a conditional generative adversarial network (cGAN) based model by fully considering the details, texture, and color information of optical remote sensing images, achieving information recovery and reconstruction in the case of optical images covered by thin clouds, fog, and thick clouds. The results show that the proposed method outperforms other methods in SAR-assisted cloud removal. © 2021, Chinese Lasers Press. All right reserved.","Geometrical optics; Image texture; Learning systems; Radar imaging; Remote sensing; Textures; Adversarial networks; Cloud removal; Color information; Information recovery; Optical image; Optical remote sensing; Satellite images; Spectral information; Deep learning","Cloud removal; Conditional generative adversarial network (cGAN); Image fusion; Optical image; Remote sensing; Synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85113278742"
"Kapilaratne R.G.C.J.; Kakuta S.; Kaneta S.","Kapilaratne, R.G.C.J. (57194537565); Kakuta, S. (56536327700); Kaneta, S. (57219049570)","57194537565; 56536327700; 57219049570","Enhanced super resolution for remote sensing imageries","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","53","60","7","10.5194/isprs-Annals-V-3-2022-53-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132043853&doi=10.5194%2fisprs-Annals-V-3-2022-53-2022&partnerID=40&md5=e739cd48d213e5e5de51b63d8bcee7db","Single image super resolution (SISR) technology has been attracted much attention from remote sensing community due to its proven potentials in remote sensing applications. Existing SISR techniques varying from conventional interpolation methods to different network architectures. Generative adversarial networks (GANs) are one of the latest network architectures proven a greater potential as a SISR method whereas least attention has been given by the remote sensing community. Several studies have already been carried out on this context. However, yet there is no generalized GAN based approach to super resolve remote sensing imageries. Therefore, this study investigated the potentials of enhanced super resolution generative adversarial (ESRGAN) model to super resolve very high to medium resolution images from high to coarse resolution images for remote sensing applications. Two models were trained and Worldview-3 (WV3) images used as for very high resolution images. Whereas, down sampled WV3 and Sentinel-2(S2) were used as low resolution counterparts. Model performances were qualitatively and quantitatively analysed using standard metrics such as PSNR, SSIM, UIQI, CC, SAM, SID. Evaluation results emphasised super resolved images were preserved the original quality of the satellite images to a greater extent while improving its ground resolution.  © Authors 2022.","Deep learning; Generative adversarial networks; Image enhancement; Network architecture; Optical resolving power; Deep learning; ESRGAN; Image super resolutions; Remote sensing applications; Remote sensing imagery; Remote-sensing; Sentinel-2.; Single images; Superresolution; Worldview-3; Remote sensing","Deep Learning; ESRGAN; Remote Sensing; Sentinel-2.; Super Resolution; WorldView-3","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132043853"
"Jamali A.; Mahdianpari M.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939)","56909712300; 57190371939","A cloud-based framework for large-scale monitoring of ocean plastics using multi-spectral satellite imagery and generative adversarial network","2021","Water (Switzerland)","13","18","2553","","","","10.3390/w13182553","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115435905&doi=10.3390%2fw13182553&partnerID=40&md5=7aa145df5785a2f6a098be414387695c","Marine debris is considered a threat to the inhabitants, as well as the marine environments. Accumulation of marine debris, besides climate change factors, including warming water, sea-level rise, and changes in oceans’ chemistry, are causing the potential collapse of the marine environment’s health. Due to the increase of marine debris, including plastics in coastlines, ocean and sea surfaces, and even in deep ocean layers, there is a need for developing new advanced technology for the detection of large-sized marine pollution (with sizes larger than 1 m) using state-of-the-art remote sensing and machine learning tools. Therefore, we developed a cloud-based framework for large-scale marine pollution detection with the integration of Sentinel-2 satellite imagery and advanced machine learning tools on the Sentinel Hub cloud application programming interface (API). Moreover, we evaluated the performance of two shallow machine learning algorithms of random forest (RF) and support vector machine (SVM), as well as the deep learning method of the generative adversarial network-random forest (GAN-RF) for the detection of ocean plastics in the pilot site of Mytilene Island, Greece. Based on the obtained results, the shallow algorithms of RF and SVM achieved an overall accuracy of 88% and 84%, respectively, with available training data of plastic debris. The GAN-RF classifier improved the detection of ocean plastics of the RF method by 8%, achieving an overall accuracy of 96% by generating several synthetic ocean plastic samples. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aegean Islands; Greece; Lesbos; Northern Aegean; Application programming interfaces (API); Classification (of information); Climate change; Debris; Decision trees; Deep learning; Elastomers; Learning algorithms; Marine pollution; Oil spills; Radiometers; Remote sensing; Satellite imagery; Sea level; Support vector machines; Cloud-based; Large-scale monitoring; Marine debris; Marine environment; Multi-spectral; Ocean plastic; Overall accuracies; Random forests; Sentinel hub; Support vectors machine; accuracy assessment; advanced technology; climate change; debris flow; deep water; marine environment; marine pollution; monitoring; monitoring system; plastic waste; satellite imagery; sea surface; Sentinel; spectral analysis; support vector machine; warming; Generative adversarial networks","Generative adversarial network; Marine debris; Marine pollution; Ocean plastics; Random forest; Sentinel Hub; Support vector machine","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115435905"
"Wang L.; Xu X.; Yu Y.; Yang R.; Gui R.; Xu Z.; Pu F.","Wang, Lei (57211488504); Xu, Xin (56294598500); Yu, Yue (57214104632); Yang, Rui (57208294306); Gui, Rong (57211231417); Xu, Zhaozhuo (57171068000); Pu, Fangling (13408173100)","57211488504; 56294598500; 57214104632; 57208294306; 57211231417; 57171068000; 13408173100","SAR-to-optical image translation using supervised cycle-consistent adversarial networks","2019","IEEE Access","7","","8825802","129136","129149","13","10.1109/ACCESS.2019.2939649","70","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078034428&doi=10.1109%2fACCESS.2019.2939649&partnerID=40&md5=a6086960f8b2c0f3cb16077d491dcb0b","Optical remote sensing (RS) data suffer from the limitation of bad weather and cloud contamination, whereas synthetic aperture radar (SAR) can work under all weather conditions and overcome this disadvantage of optical RS data. However, due to the imaging mechanism of SAR and the speckle noise, untrained people are difficult to recognize the land cover types visually from SAR images. Inspired by the excellent image-to-image translation performance of Generative Adversarial Networks (GANs), a supervised Cycle-Consistent Adversarial Network (S-CycleGAN) was proposed to generate large optical images from the SAR images. When the optical RS data are unavailable or partly unavailable, the generated optical images can be alternative data that aid in land cover visual recognition for untrained people. The main steps of SAR-to-optical image translation were as follows. First, the large SAR image was split to small patches. Then S-CycleGAN was used to translate the SAR patches to optical image patches. Finally, the optical image patches were stitched to generate the large optical image. A paired SAR-optical image dataset which covered 32 Chinese cities was published to evaluate the proposed method. The dataset was generated from Sentinel-1 (SEN-1) SAR images and Sentinel-2 (SEN-2) multi-spectral images. S-CycleGAN was applied to two experiments, which were SAR-to-optical image translation and cloud removal, and the results showed that S-CycleGAN could keep both the land cover and structure information well, and its performance was superior to some famous image-to-image translation models. © 2013 IEEE.","Flow visualization; Geometrical optics; Remote sensing; Spectroscopy; Synthetic aperture radar; Adversarial networks; Cloud contamination; Cloud removal; Multispectral images; Optical image; Optical remote sensing; Sentinel; Structure information; Radar imaging","cloud removal; GAN; SAR-to-optical image translation; Sentinel; visualization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078034428"
"Palsson F.; Sveinsson J.R.; Ulfarsson M.O.","Palsson, Frosti (55052918200); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","55052918200; 7003642214; 6507677875","Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","2018","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2018-September","","8747268","","","","10.1109/WHISPERS.2018.8747268","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072027457&doi=10.1109%2fWHISPERS.2018.8747268&partnerID=40&md5=5e989f4449d87f7a978a0f1c7090f7ec","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel2 datasets. © 2018 IEEE.","Convolution; Image processing; Image resolution; Infrared devices; Infrared radiation; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Electromagnetic spectra; Multispectral images; Multispectral sensors; Sentinel-2; Short wave infrared; Spatial resolution; Image fusion","convolutional network; generative adversarial network; Image fusion; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85072027457"
"Sedona R.; Paris C.; Cavallaro G.; Bruzzone L.; Riedel M.","Sedona, Rocco (57213598861); Paris, Claudia (56042202900); Cavallaro, Gabriele (55636444100); Bruzzone, Lorenzo (7006892410); Riedel, Morris (15770244400)","57213598861; 56042202900; 55636444100; 7006892410; 15770244400","A High-Performance Multispectral Adaptation GAN for Harmonizing Dense Time Series of Landsat-8 and Sentinel-2 Images","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","10134","10146","12","10.1109/JSTARS.2021.3115604","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117353591&doi=10.1109%2fJSTARS.2021.3115604&partnerID=40&md5=05a1938bd1d54b7fb5395d6ae8fdab4a","The combination of data acquired by Landsat-8 and Sentinel-2 earth observation missions produces dense time series (TSs) of multispectral images that are essential for monitoring the dynamics of land-cover and land-use classes across the earth's surface with high temporal resolution. However, the optical sensors of the two missions have different spectral and spatial properties, thus they require a harmonization processing step before they can be exploited in remote sensing applications. In this work, we propose a workflow-based on a deep learning approach to harmonize these two products developed and deployed on an high-performance computing environment. In particular, we use a multispectral generative adversarial network with a U-Net generator and a PatchGan discriminator to integrate existing Landsat-8 TSs with data sensed by the Sentinel-2 mission. We show a qualitative and quantitative comparison with an existing physical method [National Aeronautics and Space Administration (NASA) Harmonized Landsat and Sentinel (HLS)] and analyze original and generated data in different experimental setups with the support of spectral distortion metrics. To demonstrate the effectiveness of the proposed approach, a crop type mapping task is addressed using the harmonized dense TS of images, which achieved an overall accuracy of 87.83% compared to 81.66% of the state-of-the-art method.  © 2008-2012 IEEE.","Deep learning; Land use; NASA; Remote sensing; Time series; Deep learning; Dense time series; Dense-time; Generative adversarial network; Harmonisation; High performance computing; LANDSAT; Landsat-8; Performance computing; Remote sensing; Remote-sensing; Sentinel-2; Times series; Virtual constellation; land cover; land use change; Landsat; machine learning; multispectral image; remote sensing; satellite mission; Sentinel; time series analysis; Generative adversarial networks","Deep learning (DL); dense time series (TSs); generative adversarial network (GAN); harmonization; high performance computing (HPC); Landsat-8; remote sensing (RS); sentinel-2; virtual constellation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117353591"
"Pham V.-D.; Bui Q.-T.","Pham, Vu-Dong (57202350777); Bui, Quang-Thanh (57189899688)","57202350777; 57189899688","Spatial resolution enhancement method for Landsat imagery using a Generative Adversarial Network","2021","Remote Sensing Letters","12","7","","654","665","11","10.1080/2150704X.2021.1918789","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105127268&doi=10.1080%2f2150704X.2021.1918789&partnerID=40&md5=92fd70c5ee8e4a888827caa02442244e","Landsat and Sentinel-2 are two freely accessible satellite data that are relevant for global land cover monitoring. However, the uses of the latter data set are growing because of its higher spatial resolutions and the availability of benchmark data sets for deep learning applications. In this study, we integrate a style transfer (perceptual loss estimation from Sentinel 2 benchmark data) into a Generative Adversarial Network (GAN) to construct a single image super-resolution model. The proposed model upscales Landsat 8 images (using red, green, blue, and near-infrared bands at 30 m and Panchromatic band 15 m for high-resolution features exploiting) to 10 m (with Sentinel-2 as reference). Compared to pan-sharpening and other upscaling methods, the proposed method can produce more realistic, spatial convincing images at 10 m resolution and more similar to Sentinel-2 images than the other commonly used super-resolution imaging algorithms. As a result, the proposed method extends the usage of high-resolution benchmark data sets for lower resolution imagery to enrich supplement data sources for land cover classification. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Benchmarking; Classification (of information); Deep learning; Infrared devices; Optical resolving power; Adversarial networks; Land cover classification; Near infrared band; Panchromatic bands; Spatial resolution; Spatial-resolution enhancement; Super resolution imaging; Upscaling methods; algorithm; image resolution; land cover; Landsat; satellite data; satellite imagery; Sentinel; spatial resolution; Image enhancement","","Article","Final","","Scopus","2-s2.0-85105127268"
"Romero L.S.; Marcello J.; Vilaplana V.","Romero, Luis Salgueiro (57218455911); Marcello, Javier (6602158797); Vilaplana, Verónica (23394280500)","57218455911; 6602158797; 23394280500","Super-resolution of Sentinel-2 imagery using generative adversarial networks","2020","Remote Sensing","12","15","2424","","","","10.3390/RS12152424","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089853089&doi=10.3390%2fRS12152424&partnerID=40&md5=6bb6fc362a861fc8952daf3ca71d5b36","Sentinel-2 satellites provide multi-spectral optical remote sensing images with four bands at 10 m of spatial resolution. These images, due to the open data distribution policy, are becoming an important resource for several applications. However, for small scale studies, the spatial detail of these images might not be sufficient. On the other hand, WorldView commercial satellites offer multi-spectral images with a very high spatial resolution, typically less than 2 m, but their use can be impractical for large areas or multi-temporal analysis due to their high cost. To exploit the free availability of Sentinel imagery, it is worth considering deep learning techniques for single-image super-resolution tasks, allowing the spatial enhancement of low-resolution (LR) images by recovering high-frequency details to produce high-resolution (HR) super-resolved images. In this work, we implement and train a model based on the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) with pairs of WorldView-Sentinel images to generate a super-resolved multispectral Sentinel-2 output with a scaling factor of 5. Our model, named RS-ESRGAN, removes the upsampling layers of the network to make it feasible to train with co-registered remote sensing images. Results obtained outperform state-of-the-art models using standard metrics like PSNR, SSIM, ERGAS, SAM and CC. Moreover, qualitative visual analysis shows spatial improvements as well as the preservation of the spectral information, allowing the super-resolved Sentinel-2 imagery to be used in studies requiring very high spatial resolution. © 2020 by the authors.","Deep learning; Image analysis; Image resolution; Network layers; Open Data; Optical resolving power; Remote sensing; Spectroscopy; Commercial satellites; Data distribution policies; Low resolution images; Multi-temporal analysis; Optical remote sensing; Remote sensing images; Spectral information; Very high spatial resolutions; Image enhancement","Deep learning; Generative adversarial network; Sentinel-2; Super-resolution; WorldView","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089853089"
"Ren C.X.; Ziemann A.; Theiler J.; Durieux A.M.S.","Ren, Christopher X. (57004276700); Ziemann, Amanda (36134071500); Theiler, James (7004449154); Durieux, Alice M.S. (57212026338)","57004276700; 36134071500; 7004449154; 57212026338","Cycle-Consistent Adversarial Networks for Realistic Pervasive Change Generation in Remote Sensing Imagery","2020","Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation","2020-March","","9094603","42","45","3","10.1109/SSIAI49293.2020.9094603","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085490462&doi=10.1109%2fSSIAI49293.2020.9094603&partnerID=40&md5=17ae73c5ca32f7d9f596595252ae0d13","This paper introduces a new method of generating realistic pervasive changes in the context of evaluating the effectiveness of change detection algorithms in controlled settings. The method-a cycle-consistent adversarial network (CycleGAN)-requires low quantities of training data to generate realistic changes. Here we show an application of CycleGAN in creating realistic snow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how these images can be used as a test bed for anomalous change detection algorithms. © 2020 IEEE.","Remote sensing; Signal detection; Adversarial networks; Change detection algorithms; Multi-spectral; Remote sensing imagery; Training data; Image analysis","change detection; deep learning; generative adversarial networks; image analysis; multispectral; remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085490462"
"Kwak G.-H.; Park S.; Park N.-W.","Kwak, Geun-Ho (57206203736); Park, Soyeon (57215420514); Park, No-Wook (7202111787)","57206203736; 57215420514; 7202111787","Combining Conditional Generative Adversarial Network and Regression-based Calibration for Cloud Removal of Optical Imagery","2022","Korean Journal of Remote Sensing","38","6","","1357","1369","12","10.7780/kjrs.2022.38.6.1.28","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147761645&doi=10.7780%2fkjrs.2022.38.6.1.28&partnerID=40&md5=50bab79b66922c0c53360cbf3c6141c5","Cloud removal is an essential image processing step for any task requiring time-series optical images, such as vegetation monitoring and change detection. This paper presents a two-stage cloud removal method that combines conditional generative adversarial networks (cGANs) with regression-based calibration to construct a cloud-free time-series optical image set. In the first stage, the cGANs generate initial prediction results using quantitative relationships between optical and synthetic aperture radar images. In the second stage, the relationships between the predicted results and the actual values in non-cloud areas are first quantified via random forest-based regression modeling and then used to calibrate the cGAN-based prediction results. The potential of the proposed method was evaluated from a cloud removal experiment using Sentinel-2 and COSMO-SkyMed images in the rice field cultivation area of Gimje. The cGAN model could effectively predict the reflectance values in the cloud-contaminated rice fields where severe changes in physical surface conditions happened. Moreover, the regression-based calibration in the second stage could improve the prediction accuracy, compared with a regression-based cloud removal method using a supplementary image that is temporally distant from the target image. These experimental results indicate that the proposed method can be effectively applied to restore cloud-contaminated areas when cloud-free optical images are unavailable for environmental monitoring. © 2022 The authors.","","Cloud removal; Conditional generative adversarial networks (cGANs); Random forest; SAR","Article","Final","","Scopus","2-s2.0-85147761645"
"Chen C.; Ma Y.; Ren G.; Wang J.","Chen, Chen (57196286563); Ma, Yi (35220028600); Ren, Guangbo (36996652600); Wang, Jianbu (57202392623)","57196286563; 35220028600; 36996652600; 57202392623","Aboveground biomass of salt-marsh vegetation in coastal wetlands: Sample expansion of in situ hyperspectral and Sentinel-2 data using a generative adversarial network","2022","Remote Sensing of Environment","270","","112885","","","","10.1016/j.rse.2021.112885","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122621014&doi=10.1016%2fj.rse.2021.112885&partnerID=40&md5=64cd30c8abb81c68e27904759e6101a7","Coastal wetlands are main components of the “blue carbon” ecosystems in coastal zones. Salt-marsh biomass is especially important regarding climate-change mitigation. Generating high precision biomass maps for evaluating the ecological functions of coastal wetlands is essential; however, conducting accurate biomass inversions with limited in situ observations from coastal wetlands is challenging. We propose a generative adversarial network with a constrained factor model (GAN-CF) for expanding limited in situ salt-marsh biomass observations. We used Sentinel-2 images and a deep belief network based on the conjugate gradient method (CG-DBN) for obtaining land-cover maps and the salt-marsh distribution (species: Phragmites australis, Suaeda glauca, Spartina alterniflora, and mixed species dominated by Tamarix chinensis) in the study area. This study bridges in situ hyperspectral and Sentinel-2 multispectral data by a satellite-band equivalent conversion model. The biomass and multispectral data derived from Sentinel-2 were used as input for the proposed GAN-CF model, which produced and constrained the generated samples based on the features (i.e., spectra, vegetation index, and biomass) of the in situ observations. Aboveground biomass (AGB) maps at 10-m spatial resolution were produced by constructing multiple linear regression models (MLRMs) based on the generated samples of each salt-marsh type using Sentinel-2 images. The quantity and richness of the generated samples improved the AGB estimations in the study area. The inversion accuracy of S. alterniflora was significantly improved (RMSE = 3.71 Mg/ha); the estimated AGB was strongly related to the in situ observations (R = 0.923). The estimated AGB was validated using in situ observations. The total amount of salt-marsh AGB in the study area in 2019 was estimated at 2.36 × 105 Mg, with 7.95 Mg/ha average. The salt-marsh biomass in decreasing order was as follows: P. australis (12.7 Mg/ha) > S. alterniflora (11.5 Mg/ha) > mixed species (8.97 Mg/ha) > S. glauca (2.18 Mg/ha). The salt-marsh area in decreasing order was as follows: S. glauca (10,410 ha) > P. australis (7320 ha) > mixed species (6740 ha) > S. alterniflora (5240 ha). By a feasibility analysis we estimated the biomass based on the Sentinel-2 data covering the Yellow River delta wetland in May, July, and September 2019 and the Jiaozhou Bay wetland in September 2019 by using the generated samples. The generated samples based on the 2013–2019 in situ observations constitute a salt-marsh biomass database, which can be useful for quantifying the regional carbon storage and ecological restoration monitoring. © 2022 Elsevier Inc.","China; Jiaozhou Bay; Shandong; Yellow River Delta; Biodiversity; Climate change; Coastal zones; Ecology; Linear regression; Vegetation; Wetlands; Aboveground biomass; Biomass map; Coastal wetlands; HyperSpectral; In-situ observations; Mixed species; Salt marshes; Sample expansion; Sentinel-2; Study areas; aboveground biomass; carbon storage; coastal wetland; coastal zone; in situ measurement; saltmarsh; satellite data; satellite imagery; Sentinel; spectral analysis; vegetation cover; Biomass","aboveground biomass; Coastal wetland; Generative adversarial network; Sample expansion; Sentinel-2","Article","Final","","Scopus","2-s2.0-85122621014"
"Panagiotou E.; Chochlakis G.; Grammatikopoulos L.; Charou E.","Panagiotou, Emmanouil (57217281424); Chochlakis, Georgios (57217281247); Grammatikopoulos, Lazaros (16068804700); Charou, Eleni (6507509159)","57217281424; 57217281247; 16068804700; 6507509159","Generating elevation surface from a single RGB remotely sensed image using deep learning","2020","Remote Sensing","12","12","2002","","","","10.3390/rs12122002","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087000238&doi=10.3390%2frs12122002&partnerID=40&md5=9ed85b9d4bff82ed6c458fa5d78c05c5","Generating Digital Elevation Models (DEM) from satellite imagery or other data sources constitutes an essential tool for a plethora of applications and disciplines, ranging from 3D flight planning and simulation, autonomous driving and satellite navigation, such as GPS, to modeling water flow, precision farming and forestry. The task of extracting this 3D geometry from a given surface hitherto requires a combination of appropriately collected corresponding samples and/or specialized equipment, as inferring the elevation from single image data is out of reach for contemporary approaches. On the other hand, Artificial Intelligence (AI) and Machine Learning (ML) algorithms have experienced unprecedented growth in recent years as they can extrapolate rules in a data-driven manner and retrieve convoluted, nonlinear one-to-one mappings, such as an approximate mapping from satellite imagery to DEMs. Therefore, we propose an end-to-end Deep Learning (DL) approach to construct this mapping and to generate an absolute or relative point cloud estimation of a DEM given a single RGB satellite (Sentinel-2 imagery in this work) or drone image. The model has been readily extended to incorporate available information from the non-visible electromagnetic spectrum. Unlike existing methods, we only exploit one image for the production of the elevation data, rendering our approach less restrictive and constrained, but suboptimal compared to them at the same time. Moreover, recent advances in software and hardware allow us to make the inference and the generation extremely fast, even on moderate hardware. We deploy Conditional Generative Adversarial networks (CGAN), which are the state-of-the-art approach to image-to-image translation. We expect our work to serve as a springboard for further development in this field and to foster the integration of such methods in the process of generating, updating and analyzing DEMs. © 2020 by the authors.","Flight simulators; Flow of water; Mapping; Satellite imagery; Surveying; Three dimensional computer graphics; Adversarial networks; Digital elevation model; Electromagnetic spectra; Remotely sensed images; Satellite navigation; Software and hardwares; Specialized equipment; State-of-the-art approach; Deep learning","3D point cloud; Deep learning; Digital elevation models; Drones; Generative adversarial networks; Height maps; Remote sensing; Satellite imagery","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85087000238"
"Hu X.; Zhang P.; Ban Y.","Hu, Xikun (57223380183); Zhang, Puzhao (57190619273); Ban, Yifang (7202222338)","57223380183; 57190619273; 7202222338","Gan-based SAR to Optical Image Translation in Fire-Disturbed Regions","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1456","1459","3","10.1109/IGARSS46834.2022.9884234","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140393425&doi=10.1109%2fIGARSS46834.2022.9884234&partnerID=40&md5=c4d70c65dbb8e7913a15c0ab02a9331d","Climate change by anthropogenic warming leads to increases in dry fuels and promotes forest fires. Multispectral images' quality is easily affected by poor atmospheric conditions. SAR satellite sensors can penetrate through clouds and image day and night. However, the burned area mapping methods widely used for optical data are not feasible to be applied for SAR data owing to the differences in imaging mechanisms. Recent advances in deep image translation can fill this gap by using Generative Adversarial Networks (GAN). In this research, we apply a GAN-based model for SAR to optical image translation over fire-disturbed regions. Specifically, Sentinel-1 SAR images are translated into Sentinel-2 images using the ResNet-based Pix2Pix model, which is trained on 281 large fire events and tested on the other 23 events in Canada. The generated images preserve the spectral characteristics well and show high similarity to the real images with Structure Similarity Index Measure (SSIM) over 0.59. © 2022 IEEE.","Climate change; Deforestation; Fire hazards; Fires; Generative adversarial networks; Geology; Geometrical optics; Radar imaging; Remote sensing; Anthropogenic warming; C-bands; Dry fuels; GaN based; Generative adversarial network; Image translation; Optical image; Sentinel-1; Sentinel-1 C band SAR; Sentinel-2 MSI; Synthetic aperture radar","Generative Adversarial Network (GAN); image Translation; Sentinel-1 C band SAR; Sentinel-2 MSI","Conference paper","Final","","Scopus","2-s2.0-85140393425"
"Gong Y.; Liao P.; Zhang X.; Zhang L.; Chen G.; Zhu K.; Tan X.; Lv Z.","Gong, Yuanfu (57200512932); Liao, Puyun (57208162705); Zhang, Xiaodong (57192504939); Zhang, Lifei (57207389916); Chen, Guanzhou (56181390800); Zhu, Kun (57200511212); Tan, Xiaoliang (57202231708); Lv, Zhiyong (23111268400)","57200512932; 57208162705; 57192504939; 57207389916; 56181390800; 57200511212; 57202231708; 23111268400","Enlighten-gan for super resolution reconstruction in mid-resolution remote sensing images","2021","Remote Sensing","13","6","1104","","","","10.3390/rs13061104","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103080085&doi=10.3390%2frs13061104&partnerID=40&md5=5d1cb69627959e3776eec7312ce71b58","Previously, generative adversarial networks (GAN) have been widely applied on super resolution reconstruction (SRR) methods, which turn low-resolution (LR) images into high-resolution (HR) ones. However, as these methods recover high frequency information with what they observed from the other images, they tend to produce artifacts when processing unfamiliar images. Optical satellite remote sensing images are of a far more complicated scene than natural images. Therefore, applying the previous networks on remote sensing images, especially mid-resolution ones, leads to unstable convergence and thus unpleasing artifacts. In this paper, we propose Enlighten-GAN for SRR tasks on large-size optical mid-resolution remote sensing images. Specifically, we design the enlighten blocks to induce network converging to a reliable point, and bring the Self-Supervised Hierarchical Perceptual Loss to attain performance improvement overpassing the other loss functions. Furthermore, limited by memory, large-scale images need to be cropped into patches to get through the network separately. To merge the reconstructed patches into a whole, we employ the internal inconsistency loss and cropping-and-clipping strategy, to avoid the seam line. Experiment results certify that Enlighten-GAN outperforms the state-of-the-art methods in terms of gradient similarity metric (GSM) on mid-resolution Sentinel-2 remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Global system for mobile communications; Optical resolving power; Remote sensing; Adversarial networks; High-frequency informations; Low resolution images; Optical satellites; Remote sensing images; Similarity metrics; State-of-the-art methods; Super resolution reconstruction; Image reconstruction","Generative adversarial network; Mid-resolution remote sensing images; Super resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103080085"
"Martinez J.A.C.; Adarme M.X.O.; Turnes J.N.; Costa G.A.O.P.; De Almeida C.A.; Feitosa R.Q.","Martinez, J.A.C. (57218451094); Adarme, M.X.O. (57274825100); Turnes, J.N. (57216587254); Costa, G.A.O.P. (25642386000); De Almeida, C.A. (57209845177); Feitosa, R.Q. (6602453684)","57218451094; 57274825100; 57216587254; 25642386000; 57209845177; 6602453684","A COMPARISON OF CLOUD REMOVAL METHODS FOR DEFORESTATION MONITORING IN AMAZON RAINFOREST","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2022","","665","671","6","10.5194/isprs-archives-XLIII-B3-2022-665-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131917969&doi=10.5194%2fisprs-archives-XLIII-B3-2022-665-2022&partnerID=40&md5=8447700e1fefacd1f19230a481c218a7","Deforestation in tropical rainforests is a major source of carbon dioxide emissions, an important driver of climate change. For decades, the Brazilian government has maintained monitoring programs for deforestation detection in the Brazilian Legal Amazon area based on remotely sensed optical images in a protocol that involves considerable efforts of visual interpretation. However, the Amazon region is covered with clouds for most of the year, and deforestation assessment can rely only on images acquired in the dry season when cloud-free images are more likely to capture. One possibility to lessen that restriction and enable deforestation detection throughout the year is to synthesize cloud-free optical images from corresponding SAR images, which are only marginally influenced by atmospheric conditions. This work compares a set of such image synthesis methods, considering deforestation detection in the Amazon forest as the target application. Specifically, we evaluate three deep learning methods for cloud removal in Sentinel-2 images: a conditional Generative Adversarial Network (cGAN) based on the pix2pixi architecture; an extension of that method, which uses atrous convolutions (Atrous cGANi) to enhance fine image details; and a non-generative method (DSen2-CRi) based on residual networks. In the evaluation, we assess both the quality of the generated images and the accuracy obtained when performing deforestation detection from those images. We further compare those methods with an image aggregation tool available in Google Earth Engine (GEE Tooli), which creates cloud-free mosaics from sequences of images acquired at nearby dates. In this study, we considered two sites in the Brazilian Amazon, characterized by distinct vegetation and deforestation patterns. In terms of the quality metrics and classification accuracy, the Atrous cGANi was the best performing deep learning method. The GEE Tooli outperformed all those methods when dealing with images from the dry season but turned out to be the poorest performing method in the wet season.  © Authors 2022","Carbon dioxide; Data fusion; Deep learning; Deforestation; Generative adversarial networks; Geometrical optics; Image acquisition; Image enhancement; Quality control; Radar imaging; Synthetic aperture radar; Amazon rain forest; Cloud removal; Deep learning; Dry seasons; Learning methods; Optical data; Optical image; Optical imagery; Removal method; SAR-optical data fusion; Global warming","Cloud Removal; Deep learning; Deforestation; Optical imagery; SAR-optical Data fusion","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131917969"
"Efremova N.; Erten E.","Efremova, Natalia (42261427800); Erten, E. (23011359600)","42261427800; 23011359600","BIOPHYSICAL PARAMETER ESTIMATION USING EARTH OBSERVATION DATA IN A MULTI-SENSOR DATA FUSION APPROACH: CYCLEGAN","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","5965","5968","3","10.1109/IGARSS47720.2021.9553561","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125673847&doi=10.1109%2fIGARSS47720.2021.9553561&partnerID=40&md5=1ef050ca029c0e825bcd1a4f163f262c","Water management and up-to-date soil moisture (SM) information are crucial to ensure agricultural activities in dry-land farming regions. In this context, remote sensing imagery coupled with machine learning techniques can provide large scale SM information if there is enough data for training, which is really limited in reality. In this paper, we explored the potential of cycle-consistent Generative Adversarial Network (GAN) for data augmentation for training machine learning algorithms, which try to model spatial and temporal dependencies between the SM prediction (output) and the remote sensing imagery (input features). Specifically, the freely available SAR (Sentinel-1) and optical (Sentinel-2) time series data were evaluated together to predict SM using GANs. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there is not enough data to train a regression convolutional neural networks (CNN) to predict SM content. © 2021 IEEE.","","Autoencoders; CNN; CycleGAN; PCA; Ridge regression; Sentinel-1; Sentinel-2; Soil moisture; Support vector regression","Conference paper","Final","","Scopus","2-s2.0-85125673847"
"Khan A.A.; Jamil A.; Hussain D.; Ali I.; Hameed A.A.","Khan, Aftab Ahmed (57198816614); Jamil, Akhtar (49863650600); Hussain, Dostdar (57192295436); Ali, Imran (57193623081); Hameed, Alaa Ali (56338374100)","57198816614; 49863650600; 57192295436; 57193623081; 56338374100","Deep learning-based framework for monitoring of debris-covered glacier from remotely sensed images","2022","Advances in Space Research","","","","","","","10.1016/j.asr.2022.05.060","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131797739&doi=10.1016%2fj.asr.2022.05.060&partnerID=40&md5=069db8078afc5f4b947adde38f2b4418","In recent years, deep learning (DL) methods have proven their efficiency for various computer vision (CV) tasks such as image classification, natural language processing, and object detection. However, training a DL model is expensive in terms of both complexities of the network structure and the amount of labeled data needed. In addition, the imbalance among available labeled data for different classes of interest may also adversely affect the model accuracy. This paper addresses these issues using a new convolutional neural network (CNN) based architecture. The proposed network incorporates both spatial and spectral information that combines two sub-networks: spatial-CNN and spectral-CNN. The spectral-CNN extracts spectral information, while spatial-CNN captures spatial information. Moreover, to make the features more robust, a multiscale spatial CNN architecture is introduced using different kernels. The final feature vector is formed by concatenating the outputs obtained from both spatial-CNN and spectral-CNN. To address the data imbalance problem, a generative adversarial network (GAN) was used to generate data for the underrepresented class. Finally, relatively a shallower network architecture was used to reduce the number of parameters in the network and improve the processing speed. The proposed model was trained and tested on Senitel-2 images for the classification of the debris-covered glacier. The results showed that the proposed method is well-suited for mapping and monitoring debris-covered glaciers at a large scale with high classification accuracy. In addition, we compared the proposed method with conventional machine learning approaches, support vector machine (SVM), random forest (RF) and multilayer perceptron (MLP). © 2022 COSPAR","Convolution; Convolutional neural networks; Debris; Decision trees; Deep learning; Generative adversarial networks; Learning algorithms; Natural language processing systems; Network architecture; Object detection; Convolutional neural network; Debris-covered glacier; Images classification; Labeled data; Learning methods; Learning models; Remotely sensed images; Sentinel-2; Spatial informations; Spectral information; Support vector machines","Convolutional neural network; Debris-covered glacier; Generative adversarial network; Machine learning; Sentinel-2","Article","Article in press","","Scopus","2-s2.0-85131797739"
"Requena-Mesa C.; Reichstein M.; Mahecha M.; Kraft B.; Denzler J.","Requena-Mesa, Christian (57208244647); Reichstein, Markus (57206534330); Mahecha, Miguel (16444433900); Kraft, Basil (57207884535); Denzler, Joachim (6701534437)","57208244647; 57206534330; 16444433900; 57207884535; 6701534437","Predicting Landscapes from Environmental Conditions Using Generative Networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11824 LNCS","","","203","217","14","10.1007/978-3-030-33676-9_14","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076136395&doi=10.1007%2f978-3-030-33676-9_14&partnerID=40&md5=9551024c6898e2bfe7c456c97c0667f6","Landscapes are meaningful ecological units that strongly depend on the environmental conditions. Such dependencies between landscapes and the environment have been noted since the beginning of Earth sciences and cast into conceptual models describing the interdependencies of climate, geology, vegetation and geomorphology. Here, we ask whether landscapes, as seen from space, can be statistically predicted from pertinent environmental conditions. To this end we adapted a deep learning generative model in order to establish the relationship between the environmental conditions and the view of landscapes from the Sentinel-2 satellite. We trained a conditional generative adversarial network to generate multispectral imagery given a set of climatic, terrain and anthropogenic predictors. The generated imagery of the landscapes share many characteristics with the real one. Results based on landscape patch metrics, indicative of landscape composition and structure, show that the proposed generative model creates landscapes that are more similar to the targets than the baseline models while overall reflectance and vegetation cover are predicted better. We demonstrate that for many purposes the generated landscapes behave as real with immediate application for global change studies. We envision the application of machine learning as a tool to forecast the effects of climate change on the spatial features of landscapes, while we assess its limitations and breaking points. © Springer Nature Switzerland AG 2019.","Climate change; Deep learning; Earth (planet); Pattern recognition; Vegetation; Adversarial networks; Conceptual model; Ecological units; Environmental conditions; Generative model; Landscape composition; Multi-spectral imagery; Spatial features; Climate models","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076136395"
"Singh A.; Bruzzone L.","Singh, Abhishek (57221148913); Bruzzone, Lorenzo (7006892410)","57221148913; 7006892410","SIGAN: Spectral Index Generative Adversarial Network for Data Augmentation in Multispectral Remote Sensing Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3093238","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122358038&doi=10.1109%2fLGRS.2021.3093238&partnerID=40&md5=666a4b602b3b4c84986808d2c4c08de9","Generative models are typically employed to approximate the distribution of deep features. Recently, these state-of-the-art methods have been applied to estimate image transformations by an unsupervised learning approach. In this letter, a novel spectral index generative adversarial network (SIGAN) is proposed for the generation of multispectral (MS) remote sensing images. This network is defined to effectively perform data augmentation starting from a limited number of training samples in the MS remote sensing domain for training deep learning models. The SIGAN model is able to capture class-specific properties in data augmentation, by incorporating the task-specific normalized spectral indices to model class-by-class properties of MS images. Experimental results obtained on a Sentinel 2 dataset show that the proposed model provides better performance than other generative adversarial networks (GANs) in MS data generation.  © 2004-2012 IEEE.","Deep learning; Remote sensing; Data augmentation; Generative model; Image transformations; Labeled sample; Multispectral images; Multispectral remote sensing image; Remote-sensing; Spectral indices; State-of-the-art methods; Training sample; estimation method; machine learning; remote sensing; sampling; satellite data; satellite imagery; Sentinel; unsupervised classification; Generative adversarial networks","Data augmentation; labeled sample; multispectral (MS) images; remote sensing","Article","Final","","Scopus","2-s2.0-85122358038"
"Pineda F.; Ayma V.; Beltran C.","Pineda, F. (57216822078); Ayma, V. (56566776600); Beltran, C. (55602499700)","57216822078; 56566776600; 55602499700","A generative adversarial network approach for super-resolution of sentinel-2 satellite images","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B1","","9","14","5","10.5194/isprs-archives-XLIII-B1-2020-9-2020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091133545&doi=10.5194%2fisprs-archives-XLIII-B1-2020-9-2020&partnerID=40&md5=6de901c895119a77b97c4ee7b956660b","High-resolution satellite images have always been in high demand due to the greater detail and precision they offer, as well as the wide scope of the fields in which they could be applied; however, satellites in operation offering very high-resolution (VHR) images has experienced an important increase, but they remain as a smaller proportion against existing lower resolution (HR) satellites. Recent models of convolutional neural networks (CNN) are very suitable for applications with image processing, like resolution enhancement of images; but in order to obtain an acceptable result, it is important, not only to define the kind of CNN architecture but the reference set of images to train the model. Our work proposes an alternative to improve the spatial resolution of HR images obtained by Sentinel-2 satellite by using the VHR images from PeruSat1, a peruvian satellite, which serve as the reference for the super-resolution approach implementation based on a Generative Adversarial Network (GAN) model, as an alternative for obtaining VHR images. The VHR PeruSat-1 image dataset is used for the training process of the network. The results obtained were analyzed considering the Peak Signal to Noise Ratios (PSNR) and the Structural Similarity (SSIM). Finally, some visual outcomes, over a given testing dataset, are presented so the performance of the model could be analyzed as well. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Convolutional neural networks; Optical resolving power; Satellites; Signal to noise ratio; Statistical tests; Well testing; Adversarial networks; High resolution satellite images; Peak signal to noise ratio; Resolution enhancement; Satellite images; Spatial resolution; Structural similarity; Very high resolution (VHR) image; Image enhancement","GAN; PeruSat-1; Sentinel-2; Super-Resolution","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091133545"
"Jiang M.; Shen H.; Li J.","Jiang, Menghui (57210173702); Shen, Huanfeng (8359721100); Li, Jie (57214207213)","57210173702; 8359721100; 57214207213","Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5410915","","","","10.1109/TGRS.2022.3188998","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135232679&doi=10.1109%2fTGRS.2022.3188998&partnerID=40&md5=8337c148bae6361ca03536bbf9d398c0","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal-spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.  © 1980-2012 IEEE.","Deep learning; Feature extraction; Feedback; Generative adversarial networks; Optical remote sensing; Radiometers; Satellite imagery; Cloud cover; Deep residual cycle generative adversarial network; Features extraction; Generator; Heterogeneous integrated framework; Integrated frameworks; Land-cover change; Remote-sensing; Spatial resolution; Thick cloud cover; cloud cover; integrated approach; Landsat; MODIS; network analysis; remote sensing; satellite imagery; Sentinel; spatiotemporal analysis; Image fusion","Deep residual cycle generative adversarial network (GAN); heterogeneous integrated framework; land-cover change; thick cloud cover","Article","Final","","Scopus","2-s2.0-85135232679"
"Karaca A.C.; Kara O.; Güllü M.K.","Karaca, Ali Can (55292760600); Kara, Ozan (57221815490); Güllü, Mehmet Kemal (55666247200)","55292760600; 57221815490; 55666247200","MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks","2021","Journal of Visual Communication and Image Representation","81","","103385","","","","10.1016/j.jvcir.2021.103385","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119176940&doi=10.1016%2fj.jvcir.2021.103385&partnerID=40&md5=0dbc36c8a377487bfb873208d19d64b7","Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications. © 2021 Elsevier Inc.","Big data; Deep learning; Generative adversarial networks; Geographical regions; Image compression; Mean square error; Signal to noise ratio; Change detection; Compression methods; Multi-spectral; Multi-temporal; Multi-temporal image; Multispectral images; Multispectral-image compression; Reflected energy; Remote-sensing; Spectral correlation; Remote sensing","Big data; Generative adversarial networks; Multispectral image compression; Multitemporal images; Remote sensing","Article","Final","","Scopus","2-s2.0-85119176940"
"Ebel P.; Meraner A.; Schmitt M.; Zhu X.X.","Ebel, Patrick (57409415200); Meraner, Andrea (57214798706); Schmitt, Michael (7401931279); Zhu, Xiao Xiang (55696622200)","57409415200; 57214798706; 7401931279; 55696622200","Multisensor Data Fusion for Cloud Removal in Global and All-Season Sentinel-2 Imagery","2021","IEEE Transactions on Geoscience and Remote Sensing","59","7","9211498","5866","5878","12","10.1109/TGRS.2020.3024744","30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112264501&doi=10.1109%2fTGRS.2020.3024744&partnerID=40&md5=6e7f290b5f3f7d4e16976c8e3197236a","The majority of optical observations acquired via spaceborne Earth imagery are affected by clouds. While there is numerous prior work on reconstructing cloud-covered information, previous studies are, oftentimes, confined to narrowly defined regions of interest, raising the question of whether an approach can generalize to a diverse set of observations acquired at variable cloud coverage or in different regions and seasons. We target the challenge of generalization by curating a large novel data set for training new cloud removal approaches and evaluate two recently proposed performance metrics of image quality and diversity. Our data set is the first publically available to contain a global sample of coregistered radar and optical observations, cloudy and cloud-free. Based on the observation that cloud coverage varies widely between clear skies and absolute coverage, we propose a novel model that can deal with either extreme and evaluate its performance on our proposed data set. Finally, we demonstrate the superiority of training models on real over synthetic data, underlining the need for a carefully curated data set of real observations. To facilitate future research, our data set is made available online.  © 1980-2012 IEEE.","Data fusion; Cloud coverage; Cloud removal; Multisensor data fusion; Optical observations; Performance metrics; Regions of interest; Synthetic data; Training model; cloud cover; data set; detection method; seasonal variation; Sentinel; Quality control","Cloud removal; Data fusion; Deep learning; Generative adversarial network (GAN); Optical imagery; Synthetic aperture radar (SAR)-optical","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85112264501"
"Yuan X.; Tian J.; Reinartz P.","Yuan, X. (57210963035); Tian, J. (36987117000); Reinartz, P. (56216874200)","57210963035; 36987117000; 56216874200","Generating artificial near infrared spectral band from rgb image using conditional generative adversarial network","2020","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","279","285","6","10.5194/isprs-Annals-V-3-2020-279-2020","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090342564&doi=10.5194%2fisprs-Annals-V-3-2020-279-2020&partnerID=40&md5=8d8086af7fca14d2cb6f01e9f0da0207","Near infrared bands (NIR) provide rich information for many remote sensing applications. In addition to deriving useful indices to delineate water and vegetation, near infrared channels could also be used to facilitate image pre-processing. However, synthesizing bands from RGB spectrum is not an easy task. The inter-correlations between bands are not clearly identified in physical models. Generative adversarial networks (GAN) have been used in many tasks such as generating photorealistic images, monocular depth estimation and Digital Surface Model (DSM) refinement etc. Conditional GAN is different in that it observes some data as a condition. In this paper, we explore a cGAN network structure to generate a NIR spectral band that is conditioned on the input RGB image. We test different discriminators and loss functions, and evaluate results using various metrics. The best simulated NIR channel has a mean absolute error of around 5 percent in Sentinel-2 dataset. In addition, the simulated NIR image can correctly distinguish between various classes of landcover. © Authors 2020. All rights reserved.","Remote sensing; Adversarial networks; Digital surface models; Image preprocessing; Mean absolute error; Near infrared spectral; Near-infrared channels; Photorealistic images; Remote sensing applications; Infrared devices","Conditional GAN; Gerative adversarial networks; Near-infrared; RGB; Robust loss function","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090342564"
"Li Y.; Wang Y.; Li B.; Wu S.","Li, Yunhe (55647591200); Wang, Yi (57204548320); Li, Bo (57777715900); Wu, Shaohua (57189245768)","55647591200; 57204548320; 57777715900; 57189245768","Super-Resolution of Remote Sensing Images for ×4 Resolution without Reference Images","2022","Electronics (Switzerland)","11","21","3474","","","","10.3390/electronics11213474","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141875470&doi=10.3390%2felectronics11213474&partnerID=40&md5=2b0b794fd72a9dfbed4cf586766ec1e8","Sentinel-2 satellites can provide free optical remote-sensing images with a spatial resolution of up to 10 M, but the spatial details provided are not enough for many applications, so it is worth considering improving the spatial resolution of Sentinel-2 satellites images through super-resolution (SR). Currently, the most effective SR models are mainly based on deep learning, especially the generative adversarial network (GAN). Models based on GAN need to be trained on LR–HR image pairs. In this paper, a two-step super-resolution generative adversarial network (TS-SRGAN) model is proposed. The first step is having the GAN train the degraded models. Without supervised HR images, only the 10 m resolution images provided by Sentinel-2 satellites are used to generate the degraded images, which are in the same domain as the real LR images, and then to construct the near-natural LR–HR image pairs. The second step is to design a super-resolution generative adversarial network with strengthened perceptual features, to enhance the perceptual effects of the generated images. Through experiments, the proposed method obtained an average NIQE as low as 2.54, and outperformed state-of-the-art models according to other two NR-IQA metrics, such as BRISQUE and PIQE. At the same time, the comparison of the intuitive visual effects of the generated images also proved the effectiveness of TS-SRGAN. © 2022 by the authors.","","generative adversarial network; remote-sensing image; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141875470"
"Lavreniuk M.; Kussul N.; Shelestov A.; Lavrenyuk A.; Shumilo L.","Lavreniuk, Mykola (56667743100); Kussul, Nataliia (6602485938); Shelestov, Andrii (6507365226); Lavrenyuk, Alla (16444915500); Shumilo, Leonid (57208256914)","56667743100; 6602485938; 6507365226; 16444915500; 57208256914","Super Resolution Approach for the Satellite Data Based on the Generative Adversarial Networks","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1095","1098","3","10.1109/IGARSS46834.2022.9884460","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140369300&doi=10.1109%2fIGARSS46834.2022.9884460&partnerID=40&md5=9273c26a2a94b7f402c373876be0b2e1","In the past few years, medium and high-resolution data became freely available for downloading. It provides great opportunity for researchers not to select between solving the task with high-resolution data on small territory or on global scale, but with low-resolution satellite images. Due to high spectral and spatial resolution of the data, Sentinel-1 and Sentinel-2 are very popular sources of information. Nevertheless, in practice if we would like to receive final product in 10 m resolution we should use bands with 10 m resolution. Sentinel-2 has four such bands, but also has other bands, especially red-edge 20 m resolution bands that are useful for vegetation analysis and often are omitted due to lower resolution. Thus, in this study we propose methodology for enhancing resolution (super-resolution) of the existing low-resolution images to higher resolution images. The main idea is to use advanced methods of deep learning-Generative Adversarial Networks (GAN) and train it to increase the resolution for the satellite images. Experimental results for the Sentinel-2 data showed that this approach is efficient and could be used for creating high resolution products. © 2022 IEEE.","Deep learning; Image enhancement; Optical resolving power; Remote sensing; Satellites; Deep learning; Global scale; High resolution data; High spatial resolution; High spectral resolution; Lower resolution; Satellite data; Satellite images; Sentinel-2; Superresolution; Generative adversarial networks","deep learning; GAN; Generative Adversarial Networks; Sentinel-2; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140369300"
"Ma C.; Gao H.","Ma, Conghui (56562155900); Gao, Hongchao (57471481400)","56562155900; 57471481400","A GAN based method for SAR and optical images fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12166","","121664F","","","","10.1117/12.2617316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125474916&doi=10.1117%2f12.2617316&partnerID=40&md5=04c5d0df6dd2dfd1a20109005ebc35dd","With the development of the remote sensing technology, the availability of satellite images has been dramatically increased with high quantity and quality. Diverse information can be obtained from these multiple imaging sources. For example, synthetic aperture radar (SAR) imagery measures physical properties of the observed scene in all-weather and full-time situation and follows a range-based imaging geometry, while optical imagery measures chemical characteristics of the scene and follows a perspective imaging geometry and needs both daylight and a cloudless sky. These multisource remote sensing images, once fused together, provide a more comprehensive interpretation of remote sensing scenes. Recent advances in Generative adversarial networks (GANs) have shown great promise in translating imagery between modalities, as well in the generation of high resolution and realistic imagery. In this paper, a GAN architecture is used to solve the task of fusing SAR and optical remote sensing imagery. The network learns the mapping between input and output image, and learns a loss function to train this mapping. Specifically, the generated network is divided into two parts, encoding and decoding. The fused image including SAR intensity and texture information is generated by the generator. Other details of the optical image are added to the fusion image gradually by the discriminator. The structural similarity loss function of GAN is to make the training of GAN model more accurate on the whole structure. Experiments on Sentinel-1and Sentinel-2 imagery confirm the effectiveness and efficiency of the proposed method.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Generative adversarial networks; Geometrical optics; Image fusion; Mapping; Radar imaging; Remote sensing; Textures; Imaging geometry; Learn+; Loss functions; Multiple imaging; Network-based; Optical image; Optical imagery; Remote sensing technology; Satellite images; Synthetic aperture radar images; Synthetic aperture radar","Generative adversarial network; Image fusion; Optical imagery; SAR","Conference paper","Final","","Scopus","2-s2.0-85125474916"
"Requena-Mesa C.; Reichstein M.; Mahecha M.; Kraft B.; Denzler J.","Requena-Mesa, C. (57208244647); Reichstein, M. (57206534330); Mahecha, M. (16444433900); Kraft, B. (57207884535); Denzler, J. (6701534437)","57208244647; 57206534330; 16444433900; 57207884535; 6701534437","Predicting landscapes as seen from space from environmental conditions","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519427","1768","1771","3","10.1109/IGARSS.2018.8519427","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061616822&doi=10.1109%2fIGARSS.2018.8519427&partnerID=40&md5=451e8f72ed938b70ac10d3cf76deb8ee","Satellite images are information rich snapshots of ecosystems and landscapes. In consequence, the features in the images strongly depend on the environmental conditions. Such dependency between climate and landscapes has been regarded since the beginning of earth sciences; however, it has never been taken as literally as in the present study. We adapted a deep learning generative model as a first demonstration of the potential behind deep learning for spatial pattern generation in geoscience. The purpose is to build a conditional Generative Adversarial Network (cGAN) useful to establish the relationship between two loosely linked set of variables that show multitude of complex spatial features such as climate conditions to aerial image. We trained a custom cGAN to generate Sentinel-2 multispectral imagery given a set of climatic and terrain predictors. Results show that the generated imagery shares many characteristics with the real one. In some cases, the quality of the generated imagery is high enough to deceive humans. We envision that such use of deep learning for geoscience could become an important tool to test the effects of climate on landscapes and ecosystems. © 2018 IEEE.","Antennas; Earth (planet); Ecosystems; Geology; Remote sensing; Satellite imagery; Space optics; Adversarial networks; Climate; Climate condition; Environmental conditions; Generative model; Landscape ecology; Multi-spectral imagery; Sentinel 2; Deep learning","Climate; Deep learning; GAN; Landscape ecology; Satellite imagery; Sentinel 2","Conference paper","Final","","Scopus","2-s2.0-85061616822"
"Radoi A.","Radoi, Anamaria (55544717700)","55544717700","Multimodal Satellite Image Time Series Analysis Using GAN-Based Domain Translation and Matrix Profile","2022","Remote Sensing","14","15","3734","","","","10.3390/rs14153734","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137105668&doi=10.3390%2frs14153734&partnerID=40&md5=49040114ac629c520d0c8c9d5ca43ea8","The technological development of the remote sensing domain led to the acquisition of satellite image time series (SITS) for Earth Observation (EO) by a variety of sensors. The variability in terms of the characteristics of the satellite sensors requires the existence of algorithms that allow the integration of multiple modalities and the identification of anomalous spatio-temporal evolutions caused by natural hazards. The unsupervised analysis of multimodal SITS proposed in this paper follows a two-step methodology: (i) inter-modality translation and (ii) the identification of anomalies in a change-detection framework. Inter-modality translation is achieved by means of a Generative Adversarial Network (GAN) architecture, whereas, for the identification of anomalies caused by natural hazards, we adapt the task to a similarity search in SITS. In this regard, we provide an extension of the matrix profile concept, which represents an answer to identifying differences and to discovering novelties in time series. Furthermore, the proposed inter-modality translation allows the usage of standard unsupervised clustering approaches (e.g., K-means using the Dynamic Time Warping measure) for mono-modal SITS analysis. The effectiveness of the proposed methodology is shown in two use-case scenarios, namely flooding and landslide events, for which a joint acquisition of Sentinel-1 and Sentinel-2 images is performed. © 2022 by the author.","Change detection; Disasters; Hazards; K-means clustering; Modal analysis; Remote sensing; Satellites; Time series analysis; Change detection; Dynamic time warping; Image time-series; Inter-modality translation; Intermodality; matrix; Matrix profile; Multi-modal; Multimodal satellite image time series analyse; Satellite images; Time-series analysis; Generative adversarial networks","change detection; dynamic time warping; generative adversarial networks; inter-modality translation; matrix profile; multimodal SITS analysis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137105668"
"Grohnfeldt C.; Schmitt M.; Zhu X.","Grohnfeldt, Claas (55946211600); Schmitt, Michael (7401931279); Zhu, Xiaoxiang (55696622200)","55946211600; 7401931279; 55696622200","A conditional generative adversarial network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 images","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519215","1726","1729","3","10.1109/IGARSS.2018.8519215","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056354296&doi=10.1109%2fIGARSS.2018.8519215&partnerID=40&md5=86a96bdfdcfcff984120e3d9c682d5d1","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and hazefree MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-1 SAR data confirm that our extended SAR-OptcGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input. © 2018 IEEE.","Data fusion; Deep learning; Geology; Network architecture; Remote sensing; Synthetic aperture radar; Adversarial networks; Cloudremoval; Equivalent model; Multi-spectral; Optical data; Optical remote sensing; Sentinel-1; Single sensor; Radar imaging","Cloudremoval; Data fusion; Deep learning; Generative adversarial network (GAN); Optical remote sensing; SAR","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85056354296"
"Saha S.; Solano-Correa Y.T.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Solano-Correa, Yady Tatiana (57156565000); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 57156565000; 9943212600; 7006892410","Unsupervised Deep Transfer Learning-Based Change Detection for HR Multispectral Images","2021","IEEE Geoscience and Remote Sensing Letters","18","5","9089195","856","860","4","10.1109/LGRS.2020.2990284","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104852204&doi=10.1109%2fLGRS.2020.2990284&partnerID=40&md5=c51d7664d31828a2b0bd499c41222563","To overcome the limited capability of most state-of-the-art change detection (CD) methods in modeling spatial context of multispectral high spatial resolution (HR) images and exploiting all spectral bands jointly, this letter presents a novel unsupervised deep-learning-based CD method that can effectively model contextual information and handle the large number of bands in multispectral HR images. This is achieved by exploiting all spectral bands after grouping them into spectral-dedicated band groups. To eliminate the necessity of multitemporal training data, the proposed method exploits a data set targeted for image classification to train spectral-dedicated Auxiliary Classifier Generative Adversarial Networks (ACGANs). They are used to obtain pixelwise deep change hypervector from multitemporal images. Each feature in deep change hypervector is analyzed based on the magnitude to identify changed pixels. An ensemble decision fusion strategy is used to combine change information from different features. Experimental results on the urban, Alpine, and agricultural Sentinel-2 data sets confirm the effectiveness of the proposed method. © 2004-2012 IEEE.","Agricultural robots; Classification (of information); Transfer learning; Adversarial networks; Change detection; Contextual information; High spatial resolution; Multi-temporal image; Multispectral images; Spatial context; State of the art; data set; detection method; image analysis; image classification; machine learning; numerical model; pixel; spatial resolution; spectral analysis; unsupervised classification; Deep learning","Change detection (CD); deep learning; generative adversarial network; high resolution; Sentinel-2","Article","Final","","Scopus","2-s2.0-85104852204"
"Singh A.; Bruzzone L.","Singh, Abhishek (57221148913); Bruzzone, Lorenzo (7006892410)","57221148913; 7006892410","Data Augmentation Through Spectrally Controlled Adversarial Networks for Classification of Multispectral Remote Sensing Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","651","654","3","10.1109/IGARSS46834.2022.9884928","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141896986&doi=10.1109%2fIGARSS46834.2022.9884928&partnerID=40&md5=8ceaa41e70af1117da230fb1f9c124f2","Availability of limited training remote sensing datasets is one of the problems in deep learning, as deep architectures require a large number of training samples for proper training. In this paper, we present a technique for data augmentation based on a spectral indexed generative adversarial network to train deep convolutional neural networks. This technique uses the spectral characteristic of multispectral (MS) images to support data augmentation in order to generate realistic training samples with respect to each land-use and land-cover class. The impact of multispectral remote sensing data generated through the spectral indexed GAN are evaluated through classification experiments. Experimental results obtained on the classification of the Sentinel-2 Eurosatallband datasets show that data augmentation through spectral indexed GAN enhances the main accuracy metrics. © 2022 IEEE.","Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image classification; Land use; Large dataset; Remote sensing; Sampling; Adversarial networks; Classification accuracy; Data augmentation; Generative model; Image-analysis; Multispectral remote sensing image; Remote sensing image analyse; Remote sensing images; Training data; Training sample; Classification (of information)","Classification Accuracy; Data Augmentation; Generative Model; Remote Sensing Image Analysis; Training Data","Conference paper","Final","","Scopus","2-s2.0-85141896986"
"Saha S.; Solano-Correa Y.T.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Solano-Correa, Yady Tatiana (57156565000); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 57156565000; 9943212600; 7006892410","Unsupervised deep learning based change detection in Sentinel-2 images","2019","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2019","","","8866899","","","","10.1109/Multi-Temp.2019.8866899","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074261635&doi=10.1109%2fMulti-Temp.2019.8866899&partnerID=40&md5=25ba3a12a30dfec24e683bb4a0a26b96","Change Detection (CD) is an important application of remote sensing. Recent technological evolution resulted in the availability of optical multispectral sensors that provide High spatial Resolution (HR) images with many spectral bands. Such characteristics allow for new applications of CD, however present new challenges on the proper exploitation of the information. HR multitemporal data processing is challenging due to spatial correlation of pixels and spatial context information needs to be exploited to benefit from multitemporal HR images. Moreover most of the state-of-The-Art CD methods exploit single or couple of spectral channels from the optical sensors to derive CD map. To overcome these challenges, this paper presents a novel unsupervised deep-learning based method that can effectively model contextual information and handle all the bands in multispectral images. In particular, we focus on the Sentinel-2 images provided by the European Space Agency (ESA) that provides both higher spatial and temporal resolution optical images with 13 spectral bands with respect to previous generation sensors. Experimental results on the urban Onera satellite CD (OSCD) dataset and on agricultural multitemporal images from Barrax, Spain confirms the effectiveness of the proposed method. © 2019 IEEE.","Data handling; Geometrical optics; Image analysis; Remote sensing; Space optics; Adversarial networks; Change detection; High resolution; High spatial resolution; Learning-based methods; Sentinel-2; Spatial and temporal resolutions; Technological evolution; Deep learning","Change detection; Deep learning; Generative Adversarial Network; High Resolution; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85074261635"
"Ciotola M.; Martinelli A.; Mazza A.; Scarpa G.","Ciotola, M. (57239080700); Martinelli, A. (57937431100); Mazza, A. (57200854745); Scarpa, G. (7004081145)","57239080700; 57937431100; 57200854745; 7004081145","An Adversarial Training Framework for Sentinel-2 Image Super-Resolution","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","3782","3785","3","10.1109/IGARSS46834.2022.9883144","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140358132&doi=10.1109%2fIGARSS46834.2022.9883144&partnerID=40&md5=978c0b9124123088c41e78a34f7b4e22","In this work is presented a new adversarial training framework for deep learning neural networks for super-resolution of Sentinel 2 images, exploiting the data fusion techniques on 10 and 20 meters bands. The proposed scheme is fully convolutional and tries to answer the need for generalization in scale, producing realistic and detailed accurate images. Furthermore, the presence of a mathcal{L}-{1} loss limits the instability of GAN training, limiting possible problems of spectral dis-tortion. In our preliminary experiments, the GAN training scheme has shown comparable results in comparison with the baseline approach. © 2022 IEEE.","Computer vision; Data fusion; Deep learning; Optical resolving power; Convo-lutional neural network; Data fusion technique; Deep learning; Generalisation; Image super resolutions; Learning neural networks; Neural-networks; Sentinel-2; Superresolution; Training framework; Generative adversarial networks","Convo-lutional Neural Network; Data-Fusion; Deep Learning; Generative Adversarial Network; Sentinel-2; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85140358132"
"Chen B.; Li J.; Jin Y.","Chen, Bin (57210117458); Li, Jing (57207737643); Jin, Yufang (7404457584)","57210117458; 57207737643; 7404457584","Deep learning for feature-level data fusion: Higher resolution reconstruction of historical landsat archive","2021","Remote Sensing","13","2","167","1","23","22","10.3390/rs13020167","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099179435&doi=10.3390%2frs13020167&partnerID=40&md5=16cb0d20fb4cfacbe0f3984caadbbf53","Long-term record of fine spatial resolution remote sensing datasets is critical for monitoring and understanding global environmental change, especially with regard to fine scale processes. However, existing freely available global land surface observations are limited by medium to coarse resolutions (e.g., 30 m Landsat) or short time spans (e.g., five years for 10 m Sentinel-2). Here we developed a feature-level data fusion framework using a generative adversarial network (GAN), a deep learning technique, to leverage the overlapping Landsat and Sentinel-2 observations during 2016–2019, and reconstruct 10 m Sentinel-2 like imagery from 30 m historical Landsat archives. Our tests with both simulated data and actual Landsat/Sentinel-2 imagery showed that the GANbased fusion method could accurately reconstruct synthetic Landsat data at an effective resolution very close to that of the real Sentinel-2 observations. We applied the GAN-based model to two dynamic systems: (1) land over dynamics including phenology change, cropping rotation, and water inundation; and (2) human landscape changes such as airport construction, coastal expansion, and urbanization, via historical reconstruction of 10 m Landsat observations from 1985 to 2018. The resulting comparison further validated the robustness and efficiency of our proposed framework. Our pilot study demonstrated the promise of transforming 30 m historical Landsat data into a 10 m Sentinel-2-like archive with advanced data fusion. This will enhance Landsat and Sentinel-2 data science, facilitate higher resolution land cover and land use monitoring, and global change research. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data fusion; Data Science; Forestry; Image reconstruction; Land use; Metadata; Remote sensing; Adversarial networks; Airport construction; Effective resolutions; Global environmental change; Global land surface; Historical reconstruction; Learning techniques; Spatial resolution; Deep learning","Data fusion; Data reconstruction; GAN; Machine learning; Super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099179435"
"Maggiolo L.; Solarna D.; Moser G.; Serpico S.B.","Maggiolo, Luca (57207878137); Solarna, David (57192703421); Moser, Gabriele (7101795745); Serpico, Sebastiano B. (7005306316)","57207878137; 57192703421; 7101795745; 7005306316","Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323235","2089","2092","3","10.1109/IGARSS39084.2020.9323235","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101981992&doi=10.1109%2fIGARSS39084.2020.9323235&partnerID=40&md5=3caf6717d140cae9e0879f4fddf8cb3a","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type \ell^{2} similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an \ell^{2} similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration. © 2020 IEEE.","Clustering algorithms; Computational efficiency; Deep learning; Geology; Optical correlation; Remote sensing; Synthetic aperture radar; Accurate registration; Adversarial networks; Automatic registration; Cross correlations; Maximization algorithm; Multisensor remote sensing; Similarity metrics; Textural properties; Radar imaging","COBYLA; conditional generative adversarial network; Multisensor image registration; \ell^{2} similarity","Conference paper","Final","","Scopus","2-s2.0-85101981992"
"Kussul N.; Shelestov A.; Yailymova H.; Shumilo L.; Drozd S.","Kussul, Nataliia (6602485938); Shelestov, Andrii (6507365226); Yailymova, Hanna (57202424721); Shumilo, Leonid (57208256914); Drozd, Sophia (57456870300)","6602485938; 6507365226; 57202424721; 57208256914; 57456870300","Agriculture Land Appraisal with Use of Remote Sensing and Infrastructure Data","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2785","2788","3","10.1109/IGARSS46834.2022.9884045","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140357841&doi=10.1109%2fIGARSS46834.2022.9884045&partnerID=40&md5=eb8b51ef384d51205e0438cb2db12049","1st July 2021 the law on the creation of land market start effect in Ukraine. As a result, land appraisal became cornerstone task in Ukrainian agriculture sector. The official methodology on land appraisal includes use of soil fertility characteristics combined with coefficients related to the distance to the infrastructure objects or settlements and placing of field in specific functional areas, like recreational, or areas with high level of radiation pollution. In this study we collected open source infostructure geospatial information and characteristics of fields obtained from remote sensing data-crop types and Normalized Difference Vegetation Index to build land price predictive model trained on the official land market information. This work designed to investigate potential of geo-informational technologies and remote sensing in the land appraisal use. We separated all available ground truth land price data into three groups by fields size-very small, small, medium and big. We found different relationships between field characteristics and prices. For very small fields the most important features are area, altitude, slope, bonitet and distances to elevators, villages and roads. For small fields the most important are bonitet, altitude, area and distances to cities and roads. For medium and big field's area, slope, distance to cities, roads and historical NDVI. © 2022 IEEE.","Agriculture; Commerce; Deep learning; Generative adversarial networks; Agriculture sectors; Deep learning; GAN; Land markets; Land prices; Remote-sensing; Sentinel-2; Soil fertility; Superresolution; Ukraine; Remote sensing","deep learning; GAN; Generative Adversarial Networks; Sentinel-2; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140357841"
"Khandelwal P.; Rammer D.; Pallickara S.; Pallickara S.L.","Khandelwal, Paahuni (57211901855); Rammer, Daniel (57147757400); Pallickara, Shrideep (6602168326); Pallickara, Sangmi Lee (8982898100)","57211901855; 57147757400; 6602168326; 8982898100","Mind the Gap: Generating imputations for satellite data collections at myriad spatiotemporal scopes","2021","Proceedings - 21st IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGrid 2021","","","","92","102","10","10.1109/CCGrid51090.2021.00019","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114895006&doi=10.1109%2fCCGrid51090.2021.00019&partnerID=40&md5=984eeba43b249141c7df27afbaa1a71c","Hyperspectral satellite data collections have been successfully leveraged in many domains such as meteorology, agriculture, forestry, and disaster management. There is also a collection of publicly available satellite observation networks. However, gaps in scanning frequencies and inadequate spatial resolutions limit the capabilities of geoscience applications. In this study, we target the temporal sparsity of high-resolution satellite images. In particular, we propose a novel methodology to estimate high-resolution images between scheduled scans. Our model SATnet, falls broadly within the class of Generative Adversarial Networks. SATnet allows us to generate accurate high-resolution, high-frequency satellite data at diverse spatial extents. SATnet achieves this by learning relations between a sequence of high-resolution/low-frequency satellite imageries (from Sentinel-2) and an ancillary satellite image that is high-frequency/low-resolution (from MODIS). Our benchmarks demonstrate that SATnet outperforms existing approaches such as ConvLSTMs, Dynamic Filter Network, and TrajGRU with a PSNR accuracy of 31.82. We trained and deployed SATnet over a distributed storage cluster to support the high-throughput generation of imputed satellite imagery via query evaluations. Our methodology preserves geospatial proximity and facilitates the dynamic construction of satellite imagery at a particular timestamp for arbitrary spatial scopes. © 2021 IEEE.","Agricultural robots; Cluster computing; Data acquisition; Digital storage; Disaster prevention; Disasters; Forestry; Adversarial networks; Dynamic construction; Geoscience applications; High resolution image; High resolution satellite images; Hyperspectral satellite; Satellite data collection; Satellite observations; Satellite imagery","deep learning; high-resolution imaging; spatial data; time series analysis","Conference paper","Final","","Scopus","2-s2.0-85114895006"
"Abady L.; Barni M.; Garzelli A.; Tondi B.","Abady, L. (55917282400); Barni, M. (7005442155); Garzelli, A. (7004594292); Tondi, B. (55389019900)","55917282400; 7005442155; 7004594292; 55389019900","GAN generation of synthetic multispectral satellite images","2020","Proceedings of SPIE - The International Society for Optical Engineering","11533","","115330L","","","","10.1117/12.2575765","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093963449&doi=10.1117%2f12.2575765&partnerID=40&md5=d39a85943c4777bd1ec0c3d653eee216","Generative Adversarial Networks (GAN) have been used for both image generation and image style translation. In this paper, we aim to apply GANs to multispectral satellite image. For the image generation, we take advantage of the progressive GAN training methodology, that is purposely modified to generate multi-band 16 bits satellite images that are similar to a Sentinel-2 level-1C product. The generated images that we obtained imitate closely the spectral signatures of the kind of terrain in the images, as it can be seen by comparing typical spectral view between synthetic and natural images. Furthermore, we consider the recent use of GAN architectures for transferring the style of the images and apply them to perform land-cover transfer of satellite images. Specifically, we used the unpaired style transfer method to modify images that are dominant in vegetation land cover into images that are dominated by bare land cover and vice versa. The land-cover transfer via GANs gives very promising results and the visual quality for the transferred images is also satisfactory, showing that the land-cover transfer is an easier task compared to the GAN generation from scratch. Especially, results are good when the target domain is bare land, in which the visual quality for the transferred images is also very good. © SPIE. Downloading of the abstract is permitted for personal use only.","Remote sensing; Satellites; Adversarial networks; Image generations; Multispectral satellite image; Natural images; Satellite images; Spectral signature; Transfer method; Visual qualities; Image processing","GAN generation; GANs for image remote sensing; NICE-GAN; ProGAN; Sentinel-2; Style-transfer GANs; Synthetic multispectral image generation","Conference paper","Final","","Scopus","2-s2.0-85093963449"
"Tao C.; Fu S.; Qi J.; Li H.","Tao, Chao (35235290700); Fu, Siyang (57486738700); Qi, Ji (57211483444); Li, Haifeng (57189334346)","35235290700; 57486738700; 57211483444; 57189334346","Thick Cloud Removal in Optical Remote Sensing Images Using a Texture Complexity Guided Self-Paced Learning Method","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5619612","","","","10.1109/TGRS.2022.3157917","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126282609&doi=10.1109%2fTGRS.2022.3157917&partnerID=40&md5=4803c292af362abba2ee4382e62af564","Thick clouds seriously impact the quality of optical remote sensing images (RSIs) and limit their application. For removing the cloud, some learning-based methods have been proposed and attracted considerable attention. However, these methods need to train paired multitemporal images with/without cloud, which are difficult and costly to collect. To solve this problem, we propose a novel texture complexity-guided self-paced learning (SPL) framework to remove the thick cloud from single RSIs. The framework does not need paired images and it exploits a texture complexity-guided mechanism to rank the self-generated cloud-corrupted training samples by texture complexity from low to high and then trains the generative adversarial cloud removal network using the SPL technique. In this way, the cloud removal network learns to restore the cloud-corrupted areas from easy to hard and thus to realize the image reconstruction for different difficulty levels. In addition, we introduce a structural similarity (SSIM) loss function to optimize the training network and improve the coherence of the image structure. Simulated and real experiments are performed on the single images acquired by Gao Fen-1 (GF-1) and Sentinel-2 satellites to validate the effectiveness of the proposed method. The results show that the proposed method has a better performance in cloud removal than other state-of-the-art methods, especially for the images of the areas with complex textures. The source codes are available at https://github.com/GeoX-Lab/TPL.  © 1980-2012 IEEE.","Complex networks; Distributed computer systems; Generative adversarial networks; Image enhancement; Image texture; Remote sensing; Cloud removal; Cloud-computing; Complexity theory; Images reconstruction; Optical imaging; Remote sensing images; Remote-sensing; Self-paced learning; Texture complexity; Thick cloud removal; cloud; complexity; image analysis; remote sensing; texture; Image reconstruction","Generative adversarial network (GAN); remote sensing; self-paced learning (SPL); thick cloud removal","Article","Final","","Scopus","2-s2.0-85126282609"
"Zhang K.; Sumbul G.; Demir B.","Zhang, Kexin (57221087691); Sumbul, Gencer (57196192158); Demir, Begum (15131434800)","57221087691; 57196192158; 15131434800","An Approach to Super-Resolution of Sentinel-2 Images Based on Generative Adversarial Networks","2020","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium, M2GARSS 2020 - Proceedings","","","9105165","69","72","3","10.1109/M2GARSS47143.2020.9105165","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086704517&doi=10.1109%2fM2GARSS47143.2020.9105165&partnerID=40&md5=9f39cc5fcc004c7aac1a60ab0390b2aa","This paper presents a generative adversarial network based super-resolution (SR) approach (which is called as S2GAN) to enhance the spatial resolution of Sentinel-2 spectral bands. The proposed approach consists of two main steps. The first step aims to increase the spatial resolution of the bands with 20m and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To this end, we introduce a generator network that performs SR on the lower resolution bands with the guidance of the bands associated to 10m spatial resolution by utilizing the convolutional layers with residual connections and a long skip-connection between inputs and outputs. The second step aims to distinguish SR bands from their ground truth bands. This is achieved by the proposed discriminator network, which alternately characterizes the high level features of the two sets of bands and applying binary classification on the extracted features. Then, we formulate the adversarial learning of the generator and discriminator networks as a min-max game. In this learning procedure, the generator aims to produce realistic SR bands as much as possible so that the discriminator incorrectly classifies SR bands. Experimental results obtained on different Sentinel-2 images show the effectiveness of the proposed approach compared to both conventional and deep learning based SR approaches. © 2020 IEEE.","Deep learning; Geology; Image resolution; Optical resolving power; Remote sensing; Adversarial learning; Adversarial networks; Binary classification; High-level features; Learning procedures; Lower resolution; Spatial resolution; Super resolution; Discriminators","generative adversarial network; remote sensing; Sentinel-2 images; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086704517"
"Ley A.; D'Hondt O.; Valade S.; Hänsch R.; Hellwich O.","Ley, Andreas (55117014800); D'Hondt, Olivier (14031115900); Valade, Sébastien (53867232900); Hänsch, Ronny (35795173000); Hellwich, Olaf (6701327873)","55117014800; 14031115900; 53867232900; 35795173000; 6701327873","Exploiting GAN-based SAR to optical image transcoding for improved classification via deep learning","2018","Proceedings of the European Conference on Synthetic Aperture Radar, EUSAR","2018-June","","","396","401","5","","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050481064&partnerID=40&md5=603e67c2074d7a27013b609d94982292","Learning the proxy task of transcoding SAR images into optical images forces an employed conditional generative adversarial network (GAN) to distinguish between different land surfaces. Such a network can then be used to build a classifier with significantly fewer free parameters that generalizes well even when trained on a very small amount of labeled data. We train such a GAN on aligned Sentinel-1 and Sentinel-2 image pairs. We then show that a pre-trained classifier using these features learned from transcoding outperforms classifiers that are trained from scratch when only a very limited amount of labeled pixels are available for training. © VDE VERLAG GMBH Â Berlin Â Offenbach.","Classification (of information); Geometrical optics; Image classification; Image enhancement; Radar imaging; Synthetic aperture radar; Adversarial networks; Free parameters; Image pairs; Labeled data; Labeled pixels; Land surface; Optical image; Transcoding; Deep learning","","Conference paper","Final","","Scopus","2-s2.0-85050481064"
"Hoque M.R.U.; Wu J.; Kwan C.; Koperski K.; Li J.","Hoque, Md Reshad Ul (57215344852); Wu, Jian (57193141747); Kwan, Chiman (7201421216); Koperski, Krzysztof (6603540174); Li, Jiang (56226550100)","57215344852; 57193141747; 7201421216; 6603540174; 56226550100","ArithFusion: An Arithmetic Deep Model for Temporal Remote Sensing Image Fusion","2022","Remote Sensing","14","23","6160","","","","10.3390/rs14236160","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143761796&doi=10.3390%2frs14236160&partnerID=40&md5=6d162c4fc285294f57b7c7e89e83f5f4","Different satellite images may consist of variable numbers of channels which have different resolutions, and each satellite has a unique revisit period. For example, the Landsat-8 satellite images have 30 m resolution in their multispectral channels, the Sentinel-2 satellite images have 10 m resolution in the pan-sharp channel, and the National Agriculture Imagery Program (NAIP) aerial images have 1 m resolution. In this study, we propose a simple yet effective arithmetic deep model for multimodal temporal remote sensing image fusion. The proposed model takes both low- and high-resolution remote sensing images at (Formula presented.) together with low-resolution images at a future time (Formula presented.) from the same location as inputs and fuses them to generate high-resolution images for the same location at (Formula presented.). We propose an arithmetic operation applied to the low-resolution images at the two time points in feature space to take care of temporal changes. We evaluated the proposed model on three modality pairs for multimodal temporal image fusion, including downsampled WorldView-2/original WorldView-2, Landsat-8/Sentinel-2, and Sentinel-2/NAIP. Experimental results show that our model outperforms traditional algorithms and recent deep learning-based models by large margins in most scenarios, achieving sharp fused images while appropriately addressing temporal changes. © 2022 by the authors.","Antennas; Deep learning; Generative adversarial networks; Image fusion; Satellite imagery; Space optics; Deep learning; Generative adversarial network; HRNet; LANDSAT; Neural-networks; Remote sensing images; Remote-sensing; Satellite images; Superresolution; U-net; Remote sensing","deep learning; generative adversarial network (GAN); HRNet; image fusion; neural networks; remote sensing; super-resolution; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143761796"
