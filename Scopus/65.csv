"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Zhang R.; Zhu L.; Du C.","Zhang, Ruixian (57407373200); Zhu, Lingyun (57301669800); Du, Chengxi (57407373300)","57407373200; 57301669800; 57407373300","Multi-angle Facial Expression Image Generation Method Based on Mask Vector Guided Generation Adversarial Network","2021","ACM International Conference Proceeding Series","","","","120","126","6","10.1145/3490725.3490743","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122619349&doi=10.1145%2f3490725.3490743&partnerID=40&md5=ba76bd22cc1d71ae5bd2aed95798cc93","In recent studies, the method of generative adversarial network to augment facial expression images has been widely used. However, the existing methods ignore the influence of face angle on facial expression image generation. To solve this problem, this paper proposes a mask vector - guided generative adversarial network to generate multi-angle expression images. Mask vectors enable a single model architecture to train multiple facial expression datasets from different angles simultaneously. Experimental results on the KDEF dataset show that, compared with the IcGAN, CycleGAN and StarGAN benchmark models, the proposed method uses fewer parameters and the classification accuracy of the generated facial expression images is improved by 1%-4%, indicating that the facial expression images generated by the proposed method are more realistic.  © 2021 ACM.","Classification (of information); Generative adversarial networks; Image enhancement; Adversarial networks; Benchmark models; Facial expression image; Facial Expressions; Generation method; Image generations; Mask vector; Modeling architecture; Multi angle; Single models; Vectors","Facial Expression Image; Generative Adversarial Network; Mask Vector; Multi-angle","Conference paper","Final","","Scopus","2-s2.0-85122619349"
"Jin S.; Zhou S.; Liu Y.; Chen C.; Sun X.; Yao H.; Hua X.-S.","Jin, Sheng (57208439418); Zhou, Shangchen (57192721201); Liu, Yao (57211087811); Chen, Chao (57203146321); Sun, Xiaoshuai (24278895900); Yao, Hongxun (57204325155); Hua, Xian-Sheng (55441195100)","57208439418; 57192721201; 57211087811; 57203146321; 24278895900; 57204325155; 55441195100","SSAH: Semi-supervised adversarial deep hashing with self-paced hard sample generation","2020","AAAI 2020 - 34th AAAI Conference on Artificial Intelligence","","","","11157","11164","7","","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106409442&partnerID=40&md5=f1d8f18c9d9982b4a47c12dfebfdf9bd","Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semisupervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semisupervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (ANet) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve state-of-the-art models on both the widelyused hashing datasets and fine-grained datasets.  Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Image enhancement; Semantics; Adversarial networks; Data-driven methods; Image generations; Sample generations; Semantic information; Semi-supervised; State of the art; Unified framework; Artificial intelligence","","Conference paper","Final","","Scopus","2-s2.0-85106409442"
"","","","14th Chinese Conference on Biometric Recognition, CCBR 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11818 LNCS","","","","","518","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075563193&partnerID=40&md5=68bd4161a16aba52d68e73f0933bd0b6","The proceedings contain 56 papers. The special focus in this conference is on Biometric Recognition. The topics include: Authentication System Design Based on Dynamic Hand Gesture; structure Feature Learning: Constructing Functional Connectivity Network for Alzheimer’s Disease Identification and Analysis; weakly Supervised Learning of Image Emotion Analysis Based on Cross-spatial Pooling; embarrassingly Easy Zero-Shot Image Recognition; On the Generalization of GAN Image Forensics; deep Residual Equivariant Mapping for Multi-angle Face Recognition; the Impact of Data Correlation on Identification of Computer-Generated Face Images; face Image Deblurring Based on Iterative Spiral Optimazation; adaptiveNet: Toward an Efficient Face Alignment Algorithm; fingerprint Presentation Attack Detection via Analyzing Fingerprint Pairs; cross-Dimension Transfer Learning for Video-Based Facial Expression Recognition; exploring Shape Deformation in 2D Images for Facial Expression Recognition; facial Attractiveness Prediction by Deep Adaptive Label Distribution Learning; LWFD: A Simple Light-Weight Network for Face Detection; dairy Cow Tiny Face Recognition Based on Convolutional Neural Networks; reconstructed Face Recognition; a Two-Stage Method for Assessing Facial Paralysis Severity by Fusing Multiple Classifiers; latent Spatial Features Based on Generative Adversarial Networks for Face Anti-spoofing; similarity Measurement Between Reconstructed 3D Face and 2D Face Based on Deep Learning; real-Time Face Occlusion Recognition Algorithm Based on Feature Fusion; finger Vein Recognition Based on Double-Orientation Coding Histogram; joint Face Detection and Alignment Using Focal Loss-Based Multi-task Convolutional Neural Networks; a Face Recognition Workflow Based Upon Similarity Measurement; 106-Point Facial Landmark Localization with Mobile Networks Based on Regression; long Range Pupil Location Algorithm Based on the Improved Circle Fitting Method.","","","Conference review","Final","","Scopus","2-s2.0-85075563193"
"Leinonen J.; Grazioli J.; Berne A.","Leinonen, Jussi (22980018800); Grazioli, Jacopo (55908599800); Berne, Alexis (8573340700)","22980018800; 55908599800; 8573340700","Reconstruction of the mass and geometry of snowfall particles from multi-angle snowflake camera (MASC) images","2021","Atmospheric Measurement Techniques","14","10","","6851","6866","15","10.5194/amt-14-6851-2021","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118234005&doi=10.5194%2famt-14-6851-2021&partnerID=40&md5=291c431ecb7fcec8fe8c36352d8c2cc6","This paper presents a method named 3D-GAN, based on a generative adversarial network (GAN), to retrieve the total mass, 3D structure and the internal mass distribution of snowflakes. The method uses as input a triplet of binary silhouettes of particles, corresponding to the triplet of stereoscopic images of snowflakes in free fall captured by a multiangle snowflake camera (MASC). The 3D-GAN method is trained on simulated snowflakes of known characteristics whose silhouettes are statistically similar to real MASC observations, and it is evaluated by means of snowflake replicas printed in 3D at 1 V 1 scale. The estimation of mass obtained by 3D-GAN has a normalized RMSE (NRMSE) of 40 %, a mean normalized bias (MNB) of 8%and largely outperforms standard relationships based on maximum size and compactness. The volume of the convex hull of the particles is retrieved with NRMSE of 35% and MNB of C19 %. In order to illustrate the potential of 3D-GAN to study snowfall microphysics and highlight its complementarity with existing retrieval algorithms, some application examples and ideas are provided, using as showcases the large available datasets of MASC images collected worldwide during various field campaigns. The combination of mass estimates (from 3D-GAN) and hydrometeor classification or riming degree estimation (from independent methods) allows, for example, to obtain mass-to-size power law parameters stratified on hydrometeor type or riming degree. The parameters obtained in this way are consistent with previous findings, with exponents overall around 2 and increasing with the degree of riming.  © Author(s) 2021.","Leucojum; geometry; image analysis; instrumentation; reconstruction; snow cover","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118234005"
"Wei Y.; Xu S.; Kang B.; Hoque S.","Wei, Yuchen (57220067783); Xu, Shuxiang (7404438024); Kang, Byeong (7401684499); Hoque, Sabera (57202160893)","57220067783; 7404438024; 7401684499; 57202160893","Generating training images with different angles by GAN for improving grocery product image recognition","2022","Neurocomputing","488","","","694","705","11","10.1016/j.neucom.2021.11.080","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120832751&doi=10.1016%2fj.neucom.2021.11.080&partnerID=40&md5=fbf655af942ab180ead5257ddc637024","Image recognition based on deep learning methods has gained remarkable achievements by feeding with abundant training data. Unfortunately, collecting a tremendous amount of annotated images is time-consuming and expensive, especially in grocery product recognition tasks. It is challenging to recognise grocery products accurately when the deep learning model is trained with insufficient data. This paper proposes multi-angle Generative Adversarial Networks (MAGAN), which can generate realistic training images with different angles for data augmentation. Mutual information is employed in the novel GAN to achieve the learning of angles in an unsupervised manner. This paper aims to create training images containing grocery products from different angles, thus improving grocery product recognition accuracy. We first enlarge the fruit dataset by using MAGAN and the state-of-the-art GAN variants. Then, we compare the top-1 accuracy results from CNN classifiers trained with different data augmentation methods. Finally, our experiments demonstrate that the MAGAN exceeds the existing GANs for grocery product recognition tasks, obtaining a significant increase in the accuracy. © 2021 Elsevier B.V.","Convolutional neural networks; Deep learning; Image enhancement; Image recognition; Convolutional neural network; Data augmentation; Generative adversarial network; Grocery product recognition; Learning methods; Multi angle; Product images; Training data; Training image; article; classifier; convolutional neural network; fruit; learning; Generative adversarial networks","Convolutional neural network (CNN); Data augmentation; Generative adversarial network (GAN); Grocery product recognition","Article","Final","","Scopus","2-s2.0-85120832751"
"Lu L.","Lu, Lihua (57222049717)","57222049717","Multi-angle face expression recognition based on generative adversarial networks","2022","Computational Intelligence","38","1","","20","37","17","10.1111/coin.12437","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101203146&doi=10.1111%2fcoin.12437&partnerID=40&md5=bdf797009f91d5931420ed91fa305446","Because of the different features of the same facial expressions from different angles, most of the methods are only suitable for face images, and the accuracy of facial expression recognition is low. Therefore, a multi-angle facial expression recognition method based on generative adversarial networks (GAN) is proposed. Firstly, the depth regression network is used to detect the key points of the face image to achieve face alignment, so as to reduce the difficulty of feature extraction. Then, the image is input to GAN. The generator is composed of encoder and decoder, and a skip connection is designed. In the encoding phase, the generator can unlock the correlation between the facial expression image and the angle, and in the decoding stage, it can generate different angle facial expression images by adding other angle information. Finally, the multi-angle facial expression images are sent to the convolution neural network for classification and learning, in which the loss weight is adjusted dynamically to improve the recognition accuracy by introducing resistance loss, recognition loss, content loss, and center loss. The experimental results on Multi-pose illumination expression (PIE) and celebrities in frontal profile (CFP) datasets show that the performance of the proposed method is the best when the learning rate is 0.0002, and the recognition accuracy under different angles is higher than other comparison methods, so it has practical application significance. © 2021 Wiley Periodicals LLC.","Decoding; Feature extraction; Image coding; Image enhancement; Signal encoding; Adversarial networks; Angle information; Comparison methods; Convolution neural network; Face expression recognition; Facial expression recognition; Facial Expressions; Recognition accuracy; Face recognition","convolutional neural network; facial expression recognition; generative adversarial networks; key point detection; multi-angle; skip connection","Article","Final","","Scopus","2-s2.0-85101203146"
"Zhang Z.; Zhang H.; Liu H.; Xin S.; Xiao N.; Zhang L.","Zhang, Zihao (57256254100); Zhang, Huayan (57209412907); Liu, Hui (57830155800); Xin, Shan (35101235100); Xiao, Ning (56966212000); Zhang, Lei (55795222100)","57256254100; 57209412907; 57830155800; 35101235100; 56966212000; 55795222100","Frontal Face Generation Based Multi-Angle Face Identification System","2021","2021 International Conference on Computer, Control and Robotics, ICCCR 2021","","","9349409","329","334","5","10.1109/ICCCR49711.2021.9349409","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101702462&doi=10.1109%2fICCCR49711.2021.9349409&partnerID=40&md5=014dd8670646668d4cf28db6c2bbc2d8","Precise identity recognition is a pre-condition for robots to enter the human living environment. Most of the existed face identification methods cannot work on the non-frontal face since the severe texture loss. In this paper, we propose a novel system to deal with multi-Angle face identification in video sequence based on frontal face generation, which replaces the process of detection, alignment in the typical face identification system. To solve the problem of face texture loss in large pose variation, we creatively combine generative adversarial networks (GAN) with the state-of-The-Art facial landmark localization method. The proposed system was tested on video database containing multi-Angle faces, and the experimental results indicate that our system can recognize more faces in the frames, and improve the accuracy of identification for multi-Angle face by 130%. © 2021 IEEE.","Agricultural robots; Robotics; Social robots; Textures; Video recording; Adversarial networks; Face identification; Facial landmark; Identity recognition; Living environment; Pose variation; State of the art; Video sequences; Face recognition","face frontalization; face identification; generative adversarial network; image processing","Conference paper","Final","","Scopus","2-s2.0-85101702462"
"Yuan D.; Yiping S.; Jie L.; Yueying J.; Yamei Z.; Jin L.","Yuan, Deng (57878275000); Yiping, Shi (57877874800); Jie, Liu (57877874900); Yueying, Jiang (57878193400); Yamei, Zhu (57878194500); Jin, Liu (26652771400)","57878275000; 57877874800; 57877874900; 57878193400; 57878194500; 26652771400","Multi-Angle Facial Expression Recognition Algorithm Combined with Dual-Channel WGAN-GP; [结合双通道 WGAN-GP 的多角度人脸表情识别算法研究]","2022","Laser and Optoelectronics Progress","59","18","","","","","10.3788/LOP202259.1810013","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137390429&doi=10.3788%2fLOP202259.1810013&partnerID=40&md5=012f1d3a536e1e45b9d0ff480c8a79a1","A multi-angle facial expression recognition algorithm combined with dual-channel WGAN-GP is suggested to address the concerns of poor performance of standard algorithms for multi-angle facial expression identification and bad quality of frontal face pictures generated under deflection angles. Traditional models only use profile features to recognize the multi-angle facial expression, which leads to low recognition accuracy due to small differences in characteristics. As a result, the generative adversarial network is used to frontalize the face first, removing the impact of the pose angle. To stabilize the training of the model and improve the quality of face generation, WGAN-GP is used as the baseline and improved into a dual-channel structure, which fuses the facial features and the global features of the face for frontalization. Finally, the lightweight network MobileNetV3 is built to detect the produced frontal facial expression photos, ensuring classification accuracy while drastically reducing parameter calculation. The experimental results demonstrate that the proposed method can well generate the frontal facial expression images at any angle and enhance the recognition rate of multi-angle facial expressions. © 2022 Universitat zu Koln. All rights reserved.","","convolutional neural network; dual<sup>-</sup>channel; face frontalization; generative adversarial network; image processing; multi<sup>-</sup>angle facial expression","Article","Final","","Scopus","2-s2.0-85137390429"
"Tao Y.; Muller J.-P.","Tao, Y. (56539197700); Muller, J.-P. (7404871794)","56539197700; 7404871794","Super-resolution restoration of spaceborne HD videos using the UCL MAGiGAN system","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","1115508","","","","10.1117/12.2532889","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078198191&doi=10.1117%2f12.2532889&partnerID=40&md5=828817e6907aad33d8482ac8726058cf","We developed a novel SRR system, called Multi-Angle Gotcha image restoration with Generative Adversarial Network (MAGiGAN), to produce resolution enhancement of 3-5 times from multi-pass EO images. The MAGiGAN SRR system uses a combination of photogrammetric and machine vision approaches including image segmentation and shadow labelling, feature matching and densification, estimation of an image degradation model, and deep learning approaches, to retrieve image information from distorted features and training networks. We have tested the MAGiGAN SRR using the NVIDIA® Jetson TX-2 GPU card for onboard processing within a smart-satellite capturing high definition satellite videos, which will enable many innovative remote-sensing applications to be implemented in the future. In this paper, we show SRR processing results from a Planet® SkySat HD 70cm spaceborne video using a GPU version of the MAGiGAN system. Image quality and effective resolution enhancement are measured and discussed. © 2019 SPIE.","Deep learning; Digital television; Earth (planet); Graphics processing unit; Image enhancement; Image segmentation; Optical resolving power; Remote sensing; Restoration; Video signal processing; Adversarial networks; Earth observations; HD videos; MAGiGAN; Multi angle; Super-resolution restoration; Image reconstruction","Earth Observation; Generative Adversarial Network; MAGiGAN; Multi-angle; Planet® SkySat® HD Video; Super-Resolution Restoration","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078198191"
"Leinonen J.; Berne A.","Leinonen, Jussi (22980018800); Berne, Alexis (8573340700)","22980018800; 8573340700","Unsupervised classification of snowflake images using a generative adversarial network and <i>K</i>-medoids classification","2020","Atmospheric Measurement Techniques","13","6","","2949","2964","15","10.5194/amt-13-2949-2020","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088702995&doi=10.5194%2famt-13-2949-2020&partnerID=40&md5=838dc85c1b02a65438150e0b9f5ffa77","The increasing availability of sensors imaging cloud and precipitation particles, like the Multi-Angle Snowflake Camera (MASC), has resulted in datasets comprising millions of images of falling snowflakes. Automated classification is required for effective analysis of such large datasets. While supervised classification methods have been developed for this purpose in recent years, their ability to generalize is limited by the representativeness of their labeled training datasets, which are affected by the subjective judgment of the expert and require significant manual effort to derive. An alternative is unsupervised classification, which seeks to divide a dataset into distinct classes without expert-provided labels. In this paper, we introduce an unsupervised classification scheme based on a generative adversarial network (GAN) that learns to extract the key features from the snowflake images. Each image is then associated with a distribution of points in the feature space, and these distributions are used as the basis of-medoids classification and hierarchical clustering. We found that the classification scheme is able to separate the dataset into distinct classes, each characterized by a particular size, shape and texture of the snowflake image, providing signatures of the microphysical properties of the snowflakes. This finding is supported by a comparison of the results to an existing supervised scheme. Although training the GAN is computationally intensive, the classification process proceeds directly from images to classes with minimal human intervention and therefore can be repeated for other MASC datasets with minor manual effort. As the algorithm is not specific to snowflakes, we also expect this approach to be relevant to other applications. © 2020 Author(s).","","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088702995"
"Yang Z.; Wang J.; Zhang J.; Jiang X.","Yang, Zhenliang (57207829701); Wang, Junliang (56331459500); Zhang, Jie (55914060000); Jiang, Xiaokang (57207815395)","57207829701; 56331459500; 55914060000; 57207815395","Data Driven Wafer Pattern Defect Pattern Recognition Method; [数据驱动的晶圆图缺陷模式识别方法]","2019","Zhongguo Jixie Gongcheng/China Mechanical Engineering","30","2","","230","236","6","10.3969/j.issn.1004-132X.2019.02.015","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062945425&doi=10.3969%2fj.issn.1004-132X.2019.02.015&partnerID=40&md5=d0d14c439f2c3ae3a4e2e472ab7ad8f6","Aiming at the characteristics of wafer map data angle and dimension diversity and quantity imbalance during wafer production processes, a wafer pattern defect recognition method was proposed based on generative adversarial networks. The two-stage wafer defect data pre-processing method was proposed to obtain standard wafer defect data, where Radon transform was designed to solve the multi-angle characteristics of the wafer map, and a resampling mechanism was used to realize the scaling of various data dimensions. The proposed wafer defect classification method used a generation mechanism to balance the number of samples of each defect type based on a generative adversarial networks, which could improve the defect pattern recognition accuracy. The experimental results show that this method may greatly improve the accuracy of small class samples, and the overall recognition rate is much better than the support vector machine and Adaboost algorithm. © 2019, China Mechanical Engineering Magazine Office. All right reserved.","Adaptive boosting; Data handling; Defects; Semiconductor device manufacture; Silicon wafers; AdaBoost algorithm; Adversarial networks; Data driven; Data preprocessing; Defect classification; Generation mechanism; Number of samples; Semiconductor manufacturing; Pattern recognition","Data driven; Generative adversarial network; Pattern recognition; Semiconductor manufacturing; Wafer defect","Article","Final","","Scopus","2-s2.0-85062945425"
"Wang R.; Zhang H.; Han B.; Zhang Y.; Guo J.; Hong W.; Sun W.; Hu W.","Wang, Ruyi (57947371800); Zhang, Hanqing (57947216300); Han, Bing (55575376300); Zhang, Yueting (57218470544); Guo, Jiayi (57194143247); Hong, Wen (55513537700); Sun, Wei (57210921364); Hu, Wenlong (47962223000)","57947371800; 57947216300; 55575376300; 57218470544; 57194143247; 55513537700; 57210921364; 47962223000","Multiangle SAR Dataset Construction of Aircraft Targets Based on Angle Interpolation Simulation; [基于角度内插仿真的飞机目标多角度SAR数据集构建方法研究]","2022","Journal of Radars","11","4","","637","651","14","10.12000/JR21193","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140906903&doi=10.12000%2fJR21193&partnerID=40&md5=251cfd1b3ec1b4bf517a3ae02edd15c6","With the expansion of Synthetic Aperture Radar (SAR) applications and the development of SAR data acquisition technology, multiangle SAR datasets of various typical targets need to be constructed. Presently, a comprehensive multiangle SAR image dataset for aircraft targets is still lacking. This study explores a method of dataset construction based on the acquisition of actual data and intelligent simulation. Multiangle SAR images of aircraft targets are collected through flight tests, and the interpolation simulations of SAR images of specific angles are realized based on scattering analysis and self-attention generative adversarial network, which provide a new solution for dataset construction and expansion. Finally, under the assumption that some data are missing, the similarities between the simulated and actual images are evaluated using six evaluation indices, which verify the effectiveness of the proposed method. © 2022 Institute of Electronics Chinese Academy of Sciences. All rights reserved.","Aircraft; Data acquisition; Flight simulators; Generative adversarial networks; Radar imaging; Technology transfer; Aircraft targets; Airplane objects; Dataset construction; Images simulations; Multi angle; Radar applications; Radar data acquisitions; Radar datasets; Self-attention; Synthetic aperture radar images; Synthetic aperture radar","Airplane object; Dataset construction; Generative adversarial network; Image simulation; Self-attention","Article","Final","","Scopus","2-s2.0-85140906903"
"Liu X.; Zhou S.; Wu S.; Tan D.; Yao R.","Liu, Xiaojuan (57190950892); Zhou, Shangbo (57549648100); Wu, Sheng (55761428100); Tan, Duo (57260479700); Yao, Rui (57260256600)","57190950892; 57549648100; 55761428100; 57260479700; 57260256600","3D visualization model construction based on generative adversarial networks","2022","PeerJ Computer Science","8","","e768","","","","10.7717/peerj-cs.768","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130340349&doi=10.7717%2fpeerj-cs.768&partnerID=40&md5=b3df5b7afefe6bdbc1730f8863e81fcc","The development of computer vision technology is rapid, which supports the automatic quality control of precision components efficiently and reliably. This paper focuses on the application of computer vision technology in manufacturing quality control. A new deep learning algorithm is presented, Multi-angle projective Generative Adversarial Networks (MapGANs), to automatically generate 3D visualization models of products and components. The generated 3D visualization models can intuitively and accurately display the product parameters and indicators. Based on these indicators, our model can accurately determine whether the product meets the standard. The working principle of the MapGANs algorithm is to automatically infer the basic three-dimensional shape distribution through the product's projection module, while using multiple angles and multiple views to improve the fineness and accuracy of the three-dimensional visualization model. The experimental results prove that MapGANs can effectively reconstruct two-dimensional images into three-dimensional visualization models, and meanwhile accurately predict whether the quality of the product meets the standard. © 2022 Liu et al.","3D modeling; Computer vision; Deep learning; Quality control; Three dimensional computer graphics; Visualization; 3D visualization model; Adversarial networks; Automatic quality control; Computer vision technology; Generation adversarial network; Model construction; Multi angle; Neural-networks; Precision components; Three dimensional visualization modeling; Generative adversarial networks","3d visualization model; Generation adversarial network; Neural network; Precision components","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130340349"
"Tao Y.; Muller J.-P.","Tao, Yu (56539197700); Muller, Jan-Peter (7404871794)","56539197700; 7404871794","Super-resolution restoration of MISR images using the UCL MAGiGAN system","2019","Remote Sensing","11","1","52","","","","10.3390/rs11010052","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059950995&doi=10.3390%2frs11010052&partnerID=40&md5=a20a39691361ba100c7146d05da7ba55","High spatial resolution Earth observation imagery is considered desirable for many scientific and commercial applications. Given repeat multi-angle imagery, an imaging instrument with a specified spatial resolution, we can use image processing and deep learning techniques to enhance the spatial resolution. In this paper, we introduce the University College London (UCL) MAGiGAN super-resolution restoration (SRR) system based on multi-angle feature restoration and deep SRR networks. We explore the application of MAGiGAN SRR to a set of 9 MISR red band images (275 m) to produce up to a factor of 3.75 times resolution enhancement. We show SRR results over four different test sites containing different types of image content including urban and rural targets, sea ice and a cloud field. Different image metrics are introduced to assess the overall SRR performance, and these are employed to compare the SRR results with the original MISR input images and higher resolution Landsat images, where available. Significant resolution improvement over various types of image content is demonstrated and the potential of SRR for different scientific application is discussed. © 2019 by the authors.","Deep learning; Image reconstruction; Image resolution; Optical resolving power; Restoration; Sea ice; Urban growth; Adversarial networks; Feature matching; Gotcha; MISR; Super-resolution restoration; Image enhancement","Deep learning; Feature matching; GAN; Generative adversarial network; Gotcha; GPT; MISR; SRR; Super-resolution restoration","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85059950995"
"Tao Y.; Muller J.-P.","Tao, Y. (56539197700); Muller, J.-P. (7404871794)","56539197700; 7404871794","Repeat multiview panchromatic super-resolution restoration using the UCL MAGiGAN system","2018","Proceedings of SPIE - The International Society for Optical Engineering","10789","","1078903","","","","10.1117/12.2500196","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059024601&doi=10.1117%2f12.2500196&partnerID=40&md5=eb5c8dab554c5cc6473c3417b6e63921","High spatial resolution imaging data is always considered desirable in the field of remote sensing, particularly Earth observation. However, given the physical constraints of the imaging instruments themselves, one needs to be able to trade-off spatial resolution against launch mass as well as telecommunications bandwidth for transmitting data back to the Earth. In this paper, we present a newly developed super-resolution restoration system, called MAGiGAN, based on our original GPT-SRR system combined with deep learning image networks to be able to restore up to 4x higher resolution enhancement using multi-angle repeat images as input. © 2018 SPIE.","Deep learning; Economic and social effects; Image enhancement; Image resolution; Observatories; Optical resolving power; Restoration; Adversarial networks; Earth observations; MAGiGAN; Multi angle; Super-resolution restoration; Remote sensing","Deep learning; Earth observation; Generative adversarial network; MAGiGAN; Multi-angle; Super-resolution restoration","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85059024601"
