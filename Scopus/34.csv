"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Liu Q.; Zhang S.; Wang N.; Ming Y.; Huang C.","Liu, Qihang (57211624190); Zhang, Shiqiang (8238620300); Wang, Ninglian (57788160200); Ming, Yisen (57786965100); Huang, Chang (56460475700)","57211624190; 8238620300; 57788160200; 57786965100; 56460475700","Fusing Landsat-8, Sentinel-1, and Sentinel-2 Data for River Water Mapping Using Multidimensional Weighted Fusion Method","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4208012","","","","10.1109/TGRS.2022.3187154","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133641338&doi=10.1109%2fTGRS.2022.3187154&partnerID=40&md5=a47aa2f7253b6102c45cf83424d5cc57","River water extent is critical for understanding river discharge or its hydrological conditions. Although numerous methods have been proposed to map river water from either optical or synthetic aperture radar (SAR) remotely sensed images, uncertainties still exist broadly. In this study, we developed an image fusion method that integrates Landsat-8, Sentinel-1, and Sentinel-2 images simultaneously for river water mapping with two major steps. Firstly, a posterior probability support vector machine (SVM) model was adopted to generate water probability maps from each individual image; and second, a multidimensional weighted fusion method (MDWFM) was developed to fuse these probability maps. Four reaches with different characteristics were selected as case study sites. High-resolution aerial images were acquired and used as the reference to evaluate our results. We found that the fusion process not only improves the quality of river water mapping but also excludes the cloud interference. The fused river water maps become more reliable after the conflicts from difference images being solved by the proposed MDWFM method that contains a proportional conflict redistribution rule. The weighted root mean square difference was reduced to 0.066, and the area under the ROC curve reached up to 0.984. The critical success index (CSI), kappa coefficient (KC), and F-measure reached up to 0.810, 0.836, and 0.895, respectively. These stable and accurate river extent mapping results obtained through fusing multiple images with high spatial resolution (SR) (10 m) and short revisit interval (0.4-4.4 days) are of great significance for enriching the data and methodology of hydrological studies.  © 1980-2012 IEEE.","Antennas; Image fusion; Mapping; Optical remote sensing; Probability; Radar imaging; Rivers; Support vector machines; Synthetic aperture radar; Index; Multi dimensional; Multi-dimensional weighted fusion; Posterior probability; Remote-sensing; River water; Support vectors machine; Uncertainty; Water mapping; Weighted fusion; river water; satellite data; Landsat","Image fusion; multidimensional weighted fusion; posterior probability; support vector machine (SVM); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85133641338"
"Li C.; Luo Z.; Wang Q.","Li, Changkai (57202303696); Luo, Zhenlin (57225173370); Wang, Qing (57225186065)","57202303696; 57225173370; 57225186065","Research on fusion method of SAR and RGB image based on wavelet transform","2021","Proceedings of SPIE - The International Society for Optical Engineering","11878","","118781V","","","","10.1117/12.2601760","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109396019&doi=10.1117%2f12.2601760&partnerID=40&md5=27e49108bc73cf47c4ee395294dad6bc","In view of the fusion of SAR image and RGB image, different methods based on wavelet transform (WT) were studied. Based on WT, SAR images are fused with different channel of RGB images respectively. In order to suppress the speckle in SAR images, Speckle Reducing Anisotropic Diffusion (SRAD) and nonlocal mean filter (NLM) were introduced respectively. The image fusion effect was comprehensively measured by using several image quality indexes. The results show that WT and NLM are more helpful to improve the quality of image fusion. © 2021 SPIE","Color image processing; Image compression; Image denoising; Image enhancement; Image fusion; Speckle; Synthetic aperture radar; Wavelet transforms; Fusion methods; Non-local mean filters; Quality indices; RGB images; SAR Images; Speckle reducing anisotropic diffusion; Radar imaging","Image fusion; Nonlocal mean filter; Speckle; Wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85109396019"
"Kong Y.; Hong F.; Leung H.; Peng X.","Kong, Yingying (35186206400); Hong, Fang (57315801600); Leung, Henry (7202811506); Peng, Xiangyang (57214935616)","35186206400; 57315801600; 7202811506; 57214935616","A fusion method of optical image and sar image based on dense-ugan and gram–schmidt transformation","2021","Remote Sensing","13","21","4274","","","","10.3390/rs13214274","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118155325&doi=10.3390%2frs13214274&partnerID=40&md5=3858ddecb0bc7adabfac80b0b9b1b717","To solve the problems such as obvious speckle noise and serious spectral distortion when existing fusion methods are applied to the fusion of optical and SAR images, this paper proposes a fusion method for optical and SAR images based on Dense-UGAN and Gram–Schmidt transformation. Firstly, dense connection with U-shaped network (Dense-UGAN) are used in GAN generator to deepen the network structure and obtain deeper source image information. Secondly, according to the particularity of SAR imaging mechanism, SGLCM loss for preserving SAR texture features and PSNR loss for reducing SAR speckle noise are introduced into the generator loss function. Meanwhile in order to keep more SAR image structure, SSIM loss is introduced to discriminator loss function to make the generated image retain more spatial features. In this way, the generated high-resolution image has both optical contour characteristics and SAR texture characteristics. Finally, the GS transformation of optical and generated image retains the necessary spectral properties. Experimental results show that the proposed method can well preserve the spectral information of optical images and texture information of SAR images, and also reduce the generation of speckle noise at the same time. The metrics are superior to other algorithms that currently perform well. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Geometrical optics; Image fusion; Image texture; Radar imaging; Remote sensing; Speckle; Synthetic aperture radar; Textures; Fusion methods; Gram-schmidt; Image information; Image-based; Loss functions; Optical image; Optical-; Remote sensing images; SAR Images; Speckle noise; Generative adversarial networks","Generative adversarial network; Gram–Schmidt; Image fusion; Loss function; Remote sensing image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118155325"
"Zou X.; Feng W.; Quan Y.; Li Q.; Dauphin G.; Xing M.","Zou, Xinshan (57959487200); Feng, Wei (57089587500); Quan, Yinghui (35181982300); Li, Qiang (57190836192); Dauphin, Gabriel (8283331300); Xing, Mengdao (7005922869)","57959487200; 57089587500; 35181982300; 57190836192; 8283331300; 7005922869","A Multi-Level Synergistic Image Decomposition Algorithm for Remote Sensing Image Fusion","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","3754","3757","3","10.1109/IGARSS46834.2022.9884942","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141584678&doi=10.1109%2fIGARSS46834.2022.9884942&partnerID=40&md5=d349f260278fbf832c6b9a2bf303df57","Image fusion is a technique for improving the image quality, and image decomposition is one of a common method of image processing. In this paper, a novel multi-level synergistic image decomposition algorithm is proposed for the fusion of remote sensing images. The fusion framework decomposes different input images at different level to extract the salient and low-rank parts, respectively. The salient parts are fused using a designed weighted fusion method based on nuclearnorm, and the low-rank parts are fused using weighted average strategy. The proposed method shows a superior fusion performance in the compared experiments with PCA and GS methods. © 2022 IEEE.","Image enhancement; Image fusion; Radar imaging; Synthetic aperture radar; Decomposition algorithm; Image decomposition; Images processing; Method of images; Multi-level image decom-position; Multilevels; Multispectral images; Remote sensing images; Remote-sensing; SAR Images; Remote sensing","image fusion; multi-level image decom-position; Multispectral image; remote sensing; SAR image","Conference paper","Final","","Scopus","2-s2.0-85141584678"
"Zhang L.; Chu Z.; Zou B.","Zhang, Lamei (56040361700); Chu, Zhongye (57938167700); Zou, Bin (55684454000)","56040361700; 57938167700; 55684454000","Multi Scale Ship Detection Based on Attention and Weighted Fusion Model for High Resolution SAR Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","631","634","3","10.1109/IGARSS46834.2022.9883844","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140403299&doi=10.1109%2fIGARSS46834.2022.9883844&partnerID=40&md5=3958f6a7dcc3b52b2ca586e834b7285f","Ship detection in SAR images is a challenging problem. CNN-based ship detection method in SAR images has achieved remarkable results. Due to the multi scale of the ships and interference from complex sea conditions or nearshore background in SAR images, many false alarms and missed detections can occur in ship detection. To solve these problems, a multi-scale ship detection network in SAR images based on attention and weighted fusion is proposed in this paper. First, a higher-resolution detect head is added based on the YOLOv5 framework for detecting tiny-scale ships in SAR images. Then, the coordinate attention block is introduced to refine the location features of ship targets and suppress the interference of complex background. Finally, in the feature fusion stage, adaptive weighted feature fusion is used to reduce feature redundancy. Experiments on the SSDD dataset show the effectiveness of the proposed method. © 2022 IEEE.","Complex networks; Image fusion; Radar imaging; Ships; Tracking radar; Coordinate attention; Features fusions; Fusion model; Multi-scales; Ship detection; Synthetic aperture radar; Synthetic aperture radar images; Weighted feature fusion; Weighted features; Weighted fusion; Synthetic aperture radar","coordinate attention; multi-scale; ship detection; Synthetic Aperture Radar (SAR); weighted feature fusion","Conference paper","Final","","Scopus","2-s2.0-85140403299"
"Emek R.A.; Demir N.","Emek, Recai Alper (57222222459); Demir, Nusret (56844945100)","57222222459; 56844945100","Building detection from sar images using unet deep learning method","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","44","4/W3","","215","218","3","10.5194/isprs-archives-XLIV-4-W3-2020-215-2020","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101898214&doi=10.5194%2fisprs-archives-XLIV-4-W3-2020-215-2020&partnerID=40&md5=ac0dda969cb34a630749324f0f39a8ae","SAR images are different from the optical images in terms of image properties with the values of scattering instead of reflectance. This makes SAR images difficult to apply the traditional object detection methodologies. In recent years, deep learning models are frequently used in segmentation and object detection purposes. In this study, we have investigated the potential of U-Net models for building detection from SAR and optical image fusion. The datasets used are Sentinel 1 SAR and Sentinel-2 multispectral images, provided from 'SpaceNet 6 Multi Sensor All- Weather Mapping' challenge. These images cover an area of 120 km2in Rotterdam, the Netherlands. As training datasets 20 pieces of 900 by 900 pixel sized HV polarized and optical image patches have been used together. The calculated loss value is 0.4 and the accuracy is 81%. © 2020 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Deep learning; Geometrical optics; Image fusion; Learning systems; Object detection; Object recognition; Synthetic aperture radar; Building detection; Image properties; Learning methods; Learning models; Multi sensor; Multispectral images; Optical image; Training data sets; Radar imaging","Building; Convolution Neural Network; Deep Learning; Object Detection; Radar; SAR; U-Net","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101898214"
"Chen Y.; Tian S.","Chen, Yujia (57213199750); Tian, Shufang (36958905700)","57213199750; 36958905700","Feature-level fusion between gaofen-5 and sentinel-1a data for tea plantation mapping","2020","Forests","11","12","1357","1","21","20","10.3390/f11121357","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097880907&doi=10.3390%2ff11121357&partnerID=40&md5=65a1f7b695157cde243c24569468343e","The accurate mapping of tea plantations is significant for government decision-making and environmental protection of tea-producing regions. Hyperspectral and Synthetic Aperture Radar (SAR) data have recently been widely used in land cover classification, but effective integration of these data for tea plantation mapping requires further study. This study developed a new feature-level image fusion method called LPPSubFus that combines locality preserving projection and subspace fusion (SubFus) to map tea plantations. Based on hyperspectral and SAR data, we first extracted spectral indexes, textures, and backscattering information. Second, this study applied LPPSubFus to tea plantation mapping with different classification algorithms. Finally, we compared the performance of LPPSubFus, SubFus, and pixel-level image fusion in tea plantation mapping. Feature-level image fusion performed better than pixel-level image fusion. An improvement of about 3% was achieved using feature-level image fusion compared to hyperspectral data alone. Regarding feature-level image fusion, LPPSubFus improved the overall accuracy by more than 3% compared to SubFus. In particular, LPPSubFus using neural network algorithms achieved the highest overall accuracy (95%) and over 90% producer and user accuracy for tea plantations and forests. In addition, LPPSubFus was more compatible with different classification algorithms than SubFus. Based on these findings, it is concluded that LPPSubFus has better and more stable performance in tea plantation mapping than pixel-level image fusion and SubFus. This study demonstrates the potential of integrating hyperspectral and SAR data via LPPSubFus for mapping tea plantations. Our work offers a promising tea plantation mapping method and contributes to the understanding of hyperspectral and SAR data fusion. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Accuracy; Algorithms; Classification; Data; Decision Making; Fusion; Mapping; Plantations; Data integration; Decision making; Image enhancement; Mapping; Pixels; Synthetic aperture radar; Tea; Textures; Classification algorithm; Feature level fusion; Government decisions; Image fusion methods; Land cover classification; Locality preserving projections; Neural network algorithm; Pixel-level image fusion; data set; environmental protection; image analysis; mapping method; Sentinel; spectral analysis; synthetic aperture radar; tea; Image fusion","Feature-level image fusion; Hyperspectral; Locality preserving projection; SAR; Tea plantation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097880907"
"Zhang C.; Feng Y.; Hu L.; Tapete D.; Pan L.; Liang Z.; Cigna F.; Yue P.","Zhang, Chenxiao (56412235100); Feng, Yukang (57867376800); Hu, Lei (57392739200); Tapete, Deodato (55221777800); Pan, Li (54393873900); Liang, Zheheng (57713064700); Cigna, Francesca (36720533600); Yue, Peng (57768830100)","56412235100; 57867376800; 57392739200; 55221777800; 54393873900; 57713064700; 36720533600; 57768830100","A domain adaptation neural network for change detection with heterogeneous optical and SAR remote sensing images","2022","International Journal of Applied Earth Observation and Geoinformation","109","","102769","","","","10.1016/j.jag.2022.102769","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136960546&doi=10.1016%2fj.jag.2022.102769&partnerID=40&md5=d59753651e778b1a13757706d403b08c","Heterogeneous remote sensing source-based change detection with optical and SAR data and their combined all-time and all-weather observation capability provides a reliable and promising solution for a wide range of applications. State-of-the-art supervised methods typically take a two-stage strategy that suffers from the loss of original image features and the introduction of noise on the transferred images. This paper proposes a domain adaptation-based multi-source change detection network (DA-MSCDNet) suitable to process heterogeneous optical and SAR images. DA-MSCDNet employs feature-level transformation to align inconsistent deep feature spaces in heterogeneous data. Feature space transformation and change detection are bridged within the network to encourage task communication. Experiments are conducted on two public datasets based on Sentinel-1A and Landsat-8 imagery acquired over the Sacramento, Yuba, and Sutter Counties (California, USA), and QuickBird-2 and TerraSAR-X imagery over Gloucester (UK), as well as one new large-scale dataset of Sentinel-2 and COSMO-SkyMed imagery over Wuhan (China). Compared with other six supervised and unsupervised approaches, the proposed method achieves the highest performance with an average precision of 80.81%, recall of 84.39%, mIOU of 73.67% and F1 score of 82.58%, beating the state-of-the-art method with 5.42% improvements on F1 score and 10 times efficiency on training time cost on the large-scale change detection task. © 2022 The Authors","California; China; Hubei; Sacramento County; Sutter County; United Kingdom; United States; Wuhan; Yuba County; artificial neural network; COSMO-SkyMed; detection method; image processing; Landsat; QuickBird; remote sensing; satellite imagery; Sentinel; synthetic aperture radar; Terra (satellite)","Domain adaptation; Feature alignment; Feature transformation; Heterogeneous change detection; Image fusion; Satellite imagery; Siamese network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136960546"
"Zou B.; Li H.; Zhang L.","Zou, Bin (55684454000); Li, Haolin (55782179900); Zhang, Lamei (56040361700)","55684454000; 55782179900; 56040361700","Multilevel Information Fusion-Based Change Detection for Multiangle PolSAR Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3041307","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097935497&doi=10.1109%2fLGRS.2020.3041307&partnerID=40&md5=6e79fe4f255a3d0e6694c9741b13f701","Change detection is a key technology in the field of polarimetric synthetic aperture radar (PolSAR) image processing. The current research on the change detection mainly focuses on studying PolSAR images with the same angle or small angle difference, and the angle problem is not considered. However, when the angle difference occurs, especially a large angle difference, some pixels might be falsely detected because the angle difference can affect the polarimetric characteristics. In this letter, we propose a multilevel information fusion-based (MIFB) method, which is suitable for extracting change information from PolSAR images with angle difference. In particular, the proposed method first adopts data resolution correction, then applies an improved feature-based registration algorithm, and finally, incorporates weighted graph theory with the superpixel segmentation algorithm to extract and merge pixel-based and object-based change areas to eliminate false alarms. Experimental results for multitemporal and multiangle PolSAR images reveal that the MIFB method can effectively eliminate false detection caused by angle differences and improve the detection accuracy.  © 2004-2012 IEEE.","Graph algorithms; Graph theory; Image fusion; Image segmentation; Information fusion; Pixels; Polarimeters; Synthetic aperture radar; Angle difference; Change detection; Detection accuracy; Feature based registration; Key technologies; Multilevel information fusion; Polarimetric synthetic aperture radars; Superpixel segmentations; detection method; image analysis; satellite imagery; synthetic aperture radar; Image enhancement","Change detection; image registration; multiangle; polarimetric synthetic aperture radar (PolSAR)","Article","Final","","Scopus","2-s2.0-85097935497"
"Zhu X.; Guo B.; Hu W.; Shi L.; Ma J.; Xue D.","Zhu, Xiaoxiu (57197872453); Guo, Baofeng (56577060100); Hu, Wenhua (57199745271); Shi, Lin (55364489800); Ma, Juntao (56388898000); Xue, Dongfang (55838628700)","57197872453; 56577060100; 57199745271; 55364489800; 56388898000; 55838628700","Scene Segmentation of Multi-Band ISAR Fusion Imaging Based on MB-PCSBL","2021","IEEE Sensors Journal","21","3","9204730","3520","3532","12","10.1109/JSEN.2020.3026109","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099338802&doi=10.1109%2fJSEN.2020.3026109&partnerID=40&md5=f56a9b3de8e0120fa279110eecc34617","We consider the problem of achieving multi-band inverse synthetic aperture radar (ISAR) fusion imaging of block structure targets with unknown block partition and develop a block-sparse recovering method based on matrix block pattern-coupled sparse Bayesian learning algorithm. Based on the sparse representation of multi-band ISAR fusion imaging model, a pattern-coupled hierarchical Gaussian prior is proposed to characterize the pattern relevance of scattering coefficients. The sparsity of each coefficient is controlled not only by its own hyperparameter, but also by the hyperparameters corresponding to its eight neighboring coefficients in the data matrix. The correlations between the coefficients in rows and columns are determined by different parameters, respectively. The proposed prior model can increase the model flexibility and promote the generation of block structures. Moreover, the whole observation scene is segmented into multiple sub-scenes to reduce the memory storage space and the computational complexity. Parameters and the fusion image result of each sub-scene are derived by the expectation-maximization method. The multi-band ISAR fusion image result of the whole scene is obtained through the stitching of the sub-scenes imaging results. Experimental results demonstrate the effectiveness and superiority of the proposed algorithm. © 2001-2012 IEEE.","Digital storage; Image fusion; Inverse problems; Inverse synthetic aperture radar; Learning algorithms; Matrix algebra; Maximum principle; Block structures; Expectation-maximization method; Inverse synthetic aperture radars (ISAR); Modeling flexibility; Scattering co-efficient; Scene segmentation; Sparse Bayesian learning; Sparse representation; Radar imaging","block-sparse signal; expectation-maximization; Inverse synthetic aperture radar (ISAR); multi-band fusion; sparse Bayesian learning","Article","Final","","Scopus","2-s2.0-85099338802"
"Xiao G.; Zhang L.","Xiao, Guangyi (57328144900); Zhang, Long (57222227721)","57328144900; 57222227721","Super-resolved synthetic aperture radar image reconstruction based on multiresolution fusion discrimination","2022","Journal of Electronic Imaging","31","4","043036","","","","10.1117/1.JEI.31.4.043036","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142247391&doi=10.1117%2f1.JEI.31.4.043036&partnerID=40&md5=dfa7f2de212cf7f1070bdf169a0185bd","Generative adversarial networks (GANs) are utilized for synthetic aperture radar (SAR) image super-resolution reconstruction, affording realistic texture details. However, existing GANs only discriminate the final generated high-resolution (HR) image after two consecutive upsampling processes, which ignore some high-frequency information of the reconstructed images. To resolve this issue, a multiresolution fusion discrimination (MRFD) algorithm is proposed to discriminate the reconstructed feature maps after each upsampling. First, a multiresolution discrimination process discriminates the authenticity of each upsampled feature map separately, which reduces the image distortion imposed during two consecutive upsampling processes. Besides, multiresolution feature fusion further preserves the consistent high-frequency texture structures. Finally, a multiscale dense network extracts image features in different scales, with multiscale dense block's dense connections improving parameter utilization. The experimental results on a SAR dataset demonstrate that the proposed MRFD algorithm performs better in reconstructing the texture details of HR images.  © 2022 SPIE and IS&T.","Image enhancement; Image fusion; Image reconstruction; Image texture; Optical resolving power; Radar imaging; Signal sampling; Synthetic aperture radar; Textures; Discrimination algorithms; Feature map; High-resolution images; Image super-resolution reconstruction; Images reconstruction; Multiresolution fusion; Multiresolution fusion discrimination; Superresolution; Synthetic aperture radar images; Upsampling; Generative adversarial networks","generative adversarial network; multiresolution fusion discrimination; super-resolution; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85142247391"
"De Borba A.A.; Marengoni M.; Frery A.C.","De Borba, Anderson A. (57215682846); Marengoni, Mauricio (6602235863); Frery, Alejandro C. (7003561251)","57215682846; 6602235863; 7003561251","Fusion of Evidences in Intensity Channels for Edge Detection in PolSAR Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3022511","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117343153&doi=10.1109%2fLGRS.2020.3022511&partnerID=40&md5=715f0876e73419c5b0222e96b8f6cb73","Polarimetric synthetic aperture radar (PolSAR) sensors have reached an essential position in remote sensing. The images they provide have speckle noise, making their processing and analysis challenging tasks. We discuss an edge detection method based on the fusion of evidences obtained in the intensity channels hh, hv, and vv of PolSAR multilook images. The method consists of detecting transition points in the thinnest possible range of data that covers two regions using maximum likelihood under the Wishart distribution. The fusion methods used are: simple average, multiresolution discrete wavelet transform (MR-DWT), principal component analysis (PCA), receiver operating characteristic (ROC) statistics, multiresolution stationary wavelet transform (MR-SWT), and a multiresolution method based on singular value decomposition (MR-SVD). A quantitative analysis suggests that PCA and MR-SVD provide the best results.  © 2004-2012 IEEE.","Discrete wavelet transforms; Edge detection; Image fusion; Polarimeters; Principal component analysis; Radar imaging; Remote sensing; Singular value decomposition; Synthetic aperture radar; Wavelet decomposition; Fusion methods; Maximum-likelihood estimation; Multiresolution; Polarimetric synthetic aperture radar; Polarimetric synthetic aperture radars; Principal-component analysis; Radar sensors; Remote-sensing; Speckle noise; Synthetic aperture radar images; detection method; image classification; satellite imagery; Maximum likelihood estimation","Edge detection; Fusion methods; Maximum likelihood estimation; Polarimetric synthetic aperture radar (PolSAR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117343153"
"Jia X.; Wang H.; Wang M.; Wang L.","Jia, Xiaoya (57291665900); Wang, Hongqiao (56322051800); Wang, Mian (57818530000); Wang, Ling (57874911400)","57291665900; 56322051800; 57818530000; 57874911400","A Fusion Method with Image Preprocessing and Improved YOLOv3 for SAR Image Ship Target Detection","2021","Lecture Notes on Data Engineering and Communications Technologies","88","","","1890","1901","11","10.1007/978-3-030-70665-4_204","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116893612&doi=10.1007%2f978-3-030-70665-4_204&partnerID=40&md5=4d88b3b531caaa1154558190c1b31ab1","Based on the application of synthetic aperture radar (SAR), a fusion method with image preprocessing and improved YOLOv3 for SAR image ship target detection is proposed. For the coherent speckle noise of SAR images, preprocessing of fast discrete curvelet transform based on Wrapping is performed to reduce noise interference. In order to fully integrate multiple feature maps, borrowing from the BiFPN structure, a top-down, bottom-up, and jump-connected three-level structure BiFPN_C3 is proposed, which increases the multi-level feature structure fusion. Experiments show that the improved algorithm has an average precision of 10.44% on the public SAR ship data set, reaching 78.52%. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Image enhancement; Image fusion; Radar imaging; Ships; Curvelet transforms; Features fusions; Fusion methods; Image preprocessing; Noise interference; Ship targets; Speckle noise; Synthetic aperture radar images; Targets detection; YOLOv3; Synthetic aperture radar","Feature fusion; Image preprocessing; SAR image; YOLOv3","Book chapter","Final","","Scopus","2-s2.0-85116893612"
"Liu R.; Zhang H.; Ling J.","Liu, Rui (57669982600); Zhang, Hongsheng (55349777400); Ling, Jing (57353491400)","57669982600; 55349777400; 57353491400","Hybrid Transformer Networks for Urban Land Use Classification from Optical and SAR Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","707","710","3","10.1109/IGARSS46834.2022.9883122","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140409042&doi=10.1109%2fIGARSS46834.2022.9883122&partnerID=40&md5=6293733aeedd0cd2836efb9e450f83dd","Mapping the land cover/use type of urban area surface plays a vital role in many remote sensing applications. The performance of classification is inevitably limited by the finite amount of information available from a single data source, the restricted atmosphere condition and the complex landscape of the urban areas. Even when multiple sources of data are used, the fusion strategy is relatively homogeneous. In this paper, we aim to explore the potential of transformer based fusion method in mapping the urban regions with optical and synthetic aperture radar images. Specifically, we propose a hybrid fusion transformer network that simultaneously implements multi-source data fusion at both the feature and the decision levels. The experiments are conducted on the high resolution multiple remote sensing images, and the results show that the hybrid fusion based on transformer can achieve 82.17% in overall accuracy (OA) and 76.91 % in kappa coefficient. Moreover, compared with convolution neural network based methods, the transformer based methods are on average 2% higher in OA and 3.6% higher in kappa coefficient. © 2022 IEEE.","Classification (of information); Image classification; Image fusion; Land use; Optical remote sensing; Radar imaging; Synthetic aperture radar; Hybrid fusions; Kappa coefficient; Land cover/use; Landuse classifications; Overall accuracies; Trans-former; Urban areas; Urban land cover/use classification; Urban land use; Urban land-cover; Mapping","image fusion; trans-former; urban land cover/use classification","Conference paper","Final","","Scopus","2-s2.0-85140409042"
"Xu Y.; Ye F.; Ren B.; Lu L.; Cui X.; Chanussot J.; Wu Z.","Xu, Yang (57188730185); Ye, Fei (57671266100); Ren, Bo (57209552437); Lu, Liangfu (26651953800); Cui, Xudong (57671565400); Chanussot, Jocelyn (6602159365); Wu, Zebin (20437030300)","57188730185; 57671266100; 57209552437; 26651953800; 57671565400; 6602159365; 20437030300","Tensor representation for remote sensing images","2021","Tensors for Data Processing: Theory, Methods, and Applications","","","","483","536","53","10.1016/B978-0-12-824447-0.00019-4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129667783&doi=10.1016%2fB978-0-12-824447-0.00019-4&partnerID=40&md5=ca5c84aa76ab39736540b03507b1ab71","Tensor representation is a feasible solution for analyzing large-volume, multirelational, and multimodal datasets, which are often conveniently represented as multiway arrays or tensors. It is therefore valuable and promising for the geoscience and remote sensing research communities to review tensor representation as an emerging tool for remote sensing data analysis. This chapter presents some relevant remote sensing data analysis methodologies and techniques, organized in two main topics: data fusion and feature extraction. To start with, we provide brief reviews on the mechanisms and conventional methods of both topics. We then introduce several methods that have achieved state-of-the-art performances by incorporating tensor representation into the models. Furthermore, three tensor representation-based methods are discussed in detail to demonstrate the superiority of tensor representation in remote sensing image processing. © 2022 Elsevier Inc. All rights reserved.","","Feature extraction; Hyperspectral image; Image fusion; Multispectral image; Polarimetric synthetic aperture radar; Remote sensing; Tensor representation","Book chapter","Final","","Scopus","2-s2.0-85129667783"
"Li Y.; Wu J.; Zhong B.; Shi X.; Xu K.; Ao K.; Sun B.; Ding X.; Wang X.; Liu Q.; Yang A.; Chen F.; Shi M.","Li, Yi (57208321890); Wu, Junjun (56066202800); Zhong, Bo (57205877879); Shi, Xiaoliang (37010493700); Xu, Kunpeng (57201117774); Ao, Kai (57215416194); Sun, Bin (56041185500); Ding, Xiangyuan (56422935600); Wang, Xinshuang (57542658600); Liu, Qinhuo (7406294260); Yang, Aixia (56089193200); Chen, Fei (57487857800); Shi, Mengqi (57468714400)","57208321890; 56066202800; 57205877879; 37010493700; 57201117774; 57215416194; 56041185500; 56422935600; 57542658600; 7406294260; 56089193200; 57487857800; 57468714400","Methods of Sandy Land Detection in a Sparse-Vegetation Scene Based on the Fusion of HJ-2A Hyperspectral and GF-3 SAR Data","2022","Remote Sensing","14","5","1203","","","","10.3390/rs14051203","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126317170&doi=10.3390%2frs14051203&partnerID=40&md5=5c091817ffd01f83fd57a9139ec2d0d1","Accurate identification of sandy land plays an important role in sandy land prevention and control. It is difficult to identify the nature of sandy land due to vegetation covering the soil in the sandy area. Therefore, HJ-2A hyperspectral data and GF-3 Synthetic Aperture Radar (SAR) data were used as the main data sources in this article. The advantages of the spectral characteristics of a hyperspectral image and the penetration characteristics of SAR data were used synthetically to carry out mixed-pixel decomposition in the “horizontal” direction and polarization decomposition in the “vertical” direction. The results showed that in the study area of the Otingdag Sandy Land, in China, the accuracy of sandy land detection based on feature-level fusion and single GF-3 data was verified to be 92% in both cases by field data; the accuracy of sandy land detection based on feature-level fusion was verified to be 88.74% by the data collected from Google high-resolution imagery, which was higher than that based on single HJ-2A (74.17%) and single GF-3 data (88.08%). To further verify the universality of the feature-level fusion method for sandy land detection, Alxa sandy land was also used as a verification area and the accuracy of sandy land detection was verified to be as high as 88.74%. The method proposed in this paper made full use of the horizontal and vertical structural information of remote sensing data. The problem of mixed pixels in sparse-vegetation scenes in the horizontal direction and the problem of vegetation covering sandy soil in the vertical direction were both well solved. Accurate identification of sandy land can be realized effectively, which can provide technical support for sandy land prevention and control. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Feature extraction; Horizontal wells; Image classification; Image fusion; Pixels; Radar imaging; Remote sensing; Spectroscopy; Support vector machines; Synthetic aperture radar; Vegetation; Feature-level fusions; Mixed pixel; Mixed pixel decomposition; Pixel decompositions; Polarization decomposition; Prevention and controls; Radar data; Sandy land; Support vector machine classification; Vegetation covering; Polarization","Image fusion; Mixed pixel decomposition; Polarization decomposition; Sandy land; Support vector machine classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126317170"
"Liu Q.; Lang L.","Liu, Qingshu (57227635300); Lang, Liang (16424539300)","57227635300; 16424539300","MMFF: Multi-manifold feature fusion based neural networks for target recognition in complex-valued SAR imagery","2021","ISPRS Journal of Photogrammetry and Remote Sensing","180","","","151","162","11","10.1016/j.isprsjprs.2021.08.008","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113325361&doi=10.1016%2fj.isprsjprs.2021.08.008&partnerID=40&md5=a2d765430da92b61f5aee72fdf0e9dc4","In this paper, we propose a novel multi-manifold feature fusion (MMFF) based deep learning framework for automatic target recognition (ATR) in complex-valued synthetic aperture radar (SAR) images. The conventional real-valued convolutional neural networks (CNN) usually ignore the intrinsic geometry structure of complex numbers, and the existing complex- and manifold-valued CNNs are not in accordance with the multimodality of SAR images. Founded on parallel manifold convolution branches for magnitude and phase data, the proposed MMFF model takes both the underlying geometry and the multimodal information of complex-valued SAR data into consideration, to learn manifold features of military and civilian targets for recognition tasks. Besides, we investigate manifold-valued layers such as activation function and batch normalization to leverage the MMFF architecture as well as the multimodal data. We validate our methods on two public SAR datasets in single- and dual-polarization, and achieve obviously better classification performance with less model parameters, compared with previously proposed real-, complex- and manifold-valued baselines. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Automatic target recognition; Classification (of information); Complex networks; Convolution; Convolutional neural networks; Deep learning; Geometrical optics; Geometry; Image fusion; Lie groups; Radar imaging; Radar target recognition; Remote sensing; Complex-valued; Complex-valued synthetic aperture radar image; Convolutional neural network; Features fusions; Lie-groups; Neural-networks; Riemannian manifold; Synthetic Aperture Radar Imagery; Synthetic aperture radar images; Target recognition; artificial neural network; data set; image analysis; military application; numerical model; synthetic aperture radar; Synthetic aperture radar","Complex-valued SAR images; Convolutional neural networks; Feature fusion; Lie group; Riemannian manifold","Article","Final","","Scopus","2-s2.0-85113325361"
"Lu C.-H.; Lin Y.-S.; Chuang R.Y.","Lu, Chih-Heng (53865228400); Lin, Ya-Shien (57150158800); Chuang, Ray Y. (36730693000)","53865228400; 57150158800; 36730693000","Pixel Offset Fusion of SAR and Optical Images for 3-D Coseismic Surface Deformation","2021","IEEE Geoscience and Remote Sensing Letters","18","6","9094251","1049","1053","4","10.1109/LGRS.2020.2991758","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106753962&doi=10.1109%2fLGRS.2020.2991758&partnerID=40&md5=ae9ef6e98a1d23e9d512df46d9dddf18","This letter presents a 3-D fusion method combining with the pixel offset results of synthetic aperture radar (SAR) and optical images for measuring 3-D coseismic deformation, which suffered the limitations of huge displacement gradient, low coherence, and large temporal interval. This method takes the advantages of the pixel offset results of the two different types of satellites. We use the hanging wall deformation of the 1999 Chi-Chi earthquake as an example and generate two sets of 3-D coseismic displacements from the fusion method and radar images only. We compare the two sets of results to GPS and field survey data, and the 3-D fusion result shows more consistent 3-D displacements than one derived from radar images with decreased root mean square (rms) values of 20%-50%. The results of this letter provide a detailed information of displacement distribution in the hanging wall of the Chelungpu fault (CLPF) during the Chi-Chi earthquake. Along the fault, the vertical displacement increases from south to north gradually and the vertical pattern follows the topography. The area where uplift starts to be higher than the average of vertical displacements is consistent with the turning point of CLPF. The 3-D fusion method appears not only the potentials to describe the uplift behavior of active fault, but also the abilities to constrain the subsurface fault geometry.  © 2004-2012 IEEE.","Satellites; Deformation; Earthquakes; Faulting; Geometrical optics; Image fusion; Pixels; Radar measurement; Synthetic aperture radar; Topography; Co-seismic displacement; Coseismic deformation; Displacement distribution; Displacement gradients; Hanging-wall deformations; Root mean square values; Surface deformation; Vertical displacements; deformation; deformation mechanism; image analysis; optical property; pixel; synthetic aperture radar; three-dimensional modeling; Radar imaging","3-D fusion method; Chi-Chi earthquake; radar; subpixel correlation; surface displacement","Article","Final","","Scopus","2-s2.0-85106753962"
"Li W.; Chen X.; Li G.; Bi Y.","Li, Wenrong (57555764300); Chen, Xia (57556332000); Li, Guochun (57556711900); Bi, Yu (57555954300)","57555764300; 57556332000; 57556711900; 57555954300","Construction of Yunnan's Agricultural Ecological Civilization Based on Intelligent UAV and SAR Image Analysis","2022","Proceedings - 4th International Conference on Smart Systems and Inventive Technology, ICSSIT 2022","","","","1639","1642","3","10.1109/ICSSIT53264.2022.9716302","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127359552&doi=10.1109%2fICSSIT53264.2022.9716302&partnerID=40&md5=5fcef410787a8e53296189a2edd369d5","This paper studies the construction of Yunnan's agricultural ecological civilization based on the analysis of intelligent drones and SAR images. This article deeply discusses several key problems faced by multi-rotor UAV SAR imaging processing from the aspects of imaging algorithm, motion compensation and auto-focusing, and puts forward an effective imaging processing scheme. The SIFT algorithm is used to extract the image feature points, and the BBF algorithm calculates and generates a matching point set. According to the matching point set, the inter-image perspective transformation model is calculated to complete the image registration, and the wavelet transform algorithm is used to realize the registration image fusion. Based on SAR image analysis and dimensionality reduction of remote sensing data, dimensionality reduction, extraction and classification of remote sensing images of agricultural ecology in Yunnan are carried out. © 2022 IEEE","Agriculture; Ecology; Geometry; Image analysis; Image compression; Image fusion; Motion compensation; Radar imaging; Remote sensing; Unmanned aerial vehicles (UAV); Wavelet transforms; Agricultural ecological civilization; Image-analysis; Imaging algorithm; Imaging processing; Intelligent UAV; Matching points; Point set; SAR Images; SAR imaging; Yunnan; Synthetic aperture radar","Agricultural Ecological Civilization; Intelligent UAV; SAR Image; Yunnan","Conference paper","Final","","Scopus","2-s2.0-85127359552"
"Kong Y.; Yan B.; Liu Y.; Leung H.; Peng X.","Kong, Yingying (35186206400); Yan, Biyuan (57214936311); Liu, Yanjuan (57214940367); Leung, Henry (7202811506); Peng, Xiangyang (57214935616)","35186206400; 57214936311; 57214940367; 7202811506; 57214935616","Feature-level fusion of polarized sar and optical images based on random forest and conditional random fields","2021","Remote Sensing","13","7","1323","","","","10.3390/rs13071323","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103906480&doi=10.3390%2frs13071323&partnerID=40&md5=ed0895de346e10ee6f237339b37f7aed","In terms of land cover classification, optical images have been proven to have good classification performance. Synthetic Aperture Radar (SAR) has the characteristics of working all-time and all-weather. It has more significant advantages over optical images for the recognition of some scenes, such as water bodies. One of the current challenges is how to fuse the benefits of both to obtain more powerful classification capabilities. This study proposes a classification model based on random forest with the conditional random fields (CRF) for feature-level fusion classification using features extracted from polarized SAR and optical images. In this paper, feature importance is introduced as a weight in the pairwise potential function of the CRF to improve the correction rate of misclassified points. The results show that the dataset combining the two provides significant improvements in feature identification when compared to the dataset using optical or polarized SAR image features alone. Among the four classification models used, the random forest-importance: conditional random fields (RF-Im_CRF) model developed in this paper obtained the best overall accuracy (OA) and Kappa coefficient, validating the effectiveness of the method. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Decision trees; Geometrical optics; Image enhancement; Image fusion; Radar imaging; Random forests; Synthetic aperture radar; Classification models; Classification performance; Conditional random field; Feature identification; Feature level fusion; Land cover classification; Overall accuracies; Potential function; Random processes","Conditional random fields; Feature-level fusion; Optical image; Polarized SAR; Random forest","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103906480"
"Tian B.; Zhang X.; Tang X.; Wei S.; Shi J.","Tian, Bokun (57207879186); Zhang, Xiaoling (52265071000); Tang, Xinxin (57189628594); Wei, Shunjun (54586127400); Shi, Jun (57226054596)","57207879186; 52265071000; 57189628594; 54586127400; 57226054596","A joint sparse recovery algorithm for coprime adjacent array synthetic aperture radar 3D sparse imaging","2021","International Journal of Remote Sensing","42","17","","6560","6580","20","10.1080/01431161.2021.1939913","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108858088&doi=10.1080%2f01431161.2021.1939913&partnerID=40&md5=d30a260aab6f28176bfa0a959208683a","In the linear array synthetic aperture radar (LASAR) three-dimensional (3D) imaging, the spacing between adjacent elements in the uniform linear array (ULA) must satisfy the Nyquist sampling theorem to avoid the grating lobes, which makes the number of elements in the ULA very large. To reduce the elements in the ULA, the coprime adjacent array (CAA) with the same aperture length as the ULA is used when conducting LASAR 3D sparse imaging by compressed sensing (CS) algorithms. However, due to the increased autocorrelation coefficient of the measurement matrix, there exists grating lobes interference in the CAA-SAR imaging results. To solve this problem, we propose a joint sparse recovery (JSR) algorithm for CAA-SAR 3D sparse imaging. Firstly, we conduct sparse imaging on the CAA and its two subarrays, respectively. Secondly, the imaging results of the CAA and its two subarrays are performed image segmentation by the OTSU algorithm to extract their target-areas’ imaging results. Finally, we perform the image fusion by the wavelet transform on the target-areas’ imaging results to obtain the final imaging results. Both simulation and experimental results indicate that the imaging quality and computational efficiency of the JSR algorithm are higher than the random sampling array (RSA) and CAA under the same number of array elements. Besides, under the same aperture length, the JSR algorithm improves the computational efficiency than the ULA without imaging-quality loss. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Computation theory; Computational efficiency; Efficiency; Image fusion; Image segmentation; Radar signal processing; Synthetic aperture radar; Wavelet transforms; Autocorrelation coefficient; Compressive sensing; Linear-array synthetic-aperture radars; Measurement matrix; Nyquist sampling theorem; Sparse recovery; Three dimensional (3-D) imaging; Uniform linear arrays; Radar imaging","","Article","Final","","Scopus","2-s2.0-85108858088"
"Wang X.; Zhu D.; Li G.; Zhang X.-P.","Wang, Xueqian (57829747800); Zhu, Dong (57001841800); Li, Gang (55547117794); Zhang, Xiao-Ping (35214025100)","57829747800; 57001841800; 55547117794; 35214025100","A New Image Fusion Method for Ship Target Enhancement in Spaceborne and Airborne SAR Collaboration","2021","Proceedings of 2021 IEEE 24th International Conference on Information Fusion, FUSION 2021","","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123422662&partnerID=40&md5=d8b61c06278e4b42bf6a459c42c2b3f1","In this paper, we investigate the fusion of spaceborne synthetic aperture radar (SAR) and airborne SAR images and its application to ship target enhancement. In this paper, we propose a new target proposal and clutter copula (TPCC)-based image fusion method for the collaboration of spaceborne and airborne SARs. TPCC enhances the common ship target areas in spaceborne and airborne SAR images via the intersection of target proposals and suppresses the clutter areas by establishing the joint distribution of clutter in the spaceborne and airborne SAR images based on the copula theory. Compared with other commonly used image fusion methods, the target dependence and clutter dependence in the spaceborne and airborne SAR images are newly exploited in TPCC. We demonstrate the superiority of TPCC in terms of target-to-clutter ratios (TCRs) by using composite images combining Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR images.  © 2021 International Society of Information Fusion (ISIF).","Antennas; Clutter (information theory); Image enhancement; Image fusion; Information fusion; Radar clutter; Radar imaging; Ships; Space-based radar; Unmanned aerial vehicles (UAV); Airborne synthetic aperture radars; Copula theory; Image applications; Image fusion methods; Sensor fusion; Ship targets; Spaceborne synthetic aperture radars; Synthetic aperture radar images; Target enhancement; Target proposal; Synthetic aperture radar","Copula theory; Sensor fusion; Target enhancement; Target proposals","Conference paper","Final","","Scopus","2-s2.0-85123422662"
"Wang X.; Zhu D.; Li G.; Zhang X.-P.; He Y.","Wang, Xueqian (57829747800); Zhu, Dong (57001841800); Li, Gang (55547117794); Zhang, Xiao-Ping (35214025100); He, You (57212448603)","57829747800; 57001841800; 55547117794; 35214025100; 57212448603","Proposal-Copula-Based Fusion of Spaceborne and Airborne SAR Images for Ship Target Detection⁎⁎","2022","Information Fusion","77","","","247","260","13","10.1016/j.inffus.2021.07.019","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113408516&doi=10.1016%2fj.inffus.2021.07.019&partnerID=40&md5=e5766ceb674dc8122c76536f555c56f9","In this paper, we consider the problem of fusion of synthetic aperture radar (SAR) images from spaceborne and airborne sensors and investigate its applications to inshore ship target detection. Existing SAR image fusion methods mainly focus on image denoising or texture enhancement, but show limited improvement of target-to-clutter ratios (TCRs) in composite images and lead to deteriorated target detection performance. To address this issue, we propose a new method for the fusion of spaceborne and airborne SAR images based on the target proposal and the copula theory (TPCT). In TPCT, target and clutter correspondence between different images are exploited to improve the TCRs of composite images. TPCT consists of three steps. First, target proposals are extracted from spaceborne and airborne SAR images and then fused to enhance the common ship target areas therein. Second, a new method to construct the joint probability density function (PDF) of clutter in spaceborne and airborne SAR images is presented to model the statistical dependence of clutter therein based on the copula theory. This copula-based joint PDF is used to suppress the clutter areas remained in the intersection of target proposals. Third, clues from the intersection of target proposals and the copula-based joint PDF of clutter are fused by the Hadamard product to generate the composite image with enhanced ship targets and the suppressed clutter. Experimental results based on measured spaceborne and airborne SAR data show that the proposed TPCT fusion method leads to higher TCRs of composite images and better performance in the ship detection task than other commonly used image fusion methods. © 2021","Chemical detection; Clutter (information theory); Image denoising; Image enhancement; Image fusion; Probability density function; Radar clutter; Ships; Space-based radar; Synthetic aperture radar; Textures; Air-borne sensors; Detection performance; Image fusion methods; Joint probability density function; Statistical dependence; Synthetic aperture radar (SAR) images; Target-to-clutter ratios; Texture enhancement; Radar imaging","airborne SAR; copula theory; information fusion; ship detection; Spaceborne synthetic aperture radar (SAR); target proposals","Article","Final","","Scopus","2-s2.0-85113408516"
"Ling J.; Zhang H.; Lin Y.","Ling, Jing (57353491400); Zhang, Hongsheng (55349777400); Lin, Yinyi (57205063110)","57353491400; 55349777400; 57205063110","Improving urban land cover classification in cloud-prone areas with polarimetric sar images","2021","Remote Sensing","13","22","4708","","","","10.3390/rs13224708","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119917415&doi=10.3390%2frs13224708&partnerID=40&md5=557ad32a70a9356c5330b4229722a9ce","Urban land cover (ULC) serves as fundamental environmental information for urban studies, while accurate and timely ULC mapping remains challenging due to cloud contamination in tropical and subtropical areas. Synthetic aperture radar (SAR) has excellent all-weather working capability to overcome the challenge, while optical SAR data fusion is often required due to the limited land surface information provided by SAR. However, the mechanism by which SAR can compensate optical images, given the occurrence of clouds, in order to improve the ULC mapping, remains unexplored. To address the issue, this study proposes a framework, through various sampling strategies and three typical supervised classification methods, to quantify the ULC classification accuracy using optical and SAR data with various cloud levels. The land cover confusions were investigated in detail to understand the role of SAR in distinguishing land cover under different types of cloud coverage. Several interesting experimental results were found. First, 50% cloud coverage over the optical images decreased the overall accuracy by 10–20%, while the incorporation of SAR images was able to improve the overall accuracy by approximately 4%, by increasing the recognition of cloud-covered ULC information, particularly the water bodies. Second, if all the training samples were not contaminated by clouds, the cloud coverage had a higher impact with a reduction of 35% in the overall accuracy, whereas the incorporation of SAR data contributed to an increase of approximately 5%. Third, the thickness of clouds also brought about different impacts on the results, with an approximately 10% higher reduction from thick clouds compared with that from thin clouds, indicating that certain spectral information might still be available in the areas covered by thin clouds. These findings provide useful references for the accurate monitoring of ULC over cloud-prone areas, such as tropical and subtropical cities, where cloud contamination is often unavoidable. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Geometrical optics; Image classification; Image enhancement; Image fusion; Mapping; Radar imaging; Tropics; Cloud contamination; Cloud coverage; Optical and synthetic aperture radar fusion; Optical image; Optical-; Overall accuracies; Radar data; Urban land cover classification; Urban land cover mappings; Urban land-cover; Synthetic aperture radar","Clouds; Optical and SAR fusion; Urban land cover","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119917415"
"Chen Y.; Tian S.","Chen, Yujia (57213199750); Tian, Shufang (36958905700)","57213199750; 36958905700","Comparison of pixel-and object-based image analysis for tea plantation mapping using hyperspectral Gaofen-5 and synthetic aperture radar data","2020","Journal of Applied Remote Sensing","14","4","044516","","","","10.1117/1.JRS.14.044516","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098648823&doi=10.1117%2f1.JRS.14.044516&partnerID=40&md5=1c3e42105f6b63768883178efd0a8b82","Accurately mapping tea plantation distribution is crucial to environmental protection and sustainable development. Hyperspectral and synthetic aperture radar (SAR) data have recently been widely used in land cover classification, but their ability to extract tea plantation regions still needs to be confirmed. Compared with traditional pixel-based image analysis (PBIA), object-based image analysis (OBIA) for tea plantation mapping is more worthy of implementation. This study explored the performance of Gaofen-5 (GF-5) and copolarized SAR data for tea plantation mapping using pixel-and object-based support vector machine algorithms in Wuyishan, China. Comparison of PBIA and OBIA demonstrated the significant differences in visual effect and classification accuracy. The object-based classifications especially offered a more contiguous depiction with fewer speckles of tea plantations than pixel-based classifications did. Moreover, object-based classifications improved overall accuracy (OA) between 1.7% and 7.9% in all scenarios when compared to pixel-based classifications. As for datasets, classifications using only GF-5 data obtained an OA of over 85%, while fusing images decreased classification accuracy due to the lower separability between tea and forest, showcasing that the fusion of hyperspectral and SAR data does not guarantee the improvement of classification accuracy. The integration of GF-5, horizontal transmit and horizontal receive (HH), and vertical transmit and vertical receive (VV) polarized data outperformed other data combinations in both pixel-and object-based classifications and achieved the highest OA (95.58%) in OBIA, with a 98.13% producer accuracy and 93.34% user accuracy of tea plantations. The results indicated OBIA could overcome the shortcomings of the PBIA and effectively improve the mapping accuracy of tea plantation, and OBIA integrating GF-5, HH, and VV polarized data could play a distinguished role in tea plantation mapping. This work provides a promising approach for mapping tea plantations and demonstrates that the integration of GF-5 and copolarized data can improve spectral separability of vegetation, which is also significant for general forest mappings.  © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Classification (of information); Environmental protection; Forestry; Image analysis; Image enhancement; Photomapping; Pixels; Radar imaging; Support vector machines; Sustainable development; Synthetic aperture radar; Tea; Classification accuracy; Land cover classification; Object based image analysis; Object based image analysis (OBIA); Object-based classifications; Pixel based classifications; Pixel- and object-based classification; Support vector machine algorithm; Data integration","copolarized data; hyperspectral; image fusion; object-based image analysis; tea plantation mapping","Article","Final","","Scopus","2-s2.0-85098648823"
"Xuan J.; Xin Z.; Liao G.; Huang P.; Wang Z.; Sun Y.","Xuan, Jiayu (57292056800); Xin, Zhihui (56260775700); Liao, Guisheng (10042143700); Huang, Penghui (56688948900); Wang, Zhixu (57292517800); Sun, Yu (57292056700)","57292056800; 56260775700; 10042143700; 56688948900; 57292517800; 57292056700","Change Detection Based on Fusion Difference Image and Multi-Scale Morphological Reconstruction for SAR Images","2022","Remote Sensing","14","15","3604","","","","10.3390/rs14153604","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137081251&doi=10.3390%2frs14153604&partnerID=40&md5=2ae133c06adeae7a2f6656fce3295948","Synthetic aperture radar (SAR) image-change detection is widely used in various fields, such as environmental monitoring and ecological monitoring. There is too much noise and insufficient information utilization, which make the results of change detection inaccurate. Thus, we propose an SAR image-change-detection method based on multiplicative fusion difference image (DI), saliency detection (SD), multi-scale morphological reconstruction (MSMR), and fuzzy c-means (FCM) clustering. Firstly, a new fusion DI method is proposed by multiplying the ratio (R) method based on the ratio of the image before and after the change and the mean ratio (MR) method based on the ratio of the image neighborhood mean value. The new DI operator ratio–mean ratio (RMR) enlarges the characteristics of unchanged areas and changed areas. Secondly, saliency detection is used in DI, which is conducive to the subsequent sub-area processing. Thirdly, we propose an improved FCM clustering-change-detection method based on MSMR. The proposed method has high computational efficiency, and the neighborhood information obtained by morphological reconstruction is fully used. Six real SAR data sets are used in different experiments to demonstrate the effectiveness of the proposed saliency ratio–mean ratio with multi-scale morphological reconstruction fuzzy c-means (SRMR-MSMRFCM). Finally, four classical noise-sensitive methods are used to detect our DI method and demonstrate the strong denoising and detail-preserving ability. © 2022 by the authors.","Computational efficiency; Fuzzy systems; Image fusion; Image reconstruction; Radar imaging; Synthetic aperture radar; Change detection; Clusterings; Difference images; Fusion difference image; Image change detection; Mean ratio; Morphological reconstruction; Multi-scales; Saliency detection; Synthetic aperture radar images; Change detection","change detection; clustering; fusion difference image; morphological reconstruction; saliency detection; SAR image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137081251"
"Zhang L.; Liu Y.; Guo Q.; Yin H.; Li Y.; Du P.","Zhang, Limin (57288897200); Liu, Yingjian (8285018300); Guo, Qingxiang (57219548407); Yin, Haoyu (57203527444); Li, Yue (57205420618); Du, Pengting (57288554600)","57288897200; 8285018300; 57219548407; 57203527444; 57205420618; 57288554600","Ship Detection in Large-scale SAR Images Based on Dense Spatial Attention and Multi-level Feature Fusion","2021","ACM International Conference Proceeding Series","","","","77","81","4","10.1145/3472634.3472654","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117385671&doi=10.1145%2f3472634.3472654&partnerID=40&md5=76f9e6cea77eac1bf43c1a26540fc80d","In recent years, ship detection in large-scale synthetic aperture radar (SAR) images attracts more attentions and becomes a research hotspot. But it still faces some challenges, such as strong interference of background noise and very small ship targets. This paper proposes a novel anchor-free detector, small target detector (STDet), based on dense spatial attention (DSA) and multi-level feature fusion (MFF). DSA is applied first to the backbone (Resnet50) in order to filter out the background noise and obtain more advanced semantic features. Then, a MFF network is used after the backbone to improve the detection accuracy, especially for small targets, by fusing the location and semantic information of different level feature maps. Finally, the refined features are fed to detection head to get the final results. Experiments are conducted on the public dataset LS-SSDD-v1.0. Experimental results prove our STDet has good performance for ship detection in large-scale SAR images. © 2021 ACM.","Feature extraction; Image fusion; Large dataset; Radar imaging; Semantics; Ships; Anchor-free; Dense spatial attention; Features fusions; Large-scales; Multi-level feature fusion; Multilevels; Ship detection; Small targets; Spatial attention; Synthetic aperture radar images; Synthetic aperture radar","anchor-free; dense spatial attention; multi-level feature fusion; SAR image; ship detection","Conference paper","Final","","Scopus","2-s2.0-85117385671"
"Fan R.; Hou B.; Liu J.; Yang J.; Hong Z.","Fan, Rongbo (57213263950); Hou, Bochuan (57214231196); Liu, Jinbao (57196034766); Yang, Jianhua (57188667674); Hong, Zenglin (23501802200)","57213263950; 57214231196; 57196034766; 57188667674; 23501802200","Registration of Multiresolution Remote Sensing Images Based on L2-Siamese Model","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9264687","237","248","11","10.1109/JSTARS.2020.3038922","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096867142&doi=10.1109%2fJSTARS.2020.3038922&partnerID=40&md5=305918d7827a6b6000c0f59d08f94361","The registration of multiresolution optical remote sensing images has been widely used in image fusion, change detection, and image stitching. However, traditional registration methods achieve poor accuracy in the registration of multiresolution remote sensing images. In this study, we propose a framework for generating deep features via a deep residual encoder (DRE) fused with shallow features for multiresolution remote sensing image registration. Through an L2 normalization Siamese network (L2-Siamese) based on the DRE, the multiscale loss function is used to learn the attribute characteristics and distance characteristics of two key points and obtain the trained feature extractor. Finally, the DRE is used to extract the deep features of the key points and their neighbors, which are concatenated with the shallow features into a fusion feature vector to complete the image registration. We performed comprehensive experiments on four sets of multiresolution optical remote sensing images and two sets of synthetic aperture radar images. The results demonstrate that the proposed registration model can achieve subpixel registration. The relative registration accuracy improved by 1.6%-7.5%, whereas the overall performance improved by 4.5%-14.1%.  © 2020 IEEE.","Image fusion; Image registration; Synthetic aperture radar; Change detection; Feature extractor; Fusion features; Image stitching; Multi-resolution remote sensing; Optical remote sensing; Registration methods; Sub-pixel registrations; computer vision; image processing; image resolution; model; remote sensing; satellite imagery; Remote sensing","Deep descriptors; L2-Siamese; multiresolution image registration; residual encoder; satellite remote sensing; Siamese network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85096867142"
"Yu N.; Ma A.; Zhong Y.; Gong X.","Yu, Ning (57457393900); Ma, Ailong (55972916000); Zhong, Yanfei (12039673900); Gong, Xiaodong (57937220400)","57457393900; 55972916000; 12039673900; 57937220400","HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2864","2867","3","10.1109/IGARSS46834.2022.9883519","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140359276&doi=10.1109%2fIGARSS46834.2022.9883519&partnerID=40&md5=a349a82425091fd96f9ab0317f8e4fc2","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation. © 2022 IEEE.","Aberrations; Color; Colorimetry; Generative adversarial networks; Geometrical optics; Image fusion; Optical remote sensing; Radar imaging; Textures; Color information; Expert knowledge; Image tranlation; Image translation; Imaging mechanism; Optical image; Optical remote sensing; Remote-sensing; SAR Images; Spatial informations; Synthetic aperture radar","Generative Adversarial Network; Image tranlation; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85140359276"
"Lin L.; Li J.; Shen H.; Zhao L.; Yuan Q.; Li X.","Lin, Liupeng (57188711703); Li, Jie (57214207213); Shen, Huanfeng (8359721100); Zhao, Lingli (55353622100); Yuan, Qiangqiang (36635300800); Li, Xinghua (55626987300)","57188711703; 57214207213; 8359721100; 55353622100; 36635300800; 55626987300","Low-Resolution Fully Polarimetric SAR and High-Resolution Single-Polarization SAR Image Fusion Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3121166","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122841440&doi=10.1109%2fTGRS.2021.3121166&partnerID=40&md5=9173be6907fbe0dfcf82dc4895b31915","The data fusion technology aims to aggregate the characteristics of different data and to obtain products with multiple data advantages. To solve the problem of reduced resolution of polarimetric synthetic aperture radar (PolSAR) images due to system limitations, we propose a fully PolSAR images and single-polarization synthetic aperture radar (SinSAR) images fusion network to generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the polarimetric information of the low-resolution PolSAR (LR-PolSAR) images and the spatial information of the high-resolution single-polarization SAR (HR-SinSAR) images, we propose a fusion framework for joint LR-PolSAR images and HR-SinSAR images and design a cross-attention mechanism to extract features from the joint input data. Besides, based on the physical imaging mechanism, we designed the PolSAR polarimetric loss functions for constrained network training. The experimental results confirm the superiority of the fusion network over traditional algorithms. The average peak signal-to-noise ratio (PSNR) is increased by more than 3.6 dB, and the average mean absolute error (MAE) is reduced to less than 0.07. Experiments on polarimetric decomposition and polarimetric signature show that it maintains polarimetric information well. © 1980-2012 IEEE.","Image fusion; Polarization; Radar imaging; Signal to noise ratio; Synthetic aperture radar; Attention mechanisms; Cross-attention mechanism; Fully polarimetric synthetic aperture radar; High resolution; Polarimetric loss; Polarimetric synthetic aperture radars; RCNN; Single polarization; Single-polarization  SinSAR; Synthetic aperture radar images; algorithm; design; experimental study; image resolution; PALSAR; polarization; satellite imagery; signal-to-noise ratio; synthetic aperture radar; training; Polarimeters","Cross-attention mechanism (CroAM); Fully polarimetric synthetic aperture radar (PolSAR); Fusion; Polarimetric loss; RCNN; Single-polarization SAR (SinSAR)","Article","Final","","Scopus","2-s2.0-85122841440"
"Cai X.; Chen L.; Xing J.; Xing X.; Luo R.; Tan S.; Wang J.","Cai, Xingmin (57222320859); Chen, Lifu (36547806500); Xing, Jin (56421262100); Xing, Xuemin (36571221100); Luo, Ru (57222314231); Tan, Siyu (57215424647); Wang, Jielan (57222330038)","57222320859; 36547806500; 56421262100; 36571221100; 57222314231; 57215424647; 57222330038","Automatic Extraction of Layover from InSAR Imagery Based on Multilayer Feature Fusion Attention Mechanism","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3105722","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122404477&doi=10.1109%2fLGRS.2021.3105722&partnerID=40&md5=4abda7b76edede19188a54d4c1352d8c","Layover is a kind of geometric distortion in radar systems with side-look imaging, especially in mountainous and dense urban areas. It causes phase distortion and alters target characteristics in the acquired images, which directly hinders the application of radar images. In this letter, the multilayer feature fusion attention mechanism (MF2AM) is proposed to extract layover from interferometric synthetic aperture radar (InSAR) imagery automatically. First, the SAR image, the corresponding coherence map, and interferometric phases are channel-fused to enhance semantic information of layover areas. Then, the fused image is fed into MF2AM to extract the essential features of layover. Finally, the detection results are produced via MF2AM. MF2AM consists of the encoder and the decoder. The encoder contains three parts: the resnet101, attention-based atrous spatial pyramid (AASP), and the semantic embedding branch (SEB). In the decoder, step decoding is used to better fuse high- and low-level features and improve the effect of edge segmentation. To verify the proposed method, the images of millimeter wave InSAR system are used for the experiment, and the performance is compared with DeepLabV3+ and Geospatial Contextual Attention Mechanism (GCAM). The results show that the MF2AM has achieved obvious performance advantages. The average pixel accuracy and average intersection over union (IOU) are 0.9601 and 0.9310, respectively, and the average test time is only 7.97 s.  © 2004-2012 IEEE.","Decoding; Deep learning; Image enhancement; Image fusion; Interferometry; Millimeter waves; Multilayers; Radar imaging; Semantics; Signal encoding; Attention mechanisms; Automatic extraction; Deep learning; Features fusions; Interferometric synthetic aperture radar; Interferometric synthetic aperture radars; Layover; Performance; Semantic segmentation; Synthetic Aperture Radar Imagery; accuracy assessment; extraction; image analysis; remote sensing; satellite data; synthetic aperture radar; Synthetic aperture radar","Attention mechanism; deep learning; interferometric synthetic aperture radar (InSAR); layover; semantic segmentation","Article","Final","","Scopus","2-s2.0-85122404477"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","Fusion and classification of multi-temporal SAR and optical imagery using convolutional neural network","2022","International Journal of Image and Data Fusion","13","2","","113","135","22","10.1080/19479832.2021.2019133","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121724919&doi=10.1080%2f19479832.2021.2019133&partnerID=40&md5=a411cfc65121ce6412df4f71115586a7","Remote sensing image classification is difficult, especially for agricultural crops with identical phenological growth periods. In this context, multi-sensor image fusion allows a comprehensive representation of biophysical and structural information. Recently, Convolutional Neural Network (CNN)-based methods are used for several applications due to their spatial-spectral interpretability. Hence, this study explores the potential of fused multi-temporal Sentinel 1 (S1) and Sentinel 2 (S2) images for Land Use/Land Cover classification over an agricultural area in India. For classification, Bayesian optimised 2D CNN-based DL and pixel-based SVM classifiers were used. For fusion, a CNN-based siamese network with Ratio-of-Laplacian pyramid method was used for the images acquired over the entire winter cropping period. This fusion strategy leads to better interpretability of results and also found that 2D CNN-based DL classifier performed well in terms of classification accuracy for both single-month (95.14% and 96.11%) as well as multi-temporal (99.87% and 99.91%) fusion in comparison to the SVM with classification accuracy for single-month (80.02% and 81.36%) and multi-temporal fusion (95.69% and 95.84%). Results indicate better performance by Vertical-Vertical polarised fused images than Vertical-Horizontal polarised fused images. Thus, implying the need to analyse classified images obtained by DL classifiers along with the classification accuracy. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Convolution; Convolutional neural networks; Crops; Image classification; Image fusion; Land use; Radar imaging; Remote sensing; Synthetic aperture radar; Bayesian optimization; Classification accuracy; Convolutional neural network; Fused images; Interpretability; Multi-temporal; Network-based; Support vector machine; Support vectors machine; artificial neural network; Bayesian analysis; image analysis; image classification; optical method; optimization; remote sensing; spatiotemporal analysis; support vector machine; synthetic aperture radar; Support vector machines","Bayesian Optimisation; Convolutional Neural Network (CNN); Fusion; Support Vector Machine (SVM)","Article","Final","","Scopus","2-s2.0-85121724919"
"Wang J.; Chen J.; Wang Q.","Wang, Jian (57221359948); Chen, Jiaqi (48160919100); Wang, Qingwei (57210324380)","57221359948; 48160919100; 57210324380","Fusion of POLSAR and Multispectral Satellite Images: A New Insight for Image Fusion","2020","Proceedings of the 2020 IEEE International Conference on Computational Electromagnetics, ICCEM 2020","","","9219457","83","84","1","10.1109/ICCEM47450.2020.9219457","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095584450&doi=10.1109%2fICCEM47450.2020.9219457&partnerID=40&md5=9ef6dede6b475205025cce468d6654d4","Polarized synthetic aperture radar (POLSAR) and multispectral images have great complementarity in information volume. That is to say, POLSAR have high resolution but poor color information. Multispectral images have rich spectral channel information, but the resolution is low. Therefore, this work has explored the fusion problem of the two data source. A framework was proposed, which merged polarized channel fusion data and multispectral images based on the Sentinel-2 and GF-3 data. The experimental results showed that the fusion results greatly integrated the characteristics of each channel of POLSAR and optical image. Therefore, our work has great application potential in improving the accuracy of feature recognition.  © 2020 IEEE.","Computational electromagnetics; Geometrical optics; Synthetic aperture radar; Color information; Data-source; Feature recognition; High resolution; Multispectral images; Multispectral satellite image; Optical image; Spectral channels; Image fusion","Feature recognition; Fusion; GF-3; Polarized synthetic aperture radar (POL-SAR); Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85095584450"
"Biswas R.; Rathore V.S.; Krishna A.P.; Singh G.; Das A.K.","Biswas, Raja (57772946000); Rathore, Virendra Singh (36869851200); Krishna, Akhouri Pramod (7103292391); Singh, Gulab (57201766355); Das, Anup Kumar (55450757300)","57772946000; 36869851200; 7103292391; 57201766355; 55450757300","Integration of C-band SAR and high-resolution optical images for delineating palaeo-channels in Nagaur and Barmer districts, western Rajasthan, India","2022","Environmental Monitoring and Assessment","194","8","589","","","","10.1007/s10661-022-10203-8","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134362713&doi=10.1007%2fs10661-022-10203-8&partnerID=40&md5=496447527ca85d3d9c358abdd3dc2f88","Identifying hitherto unknown palaeo-channels, especially in the arid regions of the Thar Desert, is crucial since these channels may form excellent aquifers, and are also associated with valuable ore deposits of many precious minerals. This study employed integrated C-band Synthetic Aperture Radar (SAR) of Sentinel-1A and high-resolution multispectral Sentinel-2A data of pre- and post-monsoon seasons (June and November) to delineate playas and palaeo-channels. This approach is the first of its kind for this area. The palaeo-channels were delineated through a detailed visual inspection of colour composite (CC) images of Sentinel-2A data, SAR backscatter (VH) images and fused SAR and optical images. Then, we studied the topographic profiles generated from the Shuttle Radar Topography Mission - Digital Elevation Model (SRTM-DEM) across the identified palaeo-channels, Normalized Difference Vegetation Index (NDVI) and Normalized Difference Water Index (NDWI) to further confirm the existence of a palaeo-channel’s course and playas. As a result, several playas and palaeo-channels in the area were successfully identified, some of which were previously unmapped and undetected. The results indicate that the post-monsoon datasets are more useful for the precise delineation of palaeo-channels due to the presence of relatively higher moisture along the palaeo-channels’ courses. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Environmental Monitoring; Groundwater; India; Radar; Barmer; India; Nagaur; Rajasthan; Thar Desert; Aquifers; Atmospheric thermodynamics; Geometrical optics; Groundwater resources; Hydrogeology; Image fusion; Radar imaging; Space-based radar; Surveying; Topography; ground water; C-bands; High resolution; High-resolution optical images; Normalized difference vegetation index; Normalized difference water index; Palaeo-channel; Post-monsoon; Radar resolution; Rajasthan; Thar Desert; aquifer; digital elevation model; moisture; multispectral image; NDVI; optical method; paleochannel; playa; Sentinel; Shuttle Radar Topography Mission; synthetic aperture radar; environmental monitoring; India; procedures; telecommunication; Synthetic aperture radar","Image fusion; NDVI; NDWI; Palaeo-channels; SAR","Article","Final","","Scopus","2-s2.0-85134362713"
"","","","Erratum regarding missing Declaration of Competing Interest statements in previously published articles (Remote Sensing Applications: Society and Environment (2018) 10 (120–127), (S2352938517301957), (10.1016/j.rsase.2018.03.009))","2021","Remote Sensing Applications: Society and Environment","21","","100452","","","","10.1016/j.rsase.2020.100452","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097448159&doi=10.1016%2fj.rsase.2020.100452&partnerID=40&md5=434781f4b0ff9978475ab4a2e679347a","The Declaration of Competing Interest statements were not included in the published version of the following articles that appeared in previous issues of Remote Sensing Applications: Society and Environment. The appropriate Declaration\Competing Interest statements, provided by authors, are included below. 1. “Impact of green roof on micro climate to reduce Urban Heat Island"" (Remote Sensing Applications: Society and Environment, 2018; 10C: 56 69) https://doi.org/10.1016/j.rsase.2018.01.003 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.2. “Semi-automatic classification method for mapping the rice-planted areas of Japan using multi-temporal Landsat images"" (Remote Sensing Applications: Society and Environment, 2018; 10C: 7–17) https://doi.org/10.1016/j.rsase.2018.02.001 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.3. “Estimating effective dust particle size from satellite observations"" (Remote Sensing Applications: Society and Environment, 2018; 11C: 186 197) https://doi.org/10.1016/j.rsase.2018.07.004 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.4. “Desertification risk analysis and assessment in Northern Nigeria"" (Remote Sensing Applications: Society and Environment, 2018; 11C: 70 82) https://doi.org/10.1016/j.rsase.2018.04.012 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.5. “Feature level image fusion of optical imagery and Synthetic Aperture Radar (SAR) for invasive alien plant species detection and mapping"" (Remote Sensing Applications: Society and Environment, 2018; 10C: 198–208) https://doi.org/10.1016/j.rsase.2018.04.007 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.6. “Flooded-area satellite monitoring within a Ramsar wetland Nature Reserve in Argentina"" (Remote Sensing Applications: Society and Environment, 2019; 15C: 230) https://doi.org/10.1016/j.rsase.2019.04.003 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.7. “Evaluation of subpixel unmixing algorithms in mapping the porphyry copper alterations using EO-1 Hyperion data, a case study from SE Iran"" (Remote Sensing Applications: Society and Environment, 2018; 10C: 120–127) https://doi.org/10.1016/j.rsase.2018.03.009 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.8. “Significant decline of forest fires in Nilgiri Biosphere Reserve, India"" (Remote Sensing Applications: Society and Environment, 2018; 11C: 172–185) https://doi.org/10.1016/j.rsase.2018.07.002 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.9. “Model inversion of BCVL based on DWD of MISR image"" (Remote Sensing Applications: Society and Environment, 2018; 13C: 401–407) https://doi.org/10.1016/j.rsase.2018.12.006 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.10. “Assessing topographic controls on vegetation characteristics in Chittagong Hill Tracts (CHT) from remotely sensed data"" (Remote Sensing Applications: Society and Environment, 2018; 11C: 198–208) https://doi.org/10.1016/j.rsase.2018.07.005 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.11. “Retreat of Pindari glacier and detection of snout position using remote sensing technology"" (Remote Sensing Applications: Society and Environment, 2018; 11C: 64–69) https://doi.org/10.1016/j.rsase.2018.04.011 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.12. “Changing land surface temperature of a rural Rarh tract river basin of India"" (Remote Sensing Applications: Society and Environment, 2018; 10C: 209–223) https://doi.org/10.1016/j.rsase.2018.04.005 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.13. “Urban expansion induced vulnerability assessment of East Kolkata Wetland using Fuzzy MCDM method"" (Remote Sensing Applications: Society and Environment, 2018; 13C: 191–203) https://doi.org/10.1016/j.rsase.2018.10.014 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.14. “Sea surface temperature and rainfall anomaly over the Bay of Bengal during the El NiÃ±o-Southern Oscillation and the extreme Indian Ocean Dipole events between 2002 and 2016"" (Remote Sensing Applications: Society and Environment, 2018; 12C: 10–22) https://doi.org/10.1016/j.rsase.2018.08.001 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement.15. “A time-domain NDVI anomaly service for intensively managed grassland agriculture"" (Remote Sensing Applications: Society and Environment, 2018; 11C: 282–290) https://doi.org/10.1016/j.rsase.2018.07.011 Declaration of competing interest: The authors were contacted after publication to request a Declaration of Interest statement. © 2020","","","Erratum","Final","","Scopus","2-s2.0-85097448159"
"Singh S.; Tiwari K.C.","Singh, Sanjay (57408319700); Tiwari, K.C. (57214612594)","57408319700; 57214612594","SAR and Optical Pixel Level Fusion Methods and Evaluations","2022","Journal of Spatial Science","","","","","","","10.1080/14498596.2022.2153754","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144080267&doi=10.1080%2f14498596.2022.2153754&partnerID=40&md5=d69bd1c013c9e8de0f9497d27023554d","In the past years, many vital improvements in image fusion have been recognised, particularly in the fusion of Synthetic Aperture Radar (SAR) and optical sensors. Many kinds of research have been published focusing on pixel-level fusion of SAR and optical images. This paper focuses on pixel-level SAR-optical fusion methods, along with performance assessment and applications. Five categories (component substitution, numerical method, model-based, multi-resolution, and hybrid methods) of fusion are presented. Subjective, objective, and comprehensive methods are surveyed to assess fusion. Experimental findings utilising Sentinel-1A and Sentinel-2A data corroborate the assessment. Finally, paper suggests that there is still space for investigation into SAR-optical fusion. © 2022 Mapping Sciences Institute, Australia and Surveying and Spatial Sciences Institute.","","comprehensive assessment; Image fusion; SAR-optical fusion; sentinel","Article","Article in press","","Scopus","2-s2.0-85144080267"
"Min F.; Liu P.","Min, Feng (35739113000); Liu, Peng (57750780500)","35739113000; 57750780500","Research on Ship Detection in the SAR Image Algorithm Based on Improved SSD","2021","ACM International Conference Proceeding Series","","","","205","211","6","10.1145/3488933.3489032","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125874407&doi=10.1145%2f3488933.3489032&partnerID=40&md5=824d30ad2c9b4070f6a282e70b775349","With the rapid development of deep learning, target detection technology has been widely used in various fields. At this stage, ship detection method in the SAR image based on the SSD algorithm, the accuracy was more improved than traditional methods, but the SSD algorithm takes a long time, and it was difficult to detect and locate the ship target in real time in the missile-borne SAR image. In addition, the SSD model has a large amount of calculation, and has a deviation in the detection of small targets. A ship detection method in the SAR image is proposed in this paper. The method improves the shallow network structure of the SSD algorithm, increases the network of different modules for feature fusion, reduces the loss of information in the training process, improves the feature extraction ability of small ships in the SAR image, and increases the receptive field of shallow features. Experiments was carried on SSDD and SAR-Ship-Dataset datasets. Compared with SSD algorithm, It was increased about 6% and 7% of mAP, which reduced the calculation amount of convolution, and the model size was reduced by about 7M and 10M.  © 2021 ACM.","Deep learning; Feature extraction; Image enhancement; Image fusion; Radar imaging; Ships; Detection methods; Detection technology; Features fusions; Image algorithms; Image-based; SAR Images; Ship detection; Ship targets; SSD algorithm; Targets detection; Synthetic aperture radar","Feature fusion; SAR image; Ship detection; SSD algorithm","Conference paper","Final","","Scopus","2-s2.0-85125874407"
"Kumar J.T.; Rani B.M.S.; Kumar M.S.; Raju M.V.; Das K.M.","Kumar, J. Thrisul (57226875990); Rani, B.M.S. (56370460500); Kumar, M. Satish (56461041100); Raju, M.V. (56350593200); Das, K. Maria (57204505363)","57226875990; 56370460500; 56461041100; 56350593200; 57204505363","Performance Evaluation of Change Detection in SAR Images Based on Hybrid Antlion DWT Fuzzy c-Means Clustering","2021","Cybernetics and Information Technologies","21","2","","45","57","12","10.2478/cait-2021-0018","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109613388&doi=10.2478%2fcait-2021-0018&partnerID=40&md5=a77cad005f6c60fdf29cb25dc47e7e56","In this paper, the main objective is to detect changes in the geographical area of Ottawa city in Canada due to floods. Two multi-temporal Synthetic Aperture Radar (SAR) images have been taken to evaluate the un-supervised change detection process. In this process, two ratio operators named as Log-Ratio and Mean-Ratio are used to generate a difference image. Performing image fusion based on DWT by selecting optimum filter coefficients by satisfying the wavelet filter coefficient properties through a novel image fusion technique is named as ADWT. GA, PSO, AntLion Optimization algorithms (ALO) and Hybridized AntLion Algorithm (HALO) have been adapted to perform the ADWT based image fusion. Segmentation has been performed based on fuzzy c-Means clustering to detect changed and unchanged pixels. Finally, the performance of the proposed method will be analysed by comparing the segmented image with the ground truth image in terms of sensitivity, accuracy, specificity, precision, F1-score. © 2021 J. Thrisul Kumar et al., published by Sciendo 2021.","","ADWT; ALO Algorithm; GA Algorithm; HALO Algorithm; PSO Algorithm","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85109613388"
"Fang J.; Ma X.; Wang J.; Qin K.; Hu S.; Zhao Y.","Fang, Jing (57191247277); Ma, Xiaole (57193220596); Wang, Jingjing (57214140268); Qin, Kai (35220430700); Hu, Shaohai (7404286949); Zhao, Yuefeng (14033580400)","57191247277; 57193220596; 57214140268; 35220430700; 7404286949; 14033580400","A noisy sar image fusion method based on nlm and gan","2021","Entropy","23","4","410","","","","10.3390/e23040410","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103906348&doi=10.3390%2fe23040410&partnerID=40&md5=0585a877bf873a8416e1291f2193192f","The unavoidable noise often present in synthetic aperture radar (SAR) images, such as speckle noise, negatively impacts the subsequent processing of SAR images. Further, it is not easy to find an appropriate application for SAR images, given that the human visual system is sensitive to color and SAR images are gray. As a result, a noisy SAR image fusion method based on nonlocal matching and generative adversarial networks is presented in this paper. A nonlocal matching method is applied to processing source images into similar block groups in the pre‐processing step. Then, adversarial networks are employed to generate a final noise‐free fused SAR image block, where the generator aims to generate a noise‐free SAR image block with color information, and the discriminator tries to increase the spatial resolution of the generated image block. This step ensures that the fused image block contains high resolution and color information at the same time. Finally, a fused image can be obtained by aggregating all the image blocks. By extensive comparative experiments on the SEN1–2 datasets and source images, it can be found that the proposed method not only has better fusion results but is also robust to image noise, indicating the superiority of the proposed noisy SAR image fusion method over the state‐of‐the‐art methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Generative adversarial networks; Image fusion; Nonlocal matching","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103906348"
"Wang Y.; Yu X.; Zhang Y.; Pei J.; Huo W.; Huang Y.; Yang J.","Wang, Yigang (57937844400); Yu, Xindi (57937501300); Zhang, Yin (55975581400); Pei, Jifang (55787739300); Huo, Weibo (56405211000); Huang, Yulin (23014806800); Yang, Jianyu (9239230100)","57937844400; 57937501300; 55975581400; 55787739300; 56405211000; 23014806800; 9239230100","An Adaptive SAR and Optical Images Registration Approach Based on SOI-SIFT","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2582","2585","3","10.1109/IGARSS46834.2022.9884231","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140359143&doi=10.1109%2fIGARSS46834.2022.9884231&partnerID=40&md5=537014f8a89c84d5cabad74955c86ec8","SAR and optical images registration is a key step for remote sensing image processing, match navigation and information fusion. Although there are many methods for SAR images registration, their performance will decrease between SAR and optical images. Moreover, these algorithms suffer from lack of matching pairs of the feature points and uneven distribution between SAR and optical images. Therefore, they cannot accurately achieve the registration between optical and SAR images. To solve the above deficiencies, we propose an efficient image registration approach based on SAR and optical image-scale invariant feature transform (SOI-SIFT). Firstly, a linear edge enhancement based on gray feature and histogram equalization is introduced. In this stage, we enhance the edge features of the image so that the number of image feature points can be greatly increased. Then, for feature points purification, we use fast sample consensus algorithm to filter duplicate and wrong matching feature points. SOI-SIFT can be more adapted to the heterogeneous image matching. Experimental results have shown the superiorities of the proposed method. © 2022 IEEE.","Geometrical optics; Image enhancement; Image fusion; Optical data processing; Optical remote sensing; Radar imaging; Synthetic aperture radar; Edge enhancements; Image scale; Images registration; Invariant feature transforms; Matchings; Optical image; Performance; Remote sensing image processing; SAR Images; Scale invariant features; Image registration","","Conference paper","Final","","Scopus","2-s2.0-85140359143"
"Yang X.; Zhou Y.; Chen T.; Shi J.; Cui G.","Yang, Xiaqing (55602675800); Zhou, Yuanyuan (57211093661); Chen, Tingjun (57468699500); Shi, Jun (57104066400); Cui, Guolong (24469695800)","55602675800; 57211093661; 57468699500; 57104066400; 24469695800","Segmentation of low scattering region in SAR images using multi-module fusion network","2022","International Journal of Remote Sensing","43","14","","5439","5451","12","10.1080/01431161.2022.2135411","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140852386&doi=10.1080%2f01431161.2022.2135411&partnerID=40&md5=5d83c25115a9c1580a6ab145c5ebb36e","The proposed multi-module fusion network (MMFNet) is designed for the segmentation of low scattering regions such as roads, waters, and shadows in synthetic aperture radar (SAR) images in this paper. It is primarily comprised of three modules, i.e. high-resolution backbone network module, spatial pyramid pooling convolution (SPPC) module, and channel attention module, and trained with weighted cross-entropy loss. The high-resolution backbone network works to retain high resolution of feature maps and reduce spatial accuracy loss, which contributes to the extraction of edge information. SPPC module performs multi-scale feature fusion, extracts target areas with different sizes and improves network accuracy. Channel attention module intensifies network expression of category information, thus further improves network performance. Our experimental analysis using real SAR data shows that MMFNet achieves good low scattering region segmentation, with mean IoU (MIoU) reaching up to (Formula presented.). © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Convolution; Convolutional neural networks; Image fusion; Image segmentation; Radar imaging; Back-bone network; Convolutional neural network; Cross entropy; High resolution; Low-scattering regions; Multimodule; Spatial pyramids; Synthetic aperture radar; Synthetic aperture radar images; artificial neural network; experimental study; radar imagery; segmentation; synthetic aperture radar; Synthetic aperture radar","convolutional neural network (CNN); low scattering region; Synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85140852386"
"Chen Q.; Liu W.; Sun G.; Li D.; Xing M.","Chen, Quan (57208129832); Liu, Wenkang (57190407532); Sun, Guangcai (26424042000); Li, Dongxu (57918979100); Xing, Mengdao (7005922869)","57208129832; 57190407532; 26424042000; 57918979100; 7005922869","An Accelerated Back-Projection Algorithm Based on Large Swath for Geosynchronous-Earch-Orbit SAR Imaging; [基于超大幅宽的高轨SAR加速BP成像方法]","2022","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","44","9","","3136","3143","7","10.11999/JEIT210560","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139383259&doi=10.11999%2fJEIT210560&partnerID=40&md5=b69226f371a8f2fc72bdb6eb8bf75a71","In the Geosynchronous-Earth-Orbit (GEO) SAR imaging, the extremely large swath width causes the imaging plane to no longer satisfy the flat plane approximation, which makes the accelerated BP algorithms based on the flat plane grid invalid. In this paper, an accelerated BP algorithm based on ground grid is proposed to process accurately and efficiently the GEO SAR signals. The imaging grids are arranged on the ground surface to correct the complex space variance of the signal caused by orbit and ground surface curvature. To solve the spectrum aliasing of sub-aperture images, a two-step spectrum compressing method is proposed to achieve the sub-aperture spectrum de-aliasing before fusion. And a multi-stage sub-aperture image fusion method is adopted to improve the imaging efficiency. Finally, simulation results are shown to verify the accuracy and efficiency of the proposed focusing approaches. © 2022 Science Press. All rights reserved.","Approximation algorithms; Efficiency; Image enhancement; Image fusion; Radar imaging; Synthetic aperture radar; Accelerated back projection  algorithm; Backprojection algorithms; Geosynchronoi-earch-orbit  SAR; Ground surface curvature; Ground surfaces; Multi-stage image fusion; Multi-stages; Spectrum compression; Subaperture; Surface curvatures; Orbits","Accelerated Back Projection (BP) algorithm; Geosynchronous-Earch-Orbit (GEO) SAR; Ground surface curvature; Multi-stage image fusion; Spectrum compression","Article","Final","","Scopus","2-s2.0-85139383259"
"Aslam K.; Zahid Khalil R.M.; Haq S.U.; Ahmed S.","Aslam, Khusharah (57214798201); Zahid Khalil, Rao. M. (57213189865); Haq, Saad Ul (57222247113); Ahmed, Salman (57225877917)","57214798201; 57213189865; 57222247113; 57225877917","Comparative Analysis between Optical and Fused Image with SAR","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324255","1528","1531","3","10.1109/IGARSS39084.2020.9324255","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102015451&doi=10.1109%2fIGARSS39084.2020.9324255&partnerID=40&md5=6b29a986a18cae574adca1f350a94a22","Image fusion is a technique that integrates complementary information from multiple remote sensing images such that the fused image is more suitable for processing task and information extraction. Passive sensors are capable of sensing the reflected electromagnetic energy in the visible and infrared region while active sensors provide additional information using microwave region. This broad spectrum provides more information of earth surface as compared to optical data alone. This study compares the land cover classification results of optical imagery (Landsat-8) and fused imagery (Landsat-8 and Sentinel-1 VV polarized data). The image fusion was then performed using wavelet transformation technique. The data were classified into four classes namely water bodies, built-up area, vegetation cover, and barren land. Google Earth and Landsat imagery were used as a reference image for accuracy assessment. The fused image showed higher accuracy than optical image i.e. Kappa coefficient increased from 0.78 to 0.9 and overall accuracy increased from 89.4% to 92.7%. This study indicates that multi-source information i.e., image fusion can significantly improve the interpretation and accuracy of classification. © 2020 IEEE.","Classification (of information); Electromagnetic waves; Geology; Geometrical optics; Image enhancement; Image fusion; Microwave sensors; Remote sensing; Accuracy assessment; Accuracy of classifications; Comparative analysis; Land cover classification; Multi-source informations; Overall accuracies; Remote sensing images; Wavelet transformations; Radar imaging","Fusion; Land cover classification; Synthetic Aperture Radar","Conference paper","Final","","Scopus","2-s2.0-85102015451"
"Ochungo P.; Veldtman R.; Abdel-Rahman E.M.; Raina S.; Muli E.; Landmann T.","Ochungo, Pamela (57498625600); Veldtman, Ruan (23013673500); Abdel-Rahman, Elfatih M. (57193979141); Raina, Suresh (16022820900); Muli, Eliud (12752394800); Landmann, Tobias (23492618800)","57498625600; 23013673500; 57193979141; 16022820900; 12752394800; 23492618800","Multi-sensor mapping of honey bee habitats and fragmentation in agro-ecological landscapes in Eastern Kenya","2021","Geocarto International","36","8","","839","860","21","10.1080/10106049.2019.1629645","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067661673&doi=10.1080%2f10106049.2019.1629645&partnerID=40&md5=2f1b3026cf92b20491e8020a5bf410b1","Extensive land transformation leads to habitat loss, which directly affects and fragments species habitats. Such land transformations can adversely affect fodder availability for bees and thus colony strength with consequences for rural communities that use bee keeping as a livelihood option. Quantification of the landscape structure is thus critical if the linkages between the landscape and honey bee colony health are to be well understood. In this study, a random forest algorithm was used on dual-polarized multi-season Sentinel-1A (S1) synthetic aperture radar (SAR) and single season Sentinel-2A (S2) optical imagery to map honey bee habitats and their degree of fragmentation in a heterogeneous agro-ecological landscape in eastern Kenya. The dry season S2 optical imagery was fused with the S1 data and class-wise mapping accuracies (with and without radar) were compared. Relevant fragmentation indices representing patch sizes, isolation and configuration were thereafter generated using the fused imagery. The fused imagery recorded an overall accuracy of 86% with a kappa of 0.83 versus the SAR imagery only, which had an overall accuracy of 76% with a kappa of 0.68. However, the S1 imagery had slightly higher user’s and producer’s accuracies for under-represented but important honey bee habitat classes, that is, natural grasslands and hedges. The variable importance analysis using the fused imagery showed that the short-wave infrared and the red-edge waveband regions were highly relevant for the classification model. Our mapping approach showed that fusing data generated from S1 and S2 with improved spectral resolution, could be effectively used for the spatially explicit mapping of honey bee habitats and their degree of fragmentation in semi-arid African agro-ecological landscapes. © 2019 Informa UK Limited, trading as Taylor & Francis Group.","Kenya; Apis mellifera; agroecology; algorithm; habitat fragmentation; honeybee; landscape structure; machine learning; mapping method; satellite imagery; Sentinel","honey bees; image fusion; Kenya; Landscape structure; random forest; Sentinels 1 and 2","Article","Final","","Scopus","2-s2.0-85067661673"
"Quan Y.; Tong Y.; Feng W.; Dauphin G.; Huang W.; Xing M.","Quan, Yinghui (35181982300); Tong, Yingping (57220011021); Feng, Wei (57089587500); Dauphin, Gabriel (8283331300); Huang, Wenjiang (9040267000); Xing, Mengdao (7005922869)","35181982300; 57220011021; 57089587500; 8283331300; 9040267000; 7005922869","A novel image fusion method of multi-spectral and sar images for land cover classification","2020","Remote Sensing","12","22","3801","1","25","24","10.3390/rs12223801","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096495742&doi=10.3390%2frs12223801&partnerID=40&md5=eb9553b2876ca8c695ecba924939a302","The fusion of multi-spectral and synthetic aperture radar (SAR) images could retain the advantages of each data, hence benefiting accurate land cover classification. However, some current image fusion methods face the challenge of producing unexpected noise. To overcome the aforementioned problem, this paper proposes a novel fusion method based on weighted median filter and Gram–Schmidt transform. In the proposed method, Sentinel-2A images and GF-3 images are respectively subjected to different preprocessing processes. Since weighted median filter does not strongly blur edges while filtering, it is applied to Sentinel-2A images for reducing noise. The processed Sentinel images are then transformed by Gram–Schmidt with GF-3 images. Two popular methods, principal component analysis method and traditional Gram–Schmidt transform, are used as the comparison methods in the experiment. In addition, random forest, a powerful ensemble model, is adopted as the land cover classifier due to its fast training speed and excellent classification performance. The overall accuracy, Kappa coefficient and classification map of the random forest are used as the evaluation criteria of the fusion method. Experiments conducted on five datasets demonstrate the superiority of the proposed method in both objective metrics and visual impressions. The experimental results indicate that the proposed method can improve the overall accuracy by up to 5% compared to using the original Sentinel-2A and has the potential to improve the satellite-based land cover classification accuracy. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Decision trees; Image classification; Median filters; Radar imaging; Random forests; Synthetic aperture radar; Classification performance; Comparison methods; Evaluation criteria; Image fusion methods; Land cover classification; Principal component analysis method; Synthetic aperture radar (SAR) images; Weighted median filter; Image fusion","Image fusion; Land cover classification; Multi-spectral; Random forest; Remote sensing","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096495742"
"Feng B.; Tang W.; Feng D.","Feng, Bo (57194436343); Tang, Wei (56108762000); Feng, Daoyan (57216978346)","57194436343; 56108762000; 57216978346","Target recognition of SAR images via hierarchical fusion of complementary features","2020","Optik","217","","164695","","","","10.1016/j.ijleo.2020.164695","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085621557&doi=10.1016%2fj.ijleo.2020.164695&partnerID=40&md5=8f84344d799e992385e07c0202e1ae91","A hierarchical fusion of complementary features is proposed with application to target recognition of synthetic aperture radar (SAR) images. Three features, i.e., principle component analysis (PCA) features, attributed scattering centers (ASCs), and target outline, are used. Sparse representation-based classification (SRC) is adopted to classify the PCA features because of its good effectiveness and robustness. A one-to-one matching algorithm is designed for the ASCs and a partially matching strategy for the target outlines are designed. The test sample is first classified using the PCA features. If the decision is judged to be unreliable, the ASC matching is performed afterwards. Similarly, the outline matching is performed when the ASC matching cannot make a reliable decision on the target label. In this way, the advantages of the complementary features can be combined in a unified framework so the overall target recognition performance can be enhanced. Experiments are implemented based on the moving and stationary target acquisition and recognition (MSTAR) dataset. The results show that the proposed method achieves a notably high recognition rate of 98.34 % under the standard operating condition (SOC) and superior robustness under the extended operating conditions (EOCs) in comparison with other methods. © 2020 Elsevier GmbH","Image fusion; Principal component analysis; Radar imaging; Synthetic aperture radar; Complementary features; Extended operating conditions; Hierarchical fusions; Principle component analysis; Scattering centers; Sparse representation based classifications; Standard operating conditions; Synthetic aperture radar (SAR) images; Radar target recognition","Attributed scattering center (ASC); Hierarchical decision fusion; Principal component analysis (PCA); Synthetic aperture radar (SAR); Target outline; Target recognition","Article","Final","","Scopus","2-s2.0-85085621557"
"Chen X.-B.; Ni Z.-M.; Chen X.-Z.; Hou Q.-W.; Yu Q.","Chen, Xiang-Bing (57826869500); Ni, Zi-Ming (57219356056); Chen, Xian-Zhong (8859426300); Hou, Qing-Wen (23569081200); Yu, Qing (57204552669)","57826869500; 57219356056; 8859426300; 23569081200; 57204552669","Dual-focus imaging algorithm of synthetic aperture radar for burning blast furnace burden surface; [高炉燃烧料面合成孔径雷达双聚焦成像算法]","2020","Kongzhi Lilun Yu Yingyong/Control Theory and Applications","37","8","","1826","1836","10","10.7641/CTA.2020.90505","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092423644&doi=10.7641%2fCTA.2020.90505&partnerID=40&md5=e06449c75f10a62545730d2c9a5c3114","The position and shape of the burden surface in ironmaking are important indicators for the control of the smelting process. Microwave imaging device based on the principle of synthetic aperture radar (SAR) effectively extracts radial surface information in the hot state. In the intricate center of burden surface, the echo is mutable and even no echo with the shadowing effect. A dual-focus measurement imaging system for the center area is constructed. Two radars are installed symmetrically at the selected position. The antenna swings with a motor moving in constant speed, and a twodimensional coordinate matrix is obtained by radar, then the matrix is transformed into a rectangular image with uniform pixels by resampling interpolation method. The scanning overlap area is determined according to the actual parameters and a dual-focus SAR algorithm (DfSAR) are designed to stitch images from two radars. Compared with mean fusion, the mutual information is improved by 10.43% and the image definition is improved by 23.91%. Finally, the constant false alarm rate (CFAR) algorithm is used to effectively filter the interference of dust in the blast furnace on the image. © 2020, Editorial Department of Control Theory & Applications South China University of Technology. All right reserved.","Antennas; Blast furnaces; Image enhancement; Imaging systems; Radar imaging; Radar signal processing; Blast furnace burdens; Constant false alarm rate algorithms; Coordinate matrices; Interpolation method; Microwave imaging; Mutual informations; Rectangular image; Surface information; Synthetic aperture radar","Blast furnace-burden; Constant false alarm rate; Dual-focus; Image fusion; Resampling interpolation","Article","Final","","Scopus","2-s2.0-85092423644"
"Chen J.; Qiu X.; Ding C.; Wu Y.","Chen, Jiankun (57216591883); Qiu, Xiaolan (18435166800); Ding, Chibiao (7202622015); Wu, Yirong (8403430500)","57216591883; 18435166800; 7202622015; 8403430500","CVCMFF Net: Complex-Valued Convolutional and Multifeature Fusion Network for Building Semantic Segmentation of InSAR Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3068124","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103910952&doi=10.1109%2fTGRS.2021.3068124&partnerID=40&md5=4bddaf78bc5f56a867aff8bee03590f3","Building segmentation of synthetic aperture radar (SAR) images is a challenging task that has not been solved well. High-resolution interferometric SAR (InSAR) images can provide delicate textures and interferometric phase images useful for building segmentation. However, current semantic segmentation networks in computer vision cannot be directly applied in InSAR building segmentation tasks to get good results because of the InSAR images' particularity. In this article, we present a novel complex-valued convolutional and multifeature fusion network (CVCMFF Net) specifically for building semantic segmentation of InSAR images. This CVCMFF Net not only learns from the complex-valued SAR images but also considers multiscale and multichannel feature fusion. It can effectively segment the layover, shadow, and background on both the simulated InSAR building images and the real airborne InSAR images. The segmentation performance of CVCMFF Net is significantly improved compared with those of other state-of-the-art networks. By feature visualization, the feature extraction rule and feature fusion mechanism of the network are explored. We hope that the proposed network can be beneficial to InSAR phase filtering, phase unwrapping, and information extraction in urban areas. © 1980-2012 IEEE.","Buildings; Complex networks; Convolution; Image fusion; Image segmentation; Information filtering; Interferometry; Radar imaging; Semantic Web; Semantics; Textures; Interferometric phase; Interferometric SAR; Multi-feature fusion; Phase unwrapping; Segmentation performance; Semantic segmentation; State of the art; Synthetic aperture radar (SAR) images; building construction; computer vision; information management; segmentation; synthetic aperture radar; urban area; Synthetic aperture radar","Building semantic segmentation; Complex-valued convolutional and multifeature fusion network (CVCMFF Net); Feature visualization; Interferometric SAR (InSAR)","Article","Final","","Scopus","2-s2.0-85103910952"
"Wang J.; Qin C.; Yang K.; Ren P.","Wang, Jian (57200017817); Qin, Chunxia (57211428985); Yang, Ke (57214786681); Ren, Ping (57211430912)","57200017817; 57211428985; 57214786681; 57211430912","A SAR Target Recognition Algorithm Based on Guided Filter Reconstruction and Denoising Sparse Autoencoder; [基于导向重构与降噪稀疏自编码器的合成孔径雷达目标识别]","2020","Binggong Xuebao/Acta Armamentarii","41","9","","1861","1870","9","10.3969/j.issn.1000-1093.2020.09.018","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096555219&doi=10.3969%2fj.issn.1000-1093.2020.09.018&partnerID=40&md5=72a31811edc9afffcc4ef580051f5c7b","The existing synthetic aperture radar(SAR) target recognition algorithms have the poor generalization ability and high complexity. For the problems above, an algorithm based on the guided filter reconstruction and denoising sparse autoencoder is proposed. The guided filter reconstruction algorithm with two-scale image fusion preprocessing of SAR image is used to generate an one-dimensional image vector and normalizate it in order to reduce the dimension of output feature of the image and increase the speed of preprocessing. The algorithm would extract and recognize the low-dimensional features of images by reducing the hidden layer neurons of the denoising autoencoder, which can effectively reduce the complexity of the algorithm. The Softmax classifier is used for classifying. The experimental results show that the SAR target recognition algorithm based on the guided filter reconstruction and denoising sparse autoencoder can not only improve the target recognition performance and generalization ability, but also reduce the number of hidden layer neurons in the autoencoder and the computational complexity, and the network structure has also been improved and optimized as well. © 2020, Editorial Board of Acta Armamentarii. All right reserved.","Complex networks; Computational complexity; Image fusion; Image processing; Learning systems; One dimensional; Radar imaging; Synthetic aperture radar; Generalization ability; Hidden layer neurons; Low dimensional; Network structures; One-dimensional image; Reconstruction algorithms; Target recognition; Target recognition algorithms; Radar target recognition","Denoising sparse autoencoder; Guided filter reconstruction; Regularized Softmax; Synthetic aperture radar; Target recognition","Article","Final","","Scopus","2-s2.0-85096555219"
"Ye Y.; Liu W.; Zhou L.; Peng T.; Xu Q.","Ye, Yuanxin (36683803000); Liu, Wanchun (57946884800); Zhou, Liang (57199016866); Peng, Tao (57207212502); Xu, Qizhi (57393652700)","36683803000; 57946884800; 57199016866; 57207212502; 57393652700","An Unsupervised SAR and Optical Image Fusion Network Based on Structure-Texture Decomposition","2022","IEEE Geoscience and Remote Sensing Letters","19","","4028305","","","","10.1109/LGRS.2022.3219341","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141564069&doi=10.1109%2fLGRS.2022.3219341&partnerID=40&md5=dece985892208c5ed2c18540a530a57d","Although the unique advantages of optical and synthetic aperture radar (SAR) images promote their fusion, the integration of complementary features from the two types of data and their effective fusion remains a vital problem. To address that, a novel framework is designed based on the observation that the structure of SAR images and the texture of optical images look complementary. The proposed framework, named SOSTF, is an unsupervised end-to-end fusion network that aims to integrate structural features from SAR images and detailed texture features from optical images into the fusion results. The proposed method adopts the nest connect-based architecture, including an encoder network, a fusion part, and a decoder network. To maintain the structure and texture information of input images, the encoder architecture is utilized to extract multiscale features from images. Then, we use the densely connected convolutional network (DenseNet) to perform feature fusion. Finally, we reconstruct the fusion image using a decoder network. In the training stage, we introduce a structure-texture decomposition model. In addition, a novel texture-preserving and structure-enhancing loss function are designed to train the DenseNet to enhance the structure and texture features of fusion results. Qualitative and quantitative comparisons of the fusion results with nine advanced methods demonstrate that the proposed method can fuse the complementary features of SAR and optical images more effectively.  © 2004-2012 IEEE.","Adaptive optics; Decoding; Image fusion; Image texture; Radar imaging; Signal encoding; Synthetic aperture radar; Textures; Complementary features; Optical image; Optical imaging; Optical reflection; Radar polarimetry; SOSTF; Structure/texture; Synthetic aperture radar images; Texture features; Unsupervised; artificial neural network; decomposition analysis; radar imagery; synthetic aperture radar; Geometrical optics","Image fusion; SOSTF; synthetic aperture radar (SAR) and optical images; unsupervised","Article","Final","","Scopus","2-s2.0-85141564069"
"Zhu D.; Wang X.; Cheng Y.; Li G.","Zhu, Dong (57001841800); Wang, Xueqian (57829747800); Cheng, Yayun (57188725437); Li, Gang (55547117794)","57001841800; 57829747800; 57188725437; 55547117794","Vessel target detection in spaceborne–airborne collaborative sar images via proposal and polarization fusion","2021","Remote Sensing","13","19","3957","","","","10.3390/rs13193957","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116439796&doi=10.3390%2frs13193957&partnerID=40&md5=ef513455d11c4f74eed62e8c2187bd09","This paper focuses on vessel detection through the fusion of synthetic aperture radar (SAR) images acquired from spaceborne–airborne collaborative observations. The vessel target detection task becomes more challenging when it features inshore interferences and structured and shaped targets. We propose a new method, based on target proposal and polarization information exploitation (TPPIE), to fuse the spaceborne–airborne collaborative SAR images for accurate vessel detection. First, a new triple-state proposal matrix (TSPM) is generated by combining the normed gradient-based target proposal and the edge-based morphological candidate map. The TSPM can be used to extract the potential target regions, as well as filtering out the sea clutter and inshore interference regions. Second, we present a new polarization feature, named the absolute polarization ratio (APR), to exploit the intensity information of dual-polarization SAR images. In the APR map, the vessel target regions are further enhanced. Third, the final fused image with enhanced targets and suppressed backgrounds (i.e., improved target-to-clutter ratio; TCR) is attained by taking the Hadamard product of the intersected TSPM from multiple sources and the composite map exploiting the APR feature. Experimental analyses using Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR imagery indicate that the proposed TPPIE fusion method can yield higher TCRs for fused images and better detection performance for vessel targets, compared to commonly used image fusion approaches. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aircraft detection; Antennas; Clutter (information theory); Image enhancement; Polarization; Radar imaging; Satellite imagery; Space-based radar; Synthetic aperture radar; Tracking radar; Unmanned aerial vehicles (UAV); Collaborative observations; matrix; Polarization fusion; Polarization ratios; Space-borne; Spaceborne–airborne collaborative observation; Synthetic aperture radar images; Target proposal; Targets detection; Vessel target detection; Image fusion","Image fusion; Polarization fusion; Spaceborne–airborne collaborative observations; Synthetic aperture radar (SAR) image; Target proposal; Vessel target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116439796"
"Kulkarni S.C.; Rege P.P.","Kulkarni, Samadhan C. (57204893706); Rege, Priti P. (6701858789)","57204893706; 6701858789","Application of Taguchi method to improve land use land cover classification using PCA-DWT-based SAR-multispectral image fusion","2021","Journal of Applied Remote Sensing","15","1","014509","","","","10.1117/1.JRS.15.014509","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103662238&doi=10.1117%2f1.JRS.15.014509&partnerID=40&md5=79649ae2d1087aff8efd3a9755abb7a0","The fusion of multispectral and synthetic aperture radar (SAR) images is of vital importance in many remote sensing applications. Spectral distortion and trade-off between the spatial and spectral quality of the fused image are significant issues in SAR-multispectral image fusion. Our study attempts to improve the performance of SAR-multispectral image fusion concerning these two issues. The primary objective of our study is to optimize the performance of hybrid fusion approach based on principal component analysis and discrete wavelet transform (PCA-DWT) using Taguchi orthogonal array. The fused data are evaluated using visual analysis and standard quality metrics. The results are compared with recent hybrid fusion approaches applied to the SAR-multispectral image fusion. The utility of the fused data is evaluated based on the remote sensing application, namely, land use land cover classification. The classification results are compared to a standard thematic map available on the Bhuvan geoportal to check classification accuracy. A comparative analysis with recent hybrid approaches conclusively demonstrates that the proposed optimization in the PCA-DWT based fusion is superior to conventional hybrid methods. © 2021 Society of Photo-Optical Instrumentation Engineers (SPIE).","Discrete wavelet transforms; Economic and social effects; Image classification; Image enhancement; Image fusion; Land use; Maps; Quality control; Remote sensing; Signal reconstruction; Synthetic aperture radar; Taguchi methods; Classification accuracy; Classification results; Comparative analysis; Land use/ land covers; Multi-spectral image fusions; Remote sensing applications; Synthetic aperture radar (SAR) images; Taguchi orthogonal arrays; Radar imaging","land use land cover classification; multispectral imagery; principal component analysis; synthetic aperture radar imagery; Taguchi orthogonal array; wavelet transform","Article","Final","","Scopus","2-s2.0-85103662238"
"Wu W.; Shao Z.; Huang X.; Teng J.; Guo S.; Li D.","Wu, Wenfu (57208187129); Shao, Zhenfeng (7202244409); Huang, Xiao (57201292422); Teng, Jiahua (57226406366); Guo, Songjing (57212678543); Li, Deren (57212271839)","57208187129; 7202244409; 57201292422; 57226406366; 57212678543; 57212271839","Quantifying the sensitivity of SAR and optical images three-level fusions in land cover classification to registration errors","2022","International Journal of Applied Earth Observation and Geoinformation","112","","102868","","","","10.1016/j.jag.2022.102868","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133268670&doi=10.1016%2fj.jag.2022.102868&partnerID=40&md5=8a78d15dc1f56d7373aa1c1bfc057dd9","Synthetic aperture radar (SAR) and optical (SAR-optical) data are two important remote sensing data sources. Fusing them is expected to lead to complementary information that benefits land cover classification (LCC). The fusion of SAR-optical images occurs in three levels, i.e., pixel-level, feature-level, and decision-level. However, accurately registering SAR-optical images is still a challenge, and geometric registration errors will bring great uncertainty to LCC based on SAR-optical images fusion. Therefore, this study quantitatively evaluates the sensitivity of SAR-optical images three-level fusions to registration errors through simulation experiments. The results show that: (1) geometric registration errors affect LCC based on SAR-optical images fusion at three levels in a significant manner. Among them, feature-level fusion is the least sensitive to registration errors. (2) the fusion taken optical image as reference image presents better tolerance to registration errors than that based on SAR image. (3) the response of SAR-optical images fusion to registration errors in LCC of heterogeneous regions is greater than that in homogeneous regions. (4) during the LCC, the fusion of SAR-optical images with comparable spatial resolution has a higher tolerance of registration errors. (5) fusing SAR-optical data does not always guarantee the improvement of LCC compared to using optical data alone, depending on fusion levels, fusion methods and classifiers. We believe these findings can greatly benefit the research community in developing new SAR-optical images fusion methods to improve LCC in the future. © 2022","error analysis; image classification; land cover; optical method; synthetic aperture radar","Land cover classification (LCC); Registration errors; SAR and optical (SAR-optical) images fusion; Sensitivity quantification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133268670"
"Pang L.; Zhang F.; Li L.; Huang Q.; Jiao Y.; Shao Y.","Pang, Lei (55431203400); Zhang, Fengli (55702260900); Li, Lu (55730902200); Huang, Qiqi (57224187510); Jiao, Yanan (57390732400); Shao, Yun (7201598892)","55431203400; 55702260900; 55730902200; 57224187510; 57390732400; 7201598892","Assessing Buildings Damage from Multi-Temporal Sar Images Fusion using Semantic Change Detection","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","6292","6295","3","10.1109/IGARSS46834.2022.9884915","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141895788&doi=10.1109%2fIGARSS46834.2022.9884915&partnerID=40&md5=e43510620c9b5d5763f83eac5c03a47d","A prompt and accurate assessment of buildings' damage is critical for disaster management and emergency response. With the development of high-resolution synthetic aperture radar (SAR) and deep-learning methods, more efficient damage assessment techniques based on building-units are possible. This paper proposes a new building damage assessment method using high-resolution SAR images based on semantic change detection. It utilizes a Siamese-based module for damage change detection together with an attention mechanism-based module for semantic segmentation of the damage maps. To evaluate the proposed model, a new damage assessment dataset is constructed from the SAR imagery originated from the battle of Aleppo, Syria, for model training and testing. The experiments performed on this dataset show an overall accuracy of 88.3%. The proposed method effectively identifies the damaged areas of the buildings and grade the damage condition. © 2022 IEEE.","Buildings; Change detection; Damage detection; Deep learning; Disaster prevention; Emergency services; Learning systems; Radar imaging; Semantic Segmentation; Semantic Web; Statistical tests; Synthetic aperture radar; Building damage; Change detection; Damage assessments; Disaster management; Emergency response; High resolution synthetic aperture radar; Learning methods; Multi-temporal SAR images; Semantic change detection; Siamese network; Semantics","Buildings damage; SAR; Semantic change detection; Siamese network","Conference paper","Final","","Scopus","2-s2.0-85141895788"
"Lin W.; Gao X.","Lin, Wenhao (57263211500); Gao, Xunzhang (35777935700)","57263211500; 35777935700","Feature fusion for inverse synthetic aperture radar image classification via learning shared hidden space","2021","Electronics Letters","57","25","","986","988","2","10.1049/ell2.12311","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127435962&doi=10.1049%2fell2.12311&partnerID=40&md5=abbaa1a50171f5d7abd582d45ddffea0","Multi-sensor fusion recognition is a meaningful task in ISAR image recognition. Compared with a single sensor, multi-sensor fusion can provide richer target information, which is conducive to more accurate and robust identification. However, previous deep learning-based fusion methods do not effectively deal with the redundancy and complementarity of information between different sources. In this letter, we construct a shared hidden space to align features from different sources. Accordingly, we design an end-to-end deep fusion framework to fuse dual ISAR images at the feature level. For combining the multi-source information, deep generalised canonical correlation analysis (DGCCA) is used as the loss item to map features extracted from dual input onto the shared hidden space. Moreover, we propose an efficient and lightweight spatial attention module, named united attention module, which can be embedded between dual-stream convolutional neural networks (CNNs) to promote DGCCA optimisation by information interaction. Compared with other deep fusion frameworks, our model obtains the competitive performance in ISAR image fusion for classification. © 2021 The Authors. Electronics Letters published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Convolution; Convolutional neural networks; Deep learning; Image classification; Image fusion; Image recognition; Inverse problems; Inverse synthetic aperture radar; Space-based radar; Features fusions; Generalized canonical correlation analysis; Images classification; Inverse synthetic aperture radar images; ISAR images; Multi-sensor fusion; Robust identification; Shared hidden spaces; Single sensor; Target information; Radar imaging","","Letter","Final","","Scopus","2-s2.0-85127435962"
"Wang J.; Li Y.; Song M.; Xing M.","Wang, Jiadong (57201328127); Li, Yachao (16307232700); Song, Ming (57428813500); Xing, Mengdao (7005922869)","57201328127; 16307232700; 57428813500; 7005922869","Joint Estimation of Absolute Attitude and Size for Satellite Targets Based on Multi-Feature Fusion of Single ISAR Image","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5111720","","","","10.1109/TGRS.2022.3159345","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126541006&doi=10.1109%2fTGRS.2022.3159345&partnerID=40&md5=7dafac9ccc08c791ef453543f3afb719","It is challenging to estimate satellite targets' absolute attitude and size with limited observational data. This article proposes an innovative way to jointly estimate satellite targets' absolute attitude and size in the 3-D stable coordinates based on inverse synthetic aperture radar (ISAR) image interpretation with only one image. By taking advantage of the rectangular solar panels commonly equipped on satellites, this article extracts solar panel's principal components, line features, and phase features of single ISAR imagery with principal component analysis (PCA), radon transform (RT), and minimum-entropy (ME)-based autofocus method, respectively. The projection relationship between these features and the absolute attitude and size of the satellite are established separately. Through multi-features fusion, a joint parameter estimation optimization function is established. This optimization is solved iteratively by the quasi-Newton method. The attitude and size parameters can be estimated simultaneously and rapidly, realizing the satellite state estimation under limited observation data. The excellent performance of the proposed algorithm is verified through different experiments.  © 1980-2012 IEEE.","Image analysis; Image fusion; Inverse problems; Inverse synthetic aperture radar; Newton-Raphson method; Parameter estimation; Principal component analysis; Satellites; Solar panels; Space-based radar; Attitude estimation; Image interpretation; Inverse synthetic aperture radar image interpretation; Inverse synthetic aperture radar images; Multi-feature fusion; Satellite target; Size estimation; Solar panels; Space vehicles; Three-dimensional display; algorithm; attitudinal survey; estimation method; public attitude; satellite data; satellite imagery; synthetic aperture radar; Radar imaging","Attitude estimation; inverse synthetic aperture radar (ISAR) image interpretation; multi-features fusion; satellite targets; size estimation","Article","Final","","Scopus","2-s2.0-85126541006"
"Zhang J.; Li Y.; Si Y.; Peng B.; Xiao F.; Luo S.; He L.","Zhang, Jinglin (57809246300); Li, Yuxia (57004263900); Si, Yu (57222243243); Peng, Bo (57207875553); Xiao, Fanghong (57192701253); Luo, Shiyu (56200560300); He, Lei (55789766400)","57809246300; 57004263900; 57222243243; 57207875553; 57192701253; 56200560300; 55789766400","A Low‐Grade Road Extraction Method using SDG‐DenseNet Based on the Fusion of Optical and SAR Images at Decision Level","2022","Remote Sensing","14","12","2870","","","","10.3390/rs14122870","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134368885&doi=10.3390%2frs14122870&partnerID=40&md5=04a441cb2f73e100476a88acc55a406a","Low‐grade roads have complex features such as geometry, reflection spectrum, and spatial topology in remotely sensing optical images due to the different materials of those roads and also because they are easily obscured by vegetation or buildings, which leads to the low accuracy of low‐grade road extraction from remote sensing images. To address this problem, this paper pro-poses a novel deep learning network referred to as SDG‐DenseNet as well as a fusion method of optical and Synthetic Aperture Radar (SAR) data on decision level to extract low‐grade roads. On one hand, in order to enlarge the receptive field and ensemble multi‐scale features in commonly used deep learning networks, we develop SDG‐DenseNet in terms of three modules: stem block, D‐ Dense block, and GIRM module, in which the Stem block applies two consecutive small‐sized convolution kernels instead of the large‐sized convolution kernel, the D‐Dense block applies three consecutive dilated convolutions after the initial Dense block, and Global Information Recovery Module (GIRM) combines the ideas of dilated convolution and attention mechanism. On the other hand, considering the penetrating capacity and oblique observation of SAR, which can obtain information from those low‐grade roads obscured by vegetation or buildings in optical images, we integrate the extracted road result from SAR images into that from optical images at decision level to enhance the extraction accuracy. The experimental result shows that the proposed SDG‐DenseNet attains higher IoU and F1 scores than other network models applied to low‐grade road extraction from optical images. Furthermore, it verifies that the decision‐level fusion of road binary maps from SAR and optical images can further significantly improve the F1, COR, and COM scores. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Convolution; Deep learning; Extraction; Geometrical optics; Image enhancement; Image fusion; Image segmentation; Learning systems; Optical remote sensing; Radar imaging; Roads and streets; Vegetation; Decision levels; Deep learning; Images segmentations; Low grade; Low‐grade road extraction; Optical image; Optical-; Remote-sensing; Road extraction; Synthetic aperture radar images; Synthetic aperture radar","deep learning; image segmentation; low‐grade road extraction; remote sensing; SAR image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134368885"
"Wu J.; Yu L.; Liu M.; Ma M.","Wu, Jie (55986087200); Yu, Lingling (57338236400); Liu, Ming (56739538000); Ma, Miao (56285857400)","55986087200; 57338236400; 56739538000; 56285857400","Texture and Intensity Fusion based SAR Image Change Detection","2021","2021 SAR in Big Data Era, BIGSARDATA 2021 - Proceedings","","","","","","","10.1109/BIGSARDATA53212.2021.9574306","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119066508&doi=10.1109%2fBIGSARDATA53212.2021.9574306&partnerID=40&md5=b1ff7ed5bd715394975d48510672a397","Due to the coherent imaging method of the SAR system, SAR images are always accompanied by multiplicative speckle. Therefore, it makes the change detection based on SAR images are challenging. Aiming at the problem of the construction and the analysis of the difference image in SAR change detection, we propose a new SAR image change detection algorithm. In this method, the texture information and intensity information of difference image are fused adaptively according to the Bayesian formula and the local consistency evaluation. In the experiments, two real SAR image sets are used for performance analysis, from which the proposed method shows a better performance.  © 2021 IEEE.","Image analysis; Image fusion; Image texture; Iterative methods; Synthetic aperture radar; Textures; Bayesian; Change detection; Coherent imaging; Difference images; Image change detection; Imaging method; Iterative fusion; Local consistency; SAR imagery; SAR Images; Radar imaging","Bayesian; Iterative fusion; Local consistency; SAR imagery","Conference paper","Final","","Scopus","2-s2.0-85119066508"
"Tong Y.; Quan Y.; Feng W.; Xing M.","Tong, Yingping (57220011021); Quan, Yinghui (35181982300); Feng, Wei (57089587500); Xing, Mengdao (7005922869)","57220011021; 35181982300; 57089587500; 7005922869","Multi-source remote sensing image fusion method based on spatial-spectrum information collaboration and Gram-Schmidt transform; [基于空谱信息协同与Gram-Schmidt变换的多源遥感图像融合方法]","2022","Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics","44","7","","2074","2083","9","10.12305/j.issn.1001-506X.2022.07.02","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132523965&doi=10.12305%2fj.issn.1001-506X.2022.07.02&partnerID=40&md5=29eab6b032f2e79b467f873c1fd9733b","The fusion of multispectral and synthetic aperture radar (SAR) images can retain the advantages of each data and improve the accuracy of land cover classification. However, some current image fusion methods cannot fully utilize the spectral information and texture details of the original data. In order to overcome these problems, a fusion method based on space-spectrum information collaboration and Gram-Schmidt transform is proposed. In the proposed method, Sentinel-2A images and GaoFen-3 (GF-3) images are preprocessed by different methods. Since the gray co-occurrence matrix can effectively extract the texture information of the image, it is applied to the Sentinel-2A image to extract the structural features, and the multispectral image coordinated by the space-spectrum information is fused with GF-3 image by the Gram-Schmidt transform. Principal component analysis (PCA) and the traditional Gram-Schmidt transform are used as the comparison methods in this experiment. In order to determine the effectiveness of the fusion algorithm, this paper uses five evaluation indicators including average gradient, spatial frequency, mean, standard deviation and correlation coefficient to measure the quality of the fusion image. In addition, due to its excellent training speed and excellent classification performance, random forest is used for land cover classification. The classification accuracy of random forest, Kappa coefficient and classification result graph are used as the evaluation criteria of the fusion method. Experimental results show that, compared with the original Sentinel-2A alone, the proposed fusion method can improve the overall accuracy by up to 5%, and has the potential to improve the accuracy of land cover classification in remote sensing satellite images. © 2022, Editorial Office of Systems Engineering and Electronics. All right reserved.","Classification (of information); Decision trees; Image classification; Image enhancement; Image fusion; Principal component analysis; Quality control; Synthetic aperture radar; Textures; Fusion methods; Gram-Schmidt transform; Image fusion methods; Information collaborations; Land cover classification; Multi-spectral; Random forests; Remote-sensing; Space spectrum; Spectrum information; Remote sensing","Classification; Image fusion; Multispectral; Remote sensing","Article","Final","","Scopus","2-s2.0-85132523965"
"Arnous F.I.; Narayanan R.M.; Li B.C.","Arnous, Ferris I. (57217305796); Narayanan, Ram M. (7202724032); Li, Bing C. (26643107500)","57217305796; 7202724032; 26643107500","Application of multidomain sensor image fusion and training data augmentation for enhanced CNN image classification","2022","Journal of Electronic Imaging","31","1","013014","","","","10.1117/1.JEI.31.1.013014","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125697473&doi=10.1117%2f1.JEI.31.1.013014&partnerID=40&md5=936ae611883d640721fefd546215f3ee","Convolutional neural networks (CNNs) provide the sensing and detection community with a discriminative approach for classifying images. However, one of the largest limitations of deep CNN image classifiers is the need for extensive training datasets containing a variety of image representations. While current methods, such as generative adversarial network data augmentation, additions of noise, rotations, and translations, can allow CNNs to better associate new images and their feature representations to ones of a learned image class, many fail to provide new contexts of ground truth feature information. To expand the association of critical class features within CNN image training datasets, an image pairing and training dataset augmentation paradigm via a multi-sensor domain image data fusion algorithm is proposed. This algorithm uses a mutual information (MI) and merit-based feature selection subroutine to pair highly correlated cross-domain images from multiple sensor domain image datasets. It then re-augments the corresponding cross-domain image pairs into the opposite sensor domain's feature set via a highest MI, cross sensor domain, and image concatenation function. This augmented image set then acts to retrain the CNN to recognize greater generalizations of image class features via cross domain, mixed representations. Experimental results indicated an increased ability of CNNs to generalize and discriminate between image classes during testing of class images from synthetic aperture radar vehicle, solar cell device reliability screening, and lung cancer detection image datasets.  © 2022 SPIE and IS&T.","Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image classification; Image enhancement; Information fusion; Radar imaging; Sensor data fusion; Synthetic aperture radar; Convolutional neural network; Cross-domain; Data augmentation; Image datasets; Image training; Mutual informations; Sensor domains; Training data; Training data augmentation; Training dataset; Image fusion","convolutional neural network; image fusion; machine learning; mutual information; training data augmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125697473"
"Zhang T.; Zhang X.","Zhang, Tianwen (57209179449); Zhang, Xiaoling (52265071000)","57209179449; 52265071000","Squeeze-and-Excitation Laplacian Pyramid Network with Dual-Polarization Feature Fusion for Ship Classification in SAR Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3119875","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117353439&doi=10.1109%2fLGRS.2021.3119875&partnerID=40&md5=aab20762fd26f4e8dbef95f9d4a80f15","This letter proposes a squeeze-and-excitation Laplacian pyramid network with dual-polarization feature fusion (SE-LPN-DPFF) for ship classification in synthetic aperture radar (SAR) images. SE-LPN-DPFF offers three contributions: 1) dual-polarization (VV and VH) feature fusion (DPFF); 2) channel modeling by the squeeze-and-excitation (SE) to balance each polarization feature's contribution; and 3) Laplacian pyramid network (LPN) to achieve multiresolution analysis (MRA). Extensive ablation studies can confirm the effectiveness of each contribution. Results on the three- and six-category OpenSARShip datasets reveal the state-of-the-art SAR ship classification performance.  © 2004-2012 IEEE.","Classification (of information); Fusion reactions; Image fusion; Polarization; Radar imaging; Ships; Synthetic aperture radar; Dual-polarization feature fusion; Dual-polarizations; Features extraction; Features fusions; Laplace equation; Laplacian Pyramid; Laplacian pyramid network; Marine vehicles; Pyramid network; Radar polarimetry; Ship classificum-tion; Squeeze-and-excitum-tion; Synthetic aperture radar; accuracy assessment; image analysis; polarization; radar imagery; remote sensing; synthetic aperture radar; Laplace transforms","Dual-polarization feature fusion (DPFF); Laplacian pyramid network (LPN); ship classification; squeeze-and-excitation (SE); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85117353439"
"Wei D.; Yang J.","Wei, Deyun (35204284300); Yang, Jun (57188982756)","35204284300; 57188982756","Two‐dimensional sparse fractional Fourier transform and its applications","2022","Signal Processing","201","","108682","","","","10.1016/j.sigpro.2022.108682","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134329278&doi=10.1016%2fj.sigpro.2022.108682&partnerID=40&md5=4bb260572e01fc21205641b2b8dfba55","The discrete fractional Fourier transform is an excellent tool in non-stationary signal processing. And an efficient and accurate computation is important for the two-dimensional discrete fractional Fourier transform (2D DFRFT). Inspired by the sparse Fourier transform algorithm, we propose a two-dimensional sparse fractional Fourier transform (2D SFRFT) algorithm to estimate the fractional Fourier spectrum efficiently. Compared with existing methods, we have achieved the lowest runtime and sample complexity. Moreover, by analyzing the errors due to noises, the 2D SFRFT algorithm is improved to be robust. The applications in image fusion, parameter estimation of multicomponent 2D chirp signal and complex maneuvering targets in SAR radar demonstrate the effectiveness of the proposed algorithms. © 2022","Fourier transforms; Radar imaging; Synthetic aperture radar; Accurate computations; Discrete fractional Fourier transforms; Efficient computation; Fourier transform algorithms; Fractional Fourier transforms; ITS applications; Nonstationary signal processing; Sparse fourier transform; Two-dimensional; Two-dimensional fractional fourier transform; Image fusion","Discrete fractional Fourier transform; Image fusion; Sparse Fourier transform; Two-dimensional fractional Fourier transform","Article","Final","","Scopus","2-s2.0-85134329278"
"Wang M.; Meng X.; Shao F.; Fu R.","Wang, Mengyao (57210968797); Meng, Xiangchao (56158755000); Shao, Feng (7006717672); Fu, Randi (14821950200)","57210968797; 56158755000; 7006717672; 14821950200","SAR-Assisted Optical Remote Sensing Image Cloud Removal Method Based on Deep Learning; [基于深度学习的SAR辅助下光学遥感图像去云方法]","2021","Guangxue Xuebao/Acta Optica Sinica","41","12","1228002","","","","10.3788/AOS202141.1228002","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113278742&doi=10.3788%2fAOS202141.1228002&partnerID=40&md5=9b89574ba859279c7a4f3fd7c4491b52","The existing deep learning based SAR-assisted cloud removal methods do not take full into account the texture and spectral information of the optical images, which results in blurring and spectral loss. In this paper, we constructed a data set for SAR-assisted cloud removal based on the Sentinel-1 and Sentinel-2 satellite images in Yuhang District of Hangzhou. In addition, we established a conditional generative adversarial network (cGAN) based model by fully considering the details, texture, and color information of optical remote sensing images, achieving information recovery and reconstruction in the case of optical images covered by thin clouds, fog, and thick clouds. The results show that the proposed method outperforms other methods in SAR-assisted cloud removal. © 2021, Chinese Lasers Press. All right reserved.","Geometrical optics; Image texture; Learning systems; Radar imaging; Remote sensing; Textures; Adversarial networks; Cloud removal; Color information; Information recovery; Optical image; Optical remote sensing; Satellite images; Spectral information; Deep learning","Cloud removal; Conditional generative adversarial network (cGAN); Image fusion; Optical image; Remote sensing; Synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85113278742"
"Song D.; Tharmarasa R.; Han K.; Wang W.; McDonald M.; Kirubarajan T.","Song, Dan (57191624180); Tharmarasa, Ratnasingham (20434464500); Han, Kun (45860914900); Wang, Wei (57216855116); McDonald, Mike (57193896563); Kirubarajan, Thiagalingam (57609654200)","57191624180; 20434464500; 45860914900; 57216855116; 57193896563; 57609654200","A Bayesian Method for ViSAR Image Fusion Using Effective Reflection Coefficient","2022","IEEE Sensors Journal","22","10","","9743","9753","10","10.1109/JSEN.2022.3166825","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128606260&doi=10.1109%2fJSEN.2022.3166825&partnerID=40&md5=57027d051144e704260efe14cb3a4b36","Since the isotropic scattering assumption does not hold in a wide-angle synthetic aperture radar, the video synthetic aperture radar (ViSAR), a new sensing mode by sequentially forming SAR images on a series of contiguous or overlapping sub-apertures, has the promising capability to capture aspect-dependent scattering behavior of objects. In this paper, a new Bayesian method for fusing ViSAR images is proposed. First, the notion of effective reflection coefficient is defined to characterize aspect-dependent scattering behavior of objects. Based on the ViSAR images sequence, the spatial effective reflection coefficient in the area of interest (AOI) when observing at different aspect angles are estimated. A Bayesian hypothesis testing is then derived to fuse the obtained effective reflection coefficient estimates for detecting scatterers in the AOI. The performance of the proposed method is evaluated and compared with that of the conventional GLRT-based ViSAR image fusion method in a simulated scenario. Numerical results demonstrate the superiority of the proposed method in capturing the aspect-dependent scattering characteristics as well as the spatial structure of objects.  © 2001-2012 IEEE.","Bayesian networks; Image fusion; Manganese; Numerical methods; Radar imaging; Statistical tests; Synthetic aperture radar; Aperture; Area of interest; Aspect-dependent scattering; Bayesian methods; Dependent scatterings; Effective reflection coefficient; Isotropic scattering; Radar polarimetry; Synthetic aperture radar images; Video synthetic aperture radar; Image resolution","Aspect-dependent scattering; Effective reflection coefficient; Video synthetic aperture radar (ViSAR)","Article","Final","","Scopus","2-s2.0-85128606260"
"Zhu X.; Liu L.; Guo B.; Hu W.; Ma J.; Shi L.","Zhu, Xiaoxiu (57197872453); Liu, Limin (57222141687); Guo, Baofeng (56577060100); Hu, Wenhua (57199745271); Ma, Juntao (56388898000); Shi, Lin (55364489800)","57197872453; 57222141687; 56577060100; 57199745271; 56388898000; 55364489800","Coherent compensation and high-resolution technology of multi-band inverse synthetic aperture radar fusion imaging","2021","IET Radar, Sonar and Navigation","15","2","","167","180","13","10.1049/rsn2.12030","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101599799&doi=10.1049%2frsn2.12030&partnerID=40&md5=f4b02d3b6f8f853b152f7591b35aaa25","Multi-band inverse synthetic aperture radar (ISAR) fusion imaging technology can effectively improve the range resolution without incurring high hardware cost. The coherent phase between sub-bands is a prerequisite to achieve multi-band ISAR fusion imaging. Here, a joint approach of coherent compensation and high-resolution imaging is proposed to compensate the incoherent phase and obtain high-resolution ISAR fusion images. First, an incoherent phase estimation model based on sparse representation is established, and the phase estimation accuracy is improved by a modified coherent dictionary in case of off-grid. Then, a multi-band ISAR fusion imaging model based on sparse representation is established. The complex Gaussian scale mixture priors and the complex Gaussian priors are imposed on the scatterers and noise, respectively. The solution is derived in the complex domain based on the variational Bayesian expectation maximization framework. The proposed method can not only achieve better incoherent phase compensation in the case of off-grid, but also obtain high-quality ISAR fusion images under low signal-to-noise ratio and low bandwidth sampling ratio. Experimental results verify the effectiveness and robustness of the proposed method based on both numerical simulations and real data. © 2021 The Authors. IET Radar, Sonar & Navigation published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Image fusion; Imaging techniques; Inverse problems; Inverse synthetic aperture radar; Maximum principle; Numerical methods; Signal to noise ratio; Coherent compensations; High resolution technology; High-resolution imaging; Inverse synthetic aperture radars (ISAR); Low signal-to-noise ratio; Phase compensation; Sparse representation; Variational bayesian; Radar imaging","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101599799"
"Zhang S.; Zhang Y.; Chou Y.; Wang Z.; Shi Y.; Sun Z.","Zhang, Shuchun (8321575700); Zhang, Yun (56097923300); Chou, Yonabin (57226379460); Wang, Ziheng (57221806189); Shi, Yifu (57226378587); Sun, Zhenyu (57226385910)","8321575700; 56097923300; 57226379460; 57221806189; 57226378587; 57226385910","Analysis of the Multispectral and SAR Image","2021","2021 IEEE 6th International Conference on Computer and Communication Systems, ICCCS 2021","","","9449213","312","315","3","10.1109/ICCCS52626.2021.9449213","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113318904&doi=10.1109%2fICCCS52626.2021.9449213&partnerID=40&md5=3f27d037003422d427bbed8acb67490c","The SAR image and the multispectral image are both used for dynamic monitoring, mineral resources investigation, urban and rural monitoring and evaluation, traffic network exploration, forest resources investigation, desertification monitoring, and so on. The multi-spectral and SAR image fusion to improve the classify quality is discussed in this paper, compared the common fusion algorithms of the SAR image and multi spectral images, that is standard color transform (Brovey) method, phase recovery (Gram-Schmidt) method and color space transform (HSV) method, principal component transformation super resolution (PCA) method and Bias method (Pansharp), by which the fused image is more relative with the multi-spectral and SAR. © 2021 IEEE.","Color; Image enhancement; Image fusion; Mineral exploration; Mineral resources; Spectroscopy; Synthetic aperture radar; Color space transform; Desertification monitoring; Dynamic monitoring; Fusion algorithms; Monitoring and evaluations; Multispectral images; Principal component transformations; Traffic networks; Radar imaging","image fusion; multi spectral; SAR","Conference paper","Final","","Scopus","2-s2.0-85113318904"
"Zhang W.; Jiao L.; Liu F.; Yang S.; Liu J.","Zhang, Wenhua (57206485064); Jiao, Licheng (7102491544); Liu, Fang (56182993400); Yang, Shuyuan (8159166000); Liu, Jia (56376104500)","57206485064; 7102491544; 56182993400; 8159166000; 56376104500","Adaptive Contourlet Fusion Clustering for SAR Image Change Detection","2022","IEEE Transactions on Image Processing","31","","","2295","2308","13","10.1109/TIP.2022.3154922","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125728122&doi=10.1109%2fTIP.2022.3154922&partnerID=40&md5=006e84a92c4cc65543dec32479da85ed","In this paper, a novel unsupervised change detection method called adaptive Contourlet fusion clustering based on adaptive Contourlet fusion and fast non-local clustering is proposed for multi-temporal synthetic aperture radar (SAR) images. A binary image indicating changed regions is generated by a novel fuzzy clustering algorithm from a Contourlet fused difference image. Contourlet fusion uses complementary information from different types of difference images. For unchanged regions, the details should be restrained while highlighted for changed regions. Different fusion rules are designed for low frequency band and high frequency directional bands of Contourlet coefficients. Then a fast non-local clustering algorithm (FNLC) is proposed to classify the fused image to generate changed and unchanged regions. In order to reduce the impact of noise while preserve details of changed regions, not only local but also non-local information are incorporated into the FNLC in a fuzzy way. Experiments on both small and large scale datasets demonstrate the state-of-the-art performance of the proposed method in real applications. © 1992-2012 IEEE.","Binary images; Clustering algorithms; Fuzzy clustering; Image compression; Image fusion; Image segmentation; Radar imaging; Speckle; Tracking radar; Wavelet transforms; Contourlet fusion; Contourlets; Images segmentations; Local clustering; Neural-networks; Non-local clustering; Nonlocal; Radar polarimetry; Unsupervised change detection; Wavelets transform; article; fuzzy clustering; noise; telecommunication; Synthetic aperture radar","Contourlet fusion; non-local clustering; Synthetic aperture radar; unsupervised change detection","Article","Final","","Scopus","2-s2.0-85125728122"
"Chen Q.; Liu W.; Sun G.-C.; Chen X.; Han L.; Xing M.","Chen, Quan (57208129832); Liu, Wenkang (57190407532); Sun, Guang-Cai (57208324136); Chen, Xiaoxiang (57209399685); Han, Liang (55634028400); Xing, Mengdao (7005922869)","57208129832; 57190407532; 57208324136; 57209399685; 55634028400; 7005922869","A Fast Cartesian Back-Projection Algorithm Based on Ground Surface Grid for GEO SAR Focusing","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3125797","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125449676&doi=10.1109%2fTGRS.2021.3125797&partnerID=40&md5=3317eaa1b9e8a58a210b851d32c59f64","Geosynchronous-Earth-orbit (GEO) synthetic aperture radar (SAR) provides excellent continuous observing capability and large swath. However, the extremely long synthetic aperture time, the curved orbit, and the nonplanar ground surface cause serious spatial variance in the GEO SAR signal. In this article, a novel fast Cartesian back-projection (BP) algorithm based on subaperture imaging on ground and multistage fusion is proposed for accurately and efficiently imaging of GEO SAR. The imaging grids are arranged on the ground surface to avoid the azimuth defocusing caused by the flat ground approximation. Then, a new two-step spectrum compression method is derived to solve the spectrum aliasing of subaperture images. Also, a multistage image fusion method is adopted to combine all the subaperture images with high efficiency. The computational complexity and the approximation of the proposed algorithm are also discussed. Simulation results verify the effectiveness of the proposed algorithm.  © 1980-2012 IEEE.","Approximation algorithms; Computational efficiency; Image fusion; Orbits; Radar imaging; Backprojection algorithms; Cartesians; Geosynchronoi-earth-orbit  synthetic aperture radar; Geosynchronous Earth orbit; Ground surface curvature; Ground surfaces; Spectrum compression; Subaperture; Surface curvatures; Surface grids; algorithm; detection method; ground penetrating radar; synthetic aperture radar; Synthetic aperture radar","Back-projection algorithm; Geosynchronous-Earth-orbit (GEO) synthetic aperture radar (SAR); Ground surface curvature; Spectrum compression","Article","Final","","Scopus","2-s2.0-85125449676"
"Sujud L.; Jaafar H.; Haj Hassan M.A.; Zurayk R.","Sujud, Lara (57408747300); Jaafar, Hadi (36628550700); Haj Hassan, Mohammad Ali (57408406400); Zurayk, Rami (6701381333)","57408747300; 36628550700; 57408406400; 6701381333","Cannabis detection from optical and RADAR data fusion: A comparative analysis of the SMILE machine learning algorithms in Google Earth Engine","2021","Remote Sensing Applications: Society and Environment","24","","100639","","","","10.1016/j.rsase.2021.100639","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122658971&doi=10.1016%2fj.rsase.2021.100639&partnerID=40&md5=3d9f4bf9164ed2abc5534197031ab5bc","Accurate crop mapping for agricultural monitoring requires the use of robust image classification algorithms. Cultivation of illegal cannabis is common in the Bekaa plain, Lebanon, as in some other areas of the world (Morocco, Afghanistan). Identification of cultivation sites is money and time consuming since it highly relies on knowledge of cultivation areas and field surveys. Machine-learning-based image classification provides an alternative method for cannabis detection. This paper presents a comparative analysis of four machine-learning classifiers implemented in Google Earth Engine: Random Forest (RF), Gradient Boosting (GBT), Classification and Regression Tree (CART), and Support Vector Machine (SVM) for cannabis and other crop type classification. We implement and test several image fusion approaches for optical (Sentinel-2, Landsat) and Synthetic Aperture Radar (SAR) imagery (Sentinel-1) along with several combinations of surface textural features from Sentinel-1. Six different crop groups were classified over three years (2016, 2017, and 2018) in the Bekaa valley of Lebanon. In general, although SVM outperformed RF, GBT, and CART classifiers with an overall classification accuracy of more than 90%, RF and GBT provided more consistent results in terms of the cannabis cultivated area. SVM appears to be sensitive to the size of the training data. RF and GBT estimate an average cannabis area of 2800 ha in 2016, 2983 ha in 2017, and 5900 ha in 2018. Our results demonstrate that the fusion of radar and optical imagery can improve image classification accuracy by up to 5%. A marginal improvement in overall accuracy was observed when textural features were added. This is the first study that used image fusion and ML to estimate illegal cannabis cultivation areas in Lebanon and help evaluate the contribution of cannabis to the local economy. The combination of machine learning algorithms and imagery fusion proved reliable for crop classification in general and cannabis in particular. © 2021 The Authors","","Bekaa valley; Cannabis; CART; Crop classification; GEE; Gradient boosting; Random forest; Support vector machine","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85122658971"
"Wang C.; Zhao L.; Zhang W.; Mu X.; Li S.","Wang, Caiqiong (57417309500); Zhao, Lei (56412229900); Zhang, Wangfei (56420951300); Mu, Xiyun (56893151300); Li, Shitao (57417268100)","57417309500; 56412229900; 56420951300; 56893151300; 57417268100","Segmentation of multi-temporal polarimetric SAR data based on mean-shift and spectral graph partitioning","2022","PeerJ","10","","e12805","","","","10.7717/peerj.12805","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123050431&doi=10.7717%2fpeerj.12805&partnerID=40&md5=5041910993a2eb8859588566a62ee966","Polarimetric SAR (PolSAR) image segmentation is a key step in its interpretation. For the targets with time series changes, the single-temporal PolSAR image segmentation algorithm is difficult to provide correct segmentation results for its target recognition, time series analysis and other applications. For this, a new algorithm for multi-temporal PolSAR image segmentation is proposed in this paper. Firstly, the over-segmentation of single-temporal PolSAR images is carried out by the mean-shift algorithm, and the over-segmentation results of single-temporal PolSAR are combined to get the over-segmentation results of multi-temporal PolSAR images. Secondly, the edge detectors are constructed to extract the edge information of single-temporal PolSAR images and fuse them to get the edge fusion results of multi-temporal PolSAR images. Then, the similarity measurement matrix is constructed based on the over-segmentation results and edge fusion results of multi-temporal PolSAR images. Finally, the normalized cut criterion is used to complete the segmentation of multi-temporal PolSAR images. The performance of the proposed algorithm is verified based on three temporal PolSAR images of Radarsat-2, and compared with the segmentation algorithm of single-temporal PolSAR image. Experimental results revealed the following findings: (1) The proposed algorithm effectively realizes the segmentation of multi-temporal PolSAR images, and achieves ideal segmentation results. Moreover, the segmentation details are excellent, and the region consistency is good. The objects which can't be distinguished by the single-temporal PolSAR image segmentation algorithm can be segmented. (2) The segmentation accuracy of the proposed multi-temporal algorithm is up to 86.5%, which is significantly higher than that of the singletemporal PolSAR image segmentation algorithm. In general, the segmentation result of proposed algorithm is closer to the optimal segmentation. The optimal segmentation of farmland parcel objects to meet the needs of agricultural production is realized. This lays a good foundation for the further interpretation of multi-temporal PolSAR image. © 2022 PeerJ Inc.. All rights reserved.","Graph theory; Image fusion; Image segmentation; Polarimeters; Radar imaging; Time series analysis; Image segmentation algorithm; Mean shift; Multi-temporal; Normalized cuts; Over segmentation; Polarimetric SAR; SAR image segmentation; SAR Images; Segmentation results; Spectral graph partitioning; agricultural land; article; diagnostic test accuracy study; segmentation algorithm; Synthetic aperture radar","Mean-shift; Multi-temporal; Normalized cut; Polarimetric SAR; Spectral graph partitioning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85123050431"
"Batra A.; Hark T.; Schorlemer J.; Pohl N.; Rolfes I.; Wiemeler M.; Gohringer D.; Kaiser T.; Barowski J.","Batra, Aman (57204001344); Hark, Tobias (57848025400); Schorlemer, Jonas (57226763155); Pohl, Nils (23992957900); Rolfes, Ilona (23992562100); Wiemeler, Michael (57195389882); Gohringer, Diana (25627939000); Kaiser, Thomas (35983476500); Barowski, Jan (55816442300)","57204001344; 57848025400; 57226763155; 23992957900; 23992562100; 57195389882; 25627939000; 35983476500; 55816442300","Fusion of Optical and Millimeter Wave SAR Sensing for Object Recognition in Indoor Environment","2022","2022 5th International Workshop on Mobile Terahertz Systems, IWMTS 2022","","","","","","","10.1109/IWMTS54901.2022.9832438","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136145521&doi=10.1109%2fIWMTS54901.2022.9832438&partnerID=40&md5=ee7dcfb2c644862b3156b839b9a54cfd","Synthetic aperture radar (SAR) sensing at the millimeter-wave (mmWave) spectrum has emerging applications in indoor environments such as high-resolution imaging and localization, material characterization, and object recognition. Although a high-resolution environment map is obtained with SAR at the mmWave spectrum, recognition of objects based on shape is trivial, especially in the case of 2D imaging, where limited shape information is available. Therefore, this paper proposes a method for indoor object recognition in mmWave SAR sensing based on fusion with optical and mmWave SAR image fusion. The recognition includes detection, localization, and classification. Besides, extraction of geometric properties of the objects is also addressed. In this paper, firstly, a 2D multi-object indoor environment mapping is implemented at the spectrum of 68-92 GHz with a frequency modulated continuous wave (FMCW) radar. It replicates a setup of drone-based sensing with side-looking SAR. Two measurement scenarios are considered which primarily differ concerning object positions. The SAR image is reconstructed using a time-domain backprojection algorithm including a calibration procedure for generating a correct imaging plane. Finally, based on the presented method of SAR image fusion with the optical image at visible spectrum obtained using a CMOS camera, object recognition is evaluated.  © 2022 IEEE.","Frequency modulation; Geometrical optics; Image fusion; Indoor positioning systems; Millimeter waves; Object detection; Object recognition; Radar imaging; Space-based radar; Synthetic aperture radar; Time domain analysis; Indoor environment; Indoor mapping; Millimeter wave spectra; Millimeter-wave synthetic aperture radar; Object classification; Objects detection; Objects recognition; Optical-; Radar sensing; Synthetic aperture radar images; Mapping","Indoor mapping; mmWave SAR; Object classification; Object detection; Radar imaging","Conference paper","Final","","Scopus","2-s2.0-85136145521"
"Yu C.; Liu Y.; Xia X.; Lan D.; Liu X.; Wu S.","Yu, Chuang (57207107755); Liu, Yunpeng (56075133600); Xia, Xin (57210516871); Lan, Deyan (57487529800); Liu, Xin (57769855500); Wu, Shuhang (57744437500)","57207107755; 56075133600; 57210516871; 57487529800; 57769855500; 57744437500","Precise and Fast Segmentation of Offshore Farms in High-Resolution SAR Images Based on Model Fusion and Half-Precision Parallel Inference","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","4861","4872","11","10.1109/JSTARS.2022.3181355","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132704846&doi=10.1109%2fJSTARS.2022.3181355&partnerID=40&md5=d0e354933416aa0e0309becc40edbf38","In aquaculture, using high-resolution synthetic aperture radar (SAR) images to precisely segment offshore farms is helpful for reasonable layout planning and statistics of breeding density. However, conventional segmentation methods tend to have low accuracy and slow inference speed. Therefore, we propose a novel, precise, and fast segmentation scheme for offshore farms in high-resolution SAR images based on model fusion and half-precision parallel inference. Specifically, we propose several new high-performance improved UNet++ methods and reasonably fuse the test results. At the same time, a simulated annealing strategy and a morphological closing operation are introduced to improve the segmentation accuracy. In addition, we find that resizing the images to 256 × 256 pixels is better than 512 × 512 pixels for this task, which not only has higher segmentation accuracy but can also increase the inference speed by nearly 13%. Furthermore, a novel half-precision parallel inference strategy is proposed, which can fully utilize the GPU and increase the inference speed by 72.6%. Compared with some state-of-the-art methods, the proposed scheme that merges two improved UNet++ achieves superior accuracy with a frequency weighted intersection over union of 0.9876 and a single image inference time of 0.0218 s on the high-resolution SAR offshore farm dataset.  © 2008-2012 IEEE.","Data mining; Image enhancement; Image fusion; Pixels; Radar imaging; Simulated annealing; Synthetic aperture radar; Features extraction; Half-precision parallel inference; Images segmentations; Improved unet++; License; Offshore farm; Radar polarimetry; SAR Images; Segmentation; Task analysis; aquaculture; image processing; image resolution; model; parallel computing; segmentation; simulated annealing; synthetic aperture radar; Image segmentation","Half-precision parallel inference; improved UNET++; SAR images; segmentation; simulated annealing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132704846"
"Mao S.; Yang J.; Gou S.; Jiao L.; Xiong T.; Xiong L.","Mao, Shasha (24825000100); Yang, Jinyuan (57221480916); Gou, Shuiping (8842235100); Jiao, Licheng (7102491544); Xiong, Tao (35753941400); Xiong, Lin (36984481300)","24825000100; 57221480916; 8842235100; 7102491544; 35753941400; 36984481300","Multi-scale fused sar image registration based on deep forest","2021","Remote Sensing","13","11","2227","","","","10.3390/rs13112227","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108459648&doi=10.3390%2frs13112227&partnerID=40&md5=2f08c84fbc7d858449190ffd2b7bb5e3","SAR image registration is a crucial problem in SAR image processing since the registration results with high precision are conducive to improving the quality of other problems, such as change detection of SAR images. Recently, for most DL-based SAR image registration methods, the problem of SAR image registration has been regarded as a binary classification problem with matching and non-matching categories to construct the training model, where a fixed scale is generally set to capture pair image blocks corresponding to key points to generate the training set, whereas it is known that image blocks with different scales contain different information, which affects the performance of registration. Moreover, the number of key points is not enough to generate a mass of class-balance training samples. Hence, we proposed a new method of SAR image registration that meanwhile utilizes the information of multiple scales to construct the matching models. Specifically, considering that the number of training samples is small, deep forest was employed to train multiple matching models. Moreover, a multi-scale fusion strategy is proposed to integrate the multiple predictions and obtain the best pair matching points between the reference image and the sensed image. Finally, experimental results on four datasets illustrate that the proposed method is better than the compared state-of-the-art methods, and the analyses for different scales also indicate that the fusion of multiple scales is more effective and more robust for SAR image registration than one single fixed scale. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Forestry; Image enhancement; Image fusion; Image registration; Sampling; Synthetic aperture radar; Balance training; Binary classification problems; Change detection; Multiple matching; Multiscale fusion; Reference image; SAR image processing; State-of-the-art methods; Radar imaging","Deep forest; Multi-scale fusion; SAR image registration; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85108459648"
"Xiao X.; Jia H.; Xiao P.; Wang H.","Xiao, Xiayang (57609130000); Jia, Hecheng (57226296459); Xiao, Penghao (57606464900); Wang, Haipeng (36603667200)","57609130000; 57226296459; 57606464900; 36603667200","Aircraft Detection in SAR Images Based on Peak Feature Fusion and Adaptive Deformable Network","2022","Remote Sensing","14","23","6077","","","","10.3390/rs14236077","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143749208&doi=10.3390%2frs14236077&partnerID=40&md5=87c5361343d46195326fa0f7b85dbf0e","Due to the unique imaging mechanism of synthetic aperture radar (SAR), targets in SAR images often shows complex scattering characteristics, including unclear contours, incomplete scattering spots, attitude sensitivity, etc. Automatic aircraft detection is still a great challenge in SAR images. To cope with these problems, a novel approach called adaptive deformable network (ADN) combined with peak feature fusion (PFF) is proposed for aircraft detection. The PFF is designed for taking full advantage of the strong scattering features of aircraft, which consists of peak feature extraction and fusion. To fully exploit the strong scattering features of the aircraft in SAR images, peak features are extracted via the Harris detector and the eight-domain pixel detection of local maxima. Then, the saliency of aircraft under multiple imaging conditions is enhanced by multi-channel blending. All the PFF-preprocessed images are fed into the ADN for training and testing. The core components of ADN contain an adaptive spatial feature fusion (ASFF) module and a deformable convolution module (DCM). ASFF is utilized to reconcile the inconsistency across different feature scales, raising the characterization capabilities of the feature pyramid and improving the detection performance of multi-scale aircraft further. DCM is introduced to determine the 2-D offsets of feature maps adaptively, improving the geometric modeling abilities of aircraft in various shapes. The well-designed ADN is established by combining the two modules to alleviate the problems of the multi-scale targets and attitude sensitivity. Extensive experiments are conducted on the GaoFen-3 (GF3) dataset to demonstrate the effectiveness of the PFF-ADN with an average precision of 89.34%, as well as an F1-score of 91.11%. Compared with other mainstream algorithms, the proposed approach achieves state-of-the-art performance. © 2022 by the authors.","Aircraft detection; Deep learning; Feature extraction; Image fusion; Radar imaging; Synthetic aperture radar; Training aircraft; Adaptive spatial feature fusion; Deep learning; Deformable convolution module; Features fusions; Multi-scales; Peak feature; Spatial features; Strong scatterings; Synthetic aperture radar; Synthetic aperture radar images; Convolution","adaptive spatial feature fusion; aircraft detection; deep learning; deformable convolution module; peak feature; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143749208"
"Yang Y.-Q.; Xia Z.; Zhao Z.; Zhang T.; Li K.; Yin X.; Shi H.; Peng T.","Yang, Yu-Qian (57190302212); Xia, Zhenghuan (55608556100); Zhao, Zhilong (57753807900); Zhang, Tao (57307273400); Li, Ke (57749140300); Yin, Xin (57226832611); Shi, Huifeng (57226826431); Peng, Tao (57216433029)","57190302212; 55608556100; 57753807900; 57307273400; 57749140300; 57226832611; 57226826431; 57216433029","Intelligent Resource Management and Optimization of Clustered UAV Airborne SAR System","2021","2021 IEEE 4th International Conference on Electronics Technology, ICET 2021","","","9451105","987","991","4","10.1109/ICET51757.2021.9451105","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112808015&doi=10.1109%2fICET51757.2021.9451105&partnerID=40&md5=56ab193f9db54b3663cd0427ae5eb301","This paper proposes a resource management method of clustered UAV airborne SAR system based on the advanced genetic algorithm (GA). The clustered UAVs is taken as a Multiple Input Multiple Output (MIMO) system for SAR imaging. The imaging result is improved by fusing the images of different UAV nodes from diverse observation angles. By considering spatial configuration of UAV nodes, the transmitting power and frequency band allocation, the system resource management is optimally scheduled for providing a satisfied image fusion results using the niching GA method. From numerical results, the proposed method can offer more features for image fusion when detecting the cloaking target. © 2021 IEEE.","Genetic algorithms; Image enhancement; Image fusion; MIMO systems; Natural resources management; Numerical methods; Resource allocation; Synthetic aperture radar; Unmanned aerial vehicles (UAV); Advanced genetic algorithms; Intelligent resource management; Numerical results; Observation angle; Resource management; Spatial configuration; System resource management; Transmitting power; Radar imaging","advanced genetic algorithm; optimization of system resource; Synthetic Aperture Radar (SAR); unmanned aerial vehicle (UAV) swarm","Conference paper","Final","","Scopus","2-s2.0-85112808015"
"Pandeeswari B.; Sutha J.; Parvathy M.","Pandeeswari, B. (57216927123); Sutha, J. (23985992900); Parvathy, M. (56210668100)","57216927123; 23985992900; 56210668100","A novel synthetic aperture radar image change detection system using radial basis function-based deep convolutional neural network","2021","Journal of Ambient Intelligence and Humanized Computing","12","1","","897","910","13","10.1007/s12652-020-02091-y","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085277650&doi=10.1007%2fs12652-020-02091-y&partnerID=40&md5=63da2ddec75ae96babbf96d020d5be49","Today, the automatic change detection and also classification as of the Synthetic Aperture Radar (SAR) images remain a hard process. In the existing research, the availability of Speckle Noise (SN), high time-consumption, and low accuracy are the chief issues. To resolve such issues, this paper proposed a novel SAR image change detection system utilizing a Radial Basis Function-based Deep Convolutional Neural Network (RBF-DCNN). The proposed methodology comprises six phases, namely, pre-processing, obtaining difference image, pixel-level image fusion, Feature Extraction (FE), Feature Selection (FS), and also change detection (CD) utilizing the classifier. Initially, the noise is eliminated as of the input, SAR image 1 and SAR image 2, utilizing the NLMSTAF approach. Subsequently, the difference image is attained by utilizing a Log-ratio operator (LRO) and Gauss-LRO, and the attained difference image is then fused. Next, the LTrP, WST, edge, and MSER features are extracted from the fused image. As of those features that were extracted, the necessary features are selected utilizing the Hybrid GWO-GA algorithm. The features (selected) are finally inputted to the RBF-DCNN classifier for detecting the changes in an image. Experimental outcomes established that the proposed work renders better performance on considering the existing system. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Convolution; Convolutional neural networks; Deep neural networks; Feature extraction; Functions; Image fusion; Image processing; Radial basis function networks; Synthetic aperture radar; Tracking radar; Automatic change detection; Change detection; Difference images; Existing systems; Pixel-level image fusion; Radial basis functions; Synthetic aperture radar (SAR) images; Time consumption; Radar imaging","Gauss-log-ratio operator; Hybrid Gray Wolf optimization-genetic algorithm (Hybrid GWO-GA); Local tetra pattern (LTrP); Log-ratio; Maximally stable external region (MSER); Non local mean spatio temporal adaptive filtering (NLMSTAF); Radial basis function-deep convolutional neural network (RBF-DCNN); Wavelet statistical transform (WST)","Article","Final","","Scopus","2-s2.0-85085277650"
"Wei J.; Zou H.; Sun L.; Cao X.; Li M.; He S.; Liu S.","Wei, Juan (57310676500); Zou, Huanxin (8366222500); Sun, Li (57311427800); Cao, Xu (57937375000); Li, Meilin (57209947350); He, Shitian (57222956907); Liu, Shuo (57219450775)","57310676500; 8366222500; 57311427800; 57937375000; 57209947350; 57222956907; 57219450775","Generative Adversarial Network for SAR-to-Optical Image Translation with Feature Cross-Fusion Inference","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","6025","6028","3","10.1109/IGARSS46834.2022.9884166","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140400952&doi=10.1109%2fIGARSS46834.2022.9884166&partnerID=40&md5=383264dae2890fd2f04bbd087fad3418","The translation of synthetic aperture radar (SAR) to optical images provides a new solution for the interpretation of SAR images. Most of the existing translation networks are based on generative adversarial networks and use 9-residual blocks or U-Net structures in the feature inference phase. Both structures cause a large amount of information lost during the conversion of SAR image features to optical features, making the outline of the translated image blurred or semantic information lost. Aiming at this problem, this paper proposes a cross-fusion inference network structure, which preserves both high-resolution features and low-resolution features in the whole process of feature inference. Our proposed method broadens the network horizontally while deepening it vertically and improving the image translation performance. The experiments conducted on the public dataset sen1-2 show that the proposed method is superior to other networks. © 2022 IEEE.","Generative adversarial networks; Geometrical optics; Image enhancement; Image fusion; Radar imaging; Semantics; Cross-fusion inference structure; Generative adversarial network; Image translation; Information lost; Large amounts; Net structures; New solutions; Optical image; Synthetic aperture radar images; Synthetic aperture radar-to-optical image translation; Synthetic aperture radar","Cross-fusion inference structure; Generative adversarial network (GAN); SAR-to-optical image translation","Conference paper","Final","","Scopus","2-s2.0-85140400952"
"Du L.; Li L.; Guo Y.; Wang Y.; Ren K.; Chen J.","Du, Lan (56430356700); Li, Lu (57197872211); Guo, Yuchen (57226823334); Wang, Yan (57189251348); Ren, Ke (57214999074); Chen, Jian (57214909139)","56430356700; 57197872211; 57226823334; 57189251348; 57214999074; 57214909139","Two-stream deep fusion network based on vae and cnn for synthetic aperture radar target recognition","2021","Remote Sensing","13","20","4021","","","","10.3390/rs13204021","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117249730&doi=10.3390%2frs13204021&partnerID=40&md5=36680365debee8968f57c517a57f1f62","Usually radar target recognition methods only use a single type of high-resolution radar signal, e.g., high-resolution range profile (HRRP) or synthetic aperture radar (SAR) images. In fact, in the SAR imaging procedure, we can simultaneously obtain both the HRRP data and the corresponding SAR image, as the information contained within them is not exactly the same. Although the information contained in the HRRP data and the SAR image are not exactly the same, both are important for radar target recognition. Therefore, in this paper, we propose a novel end-to-end two stream fusion network to make full use of the different characteristics obtained from modeling HRRP data and SAR images, respectively, for SAR target recognition. The proposed fusion network contains two separated streams in the feature extraction stage, one of which takes advantage of a variational auto-encoder (VAE) network to acquire the latent probabilistic distribution characteristic from the HRRP data, and the other uses a lightweight convolutional neural network, LightNet, to extract the 2D visual structure characteristics based on SAR images. Following the feature extraction stage, a fusion module is utilized to integrate the latent probabilistic distribution characteristic and the structure characteristic for the reflecting target information more comprehensively and sufficiently. The main contribution of the proposed method consists of two parts: (1) different characteristics from the HRRP data and the SAR image can be used effectively for SAR target recognition, and (2) an attention weight vector is used in the fusion module to adaptively integrate the different characteristics from the two sub-networks. The experimental results of our method on the HRRP data and SAR images of the MSTAR and civilian vehicle datasets obtained improvements of at least 0.96 and 2.16%, respectively, on recognition rates, compared with current SAR target recognition methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolution; Convolutional neural networks; Extraction; Feature extraction; Image enhancement; Image fusion; Network coding; Probability distributions; Radar imaging; Radar target recognition; Auto encoders; Convolutional neural network; Fusion network; High resolution range profiles; Profile data; Recognition methods; Synthetic aperture radar images; Target recognition; Two-stream; Variational auto-encoder; Synthetic aperture radar","Convolutional neural network; Fusion network; High-resolution range profile; Synthetic aperture radar; Target recognition; Variational auto-encoder","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117249730"
"Shen F.; Wang Y.; Liu C.","Shen, Fangyu (57216287705); Wang, Yanfei (36678280200); Liu, Chang (57191676667)","57216287705; 36678280200; 57191676667","Change Detection in SAR Images Based on Improved Non-Subsampled Shearlet Transform and Multi-Scale Feature Fusion CNN","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","12174","12186","12","10.1109/JSTARS.2021.3126839","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121687124&doi=10.1109%2fJSTARS.2021.3126839&partnerID=40&md5=33a01647fa4381d13ed8078635b2ecfb","Traditional methods for change detection in synthetic aperture radar images have difficulty in obtaining results from the generated differential image (DI) owing to speckle noise. In recent years, many deep learning-based methods have emerged because of their outstanding anti-noise and self-learning ability. However, they are limited by the requirement of abundant high-precision labels. Therefore, in this article, we propose a novel unsupervised method based on improved non-subsampled shearlet transform (NSST) and multi-scale feature fusion convolutional neural network for change detection. First, this method improves the traditional NSST algorithm and proposes a novel pseudo-label generator to obtain more pseudo-labels with higher confidence. It is noteworthy that the more accurate the pseudo-labels are, the better the change detection results will be. Second, this method designs a multi-scale feature fusion block in the network to make the feature images contain more complete information and reduces the number of pooling layers to avoid losing feature image details. The main idea of this method is to eliminate the step of generating the DI and directly obtain results from the original images. The theoretical analysis and final results conducted on three real datasets prove its validity. Furthermore, to verify the generality and potential of the proposed method, we apply it to the cross-region change detection and compare it with the supervised method, which achieve satisfactory results.  © 2008-2012 IEEE.","Convolution; Deep learning; Feature extraction; Image enhancement; Image fusion; Neural networks; Radar imaging; Tracking radar; Change detection; Convolutional neural network; Differential image; Features fusions; Multi-scale features; Non-subsampled shearlet transform; Shearlet transforms; Synthetic aperture radar; Synthetic aperture radar images; artificial neural network; data set; detection method; model validation; numerical model; precision; satellite imagery; speckle; supervised classification; synthetic aperture radar; theoretical study; Synthetic aperture radar","Change detection; convolutional neural network (CNN); non-subsampled shearlet transform (NSST); synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121687124"
"Li Y.; Zhu W.; Huang Q.; Li Y.; He Y.","Li, Yonggang (57223054204); Zhu, Weigang (55453985100); Huang, Qiongnan (57223034406); Li, Yuntao (36739208300); He, Yonghua (57226809864)","57223054204; 55453985100; 57223034406; 36739208300; 57226809864","Near-shore ship target detection with SAR images in complex background; [复杂背景下犛犃犚图像近岸舰船目标检测]","2022","Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics","44","10","","3096","3103","7","10.12305/j.issn.1001-506X.2022.10.13","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141410188&doi=10.12305%2fj.issn.1001-506X.2022.10.13&partnerID=40&md5=adfcd68f67ecec762b98d7f6bdb92764","In order to solve the problems of low detection rate, high false alarm rate and high missed detection rate of near-shore ship target detection from synthetic aperture radar (SAR) images caused by the vulnerability of SAR images to background clutter, a deformable feature fusion you only look once 5 (DFFYolov5) algorithm is proposed for the detection of near-shore ship targets in SAR images with complex background. The algorithm is based on the Yolov5 target detection algorithm, with two improvements in the feature extraction network: feature refinement and multi-feature fusion. A special data set for near-shore ship target ditection in complex background of SAR images is costructed. In the feature extraction network, a deformable convolutional neural network is used to change the position of the target sampling points to enhance the feature extraction capability of the target and improve the detection rate of SAR images ship targets in complex background. In the multi-feature fusion network structure, cascade and parallel pyramids are used to perform feature fusion at different levels. At the same time, cavity convolution is used to expand the visual field of feature extraction, enhance the adaptability of the network to near-shore multi-scale ship targets in complex background, and reduce the false alarm rate of SAR image ship target detection in complex backgrounds. Through the test experiments on the constructed complex background near-shore ship, the results show that the average accuracy of DFF-Yolov5 is 85.99%, compared with the original Yolov5, the average accuracy of the proposed method is improved by 5.09% and the precision is improved by 1.4%. © 2022 Chinese Institute of Electronics. All rights reserved.","Complex networks; Convolution; Convolutional neural networks; Deformation; Errors; Extraction; Feature extraction; Image enhancement; Image fusion; Image resolution; Radar imaging; Space-based radar; Complex background; Convolutional neural network; Deformable convolutional neural network; Multi-feature fusion; Near-shore ship target; Nearshores; Ship targets; Synthetic aperture radar; Synthetic aperture radar images; Targets detection; Synthetic aperture radar","deformable convolutional neural network; multi-feature fusion; near-shore ship target; synthetic aperture radar (SAR); target detection","Article","Final","","Scopus","2-s2.0-85141410188"
"Cao Y.; Wu Y.; Li M.; Liang W.; Hu X.","Cao, Yice (57201420994); Wu, Yan (7406895902); Li, Ming (56937290000); Liang, Wenkai (57192207410); Hu, Xin (57221683116)","57201420994; 7406895902; 56937290000; 57192207410; 57221683116","DFAF-Net: A Dual-Frequency PolSAR Image Classification Network Based on Frequency-Aware Attention and Adaptive Feature Fusion","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5224318","","","","10.1109/TGRS.2022.3152854","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125326277&doi=10.1109%2fTGRS.2022.3152854&partnerID=40&md5=d54b1cd21545c956ab15ea8e80c94b9d","Multifrequency (MF) polarization synthetic aperture radar (PolSAR) systems can obtain more abundant and continuous earth resource information than single-frequency ones and have been widely used in the remote sensing community. However, there are relatively a few researches for the fine MF PolSAR image classification, which is an important part of remote sensing image interpretation. The main focus currently is on the single-frequency part. Therefore, for dual-frequency PolSAR image classification, this article proposes the dual-frequency attention fusion network (DFAF-Net). It is based on frequency-aware attention and adaptive feature fusion to improve classification performance. First, the dual-frequency PolSAR data is input into the joint feature extraction (JFE) module to obtain the joint feature representation. Meanwhile, two frequency-aware attention block (FAB) modules with the same structure are constructed, which, respectively, generate frequency-specific attention masks based on the guidance of different frequency data. Subsequently, these masks are used to weigh the joint features to highlight the description of different frequency-aware importance. The activated frequency-aware features can fully mine and utilize the complementary information provided by different frequencies, thereby enhancing the discrimination of similar landcover categories. Finally, the adaptive feature fusion block (AFFB) module is utilized to adaptively aggregate different frequency-aware features multiple times, which can effectively eliminate information differences. The obtained fusion features are more compact within classes and separable between classes, thereby effectively improving the classification performance. Experiments on three measured spaceborne and airborne dual-frequency PolSAR datasets verify that DFAF-Net can better perceive frequency characteristics and fully mine the complementary. Therefore, the classification accuracy is effectively enhanced, and the inaccuracy of single-frequency classification can be eliminated. Meanwhile, due to the introduction of the attention module, the classification performance of the proposed DFAF-Net is more competitive than the related deep learning networks. Quantitatively, the overall accuracy of DFAF-Net on the three datasets is respectively 98.30%, 97.42%, and 99.45%, which is better than other methods. © 1980-2012 IEEE.","Classification (of information); Deep learning; Image fusion; Remote sensing; Synthetic aperture radar; Adaptive feature fusion; Adaptive features; Complementary information; Different frequency; Dual frequency; Dual-frequency polarization synthetic aperture radar image classification; Features fusions; Frequency-aware attention; Images classification; Synthetic aperture radar images; accuracy assessment; algorithm; image classification; remote sensing; satellite imagery; synthetic aperture radar; Image classification","Adaptive feature fusion; complementary information; dual-frequency polarization synthetic aperture radar (PolSAR) image classification; frequency-aware attention","Article","Final","","Scopus","2-s2.0-85125326277"
"Zhou F.; Yang J.; Sun G.; Zhang J.","Zhou, Fang (36761642900); Yang, Jun (56937536300); Sun, Guangcai (57208324136); Zhang, Jiajia (42862730900)","36761642900; 56937536300; 57208324136; 42862730900","A Real-Time Imaging Processing Method Based on Modified RMA with Sub-Aperture Images Fusion for Spaceborne Spotlight SAR","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324118","1905","1908","3","10.1109/IGARSS39084.2020.9324118","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101978489&doi=10.1109%2fIGARSS39084.2020.9324118&partnerID=40&md5=00b2eaf201a9f2d92b454eecaf886a43","The small satellite SAR has received increasing attention due to its flexibility and low cost. But limited by the data transmission technology, real-time transmission of a large amount of raw data generated by the spaceborne spotlight SAR can hardly be achieved. Meanwhile, the azimuth bandwidth of the spotlight mode is larger than the PRF, resulting in aliasing of the azimuth spectrum. Based on these, this paper proposes a real-time scheme for small satellite SAR with spotlight mode. The method can solve the problem of data transmission and eliminate spectrum overlap in Doppler domain by means of sub-aperture processing. The modified range migration algorithm (RMA) is used to perform range compression and range cell migration compensation (RCMC) on sub-aperture data. Then dechirp in the azimuth time domain is applied to obtain the low-resolution complex image focused in the range time-azimuth frequency domain. Finally, all theected onto a grid image with azimuth interval matching the azimuth full-resolution to complete image fusion. © 2020 IEEE.","Data transfer; Frequency domain analysis; Geology; Image fusion; Processing; Remote sensing; Small satellites; Space-based radar; Synthetic aperture radar; Time domain analysis; Transmissions; Data transmission technologies; Frequency domains; Full resolutions; Range cell migration; Range compression; Range migration algorithms; Real-time transmissions; Realtime imaging; Radar imaging","real-time scheme; real-time transmission; spaceborne spotlight SAR; sub-aperture","Conference paper","Final","","Scopus","2-s2.0-85101978489"
"Li S.; Li C.; Kang X.","Li, Shutao (7409240361); Li, Congyu (57222559098); Kang, Xudong (47061561600)","7409240361; 57222559098; 47061561600","Development status and future prospects of multi-source remote sensing image fusion; [多源遥感图像融合发展现状与未来展望]","2021","National Remote Sensing Bulletin","25","1","","148","166","18","10.11834/jrs.20210259","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103206673&doi=10.11834%2fjrs.20210259&partnerID=40&md5=e81a2ef315d87af0247f05a4769e0d87","The development of multispectral, hyperspectral, infrared, radar, and other sensing technologies in recent years has facilitated the use of remote sensing methods in precision agriculture, resource investigation, environmental monitoring, military defense, and other fields. Multi-source remote sensing images in the same scene can capture the same ground objects, while the dimensions of the observations are independent of each other. Therefore, the imaging scale, spatial resolution, time resolution, and target characteristics may be quite different in different observations. The information provided by massive multi-source remote sensing data is redundant, complementary, and cooperative. Multi-source remote sensing image fusion can utilize the complementary information obtained from different sources to achieve accurate and comprehensive Earth observations. Thus, it is one of the key technologies in remote sensing. From the perspective of data sources, this review summarizes the research status and future development trends of multi-source remote sensing image fusion. In the introduction, the importance of multi-source image fusion and the motivation of this review are illustrated briefly. The second section outlines the main sources and image characteristics of nine typical remote sensing data: panchromatic images, multispectral images, hyperspectral images, infrared images, nighttime light images, stereo images, video images, Synthetic Aperture Radar (SAR) images, and light detection and ranging (LiDAR) images. The typical applications of these multi-source data are also briefly concluded while introducing the characteristics of these multi-source remote sensing images separately. Moreover, the development trend of multi-source remote sensing image fusion is evaluated according to the number of publications. In the third section, latest studies on multi-source remote sensing image fusion are introduced in detail in the order of optical image fusion, optical and SAR image fusion, optical and LiDAR image fusion, and other types of remote sensing image fusion. The third section also puts forward some challenging problems in remote sensing image fusion. For example, the registration problem of multi-source images, the application problem of fusion in specific domain, and the representation of features during cross-modal fusion are all important problems that need to be solved urgently. In the conclusion section, this review summarizes the research status of the multi-source remote sensing image fusion field. This also section prospects the future development trend of multi-source remote sensing image fusion. First, the study of related fusion technologies for new types of remote sensing images will be a major future research. Second, the integration of data acquisition and image fusion techniques can reduce the difficulty and improve the performance of image fusion with the help of novel hardware designs. Therefore, multi-modal fusion-based computational imaging systems should be designed. Third, fusing multi-source images with other types of data, such as geographical, ground station, and web data, is an interesting research topic in addition to the fusion of remote sensing images. Finally, evaluating the performance of image fusion is an important problem. Image fusion aims to help better understand the land covers from different dimensions of Earth observations. Whether the fusion can help the understanding of the Earth is unclear. Therefore, the improvement in application performance, such as detection or classification accuracy, may be an important index compared with the enhancement in the quality of the fused image. © 2021, Science Press. All right reserved.","Agricultural robots; Data acquisition; Environmental technology; Geometrical optics; Image enhancement; Infrared imaging; Observatories; Optical radar; Radar imaging; Remote sensing; Spectroscopy; Stereo image processing; Synthetic aperture radar; Application performance; Classification accuracy; Computational imaging system; Environmental Monitoring; Image fusion techniques; Light detection and ranging; Nighttime light image; Synthetic aperture radar (SAR) images; Image fusion","Ground observation; Image fusion; Multi-modal; Remote sensing image","Article","Final","","Scopus","2-s2.0-85103206673"
"Gao Y.; Gao F.; Dong J.; Du Q.; Li H.-C.","Gao, Yunhao (57208258943); Gao, Feng (56415492400); Dong, Junyu (22634069200); Du, Qian (7202060063); Li, Heng-Chao (24174798500)","57208258943; 56415492400; 22634069200; 7202060063; 24174798500","Synthetic Aperture Radar Image Change Detection via Siamese Adaptive Fusion Network","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","10748","10760","12","10.1109/JSTARS.2021.3120381","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117815650&doi=10.1109%2fJSTARS.2021.3120381&partnerID=40&md5=e42b4bc3f4edfca528aaef7da9619884","Synthetic aperture radar (SAR) image change detection is a critical yet challenging task in the field of remote sensing image analysis. The task is nontrivial due to the following challenges: First, intrinsic speckle noise of SAR images inevitably degrades the neural network because of error gradient accumulation. Furthermore, the correlation among various levels or scales of feature maps is difficult to be achieved through summation or concatenation. Toward this end, we proposed a siamese adaptive fusion (AF) network for SAR image change detection. To be more specific, two-branch CNN is utilized to extract high-level semantic features of multitemporal SAR images. Besides, an AF module is designed to adaptively combine multiscale responses in convolutional layers. Therefore, the complementary information is exploited, and feature learning in change detection is further improved. Moreover, a correlation layer is designed to further explore the correlation between multitemporal images. Thereafter, robust feature representation is utilized for classification through a fully connected layer with softmax. Experimental results on four real SAR datasets demonstrate that the proposed method exhibits superior performance against several state-of-the-art methods. Our codes are available at https://github.com/summitgao/SAR_CD_SAFNet. © 2008-2012 IEEE.","Convolution; Deep neural networks; Image fusion; Optical radar; Radar imaging; Remote sensing; Semantics; Speckle; Synthetic aperture radar; Tracking radar; Adaptive fusion; Attention mechanisms; Change detection; Convolutional neural network; Correlation; Deep learning; Features extraction; Radar polarimetry; Siamese adaptive fusion network; Task analysis; image analysis; image classification; machine learning; remote sensing; synthetic aperture radar; Feature extraction","Attention mechanism; change detection; deep learning; siamese adaptive fusion network (SAFNet); synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117815650"
"Sikdar A.; Udupa S.; Sundaram S.; Sundararajan N.","Sikdar, Aniruddh (58029708200); Udupa, Sumanth (58030050100); Sundaram, Suresh (57225724677); Sundararajan, Narasimhan (7004407304)","58029708200; 58030050100; 57225724677; 7004407304","Fully Complex-valued Fully Convolutional Multi-feature Fusion Network(FC2MFN) for Building Segmentation of InSAR images","2022","Proceedings of the 2022 IEEE Symposium Series on Computational Intelligence, SSCI 2022","","","","581","587","6","10.1109/SSCI51031.2022.10022109","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147795284&doi=10.1109%2fSSCI51031.2022.10022109&partnerID=40&md5=de7188bb9a9e07545dae4d87f0136622","Building segmentation in high-resolution InSAR images is a challenging task that can be useful for large-scale surveillance. Although complex-valued deep learning networks perform better than their real-valued counterparts for complexvalued SAR data, phase information is not retained throughout the network, which causes a loss of information. This paper proposes a Fully Complex-valued, Fully Convolutional Multifeature Fusion Network (FC2MFN) for building semantic segmentation on InSAR images using a novel, fully complex-valued learning scheme. FC2 MFN learns multi-scale features, performs multi-feature fusion, and has a complex-valued output. For the particularity of complex-valued InSAR data, a new complexvalued pooling layer is proposed that compares complex numbers considering their magnitude and phase. This helps the network retain the phase information even through the pooling layer. Experimental results on the simulated InSAR dataset [1] show that FC2MFN achieves better results compared to other state-of theart methods in terms of segmentation performance and model complexity. © 2022 IEEE.","Complex networks; Convolution; Convolutional neural networks; Deep learning; Image fusion; Radar imaging; Semantic Web; Semantics; Synthetic aperture radar; Complex-valued; Convolutional neural network; Fully complex-valued convolutional neural network; High resolution; Interferometric synthetic aperture radar; Interferometric synthetic aperture radars; Multi-feature fusion; Phase information; Semantic segmentation; Synthetic aperture radar images; Semantic Segmentation","fully complex-valued convolutional neural network; interferometric synthetic aperture radar (InSAR); Semantic segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85147795284"
"Tang R.; Pu F.; Yang R.; Xu Z.; Xu X.","Tang, Rui (57226265014); Pu, Fangling (13408173100); Yang, Rui (57208294306); Xu, Zhaozhuo (57171068000); Xu, Xin (58044800200)","57226265014; 13408173100; 57208294306; 57171068000; 58044800200","Multi-Domain Fusion Graph Network for Semi-Supervised PolSAR Image Classification","2023","Remote Sensing","15","1","160","","","","10.3390/rs15010160","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145783533&doi=10.3390%2frs15010160&partnerID=40&md5=de00ca49d39fe38f729803317f27e085","The expensive acquisition of labeled data limits the practical use of supervised learning on polarimetric synthetic aperture radar (PolSAR) image analysis. Semi-supervised learning has attracted considerable attention as it can utilize few labeled data and very many unlabeled data. The scattering response of PolSAR data is strongly spatial distribution dependent, which provides rich information about land-cover properties. In this paper, we propose a semi-supervised learning method named multi-domain fusion graph network (MDFGN) to explore the multi-domain fused features including spatial domain and feature domain. Three major factors strengthen the proposed method for PolSAR image analysis. Firstly, we propose a novel sample selection criterion to select reliable unlabeled data for training set expansion. Multi-domain fusion graph is proposed to improve the feature diversity by extending the sample selection from the feature domain to the spatial-feature fusion domain. In this way, the selecting accuracy is improved. By few labeled data, very many accurate unlabeled data are obtained. Secondly, multi-model triplet encoder is proposed to achieve superior feature extraction. Equipped with triplet loss, limited training samples are fully utilized. For expanding training samples with different patch sizes, multiple models are obtained for the fused classification result acquisition. Thirdly, multi-level fusion strategy is proposed to apply different image patch sizes for different expanded training data and obtain the fused classification result. The experiments are conducted on Radarsat-2 and AIRSAR images. With few labeled samples (about 0.003–0.007%), the overall accuracy of the proposed method ranges between 94.78% and 99.24%, which demonstrates the proposed method’s robustness and excellence. © 2022 by the authors.","Deep learning; Feature extraction; Image analysis; Image classification; Image fusion; Learning systems; Polarimeters; Radar imaging; Sampling; Supervised learning; Synthetic aperture radar; Graph networks; Image-analysis; Labeled data; Multi-domain fusion graph network; Multi-domains; Polarimetric synthetic aperture radar  classification; Polarimetric synthetic aperture radars; Semi-supervised; Synthetic aperture radar images; Unlabeled data; Classification (of information)","multi-domain fusion graph network; polarimetric synthetic aperture radar (PolSAR) classification; semi-supervised","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145783533"
"Cheng F.; Fu Z.; Huang L.; Niu B.; Chen P.; Wang L.; Ji X.","Cheng, Feifei (57354791400); Fu, Zhitao (57199314812); Huang, Liang (55571335200); Niu, Baosheng (57781413300); Chen, Pengdi (57216808021); Wang, Leiguang (57196339250); Ji, Xinran (57777325300)","57354791400; 57199314812; 55571335200; 57781413300; 57216808021; 57196339250; 57777325300","Review of deep learning in optical and SAR image fusion; [深度学习在光学和 SAR 影像融合研究进展]","2022","National Remote Sensing Bulletin","26","9","","1744","1756","12","10.11834/jrs.20211136","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141914227&doi=10.11834%2fjrs.20211136&partnerID=40&md5=11de49e6c63e8984707b4defc5167053","Remote sensing image fusion, as the most challenging work in the field of image processing, has been an academic research hotspot. SAR has various features, such as all-weather service, and penetrating clouds. However, the image is difficult to interpret due to the speckle noise problem. In contrast, optical images can reflect the spectral information of ground objects, which are easy to interpret, but interference by clouds and fog can easily occur, resulting in information loss. The fusion of optical and SAR image data can realize the complementary information between different types of sensor imaging, which can facilitate the subsequent image analysis and interpretation. In this paper, the research status and future development trend of optical and SAR remote sensing fusion is reviewed. The introduction part presents the motivation of this paper by explaining the importance of image fusion. The second part outlines the classification of optical and SAR image fusion from traditional methods to deep learning methods. The third part presents the datasets in terms of optical and SAR images and explains each dataset in detail. The fourth part summarizes the difficulties and some challenging problems of optical and SAR image fusion and highlights the future trends in the field of optical and SAR image fusion. The future development trend of fusion has four main aspects, namely, datasets, time series image fusion, fusion evaluation system, and algorithm lightweight. First, having well-targeted and sufficient datasets is an important part of training excellent fusion models, and the production of data sets is the key to improving the applicability of future models. Second, time series optical images are being used as supplementary information to SAR data with the increasing amount of satellite time series data, thus improving the utilization of image fusion. Third, the evaluation of the image fusion performance is an essential topic. No unified evaluation metric is available for objectively and comprehensively evaluating fusion algorithms, and an evaluation metric system in the field of optical and SAR image fusion must be developed. Finally, the lightweight of deep learning algorithms is an important future research direction. This paper provides a reference for researchers in optical and SAR image fusion. © 2022 National Remote Sensing Bulletin. All rights reserved.","Deep learning; Geometrical optics; Image enhancement; Image fusion; Learning algorithms; Learning systems; Optical data processing; Optical remote sensing; Radar imaging; Time series; Academic research; Dataset; Deep learning; Development trends; Evaluation metrics; Images processing; Optical image; Remote sensing images; SAR Images; Times series; Synthetic aperture radar","datasets; deep learning; image fusion; optical image; remote sensing image; SAR","Review","Final","","Scopus","2-s2.0-85141914227"
"Zhu D.; Wang X.; Li G.; Zhang X.-P.","Zhu, Dong (57001841800); Wang, Xueqian (57829747800); Li, Gang (55547117794); Zhang, Xiao-Ping (35214025100)","57001841800; 57829747800; 55547117794; 35214025100","FUSION OF SPACEBORNE AND AIRBORNE SAR IMAGES USING SALIENCY AND FUZZY LOGIC FOR VESSEL DETECTION","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","4468","4471","3","10.1109/IGARSS47720.2021.9554207","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129860929&doi=10.1109%2fIGARSS47720.2021.9554207&partnerID=40&md5=2ff230f3f11e2793de125f5ad5da76a3","In the paper, we propose a new method based on multi-order superpixel-level saliency and fuzzy logic (MSSFL) to fuse spaceborne and airborne SAR images for vessel detection. First, we generate a new global regional contrast map (GRCM) by exploiting the multi-order superpixel-level saliency (MSS). In the generated GRCM, the vessel targets are well restored and the backgrounds are suppressed. Next, a new fuzzy logic approach is presented to fuse the MSS information provided by the GRCMs. This GRCM-based fuzzy fusion can further enhance the vessel target regions and filter out the inshore interference regions. Experimental results using Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR images show that the proposed MSSFL method yields higher target-to-cluster ratio (TCR) of fused images and improved detection performance compared with the commonly utilized image fusion approaches. © 2021 IEEE","Aircraft detection; Antennas; Computer circuits; Fuzzy filters; Fuzzy logic; Image enhancement; Radar imaging; Satellites; Space-based radar; Superpixels; Synthetic aperture radar; Unmanned aerial vehicles (UAV); Airborne SAR; Fuzzy-Logic; Multi-ordering; Saliency; SAR Images; Spaceborne and airborne SAR image fusion; Spaceborne SAR; Super pixels; Targets detection; Vessel target detection; Image fusion","Fuzzy logic; saliency; spaceborne and airborne SAR image fusion; vessel target detection","Conference paper","Final","","Scopus","2-s2.0-85129860929"
"Bandi S.R.; Anbarasan M.; Sheela D.","Bandi, Sudheer Reddy (56924993300); Anbarasan, M. (56925264400); Sheela, D. (46161506500)","56924993300; 56925264400; 46161506500","FUSION OF SAR AND OPTICAL IMAGES USING PIXEL-BASED CNN","2022","Neural Network World","32","4","","197","213","16","10.14311/NNW.2022.32.012","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148038877&doi=10.14311%2fNNW.2022.32.012&partnerID=40&md5=4934f23f2263cd93dfc470f03f7901f5","Sensors of different wavelengths in remote sensing field capture data. Each and every sensor has its own capabilities and limitations. Synthetic aperture radar (SAR) collects data that has a high spatial and radiometric resolution. The optical remote sensors capture images with good spectral information. Fused images from these sensors will have high information when implemented with a better algorithm resulting in the proper collection of data to predict weather forecasting, soil exploration, and crop classification. This work encompasses a fusion of optical and radar data of Sentinel series satellites using a deep learning-based convolutional neural network (CNN). The three-fold work of the image fusion approach is performed in CNN as layered architecture covering the image transform in the convolutional layer, followed by the activity level measurement in the max pooling layer. Finally, the decision-making is performed in the fully connected layer. The objective of the work is to show that the proposed deep learning-based CNN fusion approach overcomes some of the difficulties in the traditional image fusion approaches. To show the performance of the CNN-based image fusion, a good number of image quality assessment metrics are analyzed. The consequences demonstrate that the integration of spatial and spectral information is numerically evident in the output image and has high robustness. Finally, the objective assessment results outperform the state-of-the-art fusion methodologies. ©CTU FTS 2022.","Classification (of information); Convolution; Convolutional neural networks; Decision making; Deep learning; Geometrical optics; Image fusion; Optical remote sensing; Radar imaging; Space-based radar; Weather forecasting; Convolutional neural network; Deep learning; Image fusion approach; Optical data; Optical image; Quality metrices; Remote-sensing; Sensing fields; Spectral information; Synthetic aperture radar images; Synthetic aperture radar","deep learning; image fusion; optical data; quality metrics; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85148038877"
"Zhu W.; Dai Z.; Gu H.; Zhu X.","Zhu, Wenbin (57217417269); Dai, Zheng (57188967425); Gu, Hong (7402683256); Zhu, Xiaochun (55696719000)","57217417269; 57188967425; 7402683256; 55696719000","Water extraction method based on multi‐texture feature fusion of synthetic aperture radar images","2021","Sensors","21","14","4945","","","","10.3390/s21144945","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110769408&doi=10.3390%2fs21144945&partnerID=40&md5=73989ed6254e5fac8a35b990aaebeeea","Lakes play an important role in the water ecosystem on earth, and are vulnerable to climate change and human activities. Thus, the detection of water quality changes is of great significance for ecosystem assessment, disaster warning and water conservancy projects. In this paper, the dynamic changes of the Poyang Lake are monitored by Synthetic Aperture Radar (SAR). In order to extract water from SAR images to monitor water change, a water extraction algorithm composed of texture feature extraction, feature fusion and target segmentation was proposed. Firstly, the fractal dimension and lacunarity were calculated to construct the texture feature set of a water object. Then, an iterated function system (IFS) was constructed to fuse texture features into composite feature vectors. Finally, lake water was segmented by the multifractal spectrum method. Experimental results showed that the proposed algorithm accurately extracted water targets from SAR images of different regions and different imaging modes. Compared with common algorithms such as fuzzy C‐means (FCM), the accuracy of the proposed algorithm is significantly improved, with an accuracy of over 98%. Moreover, the proposed algorithm can accurately segment complex coastlines with mountain shadow interference. In addition, the dynamic analysis of the changes of the water area of the Poyang Lake Basin was carried out with the local hydrological data. It showed that the extracted results of the algorithm in this paper are a good match with the hydrological data. This study provides an accurate monitoring method for lake water under complex backgrounds. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Algorithms; Ecosystem; Humans; Lakes; Radar; Water; Climate change; Earth (planet); Ecosystems; Fractal dimension; Fuzzy sets; Hydrogeology; Image fusion; Image segmentation; Image texture; Lakes; Radar imaging; Synthetic aperture radar; Textures; Water management; Water quality; water; Complex background; Ecosystem assessment; Iterated function system; Multi-fractal spectrum; Target segmentation; Texture feature extraction; Water conservancy projects; Water extraction methods; algorithm; ecosystem; human; lake; telecommunication; Extraction","Fractional dimension; Synthetic aperture radar; Texture feature; Water extraction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110769408"
"Chen M.; Wang X.; Yu Y.; Yuan X.","Chen, Mingrui (57431980600); Wang, Xiaoqing (56048910600); Yu, Yin (57214104851); Yuan, Xinzhe (15844682100)","57431980600; 56048910600; 57214104851; 15844682100","Enhancement algorithm for separability of inland water body in synthetic aperture radar image via sparse representation and image fusion","2022","International Journal of Remote Sensing","43","1","","167","195","28","10.1080/01431161.2021.2006355","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123699564&doi=10.1080%2f01431161.2021.2006355&partnerID=40&md5=ee9e2d5af6ee61bc72f6c481a3ee6cff","In synthetic aperture radar (SAR) images, inland water bodies generally appear as dark textures, but due to the interference of thermal noise, speckle noise, and azimuth blur, inland water bodies can be easily confused with weakly scattered areas such as roads, wetlands, and sand. Most of the existing denoising methods have been proposed for removing speckle noise. However, the presence of thermal noise and azimuth blur still makes it difficult to distinguish inland water bodies. The denoising method based on the Doppler spectrum can effectively remove thermal noise, speckle noise, and azimuth blur, but it reduces the resolution, which blurs the contour of the water body, and it is difficult to apply to the detection and processing of small inland water bodies. Fusing a noisy high-resolution image with a low-resolution image with less noise makes it possible to remove the speckle noise and thermal noise of the SAR image while maintaining the edge texture of the water body. Based on the idea of image fusion, sparse representation is used to fuse the original noisy SAR image (HR) and the low-precision image (LR) processed by the denoising algorithm to maximise the use of all the information in HR and LR. According to this idea, an enhancement algorithm for separability of inland water body based on sparse representation is proposed. The experimental results show that the proposed model significantly improves the separability of inland water bodies in SAR images. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Image enhancement; Image fusion; Image texture; Radar imaging; Speckle; Surface waters; Textures; Denoising methods; Enhancement algorithms; Inland waters; Noise speckle; Sparse images; Sparse representation; Speckle noise; Synthetic aperture radar images; Waterbodies; —synthetic aperture radar; algorithm; experimental study; image analysis; image processing; remote sensing; satellite imagery; speckle; synthetic aperture radar; texture; Synthetic aperture radar","image fusion; sparse representation; —SAR","Article","Final","","Scopus","2-s2.0-85123699564"
"Xu Z.; Zhu J.; Geng J.; Deng X.; Jiang W.","Xu, Zhe (57225997082); Zhu, Jinbiao (57262745400); Geng, Jie (57200589477); Deng, Xinyang (57219777842); Jiang, Wen (36006357300)","57225997082; 57262745400; 57200589477; 57219777842; 36006357300","TRIPLET ATTENTION FEATURE FUSION NETWORK FOR SAR AND OPTICAL IMAGE LAND COVER CLASSIFICATION","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","4256","4259","3","10.1109/IGARSS47720.2021.9555126","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129783499&doi=10.1109%2fIGARSS47720.2021.9555126&partnerID=40&md5=fce321b87005a8d467a949e7df6d1acd","With recent advances in remote sensing, abundant multimodal data are available for applications. However, considering the redundancy and the huge domain differences among multimodal data, how to effectively integrate these data is becoming important and challenging. In this paper, we proposed a triplet attention feature fusion network (TAFFN) for SAR and optical image fusion classification. Specifically, spatial attention module and spectral attention module based on self-attention mechanism are developed to extract spatial and spectral long-range information from the SAR image and optical image respectively, at the same time, cross-attention mechanism is proposed to capture the long-range interactive representation. Triplet attentions are concatenated to further integrate the complementary information of SAR and optical images. Experiments on a SAR and optical multimodal dataset demonstrate that the proposed method can achieve the state-of-the-arts performance.  © 2021 IEEE.","Geometrical optics; Image classification; Image fusion; Remote sensing; Synthetic aperture radar; Attention mechanisms; Domain differences; Features fusions; Fusion classification; Land cover classification; Multi-modal data; Optical image; Remote-sensing; SAR Images; Spatial attention; Radar imaging","attention mechanism; Feature fusion; land cover classification; SAR image","Conference paper","Final","","Scopus","2-s2.0-85129783499"
"Yan B.; Kong Y.","Yan, Biyuan (57214936311); Kong, Yingying (35186206400)","57214936311; 35186206400","A Fusion Method of SAR Image and Optical Image Based on NSCT and Gram-Schmidt Transform","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323158","2332","2335","3","10.1109/IGARSS39084.2020.9323158","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101968678&doi=10.1109%2fIGARSS39084.2020.9323158&partnerID=40&md5=52e12ec26f7a1c462840d941013bf895","The purpose of remote sensing image fusion is to synthesize the characteristics of multi-source remote sensing images and generate a composite image with new spatial, spectral and temporal features. The Gram-Schmidt (GS) transform can better improve the spatial features and maintain the spectral characteristics of the original images to a large extent. However, for the fusion of Synthetic aperture radar (SAR) image and optical image, there are still obvious spectral distortion and detail blurring. In this paper, the GS transform method is improved with non-subsampled contourlet transform (NSCT) which is used to obtain a high-resolution image that contains both spectral information and SAR image detail information. The results show that the spectral information of the fusion image is well preserved and the indicators are better than other methods. © 2020 IEEE.","Geology; Geometrical optics; Image enhancement; Image fusion; Remote sensing; Synthetic aperture radar; Gram-Schmidt transform; High resolution image; Non subsampled contourlet transform (NSCT); Remote sensing images; Spectral characteristics; Spectral distortions; Spectral information; Synthetic aperture radar (SAR) images; Radar imaging","Gram-Schmidt transform; image fusion; NSCT; optical image; SAR","Conference paper","Final","","Scopus","2-s2.0-85101968678"
"Liu H.; Ye Y.; Zhang J.; Yang C.; Zhao Y.","Liu, Huiyu (57219030322); Ye, Yuanxin (36683803000); Zhang, Jiacheng (57196389672); Yang, Chao (56990497100); Zhao, Yangang (57937826800)","57219030322; 36683803000; 57196389672; 56990497100; 57937826800","Comparative Analysis of Pixel Level Fusion Algorithms in High Resolution SAR and Optical Image Fusion","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2829","2832","3","10.1109/IGARSS46834.2022.9883331","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140396172&doi=10.1109%2fIGARSS46834.2022.9883331&partnerID=40&md5=ba4bfb28471a6507c42344c23b2e115a","Fusion of Synthetic aperture radar (SAR) and optical images is a significant topic in the field of remote sensing. As a typical category of image fusion methods, pixel level image fusion algorithms have been widely used in SAR-optical image fusion to integrate their complementary information and facilitate the subsequent interpretation and application. The effectiveness of these methods has been demonstrated in different literatures based on the experiment carried on specific, individual datasets, which make a comprehensive comparison of these algorithms difficult to achieve. This paper builds a sub-meter SAR and optical image dataset covering different types of scenes, the performance of 11 pixel level image methods is then investigated based on qualitative and quantitative analysis. Result shows the gradient pyramid (GP) achieve a high quality fusion when dealing with Optical-SAR image fusion task of residents, the non subsampled contourlet transform (NSCT) performs best when fusing images containing farmland and mountains. © 2022 IEEE.","Contourlet transform; Geometrical optics; Image analysis; Image enhancement; Optical remote sensing; Pixels; Radar imaging; Synthetic aperture radar; Comparative analyzes; Fusion algorithms; High resolution synthetic aperture radar images; Image fusion algorithms; Image fusion methods; Optical image; Pixel level fusion; Pixel-level image fusion; Remote-sensing; Synthetic aperture radar images; Image fusion","image fusion; optical image; SAR","Conference paper","Final","","Scopus","2-s2.0-85140396172"
"Liu Y.; Yang X.; Bao N.; Gu X.","Liu, Yanhui (57212383733); Yang, Xiaoyu (57226329383); Bao, Nisha (36448853500); Gu, Xiaowei (8724010600)","57212383733; 57226329383; 36448853500; 8724010600","Estimating biomass of reclaimed vegetation in prairie mining area: Inversion method based on Worldview-3 and Sentinel-1 SAR data; [基于Worldview-3与Sentinel-1 SAR数据的草原矿区复垦植被生物量反演方法研究]","2021","Earth Science Frontiers","28","4","","219","228","9","10.13745/j.esf.sf.2020.10.10","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111218624&doi=10.13745%2fj.esf.sf.2020.10.10&partnerID=40&md5=a716395a23200ffa95782e9edf44fd11","Quantitative inversion of reclaimed vegetation biomass in prairie mining area based on the remote sensing technology is the basis of dynamic monitoring and evaluation of mining ecological environment. In this research, focussing on the reclaimed vegetation in the grassland open-pit coal mine in Inner Mongolia, we combine the advantages of optical and radar remote sensing to explore the inversion method for biomass estimation based on Worldview-3 and Sentinel-1 SAR data. The principal component-wavelet transform algorithm was selected for data fusion. We revealed the correlation between parameters such as band reflectivity, vegetation index, backscatter coefficient or texture feature and biomass, established multivariate biomass inversion models, and analyzed the spatial uncertainty of different biomass models. The results are as follows: (1) After image fusion using W-PCA method, both data entropy and average gradient of the fusion data were significantly improved, and the fused 8th band (NIR2) had the highest correlation coefficient, the lowest spectral distortion, and the highest spectral fidelity. (2) Correlation analysis revealed a significant positive correlation between biomass and EVI, NDVI, VH polarization scattering coefficient, VH mean texture or the 8th band after fusion. Compared with a single variable, using the joint variables, NDVI of WV-3 and VH mean texture of Sentinel-1, it achieved the highest model accuracy (R2=0.8340, RMSE=16.4646 g/m2, Ac=81.52%), while the 8th band after fusion gave the highest verification accuracy (R2=0.7983, RMSE=22.8283 g/m2, Ac=74.64%). (3) According to the residual uncertainty analysis of different models, the Sentinel-1 variables are more prone to overestimation and saturation, whereas the joint variables can achieve complementary advantages. Using fusion data significantly improved the biomass overestimation for biomass below 40 g/m2 and saturation for biomass greater than 100 g/m2, reducing the model uncertainty by 2.42-9.68 g/m2 on average. It can be seen that the combination of optical and microwave cooperative remote sensing can effectively improve the estimation accuracy of vegetation biomass, thereby providing effective data support for fine monitoring of reclaimed vegetation in mining areas. © 2021, Editorial Office of Earth Science Frontiers. All right reserved.","Biomass; Coal industry; Coal mines; Image enhancement; Image fusion; Open pit mining; Reclamation; Remote sensing; Synthetic aperture radar; Textures; Vegetation; Wavelet transforms; Backscatter coefficients; Correlation between parameters; Correlation coefficient; Ecological environments; Polarization scattering; Positive correlations; Remote sensing technology; Wavelet transform algorithms; Uncertainty analysis","Biomass inversion; Data fusion; Prairie mining area; Reclaimed vegetation; Sentinel-1 SAR; Worldview-3","Article","Final","","Scopus","2-s2.0-85111218624"
"Zhang A.; Jia L.; Wang J.; Wang C.","Zhang, Anjun (57657384800); Jia, Lu (55541053400); Wang, Jun (57221358111); Wang, Chuanjian (58068877800)","57657384800; 55541053400; 57221358111; 58068877800","SAR Image Classification Using Gated Channel Attention Based Convolutional Neural Network","2023","Remote Sensing","15","2","362","","","","10.3390/rs15020362","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146425590&doi=10.3390%2frs15020362&partnerID=40&md5=fb67212c80d0c81df180d32cc2440d05","Algorithms combining CNN (Convolutional Neural Network) and super-pixel based smoothing have been proposed in recent years for Synthetic Aperture Radar (SAR) image classification. However, the smoothing may lead to the damage of details. To solve this problem the feature fusion strategy is utilized, and a novel adaptive fusion module named Gated Channel Attention (GCA) is designed in this paper. In this module, the relevance between channels is embedded into the conventional gated attention module to emphasize the variation in contribution on classification results between channels of feature-maps, which is not well considered by the conventional gated attention module. A GCA-CNN network is then constructed for SAR image classification. In this network, feature-maps corresponding to the original image and the smoothed image are extracted, respectively, by feature-extraction layers and adaptively fused. The fused features are used to obtain the results. Classification can be performed by the GCA-CNN in an end-to-end way. By the adaptive feature fusion in GCA-CNN, the smoothing of misclassification and the detail keeping can be realized at the same time. Experiments have been performed on one elaborately designed synthetic image and three real world SAR images. The superiority of the GCA-CNN is demonstrated by comparing with the conventional algorithms and the relative state-of-the-art algorithms. © 2023 by the authors.","Classification (of information); Convolution; Convolutional neural networks; Image fusion; Radar imaging; Synthetic aperture radar; Adaptive feature fusion; Adaptive features; Attention mechanisms; Convolutional neural network; Detail keeping; Features fusions; Gated channel attention mechanism; Gated channels; Misclassifications; Smoothing of misclassification; Image classification","adaptive feature fusion; detail keeping; gated channel attention mechanism; smoothing of misclassification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146425590"
"Liu Y.; Wang Y.","Liu, Yutong (57391950300); Wang, Yumei (57391433900)","57391950300; 57391433900","Synthetic Aperture Radar Image Target Recognition Based on Improved Fusion of R-FCN and SRC","2021","ACM International Conference Proceeding Series","","","","53","60","7","10.1145/3494885.3494895","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122013091&doi=10.1145%2f3494885.3494895&partnerID=40&md5=f7a799b94a0e19253dba8c90f6b151ac","In order to solve the problem of inaccurate classification caused by enhanced recognition ability and coherent speckle noise in synthetic spatial aperture radar image target recognition, a synthetic aperture radar target recognition algorithm based on the fusion of improved R-FCN structure and SRC is proposed. The algorithm improves the R-FCN structure by optimizing the location-sensitive score map to improve the pooling area, in order to better obtain the target features, and combines the sparse representation classifier as the basic classifier to classify the categories. At the same time, the correct classification can be added to the original test sample through the decision criteria, which can increase the number of samples and enhance the recognition ability at the same time. Through the improvement of the structure of R-FCN, the effect of coherent speckle noise in SAR images can be suppressed based on the effective feature extraction of target features, so as to achieve better target detection. Experiments based on the MSTAR data set prove that the algorithm has better recognition capabilities. © 2021 ACM.","Automatic target recognition; Classification (of information); Feature extraction; Image classification; Image enhancement; Image fusion; Radar target recognition; Speckle; Synthetic aperture radar; Image target recognition; Improved R-FCN structure; Recognition abilities; Small samples; Sparse representation; Sparse representation classification; Speckle noise; Synthetic aperture radar images; Target feature; Target recognition; Radar imaging","improved R-FCN structure; small sample; sparse representation classification; synthetic aperture radar image; target recognition","Conference paper","Final","","Scopus","2-s2.0-85122013091"
"Ran D.; Zheng F.; Li Z.; Yin C.","Ran, Da (56459824300); Zheng, Fengjie (57219865295); Li, Zhiliang (57279896000); Yin, Canbin (35207724100)","56459824300; 57219865295; 57279896000; 35207724100","Multi-angle SAR image fusion algorithm based on optimal local image index","2020","ACM International Conference Proceeding Series","","","","49","53","4","10.1145/3421766.3421885","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095837374&doi=10.1145%2f3421766.3421885&partnerID=40&md5=e4e70ed75ec5d63ce5614cceed8ffbb5","In order to reduce the layover region in traditional single-angle synthetic aperture radar (SA-SAR) image, a multi-angle synthetic aperture radar (MA-SAR) image fusion algorithm based on optimal local image index is proposed. By utilizing every two images of N SA-SAR images obtained from different observation angles under certain combination criteria to obtain a candidate image set with C2N images, the proposed algorithm calculates the local image index of every pixel of each image in the candidate image set and selects the pixel with the optimal local image index to constitute the final MA-SAR fusion image. The MA-SAR fusion image can effectively reduce the layover region in SA-SAR image. The simulation results have verified the effectiveness of the proposed algorithm. © 2020 ACM.","Artificial intelligence; Image fusion; Manufacture; Pixels; Synthetic aperture radar; Fusion image; Image fusion algorithms; Image index; Image sets; Multi angle; Multi-Angle-SAR; Observation angle; Single angles; Radar imaging","Image Fusion; Multi-angle Synthetic Aperture Radar (MA-SAR); Optimal Local Image Index","Conference paper","Final","","Scopus","2-s2.0-85095837374"
"Mehdinia S.; Schumacher T.; Song X.; Wan E.","Mehdinia, Sina (57349519900); Schumacher, Thomas (57208170429); Song, Xubo (7402269482); Wan, Eric (7004344408)","57349519900; 57208170429; 7402269482; 7004344408","A pipeline for enhanced multimodal 2D imaging of concrete structures","2021","Materials and Structures/Materiaux et Constructions","54","6","228","","","","10.1617/s11527-021-01803-w","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119599667&doi=10.1617%2fs11527-021-01803-w&partnerID=40&md5=36d10601611d0532df2442c1bb884ee7","We present an imaging pipeline to achieve enhanced images of the interior of concrete from ground penetrating radar (GPR) and ultrasonic echo array (UEA) measurements. This work lays the foundation for an advanced yet practical imaging tool to assess concrete structures. Specifically, we propose an enhanced two-dimensional (2D) total focusing method (XTFM) to reconstruct images from raw GPR and UEA data. The proposed XTFM algorithm integrates total focusing method (TFM) and synthetic aperture focusing technique (SAFT) concepts to post-process large independent and interelement measurements from both modalities in a computationally efficient way. Furthermore, we introduce a novel 2D image fusion algorithm using wavelet multilevel decomposition and an NDT knowledge-based rule to fuse GPR and UEA images. We then compare our algorithm with conventional fusion algorithms such as averaging, maximum, and product. The results from three laboratory concrete reference specimens are evaluated in detail. The fused images are compared with each other as well as benchmarked with the original GPR and UEA images. The output image obtained from our proposed pipeline is an enhanced 2D image of the interior of concrete structures that eases interpretation by a human inspector as well as it has the potential to improve interpretation by computer vision and image analysis algorithms. © 2021, RILEM.","Concrete buildings; Concrete construction; Concretes; Focusing; Geological surveys; Geophysical prospecting; Image enhancement; Image fusion; Knowledge based systems; Nondestructive examination; Radar imaging; Radar measurement; Synthetic aperture radar; Synthetic apertures; Ultrasonic testing; Wavelet decomposition; Concrete structure; Condition assessments; Evaluation metrics; Ground Penetrating Radar; Image evaluation; Image evaluation metric; Synthetic aperture focusing techniques; Total focusing method; Ultrasonic echo; Ultrasonic echo array; Ground penetrating radar systems","Concrete structure; Condition assessment; Ground penetrating radar; Image evaluation metric; Image fusion; Non-destructive testing; Pipeline; Synthetic aperture focusing technique; Total focusing method; Ultrasonic echo array","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119599667"
"Zhang H.; Shen H.; Yuan Q.; Guan X.","Zhang, Hai (57192694132); Shen, Huanfeng (8359721100); Yuan, Qiangqiang (36635300800); Guan, Xiaobin (57191221261)","57192694132; 8359721100; 36635300800; 57191221261","Multispectral and SAR Image Fusion Based on Laplacian Pyramid and Sparse Representation","2022","Remote Sensing","14","4","870","","","","10.3390/rs14040870","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124700963&doi=10.3390%2frs14040870&partnerID=40&md5=b2620e2f3e8610763ac6ded42a6b016b","Complementary information from multi-sensors can be combined to improve the availabil-ity and reliability of stand-alone data. Typically, multispectral (MS) images contain plentiful spectral information of the Earth’s surface that is beneficial for identifying land cover types, while synthetic aperture radar (SAR) images can provide abundant information on the texture and structure of target objects. Therefore, this paper presents a fusion framework to integrate the information from MS and SAR images based on the Laplacian pyramid (LP) and sparse representation (SR) theory. LP is performed to decompose both the multispectral and SAR images into high-frequency components and low-frequency components, so that different processing strategies can be applied to multi-scale information. Low-frequency components are merged based on SR theory, whereas high-frequency components are combined based on a certain activity-level measurement, identifying salient features. Finally, LP reconstruction is performed to obtain the integrated image. We conduct experiments on several datasets to verify the effectiveness of the proposed method. Both visual interpretation and statistical analyses demonstrate that the proposed method strikes a satisfactory balance between spectral information preservation and the enhancement of spatial and textual characteristics. In addition, a further discussion regarding the adjustability property of the proposed method shows its flexibility for further application scenarios. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Image fusion; Laplace transforms; Radar imaging; Textures; Image quality assessment; Laplacian Pyramid; Laplacian pyramid representation; Multi sensor images; Multi-sensor image fusion; Multi-spectral; Sensor image fusion; Sparse representation; Spectral information; Synthetic aperture radar images; Synthetic aperture radar","Image quality assessment; Laplacian pyramid; Multi-sensor image fusion; Sparse representation; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124700963"
"Zhou G.; Zhang G.; Xue B.","Zhou, Gaoyu (57221524050); Zhang, Gong (35241577600); Xue, Biao (57221536929)","57221524050; 35241577600; 57221536929","A maximum-information-minimum-redundancy-based feature fusion framework for ship classification in moderate-resolution sar image","2021","Sensors (Switzerland)","21","2","519","1","13","12","10.3390/s21020519","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099373636&doi=10.3390%2fs21020519&partnerID=40&md5=b7c180dc9dc48aa30bb8e58fcb9cf089","High-resolution synthetic aperture radar (SAR) images are mostly used in the current field of ship classification, but in practical applications, moderate-resolution SAR images that can offer wider swath are more suitable for maritime surveillance. The ship targets in moderate-resolution SAR images occupy only a few pixels, and some of them show the shape of bright spots, which brings great difficulty for ship classification. To fully explore the deep-level feature representations of moderate-resolution SAR images and avoid the “dimension disaster”, we innovatively proposed a feature fusion framework based on the classification ability of individual features and the efficiency of overall information representation, called maximum-information-minimum-redundancy (MIMR). First, we applied the Filter method and Kernel Principal Component Analysis (KPCA) method to form two feature subsets representing the best classification ability and the highest information representation efficiency in linear space and nonlinear space. Second, the MIMR feature fusion method is adopted to assign different weights to feature vectors with different physical properties and discriminability. Comprehensive experiments on the open dataset OpenSARShip show that compared with traditional and emerging deep learning methods, the proposed method can effectively fuse non-redundant complementary feature subsets to improve the performance of ship classification in moderate-resolution SAR images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Disaster prevention; Efficiency; Image classification; Image enhancement; Image fusion; Learning systems; Radar imaging; Radar target recognition; Redundancy; Ships; Space-based radar; Synthetic aperture radar; Classification ability; Complementary features; Feature fusion method; Feature representation; High resolution synthetic aperture radar images; Information representation; Kernel principal component analyses (KPCA); Maritime surveillance; Classification (of information)","Feature fusion; Filter method; Kernel principal component analysis (KPCA); Maximum-information-minimum-redundancy (MIMR); Moderate-resolution SAR image; Ship classification","Letter","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099373636"
"Liang W.; Wu Y.; Li M.; Cao Y.; Hu X.","Liang, Wenkai (57192207410); Wu, Yan (7406895902); Li, Ming (56937290000); Cao, Yice (57201420994); Hu, Xin (57221683116)","57192207410; 7406895902; 56937290000; 57201420994; 57221683116","High‐resolution SAR image classification using multi‐scale deep feature fusion and covariance pooling manifold network","2021","Remote Sensing","13","2","328","1","30","29","10.3390/rs13020328","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099781846&doi=10.3390%2frs13020328&partnerID=40&md5=e59a28a5dc58840395592ff1e96960f0","The classification of high‐resolution (HR) synthetic aperture radar (SAR) images is of great importance for SAR scene interpretation and application. However, the presence of intricate spatial structural patterns and complex statistical nature makes SAR image classification a challenging task, especially in the case of limited labeled SAR data. This paper proposes a novel HR SAR image classification method, using a multi‐scale deep feature fusion network and covariance pooling manifold network (MFFN‐CPMN). MFFN‐CPMN combines the advantages of local spatial features and global statistical properties and considers the multi‐feature information fusion of SAR images in representation learning. First, we propose a Gabor‐filtering‐based multi‐scale feature fusion network (MFFN) to capture the spatial pattern and get the discriminative features of SAR images. The MFFN belongs to a deep convolutional neural network (CNN). To make full use of a large amount of unlabeled data, the weights of each layer of MFFN are optimized by unsupervised denoising dual‐sparse encoder. Moreover, the feature fusion strategy in MFFN can effectively exploit the com-plementary information between different levels and different scales. Second, we utilize a covari-ance pooling manifold network to extract further the global second‐order statistics of SAR images over the fusional feature maps. Finally, the obtained covariance descriptor is more distinct for various land covers. Experimental results on four HR SAR images demonstrate the effectiveness of the proposed method and achieve promising results over other related algorithms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Deep neural networks; Image classification; Image fusion; Synthetic aperture radar; Covariance descriptor; Discriminative features; Feature information; SAR image classifications; Scene interpretation; Statistical properties; Structural pattern; Synthetic aperture radar (SAR) images; Radar imaging","Covariance pooling manifold net-work; High‐resolution SAR image; Image classification; Multi‐scale feature fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099781846"
"Huang D.; Tang Y.; Wang Q.","Huang, Dengshan (55473741000); Tang, Yulin (57897181500); Wang, Qisheng (57897420300)","55473741000; 57897181500; 57897420300","An Image Fusion Method of SAR and Multispectral Images Based on Non-Subsampled Shearlet Transform and Activity Measure","2022","Sensors","22","18","7055","","","","10.3390/s22187055","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138393576&doi=10.3390%2fs22187055&partnerID=40&md5=dab8259f70845d4915888b05e5ac2538","Synthetic aperture radar (SAR) is an important remote sensing sensor whose application is becoming more and more extensive. Compared with traditional optical sensors, it is not easy to be disturbed by the external environment and has a strong penetration. Limited by its working principles, SAR images are not easily interpreted, and fusing SAR images with optical multispectral images is a good solution to improve the interpretability of SAR images. This paper presents a novel image fusion method based on non-subsampled shearlet transform and activity measure to fuse SAR images with multispectral images, whose aim is to improve the interpretation ability of SAR images easily obtained at any time, rather than producing a fused image containing more information, which is the pursuit of previous fusion methods. Three different sensors, together with different working frequencies, polarization modes and spatial resolution SAR datasets, are used to evaluate the proposed method. Both visual evaluation and statistical analysis are performed, the results show that satisfactory fusion results are achieved through the proposed method and the interpretation ability of SAR images is effectively improved compared with the previous methods. © 2022 by the authors.","Image enhancement; Image fusion; Optical remote sensing; Radar imaging; Activity measure; External environments; Image fusion methods; Image-based; Multispectral images; Optical-; Remote sensing sensors; Remote-sensing; Shearlet transforms; Synthetic aperture radar images; Synthetic aperture radar","activity measure; image fusion; multispectral images; remote sensing; SAR; shearlet transform","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138393576"
"Chu B.; Chen J.; Chen J.; Pei X.; Yang W.; Gao F.; Wang S.","Chu, Boce (55536888400); Chen, Jinyong (57191226304); Chen, Jie (57778069500); Pei, Xinyu (57928287000); Yang, Wei (57928135400); Gao, Feng (57200188704); Wang, Shicheng (57221278036)","55536888400; 57191226304; 57778069500; 57928287000; 57928135400; 57200188704; 57221278036","SDCAFNet: A Deep Convolutional Neural Network for Land-Cover Semantic Segmentation With the Fusion of PolSAR and Optical Images","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8928","8942","14","10.1109/JSTARS.2022.3213601","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139863645&doi=10.1109%2fJSTARS.2022.3213601&partnerID=40&md5=733567241780dbbdc8e0058fb6e148f9","Due to the different imaging mechanisms between optical and polarimetric synthetic aperture radar (PolSAR) images, determining how to effectively use such complementary information has become an interesting and challenging problem. Convolutional neural networks (CNNs) and other deep neural networks have achieved good experimental results in remote sensing land-cover semantic segmentation. However, the CNN convolution structure can extract only the features within the receptive field in the spatial dimension without focusing on the relationship between multiple channels; therefore, it is impossible to realize fusion and complementarity between multiple channels. In this article, we propose a novel spatial dense channel attention fusion network (SDCAFNet), which takes optical and PolSAR images as different inputs and completes feature fusion and semantic segmentation within a neural network. First, SDCAFNet uses a two-stream siamese CNN network to realize the preliminary feature coding of optical and PolSAR images. Then, a spatial dense channel attention module (SDCAM) is proposed. The channel activation values obtained at different positions are combined in the spatial dense matrix, which can describe the attention in the feature fusion process. Finally, we introduce the fused features into the symmetric skip-connection decoder composed of multiple symmetric decoder blocks to realize end-to-end land-cover semantic segmentation. Experimental results show that SDCAFNet can effectively learn the correlation between optical and PolSAR channels and has a better segmentation accuracy than other methods.  © 2008-2012 IEEE.","Convolution; Decoding; Deep neural networks; Geometrical optics; Image coding; Image fusion; Optical correlation; Optical remote sensing; Polarimeters; Semantic Segmentation; Semantics; Synthetic aperture radar; Channel attention; Decoding; Features extraction; Features fusions; Land cover; Land-cover semantic segmentation; Optical image; Optical imaging; Optical reflection; Polarimetric synthetic aperture radar; Polarimetric synthetic aperture radars; Semantic segmentation; Task analysis; artificial neural network; image analysis; land cover; segmentation; synthetic aperture radar; Adaptive optics","Channel attention; feature fusion; land-cover semantic segmentation; optical image; polarimetric synthetic aperture radar (PoLSAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139863645"
"Bao Z.; Shifaw E.; Liu G.; Deng C.; Sha J.; Li X.; Hanchiso T.; Yang W.; Gao X.; Eladawy A.","Bao, Zhongcong (57202424390); Shifaw, Eshetu (57202461712); Liu, Gengyu (57979662500); Deng, Chengbo (57954539200); Sha, Jinming (15047027800); Li, Xiaomei (35205670500); Hanchiso, Terefe (57202421323); Yang, Wuheng (57204659923); Gao, Xinrui (57979479300); Eladawy, Ahmed (55625801900)","57202424390; 57202461712; 57979662500; 57954539200; 15047027800; 35205670500; 57202421323; 57204659923; 57979479300; 55625801900","Suspected coastal reclamation areas detection by spatiotemporal fusion of landsat TM and Sentinel-1A: A case study of Fuzhou, Fujian, China","2022","Estuarine, Coastal and Shelf Science","279","","108148","","","","10.1016/j.ecss.2022.108148","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142509910&doi=10.1016%2fj.ecss.2022.108148&partnerID=40&md5=4e3ea2795b10831628d8bcf4b9e0daa0","Increasing coastal reclamation activities have been undertaken to alleviate the shortage of land resources, posing significant challenges to the coordination of coastal reclamation, economic development and environmental protection. Over the past few decades, coastal reclamation has occurred extensively in the coastal region of China, especially the east coast, while the value of coastal wetlands and ecosystems has only been well understood and appreciated in the last decade. More recently, there is an important new trend to restore parts of reclaimed land to coastal wetlands for environmental protection purposes, which requires reliable reclamation detection results as a foundation for policy. This study aimed to propose a reclamation detection framework based on the spatiotemporal fusion of Landsat TM and Sentinel-1A. To utilize the complementary information of optical and SAR images, the proposed framework models the common and proprietary information of both types of data separately in a sparse manner. Then, with more reliable and detailed information, reclamation areas can be distinguished by band math and threshold segmentation using the spatiotemporal fusion of Landsat TM and Sentinel-1A images. Finally, taking Fuzhou in eastern China as an example, we provided a detailed analysis of coastal reclamation. The results confirmed that the proposed algorithm was effective for rapidly detecting suspected locations of coastal reclamation and showed great potential for monitoring the changes occurring in the coastal zone. © 2022 Elsevier Ltd","China; Fujian; Fuzhou; economic development; environmental protection; Landsat thematic mapper; reclaimed land; satellite imagery; segmentation; Sentinel; synthetic aperture radar; threshold","Band math; Coastal reclamation; Image fusion; Landsat TM; Sentinel-1A","Article","Final","","Scopus","2-s2.0-85142509910"
"Karim Z.; van Zyl T.L.","Karim, Zainoolabadien (57216271012); van Zyl, Terence L. (26532116600)","57216271012; 26532116600","Deep/transfer learning with feature space ensemble networks (Featspaceensnets) and average ensemble networks (avgensnets) for change detection using dinsar sentinel-1 and optical sentinel-2 satellite data fusion","2021","Remote Sensing","13","21","4394","","","","10.3390/rs13214394","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118494381&doi=10.3390%2frs13214394&partnerID=40&md5=4a4401bf1aefe9471959b16ab18af8ee","Differential interferometric synthetic aperture radar (DInSAR), coherence, phase, and displacement are derived from processing SAR images to monitor geological phenomena and urban change. Previously, Sentinel-1 SAR data combined with Sentinel-2 optical imagery has improved classification accuracy in various domains. However, the fusing of Sentinel-1 DInSAR processed imagery with Sentinel-2 optical imagery has not been thoroughly investigated. Thus, we explored this fusion in urban change detection by creating a verified balanced binary classification dataset comprising 1440 blobs. Machine learning models using feature descriptors and non-deep learning classifiers, including a two-layer convolutional neural network (ConvNet2), were used as baselines. Transfer learning by feature extraction (TLFE) using various pre-trained models, deep learning from random initialization, and transfer learning by fine-tuning (TLFT) were all evaluated. We introduce a feature space ensemble family (FeatSpaceEnsNet), an average ensemble family (AvgEnsNet), and a hybrid ensemble family (HybridEnsNet) of TLFE neural networks. The FeatSpaceEnsNets combine TLFE features directly in the feature space using logistic regression. AvgEnsNets combine TLFEs at the decision level by aggregation. HybridEnsNets are a combination of FeatSpaceEnsNets and AvgEnsNets. Several FeatSpaceEnsNets, AvgEnsNets, and HybridEnsNets, comprising a heterogeneous mixture of different depth and architecture models, are defined and evaluated. We show that, in general, TLFE outperforms both TLFT and classic deep learning for the small dataset used and that larger ensembles of TLFE models do not always improve accuracy. The best performing ensemble is an AvgEnsNet (84.862%) comprised of a ResNet50, ResNeXt50, and EfficientNet B4. This was matched by a similarly composed FeatSpaceEnsNet with an F1 score of 0.001 and variance of 0.266 less. The best performing HybridEnsNet had an accuracy of 84.775%. All of the ensembles evaluated outperform the best performing single model, ResNet50 with TLFE (83.751%), except for AvgEnsNet 3, AvgEnsNet 6, and FeatSpaceEnsNet 5. Five of the seven similarly composed FeatSpaceEnsNets outperform the corresponding AvgEnsNet. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Convolutional neural networks; Deep learning; Earth (planet); Feature extraction; Image enhancement; Image fusion; Multilayer neural networks; Network layers; Radar imaging; Sensor data fusion; Space optics; Space-based radar; Synthetic aperture radar; Artificial intelligence, remote sensing; Change detection; Deep learning; Differential interferometric synthetic aperture radars; Earth observations; Features extraction; Remote-sensing; Space data; Space data science; Transfer learning; Remote sensing","Artificial intelligence, remote sensing; Change detection; Deep learning; DInSAR; Earth observation; Machine learning; Space data science; Transfer learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118494381"
"Metrikaityte G.; Visockiene J.S.; Papsys K.","Metrikaityte, Guste (57798962200); Visockiene, Jurate Suziedelyte (56548798600); Papsys, Kestutis (55383221400)","57798962200; 56548798600; 55383221400","Digital Mapping of Land Cover Changes Using the Fusion of SAR and MSI Satellite Data","2022","Land","11","7","1023","","","","10.3390/land11071023","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134038608&doi=10.3390%2fland11071023&partnerID=40&md5=325ae20ec3924f318c441f6d108ef2f8","The aim of this article is to choose the most appropriate method for identifying and managing land cover changes over time. These processes intensify due to human activities such as agriculture, urbanisation and deforestation. The study is based in the remote sensing field. The authors used four different methods of satellite image segmentation with different data: Synthetic Aperture Radar (SAR) Sentinel-1 data, Multispectral Imagery (MSI) Sentinel-2 images and a fusion of these data. The images were preprocessed under segmentation by special algorithms and the European Space Agency Sentinel Application Platform (ESA SNAP) toolbox. The analysis was performed in the western part of Lithuania, which is characterised by diverse land use. The techniques applied during the study were: the coherence of two SAR images; the method when SAR and MSI images are segmented separately and the results of segmentation are fused; the method when SAR and MSI data are fused before land cover segmentation; and an upgraded method of SAR and MSI data fusion by adding additional formulas and index images. The 2018 and 2019 results obtained for SAR image segmentation differ from the MSI segmentation results. Urban areas are poorly identified because of the similarity of spectre signatures, where urban areas overlap with classes such as nonvegetation and/or sandy territories. Therefore, it is necessary to include the field surveys in the calculations in order to improve the reliability and accuracy of the results. The authors are of the opinion that the calculation of the additional indexes may help to enhance the visibility of vegetation and urban area classes. These indexes, calculated based on two or more different bands of multispectral images, would help to improve the accuracy of the segmentation results. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","coherence; image fusion; land cover changes; LULC; MSI RGB; SAR; segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134038608"
"Pal Choudhury A.; Halder T.; Kumar Gayen R.; Misra Ray A.; Chakravarty D.","Pal Choudhury, Aritro (57547571700); Halder, Tamesh (57206167386); Kumar Gayen, Rintu (57547191400); Misra Ray, Arundhati (57547961100); Chakravarty, Debashish (7004526342)","57547571700; 57206167386; 57547191400; 57547961100; 7004526342","C-band and L-band AirSAR image fusion technique using anisotropic diffusion","2022","Materials Today: Proceedings","58","","","433","436","3","10.1016/j.matpr.2022.02.393","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126930324&doi=10.1016%2fj.matpr.2022.02.393&partnerID=40&md5=d038e3b4f348194e748b4602f9a3d7cd","Image fusion is the process of amalgamation of essential information from multiple images into a single image resulting in a higher resolution image. It is an essential step of image augmentation in satellite imagery. Various frequency bands hold feature information related to a certain frequency range. Image fusion enables to superpose of co-registered images captured from different sensors to yield a better image with features from both the source images. Image fusion helps to get a more detailed single image than multiple images with different features. In our paper, we have presented an anisotropic diffusion-based fusion method for C-band and L-band AirSAR(Airborne Synthetic Aperture Radar) images. We found that the anisotropic diffusion method works better than old pansharpening techniques and works much faster and involves much fewer resources than Convolutional Neural Networks. © 2022","Diffusion; Metals; Optical anisotropy; Radar imaging; Satellite imagery; Synthetic aperture radar; Airborne synthetic aperture radars; Anisotropic Diffusion; Augmentation; C-bands; Feature information; High-resolution images; Image fusion techniques; Multiple image; Single images; Synthetic aperture radar images; Image fusion","AirSAR; Anisotropic diffusion; Augmentation; Frequency bands; Fusion","Article","Final","","Scopus","2-s2.0-85126930324"
"Radman A.; Mahdianpari M.; Brisco B.; Salehi B.; Mohammadimanesh F.","Radman, Ali (57299128600); Mahdianpari, Masoud (57190371939); Brisco, Brian (7003505161); Salehi, Bahram (36610817400); Mohammadimanesh, Fariba (56541784200)","57299128600; 57190371939; 7003505161; 36610817400; 56541784200","Dual-Branch Fusion of Convolutional Neural Network and Graph Convolutional Network for PolSAR Image Classification","2023","Remote Sensing","15","1","75","","","","10.3390/rs15010075","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145880225&doi=10.3390%2frs15010075&partnerID=40&md5=b13c1bd98de3963bacecebf9825a6542","Polarimetric synthetic aperture radar (PolSAR) images contain useful information, which can lead to extensive land cover interpretation and a variety of output products. In contrast to optical imagery, there are several challenges in extracting beneficial features from PolSAR data. Deep learning (DL) methods can provide solutions to address PolSAR feature extraction challenges. The convolutional neural networks (CNNs) and graph convolutional networks (GCNs) can drive PolSAR image characteristics by deploying kernel abilities in considering neighborhood (local) information and graphs in considering long-range similarities. A novel dual-branch fusion of CNN and mini-GCN is proposed in this study for PolSAR image classification. To fully utilize the PolSAR image capacity, different spatial-based and polarimetric-based features are incorporated into CNN and mini-GCN branches of the proposed model. The performance of the proposed method is verified by comparing the classification results to multiple state-of-the-art approaches on the airborne synthetic aperture radar (AIRSAR) dataset of Flevoland and San Francisco. The proposed approach showed 1.3% and 2.7% improvements in overall accuracy compared to conventional methods with these AIRSAR datasets. Meanwhile, it enhanced its one-branch version by 0.73% and 1.82%. Analyses over Flevoland data further indicated the effectiveness of the dual-branch model using varied training sampling ratios, leading to a promising overall accuracy of 99.9% with a 10% sampling ratio. © 2022 by the authors.","Classification (of information); Convolution; Deep learning; Digital storage; Graph neural networks; Image classification; Image fusion; Polarimeters; Radar imaging; Synthetic aperture radar; Airborne synthetic aperture radars; Convolutional networks; Convolutional neural network; Dual-branch fusion; Graph convolutional network; Images classification; Networks and graphs; Polarimetric synthetic aperture radars; Synthetic aperture radar images; Convolutional neural networks","classification; convolutional neural network (CNNs); dual-branch fusion; graph convolutional networks (GCNs); PolSAR","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145880225"
"Xiao J.; Xiao Y.; Sun X.; Huang J.; Wang H.","Xiao, Jianming (57694871000); Xiao, Yu (57220042868); Sun, Xiyan (23013241000); Huang, Jianhua (57207780145); Wang, Haokun (57220044682)","57694871000; 57220042868; 23013241000; 57207780145; 57220044682","Land cover classification of huixian wetland based on SAR and optical image fusion","2020","2020 3rd IEEE International Conference on Information Communication and Signal Processing, ICICSP 2020","","","9232103","316","320","4","10.1109/ICICSP50920.2020.9232103","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096577725&doi=10.1109%2fICICSP50920.2020.9232103&partnerID=40&md5=8af06df1854fddda57fad386eca8d3a2","In this paper, GF-1 WVF image and Sentinel-1 SAR image covering Huixian wetland area are used as data sources. The Gram Schmidt (GS) algorithm is first used to fuse GF-1 images and SAR images with different polarization modes, and then the Random Forest (RF) algorithm is used for supervised classification. Finally, the accuracy of classification results and the ability to extract information are compared. The experimental results show that the fusion image has obvious texture features and prominent karst landform features, compared with the GF-1 WVF image. Compared with the Sentinel-1 SAR image, the fusion image has obvious spectral features. Spectral differences between typical features are large; The overall classification accuracy of GF-1 images, GF-1 and Sentinel-1 VV polarization fusion images, and GF-1 and Sentinel-1 VH polarization fusion images have reached over 80%. The classification accuracy of GF-1 and Sentinel-1 VV polarization fusion images reaches 85.15%, which is better than GF-1 and Sentinel-1 VH polarization fusion images. The classification accuracy of water bodies in the VV polarization fusion image is better than that of GF-1. Bare ground has the highest classification accuracy among all fused images. © 2020 IEEE.","Classification (of information); Decision trees; Geometrical optics; Image classification; Image fusion; Polarization; Synthetic aperture radar; Textures; Wetlands; Accuracy of classifications; Classification accuracy; Extract informations; Land cover classification; Polarization modes; Spectral differences; Spectral feature; Supervised classification; Radar imaging","GF-1 image; Huixian wetland; Image fusion; Random forest; Sar image","Conference paper","Final","","Scopus","2-s2.0-85096577725"
"Liu J.; Chen H.; Wang Y.","Liu, Jinming (57326579900); Chen, Hao (57192534817); Wang, Yu (57767662100)","57326579900; 57192534817; 57767662100","Multi-source remote sensing image fusion for ship target detection and recognition","2021","Remote Sensing","13","23","4852","","","","10.3390/rs13234852","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120609463&doi=10.3390%2frs13234852&partnerID=40&md5=e97fba86f9a31735c87c71eefc52f5b6","The active recognition of interesting targets has been a vital issue for remote sensing. In this paper, a novel multi-source fusion method for ship target detection and recognition is proposed. By introducing synthetic aperture radar (SAR) sensor images, the proposed method solves the problem of precision degradation in optical remote sensing image target detection and recognition caused by the limit of illumination and weather conditions. The proposed method obtains port slice images containing ship targets by fusing optical data with SAR data. On this basis, spectral residual saliency and region growth method are used to detect ship targets in optical image, while SAR data are introduced to improve the accuracy of ship detection based on joint shape analysis and multi-feature classification. Finally, feature point matching, contour extraction and brightness saliency are used to detect the ship parts, and the ship target types are identified according to the voting results of part information. The proposed ship detection method obtained 91.43% recognition accuracy. The results showed that this paper provides an effective and efficient ship target detection and recognition method based on multi-source remote sensing images fusion. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Feature extraction; Geometrical optics; Image enhancement; Image fusion; Radar imaging; Radar target recognition; Remote sensing; Ships; Detection methods; Detection result voting; Multi-source remote sensing image; Multi-Sources; Radar data; Remote sensing images; Saliency; Ship detection; Ship targets; Target detection and recognition; Synthetic aperture radar","Detection result voting; Multi-source remote sensing image; Saliency; Target detection and recognition","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120609463"
"Xu Y.; Li D.; Tang J.","Xu, Yiming (57238603100); Li, Dongsheng (57281546800); Tang, Jushi (57238077300)","57238603100; 57281546800; 57238077300","Single frame shadow segmentation based on image enhancement for video SAR","2021","Proceedings of SPIE - The International Society for Optical Engineering","11913","","1191306","","","","10.1117/12.2604767","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113844326&doi=10.1117%2f12.2604767&partnerID=40&md5=17b40cee2eb8654b58442d9eb4a8e126","As Video synthetic aperture radar (SAR) technology has been developing rapidly in recent years, moving target detection and tracking has gradually become a research hotspot in the field of SAR. Since moving targets in Video SAR produce relatively clear shadows at their real locations, the shadow-based approach provides a new method for ground moving target detection. In this paper, a new approach based on image fusion enhancement is proposed to improve the extraction effect of target shadow in single frame Video SAR image, and the process of shadow segmentation is studied accordingly. First, we use Median Filter to denoise the image, and then use a variety of image enhancement methods to improve the contrast between shadows and background, including piecewise linear stretching, histogram specification, and S-curve enhancement, then use adaptive threshold segmentation algorithm to realize the separation of background and target shadow, finally use morphological processing method to further highlight the target shadow. The effectiveness of the proposed approach is verified on the Video SAR dataset published by Sandia Lab. © 2021 Copyright SPIE.","Image fusion; Image segmentation; Median filters; Pattern recognition; Piecewise linear techniques; Radar imaging; Synthetic aperture radar; Adaptive threshold segmentation; Fusion enhancement; Ground moving targets; Histogram specifications; Morphological processing; Moving target detection; Piecewise linear; Shadow segmentation; Image enhancement","Image Enhancement; Moving Target Detection; Shadow Segmentation; Video SAR","Conference paper","Final","","Scopus","2-s2.0-85113844326"
"Peng B.; Zhang W.; Hu Y.; Chu Q.; Li Q.","Peng, Bo (57204516848); Zhang, Wenyi (53880844700); Hu, Yuxin (56163045500); Chu, Qingwei (57203097369); Li, Qianqian (57223215662)","57204516848; 53880844700; 56163045500; 57203097369; 57223215662","LRFFNet: Large Receptive Field Feature Fusion Network for Semantic Segmentation of SAR Images in Building Areas","2022","Remote Sensing","14","24","6291","","","","10.3390/rs14246291","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144623546&doi=10.3390%2frs14246291&partnerID=40&md5=f2d212c9d82310b8090378e21363c225","There are limited studies on the semantic segmentation of high-resolution synthetic aperture radar (SAR) images in building areas due to speckle noise and geometric distortion. For this challenge, we propose the large receptive field feature fusion network (LRFFNet), which contains a feature extractor, a cascade feature pyramid module (CFP), a large receptive field channel attention module (LFCA), and an auxiliary branch. SAR images only contain single-channel information and have a low signal-to-noise ratio. Using only one level of features extracted by the feature extractor will result in poor segmentation results. Therefore, we design the CFP module; it can integrate different levels of features through multi-path connection. Due to the problem of geometric distortion in SAR images, the structural and semantic information is not obvious. In order to pick out feature channels that are useful for segmentation, we design the LFCA module, which can reassign the weight of channels through the channel attention mechanism with a large receptive field to help the network focus on more effective channels. SAR images do not include color information, and the identification of ground object categories is prone to errors, so we design the auxiliary branch. The branch uses the full convolution structure to optimize training results and reduces the phenomenon of recognizing objects outside the building area as buildings. Compared with state-of-the-art (SOTA) methods, our proposed network achieves higher scores in evaluation indicators and shows excellent competitiveness. © 2022 by the authors.","Buildings; Image fusion; Radar imaging; Semantic Segmentation; Signal to noise ratio; Synthetic aperture radar; Auxiliary branch; Building areas; Cascade feature pyramid module; Feature pyramid; Features fusions; Field features; Large receptive field channel attention module; Receptive fields; Semantic segmentation; Synthetic aperture radar images; Semantics","auxiliary branch; cascade feature pyramid module; large receptive field channel attention module; semantic segmentation; synthetic aperture radar images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144623546"
"Alebele Y.; Zhang X.; Wang W.; Yang G.; Yao X.; Zheng H.; Zhu Y.; Cao W.; Cheng T.","Alebele, Yeshanbele (57218677047); Zhang, Xue (57218681995); Wang, Wenhui (57212659763); Yang, Gaoxiang (57218681386); Yao, Xia (14022139100); Zheng, Hengbiao (57191078957); Zhu, Yan (8921604000); Cao, Weixing (55489902600); Cheng, Tao (57216739413)","57218677047; 57218681995; 57212659763; 57218681386; 14022139100; 57191078957; 8921604000; 55489902600; 57216739413","Estimation of canopy biomass components in paddy rice from combined optical and SAR data using multi-target gaussian regressor stacking","2020","Remote Sensing","12","16","2564","","","","10.3390/BIOMEDICINES8080286","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090022185&doi=10.3390%2fBIOMEDICINES8080286&partnerID=40&md5=95e98e081ca8e977c10249405b07ac93","Crop biomass is a critical variable to make sound decisions about field crop monitoring activities (fertilizers and irrigation) and crop productivity forecasts. More importantly, crop biomass estimations by components are essential for crop growth monitoring as the yield formation of crops results from the accumulation and transportation of substances between dierent organs. Retrieval of crop biomass from synthetic aperture radar SAR or optical imagery is of paramount importance for in-season monitoring of crop growth. A combination of optical and SAR imagery can compensate for their limitations and has exhibited comparative advantages in biomass estimation. Notably, the joint estimations of biophysical parameters might be more accurate than that of an individual parameter. Previous studies have attempted to use satellite imagery to estimate aboveground biomass, but the estimation of biomass for individual organs remains a challenge. Multi-target Gaussian process regressor stacking (MGPRS), as a new machine learning method, can be suitably utilized to estimate biomass components jointly from satellite imagery data, as the model does not require a large amount of data for training and can be adjusted to the required degrees of relationship exhibited by the given data. Thus, the aim of this study was to estimate the biomass of individual organs by using MGPRS in conjunction with optical (Sentinel-2A) and SAR (Sentinel-1A) imagery. Two hybrid indices, SAR and optical multiplication vegetation index (SOMVI) and SAR and optical dierence vegetation index (SODVI), have been constructed to examine their estimation performance. The hybrid vegetation indices were used as input for the MGPRS and single-target Gaussian process regression (SGPR). The accuracy of the estimation methods was analyzed by in situ measurements of aboveground biomass (AGB) and organ biomass conducted in 2018 and 2019 over the paddy rice fields of Xinghua in Jiangsu Province, China. The results showed that the combined indices (SOMVI and SODVI) performed better than those derived from either the optical or SAR data only. The best predictive accuracy was achieved by the MGPRS using SODVI as input (r2 = 0.84, RMSE = 0.4 kg/m2 for stem biomass; r2 = 0.87, RMSE = 0.16 kg/m2 for AGB). This was higher than using SOMVI as input for the MGPRS (r2 = 0.71, RMSE = 1.12 kg/m2 for stem biomass; r2 = 0.71, RMSE = 0.56 kg/m2 for AGB) or SGPR (r2 = 0.63, RMSE = 1.08 kg/m2 for stem biomass; r2 = 0.67, RMSE = 1.08 kg/m2 for AGB). Relatively, higher accuracy for leaf biomass was achieved using SOMVI (r2 = 0.83) than using SODVI (r2 = 0.73) as input for MGPRS. Our results demonstrate that the combined indices are eective by integrating SAR and optical imagery and MGPRS outperformed SGPR with the same input variable for estimating rice crop biomass. The presented workflow will improve the estimation of crops biomass components from satellite data for eective crop growth monitoring. © 2020 by the authors.","Biomass; Crops; Ecology; Forestry; Gaussian distribution; Gaussian noise (electronic); Learning systems; Satellite imagery; Space-based radar; Synthetic aperture radar; Vegetation; Above ground biomass; Biophysical parameters; Comparative advantage; Estimation performance; Gaussian process regression; In-situ measurement; Machine learning methods; Satellite imagery data; Radar imaging","Gaussian process; Hybrid indices; Image fusion; Sentinel-1; Sentinel-2; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090022185"
"Deng B.; Lv H.","Deng, Bao (57225110065); Lv, Hao (57216710689)","57225110065; 57216710689","Research on Image Fusion Methodof SAR and Visible Image Based on CNN","2022","Proceedings of 2022 IEEE 4th International Conference on Civil Aviation Safety and Information Technology, ICCASIT 2022","","","","1400","1403","3","10.1109/ICCASIT55263.2022.9987074","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146436485&doi=10.1109%2fICCASIT55263.2022.9987074&partnerID=40&md5=e26a63c5383f7282a7ad9a581c6acf8f","Synthetic aperture radar (SAR) imaging guidance has all-weather detection capability, and has been applied to aerial vehicles in the field of air combat. The research on sar/ visible image fusion technology has important theoretical and practical significance for improving the ability of target detection and precision guidance in complex combat environment. The imaging mechanism of visible light image and SAR image is different, and there are great differences in characteristics between images. Using image fusion technology to organically combine different images, complement their advantages and disadvantages, and can better interpret their scene information. Aiming at the difference feature extraction of SAR optical images, the modal classification task and ground object classification task are proposed, and the fine-tuning VGG convolution neural network model is designed, which uses less training time and achieves better classification results. For SAR optical image mapping, space-frequency consistent generation countermeasure network model framework (SFGAN) is proposed, which makes the texture details of the generated image more realistic and the contour features of roads and rivers more clear. Training from the original data, ablation experiments were designed to verify the effectiveness of the loss function. Experimental results show that the SFGAN method can learn the matching relationship from the small-scale SAR optical image pair data, so as to realize the mapping from SAR image to optical image.  © 2022 IEEE.","Air navigation; Antennas; Geometrical optics; Image enhancement; Image fusion; Mapping; Radar imaging; Textures; Unmanned aerial vehicles (UAV); Classification tasks; Image fusion technology; Image translation; Image-based; Imaging guidance; Optical image; SFGAN; Synthetic aperture radar images; Synthetic aperture radar imaging; Visible image; Synthetic aperture radar","CNN; Image Translation; SAR; SFGAN","Conference paper","Final","","Scopus","2-s2.0-85146436485"
"Singh P.; Shree R.; Diwakar M.","Singh, Prabhishek (57192421370); Shree, Raj (57189587624); Diwakar, Manoj (55253528500)","57192421370; 57189587624; 55253528500","A new SAR image despeckling using correlation based fusion and method noise thresholding","2021","Journal of King Saud University - Computer and Information Sciences","33","3","","313","328","15","10.1016/j.jksuci.2018.03.009","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044617018&doi=10.1016%2fj.jksuci.2018.03.009&partnerID=40&md5=c4e74aeef5f972696498d1f4802c824b","This paper presents a new technique for despeckling of Synthetic Aperture Radar (SAR) images using a local correlation based fusion of high-frequency coefficients in Discrete Wavelet Transform (DWT) with method noise thresholding. The decomposition level is decided by analyzing the texture of the input image at each level by calculating entropy. The core idea of the proposed technique lies in the selection of decomposition level in 2D-DWT based on entropy parameter and on the fusion of high-frequency coefficients. On decomposition, the low-frequency coefficients remain untouched and the high-frequency coefficients are thresholded using two different shrinkage rules. Therefore the Bayesian and Bivariate shrinkage methods are applied to the high-frequency coefficients. After performing two different thresholding methods, the improved high-frequency coefficients are fused using local correlation based strategy. The threshold value is calculated by correlation strategy. Later the correlation coefficient (CC) is evaluated between the two improved high-frequency coefficients. The CC is now compared with the threshold value for the fusion purpose. On the basis of defined fusion strategy, the average and maximum operation are applied to perform the fusion of high-frequency coefficients. The despeckling scheme is followed by method noise thresholding in order to preserve the fine details of the image. The performance of the proposed method is assessed using metrics such as Signal-to-Noise Ratio (SNR), Peak-Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM) and visual appearance of the despeckled image. The experimental results demonstrate the effectiveness of proposed work over prior works on SAR image despeckling. © 2018 The Authors","","Bayesian shrinkage; Bivariate shrinkage; Correlation coefficient; DWT; Entropy; Image fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85044617018"
"Sun Y.; Lei L.; Li X.; Tan X.; Kuang G.","Sun, Yuli (56244803700); Lei, Lin (56443377400); Li, Xiao (57211171394); Tan, Xiang (57211712883); Kuang, Gangyao (7006353231)","56244803700; 56443377400; 57211171394; 57211712883; 7006353231","Structure Consistency-Based Graph for Unsupervised Change Detection with Homogeneous and Heterogeneous Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3053571","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100824994&doi=10.1109%2fTGRS.2021.3053571&partnerID=40&md5=28c84509ad0d6b2376b51d99b47f79a1","Change detection (CD) of remote sensing (RS) images is one of the important problems in earth observation, which has been extensively studied in recent years. However, with the development of RS technology, the specific characteristics of remotely sensed images, including sensor characteristics, resolutions, noises, and distortions in imagery, make the CD more complex. In this article, we propose a structure consistency-based method for CD, which detects changes by comparing the structures of two images, rather than comparing the pixel values of images. Because the image structure is imaging modality-invariant and not sensitive to noise, illumination, and other interference factors, the proposed method can be applied to a variety of CD scenarios and has strong robustness. Structural comparison is realized by constructing and mapping an improved nonlocal patch-based graph (NLPG) to avoid the data leakage of two images. First, we demonstrate the effectiveness of the method in homogeneous and heterogeneous CD, which shows that the proposed method can be used as a unified CD framework. Second, we extend the method to the heterogeneous CD with multichannel synthetic aperture radar (SAR) image, which can provide a reference for future research as the heterogeneous CD with multichannel SAR is rarely studied. Third, through the decomposition and in-depth analysis of NLPG, we modify the graph construction process, structure difference calculation, and the difference image fusion to make it more robust and accurate. Experiments on six scenarios 12 data sets demonstrate the effectiveness of the proposed method.  © 1980-2012 IEEE.","Image enhancement; Image fusion; Remote sensing; Security of data; Synthetic aperture radar; Interference factor; Multichannel synthetic aperture radars (SAR); Remote sensing images; Remotely sensed images; Sensor characteristics; Structural comparison; Structure difference; Unsupervised change detection; detection method; graphical method; image analysis; remote sensing; satellite imagery; unsupervised classification; Radar imaging","Graph; Heterogeneous data; Nonlocal similarity; Structure consistency; Unsupervised change detection (CD)","Article","Final","","Scopus","2-s2.0-85100824994"
"Zhu J.; Pan J.; Jiang W.; Yue X.; Yin P.","Zhu, Jinbiao (57262745400); Pan, Jie (57226631313); Jiang, Wen (57205439228); Yue, Xijuan (24529391300); Yin, Pengyu (57694897800)","57262745400; 57226631313; 57205439228; 24529391300; 57694897800","SAR Image Fusion Classification Based on the Decision-Level Combination of Multi-Band Information","2022","Remote Sensing","14","9","2243","","","","10.3390/rs14092243","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130158485&doi=10.3390%2frs14092243&partnerID=40&md5=d6430102c31e618e04d260178fb51a4b","Synthetic aperture radar (SAR) is an active coherent microwave remote sensing system. SAR systems working in different bands have different imaging results for the same area, resulting in different advantages and limitations for SAR image classification. Therefore, to synthesize the classification information of SAR images into different bands, an SAR image fusion classification method based on the decision-level combination of multi-band information is proposed in this pa-per. Within the proposed method, the idea of Dempster–Shafer evidence theory is introduced to model the uncertainty of the classification result of each pixel and used to combine the classification results of multiple band SAR images. The convolutional neural network is used to classify single-band SAR images. Calculate the belief entropy of each pixel to measure the uncertainty of single-band classification, and generate the basic probability assignment function. The idea of the term frequency-inverse document frequency in natural language processing is combined with the conflict coefficient to obtain the weight of different bands. Meanwhile, the neighborhood classification of each pixel in different band sensors is considered to obtain the total weight of each band sensor, generate weighted average BPA, and obtain the final ground object classification result after fusion. The validity of the proposed method is verified in two groups of multi-band SAR image classification experiments, and the proposed method has effectively improved the accuracy compared to the modified average approach. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Convolution; Convolutional neural networks; Image enhancement; Image fusion; Inverse problems; Natural language processing systems; Pixels; Radar imaging; Remote sensing; Synthetic aperture radar; Text processing; Classification methods; Classification results; Convolutional neural network; Decision levels; Fusion classification; Fusion classification method; Images classification; Multi band; Multi-band synthetic aperture radar; Synthetic aperture radar images; Image classification","convolutional neural network; fusion classification method; multi-band SAR","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130158485"
"Luo J.; Lv Y.; Guo J.","Luo, Junjie (58042190800); Lv, Yang (58042915000); Guo, Jiao (56161174200)","58042190800; 58042915000; 56161174200","Multi-temporal PolSAR Image Classification Using F-SAE-CNN","2022","3rd China International SAR Symposium, CISS 2022","","","","","","","10.1109/CISS57580.2022.9971318","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145604311&doi=10.1109%2fCISS57580.2022.9971318&partnerID=40&md5=faec20b77ac61d4cc2e5998b0d44aafd","Crop classification using polarimetric SAR data is one of the most important applications in Polarimetric Synthetic Aperture Radar (PolSAR) imagery. Obviously, for crop classification, multi-temporal PolSAR data can provide more information than single-temporal PolSAR data, but the processing method of the matching image data is relatively backward. Aiming at the high-dimensional data composed of multi-temporal PolSAR, this paper proposes a method to integrate the stacked auto-encoder network and convolutional neural network, making full use of the dimension reduction advantages of the stacked auto-encoder network and the superior classification performance of the convolutional neural network. By constructing a fusion network, the multi-temporal PolSAR images can be processed once, the classification accuracy can be improved, and the processing steps can be simplified. The experimental results show that, compared with the traditional Stacked Auto-encoder and Convolutional Neural Network (SAE-CNN) classification method, the multitemporal PolSAR image classification method based on Fusion of Stacked Auto-encoder and Convolutional Neural Network (F-SAE-CNN) proposed in this paper has the highest classification accuracy, which effectively combines the advantages of the self-encoding network and the CNN network, and provides a new idea for PolSAR image classification work. © 2022 IEEE.","Classification (of information); Clustering algorithms; Convolution; Convolutional neural networks; Crops; Image enhancement; Image fusion; Polarimeters; Radar imaging; Synthetic aperture radar; Auto encoders; Convolutional neural network; Crop classification; Fusion of stacked auto-encoder and convolutional neural network; Images classification; Multi-temporal; Polarimetric synthetic aperture radar; Polarimetric synthetic aperture radar data; Polarimetric synthetic aperture radars; Synthetic aperture radar images; Image classification","Crop Classification; F-SAE-CNN; Multi-temporal; Polarimetric Synthetic Aperture Radar (PolSAR)","Conference paper","Final","","Scopus","2-s2.0-85145604311"
"Li X.; Jing D.; Li Y.; Guo L.; Han L.; Xu Q.; Xing M.; Hu Y.","Li, Xinchen (57326684000); Jing, Dan (57222467579); Li, Yachao (16307232700); Guo, Liang (55468894500); Han, Liang (55634028400); Xu, Qing (14016919400); Xing, Mengdao (7005922869); Hu, Yihua (55695331800)","57326684000; 57222467579; 16307232700; 55468894500; 55634028400; 14016919400; 7005922869; 55695331800","Multi-Band and Polarization SAR Images Colorization Fusion","2022","Remote Sensing","14","16","4022","","","","10.3390/rs14164022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137771383&doi=10.3390%2frs14164022&partnerID=40&md5=2d388e02450c4d7516e6d455bc09663d","The image fusion of multi-band and multi-polarization synthetic aperture radar (SAR) images can improve the efficiency of band and polarization information processing. In this paper, we introduce a fusion method that simultaneously fuses multi-band and polarization SAR images. In the method, we first use non-subsampled shearlet transform (NSST) to fuse multi-band and polarization SAR images. The sub-band images decomposed from the NSST are fused by the coefficient of variation (CV) and phase consistency (PC) weighted fusion rules. Subsequently, we extract the band and polarization difference information from the multi-band and polarization SAR images. The fusion image is finally colorized according to the band and polarization differences. In the experiments, we used Ka and S-band multi-polarization SAR images to test the fusion performance. The experiment results prove that the proposed fused images not only preserve much valuable information but also can be interpreted easily. © 2022 by the authors.","Image enhancement; Polarization; Radar imaging; Synthetic aperture radar; Band fusion; Band polarization; Image colorizations; Multi band; Multi-polarization; Polarisation informations; Polarization difference; Polarization fusion; Shearlet transforms; Synthetic aperture radar images; Image fusion","band fusion; image fusion; polarization fusion; SAR image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137771383"
"Wang Z.; Cai C.; Deng M.; Zhang D.; Li Z.","Wang, Zhong (58030506200); Cai, Chenglin (35228662000); Deng, Mingjun (57193391692); Zhang, Dongbo (54890820300); Li, Zexian (57287518600)","58030506200; 35228662000; 57193391692; 54890820300; 57287518600","Rapid Subpixel Matching Method for Spaceborne Synthetic Aperture Radar Images","2022","Sensors and Materials","34","12","","4705","4715","10","10.18494/SAM3982","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145654376&doi=10.18494%2fSAM3982&partnerID=40&md5=57b25fbdee796f0c25c7c4bf0d555379","With the development of synthetic aperture radar (SAR) technology, SAR images are being widely applied, such as in SAR image fusion and transformation detection, where image matching is a key procedure. In this paper, a practical and rapid subpixel matching method is proposed for spaceborne SAR images. The subpixel SAR-oriented features from an accelerated segment test (SAR-OFAST) detection operator is employed to improve the detection accuracy of keypoints. By determining the overlapping area of master-slave images, the search range of matching is reduced, and the strategy of block matching is adopted to save computing resources. To verify the accuracy and reliability of the proposed matching method, several groups of SAR images obtained with an identical observation orientation and diverse observation angles were tested using the proposed and other methods. Experimental results demonstrate that the proposed method has a higher speed than other methods with similar performance in other indicators, making it more suitable for scenarios requiring high efficiency. © 2022 M Y U Scientific Publishing Division. All rights reserved.","Image fusion; Image matching; Motion compensation; Pixels; Radar imaging; Space-based radar; Block Matching; Detection accuracy; Keypoints; Matching methods; Oriented features; Overlapping area; Radar technology; Spaceborne synthetic aperture radars; Sub-pixels; Synthetic aperture radar images; Synthetic aperture radar","block matching; image matching; SAR images; subpixel","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145654376"
"Ning Y.; You Y.; Cao J.; Liu F.; Yan Q.; Zhang Y.","Ning, Yuanyong (57222241669); You, Yanan (54409614600); Cao, Jingyi (57210948716); Liu, Fang (57091791100); Yan, Qing (55307568700); Zhang, Yue (57839925000)","57222241669; 54409614600; 57210948716; 57091791100; 55307568700; 57839925000","FUSION DETECTION OF CLOSED WATER IN MEDIUM-LOW RESOLUTION REMOTE SENSING IMAGERY","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4027","4030","3","10.1109/IGARSS47720.2021.9553554","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126054343&doi=10.1109%2fIGARSS47720.2021.9553554&partnerID=40&md5=b537674d1f996c16fb1e1fa780654ec3","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance. © 2021 IEEE.","","Closed water detection; GAN; Image fusion","Conference paper","Final","","Scopus","2-s2.0-85126054343"
"Li W.; Xiao X.; Xiao P.; Wang H.; Xu F.","Li, Weisong (57339572400); Xiao, Xiayang (57609130000); Xiao, Penghao (57606464900); Wang, Haipeng (36603667200); Xu, Feng (9240823200)","57339572400; 57609130000; 57606464900; 36603667200; 9240823200","Change Detection in Multitemporal SAR Images Based on Slow Feature Analysis Combined With Improving Image Fusion Strategy","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","3008","3023","15","10.1109/JSTARS.2022.3166234","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128593766&doi=10.1109%2fJSTARS.2022.3166234&partnerID=40&md5=18b84c602c323030dca82fa3b04b71c3","Change detection in multitemporal synthetic aperture radar (SAR) images has been an important research content in the field of remote sensing for a long time. In this article, based on the slow feature analysis (SFA) theory and the nonsubsampled contourlet transform (NSCT) algorithm, we propose a novel unsupervised change detection method called NSCT nonlocal means (NSCT-NLM). The powerful extraction to the changed information of SFA and the superior information fusion of NSCT are jointly adopted in this method. The main framework consists of the following parts. First, SFA and the log-ratio operator are used to generate difference images (DIs) independently. Then, the NSCT is used to fuse two DIs into a new higher quality DI. The newly fused DI combines the complementary information of the two kinds of original DI. Therefore, the contrast of the changed regions and unchanged regions is greatly enhanced, as well as the changed details are preserved more completely. Furthermore, an NLM filtering algorithm is employed to suppress the strong speckles in the fused DI. We use the fuzzy C-means algorithm to generate the final binary change map. The experiments are carried out on two public datasets and three real-world SAR datasets from different scenarios. The results demonstrate that the proposed method has higher detection accuracy by comparing with the reference methods. © 2008-2012 IEEE.","Data mining; Extraction; Image analysis; Image denoising; Image enhancement; Image fusion; Radar imaging; Remote sensing; Signal detection; Synthetic aperture radar; Change detection algorithms; Difference images; Feature analysis; Features extraction; Nonsub-sampled contourlet transform; Nonsubsampled contourlet transforms; Radar polarimetry; Slow feature analyse; Synthetic aperture radar images; Unsupervised change detection; algorithm; data set; image processing; remote sensing; speckle; synthetic aperture radar; Feature extraction","Nonsubsampled contourlet transform (NSCT); slow feature analysis (SFA); synthetic aperture radar (SAR) images; unsupervised change detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128593766"
"Ren B.; Ma S.; Hou B.; Hong D.","Ren, Bo (57209552437); Ma, Shibin (57678820200); Hou, Biao (7102142690); Hong, Danfeng (56108179600)","57209552437; 57678820200; 7102142690; 56108179600","DUAL-STREAM HIGH RESOLUTION NETWORK FOR MULTI-SOURCE REMOTE SENSING IMAGE SEGMENTATION","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","3440","3443","3","10.1109/IGARSS47720.2021.9553947","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129891978&doi=10.1109%2fIGARSS47720.2021.9553947&partnerID=40&md5=1a28769012bfb51a05b4fa59b88af649","Recently, the image segmentation has been a significant research direction in the field of optical remote sensing data processing. However, due to the limitation of the optical imaging mechanism, traditional image segmentation methods are not efficient for processing the optical remote sensing images, especially influencing by the complex weather conditions. In order to ensure the classification performance, synthetic aperture radar (SAR) data are employed as complementary to the data procedure for enhancing the capability of land cover interpretation. Then a dual-stream high-resolution network (HRNet) is proposed to combine two types of heterogeneous data (SAR and optical image), and a multi-modal squeeze-and-excitation (SE) module is exploited to make feature maps fused. Experiments show that the proposed method has excellent performance on the remote sensing data acquired by GF2 and GF3 satellites. ©2021 IEEE","Data handling; Geometrical optics; Image fusion; Radar imaging; Remote sensing; Synthetic aperture radar; Heterogeneous data; Heterogeneous data fusion; High resolution; Images segmentations; Multi-modality; Multi-Sources; Optical remote sensing; Optical remote sensing data; Remote sensing images; Segmentation methods; Image segmentation","Heterogeneous Data Fusion; Images Segmentation; Multi-modalities","Conference paper","Final","","Scopus","2-s2.0-85129891978"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","Fusion and Classification of SAR and Optical Data Using Multi-Image Color Components with Differential Gradients","2023","Remote Sensing","15","1","274","","","","10.3390/rs15010274","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145980400&doi=10.3390%2frs15010274&partnerID=40&md5=d422379ceba3e153b30b896ad691c90e","This paper proposes a gradient-based data fusion and classification approach for Synthetic Aperture Radar (SAR) and optical image. This method is used to intuitively reflect the boundaries and edges of land cover classes present in the dataset. For the fusion of SAR and optical images, Sentinel 1A and Sentinel 2B data covering Central State Farm in Hissar (India) was used. The major agricultural crops grown in this area include paddy, maize, cotton, and pulses during kharif (summer) and wheat, sugarcane, mustard, gram, and peas during rabi (winter) seasons. The gradient method using a Sobel operator and color components for three directions (i.e., x, y, and z) are used for image fusion. To judge the quality of fused image, several fusion metrics are calculated. After obtaining the resultant fused image, gradient based classification methods, including Stochastic Gradient Descent Classifier, Stochastic Gradient Boosting Classifier, and Extreme Gradient Boosting Classifier, are used for the final classification. The classification accuracy is represented using overall classification accuracy and kappa value. A comparison of classification results indicates a better performance by the Extreme Gradient Boosting Classifier. © 2023 by the authors.","Classification (of information); Geometrical optics; Gradient methods; Image classification; Image fusion; Radar imaging; Stochastic systems; Synthetic aperture radar; Boosting classifiers; Color component; Extreme gradient boosting classifier; Gradient boosting; Gradient fusion; Stochastic gradient boosting; Stochastic gradient boosting classifier; Stochastic gradient descent; Stochastic gradient descent classifier; Crops","classification; Extreme Gradient Boosting Classifier; gradient fusion; Stochastic Gradient Boosting Classifier; Stochastic Gradient Descent Classifier","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145980400"
"Cai Z.; Kong Y.","Cai, Zheng (57845060600); Kong, Yingying (35186206400)","57845060600; 35186206400","Feature selection classifier for fully polarized SAR and optical images based on rotation forests","2022","IST 2022 - IEEE International Conference on Imaging Systems and Techniques, Proceedings","","","","","","","10.1109/IST55454.2022.9827663","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135879848&doi=10.1109%2fIST55454.2022.9827663&partnerID=40&md5=dbc79d952b29cd4b7817323d48325bfa","Synthetic aperture radar (SAR) has the unique advantage of not being affected by weather illumination conditions, but its interpretation is very difficult. Optical images, though readable, are greatly influenced by climate and light conditions. In order to solve the problems of single source image, such as single imaging mode and poor use scene, this paper proposes the feature-level-fusion classification of optical image and fully polarized SAR image. Considering that the effect of neural network is not ideal under the condition of small training samples, this paper aims to find a machine learning method to achieve high precision classification under the above condition. Due to the rich backscattering information of fully polarized SAR images, feature redundancy and irrelevance will inevitably be encountered in the process of feature-level-fusion classification. In order to solve this problem, we propose a rotation forest classifier combined with feature selection. The experimental results show that the proposed classifier provides significant improvements in overall accuracy (OA) when compared to SVM and random forest classifier, and the classification accuracy and operation efficiency of rotation forest classifier are improved significantly after feature selection. The OA of the proposed classifier reaches 0.96, and the operation time is reduced by 20.74% compared with the classifier without feature selection, validating the effectiveness of the methods.  © 2022 IEEE.","Classification (of information); Decision trees; Feature Selection; Geometrical optics; Image classification; Image fusion; Learning systems; Radar imaging; Remote sensing; Rotation; Support vector machines; Synthetic aperture radar; Feature sensitivity; Feature sensitivity analyse; Feature-level fusions; Feature-level-fusion classification; Features selection; Fusion classification; Optical image; Random forests; Rotation forests; Synthetic aperture radar images; Sensitivity analysis","feature sensitivity analysis; feature-level-fusion classification; random forest; rotation forest","Conference paper","Final","","Scopus","2-s2.0-85135879848"
"Huang J.; An D.; Luo Y.; Chen J.; Zhou Z.; Chen L.; Feng D.","Huang, Junnan (57721308500); An, Daoxiang (18433472600); Luo, Yuxiao (57192573746); Chen, Jingwei (57216442312); Zhou, Zhimin (7406100386); Chen, Leping (55658016300); Feng, Dong (57207008887)","57721308500; 18433472600; 57192573746; 57216442312; 7406100386; 55658016300; 57207008887","A Registration Method for Dual-Frequency, High-Spatial-Resolution SAR Images","2022","Remote Sensing","14","10","2509","","","","10.3390/rs14102509","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131138887&doi=10.3390%2frs14102509&partnerID=40&md5=64c8876903ac909466d00233009dd618","With the continuous development of synthetic-aperture-radar (SAR) technology, SAR-image data are becoming increasingly abundant. For the same scene, dual-frequency (high-frequency and low-frequency) SAR images can present different details and feature information. Image fusion of the two frequencies can combine the advantages of both, thus describing targets more comprehensively. Image registration is the key step of image fusion and determines the quality of fusion. Due to the complex geometric distortion and gray variance between dual-frequency SAR images with high resolution, it is difficult to realize accurate registration between the two. In order to solve this problem, this paper proposes a method to achieve accurate registration by combining edge features and gray information. Firstly, this paper applies the edge features of images and a registration algorithm based on fast Fourier transform (FFT) to realize rapid coarse registration. Then, combining a registration algorithm based on the enhanced correlation coefficient (ECC) with the concept of segmentation, the coarse-registration result is registered to achieve accurate registration. Finally, by processing the airborne L-band and Ku-band SAR data, the correctness, effectiveness, and practicability of the proposed method are verified, with a root mean square error (RMSE) of less than 2. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Fast Fourier transforms; Image fusion; Image registration; Mean square error; Radar imaging; Accurate registration; Coarse registration; Dual frequency; Edge features; High resolution synthetic aperture radar images; High spatial resolution; Images registration; Registration algorithms; Registration methods; Synthetic aperture radar images; Synthetic aperture radar","dual-frequency; edge feature; high-resolution SAR image; image registration","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131138887"
"Hughes L.H.; Schmitt M.","Hughes, Lloyd Haydn (57201113391); Schmitt, Michael (7401931279)","57201113391; 7401931279","COMPARATIVE EVALUATION OF DEEP LEARNING-BASED SAR-OPTICAL IMAGE MATCHING APPROACHES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","423","426","3","10.1109/IGARSS47720.2021.9553941","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129906163&doi=10.1109%2fIGARSS47720.2021.9553941&partnerID=40&md5=4fbf86d015cbbd89e69da58fa590157a","The automatic matching of corresponding pixels in SAR and optical remote sensing imagery has been an active field of research for many years. While early approaches were usually based on the measurement of image similarity by signal-based measures or hand-crafted image features, more recent matching techniques make use of deep learning. Since the different approaches proposed in the literature are usually trained and evaluated on specific, individual datasets, i.e. with unique input data and target label criteria, a direct comparison has not yet been possible. With this paper, we intend to close that gap by providing the first comparative evaluation of different state-of-the-art deep learning-based SAR-optical image matching approaches. ©2021 IEEE","Deep learning; Geometrical optics; Image fusion; Image matching; Remote sensing; Synthetic aperture radar; Active field; Automatic matching; Comparative evaluations; Deep learning; Image similarity; Measurements of; Optical image; Optical remote-sensing imagery; SAR Images; SAR remote sensing; Radar imaging","Data Fusion; Deep Learning; Image Matching; Optical Images; SAR Images","Conference paper","Final","","Scopus","2-s2.0-85129906163"
"Mahmud H.B.; Katiyar V.; Nagai M.","Mahmud, Husniyah Binti (57215431801); Katiyar, Vaibhav (55349816200); Nagai, Masahiko (36148030600)","57215431801; 55349816200; 36148030600","Improved Consistency of an Automated Multisatellite Method for Extracting Temporal Changes in Flood Extent","2021","Mathematical Problems in Engineering","2021","","6164161","","","","10.1155/2021/6164161","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122996712&doi=10.1155%2f2021%2f6164161&partnerID=40&md5=1dca99a2c399f12a259c9364d5027274","Malaysia is affected by floods almost every year. In this situation, high-frequency flood monitoring is crucial so that timely measures can be taken. However, the low revisit time of the satellites, as well as occlusion cast by clouds in optical images, limits the frequency of flood observation of the focused area. Therefore, this study proposes utilising multisatellite data from optical satellites such as Landsat 7, Landsat 8, and Moderate Resolution Imaging Spectroradiometer (MODIS), as well as Synthetic Aperture Radar (SAR) images from Advanced Land Observation Satellite (ALOS-2) and Sentinel-1, to increase observation of flood. The main objective was to utilize Otsu image segmentation over both optical and SAR satellite images to distinguish water and nonwater areas in each image separately. For this, modified normalized difference water index (MNDWI) for the optical satellite and total dual-polarization backscatter for SAR satellite images were estimated. The focused area has been divided into Universal Transverse Mercator (UTM) square-size grids of 30 pixels, and each satellite image was reprojected and resampled with a pixel size of 0.001° to standardize the flood map resolution. The second objective was to assess the potential of image fusion for increasing the consistency of water area extraction. Two pairs of satellite images with the same observation period covering a flood event in September 2017 in Perlis, Malaysia, were processed using 2D wavelet transform. Lastly, the temporal changes of the integrated surface water extent were evaluated by comparing the output from both multisatellite and fused images with the observed water level data from the Department of Drainage and Irrigation. The results showed that the proposed model can be used to estimate flood duration as well as to estimate the flood-related losses, especially in ungauged or data-poor regions. Copyright © 2021 Husniyah Binti Mahmud et al.","Geometrical optics; Image fusion; Image segmentation; Pixels; Radar imaging; Satellite imagery; Surface waters; Synthetic aperture radar; Water levels; Wavelet transforms; Flood monitoring; High frequency HF; LANDSAT; LandSat 7; Malaysia; Optical image; Optical satellites; Radar satellites; Satellite images; Temporal change; Floods","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85122996712"
"Liang W.; Wu Y.; Li M.; Cao Y.","Liang, Wenkai (57192207410); Wu, Yan (7406895902); Li, Ming (56937290000); Cao, Yice (57201420994)","57192207410; 7406895902; 56937290000; 57201420994","A Feature Fusion-Net Using Deep Spatial Context Encoder and Nonstationary Joint Statistical Model for High-Resolution SAR Image Classification","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3137029","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122081118&doi=10.1109%2fTGRS.2021.3137029&partnerID=40&md5=367021e671733a8654013d00b122ec6c","The nonstationary and non-Gaussian distribution of the high-resolution (HR) synthetic aperture radar (SAR) image provides much valuable information. However, the current methods, especially deep learning models, directly learn spatial features from HR SAR data while ignoring global statistical information. Combining the local spatial features and global statistical properties of HR SAR images is urgently needed to capture complete HR SAR characteristics. In this paper, a feature fusion network (Fusion-Net) using both deep spatial context encoder and nonstationary joint statistical model (NS-JSM) is proposed for the first time. Fusion-Net realizes the fusion description of local spatial and global statistical features in an end-to-end supervised classification framework. First, a deep spatial context encoder network (DSCEN) is designed based on multiscale group convolution (MSGC) module and channel attention (CA) module. The DSCEN expands the scope of context information extraction with few parameters and increases the interaction between high-level feature channels. Then, the NS-JSM is adopted to capture the unique SAR statistical information. Specifically, the SAR image is transformed into the Gabor wavelet domain. The produced sub-band magnitudes and phases are modeled by the log-normal and uniform distribution. The covariance matrix (CM) is calculated for mapped sub-band data to capture the interscale and intrascale nonstationary correlation. Finally, the group compression and smooth normalization units are introduced into Fusion-Net to fuse the statistical features and spatial features, which not only exploits the complementary information between different features but also optimizes the fusion feature representation. Experiments on four real HR SAR images validate the superiority of the proposed method over other related algorithms.  © 1980-2012 IEEE.","Classification (of information); Convolution; Covariance matrix; Deep learning; Image classification; Image fusion; Neural networks; Radar imaging; Signal encoding; Statistics; Synthetic aperture radar; Convolutional neural network; Features extraction; Features fusions; High resolution synthetic aperture radar images; Images classification; Nonstationary; Nonstationary joint statistical model; Radar polarimetry; Statistic modeling; artificial neural network; image analysis; remote sensing; spatial analysis; synthetic aperture radar; Data mining","Convolutional neural networks; feature fusion; high-resolution (HR) synthetic aperture radar (SAR) images; image classification; nonstationary joint statistical model (NS-JSM)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122081118"
"Sun Y.; Li Z.-L.; Luo J.; Wu T.; Liu N.","Sun, Yingwei (57204428522); Li, Zhao-Liang (57218308345); Luo, Jiancheng (7404183561); Wu, Tianjun (57193855591); Liu, Niantang (57475654200)","57204428522; 57218308345; 7404183561; 57193855591; 57475654200","Farmland parcel-based crop classification in cloudy/rainy mountains using Sentinel-1 and Sentinel-2 based deep learning","2022","International Journal of Remote Sensing","43","3","","1054","1073","19","10.1080/01431161.2022.2032458","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125641177&doi=10.1080%2f01431161.2022.2032458&partnerID=40&md5=508c2e5e65c7255a769e2a27915ab3b7","Multitemporal remote sensing data, especially those for key phenological periods, play an important role in crop classification. However, cloudy/rainy climate conditions can easily lead to a lack of valid optical data, leading to crop classification difficulties. A general solution is taking advantage of all-weather synthetic aperture radar (SAR) datasets. In practice, SAR and optical datasets are often applied in the agricultural field by the method of image fusion, but it is difficult to apply when the number of optical images is too small. To solve this problem, this research proposes a data-transfer and feature-optimize-based method, which deploy an RNN-based encoding-decoding network to add additional data to the ‘optical’ temporal features at the farmland parcel scale and improve the utilization of optical fragments. On the basis of this method, we mitigate inconsistencies in spatial scale among different datasets and optimize the time-series parameters without expert knowledge in the crop classification procedure. The experimental results illustrate the crop classification accuracy of this method, which achieves a 4.1% improvement over the traditional approach and is especially effective for dryland crops (e.g. corn and rapeseed). Thus, this research demonstrates the effectiveness of the combined use of optical and SAR data for similar applications in cloudy/rainy mountainous areas. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Classification (of information); Crops; Data transfer; Deep learning; Farms; Geometrical optics; Image fusion; Radar imaging; Remote sensing; Cloudy/rainy mountainoi area; Combined utilization; Crop classification; Feature transfers; Mountainous area; Multi-temporal data; Multi-temporal remote sensing; Optical-; Optimisations; Sentinel-1; agricultural land; crop plant; data set; machine learning; mountain region; optimization; remote sensing; Sentinel; synthetic aperture radar; Synthetic aperture radar","cloudy/rainy mountainous area; combined utilization; crop classification; feature transfer; multitemporal data; optimization","Article","Final","","Scopus","2-s2.0-85125641177"
"Ebel P.; Xu Y.; Schmitt M.; Zhu X.X.","Ebel, Patrick (57409415200); Xu, Yajin (57431339600); Schmitt, Michael (7401931279); Zhu, Xiao Xiang (55696622200)","57409415200; 57431339600; 7401931279; 55696622200","Multi-Sensor Time Series Cloud Removal Fusing Optical and SAR Satellite Information","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","5381","5384","3","10.1109/IGARSS46834.2022.9883238","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140366516&doi=10.1109%2fIGARSS46834.2022.9883238&partnerID=40&md5=e91242160b9cb5f6990eef332f102787","On average, about half of all optical satellite data observing Earth is covered by haze or clouds. These atmospheric disturbances hinder the ongoing observation of our planet and prevent the seamless application of established remote sensing methods. Accordingly, to allow for an ongoing monitoring of Earth, approaches to reconstruct optical space-borne observations are required. This work introduces a new data set, SEN12MS-CR-TS, for the purpose of multi-sensor time series cloud removal. SEN12MS-CR-TS consists of co-registered radar and optical satellite data, featuring a se-quence of bi-weekly observations throughout an entire year. Finally, we demonstrate the usability of our novel data set by developing a new multi-sensor time-series cloud removal ar-chitecture. We are positive that our curated data set as well as the proposed model will advance future research in satellite image reconstruction and benefit the expanding adaptation of global and all-weather remote sensing applications. © 2022 IEEE.","Earth (planet); Image fusion; Image reconstruction; Optical remote sensing; Radar imaging; Satellites; Space optics; Space-based radar; Time series; Cloud removal; Data set; Images reconstruction; Multi sensor; Optical imagery; Optical satellites; Optical-; Satellite data; Satellite information; Times series; Synthetic aperture radar","data fusion; image reconstruction; optical imagery; synthetic aperture radar; time series","Conference paper","Final","","Scopus","2-s2.0-85140366516"
"Majji S.R.; Chalumuri A.; Kune R.; Manoj B.S.","Majji, Sathwik Reddy (57776021500); Chalumuri, Avinash (57217177190); Kune, Raghavendra (56896059900); Manoj, B.S. (6602854837)","57776021500; 57217177190; 56896059900; 6602854837","Quantum Processing in Fusion of SAR and Optical Images for Deep Learning: A Data-Centric Approach","2022","IEEE Access","10","","","73743","73757","14","10.1109/ACCESS.2022.3189474","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134217224&doi=10.1109%2fACCESS.2022.3189474&partnerID=40&md5=3e37e2aee82369a08e0e18adfcc6eec6","Deep learning techniques are very prominent in processing remotely sensed synthetic aperture radar (SAR) images for real-time, high-impact applications, such as image classification, object detection, and semantic segmentation. The accuracy of deep learning models, such as convolutional neural networks (CNNs), depends on the quality of the input data. Compared to the model-centric approach, where the model parameters are optimized during training, the data-centric approach can enhance the performance accuracy as data quality is improved before training the models. Improving the data quality of SAR images is challenging as SAR image properties are different from optical images. Image fusion techniques proved to enhance the quality of SAR images when combined with optical images. Many fusion techniques exist for combining SAR and optical images in the classical domain. This paper proposes a novel approach to using quantum computing for the image fusion of SAR and optical images. Eight different quantum techniques are used to process and fuse the images. We designed and created a dataset for land-use classification by collecting data using the Google Earth Engine. The quality metric measurements show that the quality of SAR images has improved by using the proposed quantum processing techniques. In addition, performance evaluation of the deep learning CNNs on the dataset was carried out for all quantum processing techniques. Our approach improved the classification accuracy from 82.64%, with only SAR images for training, to 95.36% using the proposed image fusion techniques.  © 2013 IEEE.","Adaptive optics; Classification (of information); Convolution; Deep learning; Geometrical optics; Image classification; Image enhancement; Image fusion; Image segmentation; Land use; Neural networks; Object detection; Optical remote sensing; Quantum computers; Radar imaging; Semantics; Space-based radar; Deep learning; Images classification; Optical image; Optical imagery; Optical imaging; Quantum processing; Radar polarimetry; Synthetic Aperture Radar Imagery; Synthetic aperture radar images; Synthetic aperture radar","deep learning; image classification; image fusion; optical imagery; Quantum processing; synthetic aperture radar (SAR) imagery","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85134217224"
"Jiang Y.; Chen R.; Zhang Y.","Jiang, Yicheng (36064940200); Chen, Ruida (57384848400); Zhang, Yun (56097923300)","36064940200; 57384848400; 56097923300","A novel dual-radar fusion method for improving the cross-range resolution of ISAR imagery","2022","Remote Sensing Letters","13","3","","236","246","10","10.1080/2150704X.2021.2018143","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121707060&doi=10.1080%2f2150704X.2021.2018143&partnerID=40&md5=d0b6136cca0bdeaf5a2952dd94087242","For the intelligent crossroad with millimetre wave radar, two radars are placed perpendicular to each other to complete the monitoring and identification of vehicles which means the cross-radar can obtain inverse synthetic aperture radar (ISAR) image while the radial radar can not obtain, but the cross-range of a scattering point in ISAR image is equivalent to the radial range of the same point in another radar’s view. Thus, the special placement of radar is utilized to improve cross-range resolution in ISAR image. In this letter, a novel dual-radar fusion method for improving ISAR image cross-range resolution for isolated scattering points on target is proposed. First, the range information of scatters is gathered from radial radar based on multiple signal characterization (MUSIC) algorithm. Then, data interpolation and construction of the modulation function will be completed. Finally, the ISAR image cross-range resolution can be improved by fusing the ISAR image obtained from cross-radar and the modulation function. The effectiveness of the proposed method is validated by simulation and experimental results. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Image fusion; Inverse problems; Inverse synthetic aperture radar; Millimeter waves; Modulation; Radar imaging; Cross ranges; Cross-range resolution; Fusion methods; Inverse synthetic aperture radar images; Millimeter-wave radar; Millimetre-wave radar; Modulation functions; Range information; Same points; Synthetic Aperture Radar Imagery; image analysis; interpolation; numerical method; radar altimetry; satellite imagery; scattering; spatial resolution; spectral resolution; synthetic aperture radar; Image enhancement","","Article","Final","","Scopus","2-s2.0-85121707060"
"Ji Z.; Xu L.; Wang H.; Zhang Y.","Ji, Zhenyuan (27171756600); Xu, Li (57937701700); Wang, Haotian (57938248200); Zhang, Yun (56097923300)","27171756600; 57937701700; 57938248200; 56097923300","An Algorithm Based on PCGP Image Fusion for Multi-Source Remote Sensing Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2860","2863","3","10.1109/IGARSS46834.2022.9884334","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140391114&doi=10.1109%2fIGARSS46834.2022.9884334&partnerID=40&md5=f924e3999d494bd7e927147f25278723","Heterogeneous images imaged by different types of sensors have different imaging mechanisms, reflecting the characteristics of different sides of the target scene; while multi-source images formed by different working platforms or at different times have different imaging perspectives, and provide different target scene information. The use of multi-source heterogeneous images for fusion to obtain target and scene information more accurately and comprehensively has potential important applications in many fields such as military, medicine, and meteorology, and has become an important branch of image processing research. To this end, a PCGP algorithm is proposed in this paper to realize the fusion of optical images from different sources and SAR images. It first applies PCA transformation to the multi-source data images to obtain the principal component variables, then performs histogram matching on the first principal components of the transformed data sources, and finally uses the gradient pyramid decomposition algorithm to fuse the matched images to obtain a fused image. Then, the proposed fusion algorithm is tested in the fusion task of remote sensing images from different sources of GF2, GF6 and GF3. The experimental results show that the proposed fusion algorithm has better results. © 2022 IEEE.","Geometrical optics; Metadata; Military applications; Military photography; Optical data processing; Optical remote sensing; Radar imaging; Synthetic aperture radar; Fusion algorithms; Imaging mechanism; Military medicine; Multi sensor; Multi-sensor heterogeneous remote sensing image; Multi-source images; Multi-Sources; PCGP algorithm; Remote sensing images; Target scenes; Image fusion","image fusion; Multi-sensor heterogeneous remote sensing image; PCGP algorithm","Conference paper","Final","","Scopus","2-s2.0-85140391114"
"Zhu D.; Wang X.; Li G.; Zhang X.-P.","Zhu, Dong (57001841800); Wang, Xueqian (57829747800); Li, Gang (55547117794); Zhang, Xiao-Ping (57848336600)","57001841800; 57829747800; 55547117794; 57848336600","Vessel detection via multi-order saliency-based fuzzy fusion of spaceborne and airborne SAR images","2023","Information Fusion","89","","","473","485","12","10.1016/j.inffus.2022.08.022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138098620&doi=10.1016%2fj.inffus.2022.08.022&partnerID=40&md5=1f395b20952cc1ba9f83e5aa9d3d3bc8","This study focuses on vessel target detection by fusing synthetic aperture radar (SAR) remote sensing images collaboratively collected from spaceborne and airborne platforms. Accurate vessel detection is difficult in the presence of inshore interferences and in the case of structured and shaped targets. In this study, we have proposed a new method for the fusion of spaceborne and airborne SAR images based on multi-order superpixel-level saliency and fuzzy logic (MSSFL). We first generated a new global regional contrast map (GRCM) by exploiting the multi-order superpixel-level saliency (MSS). In GRCMs, the vessel targets are well restored, and the background is suppressed. Next, a new fuzzy logic approach based on regional features is presented to fuse the MSS information provided by the GRCMs of the spaceborne and airborne SAR images. This regional feature-based fuzzy fusion can further enhance the vessel target regions and filter out the inshore interference regions. Experimental results using the SAR images of Gaofen-3 satellite and unmanned aerial vehicle show that the proposed MSSFL method yields a higher target-to-clutter ratio of fused images and improved detection performance, compared with the commonly utilized image fusion approaches and the classical detection algorithms solely using spaceborne or airborne SAR images. © 2022 Elsevier B.V.","Antennas; Computer circuits; Image enhancement; Image fusion; Radar imaging; Remote sensing; Satellites; Space-based radar; Superpixels; Synthetic aperture radar; Airborne platforms; Fuzzy-Logic; Multi-ordering; Saliency; Space-borne; Spaceborne and airborne platform; Spaceborne synthetic aperture radars; Synthetic aperture radar images; Targets detection; Vessel target detection; Fuzzy logic","Fuzzy logic; Saliency; Spaceborne and airborne platforms; Synthetic aperture radar (SAR) image; Vessel target detection","Article","Final","","Scopus","2-s2.0-85138098620"
"Zhang H.; Lei L.; Ni W.; Tang T.; Wu J.; Xiang D.; Kuang G.","Zhang, Han (56012534200); Lei, Lin (56443377400); Ni, Weiping (36069966800); Tang, Tao (56419260900); Wu, Junzheng (55654208000); Xiang, Deliang (55771045300); Kuang, Gangyao (7006353231)","56012534200; 56443377400; 36069966800; 56419260900; 55654208000; 55771045300; 7006353231","Explore Better Network Framework for High-Resolution Optical and SAR Image Matching","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3126939","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125478242&doi=10.1109%2fTGRS.2021.3126939&partnerID=40&md5=6c85aab5fcffb3198d16adb45f250d7a","To fully explore the complementary information from optical and synthetic aperture radar (SAR) imageries, they need first to be coregistered with high accuracy. Due to the vast radiometric and geometric disparity, the problem to match high-resolution optical and SAR images is quite challenging. The present deep learning-based methods have shown advantages over the traditional approaches, but the performance increment is not significant. In this article, we explore a better network framework for high-resolution optical and SAR image matching from three aspects. First, we propose an effective multilevel feature fusion method, which helps to take advantage of both the low-level fine-grained features for precious feature location and the high-level semantic features for better discriminative ability. Second, a feature channel excitation procedure is conducted using a novel multifrequency channel attention module, which is able to make image features of different types and multiple levels effectively collaborate with each other and produce image matching features with high diversity. Third, the self-adaptive weighting loss is introduced, with which, each sample is assigned with an adaptive weighting factor, and therefore, information buried in all nearby samples can be better exploited. Under a pseudo-Siamese architecture, the proposed optical and SAR image matching network (OSMNet) is trained and tested on a large and diverse high-resolution optical and SAR dataset. Extensive experiments demonstrate that each component of the proposed deep framework helps to improve the matching accuracy. Also, the OSMNet shows overwhelming superior to the state-of-the-art handcrafted approaches on imageries of different land-cover types.  © 1980-2012 IEEE.","Deep learning; Image fusion; Image matching; Large dataset; Neural networks; Radar imaging; Semantics; Synthetic aperture radar; Adaptive weighting; Convolutional neural network; Features fusions; Frequency channels; High resolution; Multi frequency; Multi-frequency channel attention; Optical-; Remote-sensing; Self-adaptive weighting loss; Synthetic aperture radar; artificial neural network; exploration; image analysis; optical method; spectral resolution; synthetic aperture radar; Remote sensing","Convolutional neural networks (CNNs); Feature fusion; High resolution; Image matching; Multi-frequency channel attention; Optical; Remote sensing; Self-adaptive weighting loss (SAW); Synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85125478242"
"Zhao M.; Shi J.; Wang Y.","Zhao, Ming (57191472838); Shi, Jiaxian (57432150600); Wang, Yongjian (57206997502)","57191472838; 57432150600; 57206997502","Orientation-Aware Feature Fusion Network for Ship Detection in SAR Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2022.3145039","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123708942&doi=10.1109%2fLGRS.2022.3145039&partnerID=40&md5=96f37947442471b8d3724d83d76902c1","Recently, deep learning methods have been successfully applied to the ship detection in synthetic aperture radar (SAR) images. It is still a great challenge to detect SAR ships, due to the extremely poor image quality and complex background. To solve the problems, a novel method named orientation-aware feature fusion network (OFF-Net) for ship detection in SAR images is proposed in this letter. OFF-Net consists of global context path aggregation (GCPA) module and local rotated contrast enhance (LRCE) module, which fuses the global and local information in feature extraction. First, GCPA module is explored to integrate the global context block with path aggregation network (PAN) to learn the global background information. Second, by designing a rotation scheme based on feature map cyclic shift with four directions, LRCE module is developed to enhance the targets and suppress the background clutters in SAR images. Finally, a decoupled orientation-aware head is proposed to handle the arbitrarily rotated ships more robustly and alleviate the conflict between classification and regression tasks during detection. In addition, a high-resolution SAR-ship detection dataset (OBB-HRSDD) with rotatable bounding boxes is provided. The detection results on the SAR ship detection dataset (SSDD+) and OBB-HRSDD illustrate that our method outperforms all the compared methods. The code and OBB-HRSDD are released at https://github.com/SJX152/papercode  © 2004-2012 IEEE.","Deep learning; Extraction; Image enhancement; Image fusion; Radar imaging; Ships; Synthetic aperture radar; Tracking radar; Deep learning; Features extraction; Features fusions; Head; Marine vehicles; Orientation-aware; Radar polarimetry; Ship detection; Synthetic aperture radar; Synthetic aperture radar images; artificial neural network; detection method; satellite imagery; synthetic aperture radar; Feature extraction","Deep learning; orientation-aware; ship detection; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85123708942"
"Xue R.; Zhang X.; Soergel U.","Xue, R. (57211609893); Zhang, X. (57740532300); Soergel, U. (55955024200)","57211609893; 57740532300; 55955024200","URBAN CLASSIFICATION BASED ON TOP-VIEW POINT CLOUD AND SAR IMAGE FUSION WITH SWIN TRANSFORMER","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2022","","559","564","5","10.5194/isprs-archives-XLIII-B3-2022-559-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131946715&doi=10.5194%2fisprs-archives-XLIII-B3-2022-559-2022&partnerID=40&md5=a69101695bb36c909f525fa2d0b75944","Urban areas are complex scenarios consisting of objects with various materials. This variety poses a challenge to single-data classification schemes. In this paper, we propose a feature fusion and classification network on RGB top-view point cloud and SAR images with swin-Transformer. In this network, the heterogeneous features are learned separately by an asymmetric encoder, and then they are concatenated along the channel dimension and fed into a fusing encoder. Finally, the fused features are decoded by an UperNet for generating the semantic labels. As data we use high-resolution 3D point cloud provided by Hessigheim benchmark which are complemented by TerraSAR-X images. The overall precision and the mean intersection over union (mIoU) achieves 87.25% and 73.56%, respectively, which outperforms the single-data swin-Transformer by 4.08% and 1.91%, respectively.  © Authors 2022","Classification (of information); Deep learning; Image classification; Image fusion; Radar imaging; Semantics; Signal encoding; Cloud image; Data classification; Deep learning; Feature fusing; Point-clouds; SAR Images; Top views; Transformer; Urban areas; Urban classification; Synthetic aperture radar","Deep Learning; Feature Fusing; Point Cloud; Synthetic Aperture Radar; Transformer; Urban Classification","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131946715"
"Kumar J.T.; Yennapusa M.R.; Rao B.P.","Kumar, Jakka Thrisul (57226875990); Yennapusa, Mallikarjuna Reddy (57680166200); Rao, Bhima Prabhakara (26655008200)","57226875990; 57680166200; 26655008200","TRI-SU-L ADWT-FCM: TRI-SU-L-Based Change Detection in SAR Images with ADWT and Fuzzy C-Means Clustering","2022","Journal of the Indian Society of Remote Sensing","50","9","","1667","1687","20","10.1007/s12524-022-01547-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129842400&doi=10.1007%2fs12524-022-01547-2&partnerID=40&md5=66558c9a697fa118bd5c05b1f5979131","The Detection of Changes in Synthetic Aperture Radar (SAR) images is indispensable in the field of remote sensing. This paper presents an eccentric method to improve the overall performance of change detection in SAR images by increasing the smoothness and to achieve symmetry between changed pixels and unchanged pixels of the difference image in terms of accuracy. In this regard, three data sets were captured by SAR sensor using RADAASAT-1, RADAASAT-2 and ERS-2 satellites where each data set consisting two multitemporal SAR images, Median-subtraction operator is proposing along with the existing Log-Ratio and Mean-Ratio operators. Meanwhile, the proposing method is named as the Tri Subtraction Level (TRI-SU-L). Difference images from two ratio operators and median subtraction image are merged based on a novel image fusion technique (DWT with optimized filter coefficients), which is named as Adaptive Discrete Wavelet Transform (ADWT). A novel optimization technique which is named as Transformed Dragonfly Algorithm algorithm has been implemented to choose Optimized filter coefficients along with existing optimization techniques. FCM-Clustering has been employed to segment changed area as foreground and unchanged area as background. The segmented image is compared with the ground truth image and performance is computed in terms of accuracy. To crosscheck the validity of the proposed method, optimized filter coefficients have been chosen through genetic algorithm , particle swarm optimization algorithm and conventional dragonfly algorithm. Finally, the proposed method improved the accuracy over existing methods of (0.84%, 0.81%, 1.17%, 1.13%, 1.17%), (6.26%, 3.37%, 11.31%, 9.67%, 5.34%), (2.45%, 2.86%, 2.32%, 1.56%, 1.88%) for first data set, second data set and third data set, respectively. © 2022, Indian Society of Remote Sensing.","cluster analysis; data set; detection method; ERS; genetic algorithm; optimization; pixel; RADARSAT; satellite imagery; satellite sensor; synthetic aperture radar","ADWT; FCM; GA algorithm; PSO algorithm; SAR; TDA","Article","Final","","Scopus","2-s2.0-85129842400"
"Zhao Y.; Zhao L.-J.; Kuang G.-Y.","Zhao, Yan (57211497253); Zhao, Ling-Jun (7404454829); Kuang, Gang-Yao (7006353231)","57211497253; 7404454829; 7006353231","Attention Feature Fusion Network for Rapid Aircraft Detection in SAR Images; [基于注意力机制特征融合网络的SAR图像飞机目标快速检测]","2021","Tien Tzu Hsueh Pao/Acta Electronica Sinica","49","9","","1665","1674","9","10.12263/DZXB.20200486","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117591964&doi=10.12263%2fDZXB.20200486&partnerID=40&md5=e91cdb2d5c36aab44c7996de8ef99b3b","Aiming at the problems of high discretization of aircraft's backscattering points, complex background interference of surroundings in Synthetic Aperture Radar (SAR) images and weak representation of shallow semantic features of aircraft by existing algorithms, an Attention Feature Fusion Network (AFFN) was proposed for aircraft detection in SAR images. By introducing Bottleneck Attention Module (BAM), this article constructed an attention feature fusion strategy consisting of Attention Bidirectional Feature Fusion Module (ABFFM) and Attention Transfer Connection Block (ATCB) in AFFN, and rationally optimized the network structure so as to strengthen the abilities of extracting and discriminating shallow semantic features of aircraft. Based on a self-built Gaofen-3 and TerraSAR-X mixed aircraft dataset, AFFN was compared with several CNN-based general object detection methods and methods designed for specific objects in SAR images. The experimental results illustrated the accuracy and effectiveness of our method for aircraft detection in SAR images © 2021, Chinese Institute of Electronics. All right reserved.","Aircraft; Convolutional neural networks; Feature extraction; Image fusion; Object detection; Object recognition; Radar imaging; Semantics; Synthetic aperture radar; Attention feature fusion network; Complex background; Convolutional neural network; Discretizations; Features fusions; Rapid aircraft detection; Semantic features; Synthetic aperture radar images; Aircraft detection","Attention feature fusion network (AFFN); Convolutional neural network (CNN); Feature fusion; Rapid aircraft detection; Synthetic aperture radar (SAR) images","Article","Final","","Scopus","2-s2.0-85117591964"
"Li W.; Liu L.; Zhang J.","Li, Wanwu (36598732600); Liu, Lin (57194444751); Zhang, Jixian (56058752000)","36598732600; 57194444751; 56058752000","Fusion of SAR and Optical Image for Sea Ice Extraction","2021","Journal of Ocean University of China","20","6","","1440","1450","10","10.1007/s11802-021-4824-y","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103348455&doi=10.1007%2fs11802-021-4824-y&partnerID=40&md5=e97016e1f8541b4394aae48ce0cd4573","It is difficult to balance local details and global distribution using a single source image in marine target detection of a large scene. To solve this problem, a technique based on the fusion of optical image and synthetic aperture radar (SAR) image for the extraction of sea ice is proposed in this paper. The Band 2 (B2) image of Sentinel-2 (S2) in the research area is selected as optical image data. Preprocessing on the optical image, such as resampling, projection transformation and format conversion, are conducted to the S2 dataset before fusion. Imaging characteristics of the sea ice have been analyzed, and a new deep learning (DL) model, OceanTDL5, is built to detect sea ices. The fusion of the Sentinel-1 (S1) and S2 images is realized by solving the optimal pixel values based on deriving Poisson Equation. The experimental results indicate that the use of a fused image improves the accuracy of sea ice detection compared with the use of a single data source. The fused image has richer spatial details and a clearer texture compared with the original optical image, and its material sense and color are more abundant. © 2021, Ocean University of China, Science Press and Springer-Verlag GmbH Germany.","","image fusion; optical image; Poisson Equation; SAR image; sea ice detection","Article","Final","","Scopus","2-s2.0-85103348455"
"Ahmed U.I.; Velasco A.; Rabus B.","Ahmed, Usman Iqbal (57217787833); Velasco, Arturo (57678840800); Rabus, Bernhard (6701849102)","57217787833; 57678840800; 6701849102","SEMANTIC SEGMENTATION OF LAND USE/LAND COVER (LU/LC) TYPES USING F-CNNS ON MULTI-SENSOR (RADAR-IR-OPTICAL) IMAGE DATA","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","4700","4703","3","10.1109/IGARSS47720.2021.9554051","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129814620&doi=10.1109%2fIGARSS47720.2021.9554051&partnerID=40&md5=b8dc1f95871cc72be7c4f907f4846f7d","Land Use/Land Cover (LU/LC) segmentation is a widely studied topic in the field of remote sensing. Past focus has been on independent studies either on color (RGB) and the Normalized Vegetation Index (NDVI) or on Polarimetric Synthetic Aperture Radar (PolSAR) data. In this paper we explore the fusion potential of RGB images with additional SAR and Near Infra-red (NIR) images for enhanced LU/LC segmentation through Fully-Convolutional Neural Networks (F-CNNs). F-CNNs have been extensively studied for semantic segmentation problems with U-Net and SegNet being two well-known F-CNN architectures. Both these architectures were used as references for this study. High resolution RGB, SAR and NIR images were acquired through Google Earth (GE), German Aerospace Center (DLR) and The Planet Laboratories, respectively. IR was converted to NDVI for its higher potential of segmentation of vegetations areas. Four multi-sensor configurations as input channels to the networks were studied after precise co-registration of these images, and the results were compared to individual channels for both architectures. Simon Fraser University (SFU), Burnaby Campus and its surrounding area was selected for this study due its diverse land types. The area was divided into 5 classes i.e. Roads, Buildings, Forest, Water and No class (unclassified). An overall, best accuracy of ~86% was achieved for a five-channel configuration (R+G+B+SAR+NDVI). We show that the inclusion of SAR and IR channels to RGB based network can significantly improve the performance of LU/LC segmentation. © 2021 IEEE","Convolutional neural networks; Earth (planet); Image enhancement; Image fusion; Infrared devices; Land use; Network architecture; Radar imaging; Remote sensing; Semantics; Space optics; Synthetic aperture radar; Vegetation; Convolutional neural network; Fully-convolutional neural network; Land use/land cover; LAND USE/LAND COVER segmentation; Multi sensor; Multi-sensor fusion; Multi-spectral; SAR/multi-spectral fusion; Semantic segmentation; Vegetation index; Semantic Segmentation","F-CNNs; LU/LC Segmentation; Multi-Sensor Fusion; SAR/Multi-Spectral Fusion; Semantic Segmentation","Conference paper","Final","","Scopus","2-s2.0-85129814620"
"Wang S.-T.; Cui K.; Kong D.-M.; Liu S.-Y.; Wu X.","Wang, Shu-Tao (55714642300); Cui, Kai (57216916554); Kong, De-Ming (55513769000); Liu, Shi-Yu (57209286687); Wu, Xing (57210164510)","55714642300; 57216916554; 55513769000; 57209286687; 57210164510","Application of densely connected network in SAR and multispectral image fusion; [密集连接网络在SAR与多光谱影像融合中的应用]","2021","Guangxue Jingmi Gongcheng/Optics and Precision Engineering","29","5","","1145","1153","8","10.37188/OPE.20212905.1145","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108662401&doi=10.37188%2fOPE.20212905.1145&partnerID=40&md5=f7290d3cd617b5a8743151c6fa0cc1a7","To overcome the shortcomings of single satellite sensor imaging, a fusion algorithm for synthetic aperture radar (SAR) and multispectral images based on densely connected networks is proposed herein. Firstly, the SAR and multispectral images are preprocessed separately, and the bicubic interpolation method is used to resample the same spatial resolution. Then, the densely connected network is used to extract the feature maps of the image separately, and the fusion strategy with the largest regional energy is used to combine the depth features. The fused image is input to a pre-trained decoder for reconstruction to obtain the final fused image. The experiment uses Sentinel-1 SAR images, Landsat-8 images, and Gaofen-1 satellite images for verification and draws comparisons with methods based on component substitution, those based on multiscale decomposition, and those based on deep learning. Experimental results indicate that the accuracy of the fusion algorithm based on densely connected networks in terms of the multiscale structural similarity index is as high as 0.9307, and it is better than other fusion algorithms in terms of other evaluation indexes. Detailed information of SAR images and multispectral images are well preserved.","Deep learning; Image fusion; Image processing; Space-based radar; Synthetic aperture radar; Bicubic interpolation; Component substitution; Densely connected networks; Multi-scale Decomposition; Multi-spectral image fusions; Multispectral images; Spatial resolution; Structural similarity indices; Radar imaging","Densely connected network; Image fusion; Multispectral; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85108662401"
"Han C.; Yang D.; Lu Y.; Hou K.; An W.; Wang H.","Han, Chunlei (35761091100); Yang, Di (57755511000); Lu, Yao (57216806417); Hou, Kaifa (57465332200); An, Wenbo (57217178119); Wang, Hongmei (57196428011)","35761091100; 57755511000; 57216806417; 57465332200; 57217178119; 57196428011","Infrared and ISAR Image Fusion Based on Quadtree Decomposition and Bézier Interpolation","2021","2021 6th International Conference on Signal and Image Processing, ICSIP 2021","","","","483","487","4","10.1109/ICSIP52628.2021.9688709","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125176579&doi=10.1109%2fICSIP52628.2021.9688709&partnerID=40&md5=4a6dcd04a63d224be646ab562b9508e2","Inverse Synthetic Aperture Radar (ISAR) image obtained by stationary radar is the result of longitudinal and lateral two-dimensional high-resolution imaging of moving target. Infrared image is a thermal image corresponding to the temperature distribution of the scene, which formed by collecting infrared radiation emitted from the target and the surrounding. In order to make full use of the complementary information of infrared image and ISAR image, an algorithm that combines infrared image with ISAR image is researched. Firstly, this paper applies quadtree decomposition and Bézier interpolation to reconstruct the background of the ISAR image. Secondly, the salient features of the ISAR image are extracted by subtracting the reconstructed background from the original ISAR image. Finally, this algorithm fuses the salient features with the infrared image using the fusion rule proposed in this paper, through which to get the final fused image. Experimental results show that the algorithm can better preserve both the clear target and detailed information of infrared image and ISAR image in subjective evaluation. Compared with other fusion algorithms, experiments also verify the superiority of the algorithm in objective evaluation.  © 2021 IEEE.","Infrared imaging; Infrared radiation; Interpolation; Inverse problems; Inverse synthetic aperture radar; Radar imaging; Bezy interpolation; Fused images; Fusion rule; High-resolution imaging; Inverse synthetic aperture radar images; Moving targets; Quad-tree decomposition; Salient features; Thermal images; Two-dimensional; Image fusion","Bézier interpolation; Image fusion; Infrared image; ISAR image; Quadtree decomposition","Conference paper","Final","","Scopus","2-s2.0-85125176579"
"Xu Y.; Liu A.; Xu H.; Huang L.; Wang F.","Xu, Yifan (57447151600); Liu, Aifang (7402583778); Xu, Hui (57192210610); Huang, Long (57216432868); Wang, Fan (57192209878)","57447151600; 7402583778; 57192210610; 57216432868; 57192209878","Fine Registration of SAR and Optical Image Based on Improved Hausdorff Distance","2023","Lecture Notes in Electrical Engineering","969 LNEE","","","229","241","12","10.1007/978-981-19-8202-6_21","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144212741&doi=10.1007%2f978-981-19-8202-6_21&partnerID=40&md5=9bd43fa2e048c757c23bf66bb2b5fc83","The registration of SAR and optical image is a core step of multi-source image fusion. The quality of registration restricts the effect of multi-source image fusion. Due to the importance of SAR and optical image registration, we analyze the difficulties of SAR and optical image registration first. And we propose a fine registration method for SAR and optical image based on improved Hausdorff distance. Firstly, we correct the nonlinear distortion of SAR image in range direction by interpolation. Secondly, we use the genetic algorithm to search the mapping parameters that minimize the Hausdorff distance between the images to be registered. These parameters are used for affine transformation of optical images to achieve coarse registration. Finally, we use the Fourier operator to calculate the residual translation between SAR and optical image. Experiments on real data show the effectiveness of the proposed method, and the offset between the corresponding images is less than 2 pixels. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Genetic algorithms; Geometrical optics; Geometry; Image enhancement; Image fusion; Image registration; Radar imaging; Affine transformations; Coarse registration; Hausdorff distance; Image-based; Images registration; Mapping parameters; Multi-source images; Optical image; Registration methods; SAR Images; Synthetic aperture radar","Hausdorff distance; Image registration; Optical image; SAR image","Conference paper","Final","","Scopus","2-s2.0-85144212741"
"Chen Y.; Bruzzone L.","Chen, Yuxing (57211050178); Bruzzone, Lorenzo (7006892410)","57211050178; 7006892410","Self-Supervised SAR-Optical Data Fusion of Sentinel-1/-2 Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3128072","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118984444&doi=10.1109%2fTGRS.2021.3128072&partnerID=40&md5=dad3fe01713c9192b17e463a62816ebf","The effective combination of the complementary information provided by huge amount of unlabeled multisensor data (e.g., synthetic aperture radar (SAR) and optical images) is a critical issue in remote sensing. Recently, contrastive learning methods have reached remarkable success in obtaining meaningful feature representations from multiview data. However, these methods only focus on image-level features, which may not satisfy the requirement for dense prediction tasks such as land-cover mapping. In this work, we propose a self-supervised framework for SAR-optical data fusion and land-cover mapping tasks. SAR and optical images are fused by using a multiview contrastive loss at image level and super-pixel level according to one of those possible strategies: in the early, intermediate, and late strategies. For the land-cover mapping task, we assign each pixel a land-cover class by the joint use of pretrained features and spectral information of the image itself. Experimental results show that the proposed approach not only achieves a comparable accuracy but also reduces the dimension of features with respect to the image-level contrastive learning method. Among three fusion strategies, the intermediate fusion strategy achieves the best performance. The combination of the pixel-level fusion approach and the self-training on spectral indices leads to further improvements in the land-cover mapping task with respect to the image-level fusion approach, especially with sparse pseudo labels. The code to reproduce our results will be found at https://github.com/yusin2it/SARoptical fusion.  © 1980-2012 IEEE.","Geometrical optics; Image enhancement; Image fusion; Learning systems; Photomapping; Pixels; Radar imaging; Synthetic aperture radar; Land cover mapping; Learning methods; Optical data; Optical image; Pixel level; Remote-sensing; Self-supervised learning; Sentinel-1; Sentinel-1/-2; Synthetic aperture radar images; image analysis; land cover; learning; mapping; remote sensing; Sentinel; spectral analysis; synthetic aperture radar; Remote sensing","Data fusion; land-cover mapping; pixel level; remote sensing; self-supervised learning; Sentinel-1/-2","Article","Final","","Scopus","2-s2.0-85118984444"
"Gupta N.; Srivastava H.S.; Sivasankar T.; Patel P.","Gupta, Neeharika (57772595200); Srivastava, Hari Shanker (7102601428); Sivasankar, Thota (57202915146); Patel, Parul (57208689231)","57772595200; 7102601428; 57202915146; 57208689231","A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","2021","2021 IEEE India Geoscience and Remote Sensing Symposium, InGARSS 2021 - Proceedings","","","","488","491","3","10.1109/InGARSS51564.2021.9792062","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133166743&doi=10.1109%2fInGARSS51564.2021.9792062&partnerID=40&md5=f88fc43ad6c46be0b50dbe7a29230cc1","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.  © 2021 IEEE.","Deep learning; Generative adversarial networks; Geometrical optics; Optical remote sensing; Principal component analysis; Radar imaging; Satellite imagery; Space optics; Space-based radar; Synthetic aperture radar; Deep learning; Gradient feature; Learning frameworks; Optical image; Optical satellite imagery; Optical-; Remote sensing images; SAR Images; Single images; Source images; Image fusion","deep learning; GAN; image fusion; optical; SAR","Conference paper","Final","","Scopus","2-s2.0-85133166743"
"Eltanany A.S.; Amein A.S.; Elwan M.S.","Eltanany, Abdelhameed S. (57211990761); Amein, A.S. (55892252700); Elwan, M.S. (36618286100)","57211990761; 55892252700; 36618286100","A modified corner detector for SAR images registration","2021","International Journal of Engineering Research in Africa","53","","","123","156","33","10.4028/www.scientific.net/JERA.53.123","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102409033&doi=10.4028%2fwww.scientific.net%2fJERA.53.123&partnerID=40&md5=633670c86d251c6554e40c5856c6fc87","As a first step for image processing operations, detection of corners is a vital procedure where it can be applied for many applications as feature matching, image registration, image mosaicking, image fusion, and change detection. Image registration can be defined as process of getting the misalignment of pixel's position between two or more images. In this paper, a modified corner detector named Synthetic Aperture Radar- Phase Congruency Harris (SAR-PCH) based on a combination between both phase congruency, named later PC, and Harris corner detector is proposed where PC image can supply fundamental and significative features although the complex changes of intensities. Also, the proposed approach overcomes the Harris limitation concerning the noise since the Harris is more sensitive to the noise. The performance was similitude with Shi-Tomasi, FAST, and Harris corner detectors where experiments are conducted first with simulated images and second with real ones. Mean square error (MSE) and peak signal-to-noise ratio (PSNR) are used for the simile. Experimental results, carried out in a standard computer, verify its effectiveness where it utilizes the privileges of image constitutional depicting, allowing extraction of the most powerful key points since it preserves robustness of co-registration process using image frequency properties which are not variant to illumination. Reasonable results compared to the state of art method as Shi-Tomasi, FAST, and Harris algorithms were achieved on the expense of high computational processing time that can be recovered using hardware having high capabilities. © 2021 Trans Tech Publications Ltd, Switzerland.","Edge detection; Feature extraction; Image fusion; Image registration; Mean square error; Signal to noise ratio; Synthetic aperture radar; Computational processing time; Harris algorithm; Harris corner detector; High capabilities; Image mosaicking; Peak signal to noise ratio; Phase congruency; State-of-art methods; Radar imaging","Feature Detection; Harris Corner Detector; Image Registration; Phase Congruency; Random Sample Consensus (RANSAC)","Article","Final","","Scopus","2-s2.0-85102409033"
"Gencay S.; Ozcan C.","Gencay, Semih (57904390300); Ozcan, Caner (55856886900)","57904390300; 55856886900","The Effect of SAR Speckle Removal in SAR-Optical Image Fusion; [SAR-Optik Görüntü Füzyonunda SAR Benek Gürültüsünün Giderilme Etkisi]","2022","2022 30th Signal Processing and Communications Applications Conference, SIU 2022","","","","","","","10.1109/SIU55565.2022.9864861","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138694159&doi=10.1109%2fSIU55565.2022.9864861&partnerID=40&md5=4dc523fa66b7b3c4688429c4c9c7734f","Due to the imaging mechanism of Synthetic Aperture Radar (SAR) and the noise in the images, visual identification of objects in the scene is not as easy as in optical images. SAR images have limited color information and cannot reflect the spectral information of objects. Optical images, on the other hand, have rich spectral information. SAR-Optical image fusion is an important area of study so that SAR data can be easily evaluated by anyone, but it is difficult to find a matching SAR and optical image of the same scene. In order to overcome this difficulty, Sentinel-1 and Sentinel-2 datasets have been published and image fusion studies have been carried out with various methods. However, it has been observed that the effect of SAR noise removal before merging on image fusion methods has not been investigated. In the studies conducted to investigate this effect, five different fusion algorithms used in the literature were tested with twenty different image groups using different noise reduction ratios. The success of the fusion results obtained was compared with five different metrics that are widely used in the literature. The images and metric results obtained as a result of the tests showed that the removal of speckle noise in the SAR data has a positive effect on the fusion results. © 2022 IEEE.","Geometrical optics; Image fusion; Noise abatement; Optical remote sensing; Radar imaging; Color information; Imaging mechanism; Matchings; Optical image; Radar data; Remote-sensing; Speckle removal; Spectral information; Synthetic aperture radar images; Visual identification; Synthetic aperture radar","image fusion; optical image; remote sensing; SAR image","Conference paper","Final","","Scopus","2-s2.0-85138694159"
"Datta U.","Datta, U. (7007098861)","7007098861","Multimodal change monitoring using multitemporal satellite images","2021","Proceedings of SPIE - The International Society for Optical Engineering","11862","","118620M","","","","10.1117/12.2600099","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118562976&doi=10.1117%2f12.2600099&partnerID=40&md5=b8701ca19db314f013b32048a2075cc9","The main objective of this study is to monitor the land infrastructure growth over a period of time using multimodality of remote sensing satellite images. In this project unsupervised change detection analysis using ITPCA (Iterated Principal Component Analysis) is presented to indicate the continuous change occurring over a long period of time. The change monitoring is pixel based and multitemporal. Co-registration is an important criteria in pixel based multitemporal image analysis. The minimization of co-registration error is addressed considering 8-neighborhood pixels. Comparison of results of ITPCA analysis with LRT (likelihood ratio test) and GLRT (generalized likelihood ratio test) methods used for SAR and MS (Multispectral) images respectively in earlier publications are also presented in this paper. The datasets of Sentinel-2 around 0-3 days of the acquisition of Sentinel-1 are used for multimodal image fusion. SAR and MS both have inherent advantages and disadvantages. SAR images have the advantage of being insensitive to atmospheric and light conditions, but it suffers the presence of speckle phenomenon. In case of multispectral, challenge is to get quite a large number of datasets without cloud coverage in region of interest for multivariate distribution modelling.  © 2021 SPIE.","Image analysis; Image fusion; Image segmentation; Large dataset; Pixels; Radar imaging; Remote sensing; Synthetic aperture radar; Change detection; Coregistration; Generalized Likelihood Ratio Test; Iterated principal component analyse; Likelihood ratio tests; Multi-modal; Multi-spectral; Principal-component analysis; SAR; SAR Images; Principal component analysis","Change detection; GLRT; ITPCA; LRT; Multimodal; Multispectral; SAR","Conference paper","Final","","Scopus","2-s2.0-85118562976"
"Li K.; Zhang M.; Xu M.; Tang R.; Wang L.; Wang H.","Li, Kuoyang (57789233000); Zhang, Min (57193230552); Xu, Maiping (57610868100); Tang, Rui (57788588900); Wang, Liang (57440782600); Wang, Hai (56675478900)","57789233000; 57193230552; 57610868100; 57788588900; 57440782600; 56675478900","Ship Detection in SAR Images Based on Feature Enhancement Swin Transformer and Adjacent Feature Fusion","2022","Remote Sensing","14","13","3186","","","","10.3390/rs14133186","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133725797&doi=10.3390%2frs14133186&partnerID=40&md5=b1ef6e84f83a8f5a1cbe27b96101fece","Convolutional neural networks (CNNs) have achieved milestones in object detection of synthetic aperture radar (SAR) images. Recently, vision transformers and their variants have shown great promise in detection tasks. However, ship detection in SAR images remains a substantial challenge because of the characteristics of strong scattering, multi-scale, and complex backgrounds of ship objects in SAR images. This paper proposes an enhancement Swin transformer detection network, named ESTDNet, to complete the ship detection in SAR images to solve the above problems. We adopt the Swin transformer of Cascade-R-CNN (Cascade R-CNN Swin) as a benchmark model in ESTDNet. Based on this, we built two modules in ESTDNet: the feature enhancement Swin transformer (FESwin) module for improving feature extraction capability and the adjacent feature fusion (AFF) module for optimizing feature pyramids. Firstly, the FESwin module is employed as the backbone network, aggregating contextual information about perceptions before and after the Swin transformer model using CNN. It uses single-point channel information interaction as the primary and local spatial information interaction as the secondary for scale fusion based on capturing visual dependence through self-attention, which improves spatial-to-channel feature expression and increases the utilization of ship information from SAR images. Secondly, the AFF module is a weighted selection fusion of each high-level feature in the feature pyramid with its adjacent shallow-level features using learnable adaptive weights, allowing the ship information of SAR images to be focused on the feature maps at more scales and improving the recognition and localization capability for ships in SAR images. Finally, the ablation study conducted on the SSDD dataset validates the effectiveness of the two components proposed in the ESTDNet detector. Moreover, the experiments executed on two public datasets consisting of SSDD and SARShip demonstrate that the ESTDNet detector outperforms the state-of-the-art methods, which provides a new idea for ship detection in SAR images. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Convolution; Convolutional neural networks; Feature extraction; Image enhancement; Image fusion; Object detection; Object recognition; Radar imaging; Ships; Tracking radar; Adjacent feature; Adjacent feature fusion; Cascade R-convolutional neural network; Convolutional neural network; Feature enhancement; Feature enhancement swin transformer; Features fusions; Ship detection; Synthetic aperture radar; Synthetic aperture radar images; Synthetic aperture radar","adjacent feature fusion; Cascade R-CNN; feature enhancement Swin transformer; ship detection; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133725797"
"Jiang Y.; Li M.; Zhang P.; Tan X.; Song W.","Jiang, Yinyin (57219257050); Li, Ming (56937290000); Zhang, Peng (55205949900); Tan, Xiaofeng (57196353591); Song, Wanying (56047346700)","57219257050; 56937290000; 55205949900; 57196353591; 56047346700","Hierarchical fusion convolutional neural networks for SAR image segmentation","2021","Pattern Recognition Letters","147","","","115","123","8","10.1016/j.patrec.2021.04.005","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105090645&doi=10.1016%2fj.patrec.2021.04.005&partnerID=40&md5=f2516103bd44cc5af62d32426e9937e2","Convolutional neural network (CNN) has achieved promising results in image segmentation recently. However, for the segmentation of synthetic aperture radar (SAR) images with complicated scene, the single receptive field in CNN has a limited ability to effectively capture structural and regional information at the same time. In this paper, we propose a hierarchical fusion CNN (HIFCNN) model for SAR image segmentation. At each convolutional layer, HIFCNN sets several different-sized receptive fields, and thus extracts hierarchical features. Concretely, the larger-sized receptive field captures regional information and is robust against speckle, while the smaller one preserves the structural information well. Then, based on the Dempster-Shafer evidential theory, the proposed hierarchical network, HIFCNN, implements a decision-level fusion to integrate these hierarchical features. In this way, the structural and regional information can be accurately captured by different receptive fields, which is beneficial for edge location, structure preservation and region homogeneity in SAR image segmentation. The effectiveness of HIFCNN model is demonstrated by the application to the segmentation of the simulated images and real SAR images. © 2021 Elsevier B.V.","Convolution; Decision theory; Image fusion; Image segmentation; Neural networks; Radar imaging; Convolutional neural network; Dempster-Shafer evidential theory; Hierarchical fusion convolutional neural network; Hierarchical fusions; Images segmentations; Receptive fields; Regional information; Structural information; Synthetic Aperture Radar image segmentations; Synthetic aperture radar images; Synthetic aperture radar","Dempster-Shafer evidential theory; Hierarchical fusion convolutional neural networks; Image segmentation; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85105090645"
"Shi J.; Zhang Z.; Wu T.; Li X.; Zhou D.; Lei Y.","Shi, Jiao (55553885300); Zhang, Zeping (57221313341); Wu, Tancheng (57657227900); Li, Xiaoyang (56206619700); Zhou, Deyun (9232409200); Lei, Yu (56424742100)","55553885300; 57221313341; 57657227900; 56206619700; 9232409200; 56424742100","Multi-scale Features Fusion Network for Unsupervised Change Detection in Heterogeneous Optical and SAR Images","2021","Proceedings of 2021 7th IEEE International Conference on Cloud Computing and Intelligence Systems, CCIS 2021","","","","270","274","4","10.1109/CCIS53392.2021.9754667","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129176558&doi=10.1109%2fCCIS53392.2021.9754667&partnerID=40&md5=16089ad6143012737e6eccc37b65dd76","Change detection (CD) in heterogeneous remote sensing image applications has become an issue of increasing concern in, as they cannot be compared directly with traditional homogenous CD methods. To solve feature loss problem and generating better representations to accommodate regions of various sizes in heterogeneous images CD, a multi-scale features fusion network (MFFN) is proposed. Firstly, multi-scale representative deep features can be extracted to distinguish difference in high-dimension feature space. Then, hierarchical features from the original image pairs can be fuse to generate a difference image with more explicit semantic information owing to the strategy of multi-scale features fusion, which can better adapt different scale of changes in heterogeneous remote sensing images. It is noteworthy that the experimental results on both heterogeneous and homogeneous data set confirm the effectiveness of the proposed method.  © 2021 IEEE.","Image fusion; Radar imaging; Semantics; Space optics; Synthetic aperture radar; Change detection; Features fusions; Heterogeneous image; Multi-scale feature fusion; Multi-scale features; Neural-networks; Optical image; Remote sensing images; SAR Images; Unsupervised change detection; Remote sensing","Change detection; Heterogeneous images; Multi-scale feature fusion; Neural network","Conference paper","Final","","Scopus","2-s2.0-85129176558"
"Li X.; Zhang G.; Cui H.; Hou S.; Chen Y.; Li Z.; Li H.; Wang H.","Li, Xue (49663402500); Zhang, Guo (57995652300); Cui, Hao (57210701957); Hou, Shasha (57214136087); Chen, Yujia (57216667415); Li, Zhijiang (8677703600); Li, Haifeng (57996261300); Wang, Huabin (57208192941)","49663402500; 57995652300; 57210701957; 57214136087; 57216667415; 8677703600; 57996261300; 57208192941","Progressive fusion learning: A multimodal joint segmentation framework for building extraction from optical and SAR images","2023","ISPRS Journal of Photogrammetry and Remote Sensing","195","","","178","191","13","10.1016/j.isprsjprs.2022.11.015","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143516607&doi=10.1016%2fj.isprsjprs.2022.11.015&partnerID=40&md5=fcdbe4ec382d2f267f68e9143398c2cc","Automatic and high-precision extraction of buildings from remote sensing images has a wide range of application and importance. Optical and synthetic aperture radar (SAR) images are typical types of multimodal remote sensing data with different imaging methods. To bridge the huge gap between them and achieve high-precision joint semantic segmentation, this study proposes a progressive fusion learning framework. The framework explicitly extracts the shared features (that is, modal invariants) of multimodal images as the information medium and realizes information fusion through multistage learning. Based on this framework, we design a network called the multistage multimodal fusion network (MMFNet), which uses phase as a modal invariant to joint optical and SAR images to achieve high-precision building extraction. We conducted experiments with the Multi-Sensor All-Weather Mapping aerial dataset and the WHU-OPT-SAR_WuHan satellite dataset. This study shows MMFNet has a significant extraction effect and yields more optimized extraction of the edge details of buildings, which is improved by 0.2% to 9.5% compared to other multimodal joint segmentation methods. © 2022","Antennas; Buildings; Deep learning; Extraction; Geometrical optics; Image fusion; Optical remote sensing; Radar imaging; Semantic Segmentation; Semantics; Building extraction; Deep learning; High-precision; Multi-modal; Multi-modal fusion; Optical image; Optical-; Remote-sensing; Synthetic aperture radar; Synthetic aperture radar images; building; data set; information processing; learning; remote sensing; satellite data; segmentation; synthetic aperture radar; Synthetic aperture radar","Building extraction; Deep learning; Multimodal fusion; Optical images; Remote sensing; Synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85143516607"
"He Y.; Wang X.; Zhang Y.; Liu Y.; Jiang Z.; Li G.; He Y.","He, Yueping (58018567300); Wang, Xueqian (57829747800); Zhang, Yiming (58018782600); Liu, Yu (57858530200); Jiang, Zhizhuo (57005762600); Li, Gang (55547117794); He, You (57212448603)","58018567300; 57829747800; 58018782600; 57858530200; 57005762600; 55547117794; 57212448603","A Novel Loss Function for Optical and SAR Image Matching: Balanced Positive and Negative Samples","2022","IEEE Geoscience and Remote Sensing Letters","19","","4028805","","","","10.1109/LGRS.2022.3225965","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144061390&doi=10.1109%2fLGRS.2022.3225965&partnerID=40&md5=091720e060fd0a6dcb39c29ce0a2c738","Image matching is a primary technology for optical and synthetic aperture radar (SAR) image fusion but often shows limited performance due to the highly nonlinear differences between optical and SAR modalities. Recently, deep neural networks (DNNs) have been investigated to effectively extract nonlinear features for image matching tasks, where DNNs are trained based on the elaborated design of loss functions and a low loss value is often expected to obtain better image matching performance. In this letter, we first theoretically demonstrate that when the value of a state-of-the-art loss function decreases, the corresponding matching performance may not consistently improve due to the imbalanced effect of positive and negative samples. To tackle this issue, we proposed an improved loss function to train DNNs for image matching of SAR and optical images. We theoretically prove that the improved loss function ensures the improvement of the matching performance when the loss value decreases based on Taylor's series expansion analysis. Experimental results on an open dataset with extensive optical and SAR image pairs show that: 1) the proposed loss function is better than the original one in terms of image matching performance and 2) the combination of our loss function and existing multiscale convolutional gradient feature (MCGF)-based network provides better matching performance than the other state-of-the-art approaches.  © 2004-2012 IEEE.","Geometrical optics; Image enhancement; Image fusion; Image matching; Nonlinear optics; Radar imaging; Synthetic aperture radar; Loss functions; Matching performance; Negative samples; Optical and synthetic aperture radar image matching; Optical-; Primary technologies; Sample balance; Series expansion; Synthetic aperture radar images; Taylor’s series expansion; nonlinearity; optical method; synthetic aperture radar; Deep neural networks","Loss function; optical and synthetic aperture radar (SAR) image matching; sample balance; Taylor's series expansion","Article","Final","","Scopus","2-s2.0-85144061390"
"Zhao Z.; Zhu Z.; Chen G.; Zhao J.","Zhao, Zhenhe (58080850700); Zhu, Ziwei (58081000800); Chen, Gan (58080278500); Zhao, Jianming (57738011800)","58080850700; 58081000800; 58080278500; 57738011800","Synthetic aperture radar image change detection based on an image fusion strategy","2022","2022 International Conference on Image Processing, Computer Vision and Machine Learning, ICICML 2022","","","","151","155","4","10.1109/ICICML57342.2022.10009685","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146916633&doi=10.1109%2fICICML57342.2022.10009685&partnerID=40&md5=e2d8be8b701b619d48c6500399ee89a1","Change detection is a hot topic and of great importance in remote sensing. The logarithm operation can be an effective means of eliminating the influence of multiplicative noise in the synthetic aperture radar (SAR) image. However, due to the nature of the logarithmic function, regions of change with high gray values are attenuated. In this study, we propose SAR to detect image changes based on an image fusion strategy by combining two different methods. First, we reduce speckle by non-subsampled shearlet transform in the log-domain, since this logarithmic function has the advantage of transforming multiplicative speckle noise into additive noise. As we know, the difference method always detects the areas with the largest changes. The difference method based on saliency extraction (SE) is applied to the exponent transformational SAR image to complement the disadvantage of WLR. WLR is used to obtain a change map of WLR to reduce the influence of SE when WLR does not detect the changed areas. Finally, two change maps can be added to obtain the final result. Experimental results for real SAR image pairs show the effectiveness of the proposed method in terms of detection rate, false alarm rate, and overall accuracy.  © 2022 IEEE.","Additive noise; Change detection; Extraction; Image fusion; Radar imaging; Remote sensing; Speckle; Change detection; Difference method; Fusion strategies; Image change detection; Image fusion strategy; Logarithmic functions; Non-subsampled shearlet transform; Saliency extraction; Shearlet transforms; Synthetic aperture radar images; Synthetic aperture radar","Change detection; image fusion strategy; non-subsampled shearlet transform; saliency extraction; synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85146916633"
"Imani M.","Imani, Maryam (55913192000)","55913192000","Scattering and Regional Features Fusion Using Collaborative Representation for PolSAR Image Classification","2022","2022 9th Iranian Joint Congress on Fuzzy and Intelligent Systems, CFIS 2022","","","","","","","10.1109/CFIS54774.2022.9756487","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129518178&doi=10.1109%2fCFIS54774.2022.9756487&partnerID=40&md5=29585026fad897cbb3d58daff1450b3d","While the collaborative representation has been used for classification of multi-channel images in several works, it is suggested for scattering and spatial features fusion of polarimetric synthetic aperture radar (PolSAR) images in this work. With considering a neighboring region around each pixel of the PolSAR image, its approximation is computed by its adjacent samples in the local region by solving a convex optimization problem. The samples with more similar scattering characteristics will have more important role in the pixel representation. The obtained collaborative representation can be considered as a fused polarimetric-contextual feature space, which can be given to any arbitrary classifier. The experimental results on three real PolSAR images show the good performance of the fused feature space in providing a clear and accurate classification map. © 2022 IEEE.","Classification (of information); Convex optimization; Image fusion; Pixels; Polarimeters; Radar imaging; Synthetic aperture radar; Collaborative representations; Feature space; Features fusions; Images classification; Local region; Multi channel; Polarimetric synthetic aperture radars; Regional feature; Spatial features; Synthetic aperture radar images; Image classification","classification; collaborative representation; feature fusion; PolSAR","Conference paper","Final","","Scopus","2-s2.0-85129518178"
"Wu K.; Gu L.; Jiang M.","Wu, Kunpeng (57295001700); Gu, Lingjia (15834718400); Jiang, Mingda (57295437600)","57295001700; 15834718400; 57295437600","Research on fusion of SAR image and multispectral image using texture feature information","2021","Proceedings of SPIE - The International Society for Optical Engineering","11829","","1182916","","","","10.1117/12.2592925","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116960462&doi=10.1117%2f12.2592925&partnerID=40&md5=3a0be947723402dbeafc6fd8ba86a391","Remote sensing images have the characteristics of multiple data sources and complex data. How to integrate remote sensing image information more efficiently has always been the focus of research. In this paper, Changchun City, Jilin Province, China was selected as the experimental area, Sentinel-1 and Sentinel-2 images were used as experimental data, and a fusion method of SAR image and multispectral image using texture feature information was proposed. First, perform HIS transformation on the multi-spectral image to obtain the intensity image. After that, wavelet transform was used to extract the high-frequency and low-frequency detail components of the intensity image. At the same time, the principal component analysis method and the deep learning network VGG-19 were used to extract the texture features of the SAR image. The SAR texture image was used to enhance the high-frequency detail component of the intensity image, and combined with the original low-frequency detail component to perform inverse wavelet transform, then a new intensity image was obtained. Finally, the modulated intensity image was used to replace the original intensity image, and the inverse transformation (I-HIS) was performed to obtain an enhanced image fused from the multispectral image and the SAR image. Compared with the original image, the detailed features and boundary distinction were significantly improved. The fusion image was input into the support vector machine for feature classification, and the comprehensive classification accuracy reached 94.74%, which was 3.5% higher than the classification accuracy of the unfused image. © 2021 SPIE.","Classification (of information); Deep learning; Image compression; Image enhancement; Image fusion; Image texture; Principal component analysis; Radar imaging; Remote sensing; Spectroscopy; Support vector machines; Synthetic aperture radar; Textures; Wavelet transforms; Classification accuracy; Deep learning algorithm; Feature information; High frequency HF; Intensity images; Lower frequencies; Multispectral images; Remote sensing images; SAR Images; Texture features; Learning algorithms","Deep learning algorithm; Image fusion; Multispectral image; SAR image","Conference paper","Final","","Scopus","2-s2.0-85116960462"
"Monsalve‐tellez J.M.; Torres‐león J.L.; Garcés‐gómez Y.A.","Monsalve‐tellez, Jose Manuel (57541358800); Torres‐león, Jorge Luis (57540078900); Garcés‐gómez, Yeison Alberto (56272275600)","57541358800; 57540078900; 56272275600","Evaluation of SAR and Optical Image Fusion Methods in Oil Palm Crop Cover Classification Using the Random Forest Algorithm","2022","Agriculture (Switzerland)","12","7","955","","","","10.3390/agriculture12070955","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133705613&doi=10.3390%2fagriculture12070955&partnerID=40&md5=90b041c0b92050d070762149b0c2f67a","This paper presents an evaluation of land cover accuracy, particularly regarding oil palm crop cover, using optical/synthetic aperture radar (SAR) image fusion methods through the implementation of the random forest (RF) algorithm on cloud computing platforms using Sentinel‐1 SAR and Sentinel‐2 optical images. Among the fusion methods evaluated were Brovey (BR), high‐frequency modulation (HFM), Gram–Schmidt (GS), and principal components (PC). This work was developed using a cloud computing environment employing R and Python for statistical analysis. It was found that an optical/SAR image stack resulted in the best overall accuracy with 82.14%, which was 11.66% higher than that of the SAR image, and 7.85% higher than that of the optical image. The high‐frequency modulation (HFM) and Brovey (BR) image fusion methods showed overall accuracies higher than the Sentinel‐2 optical image classification by 3.8% and 3.09%, respectively. This demonstrates the potential of integrating optical imagery with Sentinel SAR imagery to increase land cover classification accuracy. On the other hand, the SAR images obtained very high accuracy results in classifying oil palm crops and forests, reaching 94.29% and 90%, respectively. This demonstrates the ability of synthetic aperture radar (SAR) to provide more information when fused with an optical image to improve land cover classification. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","cloud computing; image fusion; land cover classification; optical images; random forest (RF); synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133705613"
"Lin L.; Shen H.; Li J.; Yuan Q.","Lin, Liupeng (57188711703); Shen, Huanfeng (8359721100); Li, Jie (57214207213); Yuan, Qiangqiang (36635300800)","57188711703; 8359721100; 57214207213; 36635300800","FDFNet: A Fusion Network for Generating High-Resolution Fully PolSAR Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3127958","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119419650&doi=10.1109%2fLGRS.2021.3127958&partnerID=40&md5=215cca509f66d5718108a8b76fb52cee","Deep learning shows potential superiority in the image fusion field. To solve the problem of the spatial resolution degradation of polarimetric synthetic aperture radar (PolSAR) images caused by system limitation, we propose a fully PolSAR images and DualSAR images fusion network (FDFNet). We use low resolution (LR)-PolSAR super-resolution (LPSR) and modified cross attention mechanism (MCroAM) to perform data fusion on LR-PolSAR and high resolution (HR)-dual-polarization synthetic aperture radar (DualSAR) and design a polarimetric decomposition attention module to introduce the polarimetric parameters of LR-PolSAR images to maintain polarimetric information. Besides, we use the differential information between LR-PolSAR and HR-DualSAR to guide spatial resolution reconstruction. The loss function based on the L1 norm is used to constrain the network training process. The experimental results show the superiority of the proposed method over the existing methods in visual and quantitative evaluation. In addition, polarimetric decomposition experiments verify the effectiveness of the proposed method to maintain polarimetric information.  © 2004-2012 IEEE.","Deep learning; Image resolution; Polarimeters; Differential information; Dual-polarization SAR; Fully polarimetric SAR; High resolution; L1 norm; Loss functions; Polarimetric decomposition; Polarimetric informations; Polarimetric parameters; Spatial resolution; decomposition analysis; image analysis; learning; quantitative analysis; remote sensing; spatial resolution; synthetic aperture radar; Image fusion","Differential information; dual-polarization synthetic aperture radar (DualSAR); fully-polarimetric synthetic aperture radar (PolSAR); fusion; polarimetric decomposition","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119419650"
"Xu F.; Shi Y.; Ebel P.; Yu L.; Xia G.-S.; Yang W.; Zhu X.X.","Xu, Fang (57219687686); Shi, Yilei (55495784300); Ebel, Patrick (57409415200); Yu, Lei (57192162926); Xia, Gui-Song (12781686200); Yang, Wen (57155382600); Zhu, Xiao Xiang (55696622200)","57219687686; 55495784300; 57409415200; 57192162926; 12781686200; 57155382600; 55696622200","GLF-CR: SAR-enhanced cloud removal with global–local fusion","2022","ISPRS Journal of Photogrammetry and Remote Sensing","192","","","268","278","10","10.1016/j.isprsjprs.2022.08.002","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137019682&doi=10.1016%2fj.isprsjprs.2022.08.002&partnerID=40&md5=dbf6ca3b20c6b01b515b7497110ea526","The challenge of the cloud removal task can be alleviated with the aid of Synthetic Aperture Radar (SAR) images that can penetrate cloud cover. However, the large domain gap between optical and SAR images as well as the severe speckle noise of SAR images may cause significant interference in SAR-based cloud removal, resulting in performance degeneration. In this paper, we propose a novel global–local fusion based cloud removal (GLF-CR) algorithm to leverage the complementary information embedded in SAR images. Exploiting the power of SAR information to promote cloud removal entails two aspects. The first, global fusion, guides the relationship among all local optical windows to maintain the structure of the recovered region consistent with the remaining cloud-free regions. The second, local fusion, transfers complementary information embedded in the SAR image that corresponds to cloudy areas to generate reliable texture details of the missing regions, and uses dynamic filtering to alleviate the performance degradation caused by speckle noise. Extensive evaluation demonstrates that the proposed algorithm can yield high quality cloud-free images and outperform state-of-the-art cloud removal algorithms with a gain about 1.7 dB in terms of PSNR on SEN12MS-CR dataset. © 2022 The Authors","Image fusion; Information filtering; Radar imaging; Speckle; Textures; Cloud cover; Cloud removal; Cloud removal algorithms; Global-local; Large domain; Local fusion; Optical-; Speckle noise; Synthetic aperture radar images; Transformer; algorithm; cloud; data set; global change; noise; satellite data; satellite imagery; speckle; synthetic aperture radar; Synthetic aperture radar","Cloud removal; Data fusion; SAR; Transformer","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85137019682"
"Li X.; Zhang G.; Yin C.; Wu Y.; Shen X.","Li, Xuewei (57204037258); Zhang, Gang (57208224493); Yin, Canbin (57681583300); Wu, Yuquan (57549964200); Shen, Xingchen (57904640000)","57204037258; 57208224493; 57681583300; 57549964200; 57904640000","A Novel Shadow and Layover Segmentation Network for Multi-Angle SAR Images Fusion","2022","IEEE Access","10","","","117770","117781","11","10.1109/ACCESS.2022.3217510","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141519706&doi=10.1109%2fACCESS.2022.3217510&partnerID=40&md5=d70a177d659bf91db25c5823e70dcde0","Shadow and layover are geometric distortion phenomenons in side-view imaging synthetic aperture radar (SAR) systems, especially in mountainous areas and densely populated urban areas. The shadow can block the target of the observation area, making it impossible to obtain the scattering characteristics of the target. The layover causes phase distortion and alters target characteristics. Shadow and layover severely hinder the interpretation of SAR images. To confront the above problems, a multi-angle fusion algorithm based on unsupervised progressive segmentation network is proposed. Firstly, inspired by mega-constellations of low earth orbit, a spaceborne SAR collaborative observation model is proposed to generate multi-angle images of fluctuant terrain. Secondly, according to the difference of echos in the shadow and layover regions, an unsupervised progressive segmentation network is designed to sequentially segment the shadow and layover regions. Finally, to improve the contrast and brightness of the fused SAR image, a single-scale weighted fusion algorithm is designed. Experiments were conducted using the simulated multi-angle SAR images. Compared with single-angle images, the accuracy of target detection and figure-of-merit of the fused SAR image are significantly higher than those of other methods.  © 2013 IEEE.","Image enhancement; Image fusion; Image segmentation; Orbits; Radar imaging; Fusion algorithms; Geometric distortion; Mountainous area; Multi angle; Multi-angle fusion; Shadow and layover; Side view; Synthetic aperture radar images; Unsupervised progressive segmentation network; Urban areas; Synthetic aperture radar","multi-angle fusion; Shadow and layover; synthetic aperture radar; unsupervised progressive segmentation network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141519706"
"Zhou Y.; Zhang F.; Ma F.; Xiang D.; Zhang F.","Zhou, Yongsheng (22959334700); Zhang, Feixiang (57387315100); Ma, Fei (55245276100); Xiang, Deliang (55771045300); Zhang, Fan (56320587700)","22959334700; 57387315100; 55245276100; 55771045300; 56320587700","Small Vessel Detection Based on Adaptive Dual-Polarimetric Feature Fusion and Sea-Land Segmentation in SAR Images","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","2519","2534","15","10.1109/JSTARS.2022.3158807","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126516031&doi=10.1109%2fJSTARS.2022.3158807&partnerID=40&md5=69043295bd3314c9fb44d991d011ad72","Detection of small sea vessels in synthetic aperture radar (SAR) images has received much attention in recent years because the small vessels have weak scattering intensity and few image pixels. The existing detection network structures are not well adapted to small-scale targets, the polarimetric data are not properly utilized, and the sea-land segmentation process to remove land false alarms is time-consuming. Regarding these problems, first, a single low-level path aggregation network is designed specifically for small targets. The structure reduces false alarms at the feature level by finding suitable single-scale feature maps for detection and adding a semantic enhancement module. Second, adaptive dual-polarimetric feature fusion is proposed to filter the multichannel features acquired by dual-polarimetric decomposition to reduce feature redundancy. Third, a segmentation layer is added to the network to shield the land from false alarms. The detection and segmentation layers share the feature extraction and feature fusion modules and are jointly trained by a joint loss. Finally, polarimetric SAR detection and segmentation dataset containing small vessel detection and sea-land segmentation labels is created with reference to the LS-SSDDv1.0 dataset, and experimental results on this dataset verify the improvement of this proposed method over other typical methods.  © 2008-2012 IEEE.","Alarm systems; Clutter (information theory); Errors; Extraction; Image fusion; Image segmentation; Polarimeters; Radar imaging; Semantics; Synthetic aperture radar; Clutter; Features extraction; Features fusions; Images segmentations; Optical imaging; Polarimetric synthetic aperture radars; Radar polarimetry; Sea-land segmentations; Small vessel detection; Vessel detection; decomposition analysis; detection method; image analysis; pattern recognition; pixel; synthetic aperture radar; vessel; Feature extraction","Feature fusion; Polarimetric synthetic aperture radar (SAR); Sea-land segmentation; Small vessel detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126516031"
"Zhang C.; Yang C.; Cheng K.; Guan N.; Dong H.; Deng B.","Zhang, Chao (57487251400); Yang, Chule (57193607936); Cheng, Kaihui (57486749500); Guan, Naiyang (57486749600); Dong, Hongbin (14053990800); Deng, Baosong (14035095400)","57487251400; 57193607936; 57486749500; 57486749600; 14053990800; 14035095400","MSIF: Multisize Inference Fusion-Based False Alarm Elimination for Ship Detection in Large-Scale SAR Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5224811","","","","10.1109/TGRS.2022.3159035","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126284640&doi=10.1109%2fTGRS.2022.3159035&partnerID=40&md5=09231d71addd2e64e36221c4fdaf670e","Ship detection in large-scale synthetic aperture radar (SAR) images has essential value in both military and civilian applications. However, due to the complexity of the background and the simplicity of the texture, ship detection in large-scale SAR images is prone to false alarms, such as similar-shaped reefs, islands, sea clutter, and inland buildings. This article proposes a multisize inference fusion framework to eliminate false alarms and improve the overall performance of ship detection in large-scale SAR images. In this framework, a multisize slicer is proposed to expand the scale range of image expression. Then, a detection model library is built to keep various types of models for different task scenarios and requirements. Finally, two subapproaches are proposed for false alarm elimination, namely, pixel feature filtering (FAE-pff) and multisource fusion (FAE-msf), to reduce false detection results in the output of the detection model. FAE-pff calculates how obvious each target is relative to the background and eliminates less obvious results. FAE-msf obtains bounding boxes and corresponding confidences from multiple inference sources and fuses them through weighting and updating them to achieve complementation and enhancement of information. Various experiments were conducted to evaluate the performance of each module qualitatively and quantitatively. It proves the effectiveness of the proposed framework, which can achieve more correct detections while greatly reducing erroneous detections.  © 1980-2012 IEEE.","Alarm systems; Errors; Image enhancement; Image fusion; Information fusion; Military applications; Radar imaging; Ships; Tracking radar; False alarm elimination; Falsealarms; Large-scale synthetic aperture radar image processing; Large-scales; Multi sizes; Performance; Radar image processing; Ship detection; Synthetic aperture radar; Synthetic aperture radar images; alarm signal; satellite imagery; ship technology; synthetic aperture radar; Synthetic aperture radar","False alarm elimination; information fusion; large-scale SAR image processing; ship detection; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85126284640"
"Shi J.","Shi, Ji (57669978400)","57669978400","SAR target recognition method of MSTAR data set based on multi-feature fusion","2022","Proceedings - 2022 International Conference on Big Data, Information and Computer Network, BDICN 2022","","","","626","632","6","10.1109/BDICN55575.2022.00120","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129681272&doi=10.1109%2fBDICN55575.2022.00120&partnerID=40&md5=f37e27fd1c305be8b37cc524f8f19f83","To solve the problem of low recognition rate of synthetic aperture radar (SAR) target based on feature recognition, a target recognition method of SAR image based on multi-feature fusion is proposed, which combines Hu moment, Harris corner point and Gabor feature. The three kinds of features describe the target's geometric shape feature, corner feature and image texture feature respectively, which can improve the accuracy of SAR target recognition from the aspect of feature extraction. Based on the MSTAR data set, the experiment is carried out under standard and extended operating conditions. The results show that the proposed method can effectively overcome the deficiency of insufficient single feature description information and improve the SAR target recognition rate to a certain extent. © 2022 IEEE.","Automatic target recognition; Extraction; Image enhancement; Image fusion; Image texture; Radar imaging; Radar target recognition; Synthetic aperture radar; Textures; Component; Data set; Features extraction; Features recognition; Image-based; Multi-feature fusion; Radar target; Recognition methods; Synthetic aperture radar images; Target recognition; Feature extraction","component; feature extraction; multi-feature fusion; synthetic aperture radar; target recognition","Conference paper","Final","","Scopus","2-s2.0-85129681272"
"Liao M.; Liu J.; Meng Z.; You Z.","Liao, Maoyou (57339742800); Liu, Jiacheng (57218293741); Meng, Ziyang (24169677800); You, Zheng (7102207846)","57339742800; 57218293741; 24169677800; 7102207846","A sins/sar/gps fusion positioning system based on sensor credibility evaluations","2021","Remote Sensing","13","21","4463","","","","10.3390/rs13214463","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119129299&doi=10.3390%2frs13214463&partnerID=40&md5=c9097eafa1b6ae03f3c92a25e4df2df6","A reliable framework for SINS/SAR/GPS integrated positioning systems is proposed for the case that sensors are in critical environments. Credibility is used to describe the difference between the true error and the initial setting standard deviation. Credibility evaluation methods for inertial measurement unit (IMU), synthetic aperture radar (SAR), and global positioning system (GPS) are presented. In particular, IMU credibility is modeled by noises and constant drifts that are accumulated with time in a strapdown inertial navigation system (SINS). The quality of the SAR image decides the credibility of positioning based on SAR image matching. In addition, a cumulative residual chi-square test is used to evaluate GPS credibility. An extended Kalman filter based on a sensor credibility evaluation is introduced to integrate the measurements. The measurement of a sensor is either discarded when its credibility value is below a threshold or the variance matrix for the estimated state is otherwise adjusted. Simulations show that the final fusion positioning accuracy with credibility evaluation can be improved by 1–2 times compared to that without evaluation. In addition, the derived standard deviation correctly indicates the value of the position error with credibility evaluation. Moreover, the experiments on an unmanned ground vehicle partially verify the proposed evaluation method of GPS and the fusion framework in the actual environment. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Global positioning system; Image fusion; Image matching; Inertial navigation systems; Kalman filters; Radar imaging; Sensor data fusion; Statistics; Synthetic aperture radar; Chi-square tests; Credibility evaluation; Evaluation methods; Inertial measurements units; Integrated Positioning; Positioning system; Reliable frameworks; Standard deviation; Strapdown inertial navigation; Synthetic aperture radar images; Statistical tests","Chi-square test; Credibility evaluation; Data fusion; Image matching; Integrated positioning; Kalman filter","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119129299"
"Islam M.D.S.; Sun X.; Wang Z.; Cheng I.","Islam, MD Samiul (57369268400); Sun, Xinyao (57959394700); Wang, Zheng (57959838300); Cheng, Irene (54790535600)","57369268400; 57959394700; 57959838300; 54790535600","FAPNET: Feature Fusion with Adaptive Patch for Flood-Water Detection and Monitoring †","2022","Sensors","22","21","8245","","","","10.3390/s22218245","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141631320&doi=10.3390%2fs22218245&partnerID=40&md5=4b5e73e6b888bb81e9b98d7e95e4aa71","In satellite remote sensing applications, waterbody segmentation plays an essential role in mapping and monitoring the dynamics of surface water. Satellite image segmentation—examining a relevant sensor data spectrum and identifying the regions of interests to obtain improved performance—is a fundamental step in satellite data analytics. Satellite image segmentation is challenging for a number of reasons, which include cloud interference, inadequate label data, low lighting and the presence of terrain. In recent years, Convolutional Neural Networks (CNNs), combined with (satellite captured) multispectral image segmentation techniques, have led to promising advances in related research. However, ensuring sufficient image resolution, maintaining class balance to achieve prediction quality and reducing the computational overhead of the deep neural architecture are still open to research due to the sophisticated CNN hierarchical architectures. To address these issues, we propose a number of methods: a multi-channel Data-Fusion Module (DFM), Neural Adaptive Patch (NAP) augmentation algorithm and re-weight class balancing (implemented in our PHR-CB experimental setup). We integrated these techniques into our novel Fusion Adaptive Patch Network (FAPNET). Our dataset is the Sentinel-1 SAR microwave signal, used in the Microsoft Artificial Intelligence for Earth competition, so that we can compare our results with the top scores in the competition. In order to validate our approach, we designed four experimental setups and in each setup, we compared our results with the popular image segmentation models UNET, VNET, DNCNN, UNET++, U2NET, ATTUNET, FPN and LINKNET. The comparisons demonstrate that our PHR-CB setup, with class balance, generates the best performance for all models in general and our FAPNET approach outperforms relative works. FAPNET successfully detected the salient features from the satellite images. FAPNET with a MeanIoU score of 87.06% outperforms the state-of-the-art UNET, which has a score of 79.54%. In addition, FAPNET has a shorter training time than other models, comparable to that of UNET (6.77 min for 5 epochs). Qualitative analysis also reveals that our FAPNET model successfully distinguishes micro waterbodies better than existing models. FAPNET is more robust to low lighting, cloud and weather fluctuations and can also be used in RGB images. Our proposed method is lightweight, computationally inexpensive, robust and simple to deploy in industrial applications. Our research findings show that flood-water mapping is more accurate when using SAR signals than RGB images. Our FAPNET architecture, having less parameters than UNET, can distinguish micro waterbodies accurately with shorter training time. © 2022 by the authors.","Algorithms; Artificial Intelligence; Floods; Image Processing, Computer-Assisted; Neural Networks, Computer; Water; Convolutional neural networks; Cost effectiveness; Data Analytics; Deep neural networks; Feature extraction; Floods; Image enhancement; Image fusion; Image resolution; Mapping; Network architecture; Radar imaging; Remote sensing; Satellite imagery; Surface waters; Synthetic aperture radar; water; Flood waters; Flood-water mapping; Images segmentations; Performance; SAR imagery; Satellite image analysis; Satellite images; Water mapping; Waterbodies; Waterbody detection; algorithm; artificial intelligence; flooding; image processing; procedures; Image segmentation","flood-water mapping; image segmentation; SAR imagery; satellite image analysis; waterbody detection","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141631320"
"Ghoniemy T.M.; Hammad M.M.; Amein A.S.; Mahmoud T.A.","Ghoniemy, Tarek M. (57207793796); Hammad, Mahmoud M. (57344722300); Amein, A.S. (55892252700); Mahmoud, Tarek A. (7004694196)","57207793796; 57344722300; 55892252700; 7004694196","Multi-stage guided-filter for SAR and optical satellites images fusion using Curvelet and Gram Schmidt transforms for maritime surveillance","2021","International Journal of Image and Data Fusion","","","","","","","10.1080/19479832.2021.2003446","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119374374&doi=10.1080%2f19479832.2021.2003446&partnerID=40&md5=51447702bea91678ccc910b3e4abce15","Synthetic aperture radar (SAR) images depend on the dielectric properties of objects with certain incident angles. Thus, vessels and other metallic objects appear clear in SAR images however, they are difficult to be distinguished in optical images. Synergy of these two types of images leads to not only high spatial and spectral resolutions but also good explanation of the image scene. In this paper, a hybrid pixel-level image fusion method is proposed for integrating panchromatic (PAN), multispectral (MS) and SAR images. The fusion method is performed using Multi-stage guided filter (MGF) for optical images pansharpening, to get high preserving spatial details and nested Gram-Schmidt (GS) and Curvelet-Transform (CVT) methods for SAR and optical images,to increase the quality of the final fused image and benefit from the SAR image properties. The accuracy and performance of the proposed method are appraised using Landsat-8 Operational-Land-Imager (OLI) and Sentinel-1 images subjectively as well as objectively using different quality metrics. Moreover, the proposed method is compared to a number of state-of-the-art fusion techniques. The results show significant improvements in both visual quality and the spatial and spectral evaluation metrics. Consequently, the proposed method is capable of highlighting maritime activity for further processing. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Dielectric properties; Geometrical optics; Radar imaging; Synthetic aperture radar; Curvelet transforms; Curvelets; Gram-Schmidt transform; Guided filters; Maritime-surveillance; Multi-stages; Optical image; Optical satellite images; Radar satellites; Synthetic aperture radar images; Image fusion","curvelet transform; Gram-Schmidt transform; image fusion; maritime surveillance; SAR image","Article","Article in press","","Scopus","2-s2.0-85119374374"
"Masiza W.; Chirima J.G.; Hamandawana H.; Pillay R.","Masiza, Wonga (57218918626); Chirima, Johannes George (54388710100); Hamandawana, Hamisai (56616595700); Pillay, Rajendran (57205320422)","57218918626; 54388710100; 56616595700; 57205320422","Enhanced mapping of a smallholder crop farming landscape through image fusion and model stacking","2020","International Journal of Remote Sensing","41","22","","8736","8753","17","10.1080/01431161.2020.1783017","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090785369&doi=10.1080%2f01431161.2020.1783017&partnerID=40&md5=097f27e9333aa1245d836a1dc2a70d90","Globally, Smallholder farming systems (SFS) are recognized as one of the most important pillars of rural economic development and poverty alleviation because of their contribution to food security. However, support for this agricultural sector is hampered by lack of reliable information on the distributions and acreage of smallholder fields. This information is essential in not only monitoring food security and informing markets but also in guiding the determination of levels of support required from government by individual farmers. There is urgent need for robust techniques that can be used to cost-effectively and time-efficiently map smallholder crop fields especially in Sub-Saharan Africa and Asia. This study attempts to do this by using an approach in which optical and Synthetic Aperture Radar (SAR) data are systematically combined and classified using Extreme Gradient Boosting (Xgboost). We also investigated model stacking as another technique to improve classification accuracy. We combined Xgboost with Random Forest (RF), Support Vector Machine (SVM), Artificial Neural Networks (ANN), and Naïve Bayes (NB). The combined use of multi-temporal Sentinel-2 bands, spectral indices, and Sentinel-1 produced better results than exclusive use of optical data (α = 0.95, p = 0.0005). Furthermore, stacking of classification algorithms based on model comparisons achieved higher accuracy than stacking the algorithms indiscriminately (α = 0.95, p = 0.0100). Through systematic fusion of SAR and optical data and hyper-parameter tuning of Xgboost, we achieved a maximum classification accuracy of 97.71%, while achieving a maximum accuracy of 96.06% through model stacking. This highlights the importance of multi-sensor data fusion and multi-classifier systems when mapping fragmented agricultural landscapes. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Asia; Sub-Saharan Africa; Agricultural robots; Crops; Decision trees; Food supply; Image enhancement; Image fusion; Mapping; Radar imaging; Support vector machines; Synthetic aperture radar; Agricultural landscapes; Agricultural sector; Classification accuracy; Classification algorithm; Maximum accuracies; Multiclassifier system; Multisensor data fusion; Poverty alleviation; agricultural land; artificial neural network; crop; food security; image processing; mapping method; smallholder; support vector machine; synthetic aperture radar; Sensor data fusion","","Article","Final","","Scopus","2-s2.0-85090785369"
"Okolie C.J.; Smit J.L.","Okolie, Chukwuma J. (55695845300); Smit, Julian L. (57218147733)","55695845300; 57218147733","A systematic review and meta-analysis of Digital elevation model (DEM) fusion: pre-processing, methods and applications","2022","ISPRS Journal of Photogrammetry and Remote Sensing","188","","","1","29","28","10.1016/j.isprsjprs.2022.03.016","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127637343&doi=10.1016%2fj.isprsjprs.2022.03.016&partnerID=40&md5=1938d666115bcd2493ddce8b7f46a958","The remote sensing community has identified data fusion as one of the key challenging topics of the 21st century. The subject of image fusion in two-dimensional (2D) space has been covered in several published reviews. However, the special case of 2.5D/3D Digital Elevation Model (DEM) fusion has not been addressed till date. DEM fusion is a key application of data fusion in remote sensing. It takes advantage of the complementary characteristics of multi-source DEMs to deliver a more complete, accurate, and reliable elevation dataset. Although several methods for fusing DEMs have been developed, the absence of a well-rounded review has limited their proliferation among researchers and end-users. Combining knowledge from multiple studies is often required to inform a holistic perspective and guide further research. In response, this paper provides a systematic review of DEM fusion: the pre-processing workflow, methods and applications, enhanced with a meta-analysis. Through the discussion and comparative analysis, unresolved challenges and open issues are identified, and future directions for research are proposed. This review is a timely solution and an invaluable source of information for researchers within the fields of remote sensing and spatial information science, and the data fusion community at large. © 2022 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Digital instruments; Geomorphology; Image fusion; Remote sensing; Space optics; Digital elevation model; Digital elevation model fusion; InSAR; LiDAR; Model fusion; Multi-sensor fusion; Remote sensing image fusion; Remote sensing images; Systematic Review; Weight maps; data acquisition; digital elevation model; lidar; meta-analysis; remote sensing; synthetic aperture radar; Surveying","Data fusion; Digital elevation model fusion; InSAR; LiDAR; Multi-sensor fusion; Remote sensing image fusion; Weight maps","Short survey","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127637343"
"Wu H.; Song H.; Huang J.; Zhong H.; Zhan R.; Teng X.; Qiu Z.; He M.; Cao J.","Wu, Han (57216770303); Song, Huina (57191965406); Huang, Jianhua (57948904300); Zhong, Hua (57192655350); Zhan, Ronghui (12798301200); Teng, Xuyang (54421169900); Qiu, Zhaoyang (55542960100); He, Meilin (57211028474); Cao, Jiayi (57220954160)","57216770303; 57191965406; 57948904300; 57192655350; 12798301200; 54421169900; 55542960100; 57211028474; 57220954160","Flood Detection in Dual-Polarization SAR Images Based on Multi-Scale Deeplab Model","2022","Remote Sensing","14","20","5181","","","","10.3390/rs14205181","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140999024&doi=10.3390%2frs14205181&partnerID=40&md5=5eb378ba4ca3f37707d9281af966adf4","The proliferation of massive polarimetric Synthetic Aperture Radar (SAR) data helps promote the development of SAR image interpretation. Due to the advantages of powerful feature extraction capability and strong adaptability for different tasks, deep learning has been adopted in the work of SAR image interpretation and has achieved good results. However, most deep learning methods only employ single-polarization SAR images and ignore the water features embedded in multi-polarization SAR images. To fully exploit the dual-polarization SAR data and multi-scale features of SAR images, an effective flood detection method for SAR images is proposed in this paper. In the proposed flood detection method, a powerful Multi-Scale Deeplab (MS-Deeplab) model is constructed based on the dual-channel MobileNetV2 backbone and the classic DeeplabV3+ architecture to improve the ability of water feature extraction in SAR images. Firstly, the dual-channel feature extraction backbone based on the lightweight MobileNetV2 separately trains the dual-polarization SAR images, and the obtained training parameters are merged with the linear weighting to fuse dual-polarization water features. Given the multi-scale space information in SAR images, then, a multi-scale feature fusion module is introduced to effectively utilize multi-layer features and contextual information, which enhances the representation of water features. Finally, a joint loss function is constructed based on cross-entropy and a dice coefficient to deal with the imbalanced categorical distribution in the training dataset. The experimental results on the time series of Sentinel-1A SAR images show that the proposed method for flood detection has a strong ability to locate water boundaries and tiny water bodies in complex scenes. In terms of quantitative assessment, MS-Deeplab can achieve a better performance compared with other mainstream semantic segmentation models, including PSPNet, Unet and the original DeeplabV3+ model, with a 3.27% intersection over union (IoU) and 1.69% pixel accuracy (PA) improvement than the original DeeplabV3+ model. © 2022 by the authors.","Deep learning; Extraction; Feature extraction; Floods; Image analysis; Image enhancement; Image fusion; Learning systems; Polarization; Radar imaging; Semantics; Space-based radar; Tracking radar; Deeplab model; Detection methods; Dual-polarizations; Features extraction; Features fusions; Flood detections; Image interpretation; Multi-scale features; Multi-scales; Synthetic aperture radar images; Synthetic aperture radar","Deeplab model; feature fusion; flood detection; synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140999024"
"Gao F.; Xu J.; Lang R.; Wang J.; Hussain A.; Zhou H.","Gao, Fei (55821194900); Xu, Jingming (57907868700); Lang, Rongling (23393168700); Wang, Jun (57200022394); Hussain, Amir (19734290900); Zhou, Huiyu (23062556900)","55821194900; 57907868700; 23393168700; 57200022394; 19734290900; 23062556900","A Few-Shot Learning Method for SAR Images Based on Weighted Distance and Feature Fusion","2022","Remote Sensing","14","18","4583","","","","10.3390/rs14184583","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138820638&doi=10.3390%2frs14184583&partnerID=40&md5=cf45d0c4f31a46c5dc32a5b3159c8239","Convolutional Neural Network (CNN) has been widely applied in the field of synthetic aperture radar (SAR) image recognition. Nevertheless, CNN-based recognition methods usually encounter the problem of poor feature representation ability due to insufficient labeled SAR images. In addition, the large inner-class variety and high cross-class similarity of SAR images pose a challenge for classification. To alleviate the problems mentioned above, we propose a novel few-shot learning (FSL) method for SAR image recognition, which is composed of the multi-feature fusion network (MFFN) and the weighted distance classifier (WDC). The MFFN is utilized to extract input images’ features, and the WDC outputs the classification results based on these features. The MFFN is constructed by adding a multi-scale feature fusion module (MsFFM) and a hand-crafted feature insertion module (HcFIM) to a standard CNN. The feature extraction and representation capability can be enhanced by inserting the traditional hand-crafted features as auxiliary features. With the aid of information from different scales of features, targets of the same class can be more easily aggregated. The weight generation module in WDC is designed to generate category-specific weights for query images. The WDC distributes these weights along the corresponding Euclidean distance to tackle the high cross-class similarity problem. In addition, weight generation loss is proposed to improve recognition performance by guiding the weight generation module. Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset and the Vehicle and Aircraft (VA) dataset demonstrate that our proposed method surpasses several typical FSL methods. © 2022 by the authors.","Convolution; Convolutional neural networks; Image fusion; Image recognition; Learning systems; Radar imaging; Radar target recognition; Convolutional neural network; Distance classifiers; Few-shot learning; Learning methods; Multi-feature fusion; Synthetic aperture radar; Synthetic aperture radar images; Weighted distance; Weights generations; Synthetic aperture radar","convolutional neural network (CNN); few-shot learning; radar target recognition; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85138820638"
"Quan D.; Wei H.; Wang S.; Lei R.; Duan B.; Li Y.; Hou B.; Jiao L.","Quan, Dou (57192699561); Wei, Huiyuan (57727909200); Wang, Shuang (55940463600); Lei, Ruiqi (57481744700); Duan, Baorui (57482064100); Li, Yi (57207042662); Hou, Biao (7102142690); Jiao, Licheng (7102491544)","57192699561; 57727909200; 55940463600; 57481744700; 57482064100; 57207042662; 7102142690; 7102491544","Self-Distillation Feature Learning Network for Optical and SAR Image Registration","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4706718","","","","10.1109/TGRS.2022.3173476","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131341577&doi=10.1109%2fTGRS.2022.3173476&partnerID=40&md5=ff1dcdbed9c10ebb0035b8d56aceee73","Optical and synthetic aperture radar (SAR) image registration is important for multimodal remote sensing image information fusion. Recently, deep matching networks have shown better performances than traditional methods of image matching. However, due to significant differences between optical and SAR images, the performances of existing deep learning methods still need to be further improved. This article proposes a self-distillation feature learning network (SDNet) for optical and SAR image registration, improving performance from network structure and network optimization. First, we explore the impact of different weight-sharing strategies on optical and SAR image matching. Then, we design a partially unshared feature learning network for multimodal image feature learning. It has fewer parameters than the fully unshared network and has more flexibility than the fully shared network. In addition, the limited binary supervised information (matching or nonmatching) is insufficient to train the deep matching networks for optical-SAR image registration. Thus, we propose a self-distillation feature learning method to exploit more similarity information for deep network optimization enhancement, such as the similarity ordering between a series of nonmatching patch pairs. The exploited rich similarity information will significantly enhance network training and improve matching accuracy. Finally, existing deep learning methods brute-force make the matching features of the optical and SAR image patches similar, which will lead to the loss of discriminative information and degeneration of the matching performances. Thus, we build an auxiliary task reconstruction learning to optimize the feature learning network to keep more discriminative information. Extensive experiments demonstrate the effectiveness of our proposed method on multimodal image registration. © 1980-2012 IEEE.","Adaptive optics; Deep learning; Distillation; Image enhancement; Image fusion; Image matching; Image registration; Radar imaging; Remote sensing; Sensor data fusion; Structural optimization; Synthetic aperture radar; Deep matching; Features extraction; Images registration; Matchings; Multimodal images; Optical image; Optical imaging; Radar polarimetry; Representation learning; SAR Images; Self-distillation learning; image analysis; machine learning; optimization; remote sensing; synthetic aperture radar; Geometrical optics","Deep matching; image registration; multimodal image; optical image; self-distillation learning; synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85131341577"
"Yu Q.; Jiang Y.; Zhao W.; Sun T.","Yu, Qiuze (7402947731); Jiang, Yuxuan (57208133213); Zhao, Wensen (57726357500); Sun, Tao (57220544728)","7402947731; 57208133213; 57726357500; 57220544728","High-Precision Pixelwise SAR-Optical Image Registration via Flow Fusion Estimation Based on an Attention Mechanism","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","3958","3971","13","10.1109/JSTARS.2022.3172449","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131279949&doi=10.1109%2fJSTARS.2022.3172449&partnerID=40&md5=c25054f84bb0ff8b5589590ba166f027","Due to the severe speckle noise and complex local deformation in synthetic aperture radar (SAR) images, the problem of high-precision pixelwise registration (dense registration) between SAR and optical images remains far from resolved. In this article, an attention mechanism based optical flow fusion algorithm is proposed to achieve high-precision dense SAR-optical image registration. First, two descriptors, the scale-invariant feature transform (SIFT) and a descriptor based on phase congruency (PC), are used to describe SAR and optical images to eliminate their intensity differences. Then, a salient feature map is extracted as a query matrix to weight the optical flow energy function. When extracting the salient feature map, the Contour Robuste d'Ordre Non Entier detector and the ratio of exponentially weighted averages operator are used to eliminate additive and multiplicative noise in the optical and SAR images, respectively. Finally, the optical flow fields based on SIFT and the PC-based descriptor are fused to compensate for registration ambiguity. Experimental results show that our method is feasible, effective, and robust to noise, and it enables high-precision registration under local deformation. © 2008-2012 IEEE.","Adaptive optics; Deformation; Geometrical optics; Image fusion; Image registration; Optical flows; Radar imaging; High-precision; Images registration; Optical distortion; Optical image; Optical imaging; Optical-; Pixelwise registration (dense registration); Radar polarimetry; Synthetic aperture radar and optical; Synthetic aperture radar images; algorithm; deformation; estimation method; image processing; speckle; synthetic aperture radar; Synthetic aperture radar","Optical flow; pixelwise registration (dense registration); SAR and optical; synthetic aperture radar (SAR) image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131279949"
"Zhou F.; Yang J.; Jia L.; Yang X.; Xing M.","Zhou, Fang (36761642900); Yang, Jun (56937536300); Jia, Lu (55541053400); Yang, Xingming (56413137900); Xing, Mengdao (7005922869)","36761642900; 56937536300; 55541053400; 56413137900; 7005922869","Ultra-high resolution imaging method for distributed small satellite spotlight mimo-sar based on sub-aperture image fusion","2021","Sensors","21","5","1609","1","17","16","10.3390/s21051609","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101384759&doi=10.3390%2fs21051609&partnerID=40&md5=7a685df21359a990eab94b6988dc2a6d","Small satellite synthetic aperture radar (SAR) has become a new development direction of spaceborne SAR due to its advantages of flexible launch, short development cycle, and low cost. However, there are fewer researches on distributed small satellite multiple input multiple output (MIMO) SAR. This paper proposes an ultra-high resolution imaging method for the distributed small satellite spotlight MIMO-SAR, which applies the sub-aperture division technique and the sub-aperture image coherent fusion algorithm to MIMO-SAR. After deblurring the sub-aperture signal, the large bandwidth signal is obtained by using an improved time domain bandwidth synthesis (TBS) method, and then the ultra-high resolution image is obtained by using a sub-aperture image coherent fusion algorithm. Simulation results validate the feasibility and effectiveness of the proposed approach. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Bandwidth; Image enhancement; Image fusion; MIMO systems; Radar imaging; Small satellites; Space-based radar; Time domain analysis; Bandwidth synthesis; Development cycle; Development directions; Distributed small satellites; Fusion algorithms; Space-borne SAR; Sub-apertures; Ultrahigh resolution; algorithm; article; feasibility study; image analysis; simulation; synthesis; telecommunication; Synthetic aperture radar","Distributed small satellite; Spotlight multiple input multiple output synthetic aperture radar (MIMO-SAR); Sub-aperture image coherent fusion; Time domain bandwidth synthesis (TBS)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101384759"
"Jiang M.; Li J.; Shen H.","Jiang, Menghui (57210173702); Li, Jie (57214207213); Shen, Huanfeng (8359721100)","57210173702; 57214207213; 8359721100","A DEEP LEARNING-BASED HETEROGENEOUS SPATIO-TEMPORAL-SPECTRAL FUSION: SAR AND OPTICAL IMAGES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","1252","1255","3","10.1109/IGARSS47720.2021.9554031","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129873154&doi=10.1109%2fIGARSS47720.2021.9554031&partnerID=40&md5=4660e59201bacdf2be7e5cdca57bde6a","Image fusion is a powerful means to integrate complementary spatio-temporal-spectral information among multi-source remote sensing images. The existing remote sensing image fusion is mostly limited to the fusion between optical images, and most of them are limited to the fusion between two sensors. Based on this, this paper proposes a heterogeneous spatiotemporal-spectral fusion method based on deep learning. Specifically, it combines the low-spatial-resolution (LR) cloudy image with the high-spatial-resolution (HR) SAR images and the HR cloud-free optical image to remove the clouds and improve the spatial resolution of the LR cloudy image. The SAR image is acquired at the same date as the LR cloudy image, while the HR cloud-free image is acquired at another date. Experiments are performed on the images of Landsat 8, Sentinel-1, and Sentinel-2. The experimental results show that the proposed method can effectively achieve the joint goal of spatial resolution improvement and cloud removal of the Landsat image. © 2021 IEEE","Deep learning; Geometrical optics; Image acquisition; Image enhancement; Image resolution; Radar imaging; Remote sensing; Synthetic aperture radar; Cloud removal; Heterogeneous; High spatial resolution; Optical image; Optical-; Resolution improvement; SAR; SAR Images; Spatial resolution; Spatial resolution improvement; Image fusion","cloud-removal; Heterogeneous; optical; SAR; spatial resolution improvement","Conference paper","Final","","Scopus","2-s2.0-85129873154"
"Qu H.; Gao J.; Liu W.; Wang X.","Qu, Haicheng (55567658100); Gao, Jiankang (57449657500); Liu, Wanjun (35293070500); Wang, Xiaona (57449105200)","55567658100; 57449657500; 35293070500; 57449105200","An Anchor-free Method Based on Context Information Fusion and Interacting Branch for Ship Detection in SAR Images; [上下文信息融合与分支交互的SAR图像舰船无锚框检测]","2022","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","44","1","","380","389","9","10.11999/JEIT201059","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124494762&doi=10.11999%2fJEIT201059&partnerID=40&md5=93e2d50ff0a0fe6a6d0c3bbf9a6440a6","Ship targets are sparsely distributed in Synthetic Aperture Radar (SAR) images, and the design of anchor frame has a great impact on the accuracy and generalization of existing SAR image target detection method based on anchor. Therefore, an anchor-free method based on context information fusion and interacting branch for ship detection in SAR images (named as CI-Net) is proposed. Considering the diversity of ship scale in SAR images, a context fusion module is designed in the feature extraction stage, integrate high and low levels of information in a bottom-up manner and refine the extracted features to be detected by combining with the target context information. Secondly, aiming at the problem of complex targets in the scene is not accurate, interacting branch module is put forward. In the detection phase, use classification branches optimization regression testing box is used, to improve the target frame's precision. At the same time, the new Intersection over Union (IOU) is used on branches of the classification to improve detection network classification confidence, to inhibit detection box of low quality. Experimental results show that the proposed method achieves good detection results on both SSDD and SAR-Ship-Dataset, with Average Precision (AP) reaching 92.56% and 88.32%, respectively. Compared with other ship detection methods in SAR image, the proposed method not only has excellent performance in accuracy, but also has a faster detection speed after abandoning the complex calculation related to anchor frame. It also has a certain practical significance for real-time target detection in SAR image. © 2022, Science Press. All right reserved.","Complex networks; Feature extraction; Image fusion; Radar imaging; Ships; Statistical tests; Anchor-free; Context information; Detection methods; Generalisation; Self-attention; Ship detection; Ship targets; Synthetic aperture radar; Synthetic aperture radar images; Targets detection; Synthetic aperture radar","Anchor-free; Context information; Self-attention; Ship detection; Synthetic Aperture Radar (SAR)","Article","Final","","Scopus","2-s2.0-85124494762"
"Ge Y.; Xiong Z.; Lai Z.","Ge, Yuchen (57674884900); Xiong, Zhaolong (57226194490); Lai, Zuomei (57226188501)","57674884900; 57226194490; 57226188501","Image registration of SAR and Optical based on salient image sub-patches","2021","Journal of Physics: Conference Series","1961","1","012017","","","","10.1088/1742-6596/1961/1/012017","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110802569&doi=10.1088%2f1742-6596%2f1961%2f1%2f012017&partnerID=40&md5=228451c75b9d76f934a96f7c6b1021fe","As a fundamental and critical task in multi-source image fusion, the registration of optical image and synthetic aperture radar (SAR) image can identify corresponding identical or similar structures from two heterogeneous images. Although Pseudo Siamese network has achieved notable success in matching heterogeneous images, the network prone to mismatch when there are blurred, duplicate or similar scenes appeared in the search image. To improve the performance, in this paper, the OPT-to-SAR image pair is cut into sub-patch pairs through a pre-defined sliding window. Then a two-stage image filtering mechanism is proposed to maintain candidate sub-patches with ideal texture information. After sending all the qualified sub-patch pairs into the Pseudo Siamese network, the final matching result will be passed through a RANSAC module. In this way, the interference from invalid image areas can be reduced and the model robustness can be ensured by using the statistic information of the whole image pair. A series of experiments conducted under various data scenarios proved the effectiveness of the method. © Published under licence by IOP Publishing Ltd.","Geometrical optics; Image enhancement; Image fusion; Information filtering; Synthetic aperture radar; Textures; Critical tasks; Image filtering; Model robustness; Multi-source images; Sliding Window; Statistic informations; Synthetic aperture radar (SAR) images; Texture information; Radar imaging","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85110802569"
"Li C.; Cai M.; Xu P.; Liang Y.","Li, Cong (36675195200); Cai, Meng (57446591300); Xu, Ping (57446228900); Liang, Yi (55479382100)","36675195200; 57446591300; 57446228900; 55479382100","SAR and Infrared Image Fusion based on Latent Low-rank Representation","2021","CISS 2021 - 2nd China International SAR Symposium","","","","","","","10.23919/CISS51089.2021.9652254","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124339744&doi=10.23919%2fCISS51089.2021.9652254&partnerID=40&md5=a54eb15271ed66ffe2cec045d9702a9d","To solve the problems of image information loss and spectral distortion during the fusion of SAR and infrared images, this paper proposes a SAR and infrared image fusion method based on Latent Low-Rank Representation (LatLRR). First, the method uses Non-Subsampled Contourlet Transform (NSCT) to obtain the low-frequency and high-frequency information of the source image. Then, the low-frequency information determines the fusion weight of the low-frequency part and the high-frequency uses LatLRR to extract low-rank components for adaptive weighted fusion. Finally, uses inverse NSCT transformation on the fusion coefficients to obtain the fusion image. Compared with other typical fusion methods, the proposed method has better visual effects, and the objective evaluation parameter values are also improved.  © 2021 SISE.","Image fusion; Infrared imaging; Inverse problems; Radar imaging; Synthetic aperture radar; Contourlet transform; Image information; Information loss; Infrared image fusions; Latent low rank representation; Low-rank representations; Lower frequencies; Non-subsampled contourlet; Non-subsampled contourlet transform; Spectral distortions; Image enhancement","image fusion; latent low rank representation; non-subsampled contourlet transform","Conference paper","Final","","Scopus","2-s2.0-85124339744"
"Patidar D.; Sharma N.","Patidar, Dhruv (57937494700); Sharma, Nitin (57205213366)","57937494700; 57205213366","Identifying corresponding SAR-Optical patches using Inception-inspired Pseudo-Siamese CNN","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","719","722","3","10.1109/IGARSS46834.2022.9884356","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140409130&doi=10.1109%2fIGARSS46834.2022.9884356&partnerID=40&md5=5509b389c6e3ea50c06c634ad7412176","Even for humans, at times interpreting a SAR image can prove to be quite challenging, let alone finding and identifying the corresponding optical patch. The problem of identifying corresponding SAR-optical patches finds many applications in remote sensing and computer vision. While there have been attempts to solve the problem with the use of convolutional neural networks like Siamese networks, the excellence of more recent advances can be used to solve it more effectively. This paper is an attempt towards the same. We use Inception-inspired convolutional modules in a Pseudo-Siamese network to make progress in solving the problem of identifying corresponding patches in SAR and Optical images. © 2022 IEEE.","Convolution; Convolutional neural networks; Deep neural networks; Geometrical optics; Image fusion; Optical remote sensing; Convolutional neural network; Deep learning; Deep matching; Inception module; Matchings; Optical-; Pseudo-siamese network; Remote-sensing; SAR Images; Siamese network; Synthetic aperture radar","Convolutional Neural Network; Data Fusion; Deep Learning; Deep Matching; Image Matching; Inception module; Pseudo-Siamese Networks; Siamese Networks","Conference paper","Final","","Scopus","2-s2.0-85140409130"
"Paul S.; Pati U.C.","Paul, Sourabh (57189355649); Pati, Umesh C. (27467664400)","57189355649; 27467664400","High-resolution optical-to-SAR image registration using mutual information and SPSA optimisation","2021","IET Image Processing","15","6","","1319","1331","12","10.1049/ipr2.12107","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104275117&doi=10.1049%2fipr2.12107&partnerID=40&md5=b67bdc7973bc1626e8dfc145521ea45e","The high-resolution optical and synthetic aperture radar (SAR) images are widely used in many remote sensing application areas such as image fusion and change detection where image registration is a fundamental step. The latest high-resolution optical and SAR satellites and airborne systems provide geometrically corrected images which do not contain global deformations. Though the images do not have global differences, still, registration differences exist between these optical and SAR images. These registration differences should be minimised through an automatic registration method before using the images for the aforementioned applications. However, an automatic optical-to-SAR image registration is a challenging task due to the presence of significant nonlinear intensity differences as well as local geometric distortions between the images. In order to solve these problems, an automatic optical-to-SAR image registration method is proposed which can effectively handle the registration differences between the globally corrected high-resolution images. In the proposed method, initially, a coarse registration is performed by using a discrete simultaneous perturbation stochastic approximation (SPSA) optimisation. Then, a smooth continuous SPSA optimisation is utilised for the fine registration of the images. Experiments are performed on six sets of high-resolution optical-SAR image pairs and the results show the effectiveness of the proposed method. © 2020 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Image fusion; Image registration; Optical resolving power; Optimization; Remote sensing; Stochastic systems; Synthetic aperture radar; Automatic registration; Geometric distortion; High resolution image; Intensity difference; Registration methods; Remote sensing applications; Simultaneous perturbation stochastic approximation; Synthetic aperture radar (SAR) images; Radar imaging","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85104275117"
"Mao R.; Li H.; Ren G.; Yin Z.","Mao, Ruihan (57888228800); Li, Hua (57888002400); Ren, Gaofeng (57887772600); Yin, Zhangcai (57888002500)","57888228800; 57888002400; 57887772600; 57888002500","Cloud Removal Based on SAR-Optical Remote Sensing Data Fusion via a Two-Flow Network","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","7677","7686","9","10.1109/JSTARS.2022.3203508","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137874334&doi=10.1109%2fJSTARS.2022.3203508&partnerID=40&md5=a75e6bc0ef30b853b68bfcae47bffb4b","Optical remote sensing imagery plays an important role in observing the Earth's surface today. However, it is not easy to obtain complete multitemporal optical remote sensing images because of the cloud cover, how reconstructing cloud-free optical images has become a big challenge task in recent years. Inspired by the remote sensing fusion methods based on the convolutional neural network model, we propose a two-flow network to remove clouds from optical images. In the proposed method, synthetic aperture radar images are used as auxiliary data to guide optical image reconstruction, which is not influenced by cloud cover. In addition, a novel loss function called content loss is introduced to improve image quality. The ablation experiment of the loss function also proves that content loss is indeed effective. To be more in line with a real situation, the network is trained, tested, and validated on the SEN12MS-CR dataset, which is a global real cloud-removal dataset. The experimental results show that the proposed method is better than other state-of-the-art methods in many indicators (RMSE, SSIM, SAM, and PSNR).  © 2008-2012 IEEE.","Convolution; Deep learning; Geometrical optics; Image enhancement; Image fusion; Image reconstruction; Neural networks; Optical remote sensing; Radar imaging; Space-based radar; Cloud removal; Convolutional neural network; Deep learning; Images reconstruction; Optical imaging; Optical-; Radar polarimetry; Remote-sensing; Synthetic aperture radar; artificial neural network; cloud; data processing; machine learning; optical method; remote sensing; synthetic aperture radar; Synthetic aperture radar","Cloud removal; data fusion; deep learning; optical; remote sensing; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137874334"
"Yehia A.; Safy M.; Amein A.S.","Yehia, Abdelrahman (57207947584); Safy, Mohamed (36618286100); Amein, Ahmed S. (55892252700)","57207947584; 36618286100; 55892252700","Fusion of high-resolution sar and optical imageries based on a wavelet transform and ihs integrated algorithm","2021","International Journal of Engineering Research in Africa","52","","","62","72","10","10.4028/www.scientific.net/JERA.52.62","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099728601&doi=10.4028%2fwww.scientific.net%2fJERA.52.62&partnerID=40&md5=e05a3ad20530c9365835aea104e01123","Multi-sensor remote sensing data can significantly improve the interpretation and usage of large volume data sources. A combination of satellite Synthetic Aperture Radar (SAR) data and optical sensors enables the use of complementary features of the same image. In this paper, SAR data is injected into optical image using a combining fusion method based on the integration of wavelet Transform and IHS (Intensity, Hue, and Saturation) transform. Not only to preserve the spectral information of the original (MS) image, but also to maintain the spatial content of the highresolution SAR image. Two data sets are used to evaluate the proposed fusion algorithm: one of them is Pleiades, Turkey and the other one is Boulder, Colorado, USA. The different fused outputs are compared using different image quality indices. Visual and statistical assessment of the fused outputs displays that the proposed approach has an effective translation from SAR to the optical image. Hence, enhances the SAR image interpretability. © 2021 Trans Tech Publications Ltd, Switzerland.","Geometrical optics; Image enhancement; Image quality; Remote sensing; Synthetic aperture radar; Wavelet transforms; A-wavelet transform; Complementary features; Effective translation; Image quality index; Integrated algorithm; Remote sensing data; Spectral information; Statistical assessment; Radar imaging","IHS; Image fusion; SAR; optical; Wavelet","Article","Final","","Scopus","2-s2.0-85099728601"
"Song L.; Liu A.; Huang Z.","Song, Liwen (57223046611); Liu, Aifang (7402583778); Huang, Zuzhen (57188590919)","57223046611; 7402583778; 57188590919","A Multichannel SAR-GMTI Method Based on Multi-Polarization SAR Image Fusion","2021","IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","","","9390769","2678","2684","6","10.1109/IAEAC50856.2021.9390769","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104601622&doi=10.1109%2fIAEAC50856.2021.9390769&partnerID=40&md5=046d1e1def73e385c473e3220e9b9923","Displaced phase center array (DPCA) is a classical multichannel synthetic aperture radar(SAR)-based ground moving target indication (SAR-GMTI) method. It has been widely used in the real SAR system because of its simple operation and low computational complexity. However, in the complex clutter environment, there are often many false alarms in the DPCA results, such as the urban area. In this paper, a new GMTI method based on the multi-polarization SAR image information is proposed. Since the amplitude of the urban area in the cross-polarization SAR image is weaker than that in the copolarization image, the urban area in the copolarization image can be replaced with the cross-polarization image, then the DPCA processing is performed for the fusional image. Compared with traditional DPCA technique, this method can reduce the false alarms in the complex environment, such as the urban area. Finally, the effectiveness of the proposed method is verified by the real multichannel quadrature-polarization SAR data. © 2021 IEEE.","Errors; Image fusion; Polarization; Synthetic aperture radar; Complex environments; Cross polarizations; Cross-polarization images; Displaced phase centers; Ground moving target indication; Low computational complexity; Multi-polarization; Multichannel synthetic aperture radars (SAR); Radar imaging","displaced phase center array(DPCA); ground moving target indication(GMTI); polarization; synthetic aperture radar(SAR)","Conference paper","Final","","Scopus","2-s2.0-85104601622"
"Vargas H.; Ramirez J.; Pinilla S.; Martinez-Torre J.I.","Vargas, Hector (57222342394); Ramirez, Juan (57199923049); Pinilla, Samuel (57190839705); Martinez-Torre, Jose Ignacio (57959599600)","57222342394; 57199923049; 57190839705; 57959599600","Multi-Sensor Image Feature Fusion via Subspace-based approach using <inline-formula><tex-math notation=""LaTeX"">$\ell _{1}$</tex-math></inline-formula>-Gradient Regularization","2022","IEEE Journal on Selected Topics in Signal Processing","","","","1","13","12","10.1109/JSTSP.2022.3219357","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141627997&doi=10.1109%2fJSTSP.2022.3219357&partnerID=40&md5=30150e1b2492ede082dd2907d603330d","Image fusion is a technique of combining two or more images into a single image which is more informative from an interpretation point of view. With the rapid development of different synthetic aperture radar sensing satellites capturing information from the earth by measuring energy in different portions of the electromagnetic spectrum (narrow/wide-band), complementary information about the area captured by different satellites is available (e.g. high-resolution spectral and RGB images). However, the estimation of the full-resolution image may not be necessary for inference approaches, including the pixel-based classification. Instead, it is desirable to extract the relevant information embedded in the available data to improve the inference capabilities. This work proposes a computational framework to estimate features with high-spatial-resolution and appropriate spectral content by combining information from a multi-sensor system. The considered multi-sensor setup is a hyperspectral imaging system with a complementary RGB sensor. The proposed framework first extracts spatial features from the RGB image using morphological profiles. Then, the fusion model assumes that the extracted features, and the hyperspectral measurements, lie in different subspaces matrices. In addition, this work developed a joint optimization scheme to solve the feature fusion problem by integrating the alternating direction method of multipliers with the block coordinate descent method. The alternating optimization method estimates the spatial features in the fusion model by penalizing the <inline-formula><tex-math notation=""LaTeX"">$\ell _{1}$</tex-math></inline-formula>-norm of the spatial gradient magnitudes. The quality of extracted features is measured in terms of supervised pixel-based classification methods. Extensive simulations show that the proposed approach outperforms other state-of-the-art methods in terms of classification accuracy. IEEE","Estimation; Hyperspectral imaging; Image resolution; Imaging systems; Pixels; Polarimeters; Radar imaging; Radar target recognition; Remote sensing; Sensor data fusion; Spectroscopy; Synthetic aperture radar; <inline-formula xmlns:ali=""; > <tex-math notation=""LaTeX"">$\ell _{1}$</tex-math> </inline-formula>-gradient regularization; Alternating optimizations; Features extraction; Features fusions; Hyperspectral imaging systems; Optimisations; Pixel based classifications; Regularisation; Sensor systems; Spatial resolution; Subspace based methods; Xmlns:mml=""; Xmlns:xlink=""; Xmlns:xsi=""; Image fusion","<inline-formula xmlns:ali=""http://www.niso.org/schemas/ali/1.0/"" xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""> <tex-math notation=""LaTeX"">$\ell _{1}$</tex-math> </inline-formula>-gradient regularization; alternating optimization; Feature extraction; feature fusion; hyperspectral imaging system; Imaging; Optical sensors; Optimization; pixel-based classification; Sensor systems; Sensors; Spatial resolution; subspace-based method; synthetic aperture radar","Article","Article in press","","Scopus","2-s2.0-85141627997"
"Liu W.; Yang J.; Zhao J.; Guo F.","Liu, Wensong (57195479901); Yang, Jie (57190286025); Zhao, Jinqi (56236855700); Guo, Fengcheng (57195551078)","57195479901; 57190286025; 56236855700; 57195551078","A Dual-Domain Super-Resolution Image Fusion Method with SIRV and GALCA Model for PolSAR and Panchromatic Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3134099","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121398373&doi=10.1109%2fTGRS.2021.3134099&partnerID=40&md5=1b25c8467704814abe219977ee8e7723","Hyperspectral/multispectral and panchromatic of optical remote sensing images are commonly used for multisensor image fusion, which has been applied in various applications of Earth observation. However, the utilization of optical remote sensing data suffers from the limitation of bad weather and cloud contamination. To address aforementioned issue and enhance spatial details of polarimetric synthetic aperture radar (PolSAR) image, a novel dual-domain super-resolution image fusion method is proposed by combining improved spherically invariant random vector (ISIRV) model with generalized adaptive linear combination approximation (GALCA) technology in this study. The proposed method decomposes the task of image fusion into polarimetric and texture domain fusion by integrating polarimetric components of PolSAR image and texture detail component of panchromatic image, which can significantly improve spatial resolutions of the PolSAR image while preserving polarimetric information. The data fusion experiment is implemented with three data sets including panchromatic images of Gaofen-1 (GF-1) and Gaofen-2 (GF-2) and the quad-pol SAR data of Gaofen-3 (GF-3) and Radarsat-2. Results show that the proposed dual-domain image fusion method provides a better performance compared with state-of-the-art multisensor fusion methods (BT, PCA, GS, indusion, and PRACS) regarding qualitative and quantitative evaluations. In addition, results of image fusion are applied to image classification over agricultural and urban areas of China, which shows that classification accuracy is significantly improved when compared with the result using only the original image. © 1980-2012 IEEE.","China; Adaptive optics; Image enhancement; Image resolution; Image texture; Optical sensors; Polarimeters; Remote sensing; Synthetic aperture radar; Generalized ALCA method; Improved SIRV model; Optical imaging; Optical scattering; Polarimetric domain; Remote-sensing; Resolution images; Spatial resolution; Super-resolution image fusion; Superresolution; Texture domain; accuracy assessment; image classification; image resolution; satellite data; satellite imagery; synthetic aperture radar; Image fusion","Generalized adaptive linear combination approximation (ALCA) method; improved spherically invariant random vector (SIRV) model; polarimetric domain; super-resolution image fusion; texture domain","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85121398373"
"Shi W.; Hu Z.; Liu H.; Cen S.; Huang J.; Chen X.","Shi, Wurui (57834196400); Hu, Zican (57386201700); Liu, Hongkun (57226888031); Cen, Shengcai (57415253400); Huang, Jinhan (57414930400); Chen, Xueyun (56076920300)","57834196400; 57386201700; 57226888031; 57415253400; 57414930400; 56076920300","Ship Detection in SAR Images Based on Adjacent Context Guide Fusion Module and Dense Weighted Skip Connection","2022","IEEE Access","10","","","134263","134276","13","10.1109/ACCESS.2022.3230140","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144745580&doi=10.1109%2fACCESS.2022.3230140&partnerID=40&md5=58bf4fa949163a31f8c16a8fa986e885","Fusing features from different layers is essential to improve the ship target detection ability in the synthetic aperture radar (SAR) images. Mainstream methods usually perform simple addition or concatenation operations on adjacent feature layers without properly merging their semantic and spatial information, whereas the traditional skip connections are unable to explore sufficient information by the same scale. To address these issues, a ship detection network based on adjacent context guide fusion module and dense weighted skip connection (AFDN) in SAR images is proposed: Adjacent context guide fusion module is specially designed to capture the long-range dependencies of high-level features as weights to multiply with low-level features to fuse adjacent features more efficiently. Furthermore, the dual-path enhanced pyramid is constructed to refine and fuse multi-scale features. Finally, a dense weighted skip connection is proposed by weighted fusion of features of all sizes before the decoder to enrich the feature space. Our anchor-free AFDN outputs the spatial density map and clusters to obtain the rotatable bounding box. The experimental results indicate that the method proposed in this paper surpasses previous ship detection methods and achieves high accuracy on SSDD and AIRSARShip-1.0 datasets.  © 2013 IEEE.","Feature extraction; Image enhancement; Image fusion; Radar imaging; Semantics; Ships; Space-based radar; Tracking radar; Adjacent context guide fusion module; Dense weighted skip connection; Density maps; Dual path; Dual-path enhanced pyramid; Fusion modules; Ship detection; Spatial densities; Spatial density map; Synthetic aperture radar; Synthetic aperture radar","Adjacent context guide fusion module; dense weighted skip connection; dual-path enhanced pyramid; ship detection; spatial density map; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144745580"
"He Z.; Xiao H.; Gao C.; Tian Z.; Chen S.-W.","He, Zhiqiang (57208303973); Xiao, Huaitie (7401565340); Gao, Chao (57695078000); Tian, Zhuangzhuang (57194190011); Chen, Si-Wei (55214088200)","57208303973; 7401565340; 57695078000; 57194190011; 55214088200","Fusion of Sparse Model Based on Randomly Erased Image for SAR Occluded Target Recognition","2020","IEEE Transactions on Geoscience and Remote Sensing","58","11","9067033","7829","7844","15","10.1109/TGRS.2020.2984577","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095755468&doi=10.1109%2fTGRS.2020.2984577&partnerID=40&md5=bf7eea60554fec9a85f149cf5a902195","The recognition of partially occluded targets is a difficult problem in the field of synthetic aperture radar (SAR) target recognition. To eliminate the effect of occlusion, the intuitive idea is to determine the exact location and the size of the occluded area. However, this is very difficult, even impossible in practice. In order to avoid this difficulty and to improve the recognition performance for the partially occluded target, a fusion strategy of the sparse representation (SR) model based on randomly erased images is proposed to recognize the partially occluded target. The proposed method randomly erases some areas many times in both the test samples and the training samples. The erased training samples in each erasure are used to sparsely represent the corresponding erased test sample. Finally, all the SR results are fused to recognize the test sample. The proposed method utilizes random erasure to eliminate the possible occluded region. In addition, this method uses the fusion strategy to overcome under-erasing of the occluded region and erroneous erasure of the unoccluded region. The key parameter of the proposed method is the erasure ratio only. Although the erasure is random, the recognition performance of the method is relatively stable. Therefore, the method can eliminate the influence of occlusion without determining the details of occlusion. The experimental results show that the proposed method is significantly better than the state-of-the-art methods in the case of occlusion. Additionally, the recognition performance of the proposed method is similar to some comparison methods in the case of no occlusion. © 1980-2012 IEEE.","Image enhancement; Image fusion; Radar imaging; Sampling; Synthetic aperture radar; Comparison methods; Fusion strategies; Model-based OPC; Sparse modeling; Sparse representation; State-of-the-art methods; Target recognition; Training sample; data processing; image analysis; pattern recognition; radar altimetry; synthetic aperture radar; testing method; Radar target recognition","Automatic target recognition (ATR); fusion strategy; occluded target; random erasure; sparse representation (SR); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85095755468"
"Cué La Rosa L.E.; Oliveira D.A.B.; Feitosa R.Q.","Cué La Rosa, L.E. (57220834454); Oliveira, D.A.B. (27567900100); Feitosa, R.Q. (6602453684)","57220834454; 27567900100; 6602453684","INVESTIGATING FUSION STRATEGIES ON ENCODER-DECODER NETWORKS FOR CROP SEGMENTATION USING SAR AND OPTICAL IMAGE SEQUENCES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","2405","2408","3","10.1109/IGARSS47720.2021.9554011","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129898660&doi=10.1109%2fIGARSS47720.2021.9554011&partnerID=40&md5=b8759f26553973998d398a40a9d10404","Remote sensing imagery from different sensors enables accurate crop monitoring and mapping, supporting efficient and sustainable agricultural practices. This paper proposes a flexible multi-modal Encoder-Decoder architecture to implement and investigate different fusion strategies for crop segmentation using synthetic-aperture radar (SAR) and optical image sequences. The trained model handles each modal individually or both, allowing us to investigate scenarios where one modal is not available. Our architecture consists of two modality-specific encoders, a shared decoder, a fusion module, and three classifiers, one for each modal and one for the fusion output. Also, we propose using a partial loss function that allows training the network with scarce ground truths. The proposed approach was evaluated in a public dataset comprising seven multitemporal SAR images and four multitemporal optical images from a tropical agricultural region in Brazil. We report results for feature and decision fusion strategies and discuss the benefits of using each of them for multi-modal crop segmentation. ©2021 IEEE","Convolution; Decoding; Geometrical optics; Image fusion; Image segmentation; Mapping; Network architecture; Network coding; Radar imaging; Remote sensing; Synthetic aperture radar; Crop mapping; Encoder-decoder; Fully convolutional; Fusion strategies; Image sequence; Multi-modal; Multi-temporal; Optical image; Remote sensing imagery; Synthetic aperture radar images; Crops","Crop Mapping; Fully Convolutional; Multi-Modal; Multitemporal","Conference paper","Final","","Scopus","2-s2.0-85129898660"
"Shao S.; Liu H.; Zhang L.; Wang P.; Wei J.","Shao, Shuai (57206727298); Liu, Hongwei (57205480555); Zhang, Lei (55670414500); Wang, Penghui (55718432500); Wei, Jiaqi (57202004691)","57206727298; 57205480555; 55670414500; 55718432500; 57202004691","Noise-robust interferometric ISAR imaging of 3-D maneuvering motion targets with fine image registration","2022","Signal Processing","198","","108578","","","","10.1016/j.sigpro.2022.108578","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129912457&doi=10.1016%2fj.sigpro.2022.108578&partnerID=40&md5=4386fa5b2fd15d7f9c0e2e0cfb67fc20","Low signal-to-noise ratio (SNR) and three-dimensional (3-D) maneuvering motion of targets present enormous challenges to the interferometric inverse synthetic aperture radar (InISAR) imaging. In this paper, we propose a noise-robust InISAR imaging algorithm to acquire high-precision InISAR images of 3-D maneuvering motion targets at low SNR. With respect to time-variant wave path difference (TVWPD) and spatial-variant wave path difference (SVWPD), a novel algorithm, named non-coherent fusion image registration algorithm (NCFIR), is developed to realize fine image registration. By implementing the non-coherent fusion processing on the multi-look ISAR images, it improves the image SNR and therefore makes possible the joint compensation of TVWPD and SVWPD at low SNR. In addition, underpinned by the Bayesian compressive sensing (BCS) theory, a joint multi-channel BCS ISAR imaging algorithm (JMC-BCS) is also proposed. Its capability for suppressing the strong noise and high side lobes of ISAR imaging results effectively improves the SNRs of imaging results. Through iterative processing of JMC-BCS and NCFIR, high-quality 3-D InISAR images of 3-D maneuvering motion targets at low SNR can be obtained. The proposed algorithm improves the SNR of image twice through JMC-BCS and NCFIR, rendering it more robust against noise. Extensive experimental results from both simulated and real data corroborate the effectiveness and robustness of the proposed algorithm which outperforms other available InISAR imaging approaches at low SNR. © 2022","Barium compounds; Image enhancement; Image fusion; Image registration; Interferometry; Inverse problems; Inverse synthetic aperture radar; Iterative methods; Radar imaging; Signal to noise ratio; Bayesian; Bayesian compressive sensing; Compressive sensing; Fusion image; Images registration; Interferometric inverse synthetic aperture radar; Interferometric inverse synthetic aperture radars; Low signal-to-noise ratio; Manoeuvring motions; Motion target; Non-coherent fusion image registration; Three-dimensional (3-D) maneuvering motion target; Compressed sensing","Bayesian compressive sensing (BCS); Interferometric inverse synthetic aperture radar (InISAR); Low signal-to-noise ratio (SNR); Non-coherent fusion image registration (NCFIR); Three-dimensional (3-D) maneuvering motion targets","Article","Final","","Scopus","2-s2.0-85129912457"
"Ma C.; Gao H.","Ma, Conghui (56562155900); Gao, Hongchao (57471481400)","56562155900; 57471481400","A GAN based method for SAR and optical images fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12166","","121664F","","","","10.1117/12.2617316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125474916&doi=10.1117%2f12.2617316&partnerID=40&md5=04c5d0df6dd2dfd1a20109005ebc35dd","With the development of the remote sensing technology, the availability of satellite images has been dramatically increased with high quantity and quality. Diverse information can be obtained from these multiple imaging sources. For example, synthetic aperture radar (SAR) imagery measures physical properties of the observed scene in all-weather and full-time situation and follows a range-based imaging geometry, while optical imagery measures chemical characteristics of the scene and follows a perspective imaging geometry and needs both daylight and a cloudless sky. These multisource remote sensing images, once fused together, provide a more comprehensive interpretation of remote sensing scenes. Recent advances in Generative adversarial networks (GANs) have shown great promise in translating imagery between modalities, as well in the generation of high resolution and realistic imagery. In this paper, a GAN architecture is used to solve the task of fusing SAR and optical remote sensing imagery. The network learns the mapping between input and output image, and learns a loss function to train this mapping. Specifically, the generated network is divided into two parts, encoding and decoding. The fused image including SAR intensity and texture information is generated by the generator. Other details of the optical image are added to the fusion image gradually by the discriminator. The structural similarity loss function of GAN is to make the training of GAN model more accurate on the whole structure. Experiments on Sentinel-1and Sentinel-2 imagery confirm the effectiveness and efficiency of the proposed method.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Generative adversarial networks; Geometrical optics; Image fusion; Mapping; Radar imaging; Remote sensing; Textures; Imaging geometry; Learn+; Loss functions; Multiple imaging; Network-based; Optical image; Optical imagery; Remote sensing technology; Satellite images; Synthetic aperture radar images; Synthetic aperture radar","Generative adversarial network; Image fusion; Optical imagery; SAR","Conference paper","Final","","Scopus","2-s2.0-85125474916"
"Han Y.; Liu Y.; Hong Z.; Zhang Y.; Yang S.; Wang J.","Han, Yanling (55362835500); Liu, Yekun (57221959182); Hong, Zhonghua (55261320800); Zhang, Yun (56839544700); Yang, Shuhu (39062590400); Wang, Jing (55878868400)","55362835500; 57221959182; 55261320800; 56839544700; 39062590400; 55878868400","Sea ice image classification based on heterogeneous data fusion and deep learning","2021","Remote Sensing","13","4","592","1","20","19","10.3390/rs13040592","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100781978&doi=10.3390%2frs13040592&partnerID=40&md5=dc2bdd681f601246fcbfef440eaa99dc","Sea ice is one of the typical causes of marine disasters. Sea ice image classification is an important component of sea ice detection. Optical data contain rich spectral information, but they do not allow one to easily distinguish between ground objects with a similar spectrum and foreign objects with the same spectrum. Synthetic aperture radar (SAR) data contain rich texture information, but the data usually have a single source. The limitation of single-source data is that they do not allow for further improvements of the accuracy of remote sensing sea ice classification. In this paper, we propose a method for sea ice image classification based on deep learning and heterogeneous data fusion. Utilizing the advantages of convolutional neural networks (CNNs) in terms of depth feature extraction, we designed a deep learning network structure for SAR and optical images and achieve sea ice image classification through feature extraction and a feature-level fusion of heterogeneous data. For the SAR images, the improved spatial pyramid pooling (SPP) network was used and texture information on sea ice at different scales was extracted by depth. For the optical data, multi-level feature information on sea ice such as spatial and spectral information on different types of sea ice was extracted through a path aggregation network (PANet), which enabled low-level features to be fully utilized due to the gradual feature extraction of the convolution neural network. In order to verify the effectiveness of the method, two sets of heterogeneous sentinel satellite data were used for sea ice classification in the Hudson Bay area. The experimental results show that compared with the typical image classification methods and other heterogeneous data fusion methods, the method proposed in this paper fully integrates multi-scale and multi-level texture and spectral information from heterogeneous data and achieves a better classification effect (96.61%, 95.69%). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolution; Convolutional neural networks; Data mining; Deep learning; Extraction; Feature extraction; Geometrical optics; Image classification; Image enhancement; Image fusion; Image texture; Learning systems; Radar imaging; Remote sensing; Sea ice; Synthetic aperture radar; Textures; Classification methods; Convolution neural network; Feature information; Feature level fusion; Heterogeneous data; Sea ice classification; Spectral information; Texture information; Classification (of information)","Data fusion; Feature information; Heterogeneous data; Sea ice","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100781978"
"Prabhakar K.R.; Nukala V.H.; Gubbi J.; Pal A.; Balamuralidhar P.","Prabhakar, K Ram (57189591594); Nukala, Veera Harikrishna (57323973300); Gubbi, Jayavardhana (23090806600); Pal, Arpan (57203638167); Balamuralidhar, P. (16201684400)","57189591594; 57323973300; 23090806600; 57203638167; 16201684400","Improving SAR and Optical Image Fusion for Lulc Classification with Domain Knowledge","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","711","714","3","10.1109/IGARSS46834.2022.9884283","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140404254&doi=10.1109%2fIGARSS46834.2022.9884283&partnerID=40&md5=4f0ab8ef3488ddc56ca96afd52478a3e","Fusing SAR and multi-spectral images to generate a precise land cover map in a weakly supervised setting is a challenging yet essential problem. The inaccurate, noisy, and inexact ground truth labels pose difficulty training any machine learning models. In this paper, we make a fundamental and pivotal contribution towards improving the ground truth label quality using domain knowledge. We present a simple yet effective mechanism to refine the low-resolution noisy ground truth labels. The proposed approach is trained and tested on a publicly available DFC2020 dataset. Through experiments, we show the effectiveness of our method by training a deep learning model on the refined labels that outperform even the models trained with clean ground truth. © 2022 IEEE.","Deep learning; Domain Knowledge; Geometrical optics; Image classification; Image enhancement; Image fusion; Learning systems; Radar imaging; Remote sensing; Spectroscopy; Domain knowledge; Essential problems; Ground truth; Land cover maps; Machine learning models; Multi-spectral; Multispectral images; Optical image; SAR Images; Simple++; Synthetic aperture radar","CNN; image fusion; multispectral; SAR","Conference paper","Final","","Scopus","2-s2.0-85140404254"
"Shao Z.; Wu W.; Guo S.","Shao, Zhenfeng (57203905559); Wu, Wenfu (57208187129); Guo, Songjing (57212678543)","57203905559; 57208187129; 57212678543","IHS-GTF: A fusion method for optical and synthetic aperture radar data","2020","Remote Sensing","12","17","2796","1","20","19","10.3390/rs12172796","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095118988&doi=10.3390%2frs12172796&partnerID=40&md5=90050675cfe8cfc47a072c614f260e07","Optical and Synthetic Aperture Radar (SAR) fusion is addressed in this paper. Intensity–Hue–Saturation (IHS) is an easily implemented fusion method and can separate Red–Green–Blue (RGB) images into three independent components; however, using this method directly for optical and SAR images fusion will cause spectral distortion. The Gradient Transfer Fusion (GTF) algorithm is proposed firstly for infrared and gray visible images fusion, which formulates image fusion as an optimization problem and keeps the radiation information and spatial details simultaneously. However, the algorithm assumes that the spatial details only come from one of the source images, which is inconsistent with the actual situation of optical and SAR images fusion. In this paper, a fusion algorithm named IHS-GTF for optical and SAR images is proposed, which combines the advantages of IHS and GTF and considers the spatial details from the both images based on pixel saliency. The proposed method was assessed by visual analysis and ten indices and was further tested by extracting impervious surface (IS) from the fused image with random forest classifier. The results show the good preservation of spatial details and spectral information by our proposed method, and the overall accuracy of IS extraction is 2% higher than that of using optical image alone. The results demonstrate the ability of the proposed method for fusing optical and SAR data effectively to generate useful data. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Decision trees; Geometrical optics; Image fusion; Image processing; Synthetic aperture radar; Impervious surface; Independent components; Optimization problems; Overall accuracies; Radiation information; Random forest classifier; Spectral distortions; Spectral information; Radar imaging","Image fusion; Impervious surface; Optical and SAR; Pixel saliency","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85095118988"
"Gao Q.; Feng Z.; Yang S.; Chang Z.; Wang R.","Gao, Quanwei (57211472546); Feng, Zhixi (56047550500); Yang, Shuyuan (8159166000); Chang, Zhihao (57272517400); Wang, Ruyu (57203878778)","57211472546; 56047550500; 8159166000; 57272517400; 57203878778","Multi-Path Interactive Network for Aircraft Identification with Optical and SAR Images","2022","Remote Sensing","14","16","3922","","","","10.3390/rs14163922","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137803986&doi=10.3390%2frs14163922&partnerID=40&md5=3b74ca5b759c89b9cc2ece6a5ff7e45e","Aircraft identification has been a research hotspot in remote-sensing fields. However, due to the presence of clouds in satellite-borne optical imagery, it is difficult to identify aircraft using a single optical image. In this paper, a Multi-path Interactive Network (MIN) is proposed to fuse Optical and Synthetic Aperture Radar (SAR) images for aircraft identification on cloudy days. First, features are extracted from optical and SAR images separately by convolution backbones of ResNet-34. Second, a piecewise residual fusion strategy is proposed to reduce the effect of clouds. A plug-and-play Interactive Attention Sum-Max fusion module (IASM), is thus constructed to interact with features from multi-modal images. Moreover, multi-path IASM is designed to mix multi-modal features from backbones. Finally, the fused features are sent to the neck and head of MIN for regression and classification. Extensive experiments are carried out on the Fused Cloudy Aircraft Detection (FCAD) dataset that is constructed, and the results show the efficiency of MIN in identifying aircraft under clouds with different thicknesses.Compared with the single-source model, the multi-source fusion model MIN is improved by more than (Formula presented.), and the proposed method outperforms the state-of-the-art approaches. © 2022 by the authors.","Aircraft detection; Geometrical optics; Image fusion; Optical remote sensing; Radar imaging; Satellite imagery; Synthetic aperture radar; Aircraft identification; Cloudy image; Fusion modules; Hotspots; Interactive attention sum-max fusion; Multi-path interactive network; Multipath; Optical-; Remote-sensing; Synthetic aperture radar images; Aircraft","aircraft identification; cloudy images; Interactive Attention Sum-Max fusion; multi-path interactive network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137803986"
"Xie B.; Wang X.; Kang P.","Xie, Beimin (57222144125); Wang, Xinrong (57222248836); Kang, Peiran (57226378324)","57222144125; 57222248836; 57226378324","Joint Classification of Multispectral Image and SAR Image Based on Deep Feature Fusion","2021","Lecture Notes in Electrical Engineering","654 LNEE","","","1682","1690","8","10.1007/978-981-15-8411-4_221","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111406317&doi=10.1007%2f978-981-15-8411-4_221&partnerID=40&md5=bcf792547033bee81ae7e8182617d949","Different imaging sensors which are mounted on thousands of remote sensing platforms collect various information of the land covers. Since more sensors provide more information, the classification of multi-sensor data has potential advantages. Multispectral image collects information from visible spectrum, and SAR image reflects information of microwave band. However, due to the redundant information, multi-sensor data will also bring challenge to traditional classification method. This paper presents a joint classification method which combines the information from images from both multispectral sensor and synthetic-aperture radar (SAR) sensor. The proposed method is based on deep feature fusion, which is a deep network with two feature learning branches for multispectral image and SAR image separately, and then, the two branches are merged together into more fully connected layers to perform feature fusion and optimization; finally, a classification layer is added on the top of the network to predict sample label. The proposed method takes advantage of reciprocal information from different sensors and gives a strategy to utilize multi-sensor information. Experimental results demonstrate that our method is able to give better performance than using any single data. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Image classification; Image fusion; Radar imaging; Remote sensing; Synthetic aperture radar; Classification methods; Feature learning; Microwave bands; Multi-sensor data; Multi-sensor information; Multispectral images; Multispectral sensors; Remote sensing platforms; Classification (of information)","Classification; Deep feature; Information fusion; Multispectral image; Synthetic-aperture radar (SAR) image","Conference paper","Final","","Scopus","2-s2.0-85111406317"
"Liu M.; Mu X.; He X.","Liu, Mengyang (57462377800); Mu, Xiaodong (57463185700); He, Xiangchen (13103591400)","57462377800; 57463185700; 13103591400","Heterogeneous image matching based on phase consistency","2021","2021 IEEE Conference on Telecommunications, Optics and Computer Science, TOCS 2021","","","","864","868","4","10.1109/TOCS53301.2021.9689044","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125060020&doi=10.1109%2fTOCS53301.2021.9689044&partnerID=40&md5=7d73f3c2badd0f4bede4e3086fca8f12","The Image matching is an important step in image registration and image fusion. To solve the problem that SAR images and optical image matching are sensitive to radiation distortion, a novel heterogeneous image matching algorithm is proposed based on image frequent-domain wavelet decomposition based on phase congruency. Firstly, morphological denoising was carried out for image pairs, and then wavelet decomposition was performed by log-Gabor filter to obtain phase consistency of pixel points (PC graph). Then, feature descriptors were established based on the maximum index graph. Double-point matching was used as the search strategy for feature matching, and the random sampling consensus algorithm was used to eliminate false matching. Compare with traditional SIFT method. Experimental results show that the proposed algorithm has better matching effect than SIFT method and is an effective heterogeneous image matching algorithm  © 2021 IEEE.","Gabor filters; Geometrical optics; Image fusion; Morphology; Radar imaging; Synthetic aperture radar; Wavelet decomposition; Heterogeneous image matching; Image matching algorithm; Image morphology; Images registration; Optical image; Phase congruency; Phase consistencies; SAR Images; The log gabor filter; Image matching","Heterogeneous image matching; Image morphology; Phase consistency; The Log Gabor filter","Conference paper","Final","","Scopus","2-s2.0-85125060020"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","CNN-based fusion and classification of SAR and Optical data","2020","International Journal of Remote Sensing","41","22","","8839","8861","22","10.1080/01431161.2020.1783713","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091284696&doi=10.1080%2f01431161.2020.1783713&partnerID=40&md5=d676e5badabec051c631460b03f316e5","Image fusion combines the images of different spectral, spatial, multi-date, as well as radiometric data to achieve a better quality image for improved classification results. Recently, Convolution Neural Network (CNN)-based classification algorithms are extensively used for remote sensing applications. Keeping this in view, present work proposes to use CNN-based fusion and classification of Sentinel 1 (VV and VH polarization) and Sentinel 2 datasets acquired over an agricultural area near Hisar (India). For image fusion, three CNN-based approaches are used to fuse Sentinel 2 (10 m) data with VV and VH bands of Sentinel 1 data. After fusion, classification was performed using 2D-CNN classifier to judge the performance of fused images in terms of classification accuracy. Results suggest that out of the three fusion approaches, only infrared image fusion (IVF) approach performed well with the considered dataset in terms of fusion indicators and classification accuracy. Keeping in view of its better performance, this study proposes a modified IVF approach by using different image pyramid methods. Comparison of results suggests an improved performance by modified IVF approach for the fusion of Sentinel 2 and Sentinel 1 data in comparison with the original IVF approach. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Haryana; Hisar; India; Agricultural robots; Image classification; Image enhancement; Image fusion; Infrared imaging; Radar imaging; Remote sensing; Agricultural areas; Classification accuracy; Classification algorithm; Classification results; Convolution neural network; Image pyramids; Radiometric data; Remote sensing applications; algorithm; artificial neural network; image classification; satellite data; satellite imagery; Sentinel; synthetic aperture radar; Classification (of information)","","Article","Final","","Scopus","2-s2.0-85091284696"
"Yuan Y.; Meng X.; Sun W.; Yang G.; Wang L.; Peng J.; Wang Y.","Yuan, Yi (57874623700); Meng, Xiangchao (56158755000); Sun, Weiwei (55726567900); Yang, Gang (57192178476); Wang, Lihua (57221651796); Peng, Jiangtao (24833160700); Wang, Yumiao (57874832700)","57874623700; 56158755000; 55726567900; 57192178476; 57221651796; 24833160700; 57874832700","Multi-Resolution Collaborative Fusion of SAR, Multispectral and Hyperspectral Images for Coastal Wetlands Mapping","2022","Remote Sensing","14","14","3492","","","","10.3390/rs14143492","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137272381&doi=10.3390%2frs14143492&partnerID=40&md5=af4929a341e9f100543bba2531bcd73e","The hyperspectral, multispectral, and synthetic aperture radar (SAR) remote sensing images provide complementary advantages in high spectral resolution, high spatial resolution, and geometric and polarimetric properties, generally. How to effectively integrate cross-modal information to obtain a high spatial resolution hyperspectral image with the characteristics of the SAR is promising. However, due to divergent imaging mechanisms of modalities, existing SAR and optical image fusion techniques generally remain limited due to the spectral or spatial distortions, especially for complex surface features such as coastal wetlands. This paper provides, for the first time, an efficient multi-resolution collaborative fusion method for multispectral, hyperspectral, and SAR images. We improve generic multi-resolution analysis with spectral-spatial weighted modulation and spectral compensation to achieve minimal spectral loss. The backscattering gradients of SAR are guided to fuse, which is calculated from saliency gradients with edge preserving. The experiments were performed on ZiYuan-1 02D (ZY-1 02D) and GaoFen-5B (AHSI) hyperspectral, Sentinel-2 and GaoFen-5B (VIMI) multispectral, and Sentinel-1 SAR images in the challenging coastal wetlands. Specifically, the fusion results were comprehensively tested and verified on the qualitative, quantitative, and classification metrics. The experimental results show the competitive performance of the proposed method. © 2022 by the authors.","Geometrical optics; Hyperspectral imaging; Image fusion; Image resolution; Optical remote sensing; Photomapping; Radar imaging; Space-based radar; Spectral resolution; Spectroscopy; Wetlands; Coastal wetlands; Gaofen-5; High spatial resolution; HyperSpectral; Multi-spectral; Pixel level; Remote-sensing; Synthetic aperture radar images; Synthetic aperture radar remote sensing images; Ziyuan-1 02d; Synthetic aperture radar","classification; coastal wetlands; data fusion; GaoFen-5; hyperspectral; pixel-level; remote sensing; synthetic aperture radar; ZY-1 02D","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137272381"
"Gao J.; Yuan Q.; Li J.; Zhang H.; Su X.","Gao, Jianhao (57211516266); Yuan, Qiangqiang (36635300800); Li, Jie (57214207213); Zhang, Hai (57192694132); Su, Xin (57200950410)","57211516266; 36635300800; 57214207213; 57192694132; 57200950410","Cloud removal with fusion of high resolution optical and SAR images using generative adversarial networks","2020","Remote Sensing","12","1","191","","","","10.3390/RS12010191","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083956677&doi=10.3390%2fRS12010191&partnerID=40&md5=90bf07a924911d7fca557a2c635a2b74","The existence of clouds is one of the main factors that contributes to missing information in optical remote sensing images, restricting their further applications for Earth observation, so how to reconstruct the missing information caused by clouds is of great concern. Inspired by the image-to-image translation work based on convolutional neural network model and the heterogeneous information fusion thought, we propose a novel cloud removal method in this paper. The approach can be roughly divided into two steps: in the first step, a specially designed convolutional neural network (CNN) translates the synthetic aperture radar (SAR) images into simulated optical images in an object-to-object manner; in the second step, the simulated optical image, together with the SAR image and the optical image corrupted by clouds, is fused to reconstruct the corrupted area by a generative adversarial network (GAN) with a particular loss function. Between the first step and the second step, the contrast and luminance of the simulated optical image are randomly altered to make the model more robust. Two simulation experiments and one real-data experiment are conducted to confirm the effectiveness of the proposed method on Sentinel 1/2, GF 2/3 and airborne SAR/optical data. The results demonstrate that the proposed method outperforms state-of-the-art algorithms that also employ SAR images as auxiliary data. © 2020 by the authors.","Convolution; Convolutional neural networks; Geometrical optics; Image fusion; Image reconstruction; Remote sensing; Synthetic aperture radar; Adversarial networks; Earth observations; Heterogeneous information; Image translation; Missing information; Optical remote sensing; State-of-the-art algorithms; Synthetic aperture radar (SAR) images; Radar imaging","Cloud removal; Deep learning; GAN; Information fusion; SAR","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083956677"
"Rajah P.; Odindi J.; Mutanga O.","Rajah, Perushan (56658660800); Odindi, John (36521256000); Mutanga, Onisimo (55912148400)","56658660800; 36521256000; 55912148400","Synergistic potential of dual-polarized synthetic aperture radar and multispectral optical imagery for invasive alien species detection and mapping","2020","Journal of Applied Remote Sensing","14","1","014512","","","","10.1117/1.JRS.14.014512","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083038901&doi=10.1117%2f1.JRS.14.014512&partnerID=40&md5=b2c55d2993b56a86f96352f8abf73e5b","Invasive alien species are a major threat to global biodiversity and result in adverse environmental and socioeconomic implications, such as reduced ecosystem services, landscape productivity, and costly eradication initiatives. The ability to monitor the extent and spread of alien species invasions provides valuable insight for the mitigation of these adverse implications. The generation of Earth Observation (EO) Sentinel sensor provides unprecedented freely available imagery, which is suitable for both local and regional invasive species monitoring. Specifically, its radar (S1) and optical (S2) sensors offer unique tandem datasets valuable for landscape analysis. Hence, we sought to determine the synergistic potential of fused S1 dual-polarized synthetic aperture radar (SAR) imagery with S2 optical imagery for invasive alien species detection and mapping. S1 and S2 imageries were fused at the feature level, and the support vector machine algorithm used for the multiclass image classification. Results indicated that the fusion of the S1 dual-polarized imagery with S2 optical imagery produced the highest classification accuracy (85%), whereas stand-alone S2 optical bands produced the lowest classification accuracy (79%). Findings from this study underline the significant synergistic potential and complementarity of new-age S2 optical imagery and dual-polarized S1 SAR imagery for invasive alien species detection and mapping. Due to large swath, higher pixel resolution, free availability, and possible tandem complementarity between optical and SAR sensors, we recommend Sentinel EO imagery as an economically viable option for the invasive alien species detection and mapping. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Biodiversity; Ecosystems; Image classification; Mapping; Support vector machines; Synthetic aperture radar; Classification accuracy; Earth observations; Economically viable; Invasive alien species; Landscape analysis; Multispectral optical imagery; Support vector machine algorithm; Synthetic Aperture Radar Imagery; Radar imaging","dual-polarized; image fusion; invasive alien species; Sentinel-1; Sentinel-2; support vector machine; synergistic; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85083038901"
"Luo W.; Zhang H.; DIng J.","Luo, Wan (54585659700); Zhang, Hongbo (55685538900); DIng, Jing (57104168900)","54585659700; 55685538900; 57104168900","A SAR/Infrared Image Fusion Method Based on NSCT and PCNN","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048470","","","","10.1109/APSAR46974.2019.9048470","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083527643&doi=10.1109%2fAPSAR46974.2019.9048470&partnerID=40&md5=788cbda919bd5d8efb9091742f5d8af8","In order to improve the quality of the fusion image between SAR image and infrared image, a novel SAR/infrared image fusion method based on non-subsampled contourlet transform (NSCT) and pulse coupled neural network (PCNN) is proposed. Firstly, this method decomposes the SAR image and infrared image respectively via NSCT. Then PCNN is used as the fusion rule of our method. Finally, the fusion image is obtained by taking the inverse NSCT transform. The results indicate that, compared with other fusion methods, higher resolution fusion image with rich information and strong contrast can be get by the method proposed in this paper. © 2019 IEEE.","Image enhancement; Image fusion; Infrared imaging; Inverse problems; Neural networks; Synthetic aperture radar; Fusion image; Fusion methods; Fusion rule; Higher resolution; Image fusion methods; Non subsampled contourlet transform (NSCT); Pulse coupled neural network; Strong contrast; Radar imaging","image fusion; infrared; non-subsampled contourlet transform; pulse coupled neural network; SAR","Conference paper","Final","","Scopus","2-s2.0-85083527643"
"Du Y.; Liu J.; Song W.; He Q.; Huang D.","Du, Yanling (55350371200); Liu, Jiajia (57214224419); Song, Wei (57191748664); He, Qi (57195506496); Huang, Dongmei (16303959000)","55350371200; 57214224419; 57191748664; 57195506496; 16303959000","Ocean Eddy Recognition in SAR Images with Adaptive Weighted Feature Fusion","2019","IEEE Access","7","","8865024","152023","152033","10","10.1109/ACCESS.2019.2946852","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078472107&doi=10.1109%2fACCESS.2019.2946852&partnerID=40&md5=1e9bbd782c4b00ebcf1debca28e97bf2","Automatic recognition of ocean eddies has become one of the hotspots in the field of physical oceanography. Traditional methods based on either physical parameters or geometry features require manual parameter adjustment, and cannot adapt to the dynamic changes of ocean eddies caused by complicated ocean environments. To address these issues, we propose a new eddy recognition method in SAR images with adaptive weighted multi-feature fusion. Specially, to better characterize eddies, we first extract texture, shape and corner features using global Gray Level Co-occurrence Matrix (GLCM), detailed Fourier Descriptor (FD) and local salient Harris features respectively. Secondly, considering the different importance of features for eddy recognition, we propose an adaptive weighted feature fusion method with multiple kernel learning (MKL). Here, a combined kernel is derived to fuse three selected kernels for the three types of features with the weights trained by MKL. Finally, we design a SVM classifier with the combined kernel to realize the eddy recognition. The experimental results show that: 1) our proposed method can reach 93.42% of eddy recognition accuracy, which is much higher than the methods with only one single feature; 2) adaptive weighted fusion plays an important role in improving the accuracy. Our proposed method with MKL gains a 8.36% accuracy increase than the method without MKL. Through adaptive weighted fusion, our method avoids the manual parameter adjustment and is more robust and general. Experimental results have proven that our method is effective and applicable to recognize eddies. © 2013 IEEE.","Image fusion; Image processing; Image recognition; Radar imaging; Synthetic aperture radar; Textures; Adaptive weighted fusions; Multi-feature fusion; Multiple Kernel Learning; Ocean eddies; SAR Images; Oceanography","adaptive weighted fusion; image recognition; Multi-feature fusion; multiple kernel learning; ocean eddies; SAR images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078472107"
"Liu F.; Chen T.; He J.; Wen Q.; Yu F.; Gu X.; Wang Z.","Liu, Feng (57194527707); Chen, Ting (57200818542); He, Jianjun (55714974000); Wen, Qiang (55350081100); Yu, Fei (57202044647); Gu, Xinzhi (57192702842); Wang, Zhiyong (57209622015)","57194527707; 57200818542; 55714974000; 55350081100; 57202044647; 57192702842; 57209622015","The research on dryland crop classification based on the fusion of Sentinel-1a SAR and optical images","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","3","","1041","1048","7","10.5194/isprs-archives-XLII-3-1041-2018","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046934910&doi=10.5194%2fisprs-archives-XLII-3-1041-2018&partnerID=40&md5=f86d0e037b732f95dc10a9c0e91a3942","In recent years, the quick upgrading and improvement of SAR sensors provide beneficial complements for the traditional optical remote sensing in the aspects of theory, technology and data. In this paper, Sentinel-1A SAR data and GF-1 optical data were selected for image fusion, and more emphases were put on the dryland crop classification under a complex crop planting structure, regarding corn and cotton as the research objects. Considering the differences among various data fusion methods, the principal component analysis (PCA), Gram-Schmidt (GS), Brovey and wavelet transform (WT) methods were compared with each other, and the GS and Brovey methods were proved to be more applicable in the study area. Then, the classification was conducted based on the object-oriented technique process. And for the GS, Brovey fusion images and GF-1 optical image, the nearest neighbour algorithm was adopted to realize the supervised classification with the same training samples. Based on the sample plots in the study area, the accuracy assessment was conducted subsequently. The values of overall accuracy and kappa coefficient of fusion images were all higher than those of GF-1 optical image, and GS method performed better than Brovey method. In particular, the overall accuracy of GS fusion image was 79.8%, and the Kappa coefficient was 0.644. Thus, the results showed that GS and Brovey fusion images were superior to optical images for dryland crop classification. This study suggests that the fusion of SAR and optical images is reliable for dryland crop classification under a complex crop planting structure. © Authors 2018. CC BY 4.0 License.","Crops; Fusion reactions; Geometrical optics; Image classification; Principal component analysis; Radar imaging; Remote sensing; Synthetic aperture radar; Wavelet transforms; Accuracy assessment; Crop classification; Data fusion methods; Object oriented technique; Optical remote sensing; Overall accuracies; Sentinel-1; Supervised classification; Image fusion","Crop classification; Fusion; Object-oriented technique; Sentinel-1A; Synthetic aperture radar","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85046934910"
"Jia Z.; Guangchang D.; Feng C.; Xiaodan X.; Chengming Q.; Lin L.","Jia, Zhai (57208879262); Guangchang, Dong (57208882573); Feng, Chen (57212474470); Xiaodan, Xie (57208879758); Chengming, Qi (24081271700); Lin, Li (57217437073)","57208879262; 57208882573; 57212474470; 57208879758; 24081271700; 57217437073","A Deep Learning Fusion Recognition Method Based on SAR Image Data","2019","Procedia Computer Science","147","","","533","541","8","10.1016/j.procs.2019.01.229","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066021742&doi=10.1016%2fj.procs.2019.01.229&partnerID=40&md5=7895500f7776378c6378a1da01282040","In view of the research status and existing problems of synthetic aperture radar (SAR) target recognition, a new method of deep learning fusion recognition is proposed. Firstly, the 1-D features extracted with principle component analysis(PCA) are used as the input of the stacked autoencoder(SAE) network to extract deep features, which achieves target recognition based on 1-D PCA feature data. Then, the SAR target images are used as the input of convolutional neural network(CNN) to extract deep features, which achieves target recognition based on 2-D SAR image feature data. Finally, a deep learning recognition algorithm of decision-level and feature-level fusion is proposed for the diferent kinds of SAR image feature data. The experiment analysis shows that the proposed method of deep learning fusion recognition in this paper is adaptive and robust to the attitude angle, background and noise. © 2019 The Author(s).","Convolution; Data mining; Deep learning; Image fusion; Internet of things; Neural networks; Principal component analysis; Radar imaging; Synthetic aperture radar; Auto encoders; Convolutional neural network; Experiment analysis; Feature level fusion; Principle component analysis; Recognition algorithm; Recognition methods; Target recognition; Radar target recognition","convolutional neural network(CNN); principle component analysis(PCA); stacked autoencoder (SAE); synthetic aperture radar (SAR); target recognition","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85066021742"
"Zhang W.; Xu M.","Zhang, Wenyuan (57196272204); Xu, Min (57226497554)","57196272204; 57226497554","Translate SAR Data into Optical Image Using IHS and Wavelet Transform Integrated Fusion","2019","Journal of the Indian Society of Remote Sensing","47","1","","125","137","12","10.1007/s12524-018-0879-7","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055964636&doi=10.1007%2fs12524-018-0879-7&partnerID=40&md5=237f1575b3d76e94ae23316c980c9733","Although synthetic aperture radar (SAR) sensors function well at all times and under all weather conditions, the images they produce are not intuitively straightforward. A novel idea based on data fusion is introduced to translate SAR data into optical image in this paper. The proposed SAR-optical image translation is implemented using an intensity–hue–saturation (IHS) and wavelet transform integrated fusion (IHSW), so as to preserve as much as spatial details from SAR data, and minimize the spectral distortion of translated output. COSMO-SkyMed and ENVISAT-ASAR images are translated into optical images with the fusion of Landsat TM images, and the fusion results are compared with some conventional fusion methods, as well as the texture synthesis approach. Quality assessment of different fused outputs is carried out by using six quality indices. Visual and statistical comparisons of the final results indicate that the proposed approach achieves an effective translation from SAR to optical image and is superior to texture synthesis-based algorithm in terms of preserving spatial and spectral information. The proposed translation technique presents an alternative to improve the interpretability of SAR images. © 2018, Indian Society of Remote Sensing.","algorithm; detection method; image analysis; Landsat; numerical method; satellite data; satellite imagery; satellite sensor; synthetic aperture radar; wavelet analysis","Image fusion; Image translation; Synthetic aperture radar (SAR); Wavelet transform","Article","Final","","Scopus","2-s2.0-85055964636"
"Zeng L.; Liang Y.; Li Z.; Huai Y.; Xing M.","Zeng, Letian (56047977300); Liang, Yi (55479382100); Li, Zhenyu (56512574400); Huai, Yuanyuan (56740501300); Xing, Mengdao (7005922869)","56047977300; 55479382100; 56512574400; 56740501300; 7005922869","Accelerated time domain imaging algorithm and its autofocus approach","2017","Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University","44","1","","1","5and70","569","10.3969/j.issn.1001-2400.2017.01.001","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016092775&doi=10.3969%2fj.issn.1001-2400.2017.01.001&partnerID=40&md5=56acb5a5df3e2edfa9689bb09119141d","The measurement accuracy of a navigation system is inadequate for airborne synthetic aperture radar (SAR). This may seriously degrade the image quality. In this paper, we propose an accelerated time domain (ATD) imaging algorithm combined with the autofocus method. By introducing the global pseudo polar coordinate (GPPC), we construct the Fourier transform pair (FTP) relationship. Then, the weighted least square phase gradient autofocus (WLS-PGA) algorithm is adopted to implement accurate phase error compensation. This method uses fast Fourier transform to implement sub-image fusions instead of time-consuming two-dimensional interpolation. Also, it has good compatibility with the high-accuracy autofocus algorithm to estimate the residual motion errors within the radar echoes and to obtain a well-focused image. © 2017, The Editorial Board of Journal of Xidian University. All right reserved.","Computerized tomography; Error compensation; Fast Fourier transforms; Image fusion; Navigation systems; Radar; Radar imaging; Radar measurement; Synthetic aperture radar; Airborne synthetic aperture radars; Auto-focus; Fourier transform pair; Imaging algorithm; Phase error compensations; Time-domain imaging algorithm; Two-dimensional interpolation; Weighted least squares; Time domain analysis","Accelerated time domain(ATD) imaging algorithm; Fourier transform pair(FTP); Synthetic aperture radar(SAR); Weighted least square phase gradient autofocus(WLS-PGA)","Article","Final","","Scopus","2-s2.0-85016092775"
"Wang J.; Ren Y.; Wei S.","Wang, Jun (57200022394); Ren, Yuming (57215819617); Wei, Shaoming (36652006600)","57200022394; 57215819617; 36652006600","Synthetic Aperture Radar Images Target Detection and Recognition with Multiscale Feature Extraction and Fusion Based on Convolutional Neural Networks","2019","ICSIDP 2019 - IEEE International Conference on Signal, Information and Data Processing 2019","","","9172989","","","","10.1109/ICSIDP47821.2019.9172989","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091905062&doi=10.1109%2fICSIDP47821.2019.9172989&partnerID=40&md5=76c8ae348f2ca631f449a35c0c3a8e47","In order to improve the precision of target detection and recognition for synthetic aperture radar (SAR) images, in this paper, we proposed the multiscale feature extraction and fusion method for SAR images based on the convolutional neural networks. We constructed training and testing data based on the MSTAR dataset. Since there are not enough SAR image data, we used image processing methods to do the data augmentation. In order to improve the accuracy of target detection, we also used the method of transfer learning. Eventually we trained and tested the model on a small data set, the final mAP reached 96.58%, a relatively high score which proved the effectiveness of multiscale feature extraction and fusion. In order to better understand the principle of this technology, we also did some visualization analysis for the feature maps. This proved the reliability of the method. © 2019 IEEE.","Convolution; Convolutional neural networks; Data handling; Extraction; Feature extraction; Image enhancement; Image fusion; Radar target recognition; Statistical tests; Synthetic aperture radar; Tracking radar; Transfer learning; Data augmentation; Image processing - methods; Multi-scale features; Small data set; Synthetic aperture radar (SAR) images; Target detection and recognition; Training and testing; Visualization analysis; Radar imaging","convolutional neural networks; deep learning; feature visualization; multiscale feature extraction and fusion; synthetic aperture radar; target detection","Conference paper","Final","","Scopus","2-s2.0-85091905062"
"Zhu W.; Jiang H.; Zhou S.; Addison M.","Zhu, Wenzhong (57001830500); Jiang, Hualong (56225659500); Zhou, Shuwen (37035569700); Addison, Mike (57003107100)","57001830500; 56225659500; 37035569700; 57003107100","The review of prospect of remote sensing image processing","2017","Recent Patents on Computer Science","10","1","","53","61","8","10.2174/2213275909666160616115416","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028931391&doi=10.2174%2f2213275909666160616115416&partnerID=40&md5=87eb2f21f82ae5e1b3ebd2440b608778","Background: In order to further analyze the problems of remote sensing image processing technology and the future development direction, this paper analyzes the concept of remote sensing and basic process, describes the status of domestic and international development of remote sensing image processing technology, as described in various patents. Method: Several key elements of remote sensing image processing technology are studied, including five directions, remote sensing image matching, the integration of infrared and visible light remote sensing image, edge detection of SAR images, remote sensing image classification and remote sensing image change detection, and research and development status of these five directions were analyzed. The main characteristics of remote sensing image processing technology and reasons constraining the development of remote sensing image processing technology are analyzed. Result: Image fusion method based on PCA, are relatively vague and inaccurate. The image fusion method based on wavelet transform is used for image fusion, the fusion result is clear, and the image fusion method based on wavelet transform is more accurate than PCA. Conclusion: The comparison results of different methods are given by computer simulation experiment; it provides a reference for the development trends and application prospects of remote sensing image processing technology. © 2017 Bentham Science Publishers.","Edge detection; Image analysis; Image classification; Image compression; Image fusion; Image reconstruction; Radar imaging; Remote sensing; Synthetic aperture radar; Wavelet transforms; Computer simulation experiment; Image processing technology; International development; Remote sensing image classification; Remote sensing image processing; Research and development; SAR Images; Visible-light remote sensing; Image processing","Edge detection; Image processing technology; PCA; Remote sensing; SAR images; Wavelet transform","Review","Final","","Scopus","2-s2.0-85028931391"
"Mahyoub S.; Fadil A.; Mansour E.M.; Rhinane H.; Al-Nahmi F.","Mahyoub, S. (57209201721); Fadil, A. (57204953354); Mansour, E.M. (57209197135); Rhinane, H. (57189269615); Al-Nahmi, F. (57190014012)","57209201721; 57204953354; 57209197135; 57189269615; 57190014012","Fusing of optical and synthetic aperture radar (SAR) remote sensing data: A systematic literature review (SLR)","2019","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","4/W12","","127","138","11","10.5194/isprs-archives-XLII-4-W12-127-2019","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066804304&doi=10.5194%2fisprs-archives-XLII-4-W12-127-2019&partnerID=40&md5=695cd9827b5ffe563d762e76bba7a3c1","Remote sensing and image fusion have recognized many important improvements throughout the recent years, especially fusion of optical and synthetic aperture radar (SAR), there are so many published papers that worked on fusing optical and SAR data which used in many application fields in remote sensing such as Land use Mapping and monitoring. The goal of this survey paper is to summarize and synthesize the published articles from 2013 to 2018 which focused on the fusion of Optical and synthetic aperture radar (SAR) remote sensing data in a systematic literature review (SLR), based on the pre-published articles on indexed database related to this subject and outlining the latest techniques as well as the most used methods. In addition this paper highlights the most popular image fusion methods in this blending type. After conducting many researches in the indexed databases by using different key words related to the topic “fusion Optical and SAR in remote sensing”, among 705 articles, chosen 83 articles, which match our inclusion criteria and research questions as results all the systematic study ‘questions have been answered and discussed . 0.5194/isprs-archives-XLII-4-W12-127-2019","Geometrical optics; Image enhancement; Image fusion; Land use; Mapping; Radar imaging; Remote sensing; Application fields; Image fusion methods; Land-use mappings; Optical image; Remote sensing data; Research questions; Systematic literature review (SLR); Systematic study; Synthetic aperture radar","Image fusion; Optical images; Remote sensing; Synthetic aperture radar (SAR) data; Systematic literature review (SLR)","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85066804304"
"Longman F.S.; Mihaylova L.; Coca D.","Longman, Fodio S. (57194551395); Mihaylova, Lyudmila (6603292839); Coca, Daniel (9844474000)","57194551395; 6603292839; 9844474000","Oil spill segmentation in fused Synthetic Aperture Radar images","2017","4th International Conference on Control Engineering and Information Technology, CEIT 2016","","","7929055","","","","10.1109/CEIT.2016.7929055","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020700225&doi=10.1109%2fCEIT.2016.7929055&partnerID=40&md5=ef7756d411973c08b843855cf1fe39ba","Synthetic Aperture Radar (SAR) satellite systems are very efficient in oil spill monitoring due to their capability to operate under all weather conditions. Systems such as the Envisat and RADARSAT have been used independently in many studies to detect oil spill. This paper presents an automatic feature based image registration and fusion algorithm for oil spill monitoring using SAR images. A range of metrics are used to evaluate the performance of the algorithm and to demonstrate the benefits of fusing SAR images of different modalities. The proposed framework has shown 45% improvement of the oil spill location when compared with the individual images before the fusion. © 2016 IEEE.","Geodetic satellites; Image fusion; Image registration; Image segmentation; Oil spills; Radar; Synthetic aperture radar; ENVISAT; Feature-based; Fusion algorithms; Radarsat; Registration; SAR Images; Satellite system; Radar imaging","Image Fusion; Oil Spill; Registration; Segmentation; Synthetic Aperture Radar (SAR)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85020700225"
"Gao J.; Zhang H.; Yuan Q.","Gao, Jianhao (57211516266); Zhang, Hai (57192694132); Yuan, Qiangqiang (36635300800)","57211516266; 57192694132; 36635300800","Cloud removal with fusion of SAR and Optical Images by Deep Learning","2019","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2019","","","8866939","","","","10.1109/Multi-Temp.2019.8866939","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074253572&doi=10.1109%2fMulti-Temp.2019.8866939&partnerID=40&md5=9a2c84444d558efceb089852f44c5eb9","Due to the different imaging methods of SAR image and optical image, it is difficult to establish the corresponding relationship between them by traditional methods. However, with the development of deep learning, there are many researches on the transformation from SAR image to optical image based on GAN, which prove that the mapping between SAR image and optical image can be achieved. Based on this, this work will transform the SAR image into optical image and fuse to fill the cloud area of the optical image. This work will provide a method of heterogeneous image fusion to remove cloud, and get a good effect. © 2019 IEEE.","Deep learning; Geometrical optics; Image analysis; Image fusion; Remote sensing; Synthetic aperture radar; Cloud removal; Imaging method; Optical image; SAR Images; Radar imaging","Cloud removal; Deep Learning; GAN","Conference paper","Final","","Scopus","2-s2.0-85074253572"
"Abulkhanov D.; Konovalenko I.; Nikolaev D.; Savchik A.; Shvets E.; Sidorchuk D.","Abulkhanov, Dmitry (57192989832); Konovalenko, Ivan (56765780500); Nikolaev, Dmitry (36859920000); Savchik, Alexey (57196076447); Shvets, Evgeny (57053067800); Sidorchuk, Dmitry (56287813700)","57192989832; 56765780500; 36859920000; 57196076447; 57053067800; 56287813700","Neural network-based feature point descriptors for registration of optical and SAR images","2018","Proceedings of SPIE - The International Society for Optical Engineering","10696","","106960L","","","","10.1117/12.2310085","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046478658&doi=10.1117%2f12.2310085&partnerID=40&md5=3d28e9339409a34b475b5a4cdffea278","Registration of images of different nature is an important technique used in image fusion, change detection, efficient information representation and other problems of computer vision. Solving this task using feature-based approaches is usually more complex than registration of several optical images because traditional feature descriptors (SIFT, SURF, etc.) perform poorly when images have different nature. In this paper we consider the problem of registration of SAR and optical images. We train neural network to build feature point descriptors and use RANSAC algorithm to align found matches. Experimental results are presented that confirm the method's effectiveness. © Copyright 2018 SPIE.","Computer vision; Geometrical optics; Image fusion; Remote sensing; Synthetic aperture radar; Change detection; Descriptors; Feature based approaches; Feature descriptors; Information representation; Point descriptors; RANSAC algorithm; spatial consistent matching (SCM); Radar imaging","computer vision; feature points descriptors; optical and SAR image registration; remote sensing; spatial consistent matching (SCM)","Conference paper","Final","","Scopus","2-s2.0-85046478658"
"Kumar M.; Chauhan H.B.; Rajawat A.S.; Ajai","Kumar, Mohit (58105294400); Chauhan, H.B. (7006361547); Rajawat, A.S. (6507471483); Ajai (24292733600)","58105294400; 7006361547; 6507471483; 24292733600","Study of mangrove communities in Marine National Park and Sanctuary, Jamnagar, Gujarat, India, by fusing RISAT-1 SAR and Resourcesat-2 LISS-IV images","2017","International Journal of Image and Data Fusion","8","1","","73","91","18","10.1080/19479832.2016.1232755","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988566240&doi=10.1080%2f19479832.2016.1232755&partnerID=40&md5=ec64b2654188f561903b5a7f23311bc1","In this study, RISAT-1 (Radar Imaging Satellite-1) HH image has been fused with Resourcesat 2 LISS-IV (Linear Imaging Self Scanning-IV) image to study the mangrove communities of Jindra-Chhad island complex, in Marine National Park and Sanctuary (MNP&S), Jamnagar, Gujarat, India. Three different methods were used to fuse RISAT-1 and LISS-IV images. In one case, the Synthetic Aperture Radar (SAR) data was simply integrated as an additional band to the three bands of LISS-IV data, whereas in the other, Intensity-Hue-Saturation method was used to merge the two data sets. In yet another exercise, the vegetative and sedimentary parts were separated from the optical data by computing normalised difference vegetation index (NDVI) and by averaging the Red and Green bands, respectively. These two layers, viz., NDVI and the average of Red and Green bands, were then integrated with the SAR data. All the merged products were put to supervised classification using maximum likelihood algorithm into following seven classes: mangrove communities (Avicennia Dense, Avicennia Sparse, Rhizophora-Ceriops Dense), Intertidal Mudflat, Hightidal Mudflat, Sand and Sea. The class separability analysis indicated that the separability obtained among the classes in the case of fused products was higher than that obtained when both the data sets were classified individually. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","Gujarat; India; Jamnagar; Green computing; Image fusion; Marine radar; Maximum likelihood; Radar; Synthetic aperture radar; Vegetation; Class separability; community zonation; Intensity hue saturations; Mangroves; Maximum likelihood algorithm; National parks; Normalised difference vegetation index; Supervised classification; algorithm; community structure; image processing; LISS; mangrove; maximum likelihood analysis; NDVI; Resourcesat; supervised classification; synthetic aperture radar; Radar imaging","class separability analysis; community zonation; image fusion; Mangroves; Marine National Park & Sanctuary","Article","Final","","Scopus","2-s2.0-84988566240"
"Anandhi D.; Valli S.","Anandhi, D. (57200875841); Valli, S. (13409224400)","57200875841; 13409224400","An algorithm for multi-sensor image fusion using maximum a posteriori and nonsubsampled contourlet transform","2018","Computers and Electrical Engineering","65","","","139","152","13","10.1016/j.compeleceng.2017.04.002","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017445666&doi=10.1016%2fj.compeleceng.2017.04.002&partnerID=40&md5=fdb262ebf2b03713ddfd0dea7e8875ef","Multi-sensor image fusion draws an inference based on the information obtained from different sensors. Recently, wavelet and contourlet transforms have been widely used in multi-sensor image fusion. But these transforms have been inadequate in the representation of images due to their subsampling. Hence, a fusion algorithm based on Synthetic Aperture Radar (SAR) and Panchromatic (PAN) images in Nonsubsampled Contourlet Transform (NSCT) domain is proposed. NSCT gives flexible multiscale, multidirectional expansion for images. A high fusion accuracy is achieved by ‘Maximum A Posteriori (MAP)’ estimation based on Rayleigh and Laplacian probabilities for despeckling of SAR higher frequency coefficients. Subsequently, the despeckled SAR coefficient is directly fused with PAN coefficients using the newly developed Edge-based fusion rule. The combination of NSCT, MAP and Edge-fusion rule facilitates maximum preservation of the edge and the texture information. The performance of the proposed fusion algorithm is evaluated using reference and non-reference quality metrics. The results prove that the proposed method outperforms the existing NSCT methods by preserving maximum features. © 2017 Elsevier Ltd","Frequency estimation; Image enhancement; Radar; Radar imaging; Synthetic aperture radar; De-speckling; Maximum a posteriori; Maximum a posteriori estimation; Non subsampled contourlet transform (NSCT); Non-sub-sampled contourlet transforms; Panchromatic (Pan) image; Satellite images; Wavelet and contourlet transform; Image fusion","Despeckling; Maximum a posteriori estimation; Nonsubsampled Contourlet Transform; Satellite image fusion; Synthetic Aperture Radar","Article","Final","","Scopus","2-s2.0-85017445666"
"Pereira L.O.; Freitas C.C.; Santaanna S.J.S.; Reis M.S.","Pereira, Luciana O. (54409291400); Freitas, Corina C. (34770040600); Santaanna, Sidnei J. S. (57203064205); Reis, Mariane S. (56421416100)","54409291400; 34770040600; 57203064205; 56421416100","Evaluation of Optical and Radar Images Integration Methods for LULC Classification in Amazon Region","2018","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","9","8416958","3062","3074","12","10.1109/JSTARS.2018.2853647","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050409251&doi=10.1109%2fJSTARS.2018.2853647&partnerID=40&md5=20d0fb9acc213ee338fbbed2ced1e79b","The main objective of this study is to evaluate different methods to integrate (fusion and combination) Synthetic Aperture Radar (SAR) Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band SAR (PALSAR-1) (Fine Beam Dual mode-FDB) and LANDSAT images in order to identify those which lead to higher accuracy of land-use and land-cover (LULC) mapping in an agricultural frontier region in Amazon. One method used to integrate the multipolarized information in SAR images before the fusion process was also evaluated. In this method, the first principal component (PC1) of SAR data was used. Color compositions of fused data that presented better LULC classification were visually analyzed. Considering the proposed objective, the following fusion methods must be highlighted: Ehlers, Wavelet á trous, Intensity, Hue and Saturation (IHS), and selective principal component analysis (SPC). These latter three methods presented good results when processed using PC1 from ALOS/PALSAR-1 FBD backscatter filtered image or three SAR extracted and selected features. These results corroborate with the applicability of the proposed method for SAR data information integration. Distinct methods better discriminate different LULC classes. In general, densely forested classes were better characterized by the EhlersTM6 fusion method, in which at least the polarization HV was used. Intermediate and initial regeneration classes were better discriminated using SPC-fused data with PC1 of ALOS/PALSAR-1 FBD data. Bare soil and pasture classes were better discriminated in optical features and the PC1 of ALOS/PALSAR-1 FBD data fused by the IHS method. Soybean with approximately 40 days from seeding was better discriminated in image classification obtained from ALOS/PALSAR-1 FBD image. © 2008-2012 IEEE.","Amazonia; Brazil; Glycine max; Data integration; Data visualization; Earth (planet); Feature extraction; Image classification; Image fusion; Land use; Light polarization; Mapping; Optical sensors; Polarization; Principal component analysis; Reforestation; Space-based radar; Synthetic aperture radar; Advanced land observing satellites; Brazilian Amazon; Color composition; First principal components; Integration method; Land use and land cover; Optical features; Optical imaging; ALOS; imaging method; land classification; land cover; land use change; Landsat; PALSAR; radar imagery; synthetic aperture radar; Radar imaging","Brazilian Amazon; data integration; land-use and land-cover (LULC); multipolarized-synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85050409251"
"Zhu S.; Ran D.","Zhu, Shibing (57203133442); Ran, Da (56459824300)","57203133442; 56459824300","Multi-angle SAR image fusion algorithm based on visibility classification of non-layover region targets","2018","2017 International Conference on Security, Pattern Analysis, and Cybernetics, SPAC 2017","2018-January","","","642","647","5","10.1109/SPAC.2017.8304355","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050618545&doi=10.1109%2fSPAC.2017.8304355&partnerID=40&md5=5ef67081400fcb716dde175f97337d09","In order to reduce the layover region in traditional single-angle synthetic aperture radar (SA-SAR) image of mountainous area, a multi-angle synthetic aperture radar (MA-SAR) image fusion algorithm based on visibility classification of non-layover region targets is proposed. By defining a practical index named multi-angle visibility of non-layover region targets which used for automatic pixel classification, the proposed algorithm calculates the visibility index of every pixel of SA-SAR images obtained from different observation angles and fuses those pixels with the same visibility index to form the fused image. The fused image can effectively eliminate all adverse effects on target detection and classification which caused by the phenomenon of layover, and realize a precision MA-SAR imaging of mountainous area. The simulation results have verified the effectiveness of the proposed algorithm. © 2017 IEEE.","Cybernetics; Image classification; Image fusion; Pixels; Radar target recognition; Synthetic aperture radar; Visibility; Adverse effect; Image fusion algorithms; Mountainous area; Multi angle; Multi-Angle-SAR; Observation angle; Pixel classification; Target detection and classifications; Radar imaging","Automatic pixel classification; Image fusion; Multi-angle synthetic aperture radar (MA-SAR); Multi-angle visibility index","Conference paper","Final","","Scopus","2-s2.0-85050618545"
"Leelavathi H.P.; Prakash J.","Leelavathi, H.P. (57216771012); Prakash, J. (55649571476)","57216771012; 55649571476","Two stage fusion scheme based on a curvelet with gradient technique for integration of sar image on pan image to improve the spatial resolution.","2020","International Journal of Advanced Science and Technology","29","5","","4041","4053","12","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084522611&partnerID=40&md5=b817b9e35b424d05a0bbcfa48165b452","The process of combining the information captured by the various image sensors to improve the perception of an image is known as the multi sensor image fusion. In this paper, curvelet with gradient technique based image fusion scheme is described to integrate synthetic aperture radar (SAR) image onto panchromatic (PAN) image. The preprocessing of the SAR is first performed using effective speckle noise removal filter. The coregistration of these two images is then performed with respect to the PAN image. Grey level co occurrence matrix (GLCM) image texture analysis is next used on SAR image to obtain the texture feature. Then the texture feature of SAR and the PAN images are decomposed into fine and coarse coefficients using the wrap fast discrete curvelet transformation (WFDCT). Further the specific features of the SAR curvelet fine coefficients are integrated into PAN curvelet fine coefficients. The integration of images is performed by the block based WFDCT, which imposes the different selection rule for each block. In the final stage, the SAR image is integrated with an intermediate PAN image using the gradient fusion technique to have more information while preserving the nonadaptive sparse representation of an image. Experimental outcomes illustrated that the fusion scheme proposed in this paper has good performance compared to other fusion methods, and potential to be applied to integrate the synthetic aperture radar and panchromatic images. © 2020 SERSC.","","Co-registration; Fast Discrete Curvelet Transform; Gradient Fusion; Multi Sensor Image Fusion; PAN Image; SAR Image; Texture Feature","Article","Final","","Scopus","2-s2.0-85084522611"
"Palm S.; Sommer R.; Janssen D.; Tessmann A.; Stilla U.","Palm, Stephan (56652736300); Sommer, Rainer (23991572200); Janssen, Daniel (57189343433); Tessmann, Axel (6701536144); Stilla, Uwe (6602211600)","56652736300; 23991572200; 57189343433; 6701536144; 6602211600","Airborne circular W-Band SAR for multiple aspect urban site monitoring","2019","IEEE Transactions on Geoscience and Remote Sensing","57","9","8701523","6996","7016","20","10.1109/TGRS.2019.2909949","29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072039510&doi=10.1109%2fTGRS.2019.2909949&partnerID=40&md5=48bdb1a092851573953935545dc3bf74","This paper presents a strategy for urban site monitoring by very high-resolution circular synthetic aperture radar (CSAR) imaging of multiple aspects. We analytically derive the limits of coherent azimuth processing for nonplanar objects in CSAR if no digital surface model (DSM) is available. The result indicates the level of maximum achievable resolution of these objects in this geometry. The difficulty of constantly illuminating a specific scene in full aspect mode (360°) for such small wavelengths is solved by a hardware- and software-side integration of the radar in a mechanical tracking mode. This results in the first demonstration of full aspect airborne subaperture CSAR images collected with an active frequency-modulated continuous wave (FMCW) radar at W-band. We describe the geometry and the implementation of the real-time beam-steering mode and evaluate resulting effects in the CSAR processing chain. The physical properties in W-band allow the use of extremely short subapertures in length while generating high azimuthal bandwidths. We use this feature to generate full aspect image stacks for CSAR video monitoring in very high frame rates. This technique offers the capability of detecting and observing moving objects in single channel data by shadow tracking. Due to the relatively strong echo of roads, the shadows of moving cars are rich in contrast. The image stack is further evaluated to present wide angular anisotropic properties of targets and first results on multiple aspect image fusion. Both topics show huge potential for further investigations in terms of image analysis and scene classification. © 1980-2012 IEEE.","Frequency modulation; Image fusion; Millimeter waves; Object detection; Radar imaging; Radar signal processing; Tracking radar; Urban growth; Airborne circular synthetic aperture radar (CSAR); Anisotropic property; Digital surface models; Frequency modulated continuous wave radars; Hardware and software; Millimeter wave radar; Scene classification; video SAR; Synthetic aperture radar","Airborne circular synthetic aperture radar (CSAR); full aspect image analysis; millimeter-wave radar; nonplanar targets; radar signal processing; shadow tracking; video SAR","Article","Final","","Scopus","2-s2.0-85072039510"
"Liu Y.; Zhang P.; He Y.; Peng Z.","Liu, Yuhan (57189327510); Zhang, Pengfei (57216287658); He, Yanmin (55498870800); Peng, Zhenming (23985664500)","57189327510; 57216287658; 55498870800; 23985664500","River detection based on feature fusion from synthetic aperture radar images","2020","Journal of Applied Remote Sensing","14","1","016505","","","","10.1117/1.JRS.14.016505","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082925294&doi=10.1117%2f1.JRS.14.016505&partnerID=40&md5=849209bdafe39d3f4c71bc3d734b794e","Synthetic aperture radar (SAR) data that can collect information day and night is widely applied in both military and civilian life for security, environmental, and geographical systems. However, detection of rivers in such images is still a challenging problem because rivers are complex with various directions and branches. We aim to detect rivers from SAR images and propose an algorithm combining saliency features, multifeature fusion, and active contour model. The proposed method first filters the image and extracts the global saliency features, which are different from traditional river detection approaches that are mostly based on edge information. A feature fusion technique based on principal component analysis is then applied to merge the saliency features to achieve optimal feature map. Finally, an active contour model is applied to detect the river. Our major contributions are characterizing the rivers by their saliency features, introducing a feature fusion method, and designing an improvement strategy. Experimental results and assessments show that the algorithm is effective and can achieve competitive performance compared with other methods. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Feature extraction; Image fusion; Principal component analysis; Rivers; Synthetic aperture radar; Active contour model; Competitive performance; Edge information; Feature fusion method; Improvement strategies; Multi-feature fusion; River detections; Saliency features; Radar imaging","active contour model; feature fusion; principal component analysis; river detection; synthetic aperture radar image","Article","Final","","Scopus","2-s2.0-85082925294"
"Wu Y.; Fan J.; Li S.; Wang F.; Liang W.","Wu, Yan (7406895902); Fan, Jianwei (55964594100); Li, Siyu (57194692983); Wang, Fan (56299935600); Liang, Wenkai (57192207410)","7406895902; 55964594100; 57194692983; 56299935600; 57192207410","Fusion of synthetic aperture radar and visible images based on variational multiscale image decomposition","2017","Journal of Applied Remote Sensing","11","2","025006","","","","10.1117/1.JRS.11.025006","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021689682&doi=10.1117%2f1.JRS.11.025006&partnerID=40&md5=e47cab46e82517ba9e33bcbdb151b777","Multisensor image fusion can be used as an advanced technique for image enhancement. We propose a synthetic aperture radar (SAR) and visible image fusion method under the framework of a variational multiscale image decomposition (VMID) model. In the proposed method, two fusion rules are respectively designed to fuse the structure and texture components obtained with a VMID model of source images. A fusion rule based on the curvelet transform is employed for fusing the structure component and the local energy criterion is adopted to construct the texture component. Moreover, considering the influence of speckle noise present in an SAR image, its first two texture components are skipped. The final fused result is composed by the obtained structure and texture components. The experimental results on several pairs of SAR and visible images demonstrate the effectiveness of the proposed method. Compared with the conventional image fusion methods, our fused results have a finer structure and are more robust against speckle noise. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","Image enhancement; Image fusion; Image texture; Speckle; Synthetic aperture radar; Textures; Curvelet transforms; Fusion rule; Image fusion methods; Multisensor image fusion; Structure component; Texture components; Variational multiscale; Visible image; Radar imaging","image fusion; SAR and visible images; structure fusion rule; texture fusion rule; variational multiscale image decomposition","Article","Final","","Scopus","2-s2.0-85021689682"
"Chen B.; Xiao X.; Ye H.; Ma J.; Doughty R.; Li X.; Zhao B.; Wu Z.; Sun R.; Dong J.; Qin Y.; Xie G.","Chen, Bangqian (48261068400); Xiao, Xiangming (7402170368); Ye, Huichun (38762511500); Ma, Jun (56146653300); Doughty, Russell (57192589807); Li, Xiangping (57206741690); Zhao, Bin (56376088100); Wu, Zhixiang (48261647000); Sun, Rui (57195292604); Dong, Jinwei (22956851400); Qin, Yuanwei (26028957000); Xie, Guishui (37040014000)","48261068400; 7402170368; 38762511500; 56146653300; 57192589807; 57206741690; 56376088100; 48261647000; 57195292604; 22956851400; 26028957000; 37040014000","Mapping Forest and Their Spatial-Temporal Changes from 2007 to 2015 in Tropical Hainan Island by Integrating ALOS/ALOS-2 L-Band SAR and Landsat Optical Images","2018","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","3","","852","867","15","10.1109/JSTARS.2018.2795595","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043395560&doi=10.1109%2fJSTARS.2018.2795595&partnerID=40&md5=98e264d2fd1a6a152ca0668b008097b5","Accurately monitoring forest dynamics in the tropical regions is essential for ecological studies and forest management. In this study, images from phase-array L-band synthetic aperture radar (PALSAR), PALSAR-2, and Landsat in 2006-2010 and 2015 were combined to identify tropical forest dynamics on Hainan Island, China. Annual forest maps were first mapped from PALSAR and PALSAR-2 images using structural metrics. Those pixels with a high biomass of sugarcane or banana, which are widely distributed in the tropics and subtropics and have similar structural metrics as forests, were excluded from the SAR-based forest maps by using phenological metrics from time series Landsat imagery. The optical-SAR-based forest maps in 2010 and 2015 had high overall accuracies (OA) of 92-97% when validated with ground reference data. The resultant forest map in 2010 shows good spatial agreement with public optical-based forest maps (OA = 88-90%), and the annual forest maps (2007-2010) were spatiotemporally consistent and more accurate than the PALSAR-based forest map from the Japan Aerospace Exploration Agency (OA = 82% in 2010). The areas of forest gain, loss, and net change on Hainan Island from 2007 to 2015 were 415 000 ha (+2.17% yr-1), 179 000 ha (-0.94% yr-1), and 236 000 ha (+1.23% yr-1), respectively. About 95% of forest gain and loss occurred in those areas with an elevation less than 400 m, where deciduous rubber, eucalyptus plantations, and urbanization expanded rapidly. This study demonstrates the potential of PALSAR/PALSAR-2/Landsat image fusion for monitoring annual forest dynamics in the tropical regions. © 2008-2012 IEEE.","China; Hainan; Eucalyptus; Agriculture; Data fusion; Dynamics; Earth (planet); Forestry; Geometrical optics; Image fusion; Landforms; Optical image storage; Radar; Radar imaging; Remote sensing; Rubber plantations; Satellites; Space research; Space-based radar; Surface waters; Synthetic aperture radar; Tropical engineering; Tropics; Biomass crop; Biomedical optical imaging; Forest loss; Image data fusion; Land surface water index; Normalized difference vegetation index; Optical imaging; accuracy assessment; ALOS; data processing; forest cover; image analysis; Landsat; NDVI; satellite imagery; spatiotemporal analysis; synthetic aperture radar; tropical region; vegetation mapping; Mapping","Forest loss and gain; high biomass crops; image data fusion; land surface water index (LSWI); normalized difference vegetation index (NDVI)","Article","Final","","Scopus","2-s2.0-85043395560"
"Liu S.; Yang J.","Liu, Sikai (57200313026); Yang, Jun (57192452394)","57200313026; 57192452394","Target recognition in synthetic aperture radar images via joint multifeature decision fusion","2018","Journal of Applied Remote Sensing","12","1","016012","","","","10.1117/1.JRS.12.016012","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040777532&doi=10.1117%2f1.JRS.12.016012&partnerID=40&md5=71dd2ab0f7cad470d594f9371702c4e7","Multifeature decision fusion is an effective way to promote the performance of target recognition of synthetic aperture radar (SAR) images. This paper proposes a joint multifeature decision fusion strategy for target recognition in SAR images based on multitask compressive sensing (MtCS). The proposed method can exploit the intercorrelations among different features by enforcing the constraint on the sparsity pattern. Furthermore, the time consumption for MtCS is almost the same with that of single feature-based compressive classification, such as sparse representation-based classification. Experiments on the moving and stationary target acquisition and recognition dataset and comparison with several state-of-the-art methods demonstrate the validity of the proposed method. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Compressed sensing; Image fusion; Radar imaging; Synthetic aperture radar; Compressive sensing; Multi features; Sparse representation based classifications; Sparsity patterns; State-of-the-art methods; Stationary targets; Synthetic aperture radar (SAR) images; Target recognition; Radar target recognition","joint multifeature decision fusion; multitask compressive sensing; synthetic aperture radar; target recognition","Article","Final","","Scopus","2-s2.0-85040777532"
"Kaplan G.; Avdan U.","Kaplan, Gordana (57196402161); Avdan, Ugur (8356726300)","57196402161; 8356726300","Sentinel-1 and Sentinel-2 data fusion for wetlands mapping: Balikdami, Turkey","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","3","","729","734","5","10.5194/isprs-archives-XLII-3-729-2018","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046942525&doi=10.5194%2fisprs-archives-XLII-3-729-2018&partnerID=40&md5=ddd1d0f647761862600bb2d359bf7572","Wetlands provide a number of environmental and socio-economic benefits such as their ability to store floodwaters and improve water quality, providing habitats for wildlife and supporting biodiversity, as well as aesthetic values. Remote sensing technology has proven to be a useful and frequent application in monitoring and mapping wetlands. Combining optical and microwave satellite data can help with mapping and monitoring the biophysical characteristics of wetlands and wetlands' vegetation. Also, fusing radar and optical remote sensing data can increase the wetland classification accuracy. In this paper, data from the fine spatial resolution optical satellite, Sentinel-2 and the Synthetic Aperture Radar Satellite, Sentinel-1, were fused for mapping wetlands. Both Sentinel-1 and Sentinel-2 images were pre-processed. After the pre-processing, vegetation indices were calculated using the Sentinel-2 bands and the results were included in the fusion data set. For the classification of the fused data, three different classification approaches were used and compared. The results showed significant improvement in the wetland classification using both multispectral and microwave data. Also, the presence of the red edge bands and the vegetation indices used in the data set showed significant improvement in the discrimination between wetlands and other vegetated areas. The statistical results of the fusion of the optical and radar data showed high wetland mapping accuracy, showing an overall classification accuracy of approximately 90% in the object-based classification method. For future research, we recommend multi-temporal image use, terrain data collection, as well as a comparison of the used method with the traditional image fusion techniques. © Authors 2018.","Biodiversity; Image fusion; Mapping; Remote sensing; Satellites; Space-based radar; Synthetic aperture radar; Vegetation; Water quality; Wetlands; Biophysical characteristics; Microwave satellite data; Object-based classifications; Optical remote sensing data; Remote sensing technology; Sentinel-1; Sentinel-2; Socio-economic benefits; Classification (of information)","Image fusion; Object-based classification; Sentinel-1; Sentinel-2; Wetlands","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85046942525"
"Schmitt A.; Wendleder A.; Kleynmans R.; Hell M.; Roth A.; Hinz S.","Schmitt, Andreas (55576223700); Wendleder, Anna (36012163600); Kleynmans, Rüdiger (57215896519); Hell, Maximilian (57215897252); Roth, Achim (56256016700); Hinz, Stefan (7004082496)","55576223700; 36012163600; 57215896519; 57215897252; 56256016700; 7004082496","Multi-source and multi-temporal image fusion on hypercomplex bases","2020","Remote Sensing","12","6","943","","","","10.3390/rs12060943","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082301820&doi=10.3390%2frs12060943&partnerID=40&md5=620c5ea592e15304bf93ca6248807e97","This article spanned a new, consistent framework for production, archiving, and provision of analysis ready data (ARD) from multi-source and multi-temporal satellite acquisitions and an subsequent image fusion. The core of the image fusion was an orthogonal transform of the reflectance channels from optical sensors on hypercomplex bases delivered in Kennaugh-like elements, which are well-known from polarimetric radar. In this way, SAR and Optics could be fused to one image data set sharing the characteristics of both: the sharpness of Optics and the texture of SAR. The special properties of Kennaugh elements regarding their scaling-linear, logarithmic, normalized-applied likewise to the newelements and guaranteed their robustness towards noise, radiometric sub-sampling, and therewith data compression. This study combined Sentinel-1 and Sentinel-2 on an Octonion basis as well as Sentinel-2 and ALOS-PALSAR-2 on a Sedenion basis. The validation using signatures of typical land cover classes showed that the efficient archiving in 4 bit images still guaranteed an accuracy over 90% in the class assignment. Due to the stability of the resulting class signatures, the fuzziness to be caught by Machine Learning Algorithms was minimized at the same time. Thus, this methodology was predestined to act as new standard for ARD remote sensing data with an subsequent image fusion processed in so-called data cubes. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Data Sharing; Image analysis; Learning algorithms; Machine learning; Radar imaging; Remote sensing; Synthetic aperture radar; Textures; Time series; Time series analysis; Analysis ready data; Change detection; Data cube; Efficient archiving; Hypercomplex bases; Kennaugh framework; Quaternion; SAR sharpening; Image fusion","Analysis ready data; Change detection; Data cube; Efficient archiving; Hypercomplex bases; Image fusion; Kennaugh framework; Quaternion; SAR sharpening; Time series","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85082301820"
"Kumar J.T.; Reddy Y.M.; Rao B.P.","Kumar, J. Thrisul (57226875990); Reddy, Y. Mallikarjuna (35766908800); Rao, B. Prabhakara (26655008200)","57226875990; 35766908800; 26655008200","WHDA-FCM: Wolf Hunting-Based Dragonfly with Fuzzy C-Mean Clustering for Change Detection in SAR Images","2020","Computer Journal","63","2","","308","321","13","10.1093/comjnl/bxz130","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081565195&doi=10.1093%2fcomjnl%2fbxz130&partnerID=40&md5=dd1a78fbb9b2b6aefb62befef081c80a","For the past few years, the automated addressing of changes in remote sensing images plays a significant role. However, the change detection (CD) model often suffers from the issue of speckle noise. More investigations have been proceeded to overcome this obstacle. This paper also considers the same issue and proposes a new CD model in synthetic aperture radar (SAR) images. Here, two SAR images that are captivated at different times will be considered as the input of the detection process. At first, discrete wavelet transform is incurred for image fusion, where the coefficients are optimally selected through a hybrid model that hybridizes the gray wolf optimization and dragonfly (DA) optimization. At last, the fused images after inverse transform are clustered via the fuzzy c-mean (FCM) clustering approach, and a similarity measure is performed between the segmented image and the ground truth image. The proposed model, wolf hunting-based DA with FCM, compares its performance over other conventional methods in terms of measures like accuracy, specificity, sensitivity, precision, negative predictive value, F1 score and Matthews correlation coefficient. Similarly, the negative measures are false positive rate, false negative rate and false discovery rate, and the betterment is proven. © 2019 The British Computer Society 2019. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.","Discrete wavelet transforms; Fuzzy filters; Image fusion; Inverse problems; Inverse transforms; Remote sensing; Signal reconstruction; Synthetic aperture radar; Correlation coefficient; Filter coefficients; Fuzzy C mean; Fuzzy C mean clustering; Negative predictive value; SAR Images; Synthetic aperture radar (SAR) images; wolf hunting-based dragonfly with FCM; Radar imaging","discrete wavelet transform; filter coefficient; fuzzy c-mean; SAR image; wolf hunting-based dragonfly with FCM","Article","Final","","Scopus","2-s2.0-85081565195"
"Sidorchuk D.; Volkov V.; Gladilin S.","Sidorchuk, D. (56287813700); Volkov, V. (57202654240); Gladilin, S. (24472928800)","56287813700; 57202654240; 24472928800","Perception-oriented fusion of multi-sensor imagery: Visible, IR and SAR","2018","Proceedings of SPIE - The International Society for Optical Engineering","10696","","106961I","","","","10.1117/12.2309770","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046455642&doi=10.1117%2f12.2309770&partnerID=40&md5=42941294353f0c03e13b94dcd299596f","This paper addresses the problem of image fusion of optical (visible and thermal domain) data and radar data for the purpose of visualization. These types of images typically contain a lot of complimentary information, and their joint visualization can be useful and more convenient for human user than a set of individual images. To solve the image fusion problem we propose a novel algorithm that utilizes some peculiarities of human color perception and based on the grey-scale structural visualization. Benefits of presented algorithm are exemplified by satellite imagery. © 2018 Copyright SPIE.","Color vision; Computer vision; Data visualization; Radar imaging; Satellite imagery; Synthetic aperture radar; Visualization; Grey scale; Human color perception; Human users; Multi-sensor imagery; Novel algorithm; Radar data; Structural visualizations; Thermal domains; Image fusion","human color perception; Image fusion; structural visualization","Conference paper","Final","","Scopus","2-s2.0-85046455642"
"Kwak Y.-J.; Pelich R.; Park J.; Takeuchi W.","Kwak, Young-Joo (54408878500); Pelich, Ramona (56155778100); Park, Jonggeol (23980743300); Takeuchi, Wataru (55348898500)","54408878500; 56155778100; 23980743300; 55348898500","Improved flood mapping based on the fusion of multiple satellite data sources and in-situ data","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8517336","3521","3523","2","10.1109/IGARSS.2018.8517336","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063147445&doi=10.1109%2fIGARSS.2018.8517336&partnerID=40&md5=cea127d50ec2b04e19e87707c61cc94d","For high accuracy flood mapping, an algorithm that integrates multiple satellite data sources is essential to maximize the sensor ability and compensate the limitations of optical and SAR data. The main objective of this study is to propose an algorithm of dynamic flood detection using optical and Synthetic Aperture Radar (SAR) images that compares and combines two different statistical thresholding approaches. To improve the flood detection accuracy, image fusion technique was investigated to maximize the utilization of calibrated and optimized flood maps as the integrated flood detection approach. To showcase the advantages of the proposed methodology, we employ MODIS, Landsat-8 and Sentinel-1A images acquired over a challenging area along the Brahmaputra River where flood events often occur. © 2018 IEEE","","Dynamic flood mapping; Image fusion; MODIS; Multiple satellite data; Sentinel-1","Conference paper","Final","","Scopus","2-s2.0-85063147445"
"Vera J.E.; Mora S.F.; Torres J.A.; Avendano J.","Vera, J.E. (57188750269); Mora, S.F. (57188760236); Torres, J.A. (57199783919); Avendano, J. (57188767865)","57188750269; 57188760236; 57199783919; 57188767865","Analysis of images SAR to flood prevention implementing fusion methods","2016","2016 21st Symposium on Signal Processing, Images and Artificial Vision, STSIVA 2016","","","7743334","","","","10.1109/STSIVA.2016.7743334","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002872639&doi=10.1109%2fSTSIVA.2016.7743334&partnerID=40&md5=a5c88ed9cbbdaacf32cfd6d8d1800cfc","To improve the characteristics of images taken by the IDEAM, the results obtained applying two algorithms to Synthetic Aperture Radar images or SAR images in regions of Colombia affected by natural disasters are discussed and compared. Two techniques of digital image processing were used, pyramidal fusion Morphological and the Discrete Wavelet Transform. An analysis of the responses obtained by each method was performed for determining which method is suitable according to pixellevel image fusion, for testing purposes and compare the two techniques, data about of entropy and correlation was calculated in MATLAB®. The results obtained show that the morphological fusion method presents a high performance in the SAR image processing, significantly improving the grouping of points on the test image. © 2016 IEEE.","Data fusion; Disasters; Discrete wavelet transforms; Flood control; Image analysis; Image fusion; Image processing; Signal processing; Synthetic aperture radar; Vision; Wavelet transforms; Colombia; Flood prevention; Fusion methods; Natural disasters; Pixel-level image fusion; SAR image processing; SAR Images; Test images; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85002872639"
"Chen H.; Jiao L.; Liang M.; Liu F.; Yang S.; Hou B.","Chen, Huan (55577470000); Jiao, Licheng (7102491544); Liang, Miaomiao (57194766554); Liu, Fang (56182993400); Yang, Shuyuan (8159166000); Hou, Biao (7102142690)","55577470000; 7102491544; 57194766554; 56182993400; 8159166000; 7102142690","Fast unsupervised deep fusion network for change detection of multitemporal SAR images","2019","Neurocomputing","332","","","56","70","14","10.1016/j.neucom.2018.11.077","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058698724&doi=10.1016%2fj.neucom.2018.11.077&partnerID=40&md5=7c7ee1487907438d2680f6c51df973f0","In this paper, a fast unsupervised deep fusion framework for change detection of multitemporal synthetic aperture radar (SAR) images is presented. It mainly aim at generating a difference image (DI) in the feature learning procedure by stacked auto-encoders (SAEs). Stacked auto-encoders, as one kind of deep neural network, can learn feature maps that retain the structural information but suppress the noise in the SAR images, which will be beneficial for DI generation. Compared with shallow network, the proposed framework can extract more available features, and be favorable for getting better change results. Different with other common deep neural networks, our proposed method does not need labeled data to train the network. In addition, we find a subset of the entire samples that appropriately represent the whole dataset to speed up the training of the deep neural network without under-fitting. Moreover, we design a fusion network structure that can combine ratio operator based method to ensure that the representations of higher layers are better than that of the lower ones. To summarize, the main contribution of our work lies in using of deep fusion network for generation of DI in a fast and unsupervised way. Experiments on four real SAR images confirm that our network performs better than traditional ratio methods and convolutional neural network. © 2018 Elsevier B.V.","Deep neural networks; Image fusion; Neural networks; Signal encoding; Synthetic aperture radar; Convolutional neural network; Difference images; Multi-temporal SAR images; Multitemporal synthetic aperture radar images; Network structures; Structural information; Synthetic aperture radar (SAR) images; Unsupervised change detection; article; human; learning; noise; telecommunication; velocity; Radar imaging","Deep fusion network; Difference images (DI); Synthetic aperture radar (SAR) images; Unsupervised change detection","Article","Final","","Scopus","2-s2.0-85058698724"
"Shi Q.; Li J.; Yang W.; Zeng H.; Zhang H.","Shi, Quan (57196478592); Li, Jingwen (34882029100); Yang, Wei (56654531800); Zeng, Hongcheng (56039724400); Zhang, Haojie (56040359700)","57196478592; 34882029100; 56654531800; 56039724400; 56040359700","Multi-aspect SAR image fusion method based on wavelet transform","2017","Beijing Hangkong Hangtian Daxue Xuebao/Journal of Beijing University of Aeronautics and Astronautics","43","10","","2135","2142","7","10.13700/j.bh.1001-5965.2016.0823","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033596483&doi=10.13700%2fj.bh.1001-5965.2016.0823&partnerID=40&md5=ef84f8fe66a771cf073ec04cddcfeb8e","Considering the diversity of target scattering characteristics with different observation angle and information redundancy of multi-aspect images, this paper proposes a novel multi-aspect SAR image fusion method based on wavelet transform and edge detection. First, wavelet transform was performed to multi-aspect space-borne SAR image processing. The images were separated to different parts of frequency so that the multi-resolution representation and multi-aspect information of sequential images can be conveniently fused. Second, the improved Robinson edge detection algorithm was used to strengthen the energy of contour feature. Finally, fusion experiment and quantitative evaluation method were used to verify the effectiveness of this fusion imaging method. © 2017, Editorial Board of JBUAA. All right reserved.","Edge detection; Image compression; Image fusion; Image processing; Space-based radar; Synthetic aperture radar; Wavelet transforms; Edge detection algorithms; Fusion experiments; Information redundancies; Multi aspects; Multi resolution representation; Quantitative evaluation methods; Scattering char-acteristics; Space-borne SAR; Radar imaging","Edge detection; Image fusion; Multi-aspect images; Space-borne SAR; Wavelet transform","Article","Final","","Scopus","2-s2.0-85033596483"
"Longman F.S.; Mihaylova L.; Yang L.","Longman, Fodio S. (57194551395); Mihaylova, Lyudmila (6603292839); Yang, Le (55733080200)","57194551395; 6603292839; 55733080200","A Gaussian Process Regression Approach for Fusion of Remote Sensing Images for Oil Spill Segmentation","2018","2018 21st International Conference on Information Fusion, FUSION 2018","","","8455304","62","69","7","10.23919/ICIF.2018.8455304","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054062121&doi=10.23919%2fICIF.2018.8455304&partnerID=40&md5=e84d7d1b93e196cf712821f867412ba1","Synthetic Aperture Radar (SAR) satellite systems are very efficient in oil spill monitoring due to their capability to operate under all weather conditions. This paper presents a framework using Gaussian process (GP) to fuse SAR images of different modalities and to segment dark areas (assumed oil spill) for oil spill detection. A new covariance function; a product of an intrinsically sparse kernel and a Rational Quadratic Kernel (RQK) is used to model the prior of the estimated image allowing information to be transferred. The accuracy performance evaluation demonstrates that the proposed framework has 37% less RMSE per pixel and a compelling enhancement visually when compared with existing methods. © 2018 ISIF","Gaussian distribution; Gaussian noise (electronic); Image segmentation; Information fusion; Oil spills; Radar imaging; Rational functions; Remote sensing; Synthetic aperture radar; Covariance function; Gaussian process regression; Gaussian Processes; Oil spill detection; Performance evaluations; Quadratic kernels; Registration; Remote sensing images; Image fusion","Gaussian Processes; Image Fusion; Oil Spill; Registration; Segmentation; Synthetic Aperture Radar (SAR)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054062121"
"Zheng L.; Pei J.; Zhang Y.; Huang Y.; Wu J.; Yang J.","Zheng, Li (57239002400); Pei, Jifang (55787739300); Zhang, Yin (55975581400); Huang, Yulin (23014806800); Wu, Junjie (55713990900); Yang, Jianyu (9239230100)","57239002400; 55787739300; 55975581400; 23014806800; 55713990900; 9239230100","SAR and optical image fusion for coastal surveillance","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-January","","8900420","2802","2805","3","10.1109/IGARSS.2019.8900420","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113840160&doi=10.1109%2fIGARSS.2019.8900420&partnerID=40&md5=761b61e632d3ce9d137c4d9b653a329d","Coastal surveillance has long been paid a lot of attention for the threat of flooding due to some natural phenomena, such as global warming. Prompt and accurate reaction to the visualization of the flooded areas is the key. An image fusion rule is thus proposed in this paper to achieve image enhancement of the flooded areas. The rule, targeted at high-frequency parts of the synthetic aperture radar (SAR) and optical images, is able to exploit and combine the merits of both SAR and optical images to obtain the exact flooded areas with the clear boundaries. Experimental results validate the performance of the proposed fusion rule and show that not only the clarity of fusion images is improved, but also the texture and brightness contrast are greatly enhanced. © 2019 IEEE.","Floods; Geometrical optics; Global warming; Image enhancement; Image fusion; Remote sensing; Synthetic aperture radar; Textures; Coastal surveillance; Flooded areas; Fusion image; Fusion rule; High frequency HF; Natural phenomena; Optical image; Radar imaging","Image fusion; Monitor flooded area; Nonsubsampled contourlet transform","Conference paper","Final","","Scopus","2-s2.0-85113840160"
"Xu T.; Wu T.; Xiang H.","Xu, Tao (56783448700); Wu, Tao (57199835763); Xiang, Haibing (55576445600)","56783448700; 57199835763; 55576445600","High Resolution SAR and Panchromatic Image Fusion based on Bidimensional Empirical Mode Decomposition","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048373","","","","10.1109/APSAR46974.2019.9048373","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083512377&doi=10.1109%2fAPSAR46974.2019.9048373&partnerID=40&md5=9a7d9a2486d87e7d5d1b2090209729e7","Bidimensional empirical mode decomposition (BEMD) method is considered advantageous for analyzing non-stationary and non-linear signals. Recently, it has been introduced in non-stationary high resolution Synthetic Aperture Radar (SAR) image processing. This letter proposes a new method for fusing high resolution Synthetic Aperture Radar (SAR) and Panchromatic images, based on BEMD method. Under this method, first, multi-resolution decomposition images are obtained, from the original images based on BEMD algorithm, the features of the original images are separated into multiple scales of spatial frequencies, called intrinsic mode functions (IMF), and then an area-based image fusion scheme is applied to fuse the images at each decomposition level. Experimental results from GaoFen-1 (GF-1) Panchromatic and COSMO-SkyMed SAR images show that the proposed method not only preserves the high spatial resolution of Panchromatic image, but also combines the target and surface information in SAR image, which are difficult to identify in the Panchromatic image. The new algorithm outperforms the wavelet transform and non-subsampled contourlet transform (NSCT), in terms of both visual effect and quantitative analysis. © 2019 IEEE.","Image fusion; Image processing; Synthetic aperture radar; Wavelet transforms; Bi dimensional empirical mode decomposition (BEMD); Bi-dimensional empirical mode decompositions; High resolution synthetic aperture radar; High resolution synthetic aperture radar images; High spatial resolution; Intrinsic Mode functions; Multi resolution decomposition; Non subsampled contourlet transform (NSCT); Radar imaging","Area-based fusion scheme; Empirical mode decomposition (EMD); Image fusion; Laplace filter","Conference paper","Final","","Scopus","2-s2.0-85083512377"
"Hu J.; Hong D.; Zhu X.X.","Hu, Jingliang (57192207722); Hong, Danfeng (56108179600); Zhu, Xiao Xiang (55696622200)","57192207722; 56108179600; 55696622200","MIMA: MAPPER-Induced Manifold Alignment for Semi-Supervised Fusion of Optical Image and Polarimetric SAR Data","2019","IEEE Transactions on Geoscience and Remote Sensing","57","11","8802291","9025","9040","15","10.1109/TGRS.2019.2924113","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074472199&doi=10.1109%2fTGRS.2019.2924113&partnerID=40&md5=9eba270c075ec58bead50d6b9b84b84b","Multi-modal data fusion has recently been shown promise in classification tasks in remote sensing. Optical data and radar data, two important yet intrinsically different data sources, are attracting more and more attention for potential data fusion. It is already widely known that a machine learning-based methodology often yields excellent performance. However, the methodology relies on a large training set, which is very expensive to achieve in remote sensing. The semi-supervised manifold alignment (SSMA), a multi-modal data fusion algorithm, has been designed to amplify the impact of an existing training set by linking labeled data to unlabeled data via unsupervised techniques. In this paper, we explore the potential of SSMA in fusing optical data and polarimetric synthetic aperture radar (SAR) data, which are multi-sensory data sources. Furthermore, we propose a MAPPER-induced manifold alignment (MIMA) for the semi-supervised fusion of multi-sensory data sources. Our proposed method unites SSMA with MAPPER, which is developed from the emerging topological data analysis (TDA) field. To the best of our knowledge, this is the first time that SSMA has been applied on fusing optical data and SAR data, and also the first time that TDA has been applied in remote sensing. The conventional SSMA derives a topological structure using $k$-nearest neighbor (kNN), while MIMA employs MAPPER, which considers the field knowledge and derives a novel topological structure through the spectral clustering in a data-driven fashion. The experimental results on data fusion with respect to land cover land use classification and local climate zone classification suggest superior performance of MIMA. © 2019 IEEE.","Clustering algorithms; Geometrical optics; Hyperspectral imaging; Information analysis; Land use; Mapping; Modal analysis; Nearest neighbor search; Polarimeters; Radar imaging; Remote sensing; Sensory analysis; Spectroscopy; Synthetic aperture radar; Topology; Manifold alignments; MAPPER; Multi-modal data; Multi-Sensory; Multispectral images; Polarimetric synthetic aperture radars; Topological data analysis; data assimilation; land cover; multispecies fishery; optical property; polarization; radar imagery; sensor; supervised classification; synthetic aperture radar; topology; Image fusion","Hyperspectral image; MAPPER; multi-modal data fusion; multi-sensory data fusion; multispectral image; polarimetric synthetic aperture radar (PolSAR); semi-supervised manifold alignment (SSMA); topological data analysis (TDA)","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85074472199"
"Schmitt M.; Hughes L.H.; Körner M.; Zhu X.X.","Schmitt, M. (7401931279); Hughes, L.H. (57201113391); Körner, M. (57190168095); Zhu, X.X. (55696622200)","7401931279; 57201113391; 57190168095; 55696622200","Colorizing sentinel-1 SAR images using a variational autoencoder conditioned on Sentinel-2 imagery","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2","","1045","1051","6","10.5194/isprs-archives-XLII-2-1045-2018","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048356423&doi=10.5194%2fisprs-archives-XLII-2-1045-2018&partnerID=40&md5=c3e1ac09b9c4f7deddfcb900fa6d7afd","In this paper, we have shown an approach for the automatic colorization of SAR backscatter images, which are usually provided in the form of single-channel gray-scale imagery. Using a deep generative model proposed for the purpose of photograph colorization and a Lab-space-based SAR-optical image fusion formulation, we are able to predict artificial color SAR images, which disclose much more information to the human interpreter than the original SAR data. Future work will aim at further adaption of the employed procedure to our special case of multi-sensor remote sensing imagery. Furthermore, we will investigate if the low-level representations learned intrinsically by the deep network can be used for SAR image interpretation in an end-to-end manner. © Authors 2018.","Data fusion; Geometrical optics; Image fusion; Remote sensing; Space optics; Synthetic aperture radar; Artificial color; Deep learnig; Generative model; Low level representation; Optical remote sensing; Remote sensing imagery; Sentinel-1; Sentinel-2; Radar imaging","Data fusion; Deep learnig; Optical remote sensing; Sentinel-1; Sentinel-2; Synthetic aperture radar (SAR)","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048356423"
"Li Y.; Fu Y.; Zhang W.","Li, Yuanyuan (57202363448); Fu, Yaowen (10046659100); Zhang, Wenpeng (56367727300)","57202363448; 10046659100; 56367727300","Distributed ISAR subimage fusion of nonuniform rotating target based on matching fourier transform","2018","Sensors (Switzerland)","18","6","1806","","","","10.3390/s18061806","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048060756&doi=10.3390%2fs18061806&partnerID=40&md5=37513084cbb046b01ab384814f0067a1","In real applications, the image quality of the conventional monostatic Inverse Synthetic Aperture Radar (ISAR) for the maneuvering target is subject to the strong fluctuation of Radar Cross Section (RCS), as the target aspect varies enormously. Meanwhile, the maneuvering target introduces nonuniform rotation after translation motion compensation which degrades the imaging performance of the conventional Fourier Transform (FT)-based method in the cross-range dimension. In this paper, a method which combines the distributed ISAR technique and the Matching Fourier Transform (MFT) is proposed to overcome these problems. Firstly, according to the characteristics of the distributed ISAR, the multiple channel echoes of the nonuniform rotation target from different observation angles can be acquired. Then, by applying the MFT to the echo of each channel, the defocused problem of nonuniform rotation target which is inevitable by using the FT-based imaging method can be avoided. Finally, after preprocessing, scaling and rotation of all subimages, the noncoherent fusion image containing all the RCS information in all channels can be obtained. The accumulation coefficients of all subimages are calculated adaptively according to the their image qualities. Simulation and experimental data are used to validate the effectiveness of the proposed approach, and fusion image with improved recognizability can be obtained. Therefore, by using the distributed ISAR technique and MFT, subimages of high-maneuvering target from different observation angles can be obtained. Meanwhile, by employing the adaptive subimage fusion method, the RCS fluctuation can be alleviated and more recognizable final image can be obtained. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Fourier transforms; Image enhancement; Image quality; Inverse problems; Inverse synthetic aperture radar; Motion compensation; Radar cross section; Radar imaging; Rotation; Cross-range dimension; Distributed ISAR; Imaging performance; Inverse synthetic aperture radars (ISAR); Maneuvering targets; Nonuniform rotation; Radar cross sections (RCS); Translation motion; article; Fourier transform; image analysis; image quality; rotation; simulation; telecommunication; Image fusion","Distributed ISAR; Image fusion; Maneuvering target; Matching fourier transform; Nonuniform rotation target","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048060756"
"Gibril M.B.A.; Bakar S.A.; Yao K.; Idrees M.O.; Pradhan B.","Gibril, Mohamed Barakat A. (57188810669); Bakar, Suzana A. (57190947751); Yao, Kouame (57188822513); Idrees, Mohammed Oludare (55961980200); Pradhan, Biswajeet (12753037900)","57188810669; 57190947751; 57188822513; 55961980200; 12753037900","Fusion of RADARSAT-2 and multispectral optical remote sensing data for LULC extraction in a tropical agricultural area","2017","Geocarto International","32","7","","735","748","13","10.1080/10106049.2016.1170893","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963615019&doi=10.1080%2f10106049.2016.1170893&partnerID=40&md5=3cdea7a355a8311ecd3a0792e0036ad2","In this study, we investigated the performance of different fusion and classification techniques for land cover mapping in Hilir Perak, Peninsula Malaysia using RADAR and Landsat-8 images in a predominantly agricultural area. The fusion methods used are Brovey Transform, Wavelet Transform, Ehlers and Layer Stacking and their results classified into seven different land cover classes which include (1) pixel-based classifiers (spectral angle mapper (SAM), maximum likelihood (ML), support vector machine (SVM)) and (2) Object-based (rule-based and standard nearest neighbour (NN)) classifiers. The result shows that pixel-based classification achieved maximum accuracy of the optical data classification using SVM in Landsat-8 with 74.96% accuracy compared to SAM and ML. For multisource data classification, the highest overall accuracy recorded for layer stacking (SVM) was 79.78%, Ehlers fusion (SVM) with 45.57%, Brovey fusion (SVM) with 63.70% and Wavelet fusion (SVM) 61.16%. And for object-based classifiers, the overall classification accuracy is 95.35% for rule-based and 76.33% for NN classifier, respectively. Based on the analysis of their performances, object-based and the rule-based classifiers produced the best classification accuracy from the fused images. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","Malaysia; Perak; West Malaysia; accuracy assessment; agricultural land; image classification; land cover; Landsat; mapping; multispectral image; RADARSAT; remote sensing; satellite sensor; synthetic aperture radar; transform","image fusion; LULC; multisource data; Optical sensors; remote sensing; SAR","Article","Final","","Scopus","2-s2.0-84963615019"
"Sreeja G.; Saraniya O.","Sreeja, G. (56872917600); Saraniya, O. (56294304100)","56872917600; 56294304100","A Comparative Study on Image Registration Techniques for SAR Images","2019","2019 5th International Conference on Advanced Computing and Communication Systems, ICACCS 2019","","","8728390","947","953","6","10.1109/ICACCS.2019.8728390","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067947315&doi=10.1109%2fICACCS.2019.8728390&partnerID=40&md5=8ac48cba717ac862b7611fb86780adcc","High resolution Synthetic Aperture Radar (SAR) images are extensively employed in many applications like object tracking, object detection, image fusion, image mosaicing. Image registration is mandatory process for all these applications. To register SAR images, feature based registration methods have been successfully deployed in recent years. State of art detectors like Harris Corner, SIFT, SURF, BRIEF, ORB, etc. have been applied to align SAR images. Among all feature detectors, SIFT and SURF algorithm proved to give better solutions for the SAR image registration problem due to its invariance and robustness. So in this paper, the attempt is made to give a detailed survey of SIFT and SURF based SAR image alignment. © 2019 IEEE.","Edge detection; Feature extraction; Image fusion; Image registration; Object detection; Synthetic aperture radar; Feature based registration; Feature descriptors; Feature matching; High resolution synthetic aperture radar images; Image registration techniques; Registration problems; SIFT; SURF; Radar imaging","Feature descriptors; Feature Matching.; Image registration; SIFT; SURF; Synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85067947315"
"Rajah P.; Odindi J.; Mutanga O.","Rajah, Perushan (56658660800); Odindi, John (36521256000); Mutanga, Onisimo (55912148400)","56658660800; 36521256000; 55912148400","Feature level image fusion of optical imagery and Synthetic Aperture Radar (SAR) for invasive alien plant species detection and mapping","2018","Remote Sensing Applications: Society and Environment","10","","","198","208","10","10.1016/j.rsase.2018.04.007","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045729361&doi=10.1016%2fj.rsase.2018.04.007&partnerID=40&md5=c624c25a5775540ad2b5ccb5ab7d1099","Invasive alien plant species are regarded as a major threat to among others socio-economic systems, global biodiversity and conservation initiatives. A reliable understanding of their spatial and temporal distribution is paramount for understanding their impact on co-existing landscapes and ecosystems. While traditional passive remote sensing methods have been successful in assessing invasion of such species, limiting factors such as cost, restricted coverage, image availability, terrain and inadequate resolutions hamper mapping and detection at large spatial extents. To date, the adoption of active remote sensing techniques as complimentary data to invasive alien plant mapping has been limited. In this study, we fuse two commonly used medium spatial and spectral resolution imagery (Sentinel-2 and Landsat 8) with active remote sensing data (Synthetic Aperture Rada imagery) in determining the optimal season for detecting and mapping the American Bramble (Rubus cuneifolius). Feature level image fusion was adopted to integrate passive and active remote sensing imagery and Support vector machine (SVM) supervised classification algorithm used to discriminate the American Bramble from surrounding native vegetation. Seasonal results showed that Sentinel-2 data, fused with SAR data generated the highest classification accuracy during summer (76%), while Landsat 8 imagery fused with SAR data performed best in winter (72%). These findings demonstrate that fusion of SAR with traditional optical imagery can be used to detect and map the American Bramble at a regional scale. We conclude that SAR data can be used synergistically with optical remote sensing to improve discrimination and mapping of the American Bramble. © 2018 Elsevier B.V.","","American Bramble, Multisensor image fusion; Invasive alien plant species; Remote sensing; Sentinel-2; Synthetic Aperture Radar (SAR)","Article","Final","","Scopus","2-s2.0-85045729361"
"DIng J.; Wu Y.; Zhang H.","DIng, Jing (57104168900); Wu, Yufeng (53986855900); Zhang, Hongbo (55685538900)","57104168900; 53986855900; 55685538900","A Fast BP Algorithm for Bistatic Spotlight SAR Based on Spectrum Compression","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048432","","","","10.1109/APSAR46974.2019.9048432","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083490620&doi=10.1109%2fAPSAR46974.2019.9048432&partnerID=40&md5=5f5dffd612405dd4d65b80a63adb022c","For bistatic spotlight synthetic aperture radar (SAR) with parallel trajectory, this paper investigates a fast back-projection (BP) imaging algorithm based on spectrum compression. The long synthetic aperture is split into several short subapertures (SAs), and each of them constructs a SA image with coarse azimuth resolution in a unified rectangular coordinate system by BP integral. Then, by applying the spectrum compression technique introduced in the paper, the azimuth spectrum of the SA image can be compressed greatly and, as such, the images can be coherently accumulated after azimuth up-sampling and spectrum recovery. Since there is no need for data interpolation in the image fusion, the proposed algorithm can achieve high precision as the original BP algorithm does but with higher computational efficiency. Simulation experiments are presented to validate the effectiveness of the method. © 2019 IEEE.","Computational efficiency; Image fusion; Radar imaging; Azimuth resolution; Back projection; Data interpolation; High-precision; Imaging algorithm; Rectangular coordinates; Spectrum compression; Spectrum recoveries; Synthetic aperture radar","bistatic spotlight SAR; fast BP algorithm; rectangular coordinate system; spectrum compression","Conference paper","Final","","Scopus","2-s2.0-85083490620"
"Kwak Y.; Yorozuya A.; Iwami Y.","Kwak, Youngjoo (54408878500); Yorozuya, Atsuhiro (36810870400); Iwami, Yoichi (56258814600)","54408878500; 36810870400; 56258814600","Disaster risk reduction using image fusion of optical and SAR data before and after tsunami","2016","IEEE Aerospace Conference Proceedings","2016-June","","7500520","","","","10.1109/AERO.2016.7500520","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978484357&doi=10.1109%2fAERO.2016.7500520&partnerID=40&md5=3a93c88b0b0eeefae2e2c9eb453a6535","This study applied supervised change detection to identify and estimate damaged urban surface conditions before and after a tsunami event in order to provide more accurate information for the implementation and enhancement of disaster risk reduction policies and strategies. Advanced remote sensing is crucial to support cost-effective emergency response activities for disaster risk assessment and management. This preliminary study, as an effort to propose a good case study in risk management, suggested that three main steps, i.e., filtering, fusing and classifying, should be adopted to perform change detection before and after a natural disaster. We fused very high-spatial-resolution multi-temporal optical images (0.6 m spatial resolution) and X-band SAR images (2.5 m spatial resolution). The study also revealed that the decision-level method, i.e. morphological transform, was the most promising in image fusion of filtered images to classify urban surfaces in tsunami damage assessment, compared with the pixel-level method, i.e. wavelet transform, and feature-level method, i.e. segmentation extraction. This paper reports, coupled with the results from the image fusion, that the preliminary results are good enough to obtain urban impervious surface estimation of a wide disaster risk area but not good enough to make clear amplitude images to identify individual buildings of dwelling zone. The proposed normalized change index (NCI), an important indicator for detecting changes, was found capable of providing better estimation of urban impervious surfaces, such as transport-related land (e.g., bridges and parking lots) and building roof tops in residential and industrial areas over a coastal zone in Rikuzen-takada City, devastated in the 2011 Great East Japan Earthquake. © 2016 IEEE.","Cost effectiveness; Damage detection; Data fusion; Disasters; Emergency services; Geometrical optics; Housing; Image resolution; Image segmentation; Radar imaging; Remote sensing; Risk assessment; Risk management; Risk perception; Signal detection; Synthetic aperture radar; Tsunamis; Urban transportation; Wavelet transforms; Disaster risk assessments; Disaster risk reductions; Emergency response; Great east japan earthquakes; Spatial resolution; Supervised change detection; Urban impervious surfaces; Very high spatial resolutions; Image fusion","","Conference paper","Final","","Scopus","2-s2.0-84978484357"
"Huang B.; Li Z.; Yang C.; Sun F.; Song Y.","Huang, Binghui (57216946822); Li, Zhi (57208551292); Yang, Chao (57195032153); Sun, Fuchun (57204699218); Song, Yixu (15124457200)","57216946822; 57208551292; 57195032153; 57204699218; 15124457200","Single satellite optical imagery dehazing using SAR image prior based on conditional generative adversarial networks","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093471","1795","1802","7","10.1109/WACV45572.2020.9093471","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085521974&doi=10.1109%2fWACV45572.2020.9093471&partnerID=40&md5=4ebab6919c32b079b180d91c1193b85f","Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models. © 2020 IEEE.","Computer vision; Demulsification; Image fusion; Remote sensing; Satellite imagery; Sensor data fusion; Space-based radar; Synthetic aperture radar; Adversarial networks; Atmospheric conditions; Auxiliary information; Cloudy conditions; Multi-sensor data; Multisensor data fusion; Remote sensing images; Satellite optical imagery; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85085521974"
"Jiang X.; He Y.; Li G.; Liu Y.; Zhang X.-P.","Jiang, Xiao (55599147400); He, You (57212448603); Li, Gang (55547117794); Liu, Yu (57386764900); Zhang, Xiao-Ping (35214025100)","55599147400; 57212448603; 55547117794; 57386764900; 35214025100","Building Damage Detection via Superpixel-Based Belief Fusion of Space-Borne SAR and Optical Images","2020","IEEE Sensors Journal","20","4","8878121","2008","2022","14","10.1109/JSEN.2019.2948582","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078761899&doi=10.1109%2fJSEN.2019.2948582&partnerID=40&md5=ffb2200846d8bdcd5c7ec1e346aac33f","Space-borne synthetic aperture radar (SAR) and optical sensors are important tools for building damage detection. Fusion of SAR and optical images improves detection performance. However, when the resolutions of the two different kinds of images differ, the performance of the existing pixel-level fusion methods deteriorates significantly due to interpolation-induced distortion. To solve this problem, this paper presents a new superpixel-based belief fusion (SBBF) model for building damage detection. The superpixels on the SAR and optical images are identified by the segmentation on the pre-earthquake optical image to perform the fusion on the superpixel-level instead of the pixel-level in existing methods. Then in the fusion stage, different from the commonly used direct fusion methods that do not consider the reliability in the fusion process, a novel belief fusion method that employs a basic belief assignment (BBA) to incorporate different reliabilities of superpixels is proposed to improve the accuracy of building damage detection. For each superpixel, the BBA is assigned based on the influence of noise and resolutions. The United Nations Operational Satellite Applications Programme (UNOSAT) datasets corresponding to the 2010 Haiti earthquake and the 2011 Tōhoku earthquake, are used to evaluate the performance of the proposed method. The experimental results show that the proposed method achieves significantly better performance than existing separate SAR or optical images based methods, and the existing pixel-level fusion methods. © 2001-2012 IEEE.","Buildings; Damage detection; Earthquakes; Fusion reactions; Geometrical optics; Image enhancement; Image fusion; Image segmentation; Remote sensing; Space-based radar; Superpixels; Synthetic aperture radar; Basic belief assignment; Building damage; Detection performance; Haiti earthquakes; Optical remote sensing; Pixel level fusion; Satellite applications; Space-borne SAR; Radar imaging","Building damage detection; fusion; optical remote sensing; superpixel; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85078761899"
"Zhang H.; Shen H.; Zhang L.","Zhang, Hai (57192694132); Shen, Huanfeng (8359721100); Zhang, Liangpei (8359720900)","57192694132; 8359721100; 8359720900","Fusion of multispectral and SAR images using sparse representation","2016","International Geoscience and Remote Sensing Symposium (IGARSS)","2016-November","","7730878","7200","7203","3","10.1109/IGARSS.2016.7730878","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007425783&doi=10.1109%2fIGARSS.2016.7730878&partnerID=40&md5=98f843fbf179aac5a5505536cba9e1a6","Complementary information from multi-sensor can be integrated to effectively solve many problems in remote sensing application. Synthetic Aperture Radar (SAR) imaging can be a feasible alternative to traditional optical remote sensing techniques because it is independent of solar illumination and weather conditions. This paper proposes a novel fusion framework combining IHS transform with sparse representation theory to fuse multispectral and SAR images. In addition, the simultaneous orthogonal matching pursuit (SOMP) technique is introduced to guarantee the efficiency. Experiments on various datasets have verified the effectiveness of proposed method. © 2016 IEEE.","","Image fusion; simultaneous orthogonal matching pursuit; sparse representation; Synthetic Aperture Radar","Conference paper","Final","","Scopus","2-s2.0-85007425783"
"Sukawattanavijit C.; Chen J.; Zhang H.","Sukawattanavijit, Chanika (55349184000); Chen, Jie (55909160300); Zhang, Hongsheng (55349777400)","55349184000; 55909160300; 55349777400","GA-SVM Algorithm for Improving Land-Cover Classification Using SAR and Optical Remote Sensing Data","2017","IEEE Geoscience and Remote Sensing Letters","14","3","7831422","284","288","4","10.1109/LGRS.2016.2628406","126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010703205&doi=10.1109%2fLGRS.2016.2628406&partnerID=40&md5=cb192f9c8bb47322fbcfe9eb03b22c49","Multisource remote sensing data have been widely used to improve land-cover classifications. The combination of synthetic aperture radar (SAR) and optical imagery can detect different land-cover types, and the use of genetic algorithms (GAs) and support vector machines (SVMs) can lead to improved classifications. Moreover, SVM kernel parameters and feature selection affect the classification accuracy. Thus, a GA was implemented for feature selection and parameter optimization. In this letter, a GA-SVM algorithm was proposed as a method of classifying multifrequency RADARSAT-2 (RS2) SAR images and Thaichote (THEOS) multispectral images. The results of the GA-SVM algorithm were compared with those of the grid search algorithm, a traditional method of parameter searching. The results showed that the GA-SVM algorithm outperformed the grid search approach and provided higher classification accuracy using fewer input features. The images obtained by fusing RS2 data and THEOS data provided high classification accuracy at over 95%. The results showed improved classification accuracy and demonstrated the advantages of using the GA-SVM algorithm, which provided the best accuracy using fewer features. © 2004-2012 IEEE.","Feature extraction; Genetic algorithms; Optimization; Radar imaging; Remote sensing; Support vector machines; Synthetic aperture radar; Classification accuracy; Genetic algorithm (GAs); Grid-search algorithm; Land cover classification; Multisource remote sensing data; Optical remote sensing data; Parameter optimization; Support vector machine (SVMs); Classification (of information)","Genetic algorithms (GAs); image fusion; land-cover classification; multisource data; optical imagery; support vector machine (SVM); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85010703205"
"Lv W.; Guo L.; Xu W.; Yang X.; Wu L.","Lv, Wentao (36739475300); Guo, Lipeng (57202805382); Xu, Weiqiang (8647083900); Yang, Xiaocheng (55971485400); Wu, Long (55714357200)","36739475300; 57202805382; 8647083900; 55971485400; 55714357200","Vehicle detection in synthetic aperture radar images with feature fusion-based sparse representation","2018","Journal of Applied Remote Sensing","12","2","025020","","","","10.1117/1.JRS.12.025020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049386726&doi=10.1117%2f1.JRS.12.025020&partnerID=40&md5=9318b3dac903d393f2f503745e9afdfc","A vehicle detection algorithm is presented for synthetic aperture radar (SAR) images. This method formulates the detection mission within a sparse representation (SR) fusion frame. A set of residuals, for one specific feature, is first generated by performing the sparse reconstructions over dictionaries associated with the available set of possible targets. They are then normalized and further formed into a single residual sequence. After the collection of all residual sequences for all types of features, a linear fusion strategy is applied to the sequences to infer an optimal target estimate. As the final decision is made based on the residual fusion related with the concatenation of multiple features, this algorithm exhibits strong discriminative powers with respect to target confirmation. Moreover, a merging technique is developed to integrate a more accurate region for each vehicle. The test results based on real scene data show that the presented method is superior to some state-of-the-art alternatives. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Image fusion; Merging; Radar imaging; Tracking radar; Vehicles; Merging techniques; Multi-feature fusion; Multiple features; Region-merging; Sparse reconstruction; Sparse representation; Synthetic aperture radar (SAR) images; Vehicle detection; Synthetic aperture radar","multifeature fusion; region merging; sparse representation; synthetic aperture radar image; vehicle detection","Article","Final","","Scopus","2-s2.0-85049386726"
"Deng W.; Zhang X.; Luo J.; Peng Y.","Deng, Wanqian (57205752422); Zhang, Xin (57077122400); Luo, Jiancheng (7404183561); Peng, Yu (57216946290)","57205752422; 57077122400; 7404183561; 57216946290","South China Sea Environment Monitoring Using Remote Sensing Techniques","2020","Journal of Coastal Research","95","sp1","","29","33","4","10.2112/SI95-006.1","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085526073&doi=10.2112%2fSI95-006.1&partnerID=40&md5=72f72e80f4727f3da63d7afef2c8b54c","Deng, W.Q.; Zhang, X.; Luo, J.C., and Peng, Y., 2020. South China Sea environment monitoring using remote sensing techniques. In: Malvárez, G. and Navas, F. (eds.), Global Coastal Issues of 2020. Journal of Coastal Research, Special Issue No. 95, pp. 29-33. Coconut Creek (Florida), ISSN 0749-0208. As the South China Sea has a wide spatial coverage, remote sensing technology with wide monitoring scope and strong timeliness is increasingly becoming an important means to monitor this part of the ocean. The current monitoring trend is to combine different data sources, including common synthetic aperture radar (SAR), optical image and thermal image by image fusion method, and monitor the marine environment jointly by multiple methods, but there still exist many problems. Using a systematic means of meta-analysis, this paper undertook a literature review about the South China Sea Environment Monitoring with Remote Sensing Techniques, mainly analysing related articles in recent ten years. The current researches show several limitations, thus putting forward a comprehensive multi-angle remote sensing monitoring method is important for ecological and environmental protection in South China Sea. © Coastal Education and Research Foundation, Inc. 2020.","Pacific Ocean; South China Sea; environmental monitoring; image analysis; literature review; marine environment; meta-analysis; optical property; radar altimetry; remote sensing; synthetic aperture radar","environment monitoring; remote sensing; South China Sea","Article","Final","","Scopus","2-s2.0-85085526073"
"Kumar V.; Agrawal P.; Agrawal S.","Kumar, Vinay (57212901158); Agrawal, Prince (57202596865); Agrawal, Shefali (57210985203)","57212901158; 57202596865; 57210985203","ALOS PALSAR and Hyperion Data Fusion for Land Use Land Cover Feature Extraction","2017","Journal of the Indian Society of Remote Sensing","45","3","","407","416","9","10.1007/s12524-016-0605-2","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978115951&doi=10.1007%2fs12524-016-0605-2&partnerID=40&md5=5bb55cba5d743e89a31fa4fcc4c5abeb","Detailed and enhanced land use land cover (LULC) feature extraction is possible by merging the information extracted from two different sensors of different capability. In this study different pixel level image fusion algorithms (PCA, Brovey, Multiplicative, Wavelet and combination of PCA & IHS) are used for integrating the derived information like texture, roughness, polarization from microwave data and high spectral information from hyperspectral data. Span image which is total intensity image generated from Advanced Land observing Satellite-Phase array L-band SAR (ALOS-PALSAR) quad polarization data and EO-1 Hyperion data (242 spectral bands) were used for fusion. Overall PCA fused images had shown better result than other fusion techniques used in this study. However, Brovey fusion method was found good for differentiating urban features. Classification using support vector machines was conducted for classifying Hyperion, ALOS PALSAR and fused images. It was observed that overall classification accuracy and kappa coefficient with PCA fused images was relatively better than other fusion techniques as it was able to discriminate various LULC features more clearly. © 2016, Indian Society of Remote Sensing.","accuracy assessment; algorithm; ALOS; Hyperion; image analysis; image classification; land cover; land use; PALSAR; pixel; polarization; principal component analysis; synthetic aperture radar; texture","Brovey; Hyperspectral; Image fusion; Multiplicative & SVM; PCA; SAR","Article","Final","","Scopus","2-s2.0-84978115951"
"Karimi D.; Akbarizadeh G.; Rangzan K.; Kabolizadeh M.","Karimi, Danya (57193122143); Akbarizadeh, Gholamreza (36027930900); Rangzan, Kazem (7801505713); Kabolizadeh, Mostafa (36080758400)","57193122143; 36027930900; 7801505713; 36080758400","Effective supervised multiple-feature learning for fused radar and optical data classification","2017","IET Radar, Sonar and Navigation","11","5","","768","777","9","10.1049/iet-rsn.2016.0346","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018257055&doi=10.1049%2fiet-rsn.2016.0346&partnerID=40&md5=67e6be123b4f46f1c5909d2cbf0a1749","In multi-sensor data fusion based on multiple features, the high dimensionality of feature space increases the runtime and computational complexity. The present study proposes a new algorithm based on the combination of random subspace (RS), linear discriminant analysis and sparse regularisation (LDASR), namely RS-LDASR for feature space dimensionality reduction, supervised feature selection, and learning. The use of RSs can effectively solve the problem of high dimensionality and high feature-to-instance ratio. The extraction of multiple features from the images raises the possibility of a correlation between features which reduce classification accuracy. In this study, after the construction of several RSs, supervised feature selection and learning based on LDASR were applied with very high accuracy. Classification and image fusion for remote sensing data analysis were tested by the implementation of feature-based fusion on two pairs of fused synthetic aperture radar and optical data. Four feature matrices were constructed using attribute profiles (APs), multi-APs (MAPs), non-negative matrix factorisation (NMF), and textural features. Support vector machine and rotation forest were applied as the base classifiers. The results show that use of RS-LDASR significantly improved the classification accuracy based on NMF plus texture features and even NMF alone. © 2016, The Institution of Engineering and Technology.","Data fusion; Discriminant analysis; Factorization; Feature extraction; Image fusion; Image retrieval; Matrix algebra; Radar; Radar imaging; Remote sensing; Sensor data fusion; Space-based radar; Synthetic aperture radar; Classification accuracy; Correlation between features; Dimensionality reduction; High dimensionality; Linear discriminant analysis; Multisensor data fusion; Non-negative matrix factorisation; Remote sensing data; Classification (of information)","","Article","Final","","Scopus","2-s2.0-85018257055"
"Singh S.; Guha A.; Seshadri K.; Kumar K.V.","Singh, Swati (56565771200); Guha, Arindam (24474497600); Seshadri, K. (7007181874); Kumar, K. Vinod (57208755899)","56565771200; 24474497600; 7007181874; 57208755899","Earth observation-based approach for delineating geomorphology-guided geoenvironmental zones and its utility in regional planning: an analysis in parts of Bengal Basin, West Bengal, India","2017","Environmental Earth Sciences","76","3","109","","","","10.1007/s12665-016-6323-9","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010950538&doi=10.1007%2fs12665-016-6323-9&partnerID=40&md5=7ef6d071da8ac748a85e3e5a8a357a0a","In this paper, we have analysed active and passive earth observation data for identifying the different geomorphic sub-provinces associated with coastal and fluvial geomorphic processes in the south-western part of Bengal Basin. For this purpose, variations in the spectral response of these sub-provinces in IRS P6 LISS III data are enhanced using principal component (PC) and independent component (IC) methods to spatially delineate the geomorphic sub-provinces and associated Quaternary sediments. It has been observed that the false colour composite derived using IC (first, second and third ICs) are effective in enhancing fluvial sub-provinces, while IC band 4, band 3 and band 2 are suitable to delineate coastal sub-provinces. On the other hand, PC composite derived using band 4, band 3 and band 1 are suitable to delineate different Quaternary sediments deposited within the spatial extent of geomorphic sub-provinces. Fused image enhanced products of IRS (Indian Remote Sensing Satellite) LISS III band and ALOS (Advanced Land Observing Satellite) PALSAR (Phased Array L-band Synthetic Aperture Radar) polarisation bands are used to supplement and complement the variations observed for each geomorphic province in optical data as radar interactions are sensitive to variation in moisture and surface cover. Although there is a strong synergy between the spatial extent of geomorphic sub-provinces and the Quaternary units in terms of spatial disposition, different Quaternary units are also found within the same geomorphic province. Therefore, we have segmented each geomorphic sub-provinces based on the spatial extent of the different Quaternary deposits to identify few geoenvironmental (termed here as geomorpho-environmental) zones. In this regard, eleven zones are identified. Each zone has been characterised based on geomorphic stability, potentiality for agriculture practice, scope for urban development and reclamation. These geoenvironmental zones may provide important input to planners. © 2017, Springer-Verlag Berlin Heidelberg.","Bengal; India; West Bengal; Image fusion; Integrated circuits; Observatories; Planning; Principal component analysis; Radar; Regional planning; Remote sensing; Synthetic aperture radar; Urban growth; ALOS PALSAR; Geoenvironmental; Geomorphic provinces; Independent components; LISS-III; Principal Components; ALOS; coastal morphology; composite; deposition; EOS; fluvial geomorphology; image processing; land reclamation; LISS; PALSAR; principal component analysis; Quaternary; regional planning; Resourcesat; satellite data; urban development; Space-based radar","ALOS PALSAR; Geoenvironmental zones; Geomorphic provinces; Image fusion; Independent components; IRS P6 LISS III; Principal components","Article","Final","","Scopus","2-s2.0-85010950538"
"Xiao X.; Xie J.; Niu J.; Cao W.","Xiao, Xiaohong (36440769200); Xie, Jiangang (57216870296); Niu, Jiping (26029464500); Cao, Wei (57216865823)","36440769200; 57216870296; 26029464500; 57216865823","A novel image fusion method for water body extraction based on optimal band combination","2020","Traitement du Signal","37","2","","195","207","12","10.18280/ts.370205","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084975948&doi=10.18280%2fts.370205&partnerID=40&md5=761659dce609f3be89e00f60ccc84ec4","This paper attempts to design an image fusion method that facilitates the extraction of water bodies from remote sensing images, namely, the images taken by Enhanced Thematic Mapper Plus (ETM+) of Landsat 7 and those taken by Phased Array type L-band Synthetic Aperture Radar (PALSAR) of Advanced Land Observation Satellite (ALOS). Firstly, the water body information was extracted from ETM+ data and PALSAR data, and combined into a benchmark image. Next, several traditional image fusion methods were separately adopted to merge the ETM+453 image and the PALSAR HH image, and the water bodies extracted from the fused images were compared in details. The selected methods include principal component analysis (PCA), Brovey transform (BT), intensity-hue-saturation (IHS) transform, discrete wavelet transform (DWT), and high-pass filter (HPF). After that, a new image fusion method was designed based on optimal band combination (OBC), and the water bodies extracted by the method were compared with the benchmark image and those extracted by the traditional methods. The results show that the ALOS HH image alone achieved higher accuracy in water body extraction than the ETM+ image alone; the traditional image fusion methods, namely, PCA, BT, IHS, HPF and DWT, were more accurate than the ETM+ image alone in water body extraction, and less accurate than the ALOS HH image alone. The OBC-based image fusion method greatly outperformed all the traditional methods. The research results provide a good reference for image fusion and extraction tasks in similar cases. © 2020 Lavoisier. All rights reserved.","Discrete wavelet transforms; Extraction; High pass filters; Image enhancement; Principal component analysis; Remote sensing; Signal reconstruction; Space-based radar; Synthetic aperture radar; Band combinations; Enhanced thematic mapper plus (ETM+); Extraction of waters; Image fusion methods; Intensity hue saturations; Observation satellites; Phased array type l-band synthetic aperture radars; Remote sensing images; Image fusion","Enhanced Thematic Mapper Plus (ETM+); Optimal band combination (OBC); Phased Array type L-band Synthetic Aperture Radar (PALSAR); water body extraction","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85084975948"
"Tanaka T.; Ikefuji D.; Hoshuyama O.","Tanaka, Taichi (57200605210); Ikefuji, Daisuke (55177060300); Hoshuyama, Osamu (6602870397)","57200605210; 55177060300; 6602870397","Multitemporal Sar and Map Fusion for Extracting Persitent Scatterers on Roads","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899061","182","185","3","10.1109/IGARSS.2019.8899061","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077722256&doi=10.1109%2fIGARSS.2019.8899061&partnerID=40&md5=4ea80aac4a7ece7d157233a1e3027f5d","This paper proposes a fusion method of multitemporal SAR images and a map to extract persistent scatterers (PSs) on roads, which are special SAR pixels used to monitor millimetric displacement of roads. The proposed method evaluates phase correlation of PSs in addition to shape similarity between their distribution in the SAR image and corresponding roads in the map. since uncorrelated phase implies that PSs are on different objects, which have different heights and displacements, the proposed method can extract PSs matching to the road shape while excluding those on neighboring buildings even under severe distortion caused by layover. Evaluation results using real SAR images and map show that the proposed fusion method can find PSs corresponding to roads in an urban area highly affected by layover. © 2019 IEEE.","Geology; Image fusion; Phosphorus; Remote sensing; Roads and streets; Synthetic aperture radar; Evaluation results; InSAR; Multi-temporal SAR; Multi-temporal SAR images; Neighboring building; Persistent scatterers; Phase correlation; Road extraction; Radar imaging","InSAR; PS; Road Extraction","Conference paper","Final","","Scopus","2-s2.0-85077722256"
"Seo D.K.; Kim Y.H.; Eo Y.D.; Lee M.H.; Park W.Y.","Seo, Dae Kyo (57195943556); Kim, Yong Hyun (56195702200); Eo, Yang Dam (7004110402); Lee, Mi Hee (57198252917); Park, Wan Yong (55634630800)","57195943556; 56195702200; 7004110402; 57198252917; 55634630800","Fusion of SAR and multispectral images using random forest regression for change detection","2018","ISPRS International Journal of Geo-Information","7","10","401","","","","10.3390/ijgi7100401","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056652880&doi=10.3390%2fijgi7100401&partnerID=40&md5=27f494bb6bface09d9880bfd4356dcf8","In order to overcome the insufficiency of single remote sensing data in change detection, synthetic aperture radar (SAR) and optical image data can be used together for supplementation. However, conventional image fusion methods fail to address the differences in imaging mechanisms and cannot overcome some practical limitations such as usage in change detection or temporal requirement of the optical image. This study proposes a new method to fuse SAR and optical images, which is expected to be visually helpful and minimize the differences between two imaging mechanisms. The algorithm performs the fusion by establishing relationships between SAR and multispectral (MS) images by using a random forest (RF) regression, which creates a fused SAR image containing the surface roughness characteristics of the SAR image and the spectral characteristics of the MS image. The fused SAR image is evaluated by comparing it to those obtained using conventional image fusion methods and the proposed method shows that the spectral qualities and spatial qualities are improved significantly. Furthermore, for verification, other ensemble approaches such as stochastic gradient boosting regression and adaptive boosting regression are compared and overall it is confirmed that the performance of RF regression is superior. Then, change detection between the fused SAR and MS images is performed and compared with the results of change detection between MS images and between SAR images and the result using fused SAR images is similar to the result with MS images and is improved when compared to the result between SAR images. Lastly, the proposed method is confirmed to be applicable to change detection. © 2018 by the authors.","","Change detection; Image fusion; Multispectral; Random forest regression; SAR","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056652880"
"Chu T.; Tan Y.; Liu Q.; Bai B.","Chu, Tianyong (57215069110); Tan, Yumin (14064055300); Liu, Qiang (57214780231); Bai, Bingxin (57205687503)","57215069110; 14064055300; 57214780231; 57205687503","Novel fusion method for SAR and optical images based on non-subsampled shearlet transform","2020","International Journal of Remote Sensing","41","12","","4588","4602","14","10.1080/01431161.2020.1723175","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079817653&doi=10.1080%2f01431161.2020.1723175&partnerID=40&md5=f81ac7ffedee39ad9eb6d019db76f1b3","Due to the different imaging modes of SAR images and optical images, traditional image fusion methods are no longer suitable for the fusion of the two types of images. Fused images often have problems of spectral distortion and excessive introduction of noise. This study proposes an improved SAR-optical images fusion algorithm based on non-subsampled shearlet transform (NSST). NSST decomposition is first performed on the two types of images. Then, in the low-frequency sub-band of the decomposition image, a weighted average fusion rule using the coefficient of variation according to the different imaging characteristics of SAR images and optical images is proposed to avoid spectral distortion. In the high-frequency sub-band of the decomposition image, the effect of SAR image noise on the fused image is removed by setting the coherence coefficient threshold. The subjective visual assessment and objective index evaluation on the experimental results both show that the fusion results using the proposed algorithm are significantly improved. The proposed algorithm smoothly fuses the detailed information of the SAR image into the optical image without the excessive introduction of noise while maintaining the spectral information of the optical image. Meanwhile, the proposed algorithm has a relatively simpler mathematical structure compared to the algorithm based on sparse representation, thus reducing the operating time and manual work. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Geometrical optics; Image enhancement; Image fusion; Synthetic aperture radar; Coefficient of variation; Image fusion methods; Imaging characteristics; Mathematical structure; Shearlet transforms; Sparse representation; Spectral distortions; Spectral information; algorithm; decomposition analysis; experimental study; image analysis; optical property; remote sensing; synthetic aperture radar; Radar imaging","","Article","Final","","Scopus","2-s2.0-85079817653"
"Ji X.; Zhang G.","Ji, Xiuxia (55900734800); Zhang, Gong (35241577600)","55900734800; 35241577600","Image fusion method of SAR and infrared image based on Curvelet transform with adaptive weighting","2017","Multimedia Tools and Applications","76","17","","17633","17649","16","10.1007/s11042-015-2879-8","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939486255&doi=10.1007%2fs11042-015-2879-8&partnerID=40&md5=5548cf5ead78f0b6542b51a479cade06","This paper analyses the characteristics of infrared detection imaging, with the specific application of multi-scale analysis theory for SAR and infrared image fusion. After learning and discussing the research results in image fusion area at home and abroad, it proposes an adaptive weighted image fusion method which combines the idea of fuzzy theory based on Curvelet transform, i.e., defines the membership function with fuzzy logic variables, makes different weights to transform coefficients of different levels, and designs a kind of adaptive weighted image fusion strategy. Experimental results validate the reliability and credibility of this method in term of visual quality and objective evaluation, and it can effectively improve the fusion quality. © 2015, Springer Science+Business Media New York.","Fuzzy logic; Infrared imaging; Membership functions; Quality control; Radar imaging; Synthetic aperture radar; Adaptive weighting; Curvelet transforms; Image fusion methods; Infrared detection; Multi scale analysis; Objective evaluation; SAR Images; Transform coefficients; Image fusion","Adaptive weighting; Curvelet transform; Image fusion; Infrared image; SAR image","Article","Final","","Scopus","2-s2.0-84939486255"
"Kulkarni S.C.; Rege P.P.","Kulkarni, Samadhan C. (57204893706); Rege, Priti P. (6701858789)","57204893706; 6701858789","Fusion of RISAT-1 SAR Image and Resourcesat-2 Multispectral Images Using Wavelet Transform","2019","2019 6th International Conference on Signal Processing and Integrated Networks, SPIN 2019","","","8711589","45","52","7","10.1109/SPIN.2019.8711589","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066897202&doi=10.1109%2fSPIN.2019.8711589&partnerID=40&md5=7825390435348ef36f892396b737c46c","This paper presents a pixel level wavelet-based approach to fuse synthetic aperture radar (SAR) imagery with multispectral (MS) imagery. Image fusion combines information from two or more images to generate a new image, which is rich in information. Due to complementary nature of SAR and multispectral imagery, fusion of these images is of significant interest in the field of remote sensing. The primary objective of this work is to enhance spatial information in multispectral images by injecting structural information derived from SAR image. Due to negative correlation between SAR and multispectral data, conventional component substitution methods face the problem of spectral distortion in the fused image. Wavelet based fusion approaches overcome this problem due to excellent localization in spatial and frequency domain. Here, different wavelet-based fusion rules are applied for fusion of SAR and multispectral images. Fusion rules applied to fuse approximate sub-bands and detail sub-bands of these images consider spectral dis-similarity between them. Results are evaluated visually, as well as using standard quality metrics and are compared with component substitution fusion techniques namely, principal component analysis and generalized IHS transform. Trade-off between spectral and spatial quality of fused image has been observed while fusing SAR and multispectral images. © 2019 IEEE.","Economic and social effects; Frequency domain analysis; Image compression; Image enhancement; Image fusion; Principal component analysis; Quality control; Remote sensing; Synthetic aperture radar; Wavelet transforms; Component substitution; Conventional components; Multi-spectral imagery; SAR imagery; Spatial and frequency domain; Structural information; Synthetic Aperture Radar Imagery; Wavelet-based fusion approach; Radar imaging","Image Fusion; Multispectral Imagery; Remote Sensing; SAR Imagery; Wavelet Transform","Conference paper","Final","","Scopus","2-s2.0-85066897202"
"Blasch E.; Majumder U.; Zelnio E.; Velten V.","Blasch, Erik (7003503895); Majumder, Uttam (55904765100); Zelnio, Edmund (6602490544); Velten, Vincent (6602661576)","7003503895; 55904765100; 6602490544; 6602661576","Review of recent advances in AI/ML using the MSTAR data","2020","Proceedings of SPIE - The International Society for Optical Engineering","11393","","113930C","","","","10.1117/12.2559035","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086708375&doi=10.1117%2f12.2559035&partnerID=40&md5=6cdcbdd569615881240eb9f4c839decb","Over the past decades, there have been many approaches to synthetic aperture radar (SAR) automatic target recognition (ATR). ATR includes detection, classification, and identification of targets, scene, and context. Recently, the explosion of methods for deep learning has attracted numerous researchers to compare machine learning methods for SAR ATR. This paper reviews many approaches conducted for SAR recognition and discerns the most promising approaches. Using the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, there are comparative methods to evaluate the advances from the community. The paper reviews many of the available techniques recently published to determine the state of the art in emerging concepts. © 2020 SPIE.","Automatic target recognition; Deep learning; Learning systems; Radar imaging; Synthetic aperture radar; Tracking radar; Comparative methods; Data set; Machine learning methods; SAR ATR; State of the art; Stationary targets; Radar target recognition","Artificial Intelligence (AI); Deep Learning (DL); Image Fusion; Machine Learning (ML); Moving and Stationary Target Acquisition Recognition (MSTAR); Synthetic and Measured Paired and Labeled Experiment (SAMPLE); Synthetic Aperture Radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85086708375"
"Ma W.; Xiong Y.; Wu Y.; Yang H.; Zhang X.; Jiao L.","Ma, Wenping (57205878746); Xiong, Yunta (57204813607); Wu, Yue (56215531900); Yang, Hui (57204815281); Zhang, Xiangrong (55802358000); Jiao, Licheng (7102491544)","57205878746; 57204813607; 56215531900; 57204815281; 55802358000; 7102491544","Change detection in remote sensing images based on image mapping and a deep capsule network","2019","Remote Sensing","11","6","626","","","","10.3390/RS11060626","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067378791&doi=10.3390%2fRS11060626&partnerID=40&md5=5313354d72f18d88cc7157864712b90d","Homogeneous image change detection research has been well developed, and many methods have been proposed. However, change detection between heterogeneous images is challenging since heterogeneous images are in different domains. Therefore, direct heterogeneous image comparison in the way that we do it is difficult. In this paper, a method for heterogeneous synthetic aperture radar (SAR) image and optical image change detection is proposed, which is based on a pixel-level mapping method and a capsule network with a deep structure. The mapping method proposed transforms an image from one feature space to another feature space. Then, the images can be compared directly in a similarly transformed space. In the mapping process, some image blocks in unchanged areas are selected, and these blocks are only a small part of the image. Then, the weighted parameters are acquired by calculating the Euclidean distances between the pixel to be transformed and the pixels in these blocks. The Euclidean distance calculated according to the weighted coordinates is taken as the pixel gray value in another feature space. The other image is transformed in a similar manner. In the transformed feature space, these images are compared, and the fusion of the two different images is achieved. The two experimental images are input to a capsule network, which has a deep structure. The image fusion result is taken as the training labels. The training samples are selected according to the ratio of the center pixel label and its neighboring pixels’ labels. The capsule network can improve the detection result and suppress noise. Experiments on remote sensing datasets show the final detection results, and the proposed method obtains a satisfactory performance. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Image fusion; Mapping; Pixels; Radar imaging; Remote sensing; Space-based radar; Synthetic aperture radar; Tracking radar; Capsule network; Change detection; Deep structure; Feature space; Heterogeneous image; Image change detection; Image mapping; Mapping method; Optical image; Synthetic aperture radar images; Geometrical optics","Capsule network; Change detection; Heterogeneous image; Image mapping; Optical image; Synthetic aperture radar image","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85067378791"
"Wang X.; Feng J.; Cao Z.; Min R.","Wang, Xianyuan (57194651633); Feng, Jilan (36668529900); Cao, Zongjie (55271466500); Min, Rui (24484819100)","57194651633; 36668529900; 55271466500; 24484819100","Polarimetric-spatial classification of PolSAR images based on composite kernel feature fusion","2017","2017 IEEE Radar Conference, RadarConf 2017","","","7944436","1455","1459","4","10.1109/RADAR.2017.7944436","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021424023&doi=10.1109%2fRADAR.2017.7944436&partnerID=40&md5=2603029a609ebf69b33d9d18a98165ce","Composite kernel feature fusion is proposed in this paper for solving the classification of polarimetric synthetic aperture radar (PolSAR) images problem. The main idea is that the method of composite kernel encodes diverse information within a new kernel matrix and tunes the contribution of different type of features. The proposed approach is tested on Flevoland PolSAR data set. Experimental results verify the benefits of using both of polarimetric and spatial information by composite kernel feature fusion for the classification of PolSAR images. © 2017 IEEE.","Image classification; Image fusion; Polarimeters; Radar; Synthetic aperture radar; Composite kernels; Data set; Feature fusion; Kernel matrices; Polarimetric synthetic aperture radars; PolSAR; Spatial classification; Spatial informations; Classification (of information)","Composite Kernel; Feature Fusion; Image Classification; PolSAR","Conference paper","Final","","Scopus","2-s2.0-85021424023"
"Hu J.; Hong D.; Wang Y.; Zhu X.X.","Hu, Jingliang (57192207722); Hong, Danfeng (56108179600); Wang, Yuanyuan (38663687700); Zhu, Xiao Xiang (55696622200)","57192207722; 56108179600; 38663687700; 55696622200","A comparative review of manifold learning techniques for hyperspectral and polarimetric sar image fusion","2019","Remote Sensing","11","6","681","1","28","27","10.3390/rs11060681","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074492590&doi=10.3390%2frs11060681&partnerID=40&md5=05c4a56ab7767a407008fa7a4dda88c7","In remote sensing, hyperspectral and polarimetric synthetic aperture radar (PolSAR) images are the two most versatile data sources for a wide range of applications such as land use land cover classification. However, the fusion of these two data sources receive less attention than many other, because of their scarce data availability, and relatively challenging fusion task caused by their distinct imaging geometries. Among the existing fusion methods, including manifold learning-based, kernel-based, ensemble-based, and matrix factorization, manifold learning is one of most celebrated techniques for the fusion of heterogeneous data. Therefore, this paper aims to promote the research in hyperspectral and PolSAR data fusion, by providing a comprehensive comparison between existing manifold learning-based fusion algorithms. We conducted experiments on 16 state-of-the-art manifold learning algorithms that embrace two important research questions in manifold learning-based fusion of hyperspectral and PolSAR data: (1) in which domain should the data be aligned—the data domain or the manifold domain; and (2) how to make use of existing labeled data when formulating a graph to represent a manifold—supervised, semi-supervised, or unsupervised. The performance of the algorithms were evaluated via multiple accuracy metrics of land use land cover classification over two data sets. Results show that the algorithms based on manifold alignment generally outperform those based on data alignment (data concatenation). Semi-supervised manifold alignment fusion algorithms performs the best among all. Experiments using multiple classifiers show that they outperform the benchmark data alignment-based algorithms by ca. 3% in terms of the overall classification accuracy. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Alignment; Classification (of information); Factorization; Graph algorithms; Image fusion; Land use; Learning systems; Polarimeters; Radar imaging; Remote sensing; Semi-supervised learning; Synthetic aperture radar; Classification accuracy; Comprehensive comparisons; Fusion of heterogeneous data; Land use/ land covers; Manifold learning algorithm; Matrix factorizations; Multiple classifiers; Polarimetric synthetic aperture radars; Learning algorithms","Data alignment; Data fusion; Generalized graph fusion; Hyperspectral image; Locality preserving projections; Manifold alignment; Manifold alignment; Manifold learning; MAPPER-induced manifold alignment; MIMA; Polarimetric SAR","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85074492590"
"Chang W.; Tao H.; Sun G.; Wang Y.; Bao Z.","Chang, Wensheng (57104137600); Tao, Haihong (7202328081); Sun, Guangcai (26424042000); Wang, Yuqi (57208430553); Bao, Zheng (7202908312)","57104137600; 7202328081; 26424042000; 57208430553; 7202908312","A novel multi-angle SAR imaging system and method based on an ultrahigh speed platform","2019","Sensors (Switzerland)","19","7","1701","","","","10.3390/s19071701","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064773055&doi=10.3390%2fs19071701&partnerID=40&md5=2d746a6101e16ce7320399cdcfe48df0","Considering the difficulty of pulse repetition frequency (PRF) design in multi-angle SAR when using ultra-high speed platforms, a multi-angle SAR imaging system in a unified coordinate system is proposed. The digital multi-beamforming is used in the system and multi-angle SAR data can be obtained in one flight. Therefore, the system improves the efficiency of data recording. An improved range migration algorithm (RMA) is used for data processing, and imaging is made in a unified imaging coordinate system. The resolution of different view images is the same, and there is a fixed delay between the images. On this basis, the SAR image fusion is performed after image matching. The results of simulation and measured data confirm the effectiveness of the system and the method. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Data handling; Image enhancement; Image fusion; Imaging systems; Synthetic aperture radar; Co-ordinate system; Improved RMA; Multi-Angle-SAR; Multi-Beamforming; Pulse repetition frequencies; Range migration algorithms; SAR imaging; Ultra high speed; Radar imaging","Improved RMA; Multi-angle SAR; SAR image fusion; SAR imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85064773055"
"Rout M.; Nahak S.; Priyadarshinee S.; Mohapatra P.; Sa K.D.; Dash D.","Rout, Mantnay (57215220289); Nahak, Siddheswar (57215223489); Priyadarshinee, Subhashree (57215221369); Mohapatra, Priyanka (57190384706); Sa, Kodanda Dhar (57202156824); Dash, DIllip (57201985404)","57215220289; 57215223489; 57215221369; 57190384706; 57202156824; 57201985404","A Deep Learning Approach for SAR Image Fusion","2019","2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies, ICICICT 2019","","","8993376","335","339","4","10.1109/ICICICT46008.2019.8993376","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080111246&doi=10.1109%2fICICICT46008.2019.8993376&partnerID=40&md5=d0fe97d1cc540cda2f2259e1058b3188","Image Fusion is an application of digital image processing. Image Fusion is a phenomenon of amalgamating the substantial features from similar pair of images into a single image, where the fused image will be of superior quality than either of the source images. This research work proposes a Pixel level Deep learning method using a 3-Channel convolutional neural network to fuse two multi focus Synthetic Aperture Radar (SAR) images obtain a high definition or high quality fused image. As the environment is in static condition and the radar is in floating, the radar imagery sensor captures source image at different time stamps. Size reduction of the image is performed before the fusion procedure in order to highly reduce the computational time and make the method more immune to noise. In the proposed method, source images are decomposed into pixels using the deep learning framework. After feature extraction, appropriate weights are assigned to all pixels. Then averaging and max pooling of pixel values of both the source images are done to get the resultant features of the fused image. Then smoothening filter is used to minimize noise in the fused image. Experimental results show that the proposed fusion technique demonstrates better PSNR value and less computational time. © 2019 IEEE.","Convolution; Convolutional neural networks; Deep learning; Deep neural networks; Extraction; Feature extraction; Image fusion; Intelligent computing; Learning systems; Pixels; Signal to noise ratio; Synthetic aperture radar; Tracking radar; Computational time; Fusion techniques; Learning approach; Learning frameworks; Learning methods; Size reductions; Static conditions; Synthetic aperture radar (SAR) images; Radar imaging","Convolutional Neural Networks (CNNs); Deep Learning; Feature extraction; Image fusion; Signal to Noise Ratio (SNR); Synthetic Aperture Radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85080111246"
"Lück W.; Morin G.","Lück, Wolfgang (57196831038); Morin, Guillaume (57203371490)","57196831038; 57203371490","Cloud based processing of free and commercial earth observation data with PCI GXL, populating and analysing data with the Australian Geoscience Data Cube software","2017","Proceedings of the International Astronautical Congress, IAC","7","","","4690","4694","4","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051431120&partnerID=40&md5=49b90a935d7c2f77de4a29ca2fa75293","Over recent years, corporate and public satellite operators have provided API’s to their image archives and distributed image processing has moved into the cloud. With this in place, the development of multi-sensor cloud enabled automated image pre-processing and analysis tools, feeding data cubes such as released by Geoscience Australia, have become necessary. Such a system is PCI GXL. The system can run on an Amazon cloud and extract time series of data from the archive of Planet Labs or ESA through their respective API’s. A range of image preprocessing techniques can be undertaken with a set of software libraries and executables. GXL powered automated workflows would typically include image calibration, atmospheric corrections, image to image registration to 1/10th of the image pixel, image compositing and image fusion. Pre-processing is optimized in a distributed way to feed into a data cube released as open source by Geoscience Australia. Once imagery has been deposited in the data cube, quantitative time series analysis can be performed by GXL to enable rapid identification of changes. A new approach to the analysis of multi resolution data supports the combined use of high spatial resolution commercial data with lower spatial resolution imagery collected at a high temporal frequency. Image segmentation and feature calculation based on pure pixels only allow the abstraction of data to units of homogeneous surfaces reducing computational efforts and complexity for a multi resolution spatial and temporal image analysis. A set of software libraries dedicated to SAR image processing for polarimetry and interferometry enable the use of SAR data with all its attributes in the data stack. Near real time change detection and mapping applications for disaster management can be addressed in this way. Machine learning classifiers are able to convert data in the data cube to thematic geo-spatial information efficiently and accurately. The system can be integrated with cloud enabled multi-mission satellite ground segments such as FarEarth to complete the loop from satellite tasking to geo-spatial information delivery to end users. © 2018 International Astronautical Federation IAF. All rights reserved.","Classification (of information); Disaster prevention; Disasters; Geology; Geometry; Image analysis; Image fusion; Image registration; Image resolution; Image segmentation; Learning systems; Open source software; Pixels; Satellites; Space-based radar; Synthetic aperture radar; Time series analysis; Analysis of multi resolutions; Data cube; Distributed Image Processing; Image-to-image registration; Large volumes; Multisource data; Quantitative time-series analysis; Spatial resolution imagery; Data handling","Cloud multi source data access; Large volume data pre-processing; Populating geoscience Australia data cube","Conference paper","Final","","Scopus","2-s2.0-85051431120"
"Schmitt A.; Sieg T.; Wurm M.; Taubenböck H.","Schmitt, Andreas (55576223700); Sieg, Tobias (57195366505); Wurm, Michael (23494150600); Taubenböck, Hannes (8698790500)","55576223700; 57195366505; 23494150600; 8698790500","Investigation on the separability of slums by multi-aspect TerraSAR-X dual-co-polarized high resolution spotlight images based on the multi-scale evaluation of local distributions","2018","International Journal of Applied Earth Observation and Geoinformation","64","","","181","198","17","10.1016/j.jag.2017.09.006","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032216565&doi=10.1016%2fj.jag.2017.09.006&partnerID=40&md5=975f12d000e5802b4c577002bb07108f","Following recent advances in distinguishing settlements vs. non-settlement areas from latest SAR data, the question arises whether a further automatic intra-urban delineation and characterization of different structural types is possible. This paper studies the appearance of the structural type “slums” in high resolution SAR images. Geocoded Kennaugh elements are used as backscatter information and Schmittlet indices as descriptor of local texture. Three cities with a significant share of slums (Cape Town, Manila, Mumbai) are chosen as test sites. These are imaged by TerraSAR-X in the dual-co-polarized high resolution spotlight mode in any available aspect angle. Representative distributions are estimated and fused by a robust approach. Our observations identify a high similarity of slums throughout all three test sites. The derived similarity maps are validated with reference data sets from visual interpretation and ground truth. The final validation strategy is based on completeness and correctness versus other classes in relation to the similarity. High accuracies (up to 87%) in identifying morphologic slums are reached for Cape Town. For Manila (up to 60%) and Mumbai (up to 54%), the distinction is more difficult due to their complex structural configuration. Concluding, high resolution SAR data can be suitable to automatically trace potential locations of slums. Polarimetric information and the incidence angle seem to have a negligible impact on the results whereas the intensity patterns and the passing direction of the satellite are playing a key role. Hence, the combination of intensity images (brightness) acquired from ascending and descending orbits together with Schmittlet indices (spatial pattern) promises best results. The transfer from the automatically recognized physical similarity to the semantic interpretation remains challenging. © 2017 Elsevier B.V.","Cape Town; India; Maharashtra; Manila; Mumbai; National Capital Region; Philippines; South Africa; Western Cape; image analysis; image resolution; observational method; pattern recognition; remote sensing; satellite data; synthetic aperture radar; urban area","Image analysis; Image classification; Image fusion; Image texture analysis; Pattern recognition; Radar applications; Radar polarimetry; Radar remote sensing; Urban areas","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85032216565"
"Adelipour S.; Ghassemian H.","Adelipour, Sadjad (56879110500); Ghassemian, Hassan (57204122949)","56879110500; 57204122949","The Fusion of Morphological and Contextual Information for Building Detection from Very High-Resolution SAR Images","2018","26th Iranian Conference on Electrical Engineering, ICEE 2018","","","8472581","389","393","4","10.1109/ICEE.2018.8472581","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055635428&doi=10.1109%2fICEE.2018.8472581&partnerID=40&md5=5a66643b335e172cc9a1fcd7061784e6","Nowadays, very high-resolution synthetic aperture radar (VHR SAR) images are available for interpretation of the built-up area. Buildings are one of the most important parts of the urban area, and in this paper a new method for building detection from a single SAR image is proposed. First, the contextual information of the buildings, such as double bounce, layover and shadow areas are extracted. Using these features, a set of primary detection is made. Second, morphological profiles (MP), with different structural elements (SE), are utilized to build a differential morphological profile (DMP) that provides the building structural information. This structural information is used to make a secondary detection set of building candidates. The final detection result is made by fusion of these two sets. Performance evaluation of the proposed method is reported by the implementation of the method on two different real TerraSAR-X images. The results show that the proposed method has a high detection rate (DR), while the false alarm rate (FAR) is low. © 2018 IEEE.","Buildings; Feature extraction; Image fusion; Information fusion; Synthetic aperture radar; Building detection; Contextual information; Differential morphological profile; Morphological profile; Performance evaluations; SAR Images; Structural information; Very highresolution synthetic aperture radars (VHR SAR); Radar imaging","Building detection; Feature Extraction; Information fusion; Morphological profile; VHR SAR images","Conference paper","Final","","Scopus","2-s2.0-85055635428"
"Hunger S.; Karrasch P.; Wessollek C.","Hunger, Sebastian (57188579309); Karrasch, Pierre (55948092400); Wessollek, Christine (56725814600)","57188579309; 55948092400; 56725814600","Evaluating the potential of image fusion of multispectral and radar remote sensing data for the assessment of water body structure","2016","Proceedings of SPIE - The International Society for Optical Engineering","9998","","999814","","","","10.1117/12.2241264","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011093380&doi=10.1117%2f12.2241264&partnerID=40&md5=adc330b9fa957f2595651008ce3230e7","The European Water Framework Directive (Directive 2000/60/EC) is a mandatory agreement that guides the member states of the European Union in the field of water policy to fulfill the requirements for reaching the aim of the good ecological status of water bodies. In the last years several workflows and methods were developed to determine and evaluate the characteristics and the status of the water bodies. Due to their area measurements remote sensing methods are a promising approach to constitute a substantial additional value. With increasing availability of optical and radar remote sensing data the development of new methods to extract information from both types of remote sensing data is still in progress. Since most limitations of these data sets do not agree the fusion of both data sets to gain data with higher spectral resolution features the potential to obtain additional information in contrast to the separate processing of the data. Based thereupon this study shall research the potential of multispectral and radar remote sensing data and the potential of their fusion for the assessment of the parameters of water body structure. Due to the medium spatial resolution of the freely available multispectral Sentinel-2 data sets especially the surroundings of the water bodies and their land use are part of this study. SAR data is provided by the Sentinel-1 satellite. Different image fusion methods are tested and the combined products of both data sets are evaluated afterwards. The evaluation of the single data sets and the fused data sets is performed by means of a maximum-likelihood classification and several statistical measurements. The results indicate that the combined use of different remote sensing data sets can have an added value. © 2016 SPIE.","Agriculture; Classification (of information); Data fusion; Data handling; Ecology; Ecosystems; Environmental regulations; Hydrology; Image fusion; Land use; Maximum likelihood; Radar; Radar imaging; Synthetic aperture radar; Water conservation; Water management; European Water Framework Directive; Extract informations; Good ecological status; Image fusion methods; Maximum likelihood classifications; Radar remote sensing; Sentinel 2; Sentinel-1; Remote sensing","Image fusion; Maximum likelihood; Sentinel 1; Sentinel 2","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85011093380"
"Tang X.; Jiao L.","Tang, Xu (57020306700); Jiao, Licheng (7102491544)","57020306700; 7102491544","Fusion Similarity-Based Reranking for SAR Image Retrieval","2017","IEEE Geoscience and Remote Sensing Letters","14","2","7801895","242","246","4","10.1109/LGRS.2016.2636819","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008462903&doi=10.1109%2fLGRS.2016.2636819&partnerID=40&md5=2edfc264e9a2434004682b005e6c7ca3","A new reranking method, fusion similarity-based reranking, is proposed in this letter to improve the performance of synthetic aperture radar (SAR) image retrieval. First, the top ranked SAR images within the initial retrieval results are picked for reranking. Considering the negative influence of the speckle noise, three SAR-oriented visual features are selected to represent them. In addition, the different relevance scores corresponding to an SAR image are estimated in various modalities (i.e., different feature spaces). Second, a fusion similarity is defined under the relevance score space to measure the resemblance between two SAR images. This fusion similarity is calculated using the modal-image matrix, which is construed by the estimated scores to integrate the contributions of all modalities. Finally, an existing reranking function is adopted to rerank the SAR images with the help of the estimated scores and calculated fusion similarities. The positive experimental results demonstrate that our reranking method is effective and efficient. © 2004-2012 IEEE.","Image fusion; Image retrieval; Space-based radar; Synthetic aperture radar; Feature space; Image matrix; Re-ranking; Relevance score; SAR Images; Speckle noise; Synthetic aperture radar (SAR) images; Visual feature; Radar imaging","Fusion similarity; reranking; synthetic aperture radar (SAR) image retrieval","Article","Final","","Scopus","2-s2.0-85008462903"
"Zeng T.; Zhang T.; Tian W.; Hu C.","Zeng, Tao (35326077500); Zhang, Tian (55616987500); Tian, Weiming (26649288900); Hu, Cheng (55723108300)","35326077500; 55616987500; 26649288900; 55723108300","Space-Surface Bistatic SAR Image Enhancement Based on Repeat-Pass Coherent Fusion with Beidou-2/Compass-2 as Illuminators","2016","IEEE Geoscience and Remote Sensing Letters","13","12","7727943","1832","1836","4","10.1109/LGRS.2016.2614337","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994222235&doi=10.1109%2fLGRS.2016.2614337&partnerID=40&md5=cf03f53316c44d0597b05b6ae46c3950","Low signal power density limits the performance of space-surface bistatic synthetic aperture radar (SS-BiSAR) using Global Navigation Satellite System (GNSS) satellites as illuminators. To tackle this problem, in this letter, a novel bistatic SAR image enhancement technique based on repeat-pass coherent fusion is proposed. The works in this letter include three aspects. First, repeat-pass experiments are designed to ensure the best resolution. Second, a modified CLEAN technique is applied to remove the direct signal interference from the focused BiSAR images. Third, a coherence-processing method is proposed to implement coherence of each repeat-pass BiSAR image and then they are coherently fused to obtain a quality-improved BiSAR image. Twenty-two days of repeat-pass BiSAR experiments with Beidou-2/Compass-2 inclined geosynchronous orbit satellites as illuminators have been designed and conducted. The data were processed by the proposed method. The results show that the method can obtain better image quality compared with the traditional noncoherent fusion method and the single-day imaging result, which validates the proposed method and proves the huge potential in realizing local area monitoring with SS-BiSAR using GNSS satellites as illuminators. © 2004-2012 IEEE.","Global positioning system; Image enhancement; Image fusion; Orbits; Radio navigation; Satellites; Signal interference; Space-based radar; Synthetic aperture radar; Bistatic synthetic aperture radar; CLEAN technique; Fusion methods; Geosynchronous orbits; Global Navigation Satellite Systems; Processing method; Signal power; Space surfaces; Radar imaging","Beidou-2/Compass-2; coherent fusion; passive SAR; space-surface bistatic SAR (SS-BiSAR); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-84994222235"
"Kulkarni S.C.; Rege P.P.; Parishwad O.","Kulkarni, Samadhan C. (57204893706); Rege, Priti P. (6701858789); Parishwad, Omkar (56015522800)","57204893706; 6701858789; 56015522800","Hybrid fusion approach for synthetic aperture radar and multispectral imagery for improvement in land use land cover classification","2019","Journal of Applied Remote Sensing","13","3","034516","","","","10.1117/1.JRS.13.034516","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072517280&doi=10.1117%2f1.JRS.13.034516&partnerID=40&md5=a245234cafc0cdcb1ba564f108be4e3a","Multisensor image fusion has gained tremendous significance due to various satellites operating in different parts of the electromagnetic spectrum. We present a hybrid fusion approach to integrate information from synthetic aperture radar (SAR) and multispectral (MS) imagery to improve land use land cover (LULC) classification. The major concern in SAR and optical fusion is the spectral distortion in the fused image, which is significantly less in pansharpening algorithms. The primary objective of our work is to inject unique spatial information from the SAR image into MS images, deriving enhanced data. The proposed approach is based on the integration of principal component analysis and wavelet decomposition to reduce spectral distortion in the fused image. Fused images are evaluated visually and statistically. Results are compared with conventional fusion approaches. In order to explore the effectiveness of the proposed technique, LULC classification is performed on the fused and original data. The LULC classification results are analytically compared with the standard thematic map to derive classification accuracy. A comparative analysis with other approaches conclusively proves that the proposed hybrid approach is superior to conventional approaches. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Classification (of information); Image analysis; Image classification; Image enhancement; Image fusion; Land use; Maps; Principal component analysis; Remote sensing; Synthetic aperture radar; Tracking radar; Wavelet decomposition; Wavelet transforms; Classification accuracy; Classification results; Conventional approach; Electromagnetic spectra; Land use/ land covers; Multi-spectral imagery; Multisensor image fusion; Synthetic Aperture Radar Imagery; Radar imaging","image fusion; land use land cover classification; multispectral imagery; principal component analysis; synthetic aperture radar imagery; wavelet transform","Article","Final","","Scopus","2-s2.0-85072517280"
"Shen T.; Li J.; Wang Z.; Huang L.; Li L.; Zhang P.","Shen, Ting (57207182171); Li, Jun (55881489700); Wang, Zhirui (55850298600); Huang, Lei (57211722262); Li, Liwei (55730921800); Zhang, Ping (57198754127)","57207182171; 55881489700; 55850298600; 57211722262; 55730921800; 57198754127","The recent advances of data imaging and fusion processing for airborne X-SAR with high resolution","2016","2016 Progress In Electromagnetics Research Symposium, PIERS 2016 - Proceedings","","","7735138","2843","2848","5","10.1109/PIERS.2016.7735138","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006783203&doi=10.1109%2fPIERS.2016.7735138&partnerID=40&md5=d0ae1b6902ec78f8b39dbdbfc50ae5f5","X-SAR system is the airborne imaging radar with multi-mode synthetic aperture radar (SAR) at high-resolution, interferometer and full-polarization, which has been developed by the Institute of Remote Sensing and Digital Earth (RADI), Chinese Academy of Sciences (CAS), funded by the CAS Large Research Infrastructures. The first-stage form 2009 to 2015, X-SAR was successfully implemented to an operational SAR in X-band with high resolution (up to 0.5 m). The system performances and data imaging quality have verified by the flight tests. Many valuable results of the visual interpretation in typical images, particularly SAR image fusion processing have emphasized the X-SAR's target recognition capabilities. This paper presents the core characteristics of X-SAR images, having achieved by the spatial resolution optimized by low side-lobe, exact geographical precision and radiometric accuracy. The visual inspection of typical targets in example images is described such as the surface of desert hill, the vehicle discrimination and aircraft recognition. Meanwhile, the image fusion processing for target recognition has been implemented. The recent advances of SAR-optical image fusion used to target classification and SAR-SAR image fusion processing for change detection are also presented. © 2016 IEEE.","Fighter aircraft; Geometrical optics; Image fusion; Image processing; Radar; Radar target recognition; Remote sensing; Synthetic aperture radar; Chinese Academy of Sciences; Image fusion processing; Radiometric accuracy; Research infrastructure; Spatial resolution; Target Classification; Target recognition; Visual interpretation; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85006783203"
"Sanli F.B.; Abdikan S.; Esetlili M.T.; Sunar F.","Sanli, Fusun Balik (23098475900); Abdikan, Saygin (55515101500); Esetlili, Mustafa Tolga (16743518500); Sunar, Filiz (6603011829)","23098475900; 55515101500; 16743518500; 6603011829","Evaluation of image fusion methods using PALSAR, RADARSAT-1 and SPOT images for land use/ land cover classification","2017","Journal of the Indian Society of Remote Sensing","45","4","","591","601","10","10.1007/s12524-016-0625-y","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988689741&doi=10.1007%2fs12524-016-0625-y&partnerID=40&md5=afb0d39cfd6a6debdc7db01599a0946b","This research aimed to explore the fusion of multispectral optical SPOT data with microwave L-band ALOS PALSAR and C-band RADARSAT-1 data for a detailed land use/cover mapping to find out the individual contributions of different wavelengths. Many fusion approaches have been implemented and analyzed for various applications using different remote sensing images. However, the fusion methods have conflict in the context of land use/cover (LULC) mapping using optical and synthetic aperture radar (SAR) images together. In this research two SAR images ALOS PALSAR and RADARSAT-1 were fused with SPOT data. Although, both SAR data were gathered in same polarization, and had same ground resolution, they differ in wavelengths. As different data fusion methods, intensity hue saturation (IHS), principal component analysis, discrete wavelet transformation, high pass frequency (HPF), and Ehlers, were performed and compared. For the quality analyses, visual interpretation was applied as a qualitative analysis, and spectral quality metrics of the fused images, such as correlation coefficient (CC) and universal image quality index (UIQI) were applied as a quantitative analysis. Furthermore, multispectral SPOT image and SAR fused images were classified with Maximum Likelihood Classification (MLC) method for the evaluation of their efficiencies. Ehlers gave the best score in the quality analysis and for the accuracy of LULC on LULC mapping of PALSAR and RADARSAT images. The results showed that the HPF method is in the second place with an increased thematic mapping accuracy. IHS had the worse results in all analyses. Overall, it is indicated that Ehlers method is a powerful technique to improve the LULC classification. © 2016, Indian Society of Remote Sensing.","agriculture; ALOS; correlation; efficiency measurement; land cover; maximum likelihood analysis; multispectral image; PALSAR; RADARSAT; remote sensing; satellite imagery; SPOT; synthetic aperture radar; thematic mapping; wavelet analysis","Agriculture; Fusion; Land cover; Land use; Multispectral; SAR","Article","Final","","Scopus","2-s2.0-84988689741"
"Chen Y.-K.; Chiang S.-H.; Chung-Pai C.","Chen, Yi-Keng (57226683620); Chiang, Shou-Hao (16835870200); Chung-Pai, Chang (7407035172)","57226683620; 16835870200; 7407035172","An artificial neural network approach for optical-SAR imagery fusion and landslide mapping","2017","38th Asian Conference on Remote Sensing - Space Applications: Touching Human Lives, ACRS 2017","2017-October","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047382374&partnerID=40&md5=a7ec02ace45a903fe85e88e2d02d7c10","In the mountainous areas of Taiwan, affected by active tectonics, frequent typhoons and human activities, landslides are commonly induced by heavy rainfalls especially during typhoon seasons. During Typhoon Morakot (2009), the heavy rainfall induced large-scale landslide and caused severe damages in the mountainous region. This study focuses on landslide detection in Laonong River watershed in southern Taiwan. Image fusion can visually or statistically enhance the characteristics of land-objects. Usually, investigators mainly detect bare surface of landslides by using optical images instead of using synthetic aperture radar (SAR) images to identify the erosion, transportation and deposition patterns, which can be critical to landslide susceptibility assessment. Thus, this study aims to develop an Optical-SAR image fusion for an advanced landslide mapping task. In this study, SAR data is used to detect the change of land surface by distinguishing backscatters of images before and after the Typhoon event. In addition, the machine-learning method, artificial neural network (ANN), was operated for landslide pattern fusion and mapping practice. With applying image segmentation, Normalized Difference Sigma-naught Index (NDSI) from SAR images and Normalized Difference Vegetation Index difference (NDVIdiff) from optical images were generated, and calculated their texture statistics, such as mean, standard deviation, contrast, entropy, homogeneity and dissimilarity. Landslide detection results were assessed by overall accuracy (OA) and kappa coefficient, with comparing to a manually interpreted landslide inventory. Result shows the OA of optical-SAR image fusion is 0.896 and the kappa coefficient is 0.547, which has better performance than results with only using optical or SAR images for landslide detection. © 2017 ACRS. All rights reserved.","Geometrical optics; Hurricanes; Image enhancement; Image fusion; Image segmentation; Landslides; Learning systems; Mapping; Neural networks; Rain; Remote sensing; Space applications; Space optics; Space-based radar; Synthetic aperture radar; Artificial neural network approach; Landslide susceptibility assessments; Large-scale landslides; Machine learning methods; NDSI; NDVI; Normalized difference vegetation index; Synthetic aperture radar (SAR) images; Radar imaging",": landslide; ANN; Image fusion; NDSI; NDVI","Conference paper","Final","","Scopus","2-s2.0-85047382374"
"Mazza A.; Sica F.; Rizzoli P.; Scarpa G.","Mazza, Antonio (57200854745); Sica, Francescopaolo (56673917100); Rizzoli, Paola (36816379500); Scarpa, Giuseppe (7004081145)","57200854745; 56673917100; 36816379500; 7004081145","TanDEM-X forest mapping using convolutional neural networks","2019","Remote Sensing","11","24","2980","","","","10.3390/rs11242980","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077905780&doi=10.3390%2frs11242980&partnerID=40&md5=d277240c1d29b89afb8022a77c858481","In this work, we face the problem of forest mapping from TanDEM-X data by means of Convolutional Neural Networks (CNNs). Our study aims to highlight the relevance of domain-related features for the extraction of the information of interest thanks to their joint nonlinear processing through CNN. In particular, we focus on the main InSAR features as the backscatter, coherence, and volume decorrelation, as well as the acquisition geometry through the local incidence angle. By using different state-of-the-art CNN architectures, our experiments consistently demonstrate the great potential of deep learning in data fusion for information extraction in the context of synthetic aperture radar signal processing and specifically for the task of forest mapping from TanDEM-X images. We compare three state-of-the-art CNN architectures, such as ResNet, DenseNet, and U-Net, obtaining a large performance gain over the baseline approach for all of them, with the U-Net solution being the most effective one. © 2019 by the authors.","Convolution; Data fusion; Deep learning; Forestry; Image fusion; Network architecture; Neural networks; Radar target recognition; Signal processing; Synthetic aperture radar; Target tracking; Tracking radar; Acquisition geometry; Convolutional neural network; Digital elevation model; Forest classification; Image segmemtation; Nonlinear processing; State of the art; Volume decorrelation; Mapping","Convolutional neural network; Data fusion; Digital elevation model; Forest classification; Image segmemtation; Synthetic aperture radar; Target detection","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85077905780"
"Iervolino P.; Guida R.; Riccio D.; Rea R.","Iervolino, Pasquale (55305269200); Guida, Raffaella (22234096000); Riccio, Daniele (7006577607); Rea, Raffaele (36500168200)","55305269200; 22234096000; 7006577607; 36500168200","A Novel Multispectral, Panchromatic and SAR Data Fusion for Land Classification","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","10","8869906","3966","3979","13","10.1109/JSTARS.2019.2945188","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076233426&doi=10.1109%2fJSTARS.2019.2945188&partnerID=40&md5=cf3793109583ff4d45ced33a3581422e","Multisensor data fusion is addressed in this article for land classification purposes in a semiarid environment. A novel algorithm based on multispectral, panchromatic and synthetic aperture radar (SAR) data is here presented. The proposed multisensory data fusion approach relies on the generalized intensity-hue-saturation (G-IHS) transform and the À trous wavelet transform (ATWT). The fusion product is obtained by modulating the high features details of the panchromatic ATWT with the SAR texture and by replacing the high-pass details of the G-IHS Intensity component with this panchromatic-SAR modulation. After the fusion product is derived, a classification is performed by using a standard maximum likelihood classifier. The proposed algorithm is tested over a meaningful case study acquired over the Maspalomas Special Natural Reserve (Spain) and processing data from WorldView-2 (for both multispectral and panchromatic channels) and TerraSAR-X (for the SAR channel) missions. Results show a fine preservation of the spectral information contained in each multispectral band. Sharpened details are observed over built-up areas and a smoothing texture is perceived over homogeneous areas (lakes, sea, bare soil, and roads) due to the SAR-panchromatic modulation. This leads to a better overall classification accuracy of the fused image compared to outcomes obtained with a single sensor, resulting 7% and 2% more accurate than multispectral and pan-sharpening classification, respectively. © 2008-2012 IEEE.","Data handling; Image classification; Image fusion; Maximum likelihood; Modulation; Textures; Wavelet transforms; Classification accuracy; Intensity hue saturations; Maximum likelihood classifiers; Multisensor data fusion; Multisensory data; Multispectral imaging; Semi-arid environments; Spectral information; algorithm; data set; image analysis; image classification; image resolution; spectral analysis; synthetic aperture radar; Synthetic aperture radar","Image classification; image fusion; multispectral imaging; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85076233426"
"Zou H.; Li M.; Shao N.; Qin X.","Zou, Huanxin (8366222500); Li, Meilin (57209947350); Shao, Ningyuan (57196454992); Qin, Xianxiang (55375282700)","8366222500; 57209947350; 57196454992; 55375282700","Superpixel-Oriented Unsupervised Classification for Polarimetric SAR Images Based on Consensus Similarity Network Fusion","2019","IEEE Access","7","","8736007","78347","78366","19","10.1109/ACCESS.2019.2922473","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068207798&doi=10.1109%2fACCESS.2019.2922473&partnerID=40&md5=08a69d9d96958da203b8f30f6ced2e89","Unsupervised polarimetric synthetic aperture radar (PolSAR) image classification is an important task in PolSAR automatic image analysis and interpretation. Generally, a group of features is insufficient to effectively classify PolSAR images, especially in multiple terrain scenarios. Therefore, multiple features need to be extracted for PolSAR image classification. However, how to combine and integrate these features effectively to fully utilize each feature's information and discriminability need to be determined. Such integrated work has traditionally received little attention. In this paper, a novel unsupervised classification framework for PolSAR images is proposed. First, a PolSAR image is oversegmented via a fast superpixel segmentation method. Second, five feature vectors are extracted from PolSAR images via superpixels, resulting in five corresponding similarity matrices that are constructed by using Gaussian kernels. Third, consensus similarity network fusion (CSNF), originally proposed and widely used for biomedical sciences, is employed to combine and integrate the five similarity matrices to obtain a fused similarity matrix. Fourth, spectral clustering method, based on the fused similarity matrix, is used to cluster the PolSAR image. Finally, a novel classification postprocessing procedure is presented and exploited to smooth the initial clusters and correct some misclassified pixels. The extensive experimental results conducted on one simulated and two real-world PolSAR images demonstrate the feasibility and superiority of the proposed method compared with five other state-of-the-art classification approaches. © 2019 IEEE.","Cluster analysis; Clustering algorithms; Image fusion; Image segmentation; Polarimeters; Radar imaging; Superpixels; Synthetic aperture radar; Polarimetric synthetic aperture radars; Similarity network; Spectral clustering; Superpixels segmentations; Unsupervised classification; Image classification","consensus similarity network fusion (CSNF); Polarimetric synthetic aperture radar (PolSAR) images; spectral clustering; superpixels segmentation; unsupervised classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85068207798"
"Islam Z.U.; Iqbal J.; Khan J.A.; Qazi W.A.","Islam, Zaheer Ul (57191611492); Iqbal, Javed (57203095126); Khan, Junaid Aziz (57188997033); Qazi, Waqas A. (26326491800)","57191611492; 57203095126; 57188997033; 26326491800","Paleochannel delineation using Landsat 8 OLI and Envisat ASAR image fusion techniques in Cholistan desert, Pakistan","2016","Journal of Applied Remote Sensing","10","4","046001","","","","10.1117/1.JRS.10.046001","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992053113&doi=10.1117%2f1.JRS.10.046001&partnerID=40&md5=e1d1987ae05fb86e9efa4938be95d83c","Sustainability of desert ecosystem is highly dependent upon water availability from different sources. Paleochannels are important sources of groundwater, and exploiting such resources involves their identification/mapping and subsequent investigation for fresh groundwater. A study in which multisensor (optical/infrared Landsat 8 OLI and active microwave Envisat ASAR) images of the Cholistan desert of Pakistan were processed and analyzed to identify and map Hakra River paleochannels is presented. Radiometrically corrected optical and synthetic aperture radar datasets were fused using principal components image fusion method. The paleochannels were extracted from the analysis of this fused output, and normalized difference vegetative index analysis of Landsat 8 OLI atmospheric corrected images was used as supporting information. Identification and alignment of an identified paleochannel was validated with geophysical ground measurements (electrical resistivity and conductivity surveys) and historical records. The presence of high apparent electrical resistivity with corresponding low soil water conductivity values intersects well with the paleochannels identified from the remote sensing data. The results were also confirmed with historical evidence such as old wells beside forts and proposed ground water harvesting sites. The proposed methodology in this study could be adopted in other parts of the world for mapping of paleochannels. © 2016 Society of Photo-Optical Instrumentation Engineers (SPIE).","Aquifers; Electric conductivity; Groundwater resources; Image analysis; Landforms; Remote sensing; Soil moisture; Space-based radar; Synthetic aperture radar; Vegetation; Water conservation; ENVISAT ASAR; geophysical; Image fusion methods; Image fusion techniques; LANDSAT; Normalized differences; Paleo-channel; Soil water conductivities; Image fusion","aquifer; Envisat ASAR; geophysical; Landsat 8; paleochannel","Article","Final","","Scopus","2-s2.0-84992053113"
"Zhang Y.; Wang S.; Wang C.; Zhang H.; Wu F.; Liu M.; Fu Q.; Wang Y.","Zhang, Yan (57225165645); Wang, Shigang (55768692100); Wang, Chao (55141316100); Zhang, Hong (56179236500); Wu, Fan (57104120200); Liu, Meng (55743542100); Fu, Qiaoyan (14031628300); Wang, Yuanyuan (57195129647)","57225165645; 55768692100; 55141316100; 56179236500; 57104120200; 55743542100; 14031628300; 57195129647","SAR image change detection method based on shearlet transform","2017","Progress in Electromagnetics Research Symposium","2017-November","","","1223","1229","6","10.1109/PIERS-FALL.2017.8293318","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045326261&doi=10.1109%2fPIERS-FALL.2017.8293318&partnerID=40&md5=0b0a83e2745aff0fe4ac7230c4e106d1","Multi-temporal synthetic aperture radar (SAR) images have been successfully used for the detection of different types of terrain changes. However, SAR image change detection based on wavelet transform is still restrained from the existence of speckle noise and the nature of wavelet transform. In this paper, an unsupervised SAR image change detection fusion framework based on shearlet transform is proposed. In the proposed method, The Gauss filtering is combined with log-ratio to impair speckle. Then the difference map (DM) of Gauss-log ratio and the difference map of ratio based on Gabor feature are fused with shearlet transform. Meanwhile, DM is decomposed to low frequency image and four high frequency images, different fusion rules are used in multi-scales images respectively, the work of noise reduction is operated with mean filtering. After an inverse shearlet transformation, the final change map can be obtained via a simple OSTU segmentation. The real SAR image pairs in Bern area are used to verify proposed change detection method. The experimental results demonstrate the robustness of the proposed method. © 2018 Electromagnetics Academy. All rights reserved.","Image compression; Image fusion; Inverse problems; Noise abatement; Speckle; Synthetic aperture radar; Wavelet transforms; Change detection; High frequency HF; Low frequency images; Multi-temporal; Shearlet transforms; Speckle noise; Synthetic aperture radar (SAR) images; Terrain changes; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85045326261"
"Li L.; Chen Q.; Zhang C.; You S.; Wei H.; Fu X.","Li, Li (56102840300); Chen, Qiqi (57216289694); Zhang, Chao (56401277900); You, Shucheng (57220740785); Wei, Hai (57212003582); Fu, Xue (57216288726)","56102840300; 57216289694; 56401277900; 57220740785; 57212003582; 57216288726","Multi-source Spaceborne SAR Image Fusion Based on RNMU and Land Cover Classification; [基于RNMU的多源星载SAR影像融合与土地覆盖分类]","2020","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","51","3","","191","200","9","10.6041/j.issn.1000-1298.2020.03.022","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083036746&doi=10.6041%2fj.issn.1000-1298.2020.03.022&partnerID=40&md5=42f4bf058ea492f4c81e312ebbb2b604","Aiming to take full advantage of the backward scattering characteristics for different land cover types in different temporal and polarization SAR data, the recursive nonnegative matrix underapproximation (RNMU) was used for the fusion of multi-source SAR data, and the fused SAR image was used to achieve a high-precision land cover classification. According to the characteristics of different SAR image modes, the input SAR images were pre-processed firstly, and then the matrix decomposition of the SAR images and the iterative solution of the optimal matrix were implemented based on RNMU. To verify the effect of application of integrated SAR image on land cover classification, taking Da'an City in Jilin Province as an example, RNMU was used for the fusion of multi-temporal VV/VH dual-polarization Sentinle-1 SAR image and HH/HV dual-polarization GF-3 data. The main types of land cover in the study area were classified with the fused SAR data based on RNMU. The results illustrated that SAR data fused based on RNMU algorithm had sound performance in the land cover classification with 93.11% overall accuracy and 0.86 Kappa coefficient, which outperformed the Gram-Schmid (G-S) fusion method with 6.83 percentage points and 0.12 higher in overall accuracy and Kappa coefficient respectively. The attempt of multi-source SAR fusion provided an effective means for SAR image fusion and provided more high-precision data resources for land cover classification. © 2020, Chinese Society of Agricultural Machinery. All right reserved.","Image classification; Image fusion; Iterative methods; Matrix algebra; Polarization; Synthetic aperture radar; Backward scattering; Dual-polarizations; Iterative solutions; Land cover classification; Matrix decomposition; Non-negative matrix; Overall accuracies; Under-approximation; Radar imaging","GF-3; Image fusion; Land cover classification; Recursive nonnegative matrix underapproximation; Sentinel-1","Article","Final","","Scopus","2-s2.0-85083036746"
"Ji X.-X.; Bian X.-X.","Ji, Xiu-Xia (55900734800); Bian, Xiao-Xiao (57200566827)","55900734800; 57200566827","Multi-sensor image fusion method based on adaptive weighting","2018","Journal of Computers (Taiwan)","29","4","","57","68","11","10.3966/199115992018082904005","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053710483&doi=10.3966%2f199115992018082904005&partnerID=40&md5=7bfdc8835607ebde0af31ea56450dacf","Each kind of sensor is designed to adapt to the specific environment and using scope. Fusing the image of the same target or scene can solve some problems such as insufficient information and multivariate data redundancy of single image, and make the description of the scene or the target of the image more accurately and more comprehensively. A new multi-sensor image fusion method based on adaptive weighting is presented. Firstly, the original image is decomposed with nonsubsampled contourlet transform to obtain a series of different frequency subbands of diverse scales and directions. Secondly, the low frequency subbands are fused by the rule of adaptive weighting, and the high subbands are fused by the rule of the largest gradient value. Lastly, the fused image is obtained by the inverse nonsubsampled contourlet transform. By means of infrared image, visible image and SAR image fusion experiments, the proposed image fusion method can effectively preserve a large amount of information and significantly improve the performance of the fused image in terms of visual quality and objective evaluation indicators. © 2018 Computer Society of the Republic of China. All Rights Reserved.","Image enhancement; Infrared imaging; Inverse problems; Radar imaging; Synthetic aperture radar; Adaptive weighting; Average gradient; Different frequency; Fusion experiments; Image fusion methods; Multi sensor images; Non-sub-sampled contourlet transforms; Objective evaluation; Image fusion","Adaptive weighting; Average gradient; Multi-sensor image fusion; Nonsubsampled contourlet transform","Article","Final","","Scopus","2-s2.0-85053710483"
"Thrisul Kumar J.; Mallikarjuna Reddy Y.; Prabhakara Rao B.","Thrisul Kumar, J. (57226875990); Mallikarjuna Reddy, Y. (35766908800); Prabhakara Rao, B. (26655008200)","57226875990; 35766908800; 26655008200","Image fusion of remote sensing images using ADWT with ABC optimization algorithm","2019","International Journal of Innovative Technology and Exploring Engineering","8","11","","3865","3869","4","10.35940/ijitee.K2309.0981119","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073732626&doi=10.35940%2fijitee.K2309.0981119&partnerID=40&md5=31826e619d7f358eb337945292bb1fa1","Synthetic Aperture Radar (SAR) is imaging radar and it generates images in any weather condition. In fact, SAR images are high resolution images therefore to process these images in digital format compression is compulsory, but during reconstruction of these signals some part of the signal component in image will be loosed due to quantization error. Hence in this paper a new approach is proposed to reconstruct the image as near to the original image. Two SAR images are taken for implementing this technique. The quantization error will be reduced by using adaptive discrete wavelet transforms with Artificial bee colony optimization and the output signal would be reconstructed. DWT is used for image fusion which decomposes images. After optimizing the filter coefficients in DWT then the image will be reconstructed by using IDWT and the performance is measured and compared with Genetic Algorithm (GA) in terms of MSE and PSNR. © BEIESP.","","ABC; ADWT; DWT; GA; MSE and PSNR; SAR","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85073732626"
"Sghaier M.O.; Hadzagic M.; Patera J.","Sghaier, Moslem Ouled (56421930900); Hadzagic, Melita (8120789600); Patera, Jiri (22972569300)","56421930900; 8120789600; 22972569300","Fusion of SAR and Multispectral Satellite Images Using Multiscale Analysis and Dempster-Shafer Theory for Flood Extent Extraction","2019","FUSION 2019 - 22nd International Conference on Information Fusion","","","9011209","","","","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081788259&partnerID=40&md5=f1a0063e1de2d2f60ff7f5d9f9b7894a","Monitoring flood extent by means of Synthetic Aperture Radar (SAR) images has become a very common practice among decision makers and planners in disaster management as these images provide wide area coverage in extreme weather conditions. However, due to the satellite revisit time, their availability hinders their efficient use in disaster management. To capitalize on SAR images characteristics, this work considers both SAR and optical multispectral (MS) images, and proposes a novel method for SAR and optical image fusion in application to flood extent monitoring, which is based on two main steps: 1-Extraction of water pixels from the pre-and post-flooding images using a Modified Water Index (MWI) for water bodies identification from optical MS images and the Structural Feature Set (SFS) texture measurement for homogeneous areas extraction from SAR images, and 2-Applying the Max-Tree structure to estimate mass functions based on the multiscale and the multishape analysis of the input features map which are subsequently incorporated into the fusion module using Dempster-Shafer theory (DST). The results obtained in the evaluation of the proposed fusion method for three flood events characterized by different satellite image scenarios demonstrate the benefits of the multiscale DST fusion strategy in terms of chosen metrics in the classification of water body and monitoring of flood extent. © 2019 ISIF-International Society of Information Fusion.","Decision making; Disaster prevention; Disasters; Extraction; Extreme weather; Floods; Geometrical optics; Image analysis; Image fusion; Information fusion; Probabilistic logics; Satellites; Space-based radar; Synthetic aperture radar; Textures; Trees (mathematics); Dempster-Shafer fusion; Dempster-Shafer theory; Extreme weather conditions; Multi scale analysis; Multiscale image analysis; Multispectral images; Multispectral satellite image; Synthetic aperture radar (SAR) images; Radar imaging","Dempster-Shafer fusion; flood extent extraction; multiscale image analysis; multispectral satellite images; SAR","Conference paper","Final","","Scopus","2-s2.0-85081788259"
"Wu J.; Liu F.; Hao H.; Li L.; Jiao L.; Zhang X.","Wu, Jie (55986087200); Liu, Fang (56182993400); Hao, Hongxia (27867689800); Li, Lingling (56327010200); Jiao, Licheng (7102491544); Zhang, Xiangrong (55802358000)","55986087200; 56182993400; 27867689800; 56327010200; 7102491544; 55802358000","A nonlocal means for speckle reduction of SAR image with multiscale-fusion-based steerable kernel function","2016","IEEE Geoscience and Remote Sensing Letters","13","11","","1646","1650","4","10.1109/LGRS.2016.2600558","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027580450&doi=10.1109%2fLGRS.2016.2600558&partnerID=40&md5=337325ce917effd15386ce2f4e5b12b4","For the robustness of a patch-based metric, the nonlocal means method is widely applied for speckle reduction of synthetic aperture radar (SAR) images, where the similarity computed by the patch-based metric is used as weight, and weighted averaging is used to obtain the true value. However, not knowing the local spatial property, a fixed kernel (e.g., Gaussian kernel or uniform kernel) is always used to compute the weight. This is not good for the preservation of geometrical features (e.g., edges, lines, and points). In this letter, considering the characteristics of SAR imagery, a multiscale-fusion-based steerable kernel function was formed to explore the local spatial property of SAR images. In addition, by combining the kernel function with a ratio-based similarity metric designed with the distribution of the speckle's ratio, a new patch-based metric was formed and used with the nonlocal scheme for speckle reduction. In the experiments, by comparing with two state-of-the-art methods, a reasonable performance was obtained by our method, in terms of speckle reduction and detail preservation. © 2004-2012 IEEE.","Image denoising; Image fusion; Speckle; Synthetic aperture radar; Kernel function; Multiscale fusion; Non local means (NLM); Patch based; Speckle reduction; Radar imaging","Multiscale fusion; Nonlocal means (NLM); Patch-based similarity; Speckle reduction; Steerable kernel function (StKF)","Article","Final","","Scopus","2-s2.0-85027580450"
"Ma X.; Hu S.; Liu S.; Wang J.; Xu S.","Ma, Xiaole (57193220596); Hu, Shaohai (7404286949); Liu, Shuaiqi (55055535100); Wang, Jie (57209210159); Xu, Shuwen (57207942043)","57193220596; 7404286949; 55055535100; 57209210159; 57207942043","Noisy Remote Sensing Image Fusion Based on JSR","2020","IEEE Access","8","","8995486","31069","31082","13","10.1109/ACCESS.2020.2973435","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081049843&doi=10.1109%2fACCESS.2020.2973435&partnerID=40&md5=b79381cb919600b2fefb397b0cec2aab","Compressed sensing has shown great potential and power in image representation, especially in image reconstruction by sparse representation. Due to complementary information and unavoidable noise existing in synthetic aperture radar (SAR) and other source images, joint sparse representation (JSR) is developed to separate redundancy and complementary information with different properties in source images and obtain a fused image, where image de-noising is done simultaneously owing to that noise is not sparse and cannot be represented by sparse representation. As a result, one noisy remote sensing image fusion method based on JSR is presented in this paper. After obtaining redundant and complementary sub-images by JSR, an improved fusion rule based on pulse coupled neural network (PCNN) is employed to fuse complementary sparse coefficients together. At the same time, because the types of noise in SAR and other source images are different, they can be treated as the complementary information in source images and suppressed at this step. Finally, a fused image can be reconstructed by adding the redundant and fused complementary sub-images. Quantitative and qualitative experimental results show that the proposed method outperforms most of other fusion methods and it is more robust to noise, having better visual effects and values of objective evaluation metrics. © 2013 IEEE.","Image denoising; Image enhancement; Image reconstruction; Neural networks; Radar imaging; Remote sensing; Synthetic aperture radar; Fusion methods; Image representations; Objective evaluation; Pulse coupled neural network; Remote sensing images; SAR image processing; Sparse representation; Visual effects; Image fusion","joint sparse representation; pulse coupled neural network; Remote sensing image fusion; SAR image processing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081049843"
"Ji X.","Ji, Xiuxia (55900734800)","55900734800","An improved image fusion method of infrared image and SAR image via Shearlet and sparse representation","2016","Proceedings - 2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2016","1","","7783560","74","77","3","10.1109/IHMSC.2016.141","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010284875&doi=10.1109%2fIHMSC.2016.141&partnerID=40&md5=69552c8d0ce504c25774504d655c9826","In this paper, a novel image fusion method of infrared image and SAR image is proposed combining Shearlet transform and sparse representation to avoid the disadvantages of Wavelet transform. The registered images are decomposed by the shearlet transform to obtain the low frequency subband and a series of high frequency subbands. The low frequency subband with lower sparseness is disposed with sparse representation, construct over complete dictionary, solve sparse coefficient over the trained dictionary, and choose the low frequency coefficients with the larger energy fusion rule. And the rule of gradient absolute value maximization is applied to the high frequency subbands. Then the fusion image is obtained by the inverse Shearlet transform. Experimental results show that the proposed method can retain good visual quality and objective evaluation index, and performs some related fusion approaches. © 2016 IEEE.","Cybernetics; Gradient methods; Image fusion; Infrared imaging; Inverse problems; Man machine systems; Synthetic aperture radar; Wavelet transforms; High frequency HF; Image fusion methods; Objective evaluation; Over-complete dictionaries; Registered images; Shearlet transforms; Sparse representation; Visual qualities; Radar imaging","Gradient; Image fusion; Shearlet transform; Sparse representation","Conference paper","Final","","Scopus","2-s2.0-85010284875"
"Kulkarni S.C.; Rege P.P.","Kulkarni, Samadhan C. (57204893706); Rege, Priti P. (6701858789)","57204893706; 6701858789","Pixel level fusion techniques for SAR and optical images: A review","2020","Information Fusion","59","","","13","29","16","10.1016/j.inffus.2020.01.003","101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078213705&doi=10.1016%2fj.inffus.2020.01.003&partnerID=40&md5=66b9352e79a9f9779b5a09f634aec472","Image Fusion is a process of combining two or more images into a single image which is more informative and hence more useful from an interpretation point of view. With the rapid development of different remote sensing satellites capturing information from the earth by sensing energy in different portions of the electromagnetic spectrum, complementary information about the area captured by different satellites is available. A fusion of these images is much more helpful in different remote sensing applications than that of single sensor image data. This paper discusses the necessity of fusing synthetic aperture radar (SAR) and optical imagery. A survey is presented for various pixel level approaches used for the fusion of SAR and optical images. Quality metrics used to evaluate the performance of a fusion algorithm, are briefly introduced and visual as well as quantitative evaluation of basic component substitution and wavelet-based fusion approaches is presented for the fusion of RISAT-1 SAR and Resourcesat-2 multispectral data. Finally, the review concludes that there is scope for further research of fusion of SAR and optical images due to various microwave and optical sensors with the improved resolution being launched regularly. © 2020 Elsevier B.V.","Geometrical optics; Image enhancement; Image fusion; Microwave sensors; Petroleum reservoir evaluation; Pixels; Quality control; Remote sensing; Sensor data fusion; Space-based radar; Synthetic aperture radar; Electromagnetic spectra; Optical imagery; Quality metrics; Quantitative evaluation; Remote sensing applications; Remote sensing satellites; SAR imagery; Wavelet-based fusion approach; Radar imaging","Image fusion; Optical imagery; Quality metrics; SAR imagery","Article","Final","","Scopus","2-s2.0-85078213705"
"Yang X.; Wang J.; Zhu R.","Yang, Xiaoyuan (35217321600); Wang, Jingkai (57194421372); Zhu, Ridong (57201525544)","35217321600; 57194421372; 57201525544","Random Walks for Synthetic Aperture Radar Image Fusion in Framelet Domain","2018","IEEE Transactions on Image Processing","27","2","8022919","851","865","14","10.1109/TIP.2017.2747093","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028690674&doi=10.1109%2fTIP.2017.2747093&partnerID=40&md5=ec37c4d028ebbbcf822f7a205acb9df1","A new framelet-based random walks (RWs) method is presented for synthetic aperture radar (SAR) image fusion, including SAR-visible images, SAR-infrared images, and Multi-band SAR images. In this method, we build a novel RWs model based on the statistical characteristics of framelet coefficients to fuse the high-frequency and low-frequency coefficients. This model converts the fusion problem to estimate the probability of each framelet coefficient being assigned each input image. Experimental results show that the proposed approach improves the contrast while preserves the edges simultaneously, and outperforms many traditional and state-of-the-art fusion techniques in both qualitative and quantitative analysis. © 2017 IEEE.","Image fusion; Infrared imaging; Radar; Random processes; Synthetic aperture radar; Framelet transforms; Fusion techniques; Qualitative and quantitative analysis; Random Walk; SAR Images; Statistical characteristics; Synthetic aperture radar (SAR) images; Visible image; article; quantitative analysis; telecommunication; Radar imaging","Framelet transform; Image fusion; Infrared image; Random walks; SAR image; Visible image","Article","Final","","Scopus","2-s2.0-85028690674"
"Gruber A.; Wessel B.; Martone M.; Roth A.","Gruber, Astrid (26666711300); Wessel, Birgit (22236091400); Martone, Michele (55293317300); Roth, Achim (56256016700)","26666711300; 22236091400; 55293317300; 56256016700","The TanDEM-X DEM Mosaicking: Fusion of Multiple Acquisitions Using InSAR Quality Parameters","2016","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9","3","7109106","1047","1057","10","10.1109/JSTARS.2015.2421879","61","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929591306&doi=10.1109%2fJSTARS.2015.2421879&partnerID=40&md5=28265032e83211f7be31f2216f990a88","Since 2010, TanDEM-X and its twin satellite TerraSAR-X fly in a close orbit formation and form a single-pass synthetic aperture radar (SAR) interferometer. The formation was established to acquire a global high-precision digital elevation model (DEM) using SAR interferometry (InSAR). In order to achieve the required height accuracy of the TanDEM-X DEM, at least two global coverages have to be acquired. However, in difficult and mountainous terrain, up to five coverages are present. Here, acquisitions from ascending and descending orbits are needed to fill gaps and to overcome geometric limitations. Therefore, a strategy to properly combine the available height estimates is mandatory. The objective of this paper is the presentation of the operational TanDEM-X DEM mosaicking approach. In general, multiple InSAR DEM heights are combined by means of a weighted average with the height error as weight. Apart from this widely used mosaicking approach, one big challenge remains with the handling of larger height discrepancies between the input data, which are mainly caused by phase unwrapping errors, but also by temporal changes between acquisitions. In the case of inconsistencies, the TanDEM-X mosaicking approach performs a grouping into height levels. A priority concept is set up to evaluate the different groups of heights considering the number of DEMs and several InSAR quality parameters: the height error, the phase unwrapping method, and the height of ambiguity. This allows the identification of the most reliable height level for mosaicking. This fusion concept is verified on different test areas affected by phase unwrapping errors in flat and mountainous terrain as well as by height discrepancies in forests. The results show that the quality of the final TanDEM-X DEM mosaic benefits a lot from this mosaicking approach. © 2015 IEEE.","Errors; Orbits; Surveying; Digital elevation model; Mountainous terrain; Multiple acquisitions; Phase unwrapping; Phase unwrapping methods; Quality parameters; SAR interferometry; Weighted averages; data acquisition; digital elevation model; height determination; image processing; synthetic aperture radar; TanDEM-X; TerraSAR-X; Synthetic aperture radar","Digital elevation models (DEMs); image fusion; interferometric synthetic aperture radar (InSAR); mosaicking; TanDEM-X","Article","Final","","Scopus","2-s2.0-84929591306"
"Hu T.; Li W.; Qin X.","Hu, Tao (57199359115); Li, Weihua (57155704600); Qin, Xianxiang (55375282700)","57199359115; 57155704600; 55375282700","Semantic Segmentation of Polarimetric Synthetic Aperture Radar Images Based on Multi-Layer Deep Feature Fusion; [基于多层深度特征融合的极化合成孔径雷达图像语义分割]","2019","Zhongguo Jiguang/Chinese Journal of Lasers","46","2","0210001","","","","10.3788/CJL201946.0210001","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063003131&doi=10.3788%2fCJL201946.0210001&partnerID=40&md5=e51d0bf5759672b716165f078085d270","Aiming at the problem that the traditional feature representation ability is weak, we propose a polarization synthetic aperture radar image semantic segmentation method based on the multi-layer deep feature fusion. The pre-trained VGG-Net-16 model is used to extract multi-layer image features with strong representation ability, and then deep features of each layer are used to train the corresponding conditional random field model. The output results of multiple conditional random field models are finally merged to realize the final semantic segmentation of the images. The results show that compared with the methods based on classical features, the proposed method achieves the highest overall accuracy, indicating that the fusion features used by the proposed method have stronger representation ability than traditional features. © 2019, Chinese Lasers Press. All right reserved.","Image fusion; Image processing; Neural networks; Radar imaging; Random processes; Semantics; Synthetic aperture radar; Conditional random field; Convolutional neural network; Feature fusion; Feature representation; Overall accuracies; Polarimetric synthetic aperture radars; Semantic segmentation; Synthetic aperture radar (SAR) images; Image segmentation","Conditional random field; Convolutional neural network; Image processing; Multi-layer deep feature fusion; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85063003131"
"Xu Q.; Li W.; Xu Z.; Zheng J.","Xu, Qiang (55775105200); Li, Wei (56796098400); Xu, Zehua (57202055355); Zheng, Jiayi (57202054745)","55775105200; 56796098400; 57202055355; 57202054745","Noisy SAR image classification based on fusion filtering and deep learning","2018","2017 3rd IEEE International Conference on Computer and Communications, ICCC 2017","2018-January","","","1928","1932","4","10.1109/CompComm.2017.8322874","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049838558&doi=10.1109%2fCompComm.2017.8322874&partnerID=40&md5=0a535e1dd34576db15d99e7cf90c6567","In recent years, Convolution Neural Network (CNN) has achieved a series of breakthrough results in large-scale identification tasks such as image classification and recognition, target location and detection. There are severe speckle noise and distortion in Synthetic Aperture Radar (SAR) image, and the traditional classification method has low recognition rate and low robustness to noise. To solve these problems, a novel approach for noisy SAR image classification based on fusion filter and CNN is suggested. Firstly, the noise suppression of the noisy SAR image is carried out by using the fusion filtering algorithm, and then the convolution neural network is used to classify the image. The experimental results show that the classification accuracy can reach 94.23% in the ten categories of target classification, which exceeds the traditional classification algorithm which is classified by artificial annotation. By testing under different noisy conditions, the verification algorithm has strong robustness to noise. © 2017 IEEE.","Convolution; Deep learning; Image classification; Image fusion; Synthetic aperture radar; Classification accuracy; Classification algorithm; Classification and recognition; Convolution neural network; Fusion filtering algorithm; SAR image classifications; Synthetic aperture radar (SAR) images; Verification algorithms; Radar imaging","convolution neural network; image classification; synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85049838558"
"Adelipour S.; Ghassemian H.","Adelipour, Sadjad (56879110500); Ghassemian, Hassan (57204122949)","56879110500; 57204122949","Building extraction from very high-resolution synthetic aperture radar images based on statistical and structural information fusion","2019","International Journal of Remote Sensing","40","18","","7113","7126","13","10.1080/01431161.2019.1601280","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064496316&doi=10.1080%2f01431161.2019.1601280&partnerID=40&md5=e8633286a33b45b12b952823d2e2f813","To investigate the limits of building detection from very high-resolution (VHR) synthetic aperture radar (SAR) images, a new method, based on statistical and structural information fusion, is proposed in this paper. The proposed method contains two stages: First, using order statistics constant false alarm rate (OS-CFAR) and power ratio (PR) detectors, a set of detections are made. These detections have different statistical properties, compared to the other objects, and these properties are selected for discriminating buildings from clutters. Second, the morphological analysis is used for increasing the precision of the detection. In this stage, segments, which have the most similarities to buildings in terms of shape and size, are extracted via various structural elements (SEs). The final result is obtained by fusing the two sets of detections. The experimental results on the four real VHR SAR images show that the proposed method has a high detection rate (DR) and low false alarm rate (FAR). © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","Errors; Image fusion; Image resolution; Information fusion; Radar signal processing; Synthetic aperture radar; Building extraction; Constant false alarm rate; High detection rate; Morphological analysis; Statistical properties; Structural information; Synthetic aperture radar (SAR) images; Very high resolution; building; detection method; image processing; information management; synthetic aperture radar; variance analysis; Radar imaging","","Article","Final","","Scopus","2-s2.0-85064496316"
"Anglberger H.; Fischer J.; Frommholz D.","Anglberger, Harald (23975684500); Fischer, Jens (57659983700); Frommholz, Dirk (56566629600)","23975684500; 57659983700; 56566629600","Radar and optical image fusion using airborne sensor data from the Heligoland Island","2018","Proceedings International Radar Symposium","2018-June","","8448211","","","","10.23919/IRS.2018.8448211","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053638035&doi=10.23919%2fIRS.2018.8448211&partnerID=40&md5=39a4e5de27f67821a4a728a24d68c810","An accurate geometrical alignment of remote sensing data is the basis for higher-level image processing techniques used to extract information. Fusing radar image data with other sensor data sources states a special case because the coordinate system is based on the measured range which causes ambiguous regions due to layover effects. An accurate 3D representation of the scene is essential to find a fitting geometrical transformation between the respective sensor image spaces. This paper applies a method that accurately maps detailed 3D information of the German island of Heligoland to the slant-range-based coordinate system of radar images imaged by DLR's airborne F-SAR sensor. The highly accurate 3D information along with optical imagery has been acquired by DLR's airborne optical sensor system MACS. © 2018 German Institute of Navigation - DGON.","Geometrical optics; Image fusion; Image processing; Mathematical transformations; Optical data processing; Radar measurement; Remote sensing; Sensor data fusion; Synthetic aperture radar; 3d representations; Air-borne sensors; Co-ordinate system; Extract informations; Geometrical transformation; Image processing technique; Optical imagery; Remote sensing data; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85053638035"
"Zheng G.; Li X.; Liu B.","Zheng, Gang (56919043600); Li, Xiaofeng (55955831200); Liu, Bin (56166930900)","56919043600; 55955831200; 56166930900","AI-Based Remote Sensing Oceanography - Image Classification, Data Fusion, Algorithm Development and Phenomenon Forecast","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899779","7940","7943","3","10.1109/IGARSS.2019.8899779","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077678124&doi=10.1109%2fIGARSS.2019.8899779&partnerID=40&md5=9d7ce28f4d7af0ad2141543f44ff70e8","In the past few years, artificial intelligent (AI) technology has been widely used in many research fields for big data information mining and shown great potential applications in computer vision, natural language processing, bioinformatics, among others. In the area of remote sensing oceanography, we categorize its applications in four major categories: image classification, data fusion, algorithm development and oceanic phenomenon forecast. In this paper, we present two examples to demonstrate such applications. In the first example we applied a well-studied AI framework, U-Net, to a NASA JPL's UAVSAR Synthetic Aperture Radar (SAR) image to classify coastal zone types in the Gulf Coast of USA. In the second example, we trained the AI framework, LSTM model, using the time series of blended microwave Sea Surface Temperature (SST) data, and made the equatorial SST pattern forecast. Validation studies in both cases showed the robustness of AI-based technology for oceanography research. © 2019 IEEE.","Artificial intelligence; Classification (of information); Data mining; Forecasting; Geology; Image classification; Image fusion; Long short-term memory; NASA; Natural language processing systems; Oceanography; Surface waters; Synthetic aperture radar; Algorithm development; Artificial intelligent; Data informations; ITS applications; LSTM; NAtural language processing; Sea surface temperature (SST); Synthetic aperture radar (SAR) images; Remote sensing","AI; LSTM; remote sensing; SAR; SST; U-Net","Conference paper","Final","","Scopus","2-s2.0-85077678124"
"Li S.; Wang L.","Li, Siyu (57194692983); Wang, Ling (57022802900)","57194692983; 57022802900","Synthetic aperture radar and color visible images fusion algorithm based on region division strategy","2017","Journal of Applied Remote Sensing","11","4","045012","","","","10.1117/1.JRS.11.045012","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042593363&doi=10.1117%2f1.JRS.11.045012&partnerID=40&md5=78838bf470dfc130ff34367bc9bcd427","Multisensor image fusion can be used as an advanced technique for image enhancement. A fusion method based on region division strategy is proposed for synthetic aperture radar (SAR) and color visible images. The SAR target feature detection function is first constructed to divide the images into the SAR target region and non-SAR target region. Meanwhile, SAR image despeckling is considered during the generation of this detection function. Then, two different fusion rules are designed for these two regions. More specifically, the fusion rule adopted in the SAR target region is fusing the high-frequency feature within the SAR image, and the fusion rule employed in the non-SAR target region is maintaining the spectral and detail information of the color visible image. Experimental results on several pairs of SAR and color visible images demonstrate the effectiveness of the proposed method. Compared with the conventional image fusion methods, the proposed method provides better results for all evaluation criteria, including spectral distortions, feature similarity, mutual information, and peak signal-to-noise ratio. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","Color; Feature extraction; Image compression; Image enhancement; Image fusion; Signal detection; Signal to noise ratio; Synthetic aperture radar; Tracking radar; Wavelet transforms; Feature similarities; Multisensor image fusion; Peak signal to noise ratio; Region division; SAR image despeckling; Spectral distortions; Stationary wavelet transforms; Visible image; Radar imaging","image fusion; region division strategy; stationary wavelet transform; synthetic aperture radar and color visible images; synthetic aperture radar target feature detection function","Article","Final","","Scopus","2-s2.0-85042593363"
"Neetu; Ray S.S.","Neetu (57205241308); Ray, Shibendu Shankar (7402323955)","57205241308; 7402323955","Evaluation of different approaches to the fusion of Sentinel -1 SAR data and Resourcesat 2 LISS III optical data for use in crop classification","2020","Remote Sensing Letters","11","12","","1157","1166","9","10.1080/2150704X.2020.1832278","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097170408&doi=10.1080%2f2150704X.2020.1832278&partnerID=40&md5=d02bf0461cbe19619cb8ca0f70eb1979","This study evaluates various combinations of data fusion techniques at Pixel, Feature, and Decision level for crop classification using Sentinel-1 Synthetic Aperture Radar (SAR) data and Resourcesat-2 LISS (Linear Imaging Self Scanning) III, optical data for Yadgir District of Karnataka, India. For Pixel level data fusion, techniques such as brovey transformation (BT), principal component analysis (PCA), multiplicative transformation (MLT), and wavelet with IHS (intensity-hue-saturation) were used. Results were compared between different fusion techniques visually, statistically (using universal image quality index), and through image classification (Rule-based and Maximum likelihood) for major crops (Rice, Cotton, and Pigeon pea) in the area. The estimated crop area for all three major crops was compared with the Government statistics. Among the four pixel-level fusion techniques used, the wavelet method performed best in retaining the image quality. However, the study showed that using the feature-level fusion technique, maximum accuracy was obtained for Rice crop. In contrast, the decision-level fusion improved the efficiency for other crops (Cotton and Pigeon pea). © 2020 Informa UK Limited, trading as Taylor & Francis Group.","India; Karnataka; Cajanus cajan; Gossypium hirsutum; Cotton; Crops; Image quality; Maximum likelihood; Metadata; Pixels; Radar imaging; Synthetic aperture radar; Crop classification; Data fusion technique; Decision level fusion; Feature level fusion; Government statistics; Image quality index; Intensity hue saturations; Maximum accuracies; decision making; image classification; LISS; pixel; principal component analysis; Resourcesat; Sentinel; synthetic aperture radar; wavelet analysis; Image fusion","","Article","Final","","Scopus","2-s2.0-85097170408"
"Qin J.; Qu H.; Chen H.; Chen W.","Qin, Jitao (57189233227); Qu, Haicheng (55567658100); Chen, Hao (57192534817); Chen, Wen (57207883940)","57189233227; 55567658100; 57192534817; 57207883940","Joint detection of airplane targets based on sar images and optical images","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900167","1366","1369","3","10.1109/IGARSS.2019.8900167","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114043130&doi=10.1109%2fIGARSS.2019.8900167&partnerID=40&md5=e90e1fcc53ba5245bf302e75aca4062d","In airplane target detection, there was the drawback of weak recognition ability for dark targets and high false alarm rate for detected targets. In order to address the problem, we proposed a detection method based on SAR and optical image feature fusion. It extracted texture, moment and backscattering characteristics from SAR images and combined with optical features. Moreover, the novel airplane edge templates incorporating SAR and optical images were created to acquire saliency map. During the process of detection, first, the saliency map and the One-Class-SVM (OCSVM) classifier were used to initially recognize the suspected airplane targets. Then, the combination features were adopted to further identify the misidentified airplane target. The experimental results showed that the Precision of the proposed method was 61.82% and the False Alarm Rate was 20%, which was better than the HIS-based detection method. ©2019 IEEE","Aircraft; Aircraft detection; Errors; Geometrical optics; Image resolution; Remote sensing; Synthetic aperture radar; Textures; Airplane targets; Detection methods; False alarm rate; Joint detection; One class-SVM; Optical features; Optical image; Recognition abilities; Radar imaging","Airplane Object Detection; Image fusion; Optical Image; SAR Image","Conference paper","Final","","Scopus","2-s2.0-85114043130"
"Wang F.; Wu Y.; Zhang P.; Zhang Q.; Li M.","Wang, Fan (56299935600); Wu, Yan (7406895902); Zhang, Peng (55205949900); Zhang, Qingjun (55092505400); Li, Ming (56937290000)","56299935600; 7406895902; 55205949900; 55092505400; 56937290000","Unsupervised SAR Image Segmentation Using Ambiguity Label Information Fusion in Triplet Markov Fields Model","2017","IEEE Geoscience and Remote Sensing Letters","14","9","8004442","1479","1483","4","10.1109/LGRS.2017.2715223","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029166552&doi=10.1109%2fLGRS.2017.2715223&partnerID=40&md5=6e320ab74215b914c30e89044a406d71","The recently proposed triplet Markov fields (TMF) model enhances the nonstationary image prior modeling ability by introducing an auxiliary field. Motivated by the TMF model, we propose a generalized TMF model based on ambiguity label information fusion (ALF-TMF) for synthetic aperture radar (SAR) image segmentation. The redefined auxiliary field in ALF-TMF indicates the dominant direction of local image contents and gives explicit nonstationary divisions of SAR images. To reduce the influence of unreliable observations caused by speckle noise, the original label field is adaptively generalized by introducing ambiguity class based on image observation and local nonstationary contextual information. Given the extended label field, prior and likelihood terms are constructed and merged to provide the posterior segmentation decision via the Bayesian fusion rule. Real SAR images are utilized in the experimental analysis, and the effectiveness of the proposed method is validated accordingly. © 2017 IEEE.","Image fusion; Information fusion; Markov processes; Radar; Radar imaging; Speckle; Synthetic aperture radar; Bayes method; Bayesian fusion; Field extensions; Indexes; Nonstationary; Triplet Markov fields; Uncertainty; Image segmentation","Ambiguity label field extension; Bayesian fusion; nonstationary division; synthetic aperture radar (SAR) image segmentation; triplet Markov fields (TMF)","Article","Final","","Scopus","2-s2.0-85029166552"
"Zhou L.; Zhang X.; Wang Y.; Li L.; Pu L.; Shi J.; Wei S.","Zhou, Liming (57202955618); Zhang, Xiaoling (52265071000); Wang, Yangyang (57197876827); Li, Liang (57196179893); Pu, Liming (57202535654); Shi, Jun (57104066400); Wei, Shunjun (54586127400)","57202955618; 52265071000; 57197876827; 57196179893; 57202535654; 57104066400; 54586127400","Unambiguous Reconstruction for Multichannel Nonuniform Sampling SAR Signal Based on Image Fusion","2020","IEEE Access","8","","9064772","71558","71571","13","10.1109/ACCESS.2020.2987196","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084266424&doi=10.1109%2fACCESS.2020.2987196&partnerID=40&md5=2840220abbe7b0bb65d6689626c0ac00","Multichannel signal processing in azimuth is a vital technique to enable a wide-swath Synthetic Aperture Radar (SAR) with high azimuth resolution. However, the multichannel high-resolution and wide-swath (HRWS) SAR system always suffers from the problem of the azimuth nonuniform sampling resulting in the image ambiguity, when it does not satisfy the uniform sampling condition. In this paper, to suppress the azimuth image ambiguity, we propose a novel unambiguous reconstruction method based on image fusion. During this reconstruction processing, the Back Projection (BP) algorithm is first utilized for SAR imaging to obtain the designed sub-images. Then, the reconstruction expression is derived as the summation of the sub-images weighted by the interpolation coefficient. This method integrates the reconstruction into the imaging process and the image fusion makes the procedure simple. In addition, the interpolation period, which affects the reconstruction image quality and efficiency, is further analyzed. Moreover, as the curved trajectory platform brings more challenges for the unambiguous reconstruction, the performance of the proposed method applied to the curved trajectory platform is studied. Finally, experimental results clearly verify the effectiveness of the proposed method for ambiguity suppression and demonstrate its applicability to the curved trajectory. © 2013 IEEE.","Image fusion; Image reconstruction; Interpolation; Radar imaging; Synthetic aperture radar; Trajectories; Azimuth resolution; Backprojection algorithms; High resolution and wide swath (HRWS); Interpolation coefficients; Multi-channel signal processing; Nonuniform sampling; Reconstruction image quality; Unambiguous reconstruction; Image sampling","High resolution and wide swath; nonuniform sampling; signal reconstruction; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084266424"
"Lou X.; Jia Z.; Yang J.; Kasabov N.","Lou, Xuemei (57207761526); Jia, Zhenhong (55858535700); Yang, Jie (15039078800); Kasabov, Nikola (35585005300)","57207761526; 55858535700; 15039078800; 35585005300","Change detection in SAR images based on the ROF model semi-implicit denoising method","2019","Sensors (Switzerland)","19","5","1179","","","","10.3390/s19051179","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062843671&doi=10.3390%2fs19051179&partnerID=40&md5=23ad656e7da233b827b0f694d19e44fa","The explicit solution of the traditional ROF model in image denoising has the disadvantages of unstable results and requiring many iterations. To solve the problem, a new method, ROF model semi-implicit denoising, is proposed in this paper and applied to change detections of synthetic aperture radar (SAR) images. All remote sensing images used in this article have been calibrated by ENVI software. First, the ROF model semi-implicit denoising method is used to denoise the remote sensing images. Second, for the denoised images, difference images are obtained by the logarithmic ratio and mean ratio methods. The final difference image is obtained by principal component analysis fusion (PCA fusion) of the two difference images. Finally, the final difference image is clustered by fuzzy local information C-means clustering (FLICM) to obtain the change regions. The research results show that the proposed method has high detection accuracy and time operation efficiency © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Image fusion; Principal component analysis; Radar imaging; Remote sensing; Synthetic aperture radar; Tracking radar; Change detection; Detection accuracy; Explicit solutions; FLICM; Operation efficiencies; Remote sensing images; Semi-implicit; Synthetic aperture radar (SAR) images; Image denoising","Change detection; FLICM; PCA fusion; Remote sensing image; ROF model semi-implicit denoising","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85062843671"
"Salentinig A.; Gamba P.","Salentinig, A. (56422257700); Gamba, P. (7007165803)","56422257700; 7007165803","Data- and decision-level fusion for classification","2017","Comprehensive Remote Sensing","1-9","","","134","155","21","10.1016/B978-0-12-409548-9.10339-2","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074659533&doi=10.1016%2fB978-0-12-409548-9.10339-2&partnerID=40&md5=60aefd555b20217589d7d9e899ea5e81","This chapter deals with remote sensing data fusion in the context of landuse/landcover classification. The basic concepts of Earth Observation (EO) data fusion are outlined and a number of pixel- and decision-level image fusion approaches are presented with the help of illustrative examples from the field of urban area mapping. The discussed methods are evaluated with respect to their general strengths and weaknesses, input data requirements, complexity, and computational cost. Eventually, a few general recommendations for the selection of appropriate data fusion methods are given. © 2018 Elsevier Inc. All rights reserved.","","Classification; Data fusion; Decision-level fusion; Multisource combination; Multispectral data; Pixel-level fusion; Synthetic aperture radar data","Book chapter","Final","","Scopus","2-s2.0-85074659533"
"Kwak Y.-J.; Pelich R.","Kwak, Young-Joo (54408878500); Pelich, Ramona (56155778100)","54408878500; 56155778100","Fractional Floodwater-Pixel Fusion for Emergency Response Using ALOS-2 and Sentinel-1 Data","2019","IEEE Aerospace Conference Proceedings","2019-March","","8741708","","","","10.1109/AERO.2019.8741708","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068339113&doi=10.1109%2fAERO.2019.8741708&partnerID=40&md5=13c2b05f2c2f5b4f7c27cf78a995641a","Emergency hazard and risk mapping services are crucial in providing rapid-response information for disaster management and reducing damage of water-driven cascading disasters such as floods. For rapid-response flood mapping, this study introduces an improved flood detection algorithm to fuse two different C-band and L-band Synthetic Aperture Radar (SAR)images acquired over the same area of interest, e.g., mega-floodplain. The main objective of this study is to propose a new algorithm for dynamic flood detection using two different SAR images acquired at different times and under different conditions. As an image fusion technique, we propose an improved floodwater detection algorithm using the pixel-based water fractional fusion and wavelet-based image fusion. This approach allows investigating the optimization of fusion parameters from two different products in the case of a pre-monsoon flash flood. This preliminary study was conducted to identify and estimate large-scale flood inundation area over a challenging area, where flash flood events often occur along the Maghna River in northen Bangladesh. The resultant fused map of the maximum flood-detect extent suggested the possibility of a rapid and accurate floodwater detection to identify the flood location and extent area with the validation data from ground-truth survey in the representative experiment sites. © 2019 IEEE.","Disaster prevention; Flood control; Floods; Image acquisition; Image enhancement; Image fusion; Mapping; Pixels; Radar imaging; Signal detection; Synthetic aperture radar; Detection algorithm; Emergency response; Flash flood; Floodwater fraction; Image fusion techniques; L-band synthetic aperture radars; Wavelet based image fusion; Wavelet image; Emergency services","emergency response; flash flood; Floodwater fraction; SAR; wavelet image fusion","Conference paper","Final","","Scopus","2-s2.0-85068339113"
"Zhang Y.; Yuan H.; Tan Q.; Lu Y.; Liu C.","Zhang, Yun (56097923300); Yuan, Haoxuan (57216438510); Tan, Qimeng (36816552800); Lu, Yadong (57851294200); Liu, Can (57216432130)","56097923300; 57216438510; 36816552800; 57851294200; 57216432130","Research on Spatial Target Recognition Method Based on Multi-source Sensor Fusion","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048272","","","","10.1109/APSAR46974.2019.9048272","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083493266&doi=10.1109%2fAPSAR46974.2019.9048272&partnerID=40&md5=66ae2193feb2b71330d8213c06d10963","Radar, optical and infrared sensors are the main loads in space-based detection system. In this paper, multisensor information fusion method is applied to space target recognition. SVM, RFM and CNN is used in the recognition method. In this paper, the feature level fusion method of multisensor image is compared with extracting the features without fusion. The advantage of fusion for improving the recognition success rate is highlighted. Finally, the D-S evidence theory method is used in decision level fusion. © 2019 IEEE.","Decision theory; Information fusion; Infrared detectors; Synthetic aperture radar; D S evidence theory; Decision level fusion; Feature level fusion; Infra-red sensor; Multi sensor images; Multi-sensor information fusion; Recognition methods; Target recognition; Space-based radar","D-S evidence theory; feature-level image fusion; multi-source sensor information fusion; spatial target recognition","Conference paper","Final","","Scopus","2-s2.0-85083493266"
"Hu J.; Hong D.; Wang Y.; Zhu X.X.","Hu, Jingliang (57192207722); Hong, Danfeng (56108179600); Wang, Yuanyuan (38663687700); Zhu, Xiao Xiang (55696622200)","57192207722; 56108179600; 38663687700; 55696622200","A Topological Data Analysis Guided Fusion Algorithm: Mapper-Regularized Manifold Alignment","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898471","2822","2825","3","10.1109/IGARSS.2019.8898471","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077712529&doi=10.1109%2fIGARSS.2019.8898471&partnerID=40&md5=6534ff4ad562e99605795c4eccb31a5b","Hyperspectral images and polarimetric synthetic aperture radar (PolSAR) data are two important data sources, yet they barely appear under the same scope, even though multi-modal data fusion is attracting more and more attention. To our best knowledge, this paper investigates for the first time semi-supervised manifold alignment (SSMA) for the fusion of the hyperspectral image and PolSAR data. The SSMA searches a latent space where different data sources are aligned, which is accomplished by using the label information and the topological structure of the data. This paper is the first attempt to apply topological data analysis (TDA), a recent mathematic sub-field of data analysis, in remote sensing. It aims to reveal relevant information from the shape of a data in its feature space, and has been proven powerful in medicine. The paper also proposes a novel algorithm, MAPPER-regularized manifold alignment, which embeds the TDA into a semi-supervised manifold alignment for the fusion of the hyper-spectral image and PolSAR data. The proposed algorithm exhibits superior performance in fusing a simulated EnMAP data set and a Sentinel-1 data set for an image of Berlin. © 2019 IEEE.","Alignment; Classification (of information); Data fusion; Data handling; Geology; Hyperspectral imaging; Information analysis; Land use; Modal analysis; Remote sensing; Semi-supervised learning; Space optics; Space-based radar; Spectroscopy; Synthetic aperture radar; Topology; EnMAP; Land cover; Manifold alignments; MAPPER; PolSAR; Sentinel-1; Topological data analysis; Image fusion","Classification; data fusion; EnMAP; hyperspectral image; land cover; land use; manifold alignment; MAPPER; PolSAR; semi-supervised learning; Sentinel-1; topological data analysis (TDA)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077712529"
"Wang W.; Wu J.; Yang X.; Miao Y.; Yang J.; Yang H.","Wang, Wenjing (57197769120); Wu, Junjie (55713990900); Yang, Xiaqing (55602675800); Miao, Yuxuan (57204158293); Yang, Jianyu (9239230100); Yang, Haiguang (23971854200)","57197769120; 55713990900; 55602675800; 57204158293; 9239230100; 23971854200","Multistatic SAR information fusion based on image registration and fake color synthesis","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518875","7275","7278","3","10.1109/IGARSS.2018.8518875","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064161901&doi=10.1109%2fIGARSS.2018.8518875&partnerID=40&md5=2b4a3773f622da4f319d6fe1731bd3e2","Due to finite of information dimension of single bistatic synthetic aperture radar (SAR), to expand more information about ground objects we research multistatic SAR which can be decomposed into groups of bistatic SAR. It is known that scattering properties of the different viewing angles is different. In this paper, we focus on system of single transmitter and triple receivers and use polar format algorithm(PFA) to obtain images of ground objects for the triple receivers. A geometric distortion correction method is proposed due to elevation of ground objects. After the distortion correction, the three SAR images are registrated, then we put images into red, green, blue(RGB) channels respectively to realize fake color synthesis, and thus realize information fusion. © 2018 IEEE.","Color; Geology; Image fusion; Image processing; Information fusion; Multistatic radars; Remote sensing; Synthetic aperture radar; Bistatic synthetic aperture radar; Color synthesis; Distortion correction; Geometric distortion; Information dimensions; Multistatic SAR; Polar format algorithm; Scattering property; Radar imaging","Fake color; Geometric distortion; Information fusion; Multistatic SAR","Conference paper","Final","","Scopus","2-s2.0-85064161901"
"Auer S.; Reinartz P.; Schmitt M.","Auer, Stefan (57216043589); Reinartz, Peter (56216874200); Schmitt, Michael (7401931279)","57216043589; 56216874200; 7401931279","Object-Related Alignment of Heterogeneous Image Data in Remote Sensing","2018","2018 21st International Conference on Information Fusion, FUSION 2018","","","8455364","1607","1614","7","10.23919/ICIF.2018.8455364","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054082151&doi=10.23919%2fICIF.2018.8455364&partnerID=40&md5=204cf944a39c665bfea959f58ce5030d","The fusion of heterogeneous image data, in particular optical images and synthetic aperture radar (SAR) images, is highly worthwhile in the context of remote sensing tasks as it allows to exploit complementary information - such as spectral and distance measurements or different observation perspectives - of the two data sources while diminishing their individual weaknesses (e.g. cloud cover, difficulty of image interpretation, limited sensor revisit). However, relating the heterogeneous data on the signal level requires a data alignment step, which cannot be realized without auxiliary knowledge. This paper addresses and discusses this fundamental fusion problem in remote sensing in the context of a framework named SimGeoI, which solves the multi-sensor alignment task based on geometric knowledge from existing digital surface models. Sections of optical and SAR images are related to individual objects using interpretation layers generated with ray tracing techniques. Results of SimGeoI are presented for a test site in London in order to motivate an object-related fusion of remote sensing images. © 2018 ISIF","Image fusion; Information fusion; Radar imaging; Ray tracing; Sensor data fusion; Synthetic aperture radar; Digital surface models; Geometric knowledge; Heterogeneous data; Image interpretation; Individual objects; Ray-tracing technique; Remote sensing images; Synthetic aperture radar (SAR) images; Remote sensing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054082151"
"Liu S.; Ming L.; Shi M.; Qi X.; Qi H.","Liu, Shuaiqi (55055535100); Ming, Liu (56603389700); Shi, Mingzhu (43861823600); Qi, Xin (57197737972); Qi, Hu (57197733687)","55055535100; 56603389700; 43861823600; 57197737972; 57197733687","SAR image de-noising based on nuclear norm minimization fusion algorithm","2018","Lecture Notes in Electrical Engineering","423","","","193","201","8","10.1007/978-981-10-3229-5_21","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034232225&doi=10.1007%2f978-981-10-3229-5_21&partnerID=40&md5=7216a8ae270782c43c23b7943a54b4d7","Synthetic aperture radar (SAR) images play a quite important role in military and environmental monitoring. But the SAR image was greatly affected by coherent noise, which affects its application in the subsequent image analysis. In most of the SAR image de-noising algorithms in hand, the same operation is applied to the whole SAR image, which leads to artificial texture or edge blur. In order to overcome this shortcoming, this paper proposed a new SAR image de-noising method based on nuclear norm minimization (NNM) fusion algorithm. The noisy SAR image is de-noised by two different algorithms, and two de-noising images are fused to final de-noising image based on nuclear norm minimization fusion algorithm. Experimental results show that the proposed algorithm not only effectively improves the visual effect and objective indicators of de-noising image but preserves the local structure of the image better. © Springer Nature Singapore Pte Ltd. 2018.","Edge detection; Image denoising; Image enhancement; Image fusion; Military photography; Signal processing; Synthetic aperture radar; Coherent noise; Environmental Monitoring; Fusion algorithms; ITS applications; Local structure; Nuclear norm minimizations; Synthetic aperture radar (SAR) images; Visual effects; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85034232225"
"Zhang P.; Li M.; Song W.; Wu Y.; An L.","Zhang, Peng (55205949900); Li, Ming (56937290000); Song, Wanying (56047346700); Wu, Yan (7406895902); An, Lin (56013282600)","55205949900; 56937290000; 56047346700; 7406895902; 56013282600","Triplet Hybrid Discriminative Fields Model Based on Bayesian Fusion for Unsupervised Nonstationary SAR Image Multiclass Segmentation","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","12","8516305","4848","4861","13","10.1109/JSTARS.2018.2875935","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055866929&doi=10.1109%2fJSTARS.2018.2875935&partnerID=40&md5=a33b1122456e3a3d5bf5b47245f9ae1d","The discriminative random fields (DRF) model is suitable for analyzing images with complex textural structures and has achieved promising results in image segmentation. However, the DRF model does not consider the nonstationarity of synthetic aperture radar (SAR) images and lacks the ability to model SAR scattering statistics in nonstationary SAR image segmentation. In this paper, we propose a triplet hybrid discriminative random fields (THDF) model based on Bayesian fusion. According to its semantic structure, the THDF model belongs to hybrid discriminative models, and it provides the following promising contributions to nonstationary SAR image segmentation while inheriting the advantages of the discriminative models: first, it takes the nonstationarity of SAR images into account from the perspective of their texton appearances, and thus regulates the local label interaction patterns and considers the distribution differences of the congeneric image features in different stationary parts; and second, for nonstationary SAR images, it performs a fusion-Type treatment of the nonstationary textural features and the SAR scattering statistics based on Bayesian fusion and, thus, captures the nonstationary information from SAR data in a more complete manner. The effectiveness of the proposed model is demonstrated through applications to both synthetic images and real SAR image segmentations. © 2008-2012 IEEE.","Analytical models; Data structures; Hidden Markov models; Image fusion; Image segmentation; Scattering; Semantics; Synthetic aperture radar; Bayes method; Bayesian fusion; Discriminative random fields; Multi-class segmentations; Non-stationary properties; image analysis; remote sensing; satellite data; segmentation; synthetic aperture radar; Radar imaging","Bayesian fusion; nonstationary property; synthetic aperture radar (SAR) image multiclass segmentation; triplet hybrid discriminative random fields (THDF)","Article","Final","","Scopus","2-s2.0-85055866929"
"Hughes L.H.; Auer S.; Schmitt M.","Hughes, L.H. (57201113391); Auer, S. (57216043589); Schmitt, M. (7401931279)","57201113391; 57216043589; 7401931279","INVESTIGATION of JOINT VISIBILITY between SAR and OPTICAL IMAGES of URBAN ENVIRONMENTS","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","2","","129","136","7","10.5194/isprs-annals-IV-2-129-2018","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048414576&doi=10.5194%2fisprs-annals-IV-2-129-2018&partnerID=40&md5=fe24bec38f35be0640afee52ca22d150","In this paper, we present a work-flow to investigate the joint visibility between very-high-resolution SAR and optical images of urban scenes. For this task, we extend the simulation framework SimGeoI to enable a simulation of individual pixels rather than complete images. Using the extended SimGeoI simulator, we carry out a case study using a TerraSAR-X staring spotlight image and a Worldview-2 panchromatic image acquired over the city of Munich, Germany. The results of this study indicate that about 55 % of the scene are visible in both images and are thus suitable for matching and data fusion endeavours, while about 25 % of the scene are affected by either radar shadow or optical occlusion. Taking the image acquisition parameters into account, our findings can provide support regarding the definition of upper bounds for image fusion tasks, as well as help to improve acquisition planning with respect to different application goals. © 2018 Copernicus GmbH. All rights reserved.","","data fusion; feature visibility; optical remote sensing; Synthetic aperture radar (SAR)","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048414576"
"Dong Q.; Sun G.; Yang Z.; Zuo S.; Xing M.","Dong, Qi (57201600579); Sun, Guangcai (26424042000); Yang, Zemin (56051865100); Zuo, Shaoshan (56740294400); Xing, Mengdao (7005922869)","57201600579; 26424042000; 56051865100; 56740294400; 7005922869","Cartesian coordinates factorized back-projection algorithm for spotlight SAR","2016","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","38","6","","1482","1488","6","10.11999/JEIT150990","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975780618&doi=10.11999%2fJEIT150990&partnerID=40&md5=7d07cc9fd2419e31e57f94f96766e1f8","The Fast Factorized Back-Projection Algorithm (FFBPA) can reconstruct images in low sampling rate in Local Polar Coordinates (LPC). However, massive 2 dimensional image interpolations are required in image fusion from different LPCs. Image fusion is much easier in Cartesian Coordinates (CC), whereas, the Nyquist sampling rate of images in CC is higher, resulting in decline in the efficiency. To solve this problem, a spectrum compressing method is proposed. By compressing in range-time domain and range-frequency domain, the azimuth spectrum is greatly compressed. The image quality of the proposed method is similar to that of Back-Projection Algorithm (BPA) and is superior to that FFBPA. This method can also be used in SAR of nonlinear track. In the end, the validity of this method is proved by spaceborne SAR simulation data of 0.1 m resolution and airborne SAR real data of 0.2 m resolution. © 2016, Science Press. All right reserved.","Frequency domain analysis; Image fusion; Radar imaging; Synthetic aperture radar; Time domain analysis; 2 - Dimensional; Backprojection algorithms; Cartesian coordinate; Frequency domains; Image interpolations; Nyquist sampling rate; Polar coordinate; Space-borne SAR; Algorithms","Back-Projection Algorithm (BPA); Cartesian coordinates Factorized Back-Projection Algorithm (CFBPA); Fast Factorized Back-Projection Algorithm (FFBPA); SAR","Article","Final","","Scopus","2-s2.0-84975780618"
"Zhai A.; Wen X.; Xu H.; Yuan L.; Meng Q.","Zhai, Aobo (58041356700); Wen, Xianbin (57760399600); Xu, Haixia (55611885900); Yuan, Liming (55480260300); Meng, Qingxia (24765375000)","58041356700; 57760399600; 55611885900; 55480260300; 24765375000","Multi-Layer Model Based on Multi-Scale and Multi-Feature Fusion for SAR Images","2017","Remote Sensing","9","10","1085","","","","10.3390/rs9101085","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046950570&doi=10.3390%2frs9101085&partnerID=40&md5=8b2edeb30dbd8dae92a81480e7f0f878","A multi-layer classification approach based on multi-scales and multi-features (ML–MFM) for synthetic aperture radar (SAR) images is proposed in this paper. Firstly, the SAR image is partitioned into superpixels, which are local, coherent regions that preserve most of the characteristics necessary for extracting image information. Following this, a new sparse representation-based classification is used to express sparse multiple features of the superpixels. Moreover, a multi-scale fusion strategy is introduced into ML–MFM to construct the dictionary, which allows complementation between sample information. Finally, the multi-layer operation is used to refine the classification results of superpixels by adding a threshold decision condition to sparse representation classification (SRC) in an iterative way. Compared with traditional SRC and other existing methods, the experimental results of both synthetic and real SAR images have shown that the proposed method not only shows good performance in quantitative evaluation, but can also obtain satisfactory and cogent visualization of classification results. © 2017 by the authors.","Classification (of information); Image classification; Image fusion; Iterative methods; Radar imaging; Synthetic aperture radar; Classification results; Multi-feature fusion; Multi-layers; Multi-scale features; Multi-scales; Multilayer structures; Sparse representation; Sparse representation classification; Super pixels; Synthetic aperture radar images; Superpixels","multi-feature fusion; multi-layer structure; multi-scale; SAR image; sparse representation classification (SRC)","Letter","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85046950570"
"Salvetti F.; Martorella M.; Giusti E.; Stagliano D.","Salvetti, Federica (43861767500); Martorella, Marco (6603185380); Giusti, Elisa (55388839300); Stagliano, Daniele (55548375100)","43861767500; 6603185380; 55388839300; 55548375100","Multiview Three-Dimensional Interferometric Inverse Synthetic Aperture Radar","2019","IEEE Transactions on Aerospace and Electronic Systems","55","2","8451947","718","733","15","10.1109/TAES.2018.2864469","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052665426&doi=10.1109%2fTAES.2018.2864469&partnerID=40&md5=67a7206fbe3841a10d8bb5f618974fff","Three-dimensional (3D) inverse synthetic aperture radar (ISAR) imaging has been proven feasible by combining traditional ISAR imaging and interferometry. Such technique, namely Inteferometric ISAR (InISAR), allows for the main target scattering centers to be mapped into a 3-D spatial domain, therefore forming 3-D images under the form of 3-D point clouds. 3-D InISAR overcomes some main limitations of traditional 2-D ISAR imaging, such as the problem of cross-range scaling and unknown image projection plane. Despite the great advantage of 3-D imaging over traditional 2-D imaging, some issues remain, such as scatterer scintillation, shadowing effects, poor SNR, etc., which limit the effectiveness of 3-D imaging. A solution to these issues can be found in the use of multiple 3-D views, which can be obtained exploiting either multitemporal or multiperspective configurations or a combination of both. This paper proposes this concept and develops the image fusion algorithms that are necessary to produce multiview 3-D ISAR images. The effectiveness of the proposed technique is tested by using real data collected with a multistatic InISAR system. © 1965-2011 IEEE.","Image fusion; Imaging systems; Interferometry; Inverse problems; Inverse synthetic aperture radar; 3-D radar imaging; Image fusion algorithms; Interferometric inverse synthetic aperture radars; Inverse synthetic aperture radars (ISAR); Multi-perspective; Multi-views; Radar interferometry; Threedimensional (3-d); Radar imaging","3-D Radar Imaging; Inverse synthetic aperture radar (ISAR); multi-view radar Imaging; radar interferometry; three-dimensional (3D) incoherent image fusion","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85052665426"
"Fernandez-Beltran R.; Haut J.M.; Paoletti M.E.; Plaza J.; Plaza A.; Pla F.","Fernandez-Beltran, Ruben (55838551300); Haut, Juan M. (57215636081); Paoletti, Mercedes E. (57027389000); Plaza, Javier (57195716301); Plaza, Antonio (7006613644); Pla, Filiberto (7006504936)","55838551300; 57215636081; 57027389000; 57195716301; 7006613644; 7006504936","Remote Sensing Image Fusion Using Hierarchical Multimodal Probabilistic Latent Semantic Analysis","2018","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","12","8550740","4982","4993","11","10.1109/JSTARS.2018.2881342","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057851736&doi=10.1109%2fJSTARS.2018.2881342&partnerID=40&md5=13eb9e6ce61d5c73f7dda15a892467f7","The generative semantic nature of probabilistic topic models has recently shown encouraging results within the remote sensing image fusion field when conducting land cover categorization. However, standard topic models have not yet been adapted to the inherent complexity of remotely sensed data, which eventually may limit their resulting performance. In this scenario, this paper presents a new topic-based image fusion framework, specially designed to fuse synthetic aperture radar (SAR) and multispectral imaging (MSI) data for unsupervised land cover categorization tasks. Specifically, we initially propose a hierarchical multi-modal probabilistic latent semantic analysis (HMpLSA) model that takes advantage of two different vocabulary modalities, as well as two different levels of topics, in order to effectively uncover intersensor semantic patterns. Then, we define an SAR and MSI data fusion framework based on HMpLSA in order to perform unsupervised land cover categorization. Our experiments, conducted using three different SAR and MSI data sets, reveal that the proposed approach is able to provide competitive advantages with respect to standard clustering methods and topic models, as well as several multimodal topic model variants available in the literature. © 2008-2012 IEEE.","Competition; Data structures; Image analysis; Modal analysis; Probabilistic logics; Radar imaging; Remote sensing; Semantics; Statistics; Synthetic aperture radar; Adaptation models; Competitive advantage; Inherent complexity; Multispectral imaging; Probabilistic latent semantic analysis; Probabilistic topic models; Remote sensing images; Remotely sensed data; image processing; multispectral image; probability; remote sensing; synthetic aperture radar; Image fusion","Image fusion; multispectral imaging (MSI); probabilistic latent semantic analysis (pLSA); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85057851736"
"Ma X.; Hu S.; Yang D.","Ma, Xiaole (57193220596); Hu, Shaohai (7404286949); Yang, Dongsheng (57193743284)","57193220596; 7404286949; 57193743284","SAR Image De-noising Based on Residual Image Fusion and Sparse Representation","2019","KSII Transactions on Internet and Information Systems","37","7","","3620","3637","17","10.3837/tiis.2019.07.016","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073226148&doi=10.3837%2ftiis.2019.07.016&partnerID=40&md5=b019cb5d592edf0df4b26e37a0cecdad","Since the birth of Synthetic Aperture Radar (SAR), it has been widely used in the military field and so on. However, the existence of speckle noise makes a good deal inconvenience for the subsequent image processing. The continuous development of sparse representation (SR) opens a new field for the speckle suppressing of SAR image. Although the SR de-noising may be effective, the over-smooth phenomenon still has bad influence on the integrity of the image information. In this paper, one novel SAR image de-noising method based on residual image fusion and sparse representation is proposed. Firstly we can get the similar block groups by the non-local similar block matching method (NLS-BM). Then SR de-noising based on the adaptive K-means singular value decomposition (K-SVD) is adopted to obtain the initial de-noised image and residual image. The residual image is processed by Shearlet transform (ST), and the corresponding de-noising methods are applied on it. Finally, in ST domain the low-frequency and high-frequency components of the initial de-noised and residual image are fused respectively by relevant fusion rules. The final de-noised image can be recovered by inverse ST. Experimental results show the proposed method can not only suppress the speckle effectively, but also save more details and other useful information of the original SAR image, which could provide more authentic and credible records for the follow-up image processing. Copyright © 2019 KSII","Image denoising; Image fusion; Inverse problems; Singular value decomposition; Speckle; Synthetic aperture radar; Block matching methods; Continuous development; Denoising methods; High frequency components; Residual images; SAR Images; Shearlet transforms; Sparse representation; Radar imaging","Image fusion; Residual image; SAR image de-noising; Shearlet transform; Sparse representation","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85073226148"
"Zhang Y.; Yao X.; Jiang S.; Yang H.; Nie X.","Zhang, Yue (57219261530); Yao, Xue (56340368000); Jiang, Shirong (57219250732); Yang, Hong (57226173041); Nie, Xiangfei (14063777300)","57219261530; 56340368000; 57219250732; 57226173041; 14063777300","Information Fusion of Optical Image and SAR Image Based on DEM","2019","ICSIDP 2019 - IEEE International Conference on Signal, Information and Data Processing 2019","","","9172959","","","","10.1109/ICSIDP47821.2019.9172959","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091933023&doi=10.1109%2fICSIDP47821.2019.9172959&partnerID=40&md5=6d578d9496c7ec14f862a77c732dc152","Because the optical images obtained in geological hazard monitoring area can't display the Gaussian coordinates visually and the Synthetic Aperture Radar (SAR) images aren't easy to be identified. The Digital Elevation Model (DEM) data and Global Navigation Satellite System (GNSS) monitoring data are applied in this paper. Firstly, interpolation is used in SAR image, and the missing height Gaussian coordinates are obtained by matching with DEM data. Secondly, the cross-range and height Gaussian coordinates of each target point on the optical image are obtained by applying the pixel scale, and the range Gaussian coordinates are got by matching with DEM data. Finally, the fusion information is obtained by matching the SAR image with the optical image. The experimental results show that this method has certain feasibility. It proves that the method can provide simple and effective help for landslide hazard monitoring. © 2019 IEEE.","Data handling; Gaussian distribution; Geometrical optics; Global positioning system; Hazards; Image fusion; Monitoring; Surveying; Synthetic aperture radar; Digital elevation model; Gaussian coordinates; Geological hazards; Global Navigation Satellite Systems; Landslide hazard; Optical image; Synthetic aperture radar (SAR) images; Target point; Radar imaging","DEM data; information fusion; optical image; pixel scale; SAR image","Conference paper","Final","","Scopus","2-s2.0-85091933023"
"Jiang X.; Li G.; Liu Y.; Zhang X.-P.; He Y.","Jiang, Xiao (55599147400); Li, Gang (55547117794); Liu, Yu (57386764900); Zhang, Xiao-Ping (35214025100); He, You (57212448603)","55599147400; 55547117794; 57386764900; 35214025100; 57212448603","Change Detection in Heterogeneous Optical and SAR Remote Sensing Images Via Deep Homogeneous Feature Fusion","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9057421","1551","1566","15","10.1109/JSTARS.2020.2983993","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084467876&doi=10.1109%2fJSTARS.2020.2983993&partnerID=40&md5=fc649c1fc3a34b7ed7e862c94f2e615a","Change detection in heterogeneous remote sensing images is crucial for disaster damage assessment. Recent methods use homogenous transformation, which transforms the heterogeneous optical and synthetic aperture radar (SAR) remote sensing images into the same feature space, to achieve change detection. Such transformations mainly operate on the low-level feature space and may corrupt the semantic content, deteriorating the performance of change detection. To solve this problem, this article presents a new homogeneous transformation model termed deep homogeneous feature fusion (DHFF) based on image style transfer (IST). Unlike the existing methods, the DHFF method segregates the semantic content and the style features in the heterogeneous images to perform homogeneous transformation. The separation of the semantic content and the style in the homogeneous transformation prevents the corruption of image semantic content, especially in the regions of change. In this way, the detection performance is improved with accurate homogeneous transformation. Furthermore, we present a new iterative IST strategy, where the cost function in each IST iteration measures and thus maximizes the feature homogeneity in additional new feature subspaces for change detection. After that, change detection is accomplished accurately on the original and the transformed images that are in the same feature space. Real remote sensing images acquired by SAR and optical satellites are utilized to evaluate the performance of the proposed method. The experiments demonstrate that the proposed DHFF method achieves significant improvement for change detection in heterogeneous optical and SAR remote sensing images in terms of both accuracy rate and Kappa index. © 2008-2012 IEEE.","Cost functions; Damage detection; Feature extraction; Image enhancement; Image fusion; Iterative methods; Remote sensing; Semantics; Space optics; Space-based radar; Synthetic aperture radar; Damage assessments; Detection performance; Feature subspace; Homogeneous transformations; Low-level features; Optical satellites; Remote sensing images; SAR remote sensing; accuracy assessment; detection method; image analysis; optical method; remote sensing; satellite imagery; synthetic aperture radar; Radar imaging","Change detection; heterogeneous; image style transfer (IST); remote sensing","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084467876"
"Ma X.; Hu S.; Liu S.","Ma, Xiaole (57193220596); Hu, Shaohai (7404286949); Liu, Shuaiqi (55055535100)","57193220596; 7404286949; 55055535100","SAR image de-noising based on shift invariant K-SVD and guided filter","2017","Remote Sensing","9","12","1311","","","","10.3390/rs9121311","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038207185&doi=10.3390%2frs9121311&partnerID=40&md5=7ca9511c67f737b760c658d10ec71354","Finding a way to effectively suppress speckle in SAR images has great significance. K-means singular value decomposition (K-SVD) has shown great potential in SAR image de-noising. However, the traditional K-SVD is sensitive to the position and phase of the characteristics in the image, and the de-noised image by K-SVD has lost some detailed information of the original image. In this paper, we present one new SAR image de-noising method based on shift invariant K-SVD and guided filter. The whole method consists of two steps. The first deals mainly with the noisy image with shift invariant K-SVD and obtaining the initial de-noised image. In the second step, we do the guided filtering for the initial de-noised image. Finally, we can recover the final de-noised image. Experimental results show that our method not only has better visual effects and objective evaluation, but can also save more detailed information such as image edge and texture when de-noising SAR images. The presented shift invariant K-SVD can be widely used in image processing, such as image fusion, edge detection and super-resolution reconstruction. © 2017 by the author.","Bandpass filters; Edge detection; Image denoising; Image fusion; Image processing; Singular value decomposition; Synthetic aperture radar; Guided filtering; Guided filters; Objective evaluation; Original images; SAR Images; Shift invariance; Shift invariant; Super resolution reconstruction; Radar imaging","Guided filter; Image de-noising; K-SVD; SAR image; Shift invariance","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85038207185"
"Verma A.; Kumar A.; Lal K.","Verma, Abhinav (57210809234); Kumar, Amit (9244865300); Lal, Kanhaiya (57541263500)","57210809234; 9244865300; 57541263500","Kharif crop characterization using combination of SAR and MSI Optical Sentinel Satellite datasets","2019","Journal of Earth System Science","128","8","230","","","","10.1007/s12040-019-1260-0","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071442100&doi=10.1007%2fs12040-019-1260-0&partnerID=40&md5=f195206b8978afa6c2d81f31fd0c7f1c","In the present study, the differences in the kharif crop reflectance at varied wavelength regions and temporal SAR backscatter (at VV and VH polarizations) during different crop stages were analyzed to classify crop types in parts of Ranchi district, East India using random forest classifier. The spectral signature of crops was generated during various growth stages using temporal Sentinel-2 MSI (optical) satellite images. The temporal backscatter profile that depends on the geometric and di-electric properties of crops were studied using Sentinel-1 SAR data. The spectral profile exhibited distinctive reflectance at the NIR (0.842 µm) and SWIR (1.610 µm) wavelength regions for paddy (Oryza sativa; ~0.25 at NIR, ~0.27 at SWIR), maize (Zea mays; ~0.24 at NIR, ~0.29 at SWIR) and finger millet (Eleusine coracana, ~0.26 NIR, ~0.31 at SWIR) during pre-sowing season (mid-June). Similar variations in crop’s reflectance at their different growth stages (vegetative to harvesting) were observed at various wavelength ranges. Further, the variations in the backscatter coefficient of different crops were observed at various growth stages depending upon the differences in sowing–harvesting periods, field conditions, geometry, and water presence in the crop field, etc. The Sentinel-1 SAR based study indicated difference in the backscatter of crops (i.e., ~−18.5 dB (VH) and ~−10 dB (VV) for paddy, ~−14 dB (VH) and ~−7.5 dB (VV) for maize, ~−14.5 dB and ~−8 dB (VV) for finger millet) during late-July (transplantation for paddy; early vegetative for maize and finger millet). These variations in the reflectance and backscatter values during various stages were used to deduce the best combination of the optical and SAR layers in order to classify each crop precisely. The GLCM texture analysis was performed on SAR for better classification of crop fields with higher accuracies. The SAR-MSI based kharif crop assessment (2017) indicated that the total cropped area under paddy, maize and finger millet was 24,544.55, 1468.28 and 632.48 ha, respectively. The result was validated with ground observations, which indicated an overall accuracy of 83.87% and kappa coefficient of 0.78. The high temporal, spatial spectral agility of Sentinel satellite are highly suitable for kharif crop monitoring. The study signifies the role of combined SAR–MSI technology for accurate mapping and monitoring of kharif crops. © 2019, Indian Academy of Sciences.","India; Jharkhand; Ranchi; Eleusine coracana; Oryza sativa; Zea mays; crop; data set; image classification; machine learning; monitoring; satellite data; Sentinel; synthetic aperture radar","Crop monitoring; crop spectral profile; random forest classification; SAR texture; SAR–MSI image fusion","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85071442100"
"Sirisha B.; Paidimarry C.S.; Chandrasekhara Sastry A.S.","Sirisha, B. (57212568492); Paidimarry, Chandra Sekhar (55839556000); Chandrasekhara Sastry, A.S. (57200122599)","57212568492; 55839556000; 57200122599","Performance evaluation of distance metrics for terra SAR-X image alignment","2017","Journal of Advanced Research in Dynamical and Control Systems","9","5","","206","219","13","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039763609&partnerID=40&md5=619f751340a7a16c9c086c8bfdfd6441","Image alignment is the fundamental process in most of the remote sensing applications like image fusion, reconstruction, navigation, retrieval, and mosaicing. These applications rely on accurate image alignment, which in turn strongly depends on the distance measure used for finding image correspondence. However the suitability of distance measure depends on the characteristics of the images between which similarity has to be computed. Hence there is a dire need to identify suitable distance measure for synthetic aperture radar images. We present a rigorous performance evaluation of six distance measure namely Euclidean, Manhattan, Cosine, Chi-Square, Correlation and Bhattacharyya distance in both matching and aligning phase of the algorithm. These results are validated against ground truth values by computing precision score and image alignment error. The results prove that the most extensively used and familiar distance measures, such as the Euclidean and Manhattan distance do not always perform well and reach a desired similarity metric estimate. It has been proved that a cautious selection of a distance measure will improve the alignment accuracy of SAR images. © 2018, Institute of Advanced Scientific Research, Inc. All rights reserved.","","Distance measure; Feature descriptors; Feature detectors; Image alignment; SAR image matching","Article","Final","","Scopus","2-s2.0-85039763609"
"Du Y.; Song W.; He Q.; Huang D.; Liotta A.; Su C.","Du, Yanling (55350371200); Song, Wei (57191748664); He, Qi (57195506496); Huang, Dongmei (16303959000); Liotta, Antonio (13408852200); Su, Chen (56029230500)","55350371200; 57191748664; 57195506496; 16303959000; 13408852200; 56029230500","Deep learning with multi-scale feature fusion in remote sensing for automatic oceanic eddy detection","2019","Information Fusion","49","","","89","99","10","10.1016/j.inffus.2018.09.006","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054181910&doi=10.1016%2fj.inffus.2018.09.006&partnerID=40&md5=ec61a80d4b72d48d197fc67c0d3d594a","Oceanic eddies are ubiquitous in global oceans and play a major role in ocean energy transfer and nutrients distribution, thus being significant for understanding ocean current circulation and marine climate change. They are characterized by a combination of high-speed vertical rotations and horizontal movements, leading to irregular three-dimensional spiral structures. While the ability to detect eddies automatically and remotely is crucial to monitoring important spatial–temporal dynamics, existing methods are inaccurate because eddies are highly dynamic and the underlying physical processes are not well understood. Typically, remote sensing is used to detect eddies based on physical parameters, geometrics or other handcrafted features. In this paper, we show how Deep Learning may be used to reliably extract higher-level features and then fuse multi-scale features to identify eddies, regardless of their structures and scales. We learn eddy features using two principal component analysis convolutional layers, then perform a non-linear transformation of the features through a binary hashing layer and block-wise histograms. To handle the difficult problem of spatial variability across synthetic aperture radar (SAR) images, we introduce a spatial pyramid model to allow multi-scale features fusion. Finally, a linear support vector machine classifier recognizes the eddies. Our method, dubbed DeepEddy, is benchmarked against a dataset of 20,000 SAR image samples, achieving a 97.8 ± 1% accuracy of detection. © 2018 Elsevier B.V.","Climate change; Energy transfer; Feature extraction; Image fusion; Linear transformations; Mathematical transformations; Nutrients; Oceanography; Principal component analysis; Radar imaging; Remote sensing; Synthetic aperture radar; Eddy detection; Feature fusion; Horizontal movements; Linear Support Vector Machines; Multi-scale features; Non-linear transformations; SAR Images; Synthetic aperture radar (SAR) images; Deep learning","Deep learning; Eddy detection; Feature fusion; Remote sensing; SAR images","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054181910"
"Ahmed U.I.; Rabus B.; Beg M.F.","Ahmed, Usman Iqbal (57217787833); Rabus, Bernhard (6701849102); Beg, Mirza Faisal (7102797387)","57217787833; 6701849102; 7102797387","SAR and optical image fusion for urban infrastructure detection and monitoring","2020","Proceedings of SPIE - The International Society for Optical Engineering","11535","","115350M","","","","10.1117/12.2579480","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094955796&doi=10.1117%2f12.2579480&partnerID=40&md5=60c5f2b16983689548bedebdcd298768","Spaceborne Synthetic Aperture Radar (SAR) and Optical sensors, are one of the main sources of Earth observation in the present age. Both the data types have their inherent advantages and disadvantages. Spaceborne Optical sensor are restricted by clouds but can offer strong information content in ideal conditions. On the other hand, SAR sensors rely on their own energy and can see through clouds. SAR is potentially an all-weather day/night imager. But SAR sensors have limitations in terms of data collection geometry and algorithmic approximations. Both sensors offer complimentary information for exploitation in data fusion for enhanced results. This research is focused on capitalizing the fusion potential for spaceborne High resolution SAR and Optical data in urban settings. The fusion of high reflection of SAR energy from urban areas and optical features of such areas can be combined to enhance the urban infrastructure detection and monitoring in a SAR/Optical fused scenario. SAR/Optical fusion can take place at three levels 1) pixel level, 2) feature level; and 3) information level. Pixel level fusion is often considered most difficult for high resolution data as precise registration up to subpixel level is required and even slight misregistration results in unfavorable circumstances. Simon Fraser University (SFU) Burnaby Mountain Campus has been chosen for area of interest because of its ongoing student housing and university infrastructure developmental projects. TerraSAR-X High Resolution Spotlight (TSX-HS) Single Look Complex (SLC) images of 1.0 m resolution have continuously being acquired over SFU; along with high resolution Optical (RGB) and Infrared (IR) images (3.0 m resolution each) from ""The Planet""acquisitions. Limited high-resolution images from ""Google Earth""(GE) in the coinciding period of TSX-HS acquisitions were also acquired for the study. Six fusion techniques have been studied for urban infrastructure detection and have been categorized based on their performance. Precision change maps will be created based on time series analysis for SAR/optical fused data in conjunction with Interferometric SAR (InSAR) analysis to study the long-term effect of urban infrastructure developments over a period of two years.  © 2020 SPIE.","Geometrical optics; Housing; Image fusion; Optical sensors; Pixels; Remote sensing; Space-based radar; Synthetic aperture radar; Time series analysis; High resolution data; High resolution image; High-resolution spotlights; Precise registration; Simon Fraser University; Single-look complexes; Spaceborne synthetic aperture radars; Urban infrastructure; Radar imaging","SAR/optical fusion; Urban infrastructure detection; Urban infrastructure monitoring","Conference paper","Final","","Scopus","2-s2.0-85094955796"
"Mohd Naseem Akhter S.S.; Rege P.P.","Mohd Naseem Akhter, Shaheera Saba (57210324342); Rege, Priti P. (6701858789)","57210324342; 6701858789","Image Fusion of RISAT-1 SAR Backscattered Image with AWiFS Optical Data","2018","1st International Conference on Data Science and Analytics, PuneCon 2018 - Proceedings","","","8745384","","","","10.1109/PUNECON.2018.8745384","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070307767&doi=10.1109%2fPUNECON.2018.8745384&partnerID=40&md5=574f0953149010b297f0b8494d1472ea","Synthetic Aperture Radar (SAR) imaging is independent of solar illumination and cloud cover. The interpretability of optical images is improved by fusing with SAR images. The pixel intensity values of SAR image are often converted to physical quantity called backscattering coefficient measured in dB. It is an important property of SAR which is used to differentiate among the land surfaces. Hence, converting SAR image to backscattered image using backscattering coefficient provides more information compared to SAR image. SAR images are captured in four polarization modes. HH and VV polarization modes provide information of smooth surface and water bodies while HV and VH polarization modes provide information of vegetation and crop. In this paper, fusion of backscattered image in HH and HV polarization mode with optical image is carried out. Product from RISAT-1 and AWiFS are used for fusion. © 2018 IEEE.","Backscattering; Geometrical optics; Image enhancement; Image fusion; Polarization; Synthetic aperture radar; AWiFS; Back-scattered; Backscattering coefficients; Physical quantities; Pixel intensities; Polarization modes; RISAT-1; Solar illumination; Radar imaging","AWiFS; Backscattered image; Image Fusion; RISAT-1; Synthetic Aperture Radar","Conference paper","Final","","Scopus","2-s2.0-85070307767"
"Liu Y.; Yu H.; Yang W.; Li L.","Liu, Yan (55965376300); Yu, Huai (57188645606); Yang, Wen (57155382600); Li, Li (57037167200)","55965376300; 57188645606; 57155382600; 57037167200","SAR image registration using SAR-FAST corner detection","2017","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","39","2","","430","436","6","10.11999/JEIT160386","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014590073&doi=10.11999%2fJEIT160386&partnerID=40&md5=f46748e31e336d17c585b15060a4758c","As the basis of change detection and image fusion, SAR image registration plays an important role in the interpretation of multi-temporal SAR images. This paper presents a method of SAR image registration based on corner detection using SAR-FAST, which is a customized version of Features from Accelerated Segment Test (FAST) for processing SAR images. The proposed method firstly employs rolling guidance filter to suppress speckle noise. Secondly, the candidate corner point is determined by quantitative analysis of the dissimilarities of the detection windows on the extended circle and the center window. Finally, the error detections are removed by analyzing the intensity distribution properties of the candidate corners. The experimental results show that SAR-FAST can detect a sufficient number of corners with stability and high repeatability, and when applying to image registration, it also can get better registration results. © 2017, Science Press. All right reserved.","Edge detection; Feature extraction; Image fusion; Image registration; Image segmentation; Synthetic aperture radar; Change detection; Corner detection; Corner point; Detection windows; Feature description; Intensity distribution; Multi-temporal SAR images; Speckle noise; Radar imaging","Corner detection, Feature description; Image registration; SAR","Article","Final","","Scopus","2-s2.0-85014590073"
"Zhao J.; Huang G.; Zhang J.","Zhao, Jing (57199498465); Huang, Guoman (35233640500); Zhang, Jiaqi (57211379266)","57199498465; 35233640500; 57211379266","Fusion segmentation method based on fuzzy theory for color images","2017","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2W7","","1043","1047","4","10.5194/isprs-archives-XLII-2-W7-1043-2017","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031027265&doi=10.5194%2fisprs-archives-XLII-2-W7-1043-2017&partnerID=40&md5=6e86e09619c10cd337b505c56ba39ac5","The image segmentation method based on two-dimensional histogram segments the image according to the thresholds of the intensity of the target pixel and the average intensity of its neighborhood. This method is essentially a hard-decision method. Due to the uncertainties when labeling the pixels around the threshold, the hard-decision method can easily get the wrong segmentation result. Therefore, a fusion segmentation method based on fuzzy theory is proposed in this paper. We use membership function to model the uncertainties on each color channel of the color image. Then, we segment the color image according to the fuzzy reasoning. The experiment results show that our proposed method can get better segmentation results both on the natural scene images and optical remote sensing images compared with the traditional thresholding method. The fusion method in this paper can provide new ideas for the information extraction of optical remote sensing images and polarization SAR images. © Authors 2017. CC BY 4.0 License.","Color; Fuzzy set theory; Graphic methods; Image fusion; Image processing; Image segmentation; Membership functions; Models; Pixels; Remote sensing; Synthetic aperture radar; Uncertainty analysis; Fusion segmentations; Natural scene images; Optical remote sensing; Segmentation methods; Segmentation results; Thresholding methods; Two-dimensional histogram; Uncertainty; Color image processing","Fusion segmentation; Fuzzy set theory; Membership function; Modeling; Two-dimensional histogram; Uncertainty","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85031027265"
"Liu G.; Li L.; Gong H.; Jin Q.; Li X.; Song R.; Chen Y.; Chen Y.; He C.; Huang Y.; Yao Y.","Liu, Guang (48461511700); Li, Lei (57756339800); Gong, Hui (57220602239); Jin, Qingwen (57192237377); Li, Xinwu (57208284748); Song, Rui (55760765100); Chen, Yun (57193090033); Chen, Yu (56200332700); He, Chengxin (7402285036); Huang, Yuqing (25622974300); Yao, Yuefeng (37115053700)","48461511700; 57756339800; 57220602239; 57192237377; 57208284748; 55760765100; 57193090033; 56200332700; 7402285036; 25622974300; 37115053700","Multisource remote sensing imagery fusion scheme based on bidimensional empirical mode decomposition (BEMD) and its application to the extraction of bamboo forest","2017","Remote Sensing","9","1","19","","","","10.3390/rs9010019","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010637355&doi=10.3390%2frs9010019&partnerID=40&md5=6022a6e7480c7dcadd375b003c11a7eb","Most bamboo forests grow in humid climates in low-latitude tropical or subtropical monsoon areas, and they are generally located in hilly areas. Bamboo trunks are very straight and smooth, which means that bamboo forests have low structural diversity. These features are beneficial to synthetic aperture radar (SAR) microwave penetration and they provide special information in SAR imagery. However, some factors (e.g., foreshortening) can compromise the interpretation of SAR imagery. The fusion of SAR and optical imagery is considered an effective method with which to obtain information on ground objects. However, most relevant research has been based on two types of remote sensing image. This paper proposes a new fusion scheme, which combines three types of image simultaneously, based on two fusion methods: bidimensional empirical mode decomposition (BEMD) and the Gram-Schmidt transform. The fusion of panchromatic and multispectral images based on the Gram-Schmidt transform can enhance spatial resolution while retaining multispectral information. BEMD is an adaptive decomposition method that has been applied widely in the analysis of nonlinear signals and to the nonstable signal of SAR. The fusion of SAR imagery with fused panchromatic and multispectral imagery using BEMD is based on the frequency information of the images. It was established that the proposed fusion scheme is an effective remote sensing image interpretation method, and that the value of entropy and the spatial frequency of the fused images were improved in comparison with other techniques such as the discrete wavelet, à-trous, and non-subsampled contourlet transform methods. Compared with the original image, information entropy of the fusion image based on BEMD improves about 0.13-0.38. Compared with the other three methods it improves about 0.06-0.12. The average gradient of BEMD is 4%-6% greater than for other methods. BEMD maintains spatial frequency 3.2-4.0 higher than other methods. The experimental results showed the proposed fusion scheme could improve the accuracy of bamboo forest classification. Accuracy increased by 12.1%, and inaccuracy was reduced by 11.0%. © 2016 by the authors; licensee MDPI, Basel, Switzerland.","Bamboo; Decay; Forests; Image Analysis; Remote Sensing; Adaptive optics; Bamboo; Extraction; Forestry; Image enhancement; Image fusion; Image reconstruction; Remote sensing; Synthetic aperture radar; Textures; Adaptive decomposition; BEMD; Bi dimensional empirical mode decomposition (BEMD); Gram-Schmidt transform; Multi-spectral imagery; Multisources; Non-sub-sampled contourlet transforms; Remote sensing image interpretations; Radar imaging","Bamboo extraction; BEMD; Extraction; Multisource remote sensing; Texture","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85010637355"
"Fu B.; Wang Y.; Campbell A.; Li Y.; Zhang B.; Yin S.; Xing Z.; Jin X.","Fu, Bolin (57218665909); Wang, Yeqiao (57200374534); Campbell, Anthony (57191292668); Li, Ying (57208286533); Zhang, Bai (8453127200); Yin, Shubai (13610744300); Xing, Zefeng (56814273000); Jin, Xiaomin (57191291999)","57218665909; 57200374534; 57191292668; 57208286533; 8453127200; 13610744300; 56814273000; 57191291999","Comparison of object-based and pixel-based Random Forest algorithm for wetland vegetation mapping using high spatial resolution GF-1 and SAR data","2017","Ecological Indicators","73","","","105","117","12","10.1016/j.ecolind.2016.09.029","124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988703621&doi=10.1016%2fj.ecolind.2016.09.029&partnerID=40&md5=cf9e838e0396e01852e810b889cb2ea3","Vegetation is an integral component of wetland ecosystems. Mapping distribution, quality and quantity of wetland vegetation is important for wetland protection, management and restoration. This study evaluated the performance of object-based and pixel-based Random Forest (RF) algorithms for mapping wetland vegetation using a new Chinese high spatial resolution Gaofen-1 (GF-1) satellite image, L-band PALSAR and C-band Radarsat-2 data. This research utilized the wavelet-principal component analysis (PCA) image fusion technique to integrate multispectral GF-1 and synthetic aperture radar (SAR) images. Comparison of six classification scenarios indicates that the use of additional multi-source datasets achieved higher classification accuracy. The specific conclusions of this study include the followings:(1) the classification of GF-1, Radarsat-2 and PALSAR images found statistically significant difference between pixel-based and object-based methods; (2) object-based and pixel-based RF classifications both achieved greater 80% overall accuracy for both GF-1 and GF-1 fused with SAR images; (3) object-based classifications improved overall accuracy between 3%-10% in all scenarios when compared to pixel-based classifications; (4) object-based classifications produced by the integration of GF-1, Radarsat-2 and PALSAR images outperformed any of the lone datasets, and achieved 89.64% overall accuracy. © 2016 Elsevier Ltd","China; C (programming language); Classification (of information); Decision trees; Image fusion; Image resolution; Pixels; Principal component analysis; Radar imaging; Satellite imagery; Synthetic aperture radar; Vegetation; Wetlands; GF-1; Northeast China; Object-based classifications; Pixel based classifications; Random forest classifier; Statistically significant difference; Synthetic aperture radar (SAR) images; Wetland vegetation; accuracy assessment; algorithm; data set; forest ecosystem; image classification; PALSAR; pixel; RADARSAT; satellite imagery; spatial resolution; synthetic aperture radar; vegetation mapping; wetland; Mapping","GF-1; Image fusion; Northeast China; Random Forest classifier; SAR; Wetland vegetation mapping","Article","Final","","Scopus","2-s2.0-84988703621"
"Yuan L.; Zhu G.; Xu C.","Yuan, Lin (57202043442); Zhu, Guobin (7402633123); Xu, Chengjun (57216585205)","57202043442; 7402633123; 57216585205","Combining synthetic aperture radar and multispectral images for land cover classification: A case study of Beijing, China","2020","Journal of Applied Remote Sensing","14","2","026510","","","","10.1117/1.JRS.14.026510","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089384279&doi=10.1117%2f1.JRS.14.026510&partnerID=40&md5=fe7ebfd013e15aae9686d54d26a5f56c","We propose exploratory research combining synthetic aperture radar (SAR) data, represented by Sentinel-1A, and multispectral data, represented by Landsat-8 operational land imager (OLI), to demonstrate the applicability and effectiveness of land cover classification based on a Beijing case study. The proposed method consists of two phases. In the fusion phase, we select three methods to evaluate the performance of integrated Sentinel-1A and Landsat-8 OLI images. In the classification phase, we choose four common methods to examine the classifying capability hidden within the fused images. Experimental results indicate that the Gram-Schmidt spectral sharpening is superior in terms of maintaining the geometric structure, spectral texture, and spatial information, demonstrating a better fusion effect than other methods. The support vector machine classification exhibits the best performance of the fused images, with an overall classification accuracy of 94.01% and a kappa coefficient of 0.91. The fused images provide better classification potential as they benefit from having more spatial information and spectral information distribution, and the mean value of overall classification accuracy and the kappa coefficient are on average 5.61% and 0.08 higher, respectively, than the original Landsat-8. Finally, we conclude that the integrated use of SAR and multispectral images significantly improves classification accuracies, thus making it effective for land cover information extraction. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Classification (of information); Image enhancement; Radar imaging; Space-based radar; Support vector machines; Synthetic aperture radar; Textures; Classification accuracy; Exploratory research; Land cover classification; Land cover informations; Multispectral images; Operational land imager; Spatial informations; Support vector machine classification; Image classification","image fusion; land cover classification; Landsat-8 operational land imager; Sentinel-1A; synthetic aperture radar data","Article","Final","","Scopus","2-s2.0-85089384279"
"Liu X.; Zhao H.; Ma H.; Li J.","Liu, Xiaomin (57211139726); Zhao, Huaqi (24832563200); Ma, Huibin (55868512400); Li, Jing (57206962981)","57211139726; 24832563200; 55868512400; 57206962981","Image matching using phase congruency and log-gabor filters in the sar images and visible images","2020","Advances in Intelligent Systems and Computing","1107 AISC","","","270","278","8","10.1007/978-981-15-3308-2_31","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081987177&doi=10.1007%2f978-981-15-3308-2_31&partnerID=40&md5=ecb76ec1fc7fcb822e133e01d9618507","SAR and visible image matching provides many applications in remote sensing, image fusion and image guidance with laborious problems with regard to the potential nonlinear intensity differences between two images. This paper proposes an image matching approach which use the phase congruency (PC) to detect corners and log-gabor filters for obtaining feature descriptor in the SAR and visible images. PC can provide inherent and rich image textures for the images with intricate grayscale changes or noise, which is utilitized to detect the corners. The moments of PCs for the images are calculated to obtain the keypoints and the log-gabor filters are employed to acquire the feature descriptors. Five evaluation methods are used for testing the results of the algorithm for three pairs of images and its result is compared to the SIFT algorithm. The experiment performance show that the advocated algorithm is better than the SIFT algorithm. © Springer Nature Singapore Pte Ltd. 2020.","Evolutionary algorithms; Gabor filters; Image fusion; Image matching; Microcomputers; Radar imaging; Remote sensing; Synthetic aperture radar; Textures; Evaluation methods; Feature descriptors; Image guidances; Intensity difference; Log-gabor filter; Phase congruency; SAR Images; SIFT algorithms; Image texture","Image matching; Log-gabor filters; Phase congruency; SAR image","Conference paper","Final","","Scopus","2-s2.0-85081987177"
"Fu Z.; Qi J.; Zhang D.; Wang J.; Zhang W.; Han X.","Fu, Zhengbo (57206674533); Qi, Jianwei (55821319700); Zhang, Dandan (57206663630); Wang, Jie (57206675662); Zhang, Wei (57206658072); Han, Xu (57206658269)","57206674533; 55821319700; 57206663630; 57206675662; 57206658072; 57206658269","Comparative Analysis of the Fusion Methods Based on GF-3 Radar and GF-1 Multispectral Data","2018","5th International Workshop on Earth Observation and Remote Sensing Applications, EORSA 2018 - Proceedings","","","8598556","","","","10.1109/EORSA.2018.8598556","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061824193&doi=10.1109%2fEORSA.2018.8598556&partnerID=40&md5=72644bdbfb65be2dfb12d6b3d435b055","To make better use of the advantages of radar and promote the application of image fusion based on radar data, the author uses different fusion methods based on GF-3 SAR and GF-1 MSS, and evaluates the fusion results by analyzing mean, variance, information entropy, average gradient, spectral distortion and correlation coefficient. The results show that HSV and GS transforms have the best performances in overall. PC is recognized as the third, while it is still remarkable that it has the best ability of spectral retention. And the specialty in NIR band makes PC more conducive for extraction of vegetation. Brovey and Multiplicative transforms are not effective in comparison. © 2018 IEEE.","Fusion reactions; Observatories; Radar imaging; Remote sensing; Synthetic aperture radar; Comparative analysis; Correlation coefficient; GF-3; Information entropy; Multi-spectral data; Multiplicative transforms; Multispectral images; Spectral distortions; Image fusion","fusion; GF-3; multispectral image; remote sensing; SAR","Conference paper","Final","","Scopus","2-s2.0-85061824193"
"Guo Z.; Zhang H.; Ye S.","Guo, Zhenyu (57216439837); Zhang, Hongbo (55685538900); Ye, Shaohua (7202088255)","57216439837; 55685538900; 7202088255","Cartesian based FFBP algorithm for circular SAR using NUFFT interpolation","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048561","","","","10.1109/APSAR46974.2019.9048561","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083489420&doi=10.1109%2fAPSAR46974.2019.9048561&partnerID=40&md5=5d6909e6e91844e04f12c29f10bd1f3a","Circular SAR is able to achieve omni-directional observation and high-resolution imaging of targets. However, the traditional frequency-domain based imaging algorithm is not suitable for complicated curve trajectory. Moreover the time domain based back-projection (BP) algorithm is applicable but time consuming. Fast factorized back-projection (FFBP) algorithm based on aperture decomposition and image fusion can balance computational efficiency and accuracy. In this paper, we proposed a modified FFBP algorithm for circular SAR imaging. The principal improvement is the usage of Cartesian coordinate imaging and nonuniform fast Fourier transform (NUFFT) interpolation. First, sub-aperture BP imaging is implemented on local Cartesian coordinate system. Then azimuth bandwidth is compressed with a spatial variant phase function to reduce the sampling rate. Next the NUFFT interpolation method is applied during sub-images fusion to further improve the efficiency of the algorithm. Finally, through simulation and real data experiments, the correctness and accuracy of the algorithm is verified. © 2019 IEEE.","Computational efficiency; Efficiency; Fast Fourier transforms; Frequency domain analysis; Image enhancement; Image fusion; Interpolation; Radar imaging; Backprojection algorithms; Cartesian coordinate; Cartesian coordinate system; Frequency domains; High-resolution imaging; Imaging algorithm; Interpolation method; Non-uniform fast Fourier transforms; Synthetic aperture radar","Cartesian coordinate system; Circular synthetic aperture radar (CSAR); Fast factorized back-projection (FFBP); Nonuniform fast Fourier transform (NUFFT)","Conference paper","Final","","Scopus","2-s2.0-85083489420"
"Ravikanth G.; Sunitha K.V.N.; Eswara Reddy B.","Ravikanth, G. (57218775536); Sunitha, K.V.N. (12789729100); Eswara Reddy, B. (25923111700)","57218775536; 12789729100; 25923111700","Location Related Signals with Satellite Image Fusion Method Using Visual Image Integration Method","2020","Computer Systems Science and Engineering","35","5","","385","393","8","10.32604/CSSE.2020.35.385","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104505706&doi=10.32604%2fCSSE.2020.35.385&partnerID=40&md5=a74c96799966fce0f635f8582f2416a2","Investigations were performed on a group utilizing (General Purpose Unit) GPU and executions were evaluated for the utilization of the created parallel usages to process satellite pictures from satellite Landsat7.The usage on a realistic group gives execution change from 2 to 18 times. The nature of the considered techniques was assessed by relative dimensionless global error in synthesis (ERGAS) and Quality Without Reference (QNR) measurements. The outcomes demonstrate execution picks ups and holding of value with the bunch of GPU contrasted with the outcomes and different analysts for a CPU and single GPU. The errand of upgrading the view of a scene by combining data caught from various picture sensors is usually known as multisensor picture combination. This paper displays a territory based picture combination calculation to consolidate SAR (Synthetic Aperture Radar) and optical pictures. The co-enlistment of the two images is first led utilizing the proposed enrollment method prior to picture combination. The paper displays a parallel execution of existing picture combination techniques on a graphical group. Parallel executions of techniques in view of discrete wavelet changes are created. Division into dynamic and motionless regions is then executed on the SAR surface picture for particular injection of the SAR picture into panchromatic (PAN) picture. An integrated image in view of these two pictures is produced by the novel region based combination plot, which forces diverse combination rules for each fragmented region. At long last, this picture is melded into a multispectral(MS) picture through the half breed skillet honing technique proposed in past research. Exploratory outcomes exhibit that the proposed strategy demonstrates preferred execution over different fusion algorithms and can possibly be connected to the multisensory combination of SAR and optical pictures. © 2020 CRL Publishing. All rights reserved.","Graphics processing unit; Image fusion; Radar imaging; Satellites; Synthetic aperture radar; Combination rules; Discrete wavelets; Fusion algorithms; Integrated images; Parallel executions; SAR(synthetic aperture radar); Satellite images; Satellite pictures; Space-based radar","Area-based combination conspire; Cluster; co-enlistment; GPU; hybrid sharpening; Image combination; multisensor picture combination; Satellite.; Wavelet; wavelet change","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85104505706"
"Liu J.; Lei Y.-J.; Xing Y.-Q.; Lu C.-G.","Liu, Jian (56896429100); Lei, Ying-Jie (9236918700); Xing, Ya-Qiong (55759303100); Lu, Chuan-Guo (7404804856)","56896429100; 9236918700; 55759303100; 7404804856","Fusion technique for SAR and gray visible image based on hidden Markov model in non-subsample shearlet transform domain","2016","Kongzhi yu Juece/Control and Decision","31","3","","453","457","4","10.13195/j.kzyjc.2014.1932","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962175441&doi=10.13195%2fj.kzyjc.2014.1932&partnerID=40&md5=7692efcfe63742c32f51df72db038646","To exact more directional information and important detail information from the images effectively, an image fusion algorithm for synthetic aperture radar(SAR) and grayscale visible light images based on the hidden Markov model(HMM) in the non-subsample Shearlet transform(NSST) domain is proposed. In NSST domain, the low frequency factors are fused by standard deviation. Meanwhile, the hidden Markov tree(HMT) model is built to train the high frequency factors. Then the energy gradient is used to select the trained high frequency factors. Thus, the low frequency and high frequency images are fused by inverse transformation of NSST to get the final image. Finally, the simulation results show that, compared with other multi-scale HMT models and traditional NSST fusion strategy, the proposed method can promote the fusion quality and enhance the information of the images, while reducing noise as well, and also show its effectiveness and feasibility. © 2016, Editorial Office of Control and Decision. All right reserved.","Algorithms; Image fusion; Inverse problems; Markov processes; Radar imaging; Synthetic aperture radar; Directional information; Hidden Markov tree; Hidden Markov tree model; Image fusion algorithms; Inverse transformations; Shearlet transforms; Standard deviation; Visible light images; Hidden Markov models","Hidden Markov tree; Image fusion; Non-subsample Shearlet transform","Article","Final","","Scopus","2-s2.0-84962175441"
"Liu X.; Deng C.; Zhao B.; Chanussot J.","Liu, Xun (56963011600); Deng, Chenwei (25958671000); Zhao, Baojun (7403059245); Chanussot, Jocelyn (6602159365)","56963011600; 25958671000; 7403059245; 6602159365","Multimodal-Temporal Fusion: Blending Multimodal Remote Sensing Images to Generate Image Series with High Temporal Resolution","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898453","10083","10086","3","10.1109/IGARSS.2019.8898453","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077690431&doi=10.1109%2fIGARSS.2019.8898453&partnerID=40&md5=a6ae333a3b5f58c3d96298eae3bdd266","This paper aims to tackle a general but interesting cross-modality problem in remote sensing community: can multimodal images help to generate synthetic images in time series and improve temporal resolution? To this end, we explore multimodal-temporal fusion, in which we attempt to leverage the availability of additional cross-modality images to simulate the missing images in time series. We propose a multimodal-temporal fusion framework, and mainly focus on two kinds of information for the simulation: intra-modal cross-modality information and inter-modal temporal information. To exploit the cross-modality information, we adopt available paired images and learn a mapping between different modality images using a deep neural network. Considering temporal dependency among time-series images, we formulate a temporal constraint in the learning to encourage temporal consistent results. Experiments are conducted on two cross-modality image simulation applications (SAR to visible and visible to SWIR), and both visual and quantitative results demonstrate that the proposed model can successfully simulate missing images with cross-modality data. © 2019 IEEE.","Deep neural networks; Geology; Image fusion; Neural networks; Remote sensing; Synthetic aperture radar; Time series; High temporal resolution; Image time-series; Image translation; Multi-modal; Remote sensing images; Temporal constraints; Temporal information; Temporal resolution; Image enhancement","Cross-modality Image Translation; Deep Neural Networks; Image Time Series; Multimodal-Temporal Fusion; Temporal Resolution","Conference paper","Final","","Scopus","2-s2.0-85077690431"
"Gu M.; Liu H.; Wang Y.; Yang D.","Gu, Mingfei (57220886704); Liu, Hongwei (57205480555); Wang, Yan (57189251348); Yang, Dongwen (57213485701)","57220886704; 57205480555; 57189251348; 57213485701","PolSAR Target Detection via Reflection Symmetry and a Wishart Classifier","2020","IEEE Access","8","","9106405","103317","103326","9","10.1109/ACCESS.2020.2999472","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087073330&doi=10.1109%2fACCESS.2020.2999472&partnerID=40&md5=c198bc67d349460b5a36bc642fe06c03","Detection of man-made targets using polarimetric synthetic aperture radar (PolSAR) data has become a promising research area. The reflection symmetry is gradually being applied to man-made target detection algorithms as a physical property that can distinguish between man-made targets and natural clutter. However, the two terms related to the reflection symmetry property in the polarimetric coherency matrix, namely, the C_{12} and C_{23} terms, are not fully exploited by the traditional methods. To fully exploit the polarization information of the two terms, an image fusion strategy based on the position and scale information of the scale-invariant feature transform (SIFT) key points is proposed in this paper. Then, a new Wishart classifier based on the patch-level Wishart distance is used to realize automatic target detection of the fused image. The experimental results on measured data show that the proposed method can enhance the contrast between targets and clutter. In addition, the detection performance of the proposed method under different target-to-clutter ratios (TCRs) are verified on the synthetic data and measured data.  © 2013 IEEE.","Clutter (information theory); Image fusion; Polarimeters; Synthetic aperture radar; Automatic target detection; Detection performance; Fusion strategies; Polarimetric synthetic aperture radars; Reflection symmetry; Scale invariant feature transforms; Target detection algorithm; Target-to-clutter ratios; Radar clutter","Complex Wishart distribution; Polarimetric SAR; Reflection symmetry; Target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85087073330"
"Kai L.; Xueqing Z.","Kai, Li (56661470700); Xueqing, Zhang (6508322496)","56661470700; 6508322496","Review of research on registration of sar and optical remote sensing image based on feature","2019","2018 IEEE 3rd International Conference on Signal and Image Processing, ICSIP 2018","","","8600443","111","115","4","10.1109/SIPROCESS.2018.8600443","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061831836&doi=10.1109%2fSIPROCESS.2018.8600443&partnerID=40&md5=e6236200a3ccef85d1b6641063edd063","Synthetic Aperture Radar(SAR) and optical remote sensing image registration is the prerequisite for image fusion and it is of important theoretical significance and practical value. The image registration methods are mainly divided into the methods based on feature, the methods based on Gray-scale and others. This article systematically sorts out feature-based optical and SAR remote sensing image registration techniques, summarizes all types of image registration, points out their advantages and disadvantages and predicts the prospects of their future. © 2018 IEEE.","Image fusion; Image registration; Optical data processing; Radar imaging; Synthetic aperture radar; Feature-based; Gray scale; Optical remote sensing; Registration methods; SAR remote sensing; Remote sensing","Feature-based; Image registration; Remote sensing; Synthetic aperture radar(SAR)","Conference paper","Final","","Scopus","2-s2.0-85061831836"
"Gu W.; Wang D.; Ma X.","Gu, Wenkun (56393673100); Wang, Dangwei (8653509600); Ma, Xiaoyan (35332614000)","56393673100; 8653509600; 35332614000","Distributed MIMO-ISAR sub-image fusion method","2017","Journal of Radars","6","1","","90","97","7","10.12000/JR16042","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019058193&doi=10.12000%2fJR16042&partnerID=40&md5=1d41f21f6a539ac96480f5ff9b215349","The fast fluctuation associated with maneuvering a target's radar cross-section often affects the imaging performance stability of traditional monostatic Inverse Synthetic Aperture Radar (ISAR). To address this problem, in this study, we propose an imaging method based on the fusion of sub-images of frequency-diversity-distributed multiple Input-Multiple Output-Inverse Synthetic Aperture Radar (MIMO-ISAR). First, we establish the analytic expression of a two-dimensional ISAR sub-image acquired by different channels of distributed MIMO-ISAR. Then, we derive the distance and azimuth distortion factors of the image acquired by the different channels. By compensating for the distortion of the ISAR image, we ultimately realize distributed MIMO-ISAR fusion imaging. Simulations verify the validity of this imaging method using distributed MIMOISAR.","","Distributed radar; Image fusion; Inverse synthetic aperture radar (ISAR); Multiple input multiple output (MIMO) radar; Orthogonal frequency division multiplexing (OFDM)","Article","Final","","Scopus","2-s2.0-85019058193"
"Wang X.Y.; Guo Y.G.; He J.; Du L.T.","Wang, X.Y. (37079703600); Guo, Y.G. (56208573900); He, J. (56159692700); Du, L.T. (50661043100)","37079703600; 56208573900; 56159692700; 50661043100","Fusion of HJ1B and ALOS PALSAR data for land cover classification using machine learning methods","2016","International Journal of Applied Earth Observation and Geoinformation","52","","","192","203","11","10.1016/j.jag.2016.06.014","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997785029&doi=10.1016%2fj.jag.2016.06.014&partnerID=40&md5=1b0edd9422f5549259576a9359ddc26f","Image classification from remote sensing is becoming increasingly urgent for monitoring environmental changes. Exploring effective algorithms to increase classification accuracy is critical. This paper explores the use of multispectral HJ1B and ALOS (Advanced Land Observing Satellite) PALSAR L-band (Phased Array type L-band Synthetic Aperture Radar) for land cover classification using learning-based algorithms. Pixel-based and object-based image analysis approaches for classifying HJ1B data and the HJ1B and ALOS/PALSAR fused-images were compared using two machine learning algorithms, support vector machine (SVM) and random forest (RF), to test which algorithm can achieve the best classification accuracy in arid and semiarid regions. The overall accuracies of the pixel-based (Fused data: 79.0%; HJ1B data: 81.46%) and object-based classifications (Fused data: 80.0%; HJ1B data: 76.9%) were relatively close when using the SVM classifier. The pixel-based classification achieved a high overall accuracy (85.5%) using the RF algorithm for classifying the fused data, whereas the RF classifier using the object-based image analysis produced a lower overall accuracy (70.2%). The study demonstrates that the pixel-based classification utilized fewer variables and performed relatively better than the object-based classification using HJ1B imagery and the fused data. Generally, the integration of the HJ1B and ALOS/PALSAR imagery can improve the overall accuracy of 5.7% using the pixel-based image analysis and RF classifier. © 2016 Elsevier B.V.","algorithm; ALOS; image classification; land cover; machine learning; PALSAR; pixel; satellite imagery","ALOS/PALSAR; HJ1B; Image fusion; Land cover classification","Article","Final","","Scopus","2-s2.0-84997785029"
"Quang N.H.; Tuan V.A.; Hao N.T.P.; Hang L.T.T.; Hung N.M.; Anh V.L.; Phuong L.T.M.; Carrie R.","Quang, Nguyen Hong (57225358184); Tuan, Vu Anh (55349227600); Hao, Nguyen Thi Phuong (57212305835); Hang, Le Thi Thu (57212302530); Hung, Nguyen Manh (57212302847); Anh, Vu Le (57212307001); Phuong, Le Thi Minh (57212312613); Carrie, Rachael (56956690100)","57225358184; 55349227600; 57212305835; 57212302530; 57212302847; 57212307001; 57212312613; 56956690100","Synthetic aperture radar and optical remote sensing image fusion for flood monitoring in the Vietnam lower Mekong basin: a prototype application for the Vietnam Open Data Cube","2019","European Journal of Remote Sensing","52","1","","599","612","13","10.1080/22797254.2019.1698319","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076405399&doi=10.1080%2f22797254.2019.1698319&partnerID=40&md5=bafbd9c2580aca2dd947f52f222adfc3","Flood monitoring systems are crucial for flood management and consequence mitigation in flood prone regions. Different remote sensing techniques are increasingly used for this purpose. However, the different approaches suffer various limitations, including cloud and weather effects (optical data), and low spatial resolution and poor colour presentation (synthetic aperture radar data). This study fuses two data types (Landsat and Sentinel-1) to overcome these limitations and produce better quality images for a prototype flood application in the Vietnam Open Data Cube (VODC). Visual and quantitative evaluation of fused image quality revealed improvement in the images compared with the original scenes. Ground-truth data was used to develop the study flood extraction algorithm and we found a good agreement between our results and SERVIR Mekong (a joint initiative by the US agency for International Development (USAID), National Aeronautics and Space Administration (NASA), Myanmar, Thailand, Cambodia, Laos and Vietnam) maps. While the algorithm is run on a personal computer (PC), it has a clear potential to be developed for application on a big data system. © 2019, © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Cambodia; Laos; Myanmar; Thailand; Viet Nam; Flood control; Floods; Geometrical optics; Geometry; Image enhancement; Image fusion; NASA; Personal computers; Radar imaging; Remote sensing; Space optics; Space-based radar; Synthetic aperture radar; Extraction algorithms; Flood monitoring; Optical image; Optical remote sensing; Quantitative evaluation; Remote sensing techniques; Us agency for international development; Viet Nam; algorithm; color; computer system; flooding; image analysis; remote sensing; satellite imagery; spatial resolution; synthetic aperture radar; Open Data","flood monitoring; Image fusion; Lower Mekong Vietnam; Open Data Cube; SAR and optical images","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85076405399"
"Bioresita F.; Puissant A.; Stumpf A.; Malet J.-P.","Bioresita, Filsa (57200860429); Puissant, Anne (7102002323); Stumpf, André (37103096800); Malet, Jean-Philippe (7004001508)","57200860429; 7102002323; 37103096800; 7004001508","Fusion of Sentinel-1 and Sentinel-2 image time series for permanent and temporary surface water mapping","2019","International Journal of Remote Sensing","40","23","","9026","9049","23","10.1080/01431161.2019.1624869","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067690242&doi=10.1080%2f01431161.2019.1624869&partnerID=40&md5=e769697606cf40867713a9ba17abfc77","Monitoring the spatial and temporal extents of permanent and temporary bodies of surface water is important for various applications such as water resource management, climate modelling, and biodiversity conservation. Satellite remote sensing is an effective source of information to detect surface water over large areas and document their evolution in time. Recently, the European Space Agency (ESA) launched freely available SAR (Synthetic Aperture Radar) and optical sensors (Sentinel-1 & 2) with high revisiting time and spatial resolution. The objective of this paper is to explore the contribution of multi-temporal and multi-source (passive and active) Sentinel observations for improving the detection and mapping of surface waters by applying decision-level image fusion techniques. The approach is tested over Central Ireland using a time series of 16 Sentinel-1 images and a few Sentinel-2 images for the period 2015–2016. Compared to a mono-date approach, the combination of Sentinel-1 & 2 observations provides better accuracy for mapping permanent surface water. Decision level fusion technique allows mapping temporary surface water (such as flooding) with a high accuracy. It also gives the possibility to monitor their dynamics by providing the probability of occurrence of flooded areas at the pixel level. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","Ireland; Biodiversity; Floods; Image enhancement; Image fusion; Mapping; Radar imaging; Remote sensing; Space-based radar; Synthetic aperture radar; Time series; Water conservation; Water management; Biodiversity conservation; Decision level fusion; European Space Agency; Image fusion techniques; Probability of occurrence; SAR(synthetic aperture radar); Satellite remote sensing; Waterresource management; accuracy assessment; flooding; image processing; pixel; satellite imagery; Sentinel; spatial resolution; surface water; synthetic aperture radar; time series; Surface waters","","Article","Final","","Scopus","2-s2.0-85067690242"
"Li D.; Zhang Y.; Dong X.; Shi X.; Zhai W.","Li, Dong (56815938700); Zhang, Yunhua (55660906200); Dong, Xiao (54784323200); Shi, Xiaojin (55363443500); Zhai, Wenshuai (23494379300)","56815938700; 55660906200; 54784323200; 55363443500; 23494379300","A HSV-based fusion of INIRA SAR and googleearth optical images","2019","Asia-Pacific Microwave Conference Proceedings, APMC","2018-November","","8617352","848","850","2","10.23919/APMC.2018.8617352","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061797770&doi=10.23919%2fAPMC.2018.8617352&partnerID=40&md5=f0bf0730095f1d37a3868e498dfa86d0","Interferometric Imaging Radar Altimeter (InIRA) on board Chinese TG-2 space laboratory is dedicated to enable a wide-swath measure of the sea surface height by interferometric processing synthetic aperture radar (SAR) images. In this paper, a simple fusion algorithm is developed for InIRA SAR image and GoogleEarth optical image based on hue-saturation-value (HSV) color model. While the H and S components of optical image are reserved for the fusion, the V component of the fusion image is a weighted linear combination of SAR image and the V component of optical image, with the image entropy as weight. Experimental results exhibit not only the nice performance of the fusion scheme but also the potential of InIRA in inland water observation. © 2018 IEICE","Geometrical optics; Image fusion; Interferometry; Optical sensors; Space stations; Surface waters; Synthetic aperture radar; Fusion algorithms; Hue saturation values; Image entropy; Interferometric imaging; Interferometric processing; Sea surface height; Synthetic aperture radar (SAR) images; Weighted linear combinations; Radar imaging","HSV; Image entropy; Image fusion; Optical sensor; Synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85061797770"
"Sghaier M.O.; Hammami I.; Foucher S.; Lepage R.","Sghaier, Moslem Ouled (56421930900); Hammami, Imen (57221369679); Foucher, Samuel (6701728686); Lepage, Richard (56275061200)","56421930900; 57221369679; 6701728686; 56275061200","Flood extent mapping from time-series SAR images based on texture analysis and data fusion","2018","Remote Sensing","10","2","237","","","","10.3390/rs10020237","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042532275&doi=10.3390%2frs10020237&partnerID=40&md5=3a2e106a2b4e21fd04807b2b76d1823f","Nowadays, satellite images are considered as one of the most relevant sources of information in the context of major disasters management. Their availability in extreme weather conditions and their ability to cover wide geographic areas make them an indispensable tool toward an effective disaster response. Among the various available sensors, Synthetic Aperture Radar (SAR) is distinguished in the context of flood management by its ability to penetrate cloud cover and its robustness to unfavourable weather conditions. This work aims at developing a new technique for flooded areas extraction from high resolution time-series SAR images. The proposed approach is mainly based on three steps: first, homogeneous regions characterizing water surfaces are extracted from each SAR image using a local texture descriptor. Then, mathematical morphology is applied to filter tiny artifacts and small homogeneous areas present in the image. And finally, spatial and radiometric information embedded in each pixel are extracted and are fused with the same pixel information but from another image to decide if the current pixel belongs to a flooded region. In order to assess the performance of the proposed algorithm, our methodology was applied to time-series images acquired before and during three different flooding events: (1) Richelieu River and lake Champlain floods, Quebec, Canada in 2011; (2) Evros River floods, Greece in 2014 and (3) Western and southwestern of Iran floods in 2016. Experiments show that our approach gives very promising results compared to existing techniques. © 2018 by the authors.","Data fusion; Disasters; Extraction; Flood control; Floods; Formal logic; Image fusion; Image processing; Image texture; Mathematical morphology; Meteorology; Pixels; Synthetic aperture radar; Time series; Time series analysis; Dempster-Shafer theory; Extreme weather conditions; Homogeneous regions; Indispensable tools; Pixel information; SAR Images; Sources of informations; Structural feature; Radar imaging","Data fusion; Dempster-Shafer theory; Flood extraction; Mathematical morphology; Structural Feature Set; Time-series SAR images","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85042532275"
"Jakka T.K.; Reddy Y.M.; Rao B.P.","Jakka, Thrisul Kumar (57226875990); Reddy, Y. Mallikarjuna (35766908800); Rao, B. Prabhakara (26655008200)","57226875990; 35766908800; 26655008200","GWDWT-FCM: Change Detection in SAR Images Using Adaptive Discrete Wavelet Transform with Fuzzy C-Mean Clustering","2019","Journal of the Indian Society of Remote Sensing","47","3","","379","390","11","10.1007/s12524-018-0901-0","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057854363&doi=10.1007%2fs12524-018-0901-0&partnerID=40&md5=38d5d73406749fa699a039ee3dc715ca","Change detection in remote sensing images turns out to play a significant role for the preceding years. Change detection in synthetic aperture radar (SAR) images comprises certain complications owing to the reality that it endures from the existence of the speckle noise. Hence, to overcome this limitation, this paper intends to develop an improved model for detecting the changes in SAR image. In this model, two SAR images captivated at varied times will be considered as the input for the change detection process. Initially, discrete wavelet transform (DWT) is employed for image fusion, where the coefficients are optimized using improved grey wolf optimization (GWO) called adaptive GWO (AGWO) algorithm. Finally, the fused images after inverse transform are clustered using fuzzy C-means (FCM) clustering technique and a similarity measure is performed among the segmented image and ground truth image. With the use of all these technologies, the proposed model is termed as adaptive grey wolf-based DWT with FCM (AGWDWT-FCM). The similarity measures analyze the relevant performance measures such as accuracy, specificity and F1 score. Moreover, the performance of the AGWDWT-FCM in change detection model is compared to other conventional models, and the improvement is noted. © 2018, Indian Society of Remote Sensing.","accuracy assessment; algorithm; cluster analysis; detection method; numerical model; optimization; remote sensing; satellite imagery; segmentation; synthetic aperture radar; wavelet analysis","Adaptive discrete wavelet transform; Filter coefficient; Fuzzy C-means clustering; Grey wolf optimization; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85057854363"
"Salvetti F.; Giusti E.; Staglianò D.; Martorella M.","Salvetti, Federica (43861767500); Giusti, Elisa (55388839300); Staglianò, Daniele (55548375100); Martorella, Marco (6603185380)","43861767500; 55388839300; 55548375100; 6603185380","Multistatic 3D isar imaging of maritime targets","2020","Multidimensional Radar Imaging","","","","287","309","22","10.1049/SBRA527E_ch9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114561738&doi=10.1049%2fSBRA527E_ch9&partnerID=40&md5=83bbce70ed4fd85c79b021771a4269e3","In this chapter, it has been shown that 3D image reconstruction using multiple views greatly enhances the system capability to reconstruct 3D non -cooperative target images effectively. The fusion of 3D point clouds obtained by using several temporal and a number of node acquisitions creates a more densely populated point cloud that better represents the target of interest and allows for important features to be extracted more accurately and reliably. In fact, every single 3D fusion may add important information about the target with respect to a single reconstruction. Moreover, new information about the target of interest can be uploaded, when available, in order to update recognition and classification databases based on 3D InISAR imaging systems. Although the implementation of ATR systems that make use of 3D reconstructions is still not in place, the results shown here are a strong encouragement to use this new technology to strengthen radar image -based ATR. Chapter Contents: • 9.1 Multiview 3D InISAR image fusion • 9.1.1 3D fusion • 9.2 Multiview 3D InISAR image fusion in experimental scenarios • 9.2.1 Application of multiview 3D to multitemporal data • 9.2.2 Application of multiview 3D to multistatic data • 9.2.3 Application of multiview 3D to a combination of multistatic and multitemporal data • 9.3 Conclusion • References. © The Institution of Engineering and Technology 2020.","","3D inISAR imaging systems; 3D noncooperative target image reconstruction; 3D point cloud fusion; Atr systems; Classification databases; Feature extraction; Image classification; Image fusion; Image reconstruction; Marine radar; Maritime targets; Multistatic 3D ISAR imaging; Node acquisitions; Point cloud; Radar imaging; Single 3D fusion; Synthetic aperture radar","Book chapter","Final","","Scopus","2-s2.0-85114561738"
"Wang Z.; Li G.; Jiang X.","Wang, Zhihao (57211917337); Li, Gang (55547117794); Jiang, Xiao (55599147400)","57211917337; 55547117794; 55599147400","Flooded area detection method based on fusion of optical and sar remote sensing images; [基于光学和SAR遥感图像融合的洪灾区域检测方法]","2020","Journal of Radars","9","3","","539","553","14","10.12000/JR19095","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091079044&doi=10.12000%2fJR19095&partnerID=40&md5=06f942515eb8d47667cb1db7a7ff65a6","The flooded area detection method based on the fusion of optical and Synthetic Aperture Radar (SAR) images is applicable for all weather conditions and times. However, due to the large number of randomly distributed intensive speckle noise in SAR images, the conventional methods of detection often trigger high false alarm rates at flood-stricken zones. Inspired by the Fuzzy C-Means (FCM) clustering method, a hierarchical clustering algorithm (Hierarchical Fuzzy C-Means, H-FCM) is proposed in this paper. This method fuses the SAR image captured after the flood with the optical image captured before the flood. Based on the fused image, this method uses the proposed hierarchical clustering model to obtain the preliminary detection results of the flooded area. Additionally, the algorithm uses the proposed region-growing algorithm to obtain the river location before the flood and uses it as a spatial constraint for the preliminary detection results to further screen out suspected flooded areas and significantly improve detection performance. The experimental data used in this paper include the remote sensing images captured before and after the Gloucester floods in the United Kingdom in 1999, as well as the remote sensing images captured before and after the Nanchang floods in China in 2019. The effectiveness and validity of the H-FCM algorithm are also supported by comparison experiments. © 2020 Institute of Electronics Chinese Academy of Sciences. All rights reserved.","Clustering algorithms; Floods; Fuzzy systems; Geometrical optics; Hierarchical clustering; Image fusion; Remote sensing; Synthetic aperture radar; Conventional methods; Detection performance; Fuzzy C means clustering; Randomly distributed; Region growing algorithm; Remote sensing images; Spatial constraints; Synthetic aperture radar (SAR) images; Radar imaging","Hierarchical clustering; Hierarchical Fuzzy C-Means (H-FCM); Optical; Region-growing; Remote sensing images fusion; Spatial constraint; Synthetic Aperture Radar (SAR)","Article","Final","","Scopus","2-s2.0-85091079044"
"Ahmed T.; Singh D.; Gupta S.; Raman B.","Ahmed, Tasneem (55549594300); Singh, Dharmendra (36912015700); Gupta, Shweta (55781518700); Raman, Balasubramanian (23135470700)","55549594300; 36912015700; 55781518700; 23135470700","An efficient application of fusion approach for hot spot detection with MODIS and PALSAR-1 data","2016","Geocarto International","31","7","","715","738","23","10.1080/10106049.2015.1076061","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940571059&doi=10.1080%2f10106049.2015.1076061&partnerID=40&md5=aad2a72c932e9f557050fafa8e5a258b","Hot spot detection with satellite images, especially with synthetic aperture radar (SAR) images is still a challenging task. Several researchers have used TM/optical data for identification of hot spot but the use of SAR data is very limited for this type of application. The fusion of SAR data with TM/optical data may add additional information which in turn will lead for enhancement of detection capability of the hot spot. Therefore, this study explores the possibility of fusion of Moderate Resolution Imaging Spectroradiometer (MODIS) and Phased Array L-band Synthetic Aperture Radar (PALSAR) satellite images for the hot spot detection. Image fusion is emerging as a powerful tool where information of various sensors can be used for obtaining better results. For this purpose, vegetation greenness and roughness information which is obtained from MODIS and PALSAR satellite images, respectively, are used for fusion, and then, a contextual-based thresholding algorithm is applied to the fused image for hot spot detection. The proposed approach comprises of two steps: (1) application of genetic algorithm-based scheme for image fusion of MODIS and PALSAR satellite images, and (2) classification of the fused image as either hot spot or non-hot spot pixels by employing a contextual thresholding technique. The algorithm is tested over the Jharia Coal Field region of India, where hot spot is one of the major problems and it is observed that the proposed thresholding technique classifies the each pixel of the fused image into two categories: hot spot and non-hot spot and the proposed approach detects the hot spot with better accuracy and less false alarm. © 2015 Taylor & Francis.","genetic algorithm; MODIS; PALSAR; satellite data; satellite imagery; threshold; vegetation dynamics; vegetation index","CPR; GEMI; genetic algorithm (GA); hot spots; image fusion; MODIS; MSAVI; PALSAR; polarimetric indices; thresholding; vegetation indices","Article","Final","","Scopus","2-s2.0-84940571059"
"Horch A.; Djemal K.; Gafour A.; Taleb N.","Horch, Abdelkader (57212876633); Djemal, Khalifa (14821991200); Gafour, Abdelkader (55611988200); Taleb, Nasreddine (7003935749)","57212876633; 14821991200; 55611988200; 7003935749","Supervised fusion approach of local features extracted from SAR images for detecting deforestation changes","2019","IET Image Processing","13","14","","2866","2876","10","10.1049/iet-ipr.2019.0122","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077436529&doi=10.1049%2fiet-ipr.2019.0122&partnerID=40&md5=767860460467ef3d9803d6e0c85a0cdc","Deforestation has become a major problem consisting of a continuous regression of forested areas in the world, and for this purpose, an efficient detection of these changes has become more than necessary. In this work, a new method for deforestation change detection is proposed. This approach is based on a supervised fusion of local texture features extracted from SAR images. ALOS PALSAR (Advanced Land Observation Satellite Phased Array type L-band Synthetic Aperture Radar) multi-temporal data have been used in this work. Normalised radar cross-section (NRCS) and polarimetric features extracted from HH and HV polarised data allowed recognising different categories of land covers termed as NRCS classification. Grey-level co-occurrence matrix (GLCM) texture features were extracted by using a different moving window sizes applied on local regions previously obtained by binarisation of the NRCS results. A total of 300 samples of regions and five GLCM characteristics have been used here. The detection of deforestation appears clearly in the resulted images with a very satisfactory precision of the reached regions, and the obtained results of the proposed supervised approach have indeed led to very good detection results of the deforestation change. © The Institution of Engineering and Technology 2019.","Deforestation; Image fusion; Image processing; Radar cross section; Synthetic aperture radar; Textures; Change detection; Efficient detection; Grey-level co-occurrence matrixes; Local texture feature; Multi-temporal data; Observation satellites; Phased array type l-band synthetic aperture radars; Polarimetric features; Radar imaging","","Article","Final","","Scopus","2-s2.0-85077436529"
"Wan J.; Zang J.; Liu S.","Wan, Jianhua (18937805700); Zang, Jinxia (57192239607); Liu, Shanwei (35173190400)","18937805700; 57192239607; 35173190400","Fusion and Classification of SAR and Optical Image with Consideration of Polarization Characteristics","2017","Guangxue Xuebao/Acta Optica Sinica","37","6","0628001","","","","10.3788/AOS201737.0628001","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024838545&doi=10.3788%2fAOS201737.0628001&partnerID=40&md5=9b82e08d86c612436a46112aad45fe5b","Full polarimetric synthetic aperture radar (SAR) possessed rich polarization information, it has a significant advantage for coverings recognition. A fusion method which took into account polarization characteristics of full polarimetric SAR is proposed based on SAR and medium resolution optical image. Full polarimetric SAR is carried out polarimetric target decomposition, polarization characteristics and optical image is fused with the improved hue, saturation, value (HSV) transform method. The fusion image is classified based on the object-oriented method. The results show that the proposed fusion method is superior to the traditional HSV fusion method for effectively using the polarimetric information and texture information of full polarimetric SAR. Object-oriented classification method can reduce the speckle noise of fusion image from SAR. The overall classification accuracy is better than that of high resolution optical image, and the classification accuracy of coverings which is sensitive to the polarization information is obviously better than that of high resolution optical image. © 2017, Chinese Lasers Press. All right reserved.","Classification (of information); Geometrical optics; Image classification; Image fusion; Polarimeters; Polarization; Radar; Remote sensing; Synthetic aperture radar; Full polarimetric synthetic aperture radar (SAR); High-resolution optical images; Object oriented classification; Optical image; Polarimetric informations; Polarimetric synthetic aperture radars; Polarimetric target decomposition; Polarization characteristics; Radar imaging","Classification; Full polarimetric synthetic aperture radar; Hue transform; Image fusion; Medium resolution optical image; Remote sensing","Article","Final","","Scopus","2-s2.0-85024838545"
"Li W.","Li, Weiguo (26664080400)","26664080400","Growth monitoring of winter wheat based on optical remote sensing and SAR data fusion","2016","37th Asian Conference on Remote Sensing, ACRS 2016","1","","","149","154","5","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018986086&partnerID=40&md5=1642e8c9c89b98cc593d90fd5f53e999","Multi-spectral remote sensing image in combination with radar image are conducive to the south area of extraction and crop growth monitoring. This study used three cities about Baoying, Gaoyou and Xinghua in the centre of Jiangsu Province in China as the study area, and made the Landsat / TM image and ERS/SAR image fusion in the winter wheat early jointing period, and then explored the remote sensing method of winter wheat planted area extraction. Based on the Optimum Index Factor (OIF) and spectral separability, selected bands 3-4-5 combination as the best band to classify. The traditional pixel-based classification results vulnerable to ""the feature in different spectrum"" and ""foreign feature with the spectrum"" effects. This study used object-oriented image classification approach with an object as a procession unit, and combined with a wealth of features in space, texture information for wheat area extraction, and then compared with pixel-based classification method (SVM classification) results. The results show the classification accuracy of SVM and object-oriented classification method is 78.59% and 94.16%, respectively. The object-oriented classification method can accurately extract the planting area of winter wheat, which is much better than the SVM classification method. Based on the extraction of winter wheat planting area, this study also monitored the winter wheat growth, and availably obtained the data and spatial distribution information of winter wheat in these counties. This method can give a technical support for the winter wheat planting area and growth information rapid access in the South China.","Classification (of information); Crops; Data fusion; Extraction; Image fusion; Image processing; Image reconstruction; Pixels; Radar imaging; Space optics; Space-based radar; Synthetic aperture radar; Classification accuracy; Classification approach; Growth conditions; Object oriented classification; Optical remote sensing; Pixel based classifications; Remote sensing images; Winter wheat; Remote sensing","Data fusion; Growth condition; Object-oriented classification; Winter wheat","Conference paper","Final","","Scopus","2-s2.0-85018986086"
"Fagir J.; Schubert A.; Frioud M.; Henke D.","Fagir, Julian (57194593493); Schubert, Adrian (7103401050); Frioud, Max (6602510079); Henke, Daniel (36011653700)","57194593493; 7103401050; 6602510079; 36011653700","SAR and oblique aerial optical image fusion for urban area image segmentation","2017","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","1W1","","639","642","3","10.5194/isprs-archives-XLII-1-W1-639-2017","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021154675&doi=10.5194%2fisprs-archives-XLII-1-W1-639-2017&partnerID=40&md5=343b88a8b9fae0fd3d0cf95d3a991188","The fusion of synthetic aperture radar (SAR) and optical data is a dynamic research area, but image segmentation is rarely treated. While a few studies use low-resolution nadir-view optical images, we approached the segmentation of SAR and optical images acquired from the same airborne platform - leading to an oblique view with high resolution and thus increased complexity. To overcome the geometric differences, we generated a digital surface model (DSM) from adjacent optical images and used it to project both the DSM and SAR data into the optical camera frame, followed by segmentation with each channel. The fused segmentation algorithm was found to out-perform the single-channel version.","Antennas; Geometrical optics; Image fusion; Radar imaging; Remote sensing; Synthetic aperture radar; Airborne platforms; Digital surface model (DSM); Dynamic researches; Geometric difference; RGB images; Segmentation algorithms; Single channels; Urban areas; Image segmentation","Image fusion; Image segmentation; Remote sensing; RGB image; SAR; Urban area","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85021154675"
"Chen J.; Ding Z.; Wei Y.; Gao Q.; Li Y.","Chen, Jing (57206951855); Ding, Zegang (23766773200); Wei, Yangkai (57216845072); Gao, Qiang (56482636000); Li, Yong (57203102668)","57206951855; 23766773200; 57216845072; 56482636000; 57203102668","Road Detection in High-resolution SAR Images with Improved Multiple Feature Fusion","2019","2019 International Radar Conference, RADAR 2019","","","9079137","","","","10.1109/RADAR41533.2019.171332","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084954701&doi=10.1109%2fRADAR41533.2019.171332&partnerID=40&md5=e1044c9c12cd8437f59f747a0e674301","In this paper, we propose a novel method for road region detection in high-resolution SAR images based on the fusion of multiple features. Compared with traditional SAR road detection methods with feature fusion, we exploit more useful features such as the standard deviation of directional radiance for distinguishing between roads and buildings or flatland. Then, the features are binarized with dynamic thresholds related to the cumulative possibility distribution of features. Finally, we define a membership parameter to fuse the binarized features and select the road candidate regions according to their geometric features, thereby ensuring better detection rate and lower false alarm rate. Experimental results of GF-3 SAR images show the effectiveness of the proposed method in the detection of both urban and suburban road regions. © 2019 IEEE.","Feature extraction; Image enhancement; Image fusion; Roads and streets; Synthetic aperture radar; Dynamic threshold; Geometric feature; High-resolution SAR; Multiple feature fusion; Multiple features; Possibility distributions; Region detection; Standard deviation; Radar imaging","dynamic thresholds; GF-3 SAR images; multiple features; road region detection","Conference paper","Final","","Scopus","2-s2.0-85084954701"
"Das A.; Sahi A.; Nandini U.","Das, Ananya (57194447184); Sahi, Abhishek (57194450917); Nandini, Usha (56492911400)","57194447184; 57194450917; 56492911400","SAR image segmentation for land cover change detection","2017","Proceedings of 2016 Online International Conference on Green Engineering and Technologies, IC-GET 2016","","","7916810","","","","10.1109/GET.2016.7916810","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020232578&doi=10.1109%2fGET.2016.7916810&partnerID=40&md5=b62b9e6ad5ec3481998cd052460ede09","Synthetic aperture radar is used for land cover change detection which can either be mounted on a drone or an aircraft, spacecraft. It can be used for land cover change detection by comparing two images which are taken at different intervals of time. For this we are using differencing methods. The operators used in differencing methods are log ratio and mean ratio. For obtaining a better difference in image. The image fusion technique is applied using complementary information obtained from log ratio and mean ratio differenced image then the image is segmented using k means clustering algorithms in which k centroids are placed as one for each cluster such that they are at maximum distance away from each other. The image obtained is compared with the ground truth, which has been already implemented. Now if the image is normal there is no change, and if the image is abnormal then the change has occurred during that time interval. © 2016 IEEE.","Aircraft detection; Clustering algorithms; Fighter aircraft; Image fusion; Integrated circuits; Radar; Radar imaging; Synthetic aperture radar; Change detection; Image fusion techniques; K-Means clustering algorithm; Land-cover change; Maximum distance; Non sub sampled contoured transform (NSCT); SAR image segmentation; Time interval; Image segmentation","Change detection; Image fusion; Non sub sampled contoured transform (NSCT); Synthetic Aperture Radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85020232578"
"Brigot G.; Koeniguer E.; Simard M.; Dupuis X.","Brigot, Guillaume (57190283800); Koeniguer, Elise (26534551800); Simard, Marc (57192705528); Dupuis, Xavier (8366163700)","57190283800; 26534551800; 57192705528; 8366163700","Fusion of LIDAR and POLINSAR images for forest vertical structure retrieval","2016","Proceedings of the European Conference on Synthetic Aperture Radar, EUSAR","","","7559363","","","","","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000997461&partnerID=40&md5=444b09227c2ba8e43a9033e1439a432a","This paper presents the overall strategy of fusion of LIDAR images and P-band PolInSAR images of forests in order to exploit their synergistic information. The images considered are from the 2010 BioSAR campaign on Remningstorp forest in Sweden. As a first step, we propose a pixel to pixel coregistration method of the images, and we assess the accuracy of this coregistration. Then, we propose descriptors for a given pixel, for each type of images. The primary challenge is to find features that go beyond the difference of geometrical configurations between the two types of information, and also that compensate the effect of the incidence angle on radar observables. As of today, we propose to analyse the combination of PolInSAR and lidar feature in order to see their variations depending on radar acquisition conditions and scatterer properties. We also test a support vector regression on two selected radar and lidar feature in order to draw some conclusions and improvements points. © VDE VERLAG GMBH · Berlin · Offenbach.","Forestry; Image fusion; Pixels; Radar; Radar imaging; Synthetic aperture radar; Coregistration; Descriptors; Geometrical configurations; Incidence angles; POLinSAR; Support vector regression (SVR); Vertical structures; Optical radar","","Conference paper","Final","","Scopus","2-s2.0-85000997461"
"Hughes L.H.; Merkle N.; Burgmann T.; Auer S.; Schmitt M.","Hughes, Lloyd Haydn (57201113391); Merkle, Nina (57194604557); Burgmann, Tatjana (57211533315); Auer, Stefan (57216043589); Schmitt, Michael (7401931279)","57201113391; 57194604557; 57211533315; 57216043589; 7401931279","Deep Learning for SAR-Optical Image Matching","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898635","4877","4880","3","10.1109/IGARSS.2019.8898635","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077692111&doi=10.1109%2fIGARSS.2019.8898635&partnerID=40&md5=c382fe0d785719e2bb15b46747bf7656","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors. © 2019 IEEE.","Convolutional neural networks; Data fusion; Deep learning; Geology; Geometrical optics; Image fusion; Image matching; Network architecture; Radar target recognition; Remote sensing; Synthetic aperture radar; Adversarial networks; Automatic matching; Multi sensor images; Optical image; Remote sensing imagery; Remote sensing missions; SAR Images; Target recognition; Radar imaging","Data Fusion; Deep Learning; Image Matching; Optical Images; SAR Images","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077692111"
"Geng J.; Fan J.; Wang H.","Geng, Jie (57200589477); Fan, Jianchao (24922808200); Wang, Hongyu (22037060600)","57200589477; 24922808200; 22037060600","Weighted Fusion-Based Representation Classifiers for Marine Floating Raft Detection of SAR Images","2017","IEEE Geoscience and Remote Sensing Letters","14","3","7831379","444","448","4","10.1109/LGRS.2017.2648641","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010660432&doi=10.1109%2fLGRS.2017.2648641&partnerID=40&md5=52bdf8a04ac0f18477e6cc60570f9db3","Detection of a marine floating raft is significant for ocean utilization, which provides a basis for marine ecosystem protection. In this case study, supervised classifiers of weighted fusion-based representation are proposed to detect marine floating raft using synthetic aperture radar images. To remove the speckle noise and obtain more discriminative features, a weighted low-rank matrix factorization (WLRMF) model is developed to optimize features before detection, where the matrix of patch features is decomposed to acquire the denoised features. Weighted fusion-based representation classifiers (WFRCs) with weighted multiplication are proposed to combine the sparse representation classifier (SRC) and the collaborative representation classifier (CRC) for floating raft detection, which can capture the competition between the floating raft and water surface as well as the collaboration within-class samples. Experiments on the study area of the Bohai Sea confirm that the proposed approach produces better results than some related methods. It is demonstrated that the WLRMF model extracts effective features and overcomes the influence of speckle noise at the same time, and the WFRC model is able to take advantages of the SRC in competition and CRC in collaboration for improving detection accuracies. © 2004-2012 IEEE.","Ecosystems; Factorization; Image classification; Image fusion; Marine radar; Radar imaging; Speckle; Synthetic aperture radar; Collaborative representations; Detection accuracy; Discriminative features; Ecosystem protection; Low-rank matrices; Sparse representation; Supervised classifiers; Weighted fusion; Feature extraction","Collaborative representation; object detection; sparse representation; synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85010660432"
"Shah E.; Jayaprasad P.; James M.E.","Shah, Esha (57210882835); Jayaprasad, P. (6506108165); James, M.E. (55233401000)","57210882835; 6506108165; 55233401000","Image Fusion of SAR and Optical Images for Identifying Antarctic Ice Features","2019","Journal of the Indian Society of Remote Sensing","47","12","","2113","2127","14","10.1007/s12524-019-01040-3","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074560479&doi=10.1007%2fs12524-019-01040-3&partnerID=40&md5=975c9c3df3b1430973c863e3690bc89f","Remote sensing data plays an important role in extracting thematic information from various sensors having different spectral, spatial and temporal resolutions. The present study aims at fusion of Radar Imaging Satellite-1 Fine Resolution Stripmap-1 and ResourceSAT-2 Linear Imaging Self Scanning Scanner-4 (LISS-4) images over Indian Antarctic Research Station Maitri and its surroundings to generate a better product which contains the characteristics of both the spectral information from LISS-4 and the spatial details of SAR. Different pixel-based fusion techniques such as Brovey Transform, Principal Component Analysis (PCA), Intensity Hue Saturation and Wavelet Principal Component Analysis (W-PCA) have been used in the present study. These image fusion techniques have been applied for the whole scene as well as for individual surface features like melt ponds, crevasses, freshwater lake, blue ice, oasis and lake ice for better discrimination of features. Quality assessment is performed by evaluating the performance of these algorithms using visual, spatial (High Pass Correlation Coefficient and Entropy) and spectral (Root Mean Square Error, Correlation Coefficient, ERGAS and Universal Quality Index) parameters. It is found that the identification of certain features such as crevasses, blue ice, melt ponds and lake ice has been improved with fused images compared to the original multi-spectral and SAR images. PCA and W-PCA fusion techniques offer better performance as compared to the rest of the techniques. © 2019, Indian Society of Remote Sensing.","Antarctica; East Antarctica; Maitri; algorithm; ice; image; image analysis; LISS; optical method; principal component analysis; spatial analysis; synthetic aperture radar; wavelet analysis","Antarctic ice features; Feature extraction; Image fusion; Image merging; ResourceSAT-2 LISS-4; RISAT-1 FRS-1; SAR","Article","Final","","Scopus","2-s2.0-85074560479"
"Seo D.K.; Eo Y.D.","Seo, Dae Kyo (57195943556); Eo, Yang Dam (7004110402)","57195943556; 7004110402","A learning-based image fusion for high-resolution SAR and panchromatic imagery","2020","Applied Sciences (Switzerland)","10","9","3298","","","","10.3390/app10093298","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085065424&doi=10.3390%2fapp10093298&partnerID=40&md5=77c3405091d78c0f7ba694c67cf55d8e","Image fusion is an effective complementary method to obtain information from multi-source data. In particular, the fusion of synthetic aperture radar (SAR) and panchromatic images contributes to the better visual perception of objects and compensates for spatial information. However, conventional fusion methods fail to address the differences in imaging mechanism and, therefore, they cannot fully consider all information. Thus, this paper proposes a novel fusion method that both considers the differences in imaging mechanisms and sufficiently provides spatial information. The proposed method is learning-based; it first selects data to be used for learning. Then, to reduce the complexity, classification is performed on the stacked image, and the learning is performed independently for each class. Subsequently, to consider sufficient information, various features are extracted from the SAR image. Learning is performed based on the model's ability to establish non-linear relationships, minimizing the differences in imaging mechanisms. It uses a representative non-linear regression model, random forest regression. Finally, the performance of the proposed method is evaluated by comparison with conventional methods. The experimental results show that the proposed method is superior in terms of visual and quantitative aspects, thus verifying its applicability. © 2020 by the authors.","","high-resolution; Image fusion; panchromatic image; random forest regression; SAR image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85085065424"
"Xin H.; Feng L.","Xin, Hongqiang (57208782443); Feng, Liangjie (55634949100)","57208782443; 55634949100","Research on image denoising algorithm based on improved anisotropic diffusion synthetic aperture radar","2019","Proceedings of SPIE - The International Society for Optical Engineering","11069","","110692I","","","","10.1117/12.2524184","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065794353&doi=10.1117%2f12.2524184&partnerID=40&md5=a3b263d49c878f398fa6b18f5aca0d6f","The presence of speckle noise seriously affects the application of synthetic aperture radar (SAR) images in image fusion, so it is especially important to suppress speckle noise. According to the formation mechanism of speckle noise, this paper proposes a SAR image speckle noise removal algorithm based on improved anisotropic diffusion. The algorithm improves the diffusion coefficient c(x) based on the P-M equation diffusion filter algorithm, and adds the iterative termination condition, and obtains the filtering algorithm suitable for SAR images. This method can not only solve the problem that there are many isolated noise points in the traditional P-M model filtering, but also has a good effect on image edge preservation. The simulation results show that the improved P-M model can eliminate noise well and maintain the edge information of the image well. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Diffusion; Image enhancement; Image fusion; Iterative methods; Optical anisotropy; Radar imaging; Radar signal processing; Speckle; Synthetic aperture radar; Anisotropic Diffusion; Image denoising algorithm; P-M model; SAR Images; Speckle noise; Speckle noise removal; Synthetic aperture radar (SAR) images; Termination condition; Image denoising","Anisotropic diffusion; Diffusion coefficient; P-M model; SAR image; Speckle noise","Conference paper","Final","","Scopus","2-s2.0-85065794353"
"Yu Z.; Wang T.; Zhang X.; Zhang J.; Ren P.","Yu, Zhiqiang (7404345818); Wang, Tingwei (57201195180); Zhang, Xi (24469469100); Zhang, Jie (55963073000); Ren, Peng (25960361900)","7404345818; 57201195180; 24469469100; 55963073000; 25960361900","Locality preserving fusion of multi-source images for sea-ice classification","2019","Acta Oceanologica Sinica","38","7","","129","136","7","10.1007/s13131-019-1464-2","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068752284&doi=10.1007%2fs13131-019-1464-2&partnerID=40&md5=229ee1cf6b587873e39072f64efe3496","We present a novel sea-ice classification framework based on locality preserving fusion of multi-source images information. The locality preserving fusion arises from two-fold, i.e., the local characterization in both spatial and feature domains. We commence by simultaneously learning a projection matrix, which preserves spatial localities, and a similarity matrix, which encodes feature similarities. We map the pixels of multi-source images by the projection matrix to a set fusion vectors that preserve spatial localities of the image. On the other hand, by applying the Laplacian eigen-decomposition to the similarity matrix, we obtain another set of fusion vectors that preserve the feature local similarities. We concatenate the fusion vectors for both spatial and feature locality preservation and obtain the fusion image. Finally, we classify the fusion image pixels by a novel sliding ensemble strategy, which enhances the locality preservation in classification. Our locality preserving fusion framework is effective in classifying multi-source sea-ice images (e.g., multi-spectral and synthetic aperture radar (SAR) images) because it not only comprehensively captures the spatial neighboring relationships but also intrinsically characterizes the feature associations between different types of sea-ices. Experimental evaluations validate the effectiveness of our framework. © 2019, Chinese Society for Oceanography and Springer-Verlag GmbH Germany, part of Springer Nature.","","ensemble classification; multi-source image fusion; sea-ice classification","Article","Final","","Scopus","2-s2.0-85068752284"
"Fernandez-Beltran R.; Haut J.M.; Paoletti M.E.; Plaza J.; Plaza A.; Pla F.","Fernandez-Beltran, Ruben (55838551300); Haut, Juan M. (57215636081); Paoletti, Mercedes E. (57027389000); Plaza, Javier (57195716301); Plaza, Antonio (7006613644); Pla, Filiberto (7006504936)","55838551300; 57215636081; 57027389000; 57195716301; 7006613644; 7006504936","Multimodal probabilistic latent semantic analysis for Sentinel-1 and Sentinel-2 image fusion","2018","IEEE Geoscience and Remote Sensing Letters","15","9","8392415","1347","1351","4","10.1109/LGRS.2018.2843886","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048871759&doi=10.1109%2fLGRS.2018.2843886&partnerID=40&md5=d7774707dbb8e26268524c1ae8c47fc8","Probabilistic topic models have recently shown a great potential in the remote sensing image fusion field, which is particularly helpful in land-cover categorization tasks. This letter first studies the application of probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation to remote sensing synthetic aperture radar (SAR) and multispectral imaging (MSI) unsupervised land-cover categorization. Then, a novel pLSA-based image fusion approach is presented, which pursues to uncover multimodal feature patterns from SAR and MSI data in order to effectively fuse and categorize Sentinel-1 and Sentinel-2 remotely sensed data. Experiments conducted over two different data sets reveal the advantages of the proposed approach for unsupervised land-cover categorization tasks. © 2018 IEEE.","Data structures; Feature extraction; Image analysis; Probabilistic logics; Radar imaging; Remote sensing; Semantics; Statistics; Synthetic aperture radar; Land cover; Latent Dirichlet allocation; Multispectral imaging; Probabilistic latent semantic analysis; Probabilistic topic models; Remote sensing images; Sentinel-1; Sentinel-2; data set; image analysis; land cover; numerical model; probability; remote sensing; satellite imagery; Sentinel; Image fusion","Image fusion; land-cover categorization; probabilistic latent semantic analysis (pLSA); Sentinel-1; Sentinel-2","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85048871759"
"Yi W.; Zeng Y.; Yuan Z.","Yi, Wei (57206484579); Zeng, Yong (36624647900); Yuan, Zheng (57206482317)","57206484579; 36624647900; 57206482317","Fusion of GF-3 SAR and Optical Images Based on the Nonsubsampled Contourlet Transform; [基于NSCT变换的高分三号SAR与光学图像融合]","2018","Guangxue Xuebao/Acta Optica Sinica","38","11","1110002","","","","10.3788/AOS201838.1110002","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061739023&doi=10.3788%2fAOS201838.1110002&partnerID=40&md5=7796f0c583e8aba96a36e8f36bedaafa","Among the existing synthetic-aperture radar (SAR) satellites, the GF-3 offers the most kinds of imaging modes. The fusion of the GF-3 SAR images with the multi-spectral images can improve the visual quality of the SAR images. We show how to use the nonsubsampled contourlet transform (NSCT) for simulating high-resolution images such that both the details of the SAR image and the spectral information of the multi-spectral image can be retained. This method ensures that the fusion of SAR and multi-spectral images is not limited by a specific algorithm. To verify the effectiveness of the proposed idea, two types of resolutions are used as the experimental data: the GF-3 satellite SAR images with resolutions of 3 m and 5 m, respectively, and the GF-1 satellite multi-spectral images with a resolution of 16 m. We perform comparative experiments with different fusion algorithms. The results show the effectiveness of the proposed approach. The traditional method that directly fuses the SAR and multi-spectral images can keep the details of the SAR image. However, the noise is obvious and some information of the multi-spectral image remains. The NSCT average images and the average NSCT images can retain the spectral information. The spectral information of NSCT average images is closer to the multi-spectral images than the average NSCT images. © 2018, Chinese Lasers Press. All right reserved.","Geometrical optics; Image enhancement; Image fusion; Image processing; Optical data processing; Satellites; Space-based radar; Spectroscopy; Synthetic aperture radar; Comparative experiments; Evaluation index; High resolution image; Multispectral images; Non subsampled contourlet transform (NSCT); Non-sub-sampled contourlet transforms; Optical image; Spectral information; Radar imaging","Evaluation index; GF-3 satellite; Image processing; Nonsubsampled contourlet transform; Synthetic-aperture radar and optical image","Article","Final","","Scopus","2-s2.0-85061739023"
"Tai J.; Pan B.; Zhao S.; Zhao Y.","Tai, Jianhao (57194470574); Pan, Bin (57206682357); Zhao, Shanshan (57220796676); Zhao, Yuan (57194463764)","57194470574; 57206682357; 57220796676; 57194463764","SAR and Multispectral Remote Sensing Image Fusion Method Using Shearlet Transform","2017","Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University","42","4","","468","474","6","10.13203/j.whugis20150768","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020263870&doi=10.13203%2fj.whugis20150768&partnerID=40&md5=ed5ae8f12c7bb106b53a4abc5c549939","In terms of conventional methods for SAR image and multispectral image fusion can't integrate and reserve good spectral information and spatial resolution at the same time, a new fusion method based on the difference of imaging mechanism of SAR and multispectral images is proposed in this paper. Firstly, the original image is decomposed by Shearlet transform, and the high frequency and low frequency components are obtained respectively. The two components contain different detailed information of the image. Shearlet transform decomposes the image into multi-scale and multi-directional sub-band coefficients, which contain different image features. In addition, Shearlet inverse transform has good image reconstruction capability. And then, according to that the low frequency coefficient and the high frequency coefficient represent different meanings, we design the different fusion rule for them. The fusion rules of low frequency coefficients based on region energy and the high frequency coefficient based on improved pulse coupled neural network are designed. Finally, an information-rich image is obtained by inversing Shearlet transform. Therefore, the fusion results are richer and contain more spatial detail information and spectral information. In order to verify the effectiveness of the proposed method, a test is carried out with data from TerraSAR-X and Landsat5-TM, and the result shows that the proposed method is effective in improving the spatial resolution and keeping more spectral information. Compared with the methods of wavelet transform, contourlet transform, and NSCT transform, this method has a significant improvement in spatial information and spectral information. Cross entropy has a margin of improvement of nearly 100%. The correlation coefficient is higher than 25% increase, and the spectral distortion is better than 40% increase. © 2017, Research and Development Office of Wuhan University. All right reserved.","Image enhancement; Image fusion; Image reconstruction; Image resolution; Inverse problems; Inverse transforms; Neural networks; Remote sensing; Spectroscopy; Synthetic aperture radar; Wavelet transforms; Imaging properties; Multi-source images; Multispectral images; PCNN; Shearlet transforms; imaging method; multispectral image; radar imagery; remote sensing; spatial resolution; synthetic aperture radar; transform; Radar imaging","Difference of imaging property; Multi-spectral images; Multisource image fusion; PCNN; SAR; Shearlet transform","Article","Final","","Scopus","2-s2.0-85020263870"
"Ghoneim E.","Ghoneim, Eman (16039236900)","16039236900","Rimaal: A sand buried structure of possible impact origin in the Sahara: Optical and radar remote sensing investigation","2018","Remote Sensing","10","6","880","","","","10.3390/rs10060880","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048969693&doi=10.3390%2frs10060880&partnerID=40&md5=c1db92ec0f8e227ff051f6139492672d","This work communicates the discovery of a sandy buried 10.5 km diameter near-circular structure in the eastern part of the Great Sahara in North Africa. Rimaal, meaning ""sand"" in Arabic, is given as the name for this structure since it is largely concealed beneath the Sahara Aeolian sand. Remote sensing image fusion and transformation of multispectral data (from Landsat-8) and synthetic aperture radar (from Sentinel-1 and ALOS PALSAR), of dual wavelengths (C and L-bands) and multi-polarization (HV, VV, HH, and HV), were adopted in this work. The optical and microwave hybrid imagery enabled the combining of surface spectral properties and subsurface roughness information for better understanding of the Rimaal structure. The long wavelength of the radar, in particular, enabled the penetration of desert sands and the revealing of the proposed structure. The structure exhibits a clear outer rim with traces of concentric faults, an annular flat basin and an inner ring surrounding remnants of a highly eroded central peak. Radar imagery clearly shows the interior wall of the structure is incised with radial pattern gullies that originate at or near the crater periphery, implying a much steeper rim wall in the past. In addition, data reveals a circumferential of a paleoriver course that flows along a curved path parallel to the crater's western margin indicating the plausible presence of a concentric ring graben related to the inferred structure. The defined crater boundary is coincident with a shallow semi-circular-like basin in the SRTM elevation data. The structure portrays considerable modifications by extensive long-term Aeolian and fluvial erosion. Residing in the Cretaceous Nubian Sandstone formation suggests an old age of ≤65 Ma for the structure. If proven to be of an impact origin, the Rimaal structure could help in understanding the early evolution of the landscape of the Eastern Sahara and holds promise for hosting economically valuable ore deposits and hydrocarbon resources in the region. © 2018 by the authors.","Data fusion; Image fusion; Metadata; Ore deposits; Radar imaging; Remote sensing; Sand; Synthetic aperture radar; Tracking radar; ALOS PALSAR; Hydrocarbon resources; Minimum noise fraction; Radar remote sensing; Remote sensing images; Sandstone formations; Sentinel-1; SRTM; Space-based radar","ALOS PALSAR; Data fusion; Minimum noise fraction; Sentinel-1; SRTM","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048969693"
"Yiquan W.; Zhilai W.","Yiquan, Wu (55850753800); Zhilai, Wang (57201858289)","55850753800; 57201858289","SAR and infrared image fusion in complex contourlet domain based on joint sparse representation","2017","Journal of Radars","6","4","","349","358","9","10.12000/JR17019","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046271642&doi=10.12000%2fJR17019&partnerID=40&md5=d21589eb5dc105628771ce288a97a4f5","To investigate the problems of the large grayscale difference between infrared and Synthetic Aperture Radar (SAR) images and their fusion image not being fit for human visual perception, we propose a fusion method for SAR and infrared images in the complex contourlet domain based on joint sparse representation. First, we perform complex contourlet decomposition of the infrared and SAR images. Then, we employ the K-Singular Value Decomposition (K-SVD) method to obtain an over-complete dictionary of the low-frequency components of the two source images. Using a joint sparse representation model, we then generate a joint dictionary. We obtain the sparse representation coefficients of the low-frequency components of the source images in the joint dictionary by the Orthogonal Matching Pursuit (OMP) method and select them using the selection maximization strategy. We then reconstruct these components to obtain the fused low-frequency components and fuse the high-frequency components using two criteria——the coefficient of visual sensitivity and the degree of energy matching. Finally, we obtain the fusion image by the inverse complex contourlet transform. Compared with the three classical fusion methods and recently presented fusion methods, e.g., that based on the Non-Subsampled Contourlet Transform (NSCT) and another based on sparse representation, the method we propose in this paper can effectively highlight the salient features of the two source images and inherit their information to the greatest extent. © 2018 CSIC.","","Complex contourlet transform; Image fusion; Infrared image; Joint sparse representation; Synthetic Aperture Radar (SAR) image","Article","Final","","Scopus","2-s2.0-85046271642"
"Li L.; Liu G.; Jin Q.; He C.; Huang Y.; Yao Y.","Li, Lei (57756339800); Liu, Guang (48461511700); Jin, Qingwen (57192237377); He, Chengxin (7402285036); Huang, Yuqing (25622974300); Yao, Yuefeng (37115053700)","57756339800; 48461511700; 57192237377; 7402285036; 25622974300; 37115053700","BEMD-based high resolution image fusion for land cover classification: A case study in Guilin","2016","IOP Conference Series: Earth and Environmental Science","46","1","012053","","","","10.1088/1755-1315/46/1/012053","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002050743&doi=10.1088%2f1755-1315%2f46%2f1%2f012053&partnerID=40&md5=99643fac5c7f66becd57eb81babf2033","Analysis of image texture feature can help to reduce the adverse effects of the condition that same object but different band or same band but different object. Therefore, if it can add and highlight the texture information to the remote sensing image, it will be very helpful in the classification of ground objects. In this paper we consider to add SAR image information in classification. Bidimensional empirical mode decomposition (BEMD) has been widely applied to the analysis of non-stationary and non-linear signals. This paper proposes a new method for fusing high resolution SAR and optical image in Guilin area, based on Bidimensional empirical mode decomposition (BEMD) method. © Published under licence by IOP Publishing Ltd.","China; Guangxi Zhuangzu; Guilin; Classification (of information); Geometrical optics; Image fusion; Image reconstruction; Radar imaging; Remote sensing; Synthetic aperture radar; Adverse effect; Bi dimensional empirical mode decomposition (BEMD); High resolution image; High-resolution SAR; Land cover classification; Remote sensing images; Texture features; Texture information; decomposition analysis; empirical analysis; image classification; image processing; land cover; remote sensing; spatial resolution; Image texture","","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85002050743"
"Saidi F.; Chen J.; Wang P.","Saidi, Faycal (57194090032); Chen, Jie (55909160300); Wang, Pengbo (55507662000)","57194090032; 55909160300; 55507662000","A refined automatic co-registration method for high-resolution optical and sar images by maximizing mutual information","2017","2016 IEEE International Conference on Signal and Image Processing, ICSIP 2016","","","7888258","231","235","4","10.1109/SIPROCESS.2016.7888258","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018692058&doi=10.1109%2fSIPROCESS.2016.7888258&partnerID=40&md5=5df5ca0ef00ab849ab9a3580c83a541a","The use of multisensors images for different applications, like change detection and image fusion, require an image to image registration. Actually, the image registration becomes a crucial task with the continuous increase in image resolution, especially in urban area. In this paper, a refined automatic Mutual Information based registration approach of spaceborne Synthetic Aperture radar (SAR) and optical image is proposed. The data set include TerraSar-X (1m) and Worldview-2 (0.5m) images and cover urban area of San Francisco. Our approach is divided into two steps. First, we use Discrete Canny edge detector to extract contours from the optical image, and Gabor-Wavelet filter to extract edge from the SAR image. Second, a rigid transformation (rotation and translation) is applied to the contours image obtained by Canny detector. Then, for each value of transformation, the Mutual Information is computed between this transformed image and the feature image outcome of application of Gabor-wavelet to the SAR image. The best transformation parameters are obtained when the Mutual Information is maximal. Also, in this work, we compared the proposed approach with the basic Mutual Information intensity based registration method. The results obtained demonstrated that our approach improve the registration accuracy comparatively to the basic Mutual Information intensity based registration method. © 2016 IEEE.","Edge detection; Geometrical optics; Image fusion; Image processing; Image registration; Image resolution; Optical data processing; Synthetic aperture radar; Canny edge detectors; Gabor wavelet filters; Gabor wavelets; Image-to-image registration; Mutual informations; SAR Images; Spaceborne synthetic aperture radars; Transformation parameters; Radar imaging","Canny edge detector; Gabor-Wavelet; Image registration; Mutual information; SAR image","Conference paper","Final","","Scopus","2-s2.0-85018692058"
"Aslam K.; Khalil R.Z.; Ul Haque S.","Aslam, Khusharah (57214798201); Khalil, Rao Zahid (57188730559); Ul Haque, Saad (57222246346)","57214798201; 57188730559; 57222246346","Comparative analysis of landuse land cover between optical and fused image with SAR","2019","Proceedings of the International Astronautical Congress, IAC","2019-October","","IAC-19_B5_1_10_x55063","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079121833&partnerID=40&md5=323d60e902f77beb821f6d06f98a7cb8","Image fusion is a technique that integrates complementary information from multiple remote sensing images such that the fused image is more suitable for processing task and information extraction. Passive sensors are capable of sensing the reflected electromagnetic energy in the visible and infrared region while active sensors provide additional information using microwave region. This broad spectrum provides more information of earth surface as compared to optical data alone. This study compares the land cover classification results of optical imagery (Landsat-8) and fused imagery (Landsat-8 and Sentinel-1 VV polarized data). The image fusion was then performed using wavelet transformation technique. The data were classified into four classes namely water bodies, built-up area, vegetation cover, and barren land. Google Earth and Landsat imagery were used as a reference image for accuracy assessment. The fused image showed higher accuracy than optical image i.e. Kappa coefficient increased from 0.78 to 0.9 and overall accuracy increased from 89.4% to 92.7%. This study indicates that multi-source information i.e., image fusion can significantly improve the interpretation and accuracy of classification. Copyright © 2019 by the International Astronautical Federation (IAF). All rights reserved.","Classification (of information); Electromagnetic waves; Fusion reactions; Geometrical optics; Image enhancement; Image fusion; Microwave sensors; Remote sensing; Space-based radar; Synthetic aperture radar; Accuracy assessment; Accuracy of classifications; Comparative analysis; Land cover classification; Multi-source informations; Overall accuracies; Remote sensing images; Wavelet transformations; Radar imaging","Fusion; Land cover classification; Synthetic Aperture Radar","Conference paper","Final","","Scopus","2-s2.0-85079121833"
"Dimov D.; Kuhn J.; Conrad C.","Dimov, D. (7006792019); Kuhn, J. (57202621035); Conrad, C. (7102811204)","7006792019; 57202621035; 7102811204","ASSESSMENT of CROPPING SYSTEM DIVERSITY in the FERGANA VALLEY THROUGH IMAGE FUSION of LANDSAT 8 and SENTINEL-1","2016","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","3","","","173","180","7","10.5194/isprs-annals-III-7-173-2016","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041859846&doi=10.5194%2fisprs-annals-III-7-173-2016&partnerID=40&md5=70854c77657a54196418d67d8fab73a7","In the transitioning agricultural societies of the world, food security is an essential element of livelihood and economic development with the agricultural sector very often being the major employment factor and income source. Rapid population growth, urbanization, pollution, desertification, soil degradation and climate change pose a variety of threats to a sustainable agricultural development and can be expressed as agricultural vulnerability components. Diverse cropping patterns may help to adapt the agricultural systems to those hazards in terms of increasing the potential yield and resilience to water scarcity. Thus, the quantification of crop diversity using indices like the Simpson Index of Diversity (SID) e.g. through freely available remote sensing data becomes a very important issue. This however requires accurate land use classifications. In this study, the focus is set on the cropping system diversity of garden plots, summer crop fields and orchard plots which are the prevalent agricultural systems in the test area of the Fergana Valley in Uzbekistan. In order to improve the accuracy of land use classification algorithms with low or medium resolution data, a novel processing chain through the hitherto unique fusion of optical and SAR data from the Landsat 8 and Sentinel-1 platforms is proposed. The combination of both sensors is intended to enhance the objects textural and spectral signature rather than just to enhance the spatial context through pansharpening. It could be concluded that the Ehlers fusion algorithm gave the most suitable results. Based on the derived image fusion different object-based image classification algorithms such as SVM, Naïve Bayesian and Random Forest were evaluated whereby the latter one achieved the highest classification accuracy. Subsequently, the SID was applied to measure the diversification of the three main cropping systems.","","Crop mapping; Image classification; Image Fusion; Sentinel-1; Synthetic Aperture Radar","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85041859846"
"Yao H.; Wang H.; Lin X.","Yao, Hongyuan (57204426600); Wang, Haipeng (57194230743); Lin, Xueyuan (57204428887)","57204426600; 57194230743; 57204428887","A SAR image fast stitching algorithm based on machine learning","2018","Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","251","","","559","564","5","10.1007/978-3-030-00557-3_55","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055578549&doi=10.1007%2f978-3-030-00557-3_55&partnerID=40&md5=c80cf2bbdfa56ee8bae3d02e3e3beff8","Aiming at the splicing problem of Synthetic Aperture Radar (SAR) image, an improved algorithm for SURF is proposed to realize the fast splicing of SAR image. The SURF feature descriptor has scale invariance and rotation invariance, and has strong robustness to light intensity and affine transmission variation. The improved algorithm uses machine learning methods to build a binary classifier that identifies the key feature points in the SURF extracted feature points and eliminates the key feature points. In addition, the relief-F algorithm is used to reduce the dimensionality of the improved SURF descriptor to complete image registration. In the image fusion stage, a weighted fusion algorithm with a threshold is used to achieve seamless image mosaic. Experimental results show that the improved algorithm has strong real-time performance and robustness, and improves the efficiency of image registration. It can accurately mosaic multiple SAR images. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2018.","Artificial intelligence; Image enhancement; Image fusion; Image registration; Learning algorithms; Learning systems; Light transmission; Synthetic aperture radar; Image stitching; Machine learning methods; Real time performance; SAR Images; Seamless image mosaic; SURF; Synthetic aperture radar (SAR) images; Transmission variation; Radar imaging","Fast image stitching; Image fusion; Machine learning; SAR image; SURF","Conference paper","Final","","Scopus","2-s2.0-85055578549"
"Shen D.; Zhang J.; Yang J.; Feng D.; Li J.","Shen, Donghao (57201982474); Zhang, Junhao (56368684700); Yang, Jie (15039078800); Feng, Deying (35219755900); Li, Jiang (58044739100)","57201982474; 56368684700; 15039078800; 35219755900; 58044739100","SAR and optical image registration based on edge features","2017","2017 4th International Conference on Systems and Informatics, ICSAI 2017","2018-January","","","1272","1276","4","10.1109/ICSAI.2017.8248481","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046656558&doi=10.1109%2fICSAI.2017.8248481&partnerID=40&md5=a51de8f83019d69f917cf7f28fef93f3","As synthetic aperture radar (SAR) data is more and more popular in recent days, the fusion of SAR and optical image become inevitable and desirable problem. Image Registration is the fundamental for data fusion, and has been wildly studied. However, literature on registration between SAR and optical image remain little, which means the problem is still challenge and open. On one hand, the SAR and optical image are from totally different sensor with variant imaging mechanism. On the other hand, Simple features could not enough to maintain the accuracy and efficiency of multi-modal registration. We propose a new method for SAR and optical image registration. Different from current methods mainly based on intensity feature or point feature, our method matches two images by their boundary features. A globalized boundary detection algorithm are used for feature extraction. The extracted boundaries were then matched by coherence point drift algorithm. Experiments prove that our method could achieve pixel-level precision, and perform robust for different resolutions. © 2017 IEEE.","Feature extraction; Geometrical optics; Image fusion; Image registration; Synthetic aperture radar; Boundary detection; Boundary detection algorithms; Different resolutions; Imaging mechanism; Intensity features; Multimodal registration; Point features; Point-set registrations; Radar imaging","boundary detection; point set registration; SAR and Optical image registration; synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85046656558"
"Liu Y.; Liao G.; Li S.; Nie G.; Zeng C.; Du P.","Liu, Yang (56024956400); Liao, Guisheng (10042143700); Li, Shidong (36816137300); Nie, Guoli (57216434922); Zeng, Cao (8981022300); Du, Peiju (57216435309)","56024956400; 10042143700; 36816137300; 57216434922; 8981022300; 57216435309","SAR Image Fusion Based on Mathematical Frame","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048460","","","","10.1109/APSAR46974.2019.9048460","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083508262&doi=10.1109%2fAPSAR46974.2019.9048460&partnerID=40&md5=3467bc74d11f297915ca4b6e70d7e57d","In this paper, we proposed a frame-based approach to synthetic aperture radar (SAR) images fusion. Specifically, the method consists two parts: image fusion preprocessing and fusion reconstruction. Firstly, we fuse a set of low-resolution (LR) SAR images to form an aligned coarse SAR image which still has image blur caused by the point spread function (PSF) by fusion preprocessing. Then Frame fundamental iterative regularization is used to reconstruct high-resolution (HR) SAR image. Both simulated and real SAR images are used to verify the validity of the method. The results show that details and definition of the LR image processed with the method are effectively improved. © 2019 IEEE.","Image enhancement; Image fusion; Image reconstruction; Iterative methods; Optical transfer function; Synthetic aperture radar; Frame-based; High resolution; Image blur; Iterative regularization; Low resolution; SAR Images; Synthetic aperture radar (SAR) images; Radar imaging","frame; image fusion; regularization; SAR","Conference paper","Final","","Scopus","2-s2.0-85083508262"
"Wenyan Z.; Zhenhong J.; Yu Y.; Yang J.; Kasabov N.","Wenyan, Zhou (58029422700); Zhenhong, Jia (24478104000); Yu, Yinfeng (56143859600); Yang, Jie (15039078800); Kasabov, Nilola (35585005300)","58029422700; 24478104000; 56143859600; 15039078800; 35585005300","SAR image change detection based on equal weight image fusion and adaptive threshold in the NSST domain","2018","European Journal of Remote Sensing","51","1","","785","794","9","10.1080/22797254.2018.1491804","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050829249&doi=10.1080%2f22797254.2018.1491804&partnerID=40&md5=1326ab4e96e1bbf2dffebcfd29ceb9b1","In order to improve the accuracy of change detection and reduce the running time, a change detection method based on equal weight image fusion and adaptive threshold in the NSST domain is proposed. First, the logarithmic transformation is used to transform images and the mean filter is applied to the transformed images. The log-ratio method and the mean ratio method are adopted to generate two kinds of difference images. The final difference image is achieved by equal weight image fusion method. Then, an adaptive threshold denoising method based on non-subsampled shearlet transform (NSST) is used to achieve noise reduction. Finally, the k-means clustering algorithm is utilized to get the change detection results. The experimental results show that the proposed algorithm has better change detection performance than the reference algorithms in visual effect and objective parameters. © 2018, © 2018 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Image enhancement; K-means clustering; Noise abatement; Radar imaging; Synthetic aperture radar; Adaptive thresholds; Change detection; Image fusion methods; K-mean algorithms; Logarithmic transformations; Objective parameters; Reference algorithm; Shearlet transforms; algorithm; detection method; image analysis; noise; numerical method; synthetic aperture radar; threshold; transform; Image fusion","adaptive threshold; change detection; difference map; image fusion; k-mean algorithm; Non-subsampled shearlet transform","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85050829249"
"Alves E.I.; Andrade A.I.A.S.S.; Vaz D.A.","Alves, Eduardo I. (55666115400); Andrade, Ana I. A. S. S. (55932122500); Vaz, David A. (23399168900)","55666115400; 55932122500; 23399168900","A Better View over Titan Drainage Networks Through RGB Fusion of Cassini SAR Images","2018","IEEE Geoscience and Remote Sensing Letters","15","3","","414","418","4","10.1109/LGRS.2018.2791018","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041412294&doi=10.1109%2fLGRS.2018.2791018&partnerID=40&md5=a2ea8a2c8e3c6895e420e566f38f835a","We present a simple method to enhance the view of Titan drainage networks, thus allowing extracting relevant hydrological parameters. The method uses RGB fusion of three Cassini synthetic aperture radar images acquired at different times, and is tested on one drainage network. Comparison with previous studies of the same network shows an increase in all the measured parameters. The present results help constrain previous estimates of erosion times, terrain, and tectonic models for the area and indicate that, whenever possible, geomorphological inference from drainage network geometry should be drawn on multiply sampled scenes. © 2004-2012 IEEE.","Drainage; Image enhancement; Image fusion; Radar imaging; Cassini; Drainage networks; Hydrological parameters; Measured parameters; SAR Images; SIMPLE method; Tectonic models; Titan; drainage network; geometry; image analysis; parameter estimation; satellite imagery; source parameters; synthetic aperture radar; Titan; Synthetic aperture radar","Drainage networks; image enhancement; image fusion; synthetic aperture radar (SAR); Titan","Article","Final","","Scopus","2-s2.0-85041412294"
"Lin K.; Li W.; Liu H.; Wu J.","Lin, Kai (57219257273); Li, Wenmei (54789086100); Liu, Haiyan (57205514225); Wu, Jiaqi (57218157216)","57219257273; 54789086100; 57205514225; 57218157216","Different Levels Multi-source Remote Sensing Image Fusion","2019","ICSIDP 2019 - IEEE International Conference on Signal, Information and Data Processing 2019","","","9173281","","","","10.1109/ICSIDP47821.2019.9173281","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091925167&doi=10.1109%2fICSIDP47821.2019.9173281&partnerID=40&md5=7bccdb162a8ba32e21525f22a751e9fc","In order to study the fusion methods of multi-source remote sensing image, this paper plans to fuse the optical images and synthetic aperture radar (SAR) images in pixel level, feature level and decision level, respectively. For the purpose of combining the merits of fusion at aforementioned levels, the fusion results are assigned different weights for multi-level fusion. Several qualitative and quantitative evaluations are also presented, and they show that the multi-level fusion does pretty-well performance in brightness, sharpness, contrast, amount of information, classification effect, etc. It concludes that the multi-level fusion can significantly extract much information while retaining more details of the original image. © 2019 IEEE.","Classification (of information); Data handling; Geometrical optics; Radar imaging; Remote sensing; Synthetic aperture radar; Amount of information; Decision levels; Multi level fusion; Original images; Quantitative evaluation; Remote sensing images; Synthetic aperture radar (SAR) images; Well performance; Image fusion","fusion; Multi-source remote sensing image; qualitative and quantitative evaluation; SAR","Conference paper","Final","","Scopus","2-s2.0-85091925167"
"Abdikan S.","Abdikan, Saygin (55515101500)","55515101500","Exploring image fusion of ALOS/PALSAR data and LANDSAT data to differentiate forest area","2018","Geocarto International","33","1","","21","37","16","10.1080/10106049.2016.1222635","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983518063&doi=10.1080%2f10106049.2016.1222635&partnerID=40&md5=eb3e89a79bf195626c1247cd4d762e33","Remote sensing data utilize valuable information via various satellite sensors that have different specifications. Image fusion allows the user to combine different spatial and spectral resolutions to improve the information for purposes such as forest monitoring and land cover mapping. In this study, I assessed the contribution of dual-polarized Advanced Land Observing Satellite/Phased Array type L-band Synthetic Aperture Radar data to multispectral Landsat imagery. The research investigated the separability of forested areas using different image fusion techniques. Quality analysis of the fused images was conducted using qualitative and quantitative analyses. I applied the support vector machine image classification method for land cover mapping. Among all methods examined, the à trous wavelet transform method best differentiated the forested area with an overall accuracy (OA) of 94.316%, while Landsat had an OA of 92.626%. The findings of this study indicated that optical-SAR-fused images improve land cover classification, which results in higher quality forest inventory data and mapping. © 2016 Informa UK Limited, trading as Taylor & Francis Group.","ALOS; environmental monitoring; forest inventory; image classification; land cover; Landsat thematic mapper; mapping; PALSAR; remote sensing; satellite data; spatial resolution; spectral resolution; synthetic aperture radar","ALOS/PALSAR; image classification; image fusion; land cover; Landsat TM","Article","Final","","Scopus","2-s2.0-84983518063"
"Ma W.; Yang H.; Wu Y.; Xiong Y.; Hu T.; Jiao L.; Hou B.","Ma, Wenping (57205878746); Yang, Hui (57204815281); Wu, Yue (56215531900); Xiong, Yunta (57204813607); Hu, Tao (57220951617); Jiao, Licheng (7102491544); Hou, Biao (7102142690)","57205878746; 57204815281; 56215531900; 57204813607; 57220951617; 7102491544; 7102142690","Change detection based on multi-grained cascade forest and multi-scale fusion for SAR images","2019","Remote Sensing","11","2","142","","","","10.3390/rs11020142","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060678523&doi=10.3390%2frs11020142&partnerID=40&md5=dc5d21ed5c8e0ad1da1cacf631d8c21b","In this paper, a novel change detection approach based on multi-grained cascade forest (gcForest) and multi-scale fusion for synthetic aperture radar (SAR) images is proposed. It detects the changed and unchanged areas of the images by using the well-trained gcForest. Most existing change detection methods need to select the appropriate size of the image block. However, the single size image block only provides a part of the local information, and gcForest cannot achieve a good effect on the image representation learning ability. Therefore, the proposed approach chooses different sizes of image blocks as the input of gcForest, which can learn more image characteristics and reduce the influence of the local information of the image on the classification result as well. In addition, in order to improve the detection accuracy of those pixels whose gray value changes abruptly, the proposed approach combines gradient information of the difference image with the probability map obtained from the well-trained gcForest. Therefore, the image edge information can be enhanced and the accuracy of edge detection can be improved by extracting the image gradient information. Experiments on four data sets indicate that the proposed approach outperforms other state-of-the-art algorithms. © 2019 by the authors.","Classification (of information); Edge detection; Forestry; Fusion reactions; Image fusion; Radar imaging; Synthetic aperture radar; Change detection; Classification results; GcForest; Gradient informations; Image characteristics; Image representations; State-of-the-art algorithms; Synthetic aperture radar (SAR) images; Image enhancement","Change detection; Fusion; GcForest; Gradient information; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85060678523"
"Liu C.; Qi Y.; Ding W.","Liu, Chunhui (55264234000); Qi, Yue (35756411200); Ding, Wenrui (15833795300)","55264234000; 35756411200; 15833795300","Airborne SAR and optical image fusion based on IHS transform and joint non-negative sparse representation","2016","International Geoscience and Remote Sensing Symposium (IGARSS)","2016-November","","7730877","7196","7199","3","10.1109/IGARSS.2016.7730877","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007482665&doi=10.1109%2fIGARSS.2016.7730877&partnerID=40&md5=19ceecc26bdc9734d96f3c3ea8b932b4","In this paper, a novel airborne synthetic aperture radar (SAR) and optical image fusion method based on Intensity-Hue-Saturation (IHS) and joint non-negative sparse representation (JNNSR) is proposed. Firstly, the color optical image is transformed into IHS space. Then, the intensity component of the optical image and the SAR image are decomposed by JNNSR into common component and innovation components. Based on the non-negative property of the sparse coefficients, the innovation component of the SAR image is fused with the intensity component of the optical image. Finally, the fused result is obtained by performing inverse IHS transform. The experimental result shows that our method is superior to the traditional methods in terms of several universal quality evaluation indexes, as well as in the visual quality. © 2016 IEEE.","","Airborne optical image; Airborne SAR image; IHS; Image fusion; Joint sparse representation","Conference paper","Final","","Scopus","2-s2.0-85007482665"
"Seo D.K.; Kim Y.H.; Eo Y.D.; Park W.Y.","Seo, Dae Kyo (57195943556); Kim, Yong Hyun (56195702200); Eo, Yang Dam (7004110402); Park, Wan Yong (55634630800)","57195943556; 56195702200; 7004110402; 55634630800","COSMO-SkyMed 2 image color mapping using random forest regression","2017","Journal of the Korean Society of Surveying, Geodesy, Photogrammetry and Cartography","35","4","","319","326","7","10.7848/ksgpc.2017.35.4.319","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030330561&doi=10.7848%2fksgpc.2017.35.4.319&partnerID=40&md5=1873d227922cbca31f39dfc6ea404a80","SAR (Synthetic aperture radar) images are less affected by the weather compared to optical images and can be obtained at any time of the day. Therefore, SAR images are being actively utilized for military applications and natural disasters. However, because SAR data are in grayscale, it is difficult to perform visual analysis and to decipher details. In this study, we propose a color mapping method using RF (random forest) regression for enhancing the visual decipherability of SAR images. COSMO-SkyMed 2 and WorldView-3 images were obtained for the same area and RF regression was used to establish color configurations for performing color mapping. The results were compared with image fusion, a traditional color mapping method. The UIQI (universal image quality index), the SSIM (structural similarity) index, and CC (correlation coefficients) were used to evaluate the image quality. The color-mapped image based on the RF regression had a significantly higher quality than the images derived from the other methods. From the experimental result, the use of color mapping based on the RF regression for SAR images was confirmed.","color; COSMO-SkyMed; forest dynamics; image analysis; mapping; regression analysis; synthetic aperture radar; WorldView","Color mapping; COSMO-SkyMed 2; Random forest regression; Synthetic aperture radar; WorldView-3","Article","Final","","Scopus","2-s2.0-85030330561"
"Kedar M.; Rege P.P.","Kedar, Manali (57204896286); Rege, Priti P. (6701858789)","57204896286; 6701858789","Wavelet Transform-Based Fusion of SAR and Multispectral Images","2020","Lecture Notes in Electrical Engineering","642","","","261","275","14","10.1007/978-981-15-2854-5_24","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084000942&doi=10.1007%2f978-981-15-2854-5_24&partnerID=40&md5=be2bb2097c2f54cc933e6243782f3372","Synthetic Aperture Radar (SAR) image is rich in spatial information while the optical Multispectral (MS) image is rich in spectral information. In this research, the potential of wavelet transforms for fusion of SAR and MS images is investigated. The research aims to study the effect of fusion of MS image with SAR image using different wavelet families and the influence of wavelet decomposition level and filter length on the fusion results. A fusion algorithm based on wavelet transform with multiple fusion rules is used. A 36 m spatial resolution SAR image acquired from RISAT-1 and an optical MS image of 56 m spatial resolution acquired from AWiFS are used for fusion. Fusion results are analysed visually and statistically. © 2020, Springer Nature Singapore Pte Ltd.","Image compression; Image fusion; Image resolution; Nanoelectronics; Synthetic aperture radar; Wavelet decomposition; And filters; Different wavelets; Fusion algorithms; Multispectral images; Spatial informations; Spatial resolution; Spectral information; Synthetic aperture radar (SAR) images; Radar imaging","Image fusion; Multispectral image; SAR image; Wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85084000942"
"Liu C.; Zhang D.; Zhao X.","Liu, Chunhui (55264234000); Zhang, Duona (57203076170); Zhao, Xintao (57201726329)","55264234000; 57203076170; 57201726329","Multitask saliency detection model for synthetic aperture radar (SAR) image and its application in SAR and optical image fusion","2018","Journal of Electronic Imaging","27","2","023026","","","","10.1117/1.JEI.27.2.023026","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045956258&doi=10.1117%2f1.JEI.27.2.023026&partnerID=40&md5=499945409fdd583e1d2bb21ef707a9c5","Saliency detection in synthetic aperture radar (SAR) images is a difficult problem. This paper proposed a multitask saliency detection (MSD) model for the saliency detection task of SAR images. We extract four features of the SAR image, which include the intensity, orientation, uniqueness, and global contrast, as the input of the MSD model. The saliency map is generated by the multitask sparsity pursuit, which integrates the multiple features collaboratively. Detection of different scale features is also taken into consideration. Subjective and objective evaluation of the MSD model verifies its effectiveness. Based on the saliency maps obtained by the MSD model, we apply the saliency map of the SAR image to the SAR and color optical image fusion. The experimental results of real data show that the saliency map obtained by the MSD model helps to improve the fusion effect, and the salient areas in the SAR image can be highlighted in the fusion results. © 2018 SPIE and IS&T.","Feature extraction; Geometrical optics; Image enhancement; Image fusion; Image segmentation; Synthetic aperture radar; Tracking radar; Features extraction; Global contrasts; ITS applications; Multiple features; Optical image; Saliency detection; Subjective and objective evaluations; Synthetic aperture radar (SAR) images; Radar imaging","features extraction; image fusion; saliency detection; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85045956258"
"Li B.; Liu B.; Guo W.; Zhang Z.; Yu W.","Li, Boying (57284043500); Liu, Bin (56166930900); Guo, Weiwei (57202314666); Zhang, Zenghui (14520653700); Yu, Wenxian (7403913710)","57284043500; 56166930900; 57202314666; 14520653700; 7403913710","Ship Size Extraction for Sentinel-1 Images Based on Dual-Polarization Fusion and Nonlinear Regression: Push Error under One Pixel","2018","IEEE Transactions on Geoscience and Remote Sensing","56","8","","4887","4905","18","10.1109/TGRS.2018.2841882","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049088415&doi=10.1109%2fTGRS.2018.2841882&partnerID=40&md5=34643ad603349d8597d7652317c50b2d","In this paper, we present a method of ship size extraction for Sentinel-1 synthetic aperture radar (SAR) images, which is composed of the image processing stage and the regression stage. In order to achieve extraction with high accuracy, considering the data characteristics of Sentinel-1 images, we propose to use the dual-polarization fusion and the nonlinear regression with the gradient boosting. The experiments and analyses on a relatively large data set show that: 1) compared with the existing and related studies, the proposed method achieves an improved performance. The extraction errors are pushed under one pixel, and they are 4.66% (8.80 m) and 7.01% (2.17 m) for length and width, respectively; 2) the dual-polarization information fusion does improve the size extraction accuracy; and 3) the nonlinear regression does exploit the relationship between the influential factors and the size parameters and provide a better performance than the linear regression. The experimental results verify that the proposed design is suitable for ship size extraction in Sentinel-1 SAR images. © 1980-2012 IEEE.","Data mining; Extraction; Image fusion; Image resolution; Marine radar; Pixels; Polarization; Radar imaging; Regression analysis; Scattering; Ships; Space surveillance; Synthetic aperture radar; Azimuth; Dual-polarizations; Marine vehicles; Non-linear regression; Sentinel-1; Synthetic aperture radar (SAR) images; data set; error analysis; image processing; nonlinearity; performance assessment; pixel; polarization; regression analysis; satellite imagery; Sentinel; ship design; synthetic aperture radar; Image processing","Dual-polarization fusion; nonlinear regression; Sentinel-1; ship size extraction; synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85049088415"
"Liu T.; Li J.; Pi Y.; Yang X.","Liu, Tong (56278190100); Li, Jin (57209630146); Pi, Yiming (7004918019); Yang, Xu (57191642329)","56278190100; 57209630146; 7004918019; 57191642329","Inverse synthetic aperture radar imaging of maneuvering target with distributed high resolution radars","2018","Journal of Applied Remote Sensing","12","2","025009","","","","10.1117/1.JRS.12.025009","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047222952&doi=10.1117%2f1.JRS.12.025009&partnerID=40&md5=0ec35f89e676bd2f7e821b9862dc09f0","The high-resolution inverse synthetic aperture radar (ISAR) imaging of the maneuvering target with small-angle measurements is expected, which can be achieved by using the radar operating at a higher central frequency. However, because the defocus induced by the spatially variant phase error becomes significant with the resolution improvement and the central frequency increasing, the global phase correction is invalid. Moreover, due to the scattering characteristics of aspect-dependent scatterers and small-angle measurements, we can rarely obtain the rough outline of the maneuvering target. To deal with these difficulties, we propose an ISAR imaging method for the maneuvering target with distributed high resolution radars. To mitigate the spatially variant phase error, we correct phase errors in patches based on the weighted least-squares algorithm, which makes the image well-focused. Then, the ISAR imaging method with distributed radars is presented to obtain the fusion image, with the object to capture enough scatterers to represent the maneuvering target. The fusion image could provide more details. Real data have verified the effectiveness of the proposed method. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Angle measurement; Errors; Image fusion; Inverse problems; Inverse synthetic aperture radar; Radar measurement; aspect-dependent scatterers; High resolution; Maneuvering targets; Phase corrections; Phase error; Radar imaging","aspect-dependent scatterers; distributed radars; high resolution inverse synthetic aperture radar imaging; maneuvering target; phase correction in patches; spatially variant phase error","Article","Final","","Scopus","2-s2.0-85047222952"
"Zhou Y.; Zhang L.; Cao Y.; Huang Y.","Zhou, Yejian (57192650411); Zhang, Lei (55670414500); Cao, Yunhe (55260955900); Huang, Yan (57030884400)","57192650411; 55670414500; 55260955900; 57030884400","Optical-and-Radar Image Fusion for Dynamic Estimation of Spin Satellites","2020","IEEE Transactions on Image Processing","29","","8917816","2963","2976","13","10.1109/TIP.2019.2955248","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079574848&doi=10.1109%2fTIP.2019.2955248&partnerID=40&md5=1dfde14d10bd90fa10130703e8c78c1d","As more and more satellites are launched into the space, dynamic estimation of spin satellites has become a critical component of the space situation awareness application. Some explored studies using exterior measurements from different sensors such as optical device and inverse synthetic aperture radar (ISAR) to estimate dynamic parameters of spin satellites. As a single sensor normally provides two-dimensional observation, three-dimensional estimations resulting from these algorithms are strictly related to the prior knowledge of targets characteristics. As a result, it is difficult to expand these methods to other satellites. In order to support the dynamic estimation of most spin satellites, this paper presents a novel dynamic estimation approach which employs synchronized optical-and-radar images. The optical-and-radar fusion strategy has demonstrated its superiority in image analysis field, and breaks down the dynamic estimation of spin satellites into two sub-problems: target attitude estimation and spin parameters estimation. In this work, the proposed algorithm deduces two explicit expressions of target dynamic parameters under the imaging projection model of the joint optical-and-radar observation. Through the particle swarm optimization (PSO), target dynamic parameters are determined in two stages. This paper presents some experiments illustrating the feasibility of the proposed method and subsequent conclusions, which reflect advantages of the joint optical-and-radar observation mode in image interpretation. © 1992-2012 IEEE.","Image analysis; Image fusion; Inverse problems; Inverse synthetic aperture radar; Parameter estimation; Particle swarm optimization (PSO); Radar measurement; Radar target recognition; Satellites; Space-based radar; Critical component; Dynamic estimation; Image interpretation; Inverse synthetic aperture radars (ISAR); Parameters estimation; Projection models; Radar observations; Space situation awareness; Radar imaging","Dynamic estimation; image interpretation; optical-and-radar fusion; spin satellites","Article","Final","","Scopus","2-s2.0-85079574848"
"Ji X.","Ji, Xiuxia (55900734800)","55900734800","SAR and Optical Image Registration Method Based on Quantum Particle Swarm Optimization","2018","Proceedings - 2018 10th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2018","1","","8530348","365","368","3","10.1109/IHMSC.2018.00091","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058453360&doi=10.1109%2fIHMSC.2018.00091&partnerID=40&md5=4081bbf5bbe2742f37724b8defd2fa14","Image registration is the most important step of SAR and optical image fusion. In order to improve the registration accuracy and efficiency of the strategic targets, a new image registration method based on quantum particle swarm optimization is presented. The proposed method consists of three steps in sequence. Firstly, it decomposes the optical image into the high frequency detail components and the low frequency approximation component with the wavelet transform, and extracts feature corner points with Harris corner detection algorithm for the low frequency component. Secondly, it constructs the similarity measure criterion by migrating the pixel feature points to SAR image, and searches the most optimal registration parameter for the SAR image using quantum particle swarm optimization algorithm with individual optimal selection constraint. Finally, it completes affine transform according to the optimal registration parameters, and finishes image registration through interpolation resampling. The experimental results prove that the proposed method is effective and feasible, it can achieve high accuracy of sub-pixel level for the large strategic targets. © 2018 IEEE.","Affine transforms; Approximation algorithms; Cybernetics; Edge detection; Feature extraction; Geometrical optics; Image enhancement; Image fusion; Image registration; Man machine systems; Particle swarm optimization (PSO); Pixels; Synthetic aperture radar; Wavelet transforms; Harris corner detection; Low-frequency approximation; Low-frequency components; Optical image; Quantum particle swarm optimization; Quantum particle swarm optimization algorithm; Registration accuracy; SAR Images; Radar imaging","Image registration; optical image; quantum particle swarm optimization; SAR image","Conference paper","Final","","Scopus","2-s2.0-85058453360"
"Liao W.; Chanussot J.; Philips W.","Liao, Wenzhi (24830628200); Chanussot, Jocelyn (6602159365); Philips, Wilfried (7005218159)","24830628200; 6602159365; 7005218159","Remote sensing data fusion: Guided filter-based hyperspectral pansharpening and graph-based feature-level fusion","2018","Signals and Communication Technology","","","","243","275","32","10.1007/978-3-319-66330-2_6","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063322322&doi=10.1007%2f978-3-319-66330-2_6&partnerID=40&md5=a34da8d03e22917eac21bd1b87af3388","Recent advances in remote sensing technology have led to an increased availability of a multitude of satellite and airborne data sources, with increasing resolution. The term resolution here includes spatial and spectral resolutions. Additionally, at lower altitudes, airplanes and Unmanned Aerial Vehicles (UAVs) can deliver very high-resolution data from targeted locations. Remote sensing acquisitions employ both passive (optical and thermal range, multispectral, and hyperspectral) and active devices such as Synthetic Aperture Radar (SAR) and Light Detection and Ranging (LiDAR). Diverse information of the Earth’s surface can be obtained from these multiple imaging sources. Optical and SAR characterize the surface of the ground, LiDAR provides the elevation, while multispectral and hyperspectral sensors reveal the material composition. These multisource remote sensing images, once combined/fused together, provide a more comprehensive interpretation of land cover/use (urban and climatic changes), natural disasters (floods, hurricanes, and earthquakes), and potential exploitation (oil fields and minerals). However, automatic interpretation of remote sensing data remains challenging. Two fundamental problems in data fusion of multisource remote sensing images are (1) differences in resolution hamper the ability to fastly interpret multisource remote sensing images and (2) there is no clear methodology yet on combining the diverse information of different data sources. In this chapter, we will introduce our recent solutions for these two problems, with an introduction on signal-level fusion (hyperspectral image pansharpening) first, followed by feature-level fusion (graph-based fusion model for multisource data classification). © Springer International Publishing AG 2018.","Antennas; Disasters; Graphic methods; Hyperspectral imaging; Image fusion; Oil fields; Radar imaging; Remote sensing; Spectroscopy; Synthetic aperture radar; Data-source; Feature-level fusions; Filter-based; Guided filters; HyperSpectral; Light detection and ranging; Multisource remote sensing images; Optical-; Pan-sharpening; Remote sensing data fusion; Optical radar","","Book chapter","Final","","Scopus","2-s2.0-85063322322"
"Wu W.; Guo S.; Cheng Q.","Wu, Wenfu (57208187129); Guo, Songjing (57212678543); Cheng, Qimin (7202917983)","57208187129; 57212678543; 7202917983","Fusing optical and synthetic aperture radar images based on shearlet transform to improve urban impervious surface extraction","2020","Journal of Applied Remote Sensing","14","2","024506","","","","10.1117/1.JRS.14.024506","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092069421&doi=10.1117%2f1.JRS.14.024506&partnerID=40&md5=64c5c455eaa6137273715daffc39926a","In order to extract urban impervious surfaces (ISs) accurately, optical and synthetic aperture radar (SAR) images fusion was recognized as one promising method. However, most fusion methods currently focus on feature-level and decision-level fusions. There are only a few studies exploring the performance of the fused image at the pixel level for IS extraction. Therefore, we introduced the shearlet transform to fuse Landsat-8 and TerraSAR images and evaluated the fused image by comparing it to those obtained using conventional image fusion methods. The IS from the fused images using the support vector machine algorithm is extracted and compared. Experimental results indicate some interesting findings. First, the shearlet transform can fully retain the spectral information from the optical image and the spatial information from the SAR image. Second, the IS extraction from the fused image with the shearlet transform achieved the highest accuracy with an overall accuracy of 95.1% and a Kappa coefficient of 0.8792, which confirmed the proposed method is applicable to IS extraction. We can conclude that an effective pixel-level fusion algorithm for optical and SAR images can significantly improve the extraction accuracy of urban IS. Our research could provide an innovative fusion technique and also could serve as a meaningful reference for further applications of optical and SAR imagery. In addition, the potential of SAR data in IS extraction should be further investigated. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Extraction; Geometrical optics; Image enhancement; Image fusion; Pixels; Support vector machines; Synthetic aperture radar; Decision level fusion; Extraction accuracy; Image fusion methods; Spatial informations; Spectral information; Support vector machine algorithm; Synthetic aperture radar (SAR) images; Urban impervious surfaces; Radar imaging","fusion; shearlet transform; support vector machine; synthetic aperture radar; urban impervious surface","Article","Final","","Scopus","2-s2.0-85092069421"
"Hammami I.; Dezert J.; Mercier G.","Hammami, Imen (57221369679); Dezert, Jean (57208456569); Mercier, Grégoire (7005944236)","57221369679; 57208456569; 7005944236","Kohonen-Based Credal Fusion of Optical and Radar Images for Land Cover Classification","2018","2018 21st International Conference on Information Fusion, FUSION 2018","","","8455272","1623","1630","7","10.23919/ICIF.2018.8455272","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054078867&doi=10.23919%2fICIF.2018.8455272&partnerID=40&md5=7e47cdba9dbed015da6e9a737ece4d19","This paper presents a Credal algorithm to perform land cover classification from a pair of optical and radar remote sensing images. SAR (Synthetic Aperture Radar) /optical multispectral information fusion is investigated in this study for making the joint classification. The approach consists of two main steps: 1) relevant features extraction applied to each sensor in order to model the sources of information and 2) a Kohonen map-based estimation of Basic Belief Assignments (BBA) dedicated to heterogeneous data. This framework deals with co-registered images and is able to handle complete optical data as well as optical data affected by missing value due to the presence of clouds and shadows during observation. A pair of SPOT-5 and RADARSAT-2 real images is used in the evaluation, and the proposed experiment in a farming area shows very promising results in terms of classification accuracy and missing optical data reconstruction when some data are hidden by clouds. © 2018 ISIF","Classification (of information); Data mining; Image classification; Image fusion; Information fusion; Remote sensing; Self organizing maps; Synthetic aperture radar; Uncertainty analysis; Basic belief assignment; Belief function; Classification accuracy; Heterogeneous data; Land cover classification; Radar remote sensing; SAR(synthetic aperture radar); Sources of informations; Radar imaging","belief functions; image fusion; Kohonen map; Land cover classification; remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054078867"
"Kalpana N.; Sivasankar A.","Kalpana, N. (57191341591); Sivasankar, A. (57191344443)","57191341591; 57191344443","Detection of flooded areas from multitemporal SAR images","2016","2016 2nd International Conference on Science Technology Engineering and Management, ICONSTEM 2016","","","7560951","1","5","4","10.1109/ICONSTEM.2016.7560951","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988977542&doi=10.1109%2fICONSTEM.2016.7560951&partnerID=40&md5=cc190f723cf642a1fe4b59f53c546f91","Multi temporal synthetic aperture radar images are available, precise calibration and perfect spatial register are required to get a useful image for displaying changes that contain occurred. SAR calibration is a extremely complex and sensitive problem; a few errors may persist after calibration that interferes with subsequent steps in the data fusion and visualization process. Because of the non-Gaussian model of radar backscattering, traditional image pre processing procedures cannot be used here. To solve this problem 'cross-calibration/normalization,' method can be used. In image enhancement and the numerical comparison of many image takes together with data fusion and visualization processes. The proposed processing which contain filtering, histogram truncation, and equalization steps and region growing and merging algorithm applied in an adaptive way to the images. RGB composition is used to combining an pre & post flood image or identify an flooded areas. © 2016 IEEE.","Calibration; Data fusion; Data visualization; Floods; Image enhancement; Image fusion; Image processing; Radar; Synthetic aperture radar; Visualization; Flood detections; Image preprocessing; Multi-temporal SAR images; Non-Gaussian models; Numerical comparison; Radar backscattering; Synthetic Aperture Radar Imagery; Visualization process; Radar imaging","Data fusion; flood detection; image enhancement; multi temporal synthetic aperture radar (SAR) imagery; RGB composition","Conference paper","Final","","Scopus","2-s2.0-84988977542"
"Shi Y.; Zhu X.X.; Yin W.; Bamler R.","Shi, Yilei (55495784300); Zhu, Xiao Xiang (55696622200); Yin, Wotao (8729349300); Bamler, Richard (7004572990)","55495784300; 55696622200; 8729349300; 7004572990","A fast and accurate basis pursuit denoising algorithm with application to super-resolving tomographic SAR","2018","IEEE Transactions on Geoscience and Remote Sensing","56","10","8412239","6148","6158","10","10.1109/TGRS.2018.2832721","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053603100&doi=10.1109%2fTGRS.2018.2832721&partnerID=40&md5=d8284361f446052d88f81e6acf25167e","L1 regularization is used for finding sparse solutions to an underdetermined linear system. As sparse signals are widely expected in remote sensing, this type of regularization scheme and its extensions have been widely employed in many remote sensing problems, such as image fusion, target detection, image super-resolution, and others, and have led to promising results. However, solving such sparse reconstruction problems is computationally expensive and has limitations in its practical use. In this paper, we proposed a novel efficient algorithm for solving the complex-valued L1 regularized least squares problem. Taking the high-dimensional tomographic synthetic aperture radar (TomoSAR) as a practical example, we carried out extensive experiments, both with the simulation data and the real data, to demonstrate that the proposed approach can retain the accuracy of the second-order methods while dramatically speeding up the processing by one or two orders. Although we have chosen TomoSAR as the example, the proposed method can be generally applied to any spectral estimation problems. © 2018 IEEE.","","Basis pursuit denoising (BPDN); L<sub>1</sub> regularization; proximal gradient (PG); second-order cone programming (SOCP); TomoSAR","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85053603100"
"Xin H.; Feng L.","Xin, Hongqiang (57208782443); Feng, Liangjie (55634949100)","57208782443; 55634949100","Remote sensing image fusion algorithm based on à trous wavelet transform and HIS transform","2019","Proceedings of SPIE - The International Society for Optical Engineering","11179","","1117908","","","","10.1117/12.2539667","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072620399&doi=10.1117%2f12.2539667&partnerID=40&md5=8c0ba60d0049e09a364b62de92d71316","The image fusion of optical images and synthetic aperture radar (SAR) images are of great significance. By using the complementary advantages of both, the target detection and recognition can be relatively simple and the accuracy will be relatively improved. The image information reflected by the optical image and the SAR image is very different, and the image fusion can combine the two information to give greater advantages. Aiming at the limitations of single sensor in terms of spectrum and spatial resolution, the multi-source sensor fusion technology can maximize the information description of the target scene. The fusion experiment and evaluation of optical images and SAR images are carried out by combining àtrous wavelet transform and IHS transform, and compared with the traditional HIS transform and wavelet transform fusion methods. The results show that the fusion of àtrous wavelet and HIS transform is the best, and the advantages of two single fusion methods are absorbed. It not only improves the spatial detail expression of the original image, but also preserves the spectral information of the original image, providing more accurate data for remote sensing applications. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Geometrical optics; Image compression; Image enhancement; Optical data processing; Radar imaging; Remote sensing; Synthetic aperture radar; Wavelet transforms; Information descriptions; Optical image; Remote sensing applications; Remote sensing images; SAR Images; Spectral information; Synthetic aperture radar (SAR) images; Target detection and recognition; Image fusion","HIS transform; Image fusion; Optical image; SAR image; À trous wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85072620399"
"Zhang H.; Shen H.; Yuan Q.","Zhang, Hai (57192694132); Shen, Huanfeng (8359721100); Yuan, Qiangqiang (36635300800)","57192694132; 8359721100; 36635300800","Multispectral and SAR image fusion based on Laplacian pyramid and sparse representation","2018","Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","4","","","2164","2172","8","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071842145&partnerID=40&md5=ced386d4c98d971098e808f5e4586684","Multispectral images contain plentiful spectral information of the Earth's surface, which is beneficial for identifying land cover types. While Synthetic Aperture Radar (SAR) images can provide abundant information on texture and structure of target objects. Complementary information from multi-sensors can be combined to improve the availability and reliability of stand-alone dataset. Therefore, this paper presents a fusion framework based on Laplacian pyramid (LP) and sparse representation theory to integrate information from multispectral and SAR images. Laplacian pyramid is performed to decompose both the multispectral and SAR images into multi-level detail images (or high-frequency components) and approximation images (or low-frequency components). High-frequency components are combined based on the absolute values of coefficients while low-frequency components are merged based on sparse representation theory. Finally, LP reconstruction is performed on fused high-frequency and low-frequency components to obtain the integrated image. We conduct experiments on several datasets to verify the effectiveness of proposed method. Both visual interpretation and statistical analyses demonstrate that the proposed method strike a satisfactory balance between spectral information preservation and enhancement of spatial and textual characteristics. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Algebra; Electric arcs; Image fusion; Laplace transforms; Remote sensing; Synthetic aperture radar; Textures; High frequency components; Image quality assessment; Laplacian Pyramid; Low-frequency components; Multi sensor images; Sparse representation; Synthetic aperture radar (SAR) images; Visual interpretation; Radar imaging","Image quality assessment; Laplacian pyramid; Multi-sensor image fusion; Sparse representation; Synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85071842145"
"Yuhendra; Minarni","Yuhendra (57219044531); Minarni (57205220449)","57219044531; 57205220449","Optical SAR images fusion: Comparative analysis of resulting images data","2018","MATEC Web of Conferences","215","","01002","","","","10.1051/matecconf/201821501002","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059173699&doi=10.1051%2fmatecconf%2f201821501002&partnerID=40&md5=ce3afbcc04dfde1459dc542a9522f6a7","Image fusion is a useful tool for integrating low spatial resolution multispectral (MS) images with a high spatial resolution panchromatic (PAN) image, thus producing a high resolution multispectral image for better understanding of the observed earth surface. A main proposed the research were the effectiveness of different image fusion methods while filtering methods added to speckle suppression in synthetic aperture radar (SAR) images. The quality assessment of the filtering fused image implemented by statistical parameter namely mean, standard deviation, bias, universal index quality image (UIQI) and root mean squared error (RMSE). In order to test the robustness of the image quality, either speckle noise (Gamma map filter) is intentionally added to the fused image. When comparing and testing result, Gram Scmidth (GS) methods have shown better results for good colour reproduction, as compared with high pass filtering (HPF). And the other hands, GS, and wavelet intensity hue saturation (W-IHS) have shown the preserving good colour with original image for Landsat TM data. © The Authors, published by EDP Sciences, 2018.","Color printing; Engineering research; Facsimile; High pass filters; Image resolution; Mean square error; Radar imaging; Speckle; Synthetic aperture radar; Comparative analysis; High spatial resolution; Image fusion methods; Intensity hue saturations; Panchromatic (Pan) image; Root mean squared errors; Statistical parameters; Synthetic aperture radar (SAR) images; Image fusion","","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85059173699"
"Yao X.; Cui G.; Liu Y.; Nie X.","Yao, Xue (56340368000); Cui, Guolong (24469695800); Liu, Yu (55966557000); Nie, Xiangfei (14063777300)","56340368000; 24469695800; 55966557000; 14063777300","Three dimensional image reconstruction method based on two dimensional radar image and optical image","2019","2019 IEEE 4th International Conference on Signal and Image Processing, ICSIP 2019","","","8868808","959","965","6","10.1109/SIPROCESS.2019.8868808","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074384707&doi=10.1109%2fSIPROCESS.2019.8868808&partnerID=40&md5=d6bdba8a05e294d9bb75b06e939cb70f","Since the building room consists mainly of three brick walls and one glass wall, Unmanned Aerial Vehicle (UAV) equipped with optical sensor and radar sensor detects targets only from a specific view during monitoring the building fire explosion site. The unknown height information of combustible objects in the two dimensional (2-D) radar image and the unknown range information of combustible objects in the 2-D optical image makes it difficult to get comprehensive information. By fusion of the range and azimuth information extracted by the 2-D radar image and the height and azimuth information extracted by the 2-D optical image, in this paper, a new algorithm is proposed to reconstruct a three dimensional (3-D) image with the correct range, azimuth and height information of combustible objects. The simulation results validate the correctness and feasibility of the algorithm. © 2019 IEEE.","Antennas; Fusion reactions; Geometrical optics; Image fusion; Image reconstruction; Information analysis; Optical data processing; Synthetic aperture radar; Unmanned aerial vehicles (UAV); Walls (structural partitions); 3-D image; Building fires; Comprehensive information; Radar sensors; Range information; Three-dimensional image reconstruction; Threedimensional (3-d); Two Dimensional (2 D); Radar imaging","3-D image; Fusion; SAR; Unmanned aerial vehicle","Conference paper","Final","","Scopus","2-s2.0-85074384707"
"Yang H.; Zhang Y.; Ding W.","Yang, Hong (57191200476); Zhang, Yasheng (55910549400); Ding, Wenzhe (57199850984)","57191200476; 55910549400; 57199850984","A Fast Recognition Method for Space Targets in ISAR Images Based on Local and Global Structural Fusion Features with Lower Dimensions","2020","International Journal of Aerospace Engineering","2020","","3412582","","","","10.1155/2020/3412582","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080971190&doi=10.1155%2f2020%2f3412582&partnerID=40&md5=c0c6e2040cea8c30d9085b9e0603d926","Feature extraction is the key step of Inverse Synthetic Aperture Radar (ISAR) image recognition. However, limited by the cost and conditions of ISAR image acquisition, it is relatively difficult to obtain large-scale sample data, which makes it difficult to obtain target deep features with good discriminability by using the currently popular deep learning method. In this paper, a new method for low-dimensional, strongly robust, and fast space target ISAR image recognition based on local and global structural feature fusion is proposed. This method performs the trace transformation along the longest axis of the ISAR image to generate the global trace feature of the space target ISAR image. By introducing the local structural feature, Local Binary Pattern (LBP), the complementary fusion of the global and local features is achieved, which makes up for the missing structural information of the trace feature and ensures the integrity of the ISAR image feature information. The representation of trace and LBP features in a low-dimensional mapping feature space is found by using the manifold learning method. Under the condition of maintaining the local neighborhood relationship in the original feature space, the effective fusion of trace and LBP features is achieved. So, in the practical application process, the target recognition accuracy is no longer affected by trace function, LBP feature block number selection, and other factors, realizing the high robustness of the algorithm. To verify the effectiveness of the proposed algorithm, an ISAR image database containing 1325 samples of 5 types of space targets is used for experiments. The results show that the classification accuracy of the 5 types of space targets can reach more than 99%, and the recognition accuracy is no longer affected by the trace feature and LBP feature selection, which has strong robustness. The proposed method provides a fast and effective high-precision model for space target feature extraction, which can give some references for solving the problem of space object efficient identification under the condition of small sample data. © 2020 Hong Yang et al.","Deep learning; Extraction; Feature extraction; Image fusion; Image recognition; Inverse problems; Inverse synthetic aperture radar; Learning systems; Space-based radar; Classification accuracy; Global structural features; High-precision models; Inverse synthetic aperture radars (ISAR); Local binary patterns; Recognition accuracy; Structural information; Trace-transformation; Radar imaging","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85080971190"
"Bahmanyar R.; Espinoza-Molina D.; Datcu M.","Bahmanyar, Reza (56042026400); Espinoza-Molina, Daniela (26642362900); Datcu, Mihai (7004523124)","56042026400; 26642362900; 7004523124","Multisensor Earth Observation Image Classification Based on a Multimodal Latent Dirichlet Allocation Model","2018","IEEE Geoscience and Remote Sensing Letters","15","3","","459","463","4","10.1109/LGRS.2018.2794511","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041660854&doi=10.1109%2fLGRS.2018.2794511&partnerID=40&md5=61a846a17f2d3db8689a08f156c74801","Many previous researches have already shown the advantages of multisensor land-cover classification. Here, we propose an innovative land-cover classification approach based on learning a joint latent model of synthetic aperture radar (SAR) and multispectral satellite images using multimodal latent Dirichlet allocation (mmLDA), a probabilistic generative model. It has already been successfully applied to various other problems dealing with multimodal data. For our experiments, we chose overlapping SAR and multispectral images of two regions of interest. The images were tiled into patches and their local primitive features were extracted. Then each image patch is represented by SAR and multispectral bag-of-words (BoW) models. The BoW values are both fed to the mmLDA, resulting in a joint latent data model. A qualitative and quantitative validation of the topics based on ground-truth data demonstrate that the land-cover categories of the regions are correctly classified, outperforming the topics obtained using individual single modality data. © 2004-2012 IEEE.","Image classification; Image fusion; Statistics; Synthetic aperture radar; Earth observation images; Land cover classification; Latent Dirichlet allocation; Multispectral images; Multispectral satellite image; Quantitative validation; Regions of interest; Synthetic aperture radar (SAR) images; EOS; image classification; image processing; land classification; land cover; model; multispectral image; numerical method; radar imagery; sensor; synthetic aperture radar; Radar imaging","Image fusion; land-cover classification; multimodal latent Dirichlet allocation (mmLDA); multispectral images; synthetic aperture radar (SAR) images","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85041660854"
"Munkh-Erdene A.; Amarsaikhan D.; Ganzorig M.; Battsengel B.; Nyamjargal E.; Enkhjargal D.; Bolorchuluun C.; Byambadolgor B.","Munkh-Erdene, A. (55355417800); Amarsaikhan, D. (16052116000); Ganzorig, M. (16052616200); Battsengel, B. (6506481572); Nyamjargal, E. (57209344014); Enkhjargal, D. (54891827500); Bolorchuluun, C. (55349069600); Byambadolgor, B. (56568341100)","55355417800; 16052116000; 16052616200; 6506481572; 57209344014; 54891827500; 55349069600; 56568341100","Applications of optical and radar images for forest resources study in Mongolia","2018","Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","4","","","2298","2303","5","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071865471&partnerID=40&md5=fbe44e9556c0c7794a2298dc01d68ad5","The aim of this research is to conduct a forest resources study in Mongolia using the integrated optical and SAR data sets. As data sources, optical Sentinel-2A images and microwave Sentinel-1A C-band dual-polarization data as well as multitemporal ALOS-2 Palsar HH polarization L-band data are available. As part of the study, initially, a forest resources mapping using a fused multichannel and SAR images of Sentinel is conducted. For the image fusion, a principal component analysis (PCA) is applied. For the classification of forest types, a support vector machine (SVM) classification method is used. After creation of the forest resources map, a biomass estimation is conducted using the combined SAR features of ALOS Palsar and Sentinel. For the biomass assessment, a Mahalanobis distance classifier is applied. Overall, the research demonstrated that the combined optical and SAR features can be successfully used for forest resources study in Mongolia. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Classification (of information); Forestry; Fusion reactions; Image fusion; Mapping; Natural resources; Polarization; Principal component analysis; Remote sensing; Support vector machines; Synthetic aperture radar; Biomass assessment; Biomass estimation; Classification methods; Dual-polarizations; Forest resources; Forest study; Mahalanobis distances; Optical data; Radar imaging","Classification; Forest study; Fusion; Optical data; Synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85071865471"
"De Borba A.A.; Marengoni M.; Frery A.C.","De Borba, Anderson A. (57215682846); Marengoni, Mauricio (6602235863); Frery, Alejandro C. (7003561251)","57215682846; 6602235863; 7003561251","Fusion of Evidences for Edge Detection in PolSAR Images","2019","Proceedings of the 2019 IEEE Recent Advances in Geoscience and Remote Sensing: Technologies, Standards and Applications, TENGARSS 2019","","","8976040","80","85","5","10.1109/TENGARSS48957.2019.8976040","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078744978&doi=10.1109%2fTENGARSS48957.2019.8976040&partnerID=40&md5=acbef14ef785fa3f5f99eab6050e38c2","Polarimetric Synthetic Aperture Radar (PolSAR) has achieved an important position as a remote sensing imaging method. However, PolSAR images are contaminated with speckle noise, making its processing and analysis challenging tasks. The present study discusses a detection method based on the fusion of evidences obtained in the intensity channels of multilook PolSAR images. The method consists of detecting transition points in the finest strip of data which spans two regions using the maximum likelihood. This is applied to each of the three intensity channels (hh), (hv) and (vv). The fusion methods are simple average, stationary wavelet transform (SWT), principal component analysis (PCA), and ROC statistics. The results indicate improvement performance of the approach in detecting edges with possible paths for future research. © 2019 IEEE.","Edge detection; Fusion reactions; Geology; Maximum likelihood estimation; Principal component analysis; Remote sensing; Synthetic aperture radar; Wavelet transforms; Detection methods; Fusion methods; Polarimetric synthetic aperture radars; PolSAR; Remote sensing imaging; Speckle noise; Stationary wavelet transforms; Transition point; Image fusion","edge detection; fusion; maximum likelihood estimation; PolSAR","Conference paper","Final","","Scopus","2-s2.0-85078744978"
"Liu K.; Li Y.","Liu, Kaixuan (57210150004); Li, Yufeng (56006987500)","57210150004; 56006987500","SAR and multispectral image fusion algorithm based on sparse representation and NSST","2019","AIP Conference Proceedings","2122","","020059","","","","10.1063/1.5116498","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069658842&doi=10.1063%2f1.5116498&partnerID=40&md5=b9aca26ac747684a3c7c1817019b315d","Aiming at the problem of spectral distortion and texture detail loss in synthetic aperture radar (SAR) image and multi-spectral (MS) image fusion, an image fusion algorithm combining sparse representation (SR) and non-subsampled Shearlet transform (NSST) is proposed. The algorithm uses the multi-scale, multi-directional and translation-invariant characteristics of NSST to transform and decompose the luminance components of SAR images and multi-spectral images. Then, the low-frequency sub-band is represented by SR, and the fusion is performed by an energy-adaptive method. The high-frequency sub-band is fused with the correlation coefficient as the saliency index, and finally the fused image is obtained by inverse transformation. The simulation experiments show that the proposed algorithm effectively preserves the subject information and feature information of the source image, so that the contrast of the fused image is significantly improved, the image outline is clear, and the overall sharpness. The spectral resolution and spatial resolution are closer to the fused reference image. © 2019 Author(s).","","","Conference paper","Final","","Scopus","2-s2.0-85069658842"
"Zhu X.; Shang C.; Guo B.; Shi L.; Hu W.; Zeng H.","Zhu, Xiaoxiu (57197872453); Shang, Chaoxuan (7005398312); Guo, Baofeng (56577060100); Shi, Lin (55364489800); Hu, Wenhua (57199745271); Zeng, Huiyan (57205760121)","57197872453; 7005398312; 56577060100; 55364489800; 57199745271; 57205760121","Multiband fusion inverse synthetic aperture radar imaging based on variational Bayesian inference","2020","Journal of Applied Remote Sensing","14","3","036511","","","","10.1117/1.JRS.14.036511","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092644278&doi=10.1117%2f1.JRS.14.036511&partnerID=40&md5=c05bbd462b7c1e6cfeec724f424f24c6","Images from high-resolution inverse synthetic aperture radar (ISAR) can provide more information about the targets. Multiband fusion imaging techniques can achieve higher range resolution without increasing hardware costs. A multiband fusion imaging algorithm based on variational Bayesian inference (VBI) is proposed to improve the range resolution of ISAR images. First, a multiband fusion ISAR imaging model is established based on sparse representation. Second, the scattering coefficients and noise are assumed to be the Laplacian scale mixture distribution and the complex Gaussian distribution, respectively. Finally, the fusion image is directly reconstructed in the complex domain by the VBI based on Laplace approximation method. The effectiveness and robustness of the proposed algorithm are verified by the experimental fusion results of one-dimensional signals and two-dimensional ISAR images. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Bayesian networks; Image enhancement; Image fusion; Imaging techniques; Inference engines; Inverse problems; Inverse synthetic aperture radar; Laplace transforms; Complex Gaussian distribution; Inverse synthetic aperture radars (ISAR); Laplace approximation; One dimensional signal; Range resolution; Scattering co-efficient; Sparse representation; Variational Bayesian inferences; Radar imaging","inverse synthetic aperture radar; Laplacian scale mixture prior; multiband fusion; sparse representation; variational Bayesian inference","Article","Final","","Scopus","2-s2.0-85092644278"
"Guo F.; Zhang G.; Zhang Q.; Zhao R.; Deng M.; Xu K.; Jia P.; Hao X.","Guo, Fengcheng (57195551078); Zhang, Guo (57204670748); Zhang, Qingjun (55092505400); Zhao, Ruishan (57193381149); Deng, Mingjun (57193391692); Xu, Kai (57225875149); Jia, Peng (57211095925); Hao, Xiaoyun (57211499641)","57195551078; 57204670748; 55092505400; 57193381149; 57193391692; 57225875149; 57211095925; 57211499641","Fusion Despeckling Based on Surface Variation Anisotropic Diffusion Filter and Ratio Image Filter","2020","IEEE Transactions on Geoscience and Remote Sensing","58","4","8901422","2398","2411","13","10.1109/TGRS.2019.2948890","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082884386&doi=10.1109%2fTGRS.2019.2948890&partnerID=40&md5=30fe20301190ae0a85e46c73d52ec5e9","This article proposes a novel fusing filter algorithm based on a surface variation anisotropic diffusion (SVAD) filter and a ratio image filter to achieve good speckle reduction and edge preservation. The proposed algorithm can be divided into three steps. First, the proposed SVAD filter effectively calculates the diffusion coefficient of each pixel to obtain filtering results on different scales. Second, the proposed ratio image filter obtains a new denoising result that can effectively recover some details lost with the SVAD filter. Then, the two filtering results are fused to obtain the final despeckling result. Furthermore, the effects of the weighting coefficients of the fusion processing and the number of iterations of the ratio image filter on the final filtering results are analyzed. The proposed algorithm is effectively evaluated by conducting some experiments on the added noise image and real synthetic aperture radar (SAR) images. The experimental results confirm that the proposed method can not only significantly reduce speckle but also effectively preserve the edge information of images. © 2019 IEEE.","Diffusion; Optical anisotropy; Radar imaging; Speckle; Synthetic aperture radar; Anisotropic Diffusion; Anisotropic diffusion filters; Number of iterations; Ratio images; Speckle reduction; Surface variations; Synthetic aperture radar (SAR) images; Weighting coefficient; algorithm; anisotropy; data processing; diffusion; experimental study; image analysis; pixel; spatial variation; speckle; synthetic aperture radar; Image fusion","Anisotropic diffusion (AD); ratio image filter; speckle reduction; surface variation","Article","Final","","Scopus","2-s2.0-85082884386"
"Zhang Y.; Zou H.; Shao N.; Qin X.; Zhou S.; Ji K.","Zhang, Yue (35115854100); Zou, Huanxin (8366222500); Shao, Ningyuan (57196454992); Qin, Xianxiang (55375282700); Zhou, Shilin (36555250200); Ji, Kefeng (7005637029)","35115854100; 8366222500; 57196454992; 55375282700; 36555250200; 7005637029","Terrain classification of polarimetric SAR images based on consensus similarity network fusion","2018","Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics","40","2","","295","302","7","10.3969/j.issn.1001-506X.2018.02.09","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046795648&doi=10.3969%2fj.issn.1001-506X.2018.02.09&partnerID=40&md5=4838d8c0c28f54813000d60c4f8840ae","A variety of feature vectors are usually extracted from a polarimetric synthetic aperture radar (SAR) image and stacked directly into a high-dimension feature vector to classify the different terrains in polarimetric SAR images, which results in the loss of some feature vectors' discriminability. To address this problem, each feature vector is regarded as data from a different view of the image in this paper. Firstly, the polarimetric SAR image is over-segmented to obtain a number of superpixels, and five similarity matrices are respectively constructed from five feature vectors extracted from polarimetric SAR images based on superpixels. Secondly, consensus similarity network fusion, which belongs to the multi-view learning algorithms, is used to generate a fused similarity matrix. Thirdly, spectral clustering is performed on the fused similarity matrix. Finally, a novel classification post-processing strategy is proposed to correct the misclassified pixels. Extensive experimental results conducted on a simulated and a real-world polarimetric SAR images demonstrate the superiority of the proposed method, compared with five other classical methods. © 2018, Editorial Office of Systems Engineering and Electronics. All right reserved.","Classification (of information); Clustering algorithms; Image classification; Image fusion; Learning algorithms; Matrix algebra; Pixels; Polarimeters; Superpixels; Synthetic aperture radar; Vectors; Multi-view learning; PolSAR image; Similarity network; Spectral clustering; Unsupervised classification; Radar imaging","Consensus similarity network fusion; Multi-view learning; PolSAR image; Spectral clustering; Unsupervised classification","Article","Final","","Scopus","2-s2.0-85046795648"
"Swarna Priya R.M.; Prabu S.; Dharun V.S.","Swarna Priya, R.M. (55808654600); Prabu, S. (55755787400); Dharun, V.S. (55823607900)","55808654600; 55755787400; 55823607900","A hybrid particle swarm optimization with affine transformation approach for cloud free multi-temporal image registration","2015","Electronic Letters on Computer Vision and Image Analysis","14","2","","74","89","15","10.5565/rev/elcvia.775","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959236169&doi=10.5565%2frev%2felcvia.775&partnerID=40&md5=5a2160ddbe5801edb94968628d88c16d","Image Registration (IR) is the process of transformation of different data into the coordinate system and provides the geometric alignment of two images used in the computer vision, medical imaging and remote sensing applications. An image registration is an important stage in multi-temporal image processing since, the recovery of information from cloud shadow is difficult. Traditionally, the Demons, Combined Registration and Segmentation (CRS) approach, Markov Random Field (MRF) and Mutual Information (MI) based approaches offers more computational complexity, minimum edge preservation measure (QAB/F) during image registration process. To maximize the quality of edge preservation measure and MI with minimum computational time, this paper proposes hybrid Particle Swarm Optimization (PSO)-Affine Transformation (AT) technique for an image registration. An enhanced registration process and the cloud removal technique are proposed for quality improvement of an image. Initially, Gaussian filtering in the preprocessing stage removes the noises present in an image. The proposed PSO extracts the matching points between the reference image and target image in the multi-temporal image dataset. Then, the AT on extracted matching points provides the specific feature points from main features. Finally, the Relevance Vector Machine (RVM) classification forms the cluster of specific feature points. The extracted feature points from PSO-AT maximize the quality of edge preservation and MI with efficient cloud removal. The comparative analysis with the traditional methods of Control Point -Least Square (CP-LS), Multi-Focus Image Fusion (MFIF) and Discrete Wavelet Transform (DWT) on the parameters of QAB/F and MI shows the effectiveness of proposed PSO-AT.","","Affine transformation (AT); Combined registration segmentation (CRS); Control point-least square (CP-LS); Edge detection; Image registration; Markov random field (MRF); Multi-temporal image (MTI); Multi-temporal image processing (MTIP); Mutual information (MI); Particle swarm optimization (PSO); Relevance vector machine (RVM); Remote sensing (RS) and synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84959236169"
"Abd Manaf S.; Mustapha N.; Sulaiman M.N.; Husin N.A.; Abdul Hamid M.R.","Abd Manaf, Syaifulnizam (55185057700); Mustapha, Norwati (24802568600); Sulaiman, Md Nasir (22434244300); Husin, Nor Azura (25825147600); Abdul Hamid, Mohd Radzi (57190281942)","55185057700; 24802568600; 22434244300; 25825147600; 57190281942","Fusion of optical and SAR in extracting shoreline at northeast coast of peninsular Malaysia","2015","ACRS 2015 - 36th Asian Conference on Remote Sensing: Fostering Resilient Growth in Asia, Proceedings","","","","","","","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964057231&partnerID=40&md5=ea1b71bb8b9fecedcfca8cb0d49b0cc6","Shoreline extraction is important to identify exact land and water boundary of a country. However, it is difficult and time consuming for a large region when using traditional ground survey techniques. Alternatively, by using remote sensing for extracting shoreline is rapid and highly accurate thus minimizing the mapping errors. Although Google Map and Google Earth are open freely for public, they are not suitable to extract shoreline due to some important spectral information have been remove out. The problem in shoreline detection is the difficulty of extraction according to hydrodynamic condition of coastal area such as tides, current, etc. that leads to low accuracy rate. In extracting shoreline, remotely sensed images could be analyzed by using satellite image processing techniques. By using fusion of multispectral and SAR images, shoreline could be extracted with a higher accuracy rate.","Extraction; Image fusion; Optical data processing; Radar imaging; Remote sensing; Satellite imagery; Space-based radar; Synthetic aperture radar; Ground surveys; Highly accurate; Hydrodynamic conditions; Optical satellite images; Remotely sensed images; Satellite image processing; Spectral information; Water boundaries; Image processing","Image fusion; Optical satellite image; Shoreline extraction; Synthetic Aperture Radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-84964057231"
"Schmitt A.; Wendleder A.","Schmitt, Andreas (55576223700); Wendleder, Anna (36012163600)","55576223700; 36012163600","Geometric and polarimetric sharpening of SAR images by kennaugh-and schmittlet-based multi-frequency data fusion","2016","Proceedings of the European Conference on Synthetic Aperture Radar, EUSAR","","","7559433","","","","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000996011&partnerID=40&md5=e525d6ddb62a45df8f07c0baa52376fb","The joint use of diverse sensors is of major interest in the remote sensing community. As each sensor has its own characteristics in terms of geometric, radiometric and polarimetric resolution, it provides certain advantages and disadvantages for special applications. This contribution presents a way of image sharpening both in the geometric and polarimetric domain by joining the acquisitions of TerraSAR-X and RADARSAT-2. The image fusion is performed by the help of the Kennaugh element framework and the Schmittlet image enhancement. An unexpected high richness of detail characterizes the combined image and facilitates the following image interpretation like land cover classification. © VDE VERLAG GMBH · Berlin · Offenbach.","Data fusion; Geometry; Image enhancement; Image fusion; Polarimeters; Radar; Radar imaging; Remote sensing; Combined images; Image interpretation; Image sharpening; Land cover classification; Multi frequency; SAR Images; Special applications; TerraSAR-X; Synthetic aperture radar","","Conference paper","Final","","Scopus","2-s2.0-85000996011"
"Sun X.; Zhang J.; Zhai L.","Sun, X. (56370049900); Zhang, J. (56058752000); Zhai, L. (55977430400)","56370049900; 56058752000; 55977430400","Multipolarimetric SAR image change detection based on multiscale feature-level fusion","2015","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","40","7W4","","155","158","3","10.5194/isprsarchives-XL-7-W4-155-2015","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975056654&doi=10.5194%2fisprsarchives-XL-7-W4-155-2015&partnerID=40&md5=4339c00fe2b8be8989b0bec5bec677f5","Many methodologies of change detection have been discussed in the literature, but most of them are tested on only optical images or traditional synthetic-aperture radar (SAR) images. Few studies have investigated multipolarimetric SAR image change detection. In this study, we presented a type of multipolarimetric SAR image change detection approach based on nonsubsampled contourlet transform and multiscale feature-level fusion techniques. In this approach, Instead of denoising an image in advance, the nonsubsampled contourlet transform multiscale decomposition was used to reduce the effect of speckle noise by processing only the low-frequency sub-band coefficients of the decomposed image, and the multiscale feature-level fusion technique was employed to integrate the rich information obtained from various polarization images. Because SAR image information is dependent on scale, a multiscale multipolarimetric feature-level fusion strategy is introduced into the change detection to improve change detection precision; this feature-level fusion can not only achieve complementation of information with different polarizations and on different scales, but also has better robustness against noise. Compared with PCA methods, the proposed method constructs better differential images, resulting in higher change detection precision.","Data fusion; Feature extraction; Fusion reactions; Geometrical optics; Image enhancement; Image fusion; Polarization; Signal detection; Synthetic aperture radar; Change detection; Feature level fusion; Multi-scale Decomposition; Multipolarimetric SAR; Non-sub-sampled contourlet transforms; NSCT; Robustness against noise; Synthetic aperture radar (SAR) images; Radar imaging","Change detection; Fusion; Multipolarimetric SAR; NSCT","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84975056654"
"Jiang L.; Ling X.; Geng J.; Cheng Y.","Jiang, Lei (57189635143); Ling, Xianqing (43361351000); Geng, Jian (57189627834); Cheng, Yongsheng (57189627549)","57189635143; 43361351000; 57189627834; 57189627549","SAR image edge detection based on fuzzy theory and information fusion","2015","IET Conference Publications","2015","CP677","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973535707&partnerID=40&md5=e20f449a4fe9122c2e3b50df95b3e679","Synthetic Aperture Radar (SAR) image edge detection is one of the important steps for earth observation and target recognition, and it is recognized a difficult problem by domestic and foreign researchers because of the presence of speckle noise. A study was undertaken in this paper to speckle noise suppression and edge detail holding for SAR image edge detection, and we present an intelligent edge detection approach based on fuzzy theory and information fusion. Some typical SAR images were selected for edge detection using this approach in order to verify the result of relevant speckle noise suppression and edge detail holding. Results shows that this method can effectively inhibit the SAR image speckle noise and has smart edge detection ability. Unlike classical SAR image edge detection methods, this approach is a comprehensive integrated of different intelligent information processing methods, and it shows good robustness and universality.","Edge detection; Image fusion; Information fusion; Radar; Radar target recognition; Speckle; Spurious signal noise; Synthetic aperture radar; Detection approach; Earth observations; Fuzzy theory; Intelligent information processing; SAR Images; Speckle noise; Synthetic aperture radar (SAR) images; Target recognition; Radar imaging","Edge detection; Fuzzy theory; SAR image; Speckle noise suppression","Conference paper","Final","","Scopus","2-s2.0-84973535707"
"de Furtado L.F.A.; Silva T.S.F.; Fernandes P.J.F.; de Novo E.M.L.M.","de Furtado, Luiz Felipe Almeida (56106698500); Silva, Thiago Sanna Freire (23994716100); Fernandes, Pedro José Farias (57190373887); de Novo, Evlyn Márcia Leão de Moraes (22235086500)","56106698500; 23994716100; 57190373887; 22235086500","Classificação da cobertura da terra na planície de inundação do Lago Grande de Curuai (Amazônia, Brasil) utilizando dados multisensor e fusão de imagens; [Land cover classification of Lago Grande de Curuai floodplain (Amazon, Brazil) using multi-sensor and image fusion techniques]","2015","Acta Amazonica","45","2","","195","202","7","10.1590/1809-4392201401439","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924144165&doi=10.1590%2f1809-4392201401439&partnerID=40&md5=3d0f4a15c9b887149774853dab22b319","Given the limitations of different types of remote sensing images, automated land-cover classifications of the Amazon várzea may yield poor accuracy indexes. One way to improve accuracy is through the combination of images from different sensors, by either image fusion or multi-sensor classifications. Therefore, the objective of this study was to determine which classification method is more efficient in improving land cover classification accuracies for the Amazon várzea and similar wetland environments - (a) synthetically fused optical and SAR images or (b) multi-sensor classification of paired SAR and optical images. Land cover classifications based on images from a single sensor (Landsat TM or Radarsat-2) are compared with multi-sensor and image fusion classifications. Object-based image analyses (OBIA) and the J.48 data-mining algorithm were used for automated classification, and classification accuracies were assessed using the kappa index of agreement and the recently proposed allocation and quantity disagreement measures. Overall, optical-based classifications had better accuracy than SAR-based classifications. Once both datasets were combined using the multi-sensor approach, there was a 2% decrease in allocation disagreement, as the method was able to overcome part of the limitations present in both images. Accuracy decreased when image fusion methods were used, however. We therefore concluded that the multi-sensor classification method is more appropriate for classifying land cover in the Amazon várzea. © 2015, Instituto Nacional de Pesquisas da Amazonia. All rights reserved.","","Remote sensing; Synthetic aperture radar; Wetlands","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84924144165"
"Murali Mohan Babu Y.; Subramanyam M.V.; Giri Prasad M.N.","Murali Mohan Babu, Y. (57188737305); Subramanyam, M.V. (34975973700); Giri Prasad, M.N. (36093770500)","57188737305; 34975973700; 36093770500","Fusion and texture based classification of Indian microwave data - A comparative study","2015","International Journal of Applied Engineering Research","10","1","","1003","1010","7","","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926645807&partnerID=40&md5=d45d06afc9794314a442b1d39d4ffc3f","Radar Imaging Satellite (RISAT-1) is the first and only one microwave satellite that is operated by the INDIA. SAR image was fused with a Landsat- 8 image using two different image fusion techniques. Image classification was performed and the classification accuracy was assessed. The best results were achieved by the image originated from combining the RISAT-1 image with the Landsat-8 image using the PCA fusion. This paper aims to estimate the fusion of Landsat-8 imagery with RISAT-1 satellite imaging data to assess land cover classification in a representative area of the Brazilian Atlantic forest domain. © Research India Publications.","","Classification; Error matrix; Image fusion; Landsat-8; Synthetic Aperture Radar (SAR)","Article","Final","","Scopus","2-s2.0-84926645807"
"Zhi-She W.; Feng-Bao Y.; Zhi-Hao P.; Lei C.; Li-E J.","Zhi-She, Wang (57028521600); Feng-Bao, Yang (57028403300); Zhi-Hao, Peng (57028299800); Lei, Chen (57027002200); Li-E, Ji (57028478900)","57028521600; 57028403300; 57028299800; 57027002200; 57028478900","Multi-sensor image enhanced fusion algorithm based on NSST and top-hat transformation","2015","Optik","126","23","","4184","4190","6","10.1016/j.ijleo.2015.08.118","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952311342&doi=10.1016%2fj.ijleo.2015.08.118&partnerID=40&md5=347e6088b865040e57109bb991bab42c","In this paper, a novel method based on nonsubsampled shearlet transform (NSST) and multi-scale top-hat transform (MTHT) is proposed to fuse the SAR image, infrared image and visible light image, which will produce a fused image with higher contrast and better edges. First, three original images are decomposed into low frequency subband coefficients and the bandpass direction subband coefficients by NSST. Second, the effective bright and dark image features from low-frequency coefficients of three original images are extracted by MTHT, then by using the pixel-wise maximum operation, the salient fused bright and dark features are obtained and joined into the combined low frequency coefficients. To reduce the influence of noises of SAR and infrared image on the fused image, the significant factor of local directional information entropy, which can distinguish meaningful image characteristics and noises, is employed as the measurement to select the bandpass direction subband coefficients. Finally, the fused image is obtained by the inverse NSST on the combined coefficients. The experiment results demonstrate the superiority of our method compared with conventional methods in terms of visual inspection and objective measures. © 2015 Elsevier GmbH. All rights reserved.","Algorithms; Image processing; Infrared imaging; Inverse problems; Radar imaging; Synthetic aperture radar; Conventional methods; Directional information; Image characteristics; NSST; Shearlet transforms; Subband coefficients; Top-hat transformation; Visible light images; Image fusion","Image fusion; Local directional information entropy; NSST; Top-hat transformation","Article","Final","","Scopus","2-s2.0-84952311342"
"Nurmemet I.; Ghulam A.; Tiyip T.; Elkadiri R.; Ding J.-L.; Maimaitiyiming M.; Abliz A.; Sawut M.; Zhang F.; Abliz A.; Sun Q.","Nurmemet, Ilyas (56575085200); Ghulam, Abduwasit (6507293434); Tiyip, Tashpolat (35610692100); Elkadiri, Racha (56239234100); Ding, Jian-Li (13610395400); Maimaitiyiming, Matthew (56023783000); Abliz, Abdulla (57188854762); Sawut, Mamat (36184652100); Zhang, Fei (55479495800); Abliz, Abdugheni (57202460163); Sun, Qian (55321641500)","56575085200; 6507293434; 35610692100; 56239234100; 13610395400; 56023783000; 57188854762; 36184652100; 55479495800; 57202460163; 55321641500","Monitoring soil salinization in Keriya River Basin, Northwestern China using passive reflective and active microwave remote sensing data","2015","Remote Sensing","7","7","","8803","8829","26","10.3390/rs70708803","49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937944001&doi=10.3390%2frs70708803&partnerID=40&md5=ed35b3d7213c46c7e8661de70d780c87","Soil salinization is one of the most widespread soil degradation processes on Earth, especially in arid and semi-arid areas. The salinized soil in arid to semi-arid Xinjiang Uyghur Autonomous Region in China accounts for 31% of the area of cultivated land, and thus it is pivotal for the sustainable agricultural development of the area to identify reliable and cost-effective methodologies to monitor the spatial and temporal variations in soil salinity. This objective was accomplished over the study area (Keriya River Basin, northwestern China) by adopting technologies that heavily rely on, and integrate information contained in, a readily available suite of remote sensing datasets. The following procedures were conducted: (1) a selective principle component analysis (S-PCA) fusion image was generated using Phased Array Type L-band SAR (PALSAR) backscattering coefficient (σ°) and Landsat Enhanced Thematic Mapper Plus (ETM+) multispectral image of Keriya River Basin; and (2) a support vector machines (SVM) classification method was employed to classify land cover types with a focus on mapping salinized soils; (3) a cross-validation method was adopted to identify the optimum classification parameters, and obtain an optimal SVM classification model; (4) Radarsat-2 (C band) and PALSAR polarimetric images were used to analyze polarimetric backscattering behaviors in relation to the variation in soil salinization; (5) a decision tree (DT) scheme for multi-source optical and polarimetric SAR data integration was proposed to improve the estimation and monitoring accuracies of soil salinization; and (6) detailed field observations and ground truthing were used for validation of the adopted methodology, and quantity and allocation disagreement measures were applied to assess classification outcome. Results showed that the fusion of passive reflective and active microwave remote sensing data provided an effective tool in detecting soil salinization. Overall accuracy of the adopted SVM classifier with optimal parameters for fused image of ETM+ and PALSAR data was 91.25% with a Kappa coefficient of 0.89, which was further improved by the DT data integration and classification method yielding an accuracy of 93.01% with a Kappa coefficient of 0.92 and lower disagreement of quantity and allocation. © 2015 by the authors.","Agricultural machinery; Arid regions; Backscattering; Cost effectiveness; Data integration; Decision trees; Image fusion; Mapping; Polarimeters; Principal component analysis; Rivers; Soils; Support vector machines; Synthetic aperture radar; Trees (mathematics); Watersheds; Backscattering coefficients; Enhanced thematic mapper plus (ETM+); Principle component analysis; River basins; Soil salinization; Spatial and temporal variation; Sustainable agricultural development; SVM classification; Remote sensing","Decision tree; Image fusion; Keriya River basin; Soil salinization; SVM classification","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84937944001"
"Li W.-H.; Sun M.-X.; Yang H.-N.; Du X.-X.","Li, Wei-Hua (57103964300); Sun, Ming-Xin (55218457500); Yang, Huai-Ning (55576501200); Du, Xiao-Xia (56580927700)","57103964300; 55218457500; 55576501200; 56580927700","Multi-angle SAR images for earthquake damage assessment","2015","Proceedings of the 2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2015","","","7306305","714","716","2","10.1109/APSAR.2015.7306305","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957684143&doi=10.1109%2fAPSAR.2015.7306305&partnerID=40&md5=e61d45d0e1c4d5e2672cb0fc4f3d4501","Space-borne synthetic aperture radar (SAR) is very useful due to the whole-day-work capability, which has been widely used in earthquake damage assessment and rescue. This paper presents a novel method for earthquake damage assessment by using multi-angle SAR images. The scatter of building, especially for the damaged buildings, will change with the elevation angle and squint angle, which means a serious lack of information. In order to get more information, the multi-angle SAR images should be obtained. Moreover, a fast and accurate image information algorithm should be used. Furthermore, image geometry correction is realized in slant range plane. After correction operation, fusion operation is implemented. Finally, the simulation results justify the proposed method. © 2015 IEEE.","Damage detection; Earthquakes; Geophysics; Image fusion; Radar; Synthetic aperture radar; Earthquake damages; Elevation angle; Image geometries; Image information; Multi angle; Multi-Angle-SAR; Space-borne; Squint angles; Radar imaging","geometry correction; image fusion; multi-angles; SAR","Conference paper","Final","","Scopus","2-s2.0-84957684143"
"Hossain M.A.; Elshafiey I.; Alkanhal M.A.S.","Hossain, Md Anowar (42261732000); Elshafiey, Ibrahim (6603244260); Alkanhal, Majeed A. S. (6603030382)","42261732000; 6603244260; 6603030382","High-resolution and Wide-swath UWB OFDM MIMO Synthetic Aperture Radar System Using Image Fusion","2015","Journal of the Indian Society of Remote Sensing","43","2","406","225","242","17","10.1007/s12524-014-0406-4","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929711070&doi=10.1007%2fs12524-014-0406-4&partnerID=40&md5=6714af585528dd318756e65303549729","MIMO radar has received significant attention to radar communities recently. This paper presents a novel scheme for wide-swath and high-resolution synthetic aperture radar based on a MIMO UWB-OFDM system. The swath is increased based on a new approach for orthogonal pulse-shaping and MIMO wide-swath SAR topology. The resolution is improved using UWB-OFDM waveforms. Performance is further enhanced using image fusion techniques to benefit from the potentials of multi-sensor imagery such as noise level reduction. © 2014, Indian Society of Remote Sensing.","image resolution; input-output analysis; pixel; swath bathymetry; synthetic aperture radar","Image fusion; Multiple input multiple output (MIMO); Orthogonal frequency division multiplexing (OFDM); Synthetic aperture radar (SAR); Ultra-wideband (UWB)","Article","Final","","Scopus","2-s2.0-84929711070"
"Ding H.","Ding, H. (56585192800)","56585192800","Similarity measures of full polarimetric SAR images fusion for improved SAR image matching","2015","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","40","7W4","","13","16","3","10.5194/isprsarchives-XL-7-W4-13-2015","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975089790&doi=10.5194%2fisprsarchives-XL-7-W4-13-2015&partnerID=40&md5=05d8abe701612c765a99862c835fa0bb","China's first airborne SAR mapping system (CASMSAR) developed by Chinese Academy of Surveying and Mapping can acquire high-resolution and full polarimetric (HH, HV, VH and VV) Synthetic aperture radar (SAR) data. It has the ability to acquire X-band full polarimetric SAR data at a resolution of 0.5m. However, the existence of speckles which is inherent in SAR imagery affects visual interpretation and image processing badly, and challenges the assumption that conjugate points appear similar to each other in matching processing. In addition, researches show that speckles are multiplicative speckles, and most similarity measures of SAR image matching are sensitive to them. Thus, matching outcomes of SAR images acquired by most similarity measures are not reliable and with bad accuracy. Meanwhile, every polarimetric SAR image has different backscattering information of objects from each other and four polarimetric SAR data contain most basic and a large amount of redundancy information to improve matching. Therefore, we introduced logarithmically transformation and a stereo matching similarity measure into airborne full polarimetric SAR imagery. Firstly, in order to transform the multiplicative speckles into additivity ones and weaken speckles' influence on similarity measure, logarithmically transformation have to be taken to all images. Secondly, to prevent performance degradation of similarity measure caused by speckles, measure must be free or insensitive of additivity speckles. Thus, we introduced a stereo matching similarity measure, called Normalized Cross-Correlation (NCC), into full polarimetric SAR image matching. Thirdly, to take advantage of multi-polarimetric data and preserve the best similarity measure value, four measure values calculated between left and right single polarimetric SAR images are fused as final measure value for matching. The method was tested for matching under CASMSAR data. The results showed that the method delivered an effective performance on experimental imagery and can be used for airborne SAR matching applications.","Data fusion; Image fusion; Image matching; Image processing; Mapping; Metadata; Polarimeters; Speckle; Stereo image processing; Synthetic aperture radar; Effective performance; Image transformations; NCC measure; Normalized cross correlation; Performance degradation; Polarimetric SAR data; Redundancy information; Similarity measure; Radar imaging","Full polarimetric SAR data; Information-based entropy; Logarithmically image transformation; NCC measure; SAR image matching; Similarity measures fusion","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84975089790"
"Ran D.; Yin C.-B.; Zhu W.-Q.; Lao G.-C.","Ran, Da (56459824300); Yin, Can-Bin (35207724100); Zhu, Wei-Qiang (57191994150); Lao, Guo-Chao (56513686900)","56459824300; 35207724100; 57191994150; 56513686900","Multi-angle SAR non-coherent image fusion algorithm based on HIS statistic characteristics","2016","Proceedings of SPIE - The International Society for Optical Engineering","10033","","100331X","","","","10.1117/12.2244871","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000786729&doi=10.1117%2f12.2244871&partnerID=40&md5=374bf2d23e8213e6fb412afd40db0179","In order to reduce the shadow in traditional linear SAR image, a multi-angle SAR non-coherent image fusion algorithm based on HIS statistic characteristics is proposed. By converting SAR image to HIS space, a threshold based on the statistic characteristics of SAR image's HIS parameter is calculated and SAR images of different observation angles, which have been filtered according to the threshold previously calculated, are fused by non-coherent accumulation method. The fused image not only effectively reduces the image shadow, but also improves the detection probability of targets. The simulation results verify the effectiveness of the proposed algorithm. © 2016 SPIE.","Image fusion; Image processing; Synthetic aperture radar; Coherent images; Detection probabilities; Fused images; Multi-Angle-SAR; Observation angle; SAR Images; Statistic characteristics; Radar imaging","HIS Transform.; Image Fusion; Multi-angle SAR","Conference paper","Final","","Scopus","2-s2.0-85000786729"
"Zeng T.; Ao D.; Hu C.; Zhang T.; Liu F.; Tian W.; Lin K.","Zeng, Tao (35326077500); Ao, Dongyang (56047385000); Hu, Cheng (55723108300); Zhang, Tian (55616987500); Liu, Feifeng (35098797600); Tian, Weiming (26649288900); Lin, Kuan (55466941700)","35326077500; 56047385000; 55723108300; 55616987500; 35098797600; 26649288900; 55466941700","Multiangle BSAR Imaging Based on BeiDou-2 Navigation Satellite System: Experiments and Preliminary Results","2015","IEEE Transactions on Geoscience and Remote Sensing","53","10","7112619","5760","5773","13","10.1109/TGRS.2015.2430312","44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933555884&doi=10.1109%2fTGRS.2015.2430312&partnerID=40&md5=b9875bfb3352d3b7be2b84a4627280db","This paper analyzes the multiangle imaging results for bistatic synthetic aperture radar (BSAR) based on global navigation satellite systems (GNSS-BSAR). Due to the shortcoming of GNSS-BSAR images, a multiangle observation and data processing strategy based on BeiDou-2 navigation satellites was put forward to improve the quality of images and the value of system application. Twenty-six BSAR experiments were conducted and analyzed in different configurations. Furthermore, a region-based fusion algorithm using region-of-interest (ROI) segmentation was proposed to generate a high-quality fusion image. Based on the fusion image, typical targets such as water area, vegetation area, and artificial targets were compared and interpreted among single/multiple-angle images. The results reveal that the multiangle imaging method was a good technique to enhance image information, which might extend the applications of GNSS-BSAR. © 2015 IEEE.","Communication satellites; Data handling; Image fusion; Image segmentation; Navigation; Radar; Radio navigation; Satellites; Space-based radar; Synthetic aperture radar; Bistatic synthetic aperture radar; Global Navigation Satellite Systems; Image interpretation; Multi-angle; Multi-angle observations; Navigation satellites; Navigation-satellite systems; Processing strategies; BeiDou Navigation Satellite System; data interpretation; data quality; experimental study; GNSS; hydrosphere; image processing; image resolution; imaging method; satellite imagery; satellite navigation system; synthetic aperture radar; vegetation cover; Global positioning system","Bistatic synthetic aperture radar (BSAR); global navigation satellite system (GNSS); image fusion; image interpretation; multiangle","Article","Final","","Scopus","2-s2.0-84933555884"
"Liu M.; Dai Y.; Zhang J.; Zhang X.; Meng J.; Xie Q.","Liu, Meijie (57221054798); Dai, Yongshou (56048616800); Zhang, Jie (55963073000); Zhang, Xi (24469469100); Meng, Junmin (14028537000); Xie, Qinchuan (55840156400)","57221054798; 56048616800; 55963073000; 24469469100; 14028537000; 55840156400","PCA-based sea-ice image fusion of optical data by HIS transform and SAR data by wavelet transform","2015","Acta Oceanologica Sinica","34","3","","59","67","8","10.1007/s13131-015-0634-7","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924975294&doi=10.1007%2fs13131-015-0634-7&partnerID=40&md5=482e22e8ff8c96af7c516f6766761b10","Sea ice as a disaster has recently attracted a great deal of attention in China. Its monitoring has become a routine task for the maritime sector. Remote sensing, which depends mainly on SAR and optical sensors, has become the primary means for sea-ice research. Optical images contain abundant sea-ice multi-spectral information, whereas SAR images contain rich sea-ice texture information. If the characteristic advantages of SAR and optical images could be combined for sea-ice study, the ability of sea-ice monitoring would be improved. In this study, in accordance with the characteristics of sea-ice SAR and optical images, the transformation and fusion methods for these images were chosen. Also, a fusion method of optical and SAR images was proposed in order to improve sea-ice identification. Texture information can play an important role in sea-ice classification. Haar wavelet transformation was found to be suitable for the sea-ice SAR images, and the texture information of the sea-ice SAR image from Advanced Synthetic Aperture Radar (ASAR) loaded on ENVISAT was documented. The results of our studies showed that, the optical images in the hue-intensity-saturation (HIS) space could reflect the spectral characteristics of the sea-ice types more efficiently than in the red-green-blue (RGB) space, and the optical image from the China-Brazil Earth Resources Satellite (CBERS-02B) was transferred from the RGB space to the HIS space. The principal component analysis (PCA) method could potentially contain the maximum information of the sea-ice images by fusing the HIS and texture images. The fusion image was obtained by a PCA method, which included the advantages of both the sea-ice SAR image and the optical image. To validate the fusion method, three methods were used to evaluate the fused image, i.e., objective, subjective, and comprehensive evaluations. It was concluded that the fusion method proposed could improve the ability of image interpretation and sea-ice identification. © 2015, The Chinese Society of Oceanography and Springer-Verlag Berlin Heidelberg.","","HIS transform; optical remote sensing image; PCA method; SAR remote sensing image; sea ice; wavelet transform","Article","Final","","Scopus","2-s2.0-84924975294"
"Pohl C.; Loong C.K.; Van Genderen J.","Pohl, Christine (7102763531); Loong, Chong Khai (57188878635); Van Genderen, John (7003791000)","7102763531; 57188878635; 7003791000","Multisensor approach to oil palm plantation monitoring using data fusion and GIS","2015","ACRS 2015 - 36th Asian Conference on Remote Sensing: Fostering Resilient Growth in Asia, Proceedings","","","","","","","","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964048175&partnerID=40&md5=932e9c71334cc2fc286507e9763efe49","Oil palm is recognized as the golden crop. It produces the highest oil yield among oil seed crops. Malaysia, as the world's second largest producer of palm oil, has 16 % of its lands planted with oil palms. To cope with the ever-increasing global demand on edible oil, additional areas of oil palm are forecasted to increase globally by 12 to 19 million hectares by 2050. Multisensor remote sensing plays an important role by providing relevant, timely and accurate information that can be developed into a plantation monitoring system to optimize production and sustainability. The use of synthetic aperture radar (SAR), a form of microwave remote sensing, in combination with visible and infrared (VIR) data has several distinct advantages, the biggest benefits being daylight and weather independent. SAR produces 'cloud-free' images. However, SAR image are difficult to interpret. Using optical remote sensing provides important physical parameters of the plantation using multispectral data acquisition. Both types of data are complementary and need to be exploited simultaneously to obtain a holistic view on the plantation. Using interferometric SAR a topographical surface and height profiles of oil palm plantations can be derived. The information is crucial in the effort of mapping the oil palms age profile in the country. With this monitored, a replanting program could be effectively installed to maximize national production. VIR remote sensing delivers information on the plants' health and stress along with other biophysical parameters. Therefore the study aims to discover a set of parameters for oil palm plantation monitoring, which are retrievable from multisensor remote sensing data. The parameters are validated through the collection of ground. It is anticipated to derive all relevant information for the oil palm industry to implement a sustainable plantation management. The workflow on the parameter extraction and information derivation is designed and optimized.","Crops; Data acquisition; Data fusion; Image fusion; Monitoring; Oil shale; Parameter extraction; Radar; Radar imaging; Remote sensing; Synthetic aperture radar; Biophysical parameters; Interferometric SAR; Microwave remote sensing; Multi-spectral data; Multisensor remote sensing; Oil palm plantations; Optical remote sensing; Plantation managements; Palm oil","Image fusion; Palm oil; Radar; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-84964048175"
"Zhang Y.-C.; Jia Z.-H.; Qin X.-Z.; Yang J.; Kasabov N.","Zhang, Yi-Chen (55917984700); Jia, Zhen-Hong (55858535700); Qin, Xi-Zhong (25931063200); Yang, Jie (15039078800); Kasabov, Nikola (35585005300)","55917984700; 55858535700; 25931063200; 15039078800; 35585005300","Unsupervised detection of different SAR images based on improved NSCT domain image fusion algorithm","2015","Guangdianzi Jiguang/Journal of Optoelectronics Laser","26","10","","2023","2030","7","10.16136/j.joel.2015.10.0252","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946903887&doi=10.16136%2fj.joel.2015.10.0252&partnerID=40&md5=4d72a4fda5427d091c13e66a11d9cf6c","In order to reduce the total errors and obtain a better result in change detection with different SAR images, an unsupervised change detection algorithm is proposed in this paper. The technique is based on nonsubsampled contourlet transform (NSCT) domain with an improved fusion algorithm and fuzzy local information C-means clustering (FLICMC). First of all, mean-ratio operation and log-ratio operation are adopted to obtain the difference image, by using two images acquired in the same geographical area at different moments. Secondly, the two difference images are transformed into NSCT domain. The low frequency coefficients of two images of difference images respectively are fused by the rule of the maximum gradient quadratic sum; the high frequency coefficients of two images of difference images respectively are fused by the rule of the weighted fusion. And the final difference image can be got by using NSCT inverse transform. Finally, on the basis of FLICMC algorithm, the final difference image is divided into change area and unchanged area. According to the research between the real remote sensing data set and the simulated remote sensing data set, the experimental results show that the proposed algorithm can effectively suppress speckle noise impact on the detection results, and is not limited by the statistical distribution of change class and unchanged class. It has great applicability and doesn't require prior knowledge. So the proposed algorithm can get better detection effect. Compared with DWT2-FLICMC and MRF-FCMC, the proposed algorithm can obtain higher detection accuracy and shorter running time. ©, 2015, Board of Optronics Lasers. All right reserved.","Clustering algorithms; Image fusion; Inverse transforms; Magnetorheological fluids; Noise pollution; Radar imaging; Remote sensing; Synthetic aperture radar; C-means; Change detection; Image fusion algorithms; Non subsampled contourlet transform (NSCT); Nonsubsampled confourlet transform (NSCT); Statistical distribution; Unsupervised change detection; Unsupervised detection; Image enhancement","Change detection; Fuzzy local information C-means clutering (FLICMC); Image fusion; Nonsubsampled confourlet transform (NSCT)","Article","Final","","Scopus","2-s2.0-84946903887"
"Wei C.","Wei, Chunzhu (57669890800)","57669890800","Detecting and analysing ""urban villages"" in the Pearl River delta using multisource remote sensing data","2015","CEUR Workshop Proceedings","1598","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977578415&partnerID=40&md5=cfc9edb1af2294d2b95580c899c5d3ee","'Urban villages' is the Chinese version of informal settlement. It is a unique phenomenon that comprises mainly low-rise and congested, often illegal buildings surrounded by new constructions and high-rise buildings. Due to a lack of an unambiguous definition allowing for a spatial delineation of such areas, this article investigates a joint use of high-resolution optical and SAR satellite data through building extraction and 3D reconstruction of urban villages in Shenzhen, China. First, potential urban village footprints are extracted through a combined image fusion analysis of multispectral GaoFen-1 (GF-1) and high resolution TerraSAR-X radar (SAR) imagery. Then, building height estimation is performed on the basis of interferometry principles using interferometric X-band SAR (InSAR) from the Tandem-X mission. It can be demonstrated that urban villages and surrounding urban areas are clearly distinguishable through particular combinations of optical data, SAR data and height information. In particular, a rigid analysis identified three types of information as most suitable: 1) Normalized Difference Vegetation Index (NDVI), 2) contextual parameters such as edge and line density from GF-1 multi-spectral imagery, and 3) textural parameters such as Grey-Level Co-occurrence Matrix (GLCM) variables from TerraSAR-X imagery. The additional height information from InSAR clearly improves the detecting of taller buildings surrounding the urban villages. In conclusion, the fusion of SAR and optical imagery can effectively reveal the footprint characteristics of urban villages. It is an effective means to reduce the effects of layover, shadow and dominant scattering at building location. The 3D building reconstruction model based on urban village footprint maps can reduce the continuous alteration of layover and shadow areas from high-rise buildings in the dense urban area.","Buildings; Image fusion; Interferometry; Maintenance; Remote sensing; Rural areas; Satellites; Spectroscopy; Synthetic aperture radar; Tall buildings; Three dimensional computer graphics; 3-d building reconstruction; Building extraction; Grey-level co-occurrence matrixes; Informal settlements; Multi-spectral imagery; Multisource remote sensing data; Normalized difference vegetation index; Spatial delineation; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-84977578415"
"Amarsaikhan D.; Battsengel V.; Bolor G.; Enkhjargal D.; Jargaldalai E.","Amarsaikhan, D. (16052116000); Battsengel, V. (54892039700); Bolor, G. (57188881236); Enkhjargal, D. (54891827500); Jargaldalai, E. (56568219800)","16052116000; 54892039700; 57188881236; 54891827500; 56568219800","Fusion of optical and SAR images for enhancement of forest classes","2015","ACRS 2015 - 36th Asian Conference on Remote Sensing: Fostering Resilient Growth in Asia, Proceedings","","","","","","","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964049665&partnerID=40&md5=e1ec8804f9c8d76d852235a6c2f941fe","The aim of this study is to evaluate the performances of different image fusion techniques for the enhancement of spectral and textural variations of different forest types using optical and microwave data sets. For the data fusion, modified intensity-hue-saturation (IHS) transformation, principal components analysis (PCA) method, Gram-Schmidt fusion, and wavelet-based method are used and the results are compared. Of these methods, the better results are obtained through the use of the modified IHS transformation and wavelet-based fusion. Overall, the research indicates that multisource data fusion can significantly improve the interpretation and analysis of variety of forest types.","Data fusion; Forestry; Fusion reactions; Metadata; Principal component analysis; Radar imaging; Remote sensing; Synthetic aperture radar; Wavelet analysis; Forest classes; IHS transformation; Image fusion techniques; Intensity hue saturations; Multisource data; Optical; Principal components analysis; Wavelet-based methods; Image fusion","Forest classes; Fusion; Optical; Synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-84964049665"
"Suman Babu P.; Majumdar T.J.; Bhattacharya A.K.","Suman Babu, P. (56423667100); Majumdar, T.J. (7005822303); Bhattacharya, Amit K. (57215032766)","56423667100; 7005822303; 57215032766","Study of spectral signatures for exploration of Bauxite ore deposits in Panchpatmali, India","2015","Geocarto International","30","5","","545","559","14","10.1080/10106049.2014.973066","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929133110&doi=10.1080%2f10106049.2014.973066&partnerID=40&md5=8328c735bd02d56f2b1551c3e4f62b32","Spectral analysis technique has been utilized to identify the Bauxite mineral occurrences in Panchpatmali, Orissa, India. Spectral processing of Landsat ETM+ data has been carried out by converting the digital data from quantized and calibrated values to reflectance values. Minimum noise fraction transformation is used to determine the inherent dimensionality of reflected Landsat ETM+ data, to segregate noise in the data, and to reduce the computational requirements for subsequent processing and interactively to locate pure pixels within the data-set, projecting n-dimensional scatterplots. Spectral processing results are displayed in the form of images corresponding to each group of pixels (endmembers). Mixed tune matched filtering method has been applied on Landsat ETM+ images which gave three score (abundance) images for three different classes (endmembers) such as Bauxite, vegetation and soil. Further, mineralized zones are identified using image fusion of ERS-2 SAR and Landsat ETM+ data using intensity-hue-saturation technique. © 2014, © 2014 Taylor & Francis.","India; Odisha; bauxite; data set; Landsat thematic mapper; mineral exploration; noise; ore deposit; pixel; satellite imagery; spectral analysis; synthetic aperture radar","Bauxite; endmembers; Landsat ETM+; MTMF; Panchpatmali; spectral analysis","Article","Final","","Scopus","2-s2.0-84929133110"
"Kong Y.; Chen W.; Leung H.","Kong, Yingying (35186206400); Chen, Weiyang (55911870500); Leung, Henry (7202811506)","35186206400; 55911870500; 7202811506","Target type recognition algorithm for SAR image based on multi-feature fusion classifier of KPFD","2015","ICEIEC 2015 - Proceedings of 2015 IEEE 5th International Conference on Electronics Information and Emergency Communication","","","7284576","435","439","4","10.1109/ICEIEC.2015.7284576","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958667131&doi=10.1109%2fICEIEC.2015.7284576&partnerID=40&md5=730a96c58261f9effe3869e76682800d","Due to the presence of speckle, the target recognition algorithm of SAR image is different from other algorithms. There exists nuances in the detail of type recognition. This paper proposes Multi-feature fusion classifier of KPFD. Based on data from MSTAR database, the results of experiment show the new algorithm is more effective than other 5 kinds of recognition algorithm and recently recognition algorithm. In addition, when the KPFD recognition algorithm is combined with feature fusion classifier in the decision level and measure level, the feature fusion classifier brings good performance on the identification of the types of tank by using Naive Bayesian Classification algorithm(NBC). The recognition rate is up to 87%. © 2015 IEEE.","Classification (of information); Image fusion; Image processing; Optical character recognition; Radar imaging; Radar target recognition; Synthetic aperture radar; Decision levels; measure level; Multi-feature fusion; SAR Images; Target recognition; Algorithms","decision level; measure level; Multi-feature fusion; PCA; SAR image; target recognition","Conference paper","Final","","Scopus","2-s2.0-84958667131"
"Wang W.; Ji Y.; Lin X.","Wang, Wenguang (55714172700); Ji, Yu (56883569700); Lin, Xiaoxia (55657601000)","55714172700; 56883569700; 55657601000","A novel fusion-based ship detection method from pol-SAR images","2015","Sensors (Switzerland)","15","10","A21","25072","25089","17","10.3390/s151025072","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942925822&doi=10.3390%2fs151025072&partnerID=40&md5=fa64828fdfc7b748fd10a8a806374bb9","A novel fusion-based ship detection method from polarimetric Synthetic Aperture Radar (Pol-SAR) images is proposed in this paper. After feature extraction and constant false alarm rate (CFAR) detection, the detection results of HH channel, diplane scattering by Pauli decomposition and helical factor by Barnes decomposition are fused together. The confirmed targets and potential target pixels can be obtained after the fusion process. Using the difference degree of the target, potential target pixels can be classified. The fusion-based ship detection method works accurately by utilizing three different features comprehensively. The result of applying the technique to measured Airborne Synthetic Radar (AIRSAR) data shows that the novel detection method can achieve better performance in both ship’s detection and ship’s shape preservation compared to the result of K-means clustering method and the Notch Filter method. © 2015 by the authors; licensee MDPI, Basel, Switzerland.","Clustering algorithms; Feature extraction; Image fusion; Pixels; Radar; Radar measurement; Ships; Synthetic aperture radar; Constant false alarm rate detections; Difference degrees; K-means clustering method; Pauli decomposition; Pol-SAR; Polarimetric synthetic aperture radars; Ship detection; Target decomposition; Radar imaging","Difference degree; Pol-SAR; Ship detection; Target decomposition","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84942925822"
"Merkle N.; Müller R.; Reinartz P.","Merkle, N. (57194604557); Müller, R. (7404246697); Reinartz, P. (56216874200)","57194604557; 7404246697; 56216874200","Registration of optical and SAR satellite images based on geometric feature templates","2015","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","40","1W5","","447","452","5","10.5194/isprsarchives-XL-1-W5-447-2015","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974536576&doi=10.5194%2fisprsarchives-XL-1-W5-447-2015&partnerID=40&md5=0657dffb3aa3f27ae704d1bd6cf5986b","Image registration is required for different remote sensing applications, like change detection or image fusion. Since research studies have shown the outstanding absolute geometric accuracy of high resolution radar satellites images like TerraSAR-X, the importance of SAR images as source for geolocation enhancement has increased. Due to this fact, multi-sensor image to image registration of optical and SAR images can be used for the improvement of the absolute geometric processing and accuracy of optical ima ges with TerraSAR-X as reference. In comparison to the common optical and SAR image registration methods the proposed method is a combination of intensity-based and feature-based approaches. The proposed method avoids the direct and often difficult detection of features from the SAR images. SAR-like templates are generated from features detected from the optical image. These templates are used for an intensity-based matching with the SAR image. The results of the matching process are ground control points, which are used for the estimation of translation parameters followed by a subpixel translation of the optical image. The proposed image registration method is tested for two pairs of TerraSAR-X and QuickBird images and one pair of TerraSAR-X andWorldView-2 i mages of a suburban area. The results show that with the proposed method the geometric accuracy of optical images can be enhanced.","Feature extraction; Geometrical optics; Geometry; Image fusion; Image matching; Image processing; Image registration; Remote sensing; Rock mechanics; Satellites; Space-based radar; Synthetic aperture radar; Image; Matching; Multi sensor; Multi-spectral; Optical; Registration; Radar imaging","Image; Matching; Multisensor; Multispectral; Optical; Registration; SAR","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84974536576"
"Sukawattanavijit C.; Chen J.","Sukawattanavijit, Chanika (55349184000); Chen, Jie (55909160300)","55349184000; 55909160300","Fusion of RADARSAT-2 imagery with LANDSAT-8 multispectral data for improving land cover classification performance using SVM","2015","Proceedings of the 2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2015","","","7306273","567","572","5","10.1109/APSAR.2015.7306273","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957645277&doi=10.1109%2fAPSAR.2015.7306273&partnerID=40&md5=6cb45bb348abe8fb999d71d17e6323ef","Study of the land cover classification using multi-source data are very important for eco-environment monitoring, land use planning and climatic change detection. In this study, the utility of multi-source RADARSAT-2 and LANDSAT-8 multi-spectral images for improving land cover classification performance using Support Vector Machine (SVM) classifier. HH polarized C band RADARSAT-2 images were fused with the three band (6, 5, and 4) LANDSAT-8 multispectral image for land cover classification. Wavelet-based fusion (WT) techniques are implemented in the data fusion process. The Radial Basic Function (RBF) kernel function were used for SVM classifier in order to classify land cover types in the study area. The results of the SVM classification were compared with those using standard method Maximum Likelihood (ML) classifier, and it demonstrates a higher accuracy. Finally, it was indicated by the study that the fusion of SAR and optical images can significantly improve the classification accuracy with respect to use single dataset, and the SVM classifier could clearly outperform the standard method the ML classifier. © 2015 IEEE.","Data fusion; Geometrical optics; Image classification; Image fusion; Land use; Maximum likelihood; Radar; Space-based radar; Spectroscopy; Support vector machines; Synthetic aperture radar; Wavelet analysis; Classification accuracy; Land cover classification; LANDSAT; Maximum likelihood classifiers; Multi-spectral data; Multispectral images; Radarsat-2; Radial basic function; Classification (of information)","image fusion; land cover classification; LANDSAT-8; RADARSAT-2; Support Vector Machine (SVM)","Conference paper","Final","","Scopus","2-s2.0-84957645277"
"Kudriashov V.V.; Garbar A.Y.; Lukin K.A.; Maslikowski L.; Samczynski P.; Kulpa K.S.","Kudriashov, Volodymyr V. (52663657700); Garbar, Artem Y. (57041070400); Lukin, Konstantin A. (16402414700); Maslikowski, Lukasz (35956594900); Samczynski, Piotr (24175061000); Kulpa, Krzysztof S. (6602483683)","52663657700; 57041070400; 16402414700; 35956594900; 24175061000; 6602483683","Fusion of images generated by radiometric and active noise SAR","2015","Cybernetics and Information Technologies","15","7","","58","66","8","10.1515/cait-2015-0089","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953384152&doi=10.1515%2fcait-2015-0089&partnerID=40&md5=8c5e985424ff4b15b2e2509a03a74951","The work is devoted to fusion of radar and radiometer images. Noise waveform SAR generates radar images of reflective objects of its field of view. A bistatic radiometer with synthetic aperture estimates the thermal radio emissions of the objects along their angular coordinates and even range. The estimated brightness temperatures of rough and smooth surfaces are different, as well as the radar responses from them. Identification of the parameters of objects surfaces may be done using results of joint processing of images generated by both sensors. The optimum and quasi-optimum criteria for fusion of the images were obtained. The latter was experimentally checked. It approves the opportunity to fuse the images for further estimation of some parameters of objects surfaces. The results obtained may be used in environmental and security applications.","Image fusion; Microwave devices; Parameter estimation; Radar; Radiometers; Surface roughness; Synthetic aperture radar; Angular coordinates; Bistatic radiometers; Brightness temperatures; Detection criteria; Identification of the parameters; Microwave radiometers; Noise; Security application; Radar imaging","Bistatic radiometer; Detection criteria; Image fusion; Microwave radiometer; Noise; Surface roughness; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84953384152"
"Lim Y.; Kim T.; Kim S.; Song W.; Kim K.-T.; Kim S.","Lim, Yunji (57197274270); Kim, Taehun (57030754600); Kim, Sungho (57216680387); Song, WooJin (35264932800); Kim, Kyung-Tae (56401277600); Kim, Sohyeon (56954298600)","57197274270; 57030754600; 57216680387; 35264932800; 56401277600; 56954298600","IR and SAR sensor fusion based target detection using BMVT-M","2015","Journal of Institute of Control, Robotics and Systems","21","11","","1017","1026","9","10.5302/J.ICROS.2015.15.0147","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946718702&doi=10.5302%2fJ.ICROS.2015.15.0147&partnerID=40&md5=e244b4effca5364a9d33c1605ef438e5","Infrared (IR) target detection is one of the key technologies in Automatic Target Detection/Recognition (ATD/R) for military applications. However, IR sensors have limitations due to the weather sensitivity and atmospheric effects. In recent years, sensor information fusion study is an active research topic to overcome these limitations. SAR sensor is adopted to sensor fusion, because SAR is robust to various weather conditions. In this paper, a Boolean Map Visual Theory-Morphology (BMVT-M) method is proposed to detect targets in SAR and IR images. Moreover, we suggest the IR and SAR image registration and decision level fusion algorithm. The experimental results using OKTAL-SE synthetic images validate the feasibility of sensor fusion-based target detection. © ICROS 2015.","Agricultural robots; Image fusion; Infrared imaging; Iridium; Military applications; Synthetic aperture radar; Target tracking; Atmospheric effects; Automatic target detection; Boolean map visual theory; Decision level fusion; Key technologies; Sensor fusion; Sensor information fusions; Weather sensitivity; Radar imaging","Boolean map visual theory; IR; OKTAL-SE; SAR; Sensor fusion; Target detection","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-84946718702"
"Abdikan S.; Bilgin G.; Sanli F.B.; Uslu E.; Ustuner M.","Abdikan, Saygin (55515101500); Bilgin, Gokhan (8362224100); Sanli, Fusun Balik (23098475900); Uslu, Erkan (25652333200); Ustuner, Mustafa (56246446800)","55515101500; 8362224100; 23098475900; 25652333200; 56246446800","Enhancing land use classification with fusing dual-polarized TerraSAR-X and multispectral RapidEye data","2015","Journal of Applied Remote Sensing","9","1","15125","","","","10.1117/1.JRS.9.096054","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929206896&doi=10.1117%2f1.JRS.9.096054&partnerID=40&md5=abec4cab5c9f901ec84144f6220cb1b2","The contribution of dual-polarized synthetic aperture radar (SAR) to optical data for the accuracy of land use classification is investigated. For this purpose, different image fusion algorithms are implemented to achieve spatially improved images while preserving the spectral information. To compare the performance of the fusion techniques, both the microwave X-band dual-polarized TerraSAR-X data and the multispectral (MS) optical image RapidEye data are used. Our test site, Gediz Basin, covers both agricultural fields and artificial structures. Before the classification phase, four data fusion approaches: (1) adjustable SAR-MS fusion, (2) Ehlers fusion, (3) high-pass filtering, and (4) Bayesian data fusion are applied. The quality of the fused images was evaluated with statistical analyses. In this respect, several methods are performed for quality assessments. Then the classification performances of the fused images are also investigated using the support vector machines as a kernel-based method, the random forests as an ensemble learning method, the fundamental k-nearest neighbor, and the maximum likelihood classifier methods comparatively. Experiments provide promising results for the fusion of dual polarimetric SAR data and optical data in land use/cover mapping. © 2015 The Authors. Published by SPIE.","Classification (of information); Decision trees; Geometrical optics; High pass filters; Image enhancement; Land use; Maximum likelihood; Nearest neighbor search; Satellites; Support vector machines; Synthetic aperture radar; Bayesian data fusions; Classification performance; Image fusion algorithms; Landuse classifications; Maximum likelihood classifiers; Multi-spectral; Rapideye; TerraSAR-X; Image fusion","Classification; Image fusion; Multispectral; RapidEye image; Synthetic aperture radar; TerraSAR-X image","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-84929206896"
"Wang Y.; He C.; Tu F.; Chen D.; Liao M.","Wang, Yunyan (55734131800); He, Chu (12345438500); Tu, Feng (56443691100); Chen, Dong (57223758795); Liao, Mingsheng (7202371636)","55734131800; 12345438500; 56443691100; 57223758795; 7202371636","PolSAR image classification using feature fusion algorithm based on feature selection and bilayer SVM","2015","Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University","40","9","","1157","1162","5","10.13203/j.whugis20140351","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941559554&doi=10.13203%2fj.whugis20140351&partnerID=40&md5=80901a82f641eae6a31deb18799dbabc","Single type of feature vector cannot fully describe objects, in order to fully use the rich object information of polarimetric SAR images and solve this problem, this paper put forward a novel feature fusion algorithm based on feature selection and bilayer SVM for polarimetric SAR image classificationthat can make full use of the completeness and dissimilarity between the features to form a more effective feature vector. Various types of feature vectors were extracted from an original image by different methods for fully describing the PolSAR data. The feature vectors were normalized to ensure each feature vector can be selected under the same standards and have the same role in classification. A spatial pyramid is introduced to get the feature vector in different size or spatial location. A mRMR feature selection method was used to obtain the optimal feature subset for given categories to avoid redundancy and overfitting phenomenon caused by the simple combination of various feature vectors. Finally, the multilayer concept was introduced and a bilayer SVM model was constructed to optimize and re-process the probabilities of the target category obtained by the first SVM. Experimental results on the two polarimetric SAR images achieved by the Jet Propulsion Laboratory show the superiority of the proposed approach.","Classification (of information); Image classification; Image fusion; Polarimeters; Radar imaging; Support vector machines; Synthetic aperture radar; Vectors; Bi-layer; Feature selection methods; Features fusions; Jet Propulsion Laboratory; MRMR; Object information; Polarimetric synthetic aperture radars; Spatial pyramids; algorithm; image classification; probability; support vector machine; synthetic aperture radar; Feature extraction","Bilayer support vector machine; Feature selection; Features fusion; MRMR; Polarimetric synthetic aperture radar","Article","Final","","Scopus","2-s2.0-84941559554"
"Abila M.","Abila, M. (56411981400)","56411981400","Uncovering changes in images using a supervised classifier and image exploration by NSCT fusion","2016","Journal of Chemical and Pharmaceutical Sciences","9","3","","1177","1181","4","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988959407&partnerID=40&md5=17046c3e47a0dc0a414bb6f42889a194","This proposed paper presents alteration recognition approach for synthetic aperture radar (SAR) images based on an image fusion and supervised classifier scheme. The image fusion technique is introduced to generate a difference image by using complementary evidence from a mean-ratio image and a log-ratio image. NSCT (Non-sub sampled contour let transform) fusion rules based on an average operator and minimum local area gradient are chosen to fuse the contour let coefficients for a low-frequency band and a high-frequency band, correspondingly to detain the background information and enhance the information of reformed regions in the merged difference image. On behalf of the remote sensing images, differencing (subtraction operator) and rationing (ratio operator) are renowned techniques for producing a difference image. An artificial neural network type multi-layer perception or else back propagation with feed forward network will be proposed for classifying changed and unchanged regions in the fused difference image. The results will be proven that rationing generates better difference image for change detection consuming supervised classifier segmentation approach and efficiency of this algorithm will be exhibited by sensitivity and correlation evaluation.","artificial neural network; back propagation; classifier; nervous system; perception; remote sensing","Contour let transform; Correlation; Differencing; Fusion; Gradient; Neural network; Rationing","Article","Final","","Scopus","2-s2.0-84988959407"
"Bovolo F.; Bruzzone L.","Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","9943212600; 7006892410","The Time Variable in Data Fusion: A Change Detection Perspective","2015","IEEE Geoscience and Remote Sensing Magazine","3","3","7284786","8","26","18","10.1109/MGRS.2015.2443494","105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943795616&doi=10.1109%2fMGRS.2015.2443494&partnerID=40&md5=76637b337053feb10da47ba928d29ab4","This paper presents an overview on the image fusion concept in the context of multitemporal remote sensing image processing. In the remote sensing literature, multitemporal image analysis mainly deals with the detection of changes and land-cover transitions. Thus the paper presents and analyses the most relevant literature contributions on these topics. From the perspective of change detection and detection of land-cover transitions, multitemporal image analysis techniques can be divided into two main groups: i) those based on the fusion of the multitemporal information at feature level, and ii) those based on the fusion of the multitemporal information at decision level. The former mainly exploit multitemporal image comparison techniques, which aim at highlighting the presence/absence of changes by generating change indices. These indices are then analyzed by unsupervised algorithms for extracting the change information. The latter rely mainly on classification and include both supervised and semi/partially-supervised/unsupervised methods. The paper focuses the attention on both standard (and largely used) methods and techniques proposed in the recent literature. The analysis is conducted by considering images acquired by optical and SAR systems at medium, high and very high spatial resolution. © 2013 IEEE.","Image fusion; Optical data processing; Remote sensing; Change detection; Detection of changes; Multi-temporal image; Multi-temporal remote sensing; Multitemporal image analysis; Presence/absence; Unsupervised algorithms; Very high spatial resolutions; comparative study; data assimilation; data interpretation; image analysis; land cover; literature review; remote sensing; spatial resolution; supervised classification; synthetic aperture radar; time series analysis; unsupervised classification; Image analysis","","Article","Final","","Scopus","2-s2.0-84943795616"
"Liu X.J.; Wang X.D.; Chen B.","Liu, X.J. (36053167000); Wang, X.D. (56811750200); Chen, B. (55534019800)","36053167000; 56811750200; 55534019800","Target detection and decision fusion of multi-band SAR images","2015","Control Engineering and Information Systems - Proceedings of the International Conference on Control Engineering and Information System, ICCEIS 2014","","","","191","194","3","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940540244&partnerID=40&md5=765ff4975aa4d25676590d214e32e5bc","CFAR detection and decision fusion is proposed in this paper to improve multi-band SAR images target detection performance Experiments on real images prove the advantage of this method. © 2015 Taylor & Francis Group, London.","Image fusion; Information systems; Synthetic aperture radar; CFAR detection; Decision fusion; Detection performance; Multi band; Real images; SAR Images; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-84940540244"
"Wang J.; Jiao S.; Sun Z.","Wang, Jiajing (56568510800); Jiao, Shuhong (7102094616); Sun, Zhenyu (56566960900)","56568510800; 7102094616; 56566960900","A novel SAR fusion image segmentation method based on triplet Markov field","2015","Proceedings of SPIE - The International Society for Optical Engineering","9443","","944310","","","","10.1117/12.2178776","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925438699&doi=10.1117%2f12.2178776&partnerID=40&md5=7fc67103d3056bf2f177c58f110fd511","Markov random field (MRF) has been widely used in SAR image segmentation because of the advantage of directly modeling the posterior distribution and suppresses the speckle on the influence of the segmentation result. However, when the real SAR images are nonstationary images, the unsupervised segmentation results by MRF can be poor. The recent proposed triplet Markov field (TMF) model is well appropriate for nonstationary SAR image processing due to the introduction of an auxiliary field which reflects the nonstationarity. In addition, on account of the texture features of SAR image, a fusion image segmentation method is proposed by fusing the gray level image and texture feature image. The effectiveness of the proposed method in this paper is demonstrated by a synthesis SAR image and the real SAR images segmentation experiments, and it is better than the state-of-art methods. © 2015 SPIE.","Image fusion; Image processing; Image segmentation; Image texture; Markov processes; Synthetic aperture radar; Textures; Fusion image; Markov Random Fields; Nonstationary; SAR Images; Texture features; Triplet Markov fields; Radar imaging","fusion image; Markov random field; nonstationary image; SAR image; texture feature; triplet Markov field","Conference paper","Final","","Scopus","2-s2.0-84925438699"
"Xu J.; Yu X.; Pei W.; Hu D.; Zhang L.","Xu, Jindong (35176864300); Yu, Xianchuan (12785792300); Pei, Wenjing (55195859000); Hu, Dan (36161111200); Zhang, Libao (35325855000)","35176864300; 12785792300; 55195859000; 36161111200; 35325855000","A remote sensing image fusion method based on feedback sparse component analysis","2015","Computers and Geosciences","85","","","115","123","8","10.1016/j.cageo.2015.09.022","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999715889&doi=10.1016%2fj.cageo.2015.09.022&partnerID=40&md5=2a52e0ce0fc78a25e3a0010faee0c081","We propose a new remote sensing image (RSI) fusion technique based on sparse blind source separation theory. Our method employs feedback sparse component analysis (FSCA), which can extract the original image in a step-by-step manner and is robust against noise. For RSIs from the China–Brazil Earth Resources Satellite, FSCA can separate useful surface feature information from redundant information and noise. The FSCA algorithm is therefore used to develop two RSI fusion schemes: one focuses on fusing high-resolution and multi-spectral images, while the other fuses synthetic aperture radar bands. The experimental results show that the proposed method can preserve spectral and spatial details of the source images. For certain evaluation indexes, our method performs better than classical fusion methods. © 2015 Elsevier Ltd","Blind source separation; Image analysis; Remote sensing; Spectroscopy; Synthetic aperture radar; Earth resources satellites; Evaluation index; Fusion techniques; High resolution; Multispectral images; Remote sensing images; Sparse component analysis; Surface feature; Image fusion","Blind source separation; Image fusion; Multi-spectral image; Remote sensing; Sparse component analysis; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-84999715889"
"Zhouping Y.","Zhouping, Yin (24777774600)","24777774600","Fusion algorithm of optical images and SAR with SVT and sparse representation","2015","International Journal on Smart Sensing and Intelligent Systems","8","2","","1123","1141","18","10.21307/ijssis-2017-799","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929683158&doi=10.21307%2fijssis-2017-799&partnerID=40&md5=1affb0c6a1ff8806e7c722043380a63d","Due to the different imaging mechanism of optical image and Synthetic Aperture Radar (SAR) image, they have the large different characteristics between the images, so fusing optical image and SAR image with image fusion technology could complement advantages and be able to better interpret the scenes information. A fusion algorithm of Synthetic Aperture Radar and optical image with fast sparse representation on low-frequency images was proposed. For the disadvantage of target information easily missing and the contrast low in fused image, and the fusion method with sparse representation could effectively retain target information of Synthetic Aperture Radar image, so the paper fuses low frequency images of Synthetic Aperture Radar and optical images using sparse representation. Moreover a new sparse coefficient fusion rules is proposed, and sparse decomposition process is improved to reduce the algorithm running time. Experimental results demonstrate the effectiveness of the algorithm.","Geometrical optics; Image fusion; Image representation; Radar imaging; Algorithm running time; Fusion algorithms; Low frequency images; Optical image; Running time; Sparse representation; Support value; Support value transform; Synthetic aperture radar images; Value transform; Synthetic aperture radar","Algorithm running time; Fusion algorithm; Optical images; SAR; Sparse representation; Support value transform","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-84929683158"
"Chang C.H.; Hsieh Y.T.; Wu S.T.; Chen C.T.; Chen J.C.","Chang, C.H. (56939376000); Hsieh, Y.T. (35336856900); Wu, S.T. (14049496500); Chen, C.T. (8943452800); Chen, J.C. (36930042500)","56939376000; 35336856900; 14049496500; 8943452800; 36930042500","Applying image fusion to integrate radar images and SPOT multispectral satellite images for forest type classification","2015","Taiwan Journal of Forest Science","30","3","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946097208&partnerID=40&md5=c47f5ea45becdf59f31b0b574b3e5370","[No abstract available]","","Forest type classification; Image fusion; Multispectral image; Synthetic aperture radar","Note","Final","","Scopus","2-s2.0-84946097208"
"Ji X.","Ji, Xiuxia (55900734800)","55900734800","An improved image fusion method of infrared image and SAR image based on contourlet and sparse representation","2015","Proceedings - 2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics, IHMSC 2015","1","","7334704","282","285","3","10.1109/IHMSC.2015.11","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954525454&doi=10.1109%2fIHMSC.2015.11&partnerID=40&md5=e6d9cf14f3c9e575231eb38075ae6edc","In this paper, we propose an improved image fusion method of infrared image and SAR image based on the Contour let transform and sparse representation. For the method, it decompose the source image into the low frequency sub band coefficients and the high frequency sub band coefficients with the Contour let transform. The low frequency coefficients with lower sparseness are dealed with sparse representation, construct over complete dictionary, solve sparse coefficient over the trained dictionary, and choose the low frequency coefficients with the larger energy fusion rule. The high frequency sub band coefficients are fused by gradient maxim in. Different frequency coefficients are used to reconstruct the fused image by the inverse Contour let transform. Experimental results show that the proposed method is a feasible and effective image fusion method in term of visual quality and objective evaluation. © 2015 IEEE.","Cybernetics; Image enhancement; Image fusion; Infrared imaging; Inverse problems; Man machine systems; Synthetic aperture radar; Contourlet transform; Different frequency; High frequency HF; Image fusion methods; Objective evaluation; Over-complete dictionaries; SAR Images; Sparse representation; Radar imaging","Contourlet transform; Image fusion; Infrared image; SAR image; Sparse representation","Conference paper","Final","","Scopus","2-s2.0-84954525454"
"Yang Z.; Dong Q.; Sun G.; Xing M.","Yang, Zemin (56051865100); Dong, Qi (57201600579); Sun, Guangcai (26424042000); Xing, Mengdao (7005922869)","56051865100; 57201600579; 26424042000; 7005922869","A fast implementation method for the FFBP algorithm","2015","IEEE National Radar Conference - Proceedings","2015-June","June","7131034","411","414","3","10.1109/RADAR.2015.7131034","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937857688&doi=10.1109%2fRADAR.2015.7131034&partnerID=40&md5=1b989c737e08b051b677f31e93be2c04","A fast implementation method for the fast factorized back-projection (FIM-FFBP) algorithm is proposed to focus high-resolution spotlight synthetic aperture radar (SAR) data. Different from the original FFBP utilizing two-dimensional image-domain interpolation for sub-aperture fusion, FIM-FFBP finishes the image fusion using chirp-z transform and circular shifting. FIM-FFBP yields enhanced efficiency over the nearest interpolation with 4 times upsampling based FFBP, and keeps the high precision simultaneously. Real-data experiment verifies the efficiency superiorities of the FIM-FFBP. © 2015 IEEE.","Computerized tomography; Efficiency; Image fusion; Interpolation; Z transforms; Back projection; Chirp Z transform; Enhanced efficiency; Fast implementation; High-precision; High-resolution spotlights; Spotlight; Two dimensional images; Synthetic aperture radar","fast factorized back-projection (FFBP); Spotlight; synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-84937857688"
"Otukei J.R.; Blaschke T.; Collins M.","Otukei, John Richard (35213734700); Blaschke, Thomas (7005427503); Collins, Michael (25937880300)","35213734700; 7005427503; 25937880300","Fusion of TerraSAR-x and Landsat ETM+ data for protected area mapping in Uganda","2015","International Journal of Applied Earth Observation and Geoinformation","38","","","99","104","5","10.1016/j.jag.2014.12.012","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998699246&doi=10.1016%2fj.jag.2014.12.012&partnerID=40&md5=84c7134415028efb08c44877662e9ccc","TerraSAR-X satellite acquires very high spatial resolution data with potential for detailed land cover mapping. A known problem with synthetic aperture radar (SAR) data is the lack of spectral information. Fusion of SAR and multispectral data provides opportunities for better image interpretation and information extraction. The aim of this study was to investigate the fusion between TerraSAR-X and Landsat ETM+ for protected area mapping using high pass filtering (HPF), principal component analysis with band substitution (PCA) and principal component with wavelet transform (WPCA). A total of thirteen land cover classes were identified for classification using a non-parametric C 4.5 decision tree classifier. Overall classification accuracies of 74.99%, 83.12% and 85.38% and kappa indices of 0.7220, 0.8100 and 0.8369 were obtained for HPF, PCA and WPCA fusion approaches respectively. These results indicate a high potential for a combined use of TerraSAR-X and Landsat ETM+ data for protected area mapping in Uganda. © 2014 Elsevier B.V.","Uganda; accuracy assessment; decision analysis; land cover; Landsat thematic mapper; mapping; principal component analysis; satellite data; spatial resolution; TerraSAR-X","Decision trees; Image fusion; Landsat ETM+; TerraSAR-X","Article","Final","","Scopus","2-s2.0-84998699246"
"Wei C.; Blaschke T.; Taubenböck H.","Wei, Chunzhu (57669890800); Blaschke, Thomas (7005427503); Taubenböck, Hannes (8698790500)","57669890800; 7005427503; 8698790500","Monitoring of ""urban Villages"" in Shenzhen, China from High-Resolution GF-1 and TerraSAR-X data","2015","Proceedings of SPIE - The International Society for Optical Engineering","9642","","964210","","","","10.1117/12.2194877","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961613968&doi=10.1117%2f12.2194877&partnerID=40&md5=80272aac6717c5514c198c559bb43cef","Urban villages comprise mainly low-rise and congested, often informal settlements surrounded by new constructions and high-rise buildings whereby structures can be very different between neighboring areas. Monitoring urban villages and analyzing their characteristics are crucial for urban development and sustainability research. In this study, we carried out a combined analysis of multispectral GaoFen-1 (GF-1) and high resolution TerraSAR-X radar (TSX) imagery to extract the urban village information. GF-1 and TSX data are combined with the Gramshmidt spectral sharpening method so as to provide new input data for urban village classification. The Grey-Level Co-occurrence Matrix (GLCM) approach was also applied to four directions to provide another four types (all, 0°, 90°, 45° directions) of TSX-based inputs for urban village detection. We analyzed the urban village mapping performance using the Random Forest approach. The results demonstrate that the best overall accuracy and the best producer accuracy of urban villages reached with the GLCM 90° dataset (82.33%, 68.54% respectively). Adding single polarization TSX data as input information to the optical image GF-1 provided an average product accuracy improvement of around 7% in formal built-up area classification. The SAR and optical fusion imagery also provided an effective means to eliminate some layover, shadow effects, and dominant scattering at building locations and green spaces, improving the producer accuracy by 7% in urban area classification. To sum up, the added value of SAR information is demonstrated by the enhanced results achievable over built-up areas, including formal and informal settlements. © 2015 SPIE.","Classification (of information); Decision trees; Geometrical optics; Image fusion; Radar imaging; Rural areas; Satellites; Synthetic aperture radar; Tall buildings; Urban growth; Accuracy Improvement; GF-1; Grey-level co-occurrence matrixes; Informal settlements; Random forests; TerraSAR-X; Urban area classification; Urban villages; Image analysis","GF-1; Image fusion; Random Forest; TerraSAR-X; Urban villages","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84961613968"
"Anat R.; Samuel A.; Waizman G.; Tsodikovich D.; Tfilin S.; Cotti J.; Feingersh T.","Anat, Rockah (57191575848); Samuel, A. (57204342008); Waizman, G. (57191574791); Tsodikovich, D. (57191573029); Tfilin, S. (57191575744); Cotti, J. (57492728900); Feingersh, T. (6507356342)","57191575848; 57204342008; 57191574791; 57191573029; 57191575744; 57492728900; 6507356342","ASIF - Automated system for image fusion","2015","Proceedings of the International Astronautical Congress, IAC","7","","","4888","4894","6","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991579904&partnerID=40&md5=b67d223df89056a6c5b0742d66dfbbde","Remote sensing involves various types of sensors that scan earth's surface and produce images containing diverse types of information. Each sensor provides unique data that is transformed to information from the scene. We are presenting here a software tool called ASIF (Automated System for image fusion) which automatically combines each sensor's data into one synergistic combination of imagery (a fused image). With ASIF the user can quickly generate a single image containing the fused data instead of investigating several images, providing a new way of observing convenient information at a region of interest. ASIF, as an integrating tool of EO (Electro-Optical) and SAR (Synthetic Aperture Radar) images provides fused information from the images. It performs optimal pre-processing algorithms for overcoming the difficulties in combining different contexts. Every optimized process was evolved after an extensive research on a diverse group of EO-SAR datasets. The fusion optimization is oriented for visual interpretation enhancement based on: surface, objects and infrastructures. ASIF involves state of the art image and signal processing steps that estimate the required processing of inputs in an adaptive manner, oriented for optimal results for visual human perception and analysis. These handle automatically all required geometric and radiometric pre-processing stages. The fusion modes are determined by the input EO-SAR images that are available over the requested fegion of interest. Input data may be: panchromatic EO, RGB only EO, multi-spectral EO, multi- polarized SAR, multi-frequency and multi-Temporal SAR data. At each resulting fusion product we reveal the latent added value of its member input images, in order to get resolution improvement, data certainty, objects detection and verification, change detection, completion of details and so on. ASIF system allows a new interesting interpretation product that resolves open questions that rise from the surface and provides us a comprehensive view of it. It provides us opportunities that better exploit the use of the sensors' data beyond the conventional and straightforward understanding. © 2015 by the American Institute Federation of Aeronautics and Astronautics. Inc. All rights reserved.","Adaptive optics; Automation; Data fusion; Image fusion; Image processing; Image segmentation; Reconfigurable hardware; Remote sensing; Sensor data fusion; Signal processing; Space-based radar; Synthetic aperture radar; Fusion optimization; Multi-temporal SAR; Pre-processing algorithms; Region of interest; Resolution improvement; SAR(synthetic aperture radar); Synergistic combinations; Visual interpretation; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-84991579904"
"Wang X.L.; Chen C.X.","Wang, X.L. (57188693490); Chen, C.X. (56867778200)","57188693490; 56867778200","Image fusion for synthetic aperture radar and multispectral images based on sub-bandmodulated non-subsampled contourlet transform and pulse coupled neural network methods","2016","Imaging Science Journal","64","2","","87","93","6","10.1080/13682199.2015.1136101","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962240757&doi=10.1080%2f13682199.2015.1136101&partnerID=40&md5=6a01bdd427596dc6c8676a9ea39d3c76","Fusion of synthetic aperture radar (SAR) and multispectral (MS) images can contribute to a better visual perception of the objects observed. Unfortunately, many classical approaches have been proven to be unsuitable for this task due to their intrinsic differences in imaging mechanism. In the non-subsampled contourlet transform domain, an alternative fusion method based on pulse coupled neural networks is proposed. To control the amount of SAR features to be integrated into MS image, a gradient-threshold combined modulation is designed for modulating the SAR sub-band coefficients. Experiments demonstrate that the proposed method outperforms its counterparts in spectral preservation and feature enhancement. © 2016 The Royal Photographic Society.","Image enhancement; Image fusion; Neural networks; Object recognition; Radar; Radar imaging; Combined modulation; Feature enhancement; Gradient thresholds; Intrinsic differences; Multi-spectral; Multispectral images; Non-sub-sampled contourlet transforms; Pulse coupled neural network; Synthetic aperture radar","Image fusion; Multispectral; Non-subsampled contourlet transform; Pulse coupled neural networks; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-84962240757"
