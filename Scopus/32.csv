"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Rajah P.; Odindi J.; Mutanga O.","Rajah, Perushan (56658660800); Odindi, John (36521256000); Mutanga, Onisimo (55912148400)","56658660800; 36521256000; 55912148400","Synergistic potential of dual-polarized synthetic aperture radar and multispectral optical imagery for invasive alien species detection and mapping","2020","Journal of Applied Remote Sensing","14","1","014512","","","","10.1117/1.JRS.14.014512","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083038901&doi=10.1117%2f1.JRS.14.014512&partnerID=40&md5=b2c55d2993b56a86f96352f8abf73e5b","Invasive alien species are a major threat to global biodiversity and result in adverse environmental and socioeconomic implications, such as reduced ecosystem services, landscape productivity, and costly eradication initiatives. The ability to monitor the extent and spread of alien species invasions provides valuable insight for the mitigation of these adverse implications. The generation of Earth Observation (EO) Sentinel sensor provides unprecedented freely available imagery, which is suitable for both local and regional invasive species monitoring. Specifically, its radar (S1) and optical (S2) sensors offer unique tandem datasets valuable for landscape analysis. Hence, we sought to determine the synergistic potential of fused S1 dual-polarized synthetic aperture radar (SAR) imagery with S2 optical imagery for invasive alien species detection and mapping. S1 and S2 imageries were fused at the feature level, and the support vector machine algorithm used for the multiclass image classification. Results indicated that the fusion of the S1 dual-polarized imagery with S2 optical imagery produced the highest classification accuracy (85%), whereas stand-alone S2 optical bands produced the lowest classification accuracy (79%). Findings from this study underline the significant synergistic potential and complementarity of new-age S2 optical imagery and dual-polarized S1 SAR imagery for invasive alien species detection and mapping. Due to large swath, higher pixel resolution, free availability, and possible tandem complementarity between optical and SAR sensors, we recommend Sentinel EO imagery as an economically viable option for the invasive alien species detection and mapping. © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Biodiversity; Ecosystems; Image classification; Mapping; Support vector machines; Synthetic aperture radar; Classification accuracy; Earth observations; Economically viable; Invasive alien species; Landscape analysis; Multispectral optical imagery; Support vector machine algorithm; Synthetic Aperture Radar Imagery; Radar imaging","dual-polarized; image fusion; invasive alien species; Sentinel-1; Sentinel-2; support vector machine; synergistic; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85083038901"
"Xiao D.; Niu H.; Guo F.; Zhao S.; Fan L.","Xiao, Dongyang (57211437892); Niu, Haipeng (8217681500); Guo, Fuchen (57396210900); Zhao, Suxia (35110237900); Fan, Liangxin (36801644300)","57211437892; 8217681500; 57396210900; 35110237900; 36801644300","Monitoring irrigation dynamics in paddy fields using spatiotemporal fusion of Sentinel-2 and MODIS","2022","Agricultural Water Management","263","","107409","","","","10.1016/j.agwat.2021.107409","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122235972&doi=10.1016%2fj.agwat.2021.107409&partnerID=40&md5=c6197e947cb84e71f06194e769deef3a","Rapid and accurate monitoring of irrigation dynamics in paddy fields (the start, end, duration and irrigation peak, etc.) at field scales is crucial to the fine management of agricultural water resources, especially in typical areas with water shortages. However, there is still a lack of sufficient research to depict irrigation dynamics in paddy fields at high temporal and spatial levels. To this end, this study fused Sentinel-2 and MODIS images to map the spatio-temporal dynamics of irrigation events in paddy fields. A popular spatiotemporal fusion algorithm (enhanced spatial and temporal adaptive reflectance fusion model, ESTARFM) was used to generate 25 high-spatial resolution (10 m) remote sensing images based on 9 Sentinel-2 images and 24 MODIS images. Random forest algorithm was used to extract the spatial distribution of irrigated paddy fields. Water body index and vegetation index were employed to identify the start, end and duration of irrigation in paddy fields. Penman-Monteith model was used to estimate water surplus and deficit of irrigation during the critical irrigation period with daily observation data from meteorological stations. This study was carried out in rice-growing areas in the middle and lower reaches of the Yellow River in China. The results indicated that the spatial distribution difference of irrigation events in paddy fields with the shortest 3-day interval could be monitored in collaboration with Sentinel-2 and MODIS images. The start, end, and duration of irrigation in paddy fields presented significant spatial differences at field scales. A large amount of water from groundwater and Yellow River was needed, because the total water shortage of irrigation in paddy fields in the study area was about 80.79% of the total water demand. In addition, paddy fields with higher water demand were more concentrated spatially, as were paddy fields with lower water demand. The feasibility of spatiotemporal fusion of multi-source remote sensing data makes it possible to continuously monitor irrigation dynamics in paddy fields on high spatial resolution scales, which is conducive to the construction of spatiotemporal database and big data platform of agricultural irrigation information. This would not only help in promoting the high-quality development of agricultural water resources management but also alleviating the contradiction of regional water resources. © 2022 Elsevier B.V.","China; Yellow River; Decision trees; Groundwater; Groundwater resources; Image fusion; Image resolution; Radiometers; Remote sensing; Rivers; Spatial distribution; Agricultural water; Field scale; High spatial resolution; Paddy fields; Sentinel-2; Spatio-temporal fusions; Water demand; Water dynamics; Water shortages; Yellow River basin; algorithm; environmental monitoring; irrigation; MODIS; paddy field; satellite imagery; Sentinel; spatiotemporal analysis; Image enhancement","Paddy fields; Sentinel-2; Spatiotemporal fusion; Water dynamics; Yellow River Basin","Article","Final","","Scopus","2-s2.0-85122235972"
"Liu Q.; Zhang S.; Wang N.; Ming Y.; Huang C.","Liu, Qihang (57211624190); Zhang, Shiqiang (8238620300); Wang, Ninglian (57788160200); Ming, Yisen (57786965100); Huang, Chang (56460475700)","57211624190; 8238620300; 57788160200; 57786965100; 56460475700","Fusing Landsat-8, Sentinel-1, and Sentinel-2 Data for River Water Mapping Using Multidimensional Weighted Fusion Method","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4208012","","","","10.1109/TGRS.2022.3187154","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133641338&doi=10.1109%2fTGRS.2022.3187154&partnerID=40&md5=a47aa2f7253b6102c45cf83424d5cc57","River water extent is critical for understanding river discharge or its hydrological conditions. Although numerous methods have been proposed to map river water from either optical or synthetic aperture radar (SAR) remotely sensed images, uncertainties still exist broadly. In this study, we developed an image fusion method that integrates Landsat-8, Sentinel-1, and Sentinel-2 images simultaneously for river water mapping with two major steps. Firstly, a posterior probability support vector machine (SVM) model was adopted to generate water probability maps from each individual image; and second, a multidimensional weighted fusion method (MDWFM) was developed to fuse these probability maps. Four reaches with different characteristics were selected as case study sites. High-resolution aerial images were acquired and used as the reference to evaluate our results. We found that the fusion process not only improves the quality of river water mapping but also excludes the cloud interference. The fused river water maps become more reliable after the conflicts from difference images being solved by the proposed MDWFM method that contains a proportional conflict redistribution rule. The weighted root mean square difference was reduced to 0.066, and the area under the ROC curve reached up to 0.984. The critical success index (CSI), kappa coefficient (KC), and F-measure reached up to 0.810, 0.836, and 0.895, respectively. These stable and accurate river extent mapping results obtained through fusing multiple images with high spatial resolution (SR) (10 m) and short revisit interval (0.4-4.4 days) are of great significance for enriching the data and methodology of hydrological studies.  © 1980-2012 IEEE.","Antennas; Image fusion; Mapping; Optical remote sensing; Probability; Radar imaging; Rivers; Support vector machines; Synthetic aperture radar; Index; Multi dimensional; Multi-dimensional weighted fusion; Posterior probability; Remote-sensing; River water; Support vectors machine; Uncertainty; Water mapping; Weighted fusion; river water; satellite data; Landsat","Image fusion; multidimensional weighted fusion; posterior probability; support vector machine (SVM); synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85133641338"
"Xu F.; Somers B.","Xu, Fei (57192701694); Somers, Ben (12645025400)","57192701694; 12645025400","Unmixing-based Sentinel-2 downscaling for urban land cover mapping","2021","ISPRS Journal of Photogrammetry and Remote Sensing","171","","","133","154","21","10.1016/j.isprsjprs.2020.11.009","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096840638&doi=10.1016%2fj.isprsjprs.2020.11.009&partnerID=40&md5=db293498ffd6ce52070dd2d27d39fa24","With the launch of Sentinel-2 new opportunities for large scale urban mapping arise. However, the spectral information embedded in the Sentinel-2 20 m spatial resolution bands cannot yet be fully explored in heterogeneous urban landscapes. The 20 m image pixels are often composed of different land covers, resulting in a difficult to interpret mixed pixel spectrum. Here, we propose an unmixing-based image fusion algorithm (UnFuSen2) that self-adapts to the spectral variability of varying land covers and improves the image fusion accuracy by constraining the unmixing equations on the basis of spectral mixing models and the correlation between spectral bands of coarse and fine spatial resolution, respectively. When compared to alternative state-of-the-art downscaling methods UnFuSen2 consistently showed the highest accuracy when applied across test sites in three different European cities (RMSEUnFuSen2 = 203 vs RMSEalternatives = [252, 337]). In a next step, we applied Multiple Endmember Spectral Mixture Analysis (MESMA) on the downscaled Sentinel-2 image cube (i.e. ten 10 m bands) to generate subpixel urban land cover fractions. We compared our MESMA results against the traditional MESMA output as applied on the original Sentinel-2 image cube (i.e. four 10 m bands and six 20 m bands) and tested its robustness against reference data obtained over all three study sites. Results revealed an average decrease in RMSE of respectively 18% and 8% for impervious surface and vegetation fractions when our approach was compared to the traditional MESMA outcomes. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Image fusion; Image resolution; Mapping; Pixels; Downscaling methods; Image fusion algorithms; Multiple endmember spectral mixture analysis; Spatial resolution; Spectral information; Spectral variability; Urban land cover mappings; Vegetation fractions; algorithm; downscaling; land cover; mapping method; Sentinel; spatial resolution; urban area; Image enhancement","Image fusion; Sentinel-2; Spectral mixture analysis; Urban land cover mapping","Article","Final","","Scopus","2-s2.0-85096840638"
"Manocha A.; Afaq Y.; Bhatia M.","Manocha, Ankush (57207915903); Afaq, Yasir (57223431081); Bhatia, Munish (57190168274)","57207915903; 57223431081; 57190168274","Mapping of water bodies from sentinel-2 images using deep learning-based feature fusion approach","2022","Neural Computing and Applications","","","","","","","10.1007/s00521-022-08177-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144654184&doi=10.1007%2fs00521-022-08177-2&partnerID=40&md5=fba5c2933b716bc884b9629df6848604","As water is considered one of the essential assets of nature, the recognition of the availability of water at a specific location can help government bodies to take necessary action toward water conservation. Monitoring water from satellite images is considered one of the most difficult areas of pattern recognition. In this manner, a novel multi-level feature fusion approach is proposed to predict the pattern of water concerning a specific location to analyze the scale and availability. The proposed framework can access the spatial features from sentinel-2 images by utilizing the concept of structural learning. For evaluating the prediction performance, the calculated outcomes are compared with the traditional and modern pattern recognition approaches. It has been observed that the proposed approach is more robust in terms of pattern analysis as compared to the state-of-the-art approaches. Moreover, the performance of the proposed approach is evaluated on different training and testing ratios such as 70:30, 75:25, and 80:20. In this manner, the calculated outcomes define the pattern recognition efficiency of the proposed approach over the state-of-the-art approaches by achieving 94.51% of accuracy. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning; Image fusion; Water conservation; Water resources; Deep learning; Features fusions; Multi-level feature fusion approach; Multilevels; Pattern analysis; Sentinel-2 image; Specific location; State-of-the-art approach; Waterbodies; Waters resources; Pattern recognition","Deep learning; Multi-level feature fusion approach (MFFA); Pattern analysis; Sentinel-2 images; Water resource","Article","Article in press","","Scopus","2-s2.0-85144654184"
"Benedetti A.; Picchiani M.; Del Frate F.","Benedetti, Alessia (57207880738); Picchiani, Matteo (36004934300); Del Frate, Fabio (8255132900)","57207880738; 36004934300; 8255132900","Sentinel-1 and sentinel-2 data fusion for urban change detection","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8517586","1962","1965","3","10.1109/IGARSS.2018.8517586","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063138752&doi=10.1109%2fIGARSS.2018.8517586&partnerID=40&md5=a0d8d0605d22f4a619a8dfa325270137","In this paper a new approach based on the fusion of Sentinel-1 and Sentinel-2 products to map urban change detection and to observe suburb's development is presented. The algorithm developed can process data in a fast, automatic and accurate way. To reach this goal, the processing chain uses an iterative multitemporal approach based, for each iteration, on three procedures. The first and second ones are based on Pulse Coupled Neural Network (PCNN) applied to SAR and optical images, respectively, while the third processing is an optical multiband filter, implementing the spectral difference computation. The three outputs of each iteration are fused together by means of a weighted average formulation. The algorithm may deal with multitemporal acquisitions to improve the overall accuracy in the detection of urban changes by the integration of the outputs at different time intervals. © 2018 IEEE","","Change detection; Global monitoring urbanization; Image fusion; Sentinel-1; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85063138752"
"Guan H.; Su Y.; Hu T.; Chen J.; Guo Q.","Guan, Hongcan (57190177591); Su, Yanjun (56329651900); Hu, Tianyu (55490127200); Chen, Jin (55717837500); Guo, Qinghua (56820509800)","57190177591; 56329651900; 55490127200; 55717837500; 56820509800","An object-based strategy for improving the accuracy of spatiotemporal satellite imagery fusion for vegetation-mapping applications","2019","Remote Sensing","11","24","2927","","","","10.3390/rs11242927","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077901527&doi=10.3390%2frs11242927&partnerID=40&md5=9da990549549750c403a260c2b7d2b30","Spatiotemporal data fusion is a key technique for generating unified time-series images from various satellite platforms to support the mapping and monitoring of vegetation. However, the high similarity in the reflectance spectrum of different vegetation types brings an enormous challenge in the similar pixel selection procedure of spatiotemporal data fusion, which may lead to considerable uncertainties in the fusion. Here, we propose an object-based spatiotemporal data-fusion framework to replace the original similar pixel selection procedure with an object-restricted method to address this issue. The proposed framework can be applied to any spatiotemporal data-fusion algorithm based on similar pixels. In this study, we modified the spatial and temporal adaptive reflectance fusion model (STARFM), the enhanced spatial and temporal adaptive reflectance fusion model (ESTARFM) and the flexible spatiotemporal data-fusion model (FSDAF) using the proposed framework, and evaluated their performances in fusing Sentinel 2 and Landsat 8 images, Landsat 8 and Moderate-resolution Imaging Spectroradiometer (MODIS) images, and Sentinel 2 and MODIS images in a study site covered by grasslands, croplands, coniferous forests, and broadleaf forests. The results show that the proposed object-based framework can improve all three data-fusion algorithms significantly by delineating vegetation boundaries more clearly, and the improvements on FSDAF is the greatest among all three algorithms, which has an average decrease of 2.8% in relative root-mean-square error (rRMSE) in all sensor combinations. Moreover, the improvement on fusing Sentinel 2 and Landsat 8 images is more significant (an average decrease of 2.5% in rRMSE). By using the fused images generated from the proposed object-based framework, we can improve the vegetation mapping result by significantly reducing the ""pepper-salt"" effect. We believe that the proposed object-based framework has great potential to be used in generating time-series high-resolution remote-sensing data for vegetation mapping applications. © 2019 by the authors.","Forestry; Image fusion; Mapping; Mean square error; Pixels; Radiometers; Reflection; Remote sensing; Satellite imagery; Sensor data fusion; Time series; Vegetation; Data fusion algorithm; High resolution remote sensing; Moderate resolution imaging spectroradiometer; Object based; Reflectance spectrum; Root mean square errors; Spatio-temporal data; Vegetation mapping; Image enhancement","Object-based framework; Similar pixel; Spatiotemporal data fusion; Vegetation mapping","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85077901527"
"Bousbaa M.; Htitiou A.; Boudhar A.; Eljabiri Y.; Elyoussfi H.; Bouamri H.; Ouatiki H.; Chehbouni A.","Bousbaa, Mostafa (57983775500); Htitiou, Abdelaziz (57212145169); Boudhar, Abdelghani (35090979500); Eljabiri, Youssra (57982372500); Elyoussfi, Haytam (57983079400); Bouamri, Hafsa (57204929689); Ouatiki, Hamza (57195350633); Chehbouni, Abdelghani (7006296776)","57983775500; 57212145169; 35090979500; 57982372500; 57983079400; 57204929689; 57195350633; 7006296776","High-Resolution Monitoring of the Snow Cover on the Moroccan Atlas through the Spatio-Temporal Fusion of Landsat and Sentinel-2 Images","2022","Remote Sensing","14","22","5814","","","","10.3390/rs14225814","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142699868&doi=10.3390%2frs14225814&partnerID=40&md5=9b59b50187e0d0264b674f43f70c6ef3","Mapping seasonal snow cover dynamics provides essential information to predict snowmelt during spring and early summer. Such information is vital for water supply management and regulation by national stakeholders. Recent advances in remote sensing have made it possible to reliably estimate and quantify the spatial and temporal variability of snow cover at different scales. However, because of technological constraints, there is a compromise between the temporal, spectral, and spatial resolutions of available satellites. In addition, atmospheric conditions and cloud contamination may increase the number of missing satellite observations. Therefore, data from a single satellite is insufficient to accurately capture snow dynamics, especially in semi-arid areas where snowfall is extremely variable in both time and space. Considering these limitations, the combined use of the next generation of multispectral sensor data from the Landsat-8 (L8) and Sentinel-2 (S2), with a spatial resolution ranging from 10 to 30 m, provides unprecedented opportunities to enhance snow cover mapping. Hence, the purpose of this study is to examine the effectiveness of the combined use of optical sensors through image fusion techniques for capturing snow dynamics and producing detailed and dense normalized difference snow index (NDSI) time series within a semi-arid context. Three different models include the enhanced spatial and temporal adaptive reflectance fusion model (ESTARFM), the flexible spatio-temporal data fusion model (FSDAF), and the pre-classification flexible spatio-temporal data fusion model (pre-classification FSDAF) were tested and compared to merge L8 and S2 data. The results showed that the pre-classification FSDAF model generates the most accurate precise fused NDSI images and retains spatial detail compared to the other models, with the root mean square error (RMSE = 0.12) and the correlation coefficient (R = 0.96). Our results reveal that, the pre-classification FSDAF model provides a high-resolution merged snow time series and can compensate the lack of ground-based snow cover data. © 2022 by the authors.","Classification (of information); Dynamics; Image fusion; Mapping; Mean square error; Optical remote sensing; Snow; Time series; Water management; Water supply; Atlas mountain; Fusion model; LANDSAT; Landsat-8; Normalized difference snow index; Normalized difference snow indices; Sentinel-2; Snow covers; Spatial resolution; Spatio-temporal data; Landsat","Atlas Mountain; image fusion; Landsat-8; normalized difference snow index (NDSI); Sentinel-2; snow cover","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142699868"
"Sigurdsson J.; Ulfarsson M.O.; Sveinsson J.R.","Sigurdsson, Jakob (7006736374); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214)","7006736374; 6507677875; 7003642214","FUSING SENTINEL-2 SATELLITE IMAGES AND AERIAL RGB IMAGES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","4444","4447","3","10.1109/IGARSS47720.2021.9554406","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130050774&doi=10.1109%2fIGARSS47720.2021.9554406&partnerID=40&md5=a9f6932471da61c30ed82c7847d60469","Sentinel-2 (S2) is a constellation of two satellites that frequently acquire optical imagery over land and coastal waters. The S2 sensors have three spatial resolutions: 10, 20, and 60 m. Many remote sensing applications require the spatial resolution to be at the highest resolution, i.e., 10 m for S2. To address this demand, researchers have proposed various methods that exploit the spectral and spatial correlation in multispectral data to sharpen the S2 bands to 10 m. In this paper, we fuse S2 data with high-resolution aerial RGB images. A method called S2Sharp is modified to include the red, green, and blue bands of the aerial image and sharpen S2 data to the resolution of the RGB image. The method, termed S2PF, is evaluated using an S2 image and aerial photographs of Reykjavik, Iceland. © 2021 IEEE","Image fusion; Remote sensing; Satellite imagery; Coastal waters; High resolution; Image sharpening; Optical imagery; Remote sensing applications; RGB images; Satellite images; Sentinel-2  constellation; Spatial resolution; Superresolution; Antennas","Data fusion; Image sharpening; RGB images; Sentinel-2 (S2) constellation; Superresolution","Conference paper","Final","","Scopus","2-s2.0-85130050774"
"Emek R.A.; Demir N.","Emek, Recai Alper (57222222459); Demir, Nusret (56844945100)","57222222459; 56844945100","Building detection from sar images using unet deep learning method","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","44","4/W3","","215","218","3","10.5194/isprs-archives-XLIV-4-W3-2020-215-2020","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101898214&doi=10.5194%2fisprs-archives-XLIV-4-W3-2020-215-2020&partnerID=40&md5=ac0dda969cb34a630749324f0f39a8ae","SAR images are different from the optical images in terms of image properties with the values of scattering instead of reflectance. This makes SAR images difficult to apply the traditional object detection methodologies. In recent years, deep learning models are frequently used in segmentation and object detection purposes. In this study, we have investigated the potential of U-Net models for building detection from SAR and optical image fusion. The datasets used are Sentinel 1 SAR and Sentinel-2 multispectral images, provided from 'SpaceNet 6 Multi Sensor All- Weather Mapping' challenge. These images cover an area of 120 km2in Rotterdam, the Netherlands. As training datasets 20 pieces of 900 by 900 pixel sized HV polarized and optical image patches have been used together. The calculated loss value is 0.4 and the accuracy is 81%. © 2020 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Deep learning; Geometrical optics; Image fusion; Learning systems; Object detection; Object recognition; Synthetic aperture radar; Building detection; Image properties; Learning methods; Learning models; Multi sensor; Multispectral images; Optical image; Training data sets; Radar imaging","Building; Convolution Neural Network; Deep Learning; Object Detection; Radar; SAR; U-Net","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101898214"
"Rumora L.; Gašparović M.; Miler M.; Medak D.","Rumora, Luka (57211063553); Gašparović, Mateo (36987936900); Miler, Mario (57086384000); Medak, Damir (26642614700)","57211063553; 36987936900; 57086384000; 26642614700","Quality assessment of fusing Sentinel-2 and WorldView-4 imagery on Sentinel-2 spectral band values: a case study of Zagreb, Croatia","2020","International Journal of Image and Data Fusion","11","1","","77","96","19","10.1080/19479832.2019.1683624","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074924779&doi=10.1080%2f19479832.2019.1683624&partnerID=40&md5=aa7d4be24f8ba3650cd094a2600f96c6","Image fusion methods aim at fusing low resolution and high-resolution image to obtain a new image that provides new information for the specific application. The main goal of this article is multitemporal Sentinel-2 image fusion using single WorldView-4 satellite image for urban area monitoring. Fusing those images should provide Sentinel-2 image with similar radiometric band value as original Sentinel-2 image, but with a spatial resolution of WorldView-4. Ehlers, Brovey Transform, Modified Intensity-Hue-Saturation, High-Pass Filtering, Hyperspherical Colour Space and Wavelet resolution merge fusion techniques were used for spatial enhancement of Sentinel-2 images. Original and fused images were first compared using standard statistical parameters, mean, median and standard deviation. Image quality analysis was conducted with different objective image quality measures like root mean square error, peak signal to noise ratio, universal image quality index, structural similarity index, relative dimensionless global error, spatial correlation coefficient, relative average spectral error, spectral angle mapper, multi-scale structural similarity index. Using these quality measures helped in determining the spectral and spatial preservation of fused images. Hyperspherical colour space method was selected as the best method for image fusion of Sentinel-2 and WorldView-4 image-based on standard statistical parameters and quality measures. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","Croatia; Zagreb; Color; Errors; High pass filters; Image enhancement; Image fusion; Mean square error; Signal to noise ratio; Colour spaces; Intensity hue saturations; Peak signal to noise ratio; Sentinel-2; Spatial correlation coefficients; Spectral value; Structural similarity indices; WorldView-4; data quality; image processing; qualitative analysis; satellite imagery; Sentinel; signal-to-noise ratio; spectral resolution; WorldView; Image quality","hyperspherical colour space; Image fusion; Sentinel-2; spectral values; WorldView-4","Article","Final","","Scopus","2-s2.0-85074924779"
"Khawary H.; Zeaieanfirouzabadi P.; Sadidi J.","Khawary, H. (58080210700); Zeaieanfirouzabadi, P. (57193538279); Sadidi, J. (56469575500)","58080210700; 57193538279; 56469575500","REMOTE SENSING METHODS TO DETECT AND IDENTIFY CLAY-MADE HOUSES","2023","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","10","4/W1-2022","","399","404","5","10.5194/isprs-annals-X-4-W1-2022-399-2023","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146955361&doi=10.5194%2fisprs-annals-X-4-W1-2022-399-2023&partnerID=40&md5=431c36661ac94d7e4c0e087838aab326","One of the most important issues in urban management is the existence of clay-made houses in or around cities. By the hypothesis that these phenomena are spectrally and spatially behaviour differently from other land uses and land covers. This study attempts to detect and identify clay-made houses through different image processing methods. In this regard, Sentinel 1, 2, and Landsat 8 OLI thermal images of Yazd city have been analysed. After pre-processing, a number of image processing techniques, including optical and radar image composite generation, band ratio, image fusion, image filtering, and image classification (i.e. MLC and ANN), have been used. The result shows that RGB from a combination of Red=radar band C, Green=sentinel band 8, and Blue=sentinel band 4 are the best for visual interpretation of clay-made houses. Also, the most suitable band ratio is the ratio corresponding to radar C and NIR, the Red and Green bands, and the best fusion is derived with Sentinel 1 (band C) and Sentinel 2 (bands 4 and 8) images through the HSV algorithm, kappa coefficients of the agreement for MLC and ANN classifiers are 0.89 and 0.29 respectively. It is concluded that remote sensing image analysis can effectively be used to detect and identify clay-made houses. © Author(s) 2023. CC BY 4.0 License.","Houses; Image analysis; Image classification; Image fusion; Land use; Optical data processing; Optical remote sensing; Radar imaging; Band ratios; Image processing - methods; Land use and land cover; LANDSAT; Management IS; Remote-sensing; Sentinel-1; Thermal data; Urban management; Yazd; Classification (of information)","Classification; Data fusion; Radar; Remote sensing; Thermal Data; Yazd","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146955361"
"Miura H.","Miura, Hiroyuki (36842132600)","36842132600","Fusion analysis of optical satellite images and digital elevation model for quantifying volume in debris flow disaster","2019","Remote Sensing","11","9","1096","","","","10.3390/rs11091096","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065720493&doi=10.3390%2frs11091096&partnerID=40&md5=b85dd2dd148118129234ca7570d272e6","Rapid identification of affected areas and volumes in a large-scale debris flow disaster is important for early-stage recovery and debris management planning. This study introduces a methodology for fusion analysis of optical satellite images and digital elevation model (DEM) for simplified quantification of volumes in a debris flow event. The LiDAR data, the pre- and post-event Sentinel-2 images and the pre-event DEM in Hiroshima, Japan affected by the debris flow disaster on July 2018 are analyzed in this study. Erosion depth by the debris flows is empirically modeled from the pre- and post-event LiDAR-derived DEMs. Erosion areas are detected from the change detection of the satellite images and the DEM-based debris flow propagation analysis by providing predefined sources. The volumes and their pattern are estimated from the detected erosion areas by multiplying the empirical erosion depth. The result of the volume estimations show good agreement with the LiDAR-derived volumes. © 2019 by the authors.","Debris; Digital instruments; Disasters; Erosion; Geomorphology; Image analysis; Image fusion; Optical radar; Satellites; Change detection; Debris flows; Debris-flow propagation; Erosion depth; Volume; Surveying","Change detection; Debris flow; Debris flow propagation analysis; Erosion depth; Volume","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85065720493"
"Xiong S.; Du S.; Zhang X.; Ouyang S.; Cui W.","Xiong, Shuping (57549649100); Du, Shihong (8986148800); Zhang, Xiuyuan (57774580400); Ouyang, Song (57614380800); Cui, Weihong (57658783700)","57549649100; 8986148800; 57774580400; 57614380800; 57658783700","Fusing Landsat-7, Landsat-8 and Sentinel-2 surface reflectance to generate dense time series images with 10m spatial resolution","2022","International Journal of Remote Sensing","43","5","","1630","1654","24","10.1080/01431161.2022.2047240","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127056951&doi=10.1080%2f01431161.2022.2047240&partnerID=40&md5=8eb6a948088e30d05fd4e80e88a6fd51","Due to the influence of the external atmospheric environment and the physical constraints of the sensors, it is difficult for existing medium spatial resolution sensors to produce intensive time-series images. The easy accessibility and similar sensor configuration of Landsat and Sentinel-2 data make them suitable for synthesizing time-series images. Currently, some deep learning-based studies have achieved significant success in multi-sensor fusion, but these deep-learning fusion networks only use single features that are not enough to produce intensive time-series images. Shallow features are easy to extract but have weak representation, while the deep network can extract abstract features with strong representation ability but faces the problem of gradient disappearance and model degradation. To address these problems, we propose a novel spatial-temporal-spectral integration model that can simultaneously fuse Landsat-7,8 and Sentinel-2 sensors. First, an enhanced residual dense network (ERDN) is proposed to improve Landsat’s four bands (Blue, Green, Red, and Near-infrared) to 10 m spatial resolution. ERDN’s residual dense structure utilizes abundant shallow features (i.e. texture and spectral information) and deep features (i.e. angles of view and aspect ratios of objects) to reconstruct spatial structures. At the same time, the introduced high-resolution images can provide reliable spatial auxiliary information for the missing image parts. Second, we employ a time-series-based reflectance adjustment (TRA) method to reduce the reflectance differences between Landsat and Sentinel-2 images. The fusion results indicate that our fusion framework can not only fill the gap caused by the Landsat-7 scan-line corrector (SLC) failure but also restore more spatial details than previous methods. More importantly, it can predict tiny reflectance changes in land cover. Overall, our study can produce continuous reflection observations with a higher time density and provides important data guarantee for extracting dynamic changes in land coverages. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Aspect ratio; Deep learning; Image fusion; Infrared devices; Reflection; Textures; Time series; Woolen and worsted fabrics; Deep learning; LANDSAT; LandSat 7; Landsat-8; Sentinel-2; Spatial resolution; Spatial temporals; Spatial-temporal fusion; Surface reflectance; Times series; Landsat; learning; sensor; Sentinel; spatial resolution; surface reflectance; time series; Image resolution","deep learning; Landsat-7; Landsat-8; Sentinel-2; Spatial-temporal fusion; surface reflectance","Article","Final","","Scopus","2-s2.0-85127056951"
"Vaithiyanathan D.; Sudalaimuthu K.","Vaithiyanathan, Dhayalan (57932535400); Sudalaimuthu, Karuppasamy (57209618364)","57932535400; 57209618364","Area-to-point regression Kriging approach fusion of Landsat 8 OLI and Sentinel 2 data for assessment of soil macronutrients at Anaimalai, Coimbatore","2022","Environmental Monitoring and Assessment","194","12","916","","","","10.1007/s10661-022-10571-1","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140080862&doi=10.1007%2fs10661-022-10571-1&partnerID=40&md5=fb5b98e7c54dff6d7a037615679e0034","Spectral indices-based soil prediction models derived from multispectral datasets are too intricate in terms of accuracy as well as resolution. Complications arise while incorporating multispectral datasets for regional-scale spatial assessment of soil macronutrients. Sporadically satellite image fusion techniques have been used for soil nutrient interpolation to circumvent the complications. The fusion of multispectral bands encompasses precise soil information that cannot be observed as accurate with single satellite dataset. In this study, fusion of near infrared regions of Landsat 8 Operational Land Imager and Sentinel 2 has been observed for its contribution on soil macronutrient assessments. Area-to-point regression Kriging (ATPRK) approach is followed in fusing the two satellite imagery and in situ soil spectral have used for the validation of the resultant. Comparative statistical analysis on Landsat 8 OLI band 5 (wavelength: 845–885 nm), Sentine-2 band 8,8A (wavelength: 785–900 nm) datasets and fused satellite bands provides R2 values of 0.8209, 0.8436, and 0.8763 respectively. Regression models y = (0.25006 ± 0.00754) + (0.0000313)x, y = (0.25252 ± 0.0062) + (0.0000810)x, and y = (0.23715 ± 0.0062) + (0.0001210)x for nitrogen, phosphorus, and potassium respectively aids for soil macronutrient interpolation and assessments. Computations reveals the ranges of nitrogen, phosphorus and potassium that floats from 48 to 295 kg/ha, 5.0 to 37 kg/ha, and 32 to 455 kg/ha in the study area. Fusion of satellite imagery by ATPRK approaches in soil macronutrient study at regional scale brings the novelty of the study. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Environmental Monitoring; Nitrogen; Nutrients; Phosphorus; Potassium; Soil; Spatial Analysis; Anaimalai Hills; India; Tamil Nadu; Western Ghats; Image fusion; Infrared devices; Interpolation; Nitrogen; Phosphorus; Potassium; Regression analysis; Soils; nitrogen; phosphorus; potassium; phosphorus; Kriging approach; LANDSAT; Landsat 8 OLI; Multi-spectral; Multispectral datasets; Regression-kriging; Satellite image fusion; Satellite images; Sentine-2; Soil macronutrient study; data set; kriging; Landsat; Sentinel; spatiotemporal analysis; spectral analysis; agriculture; area to point regression Kriging approach; Article; controlled study; electric conductivity; image processing; India; mathematical model; satellite imagery; soil acidity; soil analysis; soil chemistry; soil moisture; environmental monitoring; procedures; soil; spatial analysis; Landsat","Landsat 8 OLI; Multispectral; Satellite image fusion; Sentine-2; Soil macronutrient study","Article","Final","","Scopus","2-s2.0-85140080862"
"Zhang C.; Feng Y.; Hu L.; Tapete D.; Pan L.; Liang Z.; Cigna F.; Yue P.","Zhang, Chenxiao (56412235100); Feng, Yukang (57867376800); Hu, Lei (57392739200); Tapete, Deodato (55221777800); Pan, Li (54393873900); Liang, Zheheng (57713064700); Cigna, Francesca (36720533600); Yue, Peng (57768830100)","56412235100; 57867376800; 57392739200; 55221777800; 54393873900; 57713064700; 36720533600; 57768830100","A domain adaptation neural network for change detection with heterogeneous optical and SAR remote sensing images","2022","International Journal of Applied Earth Observation and Geoinformation","109","","102769","","","","10.1016/j.jag.2022.102769","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136960546&doi=10.1016%2fj.jag.2022.102769&partnerID=40&md5=d59753651e778b1a13757706d403b08c","Heterogeneous remote sensing source-based change detection with optical and SAR data and their combined all-time and all-weather observation capability provides a reliable and promising solution for a wide range of applications. State-of-the-art supervised methods typically take a two-stage strategy that suffers from the loss of original image features and the introduction of noise on the transferred images. This paper proposes a domain adaptation-based multi-source change detection network (DA-MSCDNet) suitable to process heterogeneous optical and SAR images. DA-MSCDNet employs feature-level transformation to align inconsistent deep feature spaces in heterogeneous data. Feature space transformation and change detection are bridged within the network to encourage task communication. Experiments are conducted on two public datasets based on Sentinel-1A and Landsat-8 imagery acquired over the Sacramento, Yuba, and Sutter Counties (California, USA), and QuickBird-2 and TerraSAR-X imagery over Gloucester (UK), as well as one new large-scale dataset of Sentinel-2 and COSMO-SkyMed imagery over Wuhan (China). Compared with other six supervised and unsupervised approaches, the proposed method achieves the highest performance with an average precision of 80.81%, recall of 84.39%, mIOU of 73.67% and F1 score of 82.58%, beating the state-of-the-art method with 5.42% improvements on F1 score and 10 times efficiency on training time cost on the large-scale change detection task. © 2022 The Authors","California; China; Hubei; Sacramento County; Sutter County; United Kingdom; United States; Wuhan; Yuba County; artificial neural network; COSMO-SkyMed; detection method; image processing; Landsat; QuickBird; remote sensing; satellite imagery; Sentinel; synthetic aperture radar; Terra (satellite)","Domain adaptation; Feature alignment; Feature transformation; Heterogeneous change detection; Image fusion; Satellite imagery; Siamese network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136960546"
"Beltrán-Marcos D.; Suárez-Seoane S.; Fernández-Guisuraga J.M.; Fernández-García V.; Marcos E.; Calvo L.","Beltrán-Marcos, David (57614797700); Suárez-Seoane, Susana (55892510400); Fernández-Guisuraga, José Manuel (56595207800); Fernández-García, Víctor (57164794000); Marcos, Elena (56255880300); Calvo, Leonor (57216042206)","57614797700; 55892510400; 56595207800; 57164794000; 56255880300; 57216042206","Relevance of UAV and sentinel-2 data fusion for estimating topsoil organic carbon after forest fire","2023","Geoderma","430","","116290","","","","10.1016/j.geoderma.2022.116290","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143547946&doi=10.1016%2fj.geoderma.2022.116290&partnerID=40&md5=f1b894a19e66acf6192a77b0d72f1b43","The evaluation at detailed spatial scale of soil status after severe fires may provide useful information on the recovery of burned forest ecosystems. Here, we aimed to assess the potential of combining multispectral imagery at different spectral and spatial resolutions to estimate soil indicators of burn severity. The study was conducted in a burned area located at the northwest of the Iberian Peninsula (Spain). One month after fire, we measured soil burn severity in the field using an adapted protocol of the Composite Burn Index (CBI). Then, we performed soil sampling to analyze three soil properties potentially indicatives of fire-induced changes: mean weight diameter (MWD), soil moisture content (SMC) and soil organic carbon (SOC). Additionally, we collected post-fire imagery from the Sentinel-2A MSI satellite sensor (10–20 m of spatial resolution), as well as from a Parrot Sequoia camera on board an unmanned aerial vehicle (UAV; 0.50 m of spatial resolution). A Gram-Schmidt (GS) image sharpening technique was used to increase the spatial resolution of Sentinel-2 bands and to fuse these data with UAV information. The performance of soil parameters as indicators of soil burn severity was determined trough a machine learning decision tree, and the relationship between soil indicators and reflectance values (UAV, Sentinel-2 and fused UAV-Sentinel-2 images) was analyzed by means of support vector machine (SVM) regression models. All the considered soil parameters decreased their value with burn severity, but soil moisture content, and, to a lesser extent, soil organic carbon discriminated at best among soil burn severity classes (accuracy = 91.18 %; Kappa = 0.82). The performance of reflectance values derived from the fused UAV-Sentinel-2 image to monitor the effects of wildfire on soil characteristics was outstanding, particularly for the case of soil organic carbon content (R2 = 0.52; RPD = 1.47). This study highlights the advantages of combining satellite and UAV images to produce spatially and spectrally enhanced images, which may be relevant for estimating main impacts on soil properties in burned forest areas where emergency actions need to be applied. © 2022 The Author(s)","Iberian Peninsula; Antennas; Decision trees; Deforestation; Ecosystems; Fires; Image enhancement; Image resolution; Moisture determination; Organic carbon; Reflection; Regression analysis; Satellite imagery; Soil moisture; Support vector machines; Burn Severity; Performance; Sentinel-2; Soil indicator; Soil moisture content; Soil organic carbon; Soil parameters; Soil property; Spatial resolution; Wildfire; forest fire; machine learning; moisture content; organic carbon; reflectance; sampling; satellite imagery; satellite sensor; topsoil; trough; wildfire; Unmanned aerial vehicles (UAV)","Image fusion; Sentinel-2; Soil organic carbon; Soil properties; UAV; Wildfire","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143547946"
"Long J.; Peng Y.","Long, Jian (57218616379); Peng, Yuanxi (7403418922)","57218616379; 7403418922","Blind fusion of hyperspectral multispectral images based on matrix factorization","2021","Remote Sensing","13","21","4219","","","","10.3390/rs13214219","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118119744&doi=10.3390%2frs13214219&partnerID=40&md5=cb9252aa3697716d4af18cf0dbdd9042","The fusion of low spatial resolution hyperspectral images and high spatial resolution multispectral images in the same scenario is important for the super-resolution of hyperspectral images. The spectral response function (SRF) and the point spread function (PSF) are two crucial prior pieces of information in fusion, and most of the current algorithms need to provide these two preliminary pieces of information in advance, even for semi-blind fusion algorithms at least the SRF. This causes limitations in the application of fusion algorithms. This paper aims to solve the dependence of the fusion method on the point spread function and proposes a method to estimate the spectral response function from the images involved in the fusion to achieve blind fusion. We conducted experiments on simulated datasets Pavia University, CAVE, and the remote sensing images acquired by two spectral cameras, Sentinel 2 and Hyperion. The experimental results show that our proposed SRF estimation method can improve the PSNR value by 5 dB on average compared with other state-of-the-art SRF estimation results. The proposed blind fusion method can improve the PSNR value of fusion results by 3–15 dB compared with other blind fusion methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Factorization; Hyperspectral imaging; Image fusion; Image resolution; Matrix algebra; Optical transfer function; Remote sensing; Spectroscopy; Function estimation; Fusion algorithms; Fusion methods; HyperSpectral; Hyperspectral imaging super-resolution; Matrix factorizations; Multispectral images; Point-Spread function; Spectral response functions; Superresolution; Matrix factorization","Hyperspectral imaging super-resolution; Image fusion; Matrix factorization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118119744"
"Wu J.; Cheng Q.; Li H.; Li S.; Guan X.; Shen H.","Wu, Jingan (57208125591); Cheng, Qing (57210813717); Li, Huifang (36782247100); Li, Shuang (56180537800); Guan, Xiaobin (57191221261); Shen, Huanfeng (8359721100)","57208125591; 57210813717; 36782247100; 56180537800; 57191221261; 8359721100","Spatiotemporal Fusion with only Two Remote Sensing Images as Input","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9210773","6206","6219","13","10.1109/JSTARS.2020.3028116","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094889057&doi=10.1109%2fJSTARS.2020.3028116&partnerID=40&md5=6c018993b148057fff1e96abc5863dcb","Spatiotemporal data fusion is an effective way of generating a dense time series with a high spatial resolution. Traditionally, the spatiotemporal fusion models, especially the popular ones such as the spatial and temporal adaptive reflectance fusion model, require at least three images as input, i.e., a coarse-resolution image on the target date and a pair of fine- and coarse-resolution images on the reference date. However, this cannot always be satisfied, as the high-quality coarse-resolution image on the reference date may be unavailable in some application scenarios. This led to efforts to achieve data fusion only using the other two images as input. In this article, we proposed an effective strategy that can be combined with any spatiotemporal fusion model to accomplish the fusion with simplified input. To confirm the validity of the method, we comprehensively compared the fusion performances under the two input modalities. In total, 38 tests were conducted with Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat, and Sentinel-2 land surface reflectance products. Results suggest that by applying the proposed method, the fusion performance with only two input images is comparable or even superior to that with three input images. This article challenges the stereotype that spatiotemporal data fusion strictly needs at least three input images. The proposed method extends the application scenarios of spatiotemporal fusion, and creates opportunities to fuse sensors with barely overlapping temporal coverages, such as the Landsat 8 Operational Land Imager and the Sentinel-2 MultiSpectral Instrument.  © 2008-2012 IEEE.","Radiometers; Reflection; Remote sensing; Application scenario; High spatial resolution; Moderate resolution imaging spectroradiometer; Multispectral instruments; Operational land imager; Remote sensing images; Spatio-temporal data; Spatio-temporal fusions; image analysis; Landsat; MODIS; remote sensing; Sentinel; spatial resolution; spatiotemporal analysis; Image fusion","Landsat 8; Moderate Resolution Imaging Spectroradiometer (MODIS); Sentinel-2; simplified input modality; spatiotemporal data fusion; two input images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85094889057"
"Asefpour Vakilian A.; Saradjian M.R.","Asefpour Vakilian, Afshin (57324349100); Saradjian, Mohammad Reza (8397399100)","57324349100; 8397399100","An object-based sparse representation model for spatiotemporal image fusion","2022","Scientific Reports","12","1","5021","","","","10.1038/s41598-022-08728-6","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126868532&doi=10.1038%2fs41598-022-08728-6&partnerID=40&md5=6efe597a4c13568435e152565b83297d","Many algorithms have been proposed for spatiotemporal image fusion on simulated data, yet only a few deal with spectral changes in real satellite images. An innovative spatiotemporal sparse representation (STSR) image fusion approach is introduced in this study to generate global dense high spatial and temporal resolution images from real satellite images. It aimed to minimize the data gap, especially when fine spatial resolution images are unavailable for a specific period. The proposed approach uses a set of real coarse- and fine-spatial resolution satellite images acquired simultaneously and another coarse image acquired at a different time to predict the corresponding unknown fine image. During the fusion process, pixels located between object classes with different spectral responses are more vulnerable to spectral distortion. Therefore, firstly, a rule-based fuzzy classification algorithm is used in STSR to classify input data and extract accurate edge candidates. Then, an object-based estimation of physical constraints and brightness shift between input data is utilized to construct the proposed sparse representation (SR) model that can deal with real input satellite images. Initial rules to adjust spatial covariance and equalize spectral response of object classes between input images are introduced as prior information to the model, followed by an optimization step to improve the STSR approach. The proposed method is applied to real fine Sentinel-2 and coarse Landsat-8 satellite data. The results showed that introducing objects in the fusion process improved spatial detail, especially over the edge candidates, and eliminated spectral distortion by preserving the spectral continuity of extracted objects. Experiments revealed the promising performance of the proposed object-based STSR image fusion approach based on its quantitative results, where it preserved almost 96.9% and 93.8% of the spectral detail over the smooth and urban areas, respectively. © 2022, The Author(s).","Algorithms; algorithm","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85126868532"
"Camacho A.; Vargas E.; Arguello H.","Camacho, Ariolfo (57203320607); Vargas, Edwin (57203158812); Arguello, Henry (44061135000)","57203320607; 57203158812; 44061135000","Hyperspectral and multispectral image fusion addressing spectral variability by an augmented linear mixing model","2022","International Journal of Remote Sensing","43","5","","1577","1608","31","10.1080/01431161.2022.2041762","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126782455&doi=10.1080%2f01431161.2022.2041762&partnerID=40&md5=bd704ff05df3ddf517c1941a20bd1a10","The fusion of hyperspectral (HS) and multispectral (MS) images with complementary high spectral and high spatial resolution information has been successfully applied to improve the low spatial resolution limitation of current satellite sensors with high spectral resolution. Fusion methods based on spectral unmixing have led to state-of-the-art results with the advantage of obtaining pure spectral signatures and their proportion in the image of analysis as a result of the fusion process. However, pure spectral signatures observed in HS images are affected by variations in atmospheric, environmental and illumination conditions which are significant sources of errors in HS image analysis. Conventional unmixing-based fusion methods neglect the spectral variability of HS images introducing and propagating errors through the fusion process, affecting further inference tasks such as classification, target detection, and change detection. Therefore, this paper proposes HS-MS image fusion algorithm based on spectral unmixing accounting for spectral variability through an augmented linear mixing model (ALMM). An alternating optimization strategy is combined with the alternating direction method of multipliers (ADMM) to efficiently minimize the underlying cost function of the fusion problem. Simulation results on realistic data demonstrate that the proposed method outperforms state-of-the-art fusion methods in different metrics. Additionally, the fusion of crop images acquired with DESIS and Sentinel-2 sensors in a region of Colombia confirms the advantages of the proposed fusion method. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Colombia; Cost functions; Image analysis; Image enhancement; Image fusion; Image resolution; Mixing; Fusion methods; Fusion process; HyperSpectral; Hyperspectral image fusions; Linear mixing models; Multi-spectral image fusions; Spectral signature; Spectral unmixing; Spectral variability; State of the art; algorithm; detection method; image analysis; image resolution; optimization; remote sensing; Sentinel; spectral analysis; Spectral resolution","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85126782455"
"Benzenati T.; Kallel A.; Kessentini Y.","Benzenati, Tayeb (57216240538); Kallel, Abdelaziz (22233967900); Kessentini, Yousri (16052461400)","57216240538; 22233967900; 16052461400","End-to-End Spectral-Temporal Fusion Using Convolutional Neural Network","2021","Communications in Computer and Information Science","1322 CCIS","","","60","72","12","10.1007/978-3-030-71804-6_5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104863916&doi=10.1007%2f978-3-030-71804-6_5&partnerID=40&md5=c9b6e49ba4f8385c3f27a8206d7c20e1","In the last few years, Earth Observation sensors received a large development, offering, therefore, various types of data with different temporal, spatial, spectral, and radiometric resolutions. However, due to physical and budget limitations, the acquisition of images with the best characteristics is not feasible. Image fusion becomes a valuable technique to deal with some specific applications. In particular, the vegetation area, which needs a high spectral resolution and frequent coverage. In this paper, we present a novel fusion technique based on Convolutional Neural Networks (CNN) to combine two kinds of remote sensing data with different but complement spectral and temporal characteristics, to produce one high spectral and temporal resolution product. To the best of our knowledge, this is the first attempt to deal with the spectral-temporal fusion problem. The feasibility of the proposed method is evaluated via Sentinel-2 data. The experimental results show that the proposed technique can achieve substantial gains in terms of fusion performance. © 2021, Springer Nature Switzerland AG.","Budget control; Convolution; Image fusion; Pattern recognition; Remote sensing; Spectral resolution; Earth observation sensors; Fusion performance; Fusion techniques; High spectral resolution; Radiometric resolution; Remote sensing data; Temporal characteristics; Temporal resolution; Convolutional neural networks","Convolutional neural networks; Image fusion; Remote sensing; Spectral enhancement; Temporal enhancement","Conference paper","Final","","Scopus","2-s2.0-85104863916"
"Karim Z.; van Zyl T.L.","Karim, Zainoolabadien (57216271012); van Zyl, Terence L. (26532116600)","57216271012; 26532116600","Deep/transfer learning with feature space ensemble networks (Featspaceensnets) and average ensemble networks (avgensnets) for change detection using dinsar sentinel-1 and optical sentinel-2 satellite data fusion","2021","Remote Sensing","13","21","4394","","","","10.3390/rs13214394","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118494381&doi=10.3390%2frs13214394&partnerID=40&md5=4a4401bf1aefe9471959b16ab18af8ee","Differential interferometric synthetic aperture radar (DInSAR), coherence, phase, and displacement are derived from processing SAR images to monitor geological phenomena and urban change. Previously, Sentinel-1 SAR data combined with Sentinel-2 optical imagery has improved classification accuracy in various domains. However, the fusing of Sentinel-1 DInSAR processed imagery with Sentinel-2 optical imagery has not been thoroughly investigated. Thus, we explored this fusion in urban change detection by creating a verified balanced binary classification dataset comprising 1440 blobs. Machine learning models using feature descriptors and non-deep learning classifiers, including a two-layer convolutional neural network (ConvNet2), were used as baselines. Transfer learning by feature extraction (TLFE) using various pre-trained models, deep learning from random initialization, and transfer learning by fine-tuning (TLFT) were all evaluated. We introduce a feature space ensemble family (FeatSpaceEnsNet), an average ensemble family (AvgEnsNet), and a hybrid ensemble family (HybridEnsNet) of TLFE neural networks. The FeatSpaceEnsNets combine TLFE features directly in the feature space using logistic regression. AvgEnsNets combine TLFEs at the decision level by aggregation. HybridEnsNets are a combination of FeatSpaceEnsNets and AvgEnsNets. Several FeatSpaceEnsNets, AvgEnsNets, and HybridEnsNets, comprising a heterogeneous mixture of different depth and architecture models, are defined and evaluated. We show that, in general, TLFE outperforms both TLFT and classic deep learning for the small dataset used and that larger ensembles of TLFE models do not always improve accuracy. The best performing ensemble is an AvgEnsNet (84.862%) comprised of a ResNet50, ResNeXt50, and EfficientNet B4. This was matched by a similarly composed FeatSpaceEnsNet with an F1 score of 0.001 and variance of 0.266 less. The best performing HybridEnsNet had an accuracy of 84.775%. All of the ensembles evaluated outperform the best performing single model, ResNet50 with TLFE (83.751%), except for AvgEnsNet 3, AvgEnsNet 6, and FeatSpaceEnsNet 5. Five of the seven similarly composed FeatSpaceEnsNets outperform the corresponding AvgEnsNet. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Convolutional neural networks; Deep learning; Earth (planet); Feature extraction; Image enhancement; Image fusion; Multilayer neural networks; Network layers; Radar imaging; Sensor data fusion; Space optics; Space-based radar; Synthetic aperture radar; Artificial intelligence, remote sensing; Change detection; Deep learning; Differential interferometric synthetic aperture radars; Earth observations; Features extraction; Remote-sensing; Space data; Space data science; Transfer learning; Remote sensing","Artificial intelligence, remote sensing; Change detection; Deep learning; DInSAR; Earth observation; Machine learning; Space data science; Transfer learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118494381"
"Rangzan K.; Kabolizadeh M.; Karimi D.; Zareie S.","Rangzan, Kazem (7801505713); Kabolizadeh, Mostafa (36080758400); Karimi, Danya (57193122143); Zareie, Sajad (57192074480)","7801505713; 36080758400; 57193122143; 57192074480","Supervised cross-fusion method: a new triplet approach to fuse thermal, radar, and optical satellite data for land use classification","2019","Environmental Monitoring and Assessment","191","8","481","","","","10.1007/s10661-019-7621-y","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068763085&doi=10.1007%2fs10661-019-7621-y&partnerID=40&md5=d675ce3b978f1640df41a559a77f5e8c","This study presents a new fusion method namely supervised cross-fusion method to improve the capability of fused thermal, radar, and optical images for classification. The proposed cross-fusion method is a combination of pixel-based and supervised feature-based fusion of thermal, radar, and optical data. The pixel-based fusion was applied to fuse optical data of Sentinel-2 and Landsat 8. According to correlation coefficient (CR) and signal to noise ratio (SNR), among the used pixel-based fusion methods, wavelet obtained the best results for fusion. Considering spectral and spatial information preservation, CR of the wavelet method is 0.97 and 0.96, respectively. The supervised feature-based fusion method is a fusion of best output of pixel-based fusion level, land surface temperature (LST) data, and Sentinel-1 radar image using a supervised approach. The supervised approach is a supervised feature selection and learning of the inputs based on linear discriminant analysis and sparse regularization (LDASR) algorithm. In the present study, the non-negative matrix factorization (NMF) was utilized for feature extraction. A comparison of the obtained results with state of the art fusion method indicated a higher accuracy of our proposed method of classification. The rotation forest (RoF) classification results improvement was 25% and the support vector machine (SVM) results improvement was 31%. The results showed that the proposed method is well classified and separated four main classes of settlements, barren land, river, river bank, and even the bridges over the river. Also, a number of unclassified pixels by SVM are very low compared to other classification methods and can be neglected. The study results showed that LST calculated using thermal data has had positive effects on improving the classification results. By comparing the results of supervised cross-fusion without using LST data to the proposed method results, SVM and RoF classifiers showed 38% and 7% of classification improvement, respectively. © 2019, Springer Nature Switzerland AG.","Algorithms; Environmental Monitoring; Image Processing, Computer-Assisted; Iran; Radar; Rivers; Support Vector Machine; Temperature; Classification (of information); Discriminant analysis; Factorization; Feature extraction; Geometrical optics; Image classification; Image enhancement; Land surface temperature; Land use; Matrix algebra; Pixels; Radar imaging; Rivers; Signal to noise ratio; Space-based radar; Support vector machines; Classification methods; Correlation coefficient; Landuse classifications; LDASR; Linear discriminant analysis; Nonnegative matrix factorization; Sentinel; Sparse regularizations; algorithm; land classification; land use; radar; satellite data; Sentinel; supervised learning; surface temperature; article; correlation coefficient; discriminant analysis; feature extraction; land use; learning; riparian ecosystem; rotation; satellite imagery; signal noise ratio; support vector machine; telecommunication; algorithm; environmental monitoring; image processing; Iran; procedures; river; telecommunication; temperature; Image fusion","Classification; Image fusion; LDASR; LST; NMF; Sentinel","Article","Final","","Scopus","2-s2.0-85068763085"
"Horváth J.; Xiang Z.; Cannas E.D.; Bestagini P.; Tubaro S.; Delp E.J.","Horváth, János (57218707036); Xiang, Ziyue (57224548383); Cannas, Edoardo Daniele (57219686714); Bestagini, Paolo (21638596100); Tubaro, Stefano (7003411765); Delp, Edward J. (7005252700)","57218707036; 57224548383; 57219686714; 21638596100; 7003411765; 7005252700","Sat U-Net: A Fusion Based Method for Forensic Splicing Localization in Satellite Images","2022","Proceedings of SPIE - The International Society for Optical Engineering","12100","","1210002","","","","10.1117/12.2616150","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135909634&doi=10.1117%2f12.2616150&partnerID=40&md5=90b50581facac8f0b10679fb4f78aef5","Satellite images are widely available to the public. These satellite images are used in various elds including natural disaster analysis, meteorology and agriculture. As with any type of images, satellite images can be altered using image manipulation tools. A common manipulation is splicing, i.e., pasting on top of an image a region coming from a di erent source image. Most manipulation detection methods designed for images captured by ""consumer cameras""tend to fail when used with satellite images. In this paper we propose a machine learning approach, Sat U-Net, to fuse the results of two exiting forensic splicing localization methods to increase their overall accuracy and robustness. Sat U-Net is a U-Net based architecture exploiting several Transformers to enhance the performance. Sat U-Net fuses the outputs of two unsupervised splicing detection methods, Gated PixelCNN Ensemble and Vision Transformer, to produce a heatmap highlighting the manipulated image region. We show that our fusion approach trained on images from one satellite can be lightly retrained on few images from another satellite to detect spliced regions. We compare our approach to well-known splicing detection methods (i.e., Noiseprint) and segmentation techniques (i.e., U-Net and Nested Attention U-Net). We conducted our experiments on two large datasets: one dataset contains images from Sentinel 2 satellites and the other one contains images from Worldview 3 satellite. Our experiments show that our proposed fusion method performs well when compared to other techniques in localizing spliced areas using Jaccard Index and Dice Score as metrics on both datasets.  © 2022 SPIE.","Deep learning; Disasters; Image fusion; Large dataset; Remote sensing; Deep learning; Detection methods; Disaster analysis; Forensic; Image manipulation; Manipulation tools; Natural disasters; Satellite images; Splicing detections; Splicing localizations; Satellites","Deep Learning; Forensic; Fusion; Satellite Images","Conference paper","Final","","Scopus","2-s2.0-85135909634"
"Luo X.; Tong X.; Hu Z.","Luo, Xin (56316646000); Tong, Xiaohua (55500134600); Hu, Zhongwen (55630272400)","56316646000; 55500134600; 55630272400","Improving Satellite Image Fusion via Generative Adversarial Training","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9212572","6969","6982","13","10.1109/TGRS.2020.3025821","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111155134&doi=10.1109%2fTGRS.2020.3025821&partnerID=40&md5=52efcfa7eaddf11c1f7bf0d58f09c968","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications. © 1980-2012 IEEE.","Geometrical optics; Image fusion; Remote sensing; Satellites; Adversarial networks; Discriminative networks; Multiresolution images; Operational land imager; Remote sensing applications; Remote sensing images; Spatial informations; Spectral information; artificial neural network; optical method; satellite data; Image enhancement","Deep learning; generative adversarial networks (GANs); Landsat 8; remote sensing image fusion; residual dense blocks; Sentinel-2","Article","Final","","Scopus","2-s2.0-85111155134"
"Khan M.F.A.; Muhammad K.; Bashir S.; Din S.U.; Hanif M.","Khan, Muhammad Fawad Akbar (57223925516); Muhammad, Khan (56651946700); Bashir, Shahid (55607240300); Din, Shahab Ud (57223932082); Hanif, Muhammad (57541167500)","57223925516; 56651946700; 55607240300; 57223932082; 57541167500","Mapping allochemical limestone formations in Hazara, Pakistan using google cloud architecture: Application of machine-learning algorithms on multispectral data","2021","ISPRS International Journal of Geo-Information","10","2","58","","","","10.3390/ijgi10020058","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106469339&doi=10.3390%2fijgi10020058&partnerID=40&md5=2812701d7a7d834e30014967305c7382","Low-resolution Geological Survey of Pakistan (GSP) maps surrounding the region of interest show oolitic and fossiliferous limestone occurrences correspondingly in Samanasuk, Lockhart, and Margalla hill formations in the Hazara division, Pakistan. Machine-learning algorithms (MLAs) have been rarely applied to multispectral remote sensing data for differentiating between limestone formations formed due to different depositional environments, such as oolitic or fossiliferous. Unlike the previous studies that mostly report lithological classification of rock types having different chemical compositions by the MLAs, this paper aimed to investigate MLAs’ potential for mapping subclasses within the same lithology, i.e., limestone. Additionally, selecting appropriate data labels, training algorithms, hyperparameters, and remote sensing data sources were also investigated while applying these MLAs. In this paper, first, oolitic (Samanasuk), fossiliferous (Lockhart and Margalla) limestone-bearing formations along with the adjoining Hazara formation were mapped using random forest (RF), support vector machine (SVM), classification and regression tree (CART), and naïve Bayes (NB) MLAs. The RF algorithm reported the best accuracy of 83.28% and a Kappa coefficient of 0.78. To further improve the targeted allochemical limestone formation map, annotation labels were generated by the fusion of maps obtained from principal component analysis (PCA), decorrelation stretching (DS), X-means clustering applied to ASTER-L1T, Landsat-8, and Sentinel-2 datasets. These labels were used to train and validate SVM, CART, NB, and RF MLAs to obtain a binary classification map of limestone occurrences in the Hazara division, Pakistan using the Google Earth Engine (GEE) platform. The classification of Landsat-8 data by CART reported 99.63% accuracy, with a Kappa coefficient of 0.99, and was in good agreement with the field validation. This binary limestone map was further classified into oolitic (Samanasuk) and fossiliferous (Lockhart and Margalla) formations by all the four MLAs; in this case, RF surpassed all the other algorithms with an improved accuracy of 96.36%. This improvement can be attributed to better annotation, resulting in a binary limestone classification map, which formed a mask for improved classification of oolitic and fossiliferous limestone in the area. © 2021 by the authors.","","Geological mapping; Google earth engine; Image fusion; Machine learning; Multispectral remote sensing; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106469339"
"Verma A.; Kumar A.; Lal K.","Verma, Abhinav (57210809234); Kumar, Amit (9244865300); Lal, Kanhaiya (57541263500)","57210809234; 9244865300; 57541263500","Kharif crop characterization using combination of SAR and MSI Optical Sentinel Satellite datasets","2019","Journal of Earth System Science","128","8","230","","","","10.1007/s12040-019-1260-0","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071442100&doi=10.1007%2fs12040-019-1260-0&partnerID=40&md5=f195206b8978afa6c2d81f31fd0c7f1c","In the present study, the differences in the kharif crop reflectance at varied wavelength regions and temporal SAR backscatter (at VV and VH polarizations) during different crop stages were analyzed to classify crop types in parts of Ranchi district, East India using random forest classifier. The spectral signature of crops was generated during various growth stages using temporal Sentinel-2 MSI (optical) satellite images. The temporal backscatter profile that depends on the geometric and di-electric properties of crops were studied using Sentinel-1 SAR data. The spectral profile exhibited distinctive reflectance at the NIR (0.842 µm) and SWIR (1.610 µm) wavelength regions for paddy (Oryza sativa; ~0.25 at NIR, ~0.27 at SWIR), maize (Zea mays; ~0.24 at NIR, ~0.29 at SWIR) and finger millet (Eleusine coracana, ~0.26 NIR, ~0.31 at SWIR) during pre-sowing season (mid-June). Similar variations in crop’s reflectance at their different growth stages (vegetative to harvesting) were observed at various wavelength ranges. Further, the variations in the backscatter coefficient of different crops were observed at various growth stages depending upon the differences in sowing–harvesting periods, field conditions, geometry, and water presence in the crop field, etc. The Sentinel-1 SAR based study indicated difference in the backscatter of crops (i.e., ~−18.5 dB (VH) and ~−10 dB (VV) for paddy, ~−14 dB (VH) and ~−7.5 dB (VV) for maize, ~−14.5 dB and ~−8 dB (VV) for finger millet) during late-July (transplantation for paddy; early vegetative for maize and finger millet). These variations in the reflectance and backscatter values during various stages were used to deduce the best combination of the optical and SAR layers in order to classify each crop precisely. The GLCM texture analysis was performed on SAR for better classification of crop fields with higher accuracies. The SAR-MSI based kharif crop assessment (2017) indicated that the total cropped area under paddy, maize and finger millet was 24,544.55, 1468.28 and 632.48 ha, respectively. The result was validated with ground observations, which indicated an overall accuracy of 83.87% and kappa coefficient of 0.78. The high temporal, spatial spectral agility of Sentinel satellite are highly suitable for kharif crop monitoring. The study signifies the role of combined SAR–MSI technology for accurate mapping and monitoring of kharif crops. © 2019, Indian Academy of Sciences.","India; Jharkhand; Ranchi; Eleusine coracana; Oryza sativa; Zea mays; crop; data set; image classification; machine learning; monitoring; satellite data; Sentinel; synthetic aperture radar","Crop monitoring; crop spectral profile; random forest classification; SAR texture; SAR–MSI image fusion","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85071442100"
"Metrikaityte G.; Visockiene J.S.; Papsys K.","Metrikaityte, Guste (57798962200); Visockiene, Jurate Suziedelyte (56548798600); Papsys, Kestutis (55383221400)","57798962200; 56548798600; 55383221400","Digital Mapping of Land Cover Changes Using the Fusion of SAR and MSI Satellite Data","2022","Land","11","7","1023","","","","10.3390/land11071023","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134038608&doi=10.3390%2fland11071023&partnerID=40&md5=325ae20ec3924f318c441f6d108ef2f8","The aim of this article is to choose the most appropriate method for identifying and managing land cover changes over time. These processes intensify due to human activities such as agriculture, urbanisation and deforestation. The study is based in the remote sensing field. The authors used four different methods of satellite image segmentation with different data: Synthetic Aperture Radar (SAR) Sentinel-1 data, Multispectral Imagery (MSI) Sentinel-2 images and a fusion of these data. The images were preprocessed under segmentation by special algorithms and the European Space Agency Sentinel Application Platform (ESA SNAP) toolbox. The analysis was performed in the western part of Lithuania, which is characterised by diverse land use. The techniques applied during the study were: the coherence of two SAR images; the method when SAR and MSI images are segmented separately and the results of segmentation are fused; the method when SAR and MSI data are fused before land cover segmentation; and an upgraded method of SAR and MSI data fusion by adding additional formulas and index images. The 2018 and 2019 results obtained for SAR image segmentation differ from the MSI segmentation results. Urban areas are poorly identified because of the similarity of spectre signatures, where urban areas overlap with classes such as nonvegetation and/or sandy territories. Therefore, it is necessary to include the field surveys in the calculations in order to improve the reliability and accuracy of the results. The authors are of the opinion that the calculation of the additional indexes may help to enhance the visibility of vegetation and urban area classes. These indexes, calculated based on two or more different bands of multispectral images, would help to improve the accuracy of the segmentation results. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","coherence; image fusion; land cover changes; LULC; MSI RGB; SAR; segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134038608"
"Kremezi M.; Kristollari V.; Karathanassi V.; Topouzelis K.; Kolokoussis P.; Taggio N.; Aiello A.; Ceriola G.; Barbone E.; Corradi P.","Kremezi, Maria (57210723775); Kristollari, Viktoria (57211816997); Karathanassi, Vassilia (6602927210); Topouzelis, Konstantinos (56460431800); Kolokoussis, Pol (54405865200); Taggio, Nicolò (57207875434); Aiello, Antonello (54398206800); Ceriola, Giulio (6507203049); Barbone, Enrico (24780662800); Corradi, Paolo (25648815100)","57210723775; 57211816997; 6602927210; 56460431800; 54405865200; 57207875434; 54398206800; 6507203049; 24780662800; 25648815100","Increasing the Sentinel-2 potential for marine plastic litter monitoring through image fusion techniques","2022","Marine Pollution Bulletin","182","","113974","","","","10.1016/j.marpolbul.2022.113974","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135137999&doi=10.1016%2fj.marpolbul.2022.113974&partnerID=40&md5=f2a1dd611eb4aa6e3950dfd53245d9e2","Sentinel-2 (S2) images have been used in several projects to detect large accumulations of marine litter and plastic targets. Their limited spatial resolution though hinders the detection of relatively small floating accumulations of marine debris. Thus, this study aims at overcoming this limit through the exploration of fusion with very high-resolution WorldView-2/3 (WV-2/3) images. Various state-of-the-art approaches (component substitution, spectral unmixing, deep learning) were applied on data collected in synchronized acquisitions of plastic targets of various sizes and materials in seawater. The fused images were evaluated for spectral and spatial distortions, as well as their ability to spectrally discriminate plastics from water. Several WV-2/3 band combinations were investigated and five litter indexes were applied. Results showed that: a) the VNIR combination is the optimal one, b) the smallest observable plastic target is 0.6 × 0.6 m2 and c) SWIR bands are important for marine litter detection. © 2022 Elsevier Ltd","Environmental Monitoring; Plastics; Seawater; Waste Products; Deep learning; Marine pollution; Spectrum analysis; sea water; plastic; Controlled experiment; Image fusion techniques; Marine debris; Marine litter; Marine plastics; Plastic litter detection; Plastic targets; Satellite data; Spatial resolution; Worldview-2; detection method; experiment; image processing; marine pollution; plastic waste; satellite data; Sentinel; spatial resolution; spectral analysis; WorldView; Article; deep learning; environmental factor; environmental impact; environmental management; environmental monitoring; image analysis; information processing; litter size; marine environment; material state; plastic waste; plasticity; satellite imagery; sea pollution; spatial analysis; environmental monitoring; procedures; waste; Image fusion","Controlled experiments; Image fusion; Marine pollution; Plastic litter detection; Satellite data; Spectral analysis","Article","Final","","Scopus","2-s2.0-85135137999"
"Jiang J.; Zhang Q.; Yao X.; Tian Y.; Zhu Y.; Cao W.; Cheng T.","Jiang, Jiale (56289787100); Zhang, Qiaofeng (57212657957); Yao, Xia (14022139100); Tian, Yongchao (9237057200); Zhu, Yan (8921604000); Cao, Weixing (55489902600); Cheng, Tao (57216739413)","56289787100; 57212657957; 14022139100; 9237057200; 8921604000; 55489902600; 57216739413","HISTIF: A New Spatiotemporal Image Fusion Method for High-Resolution Monitoring of Crops at the Subfield Level","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9165877","4607","4626","19","10.1109/JSTARS.2020.3016135","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090764205&doi=10.1109%2fJSTARS.2020.3016135&partnerID=40&md5=fc324bdc2fa912c9c320fe88ffa69e71","Satellite-based time-series crop monitoring at the subfield level is essential to the efficient implementation of precision crop management. Existing spatiotemporal image fusion techniques can be helpful, but they were often proposed to generate medium-resolution images. This study proposed a high-resolution spatiotemporal image fusion method (HISTIF) consisting of filtering for cross-scale spatial matching (FCSM) and multiplicative modulation of temporal change (MMTC). In FCSM, we considered both point spread function effect and geo-registration errors between fine and coarse resolution images. Subsequently, MMTC used pixel-based multiplicative factors to estimate the temporal change between reference and prediction dates without image classification. The performance of HISTIF was evaluated using both simulated and real datasets with one from real Gaofen-1 (GF-1) and simulated Landsat-like/Sentinel-like images, and the other from real GF-1 and real Landsat/Sentinel-2 data on two sites. HISTIF was compared with the existing methods spatial and temporal adaptive reflectance fusion model (STARFM), FSDAF, and Fit-FC. The results demonstrated that HISTIF produced substantial reduction in the fusion error from cross-scale spatial mismatch and accurate reconstruction in spatial details within fields, regardless of simulated or real data. The images predicted by STARFM exhibited pronounced blocky artifacts. While the images predicted by HISTIF and Fit-FC both showed clear within-field variability patterns, HISTIF was able to reduce the spectral distortion more significantly than Fit-FC. Furthermore, HISTIF exhibited the most stable performance across sensors. The findings suggest that HISTIF could be beneficial for the frequent and detailed monitoring of crop growth at the subfield level. © 2008-2012 IEEE.","Crops; Modulation; Optical transfer function; Efficient implementation; High-resolution monitoring; Multiplicative factors; Spatiotemporal images; Spectral distortions; Stable performance; Substantial reduction; Temporal adaptive; biomonitoring; crop plant; heterogeneity; image classification; image resolution; Landsat; pixel; spatiotemporal analysis; Image fusion","Crops; heterogeneity; image fusion; spatiotemporal fusion; subfield monitoring","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090764205"
"Scheffler D.; Frantz D.","Scheffler, Daniel (57188984457); Frantz, David (56428816500)","57188984457; 56428816500","Improved burn severity estimation by using Land Surface Phenology metrics and red edge information estimated from Landsat","2022","International Journal of Applied Earth Observation and Geoinformation","115","","103126","","","","10.1016/j.jag.2022.103126","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142522786&doi=10.1016%2fj.jag.2022.103126&partnerID=40&md5=adc26bbec733947f4b440aa358d3d194","Global wildfire activities are expected to increase substantially in the near future. Existing techniques for spaceborne burn severity estimation often rely on bi-temporal spectral indices, which are related to in-situ burn severity data. However, due to cloud coverage and limited revisit frequency, in combination with the date of field surveys, it is a challenge to find suitable and phenologically comparable pre- and -post-fire images. To overcome these issues and to improve the accuracy of burn severity estimations by incorporating ecologically relevant spectral information, we investigated the capability of using Land Surface Phenology (LSP) metrics and incorporating red edge spectral information. We examined the well-researched Jasper fire (September 2000, Black Hills, USA) with a dense time series of Landsat-5 and -7 data. We generated synthesized red edge spectral bands through a recently proposed spectral harmonization technique and computed several bi-temporal vegetation indices. Additionally, we derived various bi-annual LSP metrics from the same indices. We used linear regression between composite burn index (CBI) ground truth data and the various indices to measure the performance of each approach, and intercompared estimated burn severity maps. We found added value of both incorporating red edge spectral information into bi-temporal indices and into LSP metrics. Among the indices, NDVI and NDVIre1n performed best, with the latter being the overall winner. This was observed for both the bi-temporal indices and the bi-annual LSP metrics, wherein best estimation performance was found with Value of Peak of Season and Value of Green Mean metrics. Although the correlation between CBI point measurements and bi-temporal index data is similar to the LSP approach, the LSP-based burn severity maps show more robustness with regard to clouds and cloud shadows, altitude gradients and pre-processing uncertainty. The results are not only relevant for sensors with native red edge bands like Sentinel-2 but also suggest that back-casting the red edge spectral information to the Landsat archive combined with an LSP based estimation approach may improve existing burn severity maps, especially in more frequently clouded regions. © 2022","Black Hills; United States; burning; estimation method; Landsat; NDVI; phenology; regression analysis; satellite imagery; spectral analysis","Burn severity; Land Surface Phenology; Red edge; Satellite image fusion; Spectral harmonization","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85142522786"
"Khosravi V.; Gholizadeh A.; Saberioon M.","Khosravi, Vahid (57202034112); Gholizadeh, Asa (35322496100); Saberioon, Mohammadmehdi (55795699000)","57202034112; 35322496100; 55795699000","Soil toxic elements determination using integration of Sentinel-2 and Landsat-8 images: Effect of fusion techniques on model performance","2022","Environmental Pollution","310","","119828","","","","10.1016/j.envpol.2022.119828","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135829086&doi=10.1016%2fj.envpol.2022.119828&partnerID=40&md5=8181ac08fc885834fd3e41d04980c564","Finding an appropriate satellite image as simultaneous as possible with the sampling time campaigns is challenging. Fusion can be considered as a method of integrating images and obtaining more pixels with higher spatial, spectral and temporal resolutions. This paper investigated the impact of Landsat 8-OLI and Sentinel-2A data fusion on prediction of several toxic elements at a mine waste dump. The 30 m spatial resolution Landsat 8-OLI bands were fused with the 10 m Sentinel-2A bands using various fusion techniques namely hue-saturation-value (HSV), Brovey, principal component analysis (PCA), Gram-Schmidt (GS), wavelet, and area-to-point regression kriging (ATPRK). ATPRK was the best method preserving both spectral and spatial features of Landsat 8-OLI and Sentinel-2A after fusion. Furthermore, the partial least squares regression (PLSR) model developed on genetic algorithm (GA)-selected laboratory visible-near infrared-shortwave infrared (VNIR–SWIR) spectra yielded more accurate prediction results compared to the PLSR model calibrated on the entire spectra. It was hence, applied to both individual sensors and their ATPRK-fused image. In case of the individual sensors, except for As, Sentinel-2A provided more robust prediction models than Landsat 8-OLI. However, the best performances were obtained using the fused images, highlighting the potential of data fusion to enhance the toxic elements’ prediction models. © 2022 Elsevier Ltd","Least-Squares Analysis; Principal Component Analysis; Soil; Data integration; Forecasting; Genetic algorithms; Image enhancement; Image fusion; Infrared devices; Infrared radiation; Least squares approximations; Principal component analysis; arsenic; chromium; lead; zinc; Earth observations; Fused images; Fusion techniques; LANDSAT; Partial least squares regression models; Prediction modelling; Regression-kriging; Satellite images; Soil contamination; Toxic elements; data processing; genetic algorithm; Landsat; numerical model; performance assessment; prediction; satellite imagery; Sentinel; soil pollution; area to point regression kriging; Article; Brovey method; controlled study; data accuracy; data processing; genetic algorithm; Gram Schmidt method; hue saturation value; image processing; infrared spectroscopy; mine waste; partial least squares regression; prediction; principal component analysis; satellite imagery; soil analysis; soil pollution; statistical analysis; visible near infrared shortwave infrared spectroscopy; wavelet analysis; least square analysis; soil; Landsat","Data fusion; Earth observation; Genetic algorithm; Satellite image; Soil contamination","Article","Final","","Scopus","2-s2.0-85135829086"
"Kaplan G.; Avdan U.","Kaplan, Gordana (57196402161); Avdan, Ugur (8356726300)","57196402161; 8356726300","Sentinel-1 and Sentinel-2 data fusion for wetlands mapping: Balikdami, Turkey","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","3","","729","734","5","10.5194/isprs-archives-XLII-3-729-2018","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046942525&doi=10.5194%2fisprs-archives-XLII-3-729-2018&partnerID=40&md5=ddd1d0f647761862600bb2d359bf7572","Wetlands provide a number of environmental and socio-economic benefits such as their ability to store floodwaters and improve water quality, providing habitats for wildlife and supporting biodiversity, as well as aesthetic values. Remote sensing technology has proven to be a useful and frequent application in monitoring and mapping wetlands. Combining optical and microwave satellite data can help with mapping and monitoring the biophysical characteristics of wetlands and wetlands' vegetation. Also, fusing radar and optical remote sensing data can increase the wetland classification accuracy. In this paper, data from the fine spatial resolution optical satellite, Sentinel-2 and the Synthetic Aperture Radar Satellite, Sentinel-1, were fused for mapping wetlands. Both Sentinel-1 and Sentinel-2 images were pre-processed. After the pre-processing, vegetation indices were calculated using the Sentinel-2 bands and the results were included in the fusion data set. For the classification of the fused data, three different classification approaches were used and compared. The results showed significant improvement in the wetland classification using both multispectral and microwave data. Also, the presence of the red edge bands and the vegetation indices used in the data set showed significant improvement in the discrimination between wetlands and other vegetated areas. The statistical results of the fusion of the optical and radar data showed high wetland mapping accuracy, showing an overall classification accuracy of approximately 90% in the object-based classification method. For future research, we recommend multi-temporal image use, terrain data collection, as well as a comparison of the used method with the traditional image fusion techniques. © Authors 2018.","Biodiversity; Image fusion; Mapping; Remote sensing; Satellites; Space-based radar; Synthetic aperture radar; Vegetation; Water quality; Wetlands; Biophysical characteristics; Microwave satellite data; Object-based classifications; Optical remote sensing data; Remote sensing technology; Sentinel-1; Sentinel-2; Socio-economic benefits; Classification (of information)","Image fusion; Object-based classification; Sentinel-1; Sentinel-2; Wetlands","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85046942525"
"Htitiou A.; Boudhar A.; Benabdelouahab T.","Htitiou, Abdelaziz (57212145169); Boudhar, Abdelghani (35090979500); Benabdelouahab, Tarik (56766050800)","57212145169; 35090979500; 56766050800","Deep Learning-Based Spatiotemporal Fusion Approach for Producing High-Resolution NDVI Time-Series Datasets; [  Approche de fusion spatiotemporelle basée sur l’apprentissage profond pour produire des séries temporelles de NDVI à haute résolution]","2021","Canadian Journal of Remote Sensing","47","2","","182","197","15","10.1080/07038992.2020.1865141","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100805494&doi=10.1080%2f07038992.2020.1865141&partnerID=40&md5=e11a28e6b81aa194febd968a1bec2b11","The availability of concurrently high spatiotemporal resolution remote sensing data is highly desirable as they represent a key element for effective monitoring in various environmental applications. However, due to the tradeoff between the spatial resolution and acquisition frequency of current satellites, such data are still lacking. Many studies have been undertaken trying to overcome these problems; however, a couple of long-standing limitations remain, including accommodating abrupt temporal changes, dealing with complex and heterogeneous landscapes, and integrating other satellite datasets as well. Accordingly, this paper proposes a deep learning spatiotemporal data fusion approach based on Very Deep Super-Resolution (VDSR) to fuse the NDVI retrievals from Sentinel-2 and Landsat 8 images. The performances of VDSR are analyzed in comparison with those of two other classical methods, the enhanced spatial and temporal adaptive reflectance fusion model (ESTARFM) and the flexible spatiotemporal data fusion (FSDAF) method. The results obtained indicate that VDSR outperforms other data fusion algorithms as it generated the least blurred images and the most accurate predictions of synthetic NDVI values, particularly in areas with heterogeneous landscapes and abrupt land-cover changes. The proposed algorithm has broad prospects to improve near-real-time agricultural monitoring purposes and derivation of crop status conditions in the field-scale. ©, Copyright © CASI.","Agricultural robots; Image fusion; Remote sensing; Agricultural monitoring; Data fusion algorithm; Environmental applications; Frequency of currents; Heterogeneous landscapes; Spatio-temporal data; Spatio-temporal fusions; Spatio-temporal resolution; Deep learning","","Article","Final","","Scopus","2-s2.0-85100805494"
"Schmitt A.; Wendleder A.; Kleynmans R.; Hell M.; Roth A.; Hinz S.","Schmitt, Andreas (55576223700); Wendleder, Anna (36012163600); Kleynmans, Rüdiger (57215896519); Hell, Maximilian (57215897252); Roth, Achim (56256016700); Hinz, Stefan (7004082496)","55576223700; 36012163600; 57215896519; 57215897252; 56256016700; 7004082496","Multi-source and multi-temporal image fusion on hypercomplex bases","2020","Remote Sensing","12","6","943","","","","10.3390/rs12060943","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082301820&doi=10.3390%2frs12060943&partnerID=40&md5=620c5ea592e15304bf93ca6248807e97","This article spanned a new, consistent framework for production, archiving, and provision of analysis ready data (ARD) from multi-source and multi-temporal satellite acquisitions and an subsequent image fusion. The core of the image fusion was an orthogonal transform of the reflectance channels from optical sensors on hypercomplex bases delivered in Kennaugh-like elements, which are well-known from polarimetric radar. In this way, SAR and Optics could be fused to one image data set sharing the characteristics of both: the sharpness of Optics and the texture of SAR. The special properties of Kennaugh elements regarding their scaling-linear, logarithmic, normalized-applied likewise to the newelements and guaranteed their robustness towards noise, radiometric sub-sampling, and therewith data compression. This study combined Sentinel-1 and Sentinel-2 on an Octonion basis as well as Sentinel-2 and ALOS-PALSAR-2 on a Sedenion basis. The validation using signatures of typical land cover classes showed that the efficient archiving in 4 bit images still guaranteed an accuracy over 90% in the class assignment. Due to the stability of the resulting class signatures, the fuzziness to be caught by Machine Learning Algorithms was minimized at the same time. Thus, this methodology was predestined to act as new standard for ARD remote sensing data with an subsequent image fusion processed in so-called data cubes. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Data Sharing; Image analysis; Learning algorithms; Machine learning; Radar imaging; Remote sensing; Synthetic aperture radar; Textures; Time series; Time series analysis; Analysis ready data; Change detection; Data cube; Efficient archiving; Hypercomplex bases; Kennaugh framework; Quaternion; SAR sharpening; Image fusion","Analysis ready data; Change detection; Data cube; Efficient archiving; Hypercomplex bases; Image fusion; Kennaugh framework; Quaternion; SAR sharpening; Time series","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85082301820"
"Beltrão N.; Teodoro A.","Beltrão, Norma (56656398100); Teodoro, Ana (55889370900)","56656398100; 55889370900","Evaluating the potential of Sentinel-2 MSI and Landsat-8 OLI data fusion for land cover mapping in Brazilian Amazon","2018","Proceedings of SPIE - The International Society for Optical Engineering","10783","","107830H","","","","10.1117/12.2325576","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056488081&doi=10.1117%2f12.2325576&partnerID=40&md5=590343fed9d09e3fb89dca4be9ac4476","Capturing spatial and temporal environmental dynamics in heterogeneous tropical landscapes through remote sensing data has become an important challenge in environmental monitoring studies due to cloud contamination. In Brazilian Amazon, there is a great need of data able to provide information for a wide range of decision makers. Alongside this, a wide variety of free remotely sensed data products have been made available. The combination of information from more than one sensor could maximise the number of cloud-free images as well as their temporal and spatial resolution. Based on this framework, this study investigates the potential and quality of a set of fused images obtained from the Sentinel-2 Multispectral Imager Instrument (MSI) and Landsat OLI satellites sensors over the region of Brazilian Amazon, comparing their performance according to different land cover/land use areas. In this context, the objectives of this study were: (1) assess qualitatively the two main group of image fusion approaches, namely component substitution (CS) and multiresolution analysis (MRA), as well as their suitability for the fusion of Sentinel-2 MSI and Landsat-8 OLI image pair from selected areas in Brazilian Amazon; (2) compare three different image fusion methods for measuring efficacy and performance to determine the best spatio-Temporal information: The Intensity Hue Saturation (IHS) method, the Brovey transformation (BT) and the Gram-Schmidt (GS) method; (3) assess quantitatively the methods employed through a set of fusion quality metrics in order to identify the more accurate results based on different reference images. © SPIE. Downloading of the abstract is permitted for personal use only.","Agriculture; Decision making; Ecosystems; Hydrology; Image fusion; Amazon; Component substitution; Environmental dynamics; Environmental Monitoring; Intensity hue saturations; LANDSAT; Sentinel; Spatiotemporal information; Remote sensing","Amazon.; Landsat; Sentinel","Conference paper","Final","","Scopus","2-s2.0-85056488081"
"Pereira O.J.R.; Melfi A.J.; Montes C.R.","Pereira, Osvaldo José Ribeiro (36628953000); Melfi, Adolpho José (7003918479); Montes, Célia Regina (7005310646)","36628953000; 7003918479; 7005310646","Image fusion of Sentinel-2 and CBERS-4 satellites for mapping soil cover in the Wetlands of Pantanal","2017","International Journal of Image and Data Fusion","8","2","","148","172","24","10.1080/19479832.2016.1261946","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008682047&doi=10.1080%2f19479832.2016.1261946&partnerID=40&md5=72e4a2c326f13b2cce0018c52a8b299a","Advances in remote-sensing technology and the release of new sensor systems have been providing a wide range of optical and radar satellite images. The availability of such images gives new options for mapping relatively remote and sparsely settled territories. Given this, the main goal of this research was to perform a quantitative assessment of the quality of a set of fused images obtained by China–Brazil Earth Resources Satellite (CBERS-4) and Sentinel-2 satellites over the region of the Brazilian Pantanal, considering a zonal quantitative approach. Through the applied methods, we combined the spectral resolution of Sentinel-2 images with the spatial resolution of the CBERS-4 panchromatic band. The quantitative evaluation of the spectral and spatial quality of the fused products based on a regionalised approach was essential in order to understand the spatial variability of the fusion quality, according to each fusion method. Based on the obtained results, we observed that CBERS-4 panchromatic band could be satisfactorily applied for refining the spatial resolution of Sentinel-2 multispectral (MS) images according to À trous wavelet transform and downscaling cokriging image fusion methods, allowing the generation of MS fused images with a 5-m spatial resolution. The resulting fused images were used to map the spatial distribution of saline and crystalline lakes at high spatial resolution and with unprecedented spectral and spatial accuracies, according to a semiautomatic classification approach. © 2017 Informa UK Limited, trading as Taylor & Francis Group.","Pantanal; Image compression; Image enhancement; Image resolution; Mapping; Remote sensing; Satellite imagery; Satellites; Soils; Wavelet transforms; ATWT; Classification approach; Earth resources satellites; High spatial resolution; Quantitative approach; Quantitative assessments; Quantitative evaluation; Remote sensing technology; image analysis; mapping; quantitative analysis; remote sensing; satellite imagery; Sentinel; soil cover; wetland; Image fusion","ATWT; DCK; image enhancement; Image fusion","Article","Final","","Scopus","2-s2.0-85008682047"
"Chilkuri V.S.; Bharathi D.; Karthi R.","Chilkuri, Vishal Siddartha (58068855100); Bharathi, D. (56677613700); Karthi, R. (23985064600)","58068855100; 56677613700; 23985064600","Blending Ultra Spectral Images of Multi-Source Remote Sensors","2022","International Conference on Electrical, Computer, Communications and Mechatronics Engineering, ICECCME 2022","","","","","","","10.1109/ICECCME55909.2022.9988603","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146440057&doi=10.1109%2fICECCME55909.2022.9988603&partnerID=40&md5=24bc641f5ab7b17ccc4a875451a3c876","Images and procedures from remote sensing are effective tools for studying the earth's surface. Data quality is essential for improving remote sensing applications and producing crisp, noise-free images. Due to the different gathering methods, obtaining a free set of data is quite challenging in most cases. So, picture or information fusion is crucial in far-flung sensing applications. Spatiotemporal fusion (STF) is a method for fusing pix with the proper temporal and spatial decision by integrating (temporally dense) coarse-resolution pictures with (temporally sparse) fine-resolution pictures. This paper makes a specialty of enforcing spatiotemporal fusion of multi-supply remote sensing pictures. In this paper, STF of multi-source remote sensing images, specifically Landsat and Sentinel sensors images is performed using the ESTARFM fusion method. In total 2 experiments were conducted with Landsat 7, Landsat 8, and Sentinel 2 data. In experiment 1 Landsat 7, and Sentinel 2 images are considered fine and coarse resolution images respectively, and in experiment 2 Landsat 8, and Sentinel 2 as fine and coarse resolution images. The metrics suggest that by applying the STF method, the similarity between the fused image and the original image at the prediction time of experiment 2 is more when compared to the corresponding image results of experiment 1. © 2022 IEEE.","Image enhancement; Image fusion; Remote sensing; Coarser resolution; ESTARFM; Fine resolution; Fusion methods; LANDSAT; LandSat 7; Multi-Sources; Remote-sensing; Resolution images; Spatio-temporal fusions; Landsat","Coarse resolution; ESTARFM; Fine resolution; Spatiotemporal Fusion","Conference paper","Final","","Scopus","2-s2.0-85146440057"
"Gialampoukidis I.; Moumtzidou A.; Bakratsas M.; Vrochidis S.; Kompatsiaris I.","Gialampoukidis, Ilias (55863740200); Moumtzidou, Anastasia (25924031500); Bakratsas, Marios (57191885398); Vrochidis, Stefanos (23052810300); Kompatsiaris, Ioannis (7004756014)","55863740200; 25924031500; 57191885398; 23052810300; 7004756014","A Multimodal Tensor-Based Late Fusion Approach for Satellite Image Search in Sentinel 2 Images","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12573 LNCS","","","294","306","12","10.1007/978-3-030-67835-7_25","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101636096&doi=10.1007%2f978-3-030-67835-7_25&partnerID=40&md5=9d06b3fc79f34dad159feac3ddf8e571","Earth Observation (EO) Big Data Collections are acquired at large volumes and variety, due to their high heterogeneous nature. The multimodal character of EO Big Data requires effective combination of multiple modalities for similarity search. We propose a late fusion mechanism of multiple rankings to combine the results from several uni-modal searches in Sentinel 2 image collections. We fist create a K-order tensor from the results of separate searches by visual features, concepts, spatial and temporal information. Visual concepts and features are based on a vector representation from Deep Convolutional Neural Networks. 2D-surfaces of the K-order tensor initially provide candidate retrieved results per ranking position and are merged to obtain the final list of retrieved results. Satellite image patches are used as queries in order to retrieve the most relevant image patches in Sentinel 2 images. Quantitative and qualitative results show that the proposed method outperforms search by a single modality and other late fusion methods. © 2021, Springer Nature Switzerland AG.","Big data; Convolutional neural networks; Deep neural networks; Tensors; Data collection; Earth observations; Image collections; Multiple modalities; Satellite images; Similarity search; Temporal information; Vector representations; Image fusion","Late fusion; Multimodal search; Sentinel 2 images","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101636096"
"Gašparović M.; Jogun T.","Gašparović, Mateo (36987936900); Jogun, Tomislav (57191962007)","36987936900; 57191962007","The effect of fusing Sentinel-2 bands on land-cover classification","2018","International Journal of Remote Sensing","39","3","","822","841","19","10.1080/01431161.2017.1392640","80","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036639188&doi=10.1080%2f01431161.2017.1392640&partnerID=40&md5=37a3e194612467be67ae22c7d5896b98","The Sentinel-2 satellite currently provides freely available multispectral bands at relatively high spatial resolution but does not acquire the panchromatic band. To improve the resolution of 20 m bands to 10 m, existing pansharpening methods (Brovey transform [BT], intensity–hue–saturation [IHS], principal component analysis [PCA], the variational method [P + XS], and the wavelet method) required adjustment, which was achieved using higher resolution multispectral bands in the role of a panchromatic band to fuse bands at a lower spatial resolution. After preprocessing, six bands at lower resolution were divided into two groups because some image fusion methods (e.g. BT, IHS) are limited to a maximum of three input bands of a lower resolution at a time. With respect to the spectral range, the higher resolution band for the first group was synthesized from bands 4 and 8, and band 8 was selected for the second group. Given that one of the main remote sensing applications is land-cover classification, the classification accuracy of the fusion methods was assessed as well as the comparison with reference bands and pixels. The supervised classification methods were Maximum Likelihood Classifier, artificial neural networks, and object-based image analysis. The classification scheme contained five classes: water, built-up, bare soil, low vegetation, and forest. The results showed that most of the fusion methods, particularly P + XS and PCA, improved the overall classification accuracy, especially for the classes of forest, low vegetation, and bare soil and in the detection of coastlines. The least satisfying results were obtained from the wavelet method. © 2017 Informa UK Limited, trading as Taylor & Francis Group. All rights reserved.","Forestry; Image fusion; Image resolution; Maximum likelihood estimation; Neural networks; Remote sensing; Vegetation; Classification accuracy; Classification scheme; High spatial resolution; Land cover classification; Maximum likelihood classifiers; Object based image analysis; Remote sensing applications; Supervised classification; image processing; land cover; multispectral image; Sentinel; spatial resolution; supervised classification; wavelet analysis; Principal component analysis","","Article","Final","","Scopus","2-s2.0-85036639188"
"Lomelí-Huerta R.; Avila-George H.; Rivera-Caicedo J.P.; De-La-Torre M.","Lomelí-Huerta, Roberto (57678512600); Avila-George, Himer (36607394000); Rivera-Caicedo, Juan Pablo (54684821900); De-La-Torre, Miguel (22333630400)","57678512600; 36607394000; 54684821900; 22333630400","WATER POLLUTION DETECTION IN ACAPULCO COASTS USING MERGED DATA FROM THE SENTINEL-2 AND SENTINEL-3 SATELLITES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","1518","1521","3","10.1109/IGARSS47720.2021.9553929","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129902247&doi=10.1109%2fIGARSS47720.2021.9553929&partnerID=40&md5=f0fb826b3db5da16c03577752facbfc2","Acapulco coasts are occasionally contaminated by illegal discharges originated by temporary or permanent floods that disembogue to the pacific ocean. Plumes formed by contaminated water running through the ocean can be distinguished in satellite imagery, and their reflectance is related to the polluting elements. Although some spacial agencies provide data from diverse multispectral sensors, application-specific requirements are fulfilled by merging heterogeneous imagery (differences in spatial, temporal, and spectral resolutions). This paper proposes a continuous monitoring strategy to detect pollution in water discharges by combining data from Sentinel-2 and Sentinel-3 platforms. First, the region of interest to be monitored is detected using the bands with high spatial resolution. Then, distance-based supervised machine learning is employed to detect pixel-wise pollution in water. Finally, the historic detections over time are presented to detect recurrent discharges. ©2021 IEEE","Image fusion; Image segmentation; Monitoring; Oil spills; Pollution detection; Remote sensing; Satellite imagery; Supervised learning; Contaminated water; Illegal discharges; Monitoring system; Pacific ocean; Remote-sensing; Satellite image fusion; Satellite images; Sentinel; Water pollution detections; Water running; Water pollution","contaminated water; monitoring system; remote sensing; satellite image fusion; Sentinel","Conference paper","Final","","Scopus","2-s2.0-85129902247"
"Alebele Y.; Zhang X.; Wang W.; Yang G.; Yao X.; Zheng H.; Zhu Y.; Cao W.; Cheng T.","Alebele, Yeshanbele (57218677047); Zhang, Xue (57218681995); Wang, Wenhui (57212659763); Yang, Gaoxiang (57218681386); Yao, Xia (14022139100); Zheng, Hengbiao (57191078957); Zhu, Yan (8921604000); Cao, Weixing (55489902600); Cheng, Tao (57216739413)","57218677047; 57218681995; 57212659763; 57218681386; 14022139100; 57191078957; 8921604000; 55489902600; 57216739413","Estimation of canopy biomass components in paddy rice from combined optical and SAR data using multi-target gaussian regressor stacking","2020","Remote Sensing","12","16","2564","","","","10.3390/BIOMEDICINES8080286","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090022185&doi=10.3390%2fBIOMEDICINES8080286&partnerID=40&md5=95e98e081ca8e977c10249405b07ac93","Crop biomass is a critical variable to make sound decisions about field crop monitoring activities (fertilizers and irrigation) and crop productivity forecasts. More importantly, crop biomass estimations by components are essential for crop growth monitoring as the yield formation of crops results from the accumulation and transportation of substances between dierent organs. Retrieval of crop biomass from synthetic aperture radar SAR or optical imagery is of paramount importance for in-season monitoring of crop growth. A combination of optical and SAR imagery can compensate for their limitations and has exhibited comparative advantages in biomass estimation. Notably, the joint estimations of biophysical parameters might be more accurate than that of an individual parameter. Previous studies have attempted to use satellite imagery to estimate aboveground biomass, but the estimation of biomass for individual organs remains a challenge. Multi-target Gaussian process regressor stacking (MGPRS), as a new machine learning method, can be suitably utilized to estimate biomass components jointly from satellite imagery data, as the model does not require a large amount of data for training and can be adjusted to the required degrees of relationship exhibited by the given data. Thus, the aim of this study was to estimate the biomass of individual organs by using MGPRS in conjunction with optical (Sentinel-2A) and SAR (Sentinel-1A) imagery. Two hybrid indices, SAR and optical multiplication vegetation index (SOMVI) and SAR and optical dierence vegetation index (SODVI), have been constructed to examine their estimation performance. The hybrid vegetation indices were used as input for the MGPRS and single-target Gaussian process regression (SGPR). The accuracy of the estimation methods was analyzed by in situ measurements of aboveground biomass (AGB) and organ biomass conducted in 2018 and 2019 over the paddy rice fields of Xinghua in Jiangsu Province, China. The results showed that the combined indices (SOMVI and SODVI) performed better than those derived from either the optical or SAR data only. The best predictive accuracy was achieved by the MGPRS using SODVI as input (r2 = 0.84, RMSE = 0.4 kg/m2 for stem biomass; r2 = 0.87, RMSE = 0.16 kg/m2 for AGB). This was higher than using SOMVI as input for the MGPRS (r2 = 0.71, RMSE = 1.12 kg/m2 for stem biomass; r2 = 0.71, RMSE = 0.56 kg/m2 for AGB) or SGPR (r2 = 0.63, RMSE = 1.08 kg/m2 for stem biomass; r2 = 0.67, RMSE = 1.08 kg/m2 for AGB). Relatively, higher accuracy for leaf biomass was achieved using SOMVI (r2 = 0.83) than using SODVI (r2 = 0.73) as input for MGPRS. Our results demonstrate that the combined indices are eective by integrating SAR and optical imagery and MGPRS outperformed SGPR with the same input variable for estimating rice crop biomass. The presented workflow will improve the estimation of crops biomass components from satellite data for eective crop growth monitoring. © 2020 by the authors.","Biomass; Crops; Ecology; Forestry; Gaussian distribution; Gaussian noise (electronic); Learning systems; Satellite imagery; Space-based radar; Synthetic aperture radar; Vegetation; Above ground biomass; Biophysical parameters; Comparative advantage; Estimation performance; Gaussian process regression; In-situ measurement; Machine learning methods; Satellite imagery data; Radar imaging","Gaussian process; Hybrid indices; Image fusion; Sentinel-1; Sentinel-2; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090022185"
"Tang Y.; Wang Q.; Tong X.; Atkinson P.M.","Tang, Yijie (57211522443); Wang, Qunming (55649569623); Tong, Xiaohua (55500134600); Atkinson, Peter M. (7201906181)","57211522443; 55649569623; 55500134600; 7201906181","Integrating spatio-temporal-spectral information for downscaling Sentinel-3 OLCI images","2021","ISPRS Journal of Photogrammetry and Remote Sensing","180","","","130","150","20","10.1016/j.isprsjprs.2021.08.012","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113299870&doi=10.1016%2fj.isprsjprs.2021.08.012&partnerID=40&md5=e316b36778bd02aebd34e0727b605908","Sentinel-3 is a newly launched satellite implemented by the European Space Agency (ESA) for global observation. The Ocean and Land Colour Imager (OLCI) sensor onboard Sentinel-3 provides 21 band images with a fine spectral resolution and is of great value for ocean, land and atmospheric monitoring. The two platforms (Sentinel-3A and -3B) can provide OLCI images at an almost daily temporal resolution. The coarse spatial resolution of the 21 band OLCI images (i.e., 300 m), however, limits greatly their utility for local, precise monitoring. Sentinel-2, another satellite provided by ESA, carries the Multispectral Imager (MSI) sensor which can supply much finer spatial resolution (e.g., 10 m and 20 m) images. This paper introduces a new fusion framework integrating spatio-temporal-spectral information for downscaling Sentinel-3 OLCI images, which has two parts. Based on bands with similar wavelengths (i.e., bands 2, 3, 4 and 8a for Sentinel-2 and bands Oa4, Oa6, Oa8 and Oa17 for Sentinel-3), the four Sentinel-3 bands are first downscaled to the spatial resolution of Sentinel-2 images by applying spatio-temporal fusion to Sentinel-2 MSI and Sentinel-3 OLCI images. Then, to take full advantage of all 21 available OLCI bands of the Sentinel-3 images, the extended image pair-based spatio-spectral fusion (EIPSSF) method is proposed in this paper to downscale the other 17 bands. EIPSSF is performed based on the new concept of the extended image pair (EIP) and by exploiting existing spatio-temporal fusion approaches. The framework consisting of spatio-temporal and spatio-spectral fusion is entirely general, which provides a practical solution for comprehensive downscaling of Sentinel-3 OLCI images for fine spatial, temporal and spectral resolution monitoring. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Image resolution; Remote sensing; Spectral resolution; Color imagers; Down-scaling; European Space Agency; Image pairs; Multispectral imagers; Sentinel-2; Sentinel-3; Spatial resolution; Spatio-temporal; Spectral information; downscaling; image resolution; integrated approach; satellite sensor; Sentinel; spatial resolution; spatiotemporal analysis; spectral resolution; Image fusion","Downscaling; Image fusion; Sentinel-2; Sentinel-3","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85113299870"
"Dodangeh P.; Ebadi H.; Kiani A.","Dodangeh, Parisa (57908118600); Ebadi, Hamid (23008534300); Kiani, Abbas (56861911900)","57908118600; 23008534300; 56861911900","Deep Learning Network for Flood Extent Mapping Based on the Integration of Sentinel 2 and MODIS Satellite Imagery","2021","Journal of Environmental Studies","47","2","","181","204","23","10.22059/JES.2021.325289.1008187","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138804205&doi=10.22059%2fJES.2021.325289.1008187&partnerID=40&md5=5f94f4fdab3fd8b82924709ae14a9b40","The occurrence of floods in metropolitan areas always causes a lot of damages to various infrastructures. Therefore, providing methods with the ability to accurately assess these damages in the shortest possible time is a necessity of crisis management. In this regard, various methods for classifying remote sensing images have been developed, which always face challenges in differentiating land uses. Another challenge in flood crisis management is the lack of access to satellite imagery with high temporal resolution while maintaining spatial resolution. The purpose of this study is to evaluate the damage caused by the flood crisis in Khuzestan province following the flood of 1398, which is based on integration of images of Sentinel 2 and MODIS to produce a time series with relatively good spatial and temporal resolution. In order to prepare maps, a patch-based hierarchical convolutional neural network has been designed, which solves the challenge of extracting deep features due to the relatively weak structure of images with a resolution of more than 10 meters. Finally, the area of damage to urban land cover and various agricultural lands has been estimated consecutively during the flood period. The results indicated that the proposed approach was properly faced with the challenge of speed and accuracy in preparing a flood destruction map, implementing on images at different times of the flood shows the generality. © 2021 Journal of Environmental Studies. All rights reserved.","","Convolutional neural network; Deep learning; Flood crisis management; Flood map; Image fusion","Article","Final","","Scopus","2-s2.0-85138804205"
"Yokoya N.; Ghamisi P.; Xia J.; Sukhanov S.; Heremans R.; Tankoyeu I.; Bechtel B.; Le Saux B.; Moser G.; Tuia D.","Yokoya, Naoto (36440631200); Ghamisi, Pedram (53663404300); Xia, Junshi (35099662000); Sukhanov, Sergey (57188859804); Heremans, Roel (24467877300); Tankoyeu, Ivan (54795956500); Bechtel, Benjamin (37088140000); Le Saux, Bertrand (6506307016); Moser, Gabriele (7101795745); Tuia, Devis (15766793800)","36440631200; 53663404300; 35099662000; 57188859804; 24467877300; 54795956500; 37088140000; 6506307016; 7101795745; 15766793800","Open Data for Global Multimodal Land Use Classification: Outcome of the 2017 IEEE GRSS Data Fusion Contest","2018","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","5","","1363","1377","14","10.1109/JSTARS.2018.2799698","88","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045645809&doi=10.1109%2fJSTARS.2018.2799698&partnerID=40&md5=97dc7a51a9b14985276caa3256ddad8d","In this paper, we present the scientific outcomes of the 2017 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2017 Contest was aimed at addressing the problem of local climate zones classification based on a multitemporal and multimodal dataset, including image (Landsat 8 and Sentinel-2) and vector data (from OpenStreetMap). The competition, based on separate geographical locations for the training and testing of the proposed solution, aimed at models that were accurate (assessed by accuracy metrics on an undisclosed reference for the test cities), general (assessed by spreading the test cities across the globe), and computationally feasible (assessed by having a test phase of limited time). The techniques proposed by the participants to the Contest spanned across a rather broad range of topics, and of mixed ideas and methodologies deriving from computer vision and machine learning but also deeply rooted in the specificities of remote sensing. In particular, rigorous atmospheric correction, the use of multidate images, and the use of ensemble methods fusing results obtained from different data sources/time instants made the difference. © 2008-2012 IEEE.","Classification (of information); Crowdsourcing; Deep learning; Image analysis; Land use; Neural networks; Remote sensing; Convolutional neural network; Ensemble learning; Multi resolutions; Multi-modal; Multisources; Openstreetmap; Random fields; accuracy assessment; artificial neural network; crowdsourcing; data processing; ensemble forecasting; image analysis; land classification; land use; Landsat; map; remote sensing; Image fusion","Convolutional neural networks (CNNs); crowdsourcing; deep learning (DL); ensemble learning; image analysis and data fusion (IADF); multimodal; multiresolution; multisource; OpenStreetMap (OSM); random fields","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85045645809"
"Palsson F.; Ulfarsson M.O.; Sveinsson J.R.","Palsson, Frosti (55052918200); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214)","55052918200; 6507677875; 7003642214","Model-Based Reduced-Rank Pansharpening","2020","IEEE Geoscience and Remote Sensing Letters","17","4","8778736","656","660","4","10.1109/LGRS.2019.2926681","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082885172&doi=10.1109%2fLGRS.2019.2926681&partnerID=40&md5=97390dbfbcd2d52a7667d7cfbac8258f","Observation of the Earth using satellites mounted with optical sensors is an important application of remote sensing. Owing to physical constraints, multispectral (MS) sensors acquire images of lower spatial resolution than a single-band panchromatic (PAN) sensor that acquires images of the same scene. Pansharpening fuses the MS and PAN images to obtain an MS image with the same spatial resolution as the PAN image. In this letter, we propose to expand a method, initially developed for Sentinel-2 single-sensor sharpening, for pansharpening. The expanded method is based on solving a non-convex MS acquisition model using optimization methods based on cyclic decent and manifold optimization. The tuning parameters of the method are chosen using Bayesian optimization with reduced-scale evaluation. The proposed method is compared with a number of established pansharpening methods and is validated using both synthetic and real data sets. © 2004-2012 IEEE.","Data fusion; Image fusion; Image resolution; Remote sensing; Acquisition models; Bayesian optimization; Cyclic descents; Optimization method; Pan-sharpening; Physical constraints; Spatial resolution; Synthetic and real data; data acquisition; image analysis; observational method; optimization; panchromatic image; parameterization; remote sensing; Sentinel; spatial resolution; Image acquisition","Cyclic descent; data fusion; image fusion; pansharpening","Article","Final","","Scopus","2-s2.0-85082885172"
"Nuthammachot N.; Askar A.; Stratoulias D.; Wicaksono P.","Nuthammachot, Narissara (57204889352); Askar, Askar (57211992632); Stratoulias, Dimitris (56270527600); Wicaksono, Pramaditya (54279699900)","57204889352; 57211992632; 56270527600; 54279699900","Combined use of Sentinel-1 and Sentinel-2 data for improving above-ground biomass estimation","2022","Geocarto International","37","2","","366","376","10","10.1080/10106049.2020.1726507","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079697733&doi=10.1080%2f10106049.2020.1726507&partnerID=40&md5=516de63a505e61bd8e10ee9342d8961c","Above-ground Biomass (AGB) represents the largest amount of biomass found on earth. Passive and active remote sensors have been a useful tool in estimating AGB for this purpose; nevertheless, both data sources suffer from saturation problems in dense vegetation. A combination of optical and radar data could potentially increase the accuracy of AGB estimation. In this study we evaluate the synergistic use of Sentinel-1 and Sentinel-2 for assessing AGB in a private forest in Yogyakarta, Indonesia. Forty five sample plots of 20 m x 20 m were used as ground truth data. AGB correlated with Sentinel-1 backscatter and Sentinel-2 derived variables with R2 = 0.34 and R2 = 0.82, respectively; nevertheless, the synergistic use of Sentinel-1 and Sentinel-2 yielded the highest accuracy (i.e., R2 = 0.84). The results indicate that AGB in Yogyakarta is most accurately estimated based on the synergy of optical and radar satellite images. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Indonesia; Yogyakarta; aboveground biomass; backscatter; forest; image analysis; satellite data; Sentinel","Above-ground biomass; image fusion; private forest; Sentinel-1; Sentinel-2","Article","Final","","Scopus","2-s2.0-85079697733"
"Latte N.; Lejeune P.","Latte, Nicolas (24923269000); Lejeune, Philippe (8700431500)","24923269000; 8700431500","PlanetScope radiometric normalization and sentinel-2 super-resolution (2.5 m): A straightforward spectral-spatial fusion of multi-satellite multi-sensor images using residual convolutional neural networks","2020","Remote Sensing","12","15","2366","","","","10.3390/RS12152366","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089677000&doi=10.3390%2fRS12152366&partnerID=40&md5=7438e5dc2c6c821cc563b44167b2ad62","Sentinel-2 (S2) imagery is used in many research areas and for diverse applications. Its spectral resolution and quality are high but its spatial resolutions, of at most 10 m, is not sufficient for fine scale analysis. A novel method was thus proposed to super-resolve S2 imagery to 2.5 m. For a given S2 tile, the 10 S2 bands (four at 10mand six at 20 m) were fused with additional images acquired at higher spatial resolution by the PlanetScope (PS) constellation. The radiometric inconsistencies between PS microsatellites were normalized. Radiometric normalization and super-resolution were achieved simultaneously using state-of-the-art super-resolution residual convolutional neural networks adapted to the particularities of S2 and PS imageries (including masks of clouds and shadows). The method is described in detail, from image selection and downloading to neural network architecture, training, and prediction. The quality was thoroughly assessed visually (photointerpretation) and quantitatively, confirming that the proposed method is highly spatially and spectrally accurate. The method is also robust and can be applied to S2 images acquired worldwide at any date. © 2020 by the authors.","Convolution; Image acquisition; Network architecture; Optical resolving power; Radiometry; Diverse applications; Image selection; Microsatellites; Multi sensor images; Radiometric normalization; Spatial resolution; State of the art; Super resolution; Convolutional neural networks","CubeSat-Dove; Deep learning; Image pansharpening; Image super-resolution; Multi-sensor image fusion; Radiometric correction","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089677000"
"Mateen S.; Nuthammachot N.; Techato K.; Ullah N.","Mateen, Shabnam (58075050600); Nuthammachot, Narissara (57204889352); Techato, Kuaanan (25321184300); Ullah, Nasim (54380714500)","58075050600; 57204889352; 25321184300; 54380714500","Billion Tree Tsunami Forests Classification Using Image Fusion Technique and Random Forest Classifier Applied to Sentinel-2 and Landsat-8 Images: A Case Study of Garhi Chandan Pakistan","2023","ISPRS International Journal of Geo-Information","12","1","9","","","","10.3390/ijgi12010009","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146717923&doi=10.3390%2fijgi12010009&partnerID=40&md5=e22dd6be87621fd32f0aca641d2e6a32","In order to address the challenges of global warming, the Billion Tree plantation drive was initiated by the government of Khyber Pakhtunkhwa, Pakistan, in 2014. The land cover changes as a result of Billion Tree Tsunami project are relatively unexplored. In particular, the utilization of remote sensing techniques and satellite image classification has not yet been done. Recently, the Sentinel-2 (S2) satellite has found much utilization in remote sensing and land cover classification. Sentinel-2 (S2) sensors provide freely available images with a spatial resolution of 10, 20 and 60 m. The higher classification accuracy is directly dependent on the higher spatial resolution of the images. This research aims to classify the land cover changes as a result of the Billion Tree plantation drive in the areas of our interest using Random Forest Classifier (RFA) and image fusion techniques applied to Sentinel-2 and Landsat-8 satellite images. A state-of-the-art, model-based image-sharpening technique was used to sharpen the lower resolution Sentinel-2 bands to 10 m. Then the RFA classifier was used to classify the sharpened images and an accuracy assessment was performed for the classified images of the years 2016, 2018, 2020 and 2022. Finally, ground data samples were collected using an unmanned aerial vehicle (UAV) drone and the classified image samples were compared with the real data collected for the year 2022. The real data ground samples were matched by more than 90% with the classified image samples. The overall classification accuracies [%] for the classified images were recorded as 92.87%, 90.79%, 90.27% and 93.02% for the sample data of the years 2016, 2018, 2020 and 2022, respectively. Similarly, an overall Kappa hat classification was calculated as 0.87, 0.86, 0.83 and 0.84 for the sample data of the years 2016, 2018, 2020 and 2022, respectively. © 2022 by the authors.","","Billion Tree Tsunami project; image classification; image fusion and sharpening; Random Forest Classifier","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146717923"
"Din S.U.; Muhammad K.; Khan M.F.A.; Bashir S.; Sajid M.; Khan A.","Din, Shahab Ud (57223932082); Muhammad, Khan (56651946700); Khan, Muhammad Fawad Akbar (57223925516); Bashir, Shahid (55607240300); Sajid, Muhammad (57193223547); Khan, Asif (57688287600)","57223932082; 56651946700; 57223925516; 55607240300; 57193223547; 57688287600","A fusion of feature-oriented principal components of multispectral data to map granite exposures of Pakistan","2021","Applied Sciences (Switzerland)","11","23","11486","","","","10.3390/app112311486","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120811963&doi=10.3390%2fapp112311486&partnerID=40&md5=b33a3045af701446c8a1adec58cc90fc","Despite low spatial resolutions, thermal infrared bands (TIRs) are generally more suitable for mineral mapping due to fundamental tones and high penetration in vegetated areas compared to shortwave infrared (SWIR) bands. However, the weak overtone combinations of SWIR bands for minerals can be compensated by fusing SWIR-bearing data (Sentinel-2 and Landsat-8) with other multispectral data containing fundamental tones from TIR bands. In this paper, marble in a granitic complex in Mardan District (Khyber Pakhtunkhwa) in Pakistan is discriminated by fusing feature-oriented principal component selection (FPCS) obtained from the ASTER, Landsat-8 Operational Land Imager (OLI), Thermal Infrared Sensor (TIRS) and Sentinel-2 MSI data. Cloud computing from Google Earth Engine (GEE) was used to apply FPCS before and after the decorrelation stretching of Landsat-8, ASTER, and Sentinel-2 MSI data containing five (5) bands in the Landsat-8 OLI and TIRS and six (6) bands each in the ASTER and Sentinel-2 MSI datasets, resulting in 34 components (i.e., 2 × 17 components). A weighted linear combination of selected three components was used to map granite and marble. The samples collected during field visits and petrographic analysis confirmed the remote sensing results by revealing the region’s precise contact and extent of marble and granite rock types. The experimental results reflected the theoretical advantages of the proposed approach compared with the conventional stacking of band data for PCA-based fusion. The proposed methodology was also applied to delineate granite deposits in Karoonjhar Mountains, Nagarparker (Sindh province) and the Kotah Dome, Malakand (Khyber Pakhtunkhwa Province) in Pakistan. The paper presents a cost-effective methodology by the fusion of FPCS components for granite/marble mapping during mineral resource estimation. The importance of SWIR-bearing components in fusion represents minor minerals present in granite that could be used to model the engineering properties of the rock mass. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Ambela; Feature-oriented principal component analysis; Geological mapping; Google earth engine; Malakand; Nagarparkar; Product-level image fusion; Remote sensing; Shewa Shahbazghari","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120811963"
"Vohra R.; Tiwari K.C.","Vohra, Rubeena (57195635472); Tiwari, K.C. (57214612594)","57195635472; 57214612594","Land cover classification using multi-fusion based dense transpose convolution in fully convolutional network with feature alignment for remote sensing images","2022","Earth Science Informatics","","","","","","","10.1007/s12145-022-00891-8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141692844&doi=10.1007%2fs12145-022-00891-8&partnerID=40&md5=2b17f3fdab35c9f91487c529815c7b51","With advances in social development and economic growth, remote sensing technology has been attracted greater attention in monitoring the earth data using radar and optical sensors on satellite platforms for a wide range of applications in different fields such as coastal, hazard and natural resources. Satellite images could play a greater role in improving classification accuracy with high spatial resolution and rich spectral information for land cover classification. However, existing image fusion methods achieves low accuracy due to large-scale feature space. To focus on these issues, a deep learning network structure needs to classify different classes with high spatial resolution and rich spectral information to obtain higher accuracy. In this paper, a feature-based classification approach is proposed namely Multi-Fusion based Dense Transpose Convolutional layer in Fully Convolutional Network with Feature Alignment framework (MF-DTCFCN) to label and categorizes the label region in Remote Sensing Images (RSI). Initially, a multi-fusion feature framework is designed by adding a point-wise addition structure to handle large-scale feature space for high-resolution images. Secondly, the optimized features are pre-trained to classify the labels comprised of the most discriminative features in the pre-training network. The density of output label maps are improved by introducing dense transpose convolution in the network. Then combine the output to the feature alignment with point-wise addition is employed to balance the different features and similarities to achieve additional performance for classification. Here, the Land Use/land Cover (LULC) satellite image dataset namely, Sentinel-2 were used to classify the urban areas of Hyderabad city, India. Experimental results depict that the MF-DTCFCN approach outperforms an accurate improvement in classification accuracy than existing methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","","Deep Learning(DL); Fully Convolutional Network (FCN); Land cover classification; Multi-sensor data; Remote Sensing Images (RSI)","Article","Article in press","","Scopus","2-s2.0-85141692844"
"Liu Q.; Zhang Y.","Liu, Q. (57825230000); Zhang, Y. (56255853800)","57825230000; 56255853800","Study on snow cover change based on the fusion of sentinel-2 and modis images","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","333","338","5","10.5194/isprs-Annals-V-3-2022-333-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132013309&doi=10.5194%2fisprs-Annals-V-3-2022-333-2022&partnerID=40&md5=d4510817b69e8e0e88407f4e0a2d3ded","Snow cover is one of the features that change rapidly on the surface, and remote sensing data with high spatial and temporal resolution is an important means to monitor the dynamic changes of snow. However, due to the limitation of satellite conditions, it is difficult to acquire remote sensing images with both high temporal and spatial resolution at the same time. The classical Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model (ESTARFM) breaks through the limitations of a single sensor and effectively brings into play the complementary advantages of different observation platforms. In this paper, we take the Babao River Basin within the Qinghai-Tibet Plateau as the study area to investigate the effect of fusion of Sentinel-2 and MODIS (Moderate-resolution Imaging Spectroradiometer) based on the ESTARFM and analyze the fusion-supplemented snow accumulation period data. The results show that the fusion effect of Sentinel-2 and MODIS is good in terms of visual and multiple quantitative indicators. The snow accumulation area of the fused image is also close to the real image. By fusing to supplement the missing Sentinel-2 data, a variation map of snow NDSI (Normalized Difference Snow Index) in the study area was obtained. It was found that the snowfall process in the watershed appeared first in the surrounding uplands and then in the flat areas, and the maximum rate of snow cover area change was 116.17 km2. Therefore, the spatiotemporal data fusion of Sentinel-2 and MODIS based on ESTARFM algorithm can provide reliable satellite remote sensing data with a higher spatiotemporal resolution for snow accumulation change monitoring.  © Authors 2022.","Image fusion; Radiometers; Remote sensing; Snow; Babao river basin; Enhanced spatial and temporal adaptive reflectance fusion model; Fusion model; Moderate-resolution imaging spectroradiometers; Normalized difference snow indices; River basins; SNOMAP; Snow accumulation; Snow cover area; Temporal adaptive; Watersheds","Babao River basin; ESTARFM; NDSI; SNOMAP; Snow Cover Area","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132013309"
"Sun Y.; Li Z.-L.; Luo J.; Wu T.; Liu N.","Sun, Yingwei (57204428522); Li, Zhao-Liang (57218308345); Luo, Jiancheng (7404183561); Wu, Tianjun (57193855591); Liu, Niantang (57475654200)","57204428522; 57218308345; 7404183561; 57193855591; 57475654200","Farmland parcel-based crop classification in cloudy/rainy mountains using Sentinel-1 and Sentinel-2 based deep learning","2022","International Journal of Remote Sensing","43","3","","1054","1073","19","10.1080/01431161.2022.2032458","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125641177&doi=10.1080%2f01431161.2022.2032458&partnerID=40&md5=508c2e5e65c7255a769e2a27915ab3b7","Multitemporal remote sensing data, especially those for key phenological periods, play an important role in crop classification. However, cloudy/rainy climate conditions can easily lead to a lack of valid optical data, leading to crop classification difficulties. A general solution is taking advantage of all-weather synthetic aperture radar (SAR) datasets. In practice, SAR and optical datasets are often applied in the agricultural field by the method of image fusion, but it is difficult to apply when the number of optical images is too small. To solve this problem, this research proposes a data-transfer and feature-optimize-based method, which deploy an RNN-based encoding-decoding network to add additional data to the ‘optical’ temporal features at the farmland parcel scale and improve the utilization of optical fragments. On the basis of this method, we mitigate inconsistencies in spatial scale among different datasets and optimize the time-series parameters without expert knowledge in the crop classification procedure. The experimental results illustrate the crop classification accuracy of this method, which achieves a 4.1% improvement over the traditional approach and is especially effective for dryland crops (e.g. corn and rapeseed). Thus, this research demonstrates the effectiveness of the combined use of optical and SAR data for similar applications in cloudy/rainy mountainous areas. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Classification (of information); Crops; Data transfer; Deep learning; Farms; Geometrical optics; Image fusion; Radar imaging; Remote sensing; Cloudy/rainy mountainoi area; Combined utilization; Crop classification; Feature transfers; Mountainous area; Multi-temporal data; Multi-temporal remote sensing; Optical-; Optimisations; Sentinel-1; agricultural land; crop plant; data set; machine learning; mountain region; optimization; remote sensing; Sentinel; synthetic aperture radar; Synthetic aperture radar","cloudy/rainy mountainous area; combined utilization; crop classification; feature transfer; multitemporal data; optimization","Article","Final","","Scopus","2-s2.0-85125641177"
"Zhou F.; Zhong D.; Peiman R.","Zhou, Fuqun (57216519289); Zhong, Detang (7102032532); Peiman, Rihana (57218681744)","57216519289; 7102032532; 57218681744","Reconstruction of cloud-free sentinel-2 image time-series using an extended spatiotemporal image fusion approach","2020","Remote Sensing","12","16","2595","","","","10.36745/IJCA.341","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090003592&doi=10.36745%2fIJCA.341&partnerID=40&md5=a854757436a3a529fe46fe56b22c7f15","Time-series for medium spatial resolution satellite imagery are a valuable resource for environmental assessment and monitoring at regional and local scales. Sentinel-2 satellites from the European Space Agency (ESA) feature a multispectral instrument (MSI) with 13 spectral bands and spatial resolutions from 10 m to 60 m, oering a revisit range from 5 days at the equator to a daily approach of the poles. Since their launch, the Sentinel-2 MSI image time-series from satellites have been used widely in various environmental studies. However, the values of Sentinel-2 image time-series have not been fully realized and their usage is impeded by cloud contamination on images, especially in cloudy regions. To increase cloud-free image availability and usage of the time-series, this study attempted to reconstruct a Sentinel-2 cloud-free image time-series using an extended spatiotemporal image fusion approach. First, a spatiotemporal image fusion model was applied to predict synthetic Sentinel-2 images when clear-sky images were not available. Second, the cloudy and cloud shadow pixels of the cloud contaminated images were identified based on analysis of the dierences of the synthetic and observation image pairs. Third, the cloudy and cloud shadow pixels were replaced by the corresponding pixels of its synthetic image. Lastly, the pixels from the synthetic image were radiometrically calibrated to the observation image via a normalization process. With these processes, we can reconstruct a full length cloud-free Sentinel-2 MSI image time-series to maximize the values of observation information by keeping observed cloud-free pixels and calibrating the synthetized images by using the observed cloud-free pixels as references for better quality. © 2020 by the authors.","Image fusion; Pixels; Satellite imagery; Time series; Cloud contamination; Environmental assessment; Environmental studies; European Space Agency; Multispectral instruments; Normalization process; Observation information; Spatiotemporal images; Image reconstruction","Clouds and cloud shadow; Image fusion; Image time-series; MODIS; Sentinel-2","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090003592"
"He S.; Shao H.; Xian W.; Zhang S.; Zhong J.; Qi J.","He, Shan (57286889300); Shao, Huaiyong (35094554400); Xian, Wei (7004036924); Zhang, Shuhui (57286438200); Zhong, Jialong (57196246297); Qi, Jiaguo (57217543946)","57286889300; 35094554400; 7004036924; 57286438200; 57196246297; 57217543946","Extraction of abandoned land in hilly areas based on the spatio-temporal fusion of multi-source remote sensing images","2021","Remote Sensing","13","19","3956","","","","10.3390/rs13193956","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116455219&doi=10.3390%2frs13193956&partnerID=40&md5=0937e3fdf652572cf2864724e097c159","Hilly areas are important parts of the world’s landscape. A marginal phenomenon can be observed in some hilly areas, leading to serious land abandonment. Extracting the spatio-temporal distribution of abandoned land in such hilly areas can protect food security, improve people’s livelihoods, and serve as a tool for a rational land plan. However, mapping the distribution of abandoned land using a single type of remote sensing image is still challenging and problematic due to the fragmentation of such hilly areas and severe cloud pollution. In this study, a new approach by integrating Linear stretch (Ls), Maximum Value Composite (MVC), and Flexible Spatiotemporal DAta Fusion (FSDAF) was proposed to analyze the time-series changes and extract the spatial distribution of abandoned land. MOD09GA, MOD13Q1, and Sentinel-2 were selected as the basis of remote sensing images to fuse a monthly 10 m spatio-temporal data set. Three pieces of vegetation indices (VIs: ndvi, savi, ndwi) were utilized as the measures to identify the abandoned land. A multiple spatio-temporal scales sample database was established, and the Support Vector Machine (SVM) was used to extract abandoned land from cultivated land and woodland. The best extraction result with an overall accuracy of 88.1% was achieved by integrating Ls, MVC, and FSDAF, with the assistance of an SVM classifier. The fused VIs image set transcended the single source method (Sentinel-2) with greater accuracy by a margin of 10.8–23.6% for abandoned land extraction. On the other hand, VIs appeared to contribute positively to extract abandoned land from cultivated land and woodland. This study not only provides technical guidance for the quick acquirement of abandoned land distribution in hilly areas, but it also provides strong data support for the connection of targeted poverty alleviation to rural revitalization. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Food supply; Image fusion; Image processing; Land use; Pollution; Remote sensing; Support vector machines; Time series; Abandoned land; Cloud pollution; Cultivated lands; Hilly areas; Multi-source images; Remote sensing images; Spatio-temporal data; Spatio-temporal fusions; Time-series change; Times series; Extraction","Abandoned land; Cloud pollution; Hilly area; Multi-source images; Spatio-temporal fusion; Time-series change","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116455219"
"Workman S.; Rafique M.U.; Blanton H.; Greenwell C.; Jacobs N.","Workman, Scott (55924181800); Rafique, M. Usman (57197260044); Blanton, Hunter (57216364152); Greenwell, Connor (56705225200); Jacobs, Nathan (15520711800)","55924181800; 57197260044; 57216364152; 56705225200; 15520711800","Single Image Cloud Detection via Multi-Image Fusion","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323759","1468","1471","3","10.1109/IGARSS39084.2020.9323759","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102015568&doi=10.1109%2fIGARSS39084.2020.9323759&partnerID=40&md5=788df9e52bf4524c8727df503b7a05d6","Artifacts in imagery captured by remote sensing, such as clouds, snow, and shadows, present challenges for various tasks, including semantic segmentation and object detection. A primary challenge in developing algorithms for identifying such artifacts is the cost of collecting annotated training data. In this work, we explore how recent advances in multi-image fusion can be leveraged to bootstrap single image cloud detection. We demonstrate that a network optimized to estimate image quality also implicitly learns to detect clouds. To support the training and evaluation of our approach, we collect a large dataset of Sentinel-2 images along with a per-pixel semantic labelling for land cover. Through various experiments, we demonstrate that our method reduces the need for annotated training data and improves cloud detection performance. © 2020 IEEE.","Geology; Large dataset; Object detection; Remote sensing; Semantics; Annotated training data; Cloud detection; Land cover; Semantic segmentation; Single images; Image fusion","clouds; multi-image fusion; segmentation; weakly-supervised learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102015568"
"Schmitt M.; Hughes L.H.; Körner M.; Zhu X.X.","Schmitt, M. (7401931279); Hughes, L.H. (57201113391); Körner, M. (57190168095); Zhu, X.X. (55696622200)","7401931279; 57201113391; 57190168095; 55696622200","Colorizing sentinel-1 SAR images using a variational autoencoder conditioned on Sentinel-2 imagery","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2","","1045","1051","6","10.5194/isprs-archives-XLII-2-1045-2018","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048356423&doi=10.5194%2fisprs-archives-XLII-2-1045-2018&partnerID=40&md5=c3e1ac09b9c4f7deddfcb900fa6d7afd","In this paper, we have shown an approach for the automatic colorization of SAR backscatter images, which are usually provided in the form of single-channel gray-scale imagery. Using a deep generative model proposed for the purpose of photograph colorization and a Lab-space-based SAR-optical image fusion formulation, we are able to predict artificial color SAR images, which disclose much more information to the human interpreter than the original SAR data. Future work will aim at further adaption of the employed procedure to our special case of multi-sensor remote sensing imagery. Furthermore, we will investigate if the low-level representations learned intrinsically by the deep network can be used for SAR image interpretation in an end-to-end manner. © Authors 2018.","Data fusion; Geometrical optics; Image fusion; Remote sensing; Space optics; Synthetic aperture radar; Artificial color; Deep learnig; Generative model; Low level representation; Optical remote sensing; Remote sensing imagery; Sentinel-1; Sentinel-2; Radar imaging","Data fusion; Deep learnig; Optical remote sensing; Sentinel-1; Sentinel-2; Synthetic aperture radar (SAR)","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048356423"
"Ye N.; Morgenroth J.; Xu C.; Chen N.","Ye, Ning (57364157500); Morgenroth, Justin (23991150200); Xu, Cong (37562075600); Chen, Na (57364157600)","57364157500; 23991150200; 37562075600; 57364157600","Indigenous forest classification in New Zealand – A comparison of classifiers and sensors","2021","International Journal of Applied Earth Observation and Geoinformation","102","","102395","","","","10.1016/j.jag.2021.102395","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120689536&doi=10.1016%2fj.jag.2021.102395&partnerID=40&md5=b35af77bf1eb39ff02fd2bfff47ee836","Understanding the composition and the changes of New Zealand's woody vegetation communities is important for effective management. However, past national-scale mapped classifications emphasised mature rather than seral vegetation communities and forests were mapped in relative coarse spatial resolution. The integration of Sentinel-2 and PlanetScope imagery provides an opportunity for forest mapping with low cost and high accuracy. This study aims to investigate the feasibility of the integrated image for detailed forest mapping. Free satellite data (Sentinel-2, PlanetScope, fused data) were compared with commercial data (WorldView-2, and WorldView-2 resampled to Sentinel-2 and PlanetScope spatial resolutions) by conducting pixel-based classification with three machine learning classifiers (Support Vector Machine radial basis function kernel, Random Forest, Artificial Neural Network). The combinations of imagery type and classifier were assessed on their potential for mapping nine land cover classes in podocarp forest in New Zealand's central north island, including: conifer, low layer vegetation, broadleaf evergreen, highland softwood, wetland vegetation, water, dead tree, lowland softwood, and low-density vegetation and bare soil. Spectral features (single bands and indices), textural features, and an 8 m resolution digital terrain model (DTM) were used in classifications; the relative importance of these input features was also assessed. In this study, it was found that the overall classification accuracy was dependent on the combination of classifier and imagery, with different combinations resulting in a range of accuracies between 0.669 and 0.956. The best overall accuracy was achieved by integrating Sentinel-2 and PlanetScope imagery (0.956) which was even greater than that of WorldView-2 (0.951). The digital terrain model was the most important feature for all scenarios; Gray-Level Co-Occurrence Matrix-Mean was the most important texture variable for WorldView-2 and integrated images. Original bands, as well as GI, Norm-G, and SR-NIRR, were also crucial for vegetation classification. © 2021 The Authors","New Zealand; Coniferophyta; artificial neural network; image analysis; image classification; land cover; land use change; pixel; Sentinel; vegetation dynamics","Artificial neural network; Image fusion; Land cover; Land use; Pixel-based classification; Vegetation classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120689536"
"Wang Q.; Atkinson P.M.","Wang, Qunming (55649569623); Atkinson, Peter M. (7201906181)","55649569623; 7201906181","Spatio-temporal fusion for daily Sentinel-2 images","2018","Remote Sensing of Environment","204","","","31","42","11","10.1016/j.rse.2017.10.046","181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033214493&doi=10.1016%2fj.rse.2017.10.046&partnerID=40&md5=5d840fea0206383bd8d77cfdafdc99fc","Sentinel-2 and Sentinel-3 are two newly launched satellites for global monitoring. The Sentinel-2 Multispectral Imager (MSI) and Sentinel-3 Ocean and Land Colour Instrument (OLCI) sensors have very different spatial and temporal resolutions (Sentinel-2 MSI sensor 10 m, 20 m and 60 m, 10 days, albeit 5 days with 2 sensors, conditional upon clear skies; Sentinel-3 OLCI sensor 300 m, < 1.4 days with 2 sensors). For local monitoring (e.g., the growing cycle of plants) one either has the desired spatial or temporal resolution, but not both. In this paper, spatio-temporal fusion is considered to fuse Sentinel-2 with Sentinel-3 images to create nearly daily Sentinel-2 images. A challenging issue in spatio-temporal fusion is that there can be very few cloud-free fine spatial resolution images temporally close to the prediction time, or even available, strong temporal (i.e., seasonal) changes may exist. To this end, a three-step method consisting of regression model fitting (RM fitting), spatial filtering (SF) and residual compensation (RC) is proposed, which is abbreviated as Fit-FC. The Fit-FC method can be performed using only one Sentinel-3–Sentinel-2 pair and is advantageous for cases involving strong temporal changes (i.e., mathematically, the correlation between the two Sentinel-3 images is small). The effectiveness of the method was validated using two datasets. The created nearly daily Sentinel-2 time-series images have great potential for timely monitoring of highly dynamic environmental, agricultural or ecological phenomena. © 2017 Elsevier Inc.","Regression analysis; Down-scaling; Sentinel-2; Sentinel-3; Spatial and temporal resolutions; Spatial filterings; Spatial resolution images; Spatio-temporal fusions; Temporal resolution; correlation; data set; downscaling; image analysis; image resolution; regression analysis; satellite data; satellite imagery; satellite sensor; Sentinel; spatial resolution; spatiotemporal analysis; Image fusion","Downscaling; Image fusion; Sentinel-2; Sentinel-3","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85033214493"
"Bai B.; Tan Y.; Donchyts G.; Haag A.; Weerts A.","Bai, Bingxin (57205687503); Tan, Yumin (14064055300); Donchyts, Gennadii (55114502100); Haag, Arjen (57192919948); Weerts, Albrecht (6602950477)","57205687503; 14064055300; 55114502100; 57192919948; 6602950477","A simple spatio–temporal data fusion method based on linear regression coefficient compensation","2020","Remote Sensing","12","23","3900","1","16","15","10.3390/rs12233900","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097012979&doi=10.3390%2frs12233900&partnerID=40&md5=880c173620496811a5789a8e8871f971","High spatio–temporal resolution remote sensing images are of great significance in the dynamic monitoring of the Earth’s surface. However, due to cloud contamination and the hardware limitations of sensors, it is difficult to obtain image sequences with both high spatial and temporal resolution. Combining coarse resolution images, such as the moderate resolution imaging spectroradiometer (MODIS), with fine spatial resolution images, such as Landsat or Sentinel-2, has become a popular means to solve this problem. In this paper, we propose a simple and efficient enhanced linear regression spatio–temporal fusion method (ELRFM), which uses fine spatial resolution images acquired at two reference dates to establish a linear regression model for each pixel and each band between the image reflectance and the acquisition date. The obtained regression coefficients are used to help allocate the residual error between the real coarse resolution image and the simulated coarse resolution image upscaled by the high spatial resolution result of the linear prediction. The developed method consists of four steps: (1) linear regression (LR), (2) residual calculation, (3) distribution of the residual and (4) singular value correction. The proposed method was tested in different areas and using different sensors. The results show that, compared to the spatial and temporal adaptive reflectance fusion model (STARFM) and the flexible spatio–temporal data fusion (FSDAF) method, the ELRFM performs better in capturing small feature changes at the fine image scale and has high prediction accuracy. For example, in the red band, the proposed method has the lowest root mean square error (RMSE) (ELRFM: 0.0123 vs. STARFM: 0.0217 vs. FSDAF: 0.0224 vs. LR: 0.0221). Furthermore, the lightweight algorithm design and calculations based on the Google Earth Engine make the proposed method computationally less expensive than the STARFM and FSDAF. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Image acquisition; Image fusion; Image resolution; Linear regression; Mean square error; Radiometers; Reflection; Remote sensing; High spatial resolution; Linear regression coefficients; Linear regression models; Moderate resolution imaging spectroradiometer; Regression coefficient; Root mean square errors; Spatial and temporal resolutions; Spatial resolution images; Image enhancement","Google Earth Engine; Landsat; Sentinel-2; Spatio–temporal fusion; Time series","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097012979"
"Jiang M.; Shen H.; Li J.","Jiang, Menghui (57210173702); Shen, Huanfeng (8359721100); Li, Jie (57214207213)","57210173702; 8359721100; 57214207213","Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5410915","","","","10.1109/TGRS.2022.3188998","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135232679&doi=10.1109%2fTGRS.2022.3188998&partnerID=40&md5=8337c148bae6361ca03536bbf9d398c0","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal-spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.  © 1980-2012 IEEE.","Deep learning; Feature extraction; Feedback; Generative adversarial networks; Optical remote sensing; Radiometers; Satellite imagery; Cloud cover; Deep residual cycle generative adversarial network; Features extraction; Generator; Heterogeneous integrated framework; Integrated frameworks; Land-cover change; Remote-sensing; Spatial resolution; Thick cloud cover; cloud cover; integrated approach; Landsat; MODIS; network analysis; remote sensing; satellite imagery; Sentinel; spatiotemporal analysis; Image fusion","Deep residual cycle generative adversarial network (GAN); heterogeneous integrated framework; land-cover change; thick cloud cover","Article","Final","","Scopus","2-s2.0-85135232679"
"Park S.; Park N.-W.; Na S.-I.","Park, Soyeon (57215420514); Park, No-Wook (7202111787); Na, Sang-Il (57217287093)","57215420514; 7202111787; 57217287093","An Object-Based Weighting Approach to Spatiotemporal Fusion of High Spatial Resolution Satellite Images for Small-Scale Cropland Monitoring","2022","Agronomy","12","10","2572","","","","10.3390/agronomy12102572","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140433373&doi=10.3390%2fagronomy12102572&partnerID=40&md5=554b6813ed37b7f5497a6b0c2ae8f41b","Continuous crop monitoring often requires a time-series set of satellite images. Since satellite images have a trade-off in spatial and temporal resolution, spatiotemporal image fusion (STIF) has been applied to construct time-series images at a consistent scale. With the increased availability of high spatial resolution images, it is necessary to develop a new STIF model that can effectively reflect the properties of high spatial resolution satellite images for small-scale crop field monitoring. This paper proposes an advanced STIF model using a single image pair, called high spatial resolution image fusion using object-based weighting (HIFOW), for blending high spatial resolution satellite images. The four-step weighted-function approach of HIFOW includes (1) temporal relationship modeling, (2) object extraction using image segmentation, (3) weighting based on object information, and (4) residual correction to quantify temporal variability between the base and prediction dates and also represent both spectral patterns at the prediction date and spatial details of fine-scale images. The specific procedures tailored for blending fine-scale images are the extraction of object-based change and structural information and their application to weight determination. The potential of HIFOW was evaluated from the experiments on agricultural sites using Sentinel-2 and RapidEye images. HIFOW was compared with three existing STIF models, including the spatial and temporal adaptive reflectance fusion model (STARFM), flexible spatiotemporal data fusion (FSDAF), and Fit-FC. Experimental results revealed that the HIFOW prediction could restore detailed spatial patterns within crop fields and clear crop boundaries with less spectral distortion, which was not represented in the prediction results of the other three models. Consequently, HIFOW achieved the best prediction performance in terms of accuracy and structural similarity for all the spectral bands. Other than the reflectance prediction, HIFOW also yielded superior prediction performance for blending normalized difference vegetation index images. These findings indicate that HIFOW could be a potential solution for constructing high spatial resolution time-series images in small-scale croplands. © 2022 by the authors.","","crop monitoring; image segmentation; multi-sensor images; resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140433373"
"Rajah P.; Odindi J.; Mutanga O.","Rajah, Perushan (56658660800); Odindi, John (36521256000); Mutanga, Onisimo (55912148400)","56658660800; 36521256000; 55912148400","Feature level image fusion of optical imagery and Synthetic Aperture Radar (SAR) for invasive alien plant species detection and mapping","2018","Remote Sensing Applications: Society and Environment","10","","","198","208","10","10.1016/j.rsase.2018.04.007","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045729361&doi=10.1016%2fj.rsase.2018.04.007&partnerID=40&md5=c624c25a5775540ad2b5ccb5ab7d1099","Invasive alien plant species are regarded as a major threat to among others socio-economic systems, global biodiversity and conservation initiatives. A reliable understanding of their spatial and temporal distribution is paramount for understanding their impact on co-existing landscapes and ecosystems. While traditional passive remote sensing methods have been successful in assessing invasion of such species, limiting factors such as cost, restricted coverage, image availability, terrain and inadequate resolutions hamper mapping and detection at large spatial extents. To date, the adoption of active remote sensing techniques as complimentary data to invasive alien plant mapping has been limited. In this study, we fuse two commonly used medium spatial and spectral resolution imagery (Sentinel-2 and Landsat 8) with active remote sensing data (Synthetic Aperture Rada imagery) in determining the optimal season for detecting and mapping the American Bramble (Rubus cuneifolius). Feature level image fusion was adopted to integrate passive and active remote sensing imagery and Support vector machine (SVM) supervised classification algorithm used to discriminate the American Bramble from surrounding native vegetation. Seasonal results showed that Sentinel-2 data, fused with SAR data generated the highest classification accuracy during summer (76%), while Landsat 8 imagery fused with SAR data performed best in winter (72%). These findings demonstrate that fusion of SAR with traditional optical imagery can be used to detect and map the American Bramble at a regional scale. We conclude that SAR data can be used synergistically with optical remote sensing to improve discrimination and mapping of the American Bramble. © 2018 Elsevier B.V.","","American Bramble, Multisensor image fusion; Invasive alien plant species; Remote sensing; Sentinel-2; Synthetic Aperture Radar (SAR)","Article","Final","","Scopus","2-s2.0-85045729361"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","Fusion and classification of multi-temporal SAR and optical imagery using convolutional neural network","2022","International Journal of Image and Data Fusion","13","2","","113","135","22","10.1080/19479832.2021.2019133","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121724919&doi=10.1080%2f19479832.2021.2019133&partnerID=40&md5=a411cfc65121ce6412df4f71115586a7","Remote sensing image classification is difficult, especially for agricultural crops with identical phenological growth periods. In this context, multi-sensor image fusion allows a comprehensive representation of biophysical and structural information. Recently, Convolutional Neural Network (CNN)-based methods are used for several applications due to their spatial-spectral interpretability. Hence, this study explores the potential of fused multi-temporal Sentinel 1 (S1) and Sentinel 2 (S2) images for Land Use/Land Cover classification over an agricultural area in India. For classification, Bayesian optimised 2D CNN-based DL and pixel-based SVM classifiers were used. For fusion, a CNN-based siamese network with Ratio-of-Laplacian pyramid method was used for the images acquired over the entire winter cropping period. This fusion strategy leads to better interpretability of results and also found that 2D CNN-based DL classifier performed well in terms of classification accuracy for both single-month (95.14% and 96.11%) as well as multi-temporal (99.87% and 99.91%) fusion in comparison to the SVM with classification accuracy for single-month (80.02% and 81.36%) and multi-temporal fusion (95.69% and 95.84%). Results indicate better performance by Vertical-Vertical polarised fused images than Vertical-Horizontal polarised fused images. Thus, implying the need to analyse classified images obtained by DL classifiers along with the classification accuracy. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Convolution; Convolutional neural networks; Crops; Image classification; Image fusion; Land use; Radar imaging; Remote sensing; Synthetic aperture radar; Bayesian optimization; Classification accuracy; Convolutional neural network; Fused images; Interpretability; Multi-temporal; Network-based; Support vector machine; Support vectors machine; artificial neural network; Bayesian analysis; image analysis; image classification; optical method; optimization; remote sensing; spatiotemporal analysis; support vector machine; synthetic aperture radar; Support vector machines","Bayesian Optimisation; Convolutional Neural Network (CNN); Fusion; Support Vector Machine (SVM)","Article","Final","","Scopus","2-s2.0-85121724919"
"Wang J.; Chen J.; Wang Q.","Wang, Jian (57221359948); Chen, Jiaqi (48160919100); Wang, Qingwei (57210324380)","57221359948; 48160919100; 57210324380","Fusion of POLSAR and Multispectral Satellite Images: A New Insight for Image Fusion","2020","Proceedings of the 2020 IEEE International Conference on Computational Electromagnetics, ICCEM 2020","","","9219457","83","84","1","10.1109/ICCEM47450.2020.9219457","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095584450&doi=10.1109%2fICCEM47450.2020.9219457&partnerID=40&md5=9ef6dede6b475205025cce468d6654d4","Polarized synthetic aperture radar (POLSAR) and multispectral images have great complementarity in information volume. That is to say, POLSAR have high resolution but poor color information. Multispectral images have rich spectral channel information, but the resolution is low. Therefore, this work has explored the fusion problem of the two data source. A framework was proposed, which merged polarized channel fusion data and multispectral images based on the Sentinel-2 and GF-3 data. The experimental results showed that the fusion results greatly integrated the characteristics of each channel of POLSAR and optical image. Therefore, our work has great application potential in improving the accuracy of feature recognition.  © 2020 IEEE.","Computational electromagnetics; Geometrical optics; Synthetic aperture radar; Color information; Data-source; Feature recognition; High resolution; Multispectral images; Multispectral satellite image; Optical image; Spectral channels; Image fusion","Feature recognition; Fusion; GF-3; Polarized synthetic aperture radar (POL-SAR); Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85095584450"
"Bioresita F.; Puissant A.; Stumpf A.; Malet J.-P.","Bioresita, Filsa (57200860429); Puissant, Anne (7102002323); Stumpf, André (37103096800); Malet, Jean-Philippe (7004001508)","57200860429; 7102002323; 37103096800; 7004001508","Fusion of Sentinel-1 and Sentinel-2 image time series for permanent and temporary surface water mapping","2019","International Journal of Remote Sensing","40","23","","9026","9049","23","10.1080/01431161.2019.1624869","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067690242&doi=10.1080%2f01431161.2019.1624869&partnerID=40&md5=e769697606cf40867713a9ba17abfc77","Monitoring the spatial and temporal extents of permanent and temporary bodies of surface water is important for various applications such as water resource management, climate modelling, and biodiversity conservation. Satellite remote sensing is an effective source of information to detect surface water over large areas and document their evolution in time. Recently, the European Space Agency (ESA) launched freely available SAR (Synthetic Aperture Radar) and optical sensors (Sentinel-1 & 2) with high revisiting time and spatial resolution. The objective of this paper is to explore the contribution of multi-temporal and multi-source (passive and active) Sentinel observations for improving the detection and mapping of surface waters by applying decision-level image fusion techniques. The approach is tested over Central Ireland using a time series of 16 Sentinel-1 images and a few Sentinel-2 images for the period 2015–2016. Compared to a mono-date approach, the combination of Sentinel-1 & 2 observations provides better accuracy for mapping permanent surface water. Decision level fusion technique allows mapping temporary surface water (such as flooding) with a high accuracy. It also gives the possibility to monitor their dynamics by providing the probability of occurrence of flooded areas at the pixel level. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.","Ireland; Biodiversity; Floods; Image enhancement; Image fusion; Mapping; Radar imaging; Remote sensing; Space-based radar; Synthetic aperture radar; Time series; Water conservation; Water management; Biodiversity conservation; Decision level fusion; European Space Agency; Image fusion techniques; Probability of occurrence; SAR(synthetic aperture radar); Satellite remote sensing; Waterresource management; accuracy assessment; flooding; image processing; pixel; satellite imagery; Sentinel; spatial resolution; surface water; synthetic aperture radar; time series; Surface waters","","Article","Final","","Scopus","2-s2.0-85067690242"
"Palsson F.; Sveinsson J.R.; Ulfarsson M.O.","Palsson, Frosti (55052918200); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","55052918200; 7003642214; 6507677875","Single Sensor Image Fusion Using a Deep Residual Network","2018","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2018-September","","8747059","","","","10.1109/WHISPERS.2018.8747059","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073909973&doi=10.1109%2fWHISPERS.2018.8747059&partnerID=40&md5=5db60718951b86599e2116287ade4a70","Single sensor fusion is the fusion of two or more spectrally disjoint reflectance bands that have different spatial resolution and have been acquired by the same sensor. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m and 60 m resolution from the visible to short-wave infrared (SWIR) regions of the electromagnetic spectrum. In this paper, we present a method based on a deep residual convolutional network to fuse the fine and coarse spatial resolution bands to obtain finer spatial resolution versions of the coarse bands. The benefits of the residual design are primarily that the network converges faster and it allows for deeper networks. Also, it improves the spectral consistency of the fused image. Using a real Sentinel-2 dataset, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods. © 2018 IEEE.","Convolution; Image enhancement; Image resolution; Infrared radiation; Remote sensing; Spectroscopy; Convolutional networks; Electromagnetic spectra; Multi-spectral; Residual design; Sentinel-2; Short wave infrared regions; Spatial resolution; State of the art; Image fusion","convolutional network; deep residual network; Image fusion; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85073909973"
"Fernandez R.; Fernandez-Beltran R.; Pla F.","Fernandez, Rafael (57222243976); Fernandez-Beltran, Ruben (55838551300); Pla, Filiberto (7006504936)","57222243976; 55838551300; 7006504936","SENTINEL-3 IMAGE SUPER-RESOLUTION USING DATA FUSION AND CONVOLUTIONAL NEURAL NETWORKS","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","2867","2870","3","10.1109/IGARSS47720.2021.9554826","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129884916&doi=10.1109%2fIGARSS47720.2021.9554826&partnerID=40&md5=85e41091ceadaed2fc3a7c5855bf1103","With the increasing availability of Sentinel-2 (S2) and Sentinel-3 (S3) data, developing higher-level data products becomes a very attractive option to relieve the spatial limitations of the Ocean and Land Colour Instrument (OLCI) of S3. In this context, this paper investigates the suitability of super-resolving operational OLCI products using the Multi-Spectral Instrument (MSI) of S2 as an offline spatial reference. Specifically, the proposed approach assembles a multi-spectral data fusion scheme together with a convolutinal neural network (CNN) mapping function to project the OLCI sensor onto its corresponding spatial reference which is synthetically generated by the OLCI/MSI fusion. In this way, the trained model is able to super-resolve operational OLCI products under demand without the need of using MSI data. The experimental part of the work shows the suitability of the proposed approach in the context of the Copernicus programme. © 2021 IEEE","Convolutional neural networks; Photomapping; Sensor data fusion; Spectral resolution; Convolutional neural network; Data products; Image super resolutions; Multi-spectral; Offline; Sentinel-2; Sentinel-3; Spatial reference; Super-resolution; Superresolution; Image fusion","image fusion; Sentinel-2 (S2); Sentinel-3 (S3); super-resolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85129884916"
"Li W.; Jiang J.; Guo T.; Zhou M.; Tang Y.; Wang Y.; Zhang Y.; Cheng T.; Zhu Y.; Cao W.; Yao X.","Li, Wei (57205166392); Jiang, Jiale (56289787100); Guo, Tai (57209567732); Zhou, Meng (57203944956); Tang, Yining (57208160329); Wang, Ying (57767139300); Zhang, Yu (56662268900); Cheng, Tao (56278310400); Zhu, Yan (8921604000); Cao, Weixing (55489902600); Yao, Xia (14022139100)","57205166392; 56289787100; 57209567732; 57203944956; 57208160329; 57767139300; 56662268900; 56278310400; 8921604000; 55489902600; 14022139100","Generating red-edge images at 3M spatial resolution by fusing sentinel-2 and planet satellite products","2019","Remote Sensing","11","12","1422","","","","10.3390/rs11121422","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068177402&doi=10.3390%2frs11121422&partnerID=40&md5=c29bc81d9238fbb43dba448e861a04b9","High-resolution satellite images can be used to some extent to mitigate the mixed-pixel problem caused by the lack of intensive production, farmland fragmentation, and the uneven growth of field crops in developing countries. Specifically, red-edge (RE) satellite images can be used in this context to reduce the influence of soil background at early stages as well as saturation due to crop leaf area index (LAI) at later stages. However, the availability of high-resolution RE satellite image products for research and application globally remains limited. This study uses the weight-and-unmixing algorithm as well as the SUPer-REsolution for multi-spectral Multi-resolution Estimation (Wu-SupReME) approach to combine the advantages of Sentinel-2 spectral and Planet spatial resolution and generate a high-resolution RE product. The resultant fused image is highly correlated (R2 > 0.98) with Sentinel-2 image and clearly illustrates the persistent advantages of such products. This fused image was significantly more accurate than the originals when used to predict heterogeneous wheat LAI and therefore clearly illustrated the persistence of Sentinel-2 spectral and Planet spatial advantage, which indirectly proved that the fusion methodology of generating high-resolution red-edge products from Planet and Sentinel-2 images is possible. This study provided method reference for multi-source data fusion and image product for accurate parameter inversion in quantitative remote sensing of vegetation. © 2019 by the authors.","Crops; Developing countries; Image fusion; Image resolution; Planets; Remote sensing; Fusion image; Sentinel-2; SupReME; Unmixing; Wheat LAI; Satellites","Fusion image; Planet; Sentinel-2; SupReME; Weight-and-unmixing; Wheat LAI","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85068177402"
"Ge W.; Cheng Q.; Jing L.; Wang F.; Zhao M.; Ding H.","Ge, Wenyan (57192701268); Cheng, Qiuming (23980841500); Jing, Linhai (23492470700); Wang, Fei (56308169200); Zhao, Molei (57219295888); Ding, Haifeng (57357238400)","57192701268; 23980841500; 23492470700; 56308169200; 57219295888; 57357238400","Assessment of the capability of sentinel-2 imagery for iron-bearing minerals mapping: A case study in the cuprite area, nevada","2020","Remote Sensing","12","18","1141","","","","10.3390/RS12183028","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092081852&doi=10.3390%2fRS12183028&partnerID=40&md5=2e6ee326eae802374924c1cc91135fc5","With several bands covering iron-bearing mineral spectral features, Sentinel-2 has advantages for iron mapping. However, due to the inconsistent spatial resolution, the sensitivity of Sentinel-2 data to detect iron-bearing minerals may be decreased by excluding the 60 m bands and neglecting the 20 m vegetation red-edge bands. Hence, the capability of Sentinel-2 for iron-bearing minerals mapping were assessed by applying a multivariate (MV) method to pansharpen Sentinel-2 data. Firstly, the Sentinel-2 bands with spatial resolution 20 m and 60 m (except band 10) were pansharpened to 10 m. Then, extraction of iron-bearing minerals from the MV-fused image was explored in the Cuprite area, Nevada, USA. With the complete set of 12 bands with a fine spatial resolution, three band ratios (6/1, 6/8A and (6 + 7)/8A) of the fused image were proposed for the extraction of hematite + goethite, hematite + jarosite and the mixture of iron-bearing minerals, respectively. Additionally, band ratios of Sentinel-2 data for iron-bearing minerals in previous studies were modified with substitution of narrow near infrared band 8A for band 8. Results demonstrated that the capability for detection of iron-bearing minerals using Sentinel-2 data was improved by consideration of two extra bands and the unified fine spatial resolution. © 2020 by the authors.","Extraction; Hematite; Image processing; Image resolution; Infrared devices; Mineral exploration; Photomapping; Band ratios; Fused images; Iron-bearing minerals; Near infrared band; Nevada , USA; Pan-sharpened; Spatial resolution; Spectral feature; Iron","Cuprite; Image fusion; Iron-bearing minerals; Sentinel-2","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092081852"
"Li W.; Liu L.; Zhang J.","Li, Wanwu (36598732600); Liu, Lin (57194444751); Zhang, Jixian (56058752000)","36598732600; 57194444751; 56058752000","Fusion of SAR and Optical Image for Sea Ice Extraction","2021","Journal of Ocean University of China","20","6","","1440","1450","10","10.1007/s11802-021-4824-y","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103348455&doi=10.1007%2fs11802-021-4824-y&partnerID=40&md5=e97016e1f8541b4394aae48ce0cd4573","It is difficult to balance local details and global distribution using a single source image in marine target detection of a large scene. To solve this problem, a technique based on the fusion of optical image and synthetic aperture radar (SAR) image for the extraction of sea ice is proposed in this paper. The Band 2 (B2) image of Sentinel-2 (S2) in the research area is selected as optical image data. Preprocessing on the optical image, such as resampling, projection transformation and format conversion, are conducted to the S2 dataset before fusion. Imaging characteristics of the sea ice have been analyzed, and a new deep learning (DL) model, OceanTDL5, is built to detect sea ices. The fusion of the Sentinel-1 (S1) and S2 images is realized by solving the optimal pixel values based on deriving Poisson Equation. The experimental results indicate that the use of a fused image improves the accuracy of sea ice detection compared with the use of a single data source. The fused image has richer spatial details and a clearer texture compared with the original optical image, and its material sense and color are more abundant. © 2021, Ocean University of China, Science Press and Springer-Verlag GmbH Germany.","","image fusion; optical image; Poisson Equation; SAR image; sea ice detection","Article","Final","","Scopus","2-s2.0-85103348455"
"He J.; Li J.; Yuan Q.; Li H.; Shen H.","He, Jiang (57189887884); Li, Jie (57214207213); Yuan, Qiangqiang (36635300800); Li, Huifang (36782247100); Shen, Huanfeng (8359721100)","57189887884; 57214207213; 36635300800; 36782247100; 8359721100","Spatial-spectral fusion in different swath widths by a recurrent expanding residual convolutional neural network","2019","Remote Sensing","11","19","2203","","","","10.3390/rs11192203","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073414974&doi=10.3390%2frs11192203&partnerID=40&md5=a38d71235e40354c49e174eea90dc35e","The quality of remotely sensed images is usually determined by their spatial resolution, spectral resolution, and coverage. However, due to limitations in the sensor hardware, the spectral resolution, spatial resolution, and swath width of the coverage are mutually constrained. Remote sensing image fusion aims at overcoming the different constraints of remote sensing images, to achieve the purpose of combining the useful information in the different images. However, the traditional spatial-spectral fusion approach is to use data in the same swath width that covers the same area and only considers the mutually constrained conditions between the spectral resolution and spatial resolution. To simultaneously solve the image fusion problems of the swath width, spatial resolution, and spectral resolution, this paper introduces a method with multi-scale feature extraction and residual learning with recurrent expanding. To discuss the sensitivity of convolution operation to different variables of images in different swath widths, we set the sensitivity experiments in the coverage ratio and offset position. We also performed the simulation and real experiments to verify the effectiveness of the proposed framework with the Sentinel-2 data, which simulated the different widths. © 2019 by the authors.","Convolution; Extraction; Feature extraction; Image processing; Image resolution; Recurrent neural networks; Remote sensing; Spectral resolution; Constrained conditions; Convolutional neural network; Multi-scale features; Multispectral images; Recurrent expanding; Remote sensing images; Remotely sensed images; Spatial resolution; Image fusion","Convolutional neural network; Image fusion; Multi-scale feature extraction; Multispectral image; Recurrent expanding; Residual network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85073414974"
"Babua D.K.; Schmidtb M.; Dahmsc T.; Conradd C.","Babua, Dinesh Kumar (57200385846); Schmidtb, Marco (57189234332); Dahmsc, Thosten (57190568320); Conradd, Christopher (57193768034)","57200385846; 57189234332; 57190568320; 57193768034","Impact on quality and processing time due to change in pre-processing operation sequence onmoderate resolution satellite images","2016","Proceedings of the International Astronautical Congress, IAC","0","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016500426&partnerID=40&md5=80b825f83168ac2cd214eda2fe144065","Despite having many Earth orbiting remote sensing satellites, even with the completion of Sentinel 2 constellation the revisit time of satellites over a particular area will only come down to 5 days from 16-26 days revisit time of Landsat, SPOT and IRS satellites. This is still far-off from satisfying the need of daily high spatial resolution images required for crop monitoring and rapid changes in ecosystem. Generation of high spatial remote sensing time series by fusing high temporal moderate resolution images obtained from MODIS, MERIS, SPOTVegetation with low temporal high resolution images obtained from Landsat, SPOT, IRS and Sentinels proved to be cost effective and efficient solution. Images obtained from different sensors cannot be used in the image fusion process directly. The images should be first pre-processed to make it consistent with each other in terms of projection system, pixel size. Usually the high temporal moderate resolution images will be reprojected and resampled to match the low temporal high resolution images. Since everyday moderate resolution images are used for the time series generation, this left us with pre-processing huge amount of image data. This makes the pre-processing operation highly time consuming. It also demands huge disk space for data storage and handling and high computing power for quick processing. Several attempts were made to optimize the pre-processing run time and effective data handling. Preprocessing is a sequential process where the image is processed in several steps. In every individual pre-processing step the complete image data is being used for processing. The concerned study area in the image used in the subsequent image fusion process covers only a fraction of the image. This shows that large amount of image data which is not useful in the subsequent process is also being processed in pre-processing operations. This directly translates to longer pre-processing run time. In this paper a novel technique is proposed to optimize the run time by reducing the amount of image data used in the pre-processing steps. This is done by cropping the image in the first place and using the reduced image data in the pre-processing operations. The proposed method is tested using the Geospatial Data Abstraction Library (GDAL). In this paper different pre-processing sequences are applied on the MODIS 500m resolution data. The output from each sequence is analyzed to study the impact due to the change in pre-processing operation sequence and the results are presented. Copyright © 2016 by the International Astronautical Federation (IAF). All rights reserved.","Cost effectiveness; Data handling; Digital storage; Image fusion; Orbits; Radiometers; Remote sensing; Satellites; Space optics; Time series; GDAL; High resolution image; High spatial resolution images; Image preprocessing; Pre-processing; Pre-processing operations; Remote sensing satellites; Time-series generation; Image processing","GDAL; Image fusion; Image pre-processing; Pre-processing framework; Pre-processing sequence","Conference paper","Final","","Scopus","2-s2.0-85016500426"
"Zhou P.; Li X.; Foody G.M.; Boyd D.S.; Wang X.; Ling F.; Zhang Y.; Wang Y.; Du Y.","Zhou, Pu (57225050471); Li, Xiaodong (55878368700); Foody, Giles M. (7007014233); Boyd, Doreen S. (7202871470); Wang, Xia (57193558819); Ling, Feng (56278268300); Zhang, Yihang (55658053900); Wang, Yalan (57225059331); Du, Yun (56420121700)","57225050471; 55878368700; 7007014233; 7202871470; 57193558819; 56278268300; 55658053900; 57225059331; 56420121700","Deep Feature and Domain Knowledge Fusion Network for Mapping Surface Water Bodies by Fusing Google Earth RGB and Sentinel-2 images","2023","IEEE Geoscience and Remote Sensing Letters","","","","1","1","0","10.1109/LGRS.2023.3234306","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147288771&doi=10.1109%2fLGRS.2023.3234306&partnerID=40&md5=ad5badad508578e3fdd853999dac15ff","Mapping surface water bodies from fine spatial resolution optical remote sensing imagery is essential for the understanding of the global hydrologic cycle. Although satellite data are useful for mapping, the limited spectral information captured by some satellite systems can be sub-optimal for the task. For example, the very high resolution images of Google Earth (GE) only contain RGB bands, which often means many water bodies and land objects are confused. Sentinel-2 (S2) imagery have a spectral resolution more suitable for mapping water bodies, but its medium spatial resolution limits the ability for detailed mapping of water-land boundaries. This letter proposes a deep feature and domain knowledge fusion network (DFDKFNet) for mapping surface water bodies by fusing GE and S2 images while incorporating domain knowledge. DFDKFNet uses the remote sensing indices of normalized difference water index (NDWI) and normalized difference vegetation index (NDVI) derived from the S2 image as the representative domain knowledge to better extract water bodies from terrestrial features. A similar pixel-based approach is used to downscaling the NDWI and NDVI maps to match the spatial resolution between the GE and S2 images. The DFDKFNet uses the GE and downscaled NDWI and NDVI images to extract the deep semantic features of water bodies, which are fused with the domain knowledge extracted from the NDWI and NDVI images. DFDKFNet was compared with several state-of-the-art algorithms, and the results show that DFDKFNet can enhance water body mapping accuracy. IEEE","Climate change; Data mining; Domain Knowledge; Extraction; Image fusion; Image resolution; Optical remote sensing; Photomapping; Satellite imagery; Semantics; Surface waters; Deep learning; Domain knowledge; Features extraction; Germaniums (Ge); Google earths; Knowledge fusion; Normalized difference water index; Remote-sensing; Spatial resolution; Waterbodies; Deep learning","Convolution; deep learning; domain knowledge; Feature extraction; Germanium; Remote sensing; Satellites; Sensors; Spatial resolution; Water body","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85147288771"
"Kabolizadeh M.; Rangzan K.; Mousavi S.S.; Azhdari E.","Kabolizadeh, Mostafa (36080758400); Rangzan, Kazem (7801505713); Mousavi, Seyyed Sajedin (57772503500); Azhdari, Ehsan (57772851700)","36080758400; 7801505713; 57772503500; 57772851700","Applying optimum fusion method to improve lithological mapping of sedimentary rocks using sentinel-2 and ASTER satellite images","2022","Earth Science Informatics","15","3","","1765","1778","13","10.1007/s12145-022-00836-1","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133192561&doi=10.1007%2fs12145-022-00836-1&partnerID=40&md5=43865186b609e0d62166084776f25cde","This study was set the yield a precise map of geological formations appliying satellite image fusion techniques. Geologic maps are one of the most valuable sources for understanding the geological conditions of a region used to show the spread of different rock and soil outcrops. In the present research, the fusion of ASTER and Sentinel-2 images were applied using Brovey Transform (BT), Gram-Schmidt (GS), Color Normalized (CN), Smoothing Filter-based Intensity Modulation (SFIM) and Discrete Wavelet Transform (DWT) methods to prepare the lithological map. Based on the results of the employed methods, DWT and BT methods are good in terms of providing spectral and spatial information, respectively. The results also showed that the SFIM method has appropriate spectral and spatial accuracy. At the classification stage, all fused images were processed through supervised classification algorithm of Support Vector Machine (SVM) to identify and separate the geological formations of the study area. The evaluation of the classifications results demonstrated that the SVM method used in the fused images by SFIM method, with 83.16 overall accuracy and 0.82 kappa coefficient has evidential results for lithological mapping. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","ASTER; lithology; mapping method; numerical method; satellite imagery; sedimentary rock; Sentinel; supervised classification","ASTER; Image fusion; Lithological mapping; Sentinel-2","Article","Final","","Scopus","2-s2.0-85133192561"
"Ciotola M.; Ragosta M.; Poggi G.; Scarpa G.","Ciotola, M. (57239080700); Ragosta, M. (57482714100); Poggi, G. (7005255639); Scarpa, G. (7004081145)","57239080700; 57482714100; 7005255639; 7004081145","A FULL-RESOLUTION TRAINING FRAMEWORK FOR SENTINEL-2 IMAGE FUSION","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","1260","1263","3","10.1109/IGARSS47720.2021.9553199","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126048737&doi=10.1109%2fIGARSS47720.2021.9553199&partnerID=40&md5=46a1a617adc1e855caea5f9b48a22303","This work presents a new unsupervised framework for training deep learning models for super-resolution of Sentinel-2 images by fusion of its 10-m and 20-m bands. The proposed scheme avoids the resolution downgrade process needed to generate training data in the supervised case. On the other hand, a proper loss that accounts for cycle-consistency between the network prediction and the input components to be fused is proposed. Despite its unsupervised nature, in our preliminary experiments the proposed scheme has shown promising results in comparison to the supervised approach. Besides, by construction of the proposed loss, the resulting trained network can be ascribed to the class of multi-resolution analysis methods. © 2021 IEEE.","","Convolutional neural network; Data-fusion; Machine learning; Sentinel-2; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85126048737"
"Armannsson S.E.; Sigurdsson J.; Sveinsson J.R.; Ulfarsson M.O.","Armannsson, Sveinn E. (57224686207); Sigurdsson, Jakob (7006736374); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","57224686207; 7006736374; 7003642214; 6507677875","TUNING PARAMETER SELECTION FOR SENTINEL-2 SHARPENING USING WALD'S PROTOCOL","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","2871","2874","3","10.1109/IGARSS47720.2021.9553346","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108172775&doi=10.1109%2fIGARSS47720.2021.9553346&partnerID=40&md5=fc17329332698064df5d7ce376df6a56","In recent years numerous model-based methods for super-resolution of Sentinel-2 (S2) multispectral images have been suggested. Super-resolution aims to enhance the resolution of a captured image by upscaling and enhancing the details. The performance of model-based methods relies on carefully selecting regularizers and tuning parameters. This paper investigates whether using Wald's protocol, i.e., selecting tuning parameters at reduced-resolution, translates to a good performance at a full-scale. To investigate this, we use the recently proposed S2Sharp method and show that selecting its tuning parameters using Wald's protocol improves its performance. © 2021 IEEE.","","Image fusion; Image sharpening; Multispectral (MS) multiresolution images; Parameter selection; Scale invariance; Sentinel-2 constellation; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85108172775"
"Wang Q.; Blackburn G.A.; Onojeghuo A.O.; Dash J.; Zhou L.; Zhang Y.; Atkinson P.M.","Wang, Qunming (55649569623); Blackburn, George Alan (7201722804); Onojeghuo, Alex O. (36170984100); Dash, Jadunandan (7005262634); Zhou, Lingquan (57193897664); Zhang, Yihang (55658053900); Atkinson, Peter M. (7201906181)","55649569623; 7201722804; 36170984100; 7005262634; 57193897664; 55658053900; 7201906181","Fusion of Landsat 8 OLI and Sentinel-2 MSI Data","2017","IEEE Transactions on Geoscience and Remote Sensing","55","7","7894218","3885","3899","14","10.1109/TGRS.2017.2683444","109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017468496&doi=10.1109%2fTGRS.2017.2683444&partnerID=40&md5=440c8558d18e060053fbc5c25f551a14","Sentinel-2 is a wide-swath and fine spatial resolution satellite imaging mission designed for data continuity and enhancement of the Landsat and other missions. The Sentinel-2 data are freely available at the global scale, and have similar wavelengths and the same geographic coordinate system as the Landsat data, which provides an excellent opportunity to fuse these two types of satellite sensor data together. In this paper, a new approach is presented for the fusion of Landsat 8 Operational Land Imager and Sentinel-2 Multispectral Imager data to coordinate their spatial resolutions for continuous global monitoring. The 30 m spatial resolution Landsat 8 bands are downscaled to 10 m using available 10 m Sentinel-2 bands. To account for the land-cover/land-use (LCLU) changes that may have occurred between the Landsat 8 and Sentinel-2 images, the Landsat 8 panchromatic (PAN) band was also incorporated in the fusion process. The experimental results showed that the proposed approach is effective for fusing Landsat 8 with Sentinel-2 data, and the use of the PAN band can decrease the errors introduced by LCLU changes. By fusion of Landsat 8 and Sentinel-2 data, more frequent observations can be produced for continuous monitoring (this is particularly valuable for areas that can be covered easily by clouds, thereby, contaminating some Landsat or Sentinel-2 observations), and the observations are at a consistent fine spatial resolution of 10 m. The products have great potential for timely monitoring of rapid changes. © 2017 IEEE.","Monitoring; Continuous monitoring; Data continuity; Geographic coordinates; Global monitoring; Operational land imager; Satellite imaging; Satellite sensor data; Spatial resolution; downscaling; image analysis; land cover; land use; Landsat; multispectral image; satellite data; satellite imagery; satellite sensor; Sentinel; spatial resolution; Image resolution","Downscaling; global monitoring; image fusion; Landsat 8 Operational Land Imager (OLI); Sentinel-2 Multispectral Imager (MSI)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85017468496"
"Gašparović M.; Rumora L.; Miler M.; Medak D.","Gašparović, Mateo (36987936900); Rumora, Luka (57211063553); Miler, Mario (57086384000); Medak, Damir (26642614700)","36987936900; 57211063553; 57086384000; 26642614700","Effect of fusing Sentinel-2 and WorldView-4 imagery on the various vegetation indices","2019","Journal of Applied Remote Sensing","13","3","036503","","","","10.1117/1.JRS.13.036503","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072518110&doi=10.1117%2f1.JRS.13.036503&partnerID=40&md5=208c404c271a4618fa31c545877ec9a3","Satellite image fusion techniques have been used for more than two decades. Development of satellite sensors for very high-resolution satellite imagery monitoring contributed to the development of new image fusion techniques. We examine the quality of Ehlers, Brovey transform, modified intensity-hue-saturation (M-IHS), and high-filter resolution merge fusion methods on vegetation indices values combining high-resolution WorldView-4 and low-resolution Sentinel-2 imagery. For image fusion, four bands-blue, green, red, and near-infrared-were used. The effect of fusing Sentinel-2 and WorldView-4 imagery is tested on the various vegetation indices [normalized difference vegetation index (NDVI), blue normalized difference vegetation index (BNDVI), and green normalized difference vegetation index (GNDVI)]. M-IHS fusion showed the best result, with the least difference between indices calculated based on the original Sentinel-2 and fused bands. Difference between vegetation indices calculated using original Sentinel-2 and M-IHS fused bands for 30.08.2015, 30.09.2016, and 30.09.2017 is between-0.041 to 0.037. The second part of this research is an evaluation of three different vegetation classes (grassland, mixed vegetation, and forest) on three different years and three different months within a year. Difference between NDVI, GNDVI, and BNDVI calculated based on the original Sentinel-2, and M-IHS fused bands are ranging from-0.043 to 0.103,-0.041 to 0.050, and-0.052 to 0.030 for grassland, mixed vegetation, and forest, respectively. On the other hand, the difference between vegetation indices for grassland, mixed vegetation, and forest are from-0.110 to 0.050,-0.041 to 0.038, and-0.052 to 0.038, when observing August, September, and October of 2017, respectively. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Forestry; Infrared devices; Remote sensing; Satellite imagery; Vegetation; Green normalized difference vegetation index; Image fusion techniques; Intensity hue saturations; Normalized difference vegetation index; Sentinel-2; Vegetation index; Very high resolution satellite imagery; WorldView-4; Image fusion","image fusion; remote sensing; Sentinel-2; vegetation indices; WorldView-4","Article","Final","","Scopus","2-s2.0-85072518110"
"Benedetti P.; Ienco D.; Gaetano R.; Ose K.; Pensa R.G.; Dupuy S.","Benedetti, Paola (57204511715); Ienco, Dino (25027558600); Gaetano, Raffaele (23491959900); Ose, Kenji (26967944200); Pensa, Ruggero G. (8984218100); Dupuy, Stephane (36004740600)","57204511715; 25027558600; 23491959900; 26967944200; 8984218100; 36004740600","                         M                         3                         Fusion: A Deep Learning Architecture for Multiscale Multimodal Multitemporal Satellite Data Fusion                     ","2018","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","12","8516352","4939","4949","10","10.1109/JSTARS.2018.2876357","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055877123&doi=10.1109%2fJSTARS.2018.2876357&partnerID=40&md5=521586202aa1a2448e5914b587ba04fa","Modern Earth Observation systems provide remote sensing data at different temporal and spatial resolutions. Among all the available spatial mission, today the Sentinel-2 program supplies high temporal (every five days) and high spatial resolution (HSR) (10 m) images that can be useful to monitor land cover dynamics. On the other hand, very HSR (VHSR) imagery is still essential to figure out land cover mapping characterized by fine spatial patterns. Understanding how to jointly leverage these complementary sources in an efficient way when dealing with land cover mapping is a current challenge in remote sensing. With the aim of providing land cover mapping through the fusion of multitemporal HSR and VHSR satellite images, we propose a suitable end-To-end deep learning framework, namely M3text{Fusion}, which is able to simultaneously leverage the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR images. Experiments carried out on the Reunion Island study area confirm the quality of our proposal considering both quantitative and qualitative aspects. © 2008-2012 IEEE.","Mascarene Islands; Reunion; Data fusion; Deep learning; Feature extraction; Image fusion; Image resolution; Knowledge management; Remote sensing; Satellite imagery; Satellites; Time series analysis; Land cover mapping; Satellite images; sentinel-2; Spatial resolution; Very high spatial resolutions; data quality; land cover; mapping; satellite data; satellite imagery; Sentinel; spatial resolution; time series; Mapping","Data fusion; deep learning; land cover mapping; satellite image time series; sentinel-2; very high spatial resolution (VHSR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055877123"
"Lu P.; Shi W.; Wang Q.; Li Z.; Qin Y.; Fan X.","Lu, Ping (41861974200); Shi, Wenyang (57221999382); Wang, Qunming (55649569623); Li, Zhongbin (56289297800); Qin, Yuanyuan (57202046580); Fan, Xuanmei (12752112800)","41861974200; 57221999382; 55649569623; 56289297800; 57202046580; 12752112800","Co-seismic landslide mapping using Sentinel-2 10-m fused NIR narrow, red-edge, and SWIR bands","2021","Landslides","18","6","","2017","2037","20","10.1007/s10346-021-01636-2","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100901802&doi=10.1007%2fs10346-021-01636-2&partnerID=40&md5=c461492b75bcec65ce1dc45f40a87eb4","An accurate and timely inventory for major landslides triggered by seismic events is essential for hazard assessment and risk governance. Sentinel-2 MultiSpectral Instrument (MSI) data have been increasingly used for co-seismic landslide mapping in recent years. However, there have been few studies exploring the full use of Sentinel-2 near infrared (NIR) narrow, red-edge, and short-wave infrared (SWIR) bands for landslide mapping. In this study, Sentinel-2 10-m fused NIR narrow, red-edge-1, SWIR-1, and SWIR-2 bands were incorporated into the change detection-based Markov random field (CDMRF) that considers the spectral, spatial, and contextual information of landslides. The effectiveness of the developed method was proved in mapping co-seismic landslides triggered by two major earthquakes in the Yarlung Tsangpo Grand Canyon, China, in 2017 and Hokkaido, Japan, in 2018. The results demonstrated that the mapping performances with the fused Sentinel-2 bands were significantly improved for both seismic events. Such improvement is mainly attributed to the more abundant spectral information provided by the fused Sentinel-2 red-edge, NIR narrow, and SWIR bands. The novelties of this study are (1) the preliminary attempt to use the Sentinel-2 10-m fused bands for landslide mapping, and (2) the synergy of Sentinel-2 red-edge, NIR narrow, and SWIR bands for more reliable landslide mapping. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.","China; Hokkaido; Japan; Earthquakes; Infrared devices; Landslides; Mapping; Markov processes; Risk assessment; Contextual information; Hazard Assessment; Landslide mapping; Markov Random Fields; Multispectral instruments; Seismic landslides; Short wave infrared bands; Spectral information; earthquake event; landslide; mapping method; near infrared; seismic method; Sentinel; shortwave radiation; Infrared radiation","Image fusion; Landslide mapping; Near infrared (NIR) narrow; Red-edge; Sentinel-2; Short-wave infrared (SWIR)","Article","Final","","Scopus","2-s2.0-85100901802"
"Yokoya N.; Chan J.C.-W.; Segl K.","Yokoya, Naoto (36440631200); Chan, Jonathan Cheung-Wai (8840429000); Segl, Karl (6602866234)","36440631200; 8840429000; 6602866234","Potential of resolution-enhanced hyperspectral data for mineral mapping using simulated EnMAP and Sentinel-2 images","2016","Remote Sensing","8","3","172","","","","10.3390/rs8030172","103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962520929&doi=10.3390%2frs8030172&partnerID=40&md5=f37696e71a249d4008f2ce434e6801c3","Spaceborne hyperspectral images are useful for large scale mineral mapping. Acquired at a ground sampling distance (GSD) of 30 m, the Environmental Mapping and Analysis Program (EnMAP) will be capable of putting many issues related to environment monitoring and resource exploration in perspective with measurements in the spectral range between 420 and 2450 nm. However, a higher spatial resolution is preferable for many applications. This paper investigates the potential of fusion-based resolution enhancement of hyperspectral data for mineral mapping. A pair of EnMAP and Sentinel-2 images is generated from a HyMap scene over a mining area. The simulation is based on well-established sensor end-to-end simulation tools. The EnMAP image is fused with Sentinel-2 10-m-GSD bands using a matrix factorization method to obtain resolution-enhanced EnMAP data at a 10 m GSD. Quality assessments of the enhanced data are conducted using quantitative measures and continuum removal and both show that high spectral and spatial fidelity are maintained. Finally, the results of spectral unmixing are compared with those expected from high-resolution hyperspectral data at a 10 m GSD. The comparison demonstrates high resemblance and shows the great potential of the resolution enhancement method for EnMAP type data in mineral mapping. © 2016 by the authors.","Factorization; Image fusion; Mapping; Minerals; Spectroscopy; End-to-end simulation; EnMAP; Environment monitoring; Ground sampling distances; Mineral mapping; Quantitative measures; Resolution enhancement; Sentinel-2; Photomapping","EnMAP; Image fusion; Mineral mapping; Sentinel-2","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-84962520929"
"Gencay S.; Ozcan C.","Gencay, Semih (57904390300); Ozcan, Caner (55856886900)","57904390300; 55856886900","The Effect of SAR Speckle Removal in SAR-Optical Image Fusion; [SAR-Optik Görüntü Füzyonunda SAR Benek Gürültüsünün Giderilme Etkisi]","2022","2022 30th Signal Processing and Communications Applications Conference, SIU 2022","","","","","","","10.1109/SIU55565.2022.9864861","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138694159&doi=10.1109%2fSIU55565.2022.9864861&partnerID=40&md5=4dc523fa66b7b3c4688429c4c9c7734f","Due to the imaging mechanism of Synthetic Aperture Radar (SAR) and the noise in the images, visual identification of objects in the scene is not as easy as in optical images. SAR images have limited color information and cannot reflect the spectral information of objects. Optical images, on the other hand, have rich spectral information. SAR-Optical image fusion is an important area of study so that SAR data can be easily evaluated by anyone, but it is difficult to find a matching SAR and optical image of the same scene. In order to overcome this difficulty, Sentinel-1 and Sentinel-2 datasets have been published and image fusion studies have been carried out with various methods. However, it has been observed that the effect of SAR noise removal before merging on image fusion methods has not been investigated. In the studies conducted to investigate this effect, five different fusion algorithms used in the literature were tested with twenty different image groups using different noise reduction ratios. The success of the fusion results obtained was compared with five different metrics that are widely used in the literature. The images and metric results obtained as a result of the tests showed that the removal of speckle noise in the SAR data has a positive effect on the fusion results. © 2022 IEEE.","Geometrical optics; Image fusion; Noise abatement; Optical remote sensing; Radar imaging; Color information; Imaging mechanism; Matchings; Optical image; Radar data; Remote-sensing; Speckle removal; Spectral information; Synthetic aperture radar images; Visual identification; Synthetic aperture radar","image fusion; optical image; remote sensing; SAR image","Conference paper","Final","","Scopus","2-s2.0-85138694159"
"Datta U.","Datta, U. (7007098861)","7007098861","Multimodal change monitoring using multitemporal satellite images","2021","Proceedings of SPIE - The International Society for Optical Engineering","11862","","118620M","","","","10.1117/12.2600099","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118562976&doi=10.1117%2f12.2600099&partnerID=40&md5=b8701ca19db314f013b32048a2075cc9","The main objective of this study is to monitor the land infrastructure growth over a period of time using multimodality of remote sensing satellite images. In this project unsupervised change detection analysis using ITPCA (Iterated Principal Component Analysis) is presented to indicate the continuous change occurring over a long period of time. The change monitoring is pixel based and multitemporal. Co-registration is an important criteria in pixel based multitemporal image analysis. The minimization of co-registration error is addressed considering 8-neighborhood pixels. Comparison of results of ITPCA analysis with LRT (likelihood ratio test) and GLRT (generalized likelihood ratio test) methods used for SAR and MS (Multispectral) images respectively in earlier publications are also presented in this paper. The datasets of Sentinel-2 around 0-3 days of the acquisition of Sentinel-1 are used for multimodal image fusion. SAR and MS both have inherent advantages and disadvantages. SAR images have the advantage of being insensitive to atmospheric and light conditions, but it suffers the presence of speckle phenomenon. In case of multispectral, challenge is to get quite a large number of datasets without cloud coverage in region of interest for multivariate distribution modelling.  © 2021 SPIE.","Image analysis; Image fusion; Image segmentation; Large dataset; Pixels; Radar imaging; Remote sensing; Synthetic aperture radar; Change detection; Coregistration; Generalized Likelihood Ratio Test; Iterated principal component analyse; Likelihood ratio tests; Multi-modal; Multi-spectral; Principal-component analysis; SAR; SAR Images; Principal component analysis","Change detection; GLRT; ITPCA; LRT; Multimodal; Multispectral; SAR","Conference paper","Final","","Scopus","2-s2.0-85118562976"
"Herrero-Huerta M.; Lagüela S.; Alfieri S.M.; Menenti M.","Herrero-Huerta, M. (56293994900); Lagüela, S. (57204187670); Alfieri, S.M. (55800560800); Menenti, M. (7006145109)","56293994900; 57204187670; 55800560800; 7006145109","Generating high-temporal and spatial resolution tir image data","2017","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2W7","","737","741","4","10.5194/isprs-archives-XLII-2-W7-737-2017","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031024691&doi=10.5194%2fisprs-archives-XLII-2-W7-737-2017&partnerID=40&md5=e28e592152d2c93f954b36790bf7f3b8","Remote sensing imagery to monitor global biophysical dynamics requires the availability of thermal infrared data at high temporal and spatial resolution because of the rapid development of crops during the growing season and the fragmentation of most agricultural landscapes. Conversely, no single sensor meets these combined requirements. Data fusion approaches offer an alternative to exploit observations from multiple sensors, providing data sets with better properties. A novel spatio-temporal data fusion model based on constrained algorithms denoted as multisensor multiresolution technique (MMT) was developed and applied to generate TIR synthetic image data at both temporal and spatial high resolution. Firstly, an adaptive radiance model is applied based on spectral unmixing analysis of. TIR radiance data at TOA (top of atmosphere) collected by MODIS daily 1-km and Landsat - TIRS 16-day sampled at 30-m resolution are used to generate synthetic daily radiance images at TOA at 30-m spatial resolution. The next step consists of unmixing the 30 m (now lower resolution) images using the information about their pixel land-cover composition from co-registered images at higher spatial resolution. In our case study, TIR synthesized data were unmixed to the Sentinel 2 MSI with 10 m resolution. The constrained unmixing preserves all the available radiometric information of the 30 m images and involves the optimization of the number of landcover classes and the size of the moving window for spatial unmixing. Results are still being evaluated, with particular attention for the quality of the data streams required to apply our approach.","Constrained optimization; Data fusion; Image processing; Image resolution; Infrared radiation; Quality control; Remote sensing; Sensor data fusion; Agricultural landscapes; Constrained algorithms; Multi-resolution techniques; Remote sensing imagery; Spatio-temporal data; Spectral unmixing; Temporal and spatial; Thermal infrared data; Image fusion","Data fusion; Image processing; Multisensor system; Remote sensing; Spectral unmixing; Thermal infrared data","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85031024691"
"Wang Q.; Shi W.; Li Z.; Atkinson P.M.","Wang, Qunming (55649569623); Shi, Wenzhong (7402664815); Li, Zhongbin (56289297800); Atkinson, Peter M. (7201906181)","55649569623; 7402664815; 56289297800; 7201906181","Fusion of Sentinel-2 images","2016","Remote Sensing of Environment","187","","","241","252","11","10.1016/j.rse.2016.10.030","137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992090962&doi=10.1016%2fj.rse.2016.10.030&partnerID=40&md5=95fcd536a1573efe288f6a1537ee531c","Sentinel-2 is a very new programme of the European Space Agency (ESA) that is designed for fine spatial resolution global monitoring. Sentinel-2 images provide four 10 m bands and six 20 m bands. To provide more explicit spatial information, this paper aims to downscale the six 20 m bands to 10 m spatial resolution using the four directly observed 10 m bands. The outcome of this fusion task is the production of 10 Sentinel-2 bands with 10 m spatial resolution. This new fusion problem involves four fine spatial resolution bands, which is different to, and more complex than, the common pan-sharpening fusion problem which involves only one fine band. To address this, we extend the existing two main families of image fusion approaches (i.e., component substitution, CS, and multiresolution analysis, MRA) with two different schemes, a band synthesis scheme and a band selection scheme. Moreover, the recently developed area-to-point regression kriging (ATPRK) approach was also developed and applied for the Sentinel-2 fusion task. Using two Sentinel-2 datasets released online, the three types of approaches (eight CS and MRA-based approaches, and ATPRK) were compared comprehensively in terms of their accuracies to provide recommendations for the task of fusion of Sentinel-2 images. The downscaled ten-band 10 m Sentinel-2 datasets represent important and promising products for a wide range of applications in remote sensing. They also have potential for blending with the upcoming Sentinel-3 data for fine spatio-temporal resolution monitoring at the global scale. © 2016","Europe; Image resolution; Interpolation; Remote sensing; Component substitution; Down-scaling; European Space Agency; Image fusion approach; Regression-kriging; Sentinel-2; Spatial informations; Spatio-temporal resolution; downscaling; image analysis; image resolution; kriging; regression analysis; satellite imagery; Sentinel; spatial resolution; Image fusion","Area-to-point regression kriging (ATPRK); Downscaling; Image fusion; Sentinel-2","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-84992090962"
"El Sobky M.A.; Madani A.A.; Surour A.A.","El Sobky, Mohamed A. (57219978507); Madani, Ahmed A. (8558600100); Surour, Adel A. (6505984679)","57219978507; 8558600100; 6505984679","Spectral characterization of the Batuga granite pluton, South Eastern Desert, Egypt: influence of lithological and mineralogical variation on ASD Terraspec data","2020","Arabian Journal of Geosciences","13","23","1246","","","","10.1007/s12517-020-06282-x","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096400855&doi=10.1007%2fs12517-020-06282-x&partnerID=40&md5=abb35a4d6900054016d62178342d845e","The paper concerns with potential utilization of Landsat-8 data combined with TerraSpec spectro-radiometer and field studies for mapping and discriminating the different granitic varieties of Batuga pluton in the South Eastern Desert of Egypt. Integrated data of band combination, principal component analysis (PCA), band rationing, fusion technique, and TerraSpec measurements substantially improve visual interpretation to differentiate between granite varieties as well as basalt dikes traversing the pluton. Compilation of field, petrographic, and spectral data enables to distinguish three varieties of granite, namely alkali feldspar granite, garnet-bearing leucogranite, and two-mica granite (monzogranite). False color composite (FCC) image PC1, PC3, and PC4 in R, G, and B, respectively, is successful to discriminate the western part of Batuga pluton as mica granite (monzogranite) but it failed to discriminate the other two granite types. On the other hand, the band ratio combination 6/7, 5/6, and 2/4 successfully used to discriminate the three varieties of granite. Also, the image fusion technique of false color composite (7, 5, & 2) and band ratio (6/7, 5/6, & 2/4) combinations with the high spatial resolution band 2 of Sentinel-2 is applied using the Gram-Schmidt method. Regarding with TerraSpec measurements, the alkali feldspar granite displays two absorption features around 1.4 and 2.25 μm. Moreover, the garnet-bearing leucogranite shows four little absorption features at 1.4, 2.21, 2.25, and 2.33 μm, while the mica granite shows low reflectance values throughout the VNIR and SWIR regions compared to the other two varieties, and low four absorption features at 1.4, 2.21, 2.25, and 2.33 μm wavelengths. The basalt dikes show a low, nearly flat spectral profile in the VNIR and SWIR wavelength regions and small four absorption features also observed around the 0.65, 1.0, 1.2, and 2.32 μm wavelengths. Data materialized in the paper suggests the usefulness of Landsat-8 and TerraSpec spectroradiometer data as a powerful combination to discriminate the granitic rocks and the traversing basalt dikes as well. © 2020, Saudi Society for Geosciences.","Eastern Desert; Egypt; alkali feldspar; feldspar; Landsat; lithology; mineralogy; pluton; radiometer; satellite data; spatial resolution; spectral analysis; Terra (satellite)","ASD Terraspec data; Batuga granite; Geological mapping; Lithological discrimination; Mineralogy","Article","Final","","Scopus","2-s2.0-85096400855"
"Wendl C.; Bris A.L.; Chehata N.; Puissant A.; Postadjian T.","Wendl, Cyril (57204814998); Bris, Arnaud Le (22135725700); Chehata, Nesrine (8598290000); Puissant, Anne (7102002323); Postadjian, Tristan (57201383948)","57204814998; 22135725700; 8598290000; 7102002323; 57201383948","Late fusion SPOT 6/7 images and multitemporal Sentinel-2 data for the detection of the urban area; [Fusion tardive d'lmages spot 6/7 et de donnees multitemporelles sentinel-2 pour la detection de la tache urbaine]","2018","Revue Francaise de Photogrammetrie et de Teledetection","2018-September","217-218","","87","97","10","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057313899&partnerID=40&md5=a79e1d29c4c3efdbd3d0a249234fd887","Fusion of very high spatial resolution multispectral (VHR) images with time series of lower spatial resolution images with more spectral bands can improve land cover classification, combining geometric and semantic advantages of both sources, respectively. This study presents a strategy to extract the urban footprint using decision-level fusion of individual classifications on Sentinel-2 and SPOT 6/7 satellite images. First, both sources are classified separately in five classes, using state-of-the-art supervised Random Forest classification and Convolutional Neural Networks. The two results are then merged in order to extract individual buildings with the highest spatial precision conceivable. Secondly, detected buildings are merged again with the Sentinel-2 classification so as to extract the artificialized areas; the building labels from the regularization step are dilated in order to connect the building objects; a binary classification is derived from the original Sentinel-2 classification before being reintroduced in a fusion and contrast sensitive regularization process so as to eventually retrieve the urban footprint. Results show well the complementary between both data sources as well as the relevance of the late fusion stategy. © 2018 Soc. Francaise de Photogrammetrie et de Teledetection.","Classification (of information); Decision trees; Image enhancement; Image resolution; Image segmentation; Neural networks; Semantics; Artificialized Area; Decision fusion; Late fusion; Multi-spectral; Regularization; Urban areas; Image fusion","Artificialized Area; Classification; Decision Fusion; Late Fusion; Multispectral; Regularization; Segmentation; Urban Area","Article","Final","","Scopus","2-s2.0-85057313899"
"Wendl C.; Le Bris A.; Chehata N.; Puissant A.; Postadjian T.","Wendl, Cyril (57204814998); Le Bris, Arnaud (22135725700); Chehata, Nesrine (8598290000); Puissant, Anne (7102002323); Postadjian, Tristan (57201383948)","57204814998; 22135725700; 8598290000; 7102002323; 57201383948","Late fusion of SPOT-6/7 images and Sentinel-2 multi-temporal data for urban area detection; [FUSION TARDIVE D'IMAGES SPOT 6/7 ET DE DONNÉES MULTITEMPORELLES SENTINEL-2 POUR LA DÉTECTION DE LA TACHE URBAINE]","2018","Revue Francaise de Photogrammetrie et de Teledetection","","217-218","","","","","10.52638/rfpt.2018.415","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142401136&doi=10.52638%2frfpt.2018.415&partnerID=40&md5=223b3f8df02723d6702ff7849db70b5d","Fusion of very high spatial resolution multispectral (VHR) images with time series of lower spatial resolution images with more spectral bands can improve land cover classification, combining geometric and semantic advantages of both sources, respectively. This study presents a strategy to extract the urban footprint using decision-level fusion of individual classifications on Sentinel-2 and SPOT 6/7 satellite images. First, both sources are classified separately in five classes, using state-of-the-art supervised Random Forest classification and Convolutional Neural Networks. The two results are then merged in order to extract individual buildings with the highest spatial precision conceivable. Secondly, detected buildings are merged again with the Sentinel-2 classification so as to extract the artificialized areas; the building labels from the regularization step are dilated in order to connect the building objects; a binary classification is derived from the original Sentinel-2 classification before being reintroduced in a fusion and contrast sensitive regularization process so as to eventually retrieve the urban footprint. Results show well the complementary between both data sources as well as the relevance of the late fusion stategy. © 2022 Authors. All rights reserved.","Convolutional neural networks; Image classification; Image enhancement; Image fusion; Image resolution; Satellite imagery; Semantic Segmentation; Semantics; Area detection; Artificialized area; Decisions fusion; Late fusion; Multi-spectral; Multi-temporal data; Regularisation; Segmentation; Urban areas; Very-high spatial resolutions; Decision trees","Artificialized area; Classification; Decision fusion; Late fusion; Multispectral; Regularization; Segmentation; Urban area","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142401136"
"Li Y.; Shi Q.; He L.; Cai R.; Meng L.; Li J.; Plaza A.","Li, Yunfei (57218424664); Shi, Qian (55286447700); He, Lin (57192205017); Cai, Runlin (57283915000); Meng, Liangli (57608633700); Li, Jun (24481713500); Plaza, Antonio (7006613644)","57218424664; 55286447700; 57192205017; 57283915000; 57608633700; 24481713500; 7006613644","Fusing Sentinel-2 and Landsat-8 Surface Reflectance Data via Pixel-Wise Local Normalization","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","7359","7374","15","10.1109/JSTARS.2022.3200713","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137601727&doi=10.1109%2fJSTARS.2022.3200713&partnerID=40&md5=c66727ec77a41b6594c93b71a0549738","Medium spatial resolution surface reflectance image series from the combination of Landsat-8 Operational Land Imager and Sentinel-2 Multispectral Imager observations have great importance to the land surface monitoring tasks, for which great efforts have been paid for blending the two data. However, most of the efforts focus on the image series with spatial resolution of 30 m, which cannot meet the data demand of some applications. Therefore, it is necessary to fuse Landsat-8 and Sentinel-2 images to provide 10-m image series. Currently, there are three means to achieve that, including the area-to-point regression kriging fusion approach (ATPRK), spatiotemporal fusion methods, and deep-learning-based fusion models. However, the ATPRK and spatiotemporal fusion methods suffer from the limited fusion performance, while the deep-learning-based fusion models are hardware dependent, i.e., requiring the graphics processing units, which may not be satisfied sometimes. To address these issues, in this article, we develop a new pixel-wise local normalization-based fusion method (LN-FM) for fusing Sentinel-2 and Landsat-8 images. The newly proposed LN-FM is compared to the ATPRK and three representative spatiotemporal fusion methods in experiments, which use imagery collected from both a rural area and an urban area. The experimental results demonstrate that the newly developed LN-FM exhibits excellent qualitative and quantitative performance, as well as remarkable spatial, spectral, and pixel distribution fidelity. Furthermore, this approach is fast, which may improve its applicability  © 2008-2012 IEEE.","Computer graphics; Deep learning; Frequency modulation; Image fusion; Image resolution; Pixels; Program processors; Reflection; Remote sensing; LANDSAT; Landsat-8; Local normalization; Normalisation; Remote sensing image fusion; Remote sensing images; Remote-sensing; Sentinel-2; Spatial resolution; Spatiotemporal phenomenon; Task analysis; image processing; Landsat; pixel; remote sensing; satellite data; Sentinel; spatial resolution; surface reflectance; Landsat","Landsat-8; local normalization; remote sensing image fusion; Sentinel-2","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137601727"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","CNN-Based Fusion and Classification of Multi-Temporal Sentinel-1 & -2 Satellite Data","2021","2021 IEEE India Geoscience and Remote Sensing Symposium, InGARSS 2021 - Proceedings","","","","57","60","3","10.1109/InGARSS51564.2021.9791998","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133213710&doi=10.1109%2fInGARSS51564.2021.9791998&partnerID=40&md5=5ce29fb89f3df345bcfc7d43834c1c0c","SAR and optical data are widely used in image fusion to provide the complimentary information of each other and obtain the spatial and spectral features for improved classifications. This paper proposes to use multi-temporal data form Sentinel-1 (VV & VH polarization) and Sentinel-2 sensors for the fusion and classification over an agricultural area. Convolutional Neural Network (CNN)- based Pyramid method for fusion and Bayesian Optimized 2-D CNN for classification of fused multi-temporal data was used to extract spatial-spectral information. Results in terms of classification accuracy suggests slightly better performance by VV polarized fused images than the VH and also suggests an improved performance by multi-temporal data in comparison to the single date data over the study area.  © 2021 IEEE.","Convolution; Convolutional neural networks; Image enhancement; Image fusion; Remote sensing; Bayesian optimization; Convolutional neural network; Multi-temporal; Multi-temporal data; Network-based; Performance; SAR data; Satellite data; Sentinel-1; Classification (of information)","Bayesian Optimization; Classification; Convolutional Neural Network (CNN); Fusion","Conference paper","Final","","Scopus","2-s2.0-85133213710"
"Wang S.; Yang X.; Li G.; Jin Y.; Tian C.","Wang, Shengbo (57889670100); Yang, Xiufeng (57396526000); Li, Guohong (55714233200); Jin, YongTao (57193738879); Tian, Chuanzhao (57889571800)","57889670100; 57396526000; 55714233200; 57193738879; 57889571800","Research on spatio-temporal fusion algorithm of remote sensing image based on GF-1 WFV and Sentinel-2 satellite data","2022","2022 3rd International Conference on Geology, Mapping and Remote Sensing, ICGMRS 2022","","","","667","678","11","10.1109/ICGMRS55602.2022.9849377","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137974963&doi=10.1109%2fICGMRS55602.2022.9849377&partnerID=40&md5=e264da05e12c5cdae3622809a77c953a","The remote sensing data set with high spatial and temporal resolution is of great significance for monitoring surface change. However, the earth observation satellites at domestic and international cannot obtain high spatial resolution and high temporal resolution images at the same time. Remote sensing data spatio-temporal fusion technology is an effective means to solve this problem. In this paper, the multi-period GaoFen-1 WideField-View (GF-1 WFV) satellite images and Sentinel-2 satellite images of Beijing-Tianjin-Hebei region in 2020 are used to perform fusion simulation for the three spatio-temporal fusion algorithms of Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM), Enhanced-STARFM (ESTARFM) and Enhanced Flexible Spatiotemporal Data Fusion Model (EFSDAF) and these simulation degree of the results are quantitatively evaluated. The results show that ESTARFM algorithm is more suitable for the construction of high spatio-temporal fusion data of GF-1 WFV and Sentinel-2 satellites in Beijing-Tianjin-Hebei region.  © 2022 IEEE.","Image fusion; Remote sensing; Satellites; Enhanced flexible spatiotemporal data fusion model; Enhanced-STARFM; Fusion model; Gaofen-1 widefield-view image; Spatial and temporal adaptive reflectance fusion model; Spatio-temporal data; Temporal adaptive; Temporal and spatial; Temporal and spatial fusion; Wide-field view; Image enhancement","EFSDAF; ESTARFM; GF-1 WFV images; STARFM; Temporal and spatial fusion","Conference paper","Final","","Scopus","2-s2.0-85137974963"
"Attarzadeh R.; Bagheri H.; Khosravi I.; Niazmardi S.; Akbari D.","Attarzadeh, Reza (55532398700); Bagheri, Hossein (56153105600); Khosravi, Iman (57193664527); Niazmardi, Saeid (55603790800); Akbari, Davood (56663326900)","55532398700; 56153105600; 57193664527; 55603790800; 56663326900","Segment-based fusion of multi-sensor multi-scale satellite soil moisture retrievals","2022","Remote Sensing Letters","13","12","","1260","1270","10","10.1080/2150704X.2022.2142486","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142264771&doi=10.1080%2f2150704X.2022.2142486&partnerID=40&md5=f978f485315e88b17f41ea03faf62b24","Synergetic use of sensors for soil moisture retrieval is attracting considerable interest due to the different advantages of different sensors. Active, passive, and optic data integration could be a comprehensive solution for exploiting the advantages of different sensors aimed at preparing soil moisture maps. Typically, pixel-based methods are used for multi-sensor fusion. Since, different applications need different scales of soil moisture maps, pixel-based approaches are limited for this purpose. Object-based image analysis employing an image object instead of a pixel could help us to meet this need. This paper proposes a segment-based image fusion framework to evaluate the possibility of preparing a multi-scale soil moisture map through integrated Sentinel-1, Sentinel-2, and Soil Moisture Active Passive (SMAP) data. The results confirmed that the proposed methodology was able to improve soil moisture estimation in different scales up to 20% better compared to pixel-based fusion approach. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Data integration; Image fusion; Image segmentation; Moisture control; Pixels; Active/passive; Multi sensor; Multi-scales; Segment-based; Segmentation; Sentinel-1; Sentinel-2; Soil moisture active passive; Soil moisture maps; Soil moisture retrievals; image analysis; satellite data; segmentation; Sentinel; soil moisture; Soil moisture","Multi-scale; segmentation; Sentinel-1; Sentinel-2; SMAP; soil moisture map","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142264771"
"Wu K.; Gu L.; Jiang M.","Wu, Kunpeng (57295001700); Gu, Lingjia (15834718400); Jiang, Mingda (57295437600)","57295001700; 15834718400; 57295437600","Research on fusion of SAR image and multispectral image using texture feature information","2021","Proceedings of SPIE - The International Society for Optical Engineering","11829","","1182916","","","","10.1117/12.2592925","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116960462&doi=10.1117%2f12.2592925&partnerID=40&md5=3a0be947723402dbeafc6fd8ba86a391","Remote sensing images have the characteristics of multiple data sources and complex data. How to integrate remote sensing image information more efficiently has always been the focus of research. In this paper, Changchun City, Jilin Province, China was selected as the experimental area, Sentinel-1 and Sentinel-2 images were used as experimental data, and a fusion method of SAR image and multispectral image using texture feature information was proposed. First, perform HIS transformation on the multi-spectral image to obtain the intensity image. After that, wavelet transform was used to extract the high-frequency and low-frequency detail components of the intensity image. At the same time, the principal component analysis method and the deep learning network VGG-19 were used to extract the texture features of the SAR image. The SAR texture image was used to enhance the high-frequency detail component of the intensity image, and combined with the original low-frequency detail component to perform inverse wavelet transform, then a new intensity image was obtained. Finally, the modulated intensity image was used to replace the original intensity image, and the inverse transformation (I-HIS) was performed to obtain an enhanced image fused from the multispectral image and the SAR image. Compared with the original image, the detailed features and boundary distinction were significantly improved. The fusion image was input into the support vector machine for feature classification, and the comprehensive classification accuracy reached 94.74%, which was 3.5% higher than the classification accuracy of the unfused image. © 2021 SPIE.","Classification (of information); Deep learning; Image compression; Image enhancement; Image fusion; Image texture; Principal component analysis; Radar imaging; Remote sensing; Spectroscopy; Support vector machines; Synthetic aperture radar; Textures; Wavelet transforms; Classification accuracy; Deep learning algorithm; Feature information; High frequency HF; Intensity images; Lower frequencies; Multispectral images; Remote sensing images; SAR Images; Texture features; Learning algorithms","Deep learning algorithm; Image fusion; Multispectral image; SAR image","Conference paper","Final","","Scopus","2-s2.0-85116960462"
"Fernandez-Beltran R.; Haut J.M.; Paoletti M.E.; Plaza J.; Plaza A.; Pla F.","Fernandez-Beltran, Ruben (55838551300); Haut, Juan M. (57215636081); Paoletti, Mercedes E. (57027389000); Plaza, Javier (57195716301); Plaza, Antonio (7006613644); Pla, Filiberto (7006504936)","55838551300; 57215636081; 57027389000; 57195716301; 7006613644; 7006504936","Multimodal probabilistic latent semantic analysis for Sentinel-1 and Sentinel-2 image fusion","2018","IEEE Geoscience and Remote Sensing Letters","15","9","8392415","1347","1351","4","10.1109/LGRS.2018.2843886","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048871759&doi=10.1109%2fLGRS.2018.2843886&partnerID=40&md5=d7774707dbb8e26268524c1ae8c47fc8","Probabilistic topic models have recently shown a great potential in the remote sensing image fusion field, which is particularly helpful in land-cover categorization tasks. This letter first studies the application of probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation to remote sensing synthetic aperture radar (SAR) and multispectral imaging (MSI) unsupervised land-cover categorization. Then, a novel pLSA-based image fusion approach is presented, which pursues to uncover multimodal feature patterns from SAR and MSI data in order to effectively fuse and categorize Sentinel-1 and Sentinel-2 remotely sensed data. Experiments conducted over two different data sets reveal the advantages of the proposed approach for unsupervised land-cover categorization tasks. © 2018 IEEE.","Data structures; Feature extraction; Image analysis; Probabilistic logics; Radar imaging; Remote sensing; Semantics; Statistics; Synthetic aperture radar; Land cover; Latent Dirichlet allocation; Multispectral imaging; Probabilistic latent semantic analysis; Probabilistic topic models; Remote sensing images; Sentinel-1; Sentinel-2; data set; image analysis; land cover; numerical model; probability; remote sensing; satellite imagery; Sentinel; Image fusion","Image fusion; land-cover categorization; probabilistic latent semantic analysis (pLSA); Sentinel-1; Sentinel-2","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85048871759"
"Monsalve‐tellez J.M.; Torres‐león J.L.; Garcés‐gómez Y.A.","Monsalve‐tellez, Jose Manuel (57541358800); Torres‐león, Jorge Luis (57540078900); Garcés‐gómez, Yeison Alberto (56272275600)","57541358800; 57540078900; 56272275600","Evaluation of SAR and Optical Image Fusion Methods in Oil Palm Crop Cover Classification Using the Random Forest Algorithm","2022","Agriculture (Switzerland)","12","7","955","","","","10.3390/agriculture12070955","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133705613&doi=10.3390%2fagriculture12070955&partnerID=40&md5=90b041c0b92050d070762149b0c2f67a","This paper presents an evaluation of land cover accuracy, particularly regarding oil palm crop cover, using optical/synthetic aperture radar (SAR) image fusion methods through the implementation of the random forest (RF) algorithm on cloud computing platforms using Sentinel‐1 SAR and Sentinel‐2 optical images. Among the fusion methods evaluated were Brovey (BR), high‐frequency modulation (HFM), Gram–Schmidt (GS), and principal components (PC). This work was developed using a cloud computing environment employing R and Python for statistical analysis. It was found that an optical/SAR image stack resulted in the best overall accuracy with 82.14%, which was 11.66% higher than that of the SAR image, and 7.85% higher than that of the optical image. The high‐frequency modulation (HFM) and Brovey (BR) image fusion methods showed overall accuracies higher than the Sentinel‐2 optical image classification by 3.8% and 3.09%, respectively. This demonstrates the potential of integrating optical imagery with Sentinel SAR imagery to increase land cover classification accuracy. On the other hand, the SAR images obtained very high accuracy results in classifying oil palm crops and forests, reaching 94.29% and 90%, respectively. This demonstrates the ability of synthetic aperture radar (SAR) to provide more information when fused with an optical image to improve land cover classification. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","cloud computing; image fusion; land cover classification; optical images; random forest (RF); synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133705613"
"Zhang J.; Zhang D.; Ma W.; Jiao L.","Zhang, Jiaqi (57211379266); Zhang, Dan (57193607682); Ma, Wenping (57205878746); Jiao, Licheng (7102491544)","57211379266; 57193607682; 57205878746; 7102491544","Deep Self-Paced Residual Network for Multispectral Images Classification Based on Feature-Level Fusion","2018","IEEE Geoscience and Remote Sensing Letters","15","11","8424470","1740","1744","4","10.1109/LGRS.2018.2854847","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050975707&doi=10.1109%2fLGRS.2018.2854847&partnerID=40&md5=6f0a8495d39d73399261488c0d481244","The classification methods based on fusion techniques of multisource multispectral (MS) images have been studied for a long time. However, it may be difficult to classify these data based on a feature level while avoiding the inconsistency of data caused by multisource and multiple regions or cities. In this letter, we propose a deep learning structure called 2-branch SPL-ResNet which combines the self-paced learning with deep residual network to classify multisource MS data based on the feature-level fusion. First, a 2-D discrete wavelet is used to obtain the multiscale features and sparse representation of MS data. Then, a 2-branch SPL-ResNet is established to extract respective characteristics of the two satellites. Finally, we implement the feature-level fusion by cascading the two feature vectors and then classify the integrated feature vector. We conduct the experiments on Landsat-8 and Sentinel-2 MS images. Compared with the commonly used classification methods such as support vector machine and convolutional neural networks, our proposed 2-branch SPL-ResNet framework has higher accuracy and more robustness. © 2018 IEEE.","Deep learning; Image classification; Image fusion; Image retrieval; Neural networks; Classification methods; Convolutional neural network; Deep residual network (ResNet); Feature level fusion; Multi-scale features; Multi-spectral; Self-paced learning; Sparse representation; algorithm; image classification; image processing; Landsat; multispectral image; Sentinel; support vector machine; vector; wavelet analysis; Classification (of information)","Deep residual network (ResNet); image classification; image fusion; multisource multispectral (MS) data; self-paced learning (SPL)","Article","Final","","Scopus","2-s2.0-85050975707"
"Tarigan D.G.P.; Isa S.M.","Tarigan, Dietrich G. P. (57340290600); Isa, Sani M. (57216658927)","57340290600; 57216658927","A PSNR Review of ESTARFM Cloud Removal Method with Sentinel 2 and Landsat 8 Combination","2021","International Journal of Advanced Computer Science and Applications","12","9","","189","198","9","10.14569/IJACSA.2021.0120923","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119195752&doi=10.14569%2fIJACSA.2021.0120923&partnerID=40&md5=0269576fd8531e747be187bd5d44ff0d","Remote sensing images with high spatial and temporal resolution (HSHT) for GIS land use monitoring are crucial data sources. When trying to get HSHT resolution images, cloud cover is a typical problem. The effects of cloud cover reduction using the ESTARFM, one of spatiotemporal image fusion technique, is examined in this study. By merging two satellite photos of low-resolution and medium-resolution images, the Enhanced Spatial and Temporal Adaptive Reflectance Fusion Method (ESTARFM), predicts the reflectance value of the cloud cover region. ESTARFM, on the other hand, employs both medium and high-resolution satellite pictures in this study. Using Sentinel 2 and Landsat 8, the Peak Signal Noise Ratio (PSNR) statistical methods are then utilized to evaluate the ESTARFM. The PSNR explain ESTARFM cloud removal performance by comparing the level of similarity of the reference image with the reconstructed image. In remote sensing, this hypothesis was established to get high-quality HSHT pictures. Based on this study, Landsat 8 images that have been cloud removed with ESTARFM may be classed as good. The PSNR value of 21.8 to 26 backs this up, and the ESTARFM result seems good on visual examination. © 2021. All Rights Reserved.","Geographic information systems; Image enhancement; Image fusion; Land use; Remote sensing; Spurious signal noise; Cloud removal; Fusion methods; GIS-geographic information system; Peak signal noise ratio; Peak signal noise ratio-peak signal noise ratio; Remote-sensing; RS-remote sensing; Spatiotemporal image fusion; Spatiotemporal images; Reflection","Cloud removal; GIS-Geographic information system; PSNR-Peak signal noise ratio; RS-Remote sensing; spatiotemporal image fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119195752"
"Feng X.; Shao Z.; Huang X.; He L.; Lv X.; Zhuang Q.","Feng, Xiaoxiao (57209449320); Shao, Zhenfeng (57203905559); Huang, Xiao (57201292422); He, Luxiao (57201260628); Lv, Xianwei (57203917799); Zhuang, Qingwei (57212874323)","57209449320; 57203905559; 57201292422; 57201260628; 57203917799; 57212874323","Integrating Zhuhai-1 Hyperspectral Imagery With Sentinel-2 Multispectral Imagery to Improve High-Resolution Impervious Surface Area Mapping","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","2410","2424","14","10.1109/JSTARS.2022.3157755","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126325131&doi=10.1109%2fJSTARS.2022.3157755&partnerID=40&md5=14a748c54eac4d14f28e460accabdf11","Mapping impervious surface area (ISA) in an accurate and timely manner is essential for a variety of fields and applications, such as urban heat islands, hydrology, waterlogging, and urban planning and management. However, the large and complex urban landscapes pose great challenges in retrieving ISA information. Spaceborne hyperspectral (HS) remote sensing imagery provides rich spectral information with short revisit cycles, making it an ideal data source for ISA extraction from complex urban scenes. Nevertheless, insufficient single-band energy, the involvement of modulation transfer function (MTF), and the low signal-to-noise ratio (SNR) of spaceborne HS imagery usually result in poor image clarity and noises, leading to inaccurate ISA extraction. To address this challenge, we propose a new deep feature fusion-based classification method to improve 10 m resolution ISA mapping by integrating Zhuhai-1 HS imagery with Sentinel-2 multispectral (MS) imagery. We extract deep features that include spectral and spatial features, respectively, from MS and HS imagery via a 2-D convolutional neural network (CNN), aiming to increase feature diversity and improve the model's recognition capability. The Sentinel-2 imagery is used to enhance the spatial information of the Zhuhai-1 HS image, improving the urban ISA retrieval by reducing the impact of noises. By combining the deep spatial features and deep spectral features, we obtain joint spatial-spectral features, leading to high classification accuracy and robustness. We test the proposed method in two highly urbanized study areas that cover Foshan city and Wuhan city, China. The results reveal that the proposed method obtains an overall accuracy of 96.72% and 96.75% in the two study areas, 18.78% and 8.66% higher than classification results with only HS imagery as input. The final ISA extraction overall accuracy is 95.42% and 95.50% in the two study areas, the highest among the comparison methods. © 2008-2012 IEEE.","Classification (of information); Convolution; Extraction; Feature extraction; Hyperspectral imaging; Image enhancement; Image fusion; Mapping; Neural networks; Signal to noise ratio; Spectroscopy; Support vector machines; Urban planning; Area mapping; Convolutional neural network; Features extraction; Features fusions; Impervious surface area; Impervious surface area mapping; Sentinel-2 imagery; Space-borne; Spatial resolution; Support vectors machine; Zhuhai-1 spaceborne hyperspectral imagery; artificial neural network; digital mapping; satellite data; satellite imagery; Sentinel; spatial resolution; Remote sensing","Convolutional neural network (CNN); Feature fusion; Impervious surface area (ISA) mapping; Sentinel-2 imagery; Zhuhai-1 spaceborne hyperspectral (HS) imagery","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126325131"
"Nguyen H.V.; Ulfarsson M.O.; Sveinsson J.R.; Mura M.D.","Nguyen, Han V. (57222240069); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214); Mura, Mauro Dalla (57218455473)","57222240069; 6507677875; 7003642214; 57218455473","Sentinel-2 Sharpening Using a Single Unsupervised Convolutional Neural Network with MTF-Based Degradation Model","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9464640","6882","6896","14","10.1109/JSTARS.2021.3092286","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111144938&doi=10.1109%2fJSTARS.2021.3092286&partnerID=40&md5=7e5370c6a46cbd450007d030f7a0df17","The Sentinel-2 (S2) constellation provides multispectral images at 10 m, 20 m, and 60 m resolution bands. Obtaining all bands at 10 m resolution would benefit many applications. Recently, many model-based and deep learning (DL)-based sharpening methods have been proposed. However, the downside of those methods is that the DL-based methods need to be trained separately for the 20 m and the 60 m bands in a supervised manner at reduced resolution, while the model-based methods heavily depend on the hand-crafted image priors. To break the gap, this article proposes a novel unsupervised DL-based S2 sharpening method using a single convolutional neural network (CNN) to sharpen the 20 and 60 m bands at the same time at full resolution. The proposed method replaces the hand-crafted image prior by the deep image prior (DIP) provided by a CNN structure whose parameters are easily optimized using a DL optimizer. We also incorporate the modulation transfer function-based degradation model as a network layer, and add all bands to both network input and output. This setting improves the DIP and exploits the advantage of multitask learning since all S2 bands are highly correlated. Extensive experiments with real S2 data show that our proposed method outperforms competitive methods for reduced-resolution evaluation and yields very high quality sharpened image for full-resolution evaluation.  © 2008-2012 IEEE.","Convolution; Convolutional neural networks; Deep learning; Multi-task learning; Network layers; Quality control; Degradation model; Full resolutions; Highly-correlated; Model-based method; Model-based OPC; Multispectral images; Network inputs; Reduced resolution; artificial neural network; degradation; experimental study; image resolution; multispectral image; remote sensing; Sentinel; Learning systems","Convolutional neural networks (CNNs); image fusion; MTF-based degradation; Sentinel-2 image sharpening; super-resolution; unsupervised deep learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111144938"
"Hunger S.; Karrasch P.; Wessollek C.","Hunger, Sebastian (57188579309); Karrasch, Pierre (55948092400); Wessollek, Christine (56725814600)","57188579309; 55948092400; 56725814600","Evaluating the potential of image fusion of multispectral and radar remote sensing data for the assessment of water body structure","2016","Proceedings of SPIE - The International Society for Optical Engineering","9998","","999814","","","","10.1117/12.2241264","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011093380&doi=10.1117%2f12.2241264&partnerID=40&md5=adc330b9fa957f2595651008ce3230e7","The European Water Framework Directive (Directive 2000/60/EC) is a mandatory agreement that guides the member states of the European Union in the field of water policy to fulfill the requirements for reaching the aim of the good ecological status of water bodies. In the last years several workflows and methods were developed to determine and evaluate the characteristics and the status of the water bodies. Due to their area measurements remote sensing methods are a promising approach to constitute a substantial additional value. With increasing availability of optical and radar remote sensing data the development of new methods to extract information from both types of remote sensing data is still in progress. Since most limitations of these data sets do not agree the fusion of both data sets to gain data with higher spectral resolution features the potential to obtain additional information in contrast to the separate processing of the data. Based thereupon this study shall research the potential of multispectral and radar remote sensing data and the potential of their fusion for the assessment of the parameters of water body structure. Due to the medium spatial resolution of the freely available multispectral Sentinel-2 data sets especially the surroundings of the water bodies and their land use are part of this study. SAR data is provided by the Sentinel-1 satellite. Different image fusion methods are tested and the combined products of both data sets are evaluated afterwards. The evaluation of the single data sets and the fused data sets is performed by means of a maximum-likelihood classification and several statistical measurements. The results indicate that the combined use of different remote sensing data sets can have an added value. © 2016 SPIE.","Agriculture; Classification (of information); Data fusion; Data handling; Ecology; Ecosystems; Environmental regulations; Hydrology; Image fusion; Land use; Maximum likelihood; Radar; Radar imaging; Synthetic aperture radar; Water conservation; Water management; European Water Framework Directive; Extract informations; Good ecological status; Image fusion methods; Maximum likelihood classifications; Radar remote sensing; Sentinel 2; Sentinel-1; Remote sensing","Image fusion; Maximum likelihood; Sentinel 1; Sentinel 2","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85011093380"
"Nguyen H.V.; Ulfarsson M.O.; Sveinsson J.R.","Nguyen, Han V. (57222240069); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214)","57222240069; 6507677875; 7003642214","SHARPENING THE 20 M BANDS OF SENTINEL-2 IMAGE USING AN UNSUPERVISED CONVOLUTIONAL NEURAL NETWORK","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","2875","2878","3","10.1109/IGARSS47720.2021.9555082","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128986256&doi=10.1109%2fIGARSS47720.2021.9555082&partnerID=40&md5=e7015a2e83d14d555904ea2217e81173","This paper proposes a novel method for sharpening the 20 m bands of the multispectral images acquired by the Sentinel- 2 (S2) constellation. We formulate the S2 sharpening as an inverse problem and solve it using an unsupervised convolutional neural network (CNN), called S2UCNN. The proposed method extends the deep image prior provided by a CNN structure with S2 domain knowledge. We incorporate a modulation transfer function-based degradation model as a network layer. We add the 10 m bands to both the network input and output to take advantage of the multitask learning. Experimental results with a real S2 dataset show that the proposed method outperforms the competitive methods on reduced-resolution data and gives very high quality sharpened image on full-resolution data.  © 2021 IEEE.","Convolution; Convolutional neural networks; Domain Knowledge; Inverse problems; Network layers; Remote sensing; Convolutional neural network; Image priors; Multispectral images; Neural networks structure; Novel methods; Remote-sensing; Sentinel-2; Sharpening; Superresolution; Unsupervised convolutional neural network; Image fusion","image fusion; Remote sensing; Sentinel-2; sharpening; super-resolution; unsupervised convolutional neural network","Conference paper","Final","","Scopus","2-s2.0-85128986256"
"Wang X.; Ren H.","Wang, Xueliang (57190819621); Ren, Honge (24781057700)","57190819621; 24781057700","DBMF: A Novel Method for Tree Species Fusion Classification Based on Multi-Source Images","2022","Forests","13","1","33","","","","10.3390/f13010033","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122432848&doi=10.3390%2ff13010033&partnerID=40&md5=3e2cc84bfe27a0fd9070b11ef79bd934","Multi-source data remote sensing provides innovative technical support for tree species recognition. Tree species recognition is relatively poor despite noteworthy advancements in image fusion methods because the features from multi-source data for each pixel in the same region cannot be deeply exploited. In the present paper, a novel deep learning approach for hyperspectral imagery is proposed to improve accuracy for the classification of tree species. The proposed method, named the double branch multi-source fusion (DBMF) method, could more deeply determine the relationship between multi-source data and provide more effective information. The DBMF method does this by fusing spectral features extracted from a hyperspectral image (HSI) captured by the HJ-1A satellite and spatial features extracted from a multispectral image (MSI) captured by the Sentinel-2 satellite. The network has two branches in the spatial branch to avoid the risk of information loss, of which, sandglass blocks are embedded into a convolutional neural network (CNN) to extract the corresponding spatial neighborhood features from the MSI. Simultaneously, to make the useful spectral feature transfer more effective in the spectral branch, we employed bidirectional long short-term memory (Bi-LSTM) with a triple attention mechanism to extract the spectral features of each pixel in the HSI with low resolution. The feature information is fused to classify the tree species after the addition of a fusion activation function, which could allow the network to obtain more interactive information. Finally, the fusion strategy allows for the prediction of the full classification map of three study areas. Experimental results on a multi-source dataset show that DBMF has a significant advantage over other state-of-the-art frameworks. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification; Data; Forestry; Fusion; Remote Sensing; Resolution; Spectroscopy; Trees; Classification (of information); Forestry; Image classification; Image enhancement; Image fusion; Long short-term memory; Pixels; Spectroscopy; Trees (mathematics); Deep learning fusion method; Fusion methods; Images classification; Multi-source fusion; Multi-source image classification; Multi-source images; Multisource data; Species classification; Tree species; Tree species classification; accuracy assessment; image classification; imaging method; machine learning; multispectral image; pattern recognition; Sentinel; tree; Remote sensing","Deep learning fusion method; Multi-source images classification; Tree species classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122432848"
"Wang M.; Meng X.; Shao F.; Fu R.","Wang, Mengyao (57210968797); Meng, Xiangchao (56158755000); Shao, Feng (7006717672); Fu, Randi (14821950200)","57210968797; 56158755000; 7006717672; 14821950200","SAR-Assisted Optical Remote Sensing Image Cloud Removal Method Based on Deep Learning; [基于深度学习的SAR辅助下光学遥感图像去云方法]","2021","Guangxue Xuebao/Acta Optica Sinica","41","12","1228002","","","","10.3788/AOS202141.1228002","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113278742&doi=10.3788%2fAOS202141.1228002&partnerID=40&md5=9b89574ba859279c7a4f3fd7c4491b52","The existing deep learning based SAR-assisted cloud removal methods do not take full into account the texture and spectral information of the optical images, which results in blurring and spectral loss. In this paper, we constructed a data set for SAR-assisted cloud removal based on the Sentinel-1 and Sentinel-2 satellite images in Yuhang District of Hangzhou. In addition, we established a conditional generative adversarial network (cGAN) based model by fully considering the details, texture, and color information of optical remote sensing images, achieving information recovery and reconstruction in the case of optical images covered by thin clouds, fog, and thick clouds. The results show that the proposed method outperforms other methods in SAR-assisted cloud removal. © 2021, Chinese Lasers Press. All right reserved.","Geometrical optics; Image texture; Learning systems; Radar imaging; Remote sensing; Textures; Adversarial networks; Cloud removal; Color information; Information recovery; Optical image; Optical remote sensing; Satellite images; Spectral information; Deep learning","Cloud removal; Conditional generative adversarial network (cGAN); Image fusion; Optical image; Remote sensing; Synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85113278742"
"Suarez P.; Medina J.","Suarez, Paola (57219662309); Medina, Javier (57197825929)","57219662309; 57197825929","Location of tourist areas: Green areas and natural parks in the city of Bucaramanga and the metropolitan area from the fusion of sentinel2b and landsat-8 satellite images; [Localización de zonas turísticas: Zonas verdes y parques naturales en la ciudad de bucaramanga y área metropolitana a partir de fusión de imágenes satelitales sentinel-2b y landsat-8]","2020","RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao","2020","E36","","270","281","11","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094614082&partnerID=40&md5=7da47b9df5b935fc2f872aadb076adc6","In this article, a thematic map is generated with the location of green areas and natural parks in the city of Bucaramanga and its metropolitan area of Girón from satellite images of free use from fusion methods and the wavelet haar transform in order to enhance the benefits of the spatial and spectral characteristics of these and obtain a thematic map that allows locating green areas and natural parks in the city of Bucaramanga and its metropolitan area of Girón in order to promote ecotourism in the city and show the advantages of this type of free access images. The city of Bucaramanga has always been recognized as the beautiful city of parks, with a large number of green areas of great attraction for tourists who often do not know where these areas are located, which is why with this map thematic it is very easy to locate the parks in order to structure a tourist trip to these areas. To carry out this thematic map, a mathematical introduction to implement the image fusion using the wavelet transform will first be addressed. The second theme corresponds to the implementation of the mathematical development of the wavelet transform using Matlab software and Sentinel-2 and Landsat-8 satellite images. The third theme corresponds to the analysis and evaluation of the transformations to determine their efficiency and the generation of the final map. Obtaining an updated map, with free access images, ideal to locate the green areas of the city of Bucaramanga and Giron and encourage tourism to these points of great interest. © 2020, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved.","","Green areas; Image fusion; Parks; Tourism; Wavelet Transformation","Article","Final","","Scopus","2-s2.0-85094614082"
"Wu J.; Lin L.; Zhang C.; Li T.; Cheng X.; Nan F.","Wu, Jingan (57208125591); Lin, Liupeng (57188711703); Zhang, Chi (57758435700); Li, Tongwen (56813980500); Cheng, Xiao (7401754355); Nan, Fang (58030684200)","57208125591; 57188711703; 57758435700; 56813980500; 7401754355; 58030684200","Generating Sentinel-2 all-band 10-m data by sharpening 20/60-m bands: A hierarchical fusion network","2023","ISPRS Journal of Photogrammetry and Remote Sensing","196","","","16","31","15","10.1016/j.isprsjprs.2022.12.017","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145352306&doi=10.1016%2fj.isprsjprs.2022.12.017&partnerID=40&md5=74bb6f371ead7c84edfee02f65a71fee","Earth observations from the Sentinel-2 mission have been extensively accepted in a variety of land services. The thirteen spectral bands of Sentinel-2, however, are collected at three spatial resolutions of 10/20/60 m, and such a difference brings difficulties to analyze multispectral imagery at a uniform resolution. To address this problem, we developed a hierarchical fusion network (HFN) to sharpen 20/60-m bands and generate Sentinel-2 all-band 10-m data. The deep learning architecture is used to learn the complex mapping between multi-resolution input and output data. Given the deficiency of previous studies in which the spatial information is inferred only from the fine-resolution bands, the proposed hierarchical fusion framework simultaneously leverages the self-similarity information from coarse-resolution bands and the spatial structure information from fine-resolution bands, to enhance the sharpening performance. Technically, the coarse-resolution bands are super-resolved by exploiting the information from themselves and then sharpened by fusing with the fine-resolution bands. Both 20-m and 60-m bands can be sharpened via the developed approach. Experimental results regarding visual comparison and quantitative assessment demonstrate that HFN outperforms the other benchmarking models, including pan-sharpening-based, model-based, geostatistical-based, and other deep-learning-based approaches, showing remarkable performance in reproducing explicit spatial details and maintaining original spectral features. Moreover, the developed model works more effectively than the other models over the heterogeneous landscape, which is usually considered a challenging application scenario. To sum up, the fusion model can sharpen Sentinel-2 20/60-m bands, and the created all-band 10-m data allows image analysis and geoscience applications to be authentically carried out at the 10-m resolution. © 2022 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Benchmarking; Convolutional neural networks; Deep learning; Image analysis; Coarser resolution; Convolutional neural network; Earth observations; Fine resolution; Hierarchical fusions; Image sharpening; Performance; Sentinel-2; Spatial resolution; Spectral band; architecture; EOS; image analysis; mapping; multispectral image; Sentinel; spatial resolution; Image fusion","Convolutional neural network; Image fusion; Image sharpening; Sentinel-2","Article","Final","","Scopus","2-s2.0-85145352306"
"Alboody A.; Puigt M.; Roussel G.; Vantrepotte V.; Jamet C.; Tran T.-K.","Alboody, Ahed (24528545800); Puigt, Matthieu (9132941600); Roussel, Gilles (57197306458); Vantrepotte, Vincent (22954813300); Jamet, Cédric (8600546000); Tran, Trung-Kien (57217442266)","24528545800; 9132941600; 57197306458; 22954813300; 8600546000; 57217442266","Deepsen3: Deep Multi-Scale Learning Model for Spatial-Spectral Fusion of Sentinel-2 and Sentinel-3 Remote Sensing Images","2022","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2022-September","","","","","","10.1109/WHISPERS56178.2022.9955139","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143137132&doi=10.1109%2fWHISPERS56178.2022.9955139&partnerID=40&md5=ae93d14c297f67122101f7ca3ece2eb1","Recently, deep learning methods that integrate image features gradually became a hot development trend in fusion of multispectral and hyperspectral remote sensing images, aka multi-sharpening. Fusion of a low spatial resolution hyperspectral image (LR-HSI datacube) with its corresponding high spatial resolution multispectral image (HR-MSI datacube) to reconstruct a high spatial resolution hyperspectral image (HR-HSI) has been a significant subject in recent years. Nevertheless, it is still difficult to achieve a high quality of spatial and spectral information fusion. In this paper, we propose a Deep Multi-Scale Learning Model (called DeepSen3) of spatial-spectral information fusion based on multi-scale inception residual convolutional neural network (CNN) for more efficient hyperspectral and multispectral image fusion from ESA remote sensing satellite missions (Sentinel-2 and Sentinel-3 images). The proposed DeepSen3 fusion network was applied to Sentinel-2 MSI (13 spectral bands with a spatial resolution ranging from 10, 20 to 60 m) and Sentinel-3 OLCI (21 spectral bands with a spatial resolution of 300 m) images. Extensive experiments demonstrate that the proposed DeepSen3 network achieves the best performance (both qualitatively and quantitatively) compared with recent state-of-the-art deep learning approaches.  © 2022 IEEE.","Convolution; Convolutional neural networks; Deep learning; Image fusion; Image resolution; Information fusion; Learning systems; Spectroscopy; Convolutional neural network; Deep learning; Features extraction; HyperSpectral; Hyperspectral image; Multi-scale inception; Multi-scales; Multi-spectral image; Multispectral images; Remote sensing images; Residual convolutional neural network (resnet-convolutional neural network); Sentinel-2 and sentinel-3 remote sensing image; Spatial-spectral image fusion; Spectral image fusions; Remote sensing","Deep Learning; Feature Extraction; HyperSpectral Images (HSI); Multi-Scale Inception; Multi-Spectral Images (MSI); Residual Convolutional Neural Network (ResNet-CNN); Sentinel-2 and Sentinel-3 Remote Sensing Images; Spatial-Spectral Image Fusion","Conference paper","Final","","Scopus","2-s2.0-85143137132"
"Nguyen H.V.; Ulfarsson M.O.; Sveinsson J.R.; Sigurdsson J.","Nguyen, Han V. (57222240069); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214); Sigurdsson, Jakob (7006736374)","57222240069; 6507677875; 7003642214; 7006736374","Zero-Shot Sentinel-2 Sharpening Using a Symmetric Skipped Connection Convolutional Neural Network","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323614","613","616","3","10.1109/IGARSS39084.2020.9323614","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102005792&doi=10.1109%2fIGARSS39084.2020.9323614&partnerID=40&md5=ec6e2abccbe671f05365f718490b7c6e","Sentinel-2 (S2) satellite constellations can provide multispectral images of 10 m, 20 m, and 60 m resolution for visible, near-infrared (NIR) and short-wave infrared (SWIR) in the electromagnetic spectrum. In this paper, we present a sharpening method based on a symmetric skipped connection convolutional neural network, called SSC-CNN, to sharpen 20 m bands using 10 m bands. The main advantage of SSC-CNN architecture is that it brings the features of the input branch to the output, thus improving convergence without using too many deep layers. The proposed method uses the reduced-scale combination of 10 m bands and 20 m bands, and the observed 20 m bands as the training pairs. The experimental results using two Sentinel-2 datasets show that our method outperforms competitive methods in quantitative metrics and visualization. © 2020 IEEE.","Convolution; Geology; Infrared devices; Infrared radiation; Remote sensing; Deep layer; Electromagnetic spectra; Multispectral images; Near infra red; Quantitative metrics; Reduced scale; Satellite constellations; Short wave infrared; Convolutional neural networks","convolutional neural network; image fusion; image sharpening; Sentinel-2; super resolution","Conference paper","Final","","Scopus","2-s2.0-85102005792"
"Sujud L.; Jaafar H.; Haj Hassan M.A.; Zurayk R.","Sujud, Lara (57408747300); Jaafar, Hadi (36628550700); Haj Hassan, Mohammad Ali (57408406400); Zurayk, Rami (6701381333)","57408747300; 36628550700; 57408406400; 6701381333","Cannabis detection from optical and RADAR data fusion: A comparative analysis of the SMILE machine learning algorithms in Google Earth Engine","2021","Remote Sensing Applications: Society and Environment","24","","100639","","","","10.1016/j.rsase.2021.100639","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122658971&doi=10.1016%2fj.rsase.2021.100639&partnerID=40&md5=3d9f4bf9164ed2abc5534197031ab5bc","Accurate crop mapping for agricultural monitoring requires the use of robust image classification algorithms. Cultivation of illegal cannabis is common in the Bekaa plain, Lebanon, as in some other areas of the world (Morocco, Afghanistan). Identification of cultivation sites is money and time consuming since it highly relies on knowledge of cultivation areas and field surveys. Machine-learning-based image classification provides an alternative method for cannabis detection. This paper presents a comparative analysis of four machine-learning classifiers implemented in Google Earth Engine: Random Forest (RF), Gradient Boosting (GBT), Classification and Regression Tree (CART), and Support Vector Machine (SVM) for cannabis and other crop type classification. We implement and test several image fusion approaches for optical (Sentinel-2, Landsat) and Synthetic Aperture Radar (SAR) imagery (Sentinel-1) along with several combinations of surface textural features from Sentinel-1. Six different crop groups were classified over three years (2016, 2017, and 2018) in the Bekaa valley of Lebanon. In general, although SVM outperformed RF, GBT, and CART classifiers with an overall classification accuracy of more than 90%, RF and GBT provided more consistent results in terms of the cannabis cultivated area. SVM appears to be sensitive to the size of the training data. RF and GBT estimate an average cannabis area of 2800 ha in 2016, 2983 ha in 2017, and 5900 ha in 2018. Our results demonstrate that the fusion of radar and optical imagery can improve image classification accuracy by up to 5%. A marginal improvement in overall accuracy was observed when textural features were added. This is the first study that used image fusion and ML to estimate illegal cannabis cultivation areas in Lebanon and help evaluate the contribution of cannabis to the local economy. The combination of machine learning algorithms and imagery fusion proved reliable for crop classification in general and cannabis in particular. © 2021 The Authors","","Bekaa valley; Cannabis; CART; Crop classification; GEE; Gradient boosting; Random forest; Support vector machine","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85122658971"
"Benzenati T.; Kessentini Y.; Kallel A.","Benzenati, Tayeb (57216240538); Kessentini, Yousri (16052461400); Kallel, Abdelaziz (22233967900)","57216240538; 16052461400; 22233967900","Spectral-Temporal Fusion of Satellite Images via an End-to-End Two-Stream Attention With an Effective Reconstruction Network","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","1308","1320","12","10.1109/JSTARS.2023.3234722","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147306147&doi=10.1109%2fJSTARS.2023.3234722&partnerID=40&md5=fc0194dea6d3188caaa258f671d7efe9","Due to technical and budget constraints on current optical satellites, the acquisition of satellite images with the best resolutions is not practicable. In this article, aiming to produce products with high spectral (HS) and temporal resolutions, we introduced a two-stream spectral-temporal fusion technique based on attention mechanism called STA-Net. STA-Net aims to combine high spectral and low temporal (HSLT) resolution images with low spectral and high temporal (LSHT) resolution images to generate products with the best characteristics. The proposed technique involves two stages. In the first one, two fused images are generated by a two-stream architecture based on residual attention blocks. The temporal difference estimator stream estimates the temporal difference between HS images at desired and neighboring dates. The reflectance difference estimator is the second stream. It predicts the reflectance difference between the input images (HS-LS) to map LS images into HS products. In the second stage, a reconstruction network combines the latter two-stream outputs via an effective learnable weighted-sum strategy. The two-stage model is trained in an end-to-end fashion using an effective loss function to ensure the best fusion quality. To the best of our knowledge, this work represents the first attempt to address the spectral-temporal fusion using an end-to-end deep neural network model. Experimental results conducted on two actual datasets of Sentinel-2 (HSLT:10 spectral bands and long revisit period) and Planetscope (LSHT: four spectral bands and daily images) images, which proved the effectiveness of the proposed technique with respect to baseline technique. © 2008-2012 IEEE.","Budget control; Deep neural networks; Image fusion; Image reconstruction; Optical remote sensing; Reflection; Satellite imagery; Attention mechanisms; Images reconstruction; Multi sensor images; Multi-sensor image fusion; Planetscope; Remote-sensing; Sensor image fusion; Sentinel-2; Spatial resolution; Spectral-temporal fusion; artificial neural network; image analysis; image resolution; satellite imagery; Sentinel; spectral analysis; Image resolution","Attention mechanism; convolutional neural network (CNN); image fusion; multisensor image fusion; Planetscope; Sentinel-2; spectral-temporal fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85147306147"
"Mileva N.; Mecklenburg S.; Gascon F.","Mileva, Nikolina (57205202523); Mecklenburg, Susanne (6603464568); Gascon, Ferran (7005867979)","57205202523; 6603464568; 7005867979","New tool for spatio-temporal image fusion in remote sensing: A case study approach using Sentinel-2 and Sentinel-3 data","2018","Proceedings of SPIE - The International Society for Optical Engineering","10789","","107890L","","","","10.1117/12.2327091","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059028854&doi=10.1117%2f12.2327091&partnerID=40&md5=13656998104e7f1f1dfb4dd46b0fd799","Remote sensing image fusion allows the spectral, spatial and temporal enhancement of images. New techniques for image fusion are constantly emerging shifting the focus from pan-sharpening to spatiotemporal fusion of data originating from different sensors and platforms. However, the application of image fusion in the field of Earth observation still remains limited. The number and complexity of the different techniques available today can be overwhelming thus preventing users from fully exploiting the potential of fusion. The aim of this study is to make fusion products more accessible to users by providing them with a simple tool for spatiotemporal fusion in Python. This tool will contribute to the better exploitation of data from available sensors making possible to bring the images to the spectral, spatial and temporal resolution required by the user. The fusion algorithm implemented in the tool is based on the spatial and temporal adaptive reflectance fusion model (STARFM) - a well established fusion technique in the field of remote sensing often used as benchmark by other algorithms. The capabilities of the tool are demonstrated by three case studies using Sentinel-2 and simulated Sentinel-3 data. The first case study is about deforestation in the Amazon forest. The other two case studies concentrate on detecting change in an agricultural site in Southern Germany and urban flooding caused by the hurricane Harvey. © 2018 SPIE.","Data fusion; Deforestation; Image enhancement; Reflection; Remote sensing; Urban growth; Case study approach; Remote sensing images; Sentinel-2; Sentinel-3; Spatial and temporal resolutions; Spatio-temporal fusions; Spatiotemporal images; Surface reflectance; Image fusion","Data fusion; Remote sensing; Sentinel-2; Sentinel-3; Surface reflectance","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85059028854"
"Palsson F.; Sveinsson J.R.; Ulfarsson M.O.","Palsson, Frosti (55052918200); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","55052918200; 7003642214; 6507677875","Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","2018","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2018-September","","8747268","","","","10.1109/WHISPERS.2018.8747268","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072027457&doi=10.1109%2fWHISPERS.2018.8747268&partnerID=40&md5=5e989f4449d87f7a978a0f1c7090f7ec","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel2 datasets. © 2018 IEEE.","Convolution; Image processing; Image resolution; Infrared devices; Infrared radiation; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Electromagnetic spectra; Multispectral images; Multispectral sensors; Sentinel-2; Short wave infrared; Spatial resolution; Image fusion","convolutional network; generative adversarial network; Image fusion; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85072027457"
"Cissé C.T.; Alboody A.; Puigt M.; Roussel G.; Vantrepotte V.; Jamet C.; Tran T.K.","Cissé, C.T. (57797817500); Alboody, A. (24528545800); Puigt, M. (9132941600); Roussel, G. (57197306458); Vantrepotte, V. (22954813300); Jamet, C. (8600546000); Tran, T.K. (57217442266)","57797817500; 24528545800; 9132941600; 57197306458; 22954813300; 8600546000; 57217442266","A NEW DEEP LEARNING METHOD FOR MULTISPECTRAL IMAGE TIME SERIES COMPLETION USING HYPERSPECTRAL DATA","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","1546","1550","4","10.1109/ICASSP43922.2022.9747895","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134049776&doi=10.1109%2fICASSP43922.2022.9747895&partnerID=40&md5=000b7f09614904d39e30e0805752f56c","The massive development of remote sensing allowed many novel applications which bring new challenges. In particular, some applications such as marine observation require a good spatial, spectral, and temporal resolution. In order to tackle the last issue, spatio-temporal fusion of remote sensing data allows to complete a time series of multispectral images from, e.g., hyperspectral images. In this paper, we propose a new deep learning approach to that end. Our main contribution lies in the error completion task which allows to improve the completion performance. We show that our proposed method is able to produce high fidelity predictions with better quality indices than state-of-the-art methods on true images taken from the CIA/LGC database and Sentinel-2/Sentinel-3 data. © 2022 IEEE","Deep learning; Image fusion; Learning systems; Marine applications; Spectroscopy; Time series; Deep learning; Hyperspectral Data; Image time-series; Learning methods; Multispectral images; Novel applications; Remote-sensing; Spatio-temporal fusions; Time-series completion; Times series; Remote sensing","Deep Learning; Remote Sensing; Spatio-Temporal Fusion; Time-Series Completion","Conference paper","Final","","Scopus","2-s2.0-85134049776"
"Masiza W.; Chirima J.G.; Hamandawana H.; Pillay R.","Masiza, Wonga (57218918626); Chirima, Johannes George (54388710100); Hamandawana, Hamisai (56616595700); Pillay, Rajendran (57205320422)","57218918626; 54388710100; 56616595700; 57205320422","Enhanced mapping of a smallholder crop farming landscape through image fusion and model stacking","2020","International Journal of Remote Sensing","41","22","","8736","8753","17","10.1080/01431161.2020.1783017","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090785369&doi=10.1080%2f01431161.2020.1783017&partnerID=40&md5=097f27e9333aa1245d836a1dc2a70d90","Globally, Smallholder farming systems (SFS) are recognized as one of the most important pillars of rural economic development and poverty alleviation because of their contribution to food security. However, support for this agricultural sector is hampered by lack of reliable information on the distributions and acreage of smallholder fields. This information is essential in not only monitoring food security and informing markets but also in guiding the determination of levels of support required from government by individual farmers. There is urgent need for robust techniques that can be used to cost-effectively and time-efficiently map smallholder crop fields especially in Sub-Saharan Africa and Asia. This study attempts to do this by using an approach in which optical and Synthetic Aperture Radar (SAR) data are systematically combined and classified using Extreme Gradient Boosting (Xgboost). We also investigated model stacking as another technique to improve classification accuracy. We combined Xgboost with Random Forest (RF), Support Vector Machine (SVM), Artificial Neural Networks (ANN), and Naïve Bayes (NB). The combined use of multi-temporal Sentinel-2 bands, spectral indices, and Sentinel-1 produced better results than exclusive use of optical data (α = 0.95, p = 0.0005). Furthermore, stacking of classification algorithms based on model comparisons achieved higher accuracy than stacking the algorithms indiscriminately (α = 0.95, p = 0.0100). Through systematic fusion of SAR and optical data and hyper-parameter tuning of Xgboost, we achieved a maximum classification accuracy of 97.71%, while achieving a maximum accuracy of 96.06% through model stacking. This highlights the importance of multi-sensor data fusion and multi-classifier systems when mapping fragmented agricultural landscapes. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Asia; Sub-Saharan Africa; Agricultural robots; Crops; Decision trees; Food supply; Image enhancement; Image fusion; Mapping; Radar imaging; Support vector machines; Synthetic aperture radar; Agricultural landscapes; Agricultural sector; Classification accuracy; Classification algorithm; Maximum accuracies; Multiclassifier system; Multisensor data fusion; Poverty alleviation; agricultural land; artificial neural network; crop; food security; image processing; mapping method; smallholder; support vector machine; synthetic aperture radar; Sensor data fusion","","Article","Final","","Scopus","2-s2.0-85090785369"
"Nguyen H.V.; Ulfarsson M.O.; Sveinsson J.R.; Dalla Mura M.","Nguyen, Han V. (57222240069); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214); Dalla Mura, Mauro (36499129800)","57222240069; 6507677875; 7003642214; 36499129800","Deep SURE for Unsupervised Remote Sensing Image Fusion","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5412613","","","","10.1109/TGRS.2022.3215902","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140790948&doi=10.1109%2fTGRS.2022.3215902&partnerID=40&md5=50450bb4c7efae9e9fc97ebec914aaae","Image fusion is utilized in remote sensing (RS) due to the limitation of the imaging sensor and the high cost of simultaneously acquiring high spatial and spectral resolution images. Optical RS imaging systems usually provide images of high spatial resolution but low spectral resolution and vice versa. Therefore, fusing those images to obtain a fused image having both high spectral and spatial resolution is desirable in many applications. This article proposes a fusion framework using an unsupervised convolutional neural network (CNN) and Stein's unbiased risk estimate (SURE). We derive a new loss function for a CNN that incorporates a backprojection mean square error (MSE) with SURE to estimate the projected mse between the fused image and the ground truth. The main motivation is that training a CNN with this SURE loss function is unsupervised and avoids overfitting. Experimental results for two fusion examples, multispectral and hyperspectral (MS-HS) image fusion and multispectral and multispectral (MS-MS) image fusion, show that the proposed method yields high-quality fused images and outperforms the competitive methods. Codes are available at https://github.com/hvn2/Deep-SURE-Fusion.  © 1980-2012 IEEE.","Convolution; Image resolution; Mean square error; Neural networks; Optical remote sensing; Risk perception; Spectral resolution; Convolutional neural network; Electronic Packaging; Hyperspectral image fusions; Multi-spectral image fusions; Pan-sharpening; Remote-sensing; Sentinel 2 sharpening; Spatial resolution; Stein’s unbiased risk estimate; Unbiased risk estimates; Unsupervised convolutional neural network; artificial neural network; error analysis; experimental study; image resolution; remote sensing; Sentinel; spatial resolution; unsupervised classification; Image fusion","Image fusion; multispectral and hyperspectral (MS-HS) image fusion; pansharpening; remote sensing (RS); Sentinel 2 sharpening; Stein's unbiased risk estimate (SURE); unsupervised convolutional neural networks (CNNs)","Article","Final","","Scopus","2-s2.0-85140790948"
"Chen R.; Wang X.","Chen, Runyu (57224512790); Wang, Xiaoqing (56048910600)","57224512790; 56048910600","An Effective Cloud Removal Algorithm Based on Sparse Expression","2021","ACM International Conference Proceeding Series","","","","194","199","5","10.1145/3447587.3447616","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107758080&doi=10.1145%2f3447587.3447616&partnerID=40&md5=f7a9e8ae5f536520c7f499df0abc3605","In order to remove the clouds cover in remote sensing images, this paper proposes a clouds removal algorithm based on spatiotemporal fusion and sparse expression. In this paper we select two images in the same area with a short time difference, one of them is covered by clouds, the other clear image is basically not. First, we use some indicators [1] to distinguish clouds and clear ground to get classified images, such as NDVI(Normalized Difference Vegetation Index), and EVI(Enhanced Vegetation Index). Then we select the cloudless regions in both images to train sparse expression dictionary. For the cloudy regions, we find the corresponding area in the cloudless area in the other image, in which we perform dictionary decomposition and get a sparse representation [2], [3]. We utilize the dictionary of cloudy image to map the sparse representation to the corresponding area in cloudy image, and replace the original cloudy area. The fusion image retains the original real information and predicts the type of ground surface under the clouds. The experimental results show that the algorithm has an excellent effect on the removal of thick clouds, and solves the problem of image distortion and grayscale mutations that may be caused by traditional clouds removal algorithms. Sentinel-2 satellite images were used to evaluate the proposed method, and it was compared with other related algorithms, for example, homomorphic filtering and LRMR(low-rank matrix recovery), the experimental results confirm that the proposed method is effective in correcting clouds contaminated images while preserving the true spectral information.  © 2021 ACM.","Genetic algorithms; Image fusion; Information filtering; Remote sensing; Vegetation; Cloud removal algorithms; Enhanced vegetation index; Homomorphic filtering; Low-rank matrix recoveries; Normalized difference vegetation index; Remote sensing images; Sparse representation; Spatio-temporal fusions; Image enhancement","Clouds removal; image algorithm; image fusion; sparse expression","Conference paper","Final","","Scopus","2-s2.0-85107758080"
"Ramírez M.; Martínez L.; Montilla M.; Sarmiento O.; Lasso J.; Diaz S.","Ramírez, M. (57220994474); Martínez, L. (57214231932); Montilla, M. (57220187808); Sarmiento, O. (57220190162); Lasso, J. (57220176809); Diaz, S. (57197656091)","57220994474; 57214231932; 57220187808; 57220190162; 57220176809; 57197656091","Obtaining agricultural land cover in sentinel-2 satellite images with drone image injection using random forest in google earth engine; [Obtención de coberturas del suelo agropecuarias en imágenes satelitales sentinel-2 con la inyección de imágenes de dron usando random forest en google earth engine]","2020","Revista de Teledeteccion","2020","56","","49","68","19","10.4995/raet.2020.14102","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097188346&doi=10.4995%2fraet.2020.14102&partnerID=40&md5=21bf9dd9240706827f2925c8397f9e73","To obtain accurate information on land cover changes in the agricultural sector, we propose a supervised classification method that integrates Sentinel-2 satellite imagery with images surveyed from Remote Piloted Aircraft Systems (RPAS). The methodology was implemented on the Google Earth Engine platform. Initially, the Sentinel-2 imagery collection was integrated into a single image through a median reduction process. Subsequently, the high-pass filter (HPF) pansharpening image fusion method was applied to the thermal spectral bands to obtain a final spatial resolution of 10 m. To perform the integration of the two image sources, the RPAS image was normalized by using a 5X5 gaussian texture filter and the pixel was resampled to five times its original size. This procedure was performed iteratively until reaching the spatial resolution of the Sentinel-2 imagery. Besides, the following inputs were added to the classification: the spectral indices, calculated from the Sentinel-2 and RPAS bands (e.g. NDVI, NDWI, SIPI, GARI), altimetric information and slopes of the zone derived from the SRTM DEM. The supervised classification was done by using the Random Forest technique (Machine Learning). The land cover seed reference to perform the classification was manually captured by a thematic expert, then, this reference was distributed in 70% for the training of the Random Forest algorithm and in 30% to validate the classification. The results show that the incorporation of the RPAS image improves thematic accuracy indicators by an average of 3% compared to a classification made exclusively with Sentinel-2 imagery. © 2020, Universidad Politecnica de Valencia.. All rights reserved.","","Google Earth Engine; Random Forest; RPAS; Sentinel-2; Supervised classification","Note","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85097188346"
"Vaiopoulos A.D.; Karantzalos K.","Vaiopoulos, A.D. (36844619800); Karantzalos, K. (6507214354)","36844619800; 6507214354","Pansharpening on the narrow VNIR and SWIR spectral bands of Sentinel-2","2016","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","41","","","723","730","7","10.5194/isprsarchives-XLI-B7-723-2016","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979529298&doi=10.5194%2fisprsarchives-XLI-B7-723-2016&partnerID=40&md5=80466425c376b89c35b8860d08a9db7a","In this paper results from the evaluation of several state-of-the-art pansharpening techniques are presented for the VNIR and SWIR bands of Sentinel-2. A procedure for the pansharpening is also proposed which aims at respecting the closest spectral similarities between the higher and lower resolution bands. The evaluation included 21 different fusion algorithms and three evaluation frameworks based both on standard quantitative image similarity indexes and qualitative evaluation from remote sensing experts. The overall analysis of the evaluation results indicated that remote sensing experts disagreed with the outcomes and method ranking from the quantitative assessment. The employed image quality similarity indexes and quantitative evaluation framework based on both high and reduced resolution data from the literature didn't manage to highlight/evaluate mainly the spatial information that was injected to the lower resolution images. Regarding the SWIR bands none of the methods managed to deliver significantly better results than a standard bicubic interpolation on the original low resolution bands.","Benchmarking; Fusion reactions; Image fusion; Image quality; Remote sensing; Bicubic interpolation; Evaluation framework; Qualitative evaluations; Quality indices; Quantitative assessments; Quantitative evaluation; UIQI; Validation; Quality control","Benchmark; Fusion; Image quality indexes; Q4; QNR; UIQI; Validation","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-84979529298"
"Shaik R.U.; Giovanni L.; Fusilli L.","Shaik, Riyaaz Uddien (57218543309); Giovanni, Laneve (57213188792); Fusilli, Lorenzo (14065793200)","57218543309; 57213188792; 14065793200","Dynamic Wildfire Fuel Mapping Using Sentinel-2 and Prisma Hyperspectral Imagery","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","5973","5976","3","10.1109/IGARSS46834.2022.9883095","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140395881&doi=10.1109%2fIGARSS46834.2022.9883095&partnerID=40&md5=9055a9de35c9428d084eaa509d96dd5b","Italy has witnessed a significant increase in wildfires in recent decades. Forest fire fuel maps play a vital role in the prevention, management and risk assessment of wildfires, and this paper presents the procedure implemented to develop a dynamic wildfire fuel map using PRISMA hyperspectral data and Sentinel-2 multispectral data. Freely available multispectral datasets are widely used for land cover and land use mapping, but they have limited utility for fuel mapping due to their coarse spectral resolution. So, in this study, hyperspectral imagery (HSI) from PRISMA has been used for fuel types classification. The feed-forward neural network showed an overall accuracy of 79% by validation. To convert the classification map into a dynamic fuel map, the knowledge of the proportion of live/dead herbaceous loads available in that area is essential. The Relative Greenness approach, which places the Normalized Difference Vegetation Index (NDVI) in the time series of measurements, was implemented using Sentinel-2 multispectral data. By fusing the fuel types classification, relative greenness map and iso-bioclimatic map, a dynamic fuel map for the west of Latium in Italy was developed with reference to Scott/Burgan fuel models. © 2022 IEEE.","Deforestation; Feedforward neural networks; Fuels; Land use; Photomapping; Remote sensing; Risk assessment; Spectroscopy; Forest fire fuels; Fuel map; Fuel types; Fuels mappings; Hyper-spectral imageries; HyperSpectral; Multi-spectral; Multi-spectral data; Type classifications; Wildfire; Image fusion","Fuel Map; Hyperspectral; Image fusion; Multispectral; Wildfires","Conference paper","Final","","Scopus","2-s2.0-85140395881"
"Shen A.; Bo Y.; Zhao W.; Zhang Y.","Shen, Aojie (57196027092); Bo, Yanchen (57720445900); Zhao, Wenzhi (56669054000); Zhang, Yusha (57719579000)","57196027092; 57720445900; 56669054000; 57719579000","Impact of the Dates of Input Image Pairs on Spatio-Temporal Fusion for Time Series with Different Temporal Variation Patterns","2022","Remote Sensing","14","10","2431","","","","10.3390/rs14102431","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131051378&doi=10.3390%2frs14102431&partnerID=40&md5=199e56e5c30eef3a801f1ded8fa9e8f1","Dense time series of remote sensing images with high spatio-temporal resolution are critical for monitoring land surface dynamics in heterogeneous landscapes. Spatio-temporal fusion is an effective solution to obtaining such time series images. Many spatio-temporal fusion methods have been developed for producing high spatial resolution images at frequent intervals by blending fine spatial images and coarse spatial resolution images. Previous studies have revealed that the accuracy of fused images depends not only on the fusion algorithm, but also on the input image pairs being used. However, the impact of input images dates on the fusion accuracy for time series with different temporal variation patterns remains unknown. In this paper, the impact of input image pairs on the fusion accuracy for monotonic linear change (MLC), monotonic non-linear change (MNLC), and non-monotonic change (NMC) time periods were evaluated, respectively, and the optimal selection strategies of input image dates for different situations were proposed. The 16-day composited NDVI time series (i.e., Collection 6 MODIS NDVI product) were used to present the temporal variation patterns of land surfaces in the study areas. To obtain sufficient observation dates to evaluate the impact of input image pairs on the spatio-temporal fusion accuracy, we utilized the Harmonized Landsat-8 Sentinel-2 (HLS) data. The ESTARFM was selected as the spatio-temporal fusion method for this study. The results show that the impact of input image date on the accuracy of spatio-temporal fusion varies with the temporal variation patterns of the time periods being fused. For the MLC period, the fusion accuracy at the prediction date (PD) is linearly correlated to the time interval between the change date (CD) of the input image and the PD, but the impact of the input image date on the fusion accuracy at the PD is not very significant. For the MNLC period, the fusion accuracy at the PD is non-linearly correlated to the time interval between the CD and the PD, the impact of the time interval between the CD and the PD on the fusion accuracy is more significant for the MNLC than for the MLC periods. Given the similar change of time intervals between the CD and the PD, the increments of R2 of fusion result for the MNLC is over ten times larger than those for the MLC. For the NMC period, a shorter time interval between the CD and the PD does not lead to higher fusion accuracies. On the contrary, it may lower the fusion accuracy. This study suggests that temporal variation patterns of the data must be taken into account when selecting optimal dates of input images in the fusion model. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Image fusion; Image resolution; Remote sensing; Surface measurement; Data selection strategies; ESTARFM; Harmonized landsat-8 sentinel-2; Input image; LANDSAT; Monotonics; Spatio-temporal fusions; Temporal variation; Temporal variation pattern; Variation pattern; Time series","data selection strategy; ESTARFM; HLS; spatio-temporal fusion; temporal variation pattern","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131051378"
"Jiang M.; Li J.; Shen H.","Jiang, Menghui (57210173702); Li, Jie (57214207213); Shen, Huanfeng (8359721100)","57210173702; 57214207213; 8359721100","A DEEP LEARNING-BASED HETEROGENEOUS SPATIO-TEMPORAL-SPECTRAL FUSION: SAR AND OPTICAL IMAGES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","1252","1255","3","10.1109/IGARSS47720.2021.9554031","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129873154&doi=10.1109%2fIGARSS47720.2021.9554031&partnerID=40&md5=4660e59201bacdf2be7e5cdca57bde6a","Image fusion is a powerful means to integrate complementary spatio-temporal-spectral information among multi-source remote sensing images. The existing remote sensing image fusion is mostly limited to the fusion between optical images, and most of them are limited to the fusion between two sensors. Based on this, this paper proposes a heterogeneous spatiotemporal-spectral fusion method based on deep learning. Specifically, it combines the low-spatial-resolution (LR) cloudy image with the high-spatial-resolution (HR) SAR images and the HR cloud-free optical image to remove the clouds and improve the spatial resolution of the LR cloudy image. The SAR image is acquired at the same date as the LR cloudy image, while the HR cloud-free image is acquired at another date. Experiments are performed on the images of Landsat 8, Sentinel-1, and Sentinel-2. The experimental results show that the proposed method can effectively achieve the joint goal of spatial resolution improvement and cloud removal of the Landsat image. © 2021 IEEE","Deep learning; Geometrical optics; Image acquisition; Image enhancement; Image resolution; Radar imaging; Remote sensing; Synthetic aperture radar; Cloud removal; Heterogeneous; High spatial resolution; Optical image; Optical-; Resolution improvement; SAR; SAR Images; Spatial resolution; Spatial resolution improvement; Image fusion","cloud-removal; Heterogeneous; optical; SAR; spatial resolution improvement","Conference paper","Final","","Scopus","2-s2.0-85129873154"
"Sigurdsson J.; Armannsson S.E.; Ulfarsson M.O.; Sveinsson J.R.","Sigurdsson, Jakob (7006736374); Armannsson, Sveinn E. (57224686207); Ulfarsson, Magnus O. (6507677875); Sveinsson, Johannes R. (7003642214)","7006736374; 57224686207; 6507677875; 7003642214","Fusing Sentinel-2 and Landsat 8 Satellite Images Using a Model-Based Method","2022","Remote Sensing","14","13","3224","","","","10.3390/rs14133224","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133860643&doi=10.3390%2frs14133224&partnerID=40&md5=36a359caa11fe4e2c3c864be45cbb3cc","The Copernicus Sentinel-2 (S2) constellation comprises of two satellites in a sun-synchronous orbit. The S2 sensors have three spatial resolutions: 10, 20, and 60 m. The Landsat 8 (L8) satellite has sensors that provide seasonal coverage at spatial resolutions of 15, 30, and 60 m. Many remote sensing applications require the spatial resolutions of all data to be at the highest resolution possible, i.e., 10 m for S2. To address this demand, researchers have proposed various methods that exploit the spectral and spatial correlations within multispectral data to sharpen the S2 bands to 10 m. In this study, we combined S2 and L8 data. An S2 sharpening method called Sentinel-2 Sharpening (S2Sharp) was modified to include the 30 m and 15 m spectral bands from L8 and to sharpen all bands (S2 and L8) to the highest resolution of the data, which was 10 m. The method was evaluated using both real and simulated data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Image fusion; Orbits; Remote sensing; High resolution; Image sharpening; LANDSAT; Landsat 8; Multi-spectral; Multiresolution images; Multispectral  multiresolution image; Sentinel-2; Spatial resolution; Superresolution; Landsat","data fusion; image sharpening; Landsat 8; multispectral (MS) multiresolution images; Sentinel-2; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133860643"
"Siebert T.; Clasen K.N.; Ravanbakhsh M.; Demir B.","Siebert, Tim (57933749500); Clasen, Kai Norman (57219287237); Ravanbakhsh, Mahdyar (57192545758); Demir, Begüm (15131434800)","57933749500; 57219287237; 57192545758; 15131434800","Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing","2022","Proceedings of SPIE - The International Society for Optical Engineering","12267","","122670L","","","","10.1117/12.2636276","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142500992&doi=10.1117%2f12.2636276&partnerID=40&md5=201d65136c2228ebb12e9621e9a350b9","With the new generation of satellite technologies, the archives of remote sensing (RS) images are growing very fast. To make the intrinsic information of each RS image easily accessible, visual question answering (VQA) has been introduced in RS. VQA allows a user to formulate a free-form question concerning the content of RS images to extract generic information. It has been shown that the fusion of the input modalities (i.e., image and text) is crucial for the performance of VQA systems. Most of the current fusion approaches use modality-specific representations in their fusion modules instead of joint representation learning. However, to discover the underlying relation between both the image and question modality, the model is required to learn the joint representation instead of simply combining (e.g., concatenating, adding, or multiplying) the modality-specific representations. We propose a multi-modal transformer-based architecture to overcome this issue. Our proposed architecture consists of three main modules: i) the feature extraction module for extracting the modality-specific features; ii) the fusion module, which leverages a user-defined number of multi-modal transformer layers of the VisualBERT model (VB); and iii) the classification module to obtain the answer. In contrast to recently proposed transformer-based models in RS VQA, the presented architecture (called VBFusion) is not limited to specific questions, e.g., questions concerning pre-defined objects. Experimental results obtained on the RSVQAxBEN and RSVQA-LR datasets (which are made up of RGB bands of Sentinel-2 images) demonstrate the effectiveness of VBFusion for VQA tasks in RS. To analyze the importance of using other spectral bands for the description of the complex content of RS images in the framework of VQA, we extend the RSVQAxBEN dataset to include all the spectral bands of Sentinel-2 images with 10m and 20m spatial resolution. Experimental results show the importance of utilizing these bands to characterize the land-use land-cover classes present in the images in the framework of VQA. The code of the proposed method is publicly available at https://git.tu-berlin.de/rsim/multimodal-fusion-transformer-for-vqa-in-rs. © 2022 SPIE.","Architecture; Deep learning; Image fusion; Image processing; Land use; Deep learning; Fusion modules; Multi-modal; Multi-modal fusion; Multi-modal transformer; Question Answering; Remote sensing images; Remote-sensing; Spectral band; Visual question answering; Remote sensing","deep learning; Multi-modal transformer; remote sensing; visual question answering","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142500992"
"Palsson F.; Sveinsson J.R.; Ulfarsson M.O.","Palsson, Frosti (55052918200); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","55052918200; 7003642214; 6507677875","Sentinel-2 image fusion using a deep residual network","2018","Remote Sensing","10","8","1290","","","","10.3390/rs10081290","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051654504&doi=10.3390%2frs10081290&partnerID=40&md5=2b75fe7ec9e8eec5d8107b069e01234a","Single sensor fusion is the fusion of two or more spectrally disjoint reflectance bands that have different spatial resolution and have been acquired by the same sensor. An example is Sentinel-2, a constellation of two satellites, which can acquire multispectral bands of 10 m, 20 m and 60 m resolution for visible, near infrared (NIR) and shortwave infrared (SWIR). In this paper, we present a method to fuse the fine and coarse spatial resolution bands to obtain finer spatial resolution versions of the coarse bands. It is based on a deep convolutional neural network which has a residual design that models the fusion problem. The residual architecture helps the network to converge faster and allows for deeper networks by relieving the network of having to learn the coarse spatial resolution part of the inputs, enabling it to focus on constructing the missing fine spatial details. Using several real Sentinel-2 datasets, we study the effects of the most important hyperparameters on the quantitative quality of the fused image, compare the method to several state-of-the-art methods and demonstrate that it outperforms the comparison methods in experiments. © 2018 by the authors.","Convolution; Deep neural networks; Image resolution; Infrared devices; Infrared radiation; Neural networks; Comparison methods; Convolutional neural network; Deep convolutional neural networks; Residual design; Sentinel-2; Short wave infrared; Spatial resolution; State-of-the-art methods; Image fusion","Convolutional neural network; Image fusion; Residual neural network; Sentinel-2","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85051654504"
"Ulfarsson M.O.; Palsson F.; Dalla Mura M.; Sveinsson J.R.","Ulfarsson, Magnus O. (6507677875); Palsson, Frosti (55052918200); Dalla Mura, Mauro (36499129800); Sveinsson, Johannes R. (7003642214)","6507677875; 55052918200; 36499129800; 7003642214","Sentinel-2 sharpening using a reduced-rank method","2019","IEEE Transactions on Geoscience and Remote Sensing","57","9","8694937","6408","6420","12","10.1109/TGRS.2019.2906048","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072023379&doi=10.1109%2fTGRS.2019.2906048&partnerID=40&md5=832a78bb32913c8da1b38d41ebecc03a","Recently, the Sentinel-2 (S2) satellite constellation was deployed for mapping and monitoring the Earth environment. Images acquired by the sensors mounted on the S2 platforms have three levels of spatial resolution: 10, 20, and 60 m. In many remote sensing applications, the availability of images at the highest spatial resolution (i.e., 10 m for S2) is often desirable. This can be achieved by generating a synthetic high-resolution image through data fusion. To this end, researchers have proposed techniques exploiting the spectral/spatial correlation inherent in multispectral data to sharpen the lower resolution S2 bands to 10 m. In this paper, we propose a novel method that formulates the sharpening process as a solution to an inverse problem. We develop a cyclic descent algorithm called S2Sharp and an associated tuning parameter selection algorithm based on generalized cross validation and Bayesian optimization. The tuning parameter selection method is evaluated on a simulated data set. The effectiveness of S2Sharp is assessed experimentally by comparisons to state-of-the-art methods using both simulated and real data sets. © 1980-2012 IEEE.","Data fusion; Image fusion; Image resolution; Remote sensing; Cyclic descents; Generalized cross validation; Image sharpening; Remote sensing applications; Satellite constellations; Sentinel-2 (S2) constellation; State-of-the-art methods; Super resolution; Inverse problems","Cyclic descent (CD); data fusion; image sharpening; Sentinel-2 (S2) constellation; superresolution","Article","Final","","Scopus","2-s2.0-85072023379"
"Li J.; Li Y.; Cai R.; He L.; Chen J.; Plaza A.","Li, Jun (24481713500); Li, Yunfei (57218424664); Cai, Runlin (57283915000); He, Lin (57192205017); Chen, Jin (55717837500); Plaza, Antonio (7006613644)","24481713500; 57218424664; 57283915000; 57192205017; 55717837500; 7006613644","Enhanced Spatiotemporal Fusion via MODIS-Like Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3106338","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124426119&doi=10.1109%2fTGRS.2021.3106338&partnerID=40&md5=38b423dcbbb745f70a72c03552a0126c","Spatiotemporal fusion (STF) aims at generating remote-sensing data with both high spatial and temporal resolution. In the literature, one of the most widely used strategies to accomplish this goal is to fuse high temporal resolution images collected by the Moderate Resolution Imaging Spectroradiometer (MODIS) with images with finer spatial resolution than those provided by MODIS (e.g., those collected by other satellite instruments such as Landsat or Sentinel-2). Current STF methods generally fuse an upsampled MODIS image with finer spatial resolution images. This leads to two main problems. First of all, the model uncertainty errors (resulting from the ill-posed upsampling problem) will be propagated into the fusion results, leading to spatial and spectral distortion. Furthermore, the spatial details of the upsampled MODIS image may be significantly different from those of the finer spatial resolution images, making the STF problem even more challenging. In order to tackle these issues, in this work, we develop a new linear regression-based STF strategy (LiSTF), which performs the reconstruction from a MODIS-like image (instead of from an upsampled MODIS image), thus reducing the model uncertainty errors and preserving better the spatial information. The MODIS-like images are built from the finer spatial resolution images via downsampling. Our experimental results, conducted using two publicly available datasets of Landsat–MODIS image pairs and one publicly available dataset of Sentinel–MODIS image pairs, reveal that our newly proposed LiSTF approach can significantly enhance the quantitative and qualitative performance of STF, particularly in terms of preserving the spatial information. © 2021 IEEE.","Image enhancement; Image fusion; Radiometers; Remote sensing; Signal sampling; Uncertainty analysis; High temporal resolution; LANDSAT; Modeling uncertainties; Moderate-resolution imaging spectroradiometers; Remote-sensing; Spatial resolution; Spatial resolution images; Spatio-temporal fusions; Superresolution; Uncertainty; MODIS; reconstruction; remote sensing; spatiotemporal analysis; Image resolution","Artificial satellites; Earth; MODIS; Remote sensing; Spatial resolution; Superresolution; Uncertainty","Article","Final","","Scopus","2-s2.0-85124426119"
"Li X.; Zhang M.; Long J.; Lin H.","Li, Xinyu (57276782800); Zhang, Meng (56746635600); Long, Jiangping (27172354400); Lin, Hui (36071585400)","57276782800; 56746635600; 27172354400; 36071585400","A novel method for estimating spatial distribution of forest above-ground biomass based on multispectral fusion data and ensemble learning algorithm","2021","Remote Sensing","13","19","3910","","","","10.3390/rs13193910","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116259290&doi=10.3390%2frs13193910&partnerID=40&md5=82fc7cdb6c8c2b9d8311c05ee4f42e71","Optical remote sensing technology has been widely used in forest resources inventory. Due to the influence of satellite orbits, sensor parameters, sensor errors, and atmospheric effects, there are great differences in vegetation spectral information captured by different satellite sensor images. Spectral fusion technology can couple the advantages of different multispectral sensor images to produce new multispectral data with high spatial and spectral resolution, it has great potential for improving the spectral sensitivity of forest vegetation and alleviating the spectral satura-tion. However, how to quickly and effectively select the multi-spectral fusion data suitable for forest above-ground biomass (AGB) estimation is a very critical issue. This study proposes a scheme (RF-S) to comprehensively evaluate multispectral fused images and develop the appropriate model for forest AGB estimation, on the basis of random forest (RF) and the stacking ensemble algorithm. First, four classic fusion methods are used to fuse the preprocessed GaoFen-2 (GF-2) multispectral image with Sentinel-2 image to generate 12 fused Sentinel-like images. Secondly, we apply a com-prehensive evaluation method to quickly select the optimal fused image for the follow-up research. Subsequently, two feature combination optimization methods are used to select feature variables from the three feature sets. Finally, the stacking ensemble algorithm based on model dynamic integration and hyperparameter automatic optimization, as well as some classic machine learners, are used to construct the forest AGB estimation model. The results show that the fused image NND_B3 (based on nearest neighbor diffusion pan sharpening method and Band3_Red) selected by the evaluation method proposed in this study has the best performance in AGB estimation. Using the stacking ensemble method and NND_B3 image, we get the highest estimation accuracy, with the adjusted R2 and relative root mean square error (RMSEr) of 0.6306 and 15.53%, respectively. The AGB estimation RMSEr of NND_B3 is 19.95% and 24.90% lower than those of GF-2 and Sentinel-2, re-spectively. We also found that the multi-window texture factor has better performance in the area with low AGB, and it can suppress the overestimation significantly. The AGB spatial distribution estimated using the NND_B3 image matches the field observations well, indicating that the multispectral fusion image combined with the Stacking algorithm can increase the accuracy and saturation of the AGB estimates. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Biomass; Decision trees; Forestry; Image enhancement; Image fusion; Learning algorithms; Orbits; Remote sensing; Spectroscopy; Textures; Vegetation; Aboveground biomass; Biomass estimation; Ensemble regression algorithm; Evaluation of fused image; Forest above-ground biomass; Fused images; Integrated multi-source data; Multisource data; Regression algorithms; Spectral saturation; Mean square error","Ensemble regression algorithm; Evaluation of fused images; Forest above-ground biomass; Integrated multi-source data; Spectral saturation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85116259290"
"Alboody A.; Puigt M.; Roussel G.; Vantrepotte V.; Jamet C.; Tran T.K.","Alboody, Ahed (24528545800); Puigt, Matthieu (9132941600); Roussel, Gilles (57197306458); Vantrepotte, Vincent (22954813300); Jamet, Cedric (8600546000); Tran, Trung Kien (57217442266)","24528545800; 9132941600; 57197306458; 22954813300; 8600546000; 57217442266","Experimental Comparison of Multi-Sharpening Methods Applied to Sentinel-2 MSI and Sentinel-3 OLCI Images","2021","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2021-March","","9484009","","","","10.1109/WHISPERS52202.2021.9484009","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112845236&doi=10.1109%2fWHISPERS52202.2021.9484009&partnerID=40&md5=2b0e58f3cf7340bccdfdc9bdab25341b","Multi-spectral images are crucial to detect and to understand phenomena in marine observation. However, in coastal areas, these phenomena are complex and their analyze requires multi-spectral images with both a high spatial and spectral resolution. Unfortunately, no satellite is able to provide both at the same time. As a consequence, multi-sharpening techniques - a.k.a. fusion or super- resolution of multi-spectral and/or hyper-spectral images - were proposed and consist of combining information from at least two multi-spectral images with different spatial and spectral resolutions. The fused image then combines their best characteristics. Various methods - based on different strategies and tools - have been proposed to solve this problem. This article presents a comparative review of fusion methods applied to Sentinel-2 MSI (13 spectral bands with a spatial resolution ranging from 10 to 60 m) and Sentinel-3 OLCI (21 spectral bands with a spatial resolution of 300 m) images. Indeed, both satellites are extensively used in marine observation and, to the best of the authors' knowledge, the fusion of their data was partially investigated (and not in the way we aim to do in this paper). To that end, we provide both a quantitative analysis of the performance of some state-of-the-art methods on simulated images, and a qualitative analysis on real images.  © 2021 IEEE.","Hyperspectral imaging; Image fusion; Image resolution; Remote sensing; Spectral resolution; Spectroscopy; Experimental comparison; Hyper-spectral images; Marine observations; Multispectral images; Qualitative analysis; Spatial resolution; State-of-the-art methods; Strategies and tools; Image analysis","Image fusion; Real data; Remote sensing; Sentinel-2 MSI; Sentinel-3 OLCI; Simulations","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112845236"
"Zhao Y.; Liu D.","Zhao, Yongquan (57192575717); Liu, Desheng (55577793400)","57192575717; 55577793400","A robust and adaptive spatial-spectral fusion model for PlanetScope and Sentinel-2 imagery","2022","GIScience and Remote Sensing","59","1","","520","546","26","10.1080/15481603.2022.2036054","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127040354&doi=10.1080%2f15481603.2022.2036054&partnerID=40&md5=adaedef06006886c69888dce4b2b86cc","Satellite sensors usually compromise between their spatial and spectral resolutions due to the limitations of data volume and signal-to-noise ratio, such as the Very High spatial Resolution (VHR) PlanetScope (PS, four or five 3-m bands) and medium spatial resolution (med-resolution) Sentinel-2 (S2, ten 10-m or 20-m bands) constellations. Concomitant with the growing demand for satellite images with ample spatial details and spectral signatures, Spatial-Spectral image Fusion (SSF) is important for blending the spatial resolution of PS and the spectral resolution of S2 to produce synthetic 3-m image products in the ten bands for different applications. However, the existing studies conducted for fusing PS and S2 data present limited spatial and spectral fidelities to original PS and S2 bands. Hence, this study presents a new SSF method named Robust and Adaptive Spatial-Spectral image Fusion Model (RASSFM) for that purpose. RASSFM improves the spatial and spectral fidelities of the results through: (1) combining the spectral mapping and the spectral correlation to obtain the high-quality spatial information sources for the S2 bands, (2) utilizing the neighborhood information considering both spatial and spectral constraints to improve the spectral fidelities of the fused bands. We conducted the SSF tests at the degraded and original spatial resolutions to comprehensively evaluate the proposed RASSFM method. Furthermore, we examined the performance of RASSFM in four study sites with highly mixed regular or desert urban, heterogeneous agricultural, and vegetation-dominated landscapes. Moreover, we compared our method with twenty representative SSF methods developed for similar purposes. The results demonstrate that RASSFM not only outperforms the other methods but has robust performance in different landscapes and comparable accuracies in different bands, thereby advancing the fusion of VHR and med-resolution images from PS and S2, respectively. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","image resolution; multispectral image; numerical model; satellite imagery; satellite sensor; Sentinel; spatial analysis; spectral analysis","medium resolution (med-resolution); multispectral; PlanetScope; sentinel-2; Spatial-spectral image fusion; very high resolution (VHR)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85127040354"
"Zhou J.; Sun W.; Meng X.; Yang G.; Ren K.; Peng J.","Zhou, Jun (57803409000); Sun, Weiwei (55726567900); Meng, Xiangchao (56158755000); Yang, Gang (57192178476); Ren, Kai (57211514962); Peng, Jiangtao (24833160700)","57803409000; 55726567900; 56158755000; 57192178476; 57211514962; 24833160700","Generalized Linear Spectral Mixing Model for Spatial-Temporal-Spectral Fusion","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5533216","","","","10.1109/TGRS.2022.3188501","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134257366&doi=10.1109%2fTGRS.2022.3188501&partnerID=40&md5=a6dd521da38ef82fb33248ccad913676","Image fusion effectively solves the trade-off among spatial resolution, temporal resolution, and spectral resolution of remote sensing sensors. However, most of existing methods focus on the fusion of two of the spatial, temporal, and spectral metrics of remote sensing images. The few spatial-temporal-spectral fusion (STSF) methods available are mainly for fusing Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat images, which are not suitable for the characteristics of the spaceborne hyperspectral images (HSIs) with low temporal resolution, such as Hyperion, ZY-1 02D, and PRISMA. For this purpose, we proposed a novel generalized linear spectral mixing model for STSF (GLMM-STSF). In the method, the GLMM is introduced into the STSF problem, and the temporal variations of images at different times are transferred to the endmember and abundance matrix variations of images for estimation. To the best of our knowledge, for the first time, the STSF task of remote sensing images is handled from the perspective of spectral unmixing. Compared with the existing STSF fusion methods, our method targets the task of fusing spaceborne HSI with low temporal and spatial resolutions with multispectral image (MSI) featured by high temporal and spatial resolutions. Taking the STSF of ZY-1 02D hyperspectral and Sentinel-2 multispectral real datasets as an example, comparisons with related state-of-the-art methods demonstrate that our proposed method achieves superior fusion performance. © 1980-2012 IEEE.","Economic and social effects; Hyperspectral imaging; Image resolution; Matrix algebra; Mixing; Remote sensing; Spectral resolution; Spectroscopy; HyperSpectral; Linear spectral mixing models; Multi-spectral; Sparse matrices; Spatial resolution; Spatial temporals; Spectral unmixing; Task analysis; Temporal resolution; MODIS; Image fusion","Hyperspectral; Image fusion; Multispectral; Spatial resolution; Spectral resolution; Spectral unmixing; Temporal resolution","Article","Final","","Scopus","2-s2.0-85134257366"
"Lin C.; Zhu A.-X.; Wang Z.; Wang X.; Ma R.","Lin, Chen (56165571100); Zhu, A-Xing (55647324800); Wang, Zhaofei (57208215526); Wang, Xiaorui (57364552200); Ma, Ronghua (55892478400)","56165571100; 55647324800; 57208215526; 57364552200; 55892478400","The refined spatiotemporal representation of soil organic matter based on remote images fusion of Sentinel-2 and Sentinel-3","2020","International Journal of Applied Earth Observation and Geoinformation","89","","102094","","","","10.1016/j.jag.2020.102094","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084938761&doi=10.1016%2fj.jag.2020.102094&partnerID=40&md5=479e5747c19b5def9730648b5cac7aa2","Remote sensing technology is important for soil organic matter (SOM) estimation, but existing studies have mainly relied on a single data source. This limitation makes it difficult to simultaneously ensure high spatial resolution, high spectral accuracy and refined temporal granularity simultaneously, which cannot meet the requirements of the spatiotemporal dynamics representation. This study aimed to introduce a new remote sensing image source into SOM modeling and spatiotemporal estimation generated by fusing together Sentinel-2 and Sentinel-3 remote sensing images that have a 5-day revisit cycle; 10 m spatial resolution; and 21 different bands in blue, green, red and NIR spectral ranges. According to the image fusion process, a total of 52 available images were acquired between November 2016 and December 2018 in Donghai County, China. The fused images were used for SOM estimation model associated with 107 field samples. The results indicated that, first, the optimal model consisted of the band reflectivity (B20) and RVI (B18/B9), which were derived from the fused images, and the R2 approached 0.7 in the two phases of the synchronized data. Second, the modeling accuracy was influenced to some extent by the actual SOM content. The R2 values exceeded 0.75 when the SOM content was higher than 24 g/kg, while the R2 was even lower than 0.35 when the SOM content was lower. Third, the averaged SOM contents remained stable in general, while the seasonal variances can also be found during the two-year interval. The SOM contents maintained a low level during autumn and winter, while higher SOM levels were found in the spring and summer. Finally, the spatial variations could be described as ‘low in the west and high in the east’. In summary, the spatiotemporal dynamics of SOM highlighted the necessity of modeling with fused remote sensing images, and more effective modeling could be expected with the continued increase in SOM in future. © 2020 The Authors","China; Donghai [Jiangsu]; Jiangsu; environmental factor; remote sensing; Sentinel; soil organic matter; spatial resolution; spatial variation; spatiotemporal analysis","Environmental factors; Estimation model; Remote sensing; Sentinel 2/3; Soil organic matter","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084938761"
"Agapiou A.","Agapiou, Athos (35188628700)","35188628700","Evaluation of Landsat 8 OLI/TIRS level-2 and sentinel 2 level-1C fusion techniques intended for image segmentation of archaeological landscapes and proxies","2020","Remote Sensing","12","3","579","","","","10.3390/rs12030579","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080882418&doi=10.3390%2frs12030579&partnerID=40&md5=1382c221b471daf10e82fa271f8dd410","The use of medium resolution, open access, and freely distributed satellite images, such as those of Landsat, is still understudied in the domain of archaeological research, mainly due to restrictions of spatial resolution. This investigation aims to showcase how the synergistic use of Landsat and Sentinel optical sensors can efficiently support archaeological research through object-based image analysis (OBIA), a relatively new scientific trend, as highlighted in the relevant literature, in the domain of remote sensing archaeology. Initially, the fusion of a 30mspatial resolution Landsat 8 OLI/TIRS Level-2 and a 10 m spatial resolution Sentinel 2 Level-1C optical images, over the archaeological site of ""Nea Paphos"" in Cyprus, are evaluated in order to improve the spatial resolution of the Landsat image. At this step, various known fusion models are implemented and evaluated, namely Gram-Schmidt, Brovey, principal component analysis (PCA), and hue-saturation-value (HSV) algorithms. In addition, all four 10mavailable spectral bands of the Sentinel 2 sensor, namely the blue, green, red, and near-infrared bands (Bands 2 to 4 and Band 8, respectively) were assessed for each of the different fusion models. On the basis of these findings, the next step of the study, focused on the image segmentation process, through the evaluation of different scale factors. The segmentation process is an important step moving from pixel-based to object-based image analysis. The overall results show that the Gram-Schmidt fusion method based on the near-infrared band of the Sentinel 2 (Band 8) at a range of scale factor segmentation to 70 are the optimum parameters for the detection of standing visible monuments, monitoring excavated areas, and detecting buried archaeological remains, without any significant spectral distortion of the original Landsat image. The new 10 m fused Landsat 8 image provides further spatial details of the archaeological site and depicts, through the segmentation process, important details within the landscape under examination. © 2020 by the authors.","Architecture; Fusion reactions; Geometrical optics; Image analysis; Image enhancement; Image fusion; Image resolution; Infrared devices; Principal component analysis; Remote sensing; Archaeological landscapes; Archaeological proxies; LANDSAT; Object based image analysis (OBIA); Sentinel 2; Image segmentation","Archaeological landscapes; Archaeological proxies; Fusion; Image segmentation; Landsat 8; Object-based image analysis (OBIA); Sentinel 2","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85080882418"
"Li Q.; Barrett B.; Williams R.; Hoey T.; Boothroyd R.","Li, Qing (57971676800); Barrett, Brian (35993628500); Williams, Richard (57198060439); Hoey, Trevor (7006871602); Boothroyd, Richard (57033931800)","57971676800; 35993628500; 57198060439; 7006871602; 57033931800","Enhancing performance of multi-temporal tropical river landform classification through downscaling approaches","2022","International Journal of Remote Sensing","43","17","","6445","6462","17","10.1080/01431161.2022.2139164","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142282905&doi=10.1080%2f01431161.2022.2139164&partnerID=40&md5=57ac70d9a645779844792f12ea8d8fa3","Multi-temporal remote sensing imagery has the potential to classify river landforms to reconstruct the evolutionary trajectory of river morphologies. Whilst open-access archives of high spatial resolution imagery are increasingly available from satellite sensors, such as Sentinel-2, there remains a fundamental challenge of maximising the utility of information in each band whilst maintaining a sufficiently fine resolution to identify landforms. Although image fusion and downscaling methods on Sentinel-2 imagery have been investigated for many years, there is a need to assess their performance for multi-temporal object-based river landform classification. This investigation first compared three downscaling methods: area to point regression kriging (ATPRK), super-resolution based on Sen2Res, and nearest neighbour resampling. We assessed performance of the three downscaling methods by accuracy, precision, recall and F1-score. ATPRK was the optimal downscaling approach, achieving an overall accuracy of 0.861. We successively engaged a set of experiments to determine an optimal training model, exploring single and multi-date scenarios. We find that not only does remote sensing imagery with better quality improve river landform classification performance, but multi-date datasets for establishing machine learning models should be considered for contributing higher classification accuracy. This paper presents a workflow for automated river landform recognition that could be applied to other tropical rivers with similar hydro-geomorphological characteristics. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Classification (of information); Image classification; Image enhancement; Image fusion; Remote sensing; Rivers; Satellite imagery; Tropics; Down-scaling; Downscaling methods; Image downscaling; Landform classification; Multi-temporal; Multitemporal classification; Performance; Remote sensing imagery; River landform; Tropical rivers; downscaling; fluvial geomorphology; image analysis; kriging; landform; machine learning; regression analysis; remote sensing; spatial resolution; Landforms","image downscaling; landform classification; multi-temporal classification; river landforms","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85142282905"
"Fernandez R.; Fernandez-Beltran R.; Pla F.","Fernandez, Rafael (57222243976); Fernandez-Beltran, Ruben (55838551300); Pla, Filiberto (7006504936)","57222243976; 55838551300; 7006504936","Inter-Sensor Remote Sensing Image Enhancement for Operational Sentinel-2 and Sentinel-3 Data Products","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324071","1504","1507","3","10.1109/IGARSS39084.2020.9324071","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102010234&doi=10.1109%2fIGARSS39084.2020.9324071&partnerID=40&md5=7e63a302423c3f8b2adf11c8230671a0","The recent availability of operational data from the Sentinel-2 and Sentinel-3 missions provides widespread opportunities to generate diverse high-level remote sensing products. However, the synergies between both multi-spectral instruments are often difficult to exploit from an operational perspective. Standard pansharpening algorithms may encounter important disadvantages due to the limited intersensor data availability in actual production environments. Moreover, the lack of a real high-resolution ground-truth for super-resolution techniques may affect the radiometric quality of the final result. In this scenario, this work investigates the viability of using the Multi-Spectral Instrument of Sentinel-2 for super-resolving data products acquired by the Ocean and Land Colour Instrument of Sentinel-3. Specifically, we define an inter-sensor image enhancement framework which combines a PCA-based component substitution pansharpening scheme with a CNN-based spatial enhancing super-resolution mapping. The conducted experiments reveal the suitability of the proposed approach for generating Level-4 data products within the Copernicus programme context. © 2020 IEEE.","Geology; Optical resolving power; Remote sensing; Component substitution; Data availability; Enhancement framework; Production environments; Radiometric quality; Remote sensing images; Sentinel-3 Mission; Super-resolution mappings; Image enhancement","image fusion; pansharpening; Sentinel-2 (S2); Sentinel-3 (S3); super-resolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85102010234"
"Lin H.; Long J.; Peng Y.; Zhou T.","Lin, Hong (58046202900); Long, Jian (57218616379); Peng, Yuanxi (7403418922); Zhou, Tong (57222290947)","58046202900; 57218616379; 7403418922; 57222290947","Hyperspectral Multispectral Image Fusion via Fast Matrix Truncated Singular Value Decomposition","2023","Remote Sensing","15","1","207","","","","10.3390/rs15010207","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145874808&doi=10.3390%2frs15010207&partnerID=40&md5=36c873f573c9b6783650e2daf5649f41","Recently, methods for obtaining a high spatial resolution hyperspectral image (HR-HSI) by fusing a low spatial resolution hyperspectral image (LR-HSI) and high spatial resolution multispectral image (HR-MSI) have become increasingly popular. However, most fusion methods require knowing the point spread function (PSF) or the spectral response function (SRF) in advance, which are uncertain and thus limit the practicability of these fusion methods. To solve this problem, we propose a fast fusion method based on the matrix truncated singular value decomposition (FTMSVD) without using the SRF, in which our first finding about the similarity between the HR-HSI and HR-MSI is utilized after matrix truncated singular value decomposition (TMSVD). We tested the FTMSVD method on two simulated data sets, Pavia University and CAVE, and a real data set wherein the remote sensing images are generated by two different spectral cameras, Sentinel 2 and Hyperion. The advantages of FTMSVD method are demonstrated by the experimental results for all data sets. Compared with the state-of-the-art non-blind methods, our proposed method can achieve more effective fusion results while reducing the fusing time to less than 1% of such methods; moreover, our proposed method can improve the PSNR value by up to 16 dB compared with the state-of-the-art blind methods. © 2022 by the authors.","Hyperspectral imaging; Image resolution; Optical transfer function; Remote sensing; Singular value decomposition; Fusion methods; High spatial resolution; High spatial resolution multispectral images; HyperSpectral; Hyperspectral imaging super-resolution; Image spatial resolution; matrix; Spectral response functions; Superresolution; Truncated singular value decomposition; Image fusion","hyperspectral imaging super-resolution; image fusion; truncated singular value decomposition","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145874808"
"Laso F.J.; Benítez F.L.; Rivas-Torres G.; Sampedro C.; Arce-Nazario J.","Laso, Francisco J. (57214998377); Benítez, Fátima L. (57200598173); Rivas-Torres, Gonzalo (24830914500); Sampedro, Carolina (57197834591); Arce-Nazario, Javier (22955451700)","57214998377; 57200598173; 24830914500; 57197834591; 22955451700","Land cover classification of complex agroecosystems in the non-protected highlands of the Galapagos Islands","2020","Remote Sensing","12","1","65","","","","10.3390/RS12010065","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079629490&doi=10.3390%2fRS12010065&partnerID=40&md5=e1ee3b0fb7237694ae8fbb0660bc03de","The humid highlands of the Galapagos are the islands' most biologically productive regions and a key habitat for endemic animal and plant species. These areas are crucial for the region's food security and for the control of invasive plants, but little is known about the spatial distribution of its land cover. We generated a baseline high-resolution land cover map of the agricultural zones and their surrounding protected areas. We combined the high spatial resolution of PlanetScope images with the high spectral resolution of Sentinel-2 images in an object-based classification using a RandomForest algorithm. We used images collected with an unmanned aerial vehicle (UAV) to verify and validate our classified map. Despite the astounding diversity and heterogeneity of the highland landscape, our classification yielded useful results (overall Kappa: 0.7, R2: 0.69) and revealed that across all four inhabited islands, invasive plants cover the largest fraction (28.5%) of the agricultural area, followed by pastures (22.3%), native vegetation (18.6%), food crops (18.3%), and mixed forest and pioneer plants (11.6%). Our results are consistent with historical trajectories of colonization and abandonment of the highlands. The produced dataset is designed to suit the needs of practitioners of both conservation and agriculture and aims to foster collaboration between the two areas. © 2019 by the authors.","Agriculture; Animals; Antennas; Conservation; Decision trees; Food supply; Image fusion; Random forests; Spectral resolution; Unmanned aerial vehicles (UAV); Vegetation; Galapagos; Invasive species; Land cover; Planetscope; Sentinel-2; Landforms","Agriculture; Conservation; Galapagos; Image fusion; Invasive species; Land cover; Planetscope; Random forest; Sentinel-2; Uav","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85079629490"
"Ma C.; Gao H.","Ma, Conghui (56562155900); Gao, Hongchao (57471481400)","56562155900; 57471481400","A GAN based method for SAR and optical images fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12166","","121664F","","","","10.1117/12.2617316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125474916&doi=10.1117%2f12.2617316&partnerID=40&md5=04c5d0df6dd2dfd1a20109005ebc35dd","With the development of the remote sensing technology, the availability of satellite images has been dramatically increased with high quantity and quality. Diverse information can be obtained from these multiple imaging sources. For example, synthetic aperture radar (SAR) imagery measures physical properties of the observed scene in all-weather and full-time situation and follows a range-based imaging geometry, while optical imagery measures chemical characteristics of the scene and follows a perspective imaging geometry and needs both daylight and a cloudless sky. These multisource remote sensing images, once fused together, provide a more comprehensive interpretation of remote sensing scenes. Recent advances in Generative adversarial networks (GANs) have shown great promise in translating imagery between modalities, as well in the generation of high resolution and realistic imagery. In this paper, a GAN architecture is used to solve the task of fusing SAR and optical remote sensing imagery. The network learns the mapping between input and output image, and learns a loss function to train this mapping. Specifically, the generated network is divided into two parts, encoding and decoding. The fused image including SAR intensity and texture information is generated by the generator. Other details of the optical image are added to the fusion image gradually by the discriminator. The structural similarity loss function of GAN is to make the training of GAN model more accurate on the whole structure. Experiments on Sentinel-1and Sentinel-2 imagery confirm the effectiveness and efficiency of the proposed method.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Generative adversarial networks; Geometrical optics; Image fusion; Mapping; Radar imaging; Remote sensing; Textures; Imaging geometry; Learn+; Loss functions; Multiple imaging; Network-based; Optical image; Optical imagery; Remote sensing technology; Satellite images; Synthetic aperture radar images; Synthetic aperture radar","Generative adversarial network; Image fusion; Optical imagery; SAR","Conference paper","Final","","Scopus","2-s2.0-85125474916"
"Issaoui W.; Alexakis D.D.; Nasr I.H.; Argyriou A.V.; Alevizos E.; Papadopoulos N.; Inoubli M.H.","Issaoui, Wissal (57223179966); Alexakis, Dimitrios D. (55901750800); Nasr, Imen Hamdi (24721723900); Argyriou, Athanasios V. (55848342500); Alevizos, Evangelos (56915984900); Papadopoulos, Nikos (55327516400); Inoubli, Mohamed Hédi (24721052000)","57223179966; 55901750800; 24721723900; 55848342500; 56915984900; 55327516400; 24721052000","Monitoring Olive Oil Mill Wastewater Disposal Sites Using Sentinel-2 and PlanetScopeSatellite Images: Case Studies in Tunisia and Greece","2022","Agronomy","12","1","90","","","","10.3390/agronomy12010090","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122194375&doi=10.3390%2fagronomy12010090&partnerID=40&md5=48d73759c235f02a520fb695636c9db8","Mediterranean countries are known worldwide for their significant contribution to olive oil production, which generates large amounts of olive mill wastewater (OMW) that degrades land and water environments near the disposal sites. OMW consists of organic substances with high concentrations of phenolic compounds along with inorganic particles. The aim of this study is to assess the effectiveness of satellite image analysis techniques using multispectral satellite data with high (PlanetScope, 3 × 3 m) and medium (Sentinel-2, 10 × 10 m) spatial resolution to detect Olive Mill Wastewater (OMW) disposal sites, both in the SidiBouzid region (Tunisia) and in the broader Rethymno region on the island of Crete, (Greece). Documentation of the sites was carried out by collecting spectral signatures of OMW at temporal periods. The study integrates the application of a variety of spectral vegetation indices (VIs), such as the Normalized Difference Vegetation Index (NDVI), in order to evaluate their efficiency in detecting OMW disposal areas. Furthermore, a set of image-processing methods was applied on satellite images to improve the monitoring of OMW ponds including the false-color composites (FCC), the Principal Component Analysis (PCA), and image fusion. Finally, different classification algorithms, such as the ISODATA, the maximum likelihood (ML), and the Support Vector Machine (SVM) were applied to both satellite images in order to assist in the overall approach to effectively detect the sites. The results obtained from different approaches were compared, evaluating the efficiency of Sentinel-2 and PlanetScope images to detect and monitor OMW disposal areas under different morphological environments. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Classification; Disposal sites; Indices; Olive mill wastewater (OMW); Remote sensing; Spectral signature","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122194375"
"Moltó E.","Moltó, Enrique (6701725706)","6701725706","Fusion of Different Image Sources for Improved Monitoring of Agricultural Plots","2022","Sensors","22","17","6642","","","","10.3390/s22176642","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137564088&doi=10.3390%2fs22176642&partnerID=40&md5=780409ac8c4af08404af9f200d15b1e9","In the Valencian Community, the applications of precision agriculture in multiannual woody crops with high added value (fruit trees, olive trees, almond trees, vineyards, etc.) are of priority interest. In these plots, canopies do not fully cover the soil and the planting frames are incompatible with the Resolution of Sentinel 2. The present work proposes a procedure for the fusion of images with different temporal and spatial resolutions and with different degrees of spectral quality. It uses images from the Sentinel 2 mission (low resolution, high spectral quality, high temporal resolution), orthophotos (high resolution, low temporal resolution) and images obtained with drones (very high spatial resolution, low temporal resolution). The procedure is applied to generate time series of synthetic RGI images (red, green, infrared) with the same high resolution of orthophotos and drone images, in which gray levels are reassigned from the combination of their own RGI bands and the values of the B3, B4 and B8 bands of Sentinel 2. Two practical examples of application are also described. The first shows the NDVI images that can be generated after the process of merging two RGI Sentinel 2 images obtained on two specific dates. It is observed how, after the merging, different NDVI values can be assigned to the soil and vegetation, which allows them to be distinguished (contrary to the original Sentinel 2 images). The second example shows how graphs can be generated to describe the evolution throughout the vegetative cycle of the estimated values of three spectral indices (NDVI, GNDVI, GCI) on a point in the image corresponding to soil and on another assigned to vegetation. The robustness of the proposed algorithm has been validated by using image similarity metrics. © 2022 by the author.","Agriculture; Algorithms; Crops, Agricultural; Soil; Drones; Forestry; Image analysis; Image enhancement; Merging; Orchards; Soils; Vegetation; Google earth engine; Google earths; Image similarity; Images processing; Multi-spectral analysis; Orthophotos; Sentinel 2; Spectral indices; Spectral quality; Temporal resolution; agriculture; algorithm; crop; procedures; soil; Image fusion","drone; google earth engine; image processing; image similarity; multispectral analysis; orthophoto; Sentinel 2; spectral indices","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137564088"
"Gašparović M.; Medak D.; Pilaš I.; Jurjević L.; Balenović I.","Gašparović, M. (36987936900); Medak, D. (26642614700); Pilaš, I. (23968152000); Jurjević, L. (57193380875); Balenović, I. (35995358100)","36987936900; 26642614700; 23968152000; 57193380875; 35995358100","Fusion of sentinel-2 and planetscope imagery for vegetation detection and monitoring","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","1","","155","160","5","10.5194/isprs-archives-XLII-1-155-2018","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056169430&doi=10.5194%2fisprs-archives-XLII-1-155-2018&partnerID=40&md5=3c32eda5e905b11431cbb8eb7bd03122","Different spatial resolutions satellite imagery with global almost daily revisit time provide valuable information about the earth surface in a short time. Based on the remote sensing methods satellite imagery can have different applications like environmental development, urban monitoring, etc. For accurate vegetation detection and monitoring, especially in urban areas, spectral characteristics, as well as the spatial resolution of satellite imagery is important. In this research, 10-m and 20-m Sentinel-2 and 3.7-m PlanetScope satellite imagery were used. Although in nowadays research Sentinel-2 satellite imagery is often used for land-cover classification or vegetation detection and monitoring, we decided to test a fusion of Sentinel-2 imagery with PlanetScope because of its higher spatial resolution. The main goal of this research is a new method for Sentinel-2 and PlanetScope imagery fusion. The fusion method validation was provided based on the land-cover classification accuracy. Three land-cover classifications were made based on the Sentinel-2, PlanetScope and fused imagery. As expected, results show better accuracy for PS and fused imagery than the Sentinel-2 imagery. PlanetScope and fused imagery have almost the same accuracy. For the vegetation monitoring testing, the Normalized Difference Vegetation Index (NDVI) from Sentinel-2 and fused imagery was calculated and mutually compared. In this research, all methods and tests, image fusion and satellite imagery classification were made in the free and open source programs. The method developed and presented in this paper can easily be applied to other sciences, such as urbanism, forestry, agronomy, ecology and geology. © Authors 2018. CC BY 4.0 License.","Fusion reactions; Image fusion; Image resolution; Regional planning; Remote sensing; Vegetation; Environmental development; Land cover classification; Normalized difference vegetation index; Open source projects; PlanetScope; Sentinel-2; Spectral characteristics; Vegetation monitoring; Satellite imagery","Fusion; PlanetScope; Remote Sensing; Sentinel-2; Vegetation","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056169430"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","Sar and optical data fusion based on anisotropic diffusion with Pca and classification using patch-based svm with Lbp","2020","2020 IEEE India Geoscience and Remote Sensing Symposium, InGARSS 2020 - Proceedings","","","9358949","25","28","3","10.1109/InGARSS48198.2020.9358949","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102250014&doi=10.1109%2fInGARSS48198.2020.9358949&partnerID=40&md5=d19956c7485f137b5429555220d6ba7f","SAR (VV and VH polarization) and optical data are widely used in image fusion to use the complimentary information of each other and to obtain the better-quality image (in terms of spatial and spectral features) for the improved classification results. The optical data acquisition depends on whether conditions while SAR data can acquire the data in presence of clouds. This paper uses anisotropic diffusion with PCA for the fusion of SAR (Sentinel 1 (S1)) and Optical (Sentinel 2 (S2)) data for patch-based SVM Classification with LBP (LBP-PSVM). Fusion results with VV polarization performed better than VH polarization using considered fusion method. Classification results suggests that the LBP-PSVM classifier is more effective in comparison to SVM and PSVM classifiers for considered data. © 2020 IEEE.","Data acquisition; Geology; Image enhancement; Image fusion; Optical anisotropy; Polarization; Remote sensing; Support vector machines; Anisotropic Diffusion; Classification results; Fusion methods; Optical data; Patch based; Quality image; Spectral feature; SVM classification; Classification (of information)","Anisotropic Diffusion; Fusion; Local Binary Pattern; Principal Component Analysis; SVM","Conference paper","Final","","Scopus","2-s2.0-85102250014"
"Awuah K.T.; Aplin P.","Awuah, Kwame T. (57204514082); Aplin, Paul (6701865572)","57204514082; 6701865572","FUSION OF SENTINEL-2 DATA WITH HIGH RESOLUTION OPEN ACCESS PLANET BASEMAPS FOR GRAZING LAWN DETECTION IN SOUTHERN AFRICAN SAVANNAHS","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","1409","1412","3","10.1109/IGARSS47720.2021.9554156","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129887543&doi=10.1109%2fIGARSS47720.2021.9554156&partnerID=40&md5=5af0884cd24f489bb195729c72e7808e","Short grass grazing lawn patches are significant components of habitat heterogeneity in southern African savannah ecosystems. Accurate maps of grazing lawn distribution is essential to enhance understanding of important ecosystem processes such as mega-herbivore population dynamics, nutrient cycling and plant community composition. The inherent heterogeneity of savannah landscapes however creates significant challenges for accurate discrimination of vegetation components and thus grazing lawn detection. Recent studies favour very high spatial resolution (VHR) multi-spectral imagery for dealing with this challenge. However, such data are costly for use in operational management. Planet Labs, through Norway's International Climate and Forests Initiative (NICFI), now grant free access to high-resolution, analysis-ready mosaics over the tropics, with great potential for fine-scale vegetation mapping. However, the spectral characteristics of these data are limited and fail to resolve the spectral similarity of different savannah vegetation components. We address these issues using Gram-Schmidt transformation to fuse Planet Basemaps and Sentinel-2A images for grazing lawn detection within the Lower Sabie region of Kruger National Park, South Africa. The original and fused images were classified using a random forest approach. Overall, the fused image achieved the best grazing lawn detection accuracy (0.85) and general map accuracy (0.72) results compared to Sentinel-2 (0.67 and 0.62) and Planet basemap (0.64 and 0.62 respectively). Our findings provide a foundation for cost-effective and accurate high spatial resolution vegetation mapping in heterogenous savannah landscapes. Further studies will investigate the potential of multi-temporal fused data and object-based approaches for enhanced savannah vegetation mapping. © 2021 IEEE","Cost effectiveness; Decision trees; Ecosystems; Forestry; Image resolution; Open Data; Photomapping; Random forests; Remote sensing; Spectroscopy; Vegetation; Fused images; Grazing lawn; High resolution; OpenAccess; Planet basemap; Random forests; Savannah; Sentinel-2; Vegetation components; Vegetation mapping; Image fusion","grazing lawn; Image fusion; Planet basemaps; Random forest; savannah; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85129887543"
"Li Z.; Zhang H.K.; Roy D.P.; Yan L.; Huang H.; Li J.","Li, Zhongbin (56289297800); Zhang, Hankui K. (34874017400); Roy, David P. (7402439028); Yan, Lin (55877448700); Huang, Haiyan (57103968500); Li, Jian (57182441800)","56289297800; 34874017400; 7402439028; 55877448700; 57103968500; 57182441800","Landsat 15-m Panchromatic-Assisted Downscaling (LPAD) of the 30-m reflective wavelength bands to Sentinel-2 20-m resolution","2017","Remote Sensing","9","7","755","","","","10.3390/rs9070755","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032188743&doi=10.3390%2frs9070755&partnerID=40&md5=39cf34cb932fd371a5e20f57a529859d","The Landsat 15-m Panchromatic-Assisted Downscaling (LPAD) method to downscale Landsat-8 Operational Land Imager (OLI) 30-m data to Sentinel-2 multi-spectral instrument (MSI) 20-m resolution is presented. The method first downscales the Landsat-8 30-m OLI bands to 15-m using the spatial detail provided by the Landsat-8 15-m panchromatic band and then reprojects and resamples the downscaled 15-m data into registration with Sentinel-2A 20-m data. The LPAD method is demonstrated using pairs of contemporaneous Landsat-8 OLI and Sentinel-2A MSI images sensed less than 19 min apart over diverse geographic environments. The LPAD method is shown to introduce less spectral and spatial distortion and to provide visually more coherent data than conventional bilinear and cubic convolution resampled 20-m Landsat OLI data. In addition, results for a pair of Landsat-8 and Sentinel-2A images sensed one day apart suggest that image fusion should be undertaken with caution when the images are acquired under different atmospheric conditions. The LPAD source code is available at GitHub for public use. © 2017 by the authors.","Image fusion; Image registration; Atmospheric conditions; Down-scaling; LANDSAT; Operational land imager; Reflective wavelength; Resampling; Sentinel-2; Spatial distortion; Data visualization","20-m resolution; Downscaling; Image registration; Landsat-8; Resampling; Sentinel-2","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85032188743"
"Shakya A.; Biswas M.; Pal M.","Shakya, Achala (57211441799); Biswas, Mantosh (55445658100); Pal, Mahesh (7101848782)","57211441799; 55445658100; 7101848782","CNN-based fusion and classification of SAR and Optical data","2020","International Journal of Remote Sensing","41","22","","8839","8861","22","10.1080/01431161.2020.1783713","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091284696&doi=10.1080%2f01431161.2020.1783713&partnerID=40&md5=d676e5badabec051c631460b03f316e5","Image fusion combines the images of different spectral, spatial, multi-date, as well as radiometric data to achieve a better quality image for improved classification results. Recently, Convolution Neural Network (CNN)-based classification algorithms are extensively used for remote sensing applications. Keeping this in view, present work proposes to use CNN-based fusion and classification of Sentinel 1 (VV and VH polarization) and Sentinel 2 datasets acquired over an agricultural area near Hisar (India). For image fusion, three CNN-based approaches are used to fuse Sentinel 2 (10 m) data with VV and VH bands of Sentinel 1 data. After fusion, classification was performed using 2D-CNN classifier to judge the performance of fused images in terms of classification accuracy. Results suggest that out of the three fusion approaches, only infrared image fusion (IVF) approach performed well with the considered dataset in terms of fusion indicators and classification accuracy. Keeping in view of its better performance, this study proposes a modified IVF approach by using different image pyramid methods. Comparison of results suggests an improved performance by modified IVF approach for the fusion of Sentinel 2 and Sentinel 1 data in comparison with the original IVF approach. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Haryana; Hisar; India; Agricultural robots; Image classification; Image enhancement; Image fusion; Infrared imaging; Radar imaging; Remote sensing; Agricultural areas; Classification accuracy; Classification algorithm; Classification results; Convolution neural network; Image pyramids; Radiometric data; Remote sensing applications; algorithm; artificial neural network; image classification; satellite data; satellite imagery; Sentinel; synthetic aperture radar; Classification (of information)","","Article","Final","","Scopus","2-s2.0-85091284696"
"Li X.; Long J.; Zhang M.; Liu Z.; Lin H.","Li, Xinyu (57276782800); Long, Jiangping (27172354400); Zhang, Meng (56746635600); Liu, Zhaohua (57206668135); Lin, Hui (36071585400)","57276782800; 27172354400; 56746635600; 57206668135; 36071585400","Coniferous plantations growing stock volume estimation using advanced remote sensing algorithms and various fused data","2021","Remote Sensing","13","17","3468","","","","10.3390/rs13173468","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114207266&doi=10.3390%2frs13173468&partnerID=40&md5=52de953a5d27c3c6fc754902e77d47d9","Spatial distribution prediction of growing stock volume (GSV) for supporting the sustain-able management of forest ecosystems, is one of the most widespread applications of remote sens-ing. For this purpose, remote sensing data were used as predictor variables in combination with ground data obtained from field sample plots. However, with the increase in forest GSV values, the spectral reflectance of remote sensing imagery is often saturated or less sensitive to the GSV changes, making accurate estimation difficult. To improve this, we examined the GSV estimation performance and data saturation of four optical remote sensing image datasets (Landsat 8, Sentinel-2, ZiYuan-3, and GaoFen-2) in the subtropical region of Central South China. First, various feature variables were extracted and three optimization methods were used to select optimal feature variable combinations. Subsequently, k-nearest-neighbor (kNN), random forest regression, and cate-gorical boosting algorithms were employed to build the GSV estimation models, and evaluate the GSV estimation accuracy and saturation. Second, Gram Schmidt (GS) and NNDiffuse pan sharpening (NND) methods were employed to fuse the optimal multispectral images and explore various image fusion schemes suitable for GSV estimation. We proposed an adaptive stacking (AdaStacking) model ensemble algorithm to further improve GSV estimation performance. The results indicated that Sentinel-2 had the highest GSV estimation accuracy exhibiting a minimum relative root mean square error of 20.06% and saturation of 434 m3/ha, followed by GaoFen-2 with a minimum relative root mean square error of 22.16% and a saturation of 409 m3/ha. Among the four fusion images, the NND-B2 image—obtained by fusing the GaoFen-2 green band and Sentinel-2 multispectral image with the NND method—had the best estimation accuracy. The estimated optimal RMSEs of NND-B2 were 24.4% and 16.5% lower than those of GaoFen-2 and Sentinel-2, respectively. Therefore, the fused image data based on GF-2 and Sentinel-2 can effectively couple the advantages of the two images and significantly improve the GSV estimation performance. Moreover, the proposed adaptive stacking model is more effective in GSV estimation than a single model. The GSV estimation saturation value of the AdaStacking model based on NND-B2 was 5.4% higher than that of the KNN-Maha model. The GSV distribution map estimated by AdaStacking model used the NND-B2 dataset corresponded accurately with the field observations. This study provides some insights into the optical image fusion scheme, feature selection, and adaptive modeling algorithm in GSV estimation for coniferous forest. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Decision trees; Ecosystems; Feature extraction; Forestry; Geometrical optics; Image enhancement; Image fusion; Mean square error; Nearest neighbor search; Adaptive modeling algorithms; Coniferous plantations; Estimation performance; K nearest neighbor (KNN); Optical remote sensing; Remote sensing algorithms; Remote sensing imagery; Root mean square errors; Remote sensing","Data saturation; Feature combination optimization; Growing stock volume; Multispectral imagery fusion; Regression modeling algorithm","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85114207266"
"Shao Z.; Cai J.; Fu P.; Hu L.; Liu T.","Shao, Zhenfeng (7202244409); Cai, Jiajun (57193551962); Fu, Peng (57207967909); Hu, Leiqiu (55642592500); Liu, Tao (56043220000)","7202244409; 57193551962; 57207967909; 55642592500; 56043220000","Deep learning-based fusion of Landsat-8 and Sentinel-2 images for a harmonized surface reflectance product","2019","Remote Sensing of Environment","235","","111425","","","","10.1016/j.rse.2019.111425","135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073688762&doi=10.1016%2fj.rse.2019.111425&partnerID=40&md5=c007c08a8b1222b20426acc2492efc67","Landsat and Sentinel-2 sensors together provide the most widely accessible medium-to-high spatial resolution multispectral data for a wide range of applications, such as vegetation phenology identification, crop yield estimation, and forest disturbance detection. Improved timely and accurate observations of the Earth's surface and dynamics are expected from the synergistic use of Landsat and Sentinel-2 data, which entails coordinating the spatial resolution gap between Landsat (30 m) and Sentinel-2 (10 m or 20 m) images. However, widely used data fusion techniques may not fulfil community's needs for generating a temporally dense reflectance product at 10 m spatial resolution from combined Landsat and Sentinel-2 images because of their inherent algorithmic weaknesses. Inspired by the recent advances in deep learning, this study developed an extended super-resolution convolutional neural network (ESRCNN) to a data fusion framework, specifically for blending Landsat-8 Operational Land Imager (OLI) and Sentinel-2 Multispectral Imager (MSI) data. Results demonstrated the effectiveness of the deep learning-based fusion algorithm in yielding a consistent and comparable dataset at 10 m from Landsat-8 and Sentinel-2. Further accuracy assessments revealed that the performance of the fusion network was influenced by both the number of input auxiliary Sentinel-2 images and temporal interval (i.e., difference in image acquisition dates) between auxiliary Sentinel-2 images and the target Landsat-8 image. Compared to the benchmark algorithm, area-to-point regression kriging (ATKPK), the deep learning-based fusion framework proved better in the quantitative assessment in terms of RMSE (root mean square error), correlation coefficient (CC), universal image quality index (UIQI), relative global-dimensional synthesis error (ERGAS), and spectral angle mapper (SAM). ESRCNN better preserved the reflectance distribution as the original image compared to ATPRK, resulting in an improved image quality. Overall, the developed data fusion network that blends Landsat-8 and Sentinel-2 images has the potential to help generate continuous reflectance observations of higher temporal frequency than that can be obtained from a single Landsat-like sensor. © 2019 Elsevier Inc.","Benchmarking; Data fusion; Image enhancement; Image fusion; Image quality; Image resolution; Learning algorithms; Mean square error; Neural networks; Reflection; Sensor data fusion; Continuous monitoring; Convolutional neural network; Correlation coefficient; High spatial resolution; LANDSAT; Quantitative assessments; RMSE (root mean square error); Sentinel-2; benchmarking; detection method; estimation method; Landsat; qualitative analysis; satellite data; satellite imagery; Sentinel; spatial resolution; surface reflectance; vegetation mapping; Deep learning","Continuous monitoring; Data fusion; Deep learning; Landsat-8; Sentinel-2","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85073688762"
"Korosov A.A.; Pozdnyakov D.V.","Korosov, Anton A. (6505884703); Pozdnyakov, Dmitry V. (56370460300)","6505884703; 56370460300","Fusion of data from Sentinel-2/MSI and Sentinel-3/OLCI","2016","European Space Agency, (Special Publication) ESA SP","SP-740","","","","","","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988527720&partnerID=40&md5=5447134425902a85de7c3bf365940a75","Multisensor image fusion is the process of combining relevant information from two or more satellite images into a single image. The fused image can have complementary spatial and spectral resolution characteristics. We suggest a method for fusion of data from Sentinel-2 multi spectral imager (MSI) and Sentinel-3 Ocean and Land Color Instrument (OLCI). In the visible range MSI measures radiance with 10 m resolution at 490, 560 and 665 nm; with 20 m resolution at 705 nm; and with 60 m resolution at 443 nm. In the visible range OLCI measures with 300 m spatial resolution at 400, 412, 443, 490, 510, 560, 620, 665, 673, 681, 708 nm. The data from the visible from both sensors is fused to get products with values of remote sensing reflectance wavelengths of OLCI and with spatial resolution of 60 m using an artificial neural network.","Image fusion; Image resolution; Neural networks; Spectroscopy; Fused images; Multi spectral imager; Multisensor image fusion; Remote-sensing reflectance; Satellite images; Single images; Spatial resolution; Visible range; Remote sensing","","Conference paper","Final","","Scopus","2-s2.0-84988527720"
"Hoque M.R.U.; Wu J.; Kwan C.; Koperski K.; Li J.","Hoque, Md Reshad Ul (57215344852); Wu, Jian (57193141747); Kwan, Chiman (7201421216); Koperski, Krzysztof (6603540174); Li, Jiang (56226550100)","57215344852; 57193141747; 7201421216; 6603540174; 56226550100","ArithFusion: An Arithmetic Deep Model for Temporal Remote Sensing Image Fusion","2022","Remote Sensing","14","23","6160","","","","10.3390/rs14236160","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143761796&doi=10.3390%2frs14236160&partnerID=40&md5=6d162c4fc285294f57b7c7e89e83f5f4","Different satellite images may consist of variable numbers of channels which have different resolutions, and each satellite has a unique revisit period. For example, the Landsat-8 satellite images have 30 m resolution in their multispectral channels, the Sentinel-2 satellite images have 10 m resolution in the pan-sharp channel, and the National Agriculture Imagery Program (NAIP) aerial images have 1 m resolution. In this study, we propose a simple yet effective arithmetic deep model for multimodal temporal remote sensing image fusion. The proposed model takes both low- and high-resolution remote sensing images at (Formula presented.) together with low-resolution images at a future time (Formula presented.) from the same location as inputs and fuses them to generate high-resolution images for the same location at (Formula presented.). We propose an arithmetic operation applied to the low-resolution images at the two time points in feature space to take care of temporal changes. We evaluated the proposed model on three modality pairs for multimodal temporal image fusion, including downsampled WorldView-2/original WorldView-2, Landsat-8/Sentinel-2, and Sentinel-2/NAIP. Experimental results show that our model outperforms traditional algorithms and recent deep learning-based models by large margins in most scenarios, achieving sharp fused images while appropriately addressing temporal changes. © 2022 by the authors.","Antennas; Deep learning; Generative adversarial networks; Image fusion; Satellite imagery; Space optics; Deep learning; Generative adversarial network; HRNet; LANDSAT; Neural-networks; Remote sensing images; Remote-sensing; Satellite images; Superresolution; U-net; Remote sensing","deep learning; generative adversarial network (GAN); HRNet; image fusion; neural networks; remote sensing; super-resolution; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143761796"
"Yuan Y.; Meng X.; Sun W.; Yang G.; Wang L.; Peng J.; Wang Y.","Yuan, Yi (57874623700); Meng, Xiangchao (56158755000); Sun, Weiwei (55726567900); Yang, Gang (57192178476); Wang, Lihua (57221651796); Peng, Jiangtao (24833160700); Wang, Yumiao (57874832700)","57874623700; 56158755000; 55726567900; 57192178476; 57221651796; 24833160700; 57874832700","Multi-Resolution Collaborative Fusion of SAR, Multispectral and Hyperspectral Images for Coastal Wetlands Mapping","2022","Remote Sensing","14","14","3492","","","","10.3390/rs14143492","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137272381&doi=10.3390%2frs14143492&partnerID=40&md5=af4929a341e9f100543bba2531bcd73e","The hyperspectral, multispectral, and synthetic aperture radar (SAR) remote sensing images provide complementary advantages in high spectral resolution, high spatial resolution, and geometric and polarimetric properties, generally. How to effectively integrate cross-modal information to obtain a high spatial resolution hyperspectral image with the characteristics of the SAR is promising. However, due to divergent imaging mechanisms of modalities, existing SAR and optical image fusion techniques generally remain limited due to the spectral or spatial distortions, especially for complex surface features such as coastal wetlands. This paper provides, for the first time, an efficient multi-resolution collaborative fusion method for multispectral, hyperspectral, and SAR images. We improve generic multi-resolution analysis with spectral-spatial weighted modulation and spectral compensation to achieve minimal spectral loss. The backscattering gradients of SAR are guided to fuse, which is calculated from saliency gradients with edge preserving. The experiments were performed on ZiYuan-1 02D (ZY-1 02D) and GaoFen-5B (AHSI) hyperspectral, Sentinel-2 and GaoFen-5B (VIMI) multispectral, and Sentinel-1 SAR images in the challenging coastal wetlands. Specifically, the fusion results were comprehensively tested and verified on the qualitative, quantitative, and classification metrics. The experimental results show the competitive performance of the proposed method. © 2022 by the authors.","Geometrical optics; Hyperspectral imaging; Image fusion; Image resolution; Optical remote sensing; Photomapping; Radar imaging; Space-based radar; Spectral resolution; Spectroscopy; Wetlands; Coastal wetlands; Gaofen-5; High spatial resolution; HyperSpectral; Multi-spectral; Pixel level; Remote-sensing; Synthetic aperture radar images; Synthetic aperture radar remote sensing images; Ziyuan-1 02d; Synthetic aperture radar","classification; coastal wetlands; data fusion; GaoFen-5; hyperspectral; pixel-level; remote sensing; synthetic aperture radar; ZY-1 02D","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137272381"
