"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Baas M.; Kamper H.","Baas, Matthew (57221126842); Kamper, Herman (55009931600)","57221126842; 55009931600","GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models","2023","2022 IEEE Spoken Language Technology Workshop, SLT 2022 - Proceedings","","","","906","911","5","10.1109/SLT54892.2023.10023153","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147793390&doi=10.1109%2fSLT54892.2023.10023153&partnerID=40&md5=f179c45a3ed089de30912e985c358647","We propose AudioStyleGAN (ASGAN), a new generative adversarial network (GAN) for unconditional speech synthesis. As in the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation to probabilistically skip discriminator updates. ASGAN achieves state-of-the-art results in unconditional speech synthesis on the Google Speech Commands dataset. It is also substantially faster than the top-performing diffusion models. Through a design that encourages disentanglement, ASGAN is able to perform voice conversion and speech editing without being explicitly trained to do so. ASGAN demonstrates that GANs are still highly competitive with diffusion models. Code, models, samples: https://github.com/RF5/simple-asgan/.  © 2023 IEEE.","Audio acoustics; Diffusion; Speech synthesis; Aliasing; Audio features; Diffusion model; Images synthesis; Latent vectors; Sampled noise; Speech disentanglement; Synthesis models; Unconditional speech synthesis; Voice conversion; Generative adversarial networks","generative adversarial networks; speech disentanglement; Unconditional speech synthesis; voice conversion","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85147793390"
"Huang W.; Luo M.; Li J.; Zhang P.; Zha Y.","Huang, Wei (56195325600); Luo, Mingyuan (57206482771); Li, Jing (56272795400); Zhang, Peng (55547108553); Zha, Yufei (16044173100)","56195325600; 57206482771; 56272795400; 55547108553; 16044173100","A novel locally-constrained GAN-based ensemble to synthesize arterial spin labeling images","2022","Information Sciences","609","","","691","710","19","10.1016/j.ins.2022.07.091","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134894890&doi=10.1016%2fj.ins.2022.07.091&partnerID=40&md5=64ab738c701f62a01d8763b0db256b85","Arterial spin labeling (ASL) images that are capable to quantitatively measure the cerebral blood flow receive increasing research attention in recent dementia diagnosis studies. However, this important imaging modality is unfortunately not commonly seen in many well-established image-based dementia datasets. Hence, synthesizing ASL images to supplement the important modality in these datasets for further improving the accuracy of dementia diseases diagnosis is quite important and valuable. In this study, a novel locally-constrained generative adversarial networks (GAN)-based ensemble is introduced to fulfill the ASL image synthesis task for improving the dementia diseases diagnosis performance. Technically, new attention-based feature pyramid-GAN models are designed as local models of the novel ensemble. Also, multi-Gaussian-distributed noise is generated from a new flow-based generative model and utilized in medical image synthesis, for the first time. Experiments have been conducted to reveal the effectiveness of the novel GAN ensemble. Comparisons between the novel GAN ensemble and many other state-of-the-art methods in medical image synthesis have been carried out. Statistical analyses have suggested that, accuracies of dementia diseases diagnosis can be significantly improved with the help of the novel GAN ensemble, which brings about 41.62% performance improvement based on a 355-demented-patient dataset and approximately 25% performance improvement from the well-known ADNI-1 dataset. © 2022 Elsevier Inc.","Diagnosis; Gaussian noise (electronic); Image enhancement; Medical imaging; Neurodegenerative diseases; Arterial spin labeling; Cerebral blood flow; Disease diagnosis; Ensemble; Images synthesis; Imaging modality; Labeling image; Network ensemble; Network-based; Performance; Generative adversarial networks","Diagnosis; Ensemble; Generative adversarial network; Image synthesis","Article","Final","","Scopus","2-s2.0-85134894890"
"Yoshikawa T.; Endo Y.; Kanamori Y.","Yoshikawa, Takato (57753921300); Endo, Yuki (37101260100); Kanamori, Yoshihiro (24168681900)","57753921300; 37101260100; 24168681900","Diversifying detail and appearance in sketch-based face image synthesis","2022","Visual Computer","38","9-10","","3121","3133","12","10.1007/s00371-022-02538-7","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132425300&doi=10.1007%2fs00371-022-02538-7&partnerID=40&md5=59ed8e64da35f819d93d9574380ced63","Sketch-based face image synthesis has gained greater attention with the increasing realism of its output images. However, existing studies have overlooked the significance of output diversity: because sketches are inherently ambiguous, it would be desirable to have various output candidates for a single-input sketch. In this paper, we explore synthesis of diverse face images from a single sketch by using a three-stage framework consisting of sketch refinement, detail enhancement, and appearance synthesis. Each stage uses supervised learning with neural networks. With this three-stage framework, we can separately control the detail (e.g., wrinkles and hair structures) and appearance (e.g., skin and hair colors) of output face images separately by using multiple latent codes. Quantitative and quantitative evaluations demonstrate that our method offers greater diversity in its output images than the state-of-the-art methods, while retaining the output realism. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Generative adversarial networks; Appearance synthesis; Deep learning; Detail enhancement; Face image synthesis; Face images; GAN; Images synthesis; Multi-modal; Single input; Sketch-based image synthesis; Image enhancement","Deep learning; GAN; Multimodal; Sketch-based image synthesis","Article","Final","","Scopus","2-s2.0-85132425300"
"Weng Z.-C.; Tsai F.-C.","Weng, Zi-Cheng (57993712300); Tsai, Fu-Ching (57202000661)","57993712300; 57202000661","A Systematic Literature Review of Law Enforcement Image Recognition Methods based on Generative Adversarial Networks Framework","2022","Procedia Computer Science","207","","","3629","3638","9","10.1016/j.procs.2022.09.423","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143358792&doi=10.1016%2fj.procs.2022.09.423&partnerID=40&md5=e6be258e58d27401111cf1307c14380c","Image recognition research based on the Generative Adversarial Networks (GAN) has been widely used in various specialized technical fields because of its universality, adaptability, and scalability of the adversarial framework. The framework achieves the tasks of image generation, synthesis, and transformation to improve the image resolution and recognition rate, which is consistent with the purpose of the police in the image recognition work. However, the task of applying the GAN method to police image recognition is not widely explored. In this paper, we systematically review related research to understand the current applications of GAN in the field of image recognition and summarize their contributions. We analyze 35 academic papers after 2018 to provide a state-of-the-art research stream. According to the GAN applications, we divided the dataset into three domains, which are image-to-image translation, image augmentation, and mixed model. The results show that there is little difference in the number of articles in the three fields. In addition, most of the papers use image conversion methods from more than two domains, which indicates that GAN is flexible in designing the framework according to the research tasks. Based on the methods and challenges in the literature, we further propose future research directions. © 2022 The Authors. Published by Elsevier B.V.","Image enhancement; Image recognition; Image resolution; Law enforcement; 'current; Image generations; Image reccognition; Image transformations; Images synthesis; Network frameworks; Network methods; Recognition methods; Systematic literature review; Technical fields; Generative adversarial networks","GAN; Generative Adversarial Networks; Image reccognition; Law Enforcement","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143358792"
"Schellenberg M.; Gröhl J.; Dreher K.K.; Nölke J.-H.; Holzwarth N.; Tizabi M.D.; Seitel A.; Maier-Hein L.","Schellenberg, Melanie (57219740778); Gröhl, Janek (57191360550); Dreher, Kris K. (57220901243); Nölke, Jan-Hinrich (57209457194); Holzwarth, Niklas (57208643821); Tizabi, Minu D. (57193956328); Seitel, Alexander (22635486300); Maier-Hein, Lena (22634618600)","57219740778; 57191360550; 57220901243; 57209457194; 57208643821; 57193956328; 22635486300; 22634618600","Photoacoustic image synthesis with generative adversarial networks","2022","Photoacoustics","28","","100402","","","","10.1016/j.pacs.2022.100402","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140074220&doi=10.1016%2fj.pacs.2022.100402&partnerID=40&md5=cbef64e28db7cddb4192a963e1c2d23a","Photoacoustic tomography (PAT) has the potential to recover morphological and functional tissue properties with high spatial resolution. However, previous attempts to solve the optical inverse problem with supervised machine learning were hampered by the absence of labeled reference data. While this bottleneck has been tackled by simulating training data, the domain gap between real and simulated images remains an unsolved challenge. We propose a novel approach to PAT image synthesis that involves subdividing the challenge of generating plausible simulations into two disjoint problems: (1) Probabilistic generation of realistic tissue morphology, and (2) pixel-wise assignment of corresponding optical and acoustic properties. The former is achieved with Generative Adversarial Networks (GANs) trained on semantically annotated medical imaging data. According to a validation study on a downstream task our approach yields more realistic synthetic images than the traditional model-based approach and could therefore become a fundamental step for deep learning-based quantitative PAT (qPAT). © 2022 The Authors","Acoustic properties; Deep learning; Inverse problems; Medical imaging; Photoacoustic effect; Supervised learning; Tissue; Tomography; Deep learning; Functional tissue; Images synthesis; Optoacoustic imaging; Optoacoustic tomography; Photo-acoustic imaging; Photoacoustic image; Photoacoustic tomography; Synthetic data; Tissue properties; Generative adversarial networks","Deep learning; Generative adversarial networks; Optoacoustic imaging; Optoacoustic tomography; Photoacoustic imaging; Photoacoustic tomography; Synthetic data","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140074220"
"Sun S.; Zhao B.; Mateen M.; Chen X.; Wen J.","Sun, Song (57210359791); Zhao, Bo (56669080300); Mateen, Muhammad (57205367635); Chen, Xin (57192473631); Wen, Junhao (8950200000)","57210359791; 56669080300; 57205367635; 57192473631; 8950200000","Mask guided diverse face image synthesis","2022","Frontiers of Computer Science","16","3","163311","","","","10.1007/s11704-020-0400-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118949259&doi=10.1007%2fs11704-020-0400-7&partnerID=40&md5=2fa6a517fc0b25db97f828e13e336b36","Recent studies have shown remarkable success in face image generation task. However, existing approaches have limited diversity, quality and controllability in generating results. To address these issues, we propose a novel end-to-end learning framework to generate diverse, realistic and controllable face images guided by face masks. The face mask provides a good geometric constraint for a face by specifying the size and location of different components of the face, such as eyes, nose and mouse. The framework consists of four components: style encoder, style decoder, generator and discriminator. The style encoder generates a style code which represents the style of the result face; the generator translate the input face mask into a real face based on the style code; the style decoder learns to reconstruct the style code from the generated face image; and the discriminator classifies an input face image as real or fake. With the style code, the proposed model can generate different face images matching the input face mask, and by manipulating the face mask, we can finely control the generated face image. We empirically demonstrate the effectiveness of our approach on mask guided face image synthesis task. © 2022, Higher Education Press.","Decoding; Image processing; Mammals; Signal encoding; End to end; Face image generation; Face image synthesis; Face images; Face masks; Geometric constraint; Image generations; Image translation; Image-guided; Learning frameworks; Generative adversarial networks","face image generation; generative adversarial networks; image translation","Article","Final","","Scopus","2-s2.0-85118949259"
"Qamar R.; Bajao N.; Suwarno I.; Jokhio F.A.","Qamar, Roheen (57991262900); Bajao, Naomi (57990984400); Suwarno, Iswanto (56596730700); Jokhio, Fareed Ahmed (57990984500)","57991262900; 57990984400; 56596730700; 57990984500","Survey on Generative Adversarial Behavior in Artificial Neural Tasks","2022","Iraqi Journal for Computer Science and Mathematics","3","2","","83","94","11","10.52866/ijcsm.2022.02.01.009","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143239628&doi=10.52866%2fijcsm.2022.02.01.009&partnerID=40&md5=546fe3dbe76bc33e02dbc2e51aaa2f7c","Generative opposing networking is a technique for learning deep representations in the absence of a large amount of annotated training data. This competitive technique employs two networks to generate background signals. Generative adversarial networks (GANs) use learned representations for a variety of applications, including image synthesis, semantic imaging, style transfer, super magnification, and segmentation. Images can be utilized in many ways. GANs are a unique class that has recently received considerable interest because of the popularity of deep generative models. GANs implicitly distribute complex and high-resolution images, sounds, and data. However, given inadvertently built network architecture, objective function usage, and optimization algorithm selection, significant difficulties, such as mode collapse, inconsistencies, and instability, develop while training GANs. This study conducts a thorough examination of the developments in GANs design and optimization strategies presented to address GANs’ difficulties. We provide intriguing study possibilities in this rapidly evolving area. GANs are a popular study topic because of their ability to generate synthetic data and the benefits of representations that can be understood regardless of the application. While various reviews for GANs in the image processing arena have been undertaken to date, none have focused on the review of GANs in multi-disciplinary domains. Thus, this study investigates the utilization of GANs in interdisciplinary application fields and their implementation issues by thoroughly searching for research articles connected to GAN. © 2022 Autoctonía. Revista de Ciencias Sociales e Historia. All rights reserved.","","Deep learning; GANs Applications; GANs challenges; Generative Adversarial Networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143239628"
"Ranjan A.; Lalwani D.; Misra R.","Ranjan, Amit (57323808400); Lalwani, Debanshu (57324742100); Misra, Rajiv (7203051070)","57323808400; 57324742100; 7203051070","GAN for synthesizing CT from T2-weighted MRI data towards MR-guided radiation treatment","2022","Magnetic Resonance Materials in Physics, Biology and Medicine","35","3","","449","457","8","10.1007/s10334-021-00974-5","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118571958&doi=10.1007%2fs10334-021-00974-5&partnerID=40&md5=096274e28a30e6b9e2de69b04107fb1a","Objective: In medical domain, cross-modality image synthesis suffers from multiple issues , such as context-misalignment, image distortion, image blurriness, and loss of details. The fundamental objective behind this study is to address these issues in estimating synthetic Computed tomography (sCT) scans from T2-weighted Magnetic Resonance Imaging (MRI) scans to achieve MRI-guided Radiation Treatment (RT). Materials and methods: We proposed a conditional generative adversarial network (cGAN) with multiple residual blocks to estimate sCT from T2-weighted MRI scans using 367 paired brain MR-CT images dataset. Few state-of-the-art deep learning models were implemented to generate sCT including Pix2Pix model, U-Net model, autoencoder model and their results were compared, respectively. Results: Results with paired MR-CT image dataset demonstrate that the proposed model with nine residual blocks in generator architecture results in the smallest mean absolute error (MAE) value of 0.030 ± 0.017 , and mean squared error (MSE) value of 0.010 ± 0.011 , and produces the largest Pearson correlation coefficient (PCC) value of 0.954 ± 0.041 , SSIM value of 0.823 ± 0.063 and peak signal-to-noise ratio (PSNR) value of 21.422 ± 3.964 , respectively. We qualitatively evaluated our result by visual comparisons of generated sCT to original CT of respective MRI input. Discussion: The quantitative and qualitative comparison of this work demonstrates that deep learning-based cGAN model can be used to estimate sCT scan from a reference T2 weighted MRI scan. The overall accuracy of our proposed model outperforms different state-of-the-art deep learning-based models. © 2021, European Society for Magnetic Resonance in Medicine and Biology (ESMRMB).","Delayed Emergence from Anesthesia; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Radiotherapy, Image-Guided; Tomography, X-Ray Computed; delayed emergence from anesthesia; human; image guided radiotherapy; image processing; nuclear magnetic resonance imaging; procedures; x-ray computed tomography","Generative adversarial networks; Image synthesis; MRI; Synthetic CT","Article","Final","","Scopus","2-s2.0-85118571958"
"Hinz T.; Heinrich S.; Wermter S.","Hinz, Tobias (57191251410); Heinrich, Stefan (36903266000); Wermter, Stefan (7003826680)","57191251410; 36903266000; 7003826680","Semantic Object Accuracy for Generative Text-to-Image Synthesis","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","3","","1552","1565","13","10.1109/TPAMI.2020.3021209","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124051702&doi=10.1109%2fTPAMI.2020.3021209&partnerID=40&md5=130bfa3f42e2e9874f988b198615f0f3","Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g., whether an image generated from 'a car driving down the street' contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.  © 1979-2012 IEEE.","Algorithms; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics; Object detection; Quality control; Semantics; Evaluation metrics; Evaluation of generative model; Generative adversarial network; Generative model; Image caption; Image modeling; Images synthesis; Semantic objects; Text-to-image synthesis; algorithm; human; image processing; procedures; semantics; Generative adversarial networks","evaluation of generative models; generative adversarial network (GAN); generative models; Text-to-image synthesis","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124051702"
"De Souza V.L.T.; Marques B.A.D.; Gois J.P.","De Souza, Vinicius Luis Trevisan (58069017100); Marques, Bruno Augusto Dorta (57193416311); Gois, Joao Paulo (22234000200)","58069017100; 57193416311; 22234000200","Fundamentals and Challenges of Generative Adversarial Networks for Image-based Applications","2022","Proceedings - 2022 35th Conference on Graphics, Patterns, and Images, SIBGRAPI 2022","","","","308","313","5","10.1109/SIBGRAPI55357.2022.9991776","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146439487&doi=10.1109%2fSIBGRAPI55357.2022.9991776&partnerID=40&md5=305dfe8736e56a9b7380a77086f5d374","Significant advances in image-based applications have been achieved in recent years, many of which are arguably due to recent developments in Generative Adversarial Networks (GANs). Although the continuous improvement in the architectures of GAN has significantly increased the quality of synthetic images, this is not without challenges such as training stability and convergence issues, to name a few. In this work, we present the fundamentals and notable architectures of GANs, especially for image-based applications. We also discuss relevant issues such as training problems, diversity generation, and quality assessment (metrics).  © 2022 IEEE.","Computer vision; Deep neural networks; Image enhancement; Network architecture; Continuous improvements; Convergence issues; Deep image synthesis; Diversity generations; Image manipulation; Image-based application; Images synthesis; Stability and convergence; Stability issues; Synthetic images; Generative adversarial networks","deep image synthesis; deep neural network; Generative Adversarial Network; image manipulation","Conference paper","Final","","Scopus","2-s2.0-85146439487"
"Wang B.; Wu T.; Zhu M.; Du P.","Wang, Bo (57550814800); Wu, Tao (57716422100); Zhu, Minfeng (57550416900); Du, Peng (57549259800)","57550814800; 57716422100; 57550416900; 57549259800","Interactive Image Synthesis with Panoptic Layout Generation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","7773","7782","9","10.1109/CVPR52688.2022.00763","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143258155&doi=10.1109%2fCVPR52688.2022.00763&partnerID=40&md5=e5d75c4154d17b23dc1166a9d9779ed4","Interactive image synthesis from user-guided input is a challenging task when users wish to control the scene structure of a generated image with ease. Although remarkable progress has been made on layout-based image synthesis approaches, existing methods require high-precision inputs such as accurately placed bounding boxes, which might be constantly violated in an interactive setting. When placement of bounding boxes is subject to perturbation, layout-based models suffer from 'missing regions' in the constructed semantic layouts and hence undesirable artifacts in the generated images. In this work, we propose Panoptic Layout Generative Adversarial Network (PLGAN) to address this challenge. The PLGAN employs panoptic theory which distinguishes object categories between 'stuff' with amorphous boundaries and 'things' with well-defined shapes, such that stuff and instance layouts are constructed through separate branches and later fused into panoptic layouts. In particular, the stuff layouts can take amorphous shapes and fill up the missing regions left out by the instance layouts. We experimentally compare our PLGAN with state-of-the-art layout-based models on the COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN are not only visually demonstrated but quantitatively verified in terms of inception score, Fréchet inception distance, classification accuracy score, and coverage. The code is available at https://github.com/wb-finalking/PLGAN. © 2022 IEEE.","Computer vision; Semantics; Bounding-box; High-precision; Image and video synthesis and generation; Images synthesis; Interactive images; Layout generations; Scene structure; User-guided; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143258155"
"Wang X.; Jin K.; Yu K.; Cheng Y.","Wang, Xuesong (57887304600); Jin, Ke (57664741500); Yu, Kun (57262750700); Cheng, Yuhu (9733966500)","57887304600; 57664741500; 57262750700; 9733966500","Asymmetric Training in RealnessGAN","2022","IEEE Transactions on Multimedia","","","","1","13","12","10.1109/TMM.2022.3233307","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146237154&doi=10.1109%2fTMM.2022.3233307&partnerID=40&md5=acc6ea65a301191441d22b08c51009c6","Generative adversarial networks (GANs) have demonstrated superior performances in image generation. In recent years, various improvements of network structure and learning theory related to GANs have undergone numerous advancement. Among these improvement techniques, the asymmetric training on the generator and discriminator networks has been widely adopted. For example, the batch normalization is used in generator while the spectral normalization is used in discriminator, or using different learning rates for the generator and discriminator. However, the asymmetric training on the real and generated samples has not been taken into consideration till now. In this paper, we proposed a novel asymmetric training-based RealnessGAN (ATRGAN) which applies the idea of asymmetric training on both samples and networks. Specifically, the asymmetric training on samples refers to performing the differential learning on the real and generated samples by controlling the information entropies of real and fake anchor distributions. The asymmetric training on networks is realized via the sampling transmission <inline-formula><tex-math notation=""LaTeX"">$G2D$</tex-math></inline-formula>, which abandons the commonly used independent random sampling. With the help of <inline-formula><tex-math notation=""LaTeX"">$G2D$</tex-math></inline-formula>, the discriminator can obtain a dominant training position than the generator, so as to ensure that the discriminator can guide the generator more effectively during training. In addition, we proposed the floating anchor distribution technique and constructed the objective function of generator for ATRGAN. Through comparative experiments, we demonstrated ATRGAN&#x0027;s ability of achieving better generation performance than various SOTA GANs on CIFAR-10, CAT, and CelebA-HQ datasets. IEEE","Job analysis; Latexes; Linear programming; Anchor distributions; Asymmetric training; Floating anchor distribution; Generator; Images synthesis; Information entropy; Linear-programming; Realnessgan; Sampling transmission; Task analysis; Generative adversarial networks","asymmetric training; floating anchor distribution; Generative adversarial networks; Generators; Image synthesis; information entropy; Linear programming; Painting; RealnessGAN; sampling transmission; Task analysis; Training","Article","Article in press","","Scopus","2-s2.0-85146237154"
"Chen H.; Wang Y.; Lagadec B.; Dantcheva A.; Bremond F.","Chen, Hao (57218309973); Wang, Yaohui (57205448482); Lagadec, Benoit (24778579200); Dantcheva, Antitza (22834321400); Bremond, Francois (6602918684)","57218309973; 57205448482; 24778579200; 22834321400; 6602918684","Learning Invariance from Generated Variance for Unsupervised Person Re-identification","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","15","14","10.1109/TPAMI.2022.3226866","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144800621&doi=10.1109%2fTPAMI.2022.3226866&partnerID=40&md5=992a5fcf8e005f82f427492c6a3f7fbc","This work focuses on unsupervised representation learning in person re-identification (ReID). Recent self-supervised contrastive learning methods learn invariance by maximizing the representation similarity between two augmented views of a same image. However, traditional data augmentation may bring to the fore undesirable distortions on identity features, which is not always favorable in id-sensitive ReID tasks. In this paper, we propose to replace traditional data augmentation with a generative adversarial network (GAN) that is targeted to generate augmented views for contrastive learning. A 3D mesh guided person image generator is proposed to disentangle a person image into id-related and id-unrelated features. Deviating from previous GAN-based ReID methods that only work in id-unrelated space (pose and camera style), we conduct GAN-based augmentation on both id-unrelated and id-related features. We further propose specific contrastive losses to help our network learn invariance from id-unrelated and id-related augmentations. By jointly training the generative and the contrastive modules, our method achieves new state-of-the-art unsupervised person ReID performance on mainstream large-scale benchmarks. IEEE","Benchmarking; Cameras; Learning systems; Lighting; Three dimensional displays; Contrastive learning; Data augmentation; Generator; Image color analysis; Images synthesis; Learn+; Person re identifications; Representation disentanglement; Shape; Three-dimensional display; Generative adversarial networks","Cameras; contrastive learning; data augmentation; Generative adversarial networks; Generators; Image color analysis; image synthesis; Lighting; Person re-identification; representation disentanglement; Shape; Three-dimensional displays","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85144800621"
"Li R.; Li W.; Yang Y.; Bai Q.","Li, Ruijun (57954043200); Li, Weihua (57190971520); Yang, Yi (57204940195); Bai, Quan (56962723200)","57954043200; 57190971520; 57204940195; 56962723200","Obj-SA-GAN: Object-Driven Text-to-Image Synthesis with Self-Attention Based Full Semantic Information Mining","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13629 LNCS","","","339","350","11","10.1007/978-3-031-20862-1_25","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144553633&doi=10.1007%2f978-3-031-20862-1_25&partnerID=40&md5=ec4ce45d2dfd6cb1baa0a84d73333eae","In recent years, text-to-image synthesis techniques have made considerable breakthroughs, but the progress is restricted to simple scenes. Such techniques turn out to be ineffective if the text appears complex and contains multiple objects. To address this challenging issue, we propose a novel text-to-image synthesis model called Object-driven Self-Attention Generative Adversarial Network (Obj-SA-GAN), where self-attention mechanisms are utilised to analyse the information with different granularities at different stages, achieving full exploitation of text semantic information from coarse to fine. Complex datasets are used to evaluate the performance of the proposed model. The experimental results explicitly show that our model outperforms the state-of-the-art methods. This is because the proposed Obj-SA-GAN model utilises textual information, which provides a better understanding of complex scenarios. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Complex networks; Data mining; Image processing; Semantics; Attention; GAN; Images synthesis; Information mining; Network objects; Self-attention; Semantics Information; Semantics mining; Synthesis techniques; Text-to-image synthesis; Generative adversarial networks","Attention; GAN; Self-Attention; Semantic mining; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85144553633"
"Berezsky O.; Liashchynskyi P.; Pitsun O.; Liashchynskyi P.; Berezkyy M.","Berezsky, Oleh (16479742300); Liashchynskyi, Petro (57202448801); Pitsun, Oleh (57190575875); Liashchynskyi, Pavlo (57202448800); Berezkyy, Mykola (58020232600)","16479742300; 57202448801; 57190575875; 57202448800; 58020232600","Comparison of Deep Neural Network Learning Algorithms for Biomedical Image Processing","2022","CEUR Workshop Proceedings","3302","","","135","145","10","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144236311&partnerID=40&md5=2478c69b18570feaf3c9fcececde6d84","In recent years, the popularity of deep neural networks used for various problem-solving tasks has increased dramatically. The main tasks include image classification and synthesis using convolutional and generative-adversarial neural networks. These types of networks need large amounts of training data to achieve the required accuracy and performance. In addition, these networks have a long training time. The authors of the paper analyzed and compared the gradient-based neural network learning algorithms. The biomedical image classification with the use of a convolutional neural network of a given architecture was carried out. A comparison of learning algorithms (SGD, Adadelta, RMSProp, Adam, Adamax, Adagrad, and Nadam) was made according to the following parameters: training time, training loss, training accuracy, test loss, and test accuracy. For the experiments, the authors used the Python programming language, the Keras machine learning library, and the Google Colaboratory development environment, which provides free use of the Nvidia Tesla K80 graphics processor. For the experiments tracking and logging the authors used the Weights & Biases service. © 2022 Copyright for this paper by its authors.","Bioinformatics; Convolution; Convolutional neural networks; Deep neural networks; Generative adversarial networks; Learning algorithms; Learning systems; Biomedical images; GAN; Images classification; Images synthesis; Machine-learning; Main tasks; Neural network learning algorithm; Optimization algorithms; Problem-solving; Training time; Image classification","biomedical images; CNN; GAN; Machine learning; optimization algorithms","Conference paper","Final","","Scopus","2-s2.0-85144236311"
"Quan F.; Lang B.; Liu Y.","Quan, Fengnan (57694000300); Lang, Bo (7201907296); Liu, Yanxi (57193687234)","57694000300; 7201907296; 57193687234","ARRPNGAN: Text-to-image GAN with attention regularization and region proposal networks","2022","Signal Processing: Image Communication","106","","116728","","","","10.1016/j.image.2022.116728","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130092847&doi=10.1016%2fj.image.2022.116728&partnerID=40&md5=5b48ac34fdf51116a8d78a19a8b0a4be","Although text-to-image synthesis has shown remarkable success in generating high-resolution photorealistic images and semantic consistency, it still faces challenges in generating images with complex backgrounds. In this paper, we address this problem by proposing a novel generative adversarial text-to-image synthesis framework based on attention regularization modules and region proposal networks (ARRPNGAN). ARRPNGAN can precisely locate the keywords in text by exploiting attention model advantages and improving the accuracy in locating the subimage of target objects with the help of an RPN. Leveraging both attention regularization and the RPN a generative adversarial network (GAN) can obtain the most text description semantics and reduce the interference of complex background information. The results of extensive experiments on the Caltech-UCSD Birds and MS COCO datasets demonstrate that the proposed ARRPNGAN significantly outperforms other state-of-the-art text-to-image methods, especially in generating photorealistic images with complex backgrounds. Codes are available at: https://github.com/quanFN/ARRPNGAN. © 2022 Elsevier B.V.","Complex networks; Image processing; Semantics; Attention model; Complex background; High resolution; Images synthesis; Photorealistic images; Region proposal network; Regularisation; Semantic consistency; Subimages; Text-to-image synthesis; Generative adversarial networks","Attention model; Generative adversarial network; Region proposal network; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85130092847"
"Zhou Q.; Zhang J.; Han G.; Ruan Z.; Wei Y.","Zhou, Qinyu (57216431566); Zhang, Jianwei (57211379130); Han, Guoqiang (7202923324); Ruan, Zhihui (57210956634); Wei, Ying (57435486300)","57216431566; 57211379130; 7202923324; 57210956634; 57435486300","Enhanced self-supervised GANs with blend ratio classification","2022","Multimedia Tools and Applications","81","6","","7651","7667","16","10.1007/s11042-022-12056-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123836294&doi=10.1007%2fs11042-022-12056-2&partnerID=40&md5=46fe46dc00b4ddf6c67fcacbe41a435a","Generative adversarial networks (GANs) have achieved remarkable success in image generation, especially training conditional GANs for deriving reliable representations. However, the main downside of conditional GANs is the requirement of labeled data. Using self-supervision information can meet such needs, but the challenges remain for discovering more reliable self-supervised signals and ways to couple different signals to describe characteristics of the training data more precisely. In this paper, we propose a novel self-supervised learning approach to automatically generating pseudo labels for autoencoder-based GANs. Specifically, we blend the input images and the corresponding reconstructed results to produce transformed samples controlled by the blend ratio. Then, an additional classifier attached to the discriminator needs to distinguish the ratio of real images from the transformed samples to derive meaningful representations. Next, we enhance GANs with multiple self-supervision guidances by two different means to further improve the capacity of the discriminator. One merges multiple supervision signals and requires the classifier to predict the mixed probability, whereas the other one utilizes these signals independently. In experiments, we evaluate the quality of the generated image and the learned representation using three datasets. Empirical results prove the effectiveness of our methods on both image synthesis and representation learning. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image classification; Image enhancement; Quality control; Supervised learning; Auto encoders; Blend ratios; Image generations; Input image; Labeled data; Real images; Representation learning; Self-supervised learning; Supervised learning approaches; Training data; Generative adversarial networks","Generative adversarial network; Image generation; Representation learning; Self-supervised learning","Article","Final","","Scopus","2-s2.0-85123836294"
"Zhang Z.; Schomaker L.","Zhang, Zhenxing (57219361101); Schomaker, Lambert (19640514000)","57219361101; 19640514000","DiverGAN: An Efficient and Effective Single-Stage Framework for Diverse Text-to-Image Generation","2022","Neurocomputing","473","","","182","198","16","10.1016/j.neucom.2021.12.005","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122483394&doi=10.1016%2fj.neucom.2021.12.005&partnerID=40&md5=c455f2d4c9d3954ce57b9935e80b01b1","In this paper, we concentrate on the text-to-image synthesis task that aims at automatically producing perceptually realistic pictures from text descriptions. Recently, several single-stage methods have been proposed to deal with the problems of a more complicated multi-stage modular architecture. However, they often suffer from the lack-of-diversity issue, yielding similar outputs given a single textual sequence. To this end, we present an efficient and effective single-stage framework (DiverGAN) to generate diverse, plausible and semantically consistent images according to a natural-language description. DiverGAN adopts two novel word-level attention modules, i.e., a channel-attention module (CAM) and a pixel-attention module (PAM), which model the importance of each word in the given sentence while allowing the network to assign larger weights to the significant channels and pixels semantically aligning with the salient words. After that, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is introduced to enable the linguistic cues from the sentence embedding to flexibly manipulate the amount of change in shape and texture, further improving visual-semantic representation and helping stabilize the training. Also, a dual-residual structure is developed to preserve more original visual features while allowing for deeper networks, resulting in faster convergence speed and more vivid details. Furthermore, we propose to plug a fully-connected layer into the pipeline to address the lack-of-diversity problem, since we observe that a dense layer will remarkably enhance the generative capability of the network, balancing the trade-off between a low-dimensional random latent code contributing to variants and modulation modules that use high-dimensional and textual contexts to strength feature maps. Inserting a linear layer after the second residual block achieves the best variety and quality. Both qualitative and quantitative results on benchmark data sets demonstrate the superiority of our DiverGAN for realizing diversity, without harming quality and semantic consistency. © 2021 The Authors","Benchmarking; Economic and social effects; Generative adversarial networks; Semantics; Textures; Attention mechanisms; Diversity issues; Image generations; Images synthesis; Lack-of-diversity issue; Modular architectures; Multi-stages; Single stage; Single-stage framework; Text-to-image generation; article; attention; embedding; human; human experiment; language; pipeline; synthesis; velocity; Pixels","attention mechanism; generative adversarial network; lack-of-diversity issue; single-stage framework; text-to-image generation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85122483394"
"Moore A.M.; Paffenroth R.C.; Ngo K.T.; Uzarski J.R.","Moore, Alexander M. (57541121000); Paffenroth, Randy Clinton (6506146565); Ngo, Ken T. (58028400900); Uzarski, Joshua R. (25230783300)","57541121000; 6506146565; 58028400900; 25230783300","Cycles Improve Conditional Generators: Synthesis and Augmentation for Data Mining","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13726 LNAI","","","352","364","12","10.1007/978-3-031-22137-8_26","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144617159&doi=10.1007%2f978-3-031-22137-8_26&partnerID=40&md5=922854af6393111e91c715f36469c005","Conditional Generative Adversarial Networks (CGANs) are diversely utilized for data synthesis in applied sciences and natural image tasks. Conditional generative models extend upon data generation to account for labeled data by estimating joint distributions of samples and labels. We present a family of modified CGANs which demonstrate the inclusion of reconstructive cycles between prior and data spaces inspired by BiGAN and CycleGAN improves upon baselines for natural image synthesis with three primary contributions. The first is a study proposing three incremental architectures for conditional data generation which demonstrate improvement on baseline generation quality for a natural image data set across multiple generative metrics. The second is a novel approach to structure latent representations by learning a paired structured condition space and weakly structured variation space with desirable sampling and supervised learning properties. The third is a proposed utilization of conditional image synthesis for supervised learner data set augmentation as an alternative generation metric. Additional experiments demonstrate the successes of inducing cycles in conditional GANs for both image synthesis and image classification over comparable models with no additional tweaks or modifications. We release our source code, models, and experiments here: https://github.com/alexander-moore/Cycles-Improve-Conditional-Generators. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Data mining; Deep learning; Image enhancement; Applied science; Data generation; Data synthesis; Deep learning; GAN; Generative model; Images synthesis; Labeled data; Natural image synthesis; Natural images; Generative adversarial networks","Deep learning; GANs; Natural image synthesis","Conference paper","Final","","Scopus","2-s2.0-85144617159"
"Kim E.; Cho H.-H.; Kwon J.; Oh Y.-T.; Ko E.S.; Park H.","Kim, Eunjin (57202505127); Cho, Hwan-Ho (57196194745); Kwon, Junmo (57216706710); Oh, Young-Tack (57640444400); Ko, Eun Sook (35113524700); Park, Hyunjin (56512679000)","57202505127; 57196194745; 57216706710; 57640444400; 35113524700; 56512679000","Tumor-Attentive Segmentation-Guided GAN for Synthesizing Breast Contrast-Enhanced MRI Without Contrast Agents","2023","IEEE Journal of Translational Engineering in Health and Medicine","11","","","32","43","11","10.1109/JTEHM.2022.3221918","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142835891&doi=10.1109%2fJTEHM.2022.3221918&partnerID=40&md5=94362ebbc78e28a81ca2ae0603955095","Objective: Breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a sensitive imaging technique critical for breast cancer diagnosis. However, the administration of contrast agents poses a potential risk. This can be avoided if contrast-enhanced MRI can be obtained without using contrast agents. Thus, we aimed to generate T1-weighted contrast-enhanced MRI (ceT1) images from pre-contrast T1 weighted MRI (preT1) images in the breast. Methods: We proposed a generative adversarial network to synthesize ceT1 from preT1 breast images that adopted a local discriminator and segmentation task network to focus specifically on the tumor region in addition to the whole breast. The segmentation network performed a related task of segmentation of the tumor region, which allowed important tumor-related information to be enhanced. In addition, edge maps were included to provide explicit shape and structural information. Our approach was evaluated and compared with other methods in the local (n = 306) and external validation (n = 140) cohorts. Four evaluation metrics of normalized mean squared error (NRMSE), Pearson cross-correlation coefficients (CC), peak signal-to-noise ratio (PSNR), and structural similarity index map (SSIM) for the whole breast and tumor region were measured. An ablation study was performed to evaluate the incremental benefits of various components in our approach. Results: Our approach performed the best with an NRMSE 25.65, PSNR 54.80 dB, SSIM 0.91, and CC 0.88 on average, in the local test set. Conclusion: Performance gains were replicated in the validation cohort. Significance: We hope that our method will help patients avoid potentially harmful contrast agents. Clinical and Translational Impact Statement - Contrast agents are necessary to obtain DCE-MRI which is essential in breast cancer diagnosis. However, administration of contrast agents may cause side effects such as nephrogenic systemic fibrosis and risk of toxic residue deposits. Our approach can generate DCE-MRI without contrast agents using a generative deep neural network. Thus, our approach could help patients avoid potentially harmful contrast agents resulting in an improved diagnosis and treatment workflow for breast cancer.  © 2013 IEEE.","Breast Neoplasms; Contrast Media; Female; Humans; Magnetic Resonance Imaging; Diagnosis; Edge detection; Generative adversarial networks; Image enhancement; Image segmentation; Magnetism; Maps; Mean square error; Medical imaging; Resonance; Signal to noise ratio; Tumors; contrast medium; Adversarial learning; Breast; Breast magnetic-resonance imaging; Contrast agent; Generator; Image edge detection; Images segmentations; Images synthesis; Segmentation-guided; Tumor-attentive; Article; breast magnetic resonance imaging; breast tumor; cohort analysis; contrast enhancement; correlation coefficient; cross correlation; edge detection; female; human; image segmentation; major clinical study; mean squared error; qualitative analysis; quantitative analysis; retrospective study; signal noise ratio; T1 weighted imaging; validation process; breast tumor; diagnostic imaging; nuclear magnetic resonance imaging; Magnetic resonance imaging","adversarial learning; Breast magnetic resonance imaging; image synthesis; segmentationguided; tumor-attentive","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85142835891"
"Dalmaz O.; Yurt M.; Cukur T.","Dalmaz, Onat (57226257796); Yurt, Mahmut (57211187325); Cukur, Tolga (23034054800)","57226257796; 57211187325; 23034054800","ResViT: Residual Vision Transformers for Multimodal Medical Image Synthesis","2022","IEEE Transactions on Medical Imaging","41","10","","2598","2614","16","10.1109/TMI.2022.3167808","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128652996&doi=10.1109%2fTMI.2022.3167808&partnerID=40&md5=8aa3e8ba47b14ca7aaf8d37a1cadf4eb","Generative adversarial models with convolutional neural network (CNN) backbones have recently been established as state-of-the-art in numerous medical image synthesis tasks. However, CNNs are designed to perform local processing with compact filters, and this inductive bias compromises learning of contextual features. Here, we propose a novel generative adversarial approach for medical image synthesis, ResViT, that leverages the contextual sensitivity of vision transformers along with the precision of convolution operators and realism of adversarial learning. ResViT's generator employs a central bottleneck comprising novel aggregated residual transformer (ART) blocks that synergistically combine residual convolutional and transformer modules. Residual connections in ART blocks promote diversity in captured representations, while a channel compression module distills task-relevant information. A weight sharing strategy is introduced among ART blocks to mitigate computational burden. A unified implementation is introduced to avoid the need to rebuild separate synthesis models for varying source-target modality configurations. Comprehensive demonstrations are performed for synthesizing missing sequences in multi-contrast MRI, and CT images from MRI. Our results indicate superiority of ResViT against competing CNN- and transformer-based methods in terms of qualitative observations and quantitative metrics.  © 1982-2012 IEEE.","Data Compression; Endoscopy; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Computer architecture; Computerized tomography; Convolution; Generative adversarial networks; Medical imaging; Network architecture; Neural networks; Adversarial; Biomedical imaging; Generative; Images synthesis; Medical image synthesis; Residual; Subspace constraint; Task analysis; Transformer; Unified; Article; compression; computer assisted tomography; convolutional neural network; deep learning; diagnostic imaging; human; multimodal imaging; nuclear magnetic resonance imaging; qualitative analysis; quantitative analysis; residual vision transformer; endoscopy; image processing; information processing; procedures; Magnetic resonance imaging","adversarial; generative; Medical image synthesis; residual; transformer; unified; vision","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128652996"
"Maniyar H.; Budihal S.V.; Siddamal S.V.","Maniyar, Huzaifa (57220205656); Budihal, Suneeta V. (23984409200); Siddamal, Saroja V. (24823082800)","57220205656; 23984409200; 24823082800","Persons facial image synthesis from audio with Generative Adversarial Networks","2022","ECTI Transactions on Computer and Information Technology","16","2","","135","141","6","10.37936/ecti-cit.2022162.246995","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134398263&doi=10.37936%2fecti-cit.2022162.246995&partnerID=40&md5=b2c0be8360814442455871796817f013","This paper proposes to build a framework with Generative Adversarial Network (GANs) to synthesize a person's facial image from audio input. Image and speech are the two main sources of information exchange be-tween two entities. In some data intensive applications, a large amount of audio has to be translated into an understandable image format, with automated system, without human interference. This paper provides an end-to-end model for intelligible image reconstruction from an audio signal. The model uses a GAN architecture, which generates image features using audio waveforms for image synthesis. The model was created to produce facial images from audio of individual identities of a synthesized image of the speakers, based on the training dataset. The images of labelled persons are generated using excitation signals and the method obtained results with an accuracy of 96.88% for ungrouped data and 93.91% for grouped data. © 2022, ECTI Association. All rights reserved.","","Deep Learning; Generative Adversarial Network(GAN); Image Synthesis; Speech Processing","Article","Final","","Scopus","2-s2.0-85134398263"
"Zhu J.; Li Z.; Wei J.; Ma H.","Zhu, Jianwei (57217164416); Li, Zhixin (55706981100); Wei, Jiahui (57203545703); Ma, Huifang (35185217200)","57217164416; 55706981100; 57203545703; 35185217200","PBGN: Phased Bidirectional Generation Network in Text-to-Image Synthesis","2022","Neural Processing Letters","54","6","","5371","5391","20","10.1007/s11063-022-10866-x","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130196944&doi=10.1007%2fs11063-022-10866-x&partnerID=40&md5=8662bd471a203f790bbb6dc4c49764de","Text-to-image synthesis methods are mainly evaluated from two aspects: one is the quality and diversity of the generated images, and the other is the semantic consistency between the generated images and the input sentences. To address the problem of semantic consistency during image generation, we propose a Phased Bidirectional Generative Network. We use a bidirectional generative mechanism based on a multi-level generative adversarial network, where the images generated at each level are used to generate text, and the generated images are constrained by introducing a reconstruction loss. At the same time, we explore the effectiveness of the self-attention mechanism and spectral normalization techniques to improve the performance of generative networks. Furthermore, we propose an efficient boundary augmentation strategy to improve the performance of the model on small-scale datasets. Our method achieves Inception Scores of 4.71, 5.13, 32.42, and R-precision scores of 92.55, 87.72, and 92.29 on Oxford-102, CUB-200, and MS-COCO datasets, respectively. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image processing; Semantics; Bidirectional generation network; Image generations; Images synthesis; Performance; Self-attention; Semantic consistency; Spectral normalization; Synthesis method; Text-to-image synthesis; Transformer; Generative adversarial networks","Bidirectional generation network; Generative adversarial network; Self-attention; Spectral normalization; Text-to-image synthesis; Transformer","Article","Final","","Scopus","2-s2.0-85130196944"
"Kawahara D.; Yoshimura H.; Matsuura T.; Saito A.; Nagata Y.","Kawahara, Daisuke (56350513700); Yoshimura, Hisanori (58084152800); Matsuura, Takaaki (57801427500); Saito, Akito (57804344000); Nagata, Yasushi (57218670889)","56350513700; 58084152800; 57801427500; 57804344000; 57218670889","MRI image synthesis for fluid-attenuated inversion recovery and diffusion-weighted images with deep learning","2023","Physical and Engineering Sciences in Medicine","","","","","","","10.1007/s13246-023-01220-z","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147122393&doi=10.1007%2fs13246-023-01220-z&partnerID=40&md5=607309841b2fb5956a813e6fe1656298","This study aims to synthesize fluid-attenuated inversion recovery (FLAIR) and diffusion-weighted images (DWI) with a deep conditional adversarial network from T1- and T2-weighted magnetic resonance imaging (MRI) images. A total of 1980 images of 102 patients were split into two datasets: 1470 (68 patients) in a training set and 510 (34 patients) in a test set. The prediction framework was based on a convolutional neural network with a generator and discriminator. T1-weighted, T2-weighted, and composite images were used as inputs. The digital imaging and communications in medicine (DICOM) images were converted to 8-bit red–green–blue images. The red and blue channels of the composite images were assigned to 8-bit grayscale pixel values in T1-weighted images, and the green channel was assigned to those in T2-weighted images. The prediction FLAIR and DWI images were of the same objects as the inputs. For the results, the prediction model with composite MRI input images in the DWI image showed the smallest relative mean absolute error (rMAE) and largest mutual information (MI), and that in the FLAIR image showed the largest relative mean-square error (rMSE), relative root-mean-square error (rRMSE), and peak signal-to-noise ratio (PSNR). For the FLAIR image, the prediction model with the T2-weighted MRI input images generated more accurate synthesis results than that with the T1-weighted inputs. The proposed image synthesis framework can improve the versatility and quality of multi-contrast MRI without extra scans. The composite input MRI image contributes to synthesizing the multi-contrast MRI image efficiently. © 2023, Australasian College of Physical Scientists and Engineers in Medicine.","adult; article; convolutional neural network; deep learning; diffusion; digital imaging and communications in medicine; female; fluid-attenuated inversion recovery imaging; human; major clinical study; male; mean absolute error; nuclear magnetic resonance imaging; prediction; root mean squared error; signal noise ratio; synthesis; T1 weighted imaging; T2 weighted imaging","Deep learning; Generative adversarial network; Image synthesis; Magnetic resonance imaging","Article","Article in press","","Scopus","2-s2.0-85147122393"
"Mao Y.; Chen C.; Wang Z.; Cheng D.; You P.; Huang X.; Zhang B.; Zhao F.","Mao, Yanyan (55459044100); Chen, Chao (57203146321); Wang, Zhenjie (57984564100); Cheng, Dapeng (7402806243); You, Panlu (57893438100); Huang, Xingdan (57670280300); Zhang, Baosheng (57226775724); Zhao, Feng (57190684473)","55459044100; 57203146321; 57984564100; 7402806243; 57893438100; 57670280300; 57226775724; 57190684473","Generative adversarial networks with adaptive normalization for synthesizing T2-weighted magnetic resonance images from diffusion-weighted images","2022","Frontiers in Neuroscience","16","","1058487","","","","10.3389/fnins.2022.1058487","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142833598&doi=10.3389%2ffnins.2022.1058487&partnerID=40&md5=f7b5972b9cb9c2a028dcf3fd8b730a27","Recently, attention has been drawn toward brain imaging technology in the medical field, among which MRI plays a vital role in clinical diagnosis and lesion analysis of brain diseases. Different sequences of MR images provide more comprehensive information and help doctors to make accurate clinical diagnoses. However, their costs are particularly high. For many image-to-image synthesis methods in the medical field, supervised learning-based methods require labeled datasets, which are often difficult to obtain. Therefore, we propose an unsupervised learning-based generative adversarial network with adaptive normalization (AN-GAN) for synthesizing T2-weighted MR images from rapidly scanned diffusion-weighted imaging (DWI) MR images. In contrast to the existing methods, deep semantic information is extracted from the high-frequency information of original sequence images, which are then added to the feature map in deconvolution layers as a modality mask vector. This image fusion operation results in better feature maps and guides the training of GANs. Furthermore, to better preserve semantic information against common normalization layers, we introduce AN, a conditional normalization layer that modulates the activations using the fused feature map. Experimental results show that our method of synthesizing T2 images has a better perceptual quality and better detail than the other state-of-the-art methods. Copyright © 2022 Mao, Chen, Wang, Cheng, You, Huang, Zhang and Zhao.","Article; clinical effectiveness; diffusion weighted imaging; feasibility study; human; image analysis; image display; image processing; image quality; intermethod comparison; mathematical model; neuroimaging; T2 weighted imaging","adaptive normalization; generative adversarial network (GAN); image fusion; images synthesis; magnetic resonance imaging (MRI)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85142833598"
"Bazangani F.; Richard F.J.P.; Ghattas B.; Guedj E.","Bazangani, Farideh (57765897600); Richard, Frédéric J. P. (7102624415); Ghattas, Badih (6602586564); Guedj, Eric (13604996000)","57765897600; 7102624415; 6602586564; 13604996000","FDG-PET to T1 Weighted MRI Translation with 3D Elicit Generative Adversarial Network (E-GAN)","2022","Sensors","22","12","4640","","","","10.3390/s22124640","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132884767&doi=10.3390%2fs22124640&partnerID=40&md5=34fdc67f0786a5f638df8a20fca5ee47","Objective: With the strengths of deep learning, computer-aided diagnosis (CAD) is a hot topic for researchers in medical image analysis. One of the main requirements for training a deep learning model is providing enough data for the network. However, in medical images, due to the difficulties of data collection and data privacy, finding an appropriate dataset (balanced, enough samples, etc.) is quite a challenge. Although image synthesis could be beneficial to overcome this issue, synthesizing 3D images is a hard task. The main objective of this paper is to generate 3D T1 weighted MRI corresponding to FDG-PET. In this study, we propose a separable convolutionbased Elicit generative adversarial network (E-GAN). The proposed architecture can reconstruct 3D T1 weighted MRI from 2D high-level features and geometrical information retrieved from a Sobel filter. Experimental results on the ADNI datasets for healthy subjects show that the proposed model improves the quality of images compared with the state of the art. In addition, the evaluation of E-GAN and the state of art methods gives a better result on the structural information (13.73% improvement for PSNR and 22.95% for SSIM compared to Pix2Pix GAN) and textural information (6.9% improvements for homogeneity error in Haralick features compared to Pix2Pix GAN). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Fluorodeoxyglucose F18; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Positron-Emission Tomography; Computer aided analysis; Computer aided diagnosis; Computer aided instruction; Data acquisition; Data privacy; Deep learning; Image enhancement; Magnetic resonance imaging; Medical imaging; fluorodeoxyglucose f 18; 3D-images; Data collection; Deep learning; FDG PET; Hot topics; Images synthesis; Learning models; Medical image analysis; Medical image synthesis; T1-weighted; human; image processing; nuclear magnetic resonance imaging; positron emission tomography; procedures; three-dimensional imaging; Generative adversarial networks","deep learning; generative adversarial network; medical image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132884767"
"Sun L.; Chen J.; Xu Y.; Gong M.; Yu K.; Batmanghelich K.","Sun, Li (57219788515); Chen, Junxiang (57210937793); Xu, Yanwu (57216900106); Gong, Mingming (36617384500); Yu, Ke (57212177474); Batmanghelich, Kayhan (24330968400)","57219788515; 57210937793; 57216900106; 36617384500; 57212177474; 24330968400","Hierarchical Amortized GAN for 3D High Resolution Medical Image Synthesis","2022","IEEE Journal of Biomedical and Health Informatics","26","8","","3966","3975","9","10.1109/JBHI.2022.3172976","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132539418&doi=10.1109%2fJBHI.2022.3172976&partnerID=40&md5=3791764a7a2b7a41e9fe6296f16f4569","Generative Adversarial Networks (GAN) have many potential medical imaging applications, including data augmentation, domain adaptation, and model explanation. Due to the limited memory of Graphical Processing Units (GPUs), most current 3D GAN models are trained on low-resolution medical images, these models either cannot scale to high-resolution or are prone to patchy artifacts. In this work, we propose a novel end-to-end GAN architecture that can generate high-resolution 3D images. We achieve this goal by using different configurations between training and inference. During training, we adopt a hierarchical structure that simultaneously generates a low-resolution version of the image and a randomly selected sub-volume of the high-resolution image. The hierarchical design has two advantages: First, the memory demand for training on high-resolution images is amortized among sub-volumes. Furthermore, anchoring the high-resolution sub-volumes to a single low-resolution image ensures anatomical consistency between sub-volumes. During inference, our model can directly generate full high-resolution images. We also incorporate an encoder with a similar hierarchical structure into the model to extract features from the images. Experiments on 3D thorax CT and brain MRI demonstrate that our approach outperforms state of the art in image generation. We also demonstrate clinical applications of the proposed model in data augmentation and clinical-relevant feature extraction.  © 2013 IEEE.","Artifacts; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Tomography, X-Ray Computed; Computerized tomography; Extraction; Generative adversarial networks; Graphics processing unit; Image reconstruction; Image resolution; Magnetic resonance imaging; Medical imaging; Program processors; Three dimensional computer graphics; Three dimensional displays; 3-D image; 3d image synthesis; 3D-images; Features extraction; High resolution; High-resolution images; Images reconstruction; Images synthesis; Solid modelling; Three-dimensional display; article; brain; controlled study; feature extraction; image reconstruction; memory; neuroimaging; nuclear magnetic resonance imaging; randomized controlled trial; synthesis; thorax; three-dimensional imaging; artifact; human; image processing; procedures; three-dimensional imaging; x-ray computed tomography; Feature extraction","3D image synthesis; generative adver- sarial networks; high resolution","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85132539418"
"Omachi S.; Omachi M.","Omachi, Shinichiro (35501320300); Omachi, Masako (18437798200)","35501320300; 18437798200","Correlation-Based Data Augmentation for Machine Learning and Its Application to Road Environment Recognition","2022","IEEE Transactions on Vehicular Technology","71","7","","7113","7121","8","10.1109/TVT.2022.3167048","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128291449&doi=10.1109%2fTVT.2022.3167048&partnerID=40&md5=db89e7d77cf8f5af7bca2ef74a4ceb11","The accuracy of machine learning depends largely on the quantity and quality of the training data. However, it is generally difficult to prepare a large number of high-quality data. To generate diverse image data, image generation techniques using deep learning, such as a generative adversarial network, can be used. However, because these methods require a large number of training data and a significant calculation time, they are unsuitable for generating training data for machine learning. In this article, we propose an image data augmentation model based on the statistical properties of the training data. With the proposed method, each image is divided into sub-regions based on the correlation calculated using a set of images. The image of each sub-region is modeled through a Gaussian mixture, and data augmentation is conducted by generating images based on this model. The proposed method does not require a large number of training data and can generate data within a relatively short calculation time. The proposed method is applied to the task of road environment recognition. The experiment results showed that the accuracy was improved through image augmentation using the proposed model.  © 1967-2012 IEEE.","Deep learning; Image enhancement; Roads and streets; Calculation time; Correlation; Data augmentation; Environment recognition; Image data; Images synthesis; Road environment; Road environment recognition; Statistic modeling; Training data; Generative adversarial networks","Correlation; Data augmentation; Road environment recognition; Statistical model","Article","Final","","Scopus","2-s2.0-85128291449"
"Zhang Y.; Hu Y.; Higashita R.; Liu J.","Zhang, Yinglin (57224990351); Hu, Yan (57214953564); Higashita, Risa (57212003102); Liu, Jiang (23389932700)","57224990351; 57214953564; 57212003102; 23389932700","A review of generative adversarial networks and the application in medical image; [生成对抗式网络及其医学影像应用研究综述]","2022","Journal of Image and Graphics","27","3","","687","703","16","10.11834/jig.210247","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127512230&doi=10.11834%2fjig.210247&partnerID=40&md5=66470e30a459b3852c58888557ccd664","The generative adversarial network (GAN) consists of a generator based on the data distribution learning and an identified sample's authenticity discriminator. They learn from each other gradually in the process of confrontation. The network enables the deep learning method to learn the loss function automatically and reduces expertise dependence. It has been widely used in natural image processing, and it is also a promising solution for related problems in medical image processing field. This paper aims to bridge the gap between GAN and specific medical field problems and point out the future improvement directions. First, the basic principle of GAN is issued. Secondly, we review the latest medical images research on data augmentation, modality migration, image segmentation, and denoising; analyze the advantage and disadvantage of each method and the scope of application. Next, the current quality assessment is summarized. At the end, the research development, issue, and future improvement direction of GAN on medical image are summarized. GAN theoretical study focus on three aspects of task splitting, introducing conditional constraints and image-to-image translation, which effectively improved the quality of the synthesized image, increased the resolution, and allowed more manipulation across the image synthesis process. However, there are some challenges as mentioned below: 1) Generate high-quality, high-resolution, and diverse images under large-scale complex data sets. 2) The manipulation of synthesized image attributes at different levels and different granularities. 3) The lack of paired training data and the guarantee of image translation quality and diversity. GAN application study in data augmentation, modality migration, image segmentation, and denoising of medical images has been widely analyzed. 1) The network model based on the Pix2pix basic framework can synthesize additional high-quality and high-resolution samples and improve the segmentation and classification performance based on data augmentation effectively. However, there are still problems such as insufficient synthetic sample diversity, basic biological structures maintenance difficulty, and limited 3D image synthesis capabilities. 2) The network model based on the CycleGAN basic framework does not require paired training images. It has been extensively analyzed in modality migration, but may lose the basic structure information. The current research on structure preservation in modality migration limits in the fusion of information, such as edges and segmentation. 3) Both the generator and the discriminator can be fused with the current segmentation model to improve the performance of the segmentation model. The generator can synthesize additional data, and the discriminator can guide model training from a high-level semantic level and make full use of unlabeled data. However, current research mainly focuses on single-modality image segmentation. 4) GAN application in image denoising can reconstruct normal-dose images from low-dose images, reducing the radiation impact suffered by patients. The critical issues of GAN in medical image processing are presented as follows: 1) Most medical image data is three-dimensional, such as MRI (magnetic resonance imaging) and CT (computed tomography), etc. The improvement of the synthesis quality and resolution of the three-dimensional data is a critical issue. 2) The difficulty in ensuring the diversity of synthesized data while keeping its basic geometric structure's rationality. 3) The question on how to make full use of unlabeled and unpaired data to generate high-quality, high-resolution, and diverse images. 4) The improvement of algorithms' cross-modality generalization performance, and the effective migration of different modality data. Future research should focus on the issues as following: 1) To optimize network architecture, objective function, and training methods for 3D data synthesis, improving model training stability, quality, resolution, and diversity of 3D synthesized images. 2) To further promote the prior geometric knowledge integration with GAN. 3) To take full advantage of the GAN's weak supervision characteristics. 4) To extract invariant features via attribute decoupling for good generalization performance and achieve attribute control at different levels, granularities, and needs in the process of modality migration. To conclude, ever since GAN was proposed, its theory has been continuously improved. A considerable evolution in medical image applications has been sorted out, such as data augmentation, modality migration, image segmentation, and denoising. Some challenging issues are still waiting to be resolved, including three-dimensional data synthesis, geometric structure rationality maintenance, unlabeled and unpaired data usage, and multi-modality data application. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Data augmentation; Deep learning; Generative adversarial network (GAN); Image denoising; Image segmentation; Medical image; Modality migration","Review","Final","","Scopus","2-s2.0-85127512230"
"Zhong L.; Chen Z.; Shu H.; Zheng Y.; Zhang Y.; Wu Y.; Feng Q.; Li Y.; Yang W.","Zhong, Liming (56966065200); Chen, Zeli (57672126200); Shu, Hai (56666767000); Zheng, Yikai (57983032300); Zhang, Yiwen (57223002396); Wu, Yuankui (15059628200); Feng, Qianjin (57911196000); Li, Yin (57983730300); Yang, Wei (56982069100)","56966065200; 57672126200; 56666767000; 57983032300; 57223002396; 15059628200; 57911196000; 57983730300; 56982069100","QACL: Quartet attention aware closed-loop learning for abdominal MR-to-CT synthesis via simultaneous registration","2023","Medical Image Analysis","83","","102692","","","","10.1016/j.media.2022.102692","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142723043&doi=10.1016%2fj.media.2022.102692&partnerID=40&md5=7fae357459165d00f0cf3673485f0b9f","Synthesis of computed tomography (CT) images from magnetic resonance (MR) images is an important task to overcome the lack of electron density information in MR-only radiotherapy treatment planning (RTP). Some innovative methods have been proposed for abdominal MR-to-CT synthesis. However, it is still challenging due to the large misalignment between preprocessed abdominal MR and CT images and the insufficient feature information learned by models. Although several studies have used the MR-to-CT synthesis to alleviate the difficulty of multi-modal registration, this misalignment remains unsolved when training the MR-to-CT synthesis model. In this paper, we propose an end-to-end quartet attention aware closed-loop learning (QACL) framework for MR-to-CT synthesis via simultaneous registration. Specifically, the proposed quartet attention generator and mono-modal registration network form a closed-loop to improve the performance of MR-to-CT synthesis via simultaneous registration. In particular, a quartet-attention mechanism is developed to enlarge the receptive fields in networks to extract the long-range and cross-dimension spatial dependencies. Experimental results on two independent abdominal datasets demonstrate that our QACL achieves impressive results with MAE of 55.30±10.59 HU, PSNR of 22.85±1.43 dB, and SSIM of 0.83±0.04 for synthesis, and with Dice of 0.799±0.129 for registration. The proposed QACL outperforms the state-of-the-art MR-to-CT synthesis and multi-modal registration methods. © 2022 Elsevier B.V.","Humans; Tomography, X-Ray Computed; Alignment; Computerized tomography; Magnetic resonance; Medical imaging; Radiotherapy; Closed loop learning; Computed tomography images; Feature information; Images synthesis; Innovative method; Magnetic resonance-only radiotherapy; Medical image synthesis; Multimodal registration; Radiotherapy treatment planning; Synthesis models; article; attention; computer assisted tomography; learning; nuclear magnetic resonance; radiotherapy; receptive field; synthesis; human; x-ray computed tomography; Generative adversarial networks","Closed-loop learning; Generative adversarial network; Medical image synthesis; MR-only radiotherapy","Article","Final","","Scopus","2-s2.0-85142723043"
"Laino M.E.; Cancian P.; Politi L.S.; Della Porta M.G.; Saba L.; Savevski V.","Laino, Maria Elena (36768026200); Cancian, Pierandrea (57194029240); Politi, Letterio Salvatore (57192333642); Della Porta, Matteo Giovanni (6603909436); Saba, Luca (16234937700); Savevski, Victor (57217259660)","36768026200; 57194029240; 57192333642; 6603909436; 16234937700; 57217259660","Generative Adversarial Networks in Brain Imaging: A Narrative Review","2022","Journal of Imaging","8","4","83","","","","10.3390/jimaging8040083","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127651324&doi=10.3390%2fjimaging8040083&partnerID=40&md5=d20037fdf4b06f3c5d1ee28f772b9d81","Artificial intelligence (AI) is expected to have a major effect on radiology as it demon-strated remarkable progress in many clinical tasks, mostly regarding the detection, segmentation, classification, monitoring, and prediction of diseases. Generative Adversarial Networks have been proposed as one of the most exciting applications of deep learning in radiology. GANs are a new approach to deep learning that leverages adversarial learning to tackle a wide array of computer vision challenges. Brain radiology was one of the first fields where GANs found their application. In neuroradiology, indeed, GANs open unexplored scenarios, allowing new processes such as image-to-image and cross-modality synthesis, image reconstruction, image segmentation, image synthesis, data augmentation, disease progression models, and brain decoding. In this narrative review, we will provide an introduction to GANs in brain imaging, discussing the clinical potential of GANs, future clinical applications, as well as pitfalls that radiologists should be aware of. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Brain mapping; Computerized tomography; Deep learning; Generative adversarial networks; Image reconstruction; Image segmentation; Radiation; Radiology; Adversarial learning; Brain imaging; Clinical tasks; Cross modality; CT; FMRI; Image modality; Image reconstruction images; Neuroradiology; New approaches; Magnetic resonance imaging","brain imaging; CT; fMRI; generative adversarial networks; MRI; PET","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127651324"
"Song W.; Zhu M.; Zhang M.; Zhao D.; He Q.","Song, Wei (57191748664); Zhu, Mengfei (57468343500); Zhang, Minghua (56130298800); Zhao, Danfeng (57217770484); He, Qi (57195506496)","57191748664; 57468343500; 56130298800; 57217770484; 57195506496","A review of monocular depth estimation techniques based on deep learning; [基于深度学习的单目深度估计技术综述]","2022","Journal of Image and Graphics","27","2","","292","328","36","10.11834/jig.210554","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125352050&doi=10.11834%2fjig.210554&partnerID=40&md5=f51e807fd9681bc65fc21b51b0e5001d","Scene depth estimation is one of the key issues in the field of computer vision and an important aspect in the applications such as 3D reconstruction and image synthesis. Monocular depth estimation techniques based on deep learning have developed fast recently. Differentiated network structures have been proposed gradually. The current development of monocular depth estimation techniques based on deep learning and a categorical review of supervised and unsupervised learning-based methods have been illustrated in terms of the characteristics of the network structures. The supervised learning methods have been segmented as following: 1) Multi-scale feature fusion strategies: Different scales images contain different kinds of information via fusing multi-scale features extracted from the images. The demonstrated results of depth estimation can be effectively improved. 2)Conditional random fields (CRFs): CRFs, as one of probabilistic graphical models, have good performance in the field of semantic segmentation. Since depth information has similar data distribution attributes as semantic information, the use of consistent CRFs can be effective for predicting continuous depth values. CRFs can be operated as the loss function in the final part of the network as well as a feature fusion module in the medium layer of the network due to its effectiveness for fuse features. 3)Ordinal relations: One category is the relative depth estimation method which uses ordinal relation straight forward to estimate the relative position of two pixels in the image. The other category defines the depth estimation as an ordinal regression issue, which needs to discretize the continuous depth values into discrete depth labels and perform multi-class classification for the global depth. 4) Multiple image information: It is beneficial to combining various image information in depth estimation to improve the accuracy of depth estimation results whereas the image information of different dimensions (time, space, semantics, etc.) can be implicitly related to the depth of the image scene. Four types of information are often adopted: semantic information, neighborhood information, temporal information and object boundary information. 5)Miscellaneous strategies: Some other supervised learning methods still cannot be easily classified into the above-mentioned methods. 6) Various optimization strategies: Acquiring efficiency optimization, using synthetic data obtained via image style transfer for domain adaptation, and the hardware-oriented optimization for underwater scene depth estimation. The unsupervised learning methods of scene depth estimation are classified as below: 1) Stereo vision: Stereo vision aims to deduce the depth information of each pixel in the image from two or more images. Conventional binocular stereo vision algorithm is based on the stereo disparity, and can reconstruct the three-dimensional geometric information of surrounding scenery from the images captured by two camera sensors in terms of the principle of trigonometry. Researchers transform the depth estimation into an image reconstruction, and unsupervised depth estimation method is realized based on binocular (or multi-ocular) images and predicted disparity maps. 2) Structure from motion (SfM): SfM is a technique that automatically recovers camera parameters and the 3D structure of a scene from multiple images or video sequences. The unsupervised method based on SfM has its similarity to the unsupervised method based on stereo vision. It also transforms the depth estimation into the image reconstruction, but there are many differences in details. First, the SfM-based image reconstruction unsupervised processing method is generally using successive frames, that is, the image of the current frame is used to reconstruct the image of the previous or the next frame. Therefore, this kind of method uses image sequence generally-video as the training data. Second, the unsupervised method based on SfM needs to introduce a module for camera pose estimation in the training process. 3) Adversarial strategies: Generative adversarial networks (GANs) facilitate many imaging tasks with their powerful performance, where a discriminator can judge the results generated by the generator to force the generator to produce the same results as the labels. Adding discriminators to unsupervised learning networks can be effective in improving depth estimation results by optimizing image reconstruction results. 4) Ordinal relationship: Similar to the ordinal regression approach that utilizes ordinal relationships in the supervised learning methods, discrete disparity estimation is also desirable in unsupervised networks. In view of the fact that discrete depth values achieve more robust and sharper depth estimates than conventional regression predictions, discrete operations are equally effective in unsupervised networks. 5) Uncertainty: Since unsupervised learning does not use ground truth depth values, the depth results predicted is in doubt. From this viewpoint, it has been proposed to use the uncertainty of the prediction results of unsupervised methods as a benchmark for judging whether the prediction results are credible, and the results can be optimized in monocular depth estimation tasks. Meanwhile, this review refers to the NYU dataset, Karlsruhe Institute Technology and Toyota Technological Institute at Chicago (KITTI) dataset, Make3D dataset and Cityscapes dataset, which are mainly used in monocular deep estimation tasks, as well as six commonly-used evaluation metrics. Based on these datasets and evaluation metrics, a comparison among the reviewed methods is illustrated. Finally, the review discusses the current status of deep learning-based monocular depth estimation techniques in terms of accuracy, generalizability, application scenarios and uncertainty studies in unsupervised networks. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Deep learning; Monocular depth estimation; Multi-scale feature fusion; Ordinal relationship; Stereo vision; Supervised learning; Unsupervised learning","Review","Final","","Scopus","2-s2.0-85125352050"
"Kweon J.; Yoo J.; Kim S.; Won J.; Kwon S.","Kweon, Juwon (57700589700); Yoo, Jisang (7402295896); Kim, Seungjong (57698701200); Won, Jaesik (57700270600); Kwon, Soonchul (54415852000)","57700589700; 7402295896; 57698701200; 57700270600; 54415852000","A Novel Method Based on GAN Using a Segmentation Module for Oligodendroglioma Pathological Image Generation","2022","Sensors","22","10","3960","","","","10.3390/s22103960","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130343088&doi=10.3390%2fs22103960&partnerID=40&md5=e9f017b46b1d460b6d596f577082285c","Digital pathology analysis using deep learning has been the subject of several studies. As with other medical data, pathological data are not easily obtained. Because deep learning-based image analysis requires large amounts of data, augmentation techniques are used to increase the size of pathological datasets. This study proposes a novel method for synthesizing brain tumor pathology data using a generative model. For image synthesis, we used embedding features extracted from a segmentation module in a general generative model. We also introduce a simple solution for training a segmentation model in an environment in which the masked label of the training dataset is not supplied. As a result of this experiment, the proposed method did not make great progress in quantitative metrics but showed improved results in the confusion rate of more than 70 subjects and the quality of the visual output. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Algorithms; Brain; Humans; Image Processing, Computer-Assisted; Oligodendroglioma; Research Design; Deep learning; Image segmentation; Large dataset; Pathology; Digital pathologies; Generative model; Image generations; Images synthesis; Medical data; Novel methods; Oligodendroglioma; Pathological data; Pathological images; Pathology image synthesis; algorithm; brain; diagnostic imaging; human; image processing; methodology; oligodendroglioma; procedures; Generative adversarial networks","digital pathology; generative adversarial networks; pathology image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130343088"
"Chen J.; Yang G.; Xia M.; Zhang D.","Chen, Jiyou (57221655154); Yang, Gaobo (8647279200); Xia, Ming (55994627200); Zhang, Dengyong (55318418900)","57221655154; 8647279200; 55994627200; 55318418900","From depth-aware haze generation to real-world haze removal","2022","Neural Computing and Applications","","","","","","","10.1007/s00521-022-08101-8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143686665&doi=10.1007%2fs00521-022-08101-8&partnerID=40&md5=bfa4f1e2289a57d758230d9d7af6d462","For deep learning-based single image dehazing works, their performances seriously depend on the designed models and training dataset. Existing state-of-the-art methods focus on the design of novel dehazing models or the improvement of training strategies to obtain better dehazing results. In this work, instead of designing a new deep dehazing model, we attempt to further improve the dehazing performance from the perspective of enriching training datasets by exploring an intuitive yet efficient way to synthesize photo-realistic hazy images. It is well known that for a natural hazy image, its perceived haze density increases with scene depth. Motivated by this, we develop a depth-aware haze generation network, namely HazeGAN, by incorporating the Generative Adversarial Network (GAN), depth estimation network, and physical atmospheric scattering to progressively synthesize hazy images. Specifically, a separate depth estimation network is embedded to obtain multi-scale depth features, which are exploited by the atmospheric scattering model to generate multi-scale hazy features. The hazy features are fused into the GAN generator to output synthetic hazy images with depth-aware haze effects. Extensive experimental results demonstrate that the proposed HazeGAN can generate diverse training pairs of depth-aware hazy images and clear images, which effectively enrich the existing benchmark datasets, and improve the generalization capabilities of existing deep image dehazing models. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning; Demulsification; Image enhancement; Dehazing; Depth Estimation; Hazy image synthesis; Image dehazing; Image hazing; Images synthesis; Multi-scales; Performance; Real-world; Training dataset; Generative adversarial networks","Generative Adversarial Network; Hazy image synthesis; Image dehazing; Image hazing","Article","Article in press","","Scopus","2-s2.0-85143686665"
"Song S.; Chang K.; Yun K.; Jun C.; Baek J.-G.","Song, Seunghwan (57216323052); Chang, Kyuchang (57215345430); Yun, Kio (57885812500); Jun, Changdong (57885812600); Baek, Jun-Geol (7103228722)","57216323052; 57215345430; 57885812500; 57885812600; 7103228722","Defect Synthesis Using Latent Mapping Adversarial Network for Automated Visual Inspection †","2022","Electronics (Switzerland)","11","17","2763","","","","10.3390/electronics11172763","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137766032&doi=10.3390%2felectronics11172763&partnerID=40&md5=09498d7cdf9bcdbfa2e1db4906385508","In Industry 4.0, internet of things (IoT) technologies are expanding and advanced smart factories are currently being developed. To build an automated visual inspection (AVI) and achieve smartization of steel manufacturing, detecting defects in products in real-time and accurately diagnosing the quality of products are essential elements. As in various manufacturing industries, the steel manufacturing process presents a class imbalance problem for products. For example, fewer defect images are available than normal images. This study developed a new image synthesis methodology for the steel manufacturing industry called a latent mapping adversarial network. Inspired by the style-based generative adversarial network (StyleGAN) structure, we constructed a mapping network for the latent space, which made it possible to compose defect images of various sizes. We discovered the most suitable loss function, and optimized the proposed method in terms of convergence and computational cost. The experimental results demonstrate the competitive performance of the proposed model compared to the traditional models in terms of classification accuracy of 92.42% and F-score of 93.15%. Consequently, the problem of data imbalance is solved, and higher productivity in steel products is expected. © 2022 by the authors.","","automated visual inspection; defect synthesis; generative adversarial networks; internet of things; latent mapping adversarial networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137766032"
"Du Z.; Gao L.; Li X.","Du, Zongwei (58064171200); Gao, Liang (56406738100); Li, Xinyu (57737636100)","58064171200; 56406738100; 57737636100","A New Contrastive GAN With Data Augmentation for Surface Defect Recognition Under Limited Data","2023","IEEE Transactions on Instrumentation and Measurement","72","","3502713","","","","10.1109/TIM.2022.3232649","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146241084&doi=10.1109%2fTIM.2022.3232649&partnerID=40&md5=3456e46aecba9c49764e44254c094dfe","Surface defect recognition (SDC) is essential in intelligent manufacturing. Deep learning (DL) is a research hotspot in SDC. Limited defective samples are available in most real-world cases, which poses challenges for DL methods. Given such circumstances, generating defective samples by generative adversarial networks (GANs) is applied. However, insufficient samples and high-frequency texture details in defects make GANs very hard to train, yield mode collapse, and poor image quality, which can further impact SDC. To solve these problems, this article proposes a new GAN called contrastive GAN, which can be trained to generate diverse defects with only extremely limited samples. Specifically, a shared data augmentation (SDA) module is proposed for avoiding overfitting. Then, a feature attention matching (FAM) module is proposed to align features for improving the quality of generated images. Finally, a contrastive loss based on hypersphere is employed to constrain GANs to generate images that differ from the traditional transform. Experiments show that the proposed GAN generates defective images with higher quality and lower variance between real defects compared to other GANs. Synthetic images contribute to pretrained DL networks with accuracies of up to 95.00%-99.56% for Northeastern University (NEU) datasets of different sizes and 91.84% for printed circuit board (PCB) cases, which proves the effectiveness of the proposed method.  © 1963-2012 IEEE.","Deep learning; Gallium nitride; Generative adversarial networks; III-V semiconductors; Image enhancement; Image recognition; Surface defects; Textures; Data augmentation; Defect image generation; Defect images; Defect recognition; Features extraction; Generative adversarial network; Generator; Image generations; Images segmentations; Images synthesis; Limited data; Surface defect recognition; Image segmentation","Data augmentation; defect image generation; generative adversarial network (GAN); limited data; surface defect recognition","Article","Final","","Scopus","2-s2.0-85146241084"
"Guo J.; Du C.; Wang J.; Huang H.; Wan P.; Huang G.","Guo, Jiayi (57211200155); Du, Chaoqun (57375696700); Wang, Jiangshan (57376308500); Huang, Huijuan (57221470943); Wan, Pengfei (57221702931); Huang, Gao (7403425368)","57211200155; 57375696700; 57376308500; 57221470943; 57221702931; 7403425368","Assessing a Single Image in Reference-Guided Image Synthesis","2022","Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022","36","","","1078","1086","8","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147694056&partnerID=40&md5=1842627038625f64a71c7b7afa15f4de","Assessing the performance of Generative Adversarial Networks (GANs) has been an important topic due to its practical significance. Although several evaluation metrics have been proposed, they generally assess the quality of the whole generated image distribution. For Reference-guided Image Synthesis (RIS) tasks, i.e., rendering a source image in the style of another reference image, where assessing the quality of a single generated image is crucial, these metrics are not applicable. In this paper, we propose a general learning-based framework, Reference-guided Image Synthesis Assessment (RISA) to quantitatively evaluate the quality of a single generated image. Notably, the training of RISA does not require human annotations. In specific, the training data for RISA are acquired by the intermediate models from the training procedure in RIS, and weakly annotated by the number of models' iterations, based on the positive correlation between image quality and iterations. As this annotation is too coarse as a supervision signal, we introduce two techniques: 1) a pixel-wise interpolation scheme to refine the coarse labels, and 2) multiple binary classifiers to replace a naïve regressor. In addition, an unsupervised contrastive loss is introduced to effectively capture the style similarity between a generated image and its reference image. Empirical results on various datasets demonstrate that RISA is highly consistent with human preference and transfers well across models. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Image quality; Quality control; Evaluation metrics; General learning; Guided images; Human annotations; Image distributions; Images synthesis; Performance; Reference image; Single images; Source images; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85147694056"
"Gan Y.; Xiang T.; Liu H.; Ye M.","Gan, Yan (57203150814); Xiang, Tao (57213003210); Liu, Hangcheng (57219900424); Ye, Mao (35241431500)","57203150814; 57213003210; 57219900424; 35241431500","Learning-aware feature denoising discriminator","2023","Information Fusion","89","","","143","154","11","10.1016/j.inffus.2022.08.006","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136469268&doi=10.1016%2fj.inffus.2022.08.006&partnerID=40&md5=12dcff536940befaee1502f9be53ee63","Although generative adversarial networks (GANs) show great prospects for the task of image synthesis, the quality of synthesized images by existing GANs is sometimes inferior to real images because their discriminators cannot effectively learn robust identification features from input images. In addition, the training process of discriminator is prone to be unstable. To this end, inspired by the denoising auto-encoders, we propose a learning-aware feature denoising discriminator. It is designed to pay attention to robust features of input images, so as to improve its robustness in identifying features and recognition ability in training process. First, we use a decoder to generate perturbing noise and add it to real image to get corrupted image. Then, we get the encodings of the corrupted image and real image through an encoder. Finally, we minimize both types of encoding to constitute a denoising penalty and add it to the loss of the discriminator. We also show that our method is compatible with most existing GANs for three image synthesis tasks. Extensive experimental results show that compared with baseline models, our proposed method not only improves the quality of synthesized images, but also stabilizes the training process of discriminator. © 2022 Elsevier B.V.","Discriminators; Encoding (symbols); Image denoising; Image enhancement; Signal encoding; Corrupted images; De-noising; Encodings; Feature denoising; Images synthesis; Input image; Real images; Robustness; Synthesized images; Training process; Generative adversarial networks","Feature denoising; GANs; Image synthesis; Robustness","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85136469268"
"Yu S.; Han H.; Shan S.; Chen X.","Yu, Shikang (57210358831); Han, Hu (30267595700); Shan, Shiguang (22235341500); Chen, Xilin (8284171300)","57210358831; 30267595700; 22235341500; 8284171300","CMOS-GAN: Semi-Supervised Generative Adversarial Model for Cross-Modality Face Image Synthesis","2023","IEEE Transactions on Image Processing","32","","","144","158","14","10.1109/TIP.2022.3226413","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144783527&doi=10.1109%2fTIP.2022.3226413&partnerID=40&md5=8d666cd1dba0c9b2e4791fa8f0b5fdd3","Cross-modality face image synthesis such as sketch-to-photo, NIR-to-RGB, and RGB-to-depth has wide applications in face recognition, face animation, and digital entertainment. Conventional cross-modality synthesis methods usually require paired training data, i.e., each subject has images of both modalities. However, paired data can be difficult to acquire, while unpaired data commonly exist. In this paper, we propose a novel semi-supervised cross-modality synthesis method (namely CMOS-GAN), which can leverage both paired and unpaired face images to learn a robust cross-modality synthesis model. Specifically, CMOS-GAN uses a generator of encoder-decoder architecture for new modality synthesis. We leverage pixel-wise loss, adversarial loss, classification loss, and face feature loss to exploit the information from both paired multi-modality face images and unpaired face images for model learning. In addition, since we expect the synthetic new modality can also be helpful for improving face recognition accuracy, we further use a modified triplet loss to retain the discriminative features of the subject in the synthetic modality. Experiments on three cross-modality face synthesis tasks (NIR-to-VIS, RGB-to-depth, and sketch-to-photo) show the effectiveness of the proposed approach compared with the state-of-the-art. In addition, we also collect a large-scale RGB-D dataset (VIPL-MumoFace-3K) for the RGB-to-depth synthesis task. We plan to open-source our code and VIPL-MumoFace-3K dataset to the community (https://github.com/skgyu/CMOS-GAN).  © 1992-2012 IEEE.","Classification (of information); CMOS integrated circuits; Generative adversarial networks; Infrared devices; Job analysis; Large dataset; Open systems; Cross modality; Cross-modality face recognition; Cross-modality synthesis; Face image synthesis; Face images; Images synthesis; Semi-supervised; Semi-supervised synthesis; Task analysis; Face recognition","cross-modality face recognition; Cross-modality synthesis; generative adversarial networks; semi-supervised synthesis","Article","Final","","Scopus","2-s2.0-85144783527"
"Talkani A.; Bhojan A.","Talkani, Ayman (57821970200); Bhojan, Anand (57352334700)","57821970200; 57352334700","Approaching Zero-shot Learning from a Text-to-Image GAN perspective","2022","2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022","","","","","","","10.1109/MMSP55362.2022.9948764","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143606506&doi=10.1109%2fMMSP55362.2022.9948764&partnerID=40&md5=4f99a41784212f7ac3aaf9fdcde40701","With the recent research advancements in generative adversarial networks, synthesizing images from textual descriptions has become an active research area. Their applications in fields such as the use of localizing phrases to identify unseen objects in images is an important part of image understanding and can be useful in many applications that rely on mappings between textual and visual information. Although most zero shot learning algorithms consider the problem to be a visual semantic embedding one, we attempt to utilize the demonstrative capability of Generative Adversarial network [7] for ZSL tasks, similar to [20], where an image is generated based on an object description utilizing a GAN architecture inspired by the cascaded GAN structure used in state of the art text to image frameworks [10], [16], [18], [19] in order to help the network generate more accurate feature vectors that relate to the noisy input text from wikipedia. By using a cascaded structure, we essentially divide the problem into consequent sub-problems that are easier to solve, thus leading to a more positive result on the zero-shot learning task. Our architecture utilizes a cascaded GAN architecture inspired by text-to-image generators like [10], [16], [18], [19] to generate feature vectors that can be applied to the ZSL task at various stages, with each stage increasing the dimension size and complexity of the vector. While the initial stages are responsible for generating low dimension primitive feature vectors, the later stages focus on increasing the complexity of the vector to generate more accurate outputs. The output of each stage is also guided by it's respective discriminator, which tries to classify the vector as real/fake, along with it's corresponding class label. We will empirically compare our model to state of the art of the zero learning models [6], [9], [12], [20] on the CUB and NAB datasets with the SCS and SCE splits introduced in [6], which have been modified by [6] to also include noisy Wikipedia article texts as input for each class. We then compare our architecture with state-of-the-art methods in the Zero-shot-recognition and Generalized Zero-shot learning tasks and present quantitative results for the same. © 2022 IEEE.","Complex networks; Computer vision; Convolutional neural networks; Generative adversarial networks; Image classification; Learning algorithms; Learning systems; Network architecture; Text processing; Vectors; Zero-shot learning; Features vector; Generalized zero-shot learning; Generative adversarial network; Images classification; Images synthesis; Learning tasks; Recent researches; Semantic correlation; State of the art; Text-to image synthesis; Semantics","Generalized zero-shot learning; Generative Adversarial Networks (GANs); image classification; Semantic correlation; Text-to Image synthesis; Zero-shot learning","Conference paper","Final","","Scopus","2-s2.0-85143606506"
"Yin X.; Zhang Z.","Yin, Xin (57993668000); Zhang, Zhancheng (57219233537)","57993668000; 57219233537","Person Image Synthesis Based on Posture Guidance and Attribute Decomposition","2022","Jisuanji Gongcheng/Computer Engineering","48","11","","224","230","6","10.19678/j.issn.1000-3428.0063376","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143323106&doi=10.19678%2fj.issn.1000-3428.0063376&partnerID=40&md5=1bf594c035112f24ee64d12e87ad5785","Pose controllable person image synthesis involves generating a new image of the source person image under a transformed pose, and the coat, pants, hair style, and other character attributes must be consistent with the source person. As directly integrating the person texture and human posture key point coding is difficult, the consistency between some key character attributes in the generated and source images is poor.Therefore, this study establishes a dual stream generation network model under cyclic consistency constraint.In the training phase, the model adds the pose condition information of the source person to the input of the texture encoder, thereby reducing the search space of the decomposition component coding and improving the controllable granularity of the person generation.A fusion module is designed to fuse the pose information of the source person with the style coding of each decomposition component for generation and confrontation training.Simultaneously, circular consistency constraints are added to ensure that the generated image matches the hidden space better.In the test phase, the texture encoding information of the source person and pose encoding information of the target are separately encoded in the network, and the pose-transformed person image is obtained through information fusion and decoding.Qualitative and quantitative tests are conducted using the DeepFashion dataset.The results show that the Peak Signal-to-Noise Ratio(PSNR), perceptual score, and Structural Similarity(SSIM) of the model reach 31.409 dB, 3.369, and 0.768, respectively.The pose guidance conditions and circular consistency constraints added to the model can simplify the probability generation and expression of the attribute decomposition, making the texture of the image generated using the characters more accurate and consistent with human visual perception characteristics. © 2022, Editorial Office of Computer Engineering. All rights reserved.","","cycle consistency; Generative Adversarial Network(GAN); human key point estimation; human semantic segmentation; person image synthesis; pose transformation","Article","Final","","Scopus","2-s2.0-85143323106"
"Al Khalil Y.; Amirrajab S.; Lorenz C.; Weese J.; Pluim J.; Breeuwer M.","Al Khalil, Yasmina (57513053600); Amirrajab, Sina (57211821982); Lorenz, Cristian (55486269900); Weese, Jürgen (7005746479); Pluim, Josien (6701614327); Breeuwer, Marcel (7004252845)","57513053600; 57211821982; 55486269900; 7005746479; 6701614327; 7004252845","On the usability of synthetic data for improving the robustness of deep learning-based segmentation of cardiac magnetic resonance images","2023","Medical Image Analysis","84","","102688","","","","10.1016/j.media.2022.102688","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144048690&doi=10.1016%2fj.media.2022.102688&partnerID=40&md5=5f09760f5fe834fccc36aa7272aa9a09","Deep learning-based segmentation methods provide an effective and automated way for assessing the structure and function of the heart in cardiac magnetic resonance (CMR) images. However, despite their state-of-the-art performance on images acquired from the same source (same scanner or scanner vendor) as images used during training, their performance degrades significantly on images coming from different domains. A straightforward approach to tackle this issue consists of acquiring large quantities of multi-site and multi-vendor data, which is practically infeasible. Generative adversarial networks (GANs) for image synthesis present a promising solution for tackling data limitations in medical imaging and addressing the generalization capability of segmentation models. In this work, we explore the usability of synthesized short-axis CMR images generated using a segmentation-informed conditional GAN, to improve the robustness of heart cavity segmentation models in a variety of different settings. The GAN is trained on paired real images and corresponding segmentation maps belonging to both the heart and the surrounding tissue, reinforcing the synthesis of semantically-consistent and realistic images. First, we evaluate the segmentation performance of a model trained solely with synthetic data and show that it only slightly underperforms compared to the baseline trained with real data. By further combining real with synthetic data during training, we observe a substantial improvement in segmentation performance (up to 4% and 40% in terms of Dice score and Hausdorff distance) across multiple data-sets collected from various sites and scanner. This is additionally demonstrated across state-of-the-art 2D and 3D segmentation networks, whereby the obtained results demonstrate the potential of the proposed method in tackling the presence of the domain shift in medical data. Finally, we thoroughly analyze the quality of synthetic data and its ability to replace real MR images during training, as well as provide an insight into important aspects of utilizing synthetic images for segmentation. © 2022 The Author(s)","Deep Learning; Heart; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Tomography, X-Ray Computed; Deep learning; Generative adversarial networks; Image enhancement; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Cardiac magnetic resonance; Cardiac magnetic resonance images; Cardiac magnetic resonance synthesis; Domain adaptation; Domain adaptation and generalization; Generalisation; Images segmentations; Learning-based segmentation; Segmentation models; Synthetic data; article; cardiovascular magnetic resonance; controlled study; deep learning; diagnostic imaging; heart; human; image segmentation; synthesis; usability; image processing; nuclear magnetic resonance imaging; procedures; x-ray computed tomography; Heart","Cardiac magnetic resonance image; CMR synthesis; Domain adaptation and generalization; Image segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85144048690"
"Sindhura D.; Pai R.M.; Bhat S.N.; Pai M.M.","Sindhura, Dn (57423642600); Pai, Radhika M (57573319800); Bhat, Shyamasunder N (58023559000); Pai, Mm Manohara (58038982600)","57423642600; 57573319800; 58023559000; 58038982600","Sub-Axial Vertebral Column Fracture CT Image Synthesis by Progressive Growing Generative Adversarial Networks (PGGANs)","2022","2022 IEEE International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics, DISCOVER 2022 - Proceedings","","","","311","315","4","10.1109/DISCOVER55800.2022.9974676","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145355233&doi=10.1109%2fDISCOVER55800.2022.9974676&partnerID=40&md5=e8bcc3dc34e7ec005e447f74e5b1a8fa","Orthopaedicians need the assistance of the Deep Learning (DL) model for easy Vertebral Column Fracture Type identification. Deep Learning models require large datasets. Due to the non-availability of large annotated data sets, the DL model needs intensive data augmentation methods. In this proposed research work, Progressive Growing Generative Adversarial Networks (PGGANs) are used to generate synthetic Vertebral Column Fracture (VCF) CT images. The synthetic CT images of VCF generated by PGGANs are high resolution, realistic yet wholly different from the real images. The PGGANs is a multi-stage generative model that generates 512 X 512 CT images that increases the accuracy of the VCF Type identification system. A total of375 vertebral column CT images were utilized for training the model, which were collected from the Spine Clinic, Orthopaedics Department, Kasturba Medical College, Manipal, Manipal Academy of Higher Education, Manipal. Among 375 images, 275 Chance fractures and 100 posterior tension band disruption fracture images were present. To analyse the effect of PGGAN augmentation on VCF type identification, lately VGG16 pre-trained model is implemented. The VGG16 model with PGGAN augmentation got an accuracy of 87.01%, which is more when compared to the model without augmentation. In conclusion, PGGAN generated VCF images are realistic and can be used for data augmentation without privacy restrictions and in VCF type identification DL models for increased performance.  © 2022 IEEE.","Deep learning; Fracture; Large dataset; Learning systems; Medical imaging; Musculoskeletal system; Orthopedics; CT Image; Data augmentation; Deep learning  model; Images synthesis; Learning models; Network augmentation; Progressive growing generative adversarial network; Sub-axial; Vertebral column; Vertebral column fracture; Computerized tomography","CT Images; Data Augmentation; Deep learning (DL) model; Progressive Growing Generative Adversarial Networks (PGGANs); Sub-Axial; Vertebral column Fracture (VCF)","Conference paper","Final","","Scopus","2-s2.0-85145355233"
"Berrahal M.; Azizi M.","Berrahal, Mohammed (57221260262); Azizi, Mostafa (25929027100)","57221260262; 25929027100","Optimal text-to-image synthesis model for generating portrait images using generative adversarial network techniques","2022","Indonesian Journal of Electrical Engineering and Computer Science","25","2","","972","979","7","10.11591/ijeecs.v25.i2.pp972-979","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123550637&doi=10.11591%2fijeecs.v25.i2.pp972-979&partnerID=40&md5=aa332c219d4cdfe90dd37e84d0ce26dc","The advancements in artificial intelligence research, particularly in computer vision, have led to the development of previously unimaginable applications, such as generating new contents based on text description. In our work we focused on the text-to-image synthesis applications (TIS) field, to transform descriptive sentences into a real image. To tackle this issue, we use unsupervised deep learning networks that can generate high quality images from text descriptions, provided by eyewitnesses to assist law enforcement in their investigations, for the purpose of generating probable human faces. We analyzed a number of existing approaches and chose the best one. Deep fusion generative adversarial networks (DF-GAN) is the network that performs better than its peers, at multiple levels, like the generated image quality or the respect of the giving descriptive text. Our model is trained on the CelebA dataset and text descriptions (generated by our algorithm using existing attributes in the dataset). The obtained results from our implementation show that the learned generative model makes excellent quantitative and visual performances, the model is capable of generating realistic and diverse samples for human faces and create a complete portrait with respect of given text description. © 2022 Institute of Advanced Engineering and Science. All rights reserved.","","Deep fusion-GAN; Deep learning; Generative adversarial network; Portrait generation; Text-to-image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85123550637"
"Anala M.R.; Hemavathy R.; Kulkarni K.; Samhitha A.","Anala, M.R. (55734811700); Hemavathy, R. (24168391100); Kulkarni, Krishna (57477533100); Samhitha, A. (57992712500)","55734811700; 24168391100; 57477533100; 57992712500","Generative Adversarial Network (GANs) for Image Translation","2022","13th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2022","8","","","277","284","7","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143342084&partnerID=40&md5=0633dbe8f1b84f2a3dab867fa04844aa","Image to Image translation deals with converting images from one form of representation to another automatically, while keeping the original structure or semantics. For example, the conversion of images represented by semantic label maps to color images, or the conversion of edge maps to full images. Most image transformation tasks require significant human involvement and artistic skill. Colorization of black and white images, is currently done by hand, a process that is both time consuming and expensive. However, due to the multitudes of coloring schema that exist for a single black and white image, the automatic colorization of images is not a trivial problem, as the mapping is notone-to-one.This paper deals with image to image translation using Generative Adversarial Networks (GANs), a technique for image synthesis and prediction which is time and cost effective. This work proposes a comparison between the proposed model (GAN) and a CNN, both were made to run on one image translation problem. The chosen problem was grayscale to color translation and the dataset was a subset of the Imagenet dataset. It was observed that the GAN took around eight hours lesser than the CNN model and the quality of images generated by the GAN were much superior than the one generated by the CNN. © Grenze Scientific Society, 2022.","Cost effectiveness; Medical applications; Semantics; Black and white images; Colour image; Edge map; Forms of representation; Image prediction; Image transformations; Image translation; Label maps; Original structures; Semantic labels; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85143342084"
"Song Q.; Li G.; Wu S.; Shen W.; Wong H.-S.","Song, Quanpeng (58024866100); Li, Guanyue (57211747687); Wu, Si (55495122900); Shen, Wenjun (57045143300); Wong, Hau-San (7402864844)","58024866100; 57211747687; 55495122900; 57045143300; 7402864844","Discriminator feature-based progressive GAN inversion","2023","Knowledge-Based Systems","261","","110186","","","","10.1016/j.knosys.2022.110186","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144446241&doi=10.1016%2fj.knosys.2022.110186&partnerID=40&md5=237a0f0541644cfe5035b6dbc6d83e8e","Generative Adversarial Networks (GANs) receive extensive attention due to its capability of high-fidelity image synthesis. The synthesized content can be edited by imposing transformations on the latent codes. To edit real images, the GAN inversion methods mainly focus on learning an inverse mapping from the data space to the latent space of a well-trained GAN through a separate encoder. In this paper, we propose a Discriminator Feature-based Progressive Inversion (DFPI) model for GAN-based image reconstruction and enhancement. We find that the expressiveness of the generator feature is richer than that of the latent code for a given input image. To achieve high-quality reconstruction, we estimate the generator features in a light-weight optimization process, conditioned on off-the-shelf discriminator features. Specifically, a recomposition matrix is learnt to map high-level discriminator features to the high-level generator features due to their reverse structure. To progressively improve the reconstruction quality, we adopt a convolutional correction module to alleviate the estimation error at a lower generator layer, conditioned on the features of a lower discriminator layer. By performing the progressive correction process, the input images can be accurately reconstructed. Note that the resulting recomposition matrix and correction module can be generalized to unseen images. The experiments demonstrate the superior performance of DFPI over competing methods on GAN inversion as well as a variety of image enhancement tasks. © 2022 Elsevier B.V.","Codes (symbols); Discriminators; Image enhancement; Image reconstruction; Inverse problems; Matrix algebra; Feature recomposition; Feature-based; Generative adversarial network inversion; High-fidelity images; Images synthesis; Input image; matrix; Network inversion; Real images; Synthesised; Generative adversarial networks","Feature recomposition; GAN inversion; Generative adversarial networks; Image enhancement","Article","Final","","Scopus","2-s2.0-85144446241"
"Endo Y.; Kanamori Y.","Endo, Yuki (37101260100); Kanamori, Yoshihiro (24168681900)","37101260100; 24168681900","Controlling StyleGANs using rough scribbles via one-shot learning","2022","Computer Animation and Virtual Worlds","33","5","e2102","","","","10.1002/cav.2102","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133560453&doi=10.1002%2fcav.2102&partnerID=40&md5=66e25ae19f3c754703c40fc94a3a9fe3","This paper tackles the challenging problem of one-shot semantic image synthesis from rough sparse annotations, which we call “semantic scribbles.” Namely, from only a single training pair annotated with semantic scribbles, we generate realistic and diverse images with layout control over, for example, facial part layouts and body poses. We present a training strategy that performs pseudo labeling for semantic scribbles using the StyleGAN prior. Our key idea is to construct a simple mapping between StyleGAN features and each semantic class from a single example of semantic scribbles. With such mappings, we can generate an unlimited number of pseudo semantic scribbles from random noise to train an encoder for controlling a pretrained StyleGAN generator. Even with our rough pseudo semantic scribbles obtained via one-shot supervision, our method can synthesize high-quality images thanks to our GAN inversion framework. We further offer optimization-based postprocessing to refine the pixel alignment of synthesized images. Qualitative and quantitative results on various datasets demonstrate improvement over previous approaches in one-shot settings. © 2022 John Wiley & Sons Ltd.","Image processing; Mapping; Semantics; Body pose; Facial parts; GAN inversion; Image editing; Images synthesis; Labelings; One-shot learning; Semantic images; Simple++; Training strategy; Generative adversarial networks","GAN inversion; generative adversarial networks; image editing","Conference paper","Final","","Scopus","2-s2.0-85133560453"
"Zhang H.; Li H.; Dillman J.R.; Parikh N.A.; He L.","Zhang, Huixian (57567872000); Li, Hailong (56767931400); Dillman, Jonathan R. (16425581500); Parikh, Nehal A. (7006642541); He, Lili (55389634500)","57567872000; 56767931400; 16425581500; 7006642541; 55389634500","Multi-Contrast MRI Image Synthesis Using Switchable Cycle-Consistent Generative Adversarial Networks","2022","Diagnostics","12","4","816","","","","10.3390/diagnostics12040816","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127917733&doi=10.3390%2fdiagnostics12040816&partnerID=40&md5=2f8718f43cb2aa8dfe56fcb4e0a95159","Multi-contrast MRI images use different echo and repetition times to highlight different tissues. However, not all desired image contrasts may be available due to scan-time limitations, suboptimal signal-to-noise ratio, and/or image artifacts. Deep learning approaches have brought revolutionary advances in medical image synthesis, enabling the generation of unacquired image contrasts (e.g., T1-weighted MRI images) from available image contrasts (e.g., T2-weighted images). Particularly, CycleGAN is an advanced technique for image synthesis using unpaired images. How-ever, it requires two separate image generators, demanding more training resources and computations. Recently, a switchable CycleGAN has been proposed to address this limitation and successfully im-plemented using CT images. However, it remains unclear if switchable CycleGAN can be applied to cross-contrast MRI synthesis. In addition, whether switchable CycleGAN is able to outperform original CycleGAN on cross-contrast MRI image synthesis is still an open question. In this paper, we developed a switchable CycleGAN model for image synthesis between multi-contrast brain MRI images using a large set of publicly accessible pediatric structural brain MRI images. We conducted extensive experiments to compare switchable CycleGAN with original CycleGAN both quantitatively and qualitatively. Experimental results demonstrate that switchable CycleGAN is able to outperform CycleGAN model on pediatric MRI brain image synthesis. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","article; artificial intelligence; brain; child; controlled study; deep learning; human; neuroimaging; nuclear magnetic resonance imaging; synthesis","artificial intelligence; CycleGAN; deep learning; MR imaging; pediatric brain; switchable CycleGAN","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85127917733"
"Gangwar A.; González-Castro V.; Alegre E.; Fidalgo E.","Gangwar, Abhishek (55339673000); González-Castro, Víctor (14055994700); Alegre, Enrique (55901820900); Fidalgo, Eduardo (36092799000)","55339673000; 14055994700; 55901820900; 36092799000","Triple-BigGAN: Semi-supervised generative adversarial networks for image synthesis and classification on sexual facial expression recognition","2023","Neurocomputing","528","","","200","216","16","10.1016/j.neucom.2023.01.027","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147254035&doi=10.1016%2fj.neucom.2023.01.027&partnerID=40&md5=d373788509295f9ea493f350f6c68a93","Automatic recognition of facial images showing erotic expressions can help to understand our social interaction and to detect non-appropriate images even when there is no nakedness present in them. This paper contemplates, for the first time, to exploit facial cues applied to automatic Sexual Facial Expression Recognition (SFER). With this goal, we introduce a new dataset named Sexual Expression and Activity Faces (SEA-Faces-30k) for SFER, which contains 30k manually labeled images under three categories: erotic, suggestive-erotic, and non-erotic. Deep Convolutional Neural Networks require large-scale annotated image datasets with diversity and variations to be properly trained. Unfortunately, gathering such a massive amount of data is not feasible in this area. Therefore, we present a new semi-supervised GAN framework named Triple-BigGAN, which learns a generative model and a classifier simultaneously. It learns both tasks in an end-to-end fashion while using unlabeled or partially labeled data. The Triple-BigGAN framework shows promising classification performance for the SFER task (i.e., 93.59%) and other five benchmark datasets, i.e., FER-2013, CIFAR-10, Expression in-the-Wild (ExpW), Modified National Institute of Standards and Technology database (MNIST), and Street View House Numbers (SVHN). Next, we evaluated the quality of samples generated by Triple-BigGAN with a resolution of 256×256 pixels using Inception Score (IS) and Frechet Inception Distance (FID). Our approach obtained the best FID (i.e., 19.94%) and IS (i.e., 97.98%) scores on the SEA-Faces-30k dataset. Further, we empirically demonstrated that synthetic erotic face images generated by Triple-BigGAN could also help in improving the classification performance of deep supervised networks. © 2023 Elsevier B.V.","Benchmarking; Classification (of information); Convolutional neural networks; Deep neural networks; Face recognition; Generative adversarial networks; Image classification; Image enhancement; Image retrieval; Large dataset; Classification performance; Deep learning; Emotion detection; Facial expression recognition; Facial Expressions; Learn+; Not safe for work; Obscene image retrieval; Pornography; Semi-supervised; article; classifier; convolutional neural network; deep learning; emotion; facial expression; facial recognition; image retrieval; pornography; protein expression; synthesis; Emotion Recognition","Deep learning; Emotion detection; Facial expressions; Not safe for work (NSFW); Obscene image retrieval; Pornography","Article","Final","","Scopus","2-s2.0-85147254035"
"Rao F.; Wu Z.; Han L.; Yang B.; Han W.; Zhu W.","Rao, Fan (57219226293); Wu, Zhuoxuan (57191827976); Han, Lu (57475425500); Yang, Bao (57102228600); Han, Weidong (56415556800); Zhu, Wentao (36516078000)","57219226293; 57191827976; 57475425500; 57102228600; 56415556800; 36516078000","Delayed PET imaging using image synthesis network and nonrigid registration without additional CT scan","2022","Medical Physics","49","5","","3233","3245","12","10.1002/mp.15574","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125670022&doi=10.1002%2fmp.15574&partnerID=40&md5=8d69da16da2dcc412e1aa436303d276a","Purpose: Attenuation correction is critical for positron emission tomography (PET) image reconstruction. The standard protocol for obtaining attenuation information in a clinical PET scanner is via coregistered computed tomography (CT) images. Therefore, for delayed PET imaging, the CT scan is repeated twice, which increases the radiation dose for the patient. In this paper, we propose a zero-extradose delayed PET imaging method that requires no additional CT scans. Methods: A deep learning-based synthesis network is designed to convert PET data into pseudo-CT images for delayed scans. Then, nonrigid registration is performed between this pseudo CT image and the CT image of the first scan, warping the CT image of the first scan to an estimated CT image for the delayed scan. Finally, the PET image attenuation correction in the delayed scan is obtained from this estimated CT image. Experiments with clinical datasets are implemented to assess the effectiveness of the proposed method with the well-recognized Generative Adversarial Networks (GAN) method. The average peak signal-to-noise ratio (PSNR) and the mean absolute percent error (MAPE) are used for comparison. We also use scoring from three experienced radiologists as subjective measurement means based on the diagnostic consistency of the PET images reconstructed from GAN and the proposed method with respect to the ground truth images. Results: The experiments show that the average PSNR is 47.04 dB (the proposed method) vs. 44.41 dB (the traditional GAN method) for the reconstructed delayed PET images in our evaluation dataset. The average MAPEs are 1.59% for the proposed method and 3.32% for the traditional GAN method across five organ regions of interest (ROIs). The scores for the GAN and the proposed method rated by three experienced radiologists are 8.08±0.60 and 9.02±0.52, indicating that the proposed method yields more consistent PET images with the ground truth. Conclusions: This work proposes a novel method for CT-less delayed PET imaging based on image synthesis network and nonrigid image registration. The PET image reconstructed using the proposed method yields delayed PET images with high image quality without artifacts and is quantitatively more accurate than the traditional GAN method. © 2022 American Association of Physicists in Medicine.","Computerized tomography; Data handling; Deep learning; Diagnosis; Generative adversarial networks; Image quality; Image reconstruction; Image registration; Medical imaging; Signal to noise ratio; Attenuation correction; Computed tomography images; Computed tomography scan; Delayed positron emission tomography imaging; Image synthesis network; Images synthesis; Network methods; Nonrigid registration; Peak signal to noise ratio; Tomography imaging; article; artifact; comparative effectiveness; computer assisted tomography; controlled study; deep learning; human; image quality; image registration; positron emission tomography; quantitative analysis; radiologist; signal noise ratio; synthesis; x-ray computed tomography; Positron emission tomography","attenuation correction; delayed PET imaging; image synthesis network; nonrigid registration","Article","Final","","Scopus","2-s2.0-85125670022"
"Xue Y.; Guo Y.-C.; Zhang H.; Xu T.; Zhang S.-H.; Huang X.","Xue, Yuan (57202313224); Guo, Yuan-Chen (57217106040); Zhang, Han (56098272800); Xu, Tao (56465290800); Zhang, Song-Hai (36968379500); Huang, Xiaolei (57218604182)","57202313224; 57217106040; 56098272800; 56465290800; 36968379500; 57218604182","Deep image synthesis from intuitive user input: A review and perspectives","2022","Computational Visual Media","8","1","","3","31","28","10.1007/s41095-021-0234-8","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118267980&doi=10.1007%2fs41095-021-0234-8&partnerID=40&md5=2b7d0359e8247afacbc1214449b3bd1c","In many applications of computer graphics, art, and design, it is desirable for a user to provide intuitive non-image input, such as text, sketch, stroke, graph, or layout, and have a computer system automatically generate photo-realistic images according to that input. While classically, works that allow such automatic image content generation have followed a framework of image retrieval and composition, recent advances in deep generative models such as generative adversarial networks (GANs), variational autoencoders (VAEs), and flow-based methods have enabled more powerful and versatile image generation approaches. This paper reviews recent works for image synthesis given intuitive user input, covering advances in input versatility, image generation methodology, benchmark datasets, and evaluation metrics. This motivates new perspectives on input representation and interactivity, cross fertilization between major image generation paradigms, and evaluation and comparison of generation methods. © 2021, The Author(s).","Arts computing; Character recognition; Computer graphics; Image processing; Image retrieval; Quality control; Computer designs; Deep generative model; Generative model; Image generations; Image quality evaluation; Images synthesis; Intuitive user input; Synthesized image quality evaluation; Synthesized images; User input; Generative adversarial networks","deep generative models; image synthesis; intuitive user input; synthesized image quality evaluation","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118267980"
"Xia X.; Zhan K.; Li Y.; Xiao G.; Yan J.; Huang Z.; Huang G.; Fang Y.","Xia, Xue (57189045358); Zhan, Kun (57997954400); Li, Ying (57872849400); Xiao, Guobei (57902116900); Yan, Jinhua (57998083000); Huang, Zhuxiang (57872462900); Huang, Guofu (57998210500); Fang, Yuming (57997691300)","57189045358; 57997954400; 57872849400; 57902116900; 57998083000; 57872462900; 57998210500; 57997691300","Eye Disease Diagnosis and Fundus Synthesis: A Large-Scale Dataset and Benchmark","2022","2022 IEEE 24th International Workshop on Multimedia Signal Processing, MMSP 2022","","","","","","","10.1109/MMSP55362.2022.9949547","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143613021&doi=10.1109%2fMMSP55362.2022.9949547&partnerID=40&md5=6d7332b2e883585c36234d81f6e943f7","As one of the most common imaging modalities, retinal fundus imaging offers images of interior surface of eyes for initial examination of disorders. Data-driven machine learning methods, especially deep learning models in recent years, provide automatic ophthalmological disease diagnosis techniques from color fundus images. Data with high quality, diversity and balanced distribution supports deep model-based eye disease diagnosis. However, many existing datasets focus on a specific kind of eye disease, and some suffer from label noise or quality degeneration, which hinders automatic screening algorithms from dealing with multiple eye diseases. To solve this, we propose a high-quality dataset containing 28877 color fundus images for deep learning-based diagnosis. Except for 15000 healthy samples, the dataset consists of 8 eye disorders including diabetic retinopathy, agerelated macular degeneration, glaucoma, pathological myopia, hypertension, retinal vein occlusion, LASIK spot and others. Based on this, we propose a co-attention network for disease diagnosis, establish benchmark on screening and grading tasks, and demonstrate that the proposed dataset supports generative adversarial network-based image synthesis. The dataset will be made publicly available. © 2022 IEEE.","Computer vision; Deep learning; Diagnosis; Eye protection; Generative adversarial networks; Large dataset; Learning systems; Medical imaging; Ophthalmology; Dataset; Disease diagnosis; Eye disease; Eye disease diagnose; Fundus image; Fundus imaging; Fundus sythesis; High quality; Imaging modality; Large-scale datasets; Grading","dataset; eye disease diagnosis; fundus sythesis","Conference paper","Final","","Scopus","2-s2.0-85143613021"
"Aljohani A.; Alharbe N.","Aljohani, Abeer (57212101721); Alharbe, Nawaf (56100322100)","57212101721; 56100322100","Generating Synthetic Images for Healthcare with Novel Deep Pix2Pix GAN","2022","Electronics (Switzerland)","11","21","3470","","","","10.3390/electronics11213470","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141732445&doi=10.3390%2felectronics11213470&partnerID=40&md5=9c9d065c2ea83919f343d9089179d286","Due to recent developments in deep learning and artificial intelligence, the healthcare industry is currently going through a significant upheaval. Despite a considerable advance in medical imaging and diagnostics, the healthcare industry still has a lot of unresolved problems and unexplored applications. The transmission of a huge number of medical images in particular is a difficult and time-consuming problem. In addition, obtaining new medical images is too expensive. To tackle these issues, we propose deep pix2pix generative adversarial networks (GAN) for generating synthetic medical images. For the comparison, we implemented CycleGAN, Pix2Pix GAN and Deep Pix2Pix GAN. The result has shown that our proposed approach can generate a new synthetic medical image from a different image with more accuracy than that of the other models. To provide a robust model, we trained and evaluated our models on a widely used brain image dataset, the IXI Dataset. © 2022 by the authors.","","CycleGAN; GAN; image synthesis; image transmission; medical images; Pix2Pix GAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141732445"
"Choi I.; Park S.; Park J.","Choi, InMoon (57991684600); Park, Soonchan (56419007600); Park, Jiyoung (55716938500)","57991684600; 56419007600; 55716938500","Generating and Modifying High Resolution Fashion Model Image using StyleGAN","2022","International Conference on ICT Convergence","2022-October","","","1536","1538","2","10.1109/ICTC55196.2022.9952574","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143253150&doi=10.1109%2fICTC55196.2022.9952574&partnerID=40&md5=e28153a77879fd718c14b89b2a201968","In this paper, a research of synthesizing fashion model images by utilizing a state-of-the-art generative adversarial network (i.e., GAN) is introduced. After training GAN with fashion model images, the network was able to generate realistic fashion model images having various characteristics such as pose and clothes. Moreover, two image modifications named Fashion Model Morphing and Fashion Transfer are also proposed by merging attributes of two generated fashion model images. The research investigates the effectiveness of using GAN for fashion to create a large number of images for exploring new design and styles. The generated images are even more beneficial for fashion industries because the generated images have no legal issues such as portrait right and copyright.  © 2022 IEEE.","Computer vision; Fashion; Fashion models; Generative neural network; High resolution; Image modification; Images synthesis; Model images; Morphing; Neural-networks; State of the art; Generative adversarial networks","Fashion; Generative Neural Network; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85143253150"
"Sushko V.; Schönfeld E.; Zhang D.; Gall J.; Schiele B.; Khoreva A.","Sushko, Vadim (57221708217); Schönfeld, Edgar (57214473141); Zhang, Dan (57221171748); Gall, Juergen (23396675200); Schiele, Bernt (55267534700); Khoreva, Anna (56406498500)","57221708217; 57214473141; 57221171748; 23396675200; 55267534700; 56406498500","OASIS: Only Adversarial Supervision for Semantic Image Synthesis","2022","International Journal of Computer Vision","130","12","","2903","2923","20","10.1007/s11263-022-01673-x","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138205793&doi=10.1007%2fs11263-022-01673-x&partnerID=40&md5=e74263a901a3d1d4e3c122b6c6ecb1f9","Despite their recent successes, generative adversarial networks (GANs) for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Previously, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limited the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity and with a better alignment to their input label maps, making the use of the perceptual loss superfluous. Furthermore, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image editing. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve a strong improvement in image synthesis quality over prior state-of-the-art models across the commonly used ADE20K, Cityscapes, and COCO-Stuff datasets using only adversarial supervision. In addition, we investigate semantic image synthesis under severe class imbalance and sparse annotations, which are common aspects in practical applications but were overlooked in prior works. To this end, we evaluate our model on LVIS, a dataset originally introduced for long-tailed object recognition. We thereby demonstrate high performance of our model in the sparse and unbalanced data regimes, achieved by means of the proposed 3D noise and the ability of our discriminator to balance class contributions directly in the loss function. Our code and pretrained models are available at https://github.com/boschresearch/OASIS. © 2022, The Author(s).","Image enhancement; Object recognition; Semantic Segmentation; Semantics; Textures; High quality; Image editing; Image translation; Images synthesis; Label maps; Label-to-image translation; Network models; Semantic image synthesis; Semantic images; Semantic segmentation; Generative adversarial networks","GAN; Image editing; Label-to-image translation; Semantic image synthesis; Semantic segmentation","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85138205793"
"Abry P.; Mauduit V.; Quemener E.; Roux S.","Abry, Patrice (6701426529); Mauduit, Vincent (57220006277); Quemener, Emmanuel (57220004964); Roux, Stephane (57196937621)","6701426529; 57220006277; 57220004964; 57196937621","Multivariate multifractal texture DCGAN synthesis: How well does it work ? How does one know ?","2022","Journal of Signal Processing Systems","94","2","","179","195","16","10.1007/s11265-021-01701-y","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123097863&doi=10.1007%2fs11265-021-01701-y&partnerID=40&md5=5ed6a4c039198f5f4f51cc967954d926","In a recent past, Deep Learning emerged as a standard tool in Image Processing, commonly involved in numerous and various tasks. Notably, Deep Learning has become increasingly popular for the synthesis of images in several applications different in nature. However, research efforts have been massively focused on designing new and increasingly complex architectures to achieve yet better performance, often at the price of overlooking the uneasy question of the assessment of the quality of the synthesized images. Focusing on the specific context of pure textures, i.e., of images with no geometrical contents, the present work aims to propose a methodology that permits to quantify the quality of Deep Learning synthesized images. It makes use of Deep Convolutional Generative Adversarial Networks, a specific class of trained neural networks, commonly used for image synthesis. Because they provide versatile and well-documented texture models, multivariate multifractal fields, with rich multiscale cross-statistics (scale-free and multifractal textures), are used. A posteriori synthesis quality indices are defined from the statistics of multiscale (wavelet) representations computed on deep learning generated multivariate textures and compared to those associated with the models. These comparisons permit to objectively quantify the quality of deep learning texture synthesis as well as the reproducibility of the training and learning procedures, an approach that departs from reporting only the training yielding best performance. This methodology further permits to quantify objectively the variation in the quality of deep learning generated multivariate textures with respect to the complexity of deep learning architectures. Moreover, a priori indices, constructed directly on loss functions, hence much easier to compute, are also proposed and shown to correlate significantly with the a posteriori and costly multiscale representation synthesis quality indices. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Complex networks; Convolutional neural networks; Deep learning; Generative adversarial networks; Multivariant analysis; Network architecture; Textures; Loss functions; Multi fractals; Multiscale statistic; Multivariate texture; Multivariate texture synthesis; Performance; Posteriori; Quality assessment; Synthesized images; Texture synthesis; Fractals","Generative Adversarial Network; Loss Functions; Multifractals; Multiscale Statistics; Multivariate Texture Synthesis; Quality Assessment","Article","Final","","Scopus","2-s2.0-85123097863"
"Chatterjee S.; Hazra D.; Byun Y.-C.; Kim Y.-W.","Chatterjee, Subhajit (57726402600); Hazra, Debapriya (57191375984); Byun, Yung-Cheol (8897891700); Kim, Yong-Woon (57215321523)","57726402600; 57191375984; 8897891700; 57215321523","Enhancement of Image Classification Using Transfer Learning and GAN-Based Synthetic Data Augmentation","2022","Mathematics","10","9","1541","","","","10.3390/math10091541","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130716749&doi=10.3390%2fmath10091541&partnerID=40&md5=15acc758ded786cff957f0a6b890f529","Plastic bottle recycling has a crucial role in environmental degradation and protection. Position and background should be the same to classify plastic bottles on a conveyor belt. The manual detection of plastic bottles is time consuming and leads to human error. Hence, the automatic classification of plastic bottles using deep learning techniques can assist with the more accurate results and reduce cost. To achieve a considerably good result using the DL model, we need a large volume of data to train. We propose a GAN-based model to generate synthetic images similar to the original. To improve the image synthesis quality with less training time and decrease the chances of mode collapse, we propose a modified lightweight-GAN model, which consists of a generator and a discriminator with an auto-encoding feature to capture essential parts of the input image and to encourage the generator to produce a wide range of real data. Then a newly designed weighted average ensemble model based on two pre-trained models, inceptionV3 and xception, to classify transparent plastic bottles obtains an improved classification accuracy of 99.06%. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","deep learning; generative adversarial networks; image classification; plastic bottle; transfer learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130716749"
"Ahn G.; Choi B.S.; Ko S.; Jo C.; Han H.-S.; Lee M.C.; Ro D.H.","Ahn, Gun (57914448500); Choi, Byung Sun (57915092000); Ko, Sunho (57209608067); Jo, Changwung (57209603411); Han, Hyuk-Soo (36742731600); Lee, Myung Chul (35070050100); Ro, Du Hyun (55644473500)","57914448500; 57915092000; 57209608067; 57209603411; 36742731600; 35070050100; 55644473500","High-resolution knee plain radiography image synthesis using style generative adversarial network adaptive discriminator augmentation","2023","Journal of Orthopaedic Research","41","1","","84","93","9","10.1002/jor.25325","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139259788&doi=10.1002%2fjor.25325&partnerID=40&md5=76e593fd6e3173c6cd200530b0996bc4","In this retrospective study, 10,000 anteroposterior (AP) radiography of the knee from a single institution was used to create medical data set that are more balanced and cheaper to create. Two types of convolutional networks were used, deep convolutional GAN (DCGAN) and Style GAN Adaptive Discriminator Augmentation (StyleGAN2-ADA). To verify the quality of generated images from StyleGAN2-ADA compared to real ones, the Visual Turing test was conducted by two computer vision experts, two orthopedic surgeons, and a musculoskeletal radiologist. For quantitative analysis, the Fréchet inception distance (FID), and principal component analysis (PCA) were used. Generated images reproduced the features of osteophytes, joint space narrowing, and sclerosis. Classification accuracy of the experts was 34%, 43%, 44%, 57%, and 50%. FID between the generated images and real ones was 2.96, which is significantly smaller than another medical data set (BreCaHAD = 15.1). PCA showed that no significant difference existed between the PCs of the real and generated images (p > 0.05). At least 2000 images were required to make reliable images optimally. By performing PCA in latent space, we were able to control the desired PC that show a progression of arthritis. Using a GAN, we were able to generate knee X-ray images that accurately reflected the characteristics of the arthritis progression stage, which neither human experts nor artificial intelligence could discern apart from the real images. In summary, our research opens up the potential to adopt a generative model to synthesize realistic anonymous images that can also solve data scarcity and class inequalities. © 2022 Orthopaedic Research Society. Published by Wiley Periodicals LLC.","Arthritis; Artificial Intelligence; Humans; Knee Joint; Radiography; Retrospective Studies; Article; artificial intelligence; computer vision; convolutional neural network; diagnostic imaging; dimensionality reduction; disease exacerbation; human; image analysis; image quality; joint cavity; knee arthritis; knee radiography; major clinical study; musculoskeletal radiologist; orthopedic surgeon; osteophyte; osteosclerosis; principal component analysis; quantitative analysis; retrospective study; subchondral bone; arthritis; artificial intelligence; knee; radiography","diagnostic imaging; knee","Article","Final","","Scopus","2-s2.0-85139259788"
"Chai L.; Wang Z.; Chen J.; Zhang G.; Alsaadi F.E.; Alsaadi F.E.; Liu Q.","Chai, Lu (57893008900); Wang, Zidong (55810114200); Chen, Jianqing (57894218700); Zhang, Guokai (57197828490); Alsaadi, Fawaz E. (57204294532); Alsaadi, Fuad E. (23566767000); Liu, Qinyuan (56383093500)","57893008900; 55810114200; 57894218700; 57197828490; 57204294532; 23566767000; 56383093500","Synthetic augmentation for semantic segmentation of class imbalanced biomedical images: A data pair generative adversarial network approach","2022","Computers in Biology and Medicine","150","","105985","","","","10.1016/j.compbiomed.2022.105985","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138219316&doi=10.1016%2fj.compbiomed.2022.105985&partnerID=40&md5=ca23efa36b6d63038b15bec928ea2c00","In recent years, deep learning (DL) has been recognized very useful in the semantic segmentation of biomedical images. Such an application, however, is significantly hindered by the lack of pixel-wise annotations. In this work, we propose a data pair generative adversarial network (DPGAN) for the purpose of synthesizing concurrently the diverse biomedical images and the segmentation labels from random latent vectors. First, a hierarchical structure is constructed consisting of three variational auto-encoder generative adversarial networks (VAEGANs) with an extra discriminator. Subsequently, to alleviate the influence from the imbalance between lesions and non-lesions areas in biomedical segmentation data sets, we divide the DPGAN into three stages, namely, background stage, mask stage and advanced stage, with each stage deploying a VAEGAN. In such a way, a large number of new segmentation data pairs are generated from random latent vectors and then used to augment the original data sets. Finally, to validate the effectiveness of the proposed DPGAN, experiments are carried out on a vestibular schwannoma data set, a kidney tumor data set and a skin cancer data set. The results indicate that, in comparison to other state-of-the-art GAN-based methods, the proposed DPGAN shows better performance in the generative quality, and meanwhile, gains an effective boost on semantic segmentation of class imbalanced biomedical images. © 2022 Elsevier Ltd","Deep learning; Semantic Segmentation; Semantic Web; Semantics; Auto encoders; Biomedical image segmentation; Biomedical image synthesis; Biomedical images; Data pairs; Data set; Hierarchical generative adversarial network; Images synthesis; Latent vectors; Semantic segmentation; acoustic neuroma; Article; artificial intelligence; artificial neural network; autoencoder; convolutional neural network; data pair generative adversarial network; data synthesis; image analysis; image processing; image segmentation; kidney tumor; skin cancer; variational autoencoder generative adversarial network; Generative adversarial networks","Artificial intelligence; Biomedical image segmentation; Biomedical image synthesis; Hierarchical generative adversarial network","Article","Final","","Scopus","2-s2.0-85138219316"
"Selim M.; Zhang J.; Fei B.; Lewis M.; Zhang G.-Q.; Chen J.","Selim, Md (57219739473); Zhang, Jie (57150866600); Fei, Baowei (7005499116); Lewis, Matthew (35515966200); Zhang, Guo-Qiang (7405271191); Chen, Jin (57203334776)","57219739473; 57150866600; 7005499116; 35515966200; 7405271191; 57203334776","UDA-CT: A General Framework for CT Image Standardization","2022","Proceedings - 2022 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2022","","","","1698","1701","3","10.1109/BIBM55620.2022.9995168","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146654565&doi=10.1109%2fBIBM55620.2022.9995168&partnerID=40&md5=fec0b386faa5cb9dd08282628ba13817","Large-scale CT image studies often suffer from a lack of homogeneity regarding radiomic characteristics due to the images acquired with scanners from different vendors or with different reconstruction algorithms. We propose a deep learning-based framework called UDA-CT to tackle the homogeneity issue by leveraging both paired and unpaired images. Using UDA-CT, the CT images can be standardized both from different acquisition protocols of the same scanner and CT images acquired using a similar protocol but scanners from different vendors. UDA-CT incorporates recent advances in deep learning including domain adaptation and adversarial augmentation. It includes a unique design for model training batch which integrates nonstandard images and their adversarial variations to enhance model generalizability. The experimental results show that UDA-CT significantly improves the performance of the cross-scanner image standardization by utilizing both paired and unpaired data.  © 2022 IEEE.","Computerized tomography; Deep learning; Generative adversarial networks; Image acquisition; Image enhancement; Medical imaging; Acquisition protocols; Computed tomography; CT Image; Domain adaptation; Image standardization; Image study; Images synthesis; Large-scales; Radiomic; Reconstruction algorithms; Standardization","Computed Tomography; Domain Adaptation; Generative Adversarial Network; Image Synthesis; Radiomics","Conference paper","Final","","Scopus","2-s2.0-85146654565"
"Kong C.; Kim J.; Han D.; Kwak N.","Kong, Chaerin (57357272700); Kim, Jeesoo (57191958513); Han, Donghoon (57222730488); Kwak, Nojun (7005248772)","57357272700; 57191958513; 57222730488; 7005248772","Few-Shot Image Generation with Mixup-Based Distance Learning","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13675 LNCS","","","563","580","17","10.1007/978-3-031-19784-0_33","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142723182&doi=10.1007%2f978-3-031-19784-0_33&partnerID=40&md5=f78d0f5ad24a658d56373bcb9862f184","Producing diverse and realistic images with generative models such as GANs typically requires large scale training with vast amount of images. GANs trained with limited data can easily memorize few training samples and display undesirable properties like “stairlike” latent space where interpolation in the latent space yields discontinuous transitions in the output space. In this work, we consider a challenging task of pretraining-free few-shot image synthesis, and seek to train existing generative models with minimal overfitting and mode collapse. We propose mixup-based distance regularization on the feature space of both a generator and the counterpart discriminator that encourages the two players to reason not only about the scarce observed data points but the relative distances in the feature space they reside. Qualitative and quantitative evaluation on diverse datasets demonstrates that our method is generally applicable to existing models to enhance both fidelity and diversity under few-shot setting. Codes are available (https://github.com/reyllama/mixdl ). © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Computer vision; Distance education; Distance-learning; Feature space; Few-shot image generation; Generative adversarial network; Generative model; Image generations; Large-scales; Latent mixup; Limited data; Realistic images; Generative adversarial networks","Few-shot image generation; Generative Adversarial Networks (GANs); Latent mixup","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142723182"
"Huang S.; He C.; Cheng R.","Huang, Shihua (57218712665); He, Cheng (56621868400); Cheng, Ran (57791394300)","57218712665; 56621868400; 57791394300","SoloGAN: Multi-domain Multimodal Unpaired Image-to-Image Translation via a Single Generative Adversarial Network","2022","IEEE Transactions on Artificial Intelligence","3","5","","722","737","15","10.1109/TAI.2022.3187384","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133755028&doi=10.1109%2fTAI.2022.3187384&partnerID=40&md5=f995978b3e0062abf5c59c3830dce267","Despite significant advances in image-to-image (I2I) translation with generative adversarial networks (GANs), it remains challenging to effectively translate an image to a set of diverse images in multiple target domains using a pair of generator and discriminator. Existing multimodal I2I translation methods adopt multiple domain-specific content encoders for different domains, where each domain-specific content encoder is trained with images from the same domain only. Nevertheless, we argue that the content (domain-invariance) features should be learned from images among all of the domains. Consequently, each domain-specific content encoder of existing schemes fails to extract the domain-invariant features efficiently. To address this issue, we present a flexible and general SoloGAN model for efficient multimodal I2I translation among multiple domains with unpaired data. In contrast to existing methods, the SoloGAN algorithm uses a single projection discriminator with an additional auxiliary classifier and shares the encoder and generator for all domains. As such, the SoloGAN model can be trained effectively with images from all domains so that the domain-invariance content representation can be efficiently extracted. Qualitative and quantitative results over a wide range of datasets against several counterparts and variants of the SoloGAN model demonstrate the merits of the method, especially for challenging I2I translation tasks, i.e., tasks that involve extreme shape variations or need to keep the complex backgrounds unchanged after translations. Furthermore, we demonstrate the contribution of each component using ablation studies.  © 2020 IEEE.","Computer vision; Signal encoding; Dog; Domain specific; Generator; Image translation; Image-to-image translation; Images synthesis; Multi-modal; Multiple domains; Shape; Generative adversarial networks","Generative adversarial network (GAN); image synthesis; image-to-image (I2I) translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133755028"
"Zhan B.; Zhou L.; Li Z.; Wu X.; Pu Y.; Zhou J.; Wang Y.; Shen D.","Zhan, Bo (57221803799); Zhou, Luping (23398846800); Li, Zhiang (57223440128); Wu, Xi (57221065403); Pu, Yifei (7103191447); Zhou, Jiliu (21234416400); Wang, Yan (56039981100); Shen, Dinggang (7401738392)","57221803799; 23398846800; 57223440128; 57221065403; 7103191447; 21234416400; 56039981100; 7401738392","D2FE-GAN: Decoupled dual feature extraction based GAN for MRI image synthesis","2022","Knowledge-Based Systems","252","","109362","","","","10.1016/j.knosys.2022.109362","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134892717&doi=10.1016%2fj.knosys.2022.109362&partnerID=40&md5=1d4902266bc7f9b6f8243786f8a3c364","Magnetic resonance imaging (MRI) technique can generate various tissue contrasts by using different pulse sequences and parameters. However, obtaining multiple different contrast images for the same patient is sometimes time-consuming and costly. In this paper, we propose a novel generative adversarial network based on decoupled dual feature representations (D2FE-GAN) for cross-modality MRI synthesis. Inspired by the previous works of image style transferring, we argue that the MRI images can be viewed as a compound of underlying information shared among the bodies of modalities (e.g., semantic information), and representative information varying with the styles of modalities (e.g., edges, contrasts). Different from the existing GAN-based methods that pay attention to either the body consistency or the style refinement, the proposed D2FE-GAN method considers both aspects for better synthesis. Specifically, our method decouples the underlying information and the representative information from the source modality and target modality, respectively, through two dissimilar encoders. In response to the invisibility of target modality in testing phase, we propose to employ a Residual Network firstly to generate an intermediate modality as the pseudo target modality. Subsequently, the decoupled two kinds of information will be integrated through a decoder. Here, we introduce the Adaptive Instance Normalization layer, in which the affine parameters are replaced by the mean and standard deviation of the representative information, thus completing the fusion processing of feature space information. Experimental results on BRATS2015 dataset and IXI dataset show that the proposed method outperforms the state-of-the-art image synthesis approaches in both qualitative and quantitative measures. © 2022 Elsevier B.V.","Extraction; Feature extraction; Magnetic resonance imaging; Semantics; Cross modality; Decoupled dual feature extraction (D2FE); Feature representation; Features extraction; Generative adversarial network; Images synthesis; Magnetic resonance imaging; Network-based; Pulse parameter; Pulse sequence; Generative adversarial networks","Decoupled dual feature extraction (D2FE); Generative adversarial networks (GAN); Image synthesis; Magnetic resonance imaging (MRI)","Article","Final","","Scopus","2-s2.0-85134892717"
"Shastri H.; Lodhavia D.; Purohit P.; Kaoshik R.; Batra N.","Shastri, Hetvi (57369729600); Lodhavia, Dhruvi (57409424900); Purohit, Palak (57288505100); Kaoshik, Ronak (57221335671); Batra, Nipun (55570910100)","57369729600; 57409424900; 57288505100; 57221335671; 55570910100","Vastr-GAN: Versatile Apparel Synthesised from Text using a Robust Generative Adversarial Network","2022","ACM International Conference Proceeding Series","","","","222","226","4","10.1145/3493700.3493721","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122653386&doi=10.1145%2f3493700.3493721&partnerID=40&md5=f420c7dbd6dd9daae81f07cfde586610","The development of the fashion industry has increased the demand for customised and meticulously designed clothes. This poses a challenge to fashion designers who need to create novel clothing designs based on the requirements specified by the customers. This work presents a generative adversarial network (GAN) based text-to-image synthesis model for fabricating intricate Indian apparel designs. We introduce an architecture that strategically combines multiple trained GAN models for a streamlined text-to-image generation. Existing fashion datasets with elaborate image descriptions cater to western fashion only. We have extracted traditional Indian images like kurtis, kurtas, etc., and then combined with an existing dataset to create an Indian Fashion dataset of around 16000 images with their corresponding text descriptions. On carrying out elaborate testing on our dataset we have achieved good visual results that can capture the details given in the text descriptions.  © 2022 ACM.","Computer vision; Image classification; Signal encoding; Statistical tests; Clothing design; Fashion designers; Fashion industry; Images synthesis; Indian fashion; Network-based; Synthesis models; Synthesised; Text encoder; Text-to-image synthesis; Generative adversarial networks","Classifier; GAN; Indian Fashion; Text encoder; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85122653386"
"Sekar A.; Perumal V.","Sekar, Aravindkumar (57391714500); Perumal, Varalakshmi (46661903700)","57391714500; 46661903700","CFC-GAN: Forecasting Road Surface Crack Using Forecasted Crack Generative Adversarial Network","2022","IEEE Transactions on Intelligent Transportation Systems","23","11","","21378","21391","13","10.1109/TITS.2022.3171433","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132523722&doi=10.1109%2fTITS.2022.3171433&partnerID=40&md5=21663f3f805adcdfb693f85fe46389b1","Forecasting the road surface crack images with given present crack images is an important task to assist the road survivors in planning for their next lay down of road with the required financial assistance. We develop a Crack ForeCast (CFC) dataset comprised of road crack images captured with certain time intervals (months), the number of vehicles traveled (collected from toll plaza), and climatic information like temperature and precipitation. We have proposed a Conditional Forecasted Crack-Generative Adversarial Network (CFC-GAN) model to forecast the road surface crack images with various conditional factors. The proposed CFC-GAN model is trained in a paired end-to-end manner to avoid accumulative blurriness and to generate the appropriate forecasted crack images. Moreover, this paper introduces a new estimation loss function to improve the accuracy of the forecasted crack images. Wide experimental results were demonstrated to showcase the better performance of our model. The quantitative and qualitative analyses were made with various evaluation metrics like structural similarity, peak signal to noise ratio, inception score, Frechet inception distance, and intersection over union for the CFC-GAN model with the proposed dataset and other existing benchmark datasets.  © 2000-2011 IEEE.","Generative adversarial networks; Image enhancement; Roads and streets; Signal to noise ratio; Surface defects; Toll highways; Conditional forecasted crack-generative adversarial network; Crack image; Crack image synthesis; Image translation; Image-to-image translation; Images synthesis; Paired image synthesis; Road crack forecasting.; Road cracks; Forecasting","Conditional forecasted crack-generative adversarial network (CFC-GAN); crack image synthesis; image-to-image translation; paired image synthesis; road crack forecasting","Article","Final","","Scopus","2-s2.0-85132523722"
"Zhang Y.; Wang Q.; Hu B.","Zhang, Yipeng (57353664800); Wang, Quan (56145277600); Hu, Bingliang (26660863200)","57353664800; 56145277600; 26660863200","MinimalGAN: diverse medical image synthesis for data augmentation using minimal training data","2023","Applied Intelligence","53","4","","3899","3916","17","10.1007/s10489-022-03609-x","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131311355&doi=10.1007%2fs10489-022-03609-x&partnerID=40&md5=072ad3d9c67005d0215036552b7f33d8","Image synthesis techniques have limited application in the medical field due to unsatisfactory authenticity and precision. Additionally, synthesizing diverse outputs is challenging when the training data are insufficient, as in many medical datasets. In this work, we propose an image-to-image network named the Minimal Generative Adversarial Network (MinimalGAN), to synthesize annotated, accurate, and diverse medical images with minimal training data. The primary concept is to make full use of the internal information of the image and decouple the style from the content by separating them in the self-coding process. After that, the generator is compelled to concentrate on content detail and style separately to synthesize diverse and high-precision images. The proposed MinimalGAN includes two image synthesis techniques; the first is style transfer. We synthesized a stylized retinal fundus dataset. The style transfer deception rate is much higher than that of traditional style transfer methods. The blood vessel segmentation performance increased when only using synthetic data. The other image synthesis technique is target variation. Unlike the traditional translation, rotation, and scaling on the whole image, this approach only performs the above operations on the segmented target being annotated. Experiments demonstrate that segmentation performance improved after utilizing synthetic data. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Blood vessels; Generative adversarial networks; Image coding; Medical imaging; Data augmentation; Image generations; Images segmentations; Images synthesis; Medical fields; Minimal training; Segmentation performance; Synthesis techniques; Synthetic data; Training data; Image segmentation","Data augmentation; Image generation; Image segmentation; Medical imaging","Article","Final","","Scopus","2-s2.0-85131311355"
"Yurt M.; Dalmaz O.; Dar S.; Ozbey M.; Tinaz B.; Oguz K.; Çukur T.","Yurt, Mahmut (57211187325); Dalmaz, Onat (57226257796); Dar, Salman (57195220338); Ozbey, Muzaffer (57213816253); Tinaz, Berk (57219362322); Oguz, Kader (35564489500); Çukur, Tolga (23034054800)","57211187325; 57226257796; 57195220338; 57213816253; 57219362322; 35564489500; 23034054800","Semi-Supervised Learning of MRI Synthesis Without Fully-Sampled Ground Truths","2022","IEEE Transactions on Medical Imaging","41","12","","3895","3906","11","10.1109/TMI.2022.3199155","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136847494&doi=10.1109%2fTMI.2022.3199155&partnerID=40&md5=aa19587594541339db8e6d1b6ffe25a7","Learning-based translation between MRI contrasts involves supervised deep models trained using high-quality source- and target-contrast images derived from fully-sampled acquisitions, which might be difficult to collect under limitations on scan costs or time. To facilitate curation of training sets, here we introduce the first semi-supervised model for MRI contrast translation (ssGAN) that can be trained directly using undersampled k-space data. To enable semi-supervised learning on undersampled data, ssGAN introduces novel multi-coil losses in image, k-space, and adversarial domains. The multi-coil losses are selectively enforced on acquired k-space samples unlike traditional losses in single-coil synthesis models. Comprehensive experiments on retrospectively undersampled multi-contrast brain MRI datasets are provided. Our results demonstrate that ssGAN yields on par performance to a supervised model, while outperforming single-coil models trained on coil-combined magnitude images. It also outperforms cascaded reconstruction-synthesis models where a supervised synthesis model is trained following self-supervised reconstruction of undersampled data. Thus, ssGAN holds great promise to improve the feasibility of learning-based multi-contrast MRI synthesis.  © 1982-2012 IEEE.","Algorithms; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Retrospective Studies; Supervised Machine Learning; Biological systems; Computer vision; Data structures; Generative adversarial networks; Image reconstruction; Supervised learning; Adversarial; Biological system modeling; Computational modelling; Images reconstruction; Images synthesis; MRI contrasts; Semi-supervised; Semi-supervised learning; Synthesis models; Under sampled; article; biology; brain; computer model; controlled study; feasibility study; human; image reconstruction; learning; neuroimaging; nuclear magnetic resonance imaging; synthesis; algorithm; image processing; nuclear magnetic resonance imaging; procedures; retrospective study; supervised machine learning; Magnetic resonance imaging","adversarial; image synthesis; Magnetic resonance imaging; semi-supervised; undersampled","Article","Final","","Scopus","2-s2.0-85136847494"
"Mendes J.; Pereira T.; Silva F.; Frade J.; Morgado J.; Freitas C.; Negrão E.; de Lima B.F.; da Silva M.C.; Madureira A.J.; Ramos I.; Costa J.L.; Hespanhol V.; Cunha A.; Oliveira H.P.","Mendes, José (57219839885); Pereira, Tania (57214024149); Silva, Francisco (57220829229); Frade, Julieta (57219841729); Morgado, Joana (57222710911); Freitas, Cláudia (57194873479); Negrão, Eduardo (57212081287); de Lima, Beatriz Flor (57222713310); da Silva, Miguel Correia (57222713528); Madureira, António J. (6603716596); Ramos, Isabel (57191864412); Costa, José Luís (55937368000); Hespanhol, Venceslau (6602524273); Cunha, António (7103392597); Oliveira, Hélder P. (23390156200)","57219839885; 57214024149; 57220829229; 57219841729; 57222710911; 57194873479; 57212081287; 57222713310; 57222713528; 6603716596; 57191864412; 55937368000; 6602524273; 7103392597; 23390156200","Lung CT image synthesis using GANs","2023","Expert Systems with Applications","215","","119350","","","","10.1016/j.eswa.2022.119350","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143869959&doi=10.1016%2fj.eswa.2022.119350&partnerID=40&md5=2c8b8d9129ddaa5defd6c43fca5b2a10","Biomedical engineering has been targeted as a potential research candidate for machine learning applications, with the purpose of detecting or diagnosing pathologies. However, acquiring relevant, high-quality, and heterogeneous medical datasets is challenging due to privacy and security issues and the effort required to annotate the data. Generative models have recently gained a growing interest in the computer vision field due to their ability to increase dataset size by generating new high-quality samples from the initial set, which can be used as data augmentation of a training dataset. This study aimed to synthesize artificial lung images from corresponding positional and semantic annotations using two generative adversarial networks and databases of real computed tomography scans: the Pix2Pix approach that generates lung images from the lung segmentation maps; and the conditional generative adversarial network (cCGAN) approach that was implemented with additional semantic labels in the generation process. To evaluate the quality of the generated images, two quantitative measures were used: the domain-specific Fréchet Inception Distance and Structural Similarity Index. Additionally, an expert assessment was performed to measure the capability to distinguish between real and generated images. The assessment performed shows the high quality of synthesized images, which was confirmed by the expert evaluation. This work represents an innovative application of GAN approaches for medical application taking into consideration the pathological findings in the CT images and the clinical evaluation to assess the realism of these features in the generated images. © 2022 Elsevier Ltd","Biological organs; Computerized tomography; Diagnosis; Generative adversarial networks; Image segmentation; Medical applications; Quality control; Semantics; CT Image; CT lung image; High quality; Images synthesis; Lung Cancer; Lung CT; Machine learning applications; Potential researches; Synthesised; Synthesized medical imaging; Medical imaging","CT Lung images; Generative adversarial networks; Lung cancer; Synthesized medical imaging","Article","Final","","Scopus","2-s2.0-85143869959"
"Zhang Z.; Zhou J.; Yu W.; Jiang N.","Zhang, Zhiqiang (57206280843); Zhou, Jinjia (35099640400); Yu, Wenxin (36610960300); Jiang, Ning (57212426361)","57206280843; 35099640400; 36610960300; 57212426361","Text-to-image synthesis: Starting composite from the foreground content","2022","Information Sciences","607","","","1265","1285","20","10.1016/j.ins.2022.06.044","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132690737&doi=10.1016%2fj.ins.2022.06.044&partnerID=40&md5=7fea85434fad2f418f90984e8069dc85","Recently, text-to-image synthesis has become a hot issue in computer vision and has been widely concerned. Many methods have achieved encouraging results in this field at present, but it is still a great challenge to improve the quality of the synthesized image further. In this paper, we propose a multi-stage synthesis method, which starts composite from the foreground content. The whole synthesis process is divided into three stages. The first stage generates the foreground results, and the third stage synthesizes the final image results. The second stage results include two situations: one is to continue to synthesize the foreground results; the other is to synthesize the image results with background information. Experiments demonstrate that the method of continuing to generate the foreground results in the second stage can achieve better results on the Caltech-UCSD Birds (CUB) and Oxford-102 datasets, while the way of synthesizing foreground results only in the first stage can obtain better performance on the Microsoft Common Objects in Context (MS COCO) dataset. Besides, our synthesized results on the three datasets are subjectively more realistic with better detail processing. It also outperforms most existing methods in quantitative comparison results, which demonstrates the effectiveness and superiority of our method. © 2022 Elsevier Inc.","Computer vision; Deep learning; Image enhancement; Background information; Caltech; Deep learning; Images synthesis; Multi-stages; Performance; Synthesis method; Synthesis process; Synthesized images; Text-to-image synthesis; Generative adversarial networks","Computer vision; Deep learning; Generative adversarial networks; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85132690737"
"Liang Z.; Huang J.X.; Antani S.","Liang, Zhaohui (57813721500); Huang, Jimmy Xiangji (57189975304); Antani, Sameer (6701355570)","57813721500; 57189975304; 6701355570","Image Translation by Ad CycleGAN for COVID-19 X-Ray Images: A New Approach for Controllable GAN","2022","Sensors","22","24","9628","","","","10.3390/s22249628","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144564055&doi=10.3390%2fs22249628&partnerID=40&md5=a8b65e64ed2573d2e123551288f493ea","We propose a new generative model named adaptive cycle-consistent generative adversarial network, or Ad CycleGAN to perform image translation between normal and COVID-19 positive chest X-ray images. An independent pre-trained criterion is added to the conventional Cycle GAN architecture to exert adaptive control on image translation. The performance of Ad CycleGAN is compared with the Cycle GAN without the external criterion. The quality of the synthetic images is evaluated by quantitative metrics including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQI), visual information fidelity (VIF), Frechet Inception Distance (FID), and translation accuracy. The experimental results indicate that the synthetic images generated either by the Cycle GAN or by the Ad CycleGAN have lower MSE and RMSE, and higher scores in PSNR, UIQI, and VIF in homogenous image translation (i.e., Y → Y) compared to the heterogenous image translation process (i.e., X → Y). The synthetic images by Ad CycleGAN through the heterogeneous image translation have significantly higher FID score compared to Cycle GAN (p < 0.01). The image translation accuracy of Ad CycleGAN is higher than that of Cycle GAN when normal images are converted to COVID-19 positive images (p < 0.01). Therefore, we conclude that the Ad CycleGAN with the independent criterion can improve the accuracy of GAN image translation. The new architecture has more control on image synthesis and can help address the common class imbalance issue in machine learning methods and artificial intelligence applications with medical images. © 2022 by the authors.","Artificial Intelligence; COVID-19; Humans; Image Processing, Computer-Assisted; Machine Learning; X-Rays; E-learning; Generative adversarial networks; Image enhancement; Image quality; Learning algorithms; Mean square error; Medical imaging; Network architecture; Quality control; Signal to noise ratio; Applied machine learning; Digital health in the midst of COVID-19; Image quality index; Image translation; Mean squared error; Peak signal to noise ratio; Root mean squared errors; Synthetic images; Visual information fidelity; X-ray image; artificial intelligence; diagnostic imaging; human; image processing; machine learning; procedures; X ray; COVID-19","applied machine learning; digital health in the midst of COVID-19; generative adversarial networks; X-ray images","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144564055"
"Xiao W.; Xu C.; Zhang H.; Xu X.","Xiao, Wenpeng (57220096209); Xu, Cheng (57198225753); Zhang, Huaidong (57188815508); Xu, Xuemiao (56517140100)","57220096209; 57198225753; 57188815508; 56517140100","Spatial-Aware GAN for Instance-Guided Cross-Spectral Face Hallucination","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13604 LNAI","","","93","105","12","10.1007/978-3-031-20497-5_8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145009114&doi=10.1007%2f978-3-031-20497-5_8&partnerID=40&md5=299a3806feae42b2b3fb9823828fdf9c","An efficient strategy to solve the Heterogeneous Face Recognition (HFR) is to translate the probes to the same spectrum domain of the galleries using generative models. However, without or with only globally-pooled appearance representation from a reference, the low-quality generated images restrict the recognition accuracy. The intuition of our paper is the spatially-distributed appearance contains details beneficial to higher-quality image synthesis. Particularly, we propose a semantic spatial adaptive alignment module to solve the inevitable misalignment between the content from the near-infrared (NIR) image and the appearance from the visible (VIS) reference. In this way, arbitrary VIS reference can provide appearance with sufficient details to assist the NIR-to-VIS translation. Based on this, we propose an unsupervised spatial-aware instance-guided cross-spectral facial hallucination network (SICFH) for visual-pleasing and identity-preserved VIS image translation. Qualitative and quantitative experiments on three challenging NIR-VIS datasets demonstrate the synthesized VIS images address the HFR problem effectively and achieve state-of-the-art recognition accuracy. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Generative adversarial networks; Infrared devices; Semantics; Cross-spectral face synthesis; Efficient strategy; Face hallucination; Face synthesis; Generative model; Heterogeneous face recognition; Low qualities; Recognition accuracy; Spectra's; Visible image; Face recognition","Cross-spectral face synthesis; Generative adversarial network; Heterogeneous face recognition","Conference paper","Final","","Scopus","2-s2.0-85145009114"
"Vo D.M.; Sugimoto A.","Vo, Duc Minh (57530870700); Sugimoto, Akihiro (23391348700)","57530870700; 23391348700","Paired-D++ GAN for image manipulation with text","2022","Machine Vision and Applications","33","3","45","","","","10.1007/s00138-022-01298-7","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127951559&doi=10.1007%2fs00138-022-01298-7&partnerID=40&md5=91d2052451f4db2f78adeb33bfc5465c","Image manipulation with text is to semantically modify the appearance of an object in a source image based on the given text describing the novel visual attributes while retaining other irrelevant information in the image, such as the background. This has a wide range of applications, such as intelligent image manipulation, and is helpful to those who are not good at painting. We propose a generative adversarial network having a pair of discriminators with different architectures, namely Paired-D++ GAN, for image manipulation with text where the two discriminators make different judgments: one for foreground synthesis and the other for background synthesis. The generator of Paired-D++ GAN has the encoder–decoder architecture with skip-connections and synthesizes an object’s appearance matching the given text description while preserving other parts of the source image. The two discriminators judge the foreground and background of the synthesized image separately to meet the given input text description and the given source image. The Paired-D++ GAN is trained using the effectively unconditional and conditional adversarial learning process in a simultaneous three-player minimax game. Our comprehensively experimental results on the Caltech-200 bird dataset and the Oxford-102 flower dataset show that Paired-D++ GAN can semantically synthesize images to match an input text description while retaining the background in a source image against the state-of-the-art methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Discriminators; Image processing; Network architecture; Appearance matching; Encoder-decoder architecture; Image manipulation; Image manipulation with text; Image-based; Images synthesis; Paired-discriminator; Source images; Visual attributes; Generative adversarial networks","Generative adversarial network; Image manipulation; Image manipulation with text; Image synthesis; Paired-discriminator","Article","Final","","Scopus","2-s2.0-85127951559"
"Li R.; Fontanini T.; Prati A.; Bhanu B.","Li, Runze (57217996164); Fontanini, Tomaso (57193277601); Prati, Andrea (7003595956); Bhanu, Bir (7005181563)","57217996164; 57193277601; 7003595956; 7005181563","Face Synthesis with a Focus on Facial Attributes Translation Using Attention Mechanisms","2023","IEEE Transactions on Biometrics, Behavior, and Identity Science","5","1","","76","90","14","10.1109/TBIOM.2022.3199707","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137592515&doi=10.1109%2fTBIOM.2022.3199707&partnerID=40&md5=1fe3cee86e8e1a1fb519ee1e347f8d42","Synthesis of face images by translating facial attributes is an important problem in computer vision and biometrics and has a wide range of applications in forensics, entertainment, etc. Recent advances in deep generative networks have made progress in synthesizing face images with certain target facial attributes. However, visualizing and interpreting generative adversarial networks (GANs) is a relatively unexplored area and generative models are still being employed as black-box tools. This paper takes the first step to visually interpret conditional GANs for facial attribute translation by using a gradient-based attention mechanism. Next, a key innovation is to include new learning objectives for knowledge distillation using attention in generative adversarial training, which result in improved synthesized face results, reduced visual confusions and boosted training for GANs in a positive way. Firstly, visual attentions are calculated to provide interpretations for GANs. Secondly, gradient-based visual attentions are used as knowledge to be distilled in a teacher-student paradigm for face synthesis with focus on facial attributes translation tasks in order to improve the performance of the model. Finally, it is shown how 'pseudo'-attentions knowledge distillation can be employed during the training of face synthesis networks when teacher and student networks are trained to generate different facial attributes. The approach is validated on facial attribute translation and human expression synthesis with both qualitative and quantitative results being presented.  © 2022 IEEE.","Behavioral research; Deep learning; Distillation; Generative adversarial networks; Job analysis; Personnel training; Deep learning; Explainable AI; Face; Face image synthesis; Face synthesis; Facial attribute translation; Facial feature; Task analysis; Visual Attention; Visual attention map; Face recognition","deep learning; explainable AI; face image synthesis; Facial attributes translation; generative adversarial network; visual attention maps","Article","Final","","Scopus","2-s2.0-85137592515"
"Chen A.; Liu R.; Xie L.; Chen Z.; Su H.; Yu J.","Chen, Anpei (57207762134); Liu, Ruiyang (57226188750); Xie, Ling (57210789544); Chen, Zhang (57193609314); Su, Hao (55208624800); Yu, Jingyi (8569656400)","57207762134; 57226188750; 57210789544; 57193609314; 55208624800; 8569656400","SofGAN: A Portrait Image Generator with Dynamic Styling","2022","ACM Transactions on Graphics","41","1","1","","","","10.1145/3470848","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124798419&doi=10.1145%2f3470848&partnerID=40&md5=c1e99475f8809fae4bd7f00d97180ee4","Recently, Generative Adversarial Networks (GANs) have been widely used for portrait image generation. However, in the latent space learned by GANs, different attributes, such as pose, shape, and texture style, are generally entangled, making the explicit control of specific attributes difficult. To address this issue, we propose a SofGAN image generator to decouple the latent space of portraits into two subspaces: a geometry space and a texture space. The latent codes sampled from the two subspaces are fed to two network branches separately, one to generate the 3D geometry of portraits with canonical pose, and the other to generate textures. The aligned 3D geometries also come with semantic part segmentation, encoded as a semantic occupancy field (SOF). The SOF allows the rendering of consistent 2D semantic segmentation maps at arbitrary views, which are then fused with the generated texturemaps and stylized to a portrait photo using our semantic instance-wise module. Through extensive experiments, we show that our system can generate high-quality portrait images with independently controllable geometry and texture attributes. The method also generalizes well in various applications, such as appearance-consistent facial animation and dynamic styling. © 2022 Association for Computing Machinery.","3D modeling; Geometry; Semantic Segmentation; Semantics; Textures; 3D geometry; 3D models; 3d-modeling; Image generations; Image generators; Images synthesis; Portrait image; Semantic parts; Shape and textures; Texture space; Generative adversarial networks","3D modeling; generative adversarial networks; Image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85124798419"
"Jindal R.; Sriram V.; Aggarwal V.; Jain V.","Jindal, Rajni (8698929800); Sriram, V. (57890018200); Aggarwal, Vishesh (57889307100); Jain, Vishesh (57889608300)","8698929800; 57890018200; 57889307100; 57889608300","Text to Image Generation Using Gan","2023","Lecture Notes in Networks and Systems","475","","","673","684","11","10.1007/978-981-19-2840-6_51","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137997210&doi=10.1007%2f978-981-19-2840-6_51&partnerID=40&md5=e4c8e7753bc14f217acbe2019f0f3f56","Text to image synthesis, one of the most fascinating applications of GANs, is one of the hottest topics in all of machine learning and artificial intelligence. This paper comprises techniques for training a GAN to synthesise human faces and images of flowers from text descriptions. In this paper, we are proposing to train the GAN progressively as proposed in the ProGAN architecture and along with that trying to improve its results by proposing a custom update rule for alpha which controls the fading rate during the progressive growth of the architecture. With experimental testing using the Oxford102 and LFW datasets, our proposed architecture and training process ensures fast learning and smooth transitions between each trained generation. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Discriminator; Equalised learning rate; Generative adversarial networks; Generator; Image generation; Mini-batch standard deviation; Nearest neighbour interpolation; Progressive GAN; stackGAN; Text to image generation; Wasserstein loss","Conference paper","Final","","Scopus","2-s2.0-85137997210"
"Li M.; Lin J.; Ding Y.; Liu Z.; Zhu J.-Y.; Han S.","Li, Muyang (57219670683); Lin, Ji (57200618213); Ding, Yaoyao (57209224323); Liu, Zhijian (57213431660); Zhu, Jun-Yan (56316642900); Han, Song (56697675800)","57219670683; 57200618213; 57209224323; 57213431660; 56316642900; 56697675800","GAN Compression: Efficient Architectures for Interactive Conditional GANs","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","12","","9331","9346","15","10.1109/TPAMI.2021.3126742","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141893017&doi=10.1109%2fTPAMI.2021.3126742&partnerID=40&md5=1ab8ac127ace48bf5193d5ef66971a72","Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more compute-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method finds efficient architectures via neural architecture search. To accelerate the search process, we decouple the model training and search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings, network architectures, and learning methods. Without losing image quality, we reduce the computation of CycleGAN by 21×, Pix2pix by 12×, MUNIT by 29×, and GauGAN by 9×, paving the way for interactive image synthesis.  © 1979-2012 IEEE.","Deep learning; Distillation; Image compression; Learning systems; Network architecture; Compression; Efficient architecture; GAN; GAN compression; Image translation; Image-to-image translation; Images synthesis; Neural architecture search; Neural architectures; Vision applications; article; compression; image quality; learning; synthesis; Generative adversarial networks","compression; distillation; GAN; GAN compression; image-to-image translation; neural architecture search","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141893017"
"Zhang C.; Lin Z.; Xu L.; Li Z.; Tang W.; Liu Y.; Meng G.; Wang L.; Li L.","Zhang, Chi (56566906100); Lin, Zihang (57221373125); Xu, Liheng (57221705094); Li, Zongliang (57219545859); Tang, Wei (57220589856); Liu, Yuehu (8891358100); Meng, Gaofeng (16317032400); Wang, Le (57189700760); Li, Li (56304455500)","56566906100; 57221373125; 57221705094; 57219545859; 57220589856; 8891358100; 16317032400; 57189700760; 56304455500","Density-Aware Haze Image Synthesis by Self-Supervised Content-Style Disentanglement","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","7","","4552","4572","20","10.1109/TCSVT.2021.3130158","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120553524&doi=10.1109%2fTCSVT.2021.3130158&partnerID=40&md5=cb9783d17b823bac94c47e6b1c77fbd3","The key procedure of haze image synthesis with adversarial training lies in the disentanglement of the feature involved only in haze synthesis, i.e., the style feature, from the feature representing the invariant semantic content, i.e., the content feature. Previous methods introduced a binary classifier to constrain the domain membership from being distinguished through the learned content feature during the training stage, thereby the style information is separated from the content feature. However, we find that these methods cannot achieve complete content-style disentanglement. The entanglement of the flawed style feature with content information inevitably leads to the inferior rendering of haze images. To address this issue, we propose a self-supervised style regression model with stochastic linear interpolation that can suppress the content information in the style feature. Ablative experiments demonstrate the disentangling completeness and its superiority in density-aware haze image synthesis. Moreover, the synthesized haze data are applied to test the generalization ability of vehicle detectors. Further study on the relation between haze density and detection performance shows that haze has an obvious impact on the generalization ability of vehicle detectors and that the degree of performance degradation is linearly correlated to the haze density, which in turn validates the effectiveness of the proposed method.  © 1991-2012 IEEE.","Classification (of information); Generative adversarial networks; Image processing; Regression analysis; Semantics; Stochastic models; Atmospheric modeling; Content information; Density aware; Features extraction; Generalization ability; Haze synthesis; Image translation; Images synthesis; Self-supervised disentanglement; Unsupervised image-to-image translation; Stochastic systems","Haze synthesis; self-supervised disentanglement; unsupervised image-to-image translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85120553524"
"Gao K.; Chang C.-C.; Horng J.-H.; Echizen I.","Gao, Kai (57218544722); Chang, Ching-Chun (56056418200); Horng, Ji-Hwei (7103278000); Echizen, Isao (6602366829)","57218544722; 56056418200; 7103278000; 6602366829","Steganographic secret sharing via AI-generated photorealistic images","2022","Eurasip Journal on Wireless Communications and Networking","2022","1","119","","","","10.1186/s13638-022-02190-8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143683205&doi=10.1186%2fs13638-022-02190-8&partnerID=40&md5=0c07e08c58c81abab74f4fb5244918aa","Steganographic secret sharing is an access control technique that transforms a secret message into multiple shares in a steganographic sense. Each share is in a human-readable format in order to dispel suspicion from a malicious party during transmission and storage. Such a human-readable format can also serve to facilitate data management. The secret can be reconstructed only when a sufficient number of authorized shareholders collaborate. In this study, we use neural networks to encode secret shares into photorealistic image shares. This approach is conceptually related to coverless image steganography in which the data are transformed directly into an image rather than concealed into a cover image. We further implement an authentication mechanism to verify the integrity of the image shares presented in the decoding phase. All coverless image steganography schemes can be used to achieve steganographic secret sharing, but our detection mechanism can further improve the robustness of these schemes. Experimental results confirm the robustness of the proposed scheme against various steganalysis and tampering attacks. © 2022, The Author(s).","Access control; Digital storage; Generative adversarial networks; Image enhancement; Information management; Control techniques; Cover-image; Coverless steganography; Human-readable; Image steganography; Images synthesis; Neural-networks; Photorealistic images; Secret messages; Secret-sharing; Steganography","Coverless steganography; Generative adversarial networks; Image synthesis; Secret sharing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143683205"
"Sun W.; Wu T.","Sun, Wei (57210921364); Wu, Tianfu (55476641200)","57210921364; 55476641200","Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","9","","5070","5087","17","10.1109/TPAMI.2021.3078577","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105852295&doi=10.1109%2fTPAMI.2021.3078577&partnerID=40&md5=2bbfe0d865fcdf314579df3f760e7404","With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable structured inputs. This paper focuses on a recently emerged task, layout-To-image, whose goal is to learn generative models for synthesizing photo-realistic images from a spatial layout (i.e., object bounding boxes configured in an image lattice) and its style codes (i.e., structural and appearance variations encoded by latent vectors). This paper first proposes an intuitive paradigm for the task, layout-To-mask-To-image, which learns to unfold object masks in a weakly-supervised way based on an input layout and object style codes. The layout-To-mask component deeply interacts with layers in the generator network to bridge the gap between an input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks (GANs) for the proposed layout-To-mask-To-image synthesis with layout and style control at both image and object levels. The controllability is realized by a proposed novel Instance-Sensitive and Layout-Aware Normalization (ISLA-Norm) scheme. A layout semi-supervised version of the proposed method is further developed without sacrificing performance. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-The-Art performance obtained.  © 1979-2012 IEEE.","Codes (symbols); Adversarial networks; Generative model; Image synthesis; Photorealistic images; Recent progress; Semi-supervised; State-of-the-art performance; Synthesized images; Image processing","deep generative learning; GAN; Image synthesis; ISLA-norm; layout-To-image; layout-To-mask-To-image","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105852295"
"Hung Y.-H.; Tan J.; Huang T.-M.; Hsu S.-C.; Chen Y.-L.; Hua K.-L.","Hung, Yu-Hsiang (57713222500); Tan, Julianne (57712606700); Huang, Tai-Ming (57397091000); Hsu, Shang-Che (57209888057); Chen, Yi-Ling (57208131176); Hua, Kai-Lung (55223901500)","57713222500; 57712606700; 57397091000; 57209888057; 57208131176; 55223901500","Unpaired Image-to-Image Translation Using Negative Learning for Noisy Patches","2022","IEEE Multimedia","29","4","","59","68","9","10.1109/MMUL.2022.3177452","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130850157&doi=10.1109%2fMMUL.2022.3177452&partnerID=40&md5=3cb4f0acbd1a170d4dc4fb26947382d7","Unpaired image-to-image translation finds a mapping between two domains that do not have paired data. One approach is patchwise contrastive learning, a one-sided translation that maximizes mutual information between corresponding input and output patches. Noncorresponding patches are treated as negatives. Previous approaches randomly select noncorresponding patches, resulting in semantically similar patches incorrectly labeled as negatives. Inspired by negative learning, we propose the novel patchwise negative learning loss to address this issue. We do not naively minimize mutual information between all noncorresponding ones, unlike prior methods. Instead, we choose one noncorresponding patch and maximize dissimilarity with the query patch. The selected noncorresponding patch reduces the chance of choosing false negatives that contain high mutual information. By further maximizing dissimilarity with that single negative, we discourage our model from fitting on noisy negative patches. We demonstrate the capabilities of our model against other prominent image translation methods on the selfie2anime, horse2zebra, and cat2dog datasets. © 1994-2012 IEEE.","Computer vision; Data visualization; Generative adversarial networks; Generator; Image translation; Image-to-image translation; Images segmentations; Images synthesis; Input and outputs; Mutual informations; Negative learning; Noise measurements; Two domains; Image segmentation","generative adversarial networks; image synthesis; image-to-image translation; negative learning","Article","Final","","Scopus","2-s2.0-85130850157"
"Gautam A.; Sit M.; Demir I.","Gautam, Akshat (57219636866); Sit, Muhammed (57199691574); Demir, Ibrahim (56425613600)","57219636866; 57199691574; 56425613600","Realistic River Image Synthesis Using Deep Generative Adversarial Networks","2022","Frontiers in Water","4","","784441","","","","10.3389/frwa.2022.784441","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126191448&doi=10.3389%2ffrwa.2022.784441&partnerID=40&md5=2704961556b73678e66702d5e167eb0e","In this paper, we demonstrated a practical application of realistic river image generation using deep learning. Specifically, we explored a generative adversarial network (GAN) model capable of generating high-resolution and realistic river images that can be used to support modeling and analysis in surface water estimation, river meandering, wetland loss, and other hydrological research studies. First, we have created an extensive repository of overhead river images to be used in training. Second, we incorporated the Progressive Growing GAN (PGGAN), a network architecture that iteratively trains smaller-resolution GANs to gradually build up to a very high resolution to generate high quality (i.e., 1,024 × 1,024) synthetic river imagery. With simpler GAN architectures, difficulties arose in terms of exponential increase of training time and vanishing/exploding gradient issues, which the PGGAN implementation seemed to significantly reduce. The results presented in this study show great promise in generating high-quality images and capturing the details of river structure and flow to support hydrological modeling and research. Copyright © 2022 Gautam, Sit and Demir.","","deep learning; generative adversarial networks; hydrological datasets; imagery synthesis; river image","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85126191448"
"Beji A.; Blaiech A.G.; Said M.; Abdallah A.B.; Bedoui M.H.","Beji, Ahmed (57719949300); Blaiech, Ahmed Ghazi (36959429300); Said, Mourad (56186636900); Abdallah, Asma Ben (25822202900); Bedoui, Mohamed Hédi (8561556100)","57719949300; 36959429300; 56186636900; 25822202900; 8561556100","An innovative medical image synthesis based on dual GAN deep neural networks for improved segmentation quality","2023","Applied Intelligence","53","3","","3381","3397","16","10.1007/s10489-022-03682-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131066859&doi=10.1007%2fs10489-022-03682-2&partnerID=40&md5=f8ed28ece56f09df41e40b7e1764eedb","Artificial intelligence networks, precisely deep learning, have emerged as a truly life-impacting potential in healthcare – particularly in the area of medical diagnosis – and have achieved ground-breaking results in data enhancement, augmentation and reconstruction. In this paper, we propose a novel pipeline of an architecture of Generative Adversarial Networks (GAN) for Improved Segmentation Quality to attain better detection, diagnosis and treatment of diseases. The proposed method consists in creating an extended medical image distribution that will not only augment the data but also contribute to the achievement of good results in semantic segmentation tasks for better computer-aided medical diagnosis. In fact, our method is based on a pipeline with two stages representing the Segmentation 2-GAN (Seg2GAN) architecture, for the synthetic data distribution generation to enhance the quality of input images, followed by the segmentation phase using the new data. To validate this approach, we demonstrate an efficient medical data synthesis for diverse segmentation structures: the blood vessels of the retinal and the coronary, and the knee cartilage. Quantitative evaluations are presented as accuracy, sensitivity, precision and a dice score for three datasets. The segmentation results show that using the new synthetic data improves the same model trained on original real images; e.g., the gain of dice score is around of 8%, 21% and 50% for the blood vessels of the retinal and the coronary and for the knee cartilage, respectively. These obtained results compete with the state-of-art on multiple performance metrics. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Blood; Blood vessels; Computer aided diagnosis; Deep neural networks; Image enhancement; Medical imaging; Network architecture; Ophthalmology; Pipelines; Semantic Segmentation; Semantics; Breakings; Data augmentation; Data reconstruction; Deep learning; Images synthesis; Intelligence network; Medical image synthesis; Segmentation; Segmentation quality; Synthetic data; Generative adversarial networks","Deep learning; Generative adversarial network; Medical image synthesis; Segmentation","Article","Final","","Scopus","2-s2.0-85131066859"
"Liu W.; Piao Z.; Tu Z.; Luo W.; Ma L.; Gao S.","Liu, Wen (57195936303); Piao, Zhixin (57215005362); Tu, Zhi (57204671090); Luo, Wenhan (49061319200); Ma, Lin (56377428300); Gao, Shenghua (35224747100)","57195936303; 57215005362; 57204671090; 49061319200; 56377428300; 35224747100","Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","9","","5114","5131","17","10.1109/TPAMI.2021.3078270","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105882963&doi=10.1109%2fTPAMI.2021.3078270&partnerID=40&md5=7361144ecd66fa9fc518528871a10b84","We tackle human image synthesis, including human motion imitation, appearance transfer, and novel view synthesis, within a unified framework. It means that the model, once being trained, can be used to handle all these tasks. The existing task-specific methods mainly use 2D keypoints (pose) to estimate the human body structure. However, they only express the position information with no ability to characterize the personalized shape of the person and model the limb rotations. In this paper, we propose to use a 3D body mesh recovery module to disentangle the pose and shape. It can not only model the joint location and rotation but also characterize the personalized body shape. To preserve the source information, such as texture, style, color, and face identity, we propose an Attentional Liquid Warping GAN with Attentional Liquid Warping Block (AttLWB) that propagates the source information in both image and feature spaces to the synthesized reference. Specifically, the source features are extracted by a denoising convolutional auto-encoder for characterizing the source identity well. Furthermore, our proposed method can support a more flexible warping from multiple sources. To further improve the generalization ability of the unseen source images, a one/few-shot adversarial learning is applied. In detail, it first trains a model in an extensive training set. Then, it finetunes the model by one/few-shot unseen image(s) in a self-supervised way to generate high-resolution (512 512512×512 and 1024 \times 10241024×1024) results. Also, we build a new dataset, namely Impersonator (iPER) dataset, for the evaluation of human motion imitation, appearance transfer, and novel view synthesis. Extensive experiments demonstrate the effectiveness of our methods in terms of preserving face identity, shape consistency, and clothes details. All codes and dataset are available on https://impersonator.org/work/impersonator-plus-plus.html.  © 1979-2012 IEEE.","Liquids; Textures; Adversarial learning; Generalization ability; Image synthesis; Motion imitations; Multiple source; Novel view synthesis; Recovery modules; Unified framework; Image enhancement","appearance transfer; generative adversarial network; Human image synthesis; motion imitation; novel view synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105882963"
"Harrie L.; Oucheikh R.; Nilsson Å.; Oxenstierna A.; Cederholm P.; Wei L.; Richter K.-F.; Olsson P.","Harrie, Lars (56396851400); Oucheikh, Rachid (56094932000); Nilsson, Åsa (57711556400); Oxenstierna, Andreas (57711044400); Cederholm, Pontus (57711297300); Wei, Lai (57709774200); Richter, Kai-Florian (35748207000); Olsson, Perola (57710788700)","56396851400; 56094932000; 57711556400; 57711044400; 57711297300; 57709774200; 35748207000; 57710788700","Label Placement Challenges in City Wayfinding Map Production—Identification and Possible Solutions","2022","Journal of Geovisualization and Spatial Analysis","6","1","16","","","","10.1007/s41651-022-00115-z","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130713237&doi=10.1007%2fs41651-022-00115-z&partnerID=40&md5=2e521de84fe9ba179ebd3368e9fc3804","Map label placement is an important task in map production, which needs to be automated since it is tedious and requires a significant amount of manual work. In this paper, we identify five cartographic labeling situations that present challenges by causing intensive manual work in map production of city wayfinding maps, e.g., label placement in high density areas, utilizing true label geometries in automated methods, and creating a good relationship between text labels and icons. We evaluate these challenges in an open source map labeling tool (QGIS), provide results from a preliminary study, and discuss if there are other techniques that could be applicable to solving these challenges. These techniques are based on quantified cartographic rules or on machine learning. We focus on deep learning for which we provide several examples of techniques from other application domains that might have a potential in map label placement. The aim of the paper is to explore those techniques and to recommend future practical studies for each of the identified five challenges in map production. We believe that targeting the revealed challenges using the proposed solutions will significantly raise the automation level for producing city wayfinding maps, thus, having a real, measurable impact on production time and costs. © 2022, The Author(s).","","Automated cartography; City wayfinding maps; Deep learning; Generative adversarial networks; Image synthesis; Map labeling; Map production challenges","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85130713237"
"Shokraei Fard A.; Reutens D.C.; Vegh V.","Shokraei Fard, Azin (57667105800); Reutens, David C. (7006822554); Vegh, Viktor (11439590200)","57667105800; 7006822554; 11439590200","From CNNs to GANs for cross-modality medical image estimation","2022","Computers in Biology and Medicine","146","","105556","","","","10.1016/j.compbiomed.2022.105556","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129385716&doi=10.1016%2fj.compbiomed.2022.105556&partnerID=40&md5=54ffd9376e9902793b3a814c822ebf9a","Cross-modality image estimation involves the generation of images of one medical imaging modality from that of another modality. Convolutional neural networks (CNNs) have been shown to be useful in image-to-image intensity projections, in addition to identifying, characterising and extracting image patterns. Generative adversarial networks (GANs) use CNNs as generators and estimated images are classified as true or false based on an additional discriminator network. CNNs and GANs within the image estimation framework may be considered more generally as deep learning approaches, since medical images tend to be large in size, leading to the need for large neural networks. Most research in the CNN/GAN image estimation literature has involved the use of MRI data with the other modality primarily being PET or CT. This review provides an overview of the use of CNNs and GANs for cross-modality medical image estimation. We outline recently proposed neural networks and detail the constructs employed for CNN and GAN image-to-image synthesis. Motivations behind cross-modality image estimation are outlined as well. GANs appear to provide better utility in cross-modality image estimation in comparison with CNNs, a finding drawn based on our analysis involving metrics comparing estimated and actual images. Our final remarks highlight key challenges faced by the cross-modality medical image estimation field, including how intensity projection can be constrained by registration (unpaired versus paired data), use of image patches, additional networks, and spatially sensitive loss functions. © 2022 Elsevier Ltd","Benchmarking; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Computerized tomography; Convolution; Convolutional neural networks; Deep neural networks; Discriminators; Magnetic resonance imaging; Medical imaging; Convolutional neural network; Cross modality; Cross-modality medical images; Deep learning; Image estimation; Image intensities; Image patterns; Imaging modality; Intensity projection; Neural-networks; convolutional neural network; deep learning; loss of function mutation; motivation; nuclear magnetic resonance imaging; review; synthesis; benchmarking; image processing; procedures; Generative adversarial networks","Convolutional neural network; Deep learning; Generative adversarial network; Image estimation; Intensity projection","Review","Final","","Scopus","2-s2.0-85129385716"
"Lin S.; Zhang Y.","Lin, ShaoYue (57739324900); Zhang, YanJun (57207473146)","57739324900; 57207473146","ACGAN: Attribute controllable person image synthesis GAN for pose transfer","2022","Journal of Visual Communication and Image Representation","87","","103572","","","","10.1016/j.jvcir.2022.103572","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133909484&doi=10.1016%2fj.jvcir.2022.103572&partnerID=40&md5=8c84e3b5933b84fb5be899a0788099f5","At present, pose transfer and attribute control tasks are still the challenges for image synthesis network. At the same time, there are often artifacts in the images generated by the image synthesis network when the above two tasks are completed. The existence of artifacts causes the loss of the generated image details or introduces some wrong image information, which leads to the decline of the overall performance of the existing work. In this paper, a generative adversarial network (GAN) named ACGAN is proposed to accomplish the above two tasks and effectively eliminate artifacts in generated images. The proposed network was compared quantitatively and qualitatively with previous works on the DeepFashion dataset and better results are obtained. Moreover, the overall network has advantages over the previous works in speed and number of parameters. © 2022 Elsevier Inc.","Image processing; Artifact; Control task; Image details; Image information; Images synthesis; Overall networks; Performance; Person image synthesis; Generative adversarial networks","Artifact; GAN; Person image synthesis","Article","Final","","Scopus","2-s2.0-85133909484"
"Choi J.; Kim D.H.; Lee S.; Lee S.H.; Song B.C.","Choi, Jaewoong (57226026571); Kim, Dae Ha (57200501592); Lee, Sanghyuk (57579762900); Lee, Sang Hyuk (57221266785); Song, Byung Cheol (36062099900)","57226026571; 57200501592; 57579762900; 57221266785; 36062099900","Synthesized rain images for deraining algorithms","2022","Neurocomputing","492","","","421","439","18","10.1016/j.neucom.2022.04.034","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128368756&doi=10.1016%2fj.neucom.2022.04.034&partnerID=40&md5=cee49c23566bc32d41c59a24f6cfd24b","Since most of the rainy scene datasets used for training single image rain removal (SIRR) algorithms are constructed by blending artificial rain streaks with source images, it is difficult for a machine trained with such datasets to understand the patterns of real or realistic rain streaks. So, several studies have been attempted to build a real rainy scene dataset. However, since collecting real rainy scenes itself requires significant costs, the real rainy scene datasets provided by some studies cover only very limited rainy environment(s). This paper presents a new approach to synthesize realistic rainy scenes using GAN, which is a world-first attempt as far as we know. The proposed method builds a representation space to which rain streaks of multiple styles are smoothly mapped by learning the distributions of various rain datasets. The representation space allows control over the generated rain streaks. Also, the proposed method can synthesize multiple rainy scenes per clean (source) scene simultaneously, thereby a synthesized rain image dataset (SyRa) (Dataset can be found here: https://github.com/jaewoong1/SyRa-Synthesized_Rain_dataset) consisting of 11 K clean images and 55 K rainy images was constructed. Finally, this paper provides benchmarking results of several SIRR methods trained with SyRa. This result will be very useful for developing SIRR algorithms that can cope well with the actual rain environment. © 2022 The Author(s)","Generative adversarial networks; Image processing; Generative model; Image translation; Image-to-image translation; Images synthesis; Rain dataset; Rain image synthesis; Rain removals; Rain streak; Single images; Synthesised; algorithm; article; benchmarking; learning; synthesis; Rain","Generative model; Image-to-image translation; Rain dataset; Rain image synthesis; Rain streak","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85128368756"
"Cao J.; Luo M.; Yu J.; Yang M.; He R.","Cao, Jie (57197831202); Luo, Mandi (57219522326); Yu, Junchi (57211745581); Yang, Ming-Hsuan (7404927015); He, Ran (35764463900)","57197831202; 57219522326; 57211745581; 7404927015; 35764463900","ScoreMix: A Scalable Augmentation Strategy for Training GANs With Limited Data","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","16","15","10.1109/TPAMI.2022.3231649","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146217932&doi=10.1109%2fTPAMI.2022.3231649&partnerID=40&md5=c1cc2fad0a48e41bc4b79fb51482d241","Generative Adversarial Networks (GANs) typically suffer from overfitting when limited training data is available. To facilitate GAN training, current methods propose to use data-specific augmentation techniques. Despite the effectiveness, it is difficult for these methods to scale to practical applications. In this work, we present ScoreMix, a novel and scalable data augmentation approach for various image synthesis tasks. We first produce augmented samples using the convex combinations of the real samples. Then, we optimize the augmented samples by minimizing the norms of the data scores, <italic>i.e.</italic>, the gradients of the log-density functions. This procedure enforces the augmented samples close to the data manifold. To estimate the scores, we train a deep estimation network with multi-scale score matching. For different image synthesis tasks, we train the score estimation network using different data. We do not require the tuning of the hyperparameters or modifications to the network architecture. The ScoreMix method effectively increases the diversity of data and reduces the overfitting problem. Moreover, it can be easily incorporated into existing GAN models with minor modifications. Experimental results on numerous tasks demonstrate that GAN models equipped with the ScoreMix method achieve significant improvements. IEEE","Image processing; Job analysis; Network architecture; Data augmentation; Few-shot image-to-image translation; Image translation; Images synthesis; Limited data; Network models; Optimisations; Task analysis; Training data; Generative adversarial networks","Data augmentation; Data models; few-shot image-to-image translation; generative adversarial networks; Generative adversarial networks; image synthesis; Image synthesis; Optimization; Task analysis; Training; Training data","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85146217932"
"Zhou W.; Jin X.; Zhu X.; Rong Y.; Cui S.","Zhou, Wu (58037283900); Jin, Xin (56984939900); Zhu, Xingfan (57222382752); Rong, Yiqing (58037284000); Cui, Shuai (57417033500)","58037283900; 56984939900; 57222382752; 58037284000; 57417033500","Part Based Face Stylization via Multiple Generative Adversarial Networks","2022","Communications in Computer and Information Science","1701 CCIS","","","267","276","9","10.1007/978-981-19-7943-9_23","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145257020&doi=10.1007%2f978-981-19-7943-9_23&partnerID=40&md5=c58bca6d7004e38328099381fcedfa90","In recent years, due to the improvement of scientific research methods and the wide-open source and acquisition of related data sets, face stylization has become a hot research field and application direction. There is a need to stylize face images in many applications, such as camera beauty, artistic photo processing, etc. However, most of the current schemes are not satisfactory, and the resultant image synthesis traces are obvious, and the effect is relatively monotonous. Based on the study of image features and style representation, this paper proposes a general-purpose face image style transfer whole process scheme. It can fill the gap in local style transfer of face images. Among the existing face stylization methods, the face stylization method is more complex, and the resulting obvious image synthesis trace along with the single effect. The project innovates the existing technology that can split the whole picture and implements the following six functions. Including the segmentation of specific portrait parts (hair), the skin buffing and whitening of the face, the defuzzification of the photos, the style transfer of the hair, the messy hair removal, and the implementation of the big eye effect. This study can realize the automatic style conversion of specific face images quickly and with high quality. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Buffing; Computer vision; Semantic Segmentation; Semantic Web; Semantics; Data set; Face images; Images synthesis; Open-source; Part based; Research fields; Research method; Scientific researches; Semantic segmentation; Style transfer; Generative adversarial networks","Computer Vision; generative adversarial network; Semantic segmentation; Style transfer","Conference paper","Final","","Scopus","2-s2.0-85145257020"
"Zhou Y.; Yang Z.; Zhang H.; Chang E.I.-C.; Fan Y.; Xu Y.","Zhou, Yang (57477146300); Yang, Zhiwen (57215896808); Zhang, Hui (56979612700); Chang, Eric I-Chao (7401837784); Fan, Yubo (55648008700); Xu, Yan (57192065052)","57477146300; 57215896808; 56979612700; 7401837784; 55648008700; 57192065052","3D Segmentation Guided Style-Based Generative Adversarial Networks for PET Synthesis","2022","IEEE Transactions on Medical Imaging","41","8","","2092","2104","12","10.1109/TMI.2022.3156614","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125743764&doi=10.1109%2fTMI.2022.3156614&partnerID=40&md5=b43f5078123770f69bdb1e1c5e050770","Potential radioactive hazards in full-dose positron emission tomography (PET) imaging remain a concern, whereas the quality of low-dose images is never desirable for clinical use. So it is of great interest to translate low-dose PET images into full-dose. Previous studies based on deep learning methods usually directly extract hierarchical features for reconstruction. We notice that the importance of each feature is different and they should be weighted dissimilarly so that tiny information can be captured by the neural network. Furthermore, the synthesis on some regions of interest is important in some applications. Here we propose a novel segmentation guided style-based generative adversarial network (SGSGAN) for PET synthesis. (1) We put forward a style-based generator employing style modulation, which specifically controls the hierarchical features in the translation process, to generate images with more realistic textures. (2) We adopt a task-driven strategy that couples a segmentation task with a generative adversarial network (GAN) framework to improve the translation performance. Extensive experiments show the superiority of our overall framework in PET synthesis, especially on those regions of interest. © 1982-2012 IEEE.","Image Processing, Computer-Assisted; Neural Networks, Computer; Positron-Emission Tomography; Deep learning; Generative adversarial networks; Image segmentation; Modulation; Positrons; Three dimensional displays; Generator; Hierarchical features; Images segmentations; Images synthesis; Low dose; Segmentation; Style modulation; Task analysis; Task-driven; Three-dimensional display; article; controlled study; image segmentation; positron emission tomography; synthesis; image processing; procedures; Positron emission tomography","GAN; PET; segmentation; style modulation; task-driven","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125743764"
"Tzaban R.; Mokady R.; Gal R.; Bermano A.; Cohen-Or D.","Tzaban, Rotem (57225171726); Mokady, Ron (57219685631); Gal, Rinon (57209318630); Bermano, Amit (53983538400); Cohen-Or, Daniel (7004252391)","57225171726; 57219685631; 57209318630; 53983538400; 7004252391","Stitch it in Time: GAN-Based Facial Editing of Real Videos","2022","Proceedings - SIGGRAPH Asia 2022 Conference Papers","","","29","","","","10.1145/3550469.3555382","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143978616&doi=10.1145%2f3550469.3555382&partnerID=40&md5=d2c5d1c385e440f5b83453f279d129fb","The ability of Generative Adversarial Networks to encode rich semantics within their latent space has been widely adopted for facial image editing. However, replicating their success with videos has proven challenging. Applying StyleGAN editing to real videos introduces two main challenges: (i) StyleGAN operates over aligned crops. When editing videos, these crops need to be pasted back into the frame, resulting in a spatial inconsistency. (ii) Videos introduce a fundamental barrier to overcome - temporal coherency. To address the first challenge, we propose a novel stitching-tuning procedure. The generator is carefully tuned to overcome the spatial artifacts at crop borders, resulting in smooth transitions even when difficult backgrounds are involved. Turning to temporal coherence, we propose that this challenge is largely artificial. The source video is already temporally coherent, and deviations arise in part due to careless treatment of individual components in the editing pipeline. We leverage the natural alignment of StyleGAN and the tendency of neural networks to learn low-frequency functions, and demonstrate that they provide a strongly consistent prior. These components are combined in an end-to-end framework for semantic editing of facial videos. We compare our pipeline to the current state-of-the-art and demonstrate significant improvements. Our method produces meaningful manipulations and maintains greater spatial and temporal consistency, even on challenging talking head videos which current methods struggle with. Our code and videos are available at https://stitch-time.github.io/. © 2022 ACM.","Generative adversarial networks; Pipelines; Semantics; 'current; Facial images; Fundamental barriers; Image editing; Images synthesis; Individual components; Neural-networks; Smooth transitions; Temporal coherence; Temporal coherency; Crops","Image Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143978616"
"Yurtsever E.; Yang D.; Koc I.M.; Redmill K.A.","Yurtsever, Ekim (57014903900); Yang, Dongfang (57194340476); Koc, Ibrahim Mert (57226326493); Redmill, Keith A. (6701619375)","57014903900; 57194340476; 57226326493; 6701619375","Photorealism in Driving Simulations: Blending Generative Adversarial Image Synthesis with Rendering","2022","IEEE Transactions on Intelligent Transportation Systems","23","12","","23114","23123","9","10.1109/TITS.2022.3193347","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135736745&doi=10.1109%2fTITS.2022.3193347&partnerID=40&md5=219a361694d92d60f5d3d0a74355324e","Driving simulators play a large role in developing and testing new intelligent vehicle systems. The visual fidelity of the simulation is critical for building vision-based algorithms and conducting human driver experiments. Low visual fidelity breaks immersion for human-in-the-loop driving experiments. Conventional computer graphics pipelines use detailed 3D models, meshes, textures, and rendering engines to generate 2D images from 3D scenes. These processes are labor-intensive, and they do not generate photorealistic imagery. Here we introduce a hybrid generative neural graphics pipeline for improving the visual fidelity of driving simulations. Given a 3D scene, we partially-render only important objects of interest, such as vehicles, and use generative adversarial processes to synthesize the background and the rest of the image. To this end, we propose a novel image formation strategy to form 2D semantic images from 3D scenery consisting of simple object models without textures. These semantic images are then converted into photorealistic RGB images with a state-of-the-art Generative Adversarial Network (GAN) trained on real-world driving scenes. This replaces repetitiveness with randomly generated but photorealistic surfaces. Finally, the partially-rendered and GAN synthesized images are blended with a blending GAN. We show that the photorealism of images generated with the proposed method is more similar to real-world driving datasets such as Cityscapes and KITTI than conventional approaches. This comparison is made using semantic retention analysis and Frechet Inception Distance (FID) measurements.  © 2000-2011 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Image texture; Pipelines; Rendering (computer graphics); Three dimensional computer graphics; Three dimensional displays; Deep learning; Driving simulation; Images synthesis; Photo-realistic; Photorealism; Rendering (computer graphic); Solid modelling; Three-dimensional display; Visual fidelity; Semantics","deep learning; Driving simulation; generative adversarial networks; image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135736745"
"Wang Y.; Wu W.; Yang Y.; Hu H.; Yu S.; Dong X.; Chen F.; Liu Q.","Wang, Yulin (57767485600); Wu, Wenyuan (57752875300); Yang, Yuxin (57701396000); Hu, Haifeng (57731488400); Yu, Shangqian (57701123200); Dong, Xiangjiang (57696491500); Chen, Feng (57190131900); Liu, Qian (57207736524)","57767485600; 57752875300; 57701396000; 57731488400; 57701123200; 57696491500; 57190131900; 57207736524","Deep learning-based 3D MRI contrast-enhanced synthesis from a 2D noncontrast T2Flair sequence","2022","Medical Physics","49","7","","4478","4493","15","10.1002/mp.15636","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130435853&doi=10.1002%2fmp.15636&partnerID=40&md5=19d26a0ac3a1b92a8f368054054d1a79","Purpose: Gadolinium-based contrast agents (GBCAs) have been successfully applied in magnetic resonance (MR) imaging to facilitate better lesion visualization. However, gadolinium deposition in the human brain raised widespread concerns recently. On the other hand, although high-resolution three-dimensional (3D) MR images are more desired for most existing medical image processing algorithms, their long scan duration and high acquiring costs make 2D MR images still much more common clinically. Therefore, developing alternative solutions for 3D contrast-enhanced MR image synthesis to replace GBCAs injection becomes an urgent requirement. Methods: This study proposed a deep learning framework that produces 3D isotropic full-contrast T2Flair images from 2D anisotropic noncontrast T2Flair image stacks. The super-resolution (SR) and contrast-enhanced (CE) synthesis tasks are completed in sequence by using an identical generative adversarial network (GAN) with the same techniques. To solve the problem that intramodality datasets from different scanners have specific combinations of orientations, contrasts, and resolutions, we conducted a region-based data augmentation technique on the fly during training to simulate various imaging protocols in the clinic. We further improved our network by introducing atrous spatial pyramid pooling, enhanced residual blocks, and deep supervision for better quantitative and qualitative results. Results: Our proposed method achieved superior CE-synthesized performance in quantitative metrics and perceptual evaluation. In detail, the PSNR, structural-similarity-index, and AUC are 32.25 dB, 0.932, and 0.991 in the whole brain and 24.93 dB, 0.851, and 0.929 in tumor regions. The radiologists’ evaluations confirmed that our proposed method has high confidence in the diagnosis. Analysis of the generalization ability showed that benefiting from the proposed data augmentation technique, our network can be applied to “unseen” datasets with slight drops in quantitative and qualitative results. Conclusion: Our work demonstrates the clinical potential of synthesizing diagnostic 3D isotropic CE brain MR images from a single 2D anisotropic noncontrast sequence. © 2022 American Association of Physicists in Medicine.","Contrast Media; Deep Learning; Gadolinium; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Deep learning; Diagnosis; Gadolinium; Image enhancement; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Optical resolving power; gadobutrol; gadolinium pentetate meglumine; contrast medium; gadolinium; Augmentation techniques; Contrast agent; Contrast synthesis; Contrast-enhanced; Data augmentation; Gadolinia; Gadolinia-based contrast agent; Isotropics; MRI contrasts; Superresolution; algorithm; Article; Bayesian learning; brain tumor; controlled study; convolutional neural network; deep learning; diagnostic accuracy; diagnostic test accuracy study; dynamic contrast-enhanced magnetic resonance imaging; fluid-attenuated inversion recovery imaging; human; image processing; intermethod comparison; major clinical study; neuroimaging; qualitative diagnosis; radiologist; residual neural network; signal noise ratio; simulation; T2 weighted imaging; three-dimensional imaging; workflow; nuclear magnetic resonance imaging; procedures; three-dimensional imaging; Generative adversarial networks","contrast synthesis; GBCAs; generative adversarial network; MR imaging; super-resolution","Article","Final","","Scopus","2-s2.0-85130435853"
"Medin S.C.; Egger B.; Cherian A.; Wang Y.; Tenenbaum J.B.; Liu X.; Marks T.K.","Medin, Safa C. (57289902400); Egger, Bernhard (57210675221); Cherian, Anoop (35104619100); Wang, Ye (57767919300); Tenenbaum, Joshua B. (7006818404); Liu, Xiaoming (35793096800); Marks, Tim K. (24780830500)","57289902400; 57210675221; 35104619100; 57767919300; 7006818404; 35793096800; 24780830500","MOST-GAN: 3D Morphable StyleGAN for Disentangled Face Image Manipulation","2022","Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022","36","","","1962","1971","9","","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139065487&partnerID=40&md5=79a3b154b83b5787f762d6c8f69cd83c","Recent advances in generative adversarial networks (GANs) have led to remarkable achievements in face image synthesis. While methods that use style-based GANs can generate strikingly photorealistic face images, it is often difficult to control the characteristics of the generated faces in a meaningful and disentangled way. Prior approaches aim to achieve such semantic control and disentanglement within the latent space of a previously trained GAN. In contrast, we propose a framework that a priori models physical attributes of the face such as 3D shape, albedo, pose, and lighting explicitly, thus providing disentanglement by design. Our method, MOST-GAN, integrates the expressive power and photorealism of style-based GANs with the physical disentanglement and flexibility of nonlinear 3D morphable models, which we couple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves photorealistic manipulation of portrait images with fully disentangled 3D control over their physical attributes, enabling extreme manipulation of lighting, facial expression, and pose variations up to full profile view. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Deep learning; Lighting; Semantics; 3-D shape; 3D Morphable model; Expressive power; Face image synthesis; Face images; Image manipulation; Photo-realistic; Photorealism; Priori model; State of the art; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85139065487"
"Wang S.; Zhang X.; Hui H.; Li F.; Wu Z.","Wang, Suzhe (57871962400); Zhang, Xueying (57878168100); Hui, Haisheng (57212565070); Li, Fenglian (55494500300); Wu, Zelin (57243644100)","57871962400; 57878168100; 57212565070; 55494500300; 57243644100","Multimodal CT Image Synthesis Using Unsupervised Deep Generative Adversarial Networks for Stroke Lesion Segmentation","2022","Electronics (Switzerland)","11","16","2612","","","","10.3390/electronics11162612","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137393794&doi=10.3390%2felectronics11162612&partnerID=40&md5=15154f75d6a5a01569cfb1a1fd66bb6b","Deep learning-based techniques can obtain high precision for multimodal stroke segmentation tasks. However, the performance often requires a large number of training examples. Additionally, existing data extension approaches for the segmentation are less efficient in creating much more realistic images. To overcome these limitations, an unsupervised adversarial data augmentation mechanism (UTC-GAN) is developed to synthesize multimodal computed tomography (CT) brain scans. In our approach, the CT samples generation and cross-modality translation differentiation are accomplished simultaneously by integrating a Siamesed auto-encoder architecture into the generative adversarial network. In addition, a Gaussian mixture translation module is further proposed, which incorporates a translation loss to learn an intrinsic mapping between the latent space and the multimodal translation function. Finally, qualitative and quantitative experiments show that UTC-GAN significantly improves the generation ability. The stroke dataset enriched by the proposed model also provides a superior improvement in segmentation accuracy, compared with the performance of current competing unsupervised models. © 2022 by the authors.","","generative adversarial network; stroke lesion segmentation; unsupervised data augmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137393794"
"Zhang B.; Gu S.; Zhang B.; Bao J.; Chen D.; Wen F.; Wang Y.; Guo B.","Zhang, Bowen (57386144600); Gu, Shuyang (57220756995); Zhang, Bo (57215034874); Bao, Jianmin (57200622052); Chen, Dong (57937101800); Wen, Fang (36549728200); Wang, Yong (57225158668); Guo, Baining (56513051700)","57386144600; 57220756995; 57215034874; 57200622052; 57937101800; 36549728200; 57225158668; 56513051700","StyleSwin: Transformer-based GAN for High-resolution Image Generation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11294","11304","10","10.1109/CVPR52688.2022.01102","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143087705&doi=10.1109%2fCVPR52688.2022.01102&partnerID=40&md5=5792808b00601d788f11a47f9b208c07","Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin. © 2022 IEEE.","Computational efficiency; Computer vision; Deep learning; Network architecture; Deep learning architecture and technique; High resolution; High-resolution images; Image and video synthesis and generation; Image generations; Images synthesis; Learning architectures; Learning techniques; Video generation; Video synthesis; Generative adversarial networks","Deep learning architectures and techniques; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143087705"
"Iqbal A.; Sharif M.; Yasmin M.; Raza M.; Aftab S.","Iqbal, Ahmed (57208081618); Sharif, Muhammad (57549289700); Yasmin, Mussarat (55243255400); Raza, Mudassar (56303305000); Aftab, Shabib (57202829701)","57208081618; 57549289700; 55243255400; 56303305000; 57202829701","Generative adversarial networks and its applications in the biomedical image segmentation: a comprehensive survey","2022","International Journal of Multimedia Information Retrieval","11","3","","333","368","35","10.1007/s13735-022-00240-x","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133585014&doi=10.1007%2fs13735-022-00240-x&partnerID=40&md5=fef8f7946d0256e969839ab002be497b","Recent advancements with deep generative models have proven significant potential in the task of image synthesis, detection, segmentation, and classification. Segmenting the medical images is considered a primary challenge in the biomedical imaging field. There have been various GANs-based models proposed in the literature to resolve medical segmentation challenges. Our research outcome has identified 151 papers; after the twofold screening, 138 papers are selected for the final survey. A comprehensive survey is conducted on GANs network application to medical image segmentation, primarily focused on various GANs-based models, performance metrics, loss function, datasets, augmentation methods, paper implementation, and source codes. Secondly, this paper provides a detailed overview of GANs network application in different human diseases segmentation. We conclude our research with critical discussion, limitations of GANs, and suggestions for future directions. We hope this survey is beneficial and increases awareness of GANs network implementations for biomedical image segmentation tasks. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","","GANs applications; GANs in medical image segmentation; Generative adversarial network","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85133585014"
"Yu X.; Chen C.; Gong Q.; Lu L.","Yu, Xiao (57207687488); Chen, Cong (57226892227); Gong, Qi (57786841400); Lu, Lina (57788339300)","57207687488; 57226892227; 57786841400; 57788339300","Application of Data Enhancement Method Based on Generative Adversarial Networks for Soybean Leaf Disease Identification","2022","American Journal of Biochemistry and Biotechnology","18","4","","417","427","10","10.3844/ajbbsp.2022.417.427","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142709470&doi=10.3844%2fajbbsp.2022.417.427&partnerID=40&md5=eea7e67714b00060f17953ebe223d5b6","Soybean leaf disease data collection is an expensive and time-consuming task. Convolutional neural network training requires a large amount of data, but traditional data enhancement methods (such as rotation, flipping, translation) are restricted by fixed rules and cannot generate images with diversity and variability. Aiming at the problem of the lack of soybean leaf disease data set, this study proposes a data enhancement method based on Generative Adversarial Networks (GANs) for soybean leaf disease identification. The method is based on a cyclic adversarial network and its discriminator uses dense connections. Strategies to reduce the size and computational complexity of the final model. Using a cyclic adversarial network to convert between healthy and diseased leaves, unsupervised learning can be performed, using limited images to learn disease characteristics, there by generating highly recognizable soybean leaf images. Synthesis images generated from GANs and original images are fed together as the model training set input and the recognition model for recognizing 9 types of soybean leaf images is obtained. An accuracy rate of 95.89% can be achieved on the verification set. Experimental results show that the generative adversarial network provided in this article can: Generate soybean leaf disease image data with high discriminative features, increase the size of the data set and provide a feasible solution for soybean leaf disease image data enhancement; as A regularization strategy to reduce over-fitting problems and improve the performance of the recognition model. © 2022 Xiao Yu, Cong Chen, Qi Gong and Lina Lu.","Convolution; Convolutional neural networks; Deep learning; Image enhancement; Learning systems; Adversarial networks; Agricultural pest and disease; Agricultural pests; Convolutional neural network; Data enhancement; Data set; Deep learning; Generating adversarial network; Leaf disease; Leaf images; agricultural pest; Article; artificial intelligence; artificial neural network; classification algorithm; convolutional neural network; deep learning; image enhancement; learning algorithm; machine learning; nonhuman; plant leaf; pustule; soybean; Generative adversarial networks","Agricultural Pests and Diseases; Convolutional Neural Networks; Deep Learning; Generating Adversarial Networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142709470"
"Jin W.; Ryu N.; Kim G.; Baek S.-H.; Cho S.","Jin, Wonjoon (58010421000); Ryu, Nuri (58010048600); Kim, Geonung (57477393400); Baek, Seung-Hwan (57820361200); Cho, Sunghyun (55487932900)","58010421000; 58010048600; 57477393400; 57820361200; 55487932900","Dr.3D: Adapting 3D GANs to Artistic Drawings","2022","Proceedings - SIGGRAPH Asia 2022 Conference Papers","","","9","","","","10.1145/3550469.3555422","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143983627&doi=10.1145%2f3550469.3555422&partnerID=40&md5=a30c674c5aece05ea82c8e7c5dcfed6e","While 3D GANs have recently demonstrated the high-quality synthesis of multi-view consistent images and 3D shapes, they are mainly restricted to photo-realistic human portraits. This paper aims to extend 3D GANs to a different, but meaningful visual form: artistic portrait drawings. However, extending existing 3D GANs to drawings is challenging due to the inevitable geometric ambiguity present in drawings. To tackle this, we present Dr.3D, a novel adaptation approach that adapts an existing 3D GAN to artistic drawings. Dr.3D is equipped with three novel components to handle the geometric ambiguity: a deformation-aware 3D synthesis network, an alternating adaptation of pose estimation and image synthesis, and geometric priors. Experiments show that our approach can successfully adapt 3D GANs to drawings and enable multi-view consistent semantic editing of drawings. © 2022 ACM.","Generative adversarial networks; Geometry; Image processing; Interactive computer graphics; 3-D shape; 3d-aware image synthesis; Artistic drawings; Domain adaptation; High quality; Images synthesis; Multi-views; Novel component; Photo-realistic; Visual forms; Semantics","3D-aware image synthesis; artistic drawings; domain adaptation; Generative adversarial networks","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143983627"
"Dervishaj E.; Cremonesi P.","Dervishaj, Ervin (57204691671); Cremonesi, Paolo (35268058400)","57204691671; 35268058400","GAN-based matrix factorization for recommender systems","2022","Proceedings of the ACM Symposium on Applied Computing","","","","1373","1381","8","10.1145/3477314.3507099","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130407501&doi=10.1145%2f3477314.3507099&partnerID=40&md5=cdd49d25806b32f0229144ececb71c85","Proposed in 2014, Generative Adversarial Networks (GAN) initiated a fresh interest in generative modelling. They immediately achieved state-of-the-art in image synthesis, image-to-image translation, text-to-image generation, image inpainting and have been used in sciences ranging from medicine to high-energy particle physics. Despite their popularity and ability to learn arbitrary distributions, GAN have not been widely applied in recommender systems (RS). Moreover, only few of the techniques that have introduced GAN in RS have employed them directly as a collaborative filtering (CF) model. In this work we propose a new GAN-based approach that learns user and item latent factors in a matrix factorization setting for the generic top-N recommendation problem. Following the vector-wise GAN training approach for RS introduced by CFGAN, we identify 2 unique issues when utilizing GAN for CF. We propose solutions for both of them by using an autoencoder as discriminator and incorporating an additional loss function for the generator. We evaluate our model, GANMF, through well-known datasets in the RS community and show improvements over traditional CF approaches and GAN-based models. Through an ablation study on the components of GANMF we aim to understand the effects of our architectural choices. Finally, we provide a qualitative evaluation of the matrix factorization performance of GANMF. © 2022 ACM.","Collaborative filtering; Generative adversarial networks; High energy physics; Matrix algebra; Recommender systems; Auto encoders; Features matching; Generative model; Image generations; Image translation; Images synthesis; Learn+; Matrix factorizations; Network-based; State of the art; Matrix factorization","autoencoder; collaborative filtering; feature matching; generative adversarial networks; matrix factorization","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85130407501"
"Atkale D.V.; Pawar M.M.; Deshpande S.C.; Yadav D.M.","Atkale, Dipali Vasant (57226020290); Pawar, Meenakshi M. (55605652600); Deshpande, Shabdali C. (57226018636); Yadav, Dhanashree M. (57225991038)","57226020290; 55605652600; 57226018636; 57225991038","Multi-scale feature fusion model followed by residual network for generation of face aging and de-aging","2022","Signal, Image and Video Processing","16","3","","753","761","8","10.1007/s11760-021-02015-z","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115094357&doi=10.1007%2fs11760-021-02015-z&partnerID=40&md5=50ac0b246a3abde03cd2210fca5ed611","Face aging is one of the most interesting style transfer ideas due to the extraordinary development in image synthesis succeeded by deep learning models that is the generative adversarial networks and its marvelous impact on practical applications such as finding missing child after few years, smart voting where we have to update the data based on the age changes of people. The existing face aging methods have proven the achievement in the case of the paired image dataset. Collecting the paired data samples of different age groups is hard and expensive. Encouraged by GAN's success in a variety of fields for image-to-image conversion problems. The main aim of this paper is to keep the original identity as it is in the face aging problem. We have designed an approach known as the multi-scale feature fusion model followed by a residual network to generate images of a person based on different age conditions. We worked on an unpaired image dataset because we do not have the dataset of the same person in different age categories. In this paper, we have used the UTKFace dataset which is publicly available. The scope of research is to consider only two age categories. The results are obtained by performing experiments and through a survey of people which indicates the modern method for face age progression and regression. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Electrical engineering; Signal processing; Adversarial networks; Age progression; Image conversion; Image datasets; Image synthesis; Learning models; Missing children; Multi-scale features; Deep learning","Deep learning; Face aging; GAN; Multi-scale feature fusion model; Style transfer","Article","Final","","Scopus","2-s2.0-85115094357"
"Bermano A.H.; Gal R.; Alaluf Y.; Mokady R.; Nitzan Y.; Tov O.; Patashnik O.; Cohen-Or D.","Bermano, A.H. (53983538400); Gal, R. (57209318630); Alaluf, Y. (57219797300); Mokady, R. (57219685631); Nitzan, Y. (57219740103); Tov, O. (57222260700); Patashnik, O. (57219796836); Cohen-Or, D. (7004252391)","53983538400; 57209318630; 57219797300; 57219685631; 57219740103; 57222260700; 57219796836; 7004252391","State-of-the-Art in the Architecture, Methods and Applications of StyleGAN","2022","Computer Graphics Forum","41","2","","591","611","20","10.1111/cgf.14503","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130446511&doi=10.1111%2fcgf.14503&partnerID=40&md5=603f58d4dfd22c207b81daee9a1fa1cb","Generative Adversarial Networks (GANs) have established themselves as a prevalent approach to image synthesis. Of these, StyleGAN offers a fascinating case study, owing to its remarkable visual quality and an ability to support a large array of downstream tasks. This state-of-the-art report covers the StyleGAN architecture, and the ways it has been employed since its conception, while also analyzing its severe limitations. It aims to be of use for both newcomers, who wish to get a grasp of the field, and for more experienced readers that might benefit from seeing current research trends and existing tools laid out. Among StyleGAN's most interesting aspects is its learned latent space. Despite being learned with no supervision, it is surprisingly well-behaved and remarkably disentangled. Combined with StyleGAN's visual quality, these properties gave rise to unparalleled editing capabilities. However, the control offered by StyleGAN is inherently limited to the generator's learned distribution, and can only be applied to images generated by StyleGAN itself. Seeking to bring StyleGAN's latent control to real-world scenarios, the study of GAN inversion and latent space embedding has quickly gained in popularity. Meanwhile, this same study has helped shed light on the inner workings and limitations of StyleGAN. We map out StyleGAN's impressive story through these investigations, and discuss the details that have made StyleGAN the go-to generator. We further elaborate on the visual priors StyleGAN constructs, and discuss their use in downstream discriminative tasks. Looking forward, we point out StyleGAN's limitations and speculate on current trends and promising directions for future research, such as task and target specific fine-tuning. © 2022 The Author(s) Computer Graphics Forum © 2022 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.","Computer graphics; Network architecture; Case-studies; CCS concept; Computing methodologies; Down-stream; Image manipulation; Images synthesis; Neural-networks; State of the art; Visual qualities; • computing methodology → learning latent representation; Generative adversarial networks","CCS Concepts; Computer graphics; Image manipulation; Neural networks; • Computing methodologies → Learning latent representations","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85130446511"
"Scoon K.; Samara K.","Scoon, Kalvin (57894643200); Samara, Kamil (57188977562)","57894643200; 57188977562","Synthesizing Pokémon Trading Cards Using Nvidia StyleGAN2 from Home","2023","Lecture Notes in Networks and Systems","543 LNNS","","","646","654","8","10.1007/978-3-031-16078-3_44","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138305728&doi=10.1007%2f978-3-031-16078-3_44&partnerID=40&md5=038814d9b122cee85e6a9cc90121a779","Artificial intelligence plays a large role in the way that we consume content acting as a director behind the scenes controlling what and how we interact with digital mediums daily. Only within the last decade has AI software and hardware advanced to a point where it can also help contribute directly to content creation with the use of technologies such as generative adversarial networks (GANs). In this paper, we will examine some modern GAN solutions as well as the viability of an at-home model training experience in 2021 and into 2022. Specifically, the task being examined is the image synthesis of Pokémon Trading Cards. This dataset was chosen for its familiarity, large number of uniform samples, accessibility, and to examine the effects of training on datasets that share many consistent elements (card layout, formatting, color schemes). Some implications of being able to generate images in such a way are faster prototyping, a more streamlined workflow, or near-instantaneous content creation for creative professionals and illustrators. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","","DALL-E; GAN; Google Colab; Google vertex; GPU; Pokémon; Python; SR3; StackGAN; StyleGAN; TPU; Trading card","Conference paper","Final","","Scopus","2-s2.0-85138305728"
"Pang H.; Qi S.; Wu Y.; Wang M.; Li C.; Sun Y.; Qian W.; Tang G.; Xu J.; Liang Z.; Chen R.","Pang, Haowen (57781616300); Qi, Shouliang (36572483500); Wu, Yanan (57394724800); Wang, Meihuan (58037431500); Li, Chen (56344405800); Sun, Yu (57202280575); Qian, Wei (36842193500); Tang, Guoyan (57203456708); Xu, Jiaxuan (57861994500); Liang, Zhenyu (57336787000); Chen, Rongchang (57200034537)","57781616300; 36572483500; 57394724800; 58037431500; 56344405800; 57202280575; 36842193500; 57203456708; 57861994500; 57336787000; 57200034537","NCCT-CECT image synthesizers and their application to pulmonary vessel segmentation","2023","Computer Methods and Programs in Biomedicine","231","","107389","","","","10.1016/j.cmpb.2023.107389","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147541607&doi=10.1016%2fj.cmpb.2023.107389&partnerID=40&md5=e5b8a81b66ca7a768bee95a16ec185b4","Background and objectives: Non-contrast CT (NCCT) and contrast-enhanced CT (CECT) are important diagnostic tools with distinct features and applications for chest diseases. We developed two synthesizers for the mutual synthesis of NCCT and CECT and evaluated their applications. Methods: Two synthesizers (S1 and S2) were proposed based on a generative adversarial network. S1 generated synthetic CECT (SynCECT) from NCCT and S2 generated synthetic NCCT (SynNCCT) from CECT. A new training procedure for synthesizers was proposed. Initially, the synthesizers were pretrained using self-supervised learning (SSL) and dual-energy CT (DECT) and then fine-tuned using the registered NCCT and CECT images. Pulmonary vessel segmentation from NCCT was used as an example to demonstrate the effectiveness of the synthesizers. Two strategies (ST1 and ST2) were proposed for pulmonary vessel segmentation. In ST1, CECT images were used to train a segmentation model (Model-CECT), NCCT images were converted to SynCECT through S1, and SynCECT was input to Model-CECT for testing. In ST2, CECT data were converted to SynNCCT through S2. SynNCCT and CECT-based annotations were used to train an additional model (Model-NCCT), and NCCT was input to Model-NCCT for testing. Three datasets, D1 (40 paired CTs), D2 (14 NCCTs and 14 CECTs), and D3 (49 paired DECTs), were used to evaluate the synthesizers and strategies. Results: For S1, the mean absolute error (MAE), mean squared error (MSE), peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM) were 14.60± 2.19, 1644± 890, 34.34± 1.91, and 0.94± 0.02, respectively. For S2, they were 12.52± 2.59, 1460± 922, 35.08± 2.35, and 0.95± 0.02, respectively. Our synthesizers outperformed the counterparts of CycleGAN, Pix2Pix, and Pix2PixHD. The results of ablation studies on SSL pretraining, DECT pretraining, and fine-tuning showed that performance worsened (for example, for S1, MAE increased to 16.53± 3.10, 17.98± 3.10, and 20.57± 3.75, respectively). Model-NCCT and Model-CECT achieved dice similarity coefficients (DSC) of 0.77 and 0.86 on D1 and 0.77 and 0.72 on D2, respectively. Conclusions: The proposed synthesizers realized mutual and high-quality synthesis between NCCT and CECT images; the training procedures, including SSL pretraining, DECT pretraining, and fine-tuning, were critical to their effectiveness. The results demonstrated the usefulness of synthesizers for pulmonary vessel segmentation from NCCT images. © 2023 Elsevier B.V.","Computerized tomography; Diagnosis; Image segmentation; Mean square error; Medical imaging; Signal to noise ratio; Computed tomography; Contrast-enhanced CT; CT Image; Dual-energy CT; Images synthesis; Mean absolute error; Medical image synthesis; Pre-training; Pulmonary vessel segmentation; Training procedures; Generative adversarial networks","Computed tomography; Generative adversarial network; Medical image synthesis; Pulmonary vessel segmentation","Article","Final","","Scopus","2-s2.0-85147541607"
"Zhou P.; Xie L.; Ni B.; Liu L.; Tian Q.","Zhou, Peng (37007198700); Xie, Lingxi (35189768200); Ni, Bingbing (57188713282); Liu, Lin (57221211366); Tian, Qi (7102891959)","37007198700; 35189768200; 57188713282; 57221211366; 7102891959","HRInversion: High-Resolution GAN Inversion for Cross-Domain Image Synthesis","2022","IEEE Transactions on Circuits and Systems for Video Technology","","","","1","1","0","10.1109/TCSVT.2022.3222456","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142838548&doi=10.1109%2fTCSVT.2022.3222456&partnerID=40&md5=4af2830ded68b112ddac7163d6e66889","We investigate GAN inversion problems of using pre-trained GANs to reconstruct real images. Recent methods for such problems typically employ a VGG perceptual loss to measure the difference between images. While the perceptual loss has achieved remarkable success in various computer vision tasks, it may cause unpleasant artifacts and is sensitive to changes in input scale. This paper delivers an important message that algorithm details are crucial for achieving satisfying performance. In particular, we propose two important but undervalued design principles: (i) not down-sampling the input of the perceptual loss to avoid high-frequency artifacts; and (ii) calculating the perceptual loss using convolutional features which are robust to scale. Integrating these designs derives the proposed framework, HRInversion, that achieves superior performance in reconstructing image details. We validate the effectiveness of HRInversion on a cross-domain image synthesis task and propose a post-processing approach named local style optimization (LSO) to synthesize clean and controllable stylized images. For the evaluation of the cross-domain images, we introduce a metric named ID retrieval which captures the similarity of face identities of stylized images to content images. We also test HRInversion on non-square images. Equipped with implicit neural representation, HRInversion applies to ultra-high resolution images with more than 10 million pixels. Furthermore, we show applications of style transfer and 3D-aware GAN inversion, paving the way for extending the application range of HRInversion. IEEE","Computer vision; Generative adversarial networks; Image reconstruction; Image resolution; Job analysis; Cross-domain; GAN inversion; Generator; High resolution; Images reconstruction; Images synthesis; Perceptual loss; Performance; Task analysis; Semantics","GAN inversion; Generative adversarial networks; Generators; Image reconstruction; Image resolution; Image synthesis; image synthesis; perceptual loss; Semantics; Task analysis","Article","Article in press","","Scopus","2-s2.0-85142838548"
"Tan H.; Yin B.; Wei K.; Liu X.; Li X.","Tan, Hongchen (57209272265); Yin, Baocai (8616230700); Wei, Kun (57215780888); Liu, Xiuping (8362860200); Li, Xin (57218467896)","57209272265; 8616230700; 57215780888; 8362860200; 57218467896","ALR-GAN: Adaptive Layout Refinement for Text-to-Image Synthesis","2023","IEEE Transactions on Multimedia","","","","1","12","11","10.1109/TMM.2023.3238554","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147288326&doi=10.1109%2fTMM.2023.3238554&partnerID=40&md5=1c94b334cbf4b687e9a4db8cab5f27b4","We propose a novel Text-to-Image Generation Network, Adaptive Layout Refinement Generative Adversarial Network (ALR-GAN), to adaptively refine the layout of synthesized images without any auxiliary information. The ALR-GAN includes an Adaptive Layout Refinement (ALR) module and a Layout Visual Refinement (LVR) loss. The ALR module aligns the layout structure (which refers to locations of objects and background) of a synthesized image with that of its corresponding real image. In ALR module, we proposed an Adaptive Layout Refinement (ALR) loss to balance the matching of hard and easy features, for more efficient layout structure matching. Based on the refined layout structure, the LVR loss further refines the visual representation within the layout area. Experimental results on two widely-used datasets show that ALR-GAN performs competitively at the Text-to-Image generation task. IEEE","Generative adversarial networks; Image processing; Job analysis; Adaptation models; Consistency constraints; Generator; Images synthesis; Information consistency constraint; Layout; Object layout refinement; Object layouts; Task analysis; Text-to-image synthesis; Semantics","Adaptation models; Generative Adversarial Network; Generators; Information Consistency Constraint; Layout; Object Layout Refinement; Semantics; Task analysis; Text-to-Image Synthesis; Training; Visualization","Article","Article in press","","Scopus","2-s2.0-85147288326"
"Shen Z.; Ouyang X.; Xiao B.; Cheng J.-Z.; Shen D.; Wang Q.","Shen, Zhenrong (57316512500); Ouyang, Xi (57188922178); Xiao, Bin (57200613168); Cheng, Jie-Zhi (9845624100); Shen, Dinggang (7401738392); Wang, Qian (57192157811)","57316512500; 57188922178; 57200613168; 9845624100; 7401738392; 57192157811","Image synthesis with disentangled attributes for chest X-ray nodule augmentation and detection","2023","Medical Image Analysis","84","","102708","","","","10.1016/j.media.2022.102708","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143879723&doi=10.1016%2fj.media.2022.102708&partnerID=40&md5=c48e90b6777c5222be5e9cfa486c9c28","Lung nodule detection in chest X-ray (CXR) images is common to early screening of lung cancers. Deep-learning-based Computer-Assisted Diagnosis (CAD) systems can support radiologists for nodule screening in CXR images. However, it requires large-scale and diverse medical data with high-quality annotations to train such robust and accurate CADs. To alleviate the limited availability of such datasets, lung nodule synthesis methods are proposed for the sake of data augmentation. Nevertheless, previous methods lack the ability to generate nodules that are realistic with the shape/size attributes desired by the detector. To address this issue, we introduce a novel lung nodule synthesis framework in this paper, which decomposes nodule attributes into three main aspects including the shape, the size, and the texture, respectively. A GAN-based Shape Generator firstly models nodule shapes by generating diverse shape masks. The following Size Modulation then enables quantitative control on the diameters of the generated nodule shapes in pixel-level granularity. A coarse-to-fine gated convolutional Texture Generator finally synthesizes visually plausible nodule textures conditioned on the modulated shape masks. Moreover, we propose to synthesize nodule CXR images by controlling the disentangled nodule attributes for data augmentation, in order to better compensate for the nodules that are easily missed in the detection task. Our experiments demonstrate the enhanced image quality, diversity, and controllability of the proposed lung nodule synthesis framework. We also validate the effectiveness of our data augmentation strategy on greatly improving nodule detection performance. © 2022 Elsevier B.V.","Humans; Lung; Lung Neoplasms; Radiographic Image Interpretation, Computer-Assisted; Radiography; Solitary Pulmonary Nodule; Tomography, X-Ray Computed; X-Rays; Biological organs; Computer aided diagnosis; Computer aided instruction; Deep learning; Image enhancement; Chest X-ray; Chest X-ray image; Computer assisted diagnosis; Data augmentation; Image Inpainting; Images synthesis; Lung Cancer; Lung nodule; Lung nodules detection; Nodule detection; Article; augmentation index; clinical evaluation; computer assisted tomography; conceptual framework; controlled study; data synthesis; decomposition; deep learning; generative adversarial network; human; image analysis; image quality; lung nodule; major clinical study; network learning; quantitative analysis; receiver operating characteristic; task positive network; thorax radiography; three-dimensional imaging; computer assisted diagnosis; diagnostic imaging; lung; lung nodule; lung tumor; procedures; radiography; X ray; x-ray computed tomography; Textures","Chest X-ray; Data augmentation; Image inpainting; Image synthesis; Nodule detection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143879723"
"Fang C.; Liu Y.; Liu Y.; Liu M.; Qiu X.; Li Y.; Wen J.; Yang Y.","Fang, Chengyijue (57217062088); Liu, Yingao (57357290500); Liu, Ying (56457779400); Liu, Mengqiu (57383016900); Qiu, Xiaohui (57202922119); Li, Yang (57480432100); Wen, Jie (57199884632); Yang, Yidong (35764114600)","57217062088; 57357290500; 56457779400; 57383016900; 57202922119; 57480432100; 57199884632; 35764114600","Label-free coronavirus disease 2019 lesion segmentation based on synthetic healthy lung image subtraction","2022","Medical Physics","49","7","","4632","4641","9","10.1002/mp.15661","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128580791&doi=10.1002%2fmp.15661&partnerID=40&md5=b8786c4870cbf00361b01a914f684813","Purpose: Coronavirus disease 2019 (COVID-19) has become a global pandemic and is still posing a severe health risk to the public. Accurate and efficient segmentation of pneumonia lesions in computed tomography (CT) scans is vital for treatment decision-making. We proposed a novel unsupervised approach using a cycle consistent generative adversarial network (cycle-GAN) which automates and accelerates the process of lesion delineation. Method: The workflow includes lung volume segmentation, healthy lung image synthesis, infected and healthy image subtraction, and binary lesion mask generation. The lung volume was first delineated using a pre-trained U-net and worked as the input for the following network. A cycle-GAN was trained to generate synthetic healthy lung CT images from infected lung images. After that, the pneumonia lesions were extracted by subtracting the synthetic healthy lung CT images from the infected lung CT images. A median filter and k-means clustering were then applied to contour the lesions. The auto segmentation approach was validated on three different datasets. Results: The average Dice coefficient reached 0.666 ± 0.178 on the three datasets. Especially, the dice reached 0.748 ± 0.121 and 0.730 ± 0.095, respectively, on two public datasets Coronacases and Radiopedia. Meanwhile, the average precision and sensitivity for lesion segmentation on the three datasets were 0.679 ± 0.244 and 0.756 ± 0.162. The performance is comparable to existing supervised segmentation networks and outperforms unsupervised ones. Conclusion: The proposed label-free segmentation method achieved high accuracy and efficiency in automatic COVID-19 lesion delineation. The segmentation result can serve as a baseline for further manual modification and a quality assurance tool for lesion diagnosis. Furthermore, due to its unsupervised nature, the result is not influenced by physicians’ experience which otherwise is crucial for supervised methods. © 2022 American Association of Physicists in Medicine.","COVID-19; Humans; Image Processing, Computer-Assisted; Lung; Pandemics; Pneumonia; Tomography, X-Ray Computed; Biological organs; Computerized tomography; Decision making; Diagnosis; Health risks; Image segmentation; K-means clustering; Median filters; Quality assurance; Computed tomography images; Computed tomography scan; Coronavirus disease 2019 pneumonia; Coronaviruses; Image subtraction; Label free; Label-free segmentation; Lesion segmentations; Lung volume; Unsupervised approaches; Article; coronavirus disease 2019; cycle consistent generative adversarial network; human; image segmentation; image subtraction; k means clustering; lung infection; lung volume; major clinical study; pneumonia; unsupervised machine learning; workflow; x-ray computed tomography; diagnostic imaging; image processing; lung; pandemic; pneumonia; procedures; Coronavirus","COVID-19 pneumonia; label-free segmentation; lesion segmentation; unsupervised approach","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128580791"
"Li Z.; Tian Q.; Ngamsombat C.; Cartmell S.; Conklin J.; Filho A.L.M.G.; Lo W.-C.; Wang G.; Ying K.; Setsompop K.; Fan Q.; Bilgic B.; Cauley S.; Huang S.Y.","Li, Ziyu (57212003399); Tian, Qiyuan (48862168000); Ngamsombat, Chanon (55635954300); Cartmell, Samuel (57119440200); Conklin, John (24079925200); Filho, Augusto Lio M. Gonçalves (57408478900); Lo, Wei-Ching (57219864511); Wang, Guangzhi (57194626987); Ying, Kui (7005162150); Setsompop, Kawin (15053610500); Fan, Qiuyun (56927493300); Bilgic, Berkin (36469704300); Cauley, Stephen (15759105700); Huang, Susie Y. (7405421635)","57212003399; 48862168000; 55635954300; 57119440200; 24079925200; 57408478900; 57219864511; 57194626987; 7005162150; 15053610500; 56927493300; 36469704300; 15759105700; 7405421635","High-fidelity fast volumetric brain MRI using synergistic wave-controlled aliasing in parallel imaging and a hybrid denoising generative adversarial network (HDnGAN)","2022","Medical Physics","49","2","","1000","1014","14","10.1002/mp.15427","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122663032&doi=10.1002%2fmp.15427&partnerID=40&md5=6f7a6f358abbcb32dcfebd5ae8eec6b2","Purpose: The goal of this study is to leverage an advanced fast imaging technique, wave-controlled aliasing in parallel imaging (Wave-CAIPI), and a generative adversarial network (GAN) for denoising to achieve accelerated high-quality high-signal-to-noise-ratio (SNR) volumetric magnetic resonance imaging (MRI). Methods: Three-dimensional (3D) T2-weighted fluid-attenuated inversion recovery (FLAIR) image data were acquired on 33 multiple sclerosis (MS) patients using a prototype Wave-CAIPI sequence (acceleration factor R = 3 × 2, 2.75 min) and a standard T2-sampling perfection with application-optimized contrasts by using flip angle evolution (SPACE) FLAIR sequence (R = 2, 7.25 min). A hybrid denoising GAN entitled “HDnGAN” consisting of a 3D generator and a 2D discriminator was proposed to denoise highly accelerated Wave-CAIPI images. HDnGAN benefits from the improved image synthesis performance provided by the 3D generator and increased training samples from a limited number of patients for training the 2D discriminator. HDnGAN was trained and validated on data from 25 MS patients with the standard FLAIR images as the target and evaluated on data from eight MS patients not seen during training. HDnGAN was compared to other denoising methods including adaptive optimized nonlocal means (AONLM), block matching with 4D filtering (BM4D), modified U-Net (MU-Net), and 3D GAN in qualitative and quantitative analysis of output images using the mean squared error (MSE) and Visual Geometry Group (VGG) perceptual loss compared to standard FLAIR images, and a reader assessment by two neuroradiologists regarding sharpness, SNR, lesion conspicuity, and overall quality. Finally, the performance of these denoising methods was compared at higher noise levels using simulated data with added Rician noise. Results: HDnGAN effectively denoised low-SNR Wave-CAIPI images with sharpness and rich textural details, which could be adjusted by controlling the contribution of the adversarial loss to the total loss when training the generator. Quantitatively, HDnGAN (λ = 10–3) achieved low MSE and the lowest VGG perceptual loss. The reader study showed that HDnGAN (λ = 10–3) significantly improved the SNR of Wave-CAIPI images (p < 0.001), outperformed AONLM (p = 0.015), BM4D (p < 0.001), MU-Net (p < 0.001), and 3D GAN (λ = 10–3) (p < 0.001) regarding image sharpness, and outperformed MU-Net (p < 0.001) and 3D GAN (λ = 10–3) (p = 0.001) regarding lesion conspicuity. The overall quality score of HDnGAN (λ = 10–3) (4.25 ± 0.43) was significantly higher than those from Wave-CAIPI (3.69 ± 0.46, p = 0.003), BM4D (3.50 ± 0.71, p = 0.001), MU-Net (3.25 ± 0.75, p < 0.001), and 3D GAN (λ = 10–3) (3.50 ± 0.50, p < 0.001), with no significant difference compared to standard FLAIR images (4.38 ± 0.48, p = 0.333). The advantages of HDnGAN over other methods were more obvious at higher noise levels. Conclusion: HDnGAN provides robust and feasible denoising while preserving rich textural detail in empirical volumetric MRI data. Our study using empirical patient data and systematic evaluation supports the use of HDnGAN in combination with modern fast imaging techniques such as Wave-CAIPI to achieve high-fidelity fast volumetric MRI and represents an important step to the clinical translation of GANs. © 2021 American Association of Physicists in Medicine.","Brain; Contrast Media; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Multiple Sclerosis; Signal-To-Noise Ratio; Image denoising; Image enhancement; Image quality; Magnetic resonance imaging; Mean square error; Patient rehabilitation; Quality control; Signal to noise ratio; contrast medium; Aliasing; Block Matching; De-noising; Fast imaging; Fluid-attenuated inversion recoveries; High-fidelity; Multiple sclerosis; Parallel A; Parallel imaging; Volumetrics; Article; artificial neural network; clinical article; digital filtering; fluid-attenuated inversion recovery imaging; geometry; human; hybrid denoising generative adversarial network; image quality; imaging and display; mean squared error; multiple sclerosis; neuroimaging; neuroradiologist; nuclear magnetic resonance imaging; qualitative analysis; quantitative analysis; signal noise ratio; simulation; T2 weighted imaging; three-dimensional imaging; volumetry; wave controlled aliasing in parallel imaging; brain; diagnostic imaging; image processing; multiple sclerosis; Generative adversarial networks","adversarial loss; convolutional neural network; fast imaging; multiple sclerosis; T<sub>2</sub>-weighted FLAIR; VGG perceptual loss","Article","Final","","Scopus","2-s2.0-85122663032"
"Luo W.; Yang S.; Zhang W.","Luo, Wuyang (57210430769); Yang, Su (35319285900); Zhang, Weishan (14038263700)","57210430769; 35319285900; 14038263700","Photo-realistic image synthesis from lines and appearance with modular modulation","2022","Neurocomputing","503","","","81","91","10","10.1016/j.neucom.2022.06.007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133948309&doi=10.1016%2fj.neucom.2022.06.007&partnerID=40&md5=780cfe4709585996fd0ba5e410273286","The image-to-image translation task has made significant progress by relying on conditional generative adversarial networks. However, for many tasks, multiple condition images are required. This paper considers a very classic application scenario, using lines and appearance to synthesize photo-realistic images, describing structure and appearance information, respectively, for example, generating realistic face images from portrait drawings and color scribbles, and generating photos from sketches and texture patches. The key to this type of task is how to fuse the two conditional information. We propose an image translation system driven by line and appearance images, introducing a modular architecture for condition fusion. Unlike the previous condition fusion schemes, its main body of the generator is composed of stacked modulation units (MUs). Here, structural features and appearance features are progressively incorporated via cascaded MUs, each of which pays attention to the local regions. The visualization experiment shows that such a scheme lets the network automatically learn to decompose the fusion process as multiple sub-steps in latent spaces. Our model produces higher quality results quantitatively and qualitatively compared to the state-of-the-art method on different tasks and datasets. The ablation study demonstrates the effectiveness of the MUs and intuitively explains the process of feature fusion through visualization. © 2022 Elsevier B.V.","Generative adversarial networks; Image fusion; Image processing; Textures; Visualization; Application scenario; Condition; Face images; Features fusions; Image translation; Image-to-image translation; Images synthesis; Modulars; Photo realistic image synthesis; Photorealistic images; article; attention; decomposition; drawing; quantitative analysis; synthesis; Modulation","Feature Fusion; Generative Adversarial Networks; Image Synthesis; Image-to-Image Translation","Article","Final","","Scopus","2-s2.0-85133948309"
"Ge N.; Liu Y.; Xu X.; Zhang X.; Jiang M.","Ge, Nan (58030382900); Liu, Yixi (58029710500); Xu, Xiang (58029538900); Zhang, Xuedian (34874077100); Jiang, Minshan (26665473300)","58030382900; 58029710500; 58029538900; 34874077100; 26665473300","A Fast Generative Adversarial Network for High-Fidelity Optical Coherence Tomography Image Synthesis","2022","Photonics","9","12","944","","","","10.3390/photonics9120944","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144668349&doi=10.3390%2fphotonics9120944&partnerID=40&md5=5f03a470de320e642e21723e8668cc63","(1) Background: We present a fast generative adversarial network (GAN) for generating high-fidelity optical coherence tomography (OCT) images. (2) Methods: We propose a novel Fourier-FastGAN (FOF-GAN) to produce OCT images. To improve the image quality of the synthetic images, a new discriminator with a Fourier attention block (FAB) and a new generator with fast Fourier transform (FFT) processes were redesigned. (3) Results: We synthesized normal, diabetic macular edema (DME), and drusen images from the Kermany dataset. When training with 2800 images with 50,000 epochs, our model used only 5 h on a single RTX 2080Ti GPU. Our synthetic images are realistic to recognize the retinal layers and pathological features. The synthetic images were evaluated by a VGG16 classifier and the Fréchet inception distance (FID). The reliability of our model was also demonstrated in the few-shot learning with only 100 pictures. (4) Conclusions: Using a small computing budget and limited training data, our model exhibited good performance for generating OCT images with a 512 × 512 resolution in a few hours. Fast retinal OCT image synthesis is an aid for data augmentation medical applications of deep learning. © 2022 by the authors.","","Fourier-FastGAN; generative adversarial network; optical coherence tomography","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144668349"
"Pu G.; Men Y.; Mao Y.; Jiang Y.; Ma W.-Y.; Lian Z.","Pu, Guo (57219749176); Men, Yifang (57207768874); Mao, Yiming (57219650503); Jiang, Yuning (57224896539); Ma, Wei-Ying (36071778600); Lian, Zhouhui (23493219700)","57219749176; 57207768874; 57219650503; 57224896539; 36071778600; 23493219700","Controllable Image Synthesis With Attribute-Decomposed GAN","2023","IEEE Transactions on Pattern Analysis and Machine Intelligence","45","2","","1514","1532","18","10.1109/TPAMI.2022.3161985","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137802864&doi=10.1109%2fTPAMI.2022.3161985&partnerID=40&md5=b3ffc7fe30b2693614de85463398e590","This paper proposes Attribute-Decomposed GAN (ADGAN) and its enhanced version (ADGAN++) for controllable image synthesis, which can produce realistic images with desired attributes provided in various source inputs. The core ideas of the proposed ADGAN and ADGAN++ are both to embed component attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations. The major difference between them is that ADGAN processes all component attributes simultaneously while ADGAN++ utilizes a serial encoding strategy. More specifically, ADGAN consists of two encoding pathways with style block connections and is capable of decomposing the original hard mapping into multiple more accessible subtasks. In the source pathway, component layouts are extracted via a semantic parser and the segmented components are fed into a shared global texture encoder to obtain decomposed latent codes. This strategy allows for the synthesis of more realistic output images and the automatic separation of un-annotated component attributes. Although the original ADGAN works in a delicate and efficient manner, intrinsically it fails to handle the semantic image synthesizing task when the number of attribute categories is huge. To address this problem, ADGAN++ employs the serial encoding of different component attributes to synthesize each part of the target real-world image, and adopts several residual blocks with segmentation guided instance normalization to assemble the synthesized component images and refine the original synthesis result. The two-stage ADGAN++ is designed to alleviate the massive computational costs required when synthesizing real-world images with numerous attributes while maintaining the disentanglement of different attributes to enable flexible control of arbitrary component attributes of the synthesized images. Experimental results demonstrate the proposed methods' superiority over the state of the art in pose transfer, face style transfer, and semantic image synthesis, as well as their effectiveness in the task of component attribute transfer. Our code and data are publicly available at https://github.com/menyifang/ADGAN.  © 1979-2012 IEEE.","Encoding (symbols); Image enhancement; Image segmentation; Semantics; Signal encoding; Attribute control; Continuous control; Encodings; Flexible control; Generative adversarial network; Images synthesis; Real-world image; Realistic images; Semantic images; Style transfer; Generative adversarial networks","attribute control; generative adversarial networks (GAN); Image Synthesis; style transfer","Article","Final","","Scopus","2-s2.0-85137802864"
"Yu Z.; Han X.; Zhang S.; Feng J.; Peng T.; Zhang X.","Yu, Ziqi (57219811211); Han, Xiaoyang (57286674800); Zhang, Shengjie (57907748800); Feng, Jianfeng (57804759800); Peng, Tingying (56245821000); Zhang, Xiao-Yong (56861682700)","57219811211; 57286674800; 57907748800; 57804759800; 56245821000; 56861682700","MouseGAN++: Unsupervised Disentanglement and Contrastive Representation for Multiple MRI Modalities Synthesis and Structural Segmentation of Mouse Brain","2022","IEEE Transactions on Medical Imaging","","","","1","1","0","10.1109/TMI.2022.3225528","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144045300&doi=10.1109%2fTMI.2022.3225528&partnerID=40&md5=a0b0fbcc923c2beaa39277c6027c0a6e","Segmenting the fine structure of the mouse brain on magnetic resonance (MR) images is critical for delineating morphological regions, analyzing brain function, and understanding their relationships. Compared to a single MRI modality, multimodal MRI data provide complementary tissue features that can be exploited by deep learning models, resulting in better segmentation results. However, multimodal mouse brain MRI data is often lacking, making automatic segmentation of mouse brain fine structure a very challenging task. To address this issue, it is necessary to fuse multimodal MRI data to produce distinguished contrasts in different brain structures. Hence, we propose a novel disentangled and contrastive GAN-based framework, named MouseGAN++, to synthesize multiple MR modalities from single ones in a structure-preserving manner, thus improving the segmentation performance by imputing missing modalities and multi-modality fusion. Our results demonstrate that the translation performance of our method outperforms the state-of-the-art methods. Using the subsequently learned modality-invariant information as well as the modality-translated images, MouseGAN++ can segment fine brain structures with averaged dice coefficients of 90.0% (T2w) and 87.9% (T1w), respectively, achieving around +10% performance improvement compared to the state-of-the-art algorithms. Our results demonstrate that MouseGAN++, as a simultaneous image synthesis and segmentation method, can be used to fuse cross-modality information in an unpaired manner and yield more robust performance in the absence of multimodal data. We release our method as a mouse brain structural segmentation tool for free academic usage at https://github.com/yu02019. IEEE","Atomic physics; Deep learning; Generative adversarial networks; Image enhancement; Image segmentation; Magnetic resonance; Mammals; Medical imaging; Brain functions; Brain structure; Disentangled representation; Fine structures; Learning models; Mouse brain; Multi-modal; Performance; Segmentation; Segmentation results; algorithm; animal experiment; article; brain; controlled study; male; mouse; nonhuman; nuclear magnetic resonance imaging; synthesis; Magnetic resonance imaging","Disentangled representations; Generative adversarial network; Mouse brain; MRI; Segmentation","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85144045300"
"Osuala R.; Kushibar K.; Garrucho L.; Linardos A.; Szafranowska Z.; Klein S.; Glocker B.; Diaz O.; Lekadir K.","Osuala, Richard (57194042562); Kushibar, Kaisar (57191381849); Garrucho, Lidia (57222261248); Linardos, Akis (57222260271); Szafranowska, Zuzanna (57226342061); Klein, Stefan (35147968700); Glocker, Ben (23396784900); Diaz, Oliver (36172316800); Lekadir, Karim (15042517700)","57194042562; 57191381849; 57222261248; 57222260271; 57226342061; 35147968700; 23396784900; 36172316800; 15042517700","Data synthesis and adversarial networks: A review and meta-analysis in cancer imaging","2023","Medical Image Analysis","84","","102704","","","","10.1016/j.media.2022.102704","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143516774&doi=10.1016%2fj.media.2022.102704&partnerID=40&md5=eac64f78f31875a834e425fd1f66642a","Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include inter-observer variability, class imbalance, dataset shifts, inter- and intra-tumour heterogeneity, malignancy determination, and treatment effect uncertainty. Given the recent advancements in image synthesis, Generative Adversarial Networks (GANs), and adversarial training, we assess the potential of these technologies to address a number of key challenges of cancer imaging. We categorise these challenges into (a) data scarcity and imbalance, (b) data access and privacy, (c) data annotation and segmentation, (d) cancer detection and diagnosis, and (e) tumour profiling, treatment planning and monitoring. Based on our analysis of 164 publications that apply adversarial training techniques in the context of cancer imaging, we highlight multiple underexplored solutions with research potential. We further contribute the Synthesis Study Trustworthiness Test (SynTRUST), a meta-analysis framework for assessing the validation rigour of medical image synthesis studies. SynTRUST is based on 26 concrete measures of thoroughness, reproducibility, usefulness, scalability, and tenability. Based on SynTRUST, we analyse 16 of the most promising cancer imaging challenge solutions and observe a high validation rigour in general, but also several desirable improvements. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on data synthesis and adversarial networks in the artificial intelligence community. © 2022","Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neoplasms; Prospective Studies; Reproducibility of Results; Clinical research; Diagnosis; Diseases; Medical imaging; Tumors; Adversarial networks; Adversarial training; Cancer imaging; Data synthesis; Images synthesis; Medical advances; Meta-analysis; Synthetic data; Technological advances; Trustworthiness; artificial intelligence; cancer diagnosis; cancer therapy; clinical practice; controlled study; data privacy; data synthesis; decision support system; deep learning; diagnostic error; environmental factor; epiluminescence microscopy; facies; feature extraction; histogram; human; image analysis; image processing; image reconstruction; image segmentation; information processing; intensity modulated radiation therapy; mammography; meta analysis; methodology; Nash equilibrium; organs at risk; outlier detection; patient information; phenotype; planning target volume; prognosis; publication; radiation dose; Review; treatment planning; tumor growth; ultrasound; usability; workload; diagnostic imaging; image processing; neoplasm; nuclear magnetic resonance imaging; procedures; prospective study; reproducibility; Generative adversarial networks","Adversarial training; Generative Adversarial Network; Synthetic data; Trustworthiness","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143516774"
"Wu X.; Li C.; Zeng X.; Wei H.; Deng H.-W.; Zhang J.; Xu M.","Wu, Xindi (57216703953); Li, Chengkun (57567773300); Zeng, Xiangrui (57201280726); Wei, Haocheng (57566964800); Deng, Hong-Wen (57567773400); Zhang, Jing (56365047200); Xu, Min (57195340740)","57216703953; 57567773300; 57201280726; 57566964800; 57567773400; 56365047200; 57195340740","CryoETGAN: Cryo-Electron Tomography Image Synthesis via Unpaired Image Translation","2022","Frontiers in Physiology","13","","760404","","","","10.3389/fphys.2022.760404","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127903291&doi=10.3389%2ffphys.2022.760404&partnerID=40&md5=7080879770e1eb5eca92d5556fbd324e","Cryo-electron tomography (Cryo-ET) has been regarded as a revolution in structural biology and can reveal molecular sociology. Its unprecedented quality enables it to visualize cellular organelles and macromolecular complexes at nanometer resolution with native conformations. Motivated by developments in nanotechnology and machine learning, establishing machine learning approaches such as classification, detection and averaging for Cryo-ET image analysis has inspired broad interest. Yet, deep learning-based methods for biomedical imaging typically require large labeled datasets for good results, which can be a great challenge due to the expense of obtaining and labeling training data. To deal with this problem, we propose a generative model to simulate Cryo-ET images efficiently and reliably: CryoETGAN. This cycle-consistent and Wasserstein generative adversarial network (GAN) is able to generate images with an appearance similar to the original experimental data. Quantitative and visual grading results on generated images are provided to show that the results of our proposed method achieve better performance compared to the previous state-of-the-art simulation methods. Moreover, CryoETGAN is stable to train and capable of generating plausibly diverse image samples. Copyright © 2022 Wu, Li, Zeng, Wei, Deng, Zhang and Xu.","article; averaging; cell organelle; controlled study; deep learning; electron tomography; human; image analysis; machine learning; nanotechnology; protein conformation; quantitative analysis; simulation; sociology; synthesis","Cryo-ET; generative adversarial network; generative model; image synthesis; image translation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127903291"
"Lin C.; Xiong S.; Chen Y.","Lin, Chengde (57208754841); Xiong, Shengwu (57203905556); Chen, Yaxiong (57195986737)","57208754841; 57203905556; 57195986737","Mutual information maximizing GAN inversion for real face with identity preservation","2022","Journal of Visual Communication and Image Representation","87","","103566","","","","10.1016/j.jvcir.2022.103566","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132867495&doi=10.1016%2fj.jvcir.2022.103566&partnerID=40&md5=c84bf0efda86eff95367edf01a26af2c","Recent generative adversarial networks (GANs) have yielded remarkable performance in face image synthesis. GAN inversion embeds an image into the latent space of a pretrained generator, enabling it to be used for real face manipulation. However, current inversion approaches for real faces suffer the dilemma of initialization collapse and identity loss. In this paper, we propose a hierarchical GAN inversion for real faces with identity preservation based on mutual information maximization. We first use a facial domain guaranteed initialization to avoid the initialization collapse. Furthermore, we prove that maximizing the mutual information between inverted faces and their identities is equivalent to minimizing the distance between identity features from inverted and original faces. Optimization for real face inversion with identity preservation is implemented on this mutual information-maximizing constraint. Extensive experimental results show that our approach outperforms state-of-the-art solutions for inverting and editing real faces, particularly in terms of face identity preservation. © 2022 Elsevier Inc.","Software engineering; 'current; Face editing; Face identity preservation; Face image synthesis; Generative adversarial network inversion; Mutual information maximization; Mutual information maximizing; Mutual informations; Network inversion; Performance; Generative adversarial networks","Face editing; Face identity preservation; GAN inversion; Generative adversarial network; Mutual information maximizing","Article","Final","","Scopus","2-s2.0-85132867495"
"Tan Y.X.; Lee C.P.; Neo M.; Lim K.M.","Tan, Yong Xuan (57579573000); Lee, Chin Poo (36519082900); Neo, Mai (35424115800); Lim, Kian Ming (36554491300)","57579573000; 36519082900; 35424115800; 36554491300","Text-to-image synthesis with self-supervised learning","2022","Pattern Recognition Letters","157","","","119","126","7","10.1016/j.patrec.2022.04.010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128379543&doi=10.1016%2fj.patrec.2022.04.010&partnerID=40&md5=ebf72e0e5bfef5874f6f5aa0c1eb0cfa","Text-to-image synthesis extracts the meaning from the text description and converts it into an image correspondingly. Text-to-image synthesis is widely leveraged in many applications, such as graphic design, image editing, etc. Text-to-image synthesis approaches are mainly built on the basis of generative adversarial networks. One of the main challenges in text-to-image synthesis is to generate images that are visually realistic. Not only that, the text-to-image synthesis model is inherently susceptible to overconfidence and training instability issues. To address these challenges, this paper proposes a self-supervised text-to-image synthesis with some enhancements, including self-supervised learning, feature matching, L1 distance loss, and one-sided label smoothing. The self-supervised learning offers more image variations thus improving the classification power of the discriminator. The feature matching and L1 distance functions motivate the generator to synthesize images that are visually more similar to the real images based on the given text description. The one-sided label smoothing adds a penalty value when the discriminator makes a correct classification to alleviate the overconfidence problem and to improve the training stability. The performance of the proposed self-supervised text-to-image synthesis is evaluated on the Oxford-102 and CUB datasets. The empirical results demonstrate that the proposed self-supervised text-to-image synthesis generates images with richer image content diversity, more visually realistic, and more semantically consistent with the given text description. The proposed self-supervised text-to-image synthesis also outshines the methods in comparison in terms of the inception score and Structural Similarity Index. © 2022 Elsevier B.V.","Image enhancement; Supervised learning; Classification power; Distance functions; Features matching; Graphic design; Image editing; Image variations; Images synthesis; Self-supervised learning; Synthesis models; Text-to-image-synthesis; Generative adversarial networks","Generative adversarial network; Self-supervised learning; Text-to-image-synthesis","Article","Final","","Scopus","2-s2.0-85128379543"
"Katsuma D.; Kawanaka H.; Prasath V.B.S.; Aronow B.J.","Katsuma, Daiki (57576038900); Kawanaka, Hiroharu (57556791200); Prasath, V.B. Surya (24829700400); Aronow, Bruce J. (7005740105)","57576038900; 57556791200; 24829700400; 7005740105","Data Augmentation Using Generative Adversarial Networks for Multi-Class Segmentation of Lung Confocal IF Images","2022","Journal of Advanced Computational Intelligence and Intelligent Informatics","26","2","","138","146","8","10.20965/jaciii.2022.p0138","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128204390&doi=10.20965%2fjaciii.2022.p0138&partnerID=40&md5=385888ca332ee956c9782327898f636a","The human lung is a complex organ with high cellular heterogeneity, and its development and maintenance require interactive gene networks and dynamic crosstalk among multiple cell types. We focus on the confocal immunofluorescent (IF) images of lung tissues from the LungMAP database to reveal lung development. Using the current state-of-the-art deep learningbased model, the authors consider obtaining accurate multi-class segmentation of lung confocal IF images. One of the primary bottlenecks in using deep Convolutional Neural Network (CNN) models is the lack of availability of large-scale training or ground-truth segmentation labels. Then, we implement the multiclass segmentation with Generative Adversarial Network (GAN) models to expand the training dataset, improve overall segmentation accuracy, and discuss the effectiveness of created synthetic images in the segmentation of IF images. Consequently, experimental results indicated that 15.1% increased the accuracy of six-class segmentation using Mask R-CNN. In particular, the accuracy of our few data was mainly improved by using our proposed method. Therefore, the synthetic dataset can moderate the imbalanced data and be used for expanding the dataset. © 2022 Fuji Technology Press. All rights reserved.","Biological organs; Convolutional neural networks; Deep neural networks; Image enhancement; Image segmentation; Cellulars; Complex organs; Data augmentation; Gene networks; Human lung; Images synthesis; Immunofluorescence image; Immunofluorescent images; Multi-class segmentations; Segmentation; Generative adversarial networks","data augmentation; generative adversarial networks; image synthesis; immunofluorescence image; segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128204390"
"Yamazaki A.; Ishida T.","Yamazaki, Asumi (16551193100); Ishida, Takayuki (35406489400)","16551193100; 35406489400","Two-View Mammogram Synthesis from Single-View Data Using Generative Adversarial Networks","2022","Applied Sciences (Switzerland)","12","23","12206","","","","10.3390/app122312206","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143497935&doi=10.3390%2fapp122312206&partnerID=40&md5=a60c91b50c0ad8eae176862c74e13460","While two-view mammography taking both mediolateral-oblique (MLO) and cranio-caudual (CC) views is the current standard method of examination in breast cancer screening, single-view mammography is still being performed in some countries on women of specific ages. The rate of cancer detection is lower with single-view mammography than for two-view mammography, due to the lack of available image information. The goal of this work is to improve single-view mammography’s ability to detect breast cancer by providing two-view mammograms from single projections. The synthesis of novel-view images from single-view data has recently been achieved using generative adversarial networks (GANs). Here, we apply complete representation GAN (CR-GAN), a novel-view image synthesis model, aiming to produce CC-view mammograms from MLO views. Additionally, we incorporate two adaptations—the progressive growing (PG) technique and feature matching loss—into CR-GAN. Our results show that use of the PG technique reduces the training time, while the synthesized image quality is improved when using feature matching loss, compared with the method using only CR-GAN. Using the proposed method with the two adaptations, CC views similar to real views are successfully synthesized for some cases, but not all cases; in particular, image synthesis is rarely successful when calcifications are present. Even though the image resolution and quality are still far from clinically acceptable levels, our findings establish a foundation for further improvements in clinical applications. As the first report applying novel-view synthesis in medical imaging, this work contributes by offering a methodology for two-view mammogram synthesis. © 2022 by the authors.","","breast cancer; deep learning; generative adversarial network; mammogram; multi-view image synthesis; novel-view image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143497935"
"Sun Y.; Wang Y.; Hu L.; Huang Y.; Liu H.; Wang S.; Zhang C.","Sun, Yuanshuang (57219850285); Wang, Yinghua (16425010100); Hu, Liping (57198490400); Huang, Yuanyuan (57193827565); Liu, Hongwei (57205480555); Wang, Siyuan (57853004800); Zhang, Chen (57199502493)","57219850285; 16425010100; 57198490400; 57193827565; 57205480555; 57853004800; 57199502493","Attribute-Guided Generative Adversarial Network With Improved Episode Training Strategy for Few-Shot SAR Image Generation","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","","","1","15","14","10.1109/JSTARS.2023.3239633","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147266303&doi=10.1109%2fJSTARS.2023.3239633&partnerID=40&md5=6a66707f5627a5a61478334f783a494f","Deep learning-based models usually require a large amount of data for training, which guarantees the effectiveness of the trained model. Generative models are no exception, and sufficient training data are necessary for the diversity of generated images. However, for SAR images, data acquisition is expensive. Therefore, SAR image generation under few training samples is still a challenging problem to be solved. In this paper, we propose an attribute-guided generative adversarial network (AGGAN) with improved episode training strategy for few-shot SAR image generation. Firstly, we design the AGGAN structure, and spectral normalization is used to stabilize the training in the few-shot situation. The attribute labels of AGGAN are designed to be the category and aspect angle labels, which are essential information for SAR images. Secondly, an improved episode training strategy is proposed according to the characteristics of the few-shot generative task, and it can improve the quality of generated images in the few-shot situation. In addition, we explore the effectiveness of the proposed method when using different auxiliary data for training and use the Moving and Stationary Target Acquisition and Recognition (MSTAR) benchmark dataset and a simulated SAR dataset for verification. The experimental results show that AGGAN and the proposed improved episode training strategy can generate images of better quality when compared with some existing methods, which have been verified through visual observation, image similarity measures, and recognition experiments. When applying the generated images to the 5-shot SAR image recognition problem, the average recognition accuracy can be improved by at least 4<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula>. Author","Data acquisition; Deep learning; Generative adversarial networks; Image enhancement; Image recognition; Job analysis; Radar imaging; Few-shot image generation; Image generations; Images synthesis; Metalearning; Radar polarimetry; Synthetic aperture radar; Synthetic aperture radar images; Task analysis; Transfer learning; Synthetic aperture radar","Data models; Few-shot image generation; generative adversarial network; Image recognition; Image synthesis; meta-learning; Radar polarimetry; synthetic aperture radar (SAR); Task analysis; Training; Transfer learning; transfer learning","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85147266303"
"Luo X.; He X.; Chen X.; Qing L.; Zhang J.","Luo, Xiaodong (57211069189); He, Xiaohai (9237988800); Chen, Xiang (57200532416); Qing, Linbo (23971364800); Zhang, Jin (57834427200)","57211069189; 9237988800; 57200532416; 23971364800; 57834427200","DualG-GAN, a Dual-channel Generator based Generative Adversarial Network for text-to-face synthesis","2022","Neural Networks","155","","","155","167","12","10.1016/j.neunet.2022.08.016","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137034413&doi=10.1016%2fj.neunet.2022.08.016&partnerID=40&md5=a6767119cad3187a1de934dd402938bd","Text-to-image synthesis is a fundamental and challenging task in computer vision, which aims to synthesize realistic images from given descriptions. Recently, text-to-image synthesis methods have achieved great improvements in the quality of synthesized images. However, very few works have explored its application in the scenario of face synthesis, which is of great potentials in face-related applications and the public safety domain. On the other side, the faces generated by existing methods are generally of poor quality and have low consistency to the given text. To tackle this issue, in this paper, we build a novel end-to-end dual-channel generator based generative adversarial network, named DualG-GAN, to improve the quality of the generated images and the consistency to the text description. In DualG-GAN, to improve the consistency between the synthesized image and the input description, a dual-channel generator block is introduced, and a novel loss is designed to improve the similarity between the generated image and the ground-truth in three different semantic levels. Extensive experiments demonstrate that DualG-GAN achieves state-of-the-art results on SCU-Text2face dataset. To further verify the performance of DualG-GAN, we compare it with the current optimal methods on text-to-image synthesis tasks, where quantitative and qualitative results show that the proposed DualG-GAN achieves optimal performance in both Fréchet inception distance (FID) and R-precision metrics. As only a few works are focusing on text-to-face synthesis, this work can be seen as a baseline for future research. © 2022 Elsevier Ltd","Image Processing, Computer-Assisted; Image enhancement; Semantics; Conditional GAN; Dual channel; Dualg-GAN; Face synthesis; Images synthesis; Realistic images; Synthesized images; Text-to-face synthesis; Text-to-image synthesis; article; quantitative analysis; synthesis; image processing; procedures; Generative adversarial networks","Conditional GAN; DualG-GAN; Face synthesis; Text-to-face synthesis; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85137034413"
"Divyanth L.G.; Marzougui A.; González-Bernal M.J.; McGee R.J.; Rubiales D.; Sankaran S.","Divyanth, L.G. (57490778100); Marzougui, Afef (57203306286); González-Bernal, Maria Jose (57203583838); McGee, Rebecca J. (7101774401); Rubiales, Diego (8930304100); Sankaran, Sindhuja (35886052200)","57490778100; 57203306286; 57203583838; 7101774401; 8930304100; 35886052200","Evaluation of Effective Class-Balancing Techniques for CNN-Based Assessment of Aphanomyces Root Rot Resistance in Pea (Pisum sativum L.)","2022","Sensors","22","19","7237","","","","10.3390/s22197237","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139782320&doi=10.3390%2fs22197237&partnerID=40&md5=63de98ae0cded0c567e4a653ecb2e8a1","Aphanomyces root rot (ARR) is a devastating disease that affects the production of pea. The plants are prone to infection at any growth stage, and there are no chemical or cultural controls. Thus, the development of resistant pea cultivars is important. Phenomics technologies to support the selection of resistant cultivars through phenotyping can be valuable. One such approach is to couple imaging technologies with deep learning algorithms that are considered efficient for the assessment of disease resistance across a large number of plant genotypes. In this study, the resistance to ARR was evaluated through a CNN-based assessment of pea root images. The proposed model, DeepARRNet, was designed to classify the pea root images into three classes based on ARR severity scores, namely, resistant, intermediate, and susceptible classes. The dataset consisted of 1581 pea root images with a skewed distribution. Hence, three effective data-balancing techniques were identified to solve the prevalent problem of unbalanced datasets. Random oversampling with image transformations, generative adversarial network (GAN)-based image synthesis, and loss function with class-weighted ratio were implemented during the training process. The result indicated that the classification F1-score was 0.92 ± 0.03 when GAN-synthesized images were added, 0.91 ± 0.04 for random resampling, and 0.88 ± 0.05 when class-weighted loss function was implemented, which was higher than when an unbalanced dataset without these techniques were used (0.83 ± 0.03). The systematic approaches evaluated in this study can be applied to other image-based phenotyping datasets, which can aid the development of deep-learning models with improved performance. © 2022 by the authors.","Aphanomyces; Disease Resistance; Genotype; Peas; Classification (of information); Deep learning; Image classification; Image enhancement; Learning algorithms; Learning systems; Balancing techniques; Deep learning; Disease identification; Growth stages; Phenotyping; Pisum sativum L; Plant breeding; Root rot; Root rot resistance; Unbalanced datasets; Aphanomyces; disease resistance; genetics; genotype; pea; Generative adversarial networks","deep learning; disease identification; generative adversarial networks; plant breeding","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139782320"
"Eskandar G.; Abdelsamad M.; Armanious K.; Yang B.","Eskandar, George (57222288248); Abdelsamad, Mohamed (57305256700); Armanious, Karim (57208782510); Yang, Bin (57220789824)","57222288248; 57305256700; 57208782510; 57220789824","USIS: Unsupervised Semantic Image Synthesis","2023","Computers and Graphics (Pergamon)","111","","","14","23","9","10.1016/j.cag.2022.12.010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146424265&doi=10.1016%2fj.cag.2022.12.010&partnerID=40&md5=cc4605eb8b6772748441c265a328a133","Semantic Image Synthesis (SIS) is a subclass of I2I (I2) translation where a photorealistic image is synthesized from a segmentation mask. SIS has mainly been addressed as a supervised problem. However, state-of-the-art methods depend on a massive amount of labeled data and cannot be applied in an unpaired setting. On the other hand, generic unpaired I2I frameworks underperform in comparison. In this work, we propose a new framework, Unsupervised Semantic Image Synthesis (USIS), as a first step towards closing the performance gap between paired and unpaired settings. We design a simple and effective learning scheme that combines the fragmented benefits of cycle losses and relationship preservation constraints. Then, we make the discovery that, contrary to I2I translation, discriminator design is crucial for label-to-image translation. To this end, we design a new discriminator with a wavelet-based encoder and a decoder to reconstruct the real images. The self-supervised reconstruction loss in the decoder prevents the encoder from overfitting on a few wavelet coefficients. We test our methodology on 3 challenging datasets and set a new standard for unpaired SIS. The generated images demonstrate significantly better diversity, quality and multimodality. © 2023 Elsevier Ltd","Decoding; Semantic Segmentation; Semantic Web; Semantics; Signal encoding; Images synthesis; Labeled data; Performance gaps; Photorealistic images; Segmentation masks; Semantic image synthesis; Semantic images; State-of-the-art methods; Synthesised; Unpaired I2I translation; Generative adversarial networks","Generative adversarial networks; Semantic image synthesis; Unpaired I2I translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85146424265"
"Jia X.; Xiao J.; Wu C.","Jia, Xinkang (57219761531); Xiao, Jun (7402564676); Wu, Chao (55628577295)","57219761531; 7402564676; 55628577295","TICS: text–image-based semantic CAPTCHA synthesis via multi-condition adversarial learning","2022","Visual Computer","38","3","","963","975","12","10.1007/s00371-021-02061-1","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100719675&doi=10.1007%2fs00371-021-02061-1&partnerID=40&md5=326d774b64b84d49f9ae44620f28a47f","CAPTCHA is used to distinguish humans from automated programs and plays an important role in multimedia security mechanisms. Traditional CAPTCHA methods like image-based CAPTCHA and text-based CAPTCHA are usually based on word-level understanding, which can be easily cracked due to the recent success of deep learning techniques. To this end, this paper proposes a text–image-based CAPTCHA based on the cognition process and semantic reasoning and a novel model to generate the CAPTCHA. This method synthesizes three features: sentence, object, and location to generate a multi-conditional CAPTCHA that can resist the attack of the classification of CNN. A quantity of experiments has been conducted, and the result showed that the classification of ResNet-50 on the proposed TIC only achieves 3.38% accuracy. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.","Deep learning; Electronic mail filters; Learning systems; Multimedia systems; Network security; Semantics; Adversarial learning; CAPTCHAs; Image-based; Learning techniques; Multimedia security; Semantic reasoning; Word level; Image processing","Generative Adversarial Network; security mechanism; semantic image synthesis; Text–image-based CAPTCHA","Article","Final","","Scopus","2-s2.0-85100719675"
"Yoon D.; Kong H.-J.; Kim B.S.; Cho W.S.; Lee J.C.; Cho M.; Lim M.H.; Yang S.Y.; Lim S.H.; Lee J.; Song J.H.; Chung G.E.; Choi J.M.; Kang H.Y.; Bae J.H.; Kim S.","Yoon, Dan (57408707200); Kong, Hyoun-Joong (16637352200); Kim, Byeong Soo (57408707300); Cho, Woo Sang (57408879400); Lee, Jung Chan (37960940900); Cho, Minwoo (56783471200); Lim, Min Hyuk (57020700000); Yang, Sun Young (36462052000); Lim, Seon Hee (7404081053); Lee, Jooyoung (57206732167); Song, Ji Hyun (55978475200); Chung, Goh Eun (24450051100); Choi, Ji Min (57322991200); Kang, Hae Yeon (15845339100); Bae, Jung Ho (57195468486); Kim, Sungwan (27169083000)","57408707200; 16637352200; 57408707300; 57408879400; 37960940900; 56783471200; 57020700000; 36462052000; 7404081053; 57206732167; 55978475200; 24450051100; 57322991200; 15845339100; 57195468486; 27169083000","Colonoscopic image synthesis with generative adversarial network for enhanced detection of sessile serrated lesions using convolutional neural network","2022","Scientific Reports","12","1","261","","","","10.1038/s41598-021-04247-y","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122702189&doi=10.1038%2fs41598-021-04247-y&partnerID=40&md5=30647bf19100b5dbc15ba231b151a4c8","Computer-aided detection (CADe) systems have been actively researched for polyp detection in colonoscopy. To be an effective system, it is important to detect additional polyps that may be easily missed by endoscopists. Sessile serrated lesions (SSLs) are a precursor to colorectal cancer with a relatively higher miss rate, owing to their flat and subtle morphology. Colonoscopy CADe systems could help endoscopists; however, the current systems exhibit a very low performance for detecting SSLs. We propose a polyp detection system that reflects the morphological characteristics of SSLs to detect unrecognized or easily missed polyps. To develop a well-trained system with imbalanced polyp data, a generative adversarial network (GAN) was used to synthesize high-resolution whole endoscopic images, including SSL. Quantitative and qualitative evaluations on GAN-synthesized images ensure that synthetic images are realistic and include SSL endoscopic features. Moreover, traditional augmentation methods were used to compare the efficacy of the GAN augmentation method. The CADe system augmented with GAN synthesized images showed a 17.5% improvement in sensitivity on SSLs. Consequently, we verified the potential of the GAN to synthesize high-resolution images with endoscopic features and the proposed system was found to be effective in detecting easily missed polyps during a colonoscopy. © 2022, The Author(s).","Colonic Polyps; Colonoscopy; Colorectal Neoplasms; Databases, Factual; Early Detection of Cancer; Humans; Image Interpretation, Computer-Assisted; Neural Networks, Computer; Predictive Value of Tests; Prospective Studies; Reproducibility of Results; Retrospective Studies; colon polyp; colonoscopy; colorectal tumor; computer assisted diagnosis; early cancer diagnosis; factual database; human; pathology; predictive value; prospective study; reproducibility; retrospective study","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85122702189"
"Bayoumi R.; Alfonse M.; Roushdy M.; Salem A.-B.M.","Bayoumi, Razan (57263716900); Alfonse, Marco (57039027100); Roushdy, Mohamed (36903823300); Salem, Abdel-Badeeh M. (36762342200)","57263716900; 57039027100; 36903823300; 36762342200","Text-to-image generation based on AttnDM-GAN and DMAttn-GAN: applications and challenges","2023","Bulletin of Electrical Engineering and Informatics","12","2","","1180","1188","8","10.11591/eei.v12i2.4199","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144010304&doi=10.11591%2feei.v12i2.4199&partnerID=40&md5=55923c60ab83a4292a37732282d6d51e","The deep fake faces generation using generative adversarial networks (GANs) has reached an incredible level of realism where people can’t differentiate the real from the fake. Text-to-face is a very challenging task compared to other text-to-image syntheses because of the detailed, precise, and complex nature of the human faces in addition to the textual description details. Providing an accurate realistic text-to-image model can be useful for many applications such as criminal identification where the model will be acting as the forensic artist. This paper presents text-to-image generation based on attention dynamic memory (AttnDM-GAN) and dynamic memory attention (DMAttn-GAN) that are applied to different datasets with an analysis that shows the different complexity of different datasets’ categories, the quality of the datasets, and their effect on the results of the resolution and consistency of the generated images. © 2023, Institute of Advanced Engineering and Science. All rights reserved.","","Computer vision; Conditional image synthesis; Generative adversarial networks; Image generation; Text-to-face","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144010304"
"Dung N.Q.; Kim H.","Dung, Nguyen Quoc (58072934600); Kim, Hakil (22034925200)","58072934600; 22034925200","Generating High-Resolution Fire Images with Controllable Attributes via Generative Adversarial Networks","2022","International Conference on Control, Automation and Systems","2022-November","","","348","353","5","10.23919/ICCAS55662.2022.10003687","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146623402&doi=10.23919%2fICCAS55662.2022.10003687&partnerID=40&md5=17f2e217706b77a54259c0fa804ca2db","Obtaining realistic fire images using deep learning models and recent versions of Generative adversarial networks (GAN) has been proven to be a difficult task due to the unnatural appearance of the generated results. This paper provides a novel approach based on StarGANv2 to generate fire kernels from any input provided as a reference. In addition, a deep learning-based image blending technique performs the migration of the fire kernels to the target scenes. By using any input as a reference, the generated fire image could be controlled to accommodate different environmental factors, resulting in a diverse but equally pseudo-real synthetic dataset. The proposed method generates images that achieve better FID and LPIPS values than StarGANv2 for both a public dataset (AI Hub) and a privately-owned dataset (Visionin). In addition, YOLOv4 is used as a fire detection model to evaluate the synthetic data on improving the performance of the detected network. Compared to the model trained on the real data, the model trained on the combined dataset outperforms 2%14% higher.  © 2022 ICROS.","Deep learning; Generative adversarial networks; Attention; Blending techniques; Environmental factors; High resolution; Image blending; Images synthesis; Learning models; Public dataset; Synthetic datasets; Target scenes; Fires","Attention; Generative adversarial network; Image blending; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85146623402"
"Finck T.; Li H.; Schlaeger S.; Grundl L.; Sollmann N.; Bender B.; Bürkle E.; Zimmer C.; Kirschke J.; Menze B.; Mühlau M.; Wiestler B.","Finck, Tom (56019522400); Li, Hongwei (57007362000); Schlaeger, Sarah (57193263109); Grundl, Lioba (8614952100); Sollmann, Nico (55553623900); Bender, Benjamin (26639042800); Bürkle, Eva (57202778090); Zimmer, Claus (7102861407); Kirschke, Jan (57184124500); Menze, Björn (35299840300); Mühlau, Mark (14070737100); Wiestler, Benedikt (40462696100)","56019522400; 57007362000; 57193263109; 8614952100; 55553623900; 26639042800; 57202778090; 7102861407; 57184124500; 35299840300; 14070737100; 40462696100","Uncertainty-Aware and Lesion-Specific Image Synthesis in Multiple Sclerosis Magnetic Resonance Imaging: A Multicentric Validation Study","2022","Frontiers in Neuroscience","16","","889808","","","","10.3389/fnins.2022.889808","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131337451&doi=10.3389%2ffnins.2022.889808&partnerID=40&md5=0910653e70cf968f0a279e4315952fef","Generative adversarial networks (GANs) can synthesize high-contrast MRI from lower-contrast input. Targeted translation of parenchymal lesions in multiple sclerosis (MS), as well as visualization of model confidence further augment their utility, provided that the GAN generalizes reliably across different scanners. We here investigate the generalizability of a refined GAN for synthesizing high-contrast double inversion recovery (DIR) images and propose the use of uncertainty maps to further enhance its clinical utility and trustworthiness. A GAN was trained to synthesize DIR from input fluid-attenuated inversion recovery (FLAIR) and T1w of 50 MS patients (training data). In another 50 patients (test data), two blinded readers (R1 and R2) independently quantified lesions in synthetic DIR (synthDIR), acquired DIR (trueDIR) and FLAIR. Of the 50 test patients, 20 were acquired on the same scanner as training data (internal data), while 30 were scanned at different scanners with heterogeneous field strengths and protocols (external data). Lesion-to-Background ratios (LBR) for MS-lesions vs. normal appearing white matter, as well as image quality parameters were calculated. Uncertainty maps were generated to visualize model confidence. Significantly more MS-specific lesions were found in synthDIR compared to FLAIR (R1: 26.7 ± 2.6 vs. 22.5 ± 2.2 p < 0.0001; R2: 22.8 ± 2.2 vs. 19.9 ± 2.0, p = 0.0005). While trueDIR remained superior to synthDIR in R1 [28.6 ± 2.9 vs. 26.7 ± 2.6 (p = 0.0021)], both sequences showed comparable lesion conspicuity in R2 [23.3 ± 2.4 vs. 22.8 ± 2.2 (p = 0.98)]. Importantly, improvements in lesion counts were similar in internal and external data. Measurements of LBR confirmed that lesion-focused GAN training significantly improved lesion conspicuity. The use of uncertainty maps furthermore helped discriminate between MS lesions and artifacts. In conclusion, this multicentric study confirms the external validity of a lesion-focused Deep-Learning tool aimed at MS imaging. When implemented, uncertainty maps are promising to increase the trustworthiness of synthetic MRI. Copyright © 2022 Finck, Li, Schlaeger, Grundl, Sollmann, Bender, Bürkle, Zimmer, Kirschke, Menze, Mühlau and Wiestler.","Article; clinical feature; contrast enhancement; deep learning; double inversion recovery; fluid attenuated inversion recovery; human; image quality; image synthesis; imaging and display; lesion to background ratio; machine learning; multiple sclerosis; neuroimaging; nuclear magnetic resonance imaging; validation study; white matter","artificial intelligence (AI); deep learning – artificial neural network (DL-ANN); double inversion recovery (DIR); magnetic resonance imaging; multiple sclerosis; neuroradiology; synthetic MRI","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131337451"
"Zhang X.; Zhang F.; Xu C.","Zhang, Xi (57567109500); Zhang, Feifei (57138854900); Xu, Changsheng (56153258200)","57567109500; 57138854900; 56153258200","Joint Expression Synthesis and Representation Learning for Facial Expression Recognition","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","3","","1681","1695","14","10.1109/TCSVT.2021.3056098","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100754640&doi=10.1109%2fTCSVT.2021.3056098&partnerID=40&md5=3c6884b67fb2c6566f6a7eb8bee58de7","Facial expression recognition (FER) is a challenging task due to the large appearance variations and the lack of sufficient training data. Conventional deep approaches either learn a good representation through deep models or synthesize images automatically to enlarge the training set. In this paper, we perform both tasks jointly and propose an end-to-end deep model for simultaneous facial expression recognition and facial image synthesis. The proposed model is based on Generative Adversarial Network (GAN) and enjoys several merits. First, the facial image synthesis and facial expression recognition tasks can boost their performance for each other via the unified model. Second, paired images are not required in our facial image synthesis network, which makes the proposed model much more general and flexible. Meanwhile, the generated facial images largely expand the training set and ease the overfitting problem in our FER task. Third, different expressions are encoded in a disentangled manner in a latent space, which enables us to synthesize facial images with arbitrary expressions by exchanging certain parts of their latent identity features. Quantitative and qualitative evaluations on both controlled and in-the-wild FER benchmarks (Multi-PIE, MMI, and RAF-DB) demonstrate the effectiveness of our proposed method on both facial image synthesis and facial expression recognition task.  © 1991-2012 IEEE.","Networks (circuits); Video signal processing; Adversarial networks; Expression synthesis; Facial expression recognition; Facial Image synthesis; Over fitting problem; Qualitative evaluations; Training data; Unified Modeling; Face recognition","Facial expression recognition; facial image synthesis; generative adversarial network; representation learning","Article","Final","","Scopus","2-s2.0-85100754640"
"Mao Q.; Tseng H.-Y.; Lee H.-Y.; Huang J.-B.; Ma S.; Yang M.-H.","Mao, Qi (57193140237); Tseng, Hung-Yu (57204283094); Lee, Hsin-Ying (57207324989); Huang, Jia-Bin (21742461700); Ma, Siwei (34872761500); Yang, Ming-Hsuan (7404927015)","57193140237; 57204283094; 57207324989; 21742461700; 34872761500; 7404927015","Continuous and Diverse Image-to-Image Translation via Signed Attribute Vectors","2022","International Journal of Computer Vision","130","2","","517","549","32","10.1007/s11263-021-01557-6","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122473645&doi=10.1007%2fs11263-021-01557-6&partnerID=40&md5=2800b97eca66e772db3b34138ce061d7","Recent image-to-image (I2I) translation algorithms focus on learning the mapping from a source to a target domain. However, the continuous translation problem that synthesizes intermediate results between two domains has not been well-studied in the literature. Generating a smooth sequence of intermediate results bridges the gap of two different domains, facilitating the morphing effect across domains. Existing I2I approaches are limited to either intra-domain or deterministic inter-domain continuous translation. In this work, we present an effectively signed attribute vector, which enables continuous translation on diverse mapping paths across various domains. In particular, we introduce a unified attribute space shared by all domains that utilize the sign operation to encode the domain information, thereby allowing the interpolation on attribute vectors of different domains. To enhance the visual quality of continuous translation results, we generate a trajectory between two sign-symmetrical attribute vectors and leverage the domain information of the interpolated results along the trajectory for adversarial training. We evaluate the proposed method on a wide range of I2I translation tasks. Both qualitative and quantitative results demonstrate that the proposed framework generates more high-quality continuous translation results against the state-of-the-art methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Generative adversarial networks; Image processing; Vector spaces; Vectors; Attribute vectors; Different domains; Domain informations; Image translation; Image-to-image translation; Images synthesis; Intermediate results; Target domain; Translation algorithms; Two domains; Mapping","Generative adversarial networks; Image synthesis; Image-to-image translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122473645"
"Luo X.; Chen X.; He X.; Qing L.; Tan X.","Luo, Xiaodong (57211069189); Chen, Xiang (57200532416); He, Xiaohai (9237988800); Qing, Linbo (23971364800); Tan, Xinyue (57871818900)","57211069189; 57200532416; 9237988800; 23971364800; 57871818900","CMAFGAN: A Cross-Modal Attention Fusion based Generative Adversarial Network for attribute word-to-face synthesis","2022","Knowledge-Based Systems","255","","109750","","","","10.1016/j.knosys.2022.109750","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137165330&doi=10.1016%2fj.knosys.2022.109750&partnerID=40&md5=0ad72cdddb4738f11972b2730e32f997","Face synthesis based on attribute words is a novel and challenging topic in computer vision, which has various application potentials in public security and multimedia. Existing attribute vector-to-face (V2F) synthesis methods mainly generate faces based on attribute label vectors that lack rich semantic feature information, which leads to low-quality generated face images. To address this challenge, we advocate attribute word-to-face (W2F) synthesis, using attribute-word sequences that contain rich semantic information as input. A novel Cross-Modal Attention Fusion based Generative Adversarial Network (CMAFGAN) is proposed to generate faces from facial attribute words. CMAFGAN is highlighted by two blocks, cross-modal attention fusion (CMAF) and word feature transformation (WFT), which are proposed to explore the correlation between image features and the corresponding attribute word features. Experimental results on the CelebA and LFW datasets demonstrate that our CMAFGAN achieves state-of-the-art performance, effectively improving the quality of the synthesised faces. In particular, the consistency between the predicted images and input attribute words (R-precision) on the CelebA and LFW datasets achieved 61.24% and 64.46% respectively, which is significantly better than previous methods. In addition, CMAFGAN achieves comparable or better performance than the current best methods of text-to-image synthesis (R-precision 83.41% on caltech-ucsd birds-200-2011, CUB). © 2022 Elsevier B.V.","Image processing; Semantics; Attribute vectors; Attribute word-to-face synthesis; Conditional generative adversarial network; Cross-modal; Cross-modal attention fusion; Face synthesis; Feature transformations; Public security; Word feature transformation; Generative adversarial networks","Attribute word-to-face synthesis; Conditional generative adversarial network; Cross-modal attention fusion; Face synthesis; Word feature transformation","Article","Final","","Scopus","2-s2.0-85137165330"
"Lee M.; Seok J.","Lee, Minhyeok (57194701375); Seok, Junhee (24069490100)","57194701375; 24069490100","Score-Guided Generative Adversarial Networks","2022","Axioms","11","12","701","","","","10.3390/axioms11120701","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144729812&doi=10.3390%2faxioms11120701&partnerID=40&md5=3cf457ed73b0a22ca4901627d928335a","We propose a generative adversarial network (GAN) that introduces an evaluator module using pretrained networks. The proposed model, called a score-guided GAN (ScoreGAN), is trained using an evaluation metric for GANs, i.e., the Inception score, as a rough guide for the training of the generator. Using another pretrained network instead of the Inception network, ScoreGAN circumvents overfitting of the Inception network such that the generated samples do not correspond to adversarial examples of the Inception network. In addition, evaluation metrics are employed only in an auxiliary role to prevent overfitting. When evaluated using the CIFAR-10 dataset, ScoreGAN achieved an Inception score of 10.36 ± 0.15, which corresponds to state-of-the-art performance. To generalize the effectiveness of ScoreGAN, the model was evaluated further using another dataset, CIFAR-100. ScoreGAN outperformed other existing methods, achieving a Fréchet Inception distance (FID) of 13.98. © 2022 by the authors.","","GAN; generative adversarial network; generative model; image generation; image synthesis; Inception score; scoreGAN","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144729812"
"Wei D.; Huang K.; Ma L.; Hua J.; Lai B.; Shen H.","Wei, Dongxu (57219750324); Huang, Kejie (54581052800); Ma, Liyuan (57370602600); Hua, Jiashen (57204468283); Lai, Baisheng (56051393700); Shen, Haibin (13309317500)","57219750324; 54581052800; 57370602600; 57204468283; 56051393700; 13309317500","OAW-GAN: occlusion-aware warping GAN for unified human video synthesis","2023","Applied Intelligence","53","1","","616","633","17","10.1007/s10489-022-03527-y","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128471416&doi=10.1007%2fs10489-022-03527-y&partnerID=40&md5=03c45569289ac50bc6d7568383961254","In this paper, we propose Occlusion-Aware Warping GAN (OAW-GAN), a unified Human Video Synthesis (HVS) framework that can uniformly tackle human video motion transfer, attribute editing, as well as inpainting. This is the first work to our knowledge that can handle all these tasks within a one-time trained model. Although existing GAN-based HVS methods have achieved great success, they either can’t preserve appearance details due to the loss of spatial consistency between the synthesized target frames and the input source images, or generate incoherent video results due to the loss of temporal consistency among frames. Besides, most of them lack the ability to create new contents while keeping existing ones, failing especially when some regions in the target are invisible in the source due to self-occlusion. To address these limitations, we first introduce Coarse-to-Fine Flow Warping Network (C2F-FWN) to estimate spatial-temporal consistent transformation between source and target, as well as occlusion mask indicating which parts in the target are invisible in the source. Then, the flow and the mask are scaled and fed into the pyramidal stages of our OAW-GAN, guiding Occlusion-Aware Synthesis (OAS) that can be abstracted into visible part re-utilization and invisible part inpainting at the feature level, which effectively alleviates the self-occlusion problem. Extensive experiments conducted on both human video (i.e., iPER, SoloDance)Keywords are desired. please provide if necessary. and image (i.e., DeepFashion) datasets demonstrate the superiority of our approach to existing state-of-the-arts. We also show that, besides motion transfer task that previous works concern, our framework can further achieve attribute editing and texture inpainting, which paves the way towards unified HVS. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Arts computing; Textures; Images synthesis; Inpainting; Motion transfer; Self occlusion; Spatial consistency; Synthesis method; Synthesised; Video motion; Video synthesis; Warpings; Generative adversarial networks","Generative adversarial networks; Image synthesis; Motion transfer; Video synthesis","Article","Final","","Scopus","2-s2.0-85128471416"
"Tango K.; Katsurai M.; Maki H.; Goto R.","Tango, Koya (57219795669); Katsurai, Marie (36866419100); Maki, Hayato (57533776900); Goto, Ryosuke (57219495526)","57219795669; 36866419100; 57533776900; 57219495526","Anime-to-real clothing: Cosplay costume generation via image-to-image translation","2022","Multimedia Tools and Applications","81","20","","29505","29523","18","10.1007/s11042-022-12576-x","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127596221&doi=10.1007%2fs11042-022-12576-x&partnerID=40&md5=0875097980928e5cf4ebb1d013dad3f3","Cosplay has grown from its origins at fan conventions into a billion-dollar global dress phenomenon. To facilitate the imagination and reinterpretation of animated images as real garments, this paper presents an automatic costume-image generation method based on image-to-image translation. Cosplay items can be significantly diverse in their styles and shapes, and conventional methods cannot be directly applied to the wide variety of clothing images that are the focus of this study. To solve this problem, our method starts by collecting and preprocessing web images to prepare a cleaned, paired dataset of the anime and real domains. Then, we present a novel architecture for generative adversarial networks (GANs) to facilitate high-quality cosplay image generation. Our GAN consists of several effective techniques to bridge the two domains and improve both the global and local consistency of generated images. Experiments demonstrated that, with quantitative evaluation metrics, the proposed GAN performs better and produces more realistic images than conventional methods. Our codes and pretrained model are available on the web. © 2022, The Author(s).","Image enhancement; Clothing image; Conventional methods; Cosplay; Dataset construction; Generation method; Image generations; Image translation; Image-to-image translation; Images synthesis; Web images; Generative adversarial networks","Clothing images; Dataset construction; Generative adversarial networks; Image synthesis; Image-to-image translation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127596221"
"Lei H.; Tian Z.; Xie H.; Zhao B.; Zeng X.; Cao J.; Liu W.; Wang J.; Zhang G.; Wang S.; Lei B.","Lei, Haijun (12753670000); Tian, Zhihui (57275371800); Xie, Hai (57193761800); Zhao, Benjian (57220585831); Zeng, Xianlu (57219326153); Cao, Jiuwen (35274436100); Liu, Weixin (57220588892); Wang, Jiantao (55742530900); Zhang, Guoming (55738989600); Wang, Shuqiang (53872228000); Lei, Baiying (26422280400)","12753670000; 57275371800; 57193761800; 57220585831; 57219326153; 35274436100; 57220588892; 55742530900; 55738989600; 53872228000; 26422280400","LAC-GAN: Lesion attention conditional GAN for Ultra-widefield image synthesis","2023","Neural Networks","158","","","89","98","9","10.1016/j.neunet.2022.11.005","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142736760&doi=10.1016%2fj.neunet.2022.11.005&partnerID=40&md5=d33760cf29ea66c26c9bd198eab83b1e","Automatic detection of retinal diseases based on deep learning technology and Ultra-widefield (UWF) images plays an important role in clinical practices in recent years. However, due to small lesions and limited data samples, it is not easy to train a detection-accurate model with strong generalization ability. In this paper, we propose a lesion attention conditional generative adversarial network (LAC-GAN) to synthesize retinal images with realistic lesion details to improve the training of the disease detection model. Specifically, the generator takes the vessel mask and class label as the conditional inputs, and processes the random Gaussian noise by a series of residual block to generate the synthetic images. To focus on pathological information, we propose a lesion feature attention mechanism based on random forest (RF) method, which constructs its reverse activation network to activate the lesion features. For discriminator, a weight-sharing multi-discriminator is designed to improve the performance of model by affine transformations. Experimental results on multi-center UWF image datasets demonstrate that the proposed method can generate retinal images with reasonable details, which helps to enhance the performance of the disease detection model. © 2022 Elsevier Ltd","Generalization, Psychological; Image Processing, Computer-Assisted; Decision trees; Deep learning; Gaussian noise (electronic); Image enhancement; Ophthalmology; Automatic Detection; Conditional GAN; Detection models; Disease detection; Images synthesis; Lesion attention; Performance; Retinal image; Ultra-widefield image; Wide-field; image processing; procedures; Generative adversarial networks","Conditional GAN; Disease detection; Lesion attention; Ultra-widefield image","Article","Final","","Scopus","2-s2.0-85142736760"
"Kim M.; Kim Y.N.; Jang M.; Hwang J.; Kim H.-K.; Yoon S.C.; Kim Y.J.; Kim N.","Kim, Mingyu (57213155752); Kim, You Na (57202800019); Jang, Miso (57928627600); Hwang, Jeongeun (46062004200); Kim, Hong-Kyu (48761449900); Yoon, Sang Chul (55387693000); Kim, Yoon Jeon (35761498000); Kim, Namkug (16550058300)","57213155752; 57202800019; 57928627600; 46062004200; 48761449900; 55387693000; 35761498000; 16550058300","Synthesizing realistic high-resolution retina image by style-based generative adversarial network and its utilization","2022","Scientific Reports","12","1","17307","","","","10.1038/s41598-022-20698-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139888168&doi=10.1038%2fs41598-022-20698-3&partnerID=40&md5=7ab4d9351890f5c3e81ada1bdbf39080","Realistic image synthesis based on deep learning is an invaluable technique for developing high-performance computer aided diagnosis systems while protecting patient privacy. However, training a generative adversarial network (GAN) for image synthesis remains challenging because of the large amounts of data required for training various kinds of image features. This study aims to synthesize retinal images indistinguishable from real images and evaluate the efficacy of the synthesized images having a specific disease for augmenting class imbalanced datasets. The synthesized images were validated via image Turing tests, qualitative analysis by retinal specialists, and quantitative analyses on amounts and signal-to-noise ratios of vessels. The efficacy of synthesized images was verified by deep learning-based classification performance. Turing test shows that accuracy, sensitivity, and specificity of 54.0 ± 12.3%, 71.1 ± 18.8%, and 36.9 ± 25.5%, respectively. Here, sensitivity represents correctness to find real images among real datasets. Vessel amounts and average SNR comparisons show 0.43% and 1.5% difference between real and synthesized images. The classification performance after augmenting synthesized images outperforms every ratio of imbalanced real datasets. Our study shows the realistic retina images were successfully generated with insignificant differences between the real and synthesized images and shows great potential for practical applications. © 2022, The Author(s).","Humans; Image Processing, Computer-Assisted; Retina; Signal-To-Noise Ratio; diagnostic imaging; human; image processing; procedures; retina; signal noise ratio","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139888168"
"Wang L.; Nie J.; Wang R.; Zhai L.","Wang, Lina (55899978500); Nie, Jiansi (57641967300); Wang, Run (55939516400); Zhai, Liming (57640452600)","55899978500; 57641967300; 55939516400; 57640452600","Analyzing deepfake provenance and forensics; [面向深度伪造的溯源取证方法]","2022","Qinghua Daxue Xuebao/Journal of Tsinghua University","62","5","","959","964","5","10.16511/j.cnki.qhdxxb.2022.21.001","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128820648&doi=10.16511%2fj.cnki.qhdxxb.2022.21.001&partnerID=40&md5=93d90d483e47479d244d40dd2120a5de","In recent years, the rapid development of generative adversarial networks (GAN) has made synthesized images more and more realistic, which poses great threats to individuals and society. Existing research has focused on passively identifying deepfakes, but real-world applications are usually insufficiently general and robust. This paper presents a method for deepfake provenance and forensics. Deepfakes hide secret information in facial images to track the source of the forged image. An end-to-end deep neural network was designed to include an embedding network, a GAN simulator, and a recovery network. The embedding network embeds the secret information in the picture while the recovery network extracts the information. The GAN simulator simulates various GAN-based image transformations. The average normalized cross correlation coefficient (NCC) of the restored images after tampering with known GANs is higher than 0.9 and the average NCC reaches around 0.8 with tampering by unknown GANs, which shows good robustness and generalization. In addition, the secret embedded information is well concealed and the average peak signal to noise ratio (PSNR) is about 30 dB. © 2022, Tsinghua University Press. All right reserved.","Computer system recovery; Deep neural networks; Digital forensics; Image processing; Network embeddings; Signal to noise ratio; Cross-correlation coefficient; Deepfake; Embedding network; Image manipulation; Images synthesis; Network simulators; Normalized cross-correlation; Provenance and forensic; Recovery network; Secret information; Generative adversarial networks","Deepfakes; Image synthesis and manipulation; Provenance and forensics","Article","Final","","Scopus","2-s2.0-85128820648"
"Ibrahem H.; Salem A.; Kang H.-S.","Ibrahem, Hatem (57216133273); Salem, Ahmed (57216273337); Kang, Hyun-Soo (35332572200)","57216133273; 57216273337; 35332572200","Exploration of Semantic Label Decomposition and Dataset Size in Semantic Indoor Scenes Synthesis via Optimized Residual Generative Adversarial Networks","2022","Sensors","22","21","8306","","","","10.3390/s22218306","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141604513&doi=10.3390%2fs22218306&partnerID=40&md5=8cdb68a4d324a1b6bba4f5ba0342ac22","In this paper, we revisit the paired image-to-image translation using the conditional generative adversarial network, the so-called “Pix2Pix”, and propose efficient optimization techniques for the architecture and the training method to maximize the architecture’s performance to boost the realism of the generated images. We propose a generative adversarial network-based technique to create new artificial indoor scenes using a user-defined semantic segmentation map as an input to define the location, shape, and category of each object in the scene, exactly similar to Pix2Pix. We train different residual connections-based architectures of the generator and discriminator on the NYU depth-v2 dataset and a selected indoor subset from the ADE20K dataset, showing that the proposed models have fewer parameters, less computational complexity, and can generate better quality images than the state of the art methods following the same technique to generate realistic indoor images. We also prove that using extra specific labels and more training samples increases the quality of the generated images; however, the proposed residual connections-based models can learn better from small datasets (i.e., NYU depth-v2) and can improve the realism of the generated images in training on bigger datasets (i.e., ADE20K indoor subset) in comparison to Pix2Pix. The proposed method achieves an LPIPS value of 0.505 and an FID value of 81.067, generating better quality images than that produced by Pix2Pix and other recent paired Image-to-image translation methods and outperforming them in terms of LPIPS and FID. © 2022 by the authors.","Convolutional neural networks; Image enhancement; Network architecture; Semantic Segmentation; Semantic Web; Semantics; Convolutional neural network; Data set size; Efficient optimisation; Image translation; Image-to-image translation; Images synthesis; Quality image; Semantic image synthesis; Semantic images; Semantic labels; antibody dependent enhancement; article; convolutional neural network; decomposition; synthesis; Generative adversarial networks","convolutional neural networks; generative adversarial networks; image-to-image translation; semantic image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141604513"
"Li S.; Zhao X.","Li, Shengyuan (57193431724); Zhao, Xuefeng (8845957900)","57193431724; 8845957900","High-resolution concrete damage image synthesis using conditional generative adversarial network","2023","Automation in Construction","147","","104739","","","","10.1016/j.autcon.2022.104739","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145781291&doi=10.1016%2fj.autcon.2022.104739&partnerID=40&md5=c22f79cd31cfb09ea155804bbc4df0e0","Concrete damage images are essential for training deep learning-based damage detection networks. Considering the manual collection of concrete damage images is time-consuming and labor-intensive, this study proposes a synthesis method for high-resolution concrete damage images using a conditional generative adversarial network (CGAN). To this end, pix2pix, CycleGAN, OASIS, and pix2pixHD with various hyperparameters were trained and tested on 500 concrete crack and spalling images. The test results show that the trained pix2pixHD with λpix2pixHD = 15 is the best CGAN for concrete damage image synthesis. Concrete damage images were synthesized by the best CGAN according to hand-painted damage maps and used to train deep learning networks. The results show that the synthesized images have excellent authenticity and can be used to train and test deep learning-based concrete damage detection networks. The proposed method can be enhanced by adding damage images to the existing database or employing a better CGAN generator. © 2023 Elsevier B.V.","Concrete testing; Damage detection; Deep learning; Image enhancement; Concrete damages; Conditional generative adversarial network; Damage images; Deep learning; Detection networks; High resolution; High-resolution image synthesis; High-resolution images; Images synthesis; Labour-intensive; Generative adversarial networks","Concrete damage; Conditional generative adversarial network; Deep learning; High-resolution image synthesis","Article","Final","","Scopus","2-s2.0-85145781291"
"Shi Z.; Shen Y.; Zhu J.; Yeung D.-Y.; Chen Q.","Shi, Zifan (57223823296); Shen, Yujun (57207766466); Zhu, Jiapeng (57215047797); Yeung, Dit-Yan (7103391392); Chen, Qifeng (55365826000)","57223823296; 57207766466; 57215047797; 7103391392; 55365826000","3D-Aware Indoor Scene Synthesis with Depth Priors","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13676 LNCS","","","406","422","16","10.1007/978-3-031-19787-1_23","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142748617&doi=10.1007%2f978-3-031-19787-1_23&partnerID=40&md5=0cb289942558e2d0b0c12a5011ab8651","Despite the recent advancement of Generative Adversarial Networks (GANs) in learning 3D-aware image synthesis from 2D data, existing methods fail to model indoor scenes due to the large diversity of room layouts and the objects inside. We argue that indoor scenes do not have a shared intrinsic structure, and hence only using 2D images cannot adequately guide the model with the 3D geometry. In this work, we fill in this gap by introducing depth as a 3D prior (Depth is essentially a 2.5D prior, but in this paper we use 3D for simplicity). Compared with other 3D data formats, depth better fits the convolution-based generation mechanism and is more easily accessible in practice. Specifically, we propose a dual-path generator, where one path is responsible for depth generation, whose intermediate features are injected into the other path as the condition for appearance rendering. Such a design eases the 3D-aware synthesis with explicit geometry information. Meanwhile, we introduce a switchable discriminator both to differentiate real v.s. fake domains and to predict the depth from a given input. In this way, the discriminator can take the spatial arrangement into account and advise the generator to learn an appropriate depth condition. Extensive experimental results suggest that our approach is capable of synthesizing indoor scenes with impressively good quality and 3D consistency, significantly outperforming state-of-the-art alternatives. (Project page can be found here.) © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","3D modeling; 2d datum; 2D images; 3D data; 3D geometry; 3d-aware image synthesis; Condition; Depth prior; Images synthesis; Intrinsic structures; Scene synthesis; Generative adversarial networks","3D-aware image synthesis; Depth priors; Scene synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142748617"
"Abdelmotaal H.; Sharaf M.; Soliman W.; Wasfi E.; Kedwany S.M.","Abdelmotaal, Hazem (57205432743); Sharaf, Mohamed (57210096650); Soliman, Wael (7801683757); Wasfi, Ehab (26021575400); Kedwany, Salma M. (57223269293)","57205432743; 57210096650; 7801683757; 26021575400; 57223269293","Bridging the resources gap: deep learning for fluorescein angiography and optical coherence tomography macular thickness map image translation","2022","BMC Ophthalmology","22","1","355","","","","10.1186/s12886-022-02577-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137110259&doi=10.1186%2fs12886-022-02577-7&partnerID=40&md5=886df230cf6c8e8d4fd66404cd125c8b","Background: To assess the ability of the pix2pix generative adversarial network (pix2pix GAN) to synthesize clinically useful optical coherence tomography (OCT) color-coded macular thickness maps based on a modest-sized original fluorescein angiography (FA) dataset and the reverse, to be used as a plausible alternative to either imaging technique in patients with diabetic macular edema (DME). Methods: Original images of 1,195 eyes of 708 nonconsecutive diabetic patients with or without DME were retrospectively analyzed. OCT macular thickness maps and corresponding FA images were preprocessed for use in training and testing the proposed pix2pix GAN. The best quality synthesized images using the test set were selected based on the Fréchet inception distance score, and their quality was studied subjectively by image readers and objectively by calculating the peak signal-to-noise ratio, structural similarity index, and Hamming distance. We also used original and synthesized images in a trained deep convolutional neural network (DCNN) to plot the difference between synthesized images and their ground-truth analogues and calculate the learned perceptual image patch similarity metric. Results: The pix2pix GAN-synthesized images showed plausible subjectively and objectively assessed quality, which can provide a clinically useful alternative to either image modality. Conclusion: Using the pix2pix GAN to synthesize mutually dependent OCT color-coded macular thickness maps or FA images can overcome issues related to machine unavailability or clinical situations that preclude the performance of either imaging technique. Trial registration: ClinicalTrials.gov Identifier: NCT05105620, November 2021. “Retrospectively registered”. © 2022, The Author(s).","","Color-coded macular thickness maps; Fluorescein angiography; Generative adversarial networks image synthesis; Optical coherence tomography; Pix2pix","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85137110259"
"Hu M.; Wu Y.; Song Y.; Yang J.; Zhang R.; Wang H.; Meng D.","Hu, Mingdi (56482609800); Wu, Yi (57757413300); Song, Yao (57696829400); Yang, Jingbing (57696201900); Zhang, Ruifang (57696516400); Wang, Hong (57195477222); Meng, Deyu (23393058400)","56482609800; 57757413300; 57696829400; 57696201900; 57696516400; 57195477222; 23393058400","The integrated evaluation and review of single image rain removal based datasets and deep learning methods; [单幅图像去雨数据集和深度学习算法的联合评估与展望]","2022","Journal of Image and Graphics","27","5","","1359","1391","32","10.11834/jig.211153","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130238354&doi=10.11834%2fjig.211153&partnerID=40&md5=86577f9e260c0cc61e811d8d2c8b9e4d","The visual quality of captured images in rainy weather conditions is constrained to the outdoor vision degradation. It is essential to design relevant rain removal algorithms. Due to the lack of temporal information, single image rain removal is challenging compared to video-based rain removal. The target of single image rain removal analysis is to restore the rain-removed background image from the corresponding rain-affected image. Current deep learning based vision tasks construct diverse data-driven frameworks like single image rain removal task via multiple network modules. Current research tasks are focusing on the quality of datasets, the design of single image deraining algorithms, the subsequent high-level vision tasks, and the design of performance evaluation metrics. Specifically, the quality of rain datasets largely affects the performance of deep learning based single image deraining methods, since the generalization ability of deep single image rain removal is highly related to the domain gap between synthesized training dataset and real testing dataset. Besides, rain removal plays an important preprocessing role in outdoor visual tasks because its result would affect the performance of the subsequent visual task. Additionally, the design of image quality assessment (IQA) metrics is quite important for the fair quantitative analysis of human perception of image quality in general image restoration tasks. We conducted critical literature review for deep learning based single image rain removal from the four aspects as mentioned below: 1) dataset generation in rain weather conditions; 2) representative deep neural network based single image rain removal algorithms; 3) the research of the downstream high-level task in rainy days and 4) performance metrics for evaluating single image rain removal algorithms. Specifically, in terms of the generation manners, the current rain image datasets are roughly divided into four categories as following: 1) synthesizing rain streaks based on photo-realistic rendering technique and then adding them on clear images based on simple physical model; 2) constructing rain images based on complex physical model via manual parameters setting; 3) generating rain images based on generative adversarial network (GAN); 4) collecting paired rain-free/rain-affected images by shooting different scenarios and adjusting camera parameters. We reviewed the download links of the existing representative rain image datasets. For deep learning based single image rain removal methods, we review the supervised and semi-/unsupervised rain removal methods for single task and joint tasks in terms of task scenarios, learning mechanisms and network design. Here, single task is relevant to rain drop removal, rain streak, rain fog, heavy rain; and the integrated analyses of removal of rain drop and rain streak, or multiple noises removal. Furthermore, we overview the construction manners of representative network architectures, including simplified convolutional neural networks based (CNNs-based) multi-branches architecture, GAN-based mechanism, recurrent and multi-stage framework, multi-scale architecture, the integration of encoder-decoder modules, attention mechanism or transformer based module as well as model-driven and data-driven learning manners. Since the implicit or explicit embedding of domain knowledge can promote network construction, we provide a detailed survey in the context of the relationship between rain removal methods and domain knowledge. We illustrated that the domain knowledge and the learning of benched networks has the potential to improve the generalization performance of single image rain removal algorithm further. Based on the real high-level outdoor vision tasks in rain weather, it would be meaningful to use the joint processing strategies of low-level and high-level tasks and the customized construction of rainy datasets. Meanwhile, we reviewed and clarified some related literatures of high-level computer vision tasks and comprehensively analyzed the performance evaluation metrics in the context of full-reference metrics and non-reference metrics. We analyzed the potential challenges of single image rain removal further in the context of feasible benchmark datasets construction, future fair evaluation metrics designing, and the optimized integration of rain removal and high-level vision tasks. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Deep neural network; Follow-up high-level vision task; Model-driven and data-driven methodology; Performance evaluation metrics; Rain image dataset; Rain image synthesis; Single image rain removal","Review","Final","","Scopus","2-s2.0-85130238354"
"Amirrajab S.; Al Khalil Y.; Lorenz C.; Weese J.; Pluim J.; Breeuwer M.","Amirrajab, Sina (57211821982); Al Khalil, Yasmina (57513053600); Lorenz, Cristian (55486269900); Weese, Jürgen (7005746479); Pluim, Josien (6701614327); Breeuwer, Marcel (7004252845)","57211821982; 57513053600; 55486269900; 7005746479; 6701614327; 7004252845","Label-informed cardiac magnetic resonance image synthesis through conditional generative adversarial networks","2022","Computerized Medical Imaging and Graphics","101","","102123","","","","10.1016/j.compmedimag.2022.102123","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138481863&doi=10.1016%2fj.compmedimag.2022.102123&partnerID=40&md5=584c297b34fec826845a64ff009c746c","Synthesis of a large set of high-quality medical images with variability in anatomical representation and image appearance has the potential to provide solutions for tackling the scarcity of properly annotated data in medical image analysis research. In this paper, we propose a novel framework consisting of image segmentation and synthesis based on mask-conditional GANs for generating high-fidelity and diverse Cardiac Magnetic Resonance (CMR) images. The framework consists of two modules: i) a segmentation module trained using a physics-based simulated database of CMR images to provide multi-tissue labels on real CMR images, and ii) a synthesis module trained using pairs of real CMR images and corresponding multi-tissue labels, to translate input segmentation masks to realistic-looking cardiac images. The anatomy of synthesized images is based on labels, whereas the appearance is learned from the training images. We investigate the effects of the number of tissue labels, quantity of training data, and multi-vendor data on the quality of the synthesized images. Furthermore, we evaluate the effectiveness and usability of the synthetic data for a downstream task of training a deep-learning model for cardiac cavity segmentation in the scenarios of data replacement and augmentation. The results of the replacement study indicate that segmentation models trained with only synthetic data can achieve comparable performance to the baseline model trained with real data, indicating that the synthetic data captures the essential characteristics of its real counterpart. Furthermore, we demonstrate that augmenting real with synthetic data during training can significantly improve both the Dice score (maximum increase of 4%) and Hausdorff Distance (maximum reduction of 40%) for cavity segmentation, suggesting a good potential to aid in tackling medical data scarcity. © 2022 The Author(s)","Databases, Factual; Heart; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Deep learning; Generative adversarial networks; Heart; Magnetic resonance imaging; Medical imaging; Quality control; Semantic Segmentation; Semantics; Cardiac image synthesis; Cardiac images; Cardiac MRI; Conditional GAN; Images segmentations; Images simulations; Images synthesis; Semantic image synthesis; Semantic images; anatomy; Article; cardiac imaging; cardiovascular magnetic resonance; clinical effectiveness; controlled study; human; image segmentation; synthesis; usability; diagnostic imaging; factual database; heart; image processing; nuclear magnetic resonance imaging; procedures; Tissue","Cardiac image synthesis; Cardiac MRI; Conditional GANs; Image segmentation; Image simulation; Semantic image synthesis","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85138481863"
"Deshpande S.; Minhas F.; Graham S.; Rajpoot N.","Deshpande, Srijay (57206158392); Minhas, Fayyaz (24399575300); Graham, Simon (56927520600); Rajpoot, Nasir (8042017200)","57206158392; 24399575300; 56927520600; 8042017200","SAFRON: Stitching Across the Frontier Network for Generating Colorectal Cancer Histology Images","2022","Medical Image Analysis","77","","102337","","","","10.1016/j.media.2021.102337","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122504442&doi=10.1016%2fj.media.2021.102337&partnerID=40&md5=d675beead458fe1aa60ca725f8dec0a8","Automated synthesis of histology images has several potential applications including the development of data-efficient deep learning algorithms. In the field of computational pathology, where histology images are large in size and visual context is crucial, synthesis of large high-resolution images via generative modeling is an important but challenging task due to memory and computational constraints. To address this challenge, we propose a novel framework called SAFRON (Stitching Across the FROntier Network) to construct realistic, large high-resolution tissue images conditioned on input tissue component masks. The main novelty in the framework is integration of stitching in its loss function which enables generation of images of arbitrarily large sizes after training on relatively small image patches while preserving morphological features with minimal boundary artifacts. We have used the proposed framework for generating, to the best of our knowledge, the largest-sized synthetic histology images to date (up to 11K×8K pixels). Compared to existing approaches, our framework is efficient in terms of the memory required for training and computations needed for synthesizing large high-resolution images. The quality of generated images was assessed quantitatively using Frechet Inception Distance as well as by 7 trained pathologists, who assigned a realism score to a set of images generated by SAFRON. The average realism score across all pathologists for synthetic images was as high as that of real images. We also show that training with additional synthetic data generated by SAFRON can significantly boost prediction performance of gland segmentation and cancer detection algorithms in colorectal cancer histology images. © 2021","Algorithms; Colorectal Neoplasms; Deep Learning; Histological Techniques; Humans; Image Processing, Computer-Assisted; Deep learning; Diseases; Histology; Image annotation; Image segmentation; Learning algorithms; Pathology; Tissue; Annotated data generation; Automated synthesis; Computational pathology; Data generation; Deep learning; Generative model; High-resolution images; Histology images; Images synthesis; Visual context; Article; artifact; cancer morphology; colorectal cancer; conceptual framework; deep learning; detection algorithm; histopathology; human; human tissue; image analysis; image segmentation; pathologist; quantitative analysis; algorithm; colorectal tumor; diagnostic imaging; histology; image processing; procedures; Generative adversarial networks","Annotated data generation; Computational pathology; Deep learning; Generative adversarial networks; Image synthesis","Article","Final","","Scopus","2-s2.0-85122504442"
"Gan M.; Wang C.","Gan, Meng (57203724637); Wang, Cong (57195135042)","57203724637; 57195135042","Esophageal optical coherence tomography image synthesis using an adversarially learned variational autoencoder","2022","Biomedical Optics Express","13","3","","1188","1201","13","10.1364/BOE.449796","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124793188&doi=10.1364%2fBOE.449796&partnerID=40&md5=19e49fea1c5b4c3f59b3ff4fc2fe0e3c","Endoscopic optical coherence tomography (OCT) imaging offers a non-invasive way to detect esophageal lesions on the microscopic scale, which is of clinical potential in the early diagnosis and treatment of esophageal cancers. Recent studies focused on applying deep learning-based methods in esophageal OCT image analysis and achieved promising results, which require a large data size. However, traditional data augmentation techniques generate samples that are highly correlated and sometimes far from reality, which may not lead to a satisfied trained model. In this paper, we proposed an adversarial learned variational autoencoder (AL-VAE) to generate high-quality esophageal OCT samples. The AL-VAE combines the generative adversarial network (GAN) and variational autoencoder (VAE) in a simple yet effective way, which preserves the advantages of VAEs, such as stable training and nice latent manifold, and requires no extra discriminators. Experimental results verified the proposed method achieved better image quality in generating esophageal OCT images when compared with the state-of-the-art image synthesis network, and its potential in improving deep learning model performance was also evaluated by esophagus segmentation. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.","Deep learning; Diagnosis; Generative adversarial networks; Image enhancement; Image segmentation; Quality control; Auto encoders; Early diagnosis; Endoscopic optical coherence tomography; Esophageal cancer; Images synthesis; Learning-based methods; Microscopic scale; Non-invasive way; Tomography image analysis; Tomography imaging; article; autoencoder; controlled study; deep learning; esophagus; human; human tissue; image quality; optical coherence tomography; synthesis; Optical tomography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85124793188"
"Chen Y.; Jin C.; Li G.; Li T.H.; Gao W.","Chen, Yuanqi (57209029113); Jin, Cece (57218452351); Li, Ge (57208803757); Li, Thomas H. (57200647563); Gao, Wei (57200706579)","57209029113; 57218452351; 57208803757; 57200647563; 57200706579","Mitigating Label Noise in GANs via Enhanced Spectral Normalization","2023","IEEE Transactions on Circuits and Systems for Video Technology","","","","1","1","0","10.1109/TCSVT.2023.3235410","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147274267&doi=10.1109%2fTCSVT.2023.3235410&partnerID=40&md5=0eedebe81be20eb6e133529d7e92cbb2","Label noise is a ubiquitous issue in GANs, which degrades the generalization ability of the discriminator and usually leads to instability when training GANs. This issue stems from both real data and generated data. Previous works either only consider one of these two sources, or are not robust enough to noisy labels. In this paper, we revisit spectral normalization in robust learning with noisy labels. Based on its pros and cons, we propose to combine spectral normalization and weight decay to regularize the discriminator, which enjoys a more robust training process. To extend to conditional GANs, we propose to balance the relative importance of marginal matching and conditional matching in the projection discriminator. The proposed Enhanced Spectral Normalization for Generative Adversarial Networks (ESNGAN) can be easily integrated into various existing GANs frameworks without excessive additional cost. The effectiveness of the proposed method is validated on the CIFAR10, LSUN Church, CelebA, and ImageNet datasets, including the unconditional image generation task and the class-conditional image generation task. We also show that the proposed method can further improve the performance of the high-resolution image generation task. IEEE","Discriminators; Image enhancement; Job analysis; Personnel training; Generator; Images synthesis; Learning with noisy label; Neural-networks; Noise measurements; Noisy labels; Robust learning; Spectral normalization; Task analysis; Generative adversarial networks","Generative adversarial networks; Generative adversarial networks; Generators; image synthesis; Image synthesis; learning with noisy labels; Neural networks; Noise measurement; robust learning; Task analysis; Training","Article","Article in press","","Scopus","2-s2.0-85147274267"
"Eberhardt B.; Poser B.A.; Shah N.J.; Felder J.","Eberhardt, Boris (57220125073); Poser, Benedikt A. (14008932900); Shah, N. Jon (8508072500); Felder, Jörg (6602168204)","57220125073; 14008932900; 8508072500; 6602168204","","2022","Zeitschrift fur Medizinische Physik","32","3","","334","345","11","10.1016/j.zemedi.2021.12.003","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124178086&doi=10.1016%2fj.zemedi.2021.12.003&partnerID=40&md5=517ee97d80aff2bff87ab153844f547c","Spoke trajectory parallel transmit (pTX) excitation in ultra-high field MRI enables B1+ inhomogeneities arising from the shortened RF wavelength in biological tissue to be mitigated. To this end, current RF excitation pulse design algorithms either employ the acquisition of field maps with subsequent non-linear optimization or a universal approach applying robust pre-computed pulses. We suggest and evaluate an intermediate method that uses a subset of acquired field maps combined with generative machine learning models to reduce the pulse calibration time while offering more tailored excitation than robust pulses (RP). The possibility of employing image-to-image translation and semantic image synthesis machine learning models based on generative adversarial networks (GANs) to deduce the missing field maps is examined. Additionally, an RF pulse design that employs a predictive machine learning model to find solutions for the non-linear (two-spokes) pulse design problem is investigated. As a proof of concept, we present simulation results obtained with the suggested machine learning approaches that were trained on a limited data-set, acquired in vivo. The achieved excitation homogeneity based on a subset of half of the B1+ maps acquired in the calibration scans and half of the B1+ maps synthesized with GANs is comparable with state of the art pulse design methods when using the full set of calibration data while halving the total calibration time. By employing RP dictionaries or machine-learning RF pulse predictions, the total calibration time can be reduced significantly as these methods take only seconds or milliseconds per slice, respectively. © 2022","Algorithms; Brain; Calibration; Computer Simulation; Deep Learning; Magnetic Resonance Imaging; Phantoms, Imaging; article; calibration; excitation; in vivo study; machine learning; nuclear magnetic resonance imaging; prediction; proof of concept; simulation; synthesis; algorithm; brain; computer simulation; imaging phantom; nuclear magnetic resonance imaging; procedures","Machine learning; MRI; Parallel transmission; RF pulse design","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124178086"
"Gnanha A.T.; Cao W.; Mao X.; Wu S.; Wong H.-S.; Li Q.","Gnanha, Aurele Tohokantche (57226117399); Cao, Wenming (57197825879); Mao, Xudong (54883408900); Wu, Si (55495122900); Wong, Hau-San (7402864844); Li, Qing (57831747500)","57226117399; 57197825879; 54883408900; 55495122900; 7402864844; 57831747500","αβ-GAN: Robust generative adversarial networks","2022","Information Sciences","593","","","177","200","23","10.1016/j.ins.2022.01.073","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124413701&doi=10.1016%2fj.ins.2022.01.073&partnerID=40&md5=63ebfc7c1c35d0937883a1ecffcae937","Generative adversarial networks (GAN) training is subject to problems including mode collapse, gradient vanishing, and instability. Although many different losses have been proposed to alleviate these shortcomings, they heavily rely on a fixed-value function with limited expressive power in terms of robustness, whereby failing to perform consistently over multiple data sets. To solve this problem, we propose a parametric and robust αβ-loss function that can improve the performances of GAN on different data sets. Specifically, unlike standard GAN loss function it exploits the αβ-divergence (AB-divergence) to weigh the likelihood ratio associated with each data point. This weighing mechanism makes the model robust to noises and yields better models in terms of FID score. To reduce the cost of searching for the optimal α and β, we further propose an adaptive version to systematically update these parameters according to statistics of the discriminator's output. Moreover, αβ-loss can be reduced to Least Square GAN (LS-GAN) and standard GAN (SGAN) loss function as special cases. We conduct extensive experiments on both synthetic and real-world data sets. Experimental results over the synthetic data sets (2D Gaussian ring and grid) demonstrate that our approach can significantly alleviate the issue of mode collapse. Additionally, by constraining the gradient of the discriminator that is fed back to the generator via finely adjusting the hyper-parameters α and β, our approach can improve the quality of synthetic images, as can be seen from the decrease of FID from 40 to 23.71 on the data set CIFAR10 using the SN-DCGAN architecture. © 2022 Elsevier Inc.","Deep learning; Image enhancement; Data set; Deep learning; Expressive power; Images synthesis; Loss functions; Multiple data sets; Network loss; Network training; Performance; Value functions; Generative adversarial networks","Deep learning; Generative adversarial networks; Image synthesis","Article","Final","","Scopus","2-s2.0-85124413701"
"Hong K.-T.; Cho Y.; Kang C.H.; Ahn K.-S.; Lee H.; Kim J.; Hong S.J.; Kim B.H.; Shim E.","Hong, Ki-Taek (57468892300); Cho, Yongwon (57207994267); Kang, Chang Ho (56040650000); Ahn, Kyung-Sik (55092942700); Lee, Heegon (57468589900); Kim, Joohui (57468122300); Hong, Suk Joo (24528601900); Kim, Baek Hyun (55955818100); Shim, Euddeum (57191920200)","57468892300; 57207994267; 56040650000; 55092942700; 57468589900; 57468122300; 24528601900; 55955818100; 57191920200","Lumbar Spine Computed Tomography to Magnetic Resonance Imaging Synthesis Using Generative Adversarial Network: Visual Turing Test","2022","Diagnostics","12","2","530","","","","10.3390/diagnostics12020530","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125293494&doi=10.3390%2fdiagnostics12020530&partnerID=40&md5=06c18b93fbb5f9707e0a89fedfbab884","(1) Introduction: Computed tomography (CT) and magnetic resonance imaging (MRI) play an important role in the diagnosis and evaluation of spinal diseases, especially degenerative spinal diseases. MRI is mainly used to diagnose most spinal diseases because it shows a higher resolution than CT to distinguish lesions of the spinal canals and intervertebral discs. When it is inevitable for CT to be selected instead of MR in evaluating spinal disease, evaluation of spinal disease may be limited. In these cases, it is very helpful to diagnose spinal disease with MR images synthesized with CT images. (2) Objective: To create synthetic lumbar magnetic resonance (MR) images from computed tomography (CT) scans using generative adversarial network (GAN) models and assess how closely the synthetic images resembled the true images using visual Turing tests (VTTs). (3) Material and Methods: Overall, 285 patients aged ≥ 40 years who underwent lumbar CT and MRI were enrolled. Based on axial CT and T2-weighted axial MR images from 285 patients, an image synthesis model using a GAN was trained using three algorithms (unsupervised, semi-supervised, and supervised methods). Furthermore, VTT to determine how similar the synthetic lumbar MR images generated from lumbar CT axial images were to the true lumbar MR axial images were conducted with 59 patients who were not included in the model training. For the VTT, we designed an evaluation form comprising 600 randomly distributed axial images (150 true and 450 synthetic images from unsupervised, semi-supervised, and supervised methods). Four readers judged the authenticity of each image and chose their first-and second-choice candidates for the true image. In addition, for the three models, structural similarities (SSIM) were evaluated and the peak signal to noise ratio (PSNR) was compared among the three methods. (4) Results: The mean accuracy for the selection of true images for all four readers for their first choice was 52.0% (312/600). The accuracies of determining the true image for each reader’s first and first + second choices, respectively, were as follows: reader 1, 51.3% and 78.0%; reader 2, 38.7% and 62.0%, reader 3, 69.3% and 84.0%, and reader 4, 48.7% and 70.7%. In the case of synthetic images chosen as first and second choices, supervised algorithm-derived images were the most often selected (supervised, 118/600 first and 164/600 second; semi-supervised, 90/600 and 144/600; and unsupervised, 80/600 and 114/600). For image quality, the supervised algorithm received the best score (PSNR: 15.987 ± 1.039, SSIM: 0.518 ± 0.042). (5) Conclusion: This was the pilot study to apply GAN to synthesize lumbar spine MR images from CT images and compare training algorithms of the GAN. Based on VTT, the axial MR images synthesized from lumbar CT using GAN were fairly realistic and the supervised training algorithm was found to provide the closest image to true images. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","accuracy; adult; aged; algorithm; Article; computer assisted tomography; convolutional neural network; deep learning; diagnostic test; female; generative adversarial network; human; image reconstruction; imaging and display; lumbar computer assisted tomography  axial image; lumbar nuclear magnetic resonance imaging axial images; lumbar spine computed tomography to magnetic resonance imaging synthesis; machine learning; male; middle aged; nuclear magnetic resonance imaging; radiologist; retrospective study; semi supervised algorithm; signal noise ratio; supervised algorithm; synthetic image; T2 weighted imaging; unsupervised algorithm; visual turing test","Convolution neural network; Deep learning; GAN; Spine; Synthetic image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85125293494"
"Sun H.; Jiang Y.; Yuan J.; Wang H.; Liang D.; Fan W.; Hu Z.; Zhang N.","Sun, Hanyu (57962829800); Jiang, Yongluo (57219121815); Yuan, Jianmin (56387930800); Wang, Haining (57223311566); Liang, Dong (57195152723); Fan, Wei (56501169400); Hu, Zhanli (24824532900); Zhang, Na (57189364047)","57962829800; 57219121815; 56387930800; 57223311566; 57195152723; 56501169400; 24824532900; 57189364047","High-quality PET image synthesis from ultra-low-dose PET/MRI using bi-task deep learning","2022","Quantitative Imaging in Medicine and Surgery","12","12","","5326","5342","16","10.21037/qims-22-116","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141808661&doi=10.21037%2fqims-22-116&partnerID=40&md5=1028c2c2dd83495aa1dcd9f6377248e2","Background: Lowering the dose for positron emission tomography (PET) imaging reduces patients' radiation burden but decreases the image quality by increasing noise and reducing imaging detail and quantifications. This paper introduces a method for acquiring high-quality PET images from an ultra-low-dose state to achieve both high-quality images and a low radiation burden. Methods: We developed a two-task-based end-to-end generative adversarial network, named bi-c-GAN, that incorporated the advantages of PET and magnetic resonance imaging (MRI) modalities to synthesize high-quality PET images from an ultra-low-dose input. Moreover, a combined loss, including the mean absolute error, structural loss, and bias loss, was created to improve the trained model's performance. Real integrated PET/MRI data from 67 patients' axial heads (each with 161 slices) were used for training and validation purposes. Synthesized images were quantified by the peak signal-to-noise ratio (PSNR), normalized mean square error (NMSE), structural similarity (SSIM), and contrast noise ratio (CNR). The improvement ratios of these four selected quantitative metrics were used to compare the images produced by bi-c-GAN with other methods. Results: In the four-fold cross-validation, the proposed bi-c-GAN outperformed the other three selected methods (U-net, c-GAN, and multiple input c-GAN). With the bi-c-GAN, in a 5% low-dose PET, the image quality was higher than that of the other three methods by at least 6.7% in the PSNR, 0.6% in the SSIM, 1.3% in the NMSE, and 8% in the CNR. In the hold-out validation, bi-c-GAN improved the image quality compared to U-net and c-GAN in both 2.5% and 10% low-dose PET. For example, the PSNR using bi-C-GAN was at least 4.46% in the 2.5% low-dose PET and at most 14.88% in the 10% low-dose PET. Visual examples also showed a higher quality of images generated from the proposed method, demonstrating the denoising and improving ability of bi-c-GAN. Conclusions: By taking advantage of integrated PET/MR images and multitask deep learning (MDL), the proposed bi-c-GAN can efficiently improve the image quality of ultra-low-dose PET and reduce radiation exposure. © 2022 AME Publishing Company. All rights reserved.","adult; article; contrast to noise ratio; controlled study; cross validation; deep learning; female; human; image quality; low drug dose; major clinical study; male; mean absolute error; mean squared error; nuclear magnetic resonance imaging; positron emission tomography; quantitative analysis; radiation exposure; signal noise ratio; synthesis","bias loss; Integrated PET/MRI; multitask deep learning (MDL); ultra-low-dose positron emission tomography (PET)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141808661"
"Sun Q.; Guo J.; Liu Y.","Sun, Qiushi (57222423197); Guo, Jingtao (57209303328); Liu, Yi (55277449400)","57222423197; 57209303328; 55277449400","Face image synthesis from facial parts","2022","Eurasip Journal on Image and Video Processing","2022","1","7","","","","10.1186/s13640-022-00585-7","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129855146&doi=10.1186%2fs13640-022-00585-7&partnerID=40&md5=a0c3fd8e2f5f6933177d1a80c9213995","Recently, inspired by the growing power of deep convolutional neural networks (CNNs) and generative adversarial networks (GANs), facial image editing has received increasing attention and has produced a series of wide-ranging applications. In this paper, we propose a new and effective approach to a challenging task: synthesizing face images based on key facial parts. The proposed approach is a novel deep generative network that can automatically align facial parts with the precise positions in a face image and then output an entire facial image conditioned on the well-aligned parts. Specifically, three loss functions are introduced in this approach, which are the key to making the synthesized realistic facial image: a reconstruction loss to generate image content in an unknown region, a perceptual loss to enhance the network's ability to model high-level semantic structures and an adversarial loss to ensure that the synthesized images are visually realistic. In this approach, the three components cooperate well to form an effective framework for parts-based high-quality facial image synthesis. Finally, extensive experiments demonstrate the superior performance of this method to existing solutions. © 2022, The Author(s).","Convolutional neural networks; Deep neural networks; Image enhancement; Image reconstruction; Semantics; Deep learning; Face image synthesis; Face images; Facial images; Facial parts; Image completion; Image editing; New approaches; Power; Wide-ranging applications; Generative adversarial networks","Deep learning; Generative adversarial network; Image completion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85129855146"
"Xu Y.; Shen Y.; Zhu J.; Yang C.; Zhou B.","Xu, Yinghao (57219692967); Shen, Yujun (57207766466); Zhu, Jiapeng (57215047797); Yang, Ceyuan (57204283344); Zhou, Bolei (36697366200)","57219692967; 57207766466; 57215047797; 57204283344; 36697366200","GH-Feat: Learning Versatile Generative Hierarchical Features From GANs","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","16","15","10.1109/TPAMI.2022.3225788","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144032393&doi=10.1109%2fTPAMI.2022.3225788&partnerID=40&md5=e73691d996e3aa185e20eec2bc82a474","Recent years witness the tremendous success of generative adversarial networks (GANs) in synthesizing photo-realistic images. GAN generator learns to compose realistic images and reproduce the real data distribution. Through that, a hierarchical visual feature with multi-level semantics spontaneously emerges. In this work we investigate that such a generative feature learned from image synthesis exhibits great potentials in solving a wide range of computer vision tasks, including both generative ones and more importantly discriminative ones. We first train an encoder by considering the pre-trained StyleGAN generator as a learned loss function. The visual features produced by our encoder, termed as <italic>Generative Hierarchical Features (GH-Feat)</italic>, highly align with the layer-wise GAN representations, and hence describe the input image adequately from the reconstruction perspective. Extensive experiments support the versatile transferability of GH-Feat across a range of applications, such as image editing, image processing, image harmonization, face verification, landmark detection, layout prediction, image retrieval, <italic>etc.</italic> We further show that, through a proper spatial expansion, our developed GH-Feat can also facilitate fine-grained semantic segmentation using only a few annotations. Both qualitative and quantitative results demonstrate the appealing performance of GH-Feat. Code and models are available at <uri>https://genforce.github.io/ghfeat/</uri>. IEEE","Computer vision; Generative adversarial networks; Image retrieval; Semantic Segmentation; Signal encoding; Data distribution; Feature learning; Generative representation; Hierarchical features; Image editing; Learn+; Multilevels; Photorealistic images; Realistic images; Visual feature; Semantics","feature learning; Generative adversarial network; generative representation; image editing","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85144032393"
"Giry-Fouquet Y.; Baussard A.; Enderli C.; Porges T.","Giry-Fouquet, Yann (57329471000); Baussard, Alexandre (6603433755); Enderli, Cyrille (56047849700); Porges, Tristan (36069744200)","57329471000; 6603433755; 56047849700; 36069744200","SAR image synthesis with GAN and continuous aspect angle and class constraints","2022","Proceedings of the European Conference on Synthetic Aperture Radar, EUSAR","2022-July","","","809","814","5","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143598382&partnerID=40&md5=b8d962fdbc679b2c12bca4de0e80fc95","Target classification generally requires large databases, especially for deep learning methods. However, it is not always possible to have access to a database of sufficient size for certain imaging modalities. For example, in synthetic aperture radar (SAR) imaging only limited incidence angles and aspect angles can be available. Unfortunately, to overcome this problem, most of the classical data augmentation methods are inappropriate for SAR data. Thus, in a previous work, we evaluated conditional Generative Adversarial Networks to generate synthetic SAR images at given aspect angles and for specific target classes. Among the various models evaluated the so-called StyleGAN2-Ada, slightly modified to take into account the specificity of SAR images, appear to be the most efficient model. However, we observed that some of the generated images had wrong aspect angles. In this contribution we propose to correct this problem by adding a regularization term recently proposed in a model called Generator Regularized-cGAN. Our experiments show that this modification strongly reduce the problem. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Classification (of information); Deep learning; Generative adversarial networks; Learning systems; Radar imaging; Aspects angle; Data augmentation; Images synthesis; Imaging modality; Incidence angles; Large database; Learning methods; Synthetic aperture radar images; Synthetic aperture radar imaging; Target Classification; Synthetic aperture radar","","Conference paper","Final","","Scopus","2-s2.0-85143598382"
"Wei H.; Li Z.; Wang S.; Li R.","Wei, Haining (57423581500); Li, Zhongsen (57424322600); Wang, Shuai (57852996700); Li, Rui (57738677100)","57423581500; 57424322600; 57852996700; 57738677100","Undersampled Multi-Contrast MRI Reconstruction Based on Double-Domain Generative Adversarial Network","2022","IEEE Journal of Biomedical and Health Informatics","26","9","","4371","4377","6","10.1109/JBHI.2022.3143104","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123381491&doi=10.1109%2fJBHI.2022.3143104&partnerID=40&md5=20c95e7e450bc914a29c3cd50c3fb2c1","Multi-contrast magnetic resonance imaging can provide comprehensive information for clinical diagnosis. However, multi-contrast imaging suffers from long acquisition time, which makes it inhibitive for daily clinical practice. Subsampling k-space is one of the main methods to speed up scan time. Missing k-space samples will lead to inevitable serious artifacts and noise. Considering the assumption that different contrast modalities share some mutual information, it may be possible to exploit this redundancy to accelerate multi-contrast imaging acquisition. Recently, generative adversarial network shows superior performance in image reconstruction and synthesis. Some studies based on k-space reconstruction also exhibit superior performance over conventional state-of-art method. In this study, we propose a cross-domain two-stage generative adversarial network for multi-contrast images reconstruction based on prior full-sampled contrast and undersampled information. The new approach integrates reconstruction and synthesis, which estimates and completes the missing k-space and then refines in image space. It takes one fully-sampled contrast modality data and highly undersampled data from several other modalities as input, and outputs high quality images for each contrast simultaneously. The network is trained and tested on a public brain dataset from healthy subjects. Quantitative comparisons against baseline clearly indicate that the proposed method can effectively reconstruct undersampled images. Even under high acceleration, the network still can recover texture details and reduce artifacts.  © 2013 IEEE.","Artifacts; Brain; Head; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Deep learning; Diagnosis; Image reconstruction; Magnetic resonance imaging; Medical imaging; Textures; Biomedical imaging; Cross-domain; Cross-domain deep learning; Deep learning; Generator; Images reconstruction; Images synthesis; K-space; Multi-contrast MRI; acceleration; adult; article; artifact; brain; controlled study; deep learning; female; human; human experiment; image reconstruction; male; noise; nuclear magnetic resonance imaging; quantitative analysis; synthesis; diagnostic imaging; head; image processing; procedures; Generative adversarial networks","cross-domain deep learning; Generative adversarial network; image reconstruction; image synthesis; multi-contrast MRI","Article","Final","","Scopus","2-s2.0-85123381491"
"Reaungamornrat S.; Sari H.; Catana C.; Kamen A.","Reaungamornrat, Sureerat (35074959000); Sari, Hasan (57534893900); Catana, Ciprian (14014150000); Kamen, Ali (36056447400)","35074959000; 57534893900; 14014150000; 36056447400","Multimodal image synthesis based on disentanglement representations of anatomical and modality specific features, learned using uncooperative relativistic GAN","2022","Medical Image Analysis","80","","102514","","","","10.1016/j.media.2022.102514","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132231577&doi=10.1016%2fj.media.2022.102514&partnerID=40&md5=51e99a5a2424b3de3d45fa1cdd2d64f4","Growing number of methods for attenuation-coefficient map estimation from magnetic resonance (MR) images have recently been proposed because of the increasing interest in MR-guided radiotherapy and the introduction of positron emission tomography (PET) MR hybrid systems. We propose a deep-network ensemble incorporating stochastic-binary-anatomical encoders and imaging-modality variational autoencoders, to disentangle image-latent spaces into a space of modality-invariant anatomical features and spaces of modality attributes. The ensemble integrates modality-modulated decoders to normalize features and image intensities based on imaging modality. Besides promoting disentanglement, the architecture fosters uncooperative learning, offering ability to maintain anatomical structure in a cross-modality reconstruction. Introduction of a modality-invariant structural consistency constraint further enforces faithful embedding of anatomy. To improve training stability and fidelity of synthesized modalities, the ensemble is trained in a relativistic generative adversarial framework incorporating multiscale discriminators. Analyses of priors and network architectures as well as performance validation were performed on computed tomography (CT) and MR pelvis datasets. The proposed method demonstrated robustness against intensity inhomogeneity, improved tissue-class differentiation, and offered synthetic CT in Hounsfield units with intensities consistent and smooth across slices compared to the state-of-the-art approaches, offering median normalized mutual information of 1.28, normalized cross correlation of 0.97, and gradient cross correlation of 0.59 over 324 images. © 2022","Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Radiotherapy, Image-Guided; Tomography, X-Ray Computed; Computerized tomography; Hybrid systems; Image enhancement; Learning systems; Magnetic resonance; Network architecture; Positron emission tomography; Stochastic systems; Attenuation coefficient; Images synthesis; Imaging modality; MAP estimation; Multimodal images; Number of methods; Relativistics; Representation disentanglement; Resonance hybrid; Uncooperative learning; article; computer assisted tomography; controlled study; cross correlation; embedding; learning; nuclear magnetic resonance; pelvis; synthesis; human; image guided radiotherapy; image processing; nuclear magnetic resonance imaging; procedures; x-ray computed tomography; Generative adversarial networks","Generative adversarial network; Image synthesis; Representation disentanglement; Uncooperative learnings","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85132231577"
"Sun B.; Jia S.; Jiang X.; Jia F.","Sun, Bin (57210588117); Jia, Shuangfu (57219465938); Jiang, Xiling (57212005947); Jia, Fucang (55784808100)","57210588117; 57219465938; 57212005947; 55784808100","Double U-Net CycleGAN for 3D MR to CT image synthesis","2023","International Journal of Computer Assisted Radiology and Surgery","18","1","","149","156","7","10.1007/s11548-022-02732-x","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136500654&doi=10.1007%2fs11548-022-02732-x&partnerID=40&md5=f7e9c2a277c578e7b21650d4b372570a","Purpose: CycleGAN and its variants are widely used in medical image synthesis, which can use unpaired data for medical image synthesis. The most commonly used method is to use a Generative Adversarial Network (GAN) model to process 2D slices and thereafter concatenate all of these slices to 3D medical images. Nevertheless, these methods always bring about spatial inconsistencies in contiguous slices. We offer a new model based on the CycleGAN to work out this problem, which can achieve high-quality conversion from magnetic resonance (MR) to computed tomography (CT) images. Methods: To achieve spatial consistencies of 3D medical images and avoid the memory-heavy 3D convolutions, we reorganized the adjacent 3 slices into a 2.5D slice as the input image. Further, we propose a U-Net discriminator network to improve accuracy, which can perceive input objects locally and globally. Then, the model uses Content-Aware ReAssembly of Features (CARAFE) upsampling, which has a large field of view and content awareness takes the place of using a settled kernel for all samples. Results: The mean absolute error (MAE), peak-signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) for double U-Net CycleGAN generated 3D image synthesis are 74.56±10.02, 27.12±0.71 and 0.84±0.03, respectively. Our method achieves preferable results than state-of-the-art methods. Conclusion: The experiment results indicate our method can realize the conversion of MR to CT images using ill-sorted pair data, and achieves preferable results than state-of-the-art methods. Compared with 3D CycleGAN, it can synthesize better 3D CT images with less computation and memory. © 2022, CARS.","Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Magnetic Resonance Spectroscopy; Tomography, X-Ray Computed; article; awareness; computer assisted tomography; controlled study; mean absolute error; memory; nuclear magnetic resonance; signal noise ratio; synthesis; three-dimensional imaging; human; image processing; nuclear magnetic resonance imaging; nuclear magnetic resonance spectroscopy; procedures; x-ray computed tomography","3D image synthesis; CycleGAN; MR-to-CT; U-Net discriminator","Article","Final","","Scopus","2-s2.0-85136500654"
"Hassan A.U.; Memon I.; Choi J.","Hassan, Ammar Ul (57210801858); Memon, Irfanullah (57945050700); Choi, Jaeyoung (56812522400)","57210801858; 57945050700; 56812522400","Real-time high quality font generation with Conditional Font GAN","2023","Expert Systems with Applications","213","","118907","","","","10.1016/j.eswa.2022.118907","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140760622&doi=10.1016%2fj.eswa.2022.118907&partnerID=40&md5=fe44fa22a003bd450de8de4bae9de2eb","Designing and generating novels fonts manually is a laborious and time-consuming process owing to the large number and complexity of characters in the majority of language systems. Recent advancements in generative adversarial networks (GANs) have significantly improved font generation. These GAN-based approaches either handle the font generation as a vanilla GAN problem (that is, by synthesizing characters from a uniform latent vector) or an image-to-image translation problem. While the former approach has no limitation in generating diverse font styles, the generated fonts contain artifacts and can operate only on low-resolution images, thus impairing their usability. The latter approach generates high-quality font images for previously observed fonts, but the quality degrades during the inference phase while designing novel fonts. Furthermore, additional fine-tuning steps are required to achieve photorealistic results, which is computationally expensive and time-consuming. To address the shortcomings of these approaches, we propose a font generation method that employs the vanilla GAN approach to generate an infinite number of font styles but focuses on the real-time generation of photo-realistic font images. Additionally, we strive to create high-resolution images that can be used in practical applications. To accomplish this, we propose a conditional font GAN (CFGAN) with a sophisticated network architecture that is designed to generate novel style-consistent diverse font character sets. We control the generated characters in the proposed network using a non-trainable fixed character vector, while the style variation sampled from a Gaussian distribution is fused at all blocks of the generator through adaptive instance normalization (AdaIN) operation. Thus, the generator architecture can simultaneously generate an infinite number of font styles with style consistency and diversity during inference. We conducted various quantitative and qualitative experiments to demonstrate the effectiveness of the proposed model in terms of both image quality and computational cost. © 2022 Elsevier Ltd","Image processing; Network architecture; Font generation; Font images; High quality; High resolution; High resolution font image synthesis; Images synthesis; Photo-realistic; Real- time; Style transfer; Weakly supervised learning; Generative adversarial networks","Font generation; Generative adversarial networks; High resolution font image synthesis; Style transfer; Weakly-supervised learning","Article","Final","","Scopus","2-s2.0-85140760622"
"Alati E.; Caracciolo C.A.; Costa M.; Sanzari M.; Russo P.; Amerini I.","Alati, Edoardo (57205342748); Caracciolo, Carlo Alberto (57716318800); Costa, Marco (57716581600); Sanzari, Marta (57189662937); Russo, Paolo (57192380577); Amerini, Irene (27567536300)","57205342748; 57716318800; 57716581600; 57189662937; 57192380577; 27567536300","aRTIC GAN: A Recursive Text-Image-Conditioned GAN","2022","Electronics (Switzerland)","11","11","1737","","","","10.3390/electronics11111737","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130947029&doi=10.3390%2felectronics11111737&partnerID=40&md5=0e6f0ea7867dd2c9cad3a00f6f1e3135","Generative Adversarial Networks have recently demonstrated the capability to synthesize photo-realistic real-world images. However, they still struggle to offer high controllability of the output image, even if several constraints are provided as input. In this work, we present a Recursive Text-Image-Conditioned GAN (aRTIC GAN), a novel approach for multi-conditional image generation under concurrent spatial and text constraints. It employs few line drawings and short descriptions to provide informative yet human-friendly conditioning. The proposed scenario is based on accessible constraints with high degrees of freedom: sketches are easy to draw and add strong restrictions on the generated objects, such as their orientation or main physical characteristics. Text on its side is so common and expressive that easily enforces information otherwise impossible to provide with minimal illustrations, such as objects components color, color shades, etc. Our aRTIC GAN is suitable for the sequential generation of multiple objects due to its compact design. In fact, the algorithm exploits the previously generated image in conjunction with the sketch and the text caption, resulting in a recurrent approach. We developed three network blocks to tackle the fundamental problems of catching captions’ semantic meanings and of handling the trade-off between smoothing grid-pattern artifacts and visual detail preservation. Furthermore, a compact three-task discriminator (covering global, local and textual aspects) was developed to preserve a lightweight and robust architecture. Extensive experiments proved the validity of aRTIC GAN and show that the combined use of sketch and description allows us to avoid explicit object labeling. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","conditional GAN; Image-to-Image Translation; multi-conditional image generation; Text-to-Image Synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130947029"
"Zhang Z.; Fu C.; Weng W.; Zhou J.","Zhang, Zhiqiang (57206280843); Fu, Chen (57222985096); Weng, Wei (57939443500); Zhou, Jinjia (35099640400)","57206280843; 57222985096; 57939443500; 35099640400","Text-Guided Customizable Image Synthesis and Manipulation","2022","Applied Sciences (Switzerland)","12","20","10645","","","","10.3390/app122010645","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140440796&doi=10.3390%2fapp122010645&partnerID=40&md5=759574721f5d9ca738c0c51f6a156af1","Due to the high flexibility and conformity to people’s usage habits, text description has been widely used in image synthesis research recently and has achieved many encouraging results. However, the text can only determine the basic content of the generated image and cannot determine the specific shape of the synthesized object, which leads to poor practicability. More importantly, the current text-to-image synthesis research cannot use new text descriptions to further modify the synthesis result. To solve these problems, this paper proposes a text-guided customizable image synthesis and manipulation method. The proposed method synthesizes the corresponding image based on the text and contour information at first. It then modifies the synthesized content based on the new text to obtain a satisfactory result. The text and contour information in the proposed method determine the specific content and object shape of the desired composite image, respectively. Aside from that, the input text, contour, and subsequent new text for content modification can be manually input, which significantly improves the artificial controllability in the image synthesis process, making the entire method superior to other methods in flexibility and practicability. Experimental results on the Caltech-UCSD Birds-200-2011 (CUB) and Microsoft Common Objects in Context (MS COCO) datasets demonstrate our proposed method’s feasibility and versatility. © 2022 by the authors.","","artificially controllable image synthesis; generative adversarial networks; image manipulation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140440796"
"Zhou Q.; Zou H.","Zhou, Qian (57546999500); Zou, Hua (56208239800)","57546999500; 56208239800","A layer-wise fusion network incorporating self-supervised learning for multimodal MR image synthesis","2022","Frontiers in Genetics","13","","937042","","","","10.3389/fgene.2022.937042","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136599343&doi=10.3389%2ffgene.2022.937042&partnerID=40&md5=9029b8b5bf1a39c71cd5061e62a2d3c9","Magnetic resonance (MR) imaging plays an important role in medical diagnosis and treatment; different modalities of MR images can provide rich and complementary information to improve the accuracy of diagnosis. However, due to the limitations of scanning time and medical conditions, certain modalities of MR may be unavailable or of low quality in clinical practice. In this study, we propose a new multimodal MR image synthesis network to generate missing MR images. The proposed model comprises three stages: feature extraction, feature fusion, and image generation. During feature extraction, 2D and 3D self-supervised pretext tasks are introduced to pre-train the backbone for better representations of each modality. Then, a channel attention mechanism is used when fusing features so that the network can adaptively weigh different fusion operations to learn common representations of all modalities. Finally, a generative adversarial network is considered as the basic framework to generate images, in which a feature-level edge information loss is combined with the pixel-wise loss to ensure consistency between the synthesized and real images in terms of anatomical characteristics. 2D and 3D self-supervised pre-training can have better performance on feature extraction to retain more details in the synthetic images. Moreover, the proposed multimodal attention feature fusion block (MAFFB) in the well-designed layer-wise fusion strategy can model both common and unique information in all modalities, consistent with the clinical analysis. We also perform an interpretability analysis to confirm the rationality and effectiveness of our method. The experimental results demonstrate that our method can be applied in both single-modal and multimodal synthesis with high robustness and outperforms other state-of-the-art approaches objectively and subjectively. Copyright © 2022 Zhou and Zou.","article; attention; clinical evaluation; feature extraction; learning; nuclear magnetic resonance imaging; synthesis","encoder–decoder; feature-level fusion; generative adversarial network; medical image synthesis; self-supervised learning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85136599343"
"Hradecka L.; Wiesner D.; Sumbal J.; Koledova Z.S.; Maska M.","Hradecka, Lucia (57919430900); Wiesner, David (57211179074); Sumbal, Jakub (57212768495); Koledova, Zuzana Sumbalova (35102471600); Maska, Martin (23398080800)","57919430900; 57211179074; 57212768495; 35102471600; 23398080800","Segmentation and Tracking of Mammary Epithelial Organoids in Brightfield Microscopy","2023","IEEE Transactions on Medical Imaging","42","1","","281","290","9","10.1109/TMI.2022.3210714","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139490407&doi=10.1109%2fTMI.2022.3210714&partnerID=40&md5=702c6e8c187f1e7847d61fe5c9fdd56d","We present an automated and deep-learning-based workflow to quantitatively analyze the spatiotemporal development of mammary epithelial organoids in two-dimensional time-lapse (2D+t) sequences acquired using a brightfield microscope at high resolution. It involves a convolutional neural network (U-Net), purposely trained using computer-generated bioimage data created by a conditional generative adversarial network (pix2pixHD), to infer semantic segmentation, adaptive morphological filtering to identify organoid instances, and a shape-similarity-constrained, instance-segmentation-correcting tracking procedure to reliably cherry-pick the organoid instances of interest in time. By validating it using real 2D+t sequences of mouse mammary epithelial organoids of morphologically different phenotypes, we clearly demonstrate that the workflow achieves reliable segmentation and tracking performance, providing a reproducible and laborless alternative to manual analyses of the acquired bioimage data.  © 1982-2012 IEEE.","Adaptive filtering; Adaptive filters; Deep learning; Medical imaging; Neural networks; Semantic Segmentation; Semantics; Bright-field microscopy; Deep learning; Images synthesis; Organoid segmentation; Organoid tracking; Organoids; Segmentation and tracking; Two-dimensional; Work-flows; animal experiment; article; deep learning; eye tracking; male; mammary gland; microscopy; mouse; nonhuman; organoid; phenotype; synthesis; workflow; Generative adversarial networks","brightfield microscopy; deep learning; image synthesis; Organoid segmentation; organoid tracking","Article","Final","","Scopus","2-s2.0-85139490407"
"Wang H.; Ke H.; Liu C.","Wang, HongXia (56899113100); Ke, Hao (57387357600); Liu, Chun (57200871924)","56899113100; 57387357600; 57200871924","An embedded method: Improve the relevance of text and face image with enhanced face attributes","2022","Signal Processing: Image Communication","108","","116815","","","","10.1016/j.image.2022.116815","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134891216&doi=10.1016%2fj.image.2022.116815&partnerID=40&md5=1a9c8f2c279d7dd30e0ead2fbfec6557","To solve the problem of low quality and lack of specific attributes in the text-to-face synthesis task, this paper proposes EFA, a general embedding method for strengthening face attributes in the text-to-image synthesis models. First, we re-encode the irregular word-level descriptions scattered in sentences to form word encoding. Then, we design the embedded local feature extraction layer for discriminators of different models to learn more specific information related to face attributes. Next, we associate the word encoding with the extracted face image feature regions to obtain face attribute domain classification loss of the real image and the generated image. Finally, in the training process, we adopt the loss function to constrain the generator and discriminator to improve their performance. This method can improve the quality of text-to-face synthesis and enhance the semantic correlation between the generated image and text description. A large number of experimental results on the newly released Multi-Modal CelebA-HQ dataset verify the validity of our method, and the experimental results are competitive compared with state of the art. Especially, our approach boosts the FID by 47.75% over AttnGAN, by 33.68% over ControlGAN, by 10.05% over DM-GAN, and by 12.52% over DF-GAN. Code is available at https://github.com/cookie-ke/EFA. © 2022 Elsevier B.V.","Encoding (symbols); Image enhancement; Large dataset; Semantics; Signal encoding; Embedding method; Encodings; Face images; Face synthesis; Image generations; Images synthesis; Low qualities; Text images; Text-to-image face image generation; Visual attributes; Generative adversarial networks","Face synthesis; Generative adversarial networks; Text-to-image face image generation; Visual attributes","Article","Final","","Scopus","2-s2.0-85134891216"
"Chen Y.; Yang X.-H.; Wei Z.; Heidari A.A.; Zheng N.; Li Z.; Chen H.; Hu H.; Zhou Q.; Guan Q.","Chen, Yizhou (57224476141); Yang, Xu-Hua (12773349200); Wei, Zihan (57388573700); Heidari, Ali Asghar (56541062900); Zheng, Nenggan (8254784900); Li, Zhicheng (55707186500); Chen, Huiling (36865973700); Hu, Haigen (30267591900); Zhou, Qianwei (55330189900); Guan, Qiu (55671678500)","57224476141; 12773349200; 57388573700; 56541062900; 8254784900; 55707186500; 36865973700; 30267591900; 55330189900; 55671678500","Generative Adversarial Networks in Medical Image augmentation: A review","2022","Computers in Biology and Medicine","144","","105382","","","","10.1016/j.compbiomed.2022.105382","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126817827&doi=10.1016%2fj.compbiomed.2022.105382&partnerID=40&md5=0c415b2abc7df580dc9b34ac9c438a5b","Object: With the development of deep learning, the number of training samples for medical image-based diagnosis and treatment models is increasing. Generative Adversarial Networks (GANs) have attracted attention in medical image processing due to their excellent image generation capabilities and have been widely used in data augmentation. In this paper, a comprehensive and systematic review and analysis of medical image augmentation work are carried out, and its research status and development prospects are reviewed. Method: This paper reviews 105 medical image augmentation related papers, which mainly collected by ELSEVIER, IEEE Xplore, and Springer from 2018 to 2021. We counted these papers according to the parts of the organs corresponding to the images, and sorted out the medical image datasets that appeared in them, the loss function in model training, and the quantitative evaluation metrics of image augmentation. At the same time, we briefly introduce the literature collected in three journals and three conferences that have received attention in medical image processing. Result: First, we summarize the advantages of various augmentation models, loss functions, and evaluation metrics. Researchers can use this information as a reference when designing augmentation tasks. Second, we explore the relationship between augmented models and the amount of the training set, and tease out the role that augmented models may play when the quality of the training set is limited. Third, the statistical number of papers shows that the development momentum of this research field remains strong. Furthermore, we discuss the existing limitations of this type of model and suggest possible research directions. Conclusion: We discuss GAN-based medical image augmentation work in detail. This method effectively alleviates the challenge of limited training samples for medical image diagnosis and treatment models. It is hoped that this review will benefit researchers interested in this field. © 2022 Elsevier Ltd","Image Processing, Computer-Assisted; Neural Networks, Computer; Deep learning; Diagnosis; Function evaluation; Medical imaging; Sampling; Augmentation; Deep learning; Diagnosis model; Evaluation metrics; Images synthesis; Loss functions; Medical image; Medical images processing; Training sample; Treatment modeling; article; attention; deep learning; human; image processing; loss of function mutation; quantitative analysis; synthesis; systematic review; image processing; procedures; Generative adversarial networks","Augmentation; Deep learning; Generative adversarial networks; Image synthesis; Medical image","Article","Final","","Scopus","2-s2.0-85126817827"
"Tang H.; Shao L.; Torr P.H.S.; Sebe N.","Tang, Hao (57208238003); Shao, Ling (57225798572); Torr, Philip H. S. (56821543600); Sebe, Nicu (57204924633)","57208238003; 57225798572; 56821543600; 57204924633","Bipartite Graph Reasoning GANs for Person Pose and Facial Image Synthesis","2023","International Journal of Computer Vision","131","3","","644","658","14","10.1007/s11263-022-01722-5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143670414&doi=10.1007%2fs11263-022-01722-5&partnerID=40&md5=aa84782fce1945798e21606a27a8877a","We present a novel bipartite graph reasoning Generative Adversarial Network (BiGraphGAN) for two challenging tasks: person pose and facial image synthesis. The proposed graph generator consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed bipartite graph reasoning (BGR) block aims to reason the long-range cross relations between the source and target pose in a bipartite graph, which mitigates some of the challenges caused by pose deformation. Moreover, we propose a new interaction-and-aggregation (IA) block to effectively update and enhance the feature representation capability of both a person’s shape and appearance in an interactive way. To further capture the change in pose of each part more precisely, we propose a novel part-aware bipartite graph reasoning (PBGR) block to decompose the task of reasoning the global structure transformation with a bipartite graph into learning different local transformations for different semantic body/face parts. Experiments on two challenging generation tasks with three public datasets demonstrate the effectiveness of the proposed methods in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/BiGraphGAN. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Graph theory; Image processing; Semantics; Bipartite graph reasoning; Bipartite graphs; Cross relation; Facial expression synthesis; Facial Image synthesis; Feature representation; GAN; Person pose synthesis; Pose synthesis; S shape; Generative adversarial networks","Bipartite graph reasoning; Facial expression synthesis; GANs; Person pose synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143670414"
"Zhang Y.; Lin X.; Zhuang Y.; Sun L.; Huang Y.; Ding X.; Wang G.; Yang L.; Yu Y.","Zhang, Yunlong (57215594528); Lin, Xin (57221142422); Zhuang, Yihong (57221147429); Sun, Liyan (57161337400); Huang, Yue (57204367647); Ding, Xinghao (57204367131); Wang, Guisheng (55995140800); Yang, Lin (57221069615); Yu, Yizhou (8554163500)","57215594528; 57221142422; 57221147429; 57161337400; 57204367647; 57204367131; 55995140800; 57221069615; 8554163500","Harmonizing Pathological and Normal Pixels for Pseudo-Healthy Synthesis","2022","IEEE Transactions on Medical Imaging","41","9","","2457","2468","11","10.1109/TMI.2022.3164095","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127461059&doi=10.1109%2fTMI.2022.3164095&partnerID=40&md5=e30f0bb691344909e52d040ef12d7775","Synthesizing a subject-specific pathology-free image from a pathological image is valuable for algorithm development and clinical practice. In recent years, several approaches based on the Generative Adversarial Network (GAN) have achieved promising results in pseudo-healthy synthesis. However, the discriminator (i.e., a classifier) in the GAN cannot accurately identify lesions and further hampers from generating admirable pseudo-healthy images. To address this problem, we present a new type of discriminator, the segmentor, to accurately locate the lesions and improve the visual quality of pseudo-healthy images. Then, we apply the generated images into medical image enhancement and utilize the enhanced results to cope with the low contrast problem existing in medical image segmentation. Furthermore, a reliable metric is proposed by utilizing two attributes of label noise to measure the health of synthetic images. Comprehensive experiments on the T2 modality of BraTS demonstrate that the proposed method substantially outperforms the state-of-the-art methods. The method achieves better performance than the existing methods with only 30% of the training data. The effectiveness of the proposed method is also demonstrated on the LiTS and the T1 modality of BraTS. The code and the pre-trained model of this study are publicly available at https://github.com/Au3C2/Generator-Versus-Segmentor.  © 1982-2012 IEEE.","Algorithms; Image Processing, Computer-Assisted; Neural Networks, Computer; Computer vision; Generative adversarial networks; Image segmentation; Medical imaging; Pathology; Adversarial training; Biomedical imaging; Generator; Images segmentations; Images synthesis; Label noise; Lesion; Low contrast; Low-contrast medical image segmentation; Medical image segmentation; Medical image synthesis; article; human; human experiment; image enhancement; image segmentation; noise; synthesis; algorithm; image processing; procedures; Image enhancement","adversarial training; image enhancement; label noise; low-contrast medical image segmentation; Medical image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127461059"
"Fang N.; Qiu L.; Zhang S.; Wang Z.; Hu K.; Wang K.","Fang, Naiyu (57243482100); Qiu, Lemiao (20735224100); Zhang, Shuyou (8375666400); Wang, Zili (57221407016); Hu, Kerui (57555656400); Wang, Kang (58023203200)","57243482100; 20735224100; 8375666400; 57221407016; 57555656400; 58023203200","A novel DAGAN for synthesizing garment images based on design attribute disentangled representation","2023","Pattern Recognition","136","","109248","","","","10.1016/j.patcog.2022.109248","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144292084&doi=10.1016%2fj.patcog.2022.109248&partnerID=40&md5=660a720f9cc88676002506e7dc08c061","In online costume design, it is vital to preview the design effect rapidly by entangling design attributes from reference images. This paper proposes a novel method, named design attributes generative adversarial network (DAGAN) for synthesizing garment images based on design attribute disentangled representation. The garment style is disentangled into the shape, texture, shadow, and decoration design attributes. The shape mask, repeating texture region, Laplace image gradient, and local logo are leveraged as visual representations for clothing design attributes from reference images. Following the design sequence from global to local, GDA-Net and LDA-Net in DAGAN entangle global and local design attributes, respectively, in the latent space. Then, the desired garment image is synthesized to represent design intentions explicitly. The DA-dataset for clothing design attributes is released. Extensive experiments demonstrate that the DAGAN is robust to various instances of design attributes on Design Attributes dataset (DA-dataset), and that is superior to the cross-domain transfer models in entangling design attributes from reference images. © 2022 Elsevier Ltd","Generative adversarial networks; Textures; Design attribute generative adversarial network; Design attributes; Disentangled representation; Garment design; Garment design attribute; Garment image synthesis; Images synthesis; Online costume design; Reference image; Design","DAGAN; Disentangled representation; Garment design attributes; Garment image synthesis; Online costume design","Article","Final","","Scopus","2-s2.0-85144292084"
"Jilani U.; Asif M.; Rashid M.; Siddique A.A.; Talha S.M.U.; Aamir M.","Jilani, Umair (57720366300); Asif, Muhammad (55530883200); Rashid, Munaf (57220177714); Siddique, Ali Akbar (57216631643); Talha, Syed Muhammad Umar (37023655900); Aamir, Muhammad (35291627300)","57720366300; 55530883200; 57220177714; 57216631643; 37023655900; 35291627300","Traffic Congestion Classification Using GAN-Based Synthetic Data Augmentation and a Novel 5-Layer Convolutional Neural Network Model","2022","Electronics (Switzerland)","11","15","2290","","","","10.3390/electronics11152290","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136810074&doi=10.3390%2felectronics11152290&partnerID=40&md5=19cce6e75609e0853b6275d79f3584bb","Private automobiles are still a widely prevalent mode of transportation. Subsequently, traffic congestion on the roads has been more frequent and severe with the continuous rise in the numbers of cars on the road. The estimation of traffic flow, or conversely, traffic congestion identification, is of critical importance in a wide variety of applications, including intelligent transportation systems (ITS). Recently, artificial intelligence (AI) has been in the limelight for sophisticated ITS solutions. However, AI-based schemes are typically heavily dependent on the quantity and quality of data. Typical traffic data have been found to be insufficient and less efficient in AI-based ITS solutions. Advanced data cleaning and preprocessing methods offer a solution for this problem. Such techniques enable quality improvement and augmenting additional information in the traffic congestion dataset. One such efficient technique is the generative adversarial network (GAN), which has attracted much interest from the research community. This research work reports on the generation of a traffic congestion dataset with enhancement through GAN-based augmentation. The GAN-enhanced traffic congestion dataset is then used for training artificial intelligence (AI)-based models. In this research work, a five-layered convolutional neural network (CNN) deep learning model is proposed for traffic congestion classification. The performance of the proposed model is compared with that of a number of other well-known pretrained models, including ResNet-50 and DenseNet-121. Promising results present the efficacy of the proposed scheme using GAN-based data augmentation in a five-layered convolutional neural network (CNN) model for traffic congestion classification. The proposed technique attains accuracy of 98.63% compared with the accuracies of ResNet-50 and DenseNet-121, 90.59% and 93.15%, respectively. The proposed technique can be used for urban traffic planning and maintenance managers and stakeholders for the efficient deployment of intelligent transportation system (ITS). © 2022 by the authors.","","generative adversarial networks (GAN); image synthesis; intelligent transportation system (ITS)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136810074"
"Nandhini Abirami R.; Durai Raj Vincent P.M.; Rajinikanth V.; Kadry S.","Nandhini Abirami, R. (57212486754); Durai Raj Vincent, P.M. (55808710700); Rajinikanth, Venkatesan (57561692500); Kadry, Seifedine (55906598300)","57212486754; 55808710700; 57561692500; 55906598300","COVID-19 Classification Using Medical Image Synthesis by Generative Adversarial Networks","2022","International Journal of Uncertainty, Fuzziness and Knowlege-Based Systems","30","3","","385","401","16","10.1142/S0218488522400128","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135409364&doi=10.1142%2fS0218488522400128&partnerID=40&md5=f6cecb9efc2e79257f68f3f62c89ca14","The outbreak of novel coronavirus disease 2019, also called COVID-19, in Wuhan, China, began in December 2019. Since its outbreak, infectious disease has rapidly spread across the globe. The testing methods adopted by the medical practitioners gave false negatives, which is a big challenge. Medical imaging using deep learning can be adopted to speed up the testing process and avoid false negatives. This work proposes a novel approach, COVID-19 GAN, to perform coronavirus disease classification using medical image synthesis by a generative adversarial network. Detecting coronavirus infections from the chest X-ray images is very crucial for its early diagnosis and effective treatment. To boost the performance of the deep learning model and improve the accuracy of classification, synthetic data augmentation is performed using generative adversarial networks. Here, the available COVID-19 positive chest X-ray images are fed into the styleGAN2 model. The styleGAN model is trained, and the data necessary for training the deep learning model for coronavirus classification is generated. The generated COVID-19 positive chest X-ray images and the normal chest X-ray images are fed into the deep learning model for training. An accuracy of 99.78% is achieved in classifying chest X-ray images using CNN binary classifier model.  © 2022 World Scientific Publishing Company.","Classification (of information); Deep learning; Diagnosis; Generative adversarial networks; Image classification; Learning systems; Medical imaging; Testing; Chest X-ray image; Corona virus; Coronavirus classification; Coronaviruses; Deep learning; False negatives; GAN; Images synthesis; Learning models; Medical image synthesis; Coronavirus; COVID-19","corona virus; coronavirus classification; COVID-19; deep learning; GAN; medical image synthesis","Article","Final","","Scopus","2-s2.0-85135409364"
"Silveira W.; Alaniz A.; Hurtado M.; Da Silva B.C.; De Bem R.","Silveira, Wellington (58068933500); Alaniz, Andrew (58068933600); Hurtado, Marina (58069073700); Da Silva, Bernardo Castello (58068867700); De Bem, Rodrigo (15022066600)","58068933500; 58068933600; 58069073700; 58068867700; 15022066600","SynLibras: A Disentangled Deep Generative Model for Brazilian Sign Language Synthesis","2022","Proceedings - 2022 35th Conference on Graphics, Patterns, and Images, SIBGRAPI 2022","","","","210","215","5","10.1109/SIBGRAPI55357.2022.9991748","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146417933&doi=10.1109%2fSIBGRAPI55357.2022.9991748&partnerID=40&md5=2f42071307244a4a8073a2abe6349e8c","Recent advances regarding deep generative models have strengthened a realm of approaches in which discriminative and generative tasks are tackled jointly in an analysis-by-synthesis manner. In this category, variational autoencoders (VAEs) and generative adversarial networks (GANs) aim for learning latent data representations from which sampling of synthetic images may be performed. However, sampling in such models normally does not allow for independent control of diverse factors of variation. Despite general efforts to overcome this issue, deep generative models tailored for sign language with disentangled factors of variation are yet not vastly explored in the literature. In this work, we introduce the SynLibras, a novel model that allows for disentangling appearance and gestural communication (i.e. body, hands and face poses) on image synthesis. Our model is capable of performing cross-language pose-transfer while maintaining the appearance of the source signer. We perform experiments on the RWTH-PHOENIX-Weather dataset and evaluation using the PSNR and the SSIM metrics. To our knowledge, the SynLibras is the first method for Brazilian sign language (Libras) synthesis in images. We compare our model with the EDN, a well-known general pose-transfer method, achieving better results on Libras synthesis. Finally, we also introduce the SynLibras-Pose, a dataset with annotated poses of Libras signers performing single words.  © 2022 IEEE.","Computer vision; Analysis by synthesis; Auto encoders; Body pose; Data representations; Generative model; Gestural communication; Independent control; Sign language; Sign language synthesis; Synthetic images; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85146417933"
"Tantawy D.; Zahran M.; Wassal A.G.","Tantawy, Dina (57496377700); Zahran, Mohamed (56860248100); Wassal, Amr G. (35568744500)","57496377700; 56860248100; 35568744500","PTcomp: Post-Training Compression Technique for Generative Adversarial Networks","2023","IEEE Access","11","","","9763","9774","11","10.1109/ACCESS.2023.3239786","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147296657&doi=10.1109%2fACCESS.2023.3239786&partnerID=40&md5=7cafd54404a71a77ee2dfbe101e73c98","In a time of virtual spaces, the usage of generative adversarial networks is inevitable. Generative adversarial networks (GANs) are generative deep-learning models that can generate realistic data. GANs have been used in many applications like text-to-image, image-to-image, image synthesis, speech synthesis, etc. Its power lies in the diversity and novelty of the generated data. Despite their advantages, GANs are resource-hungry. GANs' output resolution and high correlation make it more challenging to compress and fit on edge-devices storage and power budget. Hence, traditional compression techniques are not the best fit to use with GANs. Additionally, GANs training instability adds another dimension of difficulty. Therefore, compression techniques that require retraining are challenging for GANs. In this paper, we developed a weight clustering technique to compress GANs without the need for retraining, hence the name post-training compression technique (PTcomp). We also proposed a clustered-based pruning which adds more savings. Experiments on Cyclegan, Deep convolution gan (DCGAN), and Stargan using several datasets show the superiority of our technique against traditional post-training quantization. Our technique provides a 4x to 8x compression ratio with comparable quality to original models and 14% fewer mac operations due to pruning.  © 2013 IEEE.","Budget control; Deep neural networks; Digital storage; Generative adversarial networks; Quantization (signal); Speech synthesis; Clusterings; Compression; Compression techniques; Deep-learning; Generator; Learning models; Post-training; Pruning; Quantization (signal); Virtual spaces; Clustering algorithms","clustering; Compression; deep-learning; generative adversarial networks; post-training; pruning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85147296657"
"Liang J.; Yang X.; Huang Y.; Li H.; He S.; Hu X.; Chen Z.; Xue W.; Cheng J.; Ni D.","Liang, Jiamin (57217028259); Yang, Xin (56967210500); Huang, Yuhao (57217028887); Li, Haoming (57216642196); He, Shuangchi (57217028750); Hu, Xindi (57224901061); Chen, Zejian (57221633255); Xue, Wufeng (36457947300); Cheng, Jun (57199449617); Ni, Dong (26023577500)","57217028259; 56967210500; 57217028887; 57216642196; 57217028750; 57224901061; 57221633255; 36457947300; 57199449617; 26023577500","Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis","2022","Medical Image Analysis","79","","102461","","","","10.1016/j.media.2022.102461","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129212584&doi=10.1016%2fj.media.2022.102461&partnerID=40&md5=3b3c0191a3a1f6092258f13c2e8738e7","Ultrasound (US) imaging is widely used for anatomical structure inspection in clinical diagnosis. The training of new sonographers and deep learning based algorithms for US image analysis usually requires a large amount of data. However, obtaining and labeling large-scale US imaging data are not easy tasks, especially for diseases with low incidence. Realistic US image synthesis can alleviate this problem to a great extent. In this paper, we propose a generative adversarial network (GAN) based image synthesis framework. Our main contributions include: (1) we present the first work that can synthesize realistic B-mode US images with high-resolution and customized texture editing features; (2) to enhance structural details of generated images, we propose to introduce auxiliary sketch guidance into a conditional GAN. We superpose the edge sketch onto the object mask and use the composite mask as the network input; (3) to generate high-resolution US images, we adopt a progressive training strategy to gradually generate high-resolution images from low-resolution images. In addition, a feature loss is proposed to minimize the difference of high-level features between the generated and real images, which further improves the quality of generated images; (4) the proposed US image synthesis method is quite universal and can also be generalized to the US images of other anatomical structures besides the three ones tested in our study (lung, hip joint, and ovary); (5) extensive experiments on three large US image datasets are conducted to validate our method. Ablation studies, customized texture editing, user studies, and segmentation tests demonstrate promising results of our method in synthesizing realistic US images. © 2022 Elsevier B.V.","Algorithms; Female; Humans; Image Processing, Computer-Assisted; Ultrasonography; Deep learning; Diagnosis; Image enhancement; Image segmentation; Large dataset; Textures; Ultrasonic applications; Anatomical structures; Clinical diagnosis; COVID-19; Hip joints; Images synthesis; Ovary and follicle; Structure inspection; Ultrasound image synthesis; Ultrasound images; Ultrasound imaging; anatomical concepts; Article; artifact; B scan; conceptual framework; controlled study; coronavirus disease 2019; data synthesis; feature extraction; female; generative adversarial network; hip; human; image analysis; image segmentation; infant; lung; major clinical study; network learning; ovary; ovary follicle; algorithm; echography; image processing; procedures; Generative adversarial networks","COVID-19; Generative adversarial networks; Hip joint; Ovary and follicle; Ultrasound image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129212584"
"Luo Y.; Zhou L.; Zhan B.; Fei Y.; Zhou J.; Wang Y.; Shen D.","Luo, Yanmei (57223424973); Zhou, Luping (23398846800); Zhan, Bo (57221803799); Fei, Yuchen (57219972263); Zhou, Jiliu (21234416400); Wang, Yan (56039981100); Shen, Dinggang (7401738392)","57223424973; 23398846800; 57221803799; 57219972263; 21234416400; 56039981100; 7401738392","Adaptive rectification based adversarial network with spectrum constraint for high-quality PET image synthesis","2022","Medical Image Analysis","77","","102335","","","","10.1016/j.media.2021.102335","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121962638&doi=10.1016%2fj.media.2021.102335&partnerID=40&md5=92fe0cc3004526a58be91e042da2bf32","Positron emission tomography (PET) is a typical nuclear imaging technique, which can provide crucial functional information for early brain disease diagnosis. Generally, clinically acceptable PET images are obtained by injecting a standard-dose radioactive tracer into human body, while on the other hand the cumulative radiation exposure inevitably raises concerns about potential health risks. However, reducing the tracer dose will increase the noise and artifacts of the reconstructed PET image. For the purpose of acquiring high-quality PET images while reducing radiation exposure, in this paper, we innovatively present an adaptive rectification based generative adversarial network with spectrum constraint, named AR-GAN, which uses low-dose PET (LPET) image to synthesize standard-dose PET (SPET) image of high-quality. Specifically, considering the existing differences between the synthesized SPET image by traditional GAN and the real SPET image, an adaptive rectification network (AR-Net) is devised to estimate the residual between the preliminarily predicted image and the real SPET image, based on the hypothesis that a more realistic rectified image can be obtained by incorporating both the residual and the preliminarily predicted PET image. Moreover, to address the issue of high-frequency distortions in the output image, we employ a spectral regularization term in the training optimization objective to constrain the consistency of the synthesized image and the real image in the frequency domain, which further preserves the high-frequency detailed information and improves synthesis performance. Validations on both the phantom dataset and the clinical dataset show that the proposed AR-GAN can estimate SPET images from LPET images effectively and outperform other state-of-the-art image synthesis approaches. © 2021 Elsevier B.V.","Artifacts; Humans; Image Processing, Computer-Assisted; Phantoms, Imaging; Positron-Emission Tomography; Diagnosis; Frequency domain analysis; Generative adversarial networks; Health risks; Image enhancement; Positrons; fluorodeoxyglucose f 18; Adaptive rectification; Generative adversarial network; High quality; Images synthesis; Low dose; PET images; Positron emission tomography; Radiation Exposure; Spectra's; Spectrum constraint; adaptive rectification based generative adversarial network; Article; artificial neural network; brain tissue; clinical article; comparative study; controlled study; cross validation; deconvolution; deep learning; discrete Fourier transform; entropy; human; image quality; image reconstruction; iterative reconstruction; mild cognitive impairment; network learning; neuroimaging; ordered subset expectation maximization; positron emission tomography; prediction; qualitative analysis; quantitative analysis; radiation exposure; signal noise ratio; artifact; image processing; imaging phantom; procedures; Positron emission tomography","Adaptive rectification; Generative adversarial network (GAN); Image synthesis; Positron emission tomography (PET); Spectrum constraint","Article","Final","","Scopus","2-s2.0-85121962638"
"Zhu J.; Gao L.; Song J.; Li Y.-F.; Zheng F.; Li X.; Shen H.T.","Zhu, Junchen (57224080867); Gao, Lianli (56611089900); Song, Jingkuan (57205085174); Li, Yuan-Fang (55719200700); Zheng, Feng (36070223900); Li, Xuelong (55936260100); Shen, Heng Tao (7404523209)","57224080867; 56611089900; 57205085174; 55719200700; 36070223900; 55936260100; 7404523209","Label-Guided Generative Adversarial Network for Realistic Image Synthesis","2023","IEEE Transactions on Pattern Analysis and Machine Intelligence","45","3","","3311","3328","17","10.1109/TPAMI.2022.3186752","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133559296&doi=10.1109%2fTPAMI.2022.3186752&partnerID=40&md5=c394e9115d3e2a8a0c3893f980565413","Generating photo-realistic images from labels (e.g., semantic labels or sketch labels) is much more challenging than the general image-to-image translation task, mainly due to the large differences between extremely sparse labels and detail rich images. We propose a general framework Lab2Pix to tackle this issue from two aspects: 1) how to extract useful information from the input; and 2) how to efficiently bridge the gap between the labels and images. Specifically, we propose a Double-Guided Normalization (DG-Norm) to use the input label for semantically guiding activations in normalization layers, and use global features with large receptive fields for differentiating the activations within the same semantic region. To efficiently generate the images, we further propose Label Guided Spatial Co-Attention (LSCA) to encourage the learning of incremental visual information using limited model parameters while storing the well-synthesized part in lower-level features. Accordingly, Hierarchical Perceptual Discriminators with Foreground Enhancement Masks are proposed to toughly work against the generator thus encouraging realistic image generation and a sharp enhancement loss is further introduced for high-quality sharp image generation. We instantiate our Lab2Pix for the task of label-to-image in both unpaired (Lab2Pix-V1) and paired settings (Lab2Pix-V2). Extensive experiments conducted on various datasets demonstrate that our method significantly outperforms state-of-the-art methods quantitatively and qualitatively in both settings.  © 1979-2012 IEEE.","Computer vision; Generative adversarial networks; Image enhancement; Information use; Semantics; Generative adversarial network; Image generations; Image translation; Images synthesis; Label-to-image synthesis; Normalisation; Photo-realistic image generation; Photorealistic images; Realistic image synthesis; Semantic labels; Chemical activation","Generative Adversarial Networks (GANs); label-to-image synthesis; photo-realistic image generation","Article","Final","","Scopus","2-s2.0-85133559296"
"Wang J.; Tabassum N.; Toma T.T.; Wang Y.; Gahlmann A.; Acton S.T.","Wang, Jie (57206679695); Tabassum, Nazia (57208993360); Toma, Tanjin T. (57217028600); Wang, Yibo (57220124041); Gahlmann, Andreas (14527232400); Acton, Scott T. (7006577888)","57206679695; 57208993360; 57217028600; 57220124041; 14527232400; 7006577888","3D GAN image synthesis and dataset quality assessment for bacterial biofilm","2022","Bioinformatics (Oxford, England)","38","19","","4598","4604","6","10.1093/bioinformatics/btac529","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141891919&doi=10.1093%2fbioinformatics%2fbtac529&partnerID=40&md5=80f51ba6149aa67d2a6d5200b677d9cb","MOTIVATION: Data-driven deep learning techniques usually require a large quantity of labeled training data to achieve reliable solutions in bioimage analysis. However, noisy image conditions and high cell density in bacterial biofilm images make 3D cell annotations difficult to obtain. Alternatively, data augmentation via synthetic data generation is attempted, but current methods fail to produce realistic images. RESULTS: This article presents a bioimage synthesis and assessment workflow with application to augment bacterial biofilm images. 3D cyclic generative adversarial networks (GAN) with unbalanced cycle consistency loss functions are exploited in order to synthesize 3D biofilm images from binary cell labels. Then, a stochastic synthetic dataset quality assessment (SSQA) measure that compares statistical appearance similarity between random patches from random images in two datasets is proposed. Both SSQA scores and other existing image quality measures indicate that the proposed 3D Cyclic GAN, along with the unbalanced loss function, provides a reliably realistic (as measured by mean opinion score) 3D synthetic biofilm image. In 3D cell segmentation experiments, a GAN-augmented training model also presents more realistic signal-to-background intensity ratio and improved cell counting accuracy. AVAILABILITY AND IMPLEMENTATION: https://github.com/jwang-c/DeepBiofilm. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online. © The Author(s) 2022. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.","Biofilms; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; biofilm; image processing; procedures; three-dimensional imaging","","Article","Final","","Scopus","2-s2.0-85141891919"
"Su S.; Du S.; Lu X.","Su, Shixiang (57298982700); Du, Songlin (57196475273); Lu, Xiaobo (14627586600)","57298982700; 57196475273; 14627586600","Geometric Constraint and Image Inpainting-Based Railway Track Fastener Sample Generation for Improving Defect Inspection","2022","IEEE Transactions on Intelligent Transportation Systems","23","12","","23883","23895","12","10.1109/TITS.2022.3207490","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139527994&doi=10.1109%2fTITS.2022.3207490&partnerID=40&md5=e5528a117c9adb3086cfb0da9639c149","Defective fastener images detection is an essential task in the vision-based railway track safety inspection. Although existing methods have achieved some level of success, the detection accuracy in this field suffers from the defective fasteners being far less common than normal fasteners. One way to tackle this problem is to expand the defect sample. However, current state-of-the-art defective fastener generation methods mainly rely on generative adversarial networks or simply augment the defect data through traditional image processing. These methods may not be ideal as it is difficult to produce images with high quality and rich diversity at the same time. This paper proposes a new method for fastener sample generation that actively divides the sample generation into two independent parts: defective foregrounds generation and complete backgrounds generation. The key to this method is to generate foregrounds and backgrounds based on geometric constraint and image inpainting, respectively. Specifically, we adopt a skeleton mapping algorithm to directionally control the generated types of defective foregrounds. Meanwhile, an image inpainting network is employed to expand the background. The experiments show that this enables us to generate better-quality and richer-diversity images by combining deep learning and image processing advantages. To the best of our knowledge, our method is the first to achieve state-of-the-art performance, i.e., the classification accuracy reaches 97.97%, without using real defective fastener images during the defect classification network training process. © 2000-2011 IEEE.","Conformal mapping; Deep learning; Defects; Geometry; Image enhancement; Locks (fasteners); Musculoskeletal system; Railroad transportation; Railroads; Rails; Geometric constraint; Image generations; Image Inpainting; Images processing; Images synthesis; Rail transportation; Railway fastener inspection; Sample generations; Skeleton; Task analysis; Inspection","geometric constraint; Image generation; image inpainting; railway fastener inspection","Article","Final","","Scopus","2-s2.0-85139527994"
"Huang P.; Li D.; Jiao Z.; Wei D.; Cao B.; Mo Z.; Wang Q.; Zhang H.; Shen D.","Huang, Pu (57193329557); Li, Dengwang (36463794100); Jiao, Zhicheng (57189091634); Wei, Dongming (57211428968); Cao, Bing (57204475892); Mo, Zhanhao (56448089700); Wang, Qian (57192157811); Zhang, Han (36117322900); Shen, Dinggang (7401738392)","57193329557; 36463794100; 57189091634; 57211428968; 57204475892; 56448089700; 57192157811; 36117322900; 7401738392","Common feature learning for brain tumor MRI synthesis by context-aware generative adversarial network","2022","Medical Image Analysis","79","","102472","","","","10.1016/j.media.2022.102472","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129961137&doi=10.1016%2fj.media.2022.102472&partnerID=40&md5=87fa51341d85321da6e4f197a9e54381","Multi-modal structural Magnetic Resonance Image (MRI) provides complementary information and has been used widely for diagnosis and treatment planning of gliomas. While machine learning is popularly adopted to process and analyze MRI images, most existing tools are based on complete sets of multi-modality images that are costly and sometimes impossible to acquire in real clinical scenarios. In this work, we address the challenge of multi-modality glioma MRI synthesis often with incomplete MRI modalities. We propose 3D Common-feature learning-based Context-aware Generative Adversarial Network (CoCa-GAN) for this purpose. In particular, our proposed CoCa-GAN method adopts the encoder-decoder architecture to map the input modalities into a common feature space by the encoder, from which (1) the missing target modality(-ies) can be synthesized by the decoder, and also (2) the jointly conducted segmentation of the gliomas can help the synthesis task to better focus on the tumor regions. The synthesis and segmentation tasks share the same common feature space, while multi-task learning boosts both their performances. In particular, for the encoder to derive the common feature space, we propose and validate two different models, i.e., (1) early-fusion CoCa-GAN (eCoCa-GAN) and (2) intermediate-fusion CoCa-GAN (iCoCa-GAN). The experimental results demonstrate that the proposed iCoCa-GAN outperforms other state-of-the-art methods in synthesis of missing image modalities. Moreover, our method is flexible to handle the arbitrary combination of input/output image modalities, which makes it feasible to process brain tumor MRI data in real clinical circumstances. © 2022 Elsevier B.V.","Brain Neoplasms; Glioma; Humans; Image Processing, Computer-Assisted; Machine Learning; Magnetic Resonance Imaging; Brain; Decoding; Diagnosis; Magnetic resonance imaging; Signal encoding; Tumors; Brain tumors; Common feature learning; Common features; Context-Aware; Diagnosis planning; Feature learning; Feature space; Image modality; Images synthesis; Multi-modal; Article; calculation; comparative study; cross validation; glioma; human; image analysis; image processing; image reconstruction; image segmentation; machine learning; major clinical study; mean squared error; multimodal imaging; nuclear magnetic resonance imaging; three dimensional common feature learning based context aware generative adversarial network; Wilcoxon signed ranks test; brain tumor; diagnostic imaging; glioma; machine learning; nuclear magnetic resonance imaging; procedures; Generative adversarial networks","Common feature learning; Generative adversarial network; Image synthesis; Multi-task learning","Article","Final","","Scopus","2-s2.0-85129961137"
"Du W.-L.; Zhou Y.; Zhu H.; Zhao J.; Shao Z.; Tian X.","Du, Wen-Liang (55265123100); Zhou, Yong (35480110700); Zhu, Hancheng (55532134400); Zhao, Jiaqi (57138970300); Shao, Zhiwen (57189600890); Tian, Xiaolin (7202380154)","55265123100; 35480110700; 55532134400; 57138970300; 57189600890; 7202380154","A Semi-Supervised Image-to-Image Translation Framework for SAR-Optical Image Matching","2022","IEEE Geoscience and Remote Sensing Letters","19","","4516305","","","","10.1109/LGRS.2022.3223353","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142777987&doi=10.1109%2fLGRS.2022.3223353&partnerID=40&md5=0366797c0d795f602ca243d74eb3389a","Synthetic aperture radar (SAR) and optical image matching aims to acquire correspondences from a certain pair of SAR and optical images. Recent advances in the image-to-image translation provided a way to simplify the SAR-optical image matching into the SAR-SAR or optical-optical image matchings. The existing image-to-image translations mainly focus on supervised or unsupervised learning. However, gathering sufficient amounts of aligned training data for supervised learning is challenging, while unsupervised learning cannot guarantee enough correct correspondences. In this work, we investigate the applicability of semi-supervised image-to-image translation for SAR-optical image matching such that both aligned and unaligned SAR-optical images could be used. To this end, we combine the benefits of both supervised and unsupervised well-known image-to-image translation methods, i.e., Pix2pix and CycleGAN, and propose a simple yet effective semi-supervised image-to-image translation framework. Through extensive experimental comparisons to the baseline methods, we verify the effectiveness of the proposed framework in both semi-supervised and fully supervised settings.  © 2004-2012 IEEE.","Generative adversarial networks; Geometrical optics; Image matching; Radar imaging; Unsupervised learning; Generative adversarial network; Image translation; Images synthesis; Optical image; Optical-; Semi-supervised; Semi-supervised-image-synthesis; Synthetic aperture radar; Synthetic aperture radar images; Training data; network analysis; radar imagery; supervised learning; synthetic aperture radar; Synthetic aperture radar","Generative adversarial networks (GANs); image matching; semi-supervised image synthesis; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85142777987"
"Li J.; Qu Z.; Yang Y.; Zhang F.; Li M.; Hu S.","Li, Jitao (57427282700); Qu, Zongjin (57225721602); Yang, Yue (57826464300); Zhang, Fuchun (57606432800); Li, Meng (57210022184); Hu, Shunbo (16417168200)","57427282700; 57225721602; 57826464300; 57606432800; 57210022184; 16417168200","TCGAN: A transformer-enhanced GAN for PET synthetic CT","2022","Biomedical Optics Express","13","11","","6003","6018","15","10.1364/BOE.467683","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143196351&doi=10.1364%2fBOE.467683&partnerID=40&md5=122365358c6a222780d55e787d5dbb29","Multimodal medical images can be used in a multifaceted approach to resolve a wide range of medical diagnostic problems. However, these images are generally difficult to obtain due to various limitations, such as cost of capture and patient safety. Medical image synthesis is used in various tasks to obtain better results. Recently, various studies have attempted to use generative adversarial networks for missing modality image synthesis, making good progress. In this study, we propose a generator based on a combination of transformer network and a convolutional neural network (CNN). The proposed method can combine the advantages of transformers and CNNs to promote a better detail effect. The network is designed for positron emission tomography (PET) to computer tomography synthesis, which can be used for PET attenuation correction. We also experimented on two datasets for magnetic resonance T1- to T2-weighted image synthesis. Based on qualitative and quantitative analyses, our proposed method outperforms the existing methods.  © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.","Computerized tomography; Convolutional neural networks; Diagnosis; Generative adversarial networks; Magnetic resonance; Medical imaging; Medical problems; Attenuation correction; Convolutional neural network; Diagnostic problem; Images synthesis; Medical diagnostics; Multi-faceted approach; Multimodal medical images; Patient safety; Positron emission tomography attenuations; T2 weighted; article; computer assisted tomography; controlled study; convolutional neural network; human; patient safety; positron emission tomography; quantitative analysis; synthesis; T2 weighted imaging; Positron emission tomography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143196351"
"Wu S.; Tang H.; Jing X.-Y.; Qian J.; Sebe N.; Yan Y.; Zhang Q.","Wu, Songsong (24485676900); Tang, Hao (57208238003); Jing, Xiao-Yuan (7202420489); Qian, Jianjun (55903812900); Sebe, Nicu (57204924633); Yan, Yan (56431699600); Zhang, Qinghua (57221143247)","24485676900; 57208238003; 7202420489; 55903812900; 57204924633; 56431699600; 57221143247","Cross-view panorama image synthesis with progressive attention GANs","2022","Pattern Recognition","131","","108884","","","","10.1016/j.patcog.2022.108884","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133867166&doi=10.1016%2fj.patcog.2022.108884&partnerID=40&md5=a5eb4344280e2b8ca21ad9db835e88f9","Despite the significant progress of conditional image generation, it remains difficult to synthesize a ground-view panorama image from a top-view aerial image. Among the core challenges are the vast differences in image appearance and resolution between aerial images and panorama images, and the limited aside information available for top-to-ground viewpoint transformation. To address these challenges, we propose a new Progressive Attention Generative Adversarial Network (PAGAN) with two novel components: a multistage progressive generation framework and a cross-stage attention module. In the first stage, an aerial image is fed into a U-Net-like network to generate one local region of the panorama image and its corresponding segmentation map. Then, the synthetic panorama image region is extended and refined through the following generation stages with our proposed cross-stage attention module that passes semantic information forward stage-by-stage. In each of the successive generation stages, the synthetic panorama image and segmentation map are separately fed into an image discriminator and a segmentation discriminator to compute both later real and fake, as well as feature alignment score maps for discrimination. The model is trained with a novel orientation-aware data augmentation strategy based on the geometric relation between aerial and panorama images. Extensive experimental results on two cross-view datasets show that PAGAN generates high-quality panorama images with more convincing details than state-of-the-art methods. © 2022 Elsevier Ltd","Antennas; Discriminators; Fake detection; Image segmentation; Semantics; Cross-stage attention; Cross-view panorama image synthesis; Data augmentation; Image generations; Images synthesis; Multi-stage image generation; Multi-stages; Orientation-aware data augmentation; Panorama images; Progressive attention GAN; Generative adversarial networks","Cross-stage attention; Cross-view panorama image synthesis; Multi-stage image generation; Orientation-aware data augmentation; Progressive attention GANs","Article","Final","","Scopus","2-s2.0-85133867166"
"Chen Y.; Yu X.; Liu S.; Gao W.; Li G.","Chen, Yuanqi (57209029113); Yu, Xiaoming (57195495766); Liu, Shan (55600731300); Gao, Wei (57200706579); Li, Ge (57208803757)","57209029113; 57195495766; 55600731300; 57200706579; 57208803757","Zero-shot unsupervised image-to-image translation via exploiting semantic attributes","2022","Image and Vision Computing","124","","104489","","","","10.1016/j.imavis.2022.104489","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132725141&doi=10.1016%2fj.imavis.2022.104489&partnerID=40&md5=3a481f70df33cfd7c744d6e7a10af65e","Recent studies have shown remarkable success in unsupervised image-to-image translation. However, if there is no access to enough images in target classes, learning a mapping from source classes to the target classes always suffers from mode collapse, especially the zero shot case, which limits the application of the existing methods. In this work, we propose a zero-shot unsupervised image-to-image translation framework to address this limitation, by effectively associating categories with their side information like attributes. To generalize the translator to previously unseen classes, we introduce two strategies for exploiting the semantic attribute space. First, we propose to preserve semantic relations to the visual space for effective guidance on where to map the input image. Second, expanding attribute space is introduced by utilizing attribute vectors of unseen classes, which alleviates the mapping bias for unseen classes. Both of these strategies encourage the translator to explore the modes of unseen classes. Quantitative and qualitative results on different datasets validate the effectiveness of our proposed approach. Moreover, we demonstrate that our framework can be applied to fashion design task. The code is available at https://github.com/cyq373/ZUNIT. © 2022 Elsevier B.V.","Generative adversarial networks; Mapping; Semantics; Vector spaces; Attribute vectors; Image translation; Image-to-image translation; Images synthesis; Input image; Semantic attribute; Semantic relations; Side information; Target class; Visual space; Zero-shot learning","Generative adversarial networks; Image synthesis; Image-to-image translation; Zero-shot learning","Article","Final","","Scopus","2-s2.0-85132725141"
"Shvai N.; Hasnat A.; Nakib A.","Shvai, Nadiya (36961257700); Hasnat, Abul (57507092700); Nakib, Amir (16837255900)","36961257700; 57507092700; 16837255900","Multiple auxiliary classifiers GAN for controllable image generation: Application to license plate recognition","2023","IET Intelligent Transport Systems","17","1","","243","254","11","10.1049/itr2.12251","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139684151&doi=10.1049%2fitr2.12251&partnerID=40&md5=4fecb36af4abd23c06fb69f44deffbfb","One of the main challenges in developing machine learning (ML) applications is the lack of labeled and balanced datasets. In the literature, different techniques tackle this problem via augmentation, rendering, and over-sampling. Still, these methods produce datasets that appear less natural, exhibit poor balance, and have less variation. One potential solution is to leverage the Generative Adversarial Network (GAN) which achieves remarkable results in the generation of high-fidelity natural images. However, expanding the ability of GANs' to control generated image attributes with supervisory information remains a challenge. This research aims to propose an efficient method to generate high-fidelity natural images with total control of its main attributes. Therefore, this paper proposes a novel Multiple Auxiliary Classifiers GAN (MAC-GAN) framework based on Auxiliary Classifier GAN (AC-GAN), multi-conditioning, Wasserstein distance, gradient penalty, and dynamic loss. It is therefore presented as an efficient solution for highly controllable image synthesis red that allows to enrich and re-balance datasets beyond data augmentation. Furthermore, the effectiveness of MAC-GAN images on a target ML application called Automatic License Plate Recognition (ALPR) under limited resource constraints is probed. The improvement achieved is over 5% accuracy, which is mainly due to the ability of the MAC-GAN to create a balanced dataset with controllable synthesis and produce multiple (different) images with the same attributes, thus increasing the variation of the dataset in a more elaborate way than data augmentation techniques. © 2022 The Authors. IET Intelligent Transport Systems published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Image classification; Image enhancement; Intelligent systems; License plates (automobile); Optical character recognition; Traffic control; Balanced datasets; Data augmentation; High-fidelity; Image attributes; Image generations; Labeled dataset; Licenses plate recognition; Machine learning applications; Natural images; Over sampling; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85139684151"
"Gao J.; Zhao W.; Li P.; Huang W.; Chen Z.","Gao, Jing (57189334189); Zhao, Wenhan (57219697249); Li, Peng (57214069572); Huang, Wei (57218084563); Chen, Zhikui (56020772800)","57189334189; 57219697249; 57214069572; 57218084563; 56020772800","LEGAN: A Light and Effective Generative Adversarial Network for medical image synthesis","2022","Computers in Biology and Medicine","148","","105878","","","","10.1016/j.compbiomed.2022.105878","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134428216&doi=10.1016%2fj.compbiomed.2022.105878&partnerID=40&md5=143f44427e1e476074f9085287263cd8","Medical image synthesis plays an important role in clinical diagnosis by providing auxiliary pathological information. However, previous methods usually utilize the one-step strategy designed for wild image synthesis, which are not sensitive to local details of tissues within medical images. In addition, these methods consume a great number of computing resources in generating medical images, which seriously limits their applicability in clinical diagnosis. To address the above issues, a Light and Effective Generative Adversarial Network (LEGAN) is proposed to generate high-fidelity medical images in a lightweight manner. In particular, a coarse-to-fine paradigm is designed to imitate the painting process of humans for medical image synthesis within a two-stage generative adversarial network, which guarantees the sensitivity to local information of medical images. Furthermore, a low-rank convolutional layer is introduced to construct LEGAN for lightweight medical image synthesis, which utilizes principal components of full-rank convolutional kernels to reduce model redundancy. Additionally, a multi-stage mutual information distillation is devised to maximize dependencies of distributions between generated and real medical images in model training. Finally, extensive experiments are conducted in two typical tasks, i.e., retinal fundus image synthesis and proton density weighted MR image synthesis. The results demonstrate that LEGAN outperforms the comparison methods by a significant margin in terms of Fréchet inception distance (FID) and Number of parameters (NoP). © 2022 Elsevier Ltd","Algorithms; Fundus Oculi; Humans; Image Processing, Computer-Assisted; Convolution; Diagnosis; Distillation; Magnetic resonance imaging; Medical imaging; Clinical diagnosis; Coarse to fine; Coarse-to-fine paradigm; Computing resource; High-fidelity; Images synthesis; Knowledge distillation; Medical image synthesis; Mutual informations; Two-stage generative adversarial network; Article; convolution algorithm; diagnostic imaging; eye fundus; human; image analysis; image processing; imaging algorithm; information processing; intermethod comparison; light and effective generative adversarial network; normal human; nuclear magnetic resonance imaging; qualitative analysis; quantitative analysis; retina image; algorithm; image processing; Generative adversarial networks","Coarse-to-fine paradigm; Knowledge distillation; Medical image synthesis; Mutual information; Two-stage generative adversarial network","Article","Final","","Scopus","2-s2.0-85134428216"
"Martinez J.A.C.; Adarme M.X.O.; Turnes J.N.; Costa G.A.O.P.; De Almeida C.A.; Feitosa R.Q.","Martinez, J.A.C. (57218451094); Adarme, M.X.O. (57274825100); Turnes, J.N. (57216587254); Costa, G.A.O.P. (25642386000); De Almeida, C.A. (57209845177); Feitosa, R.Q. (6602453684)","57218451094; 57274825100; 57216587254; 25642386000; 57209845177; 6602453684","A COMPARISON OF CLOUD REMOVAL METHODS FOR DEFORESTATION MONITORING IN AMAZON RAINFOREST","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2022","","665","671","6","10.5194/isprs-archives-XLIII-B3-2022-665-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131917969&doi=10.5194%2fisprs-archives-XLIII-B3-2022-665-2022&partnerID=40&md5=8447700e1fefacd1f19230a481c218a7","Deforestation in tropical rainforests is a major source of carbon dioxide emissions, an important driver of climate change. For decades, the Brazilian government has maintained monitoring programs for deforestation detection in the Brazilian Legal Amazon area based on remotely sensed optical images in a protocol that involves considerable efforts of visual interpretation. However, the Amazon region is covered with clouds for most of the year, and deforestation assessment can rely only on images acquired in the dry season when cloud-free images are more likely to capture. One possibility to lessen that restriction and enable deforestation detection throughout the year is to synthesize cloud-free optical images from corresponding SAR images, which are only marginally influenced by atmospheric conditions. This work compares a set of such image synthesis methods, considering deforestation detection in the Amazon forest as the target application. Specifically, we evaluate three deep learning methods for cloud removal in Sentinel-2 images: a conditional Generative Adversarial Network (cGAN) based on the pix2pixi architecture; an extension of that method, which uses atrous convolutions (Atrous cGANi) to enhance fine image details; and a non-generative method (DSen2-CRi) based on residual networks. In the evaluation, we assess both the quality of the generated images and the accuracy obtained when performing deforestation detection from those images. We further compare those methods with an image aggregation tool available in Google Earth Engine (GEE Tooli), which creates cloud-free mosaics from sequences of images acquired at nearby dates. In this study, we considered two sites in the Brazilian Amazon, characterized by distinct vegetation and deforestation patterns. In terms of the quality metrics and classification accuracy, the Atrous cGANi was the best performing deep learning method. The GEE Tooli outperformed all those methods when dealing with images from the dry season but turned out to be the poorest performing method in the wet season.  © Authors 2022","Carbon dioxide; Data fusion; Deep learning; Deforestation; Generative adversarial networks; Geometrical optics; Image acquisition; Image enhancement; Quality control; Radar imaging; Synthetic aperture radar; Amazon rain forest; Cloud removal; Deep learning; Dry seasons; Learning methods; Optical data; Optical image; Optical imagery; Removal method; SAR-optical data fusion; Global warming","Cloud Removal; Deep learning; Deforestation; Optical imagery; SAR-optical Data fusion","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131917969"
"Liang K.; Lei H.; Wang M.","Liang, Kaijun (58066784200); Lei, Haopeng (52063488600); Wang, Mingwen (56027746900)","58066784200; 52063488600; 56027746900","VCL-GAN: A Variational Contrastive Learning Generative Adversarial Network for Image Synthesis","2022","Proceedings - 2022 9th International Conference on Digital Home, ICDH 2022","","","","50","54","4","10.1109/ICDH57206.2022.00015","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146318273&doi=10.1109%2fICDH57206.2022.00015&partnerID=40&md5=e6ce37c8e1dac0069b17cdc89088b04f","Generative Adversarial Networks (GANs) have worked well for image generation, but recent works have shown that their generated images lack diversity. In response to this problem, we propose an image generation network based on contrastive learning (CL) and Autoencoder (AE). Firstly, we try to let the network learn to extract different types of features by Siamese network (SN), which is a classic of contrastive learning. Secondly, we perform variational processing on the resulting codes, so that the GANs can generate high-quality images of multiple types by the codes. Finally, extensive experiments and comparisons with the state-of-the-art are conducted on the Tsinghua dog dataset that the method has made progress in generating realistic images of different types. © 2022 IEEE.","Computer vision; Learning systems; Auto encoders; Component; Contrastive learning; Features extraction; Generative network; High quality images; Image generations; Images synthesis; Learn+; Network-based; Generative adversarial networks","component; contrastive learning; feature extraction; generative network; image generation","Conference paper","Final","","Scopus","2-s2.0-85146318273"
"Iklima Z.; Kadarina T.M.; Ihsanto E.","Iklima, Zendi (57536635500); Kadarina, Trie Maya (57202162005); Ihsanto, Eko (6504754280)","57536635500; 57202162005; 6504754280","Realistic image synthesis of COVID-19 chest X-rays using depthwise boundary equilibrium generative adversarial networks","2022","International Journal of Electrical and Computer Engineering","12","5","","5444","5454","10","10.11591/ijece.v12i5.pp5444-5454","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135264429&doi=10.11591%2fijece.v12i5.pp5444-5454&partnerID=40&md5=412f20976f8fe395f8db9d60b70f5da8","Researchers in various related fields research preventing and controlling the spread of the coronavirus disease (COVID-19) virus. The spread of the COVID-19 is increasing exponentially and infecting humans massively. Preliminary detection can be observed by looking at abnormal conditions in the airways, thus allowing the entry of the virus into the patient's respiratory tract, which can be represented using computer tomography (CT) scan and chest X-ray (CXR) imaging. Particular deep learning approaches have been developed to classify COVID-19 CT or CXR images such as convolutional neural network (CNN), and deep convolutional neural network (DCNN). However, COVID-19 CXR dataset was measly opened and accessed. Particular deep learning method performance can be improved by augmenting the dataset amount. Therefore, the COVID-19 CXR dataset was possibly augmented by generating the synthetic image. This study discusses a fast and real-like image synthesis approach, namely depthwise boundary equilibrium generative adversarial network (DepthwiseBEGAN). DepthwiseBEGAN was reduced memory load 70.11% in training processes compared to the conventional BEGAN. DepthwiseBEGAN synthetic images were inspected by measuring the Fréchet inception distance (FID) score with the real-to-real score equal to 4.3866 and real-to-fake score equal to 4.4674. Moreover, generated DepthwiseBEGAN synthetic images improve 22.59% accuracy of conventional CNN models. © 2022 Institute of Advanced Engineering and Science. All rights reserved.","","Chest X-ray; Convolutional neural network; COVID-19 virus; DepthwiseBEGAN; Image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85135264429"
"Qadir H.A.; Balasingham I.; Shin Y.","Qadir, Hemin Ali (56331081100); Balasingham, Ilangko (6602773063); Shin, Younghak (44061719700)","56331081100; 6602773063; 44061719700","Simple U-net based synthetic polyp image generation: Polyp to negative and negative to polyp","2022","Biomedical Signal Processing and Control","74","","103491","","","","10.1016/j.bspc.2022.103491","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122569475&doi=10.1016%2fj.bspc.2022.103491&partnerID=40&md5=a38430e157062b76a249eea94784fc63","Synthetic polyp generation is a good alternative to overcome the privacy problem of medical data and the lack of various polyp samples. In this study, we propose a deep learning-based polyp image generation framework that generates synthetic polyp images that are similar to real ones. We suggest a framework that converts a given polyp image into a negative image (image without a polyp) using a simple conditional GAN architecture and then converts the negative image into a new-looking polyp image using the same network. In addition, by using the controllable polyp masks, polyps with various characteristics can be generated from one input condition. The generated polyp images can be used directly as training images for polyp detection and segmentation without additional labeling. To quantitatively assess the quality of generated synthetic polyps, we use public polyp image and video datasets combined with the generated synthetic images to examine the performance improvement of several detection and segmentation models. Experimental results show that we obtain performance gains when the generated polyp images are added to the training set. © 2022 Elsevier Ltd","Convolutional neural networks; Deep learning; Image enhancement; Image segmentation; Colonoscopy; Condition; Convolutional neural network; Image generations; Images synthesis; Medical data; Negative image; Polyp detection; Privacy problems; Simple++; algorithm; Article; colonoscopy; controlled study; convolutional neural network; entropy; human; image reconstruction; image segmentation; nerve cell network; nonhuman; polyp; quantitative analysis; scientific literature; synthesis; Generative adversarial networks","Colonoscopy; Convolutional neural network; Generative adversarial networks; Image synthesis; Polyp detection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122569475"
"Lee K.W.; Chin R.K.Y.","Lee, Kin Wai (57221051650); Chin, Renee Ka Yin (55387633100)","57221051650; 55387633100","A Comparative Study of COVID-19 CT Image Synthesis using GAN and CycleGAN","2022","4th IEEE International Conference on Artificial Intelligence in Engineering and Technology, IICAIET 2022","","","","","","","10.1109/IICAIET55139.2022.9936810","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142903972&doi=10.1109%2fIICAIET55139.2022.9936810&partnerID=40&md5=84aef2bd12c488eedcb90f596d8eabf1","Generative adversarial networks (GANs) have been very successful in many applications of medical image synthesis, which hold great clinical value in diagnosis and analysis tasks, especially when data is scarce. This study compares the two most adopted generative modelling algorithms in recent medical image synthesis tasks, namely the traditional Generative Adversarial Network (GAN) and Cycle-consistency Generative Adversarial Network (CycleGAN) for COVID-19 CT image synthesis. Experiments show that very plausible synthetic COVID-19 images with a clear vision of artificially generated ground glass opacity (GGO) can be generated with CycleGAN when trained using an identity loss constant at 0.5. Moreover, it is found that the synthesis of the synthetic GGO features is generalized across images with different chest and lung structures, which suggests that diverse patterns of GGO can be synthesized using a conventional Image-to- Image translation setting without additional auxiliary conditions or visual annotations. In addition, similar experiment setting achieves encouraging perceptual quality with a Fréchet Inception Distance score of 0.347, which outperforms GAN at 0.383 and CycleGAN at 0.380 with an identity loss constant of 0.005. The experiment outcomes postulate a negative correlation between the strength of the identity loss and the significance of the synthetic instances manifested on the generated images, which highlights an interesting research path to improve the quality of generated images without compromising the significance of synthetic instances upon the image translation.  © 2022 IEEE.","Computerized tomography; Diagnosis; Generative adversarial networks; Image enhancement; Medical imaging; Chest computerized tomography; Clinical value; Comparatives studies; CT Image; Generative model; Ground-glass opacity; Image translation; Image-to-image translation; Images synthesis; Medical image synthesis; COVID-19","chest computerized tomography; COVID-19; generative adversarial networks; image-to-image translation; medical image synthesis","Conference paper","Final","","Scopus","2-s2.0-85142903972"
"Tyagi S.; Yadav D.","Tyagi, Shobhit (57202332109); Yadav, Divakar (24470227900)","57202332109; 24470227900","A Comprehensive Review on Image Synthesis with Adversarial Networks: Theory, Literature, and Applications","2022","Archives of Computational Methods in Engineering","29","5","","2685","2705","20","10.1007/s11831-021-09672-w","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118103705&doi=10.1007%2fs11831-021-09672-w&partnerID=40&md5=98a1ce5222622723fa3774ae4abeac37","In recent few years, deep learning made a huge impact in engineering and scientific domains. One of the most-suited field is the image synthesis and editing techniques. Image Synthesis is an integral field of Computer vision and Expert systems. Generative adversarial networks (or GANs) have gained a lot of attention as it achieves better performance compared to the traditional methods. They can also be used in many image synthesis and editing areas like human image synthesis, face aging, text to-image synthesis and 3D image synthesis. In this survey, several state-of-the art image synthesis and editing techniques are discussed which uses convolutional neural networks to generate fake images. We also discuss the advantages, limitations and features of such methods along with how the image quality changes with the size of dataset used for learning. At last, we discuss some ways to detect fake images generated by image synthesis techniques. © 2021, CIMNE, Barcelona, Spain.","Convolutional neural networks; Deep learning; Expert systems; Image processing; 3-D image; 3D-images; Adversarial networks; Computer expert systems; Computer vision system; Image editing; Images synthesis; Integral field; Performance; State of the art; Generative adversarial networks","","Review","Final","","Scopus","2-s2.0-85118103705"
"Niu W.; Zhang K.; Li D.; Luo W.","Niu, Wenjia (57776689700); Zhang, Kaihao (57190277075); Li, Dongxu (57210855700); Luo, Wenhan (49061319200)","57776689700; 57190277075; 57210855700; 49061319200","Four-player GroupGAN for weak expression recognition via latent expression magnification","2022","Knowledge-Based Systems","251","","109304","","","","10.1016/j.knosys.2022.109304","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133277092&doi=10.1016%2fj.knosys.2022.109304&partnerID=40&md5=829f6ed545c54d17f805da218dc62843","Facial expression recognition has a wide range of applications in the real world. Although many existing deep learning methods have achieved remarkable success, weak expression recognition remains a challenging task because of the significant domain gap between a weak expression and its peak expression counterpart. One idea to solve this problem is to find an effective way to bridge the gap between the two domains by either transfer learning or cross-domain image synthesis. In this paper, we propose a Group Generative Adversarial Network (GroupGAN) that recognizes weak facial expression by magnifying the expressions to stronger or peak ones. Different from the traditional GAN which typically has only one generator and one discriminator, the proposed GroupGAN has one generator, one extractor and two discriminators. Similar to the “two-player game” analogy of the traditional GAN, in our setting the generator along with feature extractor act as one group to compete with the other group of the two distinct discriminators. Extensive experiments show that the proposed GroupGAN significantly improves the performance of weak expression recognition, and is able to magnify weak expressions, thus facilitating many expression-related vision tasks like sketch recognition. © 2022 Elsevier B.V.","Deep learning; Face recognition; Game theory; Learning systems; Deep CNN; Expression recognition; Face expression recognition; Facial expression recognition; Four player; GAN; Learning methods; Real-world; Two domains; Weak expression; Generative adversarial networks","Deep CNN; Face expression recognition; Four players; GAN; Weak expression","Article","Final","","Scopus","2-s2.0-85133277092"
"Saadatnejad S.; Li S.; Mordan T.; Alahi A.","Saadatnejad, Saeed (57211818836); Li, Siyuan (57836140300); Mordan, Taylor (57201384356); Alahi, Alexandre (34869135400)","57211818836; 57836140300; 57201384356; 34869135400","A Shared Representation for Photorealistic Driving Simulators","2022","IEEE Transactions on Intelligent Transportation Systems","23","8","","13835","13845","10","10.1109/TITS.2021.3131303","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120863313&doi=10.1109%2fTITS.2021.3131303&partnerID=40&md5=84140ed6ff727a4b423dff475c54ba67","A powerful simulator highly decreases the need for real-world tests when training and evaluating autonomous vehicles. Data-driven simulators flourished with the recent advancement of conditional Generative Adversarial Networks (cGANs), providing high-fidelity images. The main challenge is synthesizing photorealistic images while following given constraints. In this work, we propose to improve the quality of generated images by rethinking the discriminator architecture. The focus is on the class of problems where images are generated given semantic inputs, such as scene segmentation maps or human body poses. We build on successful cGAN models to propose a new semantically-aware discriminator that better guides the generator. We aim to learn a shared latent representation that encodes enough information to jointly do semantic segmentation, content reconstruction, along with a coarse-to-fine grained adversarial reasoning. The achieved improvements are generic and simple enough to be applied to any architecture of conditional image synthesis. We demonstrate the strength of our method on the scene, building, and human synthesis tasks across three different datasets. The code is available https://github.com/vita-epfl/SemDisc. © 2000-2011 IEEE.","Computer vision; Image enhancement; Network architecture; Semantic Segmentation; Semantics; Simulators; Autonomous Vehicles; Data driven; Driving simulator; High-fidelity images; Images synthesis; Photo-realistic; Photorealistic images; Real-world tests; Shared representation.; Shared representations; Generative adversarial networks","Autonomous vehicles; Generative adversarial networks; Image synthesis; Shared representation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85120863313"
"Hattori S.; Aiba K.; Takahara M.","Hattori, Shun (58073785200); Aiba, Kizuku (58074304400); Takahara, Madoka (58075368600)","58073785200; 58074304400; 58075368600","R2-B2: A Metric of Synthesized Image's Photorealism by Regression Analysis based on Recognized Objects' Bounding Box","2022","2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems, SCIS and ISIS 2022","","","","","","","10.1109/SCISISIS55246.2022.10001857","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146696450&doi=10.1109%2fSCISISIS55246.2022.10001857&partnerID=40&md5=fb6bb5504b3ed67848041b8bb37638b6","In recent years, a lot of researches on AI (Artificial Intelligence) for Image Synthesis and Image Generation have been being conducted actively, and state of the art GANs (Generative Adversarial Networks) for text-to-image have been able to generate precise images with high photorealism for a text-based user query (but also no-good images). However, it is pointed out that the precision of all images generated for a query has been not always enough high. Therefore, for practical usages, they are required to be re-ranked and/or filtered based on some sort of metric(s). This paper proposes a novel metric, R2-B2 (RR-BB), on photorealism, especially 'size balance' (i.e., balance between in-image objects' size), of a manually or automatically synthesized image by Regression analysis based on multiple Recognized objects' Bounding Box, i.e., the position (x, y) and size (width, height, or area) of objects recognized in the image. © 2022 IEEE.","Generative adversarial networks; Image quality; Object detection; Quality control; Regression analysis; Correlation analysis; Image evaluation; Image quality assessment; Image-quality metrics; No-reference image quality assessment; No-reference images; Objects detection; Objects recognition; Photorealism; Synthesized images; Object recognition","correlation analysis; image evaluation; image quality metrics; no-reference image quality assessment; object detection; object recognition; regression analysis","Conference paper","Final","","Scopus","2-s2.0-85146696450"
"You A.; Kim J.K.; Ryu I.H.; Yoo T.K.","You, Aram (57701432500); Kim, Jin Kuk (57216274046); Ryu, Ik Hee (57189390195); Yoo, Tae Keun (57226626960)","57701432500; 57216274046; 57189390195; 57226626960","Application of generative adversarial networks (GAN) for ophthalmology image domains: a survey","2022","Eye and Vision","9","1","6","","","","10.1186/s40662-022-00277-3","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126597413&doi=10.1186%2fs40662-022-00277-3&partnerID=40&md5=3854062598279123a9b33768af2ff6f8","Background: Recent advances in deep learning techniques have led to improved diagnostic abilities in ophthalmology. A generative adversarial network (GAN), which consists of two competing types of deep neural networks, including a generator and a discriminator, has demonstrated remarkable performance in image synthesis and image-to-image translation. The adoption of GAN for medical imaging is increasing for image generation and translation, but it is not familiar to researchers in the field of ophthalmology. In this work, we present a literature review on the application of GAN in ophthalmology image domains to discuss important contributions and to identify potential future research directions. Methods: We performed a survey on studies using GAN published before June 2021 only, and we introduced various applications of GAN in ophthalmology image domains. The search identified 48 peer-reviewed papers in the final review. The type of GAN used in the analysis, task, imaging domain, and the outcome were collected to verify the usefulness of the GAN. Results: In ophthalmology image domains, GAN can perform segmentation, data augmentation, denoising, domain transfer, super-resolution, post-intervention prediction, and feature extraction. GAN techniques have established an extension of datasets and modalities in ophthalmology. GAN has several limitations, such as mode collapse, spatial deformities, unintended changes, and the generation of high-frequency noises and artifacts of checkerboard patterns. Conclusions: The use of GAN has benefited the various tasks in ophthalmology image domains. Based on our observations, the adoption of GAN in ophthalmology is still in a very early stage of clinical validation compared with deep learning classification techniques because several problems need to be overcome for practical use. However, the proper selection of the GAN technique and statistical modeling of ocular imaging will greatly improve the performance of each image analysis. Finally, this survey would enable researchers to access the appropriate GAN technique to maximize the potential of ophthalmology datasets for deep learning research. © 2022, The Author(s).","","Data augmentation; Deep learning; Domain transfer; Generative adversarial network; Ophthalmology image","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85126597413"
"Cao S.; Liu X.; Mao X.; Zou Q.","Cao, Shenhao (57222901225); Liu, Xiaohui (57770096600); Mao, Xiuqing (36975998100); Zou, Qin (55628590470)","57222901225; 57770096600; 36975998100; 55628590470","A review of human face forgery and forgery-detection technologies; [人脸伪造及检测技术综述]","2022","Journal of Image and Graphics","27","4","","1023","1038","15","10.11834/jig.200466","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129735260&doi=10.11834%2fjig.200466&partnerID=40&md5=a97cce7ae29b9217bfc21c9bf84a35c1","Face image synthesis is one of the most important sub-topics in image synthesis. Deep learning methods like the generative adversarial networks and autoencoder networks enable the current generation technology to generate facial images that are indistinguishable by human eyes. The illegal use of face forgery technology has damaged citizens' portrait rights and reputation rights and weakens the national political and economic security. Based on summarizing the key technologies and critical review of face forgery and forged-face detection, our research analyzes the limitations of current forgery and detection technologies, which is intended to provide a reference for subsequent research on fake-face detection. Our analysis is shown as bellows: 1) the technologies for face forgery are mainly divided into the use of generative confrontation technology to generate a category of new faces and the use of existing face editing techniques. First, our review introduces the development of generative adversarial network and its application in human face image generation, shows the face images generated at different development stages, and targets that generative adversarial network provides the possibility of generating fake face images with high resolution, real look and feel, diversified styles and fine details; furthermore, it introduces face editing technology like face swap, face reenactment and the open-source implementation of the current face swap and face reenactment technology on the aspects of network structure, versatility and authenticity of the generated image. In particular, face exchange and face reconstruction technologies both decompose the face into two spaces of appearance and attributes, design different network structures and loss functions to transfer targeted features, and use an integrated generation adversarial network to improve the reality of the generated results. 2) The technologies for fake face detection, according to the difference of media carriers, can be divided into fake face image detection and fake face video detection. Our review first details the use of statistical distribution differences, splicing residual traces, local defects and other features to identify fake facial image generated from straightforward generative adversarial network and face editing technologies. Next, in terms of the difference analysis of extracting forged features, the fake facial video detection technology is classified into technology based on inter-frame information, intra-frame information and physiological signals. The methodology of extracting features, the design of network structures and the use scenarios were illustrated in detail. The current fake image detection technology mainly uses convolutional neural networks to extract fake features, and realizes the location and detection of fake regions simultaneously, while fake video detection technologies mainly use a integration of convolutional neural networks and recurrent neural networks to extract the same features inter and inner frames; after that, the public data sets of fake-face detection are sorted out, and the comparison results of multiple fake-face detection methods are illustrated for multiple public data sets. 3) The summary and the prospect part analyze the weaknesses of the current face forgery technologies and forged-face detection technologies, and gives feasible directions for improvement. The current face video forgery technology mainly uses the method of partially modifying the face area with the following defects. There are forgery traces in a single video frame, such as blurred side faces and missing texture details in the face parts. The relevance of video frames was not considered and there were inconsistencies amongst the generated video frames, such as frame jumps, and the large difference in the position of key points of the two frames before and after; and the generated face video lacks normal biological information, such as blinks and micro expressions. The current forgery-detection technologies have poor robustness to real scenes and poor robustness against image and video compression algorithms. The detection methods trained on high-resolution datasets are not suitable for low-resolution images and videos. Forgery detection technologies are difficult to review the issue of continuous upgrade and evolution of forged technology. The further improvement is illustrated on forgery-detection technologies. For instance, when generating videos, it would be useful to add the location information of the face into the network to improve the coherence of the generated video. In related to forgery detection, the forgery features in the space and frequency domains can be fused together for feature extraction, and the 3D convolution and metric learning can be used to form a targeted feature distribution for forged faces and the genuine faces. The development of face forgery is featured by few-shot learning, strong versatility and high fidelity. Forgery-face detection technology is intended to high versatility, strong compression resistance, few-shot learning and efficient computing. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Face forgery; Face forgery detection; Face reenactment; Face swap; Generative adversarial network (GAN)","Review","Final","","Scopus","2-s2.0-85129735260"
"Li B.; Deng S.-H.; Liu B.; Li Y.; He Z.-F.; Lai Y.-K.; Zhang C.; Chen Z.","Li, Bo (57221874131); Deng, Shu-Hai (57564192500); Liu, Bin (57218292322); Li, Yike (58068821800); He, Zhi-Fen (56376283600); Lai, Yu-Kun (57280697700); Zhang, Congxuan (53065034600); Chen, Zhen (57219421269)","57221874131; 57564192500; 57218292322; 58068821800; 56376283600; 57280697700; 53065034600; 57219421269","Controllable facial attribute editing via Gaussian mixture model disentanglement","2023","Digital Signal Processing: A Review Journal","134","","103916","","","","10.1016/j.dsp.2023.103916","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146430356&doi=10.1016%2fj.dsp.2023.103916&partnerID=40&md5=1491f200a130dc2212e00348cff09d8b","Generative adversarial networks (GANs) have made much progress in the field of high-quality and realistic facial image synthesis in recent years. However, compared with their powerful generation ability, it is difficult for users to modify the desired attributes of the resulting image while keeping the others. How to disentangle the latent space of pre-trained GANs is essential and critical for controllable image synthesis. In this paper, a novel controllable facial attribute editing algorithm based on the Gaussian mixture model (GMM) representation is proposed. First, we assume that the latent variables with respect to each facial attribute lie in a subspace of the whole latent manifold composed of a fixed number of learned features, and each attribute subspace can be modeled by a GMM. Then, to avoid unintended changes during attribute editing, a coordinate accumulation strategy with orthogonal regularization is introduced to enhance the independence of distinct attribute subspaces which helps improving the controllability of attribute editing. In addition, a resampling strategy is utilized to improve the stability of the model. Through qualitative and quantitative experimental results, the proposed method achieves the state-of-the-art performance on facial attribute editing, and improves the controllability of desired attribute editing. © 2023 Elsevier Inc.","Gaussian distribution; Generative adversarial networks; Object recognition; Decouplings; Facial Image synthesis; Gaussian Mixture Model; High quality; Images synthesis; Latent space decoupling; Model representation; Orthogonal constraint on manifold space; Orthogonal constraints; Semantic editor; Semantics","Gaussian mixture model; Latent space decoupling; Orthogonal constraints on manifold spaces; Semantic editor","Article","Final","","Scopus","2-s2.0-85146430356"
"Platscher M.; Zopes J.; Federau C.","Platscher, Moritz (56964505700); Zopes, Jonathan (57193278307); Federau, Christian (23027557700)","56964505700; 57193278307; 23027557700","Image translation for medical image generation: Ischemic stroke lesion segmentation","2022","Biomedical Signal Processing and Control","72","","103283","","","","10.1016/j.bspc.2021.103283","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118474376&doi=10.1016%2fj.bspc.2021.103283&partnerID=40&md5=700b52ae411d67b9cf44fa9bbdc971a5","Deep learning based disease detection and segmentation algorithms promise to improve many clinical processes. However, such algorithms require vast amounts of annotated training data, which are typically not available in the medical context due to data privacy, legal obstructions, and non-uniform data acquisition protocols. Synthetic databases with annotated pathologies could provide the required amounts of training data. We demonstrate with the example of ischemic stroke that an improvement in lesion segmentation is feasible using deep learning based augmentation. To this end, we train different image-to-image translation models to synthesize magnetic resonance images of brain volumes with and without stroke lesions from semantic segmentation maps. In addition, we train a generative adversarial network to generate synthetic lesion masks. Subsequently, we combine these two components to build a large database of synthetic stroke images. The performance of the various models is evaluated using a U-Net which is trained to segment stroke lesions on a clinical test set. We report a Dice score of 72.8% [70.8±1.0%] for the model with the best performance, which outperforms the model trained on the clinical images alone 67.3% [63.2±1.9%], and is close to the human inter-reader Dice score of 76.9%. Moreover, we show that for a small database of only 10 or 50 clinical cases, synthetic data augmentation yields significant improvement compared to a setting where no synthetic data is used. To the best of our knowledge, this presents the first comparative analysis of synthetic data augmentation based on image-to-image translation, and first application to ischemic stroke. © 2021 The Authors","Data acquisition; Data privacy; Database systems; Deep learning; Generative adversarial networks; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Semantics; Generative model; Image translation; Image-to-image translation; Images synthesis; Ischemic strokes; Lesion segmentations; Performance; Stroke lesion segmentation; Stroke lesions; Synthetic data; adult; anatomical location; Article; brain radiography; brain size; computer model; contrast enhancement; controlled study; data accuracy; data analysis; data consistency; data synthesis; data warehouse; deep learning; female; gray matter; human; image analysis; image processing; image quality; image segmentation; intermethod comparison; ischemic stroke; lesion volume; major clinical study; male; measurement accuracy; noise measurement; nuclear magnetic resonance imaging; sample size; scoring system; tissue structure; white matter; Semantic Segmentation","generative models; image synthesis; image-to-image translation; stroke lesion segmentation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85118474376"
"Kalpana A.; John A.","Kalpana, A. (57153137900); John, Anju (58066682300)","57153137900; 58066682300","Pix2Pix GAN Image Synthesis To Detect Electric Vehicle License Plate","2022","Proceedings of 4th International Conference on Cybernetics, Cognition and Machine Learning Applications, ICCCMLA 2022","","","","439","443","4","10.1109/ICCCMLA56841.2022.9989063","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146339089&doi=10.1109%2fICCCMLA56841.2022.9989063&partnerID=40&md5=cc5a27d80b9f7a4fae6a4753e263e68b","The area of image processing is more intensive in development and research activities for decades. The role of image processing is huge in modeling, analytics, communication, computation, information security, information forensics and smart city application. Images are ubiquitous in day to day life and images or videos play dominant role in monitoring applications. But when it comes to development of specific application, collection of data is a very challenging task. Nowadays deep learning plays a significant role for generation of data. Robust technologies like Generative Adversarial Network (GAN) and Cycle GAN play a crucial role for generating realistic images with super resolution. GAN and its associated methods used for image synthesis improve the accuracy of deep learning models. In this paper, we analyze challenges of license plate recognition in realistic situation and experiments demonstrate that GAN can generate realistic images to improve the accuracy of license plate recognition. © 2022 IEEE.","Deep learning; Image enhancement; License plates (automobile); Optical character recognition; Security of data; Deep learning; Development activity; Generator; Images processing; Images synthesis; Information Forensics; Licenses plate recognition; Realistic images; Research activities; Vehicle license plates; Generative adversarial networks","Deep Learning; Generative Adversarial Network; Generator; Image synthesis; License Plate Recognition","Conference paper","Final","","Scopus","2-s2.0-85146339089"
"Liu H.; Xu Y.; Chen F.","Liu, Heng (57022065500); Xu, Yao (58042772000); Chen, Feng (57213591008)","57022065500; 58042772000; 57213591008","Sketch2Photo: Synthesizing photo-realistic images from sketches via global contexts","2023","Engineering Applications of Artificial Intelligence","117","","105608","","","","10.1016/j.engappai.2022.105608","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145590033&doi=10.1016%2fj.engappai.2022.105608&partnerID=40&md5=fb675542fbcddc223911ee2214b7508a","Sketch-to-image synthesis aims to generate realistic images that match the input sketches or edge maps exactly. Most known sketch-to-image synthesis methods use various generative adversarial networks (GANs) that are trained with numerous pairs of sketches and real images. Because of the convolution locality, the low-level layers of the generators in these GANs lack global perception ability, causing feature maps derived from them easily to overlook global cues. Since the global receptive field is crucial for acquiring the non-local structures and features of sketches, the absence of global contexts will impact the generation of high-quality images. Some recent models turn to self-attention to construct global dependencies. However, they are not viable for large feature maps for the quadratic computational complexity concerning the size of feature maps. To address these problems, in this work, we propose Sketch2Photo — a new image synthesis approach that can capture global contexts as well as local features to generate photo-realistic images from weak or partial sketches or edge maps. We employ fast Fourier convolution (FFC) residual blocks to create global receptive fields in the bottom layers of the network and incorporate Swin Transformer block (STB) units to obtain long-range global contexts for large-size feature maps efficiently. We also present an improved spatial attention pooling (ISAP) module to relax the strict alignment requirements between incomplete sketches and generated images. Quantitative and qualitative experiments on multiple public datasets demonstrate the superiority of the proposed approach over many other sketch-to-image synthesis methods. The project code is available at https://github.com/hengliusky/Skecth2Photo. © 2022 Elsevier Ltd","Generative adversarial networks; Image enhancement; Network layers; Signal encoding; Encoder-decoder; Fast fourier; Fast fourier convolution; Feature map; Global context; Images synthesis; Photorealistic images; Sketch maps; Sketch-based image synthesis; Swin transformer; Convolution","Encoder–decoder; Fast Fourier convolution; Global contexts; Sketch-based image synthesis; Swin transformer","Article","Final","","Scopus","2-s2.0-85145590033"
"Haque E.; Khan M.F.K.; Jubair M.I.; Anjum J.; Niloy A.Z.","Haque, Emdadul (57913639700); Khan, Md. Faraz Kabir (57970097300); Jubair, Mohammad Imrul (55193485600); Anjum, Jarin (57969629300); Niloy, Abrar Zahir (57970327500)","57913639700; 57970097300; 55193485600; 57969629300; 57970327500","Book Cover Synthesis from the Summary","2022","Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA","2022-December","","","","","","10.1109/AICCSA56895.2022.10017541","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146984765&doi=10.1109%2fAICCSA56895.2022.10017541&partnerID=40&md5=f4559d2566490132fd8ec8ea572acac0","The cover is the face of a book and is a point of attraction for the readers. Designing book covers is an essential task in the publishing industry. One of the main challenges in creating a book cover is representing the theme of the book's content in a single image. In this research, we explore ways to produce a book cover using artificial intelligence based on the fact that there exists a relationship between the summary of the book and its cover. Our key motivation is the application of text-to-image synthesis methods to generate images from given text or captions. We explore several existing text-to-image conversion techniques for this purpose and propose an approach to exploit these frameworks for producing book covers from provided summaries. We construct a dataset of English books that contains a large number of samples of summaries of existing books and their cover images. In this paper, we describe our approach to collecting, organizing, and pre-processing the dataset to use it for training models. We apply different text-to-image synthesis techniques to generate book covers from the summary and exhibit the results in this paper.  © 2022 IEEE.","Generative adversarial networks; Image processing; Book cover synthesis; Book summary; Cover synthesis; Image conversion; Images synthesis; Number of samples; Publishing industry; Single images; Synthesis method; Text-to-image synthesis; Large dataset","Book cover synthesis; Book summary; Generative Adversarial Networks; Text-to-image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85146984765"
"Dong P.; Wu L.; Meng L.; Meng X.","Dong, Pei (57797372800); Wu, Lei (12800514800); Meng, Lei (56224317800); Meng, Xiangxu (7401629599)","57797372800; 12800514800; 56224317800; 7401629599","Disentangled Representations and Hierarchical Refinement of Multi-Granularity Features for Text-to-Image Synthesis","2022","ICMR 2022 - Proceedings of the 2022 International Conference on Multimedia Retrieval","","","","268","276","8","10.1145/3512527.3531389","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134046847&doi=10.1145%2f3512527.3531389&partnerID=40&md5=518dd0d095d530847ac46e15cc22c1a9","In this paper, we focus on generating photo-realistic images from given text descriptions. Current methods first generate an initial image and then progressively refine it to a high-resolution one. These methods typically indiscriminately refine all granularity features output from the previous stage. However, the ability to express different granularity features in each stage is not consistent, and it is difficult to express precise semantics by further refining the features with poor quality generated in the previous stage. Current methods cannot refine different granularity features independently, resulting in that it is challenging to clearly express all factors of semantics in generated image, and some features even become worse. To address this issue, we propose a Hierarchical Disentangled Representations Generative Adversarial Networks (HDR-GAN) to generate photo-realistic images by explicitly disentangling and individually modeling the factors of semantics in the image. HDR-GAN introduces a novel component called multi-granularity feature disentangled encoder to represent image information comprehensively through explicitly disentangling multi-granularity features including pose, shape and texture. Moreover, we develop a novel Multi-granularity Feature Refinement (MFR) containing a Coarse-grained Feature Refinement (CFR) model and a Fine-grained Feature Refinement (FFR) model. CFR utilizes coarse-grained disentangled representations (e.g., pose and shape) to clarify category information, while FFR employs fine-grained disentangled representations (e.g., texture) to reflect instance-level details. Extensive experiments on two well-studied and publicly available datasets (i.e., CUB-200 and CLEVR-SV) demonstrate the rationality and superiority of our method. © 2022 ACM.","Image processing; Semantics; Textures; 'current; Coarse-grained; Feature refinement; Fine grained; Hierarchical disentangled representation; Images synthesis; Multi-granularity; Multi-granularity feature; Photorealistic images; Text-to-image synthesis; Generative adversarial networks","generative adversarial networks; hierarchical disentangled representations; multi-granularity features; text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85134046847"
"Fujioka T.; Satoh Y.; Imokawa T.; Mori M.; Yamaga E.; Takahashi K.; Kubota K.; Onishi H.; Tateishi U.","Fujioka, Tomoyuki (55653670000); Satoh, Yoko (37038441900); Imokawa, Tomoki (57330423300); Mori, Mio (57207256127); Yamaga, Emi (57194718687); Takahashi, Kanae (57433035100); Kubota, Kazunori (7402692166); Onishi, Hiroshi (57226165623); Tateishi, Ukihide (7003533919)","55653670000; 37038441900; 57330423300; 57207256127; 57194718687; 57433035100; 7402692166; 57226165623; 7003533919","Proposal to Improve the Image Quality of Short-Acquisition Time-Dedicated Breast Positron Emission Tomography Using the Pix2pix Generative Adversarial Network","2022","Diagnostics","12","12","3114","","","","10.3390/diagnostics12123114","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144841889&doi=10.3390%2fdiagnostics12123114&partnerID=40&md5=ede505095ef18726269a50aa40ef1b59","This study aimed to evaluate the ability of the pix2pix generative adversarial network (GAN) to improve the image quality of low-count dedicated breast positron emission tomography (dbPET). Pairs of full- and low-count dbPET images were collected from 49 breasts. An image synthesis model was constructed using pix2pix GAN for each acquisition time with training (3776 pairs from 16 breasts) and validation data (1652 pairs from 7 breasts). Test data included dbPET images synthesized by our model from 26 breasts with short acquisition times. Two breast radiologists visually compared the overall image quality of the original and synthesized images derived from the short-acquisition time data (scores of 1–5). Further quantitative evaluation was performed using a peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). In the visual evaluation, both readers revealed an average score of >3 for all images. The quantitative evaluation revealed significantly higher SSIM (p < 0.01) and PSNR (p < 0.01) for 26 s synthetic images and higher PSNR for 52 s images (p < 0.01) than for the original images. Our model improved the quality of low-count time dbPET synthetic images, with a more significant effect on images with lower counts. © 2022 by the authors.","fluorodeoxyglucose f 18; adult; aged; algorithm; Article; breast cancer; clinical article; clinical practice; comparative study; controlled study; digital imaging and communications in medicine; female; human; image quality; image reconstruction; interrater reliability; pix2pix generative adversarial network; positron emission tomography; quantitative analysis; retrospective study; signal noise ratio; validation study","artificial intelligence; breast cancer; dedicated breast positron emission tomography; generative adversarial network; image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144841889"
"Naveen S.; Ram Kiran M.S.S.; Indupriya M.; Manikanta T.V.; Sudeep P.V.","Naveen, S. (57248745500); Ram Kiran, M.S.S. (57248745600); Indupriya, M. (57248940100); Manikanta, T.V. (57249016100); Sudeep, P.V. (37862221600)","57248745500; 57248745600; 57248940100; 57249016100; 37862221600","Transformer models for enhancing AttnGAN based text to image generation","2021","Image and Vision Computing","115","","104284","","","","10.1016/j.imavis.2021.104284","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114384410&doi=10.1016%2fj.imavis.2021.104284&partnerID=40&md5=56f2c333387b8ff82f8f1bea3f371023","Deep neural networks are capable of producing photographic images that depict given natural language text descriptions. Such models have huge potential in applications such as interior designing, video games, editing and facial sketching for digital forensics. However, only a limited number of methods in the literature have been developed for text to image (TTI) generation. Most of them use Generative Adversarial Networks (GAN) based deep learning methods. Attentional GAN (AttnGAN) is a popular GAN based TTI method that extracts meaningful information from the given text descriptions using attention mechanism. In this paper, we investigate the use of different Transformer models such as BERT, GPT2, XLNet with AttnGAN to solve the challenge of extracting semantic information from the text descriptions. Hence, the proposed AttnGANTRANS architecture has three variants AttnGANBERT, AttnGANXL and AttnGANGPT. The proposed method is successful over the conventional AttnGAN and gives a boosted inception score by 27.23% and a decline of Frechet inception distance by 49.9%. The results in our experiments indicate that the proposed method has the potential to outperform the contemporary state-of-the-art methods and validate the use of Transformer models in improving the performance of TTI generation. The code is made publicly available at https://github.com/sairamkiran9/AttnGAN-trans. © 2021 Elsevier B.V.","Deep learning; Deep neural networks; Digital forensics; Image enhancement; Photography; Semantics; Adversarial networks; Attention mechanisms; Image generations; Natural language text; Photographic image; Semantic information; State-of-the-art methods; Transformer models; Learning systems","Attention mechanism; Generative Adversarial Networks (GANs); Natural Language Processing (NLP); Text to image synthesis; Transformers","Article","Final","","Scopus","2-s2.0-85114384410"
"Li F.; Huang W.; Luo M.; Zhang P.; Zha Y.","Li, Feihong (57250431100); Huang, Wei (56195325600); Luo, Mingyuan (57206482771); Zhang, Peng (55547108553); Zha, Yufei (16044173100)","57250431100; 56195325600; 57206482771; 55547108553; 16044173100","A new VAE-GAN model to synthesize arterial spin labeling images from structural MRI","2021","Displays","70","","102079","","","","10.1016/j.displa.2021.102079","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114467366&doi=10.1016%2fj.displa.2021.102079&partnerID=40&md5=a32cedc2d932b706e76b3bfa7f0e7b17","Arterial spin labeling (ASL) is a relatively new MRI technique that can measure cerebral blood flow, which is of great importance for the diagnosis of dementia diseases. Besides, this valuable imaging modality does not need exogenous tracers and has no radiation, which makes it favorable for elder patients. However, ASL data does lack in many contemporary image-based dementia diseases datasets, which include popular ADNI-1/GO/2/3 datasets. In order to supplement the valuable ASL data, a new Generative adversarial network (GAN)-based model is proposed to synthesize ASL images in this study. This new model is unique, as the popular variational auto-encoder (VAE) has been utilized as the generator of the GAN-based model. Hence, a new VAE-GAN architecture is introduced in this study. In order to demonstrate its superiority, dozens of experiments have been conducted. Experimental results demonstrate that, this new VAE-GAN model is superior to other state-of-the-art ASL image synthesis methods, and the accuracy improvement after incorporating synthesized ASL images from the new model can be as high as 42.41% in dementia diagnosis tasks. © 2021 Elsevier B.V.","Diagnosis; Magnetic resonance imaging; Neurodegenerative diseases; Accuracy Improvement; Adversarial networks; Arterial spin labeling; Auto encoders; Cerebral blood flow; Image synthesis; Imaging modality; State of the art; Image enhancement","Generative adversarial network; Image synthesis; Variational auto-encoder","Article","Final","","Scopus","2-s2.0-85114467366"
"Chang Z.; Zhang X.; Wang S.; Ma S.; Gao W.","Chang, Zheng (57440878400); Zhang, Xinfeng (57211151919); Wang, Shanshe (36645440900); Ma, Siwei (34872761500); Gao, Wen (57218664611)","57440878400; 57211151919; 36645440900; 34872761500; 57218664611","STRPM: A Spatiotemporal Residual Predictive Model for High-Resolution Video Prediction","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","13926","13935","9","10.1109/CVPR52688.2022.01356","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140994175&doi=10.1109%2fCVPR52688.2022.01356&partnerID=40&md5=73e4f79eac36dbee7c98cdb66a307d12","Although many video prediction methods have obtained good performance in low-resolution (64∼128) videos, predictive models for high-resolution (512∼4K) videos have not been fully explored yet, which are more meaningful due to the increasing demand for high-quality videos. Compared with low-resolution videos, high-resolution videos contain richer appearance (spatial) information and more complex motion (temporal) information. In this paper, we propose a Spatiotemporal Residual Predictive Model (STRPM) for high-resolution video prediction. On the one hand, we propose a Spatiotemporal Encoding-Decoding Scheme to preserve more spatiotemporal information for high-resolution videos. In this way, the appearance details for each frame can be greatly preserved. On the other hand, we design a Residual Predictive Memory (RPM) which focuses on modeling the spatiotemporal residual features (STRF) between previous and future frames instead of the whole frame, which can greatly help capture the complex motion information in high-resolution videos. In addition, the proposed RPM can supervise the spatial encoder and temporal encoder to extract different features in the spatial domain and the temporal domain, respectively. Moreover, the proposed model is trained using generative adversarial networks (GANs) with a learned perceptual loss (LP-loss) to improve the perceptual quality of the predictions. Experimental results show that STRPM can generate more satisfactory results compared with various existing methods. © 2022 IEEE.","Complex networks; Generative adversarial networks; Signal encoding; Behavior analysis; High resolution; Image and video synthesis and generation; Images synthesis; Predictive models; Representation learning; Video analysis; Video generation; Video synthesis; Video understanding; Forecasting","Behavior analysis; Image and video synthesis and generation; Representation learning; Video analysis and understanding","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140994175"
"Kim H.G.; Nanni D.; Süsstrunk S.","Kim, Hak Gu (57724664100); Nanni, Davide (57725319700); Süsstrunk, Sabine (6603829965)","57724664100; 57725319700; 6603829965","NATURAL-LOOKING ADVERSARIAL EXAMPLES FROM FREEHAND SKETCHES","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","3723","3727","4","10.1109/ICASSP43922.2022.9747480","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131246020&doi=10.1109%2fICASSP43922.2022.9747480&partnerID=40&md5=8c82960c61be28ae3e20620618325b0a","Deep neural networks (DNNs) have achieved great success in image classification and recognition compared to previous methods. However, recent works have reported that DNNs are very vulnerable to adversarial examples that are intentionally generated to mislead the predictions of the DNNs. Here, we present a novel freehand sketch-based natural-looking adversarial example generator that we call SketchAdv. To generate a natural-looking adversarial example from a sketch, we force the encoded edge information (i.e., the visual attributes) to be close to the latent random vector fed to the edge generator and adversarial example generator. This preserves the spatial consistency of the adversarial example generated from the random vector with the edge information. In addition, by employing a sketch-edge encoder with a novel sketch-edge matching loss, we reduce the gap between edges and sketches. We evaluate the proposed method on several dominant classes of SketchyCOCO, the benchmark dataset for sketch to image translation. Our experiments show that our SketchAdv produces visually plausible adversarial examples while remaining competitive with other adversarial attack methods. © 2022 IEEE","Computer vision; Generative adversarial networks; Image classification; Adversarial example; Classification and recognition; Edge information; Freehand sketch; Image translation; Images classification; Images synthesis; Random vectors; Spatial consistency; Visual attributes; Deep neural networks","adversarial examples; generative adversarial network; image classification; image synthesis; image translation","Conference paper","Final","","Scopus","2-s2.0-85131246020"
"Du C.; Huang Y.; Zeng H.","Du, Changan (57762399000); Huang, Yukun (57763346700); Zeng, Huajie (57762399100)","57762399000; 57763346700; 57762399100","Adversarial Net and Its Variants","2022","Proceedings of SPIE - The International Society for Optical Engineering","12259","","122595H","","","","10.1117/12.2639238","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132763673&doi=10.1117%2f12.2639238&partnerID=40&md5=69919cf12e809910519565bf1f566dbd","With the rapid development of computer science, generative adversarial networks (GAN) have become increasingly important in machine learning. GAN is a machine learning model consisting of a generative network and discriminative network and it is a two-player game proposed by Goodfellow. GAN plays a significant role in real-life application, especially in the image to image translation, image synthesis, data augmentation, and image editing. However, GANs have several issues that have not been well addressed: non-convergence problem, collapse problem, and so on. This article provides a comprehensive overview and analysis of GAN. Firstly, the theory, architecture, and training procedure of the original GAN are introduced in detail. Moreover, we present several variants of GAN, including DCGAN, BEGAN, LSGAN, CIAGAN, and WGAN. We discuss the motivation and networks structures of these variants and list and summarize their advantages and difference. Finally, we provide readers with several evaluation metrics for GAN and point out future open research challenges. © 2022 SPIE","Game theory; Adversarial; Discriminative networks; Image translation; Images synthesis; Machine learning models; Machine-learning; Net; Real-life applications; Two-player games; Variant; Generative adversarial networks","Adversarial; Net; Variants","Conference paper","Final","","Scopus","2-s2.0-85132763673"
"Dalmaz O.; Saglam B.; Gonc K.; Cukur T.","Dalmaz, Onat (57226257796); Saglam, Baturay (57283902400); Gonc, Kaan (57716772000); Cukur, Tolga (23034054800)","57226257796; 57283902400; 57716772000; 23034054800","edaGAN: Encoder-Decoder Attention Generative Adversarial Networks for Multi-contrast MR Image Synthesis","2022","2022 9th International Conference on Electrical and Electronics Engineering, ICEEE 2022","","","","320","324","4","10.1109/ICEEE55327.2022.9772555","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130897685&doi=10.1109%2fICEEE55327.2022.9772555&partnerID=40&md5=3eafff80ae5b2468989c20cc9bff7d44","Magnetic resonance imaging (MRI) is the preferred modality among radiologists in the clinic due to its superior depiction of tissue contrast. Its ability to capture different contrasts within an exam session allows it to collect additional diagnostic information. However, such multi-contrast MRI exams take a long time to scan, resulting in acquiring just a portion of the required contrasts. Consequently, synthetic multi-contrast MRI can improve subsequent radiological observations and image analysis tasks like segmentation and detection. Because of this significant potential, multi-contrast MRI synthesis approaches are gaining popularity. Recently, generative adversarial networks (GAN) have become the de facto choice for synthesis tasks in medical imaging due to their sensitivity to realism and high-frequency structures. In this study, we present a novel generative adversarial approach for multi-contrast MRI synthesis that combines the learning of deep residual convolutional networks and spatial modulation introduced by an attention gating mechanism to synthesize high-quality MR images. We show the superiority of the proposed approach against various synthesis models on multi-contrast MRI datasets.  © 2022 IEEE.","Deep learning; Diagnosis; Generative adversarial networks; Image enhancement; Image segmentation; Medical imaging; Network coding; Adversarial; Attention; Diagnostics informations; Encoder-decoder; Frequency structure; Generative; High frequency HF; Image-analysis; Images synthesis; MR-images; Magnetic resonance imaging","Adversarial; Attention; Generative; MRI; Synthesis","Conference paper","Final","","Scopus","2-s2.0-85130897685"
"Li X.; Teng G.; An P.; Yao H.-Y.","Li, Xiang (57192014282); Teng, Guowei (8681431300); An, Ping (35242112900); Yao, Hai-Yan (57195219347)","57192014282; 8681431300; 35242112900; 57195219347","Image synthesis via adversarial geometric consistency pursuit","2021","Signal Processing: Image Communication","99","","116489","","","","10.1016/j.image.2021.116489","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115618185&doi=10.1016%2fj.image.2021.116489&partnerID=40&md5=bcd8f55cb35255eb7f17405c70ac18c0","Realistic image synthesis is a class of vision problems where the goal is to synthesize new images by fusing a source image and a target image. It is a challenging problem due to the visual gap between the two images, including geometry and appearance. To synthetically address the geometric distortion and appearance realism, we propose a novel method, the adversarial geometric consistency pursuit model (AGCP), which explicitly allows seamless image synthesis. The proposed method takes geometric correction and appearance harmonization into account, handling the relative scaling, spatial layout, color, viewpoint, and distortion transformation to generate a realistic composite image. Moreover, we also propose a joint geometric consistency pursuit loss that handles the geometric consistency and enhances the network to generalize better for different scales of source images. Our comparative evaluation demonstrates the effectiveness of the proposed method in the aforementioned challenging cases. © 2021","Geometry; Image enhancement; Appearance harmonization; Geometric consistency; Geometric distortion; Harmonisation; Images synthesis; Realistic image synthesis; Source images; Spatial transformer; Target images; Vision problems; Generative adversarial networks","Appearance harmonization; Generative adversarial network; Geometric consistency; Image synthesis; Spatial transformer","Article","Final","","Scopus","2-s2.0-85115618185"
"Fawakherji M.; Potena C.; Pretto A.; Bloisi D.D.; Nardi D.","Fawakherji, Mulham (57208207588); Potena, Ciro (56993817600); Pretto, Alberto (23393749700); Bloisi, Domenico D. (57189023230); Nardi, Daniele (7006582655)","57208207588; 56993817600; 23393749700; 57189023230; 7006582655","Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming","2021","Robotics and Autonomous Systems","146","","103861","","","","10.1016/j.robot.2021.103861","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114121190&doi=10.1016%2fj.robot.2021.103861&partnerID=40&md5=126f68cc8b0efc0703fcf9eb669ff3a7","An effective perception system is a fundamental component for farming robots, as it enables them to properly perceive the surrounding environment and to carry out targeted operations. The most recent methods make use of state-of-the-art machine learning techniques to learn a valid model for the target task. However, those techniques need a large amount of labeled data for training. A recent approach to deal with this issue is data augmentation through Generative Adversarial Networks (GANs), where entire synthetic scenes are added to the training data, thus enlarging and diversifying their informative content. In this work, we propose an alternative solution with respect to the common data augmentation methods, applying it to the fundamental problem of crop/weed segmentation in precision farming. Starting from real images, we create semi-artificial samples by replacing the most relevant object classes (i.e., crop and weeds) with their synthesized counterparts. To do that, we employ a conditional GAN (cGAN), where the generative model is trained by conditioning the shape of the generated object. Moreover, in addition to RGB data, we take into account also near-infrared (NIR) information, generating four channel multi-spectral synthetic images. Quantitative experiments, carried out on three publicly available datasets, show that (i) our model is capable of generating realistic multi-spectral images of plants and (ii) the usage of such synthetic images in the training process improves the segmentation performance of state-of-the-art semantic segmentation convolutional networks. © 2021 Elsevier B.V.","Agricultural robots; Convolutional neural networks; Crops; Image enhancement; Infrared devices; Learning systems; Semantics; Spectroscopy; Alternative solutions; Convolutional networks; Fundamental component; Machine learning techniques; Quantitative experiments; Segmentation performance; Semantic segmentation; Surrounding environment; Image segmentation","Agricultural robotics; cGANs; Crop/weed detection; Semantic segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85114121190"
"Wang P.; Nie P.; Dang Y.; Wang L.; Zhu K.; Wang H.; Wang J.; Liu R.; Ren J.; Feng J.; Fan H.; Yu J.; Chen B.","Wang, Pingping (57383094500); Nie, Pin (57211385138); Dang, Yanli (57383746600); Wang, Lifang (57384172500); Zhu, Kaiguo (57382647500); Wang, Hongyu (57202390294); Wang, Jiawei (57881942400); Liu, Rumei (57384172600); Ren, Jialiang (57210265966); Feng, Jun (57189841077); Fan, Haiming (57270707500); Yu, Jun (7405530847); Chen, Baoying (8836680100)","57383094500; 57211385138; 57383746600; 57384172500; 57382647500; 57202390294; 57881942400; 57384172600; 57210265966; 57189841077; 57270707500; 7405530847; 8836680100","Synthesizing the First Phase of Dynamic Sequences of Breast MRI for Enhanced Lesion Identification","2021","Frontiers in Oncology","11","","792516","","","","10.3389/fonc.2021.792516","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121587393&doi=10.3389%2ffonc.2021.792516&partnerID=40&md5=5b95698e97dea64dc11312bd8e8d8b99","Objective: To develop a deep learning model for synthesizing the first phases of dynamic (FP-Dyn) sequences to supplement the lack of information in unenhanced breast MRI examinations. Methods: In total, 97 patients with breast MRI images were collected as the training set (n = 45), the validation set (n = 31), and the test set (n = 21), respectively. An enhance border lifelike synthesize (EDLS) model was developed in the training set and used to synthesize the FP-Dyn images from the T1WI images in the validation set. The peak signal-to-noise ratio (PSNR), structural similarity (SSIM), mean square error (MSE) and mean absolute error (MAE) of the synthesized images were measured. Moreover, three radiologists subjectively assessed image quality, respectively. The diagnostic value of the synthesized FP-Dyn sequences was further evaluated in the test set. Results: The image synthesis performance in the EDLS model was superior to that in conventional models from the results of PSNR, SSIM, MSE, and MAE. Subjective results displayed a remarkable visual consistency between the synthesized and original FP-Dyn images. Moreover, by using a combination of synthesized FP-Dyn sequence and an unenhanced protocol, the sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of MRI were 100%, 72.73%, 76.92%, and 100%, respectively, which had a similar diagnostic value to full MRI protocols. Conclusions: The EDLS model could synthesize the realistic FP-Dyn sequence to supplement the lack of enhanced images. Compared with full MRI examinations, it thus provides a new approach for reducing examination time and cost, and avoids the use of contrast agents without influencing diagnostic accuracy. Copyright © 2021 Wang, Nie, Dang, Wang, Zhu, Wang, Wang, Liu, Ren, Feng, Fan, Yu and Chen.","Article; breast; controlled study; data processing; diagnostic accuracy; diagnostic test accuracy study; diagnostic value; digital imaging and communications in medicine; human; image analysis; image quality; learning algorithm; Likert scale; machine learning; major clinical study; mathematical model; mean absolute error; mean square error; nuclear magnetic resonance imaging; predictive value; quantitative analysis; radiologist; reliability; retrospective study; signal noise ratio; structural similarity; training","breast cancer; deep learning; generative adversarial network (GAN); images synthesis; magnetic resonance imaging (MRI)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85121587393"
"Dalmaz O.; Saglam B.; Gonc K.; Dar S.U.; Cukur T.","Dalmaz, Onat (57226257796); Saglam, Baturay (57283902400); Gonc, Kaan (57716772000); Dar, Salman Uh. (57195220338); Cukur, Tolga (23034054800)","57226257796; 57283902400; 57716772000; 57195220338; 23034054800","Bottleneck Sharing Generative Adversarial Networks for Unified Multi-Contrast MR Image Synthesis; [Darboǧaz Paylaşan Üretken Çekişmeli Aǧlar ile Birleşik Çoklu Kontrast MR Görüntü Sentezi]","2022","2022 30th Signal Processing and Communications Applications Conference, SIU 2022","","","","","","","10.1109/SIU55565.2022.9864880","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138702166&doi=10.1109%2fSIU55565.2022.9864880&partnerID=40&md5=1c005f9edd180bea09717bb2abe9a4d6","Magnetic Resonance Imaging (MRI) is the favored modality in multi-modal medical imaging due to its safety and ability to acquire various different contrasts of the anatomy. Availability of multiple contrasts accumulates diagnostic information and, therefore, can improve radiological observations. In some scenarios, acquiring all contrasts might be challenging due to reluctant patients and increased costs associated with additional scans. That said, synthetically obtaining missing MRI pulse sequences from the acquired sequences might prove to be useful for further analyses. Recently introduced Generative Adversarial Network (GAN) models offer state-of-the-art performance in learning MRI synthesis. However, the proposed generative approaches learn a distinct model for each conditional contrast to contrast mapping. Learning a distinct synthesis model for each individual task increases the time and memory demands due to the increased number of parameters and training time. To mitigate this issue, we propose a novel unified synthesis model, bottleneck sharing GAN (bsGAN), to consolidate learning of synthesis tasks in multi-contrast MRI. bsGAN comprises distinct convolutional encoders and decoders for each contrast to increase synthesis performance. A central information bottleneck is employed to distill hidden representations. The bottleneck, based on residual convolutional layers, is shared across contrasts to avoid introducing many learnable parameters. Qualitative and quantitative comparisons on a multi-contrast brain MRI dataset show the effectiveness of the proposed method against existing unified synthesis methods. © 2022 IEEE.","Computer vision; Convolutional neural networks; Diagnosis; Learning systems; Magnetic resonance imaging; Medical imaging; Bottleneck; Bottleneck sharing; Diagnostics informations; Images synthesis; Magnetic resonance imaging synthesis; MR-images; Multi-modal; Parameter sharing; Synthesis models; Unified; Generative adversarial networks","bottleneck; generative adversarial networks; MRI synthesis; parameter-sharing; unified","Conference paper","Final","","Scopus","2-s2.0-85138702166"
"Brkic K.; Hrkac T.; Kalafatic Z.","Brkic, Karla (57204382389); Hrkac, Tomislav (57189322298); Kalafatic, Zoran (55967232400)","57204382389; 57189322298; 55967232400","A Privacy Preservation Pipeline for Personally Identifiable Data in Images Using Convolutional and Transformer Architectures","2022","2022 45th Jubilee International Convention on Information, Communication and Electronic Technology, MIPRO 2022 - Proceedings","","","","924","929","5","10.23919/MIPRO55190.2022.9803731","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133940653&doi=10.23919%2fMIPRO55190.2022.9803731&partnerID=40&md5=9e0abc518df13d727665e0b4f74d0ca7","Image and video data of people, shared voluntarily and involuntarily, is ubiquitous. There is an increased need for techniques that enable privacy protection via removal of personally identifiable information in such data, spurred by regulatory interest and increased social awareness of privacy implications. In this paper, we introduce a privacy preservation pipeline that enables de-identifying personal data in images and videos via replacement image synthesis while retaining data utility. We utilize the recently proposed convolutional VQGANs with autoregressive transformers to synthesize realistic and fully de-identified images of people that are then blended with the original scene. Experimental results show that the method provides strong de-identification while retaining the realism of the scene. © 2022 Croatian Society MIPRO.","Convolution; Deep learning; Image processing; Microelectronics; Pipelines; Sensitive data; De-identification; Deep learning; Image data; Machine-learning; Personally identifiable information; Privacy preservation; Privacy protection; Sensitive datas; Transformer; Video data; Generative adversarial networks","de-identification; deep learning; generative adversarial networks; machine learning; privacy protection; sensitive data; transformers","Conference paper","Final","","Scopus","2-s2.0-85133940653"
"Gan Y.; Xiang T.; Liu H.; Ye M.; Zhou M.","Gan, Yan (57203150814); Xiang, Tao (57213003210); Liu, Hangcheng (57219900424); Ye, Mao (35241431500); Zhou, Mingliang (56621809400)","57203150814; 57213003210; 57219900424; 35241431500; 56621809400","Generative adversarial networks with adaptive learning strategy for noise-to-image synthesis","2022","Neural Computing and Applications","","","","","","","10.1007/s00521-022-08002-w","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142191040&doi=10.1007%2fs00521-022-08002-w&partnerID=40&md5=2ccffeffde57d478a8167f9f0916c0ca","Generative adversarial networks (GANs) directly learn from an unknown real distribution through adversarial training. However, training the generator only by the feedback of the discriminator cannot make GANs learn adaptively from the unknown complex real distribution, and for this reason the quality of generated images is unsatisfactory sometimes. To address this problem, we propose a framework for training GANs with an adaptive learning strategy from simpleness to complexity. First, we employ a pre-trained encoder and a generator to construct a simple task that looks like a real image. Second, an adaptive learning strategy is designed based on the mathematical expectation of the discriminating results of the real image and the simple task. The designed adaptive learning strategy is well compatible with various GANs architectures. Experimental results demonstrate the proposed method can improve the performance of existing GANs. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Complex networks; Learning systems; Adaptive learning; Auto encoders; Images synthesis; Learn+; Learning strategy; Mathematical expectation; Prior-knowledge; Real distribution; Real images; Simple++; Generative adversarial networks","AutoEncoder; GANs; Learning strategy; Prior knowledge","Article","Article in press","","Scopus","2-s2.0-85142191040"
"Mahapatra A.; Kulkarni K.","Mahapatra, Aniruddha (57376144000); Kulkarni, Kuldeep (55617412500)","57376144000; 55617412500","Controllable Animation of Fluid Elements in Still Images","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","3657","3666","9","10.1109/CVPR52688.2022.00365","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141751637&doi=10.1109%2fCVPR52688.2022.00365&partnerID=40&md5=1ec6020a769e8d7bdd010716368fa351","We propose a method to interactively control the animation of fluid elements in still images to generate cinemagraphs. Specifically, we focus on the animation of fluid elements like water, smoke, fire, which have the properties of repeating textures and continuous fluid motion. Taking inspiration from prior works, we represent the motion of such fluid elements in the image in the form of a constant 2D optical flow map. To this end, we allow the user to provide any number of arrow directions and their associated speeds along with a mask of the regions the user wants to animate. The user-provided input arrow directions, their corresponding speed values, and the mask are then converted into a dense flow map representing a constant optical flow map (FD). We observe that FD, obtained using simple exponential operations can closely approximate the plausible motion of elements in the image. We further refine computed dense optical flow map FD using a generative-adversarial network (GAN) to obtain a more realistic flow map. We devise a novel UNet based architecture to autoregressively generate future frames using the refined optical flow map by forward-warping the input image features at different resolutions. We conduct extensive experiments on a publicly available dataset and show that our method is superior to the baselines in terms of qualitative and quantitative metrics. In addition, we show the qualitative animations of the objects in directions that did not exist in the training set and provide a way to synthesize videos that otherwise would not exist in the real world. Project url: https://controllable-cinemagraphs.github.io/ © 2022 IEEE.","Animation; Computer vision; Finite difference method; Generative adversarial networks; Motion analysis; Smoke; Textures; Flow maps; Fluid element; Image and video synthesis and generation; Images synthesis; Motion and tracking; Still-images; Video analysis; Video generation; Video synthesis; Video understanding; Optical flows","Image and video synthesis and generation; Motion and tracking; Video analysis and understanding","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141751637"
"Nehra R.; Pal A.; Baranidharan B.","Nehra, Rahul (57438962600); Pal, Abhisikta (57211340679); Baranidharan, B. (53866078700)","57438962600; 57211340679; 53866078700","Radiological Image Synthesis Using Cycle-Consistent Generative Adversarial Network","2022","Lecture Notes in Networks and Systems","341","","","391","402","11","10.1007/978-981-16-7118-0_34","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124022132&doi=10.1007%2f978-981-16-7118-0_34&partnerID=40&md5=3af013464d618f198aafd58f9e9cf8c9","Radiology is the branch of science that deals with the study of energetic radiations and their use in generating medical images. MRI (Magnetic Resonance imaging) and CT (Computed tomography) are the two widely used modalities in radiology. CT comes with the disadvantage of high radiation risk which may have side effects. Thus, medical image from MRI-only radiation which is much safer than CT can be used to synthesize CT images using Deep Learning techniques. In this paper, we propose to build an architecture of fully convolutional neural network (FCN) along with a cyclic Generative Adversarial network (GAN). Our model has successfully generated CT images from the given MRI images from an unpaired ADNI image dataset. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","ADNI dataset; cGAN; CT generation; Deep learning; FCN; MRI; Radiology","Conference paper","Final","","Scopus","2-s2.0-85124022132"
"Bao S.; Tang Y.; Lee H.H.; Gao R.; Yang Q.; Yu X.; Chiron S.; Coburn L.A.; Wilson K.T.; Roland J.T.; Landman B.A.; Huo Y.","Bao, Shunxing (57189987093); Tang, Yucheng (57212614566); Lee, Ho Hin (57219409783); Gao, Riqiang (57209643243); Yang, Qi (57741612800); Yu, Xin (57309160700); Chiron, Sophie (57222559307); Coburn, Lori A. (6603410655); Wilson, Keith T. (57202597534); Roland, Joseph T. (8094809800); Landman, Bennett A. (16679175200); Huo, Yuankai (56830058500)","57189987093; 57212614566; 57219409783; 57209643243; 57741612800; 57309160700; 57222559307; 6603410655; 57202597534; 8094809800; 16679175200; 56830058500","Inpainting missing tissue in multiplexed immunofluorescence imaging","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12039","","120390K","","","","10.1117/12.2611827","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132818114&doi=10.1117%2f12.2611827&partnerID=40&md5=2454d720c0671829ebda6c46f80e5363","Multiplex immunofluorescence (MxIF) is an emerging technique that allows for staining multiple cellular and histological markers to stain simultaneously on a single tissue section. However, with multiple rounds of staining and bleaching, it is inevitable that the scarce tissue may be physically depleted. Thus, a digital way of synthesizing such missing tissue would be appealing since it would increase the useable areas for the downstream single-cell analysis. In this work, we investigate the feasibility of employing generative adversarial network (GAN) approaches to synthesize missing tissues using 11 MxIF structural molecular markers (i.e., epithelial and stromal). Briefly, we integrate a multi-channel high-resolution image synthesis approach to synthesize the missing tissue from the remaining markers. The performance of different methods is quantitatively evaluated via the downstream cell membrane segmentation task. Our contribution is that we, for the first time, assess the feasibility of synthesizing missing tissues in MxIF via quantitative segmentation. The proposed synthesis method has comparable reproducibility with the baseline method on performance for the missing tissue region reconstruction only, but it improves 40% on whole tissue synthesis that is crucial for practical application. We conclude that GANs are a promising direction of advancing MxIF imaging with deep image synthesis.  © COPYRIGHT SPIE.","Cytology; Fluorescence; Generative adversarial networks; Histology; Medical imaging; Cellulars; Down-stream; Images synthesis; Inpainting; Multi channel; Multiplex immunofluorescence; Performance; Reproducibilities; Single cells analysis; Tissue sections; Tissue","inpainting; multi-channel; MxIF; reproducibility","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85132818114"
"Song H.; Zhang X.; Liu F.; Yang Y.","Song, Huajun (10640262500); Zhang, Xiuhui (57337994700); Liu, Fugui (57324816000); Yang, Yongfei (7409389696)","10640262500; 57337994700; 57324816000; 7409389696","Conditional Generative Adversarial Networks for 2D core grayscale image reconstruction from pore parameters","2022","Journal of Petroleum Science and Engineering","208","","109742","","","","10.1016/j.petrol.2021.109742","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119040547&doi=10.1016%2fj.petrol.2021.109742&partnerID=40&md5=dbb8ddc2b1cfee885d8678cb9b5dca29","Digital cores are of great significance for reservoir structure simulation, oil and gas exploration and development. Most existing digital core reconstruction methods only generate binary cores with complicated implementation processes, among other problems. To address these problems, this study proposed a combination of core pore parameters and conditional generative adversarial network (CGAN) to realize the 2D reconstruction of core grayscale images from only pore parameters (namely, text-to-image synthesis). The current text-to-image synthesis approaches still have many difficulties in generating fine images, but the technologies of image-to-image generation have improved drastically in recent years. Therefore, the proposed method involves two stages to avoid the difficulty of directly generating core grayscale images from pore parameters. In stage I, we preprocessed core sample images to obtain binary-grayscale image pairs, and then used the CGAN to learn the mapping from core binary images to real sample images. At the same time, the pores in the binary images were segmented and extracted to construct the pore component library. In stage II, on the basis of the given pore parameters, the corresponding pores were randomly extracted from the pore component library to generate binary images, and then the generated binary images were used as input for the trained CGAN model to produce core grayscale images. The experimental results showed that the core grayscale images reconstructed by the proposed method meet the pore conditions and reflect the basic characteristics of real cores. © 2021 Elsevier B.V.","Binary images; Image enhancement; Image reconstruction; Petroleum prospecting; Petroleum reservoir engineering; Pore size; Component libraries; Conditional generative adversarial network; Digital core; Gray-scale image reconstruction; Gray-scale images; Images reconstruction; Images synthesis; Oil and gas exploration; Pore-size distribution; Structure simulations; computer simulation; hydrocarbon exploration; image analysis; size distribution; Generative adversarial networks","Conditional generative adversarial networks; Digital core; Image reconstruction; Pore size distribution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119040547"
"Jin R.; Li X.","Jin, Ruinan (57810843900); Li, Xiaoxiao (57202387155)","57810843900; 57202387155","Backdoor Attack is a Devil in Federated GAN-Based Medical Image Synthesis","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13570 LNCS","","","154","165","11","10.1007/978-3-031-16980-9_15","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140479241&doi=10.1007%2f978-3-031-16980-9_15&partnerID=40&md5=6e7eedda9b7cd11b917a55715fc8957f","Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research. Training generative adversarial neural networks (GAN) usually requires large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data from different medical institutions while keeping raw data locally. However, FL is vulnerable to backdoor attack, an adversarial by poisoning training data, given the central server cannot access the original data directly. Most backdoor attack strategies focus on classification models and centralized domains. In this study, we propose a way of attacking federated GAN (FedGAN) by treating the discriminator with a commonly used data poisoning strategy in backdoor attack classification models. We demonstrate that adding a small trigger with size less than 0.5% of the original image size can corrupt the FedGAN model. Based on the proposed attack, we provide two effective defense strategies: global malicious detection and local training regularization. We show that combining the two defense strategies yields a robust medical image generation. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep learning; Medical imaging; Network security; Backdoor attack; Backdoors; Classification models; Defense strategy; Federated learning; Generative adversarial neural network; Images synthesis; Network-based; Neural-networks; Training data; Generative adversarial networks","Backdoor attack; Federated learning; GAN","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140479241"
"Li J.; Li T.; Lin R.; Nie Q.","Li, Jiaqi (57980531900); Li, Tujie (57980345600); Lin, Ruoxuan (57979969200); Nie, Qizhou (57979604200)","57980531900; 57980345600; 57979969200; 57979604200","GAN-based models and applications","2022","2022 IEEE 5th International Conference on Information Systems and Computer Aided Education, ICISCAE 2022","","","","848","852","4","10.1109/ICISCAE55891.2022.9927647","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142478213&doi=10.1109%2fICISCAE55891.2022.9927647&partnerID=40&md5=a35be6b32628a87dc86925be84a1a911","GANs have achieved great success in image generation. Gan consists of two main parts, the generator and the discriminator, the generator tries to generate real samples that fool the discriminator, and the discriminator tries to distinguish between real samples and generated samples. Here The continuous improvement of simulation ability and generation ability under this kind of confrontation game can realize the generation of fake images. Since GAN appeared in 2014, articles of various types of GAN have been published in major journals and conferences, and the application of GAN in image generation (specifying image synthesis, text-to-image, image-to-image, video) and GAN in NLP and other areas of application are the most studied, and research in this area has demonstrated the great potential of using GANs in image synthesis. In this paper, the classical basic GAN model is introduced at first. Then, the paper analysis the differences and characteristics of the recent GAN-based models, and introduces the applications of GAN-based models in different tasks. © 2022 IEEE.","Fake detection; Generative adversarial networks; Image enhancement; Continuous improvements; Deep learning; GAN model; Image generations; Images synthesis; Paper analysis; Real samples; Deep learning","deep learning; GAN models; image synthesis","Conference paper","Final","","Scopus","2-s2.0-85142478213"
"Bayoumi R.; Alfonse M.; Salem A.-B.M.","Bayoumi, Razan (57263716900); Alfonse, Marco (57039027100); Salem, Abdel-Badeeh M. (36762342200)","57263716900; 57039027100; 36762342200","Text-to-Image Synthesis: A Comparative Study","2022","Lecture Notes in Networks and Systems","224","","","229","251","22","10.1007/978-981-16-2275-5_14","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115175239&doi=10.1007%2f978-981-16-2275-5_14&partnerID=40&md5=0900efb53efe7477e2823597228bee00","Text-to-image synthesis or conditional image generation is referring to converting natural language text descriptions into realistic images, where both are semantically consistence. Deep convolutional generative adversarial networks (GANs) are rapidly changing field that have shown great revolution in the generative models. GANs are the recent widely used networks in many different frameworks to conditionally and unconditionally generate realistic images. In this paper, we aim to explore and review the majority of the work in this domain and provide a comparative study to get a full overview image about this domain. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Computer vision; Conditional generation; Deep learning; GAN; Generative models; Text-to-image","Conference paper","Final","","Scopus","2-s2.0-85115175239"
"Ko A.; Cho J.","Ko, Ara (57918974800); Cho, Jungwon (57755566900)","57918974800; 57755566900","Ultra-wide-field Fundus Image Synthesis Using Various GAN Models","2022","International Journal on Informatics Visualization","6","3","","618","622","4","10.30630/joiv.6.3.1256","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139427468&doi=10.30630%2fjoiv.6.3.1256&partnerID=40&md5=ac5e3fff4b5a047cba6652697aa50780","Many people lose sight due to diabetic retinopathy. The reason that diabetic retinopathy is dangerous is that it cannot return to its pre-onset state after the disease's onset. Most patients take fundus images that capture the retina, and the doctor uses the fundus images to determine the presence of disease. Existing fundus images could only identify a narrow range, making it difficult to diagnose the disease accurately. However, with technological advances, ultra-wide-field fundus images that allow the wider retina to be seen have emerged. However, in deep learning research, many studies use existing fundus images due to the lack of new data. In the case of new technologies such as ultra-wide-field fundus images, it was often difficult to obtain data, so deep learning research could not be done properly. In the case of ultra-wide-field fundus images, research was conducted using data from hundreds to ten thousand sheets, but compared to large-scale data sets, the deep learning performance is inevitably inferior compared to large-scale data sets. In this study, synthetic data were created using ultra-wide-field fundus images and various GAN models to solve this problem. As a result of the study, BEGAN was derived similarly to the real image in qualitative and quantitative evaluation. However, it fell into mode collapse and showed the same output even when a new input came in. Mode collapse in BEGAN could be appeared depending on the amount and size of data, so various studies using BEGAN are needed. © 2022, Politeknik Negeri Padang. All rights reserved.","","deep learning; diabetic retinopathy; GAN; generative adversarial networks; ultra-wide-field fundus image","Article","Final","","Scopus","2-s2.0-85139427468"
"Khine W.S.S.; Siritanawan P.; Kotani K.","Khine, Win Shwe Sin (57219971870); Siritanawan, Prarinya (36186132800); Kotani, Kazunori (56038824900)","57219971870; 36186132800; 56038824900","Disentangled Facial Expressions Editing in Trained Latent Space","2022","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2022-October","","","1700","1706","6","10.1109/SMC53654.2022.9945408","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142695004&doi=10.1109%2fSMC53654.2022.9945408&partnerID=40&md5=f1118d2d6312c26162aaf7db6172931e","In recent years, Generative Adversarial Networks (GANs) have gained attention in image synthesis mapping from the latent space onto image space. Trained latent space carries the visual semantics for generated images. Past studies observed that arithmetic operation and linear interpolation in latent space could change the visible facial attributes, such as beards and glasses, in image space. In this work, the visual concepts in the latent space are observed, allowing to change the emotion attribute per facial expressions in the image space. We observed interpolation of a sample while disentangling the emotional attributes to edit the emotion-related facial expressions in the synthesized images. For the experiment, the Deep Convolution Generative Adversarial Networks (DCGANs) are utilized for image synthesis, and Extended Cohn Kanade (CK +) facial expression dataset is applied as the input. Our results showed that manipulating the latent space of the well-trained GANs can edit the emotional aspects of the image space. Moreover, editing facial expressions in the latent space is helpful for the recognition task to improve accuracy. Empirical results showed that the facial expressions classifier improved its performance in the recognition sadness class from 20% to 80% on the imbalance dataset.  © 2022 IEEE.","Classification (of information); Computer vision; Emotion Recognition; Generative adversarial networks; Interpolation; Semantics; Arithmetic operations; Deep convolution generative adversarial network; Disentanglement of emotional attribute; Facial Expressions; Glasses In; Image space; Images synthesis; Linear Interpolation; Visual concept; Visual semantics; Convolution","Deep Convolution Generative Adversarial Networks (DCGANs); Disentanglement of emotional attribute; Linear Interpolation","Conference paper","Final","","Scopus","2-s2.0-85142695004"
"Zeng Y.; Fu J.; Chao H.; Guo B.","Zeng, Yanhong (57201095055); Fu, Jianlong (36731082400); Chao, Hongyang (7202973656); Guo, Baining (56513051700)","57201095055; 36731082400; 7202973656; 56513051700","Aggregated Contextual Transformations for High-Resolution Image Inpainting","2022","IEEE Transactions on Visualization and Computer Graphics","","","","","","","10.1109/TVCG.2022.3156949","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126289827&doi=10.1109%2fTVCG.2022.3156949&partnerID=40&md5=9c87a86ad821800fe09a5fb8f3943cf6","Image inpainting that completes large free-form missing regions in images is a promising yet challenging task. State-of-the-art approaches have achieved significant progress by taking advantage of generative adversarial networks (GAN). However, these approaches can suffer from generating distorted structures and blurry textures in high-resolution images (e.g.,512 512). The challenges mainly drive from (1) image content reasoning from distant contexts, and (2) fine-grained texture synthesis for a large missing region. To overcome these two challenges, we propose an enhanced GAN-based model, named Aggregated COntextual-Transformation GAN (AOT-GAN), for high-resolution image inpainting. Specifically, to enhance context reasoning, we construct the generator of AOT-GAN by stacking multiple layers of a proposed AOT block. The AOT blocks aggregate contextual transformations from various receptive fields, allowing to capture both informative distant image contexts and rich patterns of interest for context reasoning. For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task. Such a training objective forces the discriminator to distinguish the detailed appearances of real and synthesized patches, and in turn facilitates the generator to synthesize clear textures. Extensive comparisons on Places2, the most challenging benchmark with 1.8 million high-resolution images of 365 complex scenes, show that our model outperforms the state-of-the-art. A user study including more than 30 subjects further validates the superiority of AOT-GAN. We further evaluate the proposed AOT-GAN in practical applications, e.g., logo removal, face editing, and object removal. Results show that our model achieves promising completions in the real world. IEEE","Image enhancement; Job analysis; Cognition; Context reasoning; Generative adversarial network; Generator; High-resolution images; Image Inpainting; Images synthesis; Object removal; Task analysis; Texture synthesis; Generative adversarial networks","Cognition; Convolution; Filling; Generative adversarial networks; generative adversarial networks (GAN); Generators; image inpainting; Image synthesis; object removal; Task analysis; Training","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85126289827"
"Din N.U.; Bae S.; Javed K.; Park H.; Yi J.","Din, Nizam Ud (57212389381); Bae, Seho (56579616500); Javed, Kamran (57204550350); Park, Hyunkyu (57918095900); Yi, Juneho (8906409400)","57212389381; 56579616500; 57204550350; 57918095900; 8906409400","Cross Modal Facial Image Synthesis Using a Collaborative Bidirectional Style Transfer Network","2022","IEEE Access","10","","","99077","99087","10","10.1109/ACCESS.2022.3207288","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139445938&doi=10.1109%2fACCESS.2022.3207288&partnerID=40&md5=9e99ddf0746b1d4ead52e14df4e11260","In this paper, we present a novel collaborative bidirectional style transfer network based on generative adversarial network (GAN) for cross modal facial image synthesis, possibly with large modality gap. We think that representation decomposed into content and style can be effectively exploited for cross modal facial image synthesis. However, we have observed that unidirectional application of decomposed representation based style transfer in case of large modality gap does not work well for this purpose. Unlike existing image synthesis methods that typically formulate image synthesis as an unidirectional feed forward mapping, our network utilizes mutual interaction between two opposite mappings in a collaborative way to address complex image synthesis problem with large modality gap. The proposed bidirectional network aligns shape content from two modalities and exchanges their appearance styles using feature maps of the layers in the encoder space. This allows us to effectively retain the shape content and transfer style details for synthesizing each modality. Focusing on facial images, we consider facial photo, sketch, and color-coded semantic segmentation as different modalities. The bidirectional synthesis results for the pairs of these modalities show the effectiveness of the proposed approach. We further apply our network to style-content manipulation to generate multiple photo images with various appearance styles for a same content shape. The proposed method can be adopted for solving other cross modal image synthesis tasks. The dataset and source code are available at https://github.com/kamranjaved/Bidirectional-style-transfer-network. © 2013 IEEE.","Job analysis; Mapping; Modal analysis; Semantic Segmentation; Semantics; Bidirectional style transfer network; Collaboration; Collaborative learning; Face; Images segmentations; Images synthesis; Shape; Task analysis; Transfer network; Unidirectional style transfer network; Generative adversarial networks","bidirectional style transfer network; collaborative learning; Generative adversarial network; image synthesis; unidirectional style transfer network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139445938"
"Du Y.; Quan Q.; Han H.; Zhou S.K.","Du, Yuanqi (57219269087); Quan, Quan (57221839758); Han, Hu (57712615100); Zhou, S. Kevin (57307954200)","57219269087; 57221839758; 57712615100; 57307954200","Semi-Supervised Pseudo-Healthy Image Synthesis via Confidence Augmentation","2022","Proceedings - International Symposium on Biomedical Imaging","2022-March","","","","","","10.1109/ISBI52829.2022.9761522","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129621770&doi=10.1109%2fISBI52829.2022.9761522&partnerID=40&md5=d1335562c0d7953c77cd585047b9903a","Pseudo-healthy image synthesis, which computationally synthesizes a pathology-free image from a pathological one, has been proved valuable in many downstream medical image analysis tasks, from lesion detection, data augmentation to clinical surgery suggestion. Thanks to the advancement of generative adversarial networks (GANs), recent studies have made steady progress to synthesize realistic-looking pseudohealthy images with the perseverance of the structure identity as well as the healthy-looking appearance. Nevertheless, it is challenging to generate high-quality pseudo-healthy images in the absence of the lesion segmentation mask. In this paper, we aim to alleviate the needs of a large amount of lesion segmentation labeled data when synthesizing pseudo-healthy images. We propose a semi-supervised pseudo-healthy image synthesis framework which leverages unlabeled pathological image data for efficient pseudo-healthy image synthesis based on a novel confidence augmentation trick. Furthermore, we re-design the network architecture which takes advantage of previous studies and allows for more flexible applications. Extensive experiments have demonstrated the effectiveness of the proposed method in generating realistic-looking pseudo-healthy images and improving downstream task performances. © 2022 IEEE.","Computer vision; Image enhancement; Image segmentation; Medical imaging; Network architecture; Supervised learning; Clinical surgery; Data augmentation; Down-stream; High quality; Images synthesis; Lesion detection; Lesion segmentations; Medical image analysis; Medical image synthesis; Semi-supervised; Generative adversarial networks","Generative adversarial networks; medical image synthesis; semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85129621770"
"Liu L.; Qiu Z.; Lin J.; Li M.; Liu Q.; Huang H.","Liu, Lihao (57375172800); Qiu, Zhao (35211434100); Lin, Jiale (57822111100); Li, Mengyang (57849207900); Liu, Qianfan (57820834000); Huang, Hancheng (57801853800)","57375172800; 35211434100; 57822111100; 57849207900; 57820834000; 57801853800","EAC-GAN: Semi-supervised Image Enhancement Technology to Improve CNN Classification Performance","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13339 LNCS","","","360","372","12","10.1007/978-3-031-06788-4_31","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135016977&doi=10.1007%2f978-3-031-06788-4_31&partnerID=40&md5=b0fb354d0ef824d626a76ff196a1eaee","Deep neural networks require a large amount of data for supervised training and learning, but it is often difficult to obtain a large amount of label data in practical applications. Since semi-supervised learning can reduce the dependence of deep networks on label data, the generative confrontation network based on semi-supervised learning can improve the classification effect. Although researchers have made progress in semi-supervised learning, small-scale, fully-supervised tasks have not been solved well, because even unlabeled data cannot be obtained in large quantities in such tasks. Therefore, we propose a new GAN model EAC-GAN based on the auxiliary classifier generative adversarial network (ACGAN), which uses genetic algorithms and semi-supervised algorithms to improve classification under fully supervised conditions. Our method utilizes ACGAN to generate artificial data for supplementing supervised classification. More specifically, we attached an external classifier to the original ACGAN generator, so it was named EAC-GAN instead of sharing an architecture with the discriminator. Our experiments show that the performance of EAC-GAN is far superior to standard methods based on data augmentation and regularization, and it is effective on a small amount of real data sets. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep neural networks; Genetic algorithms; Image classification; Image enhancement; Learning algorithms; Learning systems; Semi-supervised learning; Classification performance; Data enhancement; Image enhancement technologies; Images synthesis; Large amounts; Large amounts of data; Network-based; Semi-supervised; Semi-supervised learning; Supervised trainings; Generative adversarial networks","Data enhancement; Generative adversarial network; Image synthesis; Semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85135016977"
"Chen J.; Liu G.; Ke A.","Chen, Jian (57224917158); Liu, Gang (56205673200); Ke, Aihua (57558470800)","57224917158; 56205673200; 57558470800","Asymmetric Generative Adversarial Networks with a New Attention Mechanism","2022","Proceedings - 2022 Asia Conference on Algorithms, Computing and Machine Learning, CACML 2022","","","","186","192","6","10.1109/CACML55074.2022.00038","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137973938&doi=10.1109%2fCACML55074.2022.00038&partnerID=40&md5=d2bb8631536d699a9974f5595683b8eb","In this paper, a new residual decoding network is proposed to solve the problem that semantic label maps are transformed into real images in image processing tasks, which is a very challenging and difficult task. Since the input semantic label map lacks rich detailed information, it will generate blurry, low-detailed, and color-distorted images during con-version. We propose a new residual decoding network to solve the above problems, calling AsymmetricGAN. Compared with the traditional upsampling network, our proposed new residual module with skip and attention connections can better preserve the information in the original image to avoid the loss of details. We also propose a new module with channel and spatial attention mechanism as the main component of the network which can better retain useful information and make the edges of the synthesized image clearer and more abundant in the details. The experimental results on Cityscapes and ADE20K datasets demonstrate the advantage of AsymmetricGAN over the state-of-the-art approaches, regarding both visual quality and the representative evaluating criteria.  © 2022 IEEE.","Computer vision; Decoding; Semantics; Attention mechanisms; Channel attention residual; Distorted images; Generative adversarial net-work; Images processing; Images synthesis; Label maps; Net work; Real images; Semantic labels; Generative adversarial networks","channel attention residual; Generative Adversarial Net-works; image synthesis; semantic label","Conference paper","Final","","Scopus","2-s2.0-85137973938"
"Moriz A.; Wolfschläger D.; Montavon B.; Schmitt R.H.","Moriz, Alexander (58042899900); Wolfschläger, Dominik (57216753736); Montavon, Benjamin (57200193021); Schmitt, Robert H. (55418282900)","58042899900; 57216753736; 57200193021; 55418282900","Augmenting image datasets for quality control models using CycleGANs","2022","European Society for Precision Engineering and Nanotechnology, Conference Proceedings - 22nd International Conference and Exhibition, EUSPEN 2022","","","","217","220","3","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140844898&partnerID=40&md5=34882a73be3ca0f7f9fbc2658dcd4b83","Deep learning (DL) has proven to be a powerful tool for solving common machine vision tasks, such as image classification, defect segmentation and defect recognition. Usually, training DL models requires significant amounts of annotated data samples, which are generally sparse or of inadequate quality in many quality assurance applications in the engineering domain. Especially the thorough annotation of data yields a major obstacle for the generation of industrial datasets, since it is a complex, time-consuming task requiring expert knowledge of the process under examination. Further, the rareness of defects in rather stable production processes can lead to highly unbalanced datasets, hampering the training process. Combined with the seldom distribution of industrial data due to privacy concerns, the lack of data often hinders the adoption of DL approaches for quality assurance. Recently, network structures following the design of Generative Adversarial Networks (GANs) show astonishing results in the field of image synthesis and neural style transfer. Given a set of unpaired images from two domains, cycle-consistent GANs (CycleGANs) learn how to translate a given image from one domain to the other and vice-versa. This capability can be exploited to augment datasets in a controllable manner in order to alleviate the problems arising in the application of DL for realizing vision-based quality control. This work investigates the employment of CycleGANs to extend the image datasets for two use cases, the detection of pores in computed tomography data and the detection of surface defects on sheared edges of fine blanked parts. Given randomly generated binary masks, the trained CycleGANs are capable of generating an arbitrary amount of synthetic yet realistic images in the desired domains, alleviating the problems of both the data amount and the necessary annotations and demonstrating the great potential of image synthesis using GANs. © European Society for Precision Engineering and Nanotechnology, Conference Proceedings - 22nd International Conference and Exhibition, EUSPEN 2022. All rights reserved.","Computerized tomography; Deep learning; Generative adversarial networks; Image segmentation; Nanotechnology; Neural networks; Precision engineering; Quality control; Surface defects; Computed tomography; Control model; Cyclegan; Deep learning; Defect detection; Defect recognition; Image datasets; Images classification; Images synthesis; Machine-vision; Quality assurance","computed tomography; CycleGAN; deep learning; defect detection; image synthesis; quality assurance","Conference paper","Final","","Scopus","2-s2.0-85140844898"
"Zhang L.; Chen X.; Tu X.; Wan P.; Xu N.; Ma K.","Zhang, Linfeng (55235413600); Chen, Xin (57827214600); Tu, Xiaobing (57555714500); Wan, Pengfei (57221702931); Xu, Ning (56435225300); Ma, Kaisheng (55619478600)","55235413600; 57827214600; 57555714500; 57221702931; 56435225300; 55619478600","Wavelet Knowledge Distillation: Towards Efficient Image-to-Image Translation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","12454","12464","10","10.1109/CVPR52688.2022.01214","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135620830&doi=10.1109%2fCVPR52688.2022.01214&partnerID=40&md5=b5c867128c83c1ec36d377d350d6b983","Remarkable achievements have been attained with Generative Adversarial Networks (GANs) in image-to-image translation. However, due to a tremendous amount of parameters, state-of-the-art GANs usually suffer from low efficiency and bulky memory usage. To tackle this challenge, firstly, this paper investigates GANs performance from a frequency perspective. The results show that GANs, especially small GANs lack the ability to generate high-quality high frequency information. To address this problem, we propose a novel knowledge distillation method referred to as wavelet knowledge distillation. Instead of directly distilling the generated images of teachers, wavelet knowledge distillation first decomposes the images into different frequency bands with discrete wavelet transformation and then only distills the high frequency bands. As a result, the student GAN can pay more attention to its learning on high frequency bands. Experiments demonstrate that our method leads to 7.08× compression and 6.80× acceleration on CycleGAN with almost no performance drop. Additionally, we have studied the relation between discriminators and generators which shows that the compression of discriminators can promote the performance of compressed generators. © 2022 IEEE.","Computer vision; Deep learning; Discrete wavelet transforms; Distillation; Deep learning architecture and technique; Efficient learning; Efficient learning and inference; Image and video synthesis and generation; Image translation; Images synthesis; Learning architectures; Learning techniques; Video generation; Video synthesis; Generative adversarial networks","Deep learning architectures and techniques; Efficient learning and inferences; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135620830"
"Diviya M.; Karmel A.","Diviya, M. (57191361356); Karmel, A. (56592803500)","57191361356; 56592803500","Review on Technological Advancement and Textual Data Management Algorithms in NLP and CBIR Systems","2022","Lecture Notes in Electrical Engineering","806","","","311","321","10","10.1007/978-981-16-6448-9_32","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122513417&doi=10.1007%2f978-981-16-6448-9_32&partnerID=40&md5=520dbd85c2ddee0f1fd4f80eb2263c2c","Natural language processing (NLP) and content-based image retrieval (CBIR) systems functioned efficiently with textual input. This offers a broader framework for on-going work such as text processing applications that include spam identification, content visualization, and image retrieval for the text being queried. The preprocessing of the text handling algorithm includes part of speech tagging (POS), text encoding, and text extraction function using the word embedding encoding algorithms like Word2vec, boot strapping, hidden Markov model (HMM), and so on. The derived textual input or image attribute plays a crucial role. Following this processing, the future phase can include either of the following approach, such as content-based recovery, image recovery, or speech processing. The techniques, such as content-based retrieval, image retrieval or speech processing, each of these algorithms requires their own training period in continuation with the available technical orientations. Convolutional neural network (CNN) and recurrent neural network (RNN) play a prominent role in analyzing the input dataset when it comes to testing. Generative adversarial networks (GANs) have now launched a new era in managing NLP applications in its place. This survey provides a glimpse of various algorithms that respond to text data and return with prominent accuracy involving CBIR issues and applications for text-based image synthesis. The examination of the methodologies deployed paves way for researchers to analyze and process textual data. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Computational linguistics; Convolutional neural networks; Encoding (symbols); Hidden Markov models; Image segmentation; Information management; Long short-term memory; Natural language processing systems; Search engines; Signal encoding; Speech processing; Statistical tests; Content-Based Image Retrieval; Contents-based image retrievals; Convolutional neural network; Hidden-Markov models; Language content; Part of speech tagging; Part of speech tagging tagging; Parts-of-speech tagging; Retrieval efficiency; Textual data; Generative adversarial networks","CBIR; CNN; GANs; HMM; Image retrieval; NLP; POS tagging; Retrieval efficiency; RNN","Conference paper","Final","","Scopus","2-s2.0-85122513417"
"Kaneko T.","Kaneko, Takuhiro (57191894644)","57191894644","AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural Images with Aperture Rendering Neural Radiance Fields","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18366","18376","10","10.1109/CVPR52688.2022.01784","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141808972&doi=10.1109%2fCVPR52688.2022.01784&partnerID=40&md5=138e1ed83161168777f5fe2bbee01ced","Fully unsupervised 3D representation learning has gained attention owing to its advantages in data collection. A successful approach involves a viewpoint-aware approach that learns an image distribution based on generative models (e.g., generative adversarial networks (GANs)) while generating various view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)). However, they require images with various views for training, and consequently, their application to datasets with few or limited viewpoints remains a challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that employs a defocus cue was proposed. However, an AR-GAN is a CNN-based model and represents a defocus independently from a viewpoint change despite its high correlation, which is one of the reasons for its performance. As an alternative to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can utilize viewpoint and defocus cues in a unified manner by representing both factors in a common ray-tracing framework. Moreover, to learn defocus-aware and defocus-independent representations in a disentangled manner, we propose aperture randomized training, for which we learn to generate images while randomizing the aperture size and latent codes independently. During our experiments, we applied AR-NeRF to various natural image datasets, including flower, bird, and face images, the results of which demonstrate the utility of AR-NeRF for un-supervised learning of the depth and defocus effects. © 2022 IEEE.","Computer vision; Rendering (computer graphics); Three dimensional computer graphics; 3d from single image; Defocus; Defocus effect; Depth effects; Image and video synthesis and generation; Images synthesis; Learn+; Single images; Video generation; Video synthesis; Generative adversarial networks","3D from single images; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141808972"
"Zhang D.; Dongru H.; Kang L.; Zhang W.","Zhang, Dongbo (57767647800); Dongru, Huang (57212312646); Kang, Lanlan (24831498300); Zhang, Wei (57775312300)","57767647800; 57212312646; 24831498300; 57775312300","The generative adversarial networks and its application in machine vision","2022","Enterprise Information Systems","16","2","","326","346","20","10.1080/17517575.2019.1701714","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076410685&doi=10.1080%2f17517575.2019.1701714&partnerID=40&md5=29d6d1b502c996ecd3a8b792b9f90387","In recent years, the model of improved GAN has been widely applied in the field of machine vision. It not only covers the traditional image processing, but also includes image conversion, image synthesis and so on. Firstly, this paper describes the basic principles and existing problems of GAN, then introduces several improved GAN models, including Info-GAN, DC-GAN, f-GAN, Cat-GAN and others. Secondly, several improved GAN models for different applications in the field of machine vision are described. Finally, the future trend and development of GAN are prospected. © 2019 Informa UK Limited, trading as Taylor & Francis Group.","Deep learning; Adversarial networks; Basic principles; Discriminative networks; Existing problems; Future trends; Image conversion; Image synthesis; ITS applications; Computer vision","Deep learning; discriminative network; generative adversarial network; generative network; machine vision","Article","Final","","Scopus","2-s2.0-85076410685"
"Kim J.; Choi Y.; Uh Y.","Kim, Junho (57478151900); Choi, Yunjey (57207765183); Uh, Youngjung (54785324800)","57478151900; 57207765183; 54785324800","Feature Statistics Mixing Regularization for Generative Adversarial Networks","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11284","11293","9","10.1109/CVPR52688.2022.01101","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136484352&doi=10.1109%2fCVPR52688.2022.01101&partnerID=40&md5=738b76dfdaccbfe1c0de98add2ffcb32","In generative adversarial networks, improving discriminators is one of the key components for generation performance. As image classifiers are biased toward texture and debiasing improves accuracy, we investigate 1) if the discriminators are biased, and 2) if debiasing the discriminators will improve generation performance. Indeed, we find empirical evidence that the discriminators are sensitive to the style (e.g., texture and color) of images. As a remedy, we propose feature statistics mixing regularization (FSMR) that encourages the discriminator's prediction to be invariant to the styles of input images. Specifically, we generate a mixed feature of an original and a reference image in the discriminator's feature space and we apply regularization so that the prediction for the mixed feature is consistent with the prediction for the original image. We conduct extensive experiments to demonstrate that our regularization leads to reduced sensitivity to style and consistently improves the performance of various GAN architectures on nine datasets. In addition, adding FSMR to recently-proposed augmentation-based GAN methods further improves image quality. Our code is available at https://github.com/naver-ai/FSMR. © 2022 IEEE.","Computer vision; Forecasting; Image enhancement; Image texture; Mixing; Textures; De-biasing; Image and video synthesis and generation; Image Classifiers; Images synthesis; Input image; Performance; Reference image; Regularisation; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85136484352"
"Mathesul S.; Bhutkar G.; Rambhad A.","Mathesul, Shubham (57218938588); Bhutkar, Ganesh (35752866000); Rambhad, Ayush (57218936619)","57218938588; 35752866000; 57218936619","AttnGAN: Realistic Text-to-Image Synthesis with Attentional Generative Adversarial Networks","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13198 LNCS","","","397","403","6","10.1007/978-3-030-98388-8_35","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127197865&doi=10.1007%2f978-3-030-98388-8_35&partnerID=40&md5=0be8960cdd249320471f9c50047c7e8b","In this paper, we propose a prototype design for manifold refinement to fine grained text-to-image generation by using Attentional Generative Adversarial Network (AttnGAN) We concentrate on creating realistic images from text descriptions. We have used a collection of Attentional Generative Adversarial Network layers that are able to correctly select the modal meaning at the word-level and sentence-level. Generative Adversarial Networks (GANs) prove to be fundamental structure for many design applications from Game design, Art, Science and Modelling applications. We use GANs for contrastive learning and as a information maximisation approach, and we do extensive research to find the further advancements in image generation. Our prototype is easy to implement and practical; choosing the most relevant word vectors and using those vectors to generate related image sub-regions. The prototype in its current state generates image designs only for the bird species to satisfy the claim for its image generation ability. With due consideration to findings of usability testing, the develpment team in future iterations of the application, hopes to improve the generated image resolution. They plan to provide a choice for created variety of images with further improvements to the image generation algorithm. © 2022, IFIP International Federation for Information Processing.","Generative adversarial networks; Image enhancement; Image resolution; Network layers; Attentional generative adversarial network; DAMSM; Design applications; Fine grained; Image generations; Images synthesis; Prototype designs; Realistic images; Text-to-image synthesis; Word level; Neural networks","Artificial intelligence; Artificial neural networks; Attentional Generative Adversarial Networks; DAMSM; GAN; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85127197865"
"Li Y.-K.; Lien Y.-H.; Wang Y.-S.","Li, Yuan-Kui (57962352200); Lien, Yun-Hsuan (57962170700); Wang, Yu-Shuen (22936383100)","57962352200; 57962170700; 22936383100","Style-Structure Disentangled Features and Normalizing Flows for Diverse Icon Colorization","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11234","11243","9","10.1109/CVPR52688.2022.01096","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141787510&doi=10.1109%2fCVPR52688.2022.01096&partnerID=40&md5=34f6073ad0b76c8db7d63e438d4694de","We present a colorization network that generates flat-color icons according to given sketches and semantic colorization styles. Our network contains a style-structure disentangled colorization module and a normalizing flow. The colorization module transforms a paired sketch image and style image into a flat-color icon. To enhance network generalization and the quality of icons, we present a pixel-wise decoder, a global style code, and a contour loss to reduce color gradients at flat regions and increase color discontinuity at boundaries. The normalizing flow maps Gaussian vectors to diverse style codes conditioned on the given semantic colorization label. This conditional sampling enables users to control attributes and obtain diverse colorization results. Compared to previous methods built upon conditional generative adversarial networks, our approach enjoys the advantages of both high image quality and diversity. To evaluate its effectiveness, we compared the flat-color icons generated by our approach and recent colorization and image-to-image translation methods on various conditions. Experiment results verify that our method out- performs state-of-the-arts qualitatively and quantitatively. © 2022 IEEE.","Arts computing; Codes (symbols); Computer vision; Generative adversarial networks; Semantics; Color gradients; Conditional sampling; Flow maps; Gaussian vector; Generalisation; Image and video synthesis and generation; Images synthesis; Video generation; Video synthesis; Vision + graphic; Color","Image and video synthesis and generation; Vision + graphics","Conference paper","Final","","Scopus","2-s2.0-85141787510"
"Diamantis D.E.; Gatoula P.; Iakovidis D.K.","Diamantis, Dimitrios E. (56594997600); Gatoula, Panagiota (57226190947); Iakovidis, Dimitris K. (6603967427)","56594997600; 57226190947; 6603967427","EndoVAE: Generating Endoscopic Images with a Variational Autoencoder","2022","IVMSP 2022 - 2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop","","","","","","","10.1109/IVMSP54334.2022.9816329","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135175543&doi=10.1109%2fIVMSP54334.2022.9816329&partnerID=40&md5=d9b8a38b1e8a3cf9f3d5084689c162ff","The generalization performance of deep learning models is closely associated with the number and diversity of data available upon training. While in many applications there is a large number of data available in public, in domains such as medical image analysis, the data availability is limited. This can be largely attributed to data privacy legislations, including the General Data Protection Regulation (GDPR), and the cost of data annotation by experts. Aiming to address this issue, data augmentation approaches employing deep generative models have emerged. Existing augmentation techniques are primarily based on Generative Adversarial Networks (GANs). However, ill-posed training issues of GANs such as nonconvergence, mode collapse and instability in conjunction with their demand for large scale training datasets, complicate their use in medical imaging modalities. Motivated by these issues, this paper investigates the performance of alternative generative models i.e., Variational Autoencoders (VAEs) in endoscopic image synthesis tasks. Contrary to the conventional GAN-based approaches that aiming at augmenting the existing endoscopic datasets the proposed methodology constitutes feasible the complete substitution of medical imaging datasets from real individuals with artificially generated ones. The experimental results obtained validate the effectiveness of the proposed methodology over the state-of-art.  © 2022 IEEE.","Data privacy; Deep learning; Endoscopy; Generative adversarial networks; Large dataset; Learning systems; Auto encoders; Endoscopic image; Generalization performance; Generative model; Images synthesis; Learning models; Medical image synthesis; Number of datum; Variational autoencoder; Wireless capsule endoscopy; Medical imaging","Medical Image Synthesis; Variational Autoencoders; Wireless Capsule Endoscopy","Conference paper","Final","","Scopus","2-s2.0-85135175543"
"Tibebu H.; Malik A.; De Silva V.","Tibebu, Haileleol (57218394616); Malik, Aadin (57814016400); De Silva, Varuna (57200569349)","57218394616; 57814016400; 57200569349","Text to Image Synthesis Using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks","2022","Lecture Notes in Networks and Systems","506 LNNS","","","560","580","20","10.1007/978-3-031-10461-9_38","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135048816&doi=10.1007%2f978-3-031-10461-9_38&partnerID=40&md5=11af60d22d7488bf8713e93e2af3d960","Synthesizing a realistic image from textual description is a major challenge in computer vision. Current text to image synthesis approaches falls short of producing a high-resolution image that represent a text descriptor. Most existing studies rely either on Generative Adversarial Networks (GANs) or Variational Auto Encoders (VAEs). GANs has the capability to produce sharper images but lacks the diversity of outputs, whereas VAEs are good at producing a diverse range of outputs, but the images generated are often blurred. Taking into account the relative advantages of both GANs and VAEs, we proposed a new stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture for synthesizing images conditioned on a text description. This study uses Conditional VAEs as an initial generator to produce a high-level sketch of the text descriptor. This high-level sketch output from first stage and a text descriptor is used as an input to the conditional GAN network. The second stage GAN produces a 256 × 256 high resolution image. The proposed architecture benefits from a conditioning augmentation and a residual block on the Conditional GAN network to achieve the results. Multiple experiments were conducted using CUB and Oxford-102 dataset and the result of the proposed approach is compared against state-of-the-art techniques such as StackGAN. The experiments illustrate that the proposed method generates a high-resolution image conditioned on text descriptions and yield competitive results based on Inception and Fréchet Inception Score using both datasets. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","","Conditional GAN; Conditional VAE; Constrained image synthesis; Stacked network; Super-resolution; Text to image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135048816"
"Li J.; Sun T.; Yang Z.; Yuan Z.","Li, Junjie (57980342200); Sun, Tongfei (57979227200); Yang, Zhenkun (57979965900); Yuan, Zhoutong (57979966000)","57980342200; 57979227200; 57979965900; 57979966000","Methods and Datasets of Text to Image Synthesis Based on Generative Adversarial Network","2022","2022 IEEE 5th International Conference on Information Systems and Computer Aided Education, ICISCAE 2022","","","","843","847","4","10.1109/ICISCAE55891.2022.9927634","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142479322&doi=10.1109%2fICISCAE55891.2022.9927634&partnerID=40&md5=420e861d88ee1eb0ef80f34ff07bc3ad","Text-to-Image (T2I) synthesis refers to the computational method of translating normal human text description into images which have alike semantic meaning as text with the use of keywords or sentences. T2I has achieved great success in a few areas, especially in generating vivid, realistic visual and photographic images. Recent years, the development of deep learning has brought some new methods for unsupervised deep learning area, which provide some models to generate visually natural images with suitably trained neural network models. However, T2I now still faces some challenges. For example, current T2I models are unable to generate high resolution images with multiple objects. And it can be difficult to reproduce the results of many approaches. Since present methods are mostly based on GAN (Generative Adversarial Network) models, this paper will focus on the methods depending on it and systematically clear up the developments of T2I based on GAN. Also, datasets always play a supporting role in task development, several current T2I-related datasets will be discussed in the paper to explain more about T2I's development. A brief discussion of the future work and challenges will be showed at the end of the paper. © 2022 IEEE.","Deep learning; Learning systems; Photography; Semantics; 'current; High-resolution images; Images synthesis; Natural images; Neural network model; Neural-networks; Photographic image; Text-to-image; Trained neural networks; Visual image; Generative adversarial networks","GAN; Neural Network; synthesis; Text-to-Image","Conference paper","Final","","Scopus","2-s2.0-85142479322"
"Xia Y.; Ravikumar N.; Frangi A.F.","Xia, Yan (57219626194); Ravikumar, Nishant (57190258888); Frangi, Alejandro F. (7005249248)","57219626194; 57190258888; 7005249248","Image imputation in cardiac MRI and quality assessment","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","347","367","20","10.1016/B978-0-12-824349-7.00024-4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137570204&doi=10.1016%2fB978-0-12-824349-7.00024-4&partnerID=40&md5=2c4da69e2cead9d6e191a6f1b75e92d1","Missing data is common in medical image research. For instance, corrupted or unusable slices owing to the presence of artifacts such as respiratory or motion ghosting, aliasing, and signal loss in images significantly reduce image quality and diagnostic accuracy. Also, medical image acquisition time is often limited by cost and physical or patient care constraints, resulting in highly under-sampled images, which can be formulated as missing in-between slices. Such clinically acquired scans violate underlying assumptions of many downstream algorithms. Another important application lies in multi-modal/multi-contrast imaging, where different medical images contain complementary information for improving the diagnosis. However, a complete set of different images is often difficult to obtain. All of these can be considered as missing image data, which can lead to a reduced statistical power and potentially biased results, if not handled appropriately. Thanks to the recent advances in deep neural networks and generative adversarial networks (GANs), the problem of missing image imputation can be viewed as an image synthesis problem, and its performance has been remarkably improved. In this chapter, we present cardiac MR imaging as a use case and investigate a robust approach, namely Image Imputation Generative Adversarial Network (I2-GAN), and compare it with several traditional and state-of-the-art image imputation techniques in context of missing slices. © 2022 Elsevier Inc. All rights reserved.","","Cardiac MRI; Generative adversarial network; Image imputation; Super resolution","Book chapter","Final","","Scopus","2-s2.0-85137570204"
"Jakoel K.; Efraim L.; Shaham T.R.","Jakoel, Karin (57484265900); Efraim, Liron (57483753700); Shaham, Tamar Rott (57191429022)","57484265900; 57483753700; 57191429022","GANs Spatial Control via Inference-Time Adaptive Normalization","2022","Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022","","","","31","40","9","10.1109/WACV51458.2022.00011","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126135210&doi=10.1109%2fWACV51458.2022.00011&partnerID=40&md5=f7bdfc9f4694d07e0b35566a3ade8585","We introduce a new approach for spatial control over the generation process of Generative Adversarial Networks (GANs). Our approach includes modifying the normalization scheme of a pre-trained GAN at test time, so as to act differently at different image regions, according to guidance from the user. This enables to achieve different generation effects at different locations across the image. In contrast to previous works that require either fine-tuning the model's parameters or training an additional network, our approach uses the pre-trained GAN as is, without any further modifications or training phase. Our method is thus completely generic and can be easily incorporated into common GAN models. We prove our technique to be useful for solving a line of image manipulation tasks, allowing different generation effects across the image, while preserving the GAN's high visual quality.  © 2022 IEEE.","Air navigation; Color photography; Computer vision; Deep learning; Auto encoders; Generation process; Generative model; Image and video synthesis deep learning; Images synthesis; Neural generative model; New approaches; Normalisation; Spatial control; Video synthesis; Generative adversarial networks","Autoencoders; Computational Photography; GANs; Image and Video Synthesis Deep Learning; Neural Generative Models","Conference paper","Final","","Scopus","2-s2.0-85126135210"
"Cai C.; Xia X.; Fang Y.","Cai, Chao (57982287300); Xia, Xue (57189045358); Fang, Yuming (8435698900)","57982287300; 57189045358; 8435698900","FundusGAN: A One-Stage Single Input GAN for Fundus Synthesis","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13535 LNCS","","","28","40","12","10.1007/978-3-031-18910-4_3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142697407&doi=10.1007%2f978-3-031-18910-4_3&partnerID=40&md5=7583231d31b86a45d4dfc05bc5333ce0","Annotating medical images, especially fundus images that contain complex structures, needs expertise and time. To this end, fundus image synthesis methods were proposed to obtain specific categories of samples by combining vessel components and basic fundus images, during which well-segmented vessels from real fundus images were always required. Being different from these methods, We present a one-stage fundus image generating network to obtain healthy fundus images from scratch. First, we propose a basic attention Generator to present both global and local features. Second, we guide the Generator to focus on multi-scale fundus texture and structure features for better synthesis. Third, we design a self-motivated strategy to construct a vessel assisting module for vessel refining. By integrating the three proposed sub-modules, our fundus synthesis network, termed as FundusGAN, is built to provide one-stage fundus image generation without extra references. As a result, the synthetic fundus images are anatomically consistent with real images and demonstrate both diversity and reasonable visual quality. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Medical imaging; Textures; Complexes structure; Fundus image; Global feature; Images synthesis; Local feature; Medical images processing; Multi-scales; Retinal fundus; Single input; Synthesis method; Generative adversarial networks","Generative adversarial networks; Medical image processing; Retinal fundus; Synthesis","Conference paper","Final","","Scopus","2-s2.0-85142697407"
"Fei Y.; Zu C.; Jiao Z.; Wu X.; Zhou J.; Shen D.; Wang Y.","Fei, Yuchen (57219972263); Zu, Chen (55377165500); Jiao, Zhengyang (57671812300); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Shen, Dinggang (7401738392); Wang, Yan (56039981100)","57219972263; 55377165500; 57671812300; 57221065403; 21234416400; 7401738392; 56039981100","Classification-Aided High-Quality PET Image Synthesis via Bidirectional Contrastive GAN with Shared Information Maximization","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13436 LNCS","","","527","537","10","10.1007/978-3-031-16446-0_50","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139154802&doi=10.1007%2f978-3-031-16446-0_50&partnerID=40&md5=b8d71382277b2428a8a2ea9f8a6b7210","Positron emission tomography (PET) is a pervasively adopted nuclear imaging technique, however, its inherent tracer radiation inevitably causes potential health hazards to patients. To obtain high-quality PET image while reducing radiation exposure, this paper proposes an algorithm for high-quality standard-dose PET (SPET) synthesis from low-dose PET (LPET) image. Specifically, considering that LPET images and SPET images come from the same subjects, we argue that there is abundant shared content and structural information between LPET and SPET domains, which is helpful for improving synthesis performance. To this end, we innovatively propose a bi-directional contrastive generative adversarial network (BiC-GAN), containing a master network and an auxiliary network. Both networks implement intra-domain reconstruction and inter-domain synthesis tasks, aiming to extract shared information from LPET and SPET domains, respectively. Meanwhile, the contrastive learning strategy is also introduced to two networks for enhancing feature representation capability and acquiring more domain-independent information. To maximize the shared information extracted from two domains, we further design a domain alignment module to constrain the consistency of the shared information extracted from the two domains. On the other hand, since synthesized PET images can be used to assist disease diagnosis, such as mild cognitive impairment (MCI) identification, the MCI classification task is incorporated into PET image synthesis to further improve clinical applicability of the synthesized PET image through direct feedback from the classification task. Evaluated on a Real Human Brain dataset, our proposed method is demonstrated to achieve state-of-the-art performance quantitatively and qualitatively. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Classification (of information); Diagnosis; Health hazards; Image enhancement; Learning systems; Medical imaging; Positron emission tomography; Positrons; Contrastive learning; Generative adversarial network; High quality; Images synthesis; Low dose; PET images; Positron emission tomography; Shared information; Synthesised; Two domains; Generative adversarial networks","Contrastive learning; Generative adversarial network (GAN); Image synthesis; Positron emission tomography (PET); Shared information","Conference paper","Final","","Scopus","2-s2.0-85139154802"
"Xu Y.; Peng S.; Yang C.; Shen Y.; Zhou B.","Xu, Yinghao (57219692967); Peng, Sida (57214453118); Yang, Ceyuan (57204283344); Shen, Yujun (57207766466); Zhou, Bolei (36697366200)","57219692967; 57214453118; 57204283344; 57207766466; 36697366200","3D-aware Image Synthesis via Learning Structural and Textural Representations","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18409","18418","9","10.1109/CVPR52688.2022.01788","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134981141&doi=10.1109%2fCVPR52688.2022.01788&partnerID=40&md5=41de9e94d471c19d5a5cafb2f8be5d35","Making generative models 3D-aware bridges the 2D image space and the 3D physical world yet remains challenging. Recent attempts equip a Generative Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D coordinates to pixel values, as a 3D prior. However, the implicit function in NeRF has a very local receptive field, making the generator hard to become aware of the global structure. Meanwhile, NeRF is built on volume rendering which can be too costly to produce high-resolution results, increasing the optimization difficulty. To alleviate these two problems, we propose a novel framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis, through explicitly learning a structural representation and a textural representation. We first learn a feature volume to represent the underlying structure, which is then converted to a feature field using a NeRF-like model. The feature field is further accumulated into a 2D feature map as the textural representation, followed by a neural renderer for appearance synthesis. Such a design enables independent control of the shape and the appearance. Project page is at https://genforce.github.io/volumegan. © 2022 IEEE.","Computer vision; Generative adversarial networks; 2D images; 3d from single image; Generative model; Image and video synthesis and generation; Image space; Images synthesis; Physical world; Single images; Video generation; Video synthesis; Volume rendering","3D from single images; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134981141"
"Zhou H.; Liu X.; Wang H.; Chen Q.; Wang R.; Pang Z.-F.; Zhang Y.; Hu Z.","Zhou, Haojie (57338027300); Liu, Xinfeng (57212230077); Wang, Haiyan (57226432469); Chen, Qihang (57218526828); Wang, Rongpin (24476635900); Pang, Zhi-Feng (35280931900); Zhang, Yong (57840613200); Hu, Zhanli (24824532900)","57338027300; 57212230077; 57226432469; 57218526828; 24476635900; 35280931900; 57840613200; 24824532900","The synthesis of high-energy CT images from low-energy CT images using an improved cycle generative adversarial network","2022","Quantitative Imaging in Medicine and Surgery","12","1","","28","42","14","10.21037/qims-21-182","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119037587&doi=10.21037%2fqims-21-182&partnerID=40&md5=aac39a3fe981fb64904fee10928ebca0","Background: The dose of radiation a patient receives when undergoing dual-energy computed tomography (CT) is of significant concern to the medical community, and balancing the tradeoffs between the level of radiation used and the quality of CT images is challenging. This paper proposes a method of synthesizing high-energy CT (HECT) images from low-energy CT (LECT) images using a neural network that achieves an alternative to HECT scanning by employing an LECT scan, which greatly reduces the radiation dose a patient receives. Methods: In the training phase, the proposed structure cyclically generates HECT and LECT images to improve the accuracy of extracting edge and texture features. Specifically, we combine multiple connection methods with channel attention (CA) and pixel attention (PA) mechanisms to improve the network's mapping ability of image features. In the prediction phase, we use a model consisting of only the network component that synthesizes HECT images from LECT images. Results: Our proposed method was conducted on clinical hip CT image data sets from Guizhou Provincial People’s Hospital. In a comparison with other available methods [a generative adversarial network (GAN), a residual encoder-to-decoder network with a visual geometry group (VGG) pretrained model (RED-VGG), a Wasserstein GAN (WGAN), and CycleGAN] in terms of metrics of peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), normalized mean square error (NMSE), and a visual effect evaluation, the proposed method was found to perform better on each of these evaluation criteria. Compared with the results produced by CycleGAN, the proposed method improved the PSNR by 2.44%, the SSIM by 1.71%, and the NMSE by 15.2%. Furthermore, the differences in the statistical indicators are statistically significant, proving the strength of the proposed method. Conclusions: The proposed method synthesizes high-energy CT images from low-energy CT images, which significantly reduces both the cost of treatment and the radiation dose received by patients. Based on both image quality score metrics and visual effects comparisons, the results of the proposed method are superior to those obtained by other methods. © Quantitative Imaging in Medicine and Surgery. All rights reserved.","adult; article; attention; clinical assessment; computer assisted tomography; controlled study; deep learning; geometry; hip; human; image quality; intermethod comparison; prediction; radiation dose; signal noise ratio; synthesis","Computed tomography (CT); Cycle generative adversarial network; Deep learning; High-energy image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85119037587"
"Divya S.; Suresh L.P.; John A.","Divya, S. (57212836981); Suresh, L. Padma (54917154800); John, Ansamma (55744633800)","57212836981; 54917154800; 55744633800","Medical MR Image Synthesis using DCGAN","2022","2022 1st International Conference on Electrical, Electronics, Information and Communication Technologies, ICEEICT 2022","","","","","","","10.1109/ICEEICT53079.2022.9768647","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130259168&doi=10.1109%2fICEEICT53079.2022.9768647&partnerID=40&md5=e5b56f5f8375b66142d9526d5861953c","Generative Adversarial Networks (GANs) have been extensively gained considerable attention since 2014. Irrefutably saying, their most remarkable success has been made in domains such as computer vision and medical image processing. Despite the noteworthy success attained to date, applying GANs to real world problems still posses significant challenges, one among which is diversity of image generation and detection of fake images from real ones. Focusing on the extend to which various GAN models have made headway against these challenges, this study provides an overview of DCGAN architecture and its application as a synthetic data generator and act an a binary classifier, which detects real or fake images using brain tumorous Magnetic Resonance Imaging (MRI) dataset.  © 2022 IEEE.","Classification (of information); Data handling; Fake detection; Generative adversarial networks; Magnetic resonance imaging; Medical imaging; Brain tumors; Image detection; Image generations; Images synthesis; ITS applications; Medical images processing; MR-images; Network models; Real-world problem; Synthetic data; Brain","brain tumor; GAN; medical image processing; MRI; synthetic data","Conference paper","Final","","Scopus","2-s2.0-85130259168"
"Hou X.; Zhang X.; Li Y.; Shen L.","Hou, Xianxu (57194454615); Zhang, Xiaokang (57775046600); Li, Yudong (57538788800); Shen, Linlin (7401704647)","57194454615; 57775046600; 57538788800; 7401704647","TextFace: Text-to-Style Mapping based Face Generation and Manipulation","2022","IEEE Transactions on Multimedia","","","","","","","10.1109/TMM.2022.3160360","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126695399&doi=10.1109%2fTMM.2022.3160360&partnerID=40&md5=c2cd8a5992e9717f641c84ddcec5f4b5","As a sub-topic of Text-to-Image synthesis, Text-to-Face generation has a great potential in face related applications. In this paper, we propose a generic Text-to-Face framework, namely TextFace, to achieve diverse and high-quality face image generation from text description. We introduce a novel method called Text-to-Style mapping, where the text description can be directly encoded into the latent space of a pretrained StyleGAN. Guided by our text-image similarity matching and face captioning based text alignment, the textual latent code can be fed into a well-trained StyleGAN's generator, to produce diverse face images with high resolution (1024 1024). Furthermore, our model inherently supports the semantic face editing using text descriptions. Finally, experimental results quantitatively and qualitatively demonstrate the superior performance of our model. IEEE","Image processing; Mapping; Network coding; Semantic Web; Semantics; Code; Cross-modal; Face; Face generation; GAN; Generator; Image generations; Images synthesis; Text-guided semantic face manipulation; Text-to-face generation; Text-to-image generation; Generative adversarial networks","Codes; cross modal; Faces; GANs; Generative adversarial networks; Generators; Image synthesis; Semantics; text-guided semantic face manipulation; text-to-face generation; text-to-image generation; Training","Article","Article in press","","Scopus","2-s2.0-85126695399"
"Takeshima H.","Takeshima, Hidenori (16178471900)","16178471900","Deep Learning and Its Application to Function Approximation for MR in Medicine: An Overview","2022","Magnetic Resonance in Medical Sciences","21","4","","553","568","15","10.2463/mrms.rev.2021-0040","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139426996&doi=10.2463%2fmrms.rev.2021-0040&partnerID=40&md5=4d9ff01ba050b5bedb69138552a34643","This article presents an overview of deep learning (DL) and its applications to function approximation for MR in medicine. The aim of this article is to help readers develop various applications of DL. DL has made a large impact on the literature of many medical sciences, including MR. However, its technical details are not easily understandable for non-experts of machine learning (ML). The first part of this article presents an overview of DL and its related technologies, such as artificial intelligence (AI) and ML. AI is explained as a function that can receive many inputs and produce many outputs. ML is a process of fitting the function to training data. DL is a kind of ML, which uses a composite of many functions to approximate the function of interest. This composite function is called a deep neural network (DNN), and the functions composited into a DNN are called layers. This first part also covers the underlying technologies required for DL, such as loss functions, optimization, initialization, linear layers, non-linearities, normalization, recurrent neural networks, regularization, data augmentation, residual connections, autoencoders, generative adversarial networks, model and data sizes, and complex-valued neural networks. The second part of this article presents an overview of the applications of DL in MR and explains how functions represented as DNNs are applied to various applications, such as RF pulse, pulse sequence, reconstruction, motion correction, spectroscopy, parameter mapping, image synthesis, and segmentation. © 2021 Japanese Society for Magnetic Resonance in Medicine.","Artificial Intelligence; Deep Learning; Machine Learning; Neural Networks, Computer; artificial intelligence; machine learning","artificial intelligence; deep learning; machine learning","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139426996"
"Yang Y.; Ni X.; Hao Y.; Liu C.; Wang W.; Liu Y.; Xie H.","Yang, Yuyan (57550760900); Ni, Xin (57549972900); Hao, Yanbin (57678781500); Liu, Chenyu (57551533000); Wang, Wenshan (57550761000); Liu, Yifeng (57858917400); Xie, Haiyong (8905177400)","57550760900; 57549972900; 57678781500; 57551533000; 57550761000; 57858917400; 8905177400","MF-GAN: Multi-conditional Fusion Generative Adversarial Network for Text-to-Image Synthesis","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13141 LNCS","","","41","53","12","10.1007/978-3-030-98358-1_4","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127065875&doi=10.1007%2f978-3-030-98358-1_4&partnerID=40&md5=d7269bf3b67457e5929c330db35d2091","The performance of text-to-image synthesis has been significantly boosted accompanied by the development of generative adversarial network (GAN) techniques. The current GAN-based methods for text-to-image generation mainly adopt multiple generator-discriminator pairs to explore the coarse/fine-grained textual content (e.g., words and sentences); however, they only consider the semantic consistency between the text-image pair. One drawback of such a multi-stream structure is that it results in many heavyweight models. In comparison, the single-stream counterpart bears the weakness of insufficient use of texts. To alleviate the above problems, we propose a Multi-conditional Fusion GAN (MF-GAN) to reap the benefits of both the multi-stream and the single-stream methods. MF-GAN is a single-stream model but achieves the utilization of both coarse and fine-grained textual information with the use of conditional residual block and dual attention block. More specifically, the sentence and word features are repeatedly inputted into different model stages for textual information enhancement. Furthermore, we introduce a triple loss to close the visual gap between the synthesized image and its positive image and enlarge the gap to its negative image. To thoroughly verify our method, we conduct extensive experiments on two benchmarked CUB and COCO datasets. Experimental results show that the proposed MF-GAN outperforms the state-of-the-art methods. © 2022, Springer Nature Switzerland AG.","Image processing; Semantics; 'current; Fine grained; Images synthesis; Multi-stream; Network techniques; Network-based; Performance; Text-to-image; Textual information; Triplet loss; Generative adversarial networks","GAN; Text-to-Image; Triplet loss","Conference paper","Final","","Scopus","2-s2.0-85127065875"
"Kelkar V.A.; Gotsis D.S.; Brooks F.J.; Myers K.J.; Prabhat K.C.; Zeng R.; Anastasio M.A.","Kelkar, Varun A. (57204540505); Gotsis, Dimitrios S. (57609076600); Brooks, Frank J. (42261142200); Myers, Kyle J. (7202026697); Prabhat, K.C. (57352472900); Zeng, Rongping (8610358700); Anastasio, Mark A. (7006769220)","57204540505; 57609076600; 42261142200; 7202026697; 57352472900; 8610358700; 7006769220","Evaluating Procedures for Establishing Generative Adversarial Network-based Stochastic Image Models in Medical Imaging","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12035","","120350O","","","","10.1117/12.2612893","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130012056&doi=10.1117%2f12.2612893&partnerID=40&md5=46598bae8a33621668ecbfb9d696692a","Modern generative models, such as generative adversarial networks (GANs), hold tremendous promise for several areas of medical imaging, such as unconditional medical image synthesis, image restoration, reconstruction and translation, and optimization of imaging systems. However, procedures for establishing stochastic image models (SIMs) using GANs remain generic and do not address specific issues relevant to medical imaging. In this work, canonical SIMs that simulate realistic vessels in angiography images are employed to evaluate procedures for establishing SIMs using GANs. The GAN-based SIM is compared to the canonical SIM based on its ability to reproduce those statistics that are meaningful to the particular medically realistic SIM considered. It is shown that evaluating GANs using classical metrics and medically relevant metrics may lead to different conclusions about the fidelity of the trained GANs. This work highlights the need for the development of objective metrics for evaluating GANs. © 2022 SPIE. All rights reserved.","Image quality; Image reconstruction; Medical imaging; Stochastic models; Stochastic systems; Angiography images; Generative model; Image perception; Image restoration/reconstruction; Image translation; Images synthesis; Network-based; Objective image quality assessment; Optimisations; Stochastic image models; Generative adversarial networks","Generative adversarial networks; image perception; objective image quality assessment; stochastic image models","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85130012056"
"Kawahara D.; Ozawa S.; Saito A.; Nagata Y.","Kawahara, Daisuke (56350513700); Ozawa, Shuichi (16031706800); Saito, Akito (57804344000); Nagata, Yasushi (57218670889)","56350513700; 16031706800; 57804344000; 57218670889","Image synthesis of effective atomic number images using a deep convolutional neural network-based generative adversarial network","2022","Reports of Practical Oncology and Radiotherapy","27","5","","848","855","7","10.5603/rpOr.a2022.0093","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141800425&doi=10.5603%2frpOr.a2022.0093&partnerID=40&md5=b440c240c71fcfedee2b8806ca84b6d9","Background: The effective atomic numbers obtained from dual-energy computed tomography (DECT) can aid in characterization of materials. In this study, an effective atomic number image reconstructed from a DECT image was synthesized using an equivalent single-energy CT image with a deep convolutional neural network (CNN)-based generative adversarial network (GAN). Materials and methods: The image synthesis framework to obtain the effective atomic number images from a single-energy CT image at 120 kVp using a CNN-based GAN was developed. The evaluation metrics were the mean absolute error (MAE), relative root mean square error (RMSE), relative mean square error (MSE), structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and mutual information (MI). Results: The difference between the reference and synthetic effective atomic numbers was within 9.7% in all regions of interest. The averages of MAE, RMSE, MSE, SSIM, PSNR, and MI of the reference and synthesized images in the test data were 0.09, 0.045, 0.0, 0.89, 54.97, and 1.03, respectively. Conclusions: In this study, an image synthesis framework using single-energy CT images was constructed to obtain atomic number images scanned by DECT. This image synthesis framework can aid in material decomposition without extra scans in DECT. © 2022 Greater Poland Cancer Centre. Published by Via Medica. All rights reserved. e-ISSN 2083–4640 ISSN 1507–1367","article; convolutional neural network; decomposition; deep learning; dual energy computed tomography; mean absolute error; root mean squared error; signal noise ratio; synthesis","Deep learning; Effective atomic number; Generative adversarial network","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85141800425"
"Liu H.; Sigona M.K.; Manuel T.J.; Chen M.; Caskey C.F.; Dawant B.M.","Liu, Han (57279011700); Sigona, Michelle K. (57220935174); Manuel, Thomas J. (57219033859); Chen, Min (57359618000); Caskey, Charles F. (10044625700); Dawant, Benoit M. (7007025943)","57279011700; 57220935174; 57219033859; 57359618000; 10044625700; 7007025943","Synthetic CT Skull Generation for Transcranial MR Imaging-Guided Focused Ultrasound Interventions with Conditional Adversarial Networks","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12034","","120340O","","","","10.1117/12.2612946","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131932269&doi=10.1117%2f12.2612946&partnerID=40&md5=758febb865e2be0f426a6984d3ae69b1","Transcranial MRI-guided focused ultrasound (TcMRgFUS) is a therapeutic ultrasound method that focuses sound through the skull to a small region noninvasively under MRI guidance. It is clinically approved to thermally ablate regions of the thalamus and is being explored for other therapies, such as blood brain barrier opening and neuromodulation. To accurately target ultrasound through the skull, the transmitted waves must constructively interfere at the target region. However, heterogeneity of the sound speed, density, and ultrasound attenuation in different individuals' skulls requires patient-specific estimates of these parameters for optimal treatment planning. CT imaging is currently the gold standard for estimating acoustic properties of an individual skull during clinical procedures, but CT imaging exposes patients to radiation and increases the overall number of imaging procedures required for therapy. A method to estimate acoustic parameters in the skull without the need for CT would be desirable. Here, we synthesized CT images from routinely acquired T1-weighted MRI by using a 3D patch-based conditional generative adversarial network (cGAN) and evaluated the performance of synthesized CT images for treatment planning with transcranial focused ultrasound. A dataset of 86 paired CT and T1-weighted MR images were randomly split so that 66 images were used for training, 10 for validation and parameter tuning, and 10 for acoustic testing. We compared the performance of synthetic CT (sCT) to real CT (rCT) images using an open-source treatment planning software, Kranion, and found that the number of active elements, skull density ratio, and skull thickness between rCT and sCT had Pearson's Correlation Coefficients of 0.989, 0.915, and 0.941, respectively, suggesting strong positive linear correlation. Of a total of 990 elements 95.7 ± 1.4% of active and inactive elements overlapped between rCTs and sCTs. Simulations using the acoustic toolbox, k-Wave, resulted in 23.5 ± 6.51% less maximum root-mean-squared (RMS) pressure simulated with sCTs than the corresponding rCT pressure. An average focal shift of 0.96 ± 0.56 mm and 1.07 ± 0.58 mm was observed between the thalamus target and the maximum RMS pressure location in rCTs and sCTs, respectively. Our work demonstrates the feasibility of replacing real CT with the MR-synthesized CT for TcMRgFUS planning.  © 2022 SPIE.","Acoustic properties; Brain; Computerized tomography; Correlation methods; Generative adversarial networks; Medical imaging; Open source software; Open systems; Patient treatment; Speech; Statistical tests; Tumors; Ultrasonic imaging; Ultrasonics; Acoustic simulations; Adversarial networks; Conditional adversarial network; Focused ultrasound; Image-guided; Images synthesis; Synthesised; Transcranial; Transcranial focused ultrasound; Treatment planning; Magnetic resonance imaging","Acoustic Simulations; Conditional Adversarial Networks; Image Synthesis; Image-guided; Transcranial Focused Ultrasound","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131932269"
"Cheng Y.-C.; Lin C.H.; Lee H.-Y.; Ren J.; Tulyakov S.; Yang M.-H.","Cheng, Yen-Chi (57215776267); Lin, Chieh Hubert (57204294634); Lee, Hsin-Ying (57207324989); Ren, Jian (57194466391); Tulyakov, Sergey (57213004407); Yang, Ming-Hsuan (7404927015)","57215776267; 57204294634; 57207324989; 57194466391; 57213004407; 7404927015","InOut: Diverse Image Outpainting via GAN Inversion","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11421","11430","9","10.1109/CVPR52688.2022.01114","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141260927&doi=10.1109%2fCVPR52688.2022.01114&partnerID=40&md5=6435ab0c308aed70b35778f87c38f9d0","Image outpainting seeks for a semantically consistent extension of the input image beyond its available content. Compared to inpainting - filling in missing pixels in a way coherent with the neighboring pixels - outpainting can be achieved in more diverse ways since the problem is less constrained by the surrounding pixels. Existing image outpainting methods pose the problem as a conditional image-to-image translation task, often generating repetitive structures and textures by replicating the content available in the input image. In this work, we formulate the problem from the perspective of inverting generative adversarial networks. Our generator renders micro-patches conditioned on their joint latent code as well as their individual positions in the image. To outpaint an image, we seek for multiple latent codes not only recovering available patches but also synthesizing diverse outpainting by patch-based generation. This leads to richer structure and content in the outpainted regions. Furthermore, our formulation allows for outpainting conditioned on the categorical input, thereby enabling flexible user controls. Extensive experimental results demonstrate the proposed method performs favorably against existing in- and outpainting methods, featuring higher visual quality and diversity. © 2022 IEEE.","Codes (symbols); Computer vision; Generative adversarial networks; Textures; Filling in; Image and video synthesis and generation; Image translation; Images synthesis; Inpainting; Input image; Repetitive structure; Representation learning; Video generation; Video synthesis; Pixels","Image and video synthesis and generation; Representation learning","Conference paper","Final","","Scopus","2-s2.0-85141260927"
"Hu H.; Pang J.","Hu, Hailong (57198978159); Pang, Jun (23390311500)","57198978159; 23390311500","Membership Inference Attacks against GANs by Leveraging Over-representation Regions","2021","Proceedings of the ACM Conference on Computer and Communications Security","","","","2387","2389","2","10.1145/3460120.3485338","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119350450&doi=10.1145%2f3460120.3485338&partnerID=40&md5=4a818703626aa57a81b272b1c2720efb","Generative adversarial networks (GANs) have made unprecedented performance in image synthesis and play a key role in various downstream applications of computer vision. However, GAN models trained on sensitive data also pose a distinct threat to privacy. In this poster, we present a novel over-representation based membership inference attack. Unlike prior attacks against GANs which focus on the overall metrics, such as the attack accuracy, our attack aims to make inference from the high-precision perspective, which allows the adversary to concentrate on inferring a sample as a member confidently. Initial experimental results demonstrate that the adversary can achieve a high precision attack even if the overall attack accuracy is about 50% for a well-trained GAN model. Our work will raise awareness of the importance of precision when GAN owners evaluate the privacy risks of their models. © 2021 Owner/Author.","Computer vision; Face generation; High-precision; Human face generation; Human faces; Images synthesis; Inference attacks; Membership inference attack; Network models; Over-representation; Performance; Generative adversarial networks","generative adversarial networks; human face generation; membership inference attacks; over-representation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119350450"
"Jiang B.; Huang Y.; Huang W.; Yang C.; Xu F.","Jiang, Bin (54887690400); Huang, Yun (57219092255); Huang, Wei (57199042718); Yang, Chao (56257703800); Xu, Fangqiang (57211501480)","54887690400; 57219092255; 57199042718; 56257703800; 57211501480","Multi-scale dual-modal generative adversarial networks for text-to-image synthesis","2022","Multimedia Tools and Applications","","","","","","","10.1007/s11042-022-14080-8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140997913&doi=10.1007%2fs11042-022-14080-8&partnerID=40&md5=afc2f6f163546de32eaf001ff62e308a","Generating images from text descriptions is a challenging task due to the natural gap between the textual and visual modalities. Despite the promising results of existing methods, they suffer from two limitations: (1) focus more on the image semantic information while fails to fully explore the texture information; (2) only consider to model the correlation between words and image with a fixed scale, thus decreases the diversity and discriminability of the network representations. To address above issues, we propose a Multi-scale Dual-modal Generative Networks (MD-GAN). The core components of MD-GAN are the dual-modal modulation attention (DMA) and the multi-scale consistency discriminator (MCD). The DMA includes two blocks: the textual guiding module that captures the correlation between images and text descriptions to rectify the image semantic content, and the channel sampling module that adjusts image texture by selectively aggregating the channel-wise information on spatial space. In addition, the MCD constructs the correlation between text and image region of various sizes, enhancing the semantic consistency between text and images. Extensive experiments on CUB and MS-COCO datasets show the superiority of MD-GAN over state-of-the-art methods. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image enhancement; Image texture; Semantics; Textures; Attention mechanisms; Cross-modal; Discriminability; Image semantics; Images synthesis; Multi-scales; Semantics Information; Text-to-image synthesis; Texture information; Visual modalities; Generative adversarial networks","Attention mechanism; Cross-modal; Generative adversarial network; Text-to-image synthesis","Article","Article in press","","Scopus","2-s2.0-85140997913"
"Li W.; Wen S.; Shi K.; Yang Y.; Huang T.","Li, Wei (57248838700); Wen, Shiping (25823384000); Shi, Kaibo (57639731000); Yang, Yin (57222193676); Huang, Tingwen (57225870325)","57248838700; 25823384000; 57639731000; 57222193676; 57225870325","Neural Architecture Search With a Lightweight Transformer for Text-to-Image Synthesis","2022","IEEE Transactions on Network Science and Engineering","9","3","","1567","1576","9","10.1109/TNSE.2022.3147787","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124238949&doi=10.1109%2fTNSE.2022.3147787&partnerID=40&md5=3a530115f696375cf9cd3b8609d0366c","Despite the cross-modal text-to-imagesynthesis task has achieved great success, most of the latest works in this field are based on the network architectures proposed by predecessors, such as StackGAN, AttnGAN, etc. Since the quality for text-to-image synthesis is more and more demanding, these old and tandem architectures with simple convolution operations are no longer suitable. Therefore, a novel text-to-image synthesis network combining with the latest technologies is in urgent need of exploration. To tackle with this challenge, we creatively propose a unique architecture for text-to-image synthesis, dubbed T2IGAN, which is automatically searched by neural architecture search (NAS). In addition, considering the amazing capabilities of the popular transformer in natural language processing and computer vision, a lightweight transformer is applied in our search space to efficiently integrate the text features and image features. Ultimately, the effectiveness of our searched T2IGAN is remarkable by experimentally evaluating it on the typical text-to-image synthesis datasets. Specifically, we achieve an excellent result of IS 5.12 and FID 10.48 on CUB-200 Birds, IS 4.89 and FID 13.55 on Oxford-102 Flowers, IS 31.93 and FID 26.45 on COCO. By contrast with the state-of-the-art works, ours gets better performance on CUB-200 Birds and Oxford-102 Flowers.  © 2013 IEEE.","Birds; Computer architecture; Generative adversarial networks; Image processing; Job analysis; Natural language processing systems; Network architecture; Semantic Web; Cross-modal; Images synthesis; Neural architecture search; Neural architectures; Search problem; Task analysis; Text-to-image synthesis; Transformer; Semantics","Generative adversarial network; neural architecture search; text-to-image synthesis; transformer","Article","Final","","Scopus","2-s2.0-85124238949"
"Kim J.; Oh H.; Kim S.; Tong H.; Lee S.","Kim, Jinwoo (57765279800); Oh, Heeseok (57963201600); Kim, Seongjean (57538410500); Tong, Hoseok (57962328200); Lee, Sanghoon (55746172800)","57765279800; 57963201600; 57538410500; 57962328200; 55746172800","A Brand New Dance Partner: Music-Conditioned Pluralistic Dancing Controlled by Multiple Dance Genres","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","3480","3490","10","10.1109/CVPR52688.2022.00348","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141810115&doi=10.1109%2fCVPR52688.2022.00348&partnerID=40&md5=a6f77fecd02609f213d4ba427d16cbc5","When coming up with phrases of movement, choreographers all have their habits as they are used to their skilled dance genres. Therefore, they tend to return certain patterns of the dance genres that they are familiar with. What if artificial intelligence could be used to help choreographers blend dance genres by suggesting various dances, and one that matches their choreographic style? Numerous task-specific variants of autoregressive networks have been developed for dance generation. Yet, a serious limitation remains that all existing algorithms can return repeated patterns for a given initial pose sequence, which may be inferior. To mitigate this issue, we propose MNET, a novel and scalable approach that can perform music-conditioned pluralistic dance generation synthesized by multiple dance genres using only a single model. Here, we learn a dancegenre aware latent representation by training a conditional generative adversarial network leveraging Transformer architecture. We conduct extensive experiments on AIST++ along with user studies. Compared to the state-of-the-art methods, our method synthesizes plausible and diverse outputs according to multiple dance genres as well as generates outperforming dance sequences qualitatively and quantitatively. © 2022 IEEE.","Computer vision; Deep learning; Music; Network architecture; Action recognition; Deep learning architecture and technique; Event recognition; Image and video synthesis and generation; Images synthesis; Learning architectures; Learning techniques; Video generation; Video synthesis; Vision + X; Generative adversarial networks","Action and event recognition; Deep learning architectures and techniques; Image and video synthesis and generation; Vision + X","Conference paper","Final","","Scopus","2-s2.0-85141810115"
"Pan K.; Cheng P.; Huang Z.; Lin L.; Tang X.","Pan, Kai (57551514900); Cheng, Pujin (57212007255); Huang, Ziqi (57892305800); Lin, Li (57892391800); Tang, Xiaoying (57202405783)","57551514900; 57212007255; 57892305800; 57892391800; 57202405783","Transformer-Based T2-weighted MRI Synthesis from T1-weighted Images","2022","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2022-July","","","5062","5065","3","10.1109/EMBC48229.2022.9871183","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138128774&doi=10.1109%2fEMBC48229.2022.9871183&partnerID=40&md5=d06e6dea21950b34c5532140e0b937c3","Multi-modality magnetic resonance (MR) images provide complementary information for disease diagnoses. However, modality missing is quite usual in real-life clinical practice. Current methods usually employ convolution-based generative adversarial network (GAN) or its variants to synthesize the missing modality. With the development of vision transformer, we explore its application in the MRI modality synthesis task in this work. We propose a novel supervised deep learning method for synthesizing a missing modality, making use of a transformer-based encoder. Specifically, a model is trained for translating 2D MR images from T1-weighted to T2-weighted based on conditional GAN (cGAN). We replace the encoder with transformer and input adjacent slices to enrich spatial prior knowledge. Experimental results on a private dataset and a public dataset demonstrate that our proposed model outperforms state-of-the-art supervised methods for MR image synthesis, both quantitatively and qualitatively. Clinical relevance - This work proposes a method to synthesize T2-weighted images from T1-weighted ones to address the modality missing issue in MRI. © 2022 IEEE.","Magnetic Resonance Imaging; Computer vision; Deep learning; Diagnosis; Generative adversarial networks; Learning systems; Magnetic resonance; Medical imaging; Signal encoding; 'current; Clinical practices; Disease diagnosis; ITS applications; Learning methods; Multi-modality; Public dataset; Spatial prior knowledge; T1-weighted; T2 weighted; nuclear magnetic resonance imaging; procedures; Magnetic resonance imaging","","Conference paper","Final","","Scopus","2-s2.0-85138128774"
"Li Z.; Zhang J.; Liu L.; Liu J.","Li, Zhuohang (57215844154); Zhang, Jiaxin (57077340000); Liu, Luyang (57148879800); Liu, Jian (56380662800)","57215844154; 57077340000; 57148879800; 56380662800","Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","10122","10132","10","10.1109/CVPR52688.2022.00989","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141704344&doi=10.1109%2fCVPR52688.2022.00989&partnerID=40&md5=9c771d7cf18f2511914edc1e9e728253","Federated Learning (FL) framework brings privacy benefits to distributed learning systems by allowing multiple clients to participate in a learning task under the coordination of a central server without exchanging their private data. However, recent studies have revealed that private information can still be leaked through shared gradient information. To further protect user's privacy, several defense mechanisms have been proposed to prevent privacy leakage via gradient information degradation methods, such as using additive noise or gradient compression before sharing it with the server. In this work, we validate that the private training data can still be leaked under certain defense settings with a new type of leakage, i.e., Generative Gradient Leakage (GGL). Unlike existing methods that only rely on gradient information to reconstruct data, our method leverages the latent space of generative adversarial networks (GAN) learned from public image datasets as a prior to compensate for the informational loss during gradient degradation. To address the nonlinearity caused by the gradient operator and the GAN model, we explore various gradient-free optimization methods (e.g., evolution strategies and Bayesian optimization) and empirically show their superiority in reconstructing high-quality images from gradients compared to gradient-based optimizers. We hope the proposed method can serve as a tool for empirically measuring the amount of privacy leakage to facilitate the design of more robust defense mechanisms. © 2022 IEEE.","Generative adversarial networks; Image reconstruction; Learning systems; Network security; Optimization; Defence mechanisms; Distributed learning; Gradient informations; Image and video synthesis and generation; Images synthesis; Learning frameworks; Privacy and federated learning; Privacy leakages; Video generation; Video synthesis; Additive noise","Image and video synthesis and generation; Privacy and federated learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141704344"
"Li X.; Li M.; Jiang Y.; Yin S.; Luo H.","Li, Xiang (57192491218); Li, Minglei (57218560083); Jiang, Yuchen (57189689209); Yin, Shen (35346008700); Luo, Hao (55915888200)","57192491218; 57218560083; 57189689209; 35346008700; 55915888200","An Intelligent Retinal Fundus Image Label Sharing Method by Domain Transformation Technique","2022","Lecture Notes in Networks and Systems","505 LNNS","","","233","241","8","10.1007/978-3-031-09176-6_28","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135061910&doi=10.1007%2f978-3-031-09176-6_28&partnerID=40&md5=1a2c75af46c1daffe452b3f884c2cf1a","Recent deep learning methods have shown excellent performance in retinal vessel image segmentation, but the well-trained model will no longer be effective when applied to a dataset that has a large difference from the training dataset. Moreover, only a few annotated datasets can be used for supervised training, considering that clinical processes produce a large number of un-annotated images with diverse styles. As a result, it has become a big challenge to design effective deep learning segmentation models to be practically applicable. In this paper, an unsupervised cycle retinal generative adversarial network is proposed. It can realize the mutual transformation between annotated datasets and un-annotated datasets. The transformed images still retain the original vessel structure. Only the image style has been changed, so that the annotated vessel label can be shared in two domains. Furthermore, the synthetic images and the shared vessel labels can be used to train the deep learning segmentation model. We conducted experiments on annotated datasets and un-annotated datasets. The experiment results show that the cross-domain synthetic images have authentic appearance, vessel structure is well maintained. Both domains have good segmentation results. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","","Cross domain transformation; Retinal image segmentation; Retinal image synthesis","Conference paper","Final","","Scopus","2-s2.0-85135061910"
"Benny Y.; Wolf L.","Benny, Yaniv (57219509614); Wolf, Lior (57203078732)","57219509614; 57203078732","Dynamic Dual-Output Diffusion Models","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11472","11481","9","10.1109/CVPR52688.2022.01119","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141813185&doi=10.1109%2fCVPR52688.2022.01119&partnerID=40&md5=4731b3624fd2fd4a507ce5e59c977465","Iterative denoising-based generation, also known as denoising diffusion models, has recently been shown to be comparable in quality to other classes of generative models, and even surpass them. Including, in particular, Generative Adversarial Networks, which are currently the state of the art in many subtasks of image generation. However, a major drawback of this method is that it requires hundreds of iterations to produce a competitive result. Recent works have proposed solutions that allow for faster generation with fewer iterations, but the image quality gradually deteriorates with increasingly fewer iterations being applied during generation. In this paper, we reveal some of the causes that affect the generation quality of diffusion models, especially when sampling with few iterations, and come up with a simple, yet effective, solution to mitigate them. We consider two opposite equations for the iterative denoising, the first predicts the applied noise, and the second predicts the image directly. Our solution takes the two options and learns to dynamically alternate between them through the denoising process. Our proposed solution is general and can be applied to any existing diffusion model. As we show, when applied to various SOTA architectures, our solution immediately improves their generation quality, with negligible added complexity and parameters. We experiment on multiple datasets and configurations and run an extensive ablation study to support these findings. © 2022 IEEE.","Computer vision; Diffusion; Iterative methods; De-noising; Diffusion model; Dual outputs; Generative model; Image and video synthesis and generation; Images synthesis; Iterative denoising; State of the art; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141813185"
"Ko M.; Cha E.; Suh S.; Lee H.; Han J.-J.; Shin J.; Han B.","Ko, Minsu (57963018800); Cha, Eunju (57190218998); Suh, Sungjoo (57963194700); Lee, Huijin (57962140100); Han, Jae-Joon (55646340200); Shin, Jinwoo (55834885800); Han, Bohyung (8729746700)","57963018800; 57190218998; 57963194700; 57962140100; 55646340200; 55834885800; 8729746700","Self-Supervised Dense Consistency Regularization for Image-to-Image Translation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18280","18289","9","10.1109/CVPR52688.2022.01776","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137654364&doi=10.1109%2fCVPR52688.2022.01776&partnerID=40&md5=98018ddcb92c181d86c94a20501043ec","Unsupervised image-to-image translation has gained considerable attention due to recent impressive advances in generative adversarial networks (GANs). This paper presents a simple but effective regularization technique for improving GAN-based image-to-image translation. To generate images with realistic local semantics and structures, we propose an auxiliary self-supervision loss that enforces point-wise consistency of the overlapping region between a pair of patches cropped from a single real image during training the discriminator of a GAN. Our experiment shows that the proposed dense consistency regularization improves performance substantially on various image-to-image translation scenarios. It also leads to extra performance gains through the combination with instance-level regularization methods. Furthermore, we verify that the proposed model captures domain-specific characteristics more effectively with only a small fraction of training data. © 2022 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Learning systems; Semantics; Computer vision theory; Deep learning architecture and technique; Image and video synthesis and generation; Images synthesis; Learning architectures; Learning techniques; Machine-learning; Representation learning; Video generation; Video synthesis; Vision theory; Computer vision","Computer vision theory; Deep learning architectures and techniques; Image and video synthesis and generation; Machine learning; Representation learning","Conference paper","Final","","Scopus","2-s2.0-85137654364"
"Tomczak A.; Gupta A.; Ilic S.; Navab N.; Albarqouni S.","Tomczak, Agnieszka (57221307707); Gupta, Aarushi (57221012600); Ilic, Slobodan (12775624800); Navab, Nassir (7003458998); Albarqouni, Shadi (55129204800)","57221307707; 57221012600; 12775624800; 7003458998; 55129204800","What Can We Learn About a Generated Image Corrupting Its Latent Representation?","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13436 LNCS","","","505","515","10","10.1007/978-3-031-16446-0_48","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139113921&doi=10.1007%2f978-3-031-16446-0_48&partnerID=40&md5=2ca1f3693bd442c461ce68193bf7b573","Generative adversarial networks (GANs) offer an effective solution to the image-to-image translation problem, thereby allowing for new possibilities in medical imaging. They can translate images from one imaging modality to another at a low cost. For unpaired datasets, they rely mostly on cycle loss. Despite its effectiveness in learning the underlying data distribution, it can lead to a discrepancy between input and output data. The purpose of this work is to investigate the hypothesis that we can predict image quality based on its latent representation in the GANs bottleneck. We achieve this by corrupting the latent representation with noise and generating multiple outputs. The degree of differences between them is interpreted as the strength of the representation: the more robust the latent representation, the fewer changes in the output image the corruption causes. Our results demonstrate that our proposed method has the ability to i) predict uncertain parts of synthesized images, and ii) identify samples that may not be reliable for downstream tasks, e.g., liver segmentation task. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Image quality; Image segmentation; Medical imaging; Data distribution; Effective solution; Image translation; Images synthesis; Imaging modality; Input and outputs; Input datas; Learn+; Low-costs; Uncertainty; Generative adversarial networks","GANs; Image quality; Image synthesis; Uncertainty","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85139113921"
"Tan Y.X.; Lee C.P.; Neo M.; Lim K.M.; Lim J.Y.","Tan, Yong Xuan (57579573000); Lee, Chin Poo (36519082900); Neo, Mai (35424115800); Lim, Kian Ming (36554491300); Lim, Jit Yan (57225956739)","57579573000; 36519082900; 35424115800; 36554491300; 57225956739","Enhanced Text-to-Image Synthesis Conditional Generative Adversarial Networks","2022","IAENG International Journal of Computer Science","49","1","","1","7","6","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130609642&partnerID=40&md5=13385f2a7c0411af7a36e5db3bcc4415","The text-to-image synthesis aims to synthesize an image based on a given text description, which is especially useful for applications in image editing, graphic design, etc. The main challenges of text-to-image synthesis are to generate images that are visually realistic and semantically consistent with the given text description. In this paper, we proposed some enhancements to the conditional generative model that is widely used for text-to-image synthesis. The enhancements include text conditioning augmentation, feature matching, and LI distance loss function. The text conditioning augmentation expands the text embedding feature space to improve the semantic consistency of the model. The feature matching motivates the model to synthesize more photo-realistic images and enrich the image content variations. Apart from that, the LI distance loss allows the model to generate images that have high visual resemblance to the real images. The empirical results on the CUB-200-2011 dataset demonstrate that the text-to-image synthesis conditional generative model with the proposed enhancements yield the highest Inception score and Structural Similarity Index. © 2022. All Rights Reserved.","Image enhancement; Semantics; CGAN; Conditional generative adversarial network; Features matching; GAN; Generative model; Graphic design; Image editing; Image-based; Images synthesis; Text-to-image-synthesis; Generative adversarial networks","cGANs; conditional generative adversarial networks; GANs; generative adversarial network; text-to-image-synthesis","Article","Final","","Scopus","2-s2.0-85130609642"
"","","","28th International Conference on MultiMedia Modeling, MMM 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13141 LNCS","","","","","1222","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127057690&partnerID=40&md5=2b7bd38618d427bdf703aa26e1eb34ec","The proceedings contain 107 papers. The special focus in this conference is on MultiMedia Modeling. The topics include: Non-Uniform Attention Network for Multi-modal Sentiment Analysis; combining Knowledge and Multi-modal Fusion for Meme Classification; bi-attention Modal Separation Network for Multimodal Video Fusion; Melody Generation from Lyrics Using Three Branch Conditional LSTM-GAN; a-Muze-Net: Music Generation by Composing the Harmony Based on the Generated Melody; Speech Intelligibility Enhancement By Non-Parallel Speech Style Conversion Using CWT and iMetricGAN Based CycleGAN; time-Frequency Attention for Speech Emotion Recognition with Squeeze-and-Excitation Blocks; Real-Time FPGA Design for OMP Targeting 8K Image Reconstruction; one-Stage Image Inpainting with Hybrid Attention; MF-GAN: Multi-conditional Fusion Generative Adversarial Network for Text-to-Image Synthesis; fast Single Image Dehazing Using Morphological Reconstruction and Saturation Compensation; arbitrary Style Transfer with Adaptive Channel Network; point Cloud Upsampling via a Coarse-to-Fine Network; JVCSR: Video Compressive Sensing Reconstruction with Joint In-Loop Reference Enhancement and Out-Loop Super-Resolution; SAM: Self Attention Mechanism for Scene Text Recognition Based on Swin Transformer; A Multiple Positives Enhanced NCE Loss for Image-Text Retrieval; multimodal Embedding for Lifelog Retrieval; prediction of Blood Glucose Using Contextual LifeLog Data; fall Detection Using Multimodal Data; an Investigation into Keystroke Dynamics and Heart Rate Variability as Indicators of Stress; PF-VTON: Toward High-Quality Parser-Free Virtual Try-On Network; joint Re-Detection and Re-Identification for Multi-Object Tracking; Multi-scale Cross-Modal Transformer Network for RGB-D Object Detection; A Complementary Fusion Strategy for RGB-D Face Recognition; double Granularity Relation Network with Self-criticism for Occluded Person Re-identification; Using Explainable AI to Identify Differences Between Clinical and Experimental Pain Detection Models Based on Facial Expressions; rating-Aware Self-Organizing Maps; preface.","","","Conference review","Final","","Scopus","2-s2.0-85127057690"
"Oh Y.-T.; Ko E.; Park H.","Oh, Young-Tack (57640444400); Ko, Eunsook (57208861411); Park, Hyunjin (56512679000)","57640444400; 57208861411; 56512679000","TDM-Stargan: Stargan Using Time Difference Map to Generate Dynamic Contrast-Enhanced Mri from Ultrafast Dynamic Contrast-Enhanced Mri","2022","Proceedings - International Symposium on Biomedical Imaging","2022-March","","","","","","10.1109/ISBI52829.2022.9761463","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129585220&doi=10.1109%2fISBI52829.2022.9761463&partnerID=40&md5=c36480631197e9b825f21c828ff88762","Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a sensitive imaging technique to manage many types of cancer including breast cancer. The conventional DCE-MRI takes a long time (7-12 minutes) to acquire and there is a clinical need to reduce scan time. Ultrafast DCE-MRI takes less than a minute to acquire and has sufficient information relative to conventional DCE-MRI. We propose a generative adversarial network (GAN) to generate the delay phase of synthetic conventional DCE-MRI from ultrafast DCE-MRI. We allow our model to better generate the area expected to be a lesion through the difference map of different phases to incorporate time-varying enhancement patterns. The difference map also allows us to generate pseudo tumor labels for segmentation. Our approach was trained and tested on 300 cases using three evaluation metrics. Our method showed better performance (structural similarity index map increase of 11.69%) compared to Pix2Pix baseline method. © 2022 IEEE.","Diseases; Generative adversarial networks; Image enhancement; Maps; Medical imaging; Breast Cancer; Difference map; Dynamic contrast enhanced MRI; Dynamic contrast-enhanced; Dynamic contrast-enhanced magnetic resonance imaging; Images synthesis; Time-differences; Ultra-fast dynamics; Ultrafast dynamic contrast-enhanced MRI; Magnetic resonance imaging","breast cancer; difference map; dynamic contrast-enhanced MRI; GAN; image synthesis; ultrafast dynamic contrast-enhanced MRI","Conference paper","Final","","Scopus","2-s2.0-85129585220"
"Hajij M.; Zamzmi G.; Paul R.; Thukar L.","Hajij, Mustafa (55830720500); Zamzmi, Ghada (57194182535); Paul, Rahul (57193679977); Thukar, Lokenda (57606961900)","55830720500; 57194182535; 57193679977; 57606961900","Normalizing Flow for Synthetic Medical Images Generation","2022","Healthcare Innovations and Point of Care Technologies Conference, HI-POCT 2022","","","","46","49","3","10.1109/HI-POCT54491.2022.9744072","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128599871&doi=10.1109%2fHI-POCT54491.2022.9744072&partnerID=40&md5=3fd67677e8a1752e39d1227450c53911","Deep generative models, such as generative adversarial network (GAN) and variational autoencoder (VAE), have been utilized extensively for medical image generation. While these models made remarkable progress in medical image synthesis, they can not explicitly learn the probability density function of the input data and are highly sensitive to the hyperparameter selections. To mitigate these issues, a new type of deep generative model, called Normalizing Flows (NFs), have emerged in recent years. In this paper, we investigate NFs as an alternative for synthesizing medical images. In particular, we utilize realNVP, a popular NF model for the purpose of synthesizing medical images. To evaluate our synthesized images, we propose to utilize Wasserstien distance along with the permutation test to quantify the quality of the generated images. Within our quantifying metric, our results indicate that the two sample distributions, the first being the samples obtained from our NF model and second being the original dataset, are similar providing a promising indication of normalizing flow's capability in medical images generation.  © 2022 IEEE.","Computer vision; Medical imaging; Probability density function; Auto encoders; Flow modelling; Generative model; Hyper-parameter; Image generations; Images synthesis; Input datas; Learn+; Permutation tests; Synthesized images; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85128599871"
"Saxena D.; Kulshrestha T.; Cao J.; Cheung S.-C.","Saxena, Divya (56669424900); Kulshrestha, Tarun (55583922600); Cao, Jiannong (7403354073); Cheung, Shing-Chi (7202472792)","56669424900; 55583922600; 7403354073; 7202472792","Multi-Constraint Adversarial Networks for Unsupervised Image-to-Image Translation","2022","IEEE Transactions on Image Processing","31","","","1601","1612","11","10.1109/TIP.2022.3144886","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123968479&doi=10.1109%2fTIP.2022.3144886&partnerID=40&md5=6146a97339e0d55ee259fa6b569f13b8","Unsupervised image-to-image translation aims to learn the mapping from an input image in a source domain to an output image in a target domain without paired training dataset. Recently, remarkable progress has been made in translation due to the development of generative adversarial networks (GANs). However, existing methods suffer from the training instability as gradients passing from discriminator to generator become less informative when the source and target domains exhibit sufficiently large discrepancies in appearance or shape. To handle this challenging problem, in this paper, we propose a novel multi-constraint adversarial model (MCGAN) for image translation in which multiple adversarial constraints are applied at generator's multi-scale outputs by a single discriminator to pass gradients to all the scales simultaneously and assist generator training for capturing large discrepancies in appearance between two domains. We further notice that the solution to regularize generator is helpful in stabilizing adversarial training, but results may have unreasonable structure or blurriness due to less context information flow from discriminator to generator. Therefore, we adopt dense combinations of the dilated convolutions at discriminator for supporting more information flow to generator. With extensive experiments on three public datasets, cat-to-dog, horse-to-zebra, and apple-to-orange, our method significantly improves state-of-the-arts on all datasets.  © 1992-2012 IEEE.","Convolution; Discriminators; Image resolution; Job analysis; Network coding; Generative model; Generator; Image translation; Images synthesis; Multi-constraints; Shape; Target domain; Task analysis; Unsupervised image-to-image translation; animal experiment; apple; article; cat; dog; horse; nonhuman; zebra; Generative adversarial networks","GANs; Generative adversarial networks; generative modeling; image synthesis; unsupervised image-to-image translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123968479"
"Chai L.; Gharbi M.; Shechtman E.; Isola P.; Zhang R.","Chai, Lucy (57219750569); Gharbi, Michaël (57191989681); Shechtman, Eli (55924548800); Isola, Phillip (50561507500); Zhang, Richard (57223101349)","57219750569; 57191989681; 55924548800; 50561507500; 57223101349","Any-Resolution Training for High-Resolution Image Synthesis","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13676 LNCS","","","170","188","18","10.1007/978-3-031-19787-1_10","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142703116&doi=10.1007%2f978-3-031-19787-1_10&partnerID=40&md5=98ea9c961c053638e8f72eb8ed1b2880","Generative models operate at fixed resolution, even though natural images come in a variety of sizes. As high-resolution details are downsampled away and low-resolution images are discarded altogether, precious supervision is lost. We argue that every pixel matters and create datasets with variable-size images, collected at their native resolutions. To take advantage of varied-size data, we introduce continuous-scale training, a process that samples patches at random scales to train a new generator with variable output resolutions. First, conditioning the generator on a target scale allows us to generate higher resolution images than previously possible, without adding layers to the model. Second, by conditioning on continuous coordinates, we can sample patches that still obey a consistent global layout, which also allows for scalable training at higher resolutions. Controlled FFHQ experiments show that our method can take advantage of multi-resolution training data better than discrete multi-scale approaches, achieving better FID scores and cleaner high-frequency details. We also train on other natural image domains including churches, mountains, and birds, and demonstrate arbitrary scale synthesis with both coherent global layouts and realistic local details, going beyond 2K resolution in our experiments. Our project page is available at: https://chail.github.io/anyres-gan/. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Computer vision; Computers; Continuous coordinate function; Coordinate functions; Generative model; High-Resolution Details; High-resolution images; Images synthesis; Multi-scale learning; Multi-scales; Natural images; Unconditional image synthesis; Generative adversarial networks","Continuous coordinate functions; Generative adversarial networks; Multi-scale learning; Unconditional image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142703116"
"Liang N.; Yuan L.; Wen X.; Xu H.; Wang J.","Liang, Nan (57838073800); Yuan, Liming (55480260300); Wen, Xianbin (57837891700); Xu, Haixia (55611885900); Wang, Jingyi (57838349900)","57838073800; 55480260300; 57837891700; 55611885900; 57838349900","End-To-End Retina Image Synthesis Based on CGAN Using Class Feature Loss and Improved Retinal Detail Loss","2022","IEEE Access","10","","","83125","83137","12","10.1109/ACCESS.2022.3196377","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135735667&doi=10.1109%2fACCESS.2022.3196377&partnerID=40&md5=9bd4e57c83c408f8c91a035382541095","Retinal images are the most direct and effective basis for Diabetic Retinopathy (DR) diagnosis. With the rapid development of deep learning, the technology of retinal image-assisted diagnosis based on deep learning is widely used in the field of DR intelligent diagnosis. However, the training of deep neural network usually requires a large number of annotated samples, but retinal images annotated by professional doctors are cost-expensive and difficult to obtain, which limits the application of deep learning technology in DR intelligent diagnosis. In order to alleviate the scarcity of labelled retinal images, we propose an end-to-end conditional generative adversarial network with class feature loss and improved retinal detail loss. The network combines the above two losses with the adversarial loss, and jointly constrains the generator to generate high-quality retinal images. The proposed retinal detail loss is summed over physiological detail loss which is meant to preserve high-level semantic features of the physiological details contained in the fundus images and pixel loss which ensures the low-level features in synthesized image will not deviate from the real image. In addition, the class feature loss constrains the synthesized images to be consistent with the real images in class features representation, which further makes the synthesized images have pathological features of the corresponding grade. The generated images by the proposed network are evaluated from three objective metrics including the subjective effect and the FID, SWD, which are used to evaluate the quality and diversity of generated images, and the effect of retinal vessel segmentation, respectively. Experimental results demonstrate that our synthesized images have superior performance on both the quality and quantity. © 2013 IEEE.","Deep neural networks; Eye protection; Generative adversarial networks; Image enhancement; Image segmentation; Ophthalmology; Physiology; Quality control; Semantics; Conditional generative adversarial network; Deep learning; Diabetic retinopathy; Diabetic retinopathy grading; Features extraction; Images synthesis; Retina; Retinal image; Retinal image synthesis; Grading","conditional generative adversarial network; deep learning; Diabetic retinopathy; DR~grading; retinal image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135735667"
"Chai L.; Liu Q.","Chai, Lu (57893008900); Liu, Qinyuan (56383093500)","57893008900; 56383093500","Semi-Supervised Semantic Segmentation of Class-Imbalanced Images: A Hierarchical Self-Attention Generative Adversarial Network","2022","2022 7th International Conference on Image, Vision and Computing, ICIVC 2022","","","","398","404","6","10.1109/ICIVC55077.2022.9886496","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139501084&doi=10.1109%2fICIVC55077.2022.9886496&partnerID=40&md5=5d91bab30cd214c585c6ede33b9e52a0","How to train models with unlabeled data and implement one trained model across several data sets are key problems in computer vision applications that require high-cost annotations. Recently, a generative model [1] proves its advantages in semi-supervised segmentation and out-of-domain generalization. However, this method becomes less effective when meet with class-imbalanced images whose foreground occupies small areas. To solve this problem, we introduce a hierarchical generative model with a self-attention mechanism to help with capturing features of foreground objects. Concretely, we apply a two-stage hierarchical generative model to perform image synthesis with the self-attention mechanism. Since attention maps are also semantic labels in segmentation fields, the hierarchical self-attention model can synthesize images and corresponding segmentation labels simultaneously. At test time, the segmentation is achieved by mapping input images into latent presentations with two encoders and synthesizing labels with the generative model. We evaluate our hierarchical model on three biomedical segmentation data sets. The experimental results demonstrate that our method outperforms other baselines on semi-supervised segmentation of class-imbalanced images, and meanwhile, pre-serves out-of-domain generalization ability.  © 2022 IEEE.","Generative adversarial networks; Hierarchical systems; Semantics; Attention mechanisms; Biomedical images; Data set; Generative model; Hierarchical generative model; Self-attention mechanism; Semantic segmentation; Semi-supervised; Semi-supervised learning; Semi-supervised segmentations; Semantic Segmentation","biomedical images; hierarchical generative model; self-attention mechanism; semantic segmentation; semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85139501084"
"Tan H.; Liu X.; Yin B.; Li X.","Tan, Hongchen (57209272265); Liu, Xiuping (36910875600); Yin, Baocai (8616230700); Li, Xin (57218467896)","57209272265; 36910875600; 8616230700; 57218467896","Cross-Modal Semantic Matching Generative Adversarial Networks for Text-to-Image Synthesis","2022","IEEE Transactions on Multimedia","24","","","832","845","13","10.1109/TMM.2021.3060291","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101784148&doi=10.1109%2fTMM.2021.3060291&partnerID=40&md5=787528329591043b8fe2523bb8e9301d","Synthesizing photo-realistic images based on text descriptions is a challenging image generation problem. Although many recent approaches have significantly advanced the performance of text-to-image generation, to guarantee semantic matchings between the text description and synthesized image remains very challenging. In this paper, we propose a new model, Cross-modal Semantic Matching Generative Adversarial Networks (CSM-GAN), to improve the semantic consistency between text description and synthesized image for a fine-grained text-to-image generation. Two new modules are proposed in CSM-GAN: Text Encoder Module (TEM) and Textual-Visual Semantic Matching Module (TVSMM). TVSMM is aimed at making the distance of the pairs of synthesized image and its corresponding text description closer, in global semantic embedding space, than those of mismatched pairs. This improves the semantic consistency and consequently, the generalizability of CSM-GAN. In TEM, we introduce Text Convolutional Neural Networks (Text_CNNs) to capture and highlight local visual features in textual descriptions. Thorough experiments on two public benchmark datasets demonstrated the superiority of CSM-GAN over other representative state-of-the-art methods. © 1999-2012 IEEE.","Convolutional neural networks; Semantic Web; Semantics; Adversarial networks; Benchmark datasets; Photorealistic images; Semantic consistency; Semantic embedding; State-of-the-art methods; Synthesized images; Textual description; Image enhancement","Cross-modal semantic matching; generative adversarial network (GAN); text-CNNs; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85101784148"
"Dravid A.; Schiffers F.; Wu Y.; Cossairt O.; Katsaggelos A.K.","Dravid, Amil (57219466758); Schiffers, Florian (57201069099); Wu, Yunan (57219466485); Cossairt, Oliver (8620017500); Katsaggelos, Aggelos K. (7102711302)","57219466758; 57201069099; 57219466485; 8620017500; 7102711302","INVESTIGATING THE POTENTIAL OF AUXILIARY-CLASSIFIER GANS FOR IMAGE CLASSIFICATION IN LOW DATA REGIMES","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","3318","3322","4","10.1109/ICASSP43922.2022.9747286","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131236993&doi=10.1109%2fICASSP43922.2022.9747286&partnerID=40&md5=1ffd69a561b63d35f5ffc4f0b3102bf8","Generative Adversarial Networks (GANs) have shown promise in augmenting datasets and boosting convolutional neural network (CNN) performance on image classification tasks. But they introduce more hyperparameters to tune as well as the need for additional time and computational power to train, supplementary to the CNN. In this work, we examine the potential for Auxiliary-Classifier GANs (AC-GANs) as a'one-stop-shop' architecture for image classification, particularly in low data regimes. Additionally, we explore modifications to the typical AC-GAN framework, changing the generator's latent space sampling scheme and employing a Wasserstein loss with gradient penalty to stabilize the simultaneous training of image synthesis and classification. Through experiments on images of varying resolutions and complexity, we demonstrate that AC-GANs show promise in image classification, achieving competitive performance with standard CNNs. These methods can be employed as an'all-in-one' framework with particular utility in the absence of large amounts of training data. © 2022 IEEE","Classification (of information); Computer vision; Convolution; Convolutional neural networks; Deep learning; Image classification; Classification tasks; Computational power; Convolutional neural network; Data augmentation; Deep learning; Hyper-parameter; Images classification; One stop shops; Sampling schemes; Space sampling; Generative adversarial networks","Convolutional Neural Networks; Data Augmentation; Deep Learning; Generative Adversarial Networks; Image Classification","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131236993"
"Zhang Z.; Schomaker L.","Zhang, Zhenxing (57219361101); Schomaker, Lambert (19640514000)","57219361101; 19640514000","Optimized latent-code selection for explainable conditional text-to-image GANs","2022","Proceedings of the International Joint Conference on Neural Networks","2022-July","","","","","","10.1109/IJCNN55064.2022.9892738","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140786635&doi=10.1109%2fIJCNN55064.2022.9892738&partnerID=40&md5=5469f038d0896c34c4d1bdf480087668","The task of text-to-image generation has achieved remarkable progress due to the advances in conditional generative adversarial networks (GANs). However, existing conditional text-to-image GANs approaches mostly concentrate on improving both image quality and semantic relevance but ignore the explainability of the model which plays a vital role in real-world applications. In this paper, we present a variety of techniques to take a deep look into the latent space and semantic space of a conditional text-to-image GANs model. We introduce pairwise linear interpolation of latent codes and 'linguistic' linear interpolation to study what the model has learned within the latent space and 'linguistic' embeddings. Subsequently, we extend linear interpolation to triangular interpolation conditioned on three corners to further analyze the model. After that, we build a Good/Bad data set containing unsuccessfully and successfully synthesized samples and corresponding latent codes for the image-quality research. Based on this data set, we propose a framework for finding good latent codes by utilizing a linear SVM. Experimental results on the recent DiverGAN generator trained on two benchmark data sets qualitatively prove the effectiveness of our presented techniques, with a better than 94% accuracy in predicting Good/Bad classes for latent vectors. The Good/Bad data set is publicly available at https://zenodo.org/record/5850224#.YeGMwP7MKUk. © 2022 IEEE.","Codes (symbols); Computer vision; Image enhancement; Image quality; Interpolation; Semantics; Support vector machines; A good/bad data set; Bad data; Code selection; Data set; Image generations; Images synthesis; Linear Interpolation; Semantic relevance; Text-to-image synthesis; Triangular interpolation; Generative adversarial networks","a Good/Bad data set; linear interpolation; text-to-image synthesis; triangular interpolation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140786635"
"Cao G.; Liu S.; Mao H.; Zhang S.; Chen Y.; Dai C.","Cao, Guogang (57193384068); Liu, Shunkun (57222115156); Mao, Hongdong (57222109935); Zhang, Shu (57212232484); Chen, Ying (36760397500); Dai, Cuixia (8938449600)","57193384068; 57222115156; 57222109935; 57212232484; 36760397500; 8938449600","Medical Image Synthesis Based on Optimized Cycle‑Generative Adversarial Networks; [基于优化循环生成对抗网络的医学图像合成方法]","2022","Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing","37","1","","155","163","8","10.16337/j.1004⁃9037.2022.01.013","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123985439&doi=10.16337%2fj.1004%e2%81%839037.2022.01.013&partnerID=40&md5=4883feaa2349057ab095b2712244f7f7","The radiation treatment plan system needs to calculate the dose distribution accurately based on CT images, but sometimes clinical MR images can only be obtained. Image synthesis effectively creates new modality images from another modality, which enhances image information. This paper presents a new method of synthesizing high precision and definition of CT images from MR images. To synthesize clearly pseudo CT images, an improved cycle-consistent generative adversarial network (CycleGAN) with densely connected convolutional network (DenseNet) is proposed. Avoiding the disappearance of input information and the vanishing of gradient information, the improved network can synthesize more credible CT images. Compared with the original method, the proposed method is reduced by 5.9% on mean absolute error, increased by 1.1% on structural similarity and increased by 4.4% on peak signal to ratio, which is trained and tested on the dataset of 18 patients. And compared with the deep convolutional neural network and the atlas-based method, the improved CycleGAN is reduced by 0.065% and 0.55% on relative error, respectively. The proposed method can synthesize more vivid CT images owing to the advantages of deep learning model, which better meets the requirements of dose calculation in radiation treatment planning system. © 2022 by Journal of Data Acquisition and Processing.","","Cycle-consistent generation adversarial network (CycleGAN); Densely connected convolutional network (DenseNet); Image synthesis; Unpaired data","Article","Final","","Scopus","2-s2.0-85123985439"
"Yu Z.; Zhao Y.; Hong B.; Jin Z.; Huang J.; Cai D.; Hua X.-S.","Yu, Zhengxu (57211745513); Zhao, Yilun (57219791807); Hong, Bin (57655558100); Jin, Zhongming (57214627074); Huang, Jianqiang (57197792472); Cai, Deng (57566049800); Hua, Xian-Sheng (55441195100)","57211745513; 57219791807; 57655558100; 57214627074; 57197792472; 57566049800; 55441195100","Apparel-Invariant Feature Learning for Person Re-Identification","2022","IEEE Transactions on Multimedia","24","","","4482","4492","10","10.1109/TMM.2021.3119133","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117266191&doi=10.1109%2fTMM.2021.3119133&partnerID=40&md5=078b12bd8b5645ca31706291d193a0db","With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons' appearance rarely changes. In real-world applications such as in a shopping mall, the same person may change their wearings, and different persons may wear similar apparel. It reveals a critical problem that current ReID models heavily rely on a person's apparel, resulting in an inconsistent ReID performance. Therefore, it is crucial to learn an apparel-invariant person representation under clothes changing or several persons wearing similar clothes cases. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth-changing images according to the target cloth embedding. It is worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth-changing images. Extensive experiments demonstrate that our proposal can improve the ReID performance of the baseline models.  © 1999-2012 IEEE.","Computer vision; Data transfer; Data visualization; Deep learning; GAN; Generator; Image color analysis; Images synthesis; Invariant features; Person re identifications; Proposal; Re identifications; Training data; Transfer learning; Generative adversarial networks","GAN; image synthesis; Person re-identification; transfer learning","Article","Final","","Scopus","2-s2.0-85117266191"
"Selcuk S.Y.; Dalmaz O.; Dar S.U.H.; Cukur T.","Selcuk, Sahan Yoruc (57274651400); Dalmaz, Onat (57226257796); Dar, Salman Ul Hassan (57195220338); Cukur, Tolga (23034054800)","57274651400; 57226257796; 57195220338; 23034054800","Improving Image Synthesis Quality in Multi-Contrast MRI Using Transfer Learning via Autoencoders; [Çoklu Kontrast MRG'de Otokodlayici ve Öǧrenme Aktarimi Kullanarak Görüntü Sentez Kalitesini Iyileştirme]","2022","2022 30th Signal Processing and Communications Applications Conference, SIU 2022","","","","","","","10.1109/SIU55565.2022.9864750","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138692280&doi=10.1109%2fSIU55565.2022.9864750&partnerID=40&md5=09127476ea5c4e00595f9f4e68d32577","The capacity of magnetic resonance imaging (MRI) to capture several contrasts within a session enables it to obtain increased diagnostic information. However, such multi-contrast MRI tests take a long time to scan, resulting in acquiring just a part of the essential contrasts. Synthetic multi-contrast MRI has the potential to improve radiological observations and consequent image analysis activities. Because of its ability to generate realistic results, generative adversarial networks (GAN) have recently been the most popular choice for medical imaging synthesis. This paper proposes a novel generative adversarial framework to improve the image synthesis quality in multi-contrast MRI. Our method uses transfer learning to adapt pre-trained autoencoder networks to the synthesis task and enhances the image synthesis quality by initializing the training process with more optimal network parameters. We demonstrate that the proposed method outperforms competing synthesis models by 0.95 dB on average on a well-known multi-contrast MRI dataset. © 2022 IEEE.","Diagnosis; Image enhancement; Learning systems; Magnetic resonance imaging; Medical imaging; Transfer learning; Auto encoders; Diagnostics informations; Image-analysis; Images synthesis; Imaging tests; Multi-contrast magnetic resonance imaging; Network parameters; Optimal networks; Training process; Transfer learning; Generative adversarial networks","autoencoder; generative adversarial networks; Multi-contrast MRI; transfer learning","Conference paper","Final","","Scopus","2-s2.0-85138692280"
"Sekar A.; Perumal V.","Sekar, Aravindkumar (57391714500); Perumal, Varalakshmi (46661903700)","57391714500; 46661903700","Crack Image Synthesis and Segmentation using Paired Image Translation","2022","Proceedings - IEEE International Conference on Advances in Computing, Communication and Applied Informatics, ACCAI 2022","","","","","","","10.1109/ACCAI53970.2022.9752524","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128847844&doi=10.1109%2fACCAI53970.2022.9752524&partnerID=40&md5=b6aae3c73a1572b3a3aef1cf68dbf3a5","We introduce a novel method for intelligent transportation systems to investigate the synthesising of crack images and crack image segmentation. Our model proposes Image-To-Image paired translation GAN for crack image synthesizing and U network with skip connections for segmentation. To achieve synthesis of crack images, we trained a paired imageto image translation model using generative adversarial network associated with perceptual loss function and for segmentation U-network is designed with skip connections. We conduct an extensive experiments and results for the generated crack images with ground truth images and also for crack region segmentation. The obtained result indicates that our proposed model generate more similar image and also segment crack regions accurately when compared with various existing approaches.  © 2022 IEEE.","Computer vision; Cracks; Generative adversarial networks; Intelligent systems; Crack image; Crack image generation; Crack image segmentation; Image generations; Image translation; Images segmentations; Network models; Paired image translation; U-network model; Image segmentation","Crack image generation; Crack image segmentation; Paired image translation; U-network model","Conference paper","Final","","Scopus","2-s2.0-85128847844"
"Dong C.; Kumar A.; Liu E.","Dong, Chengdong (57919309600); Kumar, Ajay (55716727200); Liu, Eryun (36016886500)","57919309600; 55716727200; 36016886500","Think Twice Before Detecting GAN-generated Fake Images from their Spectral Domain Imprints","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","7855","7864","9","10.1109/CVPR52688.2022.00771","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141794905&doi=10.1109%2fCVPR52688.2022.00771&partnerID=40&md5=a097565d9d959578900165a87ec79c87","Accurate detection of the fake but photorealistic images is one of the most challenging tasks to address social, biometrics security and privacy related concerns in our community. Earlier research has underlined the existence of spectral domain artifacts in fake images generated by powerful generative adversarial network (GAN) based methods. Therefore, a number of highly accurate frequency domain methods to detect such GAN generated images have been proposed in the literature. Our study in this paper introduces a pipeline to mitigate the spectral artifacts. We show from our experiments that the artifacts in frequency spectrum of such fake images can be mitigated by proposed methods, which leads to the sharp decrease of performance of spectrum-based detectors. This paper also presents experimental results using a large database of images that are synthesized using BigGAN, CRN, CycleGAN, IMLE, Pro-GAN, StarGAN, StyleGAN and StyleGAN2 (including synthesized high resolution fingerprint images) to illustrate effectiveness of the proposed methods. Furthermore, we select a spatial-domain based fake image detector and observe a notable decrease in the detection performance when proposed method is incorporated. In summary, our insightful analysis and pipeline presented in this paper cautions the forensic community on the reliability of GAN-generated fake image detectors that are based on the analysis of frequency artifacts as these artifacts can be easily mitigated. © 2022 IEEE.","Computer vision; Fake detection; Frequency domain analysis; Pipelines; Reliability analysis; Strain measurement; Computer vision for social good; Image and video synthesis and generation; Image detector; Images synthesis; Spectral domains; Synthesised; Video generation; Video synthesis; Vision applications; Vision systems; Generative adversarial networks","Computer vision for social good; Image and video synthesis and generation; Vision applications and systems","Conference paper","Final","","Scopus","2-s2.0-85141794905"
"Aziz N.A.; Sulaiman M.A.H.; Zabidi A.; Yassin I.M.; Ali M.S.A.M.; Rizman Z.I.","Aziz, Nurhakimah Abd (57753901900); Sulaiman, Mohd Azman Hanif (57753902000); Zabidi, Azlee (35093630300); Yassin, Ihsan Mohd (35110052600); Ali, Megat Syahirul Amin Megat (55387509700); Rizman, Zairi Ismael (36959761800)","57753901900; 57753902000; 35093630300; 35110052600; 55387509700; 36959761800","Lightweight Generative Adversarial Network Fundus Image Synthesis","2022","International Journal on Informatics Visualization","6","1-2","","270","277","7","10.30630/joiv.6.1-2.924","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132406172&doi=10.30630%2fjoiv.6.1-2.924&partnerID=40&md5=fadfeeccc7c9849215c89f46ba018a6c","Blindness is a global health problem that affects billions of lives. Recent advancements in Artificial Intelligence (AI), (Deep Learning (DL)) has the intervention potential to address the blindness issue, particularly as an accurate and non-invasive technique for early detection and treatment of Diabetic Retinopathy (DR). DL-based techniques rely on extensive examples to be robust and accurate in capturing the features responsible for representing the data. However, the number of samples required is tremendous for the DL classifier to learn properly. This presents an issue in collecting and categorizing many samples. Therefore, in this paper, we present a lightweight Generative Neural Network (GAN) to synthesize fundus samples to train AI-based systems. The GAN was trained using samples collected from publicly available datasets. The GAN follows the structure of the recent Lightweight GAN (LGAN) architecture. The implementation and results of the LGAN training and image generation are described. Results indicate that the trained network was able to generate realistic high-resolution samples of normal and diseased fundus images accurately as the generated results managed to realistically represent key structures and their placements inside the generated samples, such as the optic disc, blood vessels, exudates, and others. Successful and unsuccessful generation samples were sorted manually, yielding 56.66% realistic results relative to the total generated samples. Rejected generated samples appear to be due to inconsistencies in shape, key structures, placements, and color. © 2022, Politeknik Negeri Padang. All rights reserved.","","artificial intelligence; data synthesis; fundus; Generative Adversarial Network (GAN)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132406172"
"Alshehri H.A.; Junath N.; Panwar P.; Shukla K.; Rahin S.A.; Martin R.J.","Alshehri, Hamdan Ali (57758733600); Junath, N. (56026373200); Panwar, Poonam (55222163800); Shukla, Kirti (56209228100); Rahin, Saima Ahmed (57681105100); Martin, R. John (57118175700)","57758733600; 56026373200; 55222163800; 56209228100; 57681105100; 57118175700","Self-Attention-Based Edge Computing Model for Synthesis Image to Text through Next-Generation AI Mechanism","2022","Mathematical Problems in Engineering","2022","","4973535","","","","10.1155/2022/4973535","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132537320&doi=10.1155%2f2022%2f4973535&partnerID=40&md5=b0c6b5f1cd5bfa1d2ca7117fbcee655a","Image synthesis based on natural language description has become a research hotspot in edge computing in artificial intelligence. With the help of generative adversarial edge computing networks, the field has made great strides in high-resolution image synthesis. However, there are still some defects in the authenticity of synthetic single-target images. For example, there will be abnormal situations such as ""multiple heads""and ""multiple mouths""when synthesizing bird graphics. Aiming at such problems, a text generation single-target model SA-AttnGAN based on a self-attention mechanism is proposed. SA-AttnGAN (Attentional Generative Adversarial Network) refines text features into word features and sentence features to improve the semantic alignment of text and images; in the initialization stage of AttnGAN, the self-attention mechanism is used to improve the stability of the text-generated image model; the multistage GAN network is used to superimpose, finally synthesizing high-resolution images. Experimental data show that SA-AttnGAN outperforms other comparable models in terms of Inception Score and Frechet Inception Distance; synthetic image analysis shows that this model can learn background and colour information and correctly capture bird heads and mouths. The structural information of other components is improved, and the AttnGAN model generates incorrect images such as ""multiple heads""and ""multiple mouths.""Furthermore, SA-AttnGAN is successfully applied to description-based clothing image synthesis with good generalization ability. © 2022 Hamdan Ali Alshehri et al.","Edge computing; Generative adversarial networks; Image enhancement; Semantics; Attention mechanisms; Computing model; Edge computing; High-resolution images; Hotspots; Images synthesis; Language description; Natural languages; Target images; Text generations; Birds","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132537320"
"Li D.; Ling H.; Kim S.W.; Kreis K.; Fidler S.; Torralba A.","Li, Daiqing (57207766063); Ling, Huan (57202059633); Kim, Seung Wook (57210865814); Kreis, Karsten (55069147400); Fidler, Sanja (12139176800); Torralba, Antonio (7005432728)","57207766063; 57202059633; 57210865814; 55069147400; 12139176800; 7005432728","BigDatasetGAN: Synthesizing ImageNet with Pixel-wise Annotations","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","21298","21308","10","10.1109/CVPR52688.2022.02064","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138505466&doi=10.1109%2fCVPR52688.2022.02064&partnerID=40&md5=2f149538d214e67fbe7a794081b0b01d","Annotating images with pixel-wise labels is a time consuming and costly process. Recently, DatasetGAN [78] showcased a promising alternative - to synthesize a large labeled dataset via a generative adversarial network (GAN) by exploiting a small set of manually labeled, GAN generated images. Here, we scale DatasetGAN to ImageNet scale of class diversity. We take image samples from the class-conditional generative model BigGAN [5] trained on ImageNet, and manually annotate only 5 images per class, for all 1k classes. By training an effective feature segmentation architecture on top of BigGAN, we turn Big GAN into a labeled dataset generator. We further show that VQGAN [18] can similarly serve as a dataset generator, leveraging the already annotated data. We create a new ImageNet benchmark by labeling an additional set of real images and evaluate segmentation performance in a variety of settings. Through an extensive ablation study, we show big gains in leveraging a large generated dataset to train different supervised and self-supervised backbone models on pixel-wise tasks. Furthermore, we demonstrate that using our synthesized datasets for pre-training leads to improvements over standard ImageNet pre-training on several downstream datasets, such as PASCAL-VOC, MS-COCO, Cityscapes and chest X-ray, as well as tasks (detection, segmentation). Our benchmark will be made public and maintain a leaderboard for this challenging task. Project Page: https://nv-tlabs.github.io/big-datasetgan/ © 2022 IEEE.","Benchmarking; Computer vision; Image enhancement; Image segmentation; Large dataset; Pixels; Dataset and evaluation; Grouping and shape analyse; Image and video synthesis and generation; Images synthesis; Long tail; Representation learning; Segmentation; Self- & semi- & meta- transfer/low-shot/long-tail learning; Shape-analysis; Video generation; Video synthesis; Generative adversarial networks","Datasets and evaluation; grouping and shape analysis; Image and video synthesis and generation; Representation learning; Segmentation; Self- & semi- & meta- Transfer/low-shot/long-tail learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138505466"
"Mensing D.; Hirsch J.; Wenzel M.; Günther M.","Mensing, Daniel (57222121758); Hirsch, Jochen (7402780690); Wenzel, Markus (15057268400); Günther, Matthias (7102455457)","57222121758; 7402780690; 15057268400; 7102455457","3D (c)GAN for Whole Body MR Synthesis","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13609 LNCS","","","97","105","8","10.1007/978-3-031-18576-2_10","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141750685&doi=10.1007%2f978-3-031-18576-2_10&partnerID=40&md5=83562d504ed58735387c94e1a38571a6","Synthesis of images has recently seen many works that produce high-quality real world images. In the domain of medical imaging the application of deep generative models especially Generative Adversarial Networks (GANs) can be applied to many different tasks. Under the premise of the generation of high-quality images that match the distribution of the original data, the synthesized data can be used to increase the size of small datasets, or in combination with conditioning on meta data, to increase the size of underrepresented classes in the dataset. In this work we propose a model that generates 3D medical images. The model can easily be conditioned on meta data, for example available patient information. We evaluate the quality of the generated images and compare our model against the 3D-StyleGAN model which is also designed for 3D medical image synthesis. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","3D modeling; Medical imaging; Metadata; 3-D image; 3d image synthesis; 3D medical image; 3D-images; Conditional generative adversarial network; High quality; Images synthesis; Meta-data; Real-world image; Whole-body; Generative adversarial networks","3D Image Synthesis; Conditional GAN; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85141750685"
"Bi Z.; Cao B.; Zuo W.; Hu Q.","Bi, Zhiwei (57944031100); Cao, Bing (57204475892); Zuo, Wangmeng (56888903800); Hu, Qinghua (57477997400)","57944031100; 57204475892; 56888903800; 57477997400","Learning a Prototype Discriminator With RBF for Multimodal Image Synthesis","2022","IEEE Transactions on Image Processing","31","","","6664","6678","14","10.1109/TIP.2022.3214336","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140795344&doi=10.1109%2fTIP.2022.3214336&partnerID=40&md5=8192c96189d3171a58a04a3a7db39f34","Multimodal image synthesis has emerged as a viable solution to the modality missing challenge. Most existing approaches employ softmax-based classifiers to provide modal constraints for the generated models. These methods, however, focus on learning to distinguish inter-domain differences while failing to build intra-domain compactness, resulting in inferior synthetic results. To provide sufficient domain-specific constraint, we hereby introduce a novel prototype discriminator for generative adversarial network (PT-GAN) to effectively estimate the missing or noisy modalities. Different from most previous works, we introduce the Radial Basis Function (RBF) network, endowing the discriminator with domain-specific prototypes, to improve the optimization of generative model. Since the prototype learning extracts more discriminative representation of each domain, and emphasizes intra-domain compactness, it reduces the sensitivity of discriminator to pixel changes in generated images. To address this dilemma, we further propose a reconstructive regularization term which connects the discriminator with the generator, thus enhancing its pixel detectability. To this end, the proposed PT-GAN provides not only consistent domain-specific constraints, but also reasonable uncertainty estimation of generated images with the RBF distance. Experimental results show that our method outperforms the state-of-the-art techniques. The source code will be available at: https://github.com/zhiweibi/PT-GAN.  © 1992-2012 IEEE.","Functions; Generative adversarial networks; Image reconstruction; Image segmentation; Pixels; Radial basis function networks; Base function; Domain specific; Images synthesis; Multimodal image synthesis; Multimodal images; Prototype discriminator; Radial base function; Radial basis; Reconstructive regularization; Regularisation; article; learning; radial basis function neural network; synthesis; uncertainty; Discriminators","multimodal image synthesis; Prototype discriminator; radial basis function; reconstructive regularization","Article","Final","","Scopus","2-s2.0-85140795344"
"Zhou Y.; Wu G.; Lin Q.; Yu D.; Wu H.","Zhou, Yijie (57222128806); Wu, Gang (57814095600); Lin, Qiang (57217580995); Yu, Dingguo (34769243400); Wu, Hui (57215632399)","57222128806; 57814095600; 57217580995; 34769243400; 57215632399","Text-based Talking Facial Synthesis for Virtual Host System","2022","Proceedings - 2022 International Conference on Culture-Oriented Science and Technology, CoST 2022","","","","45","48","3","10.1109/CoST57098.2022.00019","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140902021&doi=10.1109%2fCoST57098.2022.00019&partnerID=40&md5=5fd03ed13d80978332b3b1950c941507","With the prevailing of deep learning technology, automatic virtual image synthesis has made huge progress and the popularity of virtual portraits has been growing rapidly. Traditional virtual synthesis system rely on computer graphics method driven by motion capture of a real person, which need labor and equipment costs. In view of this, our paper proposes a virtual host synthesis method based on text driven to generate lip shape and facial animation(include eye movement and head pose) from a signal facial image of a virtual host. More precisely, we use three main modules to construct a virtual host synthesis system: a speech synthesis module based on Tacotron2, a speech to landmark points module to extract mixture landmarks movement to speech, and video generation module based on conditional generative adversarial network to generate video frames and realize time-continuous automatic sport news reporting.  © 2022 IEEE.","Computer graphics; Deep learning; Eye movements; Generative adversarial networks; Image processing; Face generation; Facial synthesis; Images synthesis; Learning technology; Lip synthesis; Module-based; Talking face generation; Virtual host; Virtual images; Virtual portrait; Speech synthesis","lip synthesis; talking face generation; virtual host; virtual portraits","Conference paper","Final","","Scopus","2-s2.0-85140902021"
"Mishra R.; Sharma K.; Bhavsar A.","Mishra, Rahul (57221017913); Sharma, Krishan (57208877021); Bhavsar, Arnav (25521421900)","57221017913; 57208877021; 25521421900","Reconstruction of Visual Stimulus from the EEG Recordings via Generative Adversarial Network","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12032","","120321W","","","","10.1117/12.2613297","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131936969&doi=10.1117%2f12.2613297&partnerID=40&md5=165e30c2f157c61192db0c3c7934f2d9","In this work, we address a contemporary research problem in the domain of perceptual brain decoding, involving image synthesis from EEG signals in an adversarial deep learning framework. The specific task involves reconstructing images of different object classes, using the EEG recordings acquired when subjects are shown the images of those objects. For this work, we use an EEG encoder for generating EEG encodings. These EEG encodings act as an input to the generator of the GAN network. In addition to the adversarial loss, we also use perceptual loss for generating decent quality images. Through experiments, we demonstrate that the proposed network is generating better quality images than the available state-of-the-art methods. © 2022 SPIE","Deep learning; Encoding (symbols); Image processing; Signal encoding; Brain decoding; CNN; EEG recording; EEG signals; Images synthesis; Learning frameworks; Perceptual loss; Quality image; Research problems; Visual stimulus; Generative adversarial networks","CNN; EEG; Generative Adversarial Network; Perceptual Loss","Conference paper","Final","","Scopus","2-s2.0-85131936969"
"Zheng Z.; Liu J.; Zheng N.","Zheng, Zhentan (57219478366); Liu, Jianyi (55705850000); Zheng, Nanning (56725835000)","57219478366; 55705850000; 56725835000","P<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math></inline-formula>-GAN: Efficient Stroke Style Transfer using Single Style Image","2022","IEEE Transactions on Multimedia","","","","1","13","12","10.1109/TMM.2022.3203220","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137907001&doi=10.1109%2fTMM.2022.3203220&partnerID=40&md5=aa00ac6f461665267ffe39de5ac19259","Style transfer is a useful image synthesis technique that can re-render given image into another artistic style while preserving its content information. Generative Adversarial Network (GAN) is a widely adopted framework toward this task for its better representation ability on local style patterns than the traditional Gram-matrix based methods. However, most previous methods rely on sufficient amount of pre-collected style images to train the model. In this paper, a novel Patch Permutation GAN (P<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math></inline-formula>-GAN) network that can efficiently learn the stroke style from a single style image is proposed. We use patch permutation to generate multiple training samples from the given style image. A patch discriminator that can simultaneously process patch-wise images and natural images seamlessly is designed. We also propose a local texture descriptor based criterion to quantitatively evaluate the style transfer quality. Experimental results showed that our method can produce finer quality re-renderings from single style image with improved computational efficiency compared with many state-of-the-arts methods. IEEE","Computational efficiency; Image enhancement; Interactive computer systems; Job analysis; Learning systems; Quality control; Real time systems; Rendering (computer graphics); Features extraction; Generator; Images synthesis; Network efficient; Patch permutation; Real - Time system; Stroke style; Style transfer; Synthesis techniques; Task analysis; Generative adversarial networks","Feature extraction; Generative Adversarial Network; Generative adversarial networks; Generators; Learning systems; Patch Permutation; Real-time systems; Stroke Style; Style Transfer; Task analysis; Training","Article","Article in press","","Scopus","2-s2.0-85137907001"
"Tang H.; Torr P.H.; Sebe N.","Tang, Hao (57208238003); Torr, Philip H.S. (56821543600); Sebe, Nicu (57204924633)","57208238003; 56821543600; 57204924633","Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","16","15","10.1109/TPAMI.2022.3212915","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139849529&doi=10.1109%2fTPAMI.2022.3212915&partnerID=40&md5=f210951b5c466749aeb8e459e7827af3","We propose a novel model named Multi-Channel Attention Selection Generative Adversarial Network (SelectionGAN) for guided image-to-image translation, where we translate an input image into another while respecting an external semantic guidance. The proposed SelectionGAN explicitly utilizes the semantic guidance information and consists of two stages. In the first stage, the input image and the conditional semantic guidance are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using the proposed multi-scale spatial pooling &amp; channel selection module and the multi-channel attention selection module. Moreover, uncertainty maps automatically learned from attention maps are used to guide the pixel loss for better network optimization. Exhaustive experiments on four challenging guided image-to-image translation tasks (face, hand, body, and street view) demonstrate that our SelectionGAN is able to generate significantly better results than the state-of-the-art methods. Meanwhile, the proposed framework and modules are unified solutions and can be applied to solve other generation tasks such as semantic image synthesis. The code is available at&#x00A0;<uri>https://github.com/Ha0Tang/SelectionGAN</uri>. IEEE","Computer vision; Generative adversarial networks; Job analysis; Semantic Segmentation; Uncertainty analysis; Attention selection; Cascade generation; Deep attention selection; GAN; Guided image-to-image translation; Guided images; Image translation; Images segmentations; Images synthesis; Skeleton; Task analysis; Uncertainty; Semantics","Cascade generation; deep attention selection; GANs; Generative adversarial networks; guided image-to-image translation; Image segmentation; Image synthesis; Semantics; Skeleton; Task analysis; Uncertainty","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85139849529"
"Liao W.; Hu K.; Yang M.Y.; Rosenhahn B.","Liao, Wentong (56768729400); Hu, Kai (57226860721); Yang, Michael Ying (36015861500); Rosenhahn, Bodo (57203083760)","56768729400; 57226860721; 36015861500; 57203083760","Text to Image Generation with Semantic-Spatial Aware GAN","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18166","18175","9","10.1109/CVPR52688.2022.01765","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139192930&doi=10.1109%2fCVPR52688.2022.01765&partnerID=40&md5=0e32927236c1958e2b174bbc0413a75a","Text-to-image synthesis (T2I) aims to generate photorealistic images which are semantically consistent with the text descriptions. Existing methods are usually built upon conditional generative adversarial networks (GANs) and initialize an image from noise with sentence embedding, and then refine the features with fine-grained word embedding iteratively. A close inspection of their generated images reveals a major limitation: even though the generated image holistically matches the description, individual image regions or parts of somethings are often not recognizable or consistent with words in the sentence, e.g. 'a white crown'. To address this problem, we propose a novel framework Semantic-Spatial Aware GAN for synthesizing images from input text. Concretely, we introduce a simple and effective Semantic-Spatial Aware block, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a semantic mask in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code available at https://github.com/wtliao/text2image. © 2022 IEEE.","Computer vision; Embeddings; Generative adversarial networks; Image fusion; Iterative methods; Embeddings; Image and video synthesis and generation; Image generations; Images synthesis; Learn+; Photorealistic images; Text images; Video generation; Video synthesis; Vision + language; Semantics","Image and video synthesis and generation; Vision + language","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85139192930"
"Ding L.; Ding S.-F.; Zhang J.; Zhang Z.-C.","Ding, Ling (57218532895); Ding, Shi-Fei (24314525600); Zhang, Jian (56637434200); Zhang, Zi-Chen (57195331210)","57218532895; 24314525600; 56637434200; 57195331210","Single Image Super-Resolution Reconstruction Based on VGG Energy Loss; [使用VGG能量损失的单图像超分辨率重建]","2021","Ruan Jian Xue Bao/Journal of Software","32","11","","3659","3668","9","10.13328/j.cnki.jos.006053","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118928719&doi=10.13328%2fj.cnki.jos.006053&partnerID=40&md5=c2347a22128981c8a81e7e12b27ffe54","Single image super-resolution (SR) is an important task in image synthesis. Based on neural nets, the loss function in the SR task commonly contains a content-based reconstruction loss and a generative adversarial network (GAN) based regularization loss. However, due to the instability of GAN training, the generated discriminative signal of a high-resolution image from the GAN loss is not stable in the SRGAN model. In order to alleviate this problem, based on the commonly used VGG reconstruction loss, this study designs a stable energy-based regularization loss, which is called VGG energy loss. The proposed VGG energy loss in this study uses the VGG encoder in the reconstruction loss as an encoder, and designs the corresponding decoder to build a VGG-U-Net auto encoder: VGG-UAE; by using the VGG-UAE as the energy function, which can provide gradients for the generator, the generated high-resolution samples track the energy flow of real data. Experiments verify that a generative model using the proposed VGG energy loss can generate more effective high-resolution images. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","Energy dissipation; Image reconstruction; Optical resolving power; Signal encoding; Auto encoders; Energy functions; High-resolution images; Image super resolutions; Images synthesis; Loss functions; Regularisation; Single images; Single-image super-resolution reconstruction; Superresolution; Generative adversarial networks","Auto encoder; Energy function; Generative adversarial net; Single image super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85118928719"
"Ruan S.","Ruan, Shengrui (57956402000)","57956402000","Anime Characters Generation with Generative Adversarial Networks","2022","2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications, AEECA 2022","","","","1332","1335","3","10.1109/AEECA55500.2022.9918869","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141347633&doi=10.1109%2fAEECA55500.2022.9918869&partnerID=40&md5=a9546b44c7fa65ae5436c29cdc1e84a8","Automatic anime character generation has the potential to motivate professionals to create new characters while also lowering the cost of creating animation. Adversarial generative networks (GANs) have yielded impressive results in the field of image synthesis. There have been some attempts to apply the GAN model to produce anime character facial images. The increasing number of contributions in recent years requires a systematic summary and analysis of new findings to speed up future research. This paper presents a comprehensive analysis of the GAN-based generation of anime characters. This paper introduces the concept of generative adversarial networks at first, including the adversarial idea as well as its algorithms. Then, the author presents GANs' application in anime character generation, including the ideas and new findings in this field. Finally, the author also offers a summary of the open challenges for anime character generation.  © 2022 IEEE.","Deep learning; Anime character generation; Character generation; Comprehensive analysis; Deep learning; Facial images; Images synthesis; Network applications; Network models; Network-based; Speed up; Generative adversarial networks","anime character generation; deep learning; generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85141347633"
"Mirza M.U.; Dalmaz O.; Cukur T.","Mirza, Muhammad Usama (57882279800); Dalmaz, Onat (57226257796); Cukur, Tolga (23034054800)","57882279800; 57226257796; 23034054800","Skip Connections for Medical Image Synthesis with Generative Adversarial Networks; [Üretken Çekişmeli Aǧlar ile Medikal Görüntü Sentezi için Atlamali Baǧlantilar]","2022","2022 30th Signal Processing and Communications Applications Conference, SIU 2022","","","","","","","10.1109/SIU55565.2022.9864939","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138709867&doi=10.1109%2fSIU55565.2022.9864939&partnerID=40&md5=e4a03dfa95c90e42af382500982ff41e","Magnetic Resonance Imaging (MRI) is an imaging technique used to produce detailed anatomical images. Acquiring multiple contrast MRI images requires long scan times forcing the patient to remain still. Scan times can be reduced by synthesising unacquired contrasts from acquired contrasts. In recent years, deep generative adversarial networks have been used to synthesise contrasts using one-to-one mapping. Deeper networks can solve more complex functions, however, their performance can decline due to problems such as overfitting and vanishing gradients. In this study, we propose adding skip connections to generative models to overcome the decline in performance with increasing complexity. This will allow the network to bypass unnecessary parameters in the model. Our results show an increase in performance in one-to-one image synthesis by integrating skip connections. © 2022 IEEE.","Complex networks; Magnetic resonance imaging; Medical imaging; Anatomical images; Forcings; Images synthesis; Magnetic resonance imaging; Medical image synthesis; Multi-contrast magnetic resonance imaging; One-to-one mappings; Performance; Scan time; Skip connection; Generative adversarial networks","Generative adversarial network; Magnetic resonance imaging (MRI); Medical image synthesis; Multi-contrast MRI; Skip connections","Conference paper","Final","","Scopus","2-s2.0-85138709867"
"Hu S.; Lei B.; Wang S.; Wang Y.; Feng Z.; Shen Y.","Hu, Shengye (57217056249); Lei, Baiying (26422280400); Wang, Shuqiang (53872228000); Wang, Yong (56560340400); Feng, Zhiguang (35770539900); Shen, Yanyan (25121829200)","57217056249; 26422280400; 53872228000; 56560340400; 35770539900; 25121829200","Bidirectional Mapping Generative Adversarial Networks for Brain MR to PET Synthesis","2022","IEEE Transactions on Medical Imaging","41","1","","145","157","12","10.1109/TMI.2021.3107013","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113861594&doi=10.1109%2fTMI.2021.3107013&partnerID=40&md5=014b9087441122391100e7d6d64e1a04","Fusing multi-modality medical images, such as magnetic resonance (MR) imaging and positron emission tomography (PET), can provide various anatomical and functional information about the human body. However, PET data is not always available for several reasons, such as high cost, radiation hazard, and other limitations. This paper proposes a 3D end-to-end synthesis network called Bidirectional Mapping Generative Adversarial Networks (BMGAN). Image contexts and latent vectors are effectively used for brain MR-to-PET synthesis. Specifically, a bidirectional mapping mechanism is designed to embed the semantic information of PET images into the high-dimensional latent space. Moreover, the 3D Dense-UNet generator architecture and the hybrid loss functions are further constructed to improve the visual quality of cross-modality synthetic images. The most appealing part is that the proposed method can synthesize perceptually realistic PET images while preserving the diverse brain structures of different subjects. Experimental results demonstrate that the performance of the proposed method outperforms other competitive methods in terms of quantitative measures, qualitative displays, and evaluation metrics for classification.  © 1982-2012 IEEE.","Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Positron-Emission Tomography; Semantics; Image enhancement; Magnetic resonance; Mapping; Positron emission tomography; Radiation hazards; Semantics; Adversarial networks; Bidirectional mapping; Evaluation metrics; Functional information; Positron emission tomography (PET); Quantitative measures; Semantic information; Visual qualities; adult; article; brain; female; human; human experiment; loss of function mutation; male; nuclear magnetic resonance; positron emission tomography; quantitative analysis; synthesis; brain; diagnostic imaging; image processing; nuclear magnetic resonance imaging; semantics; Brain mapping","bidirectional mapping mechanism; generative adversarial network; Medical image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85113861594"
"Cha D.; Kim D.","Cha, Dongmin (57658029200); Kim, Daijin (24597347100)","57658029200; 24597347100","DAM-GAN: IMAGE INPAINTING USING DYNAMIC ATTENTION MAP BASED ON FAKE TEXTURE DETECTION","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","4883","4887","4","10.1109/ICASSP43922.2022.9746659","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131248712&doi=10.1109%2fICASSP43922.2022.9746659&partnerID=40&md5=55f980e24c32cb096dc976324993d09c","Deep neural advancements have recently brought remarkable image synthesis performance to the field of image inpainting. The adaptation of generative adversarial networks (GAN) in particular has accelerated significant progress in high-quality image reconstruction. However, although many notable GAN-based networks have been proposed for image inpainting, still pixel artifacts or color inconsistency occur in synthesized images during the generation process, which are usually called fake textures. To reduce pixel inconsistency disorder resulted from fake textures, we introduce a GAN-based model using dynamic attention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake texture and products dynamic attention maps to diminish pixel inconsistency from the feature maps in the generator. Evaluation results on CelebA-HQ and Places2 datasets with other image inpainting approaches show the superiority of our network. © 2022 IEEE","Computer vision; Fake detection; Image reconstruction; Image texture; Pixels; Textures; High quality images; Image completion; Image in-painting; Image Inpainting; Images reconstruction; Images synthesis; Network-based; Performance; Synthesized images; Texture detection; Generative adversarial networks","CNN; Computer Vision; GAN; Image Completion; Image In-painting","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131248712"
"Nakamura K.; Nakahara H.","Nakamura, Kennichi (57781806600); Nakahara, Hiroki (10040183800)","57781806600; 10040183800","Optimizations of Ternary Generative Adversarial Networks","2022","Proceedings of The International Symposium on Multiple-Valued Logic","2022-May","","","158","163","5","10.1109/ISMVL52857.2022.00031","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133494818&doi=10.1109%2fISMVL52857.2022.00031&partnerID=40&md5=ad3a96e1773fef5b71da0d6609767d06","Generative adversarial networks (GANs), which can generate and transform data, have been attracting attention. However, the model must be lightweight and fast when applied in the field. In terms of model weight reduction, B-DCGAN, which restricts the value of the weights to {-1, +1}, has already been proposed. We propose a ternary GAN using the ternary representation {-1, 0, +1} and an alpha-layer which makes the learning between generator and discriminator more competitive. It succeeded in improving the quality of the output image considerably while maintaining the almost same memory usage as that of B-DCGAN.  © 2022 IEEE.","Image enhancement; Learning systems; Alpha layer; Images synthesis; Layer; Machine-learning; Model weights; Optimisations; Quantisation; Ternary; Tinyml; Weight reduction; Generative adversarial networks","alpha layer; GAN; generative adversarial networks; image synthesis; layer; machine learning; quantization; Ternary; TinyML","Conference paper","Final","","Scopus","2-s2.0-85133494818"
"Yang Z.; Wang T.; Bu L.; Ouyang J.","Yang, Zhikai (57206894662); Wang, Teng (57189235790); Bu, Leping (8266284100); Ouyang, Jineng (57206905148)","57206894662; 57189235790; 8266284100; 57206905148","Training with Augmented Data: GAN-based Flame-Burning Image Synthesis for Fire Segmentation in Warehouse","2022","Fire Technology","58","1","","183","215","32","10.1007/s10694-021-01117-x","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107499588&doi=10.1007%2fs10694-021-01117-x&partnerID=40&md5=39fc88fe3b58ec8dcc481591987532c7","The training of video fire detection models based on deep learning relies on a large number of positive and negative samples, namely, fire video and scenario video with other disturbances similar to fire. Due to the prohibition of ignition in lots of indoor occasions, the fire video samples in the scene are insufficient. In this paper, a method based on generative adversarial network is proposed to generate flame images which are then migrated into specified scenes, thus increasing fire video samples in those restricted situations. Flame kernel is pre-implanted into the specified scene to keep its characteristics intact. The flame and scene are blended together by adding styling information such as blurry edge and ground reflection. This method overcomes background distortion which is caused by existing multimodal image translation on as a result of information loss and is able to guarantee the diversity of flames in specified scenes and produce perceptually realistic results. Compared with other multimodal image-to-image translation schemes, the FID and LPIPS values of images generated by our method are the highest, reaches 118.4 and 0.1322 respectively. In addition, Unet and the SA-Unet, in which a self-attention mechanism is involved, are used as fire segmenting networks to evaluate the enhancement of the augmented data on improving the accuracy of segmented network. Their F1-scores reaches 0.8905 and 0.9082 respectively after Unet and SA-Unet are trained with GAN-based augmented dataset generated by our model. The F1-scores are second only to 0.9259 and 0.9291 which are obtained when Unet and SA-Unet are trained with real picture serving as augmented dataset. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Data warehouses; Deep learning; Fires; Adversarial networks; Attention mechanisms; Background distortions; Fire segmentations; Ground reflection; Image translation; Information loss; Multi-modal image; Image segmentation","Fire image synthesis; Generative adversarial network; Image translation; Style transfer","Article","Final","","Scopus","2-s2.0-85107499588"
"Chen H.; Liu J.; Chen W.; Liu S.; Zhao Y.","Chen, Haiwei (57194787673); Liu, Jiayi (57608111800); Chen, Weikai (55570458100); Liu, Shichen (57195956295); Zhao, Yajie (56461707100)","57194787673; 57608111800; 55570458100; 57195956295; 56461707100","Exemplar-based Pattern Synthesis with Implicit Periodic Field Network","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","3698","3707","9","10.1109/CVPR52688.2022.00369","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137200086&doi=10.1109%2fCVPR52688.2022.00369&partnerID=40&md5=bb4ac1daa6e738410ba4ae28d7ae1900","Synthesis of ergodic, stationary visual patterns is widely applicable in texturing, shape modeling, and digital content creation. The wide applicability of this technique thus requires the pattern synthesis approaches to be scalable, diverse, and authentic. In this paper, we propose an exemplar-based visual pattern synthesis framework that aims to model the inner statistics of visual patterns and generate new, versatile patterns that meet the aforementioned requirements. To this end, we propose an implicit network based on generative adversarial network (GAN) and periodic encoding, thus calling our network the Implicit Periodic Field Network (IPFN). The design of IPFN ensures scalability: the implicit formulation directly maps the input coordinates to features, which enables synthesis of arbitrary size and is computationally efficient for 3D shape synthesis. Learning with a periodic encoding scheme encourages diversity: the network is constrained to model the inner statistics of the exemplar based on spatial latent codes in a periodic field. Coupled with continuously designed GAN training procedures, IPFN is shown to synthesize tileable patterns with smooth transitions and local variations. Last but not least, thanks to both the adversarial training technique and the encoded Fourier features, IPFN learns high-frequency functions that produce authentic, high-quality results. To validate our approach, we present novel experimental results on various applications in 2D texture synthesis and 3D shape synthesis. © 2022 IEEE.","Computer vision; Deep learning; Encoding (symbols); Signal encoding; Textures; Deep learning architecture and technique; Exemplar-based; Image and video synthesis and generation; Images synthesis; Learning architectures; Learning techniques; Periodic fields; Representation learning; Video generation; Video synthesis; Generative adversarial networks","Deep learning architectures and techniques; Image and video synthesis and generation; Representation learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85137200086"
"Sun M.; Wang J.; Liu J.; Li J.; Chen T.; Sun Z.","Sun, Muyi (57200658411); Wang, Jian (57224988981); Liu, Jian (57560272400); Li, Jianshu (57560051800); Chen, Tao (57560051900); Sun, Zhenan (57218404238)","57200658411; 57224988981; 57560272400; 57560051800; 57560051900; 57218404238","A Unified Framework for Biphasic Facial Age Translation with Noisy-Semantic Guided Generative Adversarial Networks","2022","IEEE Transactions on Information Forensics and Security","17","","","1513","1527","14","10.1109/TIFS.2022.3164187","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127473529&doi=10.1109%2fTIFS.2022.3164187&partnerID=40&md5=aa1afc7b998b14e8b81f2332ec1f58f9","Biphasic facial age translation aims at predicting the appearance of the input face at any age. Facial age translation has received considerable research attention in the last decade due to its practical value in cross-age face recognition and various entertainment applications. However, most existing methods model age changes between holistic images, regardless of the human face structure and the age-changing patterns of individual facial components. Consequently, the lack of semantic supervision will cause infidelity of generated faces in detail. To this end, we propose a unified framework for biphasic facial age translation with noisy-semantic guided generative adversarial networks. Structurally, we project the class-aware noisy semantic layouts to 'soft' latent maps for the following injection operation on the individual facial parts. In particular, we introduce two sub-networks, ProjectionNet and ConstraintNet. ProjectionNet introduces the low-level structural semantic information with noise map and produces 'soft' latent maps. ConstraintNet disentangles the high-level spatial features to constrain the 'soft' latent maps, which endows more age-related context into the 'soft' latent maps. Specifically, attention mechanism is employed in ConstraintNet for feature disentanglement. Meanwhile, in order to mine the strongest mapping ability of the network, we embed two types of learning strategies in the training procedure, supervised self-driven generation and unsupervised condition-driven cycle-consistent generation. As a result, extensive experiments conducted on MORPH and CACD datasets demonstrate the prominent ability of our proposed method which achieves state-of-the-art performance.  © 2005-2012 IEEE.","Generative adversarial networks; Semantic Web; Semantics; Attention mechanisms; Biphasic facial age translation; Entertainment application; Face; Feature disentanglement; Images synthesis; Layout; Noise measurements; Noisy semantic injection; Unified framework; Face recognition","attention mechanism; Biphasic facial age translation; feature disentanglement; generative adversarial network; noisy semantic injection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127473529"
"Turnes J.N.; Castro J.D.B.; Torres D.L.; Vega P.J.S.; Feitosa R.Q.; Happ P.N.","Turnes, Javier Noa (57216587254); Castro, Jose David Bermudez (57221592279); Torres, Daliana Lobo (57214144329); Vega, Pedro Juan Soto (57216790448); Feitosa, Raul Queiroz (6602453684); Happ, Patrick N. (55768214000)","57216587254; 57221592279; 57214144329; 57216790448; 6602453684; 55768214000","Atrous cGAN for SAR to Optical Image Translation","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3031199","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121785021&doi=10.1109%2fLGRS.2020.3031199&partnerID=40&md5=69801546b67a25708178ad951474a818","Conditional (cGAN)-based methods proposed so far for synthetic aperture radar (SAR)-to-optical image synthesis tend to produce noisy and unsharp optical outcomes. In this work, we propose the atrous-cGAN, a novel cGAN architecture that improves the SAR-to-optical image translation. The proposed generator and discriminator networks rely on atrous convolutions and incorporate an atrous spatial pyramid pooling (ASPP) module to enhance fine details in the generated optical image by exploiting spatial context at multiple scales. This letter reports experiments carried out to assess the performance of atrous-cGAN for the synthesis of Landsat-8 images from Sentinel-1A data based on three public data sets. The experimental analysis indicated that the atrous-cGAN consistently outperformed the classical pix2pix counterpart in terms of visual quality, similar to the true optical image, and as a feature learning tool for semantic segmentation.  © 2004-2012 IEEE.","Geometrical optics; Image enhancement; Radar imaging; Semantic Segmentation; Semantics; Synthetic aperture radar; Atrous spatial pyramid pooling; Image translation; Images synthesis; Multiple scale; Optical image; Optical-; Performance; Spatial context; Spatial pyramids; Synthetic aperture radar -optical synthesis; algorithm; satellite data; synthetic aperture radar; time series analysis; Generative adversarial networks","Atrous spatial pyramid pooling (ASPP); Generative adversarial networks; Synthetic aperture radar (SAR)-optical synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85121785021"
"Wang Y.; Wang Q.; Zhang D.","Wang, Yuehui (57204708091); Wang, Qing (57192237380); Zhang, Dongyu (57216591397)","57204708091; 57192237380; 57216591397","Generalizing Factorization of Gans by Characterizing Convolutional Layers","2022","Proceedings - IEEE International Conference on Multimedia and Expo","2022-July","","","","","","10.1109/ICME52920.2022.9859692","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137688743&doi=10.1109%2fICME52920.2022.9859692&partnerID=40&md5=9fafca1fbb0265a5fc7bf6672e57375c","Existing unsupervised disentanglement methods in latent space of the Generative Adversarial Networks (GANs) rely on the analysis and decomposition of pre-trained weight matrix. However, they only consider the weight matrix of the fully connected layers, ignoring the convolutional layers which are indispensable for image processing in modern generative models. This results in the learned latent semantics lack inter-pretability, which is unacceptable for image editing tasks. In this paper, we propose a more generalized closed-form factor-ization of latent semantics in GANs, which takes the convolutionallayers into consideration when searching for the under-lying variation factors. Our method can be applied to a wide range of deep generators with just a few lines of code. Exten-sive experiments on multiple GAN models trained on various datasets show that our approach is capable of not only finding semantically meaningful dimensions, but also maintaining the consistency and interpretability of image content.  © 2022 IEEE.","Computer vision; Convolution; Deep learning; Semantic Web; Semantics; Closed form; Deep learning; Generative model; Image editing; Images processing; Images synthesis; Latent semantic interpretation; Latent semantics; Semantic interpretation; Weight matrices; Generative adversarial networks","Deep Learning; Generative Adversarial Network; Image Synthesis; Latent Semantic Interpretation","Conference paper","Final","","Scopus","2-s2.0-85137688743"
"Hong F.-T.; Zhang L.; Shen L.; Xu D.","Hong, Fa-Ting (57555718300); Zhang, Longhao (57554093000); Shen, Li (57192385993); Xu, Dan (57204977167)","57555718300; 57554093000; 57192385993; 57204977167","Depth-Aware Generative Adversarial Network for Talking Head Video Generation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","3387","3396","9","10.1109/CVPR52688.2022.00339","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137784557&doi=10.1109%2fCVPR52688.2022.00339&partnerID=40&md5=e1aa3573a2f7b07f1f69d63b55f0729d","Talking head video generation aims to produce a synthetic human face video that contains the identity and pose information respectively from a given source image and a driving video. Existing works for this task heavily rely on 2D representations (e.g. appearance and motion) learned from the input images. However, dense 3D facial geometry (e.g. pixel-wise depth) is extremely important for this task as it is particularly beneficial for us to essentially generate accurate 3D face structures and distinguish noisy information from the possibly cluttered background. Nevertheless, dense 3D geometry annotations are prohibitively costly for videos and are typically not available for this video generation task. In this paper, we introduce a self-supervised face-depth learning method to automatically recover dense 3D facial geometry (i.e. depth) from the face videos without the requirement of any expensive 3D annotation data. Based on the learned dense depth maps, we further propose to leverage them to estimate sparse facial keypoints that capture the critical movement of the human head. In a more dense way, the depth is also utilized to learn 3D-aware cross-modal (i.e. appearance and depth) attention to guide the generation of motion fields for warping source image representations. All these contributions compose a novel depth-aware generative adversarial network (DaGAN) for talking head generation. Extensive experiments conducted demonstrate that our proposed method can generate highly realistic faces, and achieve significant results on the unseen human faces. 11https://github.com/harlanhong/CVPR2022-DaGAN © 2022 IEEE.","Computer vision; Geometry; Learning systems; Facial geometry; Human faces; Identity information; Image and video synthesis and generation; Images synthesis; Pose information; Source images; Talking heads; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85137784557"
"Aberman K.; He J.; Gandelsman Y.; Mosseri I.; Jacobs D.E.; Kohlhoff K.; Pritch Y.; Rubinstein M.","Aberman, Kfir (57190192015); He, Junfeng (57961969200); Gandelsman, Yossi (57214469458); Mosseri, Inbar (55811234800); Jacobs, David E. (25926482600); Kohlhoff, Kai (14830604100); Pritch, Yael (6507185318); Rubinstein, Michael (57814628500)","57190192015; 57961969200; 57214469458; 55811234800; 25926482600; 14830604100; 6507185318; 57814628500","Deep Saliency Prior for Reducing Visual Distraction","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","19819","19828","9","10.1109/CVPR52688.2022.01923","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141752446&doi=10.1109%2fCVPR52688.2022.01923&partnerID=40&md5=cb63c60c1add4d4146cdb60b91f61b88","Using only a model that was trained to predict where people look at images, and no additional training data, we can produce a range of powerful editing effects for reducing distraction in images. Given an image and a mask specifying the region to edit, we backpropagate through a state-of-the-art saliency model to parameterize a differentiable editing operator, such that the saliency within the masked region is reduced. We demonstrate several operators, including: a recoloring operator, which learns to apply a color transform that camouflages and blends distractors into their surroundings; a warping operator, which warps less salient image regions to cover distractors, gradually collapsing objects into themselves and effectively removing them (an effect akin to inpainting); a GAN operator, which uses a semantic prior to fully replace image regions with plausible, less salient alternatives. The resulting effects are consistent with cognitive research on the human visual system (e.g., since color mismatch is salient, the recoloring operator learns to harmonize objects' colors with their surrounding to reduce their saliency). And importantly, all effects are achieved under a zero-shot learning scenario, solely through the guidance of the pretrained saliency model, with no supervised data of the effects. We present results on a variety of natural images and conduct a perceptual study to evaluate and validate the changes in viewers' eye-gaze between the original images and our edited results. Project Webpage: https://deep-saliency-prior.github.io/ © 2022 IEEE.","Color; Color photography; Computer vision; Generative adversarial networks; Semantics; Computational photography; Image and video synthesis and generation; Image regions; Images synthesis; Learn+; Recoloring operators; Saliency modeling; Video generation; Video synthesis; Visual distractions; Zero-shot learning","Computational photography; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141752446"
"Sagong M.; Yeo Y.; Shin Y.; Ko S.","Sagong, Min-Cheol (57207117093); Yeo, Yoon-Jae (57202576707); Shin, Yong-Goo (56712241900); Ko, Sung-Jea (7403326117)","57207117093; 57202576707; 56712241900; 7403326117","Conditional Convolution Projecting Latent Vectors on Condition-Specific Space","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","1","8","7","10.1109/TNNLS.2022.3172512","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130498692&doi=10.1109%2fTNNLS.2022.3172512&partnerID=40&md5=c06612a0d74e8b8b2e01daaf5a8abf3c","Despite rapid advancements over the past several years, the conditional generative adversarial networks (cGANs) are still far from being perfect. Although one of the major concerns of the cGANs is how to provide the conditional information to the generator, there are not only no ways considered as the optimal solution but also a lack of related research. This brief presents a novel convolution layer, called the conditional convolution (cConv) layer, which incorporates the conditional information into the generator of the generative adversarial networks (GANs). Unlike the most general framework of the cGANs using the conditional batch normalization (cBN) that transforms the normalized feature maps after convolution, the proposed method directly produces conditional features by adjusting the convolutional kernels depending on the conditions. More specifically, in each cConv layer, the weights are conditioned in a simple but effective way through filter-wise scaling and channel-wise shifting operations. In contrast to the conventional methods, the proposed method with a single generator can effectively handle condition-specific characteristics. The experimental results on CIFAR, LSUN, and ImageNet datasets show that the generator with the proposed cConv layer achieves a higher quality of conditional image generation than that with the standard convolution layer. IEEE","Convolution; Deep learning; Vector spaces; Condition; Conditional image generation; Deep learning; Generative adversarial network .; Generator; Image generations; Images synthesis; Latent vectors; Normalisation; Optimal solutions; Generative adversarial networks","Conditional image generation; Convolution; deep learning; Generative adversarial networks; generative adversarial networks (GANs).; Generators; Image synthesis; Learning systems; Standards; Visualization","Article","Article in press","","Scopus","2-s2.0-85130498692"
"Yang X.; Hu J.","Yang, Xiaoxi (57979891900); Hu, Jiaxi (57979148400)","57979891900; 57979148400","Deep Neural Networks for Chinese Traditional Landscape Painting Creation","2022","Proceedings of SPIE - The International Society for Optical Engineering","12348","","123483T","","","","10.1117/12.2641585","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142471335&doi=10.1117%2f12.2641585&partnerID=40&md5=74cd1967c7fb8374890358ae8dfc11c2","Deep learning techniques have been popularly applied for artistic tasks such as turning photographs into paintings or creating paintings in the style of modern art. However, East Asian arts are largely ignored. In this paper, we aim to apply deep learning models to create Chinese traditional landscape paintings. We achieve the goal through two deep learning techniques: image style transfer and image synthesis. We apply the Visual Geometry Group (VGG) Network to do the style transfer, which is trained on a pair of a content image and a style image, and the goal is to output an image that renders the target content with the desired style. For the image synthesis, we apply the Deep Convolutional Generative Adversarial Network (DCGAN), which requires a large set of painting images to produce as realistic as possible non-exist paintings that mimic the training dataset. © 2022 SPIE. All rights reserved.","Arts computing; Deep neural networks; Image processing; Large dataset; Learning algorithms; Learning systems; Painting; Deep convolutional generative adversarial network; GAN; Group networks; Images synthesis; Learning models; Learning techniques; Style transfer; Training dataset; Generative adversarial networks","DCGAN; GAN; image synthesis; style transfer","Conference paper","Final","","Scopus","2-s2.0-85142471335"
"Mu J.; De Mello S.; Yu Z.; Vasconcelos N.; Wang X.; Kautz J.; Liu S.","Mu, Jiteng (57191578692); De Mello, Shalini (57201314496); Yu, Zhiding (27468151500); Vasconcelos, Nuno (7004438494); Wang, Xiaolong (57141185000); Kautz, Jan (7006458237); Liu, Sifei (55923631700)","57191578692; 57201314496; 27468151500; 7004438494; 57141185000; 7006458237; 55923631700","CoordGAN: Self-Supervised Dense Correspondences Emerge from GANs","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","10001","10010","9","10.1109/CVPR52688.2022.00977","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141756494&doi=10.1109%2fCVPR52688.2022.00977&partnerID=40&md5=e7eef48baa437449b4684469ed01cb5c","Recent advances show that Generative Adversarial Networks (GANs) can synthesize images with smooth variations along semantically meaningful latent directions, such as pose, expression, layout, etc. While this indicates that GANs implicitly learn pixel-level correspondences across images, few studies explored how to extract them explicitly. In this work, we introduce Coordinate GAN (CoordGAN), a structure-texture disentangled GAN that learns a dense correspondence map for each generated image. We represent the correspondence maps of different images as warped coordinate frames transformed from a canonical coordinate frame, i.e., the correspondence map, which describes the structure (e.g., the shape of a face), is controlled via a transformation. Hence, finding correspondences boils down to locating the same coordinate in different correspondence maps. In CoordGAN, we sample a transformation to represent the structure of a synthesized instance, while an independent texture branch is responsible for rendering appearance details orthogonal to the structure. Our approach can also extract dense correspondence maps for real images by adding an encoder on top of the generator. We quantitatively demonstrate the quality of the learned dense correspondences through segmentation mask transfer on multiple datasets. We also show that the proposed generator achieves better structure and texture disentanglement compared to existing approaches. Project page: https://jitengmu.github.io/CoordGAN/ © 2022 IEEE.","Computer vision; Textures; Canonical coordinates; Dense correspondences; Frames transformed; Images synthesis; Learn+; Pixel level; Self-& semi-& meta- image and video synthesis and generation; Structure/texture; Video generation; Video synthesis; Generative adversarial networks","Self-& semi-& meta- Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141756494"
"Zhou Y.; Wang B.; He X.; Cui S.; Shao L.","Zhou, Yi (57195420298); Wang, Boyang (57211998116); He, Xiaodong (37085932700); Cui, Shanshan (57211998025); Shao, Ling (55643855000)","57195420298; 57211998116; 37085932700; 57211998025; 55643855000","DR-GAN: Conditional Generative Adversarial Network for Fine-Grained Lesion Synthesis on Diabetic Retinopathy Images","2022","IEEE Journal of Biomedical and Health Informatics","26","1","","56","66","10","10.1109/JBHI.2020.3045475","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098774541&doi=10.1109%2fJBHI.2020.3045475&partnerID=40&md5=99fc4b301159bf85acf26b5cc97e1ea4","Diabetic retinopathy (DR) is a complication of diabetes that severely affects eyes. It can be graded into five levels of severity according to international protocol. However, optimizing a grading model to have strong generalizability requires a large amount of balanced training data, which is difficult to collect, particularly for the high severity levels. Typical data augmentation methods, including random flipping and rotation, cannot generate data with high diversity. In this paper, we propose a diabetic retinopathy generative adversarial network (DR-GAN) to synthesize high-resolution fundus images which can be manipulated with arbitrary grading and lesion information. Thus, large-scale generated data can be used for more meaningful augmentation to train a DR grading and lesion segmentation model. The proposed retina generator is conditioned on the structural and lesion masks, as well as adaptive grading vectors sampled from the latent grading space, which can be adopted to control the synthesized grading severity. Moreover, a multi-scale spatial and channel attention module is devised to improve the generation ability to synthesize small details. Multi-scale discriminators are designed to operate from large to small receptive fields, and joint adversarial losses are adopted to optimize the whole network in an end-to-end manner. With extensive experiments evaluated on the EyePACS dataset connected to Kaggle, as well as the FGADR dataset, we validate the effectiveness of our method, which can both synthesize highly realistic ($1280 \times 1280$) controllable fundus images and contribute to the DR grading task.  © 2013 IEEE.","Diabetes Mellitus; Diabetic Retinopathy; Fundus Oculi; Humans; Image Processing, Computer-Assisted; Retina; Eye protection; Image processing; Vector spaces; Adversarial networks; Data augmentation; Diabetic retinopathy; High resolution; Lesion segmentations; Lesion synthesis; Receptive fields; Training data; accuracy; Article; artificial neural network; controlled study; diabetic retinopathy; human; image quality; image segmentation; learning algorithm; support vector machine; training; diabetes mellitus; diagnostic imaging; eye fundus; image processing; procedures; retina; Grading","Diabetic retinopathy; generative adversarial networks; image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098774541"
"Mu C.; Zhao H.; Liu P.","Mu, Chen (57764013800); Zhao, Huaici (35319203100); Liu, Pengfei (57201678471)","57764013800; 35319203100; 57201678471","Generative Rendering Network based on U-shape Discriminator","2022","Journal of Physics: Conference Series","2284","1","012005","","","","10.1088/1742-6596/2284/1/012005","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132836640&doi=10.1088%2f1742-6596%2f2284%2f1%2f012005&partnerID=40&md5=897685752e8aac91d437f765247eb95f","This paper introduces a generative rendering network(GRN) based on a U-shape discriminator for novel view image synthesis. Recently, some generative adversarial networks start to explore 3D space and synthesize new images by rendering methods. Compared with 2D-only networks, they have achieved impressive results in synthesizing novel view images. However, they still have a common shortcoming: the discriminator cannot provide more detailed information for training a strong generator. For this problem, we design an U-shape based discriminator to extract both global information and pixel-wise information. This innovative discriminator will feedback more instructive clues to train a better generator. Benefit from this strategy, our synthesis results on ShapeNet v2 ""cars""and ""chairs""and celebA have achieved better performance than comparative methods.  © Published under licence by IOP Publishing Ltd.","Generative adversarial networks; Image processing; Rendering (computer graphics); Three dimensional computer graphics; 3D spaces; Comparative methods; Global informations; Images synthesis; Network-based; Performance; Rendering methods; Shape based; U shape; Discriminators","","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132836640"
"Lin Y.; Han H.; Kevin Zhou S.","Lin, Yang (57670598300); Han, Hu (57713189500); Kevin Zhou, S. (7404165802)","57670598300; 57713189500; 7404165802","Deep Non-Linear Embedding Deformation Network for Cross-Modal Brain MRI Synthesis","2022","Proceedings - International Symposium on Biomedical Imaging","2022-March","","","","","","10.1109/ISBI52829.2022.9761711","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129631185&doi=10.1109%2fISBI52829.2022.9761711&partnerID=40&md5=14ac1cb76afefc249cd0e49d01660a77","Multimodal MRI (e.g. T1, T2, and Flair) can provide rich anatomical and functional information, thereby facilitating clinical diagnosis and treatment. However, multimodal MRI takes a long scan time, easily leading to artifacts or corruption in certain modalities. Therefore, it is of great value to synthesize a new MRI modality from a complete MRI modality to obtain complementary information for clinical diagnosis. Existing GAN-based approaches treat cross-modal MRI synthesis as an end-to-end learning process without explicit consideration of the inherent correlations between different modalities, leading to inaccurate anatomical and lesion structure in the synthesized modality. In this paper, we propose a deep non-linear embedding deformation network (NEDNet) for cross-modal brain MRI synthesis. NEDNet represents each modality as a non-linear embedding based w.r.t. its own atlas, and learns a deformation feature that is assumed to be the same across modalities. The modality-specific atlas and multi-modal shared deformation are jointly used for generating the new MRI modality. Experiments show that our approach can obtain better cross-modality synthesis results than several baseline methods. © 2022 IEEE.","Deep learning; Deformation; Diagnosis; Magnetic resonance imaging; Medical imaging; Network embeddings; Anatomical information; Brain MRI; Clinical diagnosis; Cross-modal; Deep learning; Functional information; Images synthesis; Linear embedding; Multi-modal; Non linear; Generative adversarial networks","Deep Learning; Generative Adversarial Networks; Image Synthesis; Magnetic Resonance Imaging","Conference paper","Final","","Scopus","2-s2.0-85129631185"
"Yan H.; Zhang H.; Liu L.; Zhou D.; Xu X.; Zhang Z.; Yan S.","Yan, Han (57417465700); Zhang, Haijun (57188929355); Liu, Linlin (57202368552); Zhou, Dongliang (57218513019); Xu, Xiaofei (55606413900); Zhang, Zhao (56822575600); Yan, Shuicheng (57688295500)","57417465700; 57188929355; 57202368552; 57218513019; 55606413900; 56822575600; 57688295500","Toward Intelligent Design: An AI-based Fashion Designer Using Generative Adversarial Networks Aided by Sketch and Rendering Generators","2022","IEEE Transactions on Multimedia","","","","","","","10.1109/TMM.2022.3146010","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123782734&doi=10.1109%2fTMM.2022.3146010&partnerID=40&md5=0f6d326c3dd3d5b4522a97b5adfbe791","The traditional fashion industry is heavily dependent on designers whose talent and vision have a significant impact on their innovative designs. Through taking advantage of recent advances in image-to-image translation by generative adversarial networks (GANs), marked improvement in designers efficiency is now possible. Considering both randomness and controllability in the design process, this article presents a novel artificial intelligence (AI)-based framework for fashion design. Under this framework, a sketch-generation module which is based on latent space is firstly introduced for designing various sketches. Secondly, a rendering-generation module is proposed to learn mapping between textures and sketches to complete the task of fashion design. In order to achieve effectiveness in synthesizing semantic-aware textures on sketches, a multi-conditional feature interaction module is developed in the rendering-generation model. Moreover, two different training schemes are introduced to optimize both the sketch-generation module and the rendering-generation module. In order to evaluate the performance of our proposed models, we built a large-scale dataset which consists of 115,584 pairs of fashion item images. Experimental results demonstrate the effectiveness of our proposed method, and indicate that our model can facilitate designers design process by taking full advantage of the controllability of different conditions (e.g., sketch and texture) and the randomness of latent space. IEEE","Design; Generative adversarial networks; Image enhancement; Process control; Random processes; Semantics; Aerospace electronics; Clothing; Design-process; Fashion data; Fashion design; Image translation; Images synthesis; Intelligent designs; Rendering (computer graphic); Solid modelling; Rendering (computer graphics)","Aerospace electronics; Clothing; fashion data; Fashion design; generative adversarial network; Image synthesis; image translation; Process control; Rendering (computer graphics); Solid modeling; Training","Article","Article in press","","Scopus","2-s2.0-85123782734"
"Mashudi N.A.; Ahmad N.; Mohd Noor N.","Mashudi, Nurul Amirah (57204639523); Ahmad, Norulhusna (35316795700); Mohd Noor, Norliza (7003593814)","57204639523; 35316795700; 7003593814","LiWGAN: A Light Method to Improve the Performance of Generative Adversarial Network","2022","IEEE Access","10","","","93155","93167","12","10.1109/ACCESS.2022.3203065","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137599938&doi=10.1109%2fACCESS.2022.3203065&partnerID=40&md5=559efd45e048c7d6a3eefad13326daf8","Generative adversarial networks (GANs) gained tremendous growth due to the potency and efficiency in producing realistic samples. This study proposes a light-weight GAN (LiWGAN) to learn non-image synthesis with minimum computational time for less power computing. Hence, the LiWGAN method enhanced a new skip-layer channel-wise excitation module (SLE) and a self-supervised discriminator design for non-synthesis performance using the facemask dataset. Facemask is one of the preventative strategies pioneered by the current COVID-19 pandemic. LiWGAN manipulates a non-image synthesis of facemasks that could be beneficial for some researchers to identify an individual using lower power devices, occlusion challenges for face recognition, and alleviate the accuracy challenges due to limited datasets. The study evaluates the performance of the processing time in terms of batch sizes and image resolutions using the facemask dataset. The Fréchet inception distance (FID) was also measured on the facemask images to evaluate the quality of the augmented image using LiWGAN. The findings for 3000 generated images showed a nearly similar FID score at 220.43 with significantly less processing time per iteration at 1.03s than StyleGAN at 219.97 FID score. One experiment was conducted using the CelebA dataset to compare with GL-GAN and DRAGAN, proving LiWGAN is appropriate for other datasets. The outcomes found LiWGAN performed better than GL-GAN and DRAGAN at 91.31 FID score with 3.50s processing time per iteration. Therefore, LiWGAN could aim to enhance the FID score to be near zero in the future with less processing time by using different datasets.  © 2013 IEEE.","Deep learning; Discriminators; Face recognition; Image enhancement; Image resolution; Iterative methods; Neural networks; Data augmentation; Deep learning; Generator; Images synthesis; Light weight; Neural-networks; Non-image synthesis; Optimisations; Processing time; Self-supervised discriminator; Generative adversarial networks","data augmentation; deep learning; generative adversarial network; Non-image synthesis; self-supervised discriminator","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137599938"
"Wang T.; Zhang Y.; Fan Y.; Wang J.; Chen Q.","Wang, Tengfei (57281887900); Zhang, Yong (57207968420); Fan, Yanbo (57733439700); Wang, Jue (55932120100); Chen, Qifeng (55365826000)","57281887900; 57207968420; 57733439700; 55932120100; 55365826000","High-Fidelity GAN Inversion for Image Attribute Editing","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11369","11378","9","10.1109/CVPR52688.2022.01109","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136001856&doi=10.1109%2fCVPR52688.2022.01109&partnerID=40&md5=28b384c4e2186754cbd89180c0f6ecc9","We present a novel highfidelity generative adversarial network (GAN) inversion framework that enables attribute editing with image-specific details well-preserved (e.g., background, appearance, and illumination). We first analyze the challenges of highfidelity GAN inversion from the perspective of lossy data compression. With a low bitrate latent code, previous works have difficulties in preserving highfidelity details in reconstructed and edited images. Increasing the size of a latent code can improve the accuracy of GAN inversion but at the cost of inferior editability. To improve image fidelity without compromising editability, we propose a distortion consultation approach that employs a distortion map as a reference for highfidelity reconstruction. In the distortion consultation inversion (DCI), the distortion map is first projected to a high-rate latent map, which then complements the basic low-rate latent code with more details via consultation fusion. To achieve high-fidelity editing, we propose an adaptive distortion alignment (ADA) module with a self-supervised training scheme, which bridges the gap between the edited and inversion images. Extensive experiments in the face and car domains show a clear improvement in both inversion and editing quality. The project page is https://tengfei-wang.github.io/HFGI/. © 2022 IEEE.","Computer vision; Image enhancement; Image reconstruction; Distortion maps; High-fidelity; Image and video synthesis and generation; Image attributes; Images synthesis; Lossy data compression; Low Bit Rate; Network inversion; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85136001856"
"Kim S.W.; Kreis K.; Li D.; Torralba A.; Fidler S.","Kim, Seung Wook (57210865814); Kreis, Karsten (55069147400); Li, Daiqing (57207766063); Torralba, Antonio (7005432728); Fidler, Sanja (12139176800)","57210865814; 55069147400; 57207766063; 7005432728; 12139176800","Polymorphic-GAN: Generating Aligned Samples across Multiple Domains with Learned Morph Maps","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","10620","10630","10","10.1109/CVPR52688.2022.01037","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141749946&doi=10.1109%2fCVPR52688.2022.01037&partnerID=40&md5=4fe205dc18e0b4dda75dca7a460668da","Modern image generative models show remarkable sample quality when trained on a single domain or class of objects. In this work, we introduce a generative adversarial network that can simultaneously generate aligned image samples from multiple related domains. We leverage the fact that a variety of object classes share common attributes, with certain geometric differences. We propose Polymorphic-GAN which learns shared features across all domains and a per-domain morph layer to morph shared features according to each domain. In contrast to previous works, our framework allows simultaneous modelling of images with highly varying geometries, such as images of human faces, painted and artistic faces, as well as multiple different animal faces. We demonstrate that our model produces aligned samples for all domains and show how it can be used for applications such as segmentation transfer and cross-domain image editing, as well as training in low-data regimes. Additionally, we apply our Polymorphic-GAN on image-to-image translation tasks and show that we can greatly surpass previous approaches in cases where the geometric differences between domains are large. © 2022 IEEE.","Computer vision; Geometry; Image segmentation; Aligned samples; Grouping and shape analyse; Image and video synthesis and generation; Images synthesis; Long tail; Segmentation; Shape-analysis; Transfer/low-shot/long-tail learning; Video generation; Video synthesis; Generative adversarial networks","grouping and shape analysis; Image and video synthesis and generation; Segmentation; Transfer/low-shot/long-tail learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141749946"
"Butte S.; Wang H.; Xian M.; Vakanski A.","Butte, Sujata (57202435895); Wang, Haotian (57217028932); Xian, Min (56416053100); Vakanski, Aleksandar (36543296700)","57202435895; 57217028932; 56416053100; 36543296700","Sharp-GAN: Sharpness Loss Regularized GAN for Histopathology Image Synthesis","2022","Proceedings - International Symposium on Biomedical Imaging","2022-March","","","","","","10.1109/ISBI52829.2022.9761534","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129651945&doi=10.1109%2fISBI52829.2022.9761534&partnerID=40&md5=b351773bf5f931bdf89d4ac6a92c34f5","Existing deep learning-based approaches for histopathology image analysis require large annotated training sets to achieve good performance; but annotating histopathology images is slow and resource-intensive. Conditional generative adversarial networks have been applied to generate synthetic histopathology images to alleviate this issue, but current approaches fail to generate clear contours for overlapped and touching nuclei. In this study, We propose a sharpness loss regularized generative adversarial network to synthesize realistic histopathology images. The proposed network uses normalized nucleus distance map rather than the binary mask to encode nuclei contour information. The proposed sharpness loss enhances the contrast of nuclei contour pixels. The proposed method is evaluated using four image quality metrics and segmentation results on two public datasets. Both quantitative and qualitative results demonstrate that the proposed approach can generate realistic histopathology images with clear nuclei contours. © 2022 IEEE.","Deep learning; Image segmentation; Medical imaging; Quality control; 'current; Distance map; GAN; Histopathology image synthesis; Image-analysis; Images synthesis; Learning-based approach; Nucleus segmentation; Performance; Training sets; Generative adversarial networks","GAN; Histopathology image synthesis; Nuclei segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129651945"
"","","","28th International Conference on MultiMedia Modeling, MMM 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13142 LNCS","","","","","1222","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127121118&partnerID=40&md5=8d5c0b257d59d87c75269789cd2eec7d","The proceedings contain 107 papers. The special focus in this conference is on MultiMedia Modeling. The topics include: Non-Uniform Attention Network for Multi-modal Sentiment Analysis; combining Knowledge and Multi-modal Fusion for Meme Classification; bi-attention Modal Separation Network for Multimodal Video Fusion; Melody Generation from Lyrics Using Three Branch Conditional LSTM-GAN; a-Muze-Net: Music Generation by Composing the Harmony Based on the Generated Melody; Speech Intelligibility Enhancement By Non-Parallel Speech Style Conversion Using CWT and iMetricGAN Based CycleGAN; time-Frequency Attention for Speech Emotion Recognition with Squeeze-and-Excitation Blocks; Real-Time FPGA Design for OMP Targeting 8K Image Reconstruction; one-Stage Image Inpainting with Hybrid Attention; MF-GAN: Multi-conditional Fusion Generative Adversarial Network for Text-to-Image Synthesis; fast Single Image Dehazing Using Morphological Reconstruction and Saturation Compensation; arbitrary Style Transfer with Adaptive Channel Network; point Cloud Upsampling via a Coarse-to-Fine Network; JVCSR: Video Compressive Sensing Reconstruction with Joint In-Loop Reference Enhancement and Out-Loop Super-Resolution; SAM: Self Attention Mechanism for Scene Text Recognition Based on Swin Transformer; A Multiple Positives Enhanced NCE Loss for Image-Text Retrieval; multimodal Embedding for Lifelog Retrieval; prediction of Blood Glucose Using Contextual LifeLog Data; fall Detection Using Multimodal Data; an Investigation into Keystroke Dynamics and Heart Rate Variability as Indicators of Stress; PF-VTON: Toward High-Quality Parser-Free Virtual Try-On Network; joint Re-Detection and Re-Identification for Multi-Object Tracking; Multi-scale Cross-Modal Transformer Network for RGB-D Object Detection; A Complementary Fusion Strategy for RGB-D Face Recognition; double Granularity Relation Network with Self-criticism for Occluded Person Re-identification; Using Explainable AI to Identify Differences Between Clinical and Experimental Pain Detection Models Based on Facial Expressions; rating-Aware Self-Organizing Maps; preface.","","","Conference review","Final","","Scopus","2-s2.0-85127121118"
"Rewatbowornwong P.; Tritrong N.; Suwajanakorn S.","Rewatbowornwong, Pitchaporn (57222420860); Tritrong, Nontawat (57222421916); Suwajanakorn, Supasorn (57222528911)","57222420860; 57222421916; 57222528911","Repurposing GANs for One-shot Semantic Part Segmentation","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","12","11","10.1109/TPAMI.2022.3201285","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137563068&doi=10.1109%2fTPAMI.2022.3201285&partnerID=40&md5=562e4ccb6217f4f682866fa24cd12c47","While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? And can image synthesis serve as an &#x201C;upstream&#x201D; representation learning task? In this work, we test these hypotheses and propose a simple and effective approach based on GANs for fundamental vision tasks: semantic part segmentation and landmark detection. With our approach, these tasks only require as few as one labeled example along with an unlabeled dataset, rather than thousands of examples. Our key idea is to leverage a trained GAN to extract a pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that this GAN-derived representation is &#x201C;readily discriminative&#x201D; and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning, which can generalize to many other tasks. More results are available at <uri>https://RepurposeGANs.github.io/</uri>. IEEE","Computer vision; Generative adversarial networks; Image representation; Job analysis; Semantic Segmentation; Annotation; Few-shot learning; Generative model; Image part segmentation; Images segmentations; Representation learning; Repurposing; Semantic parts; Task analysis; Semantics","Annotations; few-shot Learning; Generative adversarial networks; generative model; image part segmentation; Image segmentation; Representation learning; representation learning; Semantics; Task analysis; Training","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85137563068"
"Huo X.; Deng G.; Wu S.; Yu Z.; Li P.","Huo, Xiaoyang (57419470300); Deng, Guangchang (57214456773); Wu, Si (55495122900); Yu, Zhiwen (56399660300); Li, Peng (57746457300)","57419470300; 57214456773; 55495122900; 56399660300; 57746457300","Semi-Supervised Generative Learning with Extended Distribution Matching for Class-Conditional Image Synthesis","2022","Proceedings - IEEE International Conference on Multimedia and Expo","2022-July","","","","","","10.1109/ICME52920.2022.9859759","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137738755&doi=10.1109%2fICME52920.2022.9859759&partnerID=40&md5=29df9c77b83b61a9eb333c38d257188e","Generative Adversarial Network (GAN)-based models have made remarkable progress in high-fidelity image synthesis. However, the performance of class-conditional image synthesis may significantly deteriorate for the case where a limited number of labeled training samples are available. To reduce the dependence on labeled data, we propose a semi-supervised GAN with Extended Distribution Matching, and our model is referred to as EDM-GAN. To prevent a class-conditional discriminator from overfitting the limited labeled data, we perform a transformation of random regional replacement on both real and synthesized samples. By matching the extended distributions, the discriminator is encouraged to focus more on the spatial regions that contain certain objects, while at the same time a class-conditional generator is induced to capture precise class semantics. The adversarial training process can be effectively stabilized and converges to a better solution. Our experimental results on multiple standard benchmarks demonstrate consistent performance gains in synthesis quality and class-semantic accuracy.  © 2022 IEEE.","Benchmarking; Image processing; Metadata; Semantics; Distribution matching; High-fidelity images; Images synthesis; Labeled data; Network-based modeling; Overfitting; Performance; Semi-supervised; Semi-supervised learning; Training sample; Generative adversarial networks","distribution matching; generative adversarial networks; Image synthesis; semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85137738755"
"Neves J.C.; Tolosana R.; Vera-Rodriguez R.; Lopes V.; Proença H.; Fierrez J.","Neves, João C. (57197639019); Tolosana, Ruben (55605251600); Vera-Rodriguez, Ruben (26434040000); Lopes, Vasco (57202607214); Proença, Hugo (14016540600); Fierrez, Julian (55664171900)","57197639019; 55605251600; 26434040000; 57202607214; 14016540600; 55664171900","GAN Fingerprints in Face Image Synthesis","2022","Advances in Computer Vision and Pattern Recognition","","","","175","204","29","10.1007/978-981-16-7621-5_8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127879070&doi=10.1007%2f978-981-16-7621-5_8&partnerID=40&md5=34a248f9978021b6c3cd13cf14e3cd51","The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. This chapter is focused on the analysis of GAN fingerprints in face image synthesis. © 2022, The Author(s).","","","Book chapter","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127879070"
"Qiao P.; Gao X.; Man W.","Qiao, Pingan (57401745900); Gao, Xiwang (57402605900); Man, Wen (57401890800)","57401745900; 57402605900; 57401890800","AttnGAN++: Enhencing the Edge of Images on AttnGAN","2022","Lecture Notes on Data Engineering and Communications Technologies","89","","","792","802","10","10.1007/978-3-030-89698-0_81","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122384565&doi=10.1007%2f978-3-030-89698-0_81&partnerID=40&md5=0b272de8da0a1c792bf15caf8be88508","The text-to-image synthesis has made great progress at present, but the extraction of key semantic information and the restoration of the edge details of the generated image are still not perfect. In this paper, we proposed Attentional Generative Adversarial Network++ (AttnGAN++) model based on AttnGAN that allows to effectively solve the problem of missing the edge information of the generated image and extracting insufficient text features. First, we introduced Bi-directional Gated Recurrent Unit model (BiGRU), which can still ensure sufficient extraction of contextual information in processing long texts. This model combined with the attention mechanism, to achieve higher weights for the important words of the text. Then, we proposed the edge enhancement network consists four modules: Edge extraction, Residual Dense Block (RDB), Edge enhancement fusion, and Up-sampling model. Edge extraction obtains the edge of the final generated image. RDB to form a continuous memory mechanism, adaptively learn from previously more effective features to enhance feature propagation. Edge enhancement fusion and up-sampling modules to fuse the edge information and global information to generate high-resolution images with clearer edges. Thorough experiments on CUB dataset demonstrate that Attentional Generative Adversarial Network++ model significantly outperforms Attentional Generative Adversarial Network, boosting the best reported inception score by 3.78% and R-precision by 9.71% on CUB dataset, which can generate clearer image edges and improve the quality of the image. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Extraction; Image enhancement; Image fusion; Semantics; Signal sampling; Attentional generative adversarial network; Bi-directional; Bi-directional gated recurrent unit; Edge enhancement network; Edge enhancements; Edge extraction; Edge information; Network models; Residual dense block; Text-to-image; Generative adversarial networks","Attentional generative adversarial network; Bi-directional gated recurrent unit; Edge enhancement network; Residual dense block; Text-to-image","Book chapter","Final","","Scopus","2-s2.0-85122384565"
"Dinh T.M.; Tran A.T.; Nguyen R.; Hua B.-S.","Dinh, Tan M. (57370606300); Tran, Anh Tuan (56754161900); Nguyen, Rang (55647995100); Hua, Binh-Son (36607920500)","57370606300; 56754161900; 55647995100; 36607920500","HyperInverter: Improving StyleGAN Inversion via Hypernetwork","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11379","11388","9","10.1109/CVPR52688.2022.01110","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128337203&doi=10.1109%2fCVPR52688.2022.01110&partnerID=40&md5=e1be44038c89fc0ca9074d25d81ca584","Real-world image manipulation has achieved fantastic progress in recent years as a result of the exploration and utilization of GAN latent spaces. GAN inversion is the first step in this pipeline, which aims to map the real image to the latent code faithfully. Unfortunately, the majority of existing GAN inversion methods fail to meet at least one of the three requirements listed below: high reconstruction quality, editability, and fast inference. We present a novel two-phase strategy in this research that fits all requirements at the same time. In the first phase, we train an encoder to map the input image to StyleGAN2 W-space, which was proven to have excellent editability but lower reconstruction quality. In the second phase, we supplement the reconstruction ability in the initial phase by leveraging a series of hypernetworks to recover the missing information during inversion. These two steps complement each other to yield high reconstruction quality thanks to the hypernetwork branch and excellent editability due to the inversion done in the W-space. Our method is entirely encoder-based, resulting in extremely fast inference. Extensive experiments on two challenging datasets demonstrate the superiority of our method.11Project page: https://di-mi-ta.github.io/HyperInverter © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Signal encoding; Deep learning architecture and technique; Fast inference; Hypernetwork; Image and video synthesis and generation; Images synthesis; Learning architectures; Learning techniques; Reconstruction quality; Video generation; Video synthesis; Image enhancement","Deep learning architectures and techniques; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128337203"
"Gan H.-S.; Ramlee M.H.; Al-Rimy B.A.S.; Lee Y.-S.; Akkaraekthalin P.","Gan, Hong-Seng (57672217700); Ramlee, Muhammad Hanif (55151528800); Al-Rimy, Bander Ali Saleh (57200494876); Lee, Yeng-Seng (57215068206); Akkaraekthalin, Prayoot (6506006755)","57672217700; 55151528800; 57200494876; 57215068206; 6506006755","Hierarchical Knee Image Synthesis Framework for Generative Adversarial Network: Data From the Osteoarthritis Initiative","2022","IEEE Access","10","","","55051","55061","10","10.1109/ACCESS.2022.3175506","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130437854&doi=10.1109%2fACCESS.2022.3175506&partnerID=40&md5=29da9228751e7a1888b7a60d17268dc9","Medical images synthesis is useful to address persistent issues such as the lack of training data diversity and inflexibility of traditional data augmentation faced by medical image analysis researchers when developing their deep learning models. Generative adversarial network (GAN) can generate realistic image to overcome the abovementioned problems. We proposed a GAN model with hierarchical framework (HieGAN) to generate high-quality synthetic knee images as a prerequisite to enable effective training data augmentation for deep learning applications. During the training, the proposed framework embraced attention mechanism before the 256 ×256 scale in generator and discriminator to capture salient information of knee images. Then, a novel pixelwise-spectral normalization configuration was implemented to stabilize the training performance of HieGAN. We evaluated the proposed HieGAN on large scale knee image dataset by using Am Score and Mode Score. The results showed that HieGAN outperformed all relevant state-of-art. Hence, HieGAN can potentially serve as an important milestone to promote future development of more robust deep learning models for knee image segmentation. Future works should extend the image synthesis evaluation to clinical-related Visual Turing Test and synthetic data augmentation for deep learning segmentation task.  © 2013 IEEE.","Deep learning; Image segmentation; Medical imaging; Biomedical imaging; Data augmentation; Deep learning; Generator; Images synthesis; Knee; Learning models; Network data; Training data; Generative adversarial networks","Biomedical image processing; Generative adversarial network; Image synthesis; Knee","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130437854"
"Cheng Q.; Wen K.; Gu X.","Cheng, Qingrong (57193678501); Wen, Keyu (57224737697); Gu, Xiaodong (7403204205)","57193678501; 57224737697; 7403204205","Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks","2022","IEEE Transactions on Multimedia","","","","1","14","13","10.1109/TMM.2022.3217384","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141459332&doi=10.1109%2fTMM.2022.3217384&partnerID=40&md5=6da3be28cd2f7693a971e811f388dbc3","Text-to-image synthesis is an attractive but challenging task that aims to generate a photo-realistic and semantic consistent image from a specific text description. The images synthesized by off-the-shelf models usually contain limited components compared with the corresponding image and text description, which decreases the image quality and the textual-visual consistency. To address this issue, we propose a novel Vision-Language Matching strategy for text-to-image synthesis, named VLMGAN*, which introduces a dual vision-language matching mechanism to strengthen the image quality and semantic consistency. The dual vision-language matching mechanism considers textual-visual matching between the generated image and the corresponding text description, and visual-visual consistent constraints between the synthesized image and the real image. Given a specific text description, VLMGAN* firstly encodes it into textual features and then feeds them to a dual vision-language matching-based generative model to synthesize a photo-realistic and textual semantic consistent image. Besides, the popular evaluation metrics for text-to-image synthesis are borrowed from simple image generation, which mainly evaluate the reality and diversity of the synthesized images. Therefore, we introduce a metric named Vision-Language Matching Score (VLMS) to evaluate the performance of text-to-image synthesis which can consider both the image quality and the semantic consistency between the synthesized image and the description. The proposed dual multi-level vision-language matching strategy can be applied to other text-to-image synthesis methods. We implement this strategy on two popular baselines, which are marked with <inline-formula><tex-math notation=""LaTeX"">${\rm{VLMGAN}_{+\rm{AttnGAN}}}$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">${\rm{VLMGAN}_{+\rm{DFGAN}}}$</tex-math></inline-formula> . The experimental results on two widely-used datasets show that the model achieves significant improvements over other state-of-the-art methods. IEEE","Computer vision; Flow visualization; Generative adversarial networks; Image quality; Quality control; Semantic Web; Visual languages; Images synthesis; Matching mechanisms; Matchings; Photo-realistic; Semantic consistency; Synthesized images; Task analysis; Text-to-image synthesis; Vision-language matching; Semantics","Generative adversarial networks; Generative Adversarial Networks; Image quality; Image synthesis; Measurement; Semantics; Task analysis; Text-to-image synthesis; vision-language matching; Visualization","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85141459332"
"Eschweiler D.; Laube I.; Stegmaier J.","Eschweiler, Dennis (57194396472); Laube, Ina (57222568017); Stegmaier, Johannes (55584699900)","57194396472; 57222568017; 55584699900","Spatiotemporal image generation for embryomics applications","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","517","541","24","10.1016/B978-0-12-824349-7.00030-X","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137561575&doi=10.1016%2fB978-0-12-824349-7.00030-X&partnerID=40&md5=f8086538df7de98c7ca2cbce604018c2","Time-resolved 3D (3D+t) fluorescence imaging techniques like light-sheet and confocal microscopy allow imaging even entire embryos of model organisms like the fruit fly, zebrafish, and mice from a fertilized egg to a complex multicellular organism. To make the most of such experiments, automatic methods are inevitable to quantify precisely orchestrated developmental processes that give rise to a living animal or plant. While several classical methods already achieved remarkable results on such data sets, tasks like detection, segmentation, and tracking are still imperfect and require a substantial degree of human intervention for error correction. Moreover, the potential of the more recent deep learning approaches cannot be maxed out yet due to a severe lack of annotated training data available for such large-scale 3D+t image data. Image synthesis thus plays an important role for training data-hungry machine learning algorithms and to systematically benchmark the performance of image analysis algorithms on realistic data sets of the target domain. In this contribution, we review past attempts for in silico modeling of embryogenesis and present our attempt on fully- and semi-synthetic generation of realistic 3D+t image data for various image analysis applications including both classical and machine learning-based components. © 2022 Elsevier Inc. All rights reserved.","","3D+t image analysis; Data synthesis; Embryogenesis; Generative adversarial networks","Book chapter","Final","","Scopus","2-s2.0-85137561575"
"Deshpande S.; Kovacheva V.; Minhas F.; Rajpoot N.","Deshpande, Srijay (57206158392); Kovacheva, Violeta (56022266000); Minhas, Fayyaz (24399575300); Rajpoot, Nasir (8042017200)","57206158392; 56022266000; 24399575300; 8042017200","Generative models for synthesis of colorectal cancer histology images","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","491","516","25","10.1016/B978-0-12-824349-7.00029-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137543887&doi=10.1016%2fB978-0-12-824349-7.00029-3&partnerID=40&md5=f0ee906af8545718f346fd59b053179a","The generation of synthetic cancer tissue images has become widely popular over the last few years to understand the underlying morphological characteristics as well as tumor heterogeneity in cancer tissues. They can be potentially instrumental in training and evaluating digital pathology algorithms, particularly where data availability is limited. Since the collection and subsequent annotation of stained histopathology slides is laborious, replacing them with synthetically generated and annotated slides can significantly reduce the annotation burden. In this chapter, we describe in detail two published methods for generating synthetic histology images of colorectal cancer tissues. The first method presents a spatial model named TheCoT, which models the tumor heterogeneity in colorectal cancer tissues. It offers flexibility to control the appearance of the tissue images based on user-defined parameters such as cancer grade, cellularity, cell-overlap ratio, objective level, and image resolution. The second method proposes a deep learning based framework to generate large colorectal cancer tissue tiles of arbitrary sizes, exhibiting the realistic appearance of glands, nuclei structure and stromal architecture. It demonstrates that the model can be effectively used for the evaluation of gland segmentation algorithms. © 2022 Elsevier Inc. All rights reserved.","","Colorectal tissue architecture; Computational pathology; Generative adversarial networks; Image synthesis","Book chapter","Final","","Scopus","2-s2.0-85137543887"
"Chen T.; Wu S.; Yang X.; Xu Y.; Wong H.-S.","Chen, Tianyi (57212621725); Wu, Si (55495122900); Yang, Xuhui (57215926336); Xu, Yong (57274194400); Wong, Hau-San (7402864844)","57212621725; 55495122900; 57215926336; 57274194400; 7402864844","Semantic Regularized Class-Conditional GANs for Semi-Supervised Fine-Grained Image Synthesis","2022","IEEE Transactions on Multimedia","24","","","2975","2985","10","10.1109/TMM.2021.3091859","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112141318&doi=10.1109%2fTMM.2021.3091859&partnerID=40&md5=08f532cd8e66304fe97e5efebc17a726","Learning effective generative models for natural image synthesis is a promising way to reduce the dependence of deep models on massive training data. This work focuses on Fine-Grained Image Synthesis (FGIS) in the semi-supervised setting where a small number of training instances are labeled. Different from generic image synthesis tasks, the available fine-grained data may be inadequate, and the differences among the object categories are typically subtle. To address these issues, we propose a Semantic Regularized class-conditional Generative Adversarial Network, which is referred to as SReGAN. We incorporate an additional discriminator and classifier into the generator-discriminator minimax game. Competing with two discriminators enforces the generator to model both marginal and class-conditional data distributions, which alleviates the problem of limited training data and labels. However, the discriminators may overlook the class separability. To induce the generator to discover the distinctions between classes, we construct semantically congruent and incongruent pairs in the generation process, and further regularize the generator by encouraging high similarities of congruent pairs, while penalizing that of incongruent ones in the classifier's feature space. We have conducted extensive experiments to verify the capability of SReGAN in generating high-fidelity images on a variety of FGIS benchmarks.  © 1999-2012 IEEE.","Semantics; Adversarial networks; Class separability; Data distribution; Generation process; Generative model; High-fidelity images; Limited training data; Object categories; Image processing","fine-grained image synthesis; generative adversarial networks; semantic regularization; Semi-supervised learning","Article","Final","","Scopus","2-s2.0-85112141318"
"Liu X.; Liu X.; Li G.; Bi S.","Liu, Xiaokai (55235756900); Liu, Xiang (57701741300); Li, Gang (57701741400); Bi, Sheng (36018455300)","55235756900; 57701741300; 57701741400; 36018455300","Pose and Color-Gamut Guided Generative Adversarial Network for Pedestrian Image Synthesis","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","1","13","12","10.1109/TNNLS.2022.3171245","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130440325&doi=10.1109%2fTNNLS.2022.3171245&partnerID=40&md5=9517d0adae3ecc693a3b52a050605558","Tremendous transfer requirements in pedestrian reidentification (Re-ID) tasks have greatly promoted the remarkable success in pedestrian image synthesis, to relieve the inconsistency in poses and lighting. However, existing approaches are confined to transferring in a particular domain and are difficult to combine, since pose and color variables locate in two independent domains. To facilitate the research toward conquering this issue, we propose a pose and color-gamut guided generative adversarial network (PC-GAN) that performs joint-domain pedestrian image synthesis conditioned on certain pose and color-gamut through a delicate supervision design. The generator of the network comprises a sequence of cross-domain conversion subnets, where the local displacement estimator, color-gamut transformer, and pose transporter coordinate their learning pace to progressively synthesize images in desired pose and color-gamut. Ablation studies have demonstrated the efficacy and efficiency of the proposed network both qualitatively and quantitatively on Market-1501 and DukeMTMC. Furthermore, the proposed architecture can generate training images for person Re-ID, alleviating the data insufficiency problem. IEEE","Color; Computer vision; Image analysis; Job analysis; Lighting; Color gamuts; Color-gamut transformation; Domain transformation; Generative adversarial network; Generator; Image color analysis; Images synthesis; Joint-domain transformation; Pedestrian image synthesis.; Task analysis; Generative adversarial networks","Cameras; Color-gamut transformation; Generative adversarial networks; generative adversarial networks (GANs); Generators; Image color analysis; Image synthesis; joint-domain transformation; Lighting; pedestrian image synthesis.; Task analysis","Article","Article in press","","Scopus","2-s2.0-85130440325"
"Yu C.; Wang W.","Yu, Cheng (57218558930); Wang, Wenmin (55441345600)","57218558930; 55441345600","Fast transformation of discriminators into encoders using pre-trained GANs","2022","Pattern Recognition Letters","153","","","92","99","7","10.1016/j.patrec.2021.11.026","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121096833&doi=10.1016%2fj.patrec.2021.11.026&partnerID=40&md5=80789cf3349da5ed05735dc4fd1d2a3e","A typical generative adversarial network (GAN) consists of a generator and a discriminator. Currently, finely tuned deep GANs can synthesize high-quality (HQ) images via their generators. However, the discriminator in typical GANs is only able to distinguish true or fake images in the training process. Moreover, some synthesized images from GANs are imperfect, and we can not reconstruct images via GANs. In this paper, we revisit pre-trained GANs and offer a self-supervised method to quickly transform GAN's discriminators into encoders. We reuse parameters of the GAN's discriminator and replace its output layer, so it can be transformed into an encoder and output reformed latent vectors. The transformation makes GAN architecture more symmetrical and allows for better performance. Based on the method, GANs can be made to reconstruct synthesized images via GAN encoders. Compared to synthesized images, these reconstructions can maintain or even attain higher quality. The code and pre-trained models are available at https://github.com/disanda/GAN-Encoder-Sym. © 2021 Elsevier B.V.","Discriminators; Image reconstruction; Signal encoding; Auto encoders; Fast transformation; Generative adversarial net; High quality images; Images reconstruction; Images synthesis; Reuse; Supervised methods; Synthesized images; Training process; Generative adversarial networks","Auto-encoder; Generative adversarial net (GAN); Image reconstruction; Image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85121096833"
"Jung S.H.; Bok Lee T.; Heo Y.S.","Jung, Soo Hyun (57221219233); Bok Lee, Tae (57483751600); Heo, Yong Seok (57201765185)","57221219233; 57483751600; 57201765185","Deep Feature Prior Guided Face Deblurring","2022","Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022","","","","884","893","9","10.1109/WACV51458.2022.00096","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126151674&doi=10.1109%2fWACV51458.2022.00096&partnerID=40&md5=fbafa8cb4f559c511347131ab7a9608f","Most recent face deblurring methods have focused on utilizing facial shape priors such as face landmarks and parsing maps. While these priors can provide facial geometric cues effectively, they are insufficient to contain local texture details that act as important clues to solve face deblurring problem. To deal with this, we focus on estimating the deep features of pre-trained face recognition networks (e.g., VGGFace network) that include rich information about sharp faces as a prior, and adopt a generative adversarial network (GAN) to learn it. To this end, we propose a deep feature prior guided network (DFPGnet) that restores facial details using the estimated the deep feature prior from a blurred image. In our DFPGnet, the generator is divided into two streams including prior estimation and deblurring streams. Since the estimated deep features of the prior estimation stream are learned from the VGGFace network which is trained for face recognition not for deblurring, we need to alleviate the discrepancy of feature distributions between the two streams. Therefore, we present feature transform modules at the connecting points of the two streams. In addition, we propose a channel-attention feature discriminator and prior loss, which encourages the generator to focus on more important channels for deblurring among the deep feature prior during training. Experimental results show that our method achieves state-of-the-art performance both qualitatively and quantitatively.  © 2022 IEEE.","Color photography; Computer vision; Generative adversarial networks; Image enhancement; Textures; Deblurring; Deblurring problems; Face landmarks; Facial shape; Images synthesis; Learn+; Local Texture; Shape priors; Two-stream; Video synthesis; Face recognition","Computational Photography; Image and Video Synthesis","Conference paper","Final","","Scopus","2-s2.0-85126151674"
"Pang M.; Wang B.; Huang S.; Cheung Y.-M.; Wen B.","Pang, Meng (55757588900); Wang, Binghui (55552239200); Huang, Siyu (57191918546); Cheung, Yiu-Ming (7202111458); Wen, Bihan (56396097300)","55757588900; 55552239200; 57191918546; 7202111458; 56396097300","A Unified Framework for Bidirectional Prototype Learning From Contaminated Faces Across Heterogeneous Domains","2022","IEEE Transactions on Information Forensics and Security","17","","","1544","1557","13","10.1109/TIFS.2022.3164215","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127514926&doi=10.1109%2fTIFS.2022.3164215&partnerID=40&md5=8e4facf4a8efd8d5785ca611fe0dc793","Existing heterogeneous face synthesis (HFS) methods focus on performing accurate image-to-image translation across domains, while they cannot effectively remove the nuisance facial variations such as poses, expressions or occlusions. To address such challenges, this paper studies a new practical heterogeneous prototype learning (HPL) problem. To be specific, given a face image contaminated by facial variations from a source domain, HPL aims to reconstruct the variation-free prototype in a specified target domain. To tackle HPL, we propose a unified and end-to-end framework named bidirectional heterogeneous prototype learning (BHPL). As a bidirectional learning framework, BHPL is able to simultaneously reconstruct the heterogeneous prototypes across source-to-target as well as target-to-source domains. Furthermore, BHPL is capable of learning the identity prototype features for the contaminated face images from both source and target domains in order to perform robust heterogeneous face recognition. BHPL consists of an encoder-decoder structural generator and two dual-task discriminators, which play an adversarial game such that the generator learns the identity prototype feature and generates the cross-domain identity-preserved prototype for each input face image from both domains, and the discriminators accurately predict face identity and distinguish real versus fake prototypes. Empirically studies on multiple heterogeneous face datasets containing facial variations demonstrate the effectiveness of BHPL.  © 2005-2012 IEEE.","Face recognition; Hafnium; Image reconstruction; Adversarial learning; Face; Face synthesis; Generator; Heterogeneous face recognition; Heterogeneous prototype learning; Images reconstruction; Images synthesis; Prototype; Prototype learning; Generative adversarial networks","adversarial learning; Face synthesis; heterogeneous face recognition; heterogeneous prototype learning","Article","Final","","Scopus","2-s2.0-85127514926"
"Belousov S.","Belousov, Sergei (57224013814)","57224013814","MobileStyleGAN.pytorch: PyTorch-based toolkit to compress StyleGAN2 model[Formula presented]","2021","Software Impacts","10","","100115","","","","10.1016/j.simpa.2021.100115","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122651283&doi=10.1016%2fj.simpa.2021.100115&partnerID=40&md5=815ddc1f2756ed2e44a70d2689999fe3","In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We introduce an open-source toolkit called MobileStyleGAN.pytorch to compress the StyleGAN2 model. © 2021 The Author(s)","","CNN compression; Generative Adversarial Networks; Image synthesis; MobileStyleGAN; StyleGAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122651283"
"Huang S.; Chen Y.","Huang, Siyue (57291388900); Chen, Ying (55985403200)","57291388900; 55985403200","Generative Adversarial Networks with Adaptive Semantic Normalization for text-to-image synthesis","2022","Digital Signal Processing: A Review Journal","120","","103267","","","","10.1016/j.dsp.2021.103267","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116860986&doi=10.1016%2fj.dsp.2021.103267&partnerID=40&md5=46549b80750db51ab1f1d8f1d0b94e7e","Transforming human natural language into photo-realistic images has always been a challenging issue. Batch Normalization (BN) is used in current text-to-image models to accelerate and stabilize the training process. However, the BN ignores feature differences between individuals and semantic relationship between modalities, which is negative for text-to-image tasks. To solve the problems, a novel module called Adaptive Semantic Instance Normalization (ASIN) is proposed. The ASIN considers the individuality of generated images and introduces text semantic information to the image normalization process, establishing a consistent and semantically close correlation between generated images and given text. Extensive experiments and ablation studies are carried out on two types of datasets. The results demonstrate the superiority of the proposed method in comparison of previous methods. © 2021 Elsevier Inc.","Image processing; Semantic Web; Semantics; Coarse to fine; Coarse-to-fine image generation; Cross-modal; Cross-modal information fusion; Fine images; Image generations; Images synthesis; Natural languages; Normalisation; Text-to-image synthesis; Generative adversarial networks","Coarse-to-fine image generation; Cross-modal information fusion; Generative Adversarial Network; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85116860986"
"Tan H.; Liu X.; Yin B.; Li X.","Tan, Hongchen (57209272265); Liu, Xiuping (36910875600); Yin, Baocai (8616230700); Li, Xin (57218467896)","57209272265; 36910875600; 8616230700; 57218467896","DR-GAN: Distribution Regularization for Text-to-Image Generation","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2022.3165573","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128654551&doi=10.1109%2fTNNLS.2022.3165573&partnerID=40&md5=9eabba5ac3c65741c7c13d2f4983a68f","This article presents a new text-to-image (T2I) generation model, named distribution regularization generative adversarial network (DR-GAN), to generate images from text descriptions from improved distribution learning. In DR-GAN, we introduce two novel modules: a semantic disentangling module (SDM) and a distribution normalization module (DNM). SDM combines the spatial self-attention mechanism (SSAM) and a new semantic disentangling loss (SDL) to help the generator distill key semantic information for the image generation. DNM uses a variational auto-encoder (VAE) to normalize and denoise the image latent distribution, which can help the discriminator better distinguish synthesized images from real images. DNM also adopts a distribution adversarial loss (DAL) to guide the generator to align with normalized real image distributions in the latent space. Extensive experiments on two public datasets demonstrated that our DR-GAN achieved a competitive performance in the T2I task. The code link: https://github.com/Tan-H-C/DR-GAN-Distribution-Regularization-for-Text-to-Image-Generation. IEEE","Generative adversarial networks; Image enhancement; Job analysis; Semantic Web; Space division multiple access; Distribution normalization; Generator; Image generations; Images synthesis; Normalisation; Regularisation; Semantic disentanglement mechanism; Stability analyze; Task analysis; Text-to-image (T2I) generation.; Semantics","Distribution normalization; generative adversarial network; Generators; Image synthesis; semantic disentanglement mechanism; Semantics; Stability analysis; Task analysis; text-to-image (T2I) generation.; Training; Visualization","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85128654551"
"Yang Y.; Hossain M.Z.; Gedeon T.; Rahman S.","Yang, Yan (57215379226); Hossain, Md Zakir (57212814547); Gedeon, Tom (24400830200); Rahman, Shafin (55435301000)","57215379226; 57212814547; 24400830200; 55435301000","S2FGAN: Semantically Aware Interactive Sketch-to-Face Translation","2022","Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022","","","","3162","3171","9","10.1109/WACV51458.2022.00322","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126146773&doi=10.1109%2fWACV51458.2022.00322&partnerID=40&md5=2cf9b8b9a8de0f0a8fe2e1e25e7fffdb","Interactive facial image manipulation attempts to edit single and multiple face attributes using a photo-realistic face and/or semantic mask as input. In the absence of the photo-realistic image (only sketch/mask available), previous methods only retrieve the original face but ignore the potential of aiding model controllability and diversity in the translation process. This paper proposes a sketch-to-image generation framework called S2FGAN, aiming to improve users' ability to interpret and flexibility of face attribute editing from a simple sketch. First, to restore a vivid face from a sketch, we propose semantic level perceptual loss to increase the translation quality. Second, we dedicate the theoretic analysis of attribute editing and build attribute mapping networks with latent semantic loss to modify latent space semantics of Generative Adversarial Networks (GANs). The users can command the model to retouch the generated images by involving the semantic information in the generation process. In this way, our method can manipulate single or multiple face attributes by only specifying attributes to be changed. Extensive experimental results on the CelebAMask-HQ dataset empirically show our superior performance and effectiveness on this task. Our method successfully outperforms state-of-the-art sketch-to-image generation and attribute manipulation methods by exploiting greater control of attribute intensity.  © 2022 IEEE.","Color photography; Computer vision; Deep learning; Image enhancement; Semantics; Auto encoders; Deep learning; Facial images; Generative adversarial network computational photography; Generative model; Image generations; Images synthesis; Interactive sketch; Neural generative model; Video synthesis; Generative adversarial networks","Autoencoders; Deep Learning; GANs Computational Photography; Image and Video Synthesis; Neural Generative Models","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126146773"
"Li X.; Zhang J.; Li K.; Vyas S.; Rawat Y.S.","Li, Xianhang (57219698864); Zhang, Junhao (57834023700); Li, Kunchang (57224933158); Vyas, Shruti (57219753505); Rawat, Yogesh S. (56297042000)","57219698864; 57834023700; 57224933158; 57219753505; 56297042000","Pose-guided Generative Adversarial Net for Novel View Action Synthesis","2022","Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022","","","","3103","3112","9","10.1109/WACV51458.2022.00316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126126947&doi=10.1109%2fWACV51458.2022.00316&partnerID=40&md5=a2694d8ad471111df6b95e83a024ea52","We focus on the problem of novel-view human action synthesis. Given an action video, the goal is to generate the same action from an unseen viewpoint. Naturally, novel view video synthesis is more challenging than image synthesis. It requires the synthesis of a sequence of realistic frames with temporal coherency. Besides, transferring different actions to a novel target view requires awareness of action category and viewpoint change simultaneously. To address these challenges we propose a novel framework named Pose-guided Action Separable Generative Adversarial Net (PAS-GAN), which utilizes pose to alleviate the difficulty of this task. First, we propose a recurrent pose-transformation module which transforms actions from the source view to the target view and generates novel view pose sequence in 2D coordinate space. Second, a well-transformed pose sequence enables us to separate the action and background in the target view. We employ a novel local-global spatial transformation module to effectively generate sequential video features in the target view using these action and background features. Finally, the generated video features are used to synthesize human action with the help of a 3D decoder. Moreover, to focus on dynamic action in the video, we propose a novel multi-scale action-separable loss which further improves the video quality. We conduct extensive experiments on two large-scale multi-view human action datasets, NTU-RGBD and PKU-MMD, demonstrating the effectiveness of PAS-GAN which outperforms existing approaches. The codes and models will be available on https://github.com/xhl-video/PAS-GAN.  © 2022 IEEE.","Color photography; Computer vision; Deep learning; Large dataset; 2D coordinates; Coordinate space; Human actions; Image and video synthesis deep learning; Images synthesis; Source view; Temporal coherency; Transformation modules; Video features; Video synthesis; Generative adversarial networks","Computational Photography; Image and Video Synthesis Deep Learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126126947"
"Ramlatchan A.; Li Y.","Ramlatchan, Andy (57219666637); Li, Yaohang (57201317848)","57219666637; 57201317848","Image Synthesis Using Conditional GANs for Selective Laser Melting Additive Manufacturing","2022","Proceedings of the International Joint Conference on Neural Networks","2022-July","","","","","","10.1109/IJCNN55064.2022.9892033","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140735686&doi=10.1109%2fIJCNN55064.2022.9892033&partnerID=40&md5=af576846196b58f9a0c1c03e295741e0","In-situ process monitoring for metals additive manufacturing is paramount to the successful build of an object for application in extreme or high stress environments. Yet in selective laser melting additive manufacturing, it is extremely difficult to evaluate the build process. The difficulty is that obtaining enough variety of data to quantify the internal microstructures for the evaluation of its physical properties is problematic, as the laser passes at high speeds over powder grains at a micrometer scale. Using generative models, a type of machine learning, has been shown here to provide new artificially generated data with the same properties as the experimental images. The Generative Adversarial Network (GAN) synthesized new computationally derived data through a process that learns the underlying features of images that correspond to the different laser process parameters in a generator network. While this technique was effective at delivering high-quality images that closely matched the training data when tested against holdout samples, modifications to the general form of the network through a conditional generative adversarial network (CGAN) showed improved capabilities at creating these new images. Using multiple evaluation metrics, it has been shown that generative models can be used to create new data for various laser process parameter combinations, thereby allowing a more comprehensive evaluation of ideal laser conditions for any particular build. The new data can supplement the experimental data, thereby growing the overall knowledge framework for build characteristics. © 2022 IEEE.","Additives; Deep learning; Image enhancement; Learning systems; Neural networks; Process monitoring; Selective laser melting; Deep learning; Generative model; High stress; Images synthesis; In-situ process monitoring; Internal microstructure; Laser process parameters; Manufacturing IS; Metal additives; Selective laser melting; Generative adversarial networks","additive manufacturing; artificial neural networks; deep learning; generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85140735686"
"Kumar R.; Bhatnagar V.; Jain A.; Singh M.; Kareem Z.H.; Sugumar R.","Kumar, Rajeev (57198684739); Bhatnagar, Vaibhav (57208904701); Jain, Amit (57226385162); Singh, Mahesh (57855942100); Kareem, Z.H. (57213839235); Sugumar, R. (6506824735)","57198684739; 57208904701; 57226385162; 57855942100; 57213839235; 6506824735","CNN-Based Cross-Modal Residual Network for Image Synthesis","2022","BioMed Research International","2022","","6399730","","","","10.1155/2022/6399730","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136484638&doi=10.1155%2f2022%2f6399730&partnerID=40&md5=f5dbac9adc701a97b5fc84b5d8d88397","This study attempts to address the issue that present cross-modal image synthesis algorithms do not capture the spatial and structural information of human tissues effectively. As a consequence, the resulting photos include flaws including fuzzy edges and a poor signal-to-noise ratio. The authors offer a cross-sectional technique that combines residual modules with generative adversarial networks. The approach incorporates an enhanced residual initial module and attention mechanism into the generator network, reducing the number of parameters and improving the generator's feature learning capabilities. To boost discriminant performance, the discriminator employs a multiscale discriminator. A multilevel structural similarity loss is included in the loss function to improve picture contrast preservation. On the ADNI data set, the algorithm is compared to the mainstream algorithms. The experimental findings reveal that the synthetic PET image's MAE index has dropped while the SSIM and PSNR indexes have improved. The experimental findings suggest that the proposed model may maintain picture structural information while improving image quality in both visual and objective measures. The residue initial module and attention mechanism are employed to increase the generator's capacity for learning, while the multiscale discriminator is utilized to improve the model's discriminative performance. The enhanced method in this study can maintain the structure and contrast information of the picture, according to comparative experimental findings using the ADNI dataset. The produced picture is hence more aesthetically similar to the genuine print.  © 2022 Rajeev Kumar et al.","Algorithms; Alzheimer Disease; Cross-Sectional Studies; Humans; Image Processing, Computer-Assisted; Signal-To-Noise Ratio; algorithm; article; attention; human; human experiment; image quality; learning; loss of function mutation; signal noise ratio; synthesis; Alzheimer disease; cross-sectional study; image processing; procedures","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136484638"
"Liu Y.; Yue L.; Xiao S.; Yang W.; Shen D.; Liu M.","Liu, Yunbi (57190948977); Yue, Ling (55634279800); Xiao, Shifu (7402022817); Yang, Wei (56982069100); Shen, Dinggang (7401738392); Liu, Mingxia (36677833300)","57190948977; 55634279800; 7402022817; 56982069100; 7401738392; 36677833300","Assessing clinical progression from subjective cognitive decline to mild cognitive impairment with incomplete multi-modal neuroimages","2022","Medical Image Analysis","75","","102266","","","","10.1016/j.media.2021.102266","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117727048&doi=10.1016%2fj.media.2021.102266&partnerID=40&md5=6d335fc15e7bb1306cf567619f8c969d","Accurately assessing clinical progression from subjective cognitive decline (SCD) to mild cognitive impairment (MCI) is crucial for early intervention of pathological cognitive decline. Multi-modal neuroimaging data such as T1-weighted magnetic resonance imaging (MRI) and positron emission tomography (PET), help provide objective and supplementary disease biomarkers for computer-aided diagnosis of MCI. However, there are few studies dedicated to SCD progression prediction since subjects usually lack one or more imaging modalities. Besides, one usually has a limited number (e.g., tens) of SCD subjects, negatively affecting model robustness. To this end, we propose a Joint neuroimage Synthesis and Representation Learning (JSRL) framework for SCD conversion prediction using incomplete multi-modal neuroimages. The JSRL contains two components: 1) a generative adversarial network to synthesize missing images and generate multi-modal features, and 2) a classification network to fuse multi-modal features for SCD conversion prediction. The two components are incorporated into a joint learning framework by sharing the same features, encouraging effective fusion of multi-modal features for accurate prediction. A transfer learning strategy is employed in the proposed framework by leveraging model trained on the Alzheimer's Disease Neuroimaging Initiative (ADNI) with MRI and fluorodeoxyglucose PET from 863 subjects to both the Chinese Longitudinal Aging Study (CLAS) with only MRI from 76 SCD subjects and the Australian Imaging, Biomarkers and Lifestyle (AIBL) with MRI from 235 subjects. Experimental results suggest that the proposed JSRL yields superior performance in SCD and MCI conversion prediction and cross-database neuroimage synthesis, compared with several state-of-the-art methods. © 2021 Elsevier B.V.","Alzheimer Disease; Australia; Cognitive Dysfunction; Humans; Magnetic Resonance Imaging; Neuroimaging; Biomarkers; Computer aided diagnosis; Learning systems; Magnetic resonance imaging; Neurodegenerative diseases; Neuroimaging; Positron emission tomography; fluorodeoxyglucose f 18; flutemetamol f 18; Pittsburgh compound B; Cognitive decline; Cognitive impairment; Conversion prediction; Early intervention; Images synthesis; Learning frameworks; Multi-modal; Multi-modal neuroimage; Subjective cognitive decline; Two-component; aged; Article; cognitive defect; data synthesis; diagnostic accuracy; diagnostic test accuracy study; disease course; early diagnosis; feature learning (machine learning); feature selection; female; human; major clinical study; male; mild cognitive impairment; multimodal imaging; neuroimaging; nuclear magnetic resonance imaging; positron emission tomography; prediction; receiver operating characteristic; sensitivity and specificity; subjective cognitive decline; transfer of learning; Alzheimer disease; Australia; cognitive defect; diagnostic imaging; neuroimaging; Forecasting","Conversion prediction; Image synthesis; Multi-modal neuroimage; Subjective cognitive decline","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117727048"
"Phyu Aung A.P.; Wang X.; Yu R.; An B.; Jayavelu S.; Li X.","Phyu Aung, Aye Phyu (57961995300); Wang, Xinrun (57201908529); Yu, Runsheng (57219434442); An, Bo (8901577100); Jayavelu, Senthilnath (35183910200); Li, Xiaoli (57219334888)","57961995300; 57201908529; 57219434442; 8901577100; 35183910200; 57219334888","DO-GAN: A Double Oracle Framework for Generative Adversarial Networks","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11265","11274","9","10.1109/CVPR52688.2022.01099","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141781929&doi=10.1109%2fCVPR52688.2022.01099&partnerID=40&md5=5c64452f9841e748e092cc8fa3796d37","In this paper, we propose a new approach to train Gen-erative Adversarial Networks (GANs) where we deploy a double-oracle framework using the generator and discrim-inator oracles. GAN is essentially a two-player zero-sum game between the generator and the discriminator. Training GANs is challenging as a pure Nash equilibrium may not exist and even finding the mixed Nash equilibrium is difficult as GANs have a large-scale strategy space. In DO-GAN, we extend the double oracle framework to GANs. We first generalize the players' strategies as the trained models of generator and discriminator from the best response or-acles. We then compute the meta-strategies using a linear program. For scalability of the framework where multi-ple generators and discriminator best responses are stored in the memory, we propose two solutions: 1) pruning the weakly-dominated players' strategies to keep the oracles from becoming intractable; 2) applying continual learning to retain the previous knowledge of the networks. We apply our framework to established GAN architectures such as vanilla GAN, Deep Convolutional GAN, Spectral Normalization GAN and Stacked GAN. Finally, we conduct experiments on MNIST, CIFAR-10 and CelebA datasets and show that DO-GAN variants have significant improvements in both subjective qualitative evaluation and quantitative metrics, compared with their respective GAN architectures. © 2022 IEEE.","Computation theory; Computer vision; Deep learning; Discriminators; Game theory; Generative adversarial networks; Linear programming; Adversarial networks; Deep learning architecture and technique; Image and video synthesis and generation; Images synthesis; Learning architectures; Learning techniques; Optimization method; Other; Video generation; Video synthesis; Network architecture","Deep learning architectures and techniques; Image and video synthesis and generation; Optimization methods; Others","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141781929"
"Liu L.; Li W.; Shi Z.; Zou Z.","Liu, Liqin (57215536317); Li, Wenyuan (57204784272); Shi, Zhenwei (23398841900); Zou, Zhengxia (56073977200)","57215536317; 57204784272; 23398841900; 56073977200","Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5528215","","","","10.1109/TGRS.2022.3173532","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130678301&doi=10.1109%2fTGRS.2022.3173532&partnerID=40&md5=d0c26ffea5447d5804eb35757065b4df","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as 'Physics-informed Deep Adversarial Spectral Synthesis (PDASS).' On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.  © 1980-2012 IEEE.","Generative adversarial networks; Hyperspectral imaging; Military applications; Optical resolving power; Pixels; Reflection; Remote sensing; Spectroscopy; Adversarial networks; Atmospheric modeling; Generation adversarial network; Images reconstruction; Imaging modeling; Remote-sensing; Spatial resolution; Spectral super-resolution; Superresolution; artificial neural network; image analysis; image resolution; imaging method; physics; pixel; remote sensing; satellite imagery; spectral analysis; Image reconstruction","Generation adversarial networks (GANs); hyperspectral image; imaging model; remote sensing; spectral super-resolution (SSR)","Article","Final","","Scopus","2-s2.0-85130678301"
"Byun Y.; Baek J.-G.","Byun, Yunseon (57216322879); Baek, Jun-Geol (7103228722)","57216322879; 7103228722","Image Synthesis with Single-type Patterns for Mixed-type Pattern Recognition on Wafer Bin Maps","2022","4th International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2022 - Proceedings","","","","39","43","4","10.1109/ICAIIC54071.2022.9722634","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127652992&doi=10.1109%2fICAIIC54071.2022.9722634&partnerID=40&md5=dcf4cbf58f97ce37e4e0ab9ab6acaff3","To increase the productivity, it is important to manage yield and reduce defects in the semiconductor industry. One of the efforts is to identify defect patterns and control the cause factors that affects the defects. Many engineers inspect the quality of each chip and check the defect pattern on the wafer bin maps. To get the accurate and consistent classification results regardless of the level for domain knowledge or experience of engineers, deep learning-based models have recently been studied. Since most previous studies aim to classify the single-type defect patterns, it is needed to consider the mixed-type defect patterns together. Also, they require a lot of labeled data to train the deep learning-based classification model. However, defects occur extremely rarely in actual manufacturing process. Therefore, the method securing the higher accuracy in a situation where enough labeled data are not given is needed. This paper proposes a deep convolutional generative adversarial network for wafer map synthesis (DCGAN-WS) which generates the mixed-type patterns by synthesizing the single-type pattern and adding the pixel-wise summation. To maintain the characteristics of the binary pixel of the wafer bin maps, a thresholding technique is added. MixedWM38 dataset is used for the experiments, and it was verified that the mixed-type patterns were synthesized well. It helps to construct more robust model for single-type pattern classification and to generate the mixed-type patterns that have not occurred before. In the future, it is expected that this model addresses the problem of the lack of labeled data for defect pattern classification models.  © 2022 IEEE.","Deep learning; Domain Knowledge; Generative adversarial networks; Image classification; Pixels; Semiconductor device manufacture; Silicon wafers; Classification models; Defect patterns; Defects control; Images synthesis; Labeled data; Mixed type; On-wafer; Patterns classification; Semiconductor industry; Wafer bin map; Defects","generative adversarial network; image synthesis; pattern classification; wafer bin maps","Conference paper","Final","","Scopus","2-s2.0-85127652992"
"Che Aminudin M.F.; Setumin S.; Suandi S.A.","Che Aminudin, Muhamad Faris (57289820800); Setumin, Samsul (36718226000); Suandi, Shahrel Azmin (6504641613)","57289820800; 36718226000; 6504641613","Investigation on Face Alignment Factor for Generating Forensic Sketch Using Deep Convolutional Generative Adversarial Network","2022","Lecture Notes in Electrical Engineering","829 LNEE","","","858","863","5","10.1007/978-981-16-8129-5_131","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125220558&doi=10.1007%2f978-981-16-8129-5_131&partnerID=40&md5=3c91c6a7e32810b29bc3f270a9be5f11","Over the years, deep learning algorithm has been rapidly improved by various architectures and optimization. Convolutional neural network (CNN) is usually the prominent architecture used for deep learning due to its effectiveness. Recently, a new architecture which is Generative Adversarial Network (GAN), becomes a popular research topic. GAN can be categorized as a generative model that is usually used to generate data similar to data distribution of what it is trained. This type of generative model is used for prediction and generating new data similar to the original distribution. Deep convolutional GAN (DCGAN) has shown that it can generate images effectively by introducing convolution layers into normal GAN. This architecture helps in generating synthetic images where the data is limited. This paper investigates face sketch images synthesis to study how face alignment affects the generated image using the DCGAN. The results show that using aligned images as training data improved the generated images visually and when trained with classifier it shows that it has 88% similarity to original images compared to using unaligned images. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Classification (of information); Convolution; Convolutional neural networks; Data visualization; Deep learning; Digital forensics; Image enhancement; Learning algorithms; Network architecture; Convolutional neural network; Data distribution; Face alignment; Forensic sketches; Generative model; Images synthesis; Optimisations; Research topics; Sketch image; Synthetic images; Generative adversarial networks","Generative adversarial network; Image synthesis; Sketch images","Conference paper","Final","","Scopus","2-s2.0-85125220558"
"Bin Ismail M.H.; Razak T.R.; Gining R.A.J.M.; Fauzi S.S.M.","Bin Ismail, Mohammad Hafiz (35810300000); Razak, Tajul Rosli (56204395500); Gining, Ray Adderley J.M. (57194237068); Fauzi, Shukor Sanim Mohd (25654831000)","35810300000; 56204395500; 57194237068; 25654831000","Simulating Bruise and Defects on Mango images using Image-to-Image Translation Generative Adversarial Networks","2022","2022 3rd International Conference on Artificial Intelligence and Data Sciences: Championing Innovations in Artificial Intelligence and Data Sciences for Sustainable Future, AiDAS 2022 - Proceedings","","","","110","114","4","10.1109/AiDAS56890.2022.9918816","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141803897&doi=10.1109%2fAiDAS56890.2022.9918816&partnerID=40&md5=55a50f4ae61a4b91940d40af09a89068","A well-balanced dataset is essential for every computer vision task. However, the process of gathering data from various sources is laborious and time-consuming. The lack of samples and class imbalance will reduce the reliability of the CNN model in image classification and recognition tasks. In this research, we examine the use of Image-to-Image translation with conditional GAN for producing synthetic mango images with bruises. We introduce a conditional GAN for producing mango images with controlled surface defects, which is suited for dataset augmentation tasks within the fruit classification problem domain. The findings shows that our networks is able to generate mango images with bruises that are very close to the ground truth with FID value of 37.0  © 2022 IEEE.","Classification (of information); Computer vision; Deep learning; Fruits; Generative adversarial networks; Image classification; Balanced datasets; CGAN; Class imbalance; CNN models; Deep learning; Generative adversatial network; Image translation; Images synthesis; Mango; Well balanced; Surface defects","cGAN; Deep Learning; Generative Adversatial Network; image synthesis; mango","Conference paper","Final","","Scopus","2-s2.0-85141803897"
"Laxman K.; Dubey S.R.; Kalyan B.; Kojjarapu S.R.V.","Laxman, Kumarapu (57219686389); Dubey, Shiv Ram (55560436600); Kalyan, Baddam (57224781532); Kojjarapu, Satya Raj Vineel (57224767927)","57219686389; 55560436600; 57224781532; 57224767927","Efficient High-Resolution Image-to-Image Translation Using Multi-Scale Gradient U-Net","2022","Communications in Computer and Information Science","1567 CCIS","","","33","44","11","10.1007/978-3-031-11346-8_4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135018299&doi=10.1007%2f978-3-031-11346-8_4&partnerID=40&md5=6492935bcdff28bfa9e80649c7ae597d","Recently, Conditional Generative Adversarial Network (Conditional GAN) has shown very promising performance in several image-to-image translation applications. However, the uses of these conditional GANs are quite limited to low-resolution images, such as 256 × 256. The Pix2Pix-HD is a recent attempt to utilize the conditional GAN for high-resolution image synthesis. In this paper, we propose a Multi-Scale Gradient based U-Net (MSG U-Net) model for high-resolution image-to-image translation up to 2048 × 1024 resolution. The proposed model is trained by allowing the flow of gradients from multiple-discriminators to a single generator at multiple scales. The proposed MSG U-Net architecture leads to photo-realistic high-resolution image-to-image translation. Moreover, the proposed model is computationally efficient as compared to the Pix2Pix-HD with an improvement in the inference time nearly by 2.5 times. We provide the code of MSG U-Net model at https://github.com/laxmaniron/MSG-U-Net. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Computer vision; Computers; Conditional GAN; Gradient based; High-resolution images; Image translation; Image-to-image translation; Multi-scale gradient U-net; Multi-scales; Multiple scale; Net model; Pix2pix-HD; Generative adversarial networks","Conditional GANs; Image-to-image translation; Multi-Scale Gradient U-Net; Multiple scales; Pix2Pix-HD","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135018299"
"Han R.; Jones C.K.; Wu P.; Vagdargi P.; Zhang X.; Uneri A.; Lee J.; Luciano M.; Anderson W.S.; Helm P.; Siewerdsen J.H.","Han, R. (57194495726); Jones, C.K. (7408260049); Wu, P. (57202402764); Vagdargi, P. (57200629590); Zhang, X. (57221514879); Uneri, A. (36622501700); Lee, J. (57338524100); Luciano, M. (7006850166); Anderson, W.S. (13805238100); Helm, P. (7006042472); Siewerdsen, J.H. (7004498585)","57194495726; 7408260049; 57202402764; 57200629590; 57221514879; 36622501700; 57338524100; 7006850166; 13805238100; 7006042472; 7004498585","Deformable Registration of MRI to Intraoperative Cone-Beam CT of the Brain Using a Joint Synthesis and Registration Network","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12034","","1203407","","","","10.1117/12.2611783","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131930125&doi=10.1117%2f12.2611783&partnerID=40&md5=d300c16c17532bab7f723a755d82097e","Purpose: Neuro-endoscopic surgery requires accurate targeting of deep-brain structures in the presence of deep-brain deformations (up to 10 mm). We report a deep learning-based method to solve deformable MR-to-CBCT registration using a joint synthesis and registration (JSR) network. Method: The JSR network first encodes the MR and CBCT images into latent variables via MR and CBCT encoders, which are then decoded by two branches: image synthesis branches for MR-CT and CBCT-CT synthesis; and a registration branch for intra-modality registration in an intermediate (synthetic) CT domain. The two branches are jointly optimized, encouraging the encoders to extract features pertinent to both synthesis and registration. The algorithm was trained and tested on a dataset of 420 paired volumes presenting a wide range of simulated deformations. The JSR method was trained in a semi-supervised manner and evaluated in comparison to an alternative, state-of-the-art, inter-modality registration method (VoxelMorph). Results: The JSR method achieved Dice of 0.67 ± 0.11, surface distance error (SD) of 0.47 ± 0.26 mm, and target registration error (TRE) of 2.23 ± 0.80 mm in a simulation study - each superior to the alternative methods considered in this work. Moreover, JSR maintained diffeomorphism and exhibited a fast runtime of 2.55 ± 0.03 s. Conclusion: The JSR algorithm demonstrates accurate, near real-time deformable registration of preoperative MRI to intraoperative CBCT and is potentially suitable to intraoperative guidance of intracranial neurosurgery.  © 2022 SPIE.","Computerized tomography; Deep learning; Deformation; Endoscopy; Generative adversarial networks; Magnetic resonance imaging; Medical imaging; Signal encoding; Brain structure; Cone-beam CT; Deep learning; Deformable registration; Endoscopic surgery; Images synthesis; Intra-operative; Multimodal registration; Registration methods; Synthesis method; Neurosurgery","Deep Learning; Deformable Registration; Generative Adversarial Network; Image Synthesis; Multimodal Registration; Neurosurgery","Conference paper","Final","","Scopus","2-s2.0-85131930125"
"Ding G.; Han X.; Wang S.; Wu S.; Jin X.; Tu D.; Huang Q.","Ding, Guanqi (57551923900); Han, Xinzhe (57204470719); Wang, Shuhui (55904652300); Wu, Shuzhe (57199080264); Jin, Xin (57118641300); Tu, Dandan (57207566207); Huang, Qingming (8435766200)","57551923900; 57204470719; 55904652300; 57199080264; 57118641300; 57207566207; 8435766200","Attribute Group Editing for Reliable Few-shot Image Generation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11184","11193","9","10.1109/CVPR52688.2022.01091","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141757153&doi=10.1109%2fCVPR52688.2022.01091&partnerID=40&md5=96a607b45e1e07744992a02ad766e15e","Few-shot image generation is a challenging task even using the state-of-the-art Generative Adversarial Networks (GANs). Due to the unstable GAN training process and the limited training data, the generated images are often of low quality and low diversity. In this work, we propose a new 'editing-based' method, i.e., Attribute Group Editing (AGE), for few-shot image generation. The basic assumption is that any image is a collection of attributes and the editing direction for a specific attribute is shared across all categories. AGE examines the internal representation learned in GANs and identifies semantically meaningful directions. Specifically, the class embedding, i.e., the mean vector of the latent codes from a specific category, is used to represent the category-relevant attributes, and the category-irrelevant attributes are learned globally by Sparse Dictionary Learning on the difference between the sample embedding and the class embedding. Given a GAN well trained on seen categories, diverse images of unseen categories can be synthesized through editing category-irrelevant attributes while keeping category-relevant attributes unchanged. Without re-training the GAN, AGE is capable of not only producing more realistic and diverse images for downstream visual applications with limited data but achieving controllable image editing with interpretable category-irrelevant directions. Code is available at https://github.com/UniBester/AGE. © 2022 IEEE.","Computer vision; Embeddings; Embeddings; Image and video synthesis and generation; Image generations; Images synthesis; Long tail; Network training; State of the art; Transfer/low-shot/long-tail learning; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation; Transfer/low-shot/long-tail learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141757153"
"Tao M.; Tang H.; Wu F.; Jing X.; Bao B.-K.; Xu C.","Tao, Ming (57219565980); Tang, Hao (57208238003); Wu, Fei (57211431144); Jing, Xiaoyuan (7202420489); Bao, Bing-Kun (24922856800); Xu, Changsheng (7404181140)","57219565980; 57208238003; 57211431144; 7202420489; 24922856800; 7404181140","DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","16494","16504","10","10.1109/CVPR52688.2022.01602","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136334410&doi=10.1109%2fCVPR52688.2022.01602&partnerID=40&md5=08d9998ce94eaf94b80e3037222d97c8","Synthesizing high-quality realistic images from text descriptions is a challenging task. Existing text-to-image Generative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws. First, the stacked architecture introduces the entanglements between generators of different image scales. Second, existing studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks. Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which enhances the text-image semantic consistency without introducing extra networks, (iii) a novel deep text-image fusion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets. Code is available at https://github.com/tobran/DF-GAN. © 2022 IEEE.","Computer vision; Image enhancement; Image fusion; Network architecture; Semantics; Image and video synthesis and generation; Image scale; Image semantics; Images synthesis; Semantic consistency; Simple++; Text images; Video generation; Video synthesis; Vision + language; Generative adversarial networks","Image and video synthesis and generation; Vision + language","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85136334410"
"Hanne L.S.; Kundana R.; Thirukkumaran R.; Parvatikar Y.V.; Madhura K.","Hanne, Lakshmi S (57638400000); Kundana, R. (57641456400); Thirukkumaran, R. (57226553850); Parvatikar, Yagna Vikas (57641958200); Madhura, K. (57641456500)","57638400000; 57641456400; 57226553850; 57641958200; 57641456500","Text-To-Image Synthesis Using Modified GANs","2022","Proceedings - IEEE International Conference on Advances in Computing, Communication and Applied Informatics, ACCAI 2022","","","","","","","10.1109/ACCAI53970.2022.9752641","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128840110&doi=10.1109%2fACCAI53970.2022.9752641&partnerID=40&md5=147ee864e338ab5650f69483f4b0c20f","Synthesis of high-resolution images for the input textual descriptions is a prominent field of research that needs to be looked into, in this world of computer vision and animations. The task of image generation has its own challenges such as, working on the main object and background elements simultaneously, considering the ambiguity of natural language and reducing the noise that may be collected along with the input. It is also equally important to determine the importance of words given in the input textual description and ranking them according to their degree of relevance to desired image. Generative Adversarial Networks (GANs) are one of the newest developments for synthesis of images from text. In our literature review article, we have provided a consolidated idea of GAN s and how their variants are helpful in producing high resolution images.  © 2022 IEEE.","Discriminators; Generative adversarial networks; Image processing; Natural language processing systems; Generative adversarial network; Generator; High-resolution images; Image generations; Images synthesis; Main objects; Natural languages; Semantic correlation; Text-to image synthesis; Textual description; Semantics","Discriminator; Generative Adversarial Networks (GANs); Generator; high resolution images; natural language processing; Semantic correlation; Text-To Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85128840110"
"Zhan F.; Yu Y.; Zhang C.; Wu R.; Hu W.; Lu S.; Ma F.; Xie X.; Shao L.","Zhan, Fangneng (57204285913); Yu, Yingchen (57221916369); Zhang, Changgong (57211686620); Wu, Rongliang (57219491203); Hu, Wenbo (57204962589); Lu, Shijian (8439329200); Ma, Feiying (57219789232); Xie, Xuansong (57210375524); Shao, Ling (55643855000)","57204285913; 57221916369; 57211686620; 57219491203; 57204962589; 8439329200; 57219789232; 57210375524; 55643855000","GMLight: Lighting Estimation via Geometric Distribution Approximation","2022","IEEE Transactions on Image Processing","31","","","2268","2278","10","10.1109/TIP.2022.3151997","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125701304&doi=10.1109%2fTIP.2022.3151997&partnerID=40&md5=a043bdc5dbae879bbee506ea0105a445","Inferring the scene illumination from a single image is an essential yet challenging task in computer vision and computer graphics. Existing works estimate lighting by regressing representative illumination parameters or generating illumination maps directly. However, these methods often suffer from poor accuracy and generalization. This paper presents Geometric Mover's Light (GMLight), a lighting estimation framework that employs a regression network and a generative projector for effective illumination estimation. We parameterize illumination scenes in terms of the geometric light distribution, light intensity, ambient term, and auxiliary depth, which can be estimated by a regression network. Inspired by the earth mover's distance, we design a novel geometric mover's loss to guide the accurate regression of light distribution parameters. With the estimated light parameters, the generative projector synthesizes panoramic illumination maps with realistic appearance and high-frequency details. Extensive experiments show that GMLight achieves accurate illumination estimation and superior fidelity in relighting for 3D object insertion. The codes are available at https://github.com/fnzhan/Illumination-Estimation © 1992-2012 IEEE.","Generative adversarial networks; Geometry; Parameter estimation; Probability distributions; Regression analysis; Three dimensional computer graphics; Three dimensional displays; Ambients; Generalisation; Geometric distribution; Illumination estimation; Images synthesis; Light distribution; Light intensity; Lighting estimation; Single images; Three-dimensional display; article; distribution parameters; illumination; light intensity; Light sources","generative adversarial networks; image synthesis; Lighting estimation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125701304"
"Stumpo V.; Kernbach J.M.; van Niftrik C.H.B.; Sebök M.; Fierstra J.; Regli L.; Serra C.; Staartjes V.E.","Stumpo, Vittorio (57204320430); Kernbach, Julius M. (57194019689); van Niftrik, Christiaan H. B. (57087287200); Sebök, Martina (57201197649); Fierstra, Jorn (35777972500); Regli, Luca (7004240836); Serra, Carlo (50263014800); Staartjes, Victor E. (57190836125)","57204320430; 57194019689; 57087287200; 57201197649; 35777972500; 7004240836; 50263014800; 57190836125","Machine Learning Algorithms in Neuroimaging: An Overview","2022","Acta Neurochirurgica, Supplementum","134","","","125","138","13","10.1007/978-3-030-85292-4_17","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120867819&doi=10.1007%2f978-3-030-85292-4_17&partnerID=40&md5=079f2756ced47ad0681d6fc3847d9630","Machine learning (ML) and artificial intelligence (AI) applications in the field of neuroimaging have been on the rise in recent years, and their clinical adoption is increasing worldwide. Deep learning (DL) is a field of ML that can be defined as a set of algorithms enabling a computer to be fed with raw data and progressively discover—through multiple layers of representation—more complex and abstract patterns in large data sets. The combination of ML and radiomics, namely the extraction of features from medical images, has proven valuable, too: Radiomic information can be used for enhanced image characterization and prognosis or outcome prediction. This chapter summarizes the basic concepts underlying ML application for neuroimaging and discusses technical aspects of the most promising algorithms, with a specific focus on Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), in order to provide the readership with the fundamental theoretical tools to better understand ML in neuroimaging. Applications are highlighted from a practical standpoint in the last section of the chapter, including: image reconstruction and restoration, image synthesis and super-resolution, registration, segmentation, classification, and outcome prediction. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Algorithms; Artificial Intelligence; Image Processing, Computer-Assisted; Machine Learning; Neural Networks, Computer; architecture; back propagation; classification algorithm; clinical assessment; clinical classification; convolutional neural network; data availability; deep learning; developing country; echography; feature extraction; feature selection; generative adversarial network; human; hyperparameter optimization; image analysis; image reconstruction; image registration; image segmentation; information processing; kernel method; machine learning; medical education; medical student; network training; neuroimaging; neuroscience; nuclear magnetic resonance imaging; outcome assessment; physician; positron emission tomography; privacy; process optimization; radiomics; residual neural network; segmentation algorithm; standardization; synthesis; transfer of learning; workflow; x-ray computed tomography; algorithm; artificial intelligence; image processing","Classification; Convolutional neural network; Deep learning; Generative adversarial network; Machine learning; Segmentation","Book chapter","Final","","Scopus","2-s2.0-85120867819"
"Ulman V.; Wiesner D.","Ulman, Vladimír (23994564300); Wiesner, David (57211179074)","23994564300; 57211179074","Review of cell image synthesis for image processing","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","447","489","42","10.1016/B978-0-12-824349-7.00028-1","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137556427&doi=10.1016%2fB978-0-12-824349-7.00028-1&partnerID=40&md5=a310e75c77fdad230cdbde2067e3c9b0","Opposites attract, also in the biomedical field and during the processing of cell microscopy images. In the same spirit, image processing, the indispensable analyst tool, is often supported by image synthesis applications. Image synthesis is a methodology implemented in computer program intended to create artificial cell images similar to images from real microscopy. The generation of artificial images has had a stable tradition in image processing and is currently gaining more attention with the rising popularity of deep learning. This chapter reviews the current state of cell image synthesis, including terminology, broader context, goals, and peculiarities. It offers a brief historical introspection and, most importantly, surveys all contemporary methodology and applications. The light descriptions of procedural methods with explicit parameters and deep learning-based methods with implicit parameters, such as the generative adversarial networks, are also included. Last but not least, this chapter discusses what kind of artificial images and ground-truth data the methods generate, including the subsequent usage of this data for image processing such as cell segmentation or data augmentation for deep learning. Among the covered methods are approaches generating artificial cell microscopy images of fluorescence stained proteins, actin filaments, chromatin stained nuclei, membranes, and even populations of cells or full cells in differential inference contrast microscopy, to name a few. The generated data is often accompanied by ground truth annotation, whose forms are also discussed, including cell detection markers, full cell segmentation, and cell tracking data. © 2022 Elsevier Inc. All rights reserved.","","Biomedical application; Cell segmentation; Deep learning; Ground-truth data; Image synthesis","Book chapter","Final","","Scopus","2-s2.0-85137556427"
"Chen B.; Tan W.; Wang Y.; Zhao G.","Chen, Beijing (36805188500); Tan, Weijin (57315853000); Wang, Yiting (57204650261); Zhao, Guoying (47661917700)","36805188500; 57315853000; 57204650261; 47661917700","Distinguishing Between Natural and GAN-Generated Face Images by Combining Global and Local Features","2022","Chinese Journal of Electronics","31","1","","59","67","8","10.1049/cje.2020.00.372","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125133695&doi=10.1049%2fcje.2020.00.372&partnerID=40&md5=1146c52b527773eaf87df95b088a7d07","With the development of face image synthesis and generation technology based on generative adversarial networks (GANs), it has become a research hotspot to determine whether a given face image is natural or generated. However, the generalization capability of the existing algorithms is still to be improved. Therefore, this paper proposes a general algorithm. To do so, firstly, the learning on important local areas, containing many face key-points, is strengthened by combining the global and local features. Secondly, metric learning based on the ArcFace loss is applied to extract common and discriminative features. Finally, the extracted features are fed into the classification module to detect GAN-generated faces. The experiments are conducted on two publicly available natural datasets (CelebA and FFHQ) and seven GAN-generated datasets. Experimental results demonstrate that the proposed algorithm achieves a better generalization performance with an average detection accuracy over 0.99 than the state-of-the-art algorithms. Moreover, the proposed algorithm is robust against additional attacks, such as Gaussian blur, and Gaussian noise addition. © 2022 Chinese Institute of Electronics","Gaussian noise (electronic); Face image synthesis; Face images; Generalization capability; Generation technologies; Global feature; Hotspots; Image generations; Local areas; Local feature; Technology-based; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85125133695"
"Gnanha A.T.; Cao W.; Mao X.; Wu S.; Wong H.-S.; Li Q.","Gnanha, Aurele Tohokantche (57226117399); Cao, Wenming (57197825879); Mao, Xudong (54883408900); Wu, Si (55495122900); Wong, Hau-San (7402864844); Li, Qing (57199178903)","57226117399; 57197825879; 54883408900; 55495122900; 7402864844; 57199178903","The residual generator: An improved divergence minimization framework for GAN","2022","Pattern Recognition","121","","108222","","","","10.1016/j.patcog.2021.108222","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112128944&doi=10.1016%2fj.patcog.2021.108222&partnerID=40&md5=f9a6c5faef942ceb70df89067958eae2","GAN is a generative modelling framework which has been proven as able to minimise various types of divergence measures under an optimal discriminator. However, there is a gap between the loss function of GAN used in theory and in practice. In theory, the proof of the Jensen divergence minimisation involves the min-max criterion, but in practice the non-saturating criterion is instead used to avoid gradient vanishing. We argue that the formulation of divergence minimization via GAN is biased and may yield a poor convergence of the algorithm. In this paper, we propose the Residual Generator for GAN (Rg-GAN), which is inspired by the closed-loop control theory, to bridge the gap between theory and practice. Rg-GAN minimizes the residual between the loss of the generated data to be real and the loss of the generated data to be fake from the perspective of the discriminator. In this setting, the loss terms of the generator depend only on the generated data and therefore contribute to the optimisation of the model. We formulate the residual generator for standard GAN and least-squares GAN and show that they are equivalent to the minimisation of reverse-KL divergence and a novel instance of f-divergence, respectively. Furthermore, we prove that Rg-GAN can be reduced to Integral Probability Metrics (IPMs) GANs (e.g., Wasserstein GAN) and bridge the gap between IPMs and f-divergence. Additionally, we further improve on Rg-GAN by proposing a loss function for the discriminator that has a better discrimination ability. Experiments on synthetic and natural images data sets show that Rg-GAN is robust to mode collapse, and improves the generation quality of GAN in terms of FID and IS scores. © 2021 Elsevier Ltd","Deep learning; Image enhancement; Deep learning; Divergence measures; Generative adversarial network; Generative model; Images synthesis; Loss functions; Minimisation; Modelling framework; Probability metrics; Residual generator; Discriminators","Deep learning; Generative adversarial networks; Image synthesis","Article","Final","","Scopus","2-s2.0-85112128944"
"Georgopoulos M.; Oldfield J.; Chrysos G.G.; Panagakis Y.","Georgopoulos, Markos (57193539057); Oldfield, James (57211991046); Chrysos, Grigorios G (57188639985); Panagakis, Yannis (35503932300)","57193539057; 57211991046; 57188639985; 35503932300","Cluster-guided Image Synthesis with Unconditional Models","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11533","11542","9","10.1109/CVPR52688.2022.01125","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141791694&doi=10.1109%2fCVPR52688.2022.01125&partnerID=40&md5=57fd6f20c3d44098b4b007a044ed7457","Generative Adversarial Networks (GANs) are the driving force behind the state-of-the-art in image generation. Despite their ability to synthesize high-resolution photo-realistic images, generating content with on-demand conditioning of different granularity remains a challenge. This challenge is usually tackled by annotating massive datasets with the attributes of interest, a laborious task that is not always a viable option. Therefore, it is vital to introduce control into the generation process of unsupervised generative models. In this work, we focus on controllable image generation by leveraging GANs that are well-trained in an unsupervised fashion. To this end, we discover that the representation space of intermediate layers of the generator forms a number of clusters that separate the data according to semantically meaningful attributes (e.g., hair color and pose). By conditioning on the cluster assignments, the proposed method is able to control the semantic class of the generated image. Our approach enables sampling from each cluster by Implicit Maximum Likelihood Estimation (IMLE). We showcase the efficacy of our approach on faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using different pre-trained generative models. The results highlight the ability of our approach to condition image generation on attributes like gender, pose and hair style on faces, as well as a variety of features on different object classes. © 2022 IEEE.","Generative adversarial networks; Maximum likelihood estimation; Semantics; Driving forces; Explainable computer vision; Generative model; Guided images; Image and video synthesis and generation; Image generations; Images synthesis; State of the art; Video generation; Video synthesis; Computer vision","Explainable computer vision; Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141791694"
"Wang C.; Pei J.; Liu X.; Huang Y.; Mao D.; Zhang Y.; Yang J.","Wang, Chenwei (57211242566); Pei, Jifang (55787739300); Liu, Xiaoyu (57222261818); Huang, Yulin (23014806800); Mao, Deqing (57194656090); Zhang, Yin (55975581400); Yang, Jianyu (9239230100)","57211242566; 55787739300; 57222261818; 23014806800; 57194656090; 55975581400; 9239230100","SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","9381","9397","16","10.1109/JSTARS.2022.3218369","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141613271&doi=10.1109%2fJSTARS.2022.3218369&partnerID=40&md5=559a1c414ff5622e9bebfb6fb4462040","Sufficient synthetic aperture radar (SAR) target images are very important for the development of research works. However, available SAR target images are often limited in practice, which hinders the progress of SAR application. In this article, we propose an azimuth-controllable generative adversarial network to generate precise SAR target images with an intermediate azimuth between two given SAR images' azimuths. This network mainly contains three parts: 1) generator, 2) discriminator, and 3) predictor. Through the proposed specific network structure, the generator can extract and fuse the optimal target features from two input SAR target images to generate an SAR target image. Then, a similarity discriminator and an azimuth predictor are designed. The similarity discriminator can differentiate the generated SAR target images from the real SAR images to ensure the accuracy of the generated while the azimuth predictor measures the difference of azimuth between the generated and the desired to ensure the azimuth controllability of the generated. Therefore, the proposed network can generate precise SAR images, and their azimuths can be controlled well by the inputs of the deep network, which can generate the target images in different azimuths to solve the small sample problem to some degree and benefit the research works of SAR images. Extensive experimental results show the superiority of the proposed method in azimuth controllability and accuracy of SAR target image generation.  © 2008-2012 IEEE.","Automatic target recognition; Deep learning; Radar imaging; Radar target recognition; Synthetic aperture radar; Automatic target recognition; Azimuth; Azimuth-controllable; Deep learning; Generative adversarial network; Generator; Image generations; Images synthesis; Radar polarimetry; Synthetic aperture radar; Target image generation; Target images; Target recognition; azimuth; experimental study; image analysis; image classification; machine learning; synthetic aperture radar; Generative adversarial networks","Automatic target recognition (ATR); azimuth-controllable; deep learning; generative adversarial network (GAN); synthetic aperture radar (SAR); target image generation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141613271"
"Osahor U.; Nasrabadi N.M.","Osahor, Uche (57211069120); Nasrabadi, Nasser M. (7006312852)","57211069120; 7006312852","Text-Guided Sketch-to-Photo Image Synthesis","2022","IEEE Access","10","","","98278","98289","11","10.1109/ACCESS.2022.3206771","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139206323&doi=10.1109%2fACCESS.2022.3206771&partnerID=40&md5=010c27c930661c1e091d87c6b1ea39b1","We propose a text-guided sketch-to-image synthesis model that semantically mixes style and content features from the latent space of an inverted Generative Adversarial Network (GAN). Our goal is to synthesize plausible images from human facial sketches and their respective text descriptions. In our approach, we adapted a generative model termed Contextual GAN (CT-GAN) that efficiently encodes visual-linguistic semantic features pre-trained on over 400 million text-image pairs at different resolutions along the model. Also, we introduced an intermediate mapping network called c-Map that combines textual and visual-based features to a disentangled latent space W+ for better feature matching. Furthermore to maximise the computational performance of our model, we implemented a linear-based attention scheme along the pipeline of our model to eliminate the drawbacks of inefficient attention modules that are quadratic in complexity. Finally, the hierarchical setting of our model ensures that textual, style and content features are synthesised based on their unique fine grained details, which result in visually appealing images.  © 2013 IEEE.","Computerized tomography; Semantics; Contextual generative adversarial network; Generative adversarial network; Generative model; Images synthesis; Linguistic semantics; Photo images; Semantic features; Style and contents; Synthesis models; Text-guided sketch-to-image synthesis; Generative adversarial networks","Contextual GAN (CT-GAN); generative adversarial network (GAN); text-guided sketch-to-image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139206323"
"Yang J.; Zhao Q.; Lyu Z.","Yang, Jingbo (57331200100); Zhao, Qijun (57221157913); Lyu, Zejun (55823616300)","57331200100; 57221157913; 55823616300","Synthesis of the expression image and its application under the dimentional emotion model; [维度情感模型下的表情图像生成及应用]","2021","Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University","48","5","","30","37","7","10.19665/j.issn1001-2400.2021.05.005","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118917177&doi=10.19665%2fj.issn1001-2400.2021.05.005&partnerID=40&md5=92dbb92e7fc4c9cd544ae9d828a7542a","In order to solve the problem that the training data of deep learning based facial expression recognition methods usually cover a limited part of the expression space and have an imbalanced distribution, we propose AV-GAN, a facial expression image synthesis method in Arousal-Valence dimensional emotion space, based on the generative adversarial network, to generate more diverse and balanced facial expression training data. The method uses label distribution to represent the expression for the face image, and employs an identity control module, an expression control module, and adversarial learning to realize the random sampling and generation of expression images in Arousal-Valence space. Evaluations on Oulu-CASIA database show that the accuracy of the recognition of the facial expression using the proposed method to augment training data is increased by 6.5%, compared with that using the original training data. It is proved that the proposed method can effectively improve the facial expression recognition accuracy under imbalanced training data. © 2021, The Editorial Board of Journal of Xidian University. All right reserved.","Deep learning; Face recognition; Arousal-valence emotion model; Control module; Data augmentation; Emotion models; Facial expression recognition; Facial Expressions; Image applications; Image generations; ITS applications; Training data; Generative adversarial networks","Arousal-Valence emotion model; Data augmentation; Facial expression recognition; Generative adversarial network; Image generation","Article","Final","","Scopus","2-s2.0-85118917177"
"Liu D.; Wu L.; Zheng F.; Liu L.; Wang M.","Liu, Deyin (57487238900); Wu, Lin (57745188500); Zheng, Feng (57210574274); Liu, Lingqiao (54956172800); Wang, Meng (57199061949)","57487238900; 57745188500; 57210574274; 54956172800; 57199061949","Verbal-Person Nets: Pose-Guided Multi-Granularity Language-to-Person Generation","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2022.3151631","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126293356&doi=10.1109%2fTNNLS.2022.3151631&partnerID=40&md5=fb2057a520a1feffa9763edcfd73259d","Person image generation conditioned on natural language allows us to personalize image editing in a user-friendly manner. This fashion, however, involves different granularities of semantic relevance between texts and visual content. Given a sentence describing an unknown person, we propose a novel pose-guided multi-granularity attention architecture to synthesize the person image in an end-to-end manner. To determine what content to draw at a global outline, the sentence-level description and pose feature maps are incorporated into a U-Net architecture to generate a coarse person image. To further enhance the fine-grained details, we propose to draw the human body parts with highly correlated textual nouns and determine the spatial positions with respect to target pose points. Our model is premised on a conditional generative adversarial network (GAN) that translates language description into a realistic person image. The proposed model is coupled with two-stream discriminators: 1) text-relevant local discriminators to improve the fine-grained appearance by identifying the region-text correspondences at the finer manipulation and 2) a global full-body discriminator to regulate the generation via a pose-weighting feature selection. Extensive experiments conducted on benchmarks validate the superiority of our method for person image generation. IEEE","Image processing; Network architecture; Semantics; Translation (languages); Fine grained; Fine-grained generation; Generative adversarial network; Generator; Human pose; Image generations; Image translation; Images synthesis; Person image generation; Text-to-image translation.; Generative adversarial networks","Computer science; Electronic mail; Fine-grained generation; Generative adversarial networks; generative adversarial networks (GANs); Generators; human poses; Image synthesis; person image generation; Semantics; text-to-image translation.; Visualization","Article","Article in press","","Scopus","2-s2.0-85126293356"
"Wang J.; Yang C.; Xu Y.; Shen Y.; Li H.; Zhou B.","Wang, Jianyuan (57221359811); Yang, Ceyuan (57204283344); Xu, Yinghao (57219692967); Shen, Yujun (57207766466); Li, Hongdong (57372338200); Zhou, Bolei (36697366200)","57221359811; 57204283344; 57219692967; 57207766466; 57372338200; 36697366200","Improving GAN Equilibrium by Raising Spatial Awareness","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11275","11283","8","10.1109/CVPR52688.2022.01100","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138911231&doi=10.1109%2fCVPR52688.2022.01100&partnerID=40&md5=c2c3a4011ef681f8eff0ea1719d07807","The success of Generative Adversarial Networks (GANs) is largely built upon the adversarial training between a generator (G) and a discriminator (D). They are expected to reach a certain equilibrium where D cannot distinguish the generated images from the real ones. However, such an equilibrium is rarely achieved in practical GAN training, instead, D almost always surpasses G. We attribute one of its sources to the information asymmetry between D and G. We observe that D learns its own visual attention when determining whether an image is real or fake, but G has no explicit clue on which regions to focus on for a particular synthesis. To alleviate the issue of D dominating the competition in GANs, we aim to raise the spatial awareness of G. Randomly sampled multi-level heatmaps are encoded into the intermediate layers of G as an inductive bias. Thus G can purposefully improve the synthesis of certain image regions. We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance. As a byproduct, the intro-duced spatial awareness facilitates interactive editing over the output synthesis. Demo video and code are available at https://genforce.github.io/eqgan-sa/ © 2022 IEEE.","Behavioral research; Computer vision; Game theory; Image enhancement; Image and video synthesis and generation; Images synthesis; Information asymmetry; Learn+; Network equilibrium; Network training; Spatial awareness; Video generation; Video synthesis; Visual Attention; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138911231"
"Peng J.; Zhou Y.; Sun X.; Cao L.; Wu Y.; Huang F.; Ji R.","Peng, Jun (57205601009); Zhou, Yiyi (57199493915); Sun, Xiaoshuai (24278895900); Cao, Liujuan (35749499000); Wu, Yongjian (57192395876); Huang, Feiyue (56452085300); Ji, Rongrong (23134935200)","57205601009; 57199493915; 24278895900; 35749499000; 57192395876; 56452085300; 23134935200","Knowledge-Driven Generative Adversarial Network for Text-To-Image Synthesis","2022","IEEE Transactions on Multimedia","24","","","4356","4366","10","10.1109/TMM.2021.3116416","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118638372&doi=10.1109%2fTMM.2021.3116416&partnerID=40&md5=b34234e65190d010f1456583ba7b8e24","Text-To-Image (T2I) synthesis is a challenging task that aims to convert natural language descriptions to real images. It remains an open problem mainly due to the diversity of text descriptions, which poses a huge obstacle in generating vivid and relevant images. Moreover, the existing evaluation metrics in T2I synthesis are mainly used to evaluate the visual quality of the generated images, while the semantic consistency between the two modalities is often ignored. To address these issues, we present a novel Knowledge-Driven Generative Adversarial Network, termed KD-GAN, and a new evaluation system, named Pseudo Turing Test (PTT for short). Concretely, KD-GAN takes a further step in imitating the behavior of human painting, i.e., drawing an image according to reference knowledge. The introduction of reference knowledge in KD-GAN not only improves the quality of the generated images but also enhances the semantic consistency between them and the input texts. In addition, KD-GAN can also greatly avoid some flaws against common sense during image generation, e.g., skiing in the blue sky. The proposed PTT is an important supplement to the existing evaluation system of T2I synthesis. It includes a set of pseudo-experts of different multimedia tasks to evaluate the semantic consistency between the given texts and the generated images. To validate the proposed KD-GAN, we conducted extensive experiments on two benchmark datasets, i.e., Caltech-UCSD Birds (CUB), and MS-COCO (COCO). The experimental results demonstrate that KD-GAN outperforms state-of-The-Art methods on IS, FID, and the proposed PTT metrics.11The codes of KD-GAN are at [Online].  © 1999-2012 IEEE.","Behavioral research; Computer vision; Generative adversarial networks; Image enhancement; Job analysis; Quality control; Features extraction; Images synthesis; Knowledge-driven; Language description; Natural languages; Pseudo turing test; Semantic consistency; Task analysis; Text-to-image; Turing tests; Semantics","Generative adversarial network; Knowledge-driven; Pseudo turing test; Text-To-iMAGe","Article","Final","","Scopus","2-s2.0-85118638372"
"Sharan L.; Romano G.; Koehler S.; Kelm H.; Karck M.; De Simone R.; Engelhardt S.","Sharan, Lalith (57212008360); Romano, Gabriele (57209205312); Koehler, Sven (57216908384); Kelm, Halvar (57226199195); Karck, Matthias (7006019086); De Simone, Raffaele (7006266889); Engelhardt, Sandy (56200795700)","57212008360; 57209205312; 57216908384; 57226199195; 7006019086; 7006266889; 56200795700","Mutually Improved Endoscopic Image Synthesis and Landmark Detection in Unpaired Image-to-Image Translation","2022","IEEE Journal of Biomedical and Health Informatics","26","1","","127","138","11","10.1109/JBHI.2021.3099858","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112626215&doi=10.1109%2fJBHI.2021.3099858&partnerID=40&md5=ced45844f55ce5bfc2bc372e362dd35d","The CycleGAN framework allows for unsupervised image-to-image translation of unpaired data. In a scenario of surgical training on a physical surgical simulator, this method can be used to transform endoscopic images of phantoms into images which more closely resemble the intra-operative appearance of the same surgical target structure. This can be viewed as a novel augmented reality approach, which we coined Hyperrealism in previous work. In this use case, it is of paramount importance to display objects like needles, sutures or instruments consistent in both domains while altering the style to a more tissue-like appearance. Segmentation of these objects would allow for a direct transfer, however, contouring of these, partly tiny and thin foreground objects is cumbersome and perhaps inaccurate. Instead, we propose to use landmark detection on the points when sutures pass into the tissue. This objective is directly incorporated into a CycleGAN framework by treating the performance of pre-trained detector models as an additional optimization goal. We show that a task defined on these sparse landmark labels improves consistency of synthesis by the generator network in both domains. Comparing a baseline CycleGAN architecture to our proposed extension (DetCycleGAN), mean precision (PPV) improved by $+61.32$, mean sensitivity (TPR) by $+37.91$, and mean $F_1$ score by $+0.4743$. Furthermore, it could be shown that by dataset fusion, generated intra-operative images can be leveraged as additional training data for the detection network itself.  © 2013 IEEE.","Endoscopy; Humans; Image Processing, Computer-Assisted; Phantoms, Imaging; Augmented reality; Endoscopy; HTTP; Surgery; Surgical equipment; Tissue; Detection networks; Foreground objects; Image translation; Landmark detection; Mean sensitivity; Optimization goals; Surgical simulators; Surgical training; Article; cross validation; deep learning; endoscopy; histogram; human; image analysis; image segmentation; intraoperative period; learning algorithm; minimally invasive surgery; mitral valve repair; network analysis; random forest; surgical training; task performance; image processing; imaging phantom; procedures; Image enhancement","CycleGAN; Generative adversarial networks; landmark detection; landmark localization; mitral valve repair; surgical simulation; surgical training","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112626215"
"Zhang Y.; Han S.; Zhang Z.; Wang J.; Bi H.","Zhang, Yubo (39763341600); Han, Shuang (57443527900); Zhang, Zhongxin (57443915200); Wang, Jianyang (57221705454); Bi, Hongbo (25026660900)","39763341600; 57443527900; 57443915200; 57221705454; 25026660900","CF-GAN: cross-domain feature fusion generative adversarial network for text-to-image synthesis","2022","Visual Computer","","","","","","","10.1007/s00371-022-02404-6","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124230130&doi=10.1007%2fs00371-022-02404-6&partnerID=40&md5=7f6d272bd170ba27d865afc106034d23","In recent years, generative adversarial networks have successfully synthesized images through text descriptions. However, there are still problems that the generated image cannot be deeply embedded in the text description semantics, the target object of the generated image is incomplete, and the texture structure of the target object is not rich enough. Consequently, we propose a network framework, cross-domain feature fusion generative adversarial network (CF-GAN), which includes two modules, feature fusion-enhanced response module (FFERM) and multi-branch residual module (MBRM), to fine-grain the generated images with the way of deep fusion. FFERM can integrate both the word-level vector features and image features deeply. MBRM is a relatively simple and innovative residual network structure instead of the traditional residual module to extract features fully. We conducted experiments on the CUB and COCO datasets, and the results reveal that the Inception Score has improved from 4.36 to 4.83 (increased by 10.78%) on the CUB dataset, compared with AttnGAN. Compared with DM-GAN, the Inception Score has increased from 30.49 to 31.13 (increased by 2.06%) on the COCO dataset. Extensive experiments and ablation studies demonstrate the proposed CF-GAN’s superiority compared to other methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Image enhancement; Image fusion; Semantics; Textures; Cross-domain; Deep learning; Domain feature; Features fusions; Images synthesis; Residual structure; Synthesized images; Target object; Text-to-image; Texture structure; Generative adversarial networks","Deep learning; Generative adversarial networks; Residual structure; Text-to-image","Article","Article in press","","Scopus","2-s2.0-85124230130"
"de Lima D.C.; Saqui D.; Mpinda S.A.T.; Saito J.H.","de Lima, Daniel Caio (57209397114); Saqui, Diego (56028559700); Mpinda, Steve Ataky Tsham (56786495100); Saito, José Hiroki (7102105877)","57209397114; 56028559700; 56786495100; 7102105877","Pix2Pix Network to Estimate Agricultural Near Infrared Images from RGB Data; [  Un réseau Pix2Pix pour générer des images dans le proche infrarouge en zones agricoles à partir de données RVB]","2022","Canadian Journal of Remote Sensing","48","2","","299","315","16","10.1080/07038992.2021.2016056","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123681785&doi=10.1080%2f07038992.2021.2016056&partnerID=40&md5=efc295efee187f1a7c7976414a8dfaeb","Remote sensing has been applied to agriculture, making it possible to acquire a large amount of data far away from crops, providing information for decision making by producers that can impact production costs and crops quality. One way of getting the production information is through vegetation indices, arithmetic operations that use spectral bands, especially the Near Infrared (NIR). However, sensors that capture this spectral information are very expensive for small producers to afford it. In a previous article, a pixel-to-pixel image synthesis model to estimate NIR images from RGB data using hyperspectral endmembers (pure hyperspectral signatures) was described. In this work, an image-to-image synthesis model, known as Pix2Pix, is used for estimating NIR images from low-cost RGB camera images. Pix2Pix is a kind of Generative Adversarial Networks (GANs), composed by two neural networks, a generator (G) and a discriminator (D), that compete. G learns to create images from a random noise inputs and D learns to verify if these images are real or fake. The results showed that the presented method generated NIR images quite similar to real ones, reaching a value of 0.912 on M3SIM similarity metric, outperforming results obtained with the previous endmembers method (0.775 on M3SIM). ©, Copyright © CASI.","Costs; Crops; Decision making; Infrared devices; Pixels; Remote sensing; Crop quality; Decisions makings; Endmembers; Images synthesis; Large amounts of data; Learn+; Near- infrared images; Production cost; Remote-sensing; Synthesis models; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85123681785"
"Chen L.; Zhao C.; Huang X.; Wang Y.; Deng J.","Chen, Lingjun (57224216829); Zhao, Caidan (23669845300); Huang, Xiangyu (57607026100); Wang, Yilin (57222662012); Deng, Junjie (57216971556)","57224216829; 23669845300; 57607026100; 57222662012; 57216971556","Dehazing Algorithm Based on Multi-Scale Feature Extraction","2022","Proceedings of SPIE - The International Society for Optical Engineering","12247","","122471R","","","","10.1117/12.2636944","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131921073&doi=10.1117%2f12.2636944&partnerID=40&md5=b8231791983587f36c380cfd6d559e68","Fog seriously affects the visual perception of human eyes and reduces the quality of captured images. This paper proposes a dehazing Generative Adversarial Network based on multi-scale feature extraction. The method is an end-to-end dehazing network that avoids the dependence on physical models. By adding the edge feature extraction module to the generator network to obtain the high-frequency information of the foggy image, the attention to the edge information of the image is effectively improved. In addition, the multi-scale features of the image are extracted, and then the foggy image is enhanced by a unique feature fusion mechanism. The discriminator network uses the global discriminator and the local discriminator to make a joint judgement, which further improves the dehazing performance. Compared with state-of-the-art approaches available in the literature, the algorithm proposed in this paper obtains better subjective and objective image quality evaluation on the cityscape foggy image synthesis dataset. © 2022 SPIE.","Demulsification; Discriminators; Extraction; Generative adversarial networks; Image enhancement; Image fusion; Image quality; Dehazing; End to end; Features extraction; Features fusions; Human eye; Image dehazing; Multi-scale features; Network-based; Physical modelling; Visual perception; Feature extraction","feature extraction; feature fusion; generative adversarial network; image dehazing; Image enhancement","Conference paper","Final","","Scopus","2-s2.0-85131921073"
"Qiao S.; Pan S.; Luo G.; Pang S.; Chen T.; Singh A.K.; Lv Z.","Qiao, Sibo (57209331290); Pan, Silin (14627564000); Luo, Gang (57214385230); Pang, Shanchen (56257339200); Chen, Taotao (57193424451); Singh, Amit Kumar (55726466900); Lv, Zhihan (55925162500)","57209331290; 14627564000; 57214385230; 56257339200; 57193424451; 55726466900; 55925162500","A Pseudo-Siamese Feature Fusion Generative Adversarial Network for Synthesizing High-quality Fetal Four-chamber Views","2022","IEEE Journal of Biomedical and Health Informatics","","","","","","","10.1109/JBHI.2022.3143319","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123374390&doi=10.1109%2fJBHI.2022.3143319&partnerID=40&md5=f103dbf986032e778b9be8d85a676438","Four-chamber (FC) views are the primary ultrasound (US) images that cardiologists diagnose whether the fetus has congenital heart disease (CHD) in prenatal diagnosis and screening. FC views intuitively depict the developmental morphology of the fetal heart. Early diagnosis of fetal CHD has always been the focus and difficulty of prenatal screening. Furthermore, deep learning technology has achieved great success in medical image analysis. Hence, applying deep learning technology in the early screening of fetal CHD helps improve diagnostic accuracy. However, the lack of large-scale and high-quality fetal FC views brings incredible difficulties to deep learning models or cardiologists. Hence, we propose a Pseudo-Siamese Feature Fusion Generative Adversarial Network (PSFFGAN), synthesizing high-quality fetal FC views using FC sketch images. In addition, we propose a novel Triplet Generative Adversarial Loss Function (TGALF), which optimizes PSFFGAN to fully extract the cardiac anatomical structure information provided by FC sketch images to synthesize the corresponding fetal FC views with speckle noises, artifacts, and other ultrasonic characteristics. The experimental results show that the fetal FC views synthesized by our proposed PSFFGAN have the best objective evaluation values: SSIM of 0.4627, MS-SSIM of 0.6224, and FID of 83.92, respectively. More importantly, two professional cardiologists evaluate healthy FC views and CHD FC views synthesized by our PSFFGAN, giving a subjective score that the average qualified rate is 82% and 79%, respectively, which further proves the effectiveness of the PSFFGAN. IEEE","Cardiology; Deep learning; Diagnosis; Diseases; Heart; Image fusion; Medical imaging; Speckle; Congenital heart disease; Deep learning; Deep) learning; Features extraction; Fetal four-chamber view; Fetal fourchamber sketch image; GAN; Generator; Images synthesis; Generative adversarial networks","Congenital heart disease; Deep learning; Deep) learning; Feature extraction; Fetal four-chamber views; Fetal fourchamber sketch images; GAN; Generative adversarial networks; Generators; Image synthesis; Images synthesis; Speckle; Training","Article","Article in press","","Scopus","2-s2.0-85123374390"
"Feng F.; Niu T.; Li R.; Wang X.","Feng, Fangxiang (56432140300); Niu, Tianrui (57217247447); Li, Ruifan (13608752200); Wang, Xiaojie (35235644100)","56432140300; 57217247447; 13608752200; 35235644100","Modality Disentangled Discriminator for Text-To-Image Synthesis","2022","IEEE Transactions on Multimedia","24","","","2112","2124","12","10.1109/TMM.2021.3075997","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105116613&doi=10.1109%2fTMM.2021.3075997&partnerID=40&md5=edddc3a707384740360c36e44dc26c54","Text-To-image (T2I) synthesis aims at generating photo-realistic images from text descriptions, which is a particularly important task in bridging vision and language. Each generated image consists of two parts: The content part related to the text and the style part irrelevant to the text. The existing discriminator does not distinguish between the content part and the style part. This not only precludes the T2I synthesis models from generating the content part effectively but also makes it difficult to manipulate the style of the generated image. In this paper, we propose a modality disentangled discriminator that distinguishes between the content part and the style part at a specific layer. Specifically, we enforce the early layers of a certain number in the discriminator to become the disentangled representation extractor through two losses. The extracted common representation for the content part can make the discriminator more effective for capturing the text-image correlation, while the extracted modality-specific representation for the style part can be directly transferred to other images. The combination of these two representations can also improve the quality of the generated images. Our proposed discriminator is used to substitute the discriminator of each stage in the representative model AttnGAN and the SOTA model DM-GAN. Extensive experiments are conducted on three widely used datasets, i.e. CUB, Oxford-102, and COCO, for the T2I synthesis task, demonstrating the superior performance of the modality disentangled discriminator over the base models. Code for DM-GAN with our modality disentangled discriminator is available at https://github.com/FangxiangFeng/DM-GAN-MDD. © 1999-2012 IEEE.","Discriminators; Base models; Image synthesis; Photorealistic images; Synthesis models; Text images; Image enhancement","generative adversarial networks; multi-modal disentangled representation learning; text-To-image synthesis","Article","Final","","Scopus","2-s2.0-85105116613"
"Lei Y.; Qiu R.L.J.; Wang T.; Curran W.J., Jr.; Liu T.; Yang X.","Lei, Yang (57202715941); Qiu, Richard L.J. (45661598900); Wang, Tonghe (57189639465); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Yang, Xiaofeng (36712893800)","57202715941; 45661598900; 57189639465; 57203070877; 26643332700; 36712893800","Generative adversarial networks for medical image synthesis","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","105","128","23","10.1016/B978-0-12-824349-7.00014-1","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137605517&doi=10.1016%2fB978-0-12-824349-7.00014-1&partnerID=40&md5=8386a5de7426fecb7b74357b965c2a84","This chapter reviews recent developments of generative adversarial network (GAN)-based methods for medical and biomedical image synthesis tasks. These methods are classified into GAN, conditional GAN (cGAN), and cycle-consistent GAN (Cycle-GAN) according to the network architecture designs. For each category, a literature survey is given, which covers discussions of the network architecture designs, loss functions used to supervise the network, and challenges of training the network. We then introduce some practical aspects of these GAN-based methods, such as network setting for different tasks' aim, image pre-processing to enhance the input image quality, and data augmentation to enlarge the training data variation. We also briefly introduce some specific applications of cGAN and Cycle-GAN. Finally, a conclusion with highlighted important contributions and discussion of some identified specific challenges for GAN-based methods are given. © 2022 Elsevier Inc. All rights reserved.","","Conditional GAN; Cycle-GAN; Generative adversarial network (GAN); Loss; Medical image synthesis; Network architecture design","Book chapter","Final","","Scopus","2-s2.0-85137605517"
"Che Aminudin M.F.; Suandi S.A.","Che Aminudin, Muhamad Faris (57289820800); Suandi, Shahrel Azmin (6504641613)","57289820800; 6504641613","Review on Generative Adversarial Neural Networks (GAN) in Text-to-Image Synthesis","2022","Lecture Notes in Electrical Engineering","829 LNEE","","","878","883","5","10.1007/978-981-16-8129-5_134","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125258351&doi=10.1007%2f978-981-16-8129-5_134&partnerID=40&md5=b66f89899597a01ecdbafd159e676d06","In recent years there is a spike in text-to-image synthesis research. Most of the researches use Generative Adversarial Network (GAN) because of its effectiveness in generating a realistic synthetic image. In this paper, we provide several recent papers that focus on GAN based text-to-image synthesis and discuss their architecture, advantages of the model, dataset, and evaluation metric. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Image processing; Evaluation metrics; Images synthesis; Network-based; Neural-networks; Research use; Synthetic images; Text-to-image synthesis; Generative adversarial networks","Generative adversarial network; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85125258351"
"Waqas N.; Safie S.I.; Kadir K.A.; Khan S.; Kaka Khel M.H.","Waqas, Nawaf (57267542500); Safie, Sairul Izwan (53980320900); Kadir, Kushsairy Abdul (37079357500); Khan, Sheroz (57370839900); Kaka Khel, Muhammad Haris (57809734600)","57267542500; 53980320900; 37079357500; 57370839900; 57809734600","DEEPFAKE Image Synthesis for Data Augmentation","2022","IEEE Access","10","","","80847","80857","10","10.1109/ACCESS.2022.3193668","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135767272&doi=10.1109%2fACCESS.2022.3193668&partnerID=40&md5=a640976192a0bc1ef1a020bf94f6a083","Field of medical imaging is scarce in terms of a dataset that is reliable and extensive enough to train distinct supervised deep learning models. One way to tackle this problem is to use a Generative Adversarial Network to synthesize DEEPFAKE images to augment the data. DEEPFAKE refers to the transfer of important features from the source image (or video) to the target image (or video), such that the target modality appears to animate the source almost close to reality. In the past decade, medical image processing has made significant advances using the latest state-of-art-methods of deep learning techniques. Supervised deep learning models produce super-human results with the help of huge amount of dataset in a variety of medical image processing and deep learning applications. DEEPFAKE images can be a useful in various applications like translating to different useful and sometimes malicious modalities, unbalanced datasets or increasing the amount of datasets. In this paper the data scarcity has been addressed by using Progressive Growing Generative Adversarial Networks (PGGAN). However, PGGAN consists of convolution layer that suffers from the training-related issues. PGGAN requires a large number of convolution layers in order to obtain high-resolution image training, which makes training a difficult task. In this work, a subjective self-attention layer has been added before 256 × 256 convolution layer for efficient feature learning and the use of spectral normalization in the discriminator and pixel normalization in the generator for training stabilization - the two tasks resulting into what is referred to as Enhanced-GAN. The performance of Enhanced-GAN is compared to PGGAN performance using the parameters of AM Score and Mode Score. In addition, the strength of Enhanced-GAN and PGGAN synthesized data is evaluated using the U-net supervised deep learning model for segmentation tasks. Dice Coefficient metrics show that U-net trained on Enhanced-GAN DEEPFAKE data optimized with real data performs better than PGGAN DEEPFAKE data with real data.  © 2013 IEEE.","Computer vision; Convolution; Deep learning; Generative adversarial networks; Job analysis; Medical imaging; Personnel training; Biomedical imaging; Data augmentation; Deepfake; Images synthesis; Progressive growing generative adversarial network; Self-attention layer; Spectral normalization; Task analysis; Unbalanced datasets; Image resolution","DEEPFAKE; PGGAN; self-attention layer; spectral normalization; unbalanced dataset","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135767272"
"Maharana A.; Hannan D.; Bansal M.","Maharana, Adyasha (57194338660); Hannan, Darryl (57207776519); Bansal, Mohit (16466939600)","57194338660; 57207776519; 16466939600","StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13697 LNCS","","","70","87","17","10.1007/978-3-031-19836-6_5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142679926&doi=10.1007%2f978-3-031-19836-6_5&partnerID=40&md5=1929e26379d95323001beb33952ff1bc","Recent advances in text-to-image synthesis have led to large pretrained transformers with excellent capabilities to generate visualizations from a given text. However, these models are ill-suited for specialized tasks like story visualization, which requires an agent to produce a sequence of images given a corresponding sequence of captions, forming a narrative. Moreover, we find that the story visualization task fails to accommodate generalization to unseen plots and characters in new narratives. Hence, we first propose the task of story continuation, where the generated visual story is conditioned on a source image, allowing for better generalization to narratives with new characters. Then, we enhance or ‘retro-fit’ the pretrained text-to-image synthesis models with task-specific modules for (a) sequential image generation and (b) copying relevant elements from an initial frame. We explore full-model finetuning, as well as prompt-based tuning for parameter-efficient adaptation, of the pretrained model. We evaluate our approach StoryDALL-E on two existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset DiDeMoSV collected from a video-captioning dataset. We also develop a model StoryGANc based on Generative Adversarial Networks (GAN) for story continuation, and compare with the StoryDALL-E model to demonstrate the advantages of our approach. We show that our retro-fitting approach outperforms GAN-based models for story continuation. We also demonstrate that the ‘retro-fitting’ approach facilitates copying of visual elements from the source image and improved continuity in visual frames. Finally, our analysis suggests that pretrained transformers struggle with comprehending narratives containing multiple characters, and translating them into appropriate imagery. Our work encourages future research into story continuation and large-scale models for the task (Code and data are available at https://github.com/adymaharana/storydalle ). © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Computer vision; Generative adversarial networks; Image enhancement; Full model; Generalisation; Image generations; Image transformers; Images synthesis; Sequence of images; Sequential images; Source images; Synthesis models; Task-specific modules; Visualization","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142679926"
"Shen Z.; Ding F.; Jolfaei A.; Yadav K.; Vashisht S.; Yu K.","Shen, Zhangyi (57202385734); Ding, Feng (56289824200); Jolfaei, Alireza (36680369700); Yadav, Kusum (57194178764); Vashisht, Sahil (57207102423); Yu, Keping (56316023300)","57202385734; 56289824200; 36680369700; 57194178764; 57207102423; 56316023300","DeformableGAN: Generating Medical Images With Improved Integrity for Healthcare Cyber Physical Systems","2022","IEEE Transactions on Network Science and Engineering","","","","1","13","12","10.1109/TNSE.2022.3190765","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135234385&doi=10.1109%2fTNSE.2022.3190765&partnerID=40&md5=03d9187855c693f297200c72afd0ca52","The development of deep learning enables the production of new images via generative adversarial networks (GANs). The GANs have now been widely applied in the industry as well as academic research that brought tremendous progress to our community. Many researchers in medical imaging also introduced this novel technology for medical image reconstruction, segmentation, synthesis, etc. On the other hand, the GAN-generated images may also suffer from exhibiting unique textures, namely the checkerboard artifacts. For medical diagnosis and healthcare, such artifacts could bring negative impacts as they may distort information collected in medical images. Improper treatment and rehabilitation plans based on the disinformation of checkerboard artifacts could be harmful for patients and healthcare cyber physical systems. Thus, we investigate the checkerboard artifact synthesized during adversarial training in this paper. Based on the theoretical analysis, we propose a method for GANs to generate images without producing checkerboard artifacts. It could protect medical images preserving high integrity for healthcare cyber physical systems. Our experiments justify the efficiency of proposed method when associating with a variety of GANs for image synthesis. Also, we prove that it is feasible to detect GAN-generated images by tracing the checkerboard artifacts. IEEE","Cyber Physical System; Cybersecurity; Deep learning; Diagnosis; E-learning; Embedded systems; Image enhancement; Image reconstruction; Image segmentation; Medical imaging; Patient rehabilitation; Patient treatment; Textures; AI-driven cybe security; Checkerboard artifact; Cybe-physical systems; Cyber security; Cyber-physical systems; Deep learning; Gray scale; Healthcare; Industry research; Medical diagnostic imaging; Generative adversarial networks","AI-driven cyber security; checkerboard artifacts; Convolution; Deep learning; Electronic mail; generative adversarial networks; Generative adversarial networks; Gray-scale; healthcare; Medical diagnostic imaging; medical imaging; Training","Article","Article in press","","Scopus","2-s2.0-85135234385"
"Vaidya A.; Stough J.V.; Patel A.A.","Vaidya, Anurag (57741160900); Stough, Joshua V. (8385827900); Patel, Aalpen A. (57199234712)","57741160900; 8385827900; 57199234712","Perceptually Improved T1-T2 MRI Translations Using Conditional Generative Adversarial Networks","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12032","","120321V","","","","10.1117/12.2608428","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131926552&doi=10.1117%2f12.2608428&partnerID=40&md5=a3be62bdccf292957f309435a96e44c6","Magnetic Resonance Imaging (MRI) encompasses a set of powerful imaging techniques for understanding brain structure and diagnosing pathology. Various MRI sequences including T1- and T2-weighted provide rich complementary information. However, significant equipment costs and acquisition times have inhibited uptake of this critical technology, adversely impacting health equity globally. To ameliorate these costs associated with brain MRIs, we present pTransGAN, a generative adversarial network (GAN) capable of translating both healthy and unhealthy T1 scans into T2 scans, potentially obviating T2 acquisition. Extending prior GAN-based image translation, we show that the addition of non-adversarial losses, like style and content loss, improves the translations provided, especially making the generated images sharper, and making the model more robust. Additionally in previous studies, separate models have been created for healthy and unhealthy brain MRI. Thus here, we also present a novel simultaneous training protocol that allows pTransGAN to concurrently train on healthy and unhealthy data sampled from two open brain MRI datasets. As measured by novel metrics that closely match perceptual similarity of human observers, our simultaneously trained pTransGAN model outperforms the models individually trained on just healthy or unhealthy data. These encouraging results should be further validated with independent paired and unpaired clinical datasets. © 2022 SPIE","Deep learning; Diagnosis; Generative adversarial networks; Image enhancement; Medical imaging; Adversarial learning; Brain structure; Deep learning; Equipment acquisition; Equipment costs; Generative/adversarial learning; Images synthesis; Imaging sequence; T1-weighted; T2 weighted; Magnetic resonance imaging","Deep learning; Generative/adversarial learning; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85131926552"
"Wang Z.; Liu L.; Zhang H.; Ma Y.; Cui H.; Chen Y.; Kong H.","Wang, Zekang (57737849700); Liu, Li (55648218000); Zhang, Huaxiang (56012965100); Ma, Yue (57205442486); Cui, Huailei (57919636700); Chen, Yuan (56438348100); Kong, Haoran (57919459200)","57737849700; 55648218000; 56012965100; 57205442486; 57919636700; 56438348100; 57919459200","Generative Adversarial Networks Based on Dynamic Word-Level Update for Text-to-Image Synthesis","2022","2022 7th International Conference on Image, Vision and Computing, ICIVC 2022","","","","641","647","6","10.1109/ICIVC55077.2022.9886095","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139490896&doi=10.1109%2fICIVC55077.2022.9886095&partnerID=40&md5=a29db4733710c02fcbf63c4ed33afbcc","The traditional fine-grained generation adversarial networks pay less attention to the importance evaluation of word-level information for text-to-image synthesis. Incorrect importance rating of word-level information may skew image generation to less critical direction and affect the generation of key features. In this paper, a novel generative adversarial network based on dynamic word-level update is proposed to solve the above problem by dynamically updating the different importance of each word in the image generation stage. The assignment module with dynamic weights is designed to update text features and image features by communicating the information of text and image. This module enables the importance rating of words to be updated. In addition, the mixed zero-center gradient penalty function and visual loss function are proposed to optimize generative adversarial networks based on dynamic word-level update. The mixed zero-center gradient penalty function allows the generator to generate image with high semantic consistency and ensures the stability of the training process. The visual loss function further improves the visual effect of the generated image by narrowing the difference between the real image and the generated image. Extensive experiments on public benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods.  © 2022 IEEE.","Computer vision; Deep learning; Image enhancement; Semantics; Deep feature learning; Dynamic weighting network; Hierarchical image generation; Image generations; Images synthesis; Importance rating; Network-based; ON dynamics; Text-to-image synthesis; Word level; Generative adversarial networks","deep feature learning; dynamic weighting network; hierarchical image generation; text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85139490896"
"Zhang X.; Zheng Z.; Gao D.; Zhang B.; Pan P.; Yang Y.","Zhang, Xuanmeng (57218711279); Zheng, Zhedong (57200174037); Gao, Daiheng (57212479188); Zhang, Bang (57208084127); Pan, Pan (57203385002); Yang, Yi (57222101012)","57218711279; 57200174037; 57212479188; 57208084127; 57203385002; 57222101012","Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18429","18438","9","10.1109/CVPR52688.2022.01790","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137191924&doi=10.1109%2fCVPR52688.2022.01790&partnerID=40&md5=1891bc9aee11ee25d241aca3853e81b9","3D-aware image synthesis aims to generate images of objects from multiple views by learning a 3D representation. However, one key challenge remains: existing approaches lack geometry constraints, hence usually fail to generate multi-view consistent images. To address this challenge, we propose Multi-View Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware image synthesis with geometry constraints. By leveraging the underlying 3D geometry information of generated images, i.e., depth and camera transformation matrix, we explicitly establish stereo correspondence between views to perform multi-view joint optimization. In particular, we enforce the photometric consistency between pairs of views and integrate a stereo mixup mechanism into the training process, encouraging the model to reason about the correct 3D shape. Besides, we design a two-stage training strategy with feature-level multi-view joint optimization to improve the image quality. Extensive experiments on three datasets demonstrate that MVCGAN achieves the state-of-the-art performance for 3D-aware image synthesis. © 2022 IEEE.","Computer vision; Geometry; Image enhancement; Linear transformations; Stereo image processing; 3d representations; Geometry constraints; High quality; Image and video synthesis and generation; Images synthesis; Joint optimization; Multi-views; Multiple views; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85137191924"
"Zhao H.; Li H.; Cheng L.","Zhao, He (57193273899); Li, Huiqi (55261539200); Cheng, Li (56428060700)","57193273899; 55261539200; 56428060700","Data augmentation for medical image analysis","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","279","302","23","10.1016/B978-0-12-824349-7.00021-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137613256&doi=10.1016%2fB978-0-12-824349-7.00021-9&partnerID=40&md5=83e1129e7335c1de8e88e7fd8b05b4e4","Deep learning methods develop very rapidly and are widely used in computer vision applications as well as for medical image analysis. The deep learning methods provide a significant improvement on medical image analysis tasks by learning a hierarchical representation of different levels directly from data instead of handcrafted features. However, their superior performance highly relies on the number of available training samples. Lack of data either causes the performance to drop or overfitting problems. Unfortunately, it is not always easy to obtain big data for many applications, especially for medical images. In this chapter, we will discuss data augmentation methods including both traditional transformations and emerging generative adversarial networks. In traditional augmentation methods, techniques including geometric transformations and photometric transformations are introduced, e.g., image color space transformation, image rotation, random cropping. The work based on synthesis to augment data is presented followed by the challenges and future directions on data augmentation. © 2022 Elsevier Inc. All rights reserved.","","Data augmentation; Generative adversarial networks; Geometric transformations; Image synthesis; Medical image analysis; Photometric transformations","Book chapter","Final","","Scopus","2-s2.0-85137613256"
"Baier G.; Deschemps A.; Schmitt M.; Yokoya N.","Baier, Gerald (57188720676); Deschemps, Antonin (57221248595); Schmitt, Michael (7401931279); Yokoya, Naoto (36440631200)","57188720676; 57221248595; 7401931279; 36440631200","Synthesizing Optical and SAR Imagery from Land Cover Maps and Auxiliary Raster Data","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3068532","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104617195&doi=10.1109%2fTGRS.2021.3068532&partnerID=40&md5=4f1d774df6d33691b5d314efc9f28290","We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fréchet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth, and we published the newly created high-resolution data set at https://ieee-dataport.org/open-access/geonrw.  © 1980-2012 IEEE.","Data fusion; HTTP; Radar imaging; Rasterization; Remote sensing; Synthetic aperture radar; Water levels; Adversarial networks; Auxiliary information; Digital elevation model; High resolution data; Information contents; Remote sensing images; Segmentation models; Spatially adaptive; land cover; mapping method; optical method; radar imagery; raster; synthetic aperture radar; Image processing","Deep learning; generative adversarial network (GAN); image synthesis; synthetic aperture radar (SAR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104617195"
"Jindal R.; Yagnik V.S.; Yadav V.; Malhotra V.","Jindal, Rajni (8698929800); Yagnik, Vagish Shanker (57874786900); Yadav, Vikas (57218133278); Malhotra, Vishesh (57741036300)","8698929800; 57874786900; 57218133278; 57741036300","Motion Transfer & Person Image Synthesis","2022","2022 2nd International Conference on Intelligent Technologies, CONIT 2022","","","","","","","10.1109/CONIT55038.2022.9848030","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137273371&doi=10.1109%2fCONIT55038.2022.9848030&partnerID=40&md5=bce07ab3bbc39c6b34a37b52d182fba2","Human motion transfer from a video to an image is one of the applications of GAN. This paper includes the techniques and GAN Models which transfers motion from a video to an image and produces digitally synthesized video of the person present in the image. This paper provides the insight of the Nested GAN model. It generates the human images along with the human features, pose, garments, etc., that is present in the wide source of images. The proposed method consists of two stages. First, the motion patterns are extracted from driving video sequences. Then image-to-image transformations are learned by maximizing the mutual information between the source image and the target image while simultaneously minimizing the distortion associated with the image-to-image transformations. Simulation results show that using our method reduces work factor values, and generates higher quality videos. Method discussed in this paper enables the generation of more realistic output images as well as provides the separated un-annotated attributes of the images. The suggested method along with some innovation outperforms the latest pose transfer and image synthesis models according to experimental results. © 2022 IEEE.","Computer vision; Generative adversarial networks; CGAN; GAN; Generator; Human motion transfer; Image transformations; Images synthesis; LSGAN; Motion transfer; Nested GAN; Pose transfer; Discriminators","CGAN; Discriminator; GANs; Generator; Image synthesis; LSGAN; Motion transfer; Nested GAN; Pose transfer","Conference paper","Final","","Scopus","2-s2.0-85137273371"
"Denck J.; Guehring J.; Maier A.; Rothgang E.","Denck, Jonas (57209571095); Guehring, Jens (13907262000); Maier, Andreas (23392966100); Rothgang, Eva (51864404400)","57209571095; 13907262000; 23392966100; 51864404400","MR-contrast-aware image-to-image translations with generative adversarial networks","2021","International Journal of Computer Assisted Radiology and Surgery","16","12","","2069","2078","9","10.1007/s11548-021-02433-x","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108517734&doi=10.1007%2fs11548-021-02433-x&partnerID=40&md5=acfbb906ed94d8f24ca57822f06eba4f","Purpose: A magnetic resonance imaging (MRI) exam typically consists of several sequences that yield different image contrasts. Each sequence is parameterized through multiple acquisition parameters that influence image contrast, signal-to-noise ratio, acquisition time, and/or resolution. Depending on the clinical indication, different contrasts are required by the radiologist to make a diagnosis. As MR sequence acquisition is time consuming and acquired images may be corrupted due to motion, a method to synthesize MR images with adjustable contrast properties is required. Methods: Therefore, we trained an image-to-image generative adversarial network conditioned on the MR acquisition parameters repetition time and echo time. Our approach is motivated by style transfer networks, whereas the “style” for an image is explicitly given in our case, as it is determined by the MR acquisition parameters our network is conditioned on. Results: This enables us to synthesize MR images with adjustable image contrast. We evaluated our approach on the fastMRI dataset, a large set of publicly available MR knee images, and show that our method outperforms a benchmark pix2pix approach in the translation of non-fat-saturated MR images to fat-saturated images. Our approach yields a peak signal-to-noise ratio and structural similarity of 24.48 and 0.66, surpassing the pix2pix benchmark model significantly. Conclusion: Our model is the first that enables fine-tuned contrast synthesis, which can be used to synthesize missing MR-contrasts or as a data augmentation technique for AI training in MRI. It can also be used as basis for other image-to-image translation tasks within medical imaging, e.g., to enhance intermodality translation (MRI → CT) or 7 T image synthesis from 3 T MR images. © 2021, The Author(s).","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Signal-To-Noise Ratio; article; deep learning; diagnostic imaging; human; knee; motion; nuclear magnetic resonance imaging; signal noise ratio; synthesis; image processing","Deep learning; Generative adversarial networks; Image synthesis; Magnetic resonance imaging","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85108517734"
"Chen J.; Xie Y.; Wang K.; Zhang C.; Vannan M.A.; Wang B.; Qian Z.","Chen, Jialei (57204017166); Xie, Yujia (57211482495); Wang, Kan (56146539600); Zhang, Chuck (55605776068); Vannan, Mani A. (7004765301); Wang, Ben (35206854400); Qian, Zhen (7201384454)","57204017166; 57211482495; 56146539600; 55605776068; 7004765301; 35206854400; 7201384454","Active Image Synthesis for Efficient Labeling","2021","IEEE Transactions on Pattern Analysis and Machine Intelligence","43","11","","3770","3781","11","10.1109/TPAMI.2020.2993221","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116600241&doi=10.1109%2fTPAMI.2020.2993221&partnerID=40&md5=b1f9195914ff4578cf5bc3edb8e36162","The great success achieved by deep neural networks attracts increasing attention from the manufacturing and healthcare communities. However, the limited availability of data and high costs of data collection are the major challenges for the applications in those fields. We propose in this work AISEL, an active image synthesis method for efficient labeling, to improve the performance of the small-data learning tasks. Specifically, a complementary AISEL dataset is generated, with labels actively acquired via a physics-based method to incorporate underlining physical knowledge at hand. An important component of our AISEL method is the bidirectional generative invertible network (GIN), which can extract interpretable features from the training images and generate physically meaningful virtual images. Our AISEL method then efficiently samples virtual images not only further exploits the uncertain regions but also explores the entire image space. We then discuss the interpretability of GIN both theoretically and experimentally, demonstrating clear visual improvements over the benchmarks. Finally, we demonstrate the effectiveness of our AISEL framework on aortic stenosis application, in which our method lowers the labeling cost by 90 percent while achieving a 15 percent improvement in prediction accuracy.  © 1979-2012 IEEE.","Computer aided diagnosis; Computer aided instruction; Deep neural networks; Image enhancement; Active Learning; Data augmentation; Data collection; Data costs; High costs; Images synthesis; Labelings; Small data; Small-data learning; Virtual images; Generative adversarial networks","Active learning; computer-aided diagnosis; data augmentation; generative adversarial networks; small-data learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116600241"
"Peng B.; Liu B.; Bin Y.; Shen L.; Lei J.","Peng, Bo (57193764467); Liu, Bingzheng (57220094602); Bin, Yi (56490132600); Shen, Lili (20436682800); Lei, Jianjun (14037882800)","57193764467; 57220094602; 56490132600; 20436682800; 14037882800","Multi-Modality MR Image Synthesis via Confidence-Guided Aggregation and Cross-Modality Refinement","2022","IEEE Journal of Biomedical and Health Informatics","26","1","","27","35","8","10.1109/JBHI.2021.3082541","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107215162&doi=10.1109%2fJBHI.2021.3082541&partnerID=40&md5=7c9fd6ed88ce206788b521f6dd32fb9a","Magnetic resonance imaging (MRI) can provide multi-modality MR images by setting task-specific scan parameters, and has been widely used in various disease diagnosis and planned treatments. However, in practical clinical applications, it is often difficult to obtain multi-modality MR images simultaneously due to patient discomfort, and scanning costs, etc. Therefore, how to effectively utilize the existing modality images to synthesize missing modality image has become a hot research topic. In this paper, we propose a novel confidence-guided aggregation and cross-modality refinement network (CACR-Net) for multi-modality MR image synthesis, which effectively utilizes complementary and correlative information of multiple modalities to synthesize high-quality target-modality images. Specifically, to effectively utilize the complementary modality-specific characteristics, a confidence-guided aggregation module is proposed to adaptively aggregate the multiple target-modality images generated from multiple source-modality images by using the corresponding confidence maps. Based on the aggregated target-modality image, a cross-modality refinement module is presented to further refine the target-modality image by mining correlative information among the multiple source-modality images and aggregated target-modality image. By training the proposed CACR-Net in an end-to-end manner, high-quality and sharp target-modality MR images are effectively synthesized. Experimental results on the widely used benchmark demonstrate that the proposed method outperforms state-of-the-art methods.  © 2013 IEEE.","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Clinical research; Diagnosis; Clinical application; Confidence maps; Disease diagnosis; Hot research topics; Multiple modalities; Multiple targets; Planned treatments; State-of-the-art methods; Article; diagnostic imaging; generative adversarial network; human; image analysis; image reconstruction; image segmentation; machine learning; mathematical analysis; mining; network analysis; nuclear magnetic resonance imaging; synthesis; training; image processing; procedures; Magnetic resonance imaging","confidence-guided aggregation; cross-modality refinement; Magnetic resonance imaging; medical image synthesis","Article","Final","","Scopus","2-s2.0-85107215162"
"Eskandar G.; Abdelsamad M.; Armanious K.; Zhang S.; Yang B.","Eskandar, George (57222288248); Abdelsamad, Mohamed (57305256700); Armanious, Karim (57208782510); Zhang, Shuai (57724956800); Yang, Bin (55584795030)","57222288248; 57305256700; 57208782510; 57724956800; 55584795030","WAVELET-BASED UNSUPERVISED LABEL-TO-IMAGE TRANSLATION","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","1760","1764","4","10.1109/ICASSP43922.2022.9746759","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131229141&doi=10.1109%2fICASSP43922.2022.9746759&partnerID=40&md5=040a8c8517382bb4e319174bcf6130ed","Semantic Image Synthesis (SIS) is a subclass of image-to-image translation where a semantic layout is used to generate a photorealistic image. State-of-the-art conditional Generative Adversarial Networks (GANs) need a huge amount of paired data to accomplish this task while generic unpaired image-to-image translation frameworks underperform in comparison, because they color-code semantic layouts and learn correspondences in appearance instead of semantic content. Starting from the assumption that a high quality generated image should be segmented back to its semantic layout, we propose a new Unsupervised paradigm for SIS (USIS) that makes use of a self-supervised segmentation loss and whole image wavelet based discrimination. Furthermore, in order to match the high-frequency distribution of real images, a novel generator architecture in the wavelet domain is proposed. We test our methodology on 3 challenging datasets and demonstrate its ability to bridge the performance gap between paired and unpaired models. © 2022 IEEE","Computer vision; Generative adversarial networks; Image compression; Semantic Segmentation; Semantics; Code semantics; Image translation; Images synthesis; Learn+; Photorealistic images; Semantic image synthesis; Semantic images; State of the art; Unsupervised training; Wavelets transform; Wavelet transforms","GANs; Semantic Image Synthesis; Unsupervised Training; Wavelet Transform","Conference paper","Final","","Scopus","2-s2.0-85131229141"
"Xing X.; Del Ser J.; Wu Y.; Li Y.; Xia J.; Lei X.; Firmin D.; Gatehouse P.; Yang G.","Xing, Xiaodan (57310434300); Del Ser, Javier (9737598300); Wu, Yinzhe (57221924364); Li, Yang (56075073900); Xia, Jun (36610952100); Lei, Xu (57491700900); Firmin, David (7007100394); Gatehouse, Peter (7003278320); Yang, Guang (57753360500)","57310434300; 9737598300; 57221924364; 56075073900; 36610952100; 57491700900; 7007100394; 7003278320; 57753360500","HDL: Hybrid Deep Learning for the Synthesis of Myocardial Velocity Maps in Digital Twins for Cardiac Analysis","2022","IEEE Journal of Biomedical and Health Informatics","","","","","","","10.1109/JBHI.2022.3158897","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126540214&doi=10.1109%2fJBHI.2022.3158897&partnerID=40&md5=28470c0e8c00f48d38f5196ca936d2dd","Synthetic digital twins based on medical data accelerate the acquisition, labelling and decision making procedure in digital healthcare. A core part of digital healthcare twins is modelbased data synthesis, which permits the generation of realistic medical signals without requiring to cope with the modelling complexity of anatomical and biochemical phenomena producing them in reality. Unfortunately, algorithms for cardiac data synthesis have been so far scarcely studied in the literature. An important imaging modality in the cardiac examination is three-directional CINE multislice myocardial velocity mapping (3Dir MVM), which provides a quantitative assessment of cardiac motion in three orthogonal directions of the left ventricle. The long acquisition time and complex acquisition produce make it more urgent to produce synthetic digital twins of this imaging modality. In this study, we propose a hybrid deep learning (HDL) network, especially for synthetic 3Dir MVM data. Our algorithm is featured by a hybrid UNet and a Generative Adversarial Network with a foreground-background generation scheme. The experimental results show that from temporally down-sampled magnitude CINE images (six times), our proposed algorithm can still successfully synthesise high temporal resolution 3Dir MVM CMR data (PSNR=42.32) with precise left ventricle segmentation (DICE=0.92). These performance scores indicate that our proposed HDL algorithm can be implemented in real-world digital twins for myocardial velocity mapping data simulation. To the best of our knowledge, this work is the first one in the literature investigating digital twins of the 3Dir MVM CMR, which has shown great potential for improving the efficiency of clinical studies via synthesised cardiac data. IEEE","Complex networks; Decision making; Deep learning; E-learning; Generative adversarial networks; Health care; Image segmentation; Magnetic resonance imaging; Mapping; Velocity; 29 image synthesis; Cardiac imaging; Cine-MRI; Deep learning; Hardware design language; Images synthesis; Myocardial velocity; Myocardial velocity mapping 30; Myocardium; Spatial resolution; Velocity mapping; Heart","29 Image Synthesis; Cardiac Imaging; CINE MRI; Data models; Deep learning; Digital twin; Digital Twins; Hardware design languages; Interpolation; Myocardial Velocity Mapping 30; Myocardium; Spatial resolution","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85126540214"
"Yang Y.; Mu K.; Deng R.H.","Yang, Yang (56427647300); Mu, Ke (57476525700); Deng, Robert H. (57203710994)","56427647300; 57476525700; 57203710994","Lightweight Privacy-Preserving GAN Framework for Model Training and Image Synthesis","2022","IEEE Transactions on Information Forensics and Security","17","","","1083","1098","15","10.1109/TIFS.2022.3156818","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125749476&doi=10.1109%2fTIFS.2022.3156818&partnerID=40&md5=aa290747ff1f606aadce90b0ac2e7e10","Generative adversarial network (GAN) has excellent performance for data generation and is widely used in image synthesis. Outsourcing GAN to cloud platform is a popular way to save local computation resources and improve the efficiency, but it still faces the privacy leakage concerns: (1) the sensitive information of the training dataset may be disclosed in the cloud; (2) the trained model may reveal the privacy of training samples since it extracts the characteristics from the data. In this paper, we propose a lightweight privacy-preserving GAN framework (LP-GAN) for model training and image synthesis based on secret sharing scheme. Specifically, we design a series of efficient secure interactive protocols for different layers (convolution, batch normalization, ReLU, Sigmoid) of neural network (NN) used in GAN. Our protocols are scalable to build secure training or inference tasks for NN-based applications. We utilize edge computing to reduce the latency and all the protocols are executed on two edge servers collaboratively. Compared with the existing schemes, the proposed solution greatly improves efficiency, reduces communication overhead, and guarantees the privacy. We prove the correctness and security of LP-GAN by theoretical analysis. Extensive experiments on different real-world datasets demonstrate the effectiveness, accuracy, and efficiency of our scheme. © 2005-2012 IEEE.","Deep learning; Efficiency; Image processing; Network security; Privacy-preserving techniques; Deep learning; Images synthesis; Model images; Model training; Network frameworks; Neural-networks; Performance; Privacy preserving; Secret-sharing; Secure computation; Generative adversarial networks","deep learning; generative adversarial network; Privacy-preserving; secret sharing; secure computation","Article","Final","","Scopus","2-s2.0-85125749476"
"Salini Y.; HariKiran J.","Salini, Yalamanchili (57871872000); HariKiran, J. (37050839700)","57871872000; 37050839700","Deepfakes on Retinal Images using GAN","2022","International Journal of Advanced Computer Science and Applications","13","8","","701","708","7","10.14569/IJACSA.2022.0130880","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137155180&doi=10.14569%2fIJACSA.2022.0130880&partnerID=40&md5=2a3f9624d209e78c4f19d79a0ce86677","In Deep Learning (DL), Generative Adversarial Networks (GAN) are a popular technique for generating synthetic images, which require extensive and balanced datasets to train. These Artificial Intelligence systems can produce synthetic images that seem authentic, known as Deep Fakes. At present, datadriven approaches to classifying medical images are prevalent. However, most medical data is inaccessible to general researchers due to standard consent forms that restrict research to medical journals or education. Our study focuses on GANs, which can create artificial fundus images that can be indistinguishable from actual fundus images. Before using these fake images, it is essential to investigate privacy concerns and hallucinations thoroughly. As well as, reviewing the current applications and limitations of GANs is very important. In this work, we present the Cycle-GAN framework, a new GAN network for medical imaging that focuses on the generation and segmentation of retinal fundus images.DRIVE retinal fundus image dataset is used to evaluate the proposed model’s performance and achieved an accuracy of 98.19%. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Deep learning; Digital storage; Image segmentation; Learning systems; Medical imaging; Ophthalmology; Auto encoders; Deep learning; Deepfake; Generative adversarial network; Images synthesis; Retinal fundus image synthesis; Retinal fundus images; Segmentation; Synthetic images; Variational autoencoder; Generative adversarial networks","Deep learning; Deepfakes; Generative adversarial network (gan); Retinal fundus image synthesis; Segmentation; Variational autoencoder (vae)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137155180"
"Yu H.; Zhu H.; Lu X.; Liu J.","Yu, Haiming (57215965978); Zhu, Hao (56668986300); Lu, Xiangju (57212319601); Liu, Junhui (57215293093)","57215965978; 56668986300; 57212319601; 57215293093","Migrating Face Swap to Mobile Devices: A Lightweight Framework and a Supervised Training Solution","2022","Proceedings - IEEE International Conference on Multimedia and Expo","2022-July","","","","","","10.1109/ICME52920.2022.9859806","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137720781&doi=10.1109%2fICME52920.2022.9859806&partnerID=40&md5=954de3bf1ce85b3e5667594f04debb13","Existing face swap methods rely heavily on large-scale networks for adequate capacity to generate visually plausible results, which inhibits its applications on resource-constraint platforms. In this work, we propose MobileFSGAN, a novel lightweight GAN for face swap that can run on mobile devices with much fewer parameters while achieving competitive performance. A lightweight encoder-decoder structure is designed especially for image synthesis tasks, which is only 10.2MB and can run on mobile devices at a real-time speed. To tackle the unstability of training such a small network, we construct the FSTriplets dataset utilizing facial attribute editing techniques. FSTriplets provides source-target-result training triplets, yielding pixel-level labels thus for the first time making the training process supervised. We also designed multi-scale gradient losses for efficient back-propagation, resulting in faster and better convergence. Experimental results show that our model reaches comparable performance towards state-of-the-art methods, while significantly reducing the number of network parameters. Codes and the dataset have been released11https://githuh.com/HoiM/MobileFSGAN.  © 2022 IEEE.","Backpropagation; Computer vision; Deep learning; Deep learning; Face swap; ITS applications; Large-scale network; Lightweight frameworks; Lightweight neural network; Neural-networks; Resource Constraint; Supervised trainings; Training solutions; Generative adversarial networks","deep learning; Face swap; generative adversarial networks; lightweight neural network","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85137720781"
"Nakazawa S.; Han C.; Hasei J.; Nakahara R.; Ozaki T.","Nakazawa, Shinji (57313731700); Han, Changhee (57192821057); Hasei, Joe (37067133300); Nakahara, Ryuichi (35322966100); Ozaki, Toshifumi (35514364900)","57313731700; 57192821057; 37067133300; 35322966100; 35514364900","BAPGAN: GAN-based bone age progression of femur and phalange x-ray images","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","12033","","120331A","","","","10.1117/12.2608065","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132839722&doi=10.1117%2f12.2608065&partnerID=40&md5=eebed453397d063a5002d0aacd40b3b3","Convolutional Neural Networks play a key role in bone age assessment for investigating endocrinology, genetic, and growth disorders under various modalities and body regions. However, no researcher has tackled bone age progression/regression despite its valuable potential applications: bone-related disease diagnosis, clinical knowledge acquisition, and museum education. Therefore, we propose Bone Age Progression Generative Adversarial Network (BAPGAN) to progress/regress both femur/phalange X-ray images while preserving identity and realism. We exhaustively confirm the BAPGAN's clinical potential via Fŕechet Inception Distance, Visual Turing Test by two expert orthopedists, and t-Distributed Stochastic Neighbor Embedding.  © 2022 SPIE.","Convolutional neural networks; Diagnosis; Medical imaging; Stochastic systems; Age progression; Bone age; Bone age assessment; Bone X-ray; Convolutional neural network; Genetic disorders; Images synthesis; Turing tests; Visual turing test; X-ray image; Generative adversarial networks","Age progression; Bone X-ray; Generative adversarial networks; Image synthesis; Visual Turing test","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85132839722"
"Xiao J.; Li L.; Wang C.; Zha Z.-J.; Huang Q.","Xiao, Jiayu (57506788900); Li, Liang (56182887500); Wang, Chaofei (57271420900); Zha, Zheng-Jun (36626639900); Huang, Qingming (8435766200)","57506788900; 56182887500; 57271420900; 36626639900; 8435766200","Few Shot Generative Model Adaption via Relaxed Spatial Structural Alignment","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","11194","11203","9","10.1109/CVPR52688.2022.01092","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139591807&doi=10.1109%2fCVPR52688.2022.01092&partnerID=40&md5=8d4a9f97c8084c2d214f32c97eb35e75","Training a generative adversarial network (GAN) with limited data has been a challenging task. A feasible solution is to start with a GAN well-trained on a large scale source domain and adapt it to the target domain with a few samples, termed as few shot generative model adaption. However, existing methods are prone to model overfitting and collapse in extremely few shot setting (less than 10). To solve this problem, we propose a relaxed spatial structural alignment (RSSA) method to calibrate the target generative models during the adaption. We design a cross-domain spatial structural consistency loss comprising the self-correlation and disturbance correlation consistency loss. It helps align the spatial structural information between the synthesis image pairs of the source and target domains. To relax the cross-domain alignment, we compress the original latent space of generative models to a subspace. Image pairs generated from the subspace are pulled closer. Qualitative and quantitative experiments show that our method consistently surpasses the state-of-the-art methods in few shot setting. Our source code: https://github.com/StevenShaw1999/RSSA. © 2022 IEEE.","Computer vision; Generative model; Image and video synthesis and generation; Images synthesis; Long tail; Model adaption; Structural alignments; Target domain; Transfer/low-shot/long-tail learning; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation; Transfer/low-shot/long-tail learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85139591807"
"Zhan B.; Li D.; Wu X.; Zhou J.; Wang Y.","Zhan, Bo (57221803799); Li, Di (57226876886); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Wang, Yan (56039981100)","57221803799; 57226876886; 57221065403; 21234416400; 56039981100","Multi-Modal MRI Image Synthesis via GAN with Multi-Scale Gate Mergence","2022","IEEE Journal of Biomedical and Health Informatics","26","1","","17","26","9","10.1109/JBHI.2021.3088866","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112202427&doi=10.1109%2fJBHI.2021.3088866&partnerID=40&md5=f2ff00fbe06c1ece27d0e9494251ac8f","Multi-modal magnetic resonance imaging (MRI) plays a critical role in clinical diagnosis and treatment nowadays. Each modality of MRI presents its own specific anatomical features which serve as complementary information to other modalities and can provide rich diagnostic information. However, due to the limitations of time consuming and expensive cost, some image sequences of patients may be lost or corrupted, posing an obstacle for accurate diagnosis. Although current multi-modal image synthesis approaches are able to alleviate the issues to some extent, they are still far short of fusing modalities effectively. In light of this, we propose a multi-scale gate mergence based generative adversarial network model, namely MGM-GAN, to synthesize one modality of MRI from others. Notably, we have multiple down-sampling branches corresponding to input modalities to specifically extract their unique features. In contrast to the generic multi-modal fusion approach of averaging or maximizing operations, we introduce a gate mergence (GM) mechanism to automatically learn the weights of different modalities across locations, enhancing the task-related information while suppressing the irrelative information. As such, the feature maps of all the input modalities at each down-sampling level, i.e., multi-scale levels, are integrated via GM module. In addition, both the adversarial loss and the pixel-wise loss, as well as gradient difference loss (GDL) are applied to train the network to produce the desired modality accurately. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art multi-modal image synthesis methods.  © 2013 IEEE.","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Diagnosis; Merging; Signal sampling; Adversarial networks; Anatomical features; Clinical diagnosis; Input modalities; Multi-modal fusion; Multi-modal image; State of the art; Unique features; adult; article; averaging; controlled study; diagnostic test accuracy study; feature extraction; human; logic; nuclear magnetic resonance imaging; synthesis; image processing; Magnetic resonance imaging","gate mergence; generative adversarial network (GAN); magnetic resonance imaging (MRI); Multi-modal synthesis; multi-scale fusion","Article","Final","","Scopus","2-s2.0-85112202427"
"Wu F.; Cheng J.; Wang X.; Wang L.; Tao D.","Wu, Fuxiang (57214770266); Cheng, Jun (14057685600); Wang, Xinchao (54406086400); Wang, Lei (57203825442); Tao, Dapeng (54411283900)","57214770266; 14057685600; 54406086400; 57203825442; 54411283900","Image Hallucination From Attribute Pairs","2022","IEEE Transactions on Cybernetics","52","1","","568","581","13","10.1109/TCYB.2020.2979258","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114130432&doi=10.1109%2fTCYB.2020.2979258&partnerID=40&md5=2481c3d32a08fc35f201452152c9e912","Recent image-generation methods have demonstrated that realistic images can be produced from captions. Despite the promising results achieved, existing caption-based generation methods confront a dilemma. On the one hand, the image generator should be provided with sufficient details for realistic hallucination, meaning that longer sentences with rich content are preferred, but on the other hand, the generator is meanwhile fragile to long sentences due to their complex semantics and syntax like long-range dependencies and the combinatorial explosion of object visual features. Toward alleviating this dilemma, a novel approach is proposed in this article to hallucinate images from attribute pairs, which can be extracted from natural language processing (NLP) toolsets in the presence of complex semantics and syntax. Attribute pairs, therefore, enable our image generator to tackle long sentences handily and alleviate the combinatorial explosion, and at the same time, allow us to enlarge the training dataset and to produce hallucinations from randomly combined attribute pairs at ease. Experiments on widely used datasets demonstrate that the proposed approach yields results superior to the state of the art.  © 2013 IEEE.","Hallucinations; Humans; Natural Language Processing; Semantics; Complex networks; Generative adversarial networks; Image processing; Natural language processing systems; Semantics; Attentional generative adversarial network; Attribute pair; Combinatorial explosion; Generation method; Image generations; Image generators; Images synthesis; Long-range dependencies; Realistic images; Text-to-image synthesis; diagnostic imaging; hallucination; human; natural language processing; semantics; Syntactics","Attentional generative adversarial network (GAN); attribute pair; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85114130432"
"Huang X.; Mallya A.; Wang T.-C.; Liu M.-Y.","Huang, Xun (57191432428); Mallya, Arun (57156219500); Wang, Ting-Chun (56612466200); Liu, Ming-Yu (22835742800)","57191432428; 57156219500; 56612466200; 22835742800","Multimodal Conditional Image Synthesis with Product-of-Experts GANs","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13676 LNCS","","","91","109","18","10.1007/978-3-031-19787-1_6","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142701105&doi=10.1007%2f978-3-031-19787-1_6&partnerID=40&md5=1e2c14782a88faf70a3368cd79aa5986","Existing conditional image synthesis frameworks generate images based on user inputs in a single modality, such as text, segmentation, or sketch. They do not allow users to simultaneously use inputs in multiple modalities to control the image synthesis output. This reduces their practicality as multimodal inputs are more expressive and complement each other. To address this limitation, we propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN) framework, which can synthesize images conditioned on multiple input modalities or any subset of them, even the empty set. We achieve this capability with a single trained model. PoE-GAN consists of a product-of-experts generator and a multimodal multiscale projection discriminator. Through our carefully designed training scheme, PoE-GAN learns to synthesize images with high quality and diversity. Besides advancing the state of the art in multimodal conditional image synthesis, PoE-GAN also outperforms the best existing unimodal conditional image synthesis approaches when tested in the unimodal setting. The project website is available at this link. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Image segmentation; GAN; Image-based; Images synthesis; Multi-modal; Multi-modal learning; Multiple modalities; Product of experts; Text segmentation; Unimodal; User input; Generative adversarial networks","GAN; Image synthesis; Multimodal learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142701105"
"Davis K.M.; De La Torre-Ortiz C.; Ruotsalo T.","Davis, Keith M. (57219115792); De La Torre-Ortiz, Carlos (57222250282); Ruotsalo, Tuukka (23390701100)","57219115792; 57222250282; 23390701100","Brain-Supervised Image Editing","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18459","18468","9","10.1109/CVPR52688.2022.01793","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141304372&doi=10.1109%2fCVPR52688.2022.01793&partnerID=40&md5=eb75c55ddd372ffc36b91aa0bfd935ca","Despite recent advances in deep neural models for semantic image editing, present approaches are dependent on explicit human input. Previous work assumes the availability of manually curated datasets for supervised learning, while for unsupervised approaches the human inspection of discovered components is required to identify those which modify worthwhile semantic features. Here, we present a novel alternative: the utilization of brain responses as a supervision signal for learning semantic feature representations. Participants $(N=30)$ in a neurophysiological experiment were shown artificially generated faces and instructed to look for a particular semantic feature, such as 'old' or 'smiling', while their brain responses were recorded via electroencephalography (EEG). Using supervision signals inferred from these responses, semantic features within the latent space of a generative adversarial network (GAN) were learned and then used to edit semantic features of new images. We show that implicit brain supervision achieves comparable semantic image editing performance to explicit manual labeling. This work demonstrates the feasibility of utilizing implicit human reactions recorded via brain-computer interfaces for semantic image editing and interpretation. © 2022 IEEE.","Brain computer interface; Electroencephalography; Electrophysiology; Semantics; Brain response; Image and video synthesis and generation; Image editing; Images synthesis; Neural modelling; Semantic features; Semantic images; Video generation; Video synthesis; Vision + X; Generative adversarial networks","Image and video synthesis and generation; Vision + X","Conference paper","Final","","Scopus","2-s2.0-85141304372"
"Darvish M.; Shanbehzadeh J.; Mansouri A.","Darvish, Mahta (57754726800); Shanbehzadeh, Jamshid (6603027683); Mansouri, Azadeh (24779674000)","57754726800; 6603027683; 24779674000","Towards the Efficiency of the Fusion Step in Language-Based Fashion Image Editing","2022","Proceedings - 2022 27th International Computer Conference, Computer Society of Iran, CSICC 2022","","","","","","","10.1109/CSICC55295.2022.9780492","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132405246&doi=10.1109%2fCSICC55295.2022.9780492&partnerID=40&md5=a67611f4004bf0668f92609113cc7ddd","Text-to-image synthesis is a new research field of image generation. The generated images must be based on textual descriptions and of acceptable quality. Among the generative models, Generative Adversarial Networks (GANs) can generate higher quality images compared to other models. Most GAN-based text-to-image generation methods use simple datasets, including flower and bird images. For more complex datasets (such as the Fashion Synthesis dataset used in this paper), the issue of generating an image from a text becomes more challenging, as the images in this dataset are much richer in content than just the flower images or birds. Most of the methods proposed so far are only able to recognize the color of an object based on the text description and have difficulty in accurately identifying the location in the image where these changes are to be made. One way to solve this problem is to generate an image from the text based on editing this image. In this paper, the methods of combining text and image features in the generation of images are discussed and the effect of different fusing models on the process in terms of quality and accuracy of the description is investigated.  © 2022 IEEE.","Birds; Character recognition; Fashion synthesis; Generative model; High quality images; Image editing; Image generations; Images synthesis; Network-based; Research fields; Text-to-image; Textual description; Generative adversarial networks","Fashion synthesis; GAN; Text-to-image","Conference paper","Final","","Scopus","2-s2.0-85132405246"
"Song S.; Baek J.-G.","Song, Seunghwan (57216323052); Baek, Jun-Geol (7103228722)","57216323052; 7103228722","Defect Information Synthesis via Latent Mapping Adversarial Networks","2022","4th International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2022 - Proceedings","","","","17","22","5","10.1109/ICAIIC54071.2022.9722628","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127652979&doi=10.1109%2fICAIIC54071.2022.9722628&partnerID=40&md5=dea9f18d7ab302e2b9809a6c3581ef2d","This research presents a new image synthesis methodology for automated visual inspection (AVI) in steel manufacturing process. We develop a novel methodology, termed Latent Mapping Adversarial Networks. As the end product of the manufacturing process is directly linked to economic factors, various methods are being utilized to improve the quality of the product. Among them, the defect detection steps carried out in advance are important as it greatly impacts productivity. However, new challenges have emerged for several reasons. First, it requires prior knowledge of the expert to define the defect image and perform detection. To alleviate this problem, various companies have started utilizing AVI to reduce this dependence on domain knowledge. Secondly, defect detection is an arduous task since fewer defect images are available compared to normal images. This underlying problem leads to a classification model that is biased toward the majority class, which degrades the final performance. In this paper, we propose a method to synthesize defect images to solve the above-mentioned problems. Inspired by StyleGAN, we build mapping networks for latent space of the generator. Through this, we can synthesize defect images of various sizes in the manufacturing process. In addition, we experiment to find the most suitable loss function to solve the common problems of Generative Adversarial Networks (GAN). We also optimized the proposed method in terms of convergence and computation speed by estimating the size of optimal latent space. The experimental results using quantitative metrics illustrate the improved performance of the proposed methodology. As a result, it is now possible to solve the quality problem and increase productivity by reducing misclassification in the model through AVI experiments using the generated images  © 2022 IEEE.","Defects; Domain Knowledge; Generative adversarial networks; Industrial research; Manufacture; Productivity; Adversarial networks; Automated visual inspection; Defect detection; Defect images; Information synthesis; Latent mapping; Manufacturing process; Mapping network; Performance; Synthesize defect; Mapping","Automated visual inspection; generative adversarial networks; latent mapping; mapping network; synthesize defect","Conference paper","Final","","Scopus","2-s2.0-85127652979"
"Isaac-Medina B.K.S.; Bhowmik N.; Willcocks C.G.; Breckon T.P.","Isaac-Medina, Brian K. S. (57193435834); Bhowmik, Neelanjan (57211269027); Willcocks, Chris G. (55179413600); Breckon, Toby P. (8661055600)","57193435834; 57211269027; 55179413600; 8661055600","Cross-modal Image Synthesis within Dual-Energy X-ray Security Imagery","2022","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2022-June","","","332","340","8","10.1109/CVPRW56347.2022.00048","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137753786&doi=10.1109%2fCVPRW56347.2022.00048&partnerID=40&md5=191a1db20a1e2ec3d410d9e193d5e83d","Dual-energy X-ray scanners are used for aviation security screening given their capability to discriminate materials inside passenger baggage. To facilitate manual operator inspection, a pseudo-colouring is assigned to the effective composition of the material. Recently, paired image to image translation models based on conditional Generative Adversarial Networks (cGAN) have shown to be effective for image colourisation. In this work, we investigate the use of such a model to translate from the raw X-ray energy responses (high, low, effective-Z) to the pseudo-coloured images and vice versa. Specifically, given N X-ray modalities, we train a cGAN conditioned in N - m domains to generate the remaining m representation. Our method achieves a mean squared error (MSE) of 16.5 and a structural similarity index (SSIM) of 0.9815 when using the raw modalities to generate the pseudo-colour representation. Additionally, raw X-ray high energy, low energy and effective-Z projections were generated given the pseudo-colour image with minimum MSE of 2.57, 5.63 and 1.43, and maximum SSIM of 0.9953, 0.9901 and 0.9921. Furthermore, we assess the quality of our synthesised pseudo-colour reconstructions by measuring the performance of two object detection models originally trained on real X-ray pseudo-colour images over our generated pseudo-colour images. Interestingly, our generated pseudo-colour images obtain marginally improved detection performance than the corresponding real X-ray pseudo-colour images, showing that meaningful representations are synthesized and that these reconstructions are applicable for differing aviation security tasks. © 2022 IEEE.","Color; Image enhancement; Image reconstruction; Mean square error; Object detection; Aviation Security; Colour image; Cross-modal; Dual-energy X-ray; Images synthesis; Pseudocolour; Similarity indices; Structural similarity; Synthesised; X-ray scanner; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85137753786"
"Pantazis Y.; Paul D.; Fasoulakis M.; Stylianou Y.; Katsoulakis M.A.","Pantazis, Yannis (8837185000); Paul, Dipjyoti (57214128589); Fasoulakis, Michail (56491245400); Stylianou, Yannis (6601991415); Katsoulakis, Markos A. (6603955402)","8837185000; 57214128589; 56491245400; 6601991415; 6603955402","Cumulant GAN","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2022.3161127","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127818008&doi=10.1109%2fTNNLS.2022.3161127&partnerID=40&md5=358e9f894846d3558092bd4291741ad3","In this article, we propose a novel loss function for training generative adversarial networks (GANs) aiming toward deeper theoretical understanding as well as improved stability and performance for the underlying optimization problem. The new loss function is based on cumulant generating functions (CGFs) giving rise to Cumulant GAN. Relying on a recently derived variational formula, we show that the corresponding optimization problem is equivalent to R&#x00E9;nyi divergence minimization, thus offering a (partially) unified perspective of GAN losses: the R&#x00E9;nyi family encompasses Kullback-Leibler divergence (KLD), reverse KLD, Hellinger distance, and &#x03C7;&#x00B2;-divergence. Wasserstein GAN is also a member of cumulant GAN. In terms of stability, we rigorously prove the linear convergence of cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions, and the standard gradient descent ascent algorithm. Finally, we experimentally demonstrate that image generation is more robust relative to Wasserstein GAN and it is substantially improved in terms of both inception score (IS) and Fr&#x00E9;chet inception distance (FID) when both weaker and stronger discriminators are considered. IEEE","Gradient methods; Image enhancement; Optimization; Cumulant generating function; Cumulant generating functions; Generative adversarial network; Generator; Image generations; Images synthesis; Minimisation; Nyi divergence.; Optimisations; R&#x00e9;; Generative adversarial networks","Cumulant generating function (CGF); Generative adversarial networks; generative adversarial networks (GANs); Generators; image generation; Image synthesis; Minimization; Optimization; R&#x00E9;nyi divergence.; Standards; Training","Article","Article in press","","Scopus","2-s2.0-85127818008"
"Faltings U.; Bettinger T.; Barth S.; Schäfer M.","Faltings, Ulrike (57404840900); Bettinger, Tobias (57404997300); Barth, Swen (57405158800); Schäfer, Michael (57405493600)","57404840900; 57404997300; 57405158800; 57405493600","Impact on Inference Model Performance for ML Tasks Using Real-Life Training Data and Synthetic Training Data from GANs","2022","Information (Switzerland)","13","1","9","","","","10.3390/info13010009","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122544858&doi=10.3390%2finfo13010009&partnerID=40&md5=05f478103c7134790bdc21b00f3f38e9","Collecting and labeling of good balanced training data are usually very difficult and challenging under real conditions. In addition to classic modeling methods, Generative Adversarial Networks (GANs) offer a powerful possibility to generate synthetic training data. In this paper, we evaluate the hybrid usage of real-life and generated synthetic training data in different fractions and the effect on model performance. We found that a usage of up to 75% synthetic training data can compensate for both time-consuming and costly manual annotation while the model performance in our Deep Learning (DL) use case stays in the same range compared to a 100% share in hand-annotated real images. Using synthetic training data specifically tailored to induce a balanced dataset, special care can be taken concerning events that happen only on rare occasions and a prompt industrial application of ML models can be executed without too much delay, making these feasible and economically attractive for a wide scope of industrial applications in process and manufacturing industries. Hence, the main outcome of this paper is that our methodology can help to leverage the implementation of many different industrial Machine Learning and Computer Vision applications by making them economically maintainable. It can be concluded that a multitude of industrial ML use cases that require large and balanced training data containing all information that is relevant for the target model can be solved in the future following the findings that are presented in this study. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Computer vision; Deep learning; Classic models; Cognitive twin; Condition; Images synthesis; Inference models; Labelings; Model method; Modeling performance; Synthetic training data; Training data; Generative adversarial networks","Cognitive Twin; Computer Vision; Digital Twin; Generative Adversarial Networks; Image synthesis; Industrial application","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122544858"
"Cintas C.; Speakman S.; Tadesse G.A.; Akinwande V.; McFowland E., III; Weldemariam K.","Cintas, Celia (56436822800); Speakman, Skyler (55857725200); Tadesse, Girmaw Abebe (57204531986); Akinwande, Victor (57194381194); McFowland, Edward (55498657800); Weldemariam, Komminist (23478544500)","56436822800; 55857725200; 57204531986; 57194381194; 55498657800; 23478544500","Pattern detection in the activation space for identifying synthesized content","2022","Pattern Recognition Letters","153","","","207","213","6","10.1016/j.patrec.2021.12.007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122043976&doi=10.1016%2fj.patrec.2021.12.007&partnerID=40&md5=e3dc591ce4dd3412455ecf97e7348794","Generative Adversarial Networks (GANs) have recently achieved unprecedented success in photo-realistic image synthesis from low-dimensional random noise. The ability to synthesize high-quality content at a large scale brings potential risks as the generated samples may lead to misinformation that can create severe social, political, health, and business hazards. We propose SubsetGAN to identify generated content by detecting a subset of anomalous node-activations in the inner layers of pre-trained neural networks. These nodes, as a group, maximize a non-parametric measure of divergence away from the expected distribution of activations created from real data. This enable us to identify synthesised images without prior knowledge of their distribution. SubsetGAN efficiently scores subsets of nodes and returns the group of nodes within the pre-trained classifier that contributed to the maximum score. The classifier can be a general fake classifier trained over samples from multiple sources or the discriminator network from different GANs. Our approach shows consistently higher detection power than existing detection methods across several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different proportions of generated content. © 2021 Elsevier B.V.","Chemical detection; Generative adversarial networks; Health hazards; Health risks; Multilayer neural networks; Pattern recognition; Content detection; Generative model; High quality; Low dimensional; Pattern detection; Photo realistic image synthesis; Random noise; Subset scanning; Synthesised; Synthetic content detection; Chemical activation","Generative models; Subset scanning; Synthetic content detection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122043976"
"Frolov S.; Hinz T.; Raue F.; Hees J.; Dengel A.","Frolov, Stanislav (57221158423); Hinz, Tobias (57191251410); Raue, Federico (57141992100); Hees, Jörn (35229784500); Dengel, Andreas (6603764314)","57221158423; 57191251410; 57141992100; 35229784500; 6603764314","Adversarial text-to-image synthesis: A review","2021","Neural Networks","144","","","187","209","22","10.1016/j.neunet.2021.07.019","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114298621&doi=10.1016%2fj.neunet.2021.07.019&partnerID=40&md5=9b411dad2b6618c7c3e55d67bcb42e4b","With the advent of generative adversarial networks, synthesizing images from text descriptions has recently become an active research area. It is a flexible and intuitive way for conditional image generation with significant progress in the last years regarding visual realism, diversity, and semantic alignment. However, the field still faces several challenges that require further research efforts such as enabling the generation of high-resolution images with multiple objects, and developing suitable and reliable evaluation metrics that correlate with human judgement. In this review, we contextualize the state of the art of adversarial text-to-image synthesis models, their development since their inception five years ago, and propose a taxonomy based on the level of supervision. We critically examine current strategies to evaluate text-to-image synthesis models, highlight shortcomings, and identify new areas of research, ranging from the development of better datasets and evaluation metrics to possible improvements in architectural design and model training. This review complements previous surveys on generative adversarial networks with a focus on text-to-image synthesis which we believe will help researchers to further advance the field. © 2021","Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics; Image enhancement; Adversarial networks; Evaluation metrics; Generative adversarial network; Image generations; Images synthesis; Research areas; Synthesis models; Text-to-image synthesis; Visual realism; Visual semantics; human; human experiment; review; synthesis; taxonomy; image processing; semantics; Semantics","Generative adversarial networks; Text-to-image synthesis","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85114298621"
"Shenkut D.; Bhagavatula V.","Shenkut, Dereje (57892332800); Bhagavatula, Vijayakumar (16068283100)","57892332800; 16068283100","Fundus GAN - GAN-based Fundus Image Synthesis for Training Retinal Image Classifiers","2022","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2022-July","","","2185","2189","4","10.1109/EMBC48229.2022.9871771","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138128119&doi=10.1109%2fEMBC48229.2022.9871771&partnerID=40&md5=bd3aa949208883c11a56c2879e3ece94","Two major challenges in applying deep learning to develop a computer-aided diagnosis of fundus images are the lack of enough labeled data and legal issues with patient privacy. Various efforts are being made to increase the amount of data either by augmenting training images or by synthesizing realistic-looking fundus images. However, augmentation is limited by the amount of available data and it does not address the patient privacy concern. In this paper, we propose a Generative Adversarial Network-based (GAN-based) fundus image synthesis method (Fundus GAN) that generates synthetic training images to solve the above problems. Fundus GAN is an improved way of generating retinal images by following a two-step generation process which involves first training a segmentation network to extract the vessel tree followed by vessel tree to fundus image-to-image translation using unsupervised generative attention networks. Our results show that the proposed Fundus GAN outperforms state of the art methods in different evaluation metrics. Our results also validate that generated retinal images can be used to train retinal image classifiers for eye diseases diagnosis. Clinical Relevance - Our proposed method Fundus GAN helps in solving the shortage of patient privacy-preserving training data in developing algorithms for automating image- based eye disease diagnosis. The proposed two-step GAN- based image synthesis can be used to improve the classification accuracy of retinal image classifiers without compromising the privacy of the patient © 2022 IEEE.","Algorithms; Diagnosis, Computer-Assisted; Diagnostic Techniques, Ophthalmological; Fundus Oculi; Humans; Deep learning; Eye protection; Forestry; Generative adversarial networks; Image classification; Image enhancement; Image segmentation; Ophthalmology; Privacy-preserving techniques; Disease diagnosis; Eye disease; Fundus image; Image Classifiers; Images synthesis; Labeled data; Network-based; Patient privacies; Retinal image; Training image; algorithm; computer assisted diagnosis; eye fundus; human; visual system examination; Computer aided diagnosis","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85138128119"
"Liu H.; Liu Q.","Liu, Hangyu (57928272000); Liu, Qicheng (57928116800)","57928272000; 57928116800","Image Creation Based on Transformer and Generative Adversarial Networks","2022","IEEE Access","10","","","108296","108306","10","10.1109/ACCESS.2022.3213079","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139854821&doi=10.1109%2fACCESS.2022.3213079&partnerID=40&md5=2ccc3cbfe3fb543a68b87ddb93fe05d0","To address the problem of low authenticity of generated images in existing generative models, the transformer super-resolution generative adversarial network(TransSRGAN) model based on the generative adversarial network is proposed. The generator of the model uses the transformer encoder sub-module as the basic module. The features of the input vector are extracted. low-definition images are generated through the transformer encoder submodule, and the low-definition image is up-sampled by the convolutional neural network to complete the image generation. The discriminator of this model uses the convolutional neural network as the basic module. To discriminate the real samples from the generated fake samples, the discriminator extracts the image features by the convolutional neural network. The experimental results show that the TransSRGAN model brings the distribution of the generated samples closer to the training samples, effectively raises the quality of the generated samples, improves the authenticity of the generated samples, and enriches the diversity of the generated samples. During the training process, there was no mode collapse or instability.  © 2013 IEEE.","Authentication; Convolution; Extraction; Image processing; Network coding; Neural networks; Optical resolving power; Convolutional neural network; Encodings; Features extraction; Image generations; Images synthesis; Self-attention; Superresolution; Training data; Transformer; Generative adversarial networks","generative adversarial network; Image generation; self-attention; transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139854821"
"Pan Y.; Liu G.; Wu X.; Chen C.; Zhou Z.; Liu X.","Pan, Yan (57841078300); Liu, Gang (56205673200); Wu, Xinyun (56584944100); Chen, Changlin (57842837700); Zhou, Zhenghao (57841078400); Liu, Xin (57208302039)","57841078300; 56205673200; 56584944100; 57842837700; 57841078400; 57208302039","Font Design Method Based on Multi-scale CycleGAN","2022","2022 2nd IEEE International Conference on Information Communication and Software Engineering, ICICSE 2022","","","","121","125","4","10.1109/ICICSE55337.2022.9828945","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135871631&doi=10.1109%2fICICSE55337.2022.9828945&partnerID=40&md5=a6b98a1688e14df45597e56630d1add4","Font design is an important research direction in art design and has high commercial value. It requires professionals to design fonts, which is not only time-consuming and costly, but also inefficient. Font-To-font translation is a commonly used font design method. Font-To-font translation is essentially the problem of image synthesis. Currently, generative adversarial networks (GANs) have been used for image synthesis and achieved some results. However, for the task of font-To-font translation the existing methods based on GANs generally have low-quality visual effects, such as incomplete fonts and distortion of font details. In order to solve the above problems, we propose a more effective multi-scale CycleGAN for font-To-font translation and the proposed method can obtain the font images with better visual quality. The proposed method is called MSM-CycleGAN. In MSM-CycleGAN, a U-net with multiple outputs (UM) is used as the generator. UM outputs the generated images of multiple scales. And then the outputs of UM are fed into the multi-scale discriminator. Our model uses the unsupervised learning method. This multi-scale discrimination method effectively improves the detailed information of the generated image. Experimental results show that our method performs better than other state-of-The-Art image synthesis methods, and can obtain the font images with higher visual quality.  © 2022 IEEE.","Design; Image enhancement; Learning systems; Unsupervised learning; Arts designs; Design method; Font design; Font images; Images synthesis; Low qualities; Multi-scales; U-net; Visual effects; Visual qualities; Generative adversarial networks","generative adversarial networks; image synthesis; multi-scale; u-net","Conference paper","Final","","Scopus","2-s2.0-85135871631"
"Sams A.; Shomee H.H.","Sams, Ataher (57444532800); Shomee, Homaira Huda (57444754200)","57444532800; 57444754200","GAN-Based Realistic Gastrointestinal Polyp Image Synthesis","2022","Proceedings - International Symposium on Biomedical Imaging","2022-March","","","","","","10.1109/ISBI52829.2022.9761447","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129673761&doi=10.1109%2fISBI52829.2022.9761447&partnerID=40&md5=5fe6c1dc7b66b4fc1974617a9e2dfeb5","Polyps in the gastrointestinal (GI) tract in the human body are one of the most significant symptoms of gastric and colorectal cancer and some other diseases. This paper proposes Generative Adversarial Networks (GANs) based methods that first use a StyleGAN2-ada to generate random polyp masks, which are used to create composite images with healthy GI images. Then a conditional GAN is used to translate these composite images into synthetic polyp images. The proposed approach can produce a high amount of realistic GI polyp images and can increase F1-score and IoU in polyp detection by around 4% when used in the training phase of the YOLOv4 object detector. © 2022 IEEE.","Diseases; Object detection; Composite images; Detection; F1 scores; Gastrointestinal; Gastrointestinal tract; Human bodies; Images synthesis; Network-based; Polyp; Polyp detection; Generative adversarial networks","Detection; generative adversarial networks; polyp; synthesis","Conference paper","Final","","Scopus","2-s2.0-85129673761"
"Chen T.; Zhang Y.; Huo X.; Wu S.; Xu Y.; Wong H.S.","Chen, Tianyi (57212621725); Zhang, Yunfei (57566388000); Huo, Xiaoyang (57419470300); Wu, Si (55495122900); Xu, Yong (57274194400); Wong, Hau San (7402864844)","57212621725; 57566388000; 57419470300; 55495122900; 57274194400; 7402864844","SphericGAN: Semi-supervised Hyper-spherical Generative Adversarial Networks for Fine-grained Image Synthesis","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","9991","10000","9","10.1109/CVPR52688.2022.00976","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141265073&doi=10.1109%2fCVPR52688.2022.00976&partnerID=40&md5=f904e239b45f61697674993f41dd1822","Generative Adversarial Network (GAN)-based models have greatly facilitated image synthesis. However, the model performance may be degraded when applied to finegrained data, due to limited training samples and subtle distinction among categories. Different from generic GAN-s, we address the issue from a new perspective of discovering and utilizing the underlying structure of real data to explicitly regularize the spatial organization of latent space. To reduce the dependence of generative models on labeled data, we propose a semi-supervised hyper-spherical GAN for class-conditional fine-grained image generation, and our model is referred to as SphericGAN. By projecting random vectors drawn from a prior distribution onto a hyper-sphere, we can model more complex distributions, while at the same time the similarity between the resulting latent vectors depends only on the angle, but not on their magnitudes. On the other hand, we also incorporate a mapping network to map real images onto the hyper-sphere, and match latent vectors with the underlying structure of real data via real-fake cluster alignment. As a result, we obtain a spatially organized latent space, which is useful for capturing class-independent variation factors. The experi-mental results suggest that our SphericGAN achieves state-of-the-art performance in synthesizing high-fidelity images with precise class semantics. © 2022 IEEE.","Computer vision; Semantics; Spheres; Fine grained; Hyper-spheres; Images synthesis; Latent vectors; Modeling performance; Network-based modeling; Self-& semi-& meta- image and video synthesis and generation; Semi-supervised; Video generation; Video synthesis; Generative adversarial networks","Self-& semi-& meta- Image and video synthesis and generation","Conference paper","Final","","Scopus","2-s2.0-85141265073"
"Yang C.; Li H.; Wu S.; Zhang S.; Yan H.; Jiao N.; Tang J.; Zhou R.; Liang X.; Zheng T.","Yang, Chaojie (57963224100); Li, Hanhui (56431577600); Wu, Shengjie (57963224200); Zhang, Shengkai (57566604700); Yan, Haonan (57962869900); Jiao, Nianhong (57566604800); Tang, Jie (57962170600); Zhou, Runnan (57963047300); Liang, Xiaodan (55926362100); Zheng, Tianxiang (57233918300)","57963224100; 56431577600; 57963224200; 57566604700; 57962869900; 57566604800; 57962170600; 57963047300; 55926362100; 57233918300","BodyGAN: General-purpose Controllable Neural Human Body Generation","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","7723","7732","9","10.1109/CVPR52688.2022.00758","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141768806&doi=10.1109%2fCVPR52688.2022.00758&partnerID=40&md5=1090ee42d8b4686379ab98feef3d1e4e","Recent advances in generative adversarial networks (GANs) have provided potential solutions for photo-realistic human image synthesis. However, the explicit and individual control of synthesis over multiple factors, such as poses, body shapes, and skin colors, remains difficult for existing methods. This is because current methods mainly rely on a single pose/appearance model, which is limited in dis-entangling various poses and appearance in human images. In addition, such a unimodal strategy is prone to causing severe artifacts in the generated images like color distortions and unrealistic textures. To tackle these issues, this paper proposes a multi-factor conditioned method dubbed BodyGAN. Specifically, given a source image, our Body-GAN aims at capturing the characteristics of the human body from multiple aspects: (i) A pose encoding branch consisting of three hybrid subnetworks is adopted, to generate the semantic segmentation based representation, the 3D surface based representation, and the key point based rep-resentation of the human body, respectively. (ii) Based on the segmentation results, an appearance encoding branch is used to obtain the appearance information of the human body parts. (iii) The outputs of these two branches are represented by user-editable condition maps, which are then processed by a generator to predict the synthesized image. In this way, our BodyGAN can achieve the fine-grained dis-entanglement of pose, body shape, and appearance, and consequently enable the explicit and effective control of syn-thesis with diverse conditions. Extensive experiments on multiple datasets and a comprehensive user study show that our BodyGAN achieves the state-of-the-art performance. © 2022 IEEE.","Computer vision; Encoding (symbols); Semantic Segmentation; Semantics; Signal encoding; Textures; Body shapes; Condition; Encodings; Human bodies; Image and video synthesis and generation; Images synthesis; Individual control; Photo-realistic; Video generation; Video synthesis; Generative adversarial networks","Image and video synthesis and generation","Conference paper","Final","","Scopus","2-s2.0-85141768806"
"Wu Z.; He C.; Yang L.; Kuang F.","Wu, Zhongze (57219399207); He, Chunmei (7402284939); Yang, Liwen (7406279568); Kuang, Fangjun (10939195800)","57219399207; 7402284939; 7406279568; 10939195800","Attentive evolutionary generative adversarial network","2021","Applied Intelligence","51","3","","1747","1761","14","10.1007/s10489-020-01917-8","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092557804&doi=10.1007%2fs10489-020-01917-8&partnerID=40&md5=97a51b7a2090d34ce24642847f382d70","Generative adversarial network (GAN) is an effective method to learn generative models from real data. But there are some drawbacks such as instability, mode collapse and low computational efficiency in the existing GANs. In this paper, attentive evolutionary generative adversarial network (AEGAN) model is proposed in order to improve these disadvantages of GANs. The modified evolutionary algorithm is designed for the AEGAN. In the AEGAN the generator evolves continuously to resist the discriminator by three independent mutations at every batch and only the well-performing offspring (i.e.,the generators) can be preserved at next batch. Furthermore, a normalized self-attention (SA) mechanism is embedded in the discriminator and generator of AEGAN to adaptively assign weights according to the importance of features. We propose careful regulation of the generators evolution and an effective weight assignment to improve diversity and long-range dependence. We also propose a superior training algorithm for AEGAN. With the algorithm, the AEGAN overcomes the shortcomings of traditional GANs brought by single loss function and deep convolution and it greatly improves the training stability and statistical efficiency. Extensive image synthesis experiments on CIFAR-10, CelebA and LSUN datasets are presented to validate the performance of AEGAN. Experimental results and comparisons with other GANs show that the proposed model is superior to the existing models. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Computational efficiency; Efficiency; Adversarial networks; Generative model; Image synthesis; Long range dependence; Loss functions; Statistical efficiency; Training algorithms; Weight assignment; Evolutionary algorithms","Attention mechanism; Evolutionary algorithm; Generative adversarial network; Image generation","Article","Final","","Scopus","2-s2.0-85092557804"
"Yu W.; Zhang X.; Zhang Y.; Zhang Z.; Zhou J.","Yu, Wenxin (36610960300); Zhang, Xuewen (57217279695); Zhang, Yunye (57211359203); Zhang, Zhiqiang (57206280843); Zhou, Jinjia (35099640400)","36610960300; 57217279695; 57211359203; 57206280843; 35099640400","Blind Image Quality Assessment for a Single Image from Text-to-Image Synthesis","2021","IEEE Access","9","","9469895","94656","94667","11","10.1109/ACCESS.2021.3094048","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112102124&doi=10.1109%2fACCESS.2021.3094048&partnerID=40&md5=429978da26e82fc2e251b52a64284c02","A fundamental bottleneck in text-to-image synthesis is that there are rarely subjective quality evaluation metrics for a single generated image. To address this issue, this paper proposed a procedure to evaluate the single generated image, which includes a specific dataset named multiple metrics quality assessment for birds(MMQA Birds) and a learning model named blind generated image evaluator(BGIE). The motivation of our proposal is twofold. On the one hand, subjective image quality evaluation is a human perceptual task; Therefore, it tends to be a process of supervised learning. To the best of our knowledge, there are not any datasets for this study. Thus, we handle this problem via designing a specific dataset. On the other hand, we observed that the spatial content of generated image attracts more attention when humans judge its quality; According to this finding, an efficient machine-learning model that combines both pixel-level features and spatial features is proposed. Extensive experiments manifest our method can solve this problem to some extent. In the generated image dataset, BGIE surpasses the state-of-art NSS-based method by 6.3% in PLCC and SRCC. In practice, we further discuss the rationality of the MMQA Birds dataset and the application of BGIE. It proves that both in subjective and objective aspects, our method achieves convincing results. © 2013 IEEE.","Arts computing; Birds; Learning systems; Quality control; Image quality assessment; Image synthesis; Learning models; Machine learning models; Quality assessment; Spatial features; Subjective image quality; Subjective quality; Image quality","Generated image quality assessment; generative adversarial networks; image quality evaluation dataset","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112102124"
"Jia F.; Xu J.; Sun X.; Ma Y.; Ni M.","Jia, Fei (57298794000); Xu, Jindong (35176864300); Sun, Xiao (57208776868); Ma, Yongli (57219779729); Ni, Mengying (26325030000)","57298794000; 35176864300; 57208776868; 57219779729; 26325030000","Blind image separation method based on cascade generative adversarial networks","2021","Applied Sciences (Switzerland)","11","20","9416","","","","10.3390/app11209416","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117199296&doi=10.3390%2fapp11209416&partnerID=40&md5=4dcab0101086c46f4edbf9800a6c341c","To solve the challenge of single-channel blind image separation (BIS) caused by unknown prior knowledge during the separation process, we propose a BIS method based on cascaded generative adversarial networks (GANs). To ensure that the proposed method can perform well in different scenarios and to address the problem of an insufficient number of training samples, a synthetic network is added to the separation network. This method is composed of two GANs: a U-shaped GAN (UGAN), which is used to learn image synthesis, and a pixel-to-attention GAN (PAGAN), which is used to learn image separation. The two networks jointly complete the task of image separation. UGAN uses the unpaired mixed image and the unmixed image to learn the mixing style, thereby generating an image with the “true” mixing characteristics which addresses the problem of an insufficient number of training samples for the PAGAN. A self-attention mechanism is added to the PAGAN to quickly extract important features from the image data. The experimental results show that the proposed method achieves good results on both synthetic image datasets and real remote sensing image datasets. Moreover, it can be used for image separation in different scenarios which lack prior knowledge and training samples. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Blind image separation; Generative adversarial networks; Remote sensing images; Visual attention","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117199296"
"Zhu J.; Li Z.; Ma H.","Zhu, Jianwei (57217164416); Li, Zhixin (55706981100); Ma, Huifang (35185217200)","57217164416; 55706981100; 35185217200","TT2INet: Text to Photo-realistic Image Synthesis with Transformer as Text Encoder","2021","Proceedings of the International Joint Conference on Neural Networks","2021-July","","","","","","10.1109/IJCNN52387.2021.9534074","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116430264&doi=10.1109%2fIJCNN52387.2021.9534074&partnerID=40&md5=58553f890a0cba91ac4017080d7b84e5","A text-to-image (T2I) generation method is mainly evaluated from two aspects, one is the quality and diversity of the generated images, and the other is the semantic consistency between the generated images and the input sentences. The feature extraction of the text is a very important part. In this paper, we propose a Transformer based Text-to-Image Network (TT2INet). we use the pre-trained Transformer model (ALBERT) to extract the sentence feature vectors and word feature vectors of the input sentences as the basis for the Generative Adversarial Networks (GANs) to generate images. In addition, we also added self-attention mechanism and spectral normalization method to the model. Adding a self-attention mechanism can make the model pay attention to more local features when generating images. Using the spectral normalization method can make the training of GANs more stable. The Inception Scores of our method on Oxford-102, CUB and COCO datasets are 3.90, 4.89 and 26.53, and R-precision scores are 92.55, 87.72 and 92.29, respectively. © 2021 IEEE.","Computer vision; Semantics; Signal encoding; Attention mechanisms; Features extraction; Generation method; Generative adversarial network; Normalization methods; Photo realistic image synthesis; Self-attention; Semantic consistency; Spectral normalization; Transformer; Generative adversarial networks","Generative Adversarial Networks (GANs); self-attention; spectral normalization; Transformer","Conference paper","Final","","Scopus","2-s2.0-85116430264"
"Liu X.; Xing F.; Fakhri G.E.; Woo J.","Liu, Xiaofeng (57193764777); Xing, Fangxu (39162147600); Fakhri, Georges El (7003672447); Woo, Jonghye (23986697400)","57193764777; 39162147600; 7003672447; 23986697400","A unified conditional disentanglement framework for multimodal brain mr image translation","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9433897","10","14","4","10.1109/ISBI48211.2021.9433897","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107178195&doi=10.1109%2fISBI48211.2021.9433897&partnerID=40&md5=aabb7a5072031c0ffdf2cce534d1a4d0","Multimodal MRI provides complementary and clinically relevant information to probe tissue condition and to characterize various diseases. However, it is often difficult to acquire sufficiently many modalities from the same subject due to limitations in study plans, while quantitative analysis is still demanded. In this work, we propose a unified conditional disentanglement framework to synthesize any arbitrary modality from an input modality. Our framework hinges on a cycleconstrained conditional adversarial training approach, where it can extract a modality-invariant anatomical feature with a modality-agnostic encoder and generate a target modality with a conditioned decoder. We validate our framework on four MRI modalities, including T1-weighted, T1 contrast enhanced, T2-weighted, and FLAIR MRI, from the BraTS'18 database, showing superior performance on synthesis quality over the comparison methods. In addition, we report results from experiments on a tumor segmentation task carried out with synthesized data.  © 2021 IEEE.","Medical imaging; Anatomical features; Brain MR images; Comparison methods; Contrast-enhanced; Input modalities; Study plans; T2 weighted; Tumor segmentation; Magnetic resonance imaging","Brain tumor; Deep learning; Generative Adversarial Networks; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107178195"
"Tao L.; Fisher J.; Anaya E.; Li X.; Levin C.S.","Tao, Li (56375019500); Fisher, Jonathan (57211237365); Anaya, Emily (57195418534); Li, Xin (57013614000); Levin, Craig S. (7005508415)","56375019500; 57211237365; 57195418534; 57013614000; 7005508415","Pseudo CT Image Synthesis and Bone Segmentation from MR Images Using Adversarial Networks with Residual Blocks for MR-Based Attenuation Correction of Brain PET Data","2021","IEEE Transactions on Radiation and Plasma Medical Sciences","5","2","9075292","193","201","8","10.1109/TRPMS.2020.2989073","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097943208&doi=10.1109%2fTRPMS.2020.2989073&partnerID=40&md5=7855094d8063d204f89d2175092106b7","For photon attenuation correction, current positron emission tomography systems combined with magnetic resonance imaging (PET/MR) imaging systems typically use methods based on MR image segmentation with subsequent assignment of empirical attenuation coefficients in PET image reconstruction. Delineation of bone in MR images has been challenging, especially in the head and neck areas, due to the difficulty of separating bone from air. In this article, we study deep learning techniques that assist the MR-based attenuation correction (MRAC) process for PET/MR systems, with focus on the brain region. We use a generative adversarial network (GAN) with residual blocks in a conditional setting for this task. We studied the performance of the designed network on image translation and segmentation tasks, which are essential for MRAC. For both tasks, the network generates pseudo CT images that resemble real CT images with normalized pixel value difference of around 5% and structural similarity (SSIM) index of around 0.8.  © 2017 IEEE.","Brain; Computerized tomography; Deep learning; Generative adversarial networks; Image reconstruction; Image segmentation; Medical imaging; Positron emission tomography; Positrons; Attenuation correction; Attenuation maps; CT Image; Deep learning; Generative adversarial network; Image translation; Images segmentations; MR-based attenuation map; Positron emission tomography system combined with magnetic resonance imaging (PET/MR); Tomography system; Magnetic resonance imaging","Deep learning; generative adversarial network (GAN); Image translation and segmentation; MR-based attenuation map; Positron emission tomography systems combined with magnetic resonance imaging (PET/MR)","Article","Final","","Scopus","2-s2.0-85097943208"
"Kumar D.; Mehta M.A.; Chatterjee I.","Kumar, Dheeraj (57205393714); Mehta, Mayuri A. (57647762900); Chatterjee, Indranath (57201465828)","57205393714; 57647762900; 57201465828","Empirical Analysis of Deep Convolutional Generative Adversarial Network for Ultrasound Image Synthesis","2021","Open Biomedical Engineering Journal","15","","","71","77","6","10.2174/1874120702115010071","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128928956&doi=10.2174%2f1874120702115010071&partnerID=40&md5=9876d77fdd72b38a729903d5ace21543","Introduction: Recent research on Generative Adversarial Networks (GANs) in the biomedical field has proven the effectiveness in generating synthetic images of different modalities. Ultrasound imaging is one of the primary imaging modalities for diagnosis in the medical domain. In this paper, we present an empirical analysis of the state-of-the-art Deep Convolutional Generative Adversarial Network (DCGAN) for generating synthetic ultrasound images. Aims: This work aims to explore the utilization of deep convolutional generative adversarial networks for the synthesis of ultrasound images and to leverage its capabilities. Background: Ultrasound imaging plays a vital role in healthcare for timely diagnosis and treatment. Increasing interest in automated medical image analysis for precise diagnosis has expanded the demand for a large number of ultrasound images. Generative adversarial networks have been proven beneficial for increasing the size of data by generating synthetic images. Objective: Our main purpose in generating synthetic ultrasound images is to produce a sufficient amount of ultrasound images with varying representations of a disease. Methods: DCGAN has been used to generate synthetic ultrasound images. It is trained on two ultrasound image datasets, namely, the common carotid artery dataset and nerve dataset, which are publicly available on Signal Processing Lab and Kaggle, respectively. Results: Results show that good quality synthetic ultrasound images are generated within 100 epochs of training of DCGAN. The quality of synthetic ultrasound images is evaluated using Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). We have also presented some visual representations of the slices of generated images for qualitative comparison. Conclusion: Our empirical analysis reveals that synthetic ultrasound image generation using DCGAN is an efficient approach. Other: In future work, we plan to compare the quality of images generated through other adversarial methods such as conditional GAN, progressive GAN. © 2021 Austin et al.","analysis; Article; artificial neural network; common carotid artery; convolutional neural network; data synthesis; deep convolutional generative adversarial network; deep learning; diagnostic imaging; echography; empirical analysis; human; image analysis; image processing; image quality; image segmentation; learning algorithm; mean squared error; signal noise ratio; signal processing","Convolutional neural network; Data synthesis; Deep Convolutional Generative Adversarial Network; Deep learning; Generative adversarial network; Healthcare; Image augmentation; Medical imaging; Radiology; Synthetic image generator; Ultrasound","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128928956"
"Sun H.; Lu Z.; Fan R.; Xiong W.; Xie K.; Ni X.; Yang J.","Sun, Hongfei (57195072153); Lu, Zhengda (57219032033); Fan, Rongbo (57213263950); Xiong, Wenjun (57213268953); Xie, Kai (57202255067); Ni, Xinye (52063553400); Yang, Jianhua (57188667674)","57195072153; 57219032033; 57213263950; 57213268953; 57202255067; 52063553400; 57188667674","Research on obtaining pseudo ct images based on stacked generative adversarial network","2021","Quantitative Imaging in Medicine and Surgery","11","5","","1983","2000","17","10.21037/qims-20-1019","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102470509&doi=10.21037%2fqims-20-1019&partnerID=40&md5=7c7b94f05c3a0e48f68a6fe5b4242e14","Background: To investigate the feasibility of using a stacked generative adversarial network (sGAN) to synthesize pseudo computed tomography (CT) images based on ultrasound (US) images. Methods: The pre-radiotherapy US and CT images of 75 patients with cervical cancer were selected for the training set of pseudo-image synthesis. In the first stage, labeled US images were used as the first conditional GAN input to obtain low-resolution pseudo CT images, and in the second stage, a super-resolution reconstruction GAN was used. The pseudo CT image obtained in the first stage was used as an input, following which a high-resolution pseudo CT image with clear texture and accurate grayscale information was obtained. Five cross validation tests were performed to verify our model. The mean absolute error (MAE) was used to compare each pseudo CT with the same patient's real CT image. Also, another 10 cases of patients with cervical cancer, before radiotherapy, were selected for testing, and the pseudo CT image obtained using the neural style transfer (NSF) and CycleGAN methods were compared with that obtained using the sGAN method proposed in this study. Finally, the dosimetric accuracy of pseudo CT images was verified by phantom experiments. Results: The MAE metric values between the pseudo CT obtained based on sGAN, and the real CT in five-fold cross validation are 66.82±1.59 HU, 66.36±1.85 HU, 67.26±2.37 HU, 66.34±1.75 HU, and 67.22±1.30 HU, respectively. The results of the metrics, namely, normalized mutual information (NMI), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR), between the pseudo CT images obtained using the sGAN method and the ground truth CT (CTgt) images were compared with those of the other two methods via the paired t-test, and the differences were statistically significant. The dice similarity coefficient (DSC) measurement results showed that the pseudo CT images obtained using the sGAN method were more similar to the CTgt images of organs at risk. The dosimetric phantom experiments also showed that the dose distribution between the pseudo CT images synthesized by the new method was similar to that of the CTgt images. Conclusions: Compared with NSF and CycleGAN methods, the sGAN method can obtain more accurate pseudo CT images, thereby providing a new method for image guidance in radiotherapy for cervical cancer. © 2021 AME Publishing Company. All rights reserved.","adult; Article; controlled study; cross validation; dosimetry; feasibility study; female; human; image processing; major clinical study; radiation dose distribution; signal noise ratio; stacked generative adversarial network; uterine cervix cancer; x-ray computed tomography","Generative adversarial networks (gans) submitted aug 30, 2020. accepted for publication nov 30, 2020; Image synthesis; Pseudo computed tomography (ct)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102470509"
"Ye X.; Lu L.","Ye, Xihong (57285420500); Lu, Lu (55312142100)","57285420500; 55312142100","CcGL-GAN: Criss-Cross Attention and Global-Local Discriminator Generative Adversarial Networks for text-to-image synthesis","2021","Proceedings of the International Joint Conference on Neural Networks","2021-July","","","","","","10.1109/IJCNN52387.2021.9533396","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116413800&doi=10.1109%2fIJCNN52387.2021.9533396&partnerID=40&md5=b6750b9e906f1fd5ff85a1510c0eb239","Text-to-image synthesis aims to generate a visually realistic image according to a linguistic text description. Visual quality and semantic consistency are two key objectives. Although remarkable progress has been made in improving visual resolutions leveraging Generative Adversarial Networks (GANs), guaranteeing the semantic conformity remains challenging. In this paper, we address it by proposing a novel Criss-Cross Attention and Global-Local Discriminator Generative Adversarial Networks(CcGL-GAN). CcGL-GAN exploits a Criss-Cross Attention mechanism to capture the variation of contextual description, which enables back generators to generate images more efficiently. Moreover, it utilizes Global-Local discriminators to project low-resolution images onto global linguistic representations, and high-resolution images onto local linguistic representations, which ensures that our model narrows the gap between images and descriptions. Experiments conducted on two publicly available datasets, the CUB and Oxford-102, demonstrate the effectiveness of the proposed CcGL-GAN model. © 2021 IEEE.","Computer vision; Discriminators; Semantics; Global-local; Images synthesis; Key objective; Linguistic representations; Quality consistency; Realistic images; Semantic consistency; Visual qualities; Visual resolutions; Visual semantics; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85116413800"
"Peng X.; Liu W.; Liu B.; Zhang K.; Lu X.; Zhou Y.","Peng, Xuyang (57271645200); Liu, Weifeng (36739405100); Liu, Baodi (16319146900); Zhang, Kai (55769748056); Lu, Xiaoping (57271646400); Zhou, Yicong (24175343600)","57271645200; 36739405100; 16319146900; 55769748056; 57271646400; 24175343600","Leveraging GANs via Non-local Features","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12892 LNCS","","","551","562","11","10.1007/978-3-030-86340-1_44","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115703099&doi=10.1007%2f978-3-030-86340-1_44&partnerID=40&md5=61607314995c2523803bfda7c1c46d77","Recent years, Generative Adversarial Networks (GANs) have achieved tremendous success in image synthesis, which usually employ the convolutional operation to extract image features. However, most existing convolutional GANs only extract features in a local neighborhood at a time, which may often cause a lack of non-local information resulting in generating the wrong semantic object in the wrong position. In this paper, we propose a Graph Convolutional Architecture (GCA) for GANs to tackle this problem. GCA constructs a pixel-level graph structure between image regions through an attention mechanism and leverages Graph Convolutional Networks (GCNs) to extract non-local features. GCA extracts the connections between different regions of the image through GCNs, which is a more effective method of using relationship information than directly adding long-range dependencies to the model. We implement the GCA into Deep Convolutional Generative Adversarial Networks (DCGAN), Self-Attention Generative Adversarial Networks (SAGAN), and Concurrent-Single-Image-GAN (ConSinGAN). Extensive experiments are conducted to verify the performance of GCA. The results demonstrate that the GCA can significantly boost the quality of the generated image with more non-local features. © 2021, Springer Nature Switzerland AG.","Computer vision; Convolution; Information use; Semantics; Attention mechanisms; Convolutional networks; Image features; Images synthesis; Local information; Local neighborhoods; Non-local features; Nonlocal; Pixel level; Semantic objects; Generative adversarial networks","Attention mechanism; Generative adversarial networks; Non-local features","Conference paper","Final","","Scopus","2-s2.0-85115703099"
"Huang J.; Ye D.","Huang, Jiaqing (57216846499); Ye, Dengpan (7102368943)","57216846499; 7102368943","Ground-to-Aerial Image Geo-Localization with Cross-View Image Synthesis","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12890 LNCS","","","412","424","12","10.1007/978-3-030-87361-5_34","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117089999&doi=10.1007%2f978-3-030-87361-5_34&partnerID=40&md5=417fe48b3aed9ea24b933ba48af35be1","The task of ground-to-aerial image geo-localization can be achieved by matching a ground view query image to aerial images with geographic labels in a reference database. It remains challenging due to the drastic change in viewpoint. In this paper, we propose a new cross-view image synthesis conditional generative adversarial networks (cGAN) called Crossview Sequential Fork (CSF) to generate ground images from aerial images. CSF achieves a more detailed synthesis effect by the generation of segmentation maps and edge detection images. And the synthesis ground images are input to the image matching framework Cross View Synthesis Net (CVS-Net) to assist geo-localization, the distance between the descriptors of source ground image and synthesis ground image is calculated to assist the training of the network. CVS-Net is leveraged on the Siamese architecture to do metric learning for the matching task. Moreover, we introduce SARE loss as part of the training procedure and improve it by our data entry form which greatly improves the convergence rate and image retrieval accuracy compared to traditional triplet loss. Experimental results demonstrate the effectiveness and superiority of our proposed method over the state-of-the-art method on two benchmark datasets. © 2021, Springer Nature Switzerland AG.","Antennas; Computer vision; Edge detection; Image enhancement; Image retrieval; Image segmentation; Query processing; Aerial images; Geo-localisation; Geographics; Image geo-localization; Images synthesis; Matchings; Query images; Reference database; Siamese network; View synthesis; Generative adversarial networks","Image geo-localization; Image synthesis; Siamese network","Conference paper","Final","","Scopus","2-s2.0-85117089999"
"Wang R.; Juefei-Xu F.; Luo M.; Liu Y.; Wang L.","Wang, Run (55939516400); Juefei-Xu, Felix (54911989900); Luo, Meng (57226799551); Liu, Yang (56911879800); Wang, Lina (55899978500)","55939516400; 54911989900; 57226799551; 56911879800; 55899978500","FakeTagger: Robust Safeguards against DeepFake Dissemination via Provenance Tracking","2021","MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia","","","","3546","3555","9","10.1145/3474085.3475518","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115702310&doi=10.1145%2f3474085.3475518&partnerID=40&md5=bd700b6275e45fd93a015349bdc84ecd","In recent years, DeepFake is becoming a common threat to our society, due to the remarkable progress of generative adversarial networks (GAN) in image synthesis. Unfortunately, existing studies that propose various approaches, in fighting against DeepFake and determining if the facial image is real or fake, is still at an early stage. Obviously, the current DeepFake detection method struggles to catch the rapid progress of GANs, especially in the adversarial scenarios where attackers can evade the detection intentionally, such as adding perturbations to fool the DNN-based detectors. While passive detection simply tells whether the image is fake or real, DeepFake provenance, on the other hand, provides clues for tracking the sources in DeepFake forensics. Thus, the tracked fake images could be blocked immediately by administrators and avoid further spread in social networks. In this paper, we investigate the potentials of image tagging in serving the DeepFake provenance tracking. Specifically, we devise a deep learning-based approach, named FakeTagger, with a simple yet effective encoder and decoder design along with channel coding to embed message to the facial image, which is to recover the embedded message after various drastic GAN-based DeepFake transformation with high confidence. The embedded message could be employed to represent the identity of facial images, which further contributed to DeepFake detection and provenance. Experimental results demonstrate that our proposed approach could recover the embedded message with an average accuracy of more than 95% over the four common types of DeepFakes. Our research finding confirms effective privacy-preserving techniques for protecting personal photos from being DeepFaked. © 2021 ACM.","Deep learning; Digital forensics; Image annotation; Image coding; Signal encoding; 'current; Deepfake forensic; Detection methods; Embedded messages; Facial images; Image tagging; Images synthesis; Learning-based approach; Passive detection; Provenance tracking; Generative adversarial networks","deepfake forensics; image tagging; provenance tracking","Conference paper","Final","","Scopus","2-s2.0-85115702310"
"Zhang Z.; Zhou J.; Yu W.; Jiang N.","Zhang, Zhiqiang (57206280843); Zhou, Jinjia (35099640400); Yu, Wenxin (36610960300); Jiang, Ning (57212426361)","57206280843; 35099640400; 36610960300; 57212426361","Drawgan: Text to image synthesis with drawing generative adversarial networks","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","4195","4199","4","10.1109/ICASSP39728.2021.9414166","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115157468&doi=10.1109%2fICASSP39728.2021.9414166&partnerID=40&md5=c83246e195969a1a9b20aa5efaee26fa","In this paper, we propose a novel drawing generative adversarial networks (DrawGAN) for text-to-image synthesis. The whole model divides the image synthesis into three stages by imitating the process of drawing. The first stage synthesizes the simple contour image based on the text description, the second stage generates the foreground image with detailed information, and the third stage synthesizes the final result. Through the step by step synthesis process from simple to complex and easy to difficult, the model can draw the corresponding results step by step and finally achieve the higher-quality image synthesis effect. Our method is validated on the Caltech-UCSD Birds 200 (CUB) dataset and the Microsoft Common Objects in Context (MS COCO) dataset. The experimental results demonstrate the effectiveness and superiority of our method. In terms of both subjective and objective evaluation, our method’s results surpass the existing state-of-the-art methods. ©2021 IEEE","Electrical engineering; Signal processing; Adversarial networks; Contour image; Foreground images; Image synthesis; Quality image; State-of-the-art methods; Subjective and objective evaluations; Synthesis process; Image processing","Computer Vision; Deep Learning; Generative Adversarial Networks; Text-to-Image Synthesis","Conference paper","Final","","Scopus","2-s2.0-85115157468"
"Emami H.; Dong M.; Nejad-Davarani S.P.; Glide-Hurst C.K.","Emami, Hajar (57203140067); Dong, Ming (55724049700); Nejad-Davarani, Siamak P. (36145518600); Glide-Hurst, Carri K. (22950635400)","57203140067; 55724049700; 36145518600; 22950635400","SA-GAN: Structure-Aware GAN for Organ-Preserving Synthetic CT Generation","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12906 LNCS","","","471","481","10","10.1007/978-3-030-87231-1_46","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116417054&doi=10.1007%2f978-3-030-87231-1_46&partnerID=40&md5=e7a14beac4e94392a3bc1147a8bbc3ae","In medical image synthesis, model training could be challenging due to the inconsistencies between images of different modalities even with the same patient, typically caused by internal status/tissue changes as different modalities are usually obtained at a different time. This paper proposes a novel deep learning method, Structure-aware Generative Adversarial Network (SA-GAN), that preserves the shapes and locations of in-consistent structures when generating medical images. SA-GAN is employed to generate synthetic computed tomography (synCT) images from magnetic resonance imaging (MRI) with two parallel streams: the global stream translates the input from the MRI to the CT domain while the local stream automatically segments the inconsistent organs, maintains their locations and shapes in MRI, and translates the organ intensities to CT. Through extensive experiments on a pelvic dataset, we demonstrate that SA-GAN provides clinically acceptable accuracy on both synCTs and organ segmentation and supports MR-only treatment planning in disease sites with internal organ status changes. © 2021, Springer Nature Switzerland AG.","Computerized tomography; Deep learning; Magnetic resonance imaging; Medical computing; Medical imaging; Radiotherapy; Images synthesis; Learning methods; Method structures; Model training; Network structures; Structure-aware; Structure-aware GAN; Synthesis models; Synthetic CT; Tissue changes; Generative adversarial networks","Radiation therapy; Structure-Aware GAN; Synthetic CT","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116417054"
"Zheng Z.; Liu H.; Yang F.; Zheng X.; Yu Z.; Zhang S.","Zheng, Ziqiang (57202612793); Liu, Hongzhi (57221581177); Yang, Fan (57196213963); Zheng, Xingyu (57221743237); Yu, Zhibin (36999020600); Zhang, Shaoda (57203909914)","57202612793; 57221581177; 57196213963; 57221743237; 36999020600; 57203909914","Representation-guided generative adversarial network for unpaired photo-to-caricature translation","2021","Computers and Electrical Engineering","90","","106999","","","","10.1016/j.compeleceng.2021.106999","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100021923&doi=10.1016%2fj.compeleceng.2021.106999&partnerID=40&md5=35c28c51da5ee5d1007a6c156f1a2b1e","Imitating the painting style of one caricature source is an interesting and important application. It requires to capture the caricature style from one reference image, and generate target caricature image with similar style representation based on one source photo. Recently, image-to-image translation is a proven and potential framework for photo-to-caricature task. However, it still suffers from three drawbacks: (1) annotating aligned photo-to-caricature pairs is expensive and time-consuming; (2) the photo-to-caricature requires to capture and exaggerate the high-level semantic representations; and (3) the multiple painting styles increase the translation difficulty. To tackle these issues, we propose an innovative representation-guided photo-to-caricature translation framework based on unpaired images. The representation-guided scheme is designed to transfer the selected caricature style. To improve image synthesis quality, we introduce one feature-pyramid adversarial network (FPAN) to provide multiple feature-level constrains. The comprehensive experiments on various caricature datasets show excellent imitation capabilities of the proposed method. © 2021","Semantics; Adversarial networks; Feature pyramid; High level semantics; Image synthesis; Image translation; Multiple features; Reference image; Image enhancement","Generative adversarial network; Image-to-image translation; Imitation; Photo-to-caricature","Article","Final","","Scopus","2-s2.0-85100021923"
"Mokalla S.R.; Bourlai T.","Mokalla, Suha Reddy (57215273795); Bourlai, Thirimachos (8850031300)","57215273795; 8850031300","Effects of Demographics and Photometric Normalization on Image Translation GANs for Cross-Spectral Face Recognition","2021","Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021","","","","2109","2118","9","10.1109/BigData52589.2021.9671292","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125329605&doi=10.1109%2fBigData52589.2021.9671292&partnerID=40&md5=6b1f82c587aaecb66d7dbdc69d99cdec","This paper focuses on thermal-to-visible face matching through image synthesis. Most of the legacy face image datasets are composed of visible band data. Thermal band as well as dual band, i.e. visible and thermal face datasets, are limited. Operating in the thermal band and therefore working on visible thermal face recognition (FR) systems can be beneficial in various scenarios. The challenge is cross-spectral matching, i.e. matching gallery, visible band, face images against thermal ones. To address this problem, we train and test two of the most popular image-to-image translation Generative Adversarial Networks (GANs). These are Pix2pix and StarGAN2. In this work, the two aforementioned GAN trained models are tested, and the visible face images generated are matched against the ground truth visible faces using one of the most powerful visible-to-visible face matching algorithms, namely Facenet. We also perform an ablation study where the original thermal and visible images are photometrically normalized before training the image synthesis-specific models. The main outcomes of our study are that the FR accuracy from the pix2pix model did not vary significantly; when using StarGAN2, the original face images yield much higher accuracy compared to the photometrically normalized ones; finally, we observe that, when using the pix2pix model for image synthesis, bearded and non-Caucasian generated face images suffer the most from different noise factors. Specifically, the FR accuracy when using pix2pix after image synthesis yields a face verification area under curve (AUC) of 58.3%, while the same models when tested on data excluding bearded and non-Caucasian faces yields an accuracy of 68.6%. © 2021 IEEE.","Computer vision; Generative adversarial networks; Photometry; Cross-spectral; Images synthesis; Multi-spectral; Photometric normalization; Pix2pix; Stargan2; Thermal; Thermal spectra; Thermal-to-visible; Visible spectrums; Face recognition","Cross-spectral; Face recognition; Image synthesis; Multi-spectral; Photometric normalization; Pix2pix; StarGAN2; Thermal spectrum; Thermal-to-visible; Visible spectrum","Conference paper","Final","","Scopus","2-s2.0-85125329605"
"Chikontwe P.; Gao Y.; Lee H.J.","Chikontwe, Philip (57195637251); Gao, Yongbin (56510441200); Lee, Hyo Jong (55706807400)","57195637251; 56510441200; 55706807400","Transformation guided representation GAN for pose invariant face recognition","2021","Multidimensional Systems and Signal Processing","32","2","","633","649","16","10.1007/s11045-020-00752-x","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098655352&doi=10.1007%2fs11045-020-00752-x&partnerID=40&md5=2ff679001a6d783e4c93f02313b184b5","Face recognition is an important topic in the field of computer vision and has been a vital biometric technique for identity authentication. It is widely used in areas such as public security, military, and daily life. However, face recognition is inherently a challenging problem due to variations in poses, facial expressions, age, and occlusion. In this work, we propose a generative adversarial network (GAN) architecture that disentangles identity and pose variations to learn generative and discriminative representations for pose-invariant face recognition. We use an iterative warping scheme that achieves better results than with the use of a single generator. The features from the encoder are considered pose-invariant features for face recognition, and evaluations on databases demonstrate the usefulness of this approach over prior methods. For example, we report 97.0% (+ 12.7%) and 90.5% (+ 8.4%) accuracy on the Feret and Caspeal datasets compared to 78.2% achieved by the best method without warping. In particular, there are two notable novelties. First, the disentangled architecture GAN (D-GAN) performs frontal face synthesis via an encoder-decoder structure in the generator with the pose variations provided to the decoder and discriminator. Second, we utilize the generator encoder as a spatial transformer network that seeks realistic image synthesis in the geometric warp parameter space. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.","Decoding; Gesture recognition; Iterative methods; Network architecture; Signal encoding; Adversarial networks; Biometric techniques; Encoder-decoder; Facial Expressions; Identity authentication; Parameter spaces; Pose-invariant face recognition; Realistic image synthesis; Face recognition","Deep learning; Pose invariant face recognition; Transformation guided representation","Article","Final","","Scopus","2-s2.0-85098655352"
"Li F.; Zhang P.; Huang W.","Li, Feihong (57250431100); Zhang, Peng (55547108553); Huang, Wei (56195325600)","57250431100; 55547108553; 56195325600","A novel framework to synthesize arterial spin labeling images using difference images","2021","Proceedings - 2021 International Conference on Computer Engineering and Artificial Intelligence, ICCEAI 2021","","","","27","31","4","10.1109/ICCEAI52939.2021.00005","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117568468&doi=10.1109%2fICCEAI52939.2021.00005&partnerID=40&md5=12bfb56c1a7a0e3480700bde62b40d92","Arterial spin labeling (ASL) images that are capable to quantitatively measure the cerebral blood flow receive increasing research attention in recent dementia diseases diagnosis studies. However, this important and relatively new imaging modality is unfortunately not commonly seen in many well-established image-based dementia datasets, including the ADNI-1/2/3/Go datasets. Hence, synthesizing ASL images to supplement this important modality is valuable. In this study, a new framework based on a cascade of generative adversarial networks (GANs) and difference images generated from a Laplacian pyramid is proposed. This framework is novel as it is the first attempt to incorporate difference images for synthesizing medical images. Experimental results based on a 355-demented patient dataset and ADNI-1 dataset suggest that, this new framework outperforms all state-of-the-arts in ASL image synthesis. Also, synthesized ASL images obtained by this new framework are capable to significantly improve the accuracy of dementia diseases diagnosis performance. © 2021 IEEE.","Arts computing; Diagnosis; Image enhancement; Medical imaging; Neurodegenerative diseases; A-Laplacian; Arterial spin labeling; Cerebral blood flow; Difference images; Disease diagnosis; Image-based; Images synthesis; Imaging modality; Labeling image; Laplacian Pyramid; Generative adversarial networks","Difference image; Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85117568468"
"Antal L.; Bodó Z.","Antal, László (57557746600); Bodó, Zalán (24478251400)","57557746600; 24478251400","Feature axes orthogonalization in semantic face editing","2021","Proceedings - 2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing, ICCP 2021","","","","163","169","6","10.1109/ICCP53602.2021.9733549","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127407721&doi=10.1109%2fICCP53602.2021.9733549&partnerID=40&md5=4022513a93ab8a357c4e914f0bed95dc","Human image synthesis is the technology that allows a computer program to create realistic photos of non-existing people. Though it is a relatively novel research topic that is mostly used to synthesize human faces, generating moving human figures is also possible using this method.At first, composing the believable and realistic images was a complex process. To achieve decent results, photo-realistic modelling, animating and mapping of the soft dynamics of the human body was required. Nowadays these methods are replaced by approaches based on machine learning and neural networks.Our system is able to create realistic images, consisting of three main components. The first component is a Generative Adversarial Network (GAN) that can generate a random face from a noise vector. Secondly, a convolutional neural network is responsible to recognize facial features on the input photos. Lastly, a regression model computes the correspondence between the input noise vector and output features of the generated face.Using a well-known face dataset, we report results applying the newly proposed model and we also analyze the accuracy and the plausibility of these results. © 2021 IEEE.","Computer vision; Convolutional neural networks; Generative adversarial networks; Regression analysis; Semantic Segmentation; Semantic Web; Composite sketch; Custom loss function; Human image synthesis; Images synthesis; Least Square; Loss functions; Noise vectors; Orthogonality; Realistic images; Semantic face editing; Semantics","composite sketches; custom loss function; Generative Adversarial Networks; human image synthesis; least squares; orthogonality; semantic face editing","Conference paper","Final","","Scopus","2-s2.0-85127407721"
"Touati R.; Le W.T.; Kadoury S.","Touati, Redha (56202730100); Le, William Trung (57219088952); Kadoury, Samuel (16230455400)","56202730100; 57219088952; 16230455400","A feature invariant generative adversarial network for head and neck MRI/CT image synthesis","2021","Physics in Medicine and Biology","66","9","095001","","","","10.1088/1361-6560/abf1bb","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105086451&doi=10.1088%2f1361-6560%2fabf1bb&partnerID=40&md5=1170d01437cbe7f6c2625e94dfa3b06a","With the emergence of online MRI radiotherapy treatments, MR-based workflows have increased in importance in the clinical workflow. However proper dose planning still requires CT images to calculate dose attenuation due to bony structures. In this paper, we present a novel deep image synthesis model that generates in an unsupervised manner CT images from diagnostic MRI for radiotherapy planning. The proposed model based on a generative adversarial network (GAN) consists of learning a new invariant representation to generate synthetic CT (sCT) images based on high frequency and appearance patterns. This new representation encodes each convolutional feature map of the convolutional GAN discriminator, leading the training of the proposed model to be particularly robust in terms of image synthesis quality. Our model includes an analysis of common histogram features in the training process, thus reinforcing the generator such that the output sCT image exhibits a histogram matching that of the ground-truth CT. This CT-matched histogram is embedded then in a multi-resolution framework by assessing the evaluation over all layers of the discriminator network, which then allows the model to robustly classify the output synthetic image. Experiments were conducted on head and neck images of 56 cancer patients with a wide range of shape sizes and spatial image resolutions. The obtained results confirm the efficiency of the proposed model compared to other generative models, where the mean absolute error yielded by our model was 26.44(0.62), with a Hounsfield unit error of 45.3(1.87), and an overall Dice coefficient of 0.74(0.05), demonstrating the potential of the synthesis model for radiotherapy planning applications. © 2021 Institute of Physics and Engineering in Medicine.","Convolution; Diagnosis; Graphic methods; Image resolution; Magnetic resonance imaging; Radiotherapy; Adversarial networks; Clinical workflow; Histogram features; Histogram matching; Invariant representation; Mean absolute error; Radiotherapy planning; Radiotherapy treatment; Computerized tomography","conditional GAN; feature invariant learning; head and neck radiotherapy; histogram matching; MRI-CT image synthesis; multi-resolution edge features","Article","Final","","Scopus","2-s2.0-85105086451"
"Cao H.; Lai Y.; Chen Q.; Yang F.","Cao, Huibin (57343995800); Lai, Yongxuan (23389417600); Chen, Quan (57275075500); Yang, Fan (57216129178)","57343995800; 23389417600; 57275075500; 57216129178","A Semi-supervised Defect Detection Method Based on Image Inpainting","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13033 LNAI","","","97","108","11","10.1007/978-3-030-89370-5_8","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119291129&doi=10.1007%2f978-3-030-89370-5_8&partnerID=40&md5=cad95ab0af1e577f888480b7041fb2c0","Defect detection plays an important role in the industrial field. Because the defective images are often insufficient and defects can be various, defective image synthesis is commonly used and models always tend to learn the distribution of defects. However, the complexity of defective image synthesis and difficulty of detecting unseen defects are still the main challenges. To solve these problems, this paper proposes a semi-supervised defect detection method based on image inpainting, denoted as SDDII, which combines the training strategies of CycleGAN and Pix2Pix. First, we train a defect generator unsupervisedly to generate defective images. Second, we train the defect inpaintor supervisedly using the generated images. Finally, the defect inpaintor is used to inpainting the defects, and the defective areas can be segmented by comparing images before and after inpainting. Without ground truth for training, SDDII achieves better results than the naive CycleGAN, and comparable results with UNET which is supervised learning. In addition, SDDII learns the distribution of contents in defect-free images so it has good adaptability for defects unseen before. © 2021, Springer Nature Switzerland AG.","Defects; Image processing; Automated optical inspection; Defect detection; Defect detection method; Image Inpainting; Images synthesis; Industrial fields; Inpainting; Learn+; Semi-supervised; Training strategy; Generative adversarial networks","Automated optical inspection; Defect detection; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85119291129"
"Buthgamumudalige V.U.; Wirasingha T.","Buthgamumudalige, Vinula Uthsara (57354866000); Wirasingha, Torin (57223039775)","57354866000; 57223039775","Neural Architecture Search for Generative Adversarial Networks: A Review","2021","2021 10th International Conference on Information and Automation for Sustainability, ICIAfS 2021","","","","246","251","5","10.1109/ICIAfS52090.2021.9605804","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119961106&doi=10.1109%2fICIAfS52090.2021.9605804&partnerID=40&md5=20acf6f89453524072e0af363b0f3ecd","Achieving competent image generation results in Generative Adversarial Networks (GANs) has been observed to be weighed down by barriers such as mode collapse and being error-prone, due to the time consumption, effort and domain expertise required to arduously design novel Neural Network Architectures through trial-and-error. Neural Architecture Search (NAS) which is a subfield of Automated Machine Learning (AutoML), has gained recent attention for its ability to automate the process of Neural Network architecture generation. NAS has achieved promising results in many computer-vision fields such as image classification, image segmentation, object detection and has now found its way into image generation tasks of Generative Adversarial Networks. Recent works on NAS systems that generate novel GAN architectures have proven this direction of applying NAS is promising by producing results that rival current state-of-the-art human-made architectures. The existing works related to NAS for GANs along with their approaches, performance and image generation results has been explored in this paper to assist and inspire research in the domain of Image Synthesis via NAS for GANs. © 2021 IEEE.","Computer vision; Image segmentation; Network architecture; Neural networks; Object detection; Automated machine learning; Automated machines; Error prones; Image generations; Images synthesis; Machine-learning; Neural architecture search; Neural architectures; Neural network architecture; Time consumption; Generative adversarial networks","Automated Machine Learning; Generative Adversarial Networks; Image Generation; Image Synthesis; Neural Architecture Search","Conference paper","Final","","Scopus","2-s2.0-85119961106"
"Espejo-Garcia B.; Mylonas N.; Athanasakos L.; Vali E.; Fountas S.","Espejo-Garcia, Borja (55991321400); Mylonas, Nikos (57215033015); Athanasakos, Loukas (57215488442); Vali, Eleanna (57217859204); Fountas, Spyros (12753870500)","55991321400; 57215033015; 57215488442; 57217859204; 12753870500","Combining generative adversarial networks and agricultural transfer learning for weeds identification","2021","Biosystems Engineering","204","","","79","89","10","10.1016/j.biosystemseng.2021.01.014","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100388257&doi=10.1016%2fj.biosystemseng.2021.01.014&partnerID=40&md5=5ffa91fe1b6d1c2c3a08c983ea032a5e","In recent years, automatic weed control has emerged as a promising alternative for reducing the amount of herbicide applied to the field, instead of conventional spraying. The use of artificial intelligence through the implementation of deep learning for early weeds identification has been one of the engines to boost this progress. However, these techniques usually need very large datasets coping with real-world conditions, which are scarce in the agricultural domain. To address the lack of such datasets, this paper proposes a methodology that combines the use of agricultural transfer learning and the creation of artificial images by generative adversarial networks (GANs). Several architectures and configurations have been evaluated on a dataset containing images of tomato and black nightshade. The best configuration was a combination of GANs creating plausible synthetic images and the Xception network, with a performance of 99.07% on the test set and 93.23% on a noisy version of the same set. Other architectures, such as Inception or DenseNet have also been evaluated, and they obtained promising results by using GANs. According to the results, the combination of advanced transfer learning and data augmentation techniques through GANs should be deeply studied in the future with more complex datasets. © 2021 IAgrE","Agricultural robots; Agriculture; Deep learning; Large dataset; Network architecture; Weed control; Adversarial networks; Artificial image; Complex datasets; Data augmentation; Large datasets; Noisy versions; Real-world; Synthetic images; Transfer learning","Deep learning; GAN; Image synthesis; Precision agriculture; Transfer learning; Weeds identification","Article","Final","","Scopus","2-s2.0-85100388257"
"Geng L.; Zhang Y.; Xiao Z.; Yan F.","Geng, Lei (35837861900); Zhang, Yanyan (57282628500); Xiao, Zhitao (7402447215); Yan, Fengfeng (57219332359)","35837861900; 57282628500; 7402447215; 57219332359","Research on Instrument Image Generation Method Based on Conditional Generative Adversarial Network","2021","ACM International Conference Proceeding Series","","","","106","112","6","10.1145/3467707.3467722","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116228055&doi=10.1145%2f3467707.3467722&partnerID=40&md5=25b7346f67618f7302c0e056b8515e52","In recent years, generative adversarial networks have achieved great success in image synthesis.They have amazing ability to generate clear and realistic images. However, as we all know, GAN networks are difficult to adapt to different data sets, for example, in the process of image coding, the images of instrument are complex, instrument characters vary in size, and characters of different sizes need different receptive fields, so new requirements are put forward for the GAN networks. In this paper, we propose UYF-Net, an instrument image generator network, to solve this problem.The network uses Y-Fusion module many times, and fuses high-level and low-level semantic features according to the importance of each channel. Experiments show that the network achieves better performance in Mish activation function, mixed loss function of L1 with MS-SSIM and LayerNormal normalization. © 2021 ACM.","Image coding; Semantics; Data generation; Data set; Different sizes; Fusion modules; Generation method; Image generations; Images synthesis; Realistic images; UYF-net; Y-fusion module; Generative adversarial networks","Data generation; Generative Adversarial Network; UYF-Net; Y-Fusion module","Conference paper","Final","","Scopus","2-s2.0-85116228055"
"Lan H.; Toga A.W.; Sepehrband F.","Lan, Haoyu (57221354760); Toga, Arthur W. (57223943025); Sepehrband, Farshid (36635002000)","57221354760; 57223943025; 36635002000","Three-dimensional self-attention conditional GAN with spectral normalization for multimodal neuroimaging synthesis","2021","Magnetic Resonance in Medicine","86","3","","1718","1733","15","10.1002/mrm.28819","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105136937&doi=10.1002%2fmrm.28819&partnerID=40&md5=d0185338b1144da95e8a81becb741f01","Purpose: To develop a new 3D generative adversarial network that is designed and optimized for the application of multimodal 3D neuroimaging synthesis. Methods: We present a 3D conditional generative adversarial network (GAN) that uses spectral normalization and feature matching to stabilize the training process and ensure optimization convergence (called SC-GAN). A self-attention module was also added to model the relationships between widely separated image voxels. The performance of the network was evaluated on the data set from ADNI-3, in which the proposed network was used to predict PET images, fractional anisotropy, and mean diffusivity maps from multimodal MRI. Then, SC-GAN was applied on a multidimensional diffusion MRI experiment for superresolution application. Experiment results were evaluated by normalized RMS error, peak SNR, and structural similarity. Results: In general, SC-GAN outperformed other state-of-the-art GAN networks including 3D conditional GAN in all three tasks across all evaluation metrics. Prediction error of the SC-GAN was 18%, 24% and 29% lower compared to 2D conditional GAN for fractional anisotropy, PET and mean diffusivity tasks, respectively. The ablation experiment showed that the major contributors to the improved performance of SC-GAN are the adversarial learning and the self-attention module, followed by the spectral normalization module. In the superresolution multidimensional diffusion experiment, SC-GAN provided superior predication in comparison to 3D Unet and 3D conditional GAN. Conclusion: In this work, an efficient end-to-end framework for multimodal 3D medical image synthesis (SC-GAN) is presented. The source code is also made available at https://github.com/Haoyulance/SC-GAN. © 2021 International Society for Magnetic Resonance in Medicine","Attention; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Neuroimaging; Anisotropy; Neuroimaging; Optical resolving power; Ablation experiments; Adversarial learning; Adversarial networks; Diffusion experiments; Fractional Anisotropy; Optimization convergence; Spectral normalization; Structural similarity; article; attention; controlled study; diffusion weighted imaging; fractional anisotropy; human; learning; mean diffusivity; neuroimaging; prediction; synthesis; attention; image processing; neuroimaging; nuclear magnetic resonance imaging; three-dimensional imaging; Magnetic resonance imaging","3D GAN; MRI; PET; self-attention; spectral normalization; synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105136937"
"Zhu Y.; Ge N.; Huang J.; Zhu Y.; Zheng B.; Zhang W.","Zhu, Yonghua (55723795700); Ge, Ning (57381503800); Huang, Jieyu (57381325500); Zhu, Yunwen (57207985692); Zheng, Binghui (57382387600); Zhang, Wenjun (57206987547)","55723795700; 57381503800; 57381325500; 57207985692; 57382387600; 57206987547","Enriching Attributes from Knowledge Graph for Fine-grained Text-to-Image Synthesis","2021","ACM International Conference Proceeding Series","","","3487155","","","","10.1145/3487075.3487155","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121498419&doi=10.1145%2f3487075.3487155&partnerID=40&md5=565a458fd59bc0d343f1a3d08ed2eeb1","In this paper, we propose an Attribute-Rich Generative Adversarial Network (AttRiGAN) for text-to-image synthesis, which enriches the simple text description by associating knowledge graph and embedding it in the synthesis task in the form of an attribute matrix. Higher fine-grained images can be synthesized with AttRiGAN, and the synthesized sample are more similar to the objects that exist in the real world, since they are driven by attributes which are enriched from the knowledge graph. The experiments conducted on two widely-used fine-grained image datasets show that our AttRiGAN allows a significant improvement in fine-grained textto- image synthesis.. © 2021 Association for Computing Machinery. All rights reserved.","Graph embeddings; Image enhancement; Knowledge graph; Attention mechanisms; Attribute embedding; Embeddings; Fine grained; Images synthesis; Knowledge embedding; Knowledge graphs; Simple++; Synthesised; Text-to-image synthesis; Generative adversarial networks","Attention mechanism; Attribute embedding; Knowledge graph; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85121498419"
"Zhan B.; Li D.; Wang Y.; Ma Z.; Wu X.; Zhou J.; Zhou L.","Zhan, Bo (57221803799); Li, Di (57226876886); Wang, Yan (56039981100); Ma, Zongqing (57191706405); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Zhou, Luping (23398846800)","57221803799; 57226876886; 56039981100; 57191706405; 57221065403; 21234416400; 23398846800","LR-cGAN: Latent representation based conditional generative adversarial network for multi-modality MRI synthesis","2021","Biomedical Signal Processing and Control","66","","102457","","","","10.1016/j.bspc.2021.102457","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100236885&doi=10.1016%2fj.bspc.2021.102457&partnerID=40&md5=c8719730cb1403f1925201023d611be1","Objective: This work aims to synthesize a real-like missing MRI modality using multiple modalities those already obtained, thus providing more abundant diagnostic information, and promoting the improvement of some downstream tasks, such as segmentation and diagnosis. Methods: With an adversarial network modelling the nonlinear mapping between the inputs and the output, our proposed LR-cGAN extracts the inherent latent representations from different MRI modalities with N collaboratively trained encoders, and fuses them by a latent space processing network (LSPN) composed of several residual blocks. Apart from L1 loss, the image gradient difference loss (GDL) is considered additionally as the objective function to alleviate the problem of insufficient image sharpening. To validate the effectiveness of LR-cGAN, corresponding experiments were evaluated by peak SNR (PSNR), structural similarity index (SSIM) and normalized root-mean-square error (NRMSE) on BRATS 2015 dataset. Results: Compared to single-modality input, two-modality input improves the synthesis results by 1.196 dB PSNR, 0.019 SSIM and 0.04 NRMSE. With more inputs added, the synthesis performance exhibits an increasing trend. Once any key component, that is, LSPN, GDL loss or adversarial loss, is removed, the quality of the results will reduce to a lower level, proving their contributions to our model. Meanwhile, the final performance of our LR-cGAN network outperforms REPLICA, M-GAN, MILR and sGAN in all metrics on different synthesis tasks, demonstrating its superiority. Conclusion: Our proposed LR-cGAN has the flexible ability of receiving multiple modalities and generating realistic images compared to real modality images, having the potential to supplement diagnostic information in clinical. © 2021 Elsevier Ltd","Biomedical engineering; Control engineering; Signal processing; Adversarial networks; Image sharpening; Multiple modalities; Nonlinear mappings; Objective functions; Realistic images; Root mean square errors; Structural similarity indices (SSIM); article; controlled study; human; nuclear magnetic resonance imaging; synthesis; Mean square error","Generative adversarial network (GAN); Image synthesis; Latent representation; Magnetic resonance imaging (MRI)","Article","Final","","Scopus","2-s2.0-85100236885"
"Mokalla S.R.; Bourlai T.","Mokalla, Suha Reddy (57215273795); Bourlai, Thirimachos (8850031300)","57215273795; 8850031300","Robust LWIR-based Eye Center Detection through Thermal to Visible Image Synthesis","2021","Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021","","","","","","","10.1109/FG52635.2021.9667069","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125036080&doi=10.1109%2fFG52635.2021.9667069&partnerID=40&md5=8bf461e5d1245977a934d9414ef0bdf3","This paper proposes a novel approach to automatically detect the eye centers in challenging thermal (Long-Wave Infrared; 8 - 14 μm) face images so they can be geometrically normalized, which is an important part of face recognition systems. Developing a new LWIR based eye center detection model would require many thermal images, which is not possible due to the unavailability of publicly available large scale LWIR face datasets. An alternative solution would be to use a pre-trained, visible band based facial landmark detection model and test it on LWIR face images. In the latter case, the challenges are the significant differences between the visible and thermal face images. In this paper, we focus on addressing this research gap by proposing a solution that is based on the following approach. First, we synthesize visible from thermal face images. Then, we exploit the existing robust visible band facial landmark detection models to detect eye centers in the synthesized visible band face images. While we empirically test different image synthesis models, we determine that StarGAN2 (an image-to-image translation generative adversarial network model that learns a mapping between the different visual domains) yields the highest eye center detection accuracy when compared to the other state-of-the-art models. Thus, we train a StarGAN2 model to be able to synthesize good quality visible band images from their thermal band counterparts. Next, we use an efficient visible band based facial landmark detection model to detect the eye centers in the synthesized visible band face images. Finally, we map these coordinates to the original LWIR face images, which are used for geometric normalization and, finally run a set of face recognition experiments. Compared to the baseline model, our approach increases the eye detection accuracy by 14%, 50%, and 30% when the normalized error (e) is set to be ≤ 0.05, ≤ 0.10, and ≤ 0.25 respectively compared to the baseline model. Our approach yields up to a 15 % and 7% increase in face recognition accuracy when using advanced deep-learning based matchers, namely Facenet and VGGFace respectively, and by 30% when using other conventional face matching techniques such as LBP-LTP matchers. © 2021 IEEE.","Deep learning; Eye protection; Generative adversarial networks; Infrared devices; Infrared radiation; Large dataset; Baseline models; Center detection; Detection accuracy; Detection models; Face images; Facial landmark detection; Images synthesis; Synthesised; Thermal; Visible band; Face recognition","","Conference paper","Final","","Scopus","2-s2.0-85125036080"
"Xie Z.; Su X.; Liu S.; Zhang G.; Ma L.","Xie, Zhifeng (10044658400); Su, Xu (57223924424); Liu, Siwei (57225134639); Zhang, Guisong (57207989715); Ma, Lizhuang (8930473900)","10044658400; 57223924424; 57225134639; 57207989715; 8930473900","Hair Attribute Transfer via Deep Feature Fusion; [深度特征融合的头发属性转移方法]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","5","","772","779","7","10.3724/SP.J.1089.2021.18544","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106474121&doi=10.3724%2fSP.J.1089.2021.18544&partnerID=40&md5=8268efef3813f0416d4df0621aa8cc7c","To tackle the problem that existing attribute transfer methods can't transfer hair attributes effectively, a method of hair attribute transfer based on deep feature fusion is presented. This method includes three subnetworks which are responsible for feature extraction, attribute vector extraction and image synthesis. Firstly, feature extraction network extracts features from original images, and keeps the identity of original images unchanged by adding a reconstruction loss. At the same time, attribute vector extraction network constructs the mapping model of hair features and hair attributes, and generates the attribute vector. Finally, the synthesis network takes the fusion result of image features and the attribute vector as input, and generates final results. Various attribute transfer experiments on FFHQ show that the proposed method can effectively transfer hair attributes and generate high-resolution results. Experiments on Celeba show that the proposed method can achieve better visual quality than existing popular attribute transfer methods. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Extraction; Feature extraction; Vectors; Attribute vectors; Feature fusion; High resolution; Image features; Image synthesis; Original images; Transfer method; Visual qualities; Image processing","Attribute transfer; Feature fusion; Generative adversarial networks; Hair","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85106474121"
"Semiletov A.; Vatian A.; Krychkov M.; Khanzhina N.; Klochkov A.; Zubanenko A.; Soldatov R.; Shalyto A.; Gusarova N.","Semiletov, Alexander (57226383318); Vatian, Aleksandra (57191870868); Krychkov, Maksim (57226389503); Khanzhina, Natalia (57192385166); Klochkov, Anton (57211665294); Zubanenko, Aleksey (57215436184); Soldatov, Roman (57215433098); Shalyto, Anatoly (56131789500); Gusarova, Natalia (56498728700)","57226383318; 57191870868; 57226389503; 57192385166; 57211665294; 57215436184; 57215433098; 56131789500; 56498728700","Comparative Evaluation of Lung Cancer CT Image Synthesis with Generative Adversarial Networks","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12744 LNCS","","","593","608","15","10.1007/978-3-030-77967-2_49","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111356340&doi=10.1007%2f978-3-030-77967-2_49&partnerID=40&md5=4e2599db4a423e2527386056ca7bc122","Generative adversarial networks have already found widespread use for the formation of artificial, but realistic images of a wide variety of content, including medical imaging. Mostly they are considered to be used for expanding and augmenting datasets in order to improve accuracy of neural networks classification. In this paper we discuss the problem of evaluating the quality of computer tomography images of lung cancer, which is characterized by small size of nodules, synthesized using two different generative adversarial network, architectures – for 2D and 3D dimensions. We select the set of metrics for estimating the quality of the generated images, including Visual Turing Test, FID and MRR metrics; then we carry out a problem-oriented modification of the Turing test in order to adapt it both to the actually obtained images and to resource constraints. We compare the constructed GANs using the selected metrics; and we show that such a parameter as the size of the generated image is very important in the development of the GAN architecture. We consider that with this work we have for the first time shown that for small neo-plasms, direct scaling of the corresponding solutions used to generate large neo-plasms (for example, gliomas) is ineffective. Developed assessment methods have shown that additional techniques like MIP and special combinations of metrics are required to generate small neoplasms. In addition, an important conclusion can be considered that it is very important to use GAN networks not only, as is usually the case, for augmentation and expansion of the datasets, but for direct use in clinical practice by radiologists. © 2021, Springer Nature Switzerland AG.","Artificial intelligence; Biological organs; Classification (of information); Diseases; Medical imaging; Network architecture; Tumors; Adversarial networks; Clinical practices; Comparative evaluations; Computer tomography images; Corresponding solutions; Realistic images; Resource Constraint; Turing tests; Computerized tomography","2D 3D GAN; CT image synthesis; Evaluation metrics; Generative adversarial networks; Lung cancer","Conference paper","Final","","Scopus","2-s2.0-85111356340"
"Parihar A.S.; Kaushik A.; Choudhary A.V.; Singh A.K.","Parihar, Anil Singh (36998900100); Kaushik, Aditya (57535684900); Choudhary, Aditya Vikram (57222371638); Singh, Amit Kumar (55726466900)","36998900100; 57535684900; 57222371638; 55726466900","HTGAN: An architecture for Hindi Text based Image Synthesis","2021","2021 5th International Conference on Computer, Communication, and Signal Processing, ICCCSP 2021","","","9465527","273","279","6","10.1109/ICCCSP52374.2021.9465527","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113876219&doi=10.1109%2fICCCSP52374.2021.9465527&partnerID=40&md5=f2eef78d3ad280711e5f1c2bcca76603","Synthesis of high-quality images that are semantically consistent with their text descriptions has emerged as an essential and onerous problem in the areas of CV and NLP. Various significant steps have been taken in the task of generation of images using English textual descriptions, with Multimodal GANs being at the forefront of all these efforts. In this work we attempt to extend the existing English text to image generation (T2I) techniques to the novel task of Hindi T2I using language translation models. To achieve this, the input Hindi sentences were translated to English using a transformer based Neural Machine Translation module the output of which was then fed to the Generative Adversarial Networks based Image Generation Module. The outcomes of this approach have been evaluated using well established metrics such as Inception Score and BLEU Score which yield scores that indicate the generation of lifelike images which semantically align with the input text descriptions.  © 2021 IEEE.","Computer aided language translation; Adversarial networks; Bleu scores; High quality images; Image generations; Image synthesis; Language translation; Machine translations; Textual description; Image processing","Generative Adversarial Networks; Neural Machine Translation; Text to Image; Transformers","Conference paper","Final","","Scopus","2-s2.0-85113876219"
"Park S.-W.; Ko J.-S.; Huh J.-H.; Kim J.-C.","Park, Sung-Wook (57205079478); Ko, Jae-Sub (35253916100); Huh, Jun-Ho (56438784800); Kim, Jong-Chan (49963957400)","57205079478; 35253916100; 56438784800; 49963957400","Review on generative adversarial networks: Focusing on computer vision and its applications","2021","Electronics (Switzerland)","10","10","1216","","","","10.3390/electronics10101216","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106180519&doi=10.3390%2felectronics10101216&partnerID=40&md5=8d5ca3347494a23908b9b448ce19d112","The emergence of deep learning model GAN (Generative Adversarial Networks) is an important turning point in generative modeling. GAN is more powerful in feature and expression learning compared to machine learning-based generative model algorithms. Nowadays, it is also used to generate non-image data, such as voice and natural language. Typical technologies include BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pretrained Transformer-3), and MuseNet. GAN differs from the machine learning-based generative model and the objective function. Training is conducted by two networks: generator and discriminator. The generator converts random noise into a true-to-life image, whereas the discriminator distinguishes whether the input image is real or synthetic. As the training continues, the generator learns more sophisticated synthesis techniques, and the discriminator grows into a more accurate differentiator. GAN has problems, such as mode collapse, training instability, and lack of evaluation matrix, and many researchers have tried to solve these problems. For example, solutions such as one-sided label smoothing, instance normalization, and minibatch discrimination have been proposed. The field of application has also expanded. This paper provides an overview of GAN and application solutions for computer vision and artificial intelligence healthcare field researchers. The structure and principle of operation of GAN, the core models of GAN proposed to date, and the theory of GAN were analyzed. Application examples of GAN such as image classification and regression, image synthesis and inpainting, image-to-image translation, super-resolution and point registration were then presented. The discussion tackled GAN’s problems and solutions, and the future research direction was finally proposed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Artificial intelligence healthcare; Computer vision; Deep learning; Generative adversarial networks","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106180519"
"Li J.; Wang Y.; Yang Y.; Zhang X.; Qu Z.; Hu S.","Li, Jitao (57427282700); Wang, Yuwen (57427546200); Yang, Yue (57222810844); Zhang, Xin (57239256600); Qu, Zongjin (57225721602); Hu, Shunbo (16417168200)","57427282700; 57427546200; 57222810844; 57239256600; 57225721602; 16417168200","Small animal PET to CT image synthesis based on conditional generation network","2021","Proceedings - 2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2021","","","","","","","10.1109/CISP-BMEI53629.2021.9624232","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123480996&doi=10.1109%2fCISP-BMEI53629.2021.9624232&partnerID=40&md5=01d8d2827f57a6fa1b8d930fa692a566","During the image reconstruction process of Positron Emission Computed Tomography (PET), the tissue of the object being photographed will undergo Compton scattering with photons generated by the annihilation of the positive and negative electrons, which will cause the reconstructed image to be blurred. Traditional PET equipment will add a Computed Tomography (CT) equipment module. During PET shooting process, CT images are taken at the same time, and the CT images are used for attenuation correction to reduce scattering effect of PET. Especially in PET equipment that requires high accuracy, attenuation correction has a greater impact on the quality of the final PET reconstruction. The CT image is generally used for attenuation correction, not for final diagnosis. The additional CT shooting will bring more radiation to the shooting target and increase manufacturing cost of the equipment. This paper proposes a method of generating Synthetic CT image from PET based on Generative Adversarial Network (GAN). The generator of our network uses an improved generator based on encoder-decoder structure, and the discriminator has a local field of view. This article compares the effects of several loss functions respectively. These loss functions successfully improve the model's capabilities and results. The quantitative and qualitative results show that our proposed method outperforms common methods. © 2021 IEEE.","Computerized tomography; Deep learning; Diagnosis; Image reconstruction; Manufacture; Medical imaging; Positron emission tomography; Attenuation correction; Computed tomography images; Deep learning; Emission Computed Tomography; Image translation; Loss functions; Medical image translation; Positron emission; Positron emission computed tomography attenuation correction; Small Animal; Generative adversarial networks","Deep learning; Generative Adversarial Network; Medical image translation; PET attenuation correction","Conference paper","Final","","Scopus","2-s2.0-85123480996"
"Sanaat A.; Akhavanallaf A.; Shiri I.; Salimi Y.; Arabi H.; Zaidi H.","Sanaat, Amirhossein (57207449502); Akhavanallaf, Azadeh (57207939421); Shiri, Isaac (57191829519); Salimi, Yazdan (57191823474); Arabi, Hossein (57220877447); Zaidi, Habib (7004977873)","57207449502; 57207939421; 57191829519; 57191823474; 57220877447; 7004977873","Time-of flight (TOF) Image Synthesis from non-TOF PET Using Deep Learning","2021","2021 IEEE Nuclear Science Symposium and Medical Imaging Conference Record, NSS/MIC 2021 and 28th International Symposium on Room-Temperature Semiconductor Detectors, RTSD 2022","","","","","","","10.1109/NSS/MIC44867.2021.9875931","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139177573&doi=10.1109%2fNSS%2fMIC44867.2021.9875931&partnerID=40&md5=fde09fbb861eafc65990a2377df5ace2","Time-of-flight (TOF) PET technology demonstrated superior image quality and quantitative performance translated into a considerable increase in SNR-gain, noise reduction, and robustness to artefacts, thus improving confidence in clinical diagnosis. This work aimed to assess the performance of TOF PET synthesis from non-TOF PET images using deep learning techniques. One hundred forty 18F-FDG brains PET/CT clinical studies were acquired in list-mode format enabling the generation of non-TOF and TOF sinograms. The TOF sinograms were split into seven time bins (0, ±1, ±2, ±3). An iterative algorithm was used to reconstruct images corresponding to non-TOF and TOF sinograms. A modified cycle-consistent generative adversarial network (CycleGAN) was implemented to predict TOF images from non-TOF images in both sinogram and image domains. In the first approach, a model was trained to predict TOF from non-TOF images, whereas in the second approach, 7 models were trained to synthesize 7 time bin sinograms from the non-TOF sinograms. Quantitative analysis revealed improvement peak signal-to-noise ratio (PSNR) by 9% and 12% in the synthesized TOF images compared to the corresponding non-TOF images in the sinogram and image domains, respectively. © 2021 IEEE.","Deep learning; Diagnosis; Image enhancement; Image quality; Iterative methods; Medical imaging; Signal to noise ratio; Image domain; Images synthesis; Noise robustness; Performance; PET technology; Sinogram domain; Sinograms; SNR gains; Time-of flight; Time-of-flight PET; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85139177573"
"Liu Y.; Huo X.; Chen T.; Zeng X.; Wu S.; Yu Z.; Wong H.-S.","Liu, Yi (57225002733); Huo, Xiaoyang (57419470300); Chen, Tianyi (57212621725); Zeng, Xiangping (57219653990); Wu, Si (55495122900); Yu, Zhiwen (56399660300); Wong, Hau-San (7402864844)","57225002733; 57419470300; 57212621725; 57219653990; 55495122900; 56399660300; 7402864844","Mask-Embedded Discriminator with Region-based Semantic Regularization for Semi-Supervised Class-Conditional Image Synthesis","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","5502","5511","9","10.1109/CVPR46437.2021.00546","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123203436&doi=10.1109%2fCVPR46437.2021.00546&partnerID=40&md5=a718d7e78094ccae85b4793cd2226a44","Semi-supervised generative learning (SSGL) makes use of unlabeled data to achieve a trade-off between the data collection/annotation effort and generation performance, when adequate labeled data are not available. Learning precise class semantics is crucial for class-conditional image synthesis with limited supervision. Toward this end, we propose a semi-supervised Generative Adversarial Network with a Mask-Embedded Discriminator, which is referred to as MED-GAN. By incorporating a mask embedding module, the discriminator features are associated with spatial information, such that the focus of the discriminator can be limited in the specified regions when distinguishing between real and synthesized images. A generator is enforced to synthesize the instances holding more precise class semantics in order to deceive the enhanced discriminator. Also benefiting from mask embedding, region-based semantic regularization is imposed on the discriminator feature space, and the degree of separation between real and fake classes and among object categories can thus be increased. This eventually improves class-conditional distribution matching between real and synthesized data. In the experiments, the superior performance of MED-GAN demonstrates the effectiveness of mask embedding and associated regularizers in facilitating SSGL. © 2021 IEEE","Computer vision; Economic and social effects; Embeddings; Generative adversarial networks; Semantics; Data collection; Embeddings; Images synthesis; Labeled data; Performance; Region-based; Regularisation; Semi-supervised; Trade off; Unlabeled data; Discriminators","","Conference paper","Final","","Scopus","2-s2.0-85123203436"
"Dursun G.; Ozkaya U.","Dursun, Gizem (57203970253); Ozkaya, Ufuk (36972694600)","57203970253; 36972694600","Microscopic fluorescence in situ hybridization (FISH) image synthesis with generative adversarial networks; [Çekişmeli üretici aǧlar ile mikroskobik floresan in situ hibridizasyon (FISH) imge sentezlenmesi]","2021","SIU 2021 - 29th IEEE Conference on Signal Processing and Communications Applications, Proceedings","","","9477999","","","","10.1109/SIU53274.2021.9477999","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111415081&doi=10.1109%2fSIU53274.2021.9477999&partnerID=40&md5=38eb74c38ea098c5030931f21ab0e1eb","One of the most important problems in biomedical image analysis is the low amount of data and the cost of accessing to the marked data by researchers. In order to provide a solution to this problem, microscopic fluorescence in situ hybridization (FISH) images are synthesized with generative adversarial network in this paper. The generative adversarial network is trained to synthesize FISH images from mask images. The trained model was implemented on 150 test images and the performance of the model both was presented with visual results and evaluated quantitatively by calculating the performance metrics. By evaluating the synthesized FISH images in terms of image quality and structural features, it is observed that they can be used to provide a solution to the problem of the lack of data. © 2021 IEEE.","Fluorescence; Fluorescence microscopy; Adversarial networks; Biomedical image analysis; FISH images; Fluorescence in situ hybridization; Image synthesis; Performance metrics; Structural feature; Test images; Fish","Fluorescence in situ hybridization; Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85111415081"
"Chaparro-Cruz I.N.; Montoya-Zegarra J.A.","Chaparro-Cruz, Israel N. (57444284400); Montoya-Zegarra, Javier A. (23397922500)","57444284400; 23397922500","BORDE: Boundary and Sub-Region Denormalization for Semantic Brain Image Synthesis","2021","Proceedings - 2021 34th SIBGRAPI Conference on Graphics, Patterns and Images, SIBGRAPI 2021","","","","81","88","7","10.1109/SIBGRAPI54419.2021.00020","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124199989&doi=10.1109%2fSIBGRAPI54419.2021.00020&partnerID=40&md5=46043bab0fd70f3bb75b04a99040ab48","Medical images are often expensive to acquire and offer limited use due to legal issues besides the lack of consistency and availability of image annotations. Thus, the use of medical datasets can be restrictive for training deep learning models. The generation of synthetic images along with their corresponding annotations can therefore aid to solve this issue. In this paper, we propose a novel Generative Adversarial Network (GAN) generator for multimodal semantic image synthesis of brain images based on a novel denormalization block named BOundary and sub-Region DEnormalization (BORDE). The new architecture consists of a decoder generator that allows: (i) an effectively sequential propagation of a-priori semantic information through the generator, (ii) noise injection at different scales to avoid mode-collapse, and (iii) the generation of rich and diverse multimodal synthetic samples along with their contours. Our model generates very realistic and plausible synthetic images that when combined with real data helps to improve the accuracy in brain segmentation tasks. Quantitative and qualitative results on challenging multimodal brain imaging datasets (BraTS 2020 [1] and ISLES 2018 [2]) demonstrate the advantages of our model over existing image-agnostic state-of-the-art techniques, improving segmentation and semantic image synthesis tasks. This allows us to prove the need for more domain-specific techniques in GANs models.  © 2021 IEEE.","Brain mapping; Computer vision; Deep learning; Image enhancement; Semantic Segmentation; Semantic Web; Semantics; Boundary regions; Brain images; Brain imaging; Denormalization; Images synthesis; Multi-modal; Normalizationlayer; Semantic image synthesis; Semantic images; Sub-regions; Generative adversarial networks","Brain Imaging; generative adversarial networks; normalizationlayers; semantic image synthesis","Conference paper","Final","","Scopus","2-s2.0-85124199989"
"Liu Y.; Chen J.","Liu, Yanfei (26325998400); Chen, Junhua (56270946600)","26325998400; 56270946600","Multi-factor joint normalisation for face recognition in the wild","2021","IET Computer Vision","15","6","","405","417","12","10.1049/cvi2.12025","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127399207&doi=10.1049%2fcvi2.12025&partnerID=40&md5=d3019e7d75572fb9db7498a1f0be1e42","Face recognition has become very challenging in unconstrained conditions due to strong intra-personal variations, such as large pose changes. Face normalisation can help to resolve these problems and effectively improve the face recognition performance in unconstrained conditions by converting non-frontal faces to frontal ones. However, there are other complex facial variations in addition to pose, such as illumination and expression, which will also influence face recognition performance. The authors propose a well-designed generative adversarial network-based multi-factor joint normalisation network (MFJNN) to normalise multiple factors simultaneously. First, a multi-encoder generator and a feature fusion strategy are designed and implemented in the MFJNN to realise the joint normalisation of multiple factors in addition to pose. Second, a convolutional neural network-based (CNN-based) network is applied in the MFJNN, which allows the MFJNN to simultaneously realise image synthesis and facial representation learning. Moreover, an identity perceptive loss is introduced based on the CNN-based network to produce reliable identity-preserving features of the input face images. The experimental results demonstrate that the proposed method can synthesise multi-factor normalisation results with identity preservation and effectively improve the face recognition performance. © 2021 The Authors. IET Computer Vision published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Convolutional neural networks; Generative adversarial networks; Condition; Convolutional neural network; Face normalization; Face recognition performance; Frontal faces; Intra-personal variations; Multi-factor; Multiple factors; Network-based; Normalisation; Face recognition","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127399207"
"Park T.H.; D’amico S.","Park, Tae Ha (57219501783); D’amico, Simone (57219502019)","57219501783; 57219502019","GENERATIVE MODEL FOR SPACECRAFT IMAGE SYNTHESIS USING LIMITED DATASET","2021","Advances in the Astronautical Sciences","175","","","3131","3146","15","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126250958&partnerID=40&md5=89917c9beb29c435d6f2111d0fa86fae","This work presents for the first time a conditional Generative Adversarial Network (GAN) to sample arbitrary high-fidelity spacecraft images from the learned distribution of spacecraft texture and illumination conditions (i.e., styles). The proposed SPEEDGAN utilizes a low-texture template of the spacecraft to empower SPEEDGAN with a priori knowledge of the spacecraft geometry and pose, allowing the model to focus on creating a spacecraft style. The SPEEDGAN also trains with a content loss from style transfer literature to improve the structural fidelity of the generated spacecraft images. Trained on a limited dataset containing images from the computer graphics renderer and the hardware-in-the-loop simulation facility, SPEEDGAN generates samples with remarkable visual qualities, measured by both human and a separate neural network for pose estimation. It is also capable of separated control over different style aspects, such as spacecraft texture and illumination effects. © 2021, Univelt Inc. All rights reserved.","Computer graphics; Computer hardware; Generative adversarial networks; Image enhancement; Space applications; Space research; Spacecraft; Generative model; Hardwarein-the-loop simulations (HIL); High-fidelity; Illumination conditions; Images synthesis; Neural-networks; Pose-estimation; Priori knowledge; Spacecraft geometry; Visual qualities; Textures","","Conference paper","Final","","Scopus","2-s2.0-85126250958"
"Tomar D.; Lortkipanidze M.; Vray G.; Bozorgtabar B.; Thiran J.-P.","Tomar, Devavrat (57221978015); Lortkipanidze, Manana (57782511600); Vray, Guillaume (57210358393); Bozorgtabar, Behzad (37109488100); Thiran, Jean-Philippe (35554798200)","57221978015; 57782511600; 57210358393; 37109488100; 35554798200","Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation","2021","IEEE Transactions on Medical Imaging","40","10","","2926","2938","12","10.1109/TMI.2021.3059265","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100854204&doi=10.1109%2fTMI.2021.3059265&partnerID=40&md5=695b812970c66e106e651493b9b17f18","Despite the successes of deep neural networks on many challenging vision tasks, they often fail to generalize to new test domains that are not distributed identically to the training data. The domain adaptation becomes more challenging for cross-modality medical data with a notable domain shift. Given that specific annotated imaging modalities may not be accessible nor complete. Our proposed solution is based on the cross-modality synthesis of medical images to reduce the costly annotation burden by radiologists and bridge the domain gap in radiological images. We present a novel approach for image-to-image translation in medical images, capable of supervised or unsupervised (unpaired image data) setups. Built upon adversarial training, we propose a learnable self-attentive spatial normalization of the deep convolutional generator network's intermediate activations. Unlike previous attention-based image-to-image translation approaches, which are either domain-specific or require distortion of the source domain's structures, we unearth the importance of the auxiliary semantic information to handle the geometric changes and preserve anatomical structures during image translation. We achieve superior results for cross-modality segmentation between unpaired MRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI (T1/T2) datasets compared to the state-of-the-art methods. We also observe encouraging results in cross-modality conversion for paired MRI and CT images on a brain dataset. Furthermore, a detailed analysis of the cross-modality image translation, thorough ablation studies confirm our proposed method's efficacy.  © 1982-2012 IEEE.","Brain Neoplasms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Neuroimaging; Convolutional neural networks; Deep neural networks; Magnetic resonance imaging; Medical imaging; Semantics; Anatomical structures; Domain adaptation; Geometric changes; Image translation; Radiological images; Semantic information; Spatial normalization; State-of-the-art methods; anatomical concepts; Article; attention network; brain tumor; comparative study; computer assisted tomography; controlled study; convolutional neural network; cross modality synthesis; data accuracy; domain adaptation; evaluation study; geometry; heart; human; image processing; image segmentation; nuclear magnetic resonance imaging; self attentive spatial normalization; sensitivity analysis; transfer of learning; validation process; brain tumor; image processing; neuroimaging; Computerized tomography","Domain adaptation; generative adversarial networks; image synthesis; self-attention; unpaired domains","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85100854204"
"Lin L.; Cheng P.; Wang Z.; Li M.; Wang K.; Tang X.","Lin, Li (57217028180); Cheng, Pujin (57212007255); Wang, Zhonghua (57224205740); Li, Meng (57135638200); Wang, Kai (55501470700); Tang, Xiaoying (57202405783)","57217028180; 57212007255; 57224205740; 57135638200; 55501470700; 57202405783","Automated segmentation of corneal nerves in confocal microscopy via contrastive learning based synthesis and quality enhancement","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9433955","1314","1318","4","10.1109/ISBI48211.2021.9433955","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107191180&doi=10.1109%2fISBI48211.2021.9433955&partnerID=40&md5=71533f92f285227c8e99a23a2f980ec6","Precise quantification of the corneal nerve plexus morphology is of great importance in diagnosing peripheral diabetic neuropathy and assessing the progression of various eye-related systemic diseases, wherein segmentation of corneal nerves is an essential component. In this paper, we proposed and validated a novel pipeline for corneal nerve segmentation, comprising corneal confocal microscopy (CCM) image synthesis, image quality enhancement and nerve segmentation. Our goal was to address three major problems existing in most CCM datasets, namely inaccurate annotations, non-uniform illumination and contrast variations. In our synthesis and enhancement steps, we employed multilayer and patchwise contrastive learning based Generative Adversarial Network (GAN) frameworks, which took full advantage of multi-scale local features. Through both qualitative and quantitative experiments on two publicly available CCM datasets, our pipeline has achieved overwhelming enhancement performance compared to several state-of-the-art methods. Moreover, the segmentation results showed that models trained on our synthetic images performed much better than those trained on a real CCM dataset, which clearly identified the effectiveness of our synthesis method. Overall, our proposed pipeline can achieve satisfactory segmentation performance for poor-quality CCM images without using any manual labels and can effectively enhance those images.  © 2021 IEEE.","Confocal microscopy; Diagnosis; Image segmentation; Medical imaging; Multilayers; Pipelines; Quality control; Adversarial networks; Automated segmentation; Image quality enhancements; Multi-scale local features; Non-uniform illumination; Quantitative experiments; Segmentation performance; State-of-the-art methods; Image enhancement","Contrastive learning; Corneal confocal microscopy.; Corneal nerve segmentation; Dataset synthesis; Quality enhancement","Conference paper","Final","","Scopus","2-s2.0-85107191180"
"Che H.; Ramanathan S.; Foran D.J.; Nosher J.L.; Patel V.M.; Hacihaliloglu I.","Che, Hui (57222340388); Ramanathan, Sumana (57226598421); Foran, David J. (7004400892); Nosher, John L. (7003279880); Patel, Vishal M. (56660008900); Hacihaliloglu, Ilker (24467768400)","57222340388; 57226598421; 7004400892; 7003279880; 56660008900; 24467768400","Realistic Ultrasound Image Synthesis for Improved Classification of Liver Disease","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12967 LNCS","","","179","188","9","10.1007/978-3-030-87583-1_18","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116496525&doi=10.1007%2f978-3-030-87583-1_18&partnerID=40&md5=61f201511d64c7d2e2c4c8ab1a4ff4bd","With the success of deep learning-based methods applied in medical image analysis, convolutional neural networks (CNNs) have been investigated for classifying liver disease from ultrasound (US) data. However, the scarcity of available large-scale labeled US data has hindered the success of CNNs for classifying liver disease from US data. In this work, we propose a novel generative adversarial network (GAN) architecture for realistic diseased and healthy liver US image synthesis. We adopt the concept of stacking to synthesize realistic liver US data. Quantitative and qualitative evaluation is performed on 550 in-vivo B-mode liver US images collected from 55 subjects. We also show that the synthesized images, together with real in vivo data, can be used to significantly improve the performance of traditional CNN architectures for Nonalcoholic fatty liver disease (NAFLD) classification. © 2021, Springer Nature Switzerland AG.","Classification (of information); Convolutional neural networks; Deep learning; Diseases; Image classification; Image enhancement; Medical imaging; Network architecture; Ultrasonics; Convolutional neural network; Deep learning; Images synthesis; In-vivo; Liver disease; Liver ultrasounds; Non-alcoholic fatty liver disease; Stacked generative adversarial network; Ultrasound data; Ultrasound images; Generative adversarial networks","Classification; Deep learning; Nonalcoholic fatty liver disease; Stacked generative adversarial network; Ultrasound","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116496525"
"Qi Z.; Fan C.; Xu L.; Li X.; Zhan S.","Qi, Zhongjian (57222994743); Fan, Chaogang (57221997428); Xu, Liangfeng (7404744295); Li, Xinke (35176053700); Zhan, Shu (24765737400)","57222994743; 57221997428; 7404744295; 35176053700; 24765737400","MRP-GAN: Multi-resolution parallel generative adversarial networks for text-to-image synthesis","2021","Pattern Recognition Letters","147","","","1","7","6","10.1016/j.patrec.2021.02.020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104320603&doi=10.1016%2fj.patrec.2021.02.020&partnerID=40&md5=dc541a21b73a7dee77f296351c16361c","Synthesizing photographic images from given text descriptions is a challenging problem. Although current methods first synthesize an initial blurred image, then refine the initial image to a high-quality one, the most existing methods are difficult to refine the initial image to an image corresponding to the text description. In this paper, the Multi-resolution Parallel Generative Adversarial Networks for Text-to-Image Synthesis (MRP-GAN) is proposed to generate photographic images. MRP-GAN introduces a Multi-resolution Parallel structure to refine the initial images when the initial images are not synthesized well. The low-resolution semantics are maintained through the whole process by Multi-resolution Parallel structure. Response Gate is designed to fully explore the capability of Multi-resolution Parallel structure by aggregating the outputs of the multi-resolution parallel subnetworks. We also utilize an attention mechanism, named Residual Attention Network, to fine-tune more fine-grained details of the generated images. We evaluate our MRP-GAN model on the CUB and MS-COCO datasets. Extensive experiments demonstrate the state-of-the-art performance of MRP-GAN. Besides, we apply a Multi-resolution Parallel structure in the existing method to verify its transferability. © 2021","Photography; 'current; Adversarial networks; Blurred image; Generative adversarial network; High quality; Image generations; Images synthesis; Parallel structures; Photographic image; Text-to-image synthesize; Semantics","Generative adversarial networks; Image generation; Text-to-image synthesize","Article","Final","","Scopus","2-s2.0-85104320603"
"Zhang S.; Xiao S.; Zhang P.; Huang W.","Zhang, Siyuan (57223926487); Xiao, Shiming (57223919474); Zhang, Peng (55547108553); Huang, Wei (56195325600)","57223926487; 57223919474; 55547108553; 56195325600","Identity-Aware Facial Expression Recognition Method Based on Synthesized Images and Deep Metric Learning; [图像生成和深度度量学习的身份感知面部表情识别方法]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","5","","724","732","8","10.3724/SP.J.1089.2021.18462","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106466604&doi=10.3724%2fSP.J.1089.2021.18462&partnerID=40&md5=4cec044fa64816b5dc9cbb6820e2ffb9","Facial expression recognition (FER) is a challenging task because the external environment and identity characteristics could affect the classification results directly. To settle down the above-mentioned challenges, this paper proposed an identity-aware facial expression recognition method which combined images synthesis techniques and deep metric learning, and made facial images features compared then classified by creating expression groups under the same identity in FER task. There are three parts in our method. The first part is a generative adversarial network, which aims to learn expression information and synthesis the expression groups. the second part is the feature extraction network, which transforms the image into feature vectors that could be used for metric learning. The third part is Mahalanobis metric learning network that could compare and classify a pair of feature values effectively. The average accuracy of proposed method reached 98.653 2% and 99.824 8% on two well-known FER dataset named CK+ and Oulu-CASIA, with more than 10% higher than the method proposed currently. By comparing with several state-of-the-art methods, the experimental results confirmed that the proposed-method was effective and progressive in FER task. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Deep learning; Learning systems; Adversarial networks; Classification results; External environments; Facial expression recognition; Feature vectors; Mahalanobis metric; State-of-the-art methods; Synthesized images; Face recognition","Deep metric learning; Facial expression recognition; Identity-aware; Image synthesis","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85106466604"
"Liu J.; Chen P.; Liang T.; Li Z.; Yu C.; Zou S.; Dai J.; Han J.","Liu, Jin (57216511443); Chen, Peng (57216430660); Liang, Tao (57216509718); Li, Zhaoxing (56927768900); Yu, Cai (57223740043); Zou, Shuqiao (57223754287); Dai, Jiao (37010322300); Han, Jizhong (23008759600)","57216511443; 57216430660; 57216509718; 56927768900; 57223740043; 57223754287; 37010322300; 23008759600","LI-NET: LARGE-POSE IDENTITY-PRESERVING FACE REENACTMENT NETWORK","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428233","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126460303&doi=10.1109%2fICME51207.2021.9428233&partnerID=40&md5=dd6b58dc8be93b79304d5e8a1ad5e5c7","Face reenactment is a challenging task, as it is difficult to maintain accurate expression, pose and identity simultaneously. Most existing methods directly apply driving facial landmarks to reenact source faces and ignore the intrinsic gap between two identities, resulting in the identity mismatch issue. Besides, they neglect the entanglement of expression and pose features when encoding driving faces, leading to inaccurate expressions and visual artifacts on large-pose reenacted faces. To address these problems, we propose a Large-pose Identity-preserving face reenactment network, LI-Net. Specifically, the Landmark Transformer is adopted to adjust driving landmark images, which aims to narrow the identity gap between driving and source landmark images. Then the Face Rotation Module and the Expression Enhancing Generator decouple the transformed landmark image into pose and expression features, and reenact those attributes separately to generate identity-preserving faces with accurate expressions and poses. Both qualitative and quantitative experimental results demonstrate the superiority of our method. © 2021 IEEE","Computer vision; Image enhancement; Face reenactment; Face rotation; Facial landmark; Images synthesis; Intrinsic gaps; Rotation modules; Visual artifacts; Generative adversarial networks","Face Reenactment; Generative Adversarial Network; Image Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126460303"
"Panchard D.; Marelli F.; De Moura Presa E.; Wellig P.; Liebling M.","Panchard, Danick (57338973300); Marelli, François (57210123315); De Moura Presa, Edouard (15727405900); Wellig, Peter (6602364991); Liebling, Michael (35242648700)","57338973300; 57210123315; 15727405900; 6602364991; 35242648700","Perspectives and limitations of visible-thermal image pair synthesis via generative adversarial networks","2021","Proceedings of SPIE - The International Society for Optical Engineering","11865","","1186509","","","","10.1117/12.2599889","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094862&doi=10.1117%2f12.2599889&partnerID=40&md5=561f8828064d0b09158d4b1d44b33d8d","Many applications rely on thermal imagers to complement or replace visible light sensors in difficult imaging conditions. Recent advances in machine learning have opened the possibility of analyzing or enhancing images, yet these methods require large annotated databases. Training approaches that leverage data augmentation via simulated and synthetically-generated images could offer promising prospects. Here, we report on a method that uses generative adversarial nets (GANs) to synthesize images of a complementary contrast. Starting from a dual-modality dataset of co-registered visible and thermal images, we trained a GAN to generate synthetic thermal images from visible images and vice versa. Our results show that the procedure yields sharp synthesized images that might be used to augment dual-modality datasets or assist in visual interpretation, yet are also subject to the limitations imposed by contrast independence between thermal and visible images.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Image enhancement; Infrared imaging; Dual-modality; Image pairs; Images synthesis; Imaging conditions; Light sensor; Thermal imagers; Thermal images; Thermal-imaging; Visible image; Visible light; Generative adversarial networks","Generative adversarial networks; Image synthesis; Thermal imaging","Conference paper","Final","","Scopus","2-s2.0-85119094862"
"Ni J.; Zhang S.; Zhou Z.; Hou L.; Hou J.; Gao F.","Ni, Jiancheng (14831535200); Zhang, Susu (57215657489); Zhou, Zili (57215660890); Hou, Lijun (57223213956); Hou, Jie (57705815000); Gao, Feng (57215650829)","14831535200; 57215657489; 57215660890; 57223213956; 57705815000; 57215650829","Background and foreground disentangled generative adversarial network for scene image synthesis","2021","Computers and Graphics (Pergamon)","97","","","54","66","12","10.1016/j.cag.2021.04.003","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105335086&doi=10.1016%2fj.cag.2021.04.003&partnerID=40&md5=63c08cbfe01437c45f1c1e6fcd9b8e5a","Despite recent generative models have made remarkable progress on adversarial image synthesis, it is still a pivotal and frontier problem to generate high-fidelity images containing diverse entities and complex scene layouts from structured descriptions. To this end, we present a Background and Foreground Disentangled Generative Adversarial Network (BFD-GAN) to synthesize high-quality images from scene graphs. First, our method uses the graph convolutional network to infer a semantic background from the input scene graph. Then, the foreground parsing module that encourages unsupervised generation, is proposed to calculate semantically related foregrounds with fine-grained geometric properties. Furthermore, we also employ the foreground-background integrating module for the final image generation, during which the foreground-relation aware attention is introduced to refine and fuse the inferred foregrounds into the background. Evaluated on the COCO-Stuff and Visual Genome datasets, we benchmark our model against existing methods and show that our BFD-GAN is more capable of generating complex backgrounds and corresponding sharp foregrounds with given scene structures. © 2021 Elsevier Ltd","Complex networks; Convolutional neural networks; Semantics; Adversarial networks; Complex background; Convolutional networks; Generative model; Geometric properties; High quality images; High-fidelity images; Image generations; Image processing","Background and foreground disentanglement; Generative adversarial network; Graph convolutional network; Image generation","Article","Final","","Scopus","2-s2.0-85105335086"
"Sun Y.; Lin H.; Liu C.; Fu Y.","Sun, Yuming (57869071300); Lin, Hangyu (57211682023); Liu, Chen (57225888392); Fu, Yanwei (36571734200)","57869071300; 57211682023; 57225888392; 36571734200","Learning Depth Information in Layout for Sketch Generation from Scene Graph","2021","Proceedings - 2021 7th International Conference on Big Data and Information Analytics, BigDIA 2021","","","","253","260","7","10.1109/BigDIA53151.2021.9619707","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123495015&doi=10.1109%2fBigDIA53151.2021.9619707&partnerID=40&md5=f85d4c3eb98ffd73056e598efe882fa8","The research of generating images from scene graphs has become a hot topic, benefiting from the success of Generative Adversarial Network (GAN). Previous works in this field are still challenged by the complexity of texture pattern and object structure in images. In fact, it is more desirable to generate sketches directly, rather than image synthesis from scene graphs. The sketch is an abstract and iconic image representation, which describes the object structure and scene layout well but ignoring complex texture patterns, leading to a more reasonable generation task. Furthermore, there are two real-world tasks can illustrate the importance of this problem: Courtroom sketch and Crime Scene sketch, which are less studied before. To this end, we, for the first time, study the task of generating sketches from scene graphs. Essentially, the main novelties are two folds. First, a new sketch generation framework is developed which is trained with both newly designed perceptual loss and adversarial loss. Second, we propose a new layout encoding block to learn the depth information for each object. Extensive experiments on several widely used datasets validate the good performance of the proposed approach. © 2021 IEEE.","Complex networks; Computer vision; Image representation; Image texture; Textures; Depth information; Generative model; Hot topics; Iconics; Images synthesis; Object structure; Pattern structure; Scene-graphs; Sketch generation; Texture patterns; Generative adversarial networks","Generative Model; Scene Graph; Sketch Generation","Conference paper","Final","","Scopus","2-s2.0-85123495015"
"Zhao Z.; Singh S.; Lee H.; Zhang Z.; Odena A.; Zhang H.","Zhao, Zhengli (57215317828); Singh, Sameer (55647398500); Lee, Honglak (15056237200); Zhang, Zizhao (57020905500); Odena, Augustus (57202500011); Zhang, Han (56098272800)","57215317828; 55647398500; 15056237200; 57020905500; 57202500011; 56098272800","Improved Consistency Regularization for GANs","2021","35th AAAI Conference on Artificial Intelligence, AAAI 2021","12B","","","11033","11041","8","","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130017359&partnerID=40&md5=4196255af8ae3fa83cc4e3461348c3b8","Recent work has increased the performance of Generative Adversarial Networks (GANs) by enforcing a consistency cost on the discriminator. We improve on this technique in several ways. We first show that consistency regularization can introduce artifacts into the GAN samples and explain how to fix this issue. We then propose several modifications to the consistency regularization procedure designed to improve its performance. We carry out extensive experiments quantifying the benefit of our improvements. For unconditional image synthesis on CIFAR-10 and CelebA, our modifications yield the best known FID scores on various GAN architectures. For conditional image synthesis on CIFAR-10, we improve the state-of-the-art FID score from 11.48 to 9.21. Finally, on ImageNet-2012, we apply our technique to the original BigGAN model and improve the FID from 6.66 to 5.38, which is the best score at that model size. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Image enhancement; Images synthesis; Model size; Performance; Regularisation; Regularization procedure; State of the art; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85130017359"
"Makhmudkhujaev F.; Kwon J.; Park I.K.","Makhmudkhujaev, Farkhod (57200140017); Kwon, Junseok (25655050500); Park, In Kyu (57670835100)","57200140017; 25655050500; 57670835100","Controllable Image Dataset Construction using Conditionally Transformed Inputs in Generative Adversarial Networks","2021","IEEE Access","","","","","","","10.1109/ACCESS.2021.3122834","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118526797&doi=10.1109%2fACCESS.2021.3122834&partnerID=40&md5=e6744eaa56bacd1d06f4053c273e7717","In this paper, we tackle the well-known problem of dataset construction from the point of its generation using generative adversarial networks (GAN). As semantic information of the dataset should have a proper alignment with images, controlling the image generation process of GAN comes to the first position. Considering this, we focus on conditioning the generative process by solely utilizing conditional information to achieve reliable control over the image generation. Unlike the existing works that consider the input (noise or image) in conjunction with conditions, our work considers transforming the input directly to the conditional space by utilizing the given conditions only. By doing so, we reveal the relations between conditions to determine their distinct and reliable feature space without the impact of input information. To fully leverage the conditional information, we propose a novel architectural framework (i.e., conditional transformation) that aims to learn features only from a set of conditions for guiding a generative model by transforming the input to the generator. Such an approach enables controlling the generator by setting its inputs according to the specific conditions necessary for semantically correct image generation. Given that the framework operates at the initial stage of generation, it can be plugged into any existing generative models and trained in an end-to-end manner together with the generator. Extensive experiments on various tasks, such as novel image synthesis and image-to-image translation, demonstrate that the conditional transformation of inputs facilitates solid control over the image generation process and thus shows its applicability for use in dataset construction. Author","Image processing; Job analysis; Process control; Reliability analysis; Semantics; Condition; Conditional image generation; Conditional transformations; Dataset construction; Generation process; Generative model; Generator; Image generations; Images synthesis; Task analysis; Generative adversarial networks","conditional image generation; conditional transformation; Dataset construction; Generative adversarial networks; generative adversarial networks; Generators; Image synthesis; Process control; Reliability; Task analysis; Transforms","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85118526797"
"Pan Y.; Chen Y.; Shen D.; Xia Y.","Pan, Yongsheng (56440550200); Chen, Yuanyuan (57196264464); Shen, Dinggang (7401738392); Xia, Yong (26427407400)","56440550200; 57196264464; 7401738392; 26427407400","Collaborative Image Synthesis and Disease Diagnosis for Classification of Neurodegenerative Disorders with Incomplete Multi-modal Neuroimages","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12905 LNCS","","","480","489","9","10.1007/978-3-030-87240-3_46","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116452877&doi=10.1007%2f978-3-030-87240-3_46&partnerID=40&md5=8c42d3c0ea26de1df3d9f960ee5093dd","The missing data issue is a common problem in multi-modal neuroimage (e.g., MRI and PET) based diagnosis of neurodegenerative disorders. Although various generative adversarial networks (GANs) have been developed to impute the missing data, most current solutions treat the image imputation and disease diagnosis as two standalone tasks without considering the impact of diagnosis on image synthesis, leading to less competent synthetic images to the diagnosis task. In this paper, we propose the collaborative diagnosis-synthesis framework (CDSF) for joint missing neuroimage imputation and multi-modal diagnosis of neurodegenerative disorders. Under the CDSF framework, there is an image synthesis module (ISM) and a multi-modal diagnosis module (MDM), which are trained in a collaborative manner. Specifically, ISM is trained under the supervision of MDM, which poses the feature-consistent constraint to the cross-modality image synthesis, while MDM learns the disease-related multi-modal information from both real and synthetic multi-modal neuroimages. We evaluated our CDSF model against five image synthesis methods and three multi-modal diagnosis models on an ADNI datasets with 1464 subjects. Our results suggest that the proposed CDSF model not only generates neuroimages with higher quality, but also achieves the state-of-the-art performance in AD identification and MCI-to-AD conversion prediction. © 2021, Springer Nature Switzerland AG.","Computer aided diagnosis; Magnetic resonance imaging; Medical computing; Medical imaging; 'current; Consistent constraint; Diagnosis modules; Disease diagnosis; Framework models; Images synthesis; Missing data; Multi-modal; Neurodegenerative disorders; Synthetic images; Neurodegenerative diseases","","Conference paper","Final","","Scopus","2-s2.0-85116452877"
"Zhang Z.; Schomaker L.","Zhang, Zhenxing (57219361101); Schomaker, Lambert (19640514000)","57219361101; 19640514000","DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation","2021","Proceedings of the International Joint Conference on Neural Networks","2021-July","","","","","","10.1109/IJCNN52387.2021.9533527","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116445662&doi=10.1109%2fIJCNN52387.2021.9533527&partnerID=40&md5=64c76a29ab4649f482351116423f1d77","Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. © 2021 IEEE.","Computer vision; Image enhancement; Image resolution; Pixels; Textures; Attention mechanisms; Conditional normalization; Generation method; Image generations; Images synthesis; Modular architectures; Multi-stages; Normalisation; Text-to-image synthesis; Visual loss; Generative adversarial networks","attention mechanism; conditional normalization; text-to-image synthesis; visual loss","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116445662"
"Chai Y.; Zhang H.; Yin Q.; Zhang J.","Chai, Yekun (57219634121); Zhang, Haidong (56045980400); Yin, Qiyue (55376863500); Zhang, Junge (50562506500)","57219634121; 56045980400; 55376863500; 50562506500","Counter-Contrastive Learning for Language GANs","2021","Findings of the Association for Computational Linguistics, Findings of ACL: EMNLP 2021","","","","4834","4839","5","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129219116&partnerID=40&md5=d93e05fcafd5521521e0692aa8a3b993","Generative Adversarial Networks (GANs) have achieved great success in image synthesis, but have proven to be difficult to generate natural language. Challenges arise from the uninformative learning signals passed from the discriminator. In other words, the poor learning signals limit the learning capacity for generating languages with rich structures and semantics. In this paper, we propose to adopt the counter-contrastive learning (CCL) method to support the generator's training in language GANs. In contrast to standard GANs that adopt a simple binary classifier to discriminate whether a sample is real or fake, we employ a counter-contrastive learning signal that advances the training of language synthesizers by (1) pulling the language representations of generated and real samples together and (2) pushing apart representations of real samples to compete with the discriminator and thus prevent the discriminator from being overtrained. We evaluate our method on both synthetic and real benchmarks and yield competitive performance compared to previous GANs for adversarial sequence generation.  © 2021 Association for Computational Linguistics.","Benchmarking; Computational linguistics; Discriminators; Semantics; Binary classifiers; Competitive performance; Images synthesis; Learning capacity; Learning methods; Natural languages; Real samples; Rich structure; Sequence generation; Simple++; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85129219116"
"Zhang H.; Koh J.Y.; Baldridge J.; Lee H.; Yang Y.","Zhang, Han (56098272800); Koh, Jing Yu (57220893064); Baldridge, Jason (14035023300); Lee, Honglak (15056237200); Yang, Yinfei (23468050000)","56098272800; 57220893064; 14035023300; 15056237200; 23468050000","Cross-Modal Contrastive Learning for Text-to-Image Generation","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","833","842","9","10.1109/CVPR46437.2021.00089","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108293394&doi=10.1109%2fCVPR46437.2021.00089&partnerID=40&md5=ca40de2146ea5b4fb0deaf81182fe51b","The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN's output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but-more importantly-people prefer XMC-GAN by 77.3% for image quality and 74.1% for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91. © 2021 IEEE","Computer vision; Image enhancement; Semantics; Cross-modal; Image correspondence; Image generations; Images synthesis; Intermodality; Mutual informations; Photo-realistic; Self modulation; State of the art; Text images; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85108293394"
"Ahn S.; Pham Q.T.M.; Shin J.; Song S.J.","Ahn, Sangil (57207986620); Pham, Quang T. M. (57216319961); Shin, Jitae (7402724357); Song, Su Jeong (22433864200)","57207986620; 57216319961; 7402724357; 22433864200","Future image synthesis for diabetic retinopathy based on the lesion occurrence probability","2021","Electronics (Switzerland)","10","6","726","1","12","11","10.3390/electronics10060726","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102687643&doi=10.3390%2felectronics10060726&partnerID=40&md5=d7ed698968c6f2d1d214079827758319","Diabetic Retinopathy (DR) is one of the major causes of blindness. If the lesions observed in DR occur in the central part of the fundus, it can cause severe vision loss, and we call this symptom Diabetic Macular Edema (DME). All patients with DR potentially have DME since DME can occur in every stage of DR. While synthesizing future fundus images, the task of predicting the progression of the disease state is very challenging since we need a lot of longitudinal data over a long period of time. Even if the longitudinal data are collected, there is a pixel-level difference between the current fundus image and the target future image. It is difficult to train a model based on deep learning for synthesizing future fundus images that considers the lesion change. In this paper, we synthesize future fundus images by considering the progression of the disease with a two-step training approach to overcome these problems. In the first step, we concentrate on synthesizing a realistic fundus image using only a lesion segmentation mask and vessel segmentation mask from a large dataset for a fundus generator. In the second step, we train a lesion probability predictor to create a probability map that contains the occurrence probability information of the lesion. Finally, based on the probability map and current vessel, the pre-trained fundus generator synthesizes a predicted future fundus image. We visually demonstrate not only the capacity of the fundus generator that can control the pathological information but also the prediction of the disease progression on fundus images generated by our framework. Our framework achieves an F1-score of 0.74 for predicting DR severity and 0.91 for predicting DME occurrence. We demonstrate that our framework has a meaningful capability by comparing the scores of each class of DR severity, which are obtained by passing the predicted future image and real future image through an evaluation model. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Future diabetic retinopathy image synthesis; Generative adversarial network; Prediction occurrence probability","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102687643"
"Singhal M.; Agarwal R.","Singhal, Mayank (57348302700); Agarwal, Ritu (57226408998)","57348302700; 57226408998","Cartoon Face to Human Face Translation using Contour Loss based CycleGAN","2021","2021 2nd Global Conference for Advancement in Technology, GCAT 2021","","","","","","","10.1109/GCAT52182.2021.9587703","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119533453&doi=10.1109%2fGCAT52182.2021.9587703&partnerID=40&md5=277bee3d76201b41052a89ef9faefa6c","Cartoon to Human Translation transforms a 2D vector cartoon face to a Real Human Face. The mapping is based on semantic similarity of both the input domains. This is an image$\rightarrow$mage translation problem that finds its applications in the entertainment and animation industry. Cartoon movies evolved from 2D animations in 1930 and became more lifelike with timeline. In image synthesis, audio, and other sorts of data, Generative Adversarial Networks have demonstrated promising outcomes. They also produce excellent results when translating images to images. In this research, a CycleGAN based methodology for generating target Human Faces from source Cartoon Faces is proposed, preserving the facial characteristics i.e. face shape, eyebrow alignment and hair style. In order to improve the mapping we have used contour loss along with cycle consistency loss in our model and patch discriminator is used with L2 norm. © 2021 IEEE.","Computer vision; Discriminators; Mapping; Semantics; 2D animation; Cycle consistency; Cyclegan; Human faces; Images synthesis; ITS applications; L2-norm; Patch discriminator; Semantic similarity; Translation transforms; Generative adversarial networks","cycle consistency; CycleGAN; Generative Adversarial Networks; l2 norm; patch discriminator","Conference paper","Final","","Scopus","2-s2.0-85119533453"
"Ikeda Y.; Doman K.; Mekada Y.; Nawano S.","Ikeda, Yusuke (57225034986); Doman, Keisuke (35931846700); Mekada, Yoshito (6602322161); Nawano, Shigeru (56274019600)","57225034986; 35931846700; 6602322161; 56274019600","Lesion image generation using conditional gan for metastatic liver cancer detection","2021","Journal of Image and Graphics(United Kingdom)","9","1","","27","30","3","10.18178/joig.9.1.27-30","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109003969&doi=10.18178%2fjoig.9.1.27-30&partnerID=40&md5=9c7096a5c02c834d1ccfdf58865eff76","—In the diagnosis of the abdomen, CT images taken under various conditions are visually checked by multiple doctors. Since diagnosing CT images requires doctors to take time and effort, a Computer-Aided Diagnosis system (CAD) based on a machine learning technique is expected. It is, however, difficult to collect a large number of case images for machine learning. In this paper, we propose a method to generate lesion images by a Conditional Generative Adversarial Networks (CGAN) and show the effectiveness of the proposed method by the accuracy of liver cancer detection from CT images. A CGAN which generates pseudo lesion images is trained with real lesion images labeled with “edge” and “non-edge” of the liver. We confirmed that the proposed method achieved the detection rate of 0.85 and the false positives per case of 0.20. The detection accuracy was higher than that of a conventional method. ©2021 Journal of Image and Graphics.","","Computer aided diagnosis; Deep learning; Image synthesis; Index Terms—CT image","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85109003969"
"Hong S.; Marinescu R.; Dalca A.V.; Bonkhoff A.K.; Bretzner M.; Rost N.S.; Golland P.","Hong, Sungmin (57221347523); Marinescu, Razvan (56878724200); Dalca, Adrian V. (23972147300); Bonkhoff, Anna K. (57194272750); Bretzner, Martin (57204160159); Rost, Natalia S. (35269850500); Golland, Polina (6603058524)","57221347523; 56878724200; 23972147300; 57194272750; 57204160159; 35269850500; 6603058524","3D-StyleGAN: A Style-Based Generative Adversarial Network for Generative Modeling of Three-Dimensional Medical Images","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13003 LNCS","","","24","34","10","10.1007/978-3-030-88210-5_3","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116863452&doi=10.1007%2f978-3-030-88210-5_3&partnerID=40&md5=7c5d1105f1fce21eddd3d6824fca4c2d","Image synthesis via Generative Adversarial Networks (GANs) of three-dimensional (3D) medical images has great potential that can be extended to many medical applications, such as, image enhancement and disease progression modeling. Current GAN technologies for 3D medical image synthesis must be significantly improved to be suitable for real-world medical problems. In this paper, we extend the state-of-the-art StyleGAN2 model, which natively works with two-dimensional images, to enable 3D image synthesis. In addition to the image synthesis, we investigate the behavior and interpretability of the 3D-StyleGAN via style vectors inherited form the original StyleGAN2 that are highly suitable for medical applications: (i) the latent space projection and reconstruction of unseen real images, and (ii) style mixing. The model can be applied to any 3D volumetric images. We demonstrate the 3D-StyleGAN’s performance and feasibility with ∼ 12,000 three-dimensional full brain MR T1 images. Furthermore, we explore different configurations of hyperparameters to investigate potential improvement of the image synthesis with larger networks. The codes and pre-trained networks are available online: https://github.com/sh4174/3DStyleGAN. © 2021, Springer Nature Switzerland AG.","3D modeling; Image enhancement; Medical applications; Medical imaging; Medical problems; Three dimensional computer graphics; Vector spaces; 'current; 3D medical image; Disease progression modelling; Generative model; Images synthesis; Network technologies; Real-world; State of the art; Three dimensional (3D) medical images; Two dimensional images; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116863452"
"Shen L.; Zhu W.; Wang X.; Xing L.; Pauly J.M.; Turkbey B.; Harmon S.A.; Sanford T.H.; Mehralivand S.; Choyke P.L.; Wood B.J.; Xu D.","Shen, Liyue (57201327444); Zhu, Wentao (57192672140); Wang, Xiaosong (57192075663); Xing, Lei (57213673917); Pauly, John M. (7101724924); Turkbey, Baris (9435311800); Harmon, Stephanie Anne (57191581147); Sanford, Thomas Hogue (36716778100); Mehralivand, Sherif (24169535900); Choyke, Peter L. (56777388300); Wood, Bradford J. (7401873523); Xu, Daguang (57191707223)","57201327444; 57192672140; 57192075663; 57213673917; 7101724924; 9435311800; 57191581147; 36716778100; 24169535900; 56777388300; 7401873523; 57191707223","Multi-Domain Image Completion for Random Missing Input Data","2021","IEEE Transactions on Medical Imaging","40","4","9302720","1113","1122","9","10.1109/TMI.2020.3046444","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098783065&doi=10.1109%2fTMI.2020.3046444&partnerID=40&md5=66f12badcc3842df79115e131a156199","Multi-domain data are widely leveraged in vision applications taking advantage of complementary information from different modalities, e.g., brain tumor segmentation from multi-parametric magnetic resonance imaging (MRI). However, due to possible data corruption and different imaging protocols, the availability of images for each domain could vary amongst multiple data sources in practice, which makes it challenging to build a universal model with a varied set of input data. To tackle this problem, we propose a general approach to complete the random missing domain(s) data in real applications. Specifically, we develop a novel multi-domain image completion method that utilizes a generative adversarial network (GAN) with a representational disentanglement scheme to extract shared content encoding and separate style encoding across multiple domains. We further illustrate that the learned representation in multi-domain image completion could be leveraged for high-level tasks, e.g., segmentation, by introducing a unified framework consisting of image completion and segmentation with a shared content encoder. The experiments demonstrate consistent performance improvement on three datasets for brain tumor segmentation, prostate segmentation, and facial expression image completion respectively.  © 1982-2012 IEEE.","Brain Neoplasms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Brain; Encoding (symbols); Image enhancement; Image reconstruction; Input output programs; Magnetic domains; Magnetic resonance imaging; Signal encoding; Tumors; nuclear magnetic resonance imaging agent; Adversarial networks; Brain tumor segmentation; Consistent performance; Facial Expressions; Multiple data sources; Prostate segmentation; Real applications; Vision applications; Article; brain tumor; data extraction; data synthesis; deep neural network; facial expression; human; image analysis; image reconstruction; image segmentation; multiparametric magnetic resonance imaging; neuroimaging; prostate; brain tumor; diagnostic imaging; image processing; male; nuclear magnetic resonance imaging; Image segmentation","medical image synthesis; missing data problem; missing-domain segmentation; multi-contrast MRI; Multi-domain image-to-image translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098783065"
"Gao L.; Chen D.; Zhao Z.; Shao J.; Shen H.T.","Gao, Lianli (56611089900); Chen, Daiyuan (57217988818); Zhao, Zhou (55959624600); Shao, Jie (57002035900); Shen, Heng Tao (7404523209)","56611089900; 57217988818; 55959624600; 57002035900; 7404523209","Lightweight dynamic conditional GAN with pyramid attention for text-to-image synthesis","2021","Pattern Recognition","110","","107384","","","","10.1016/j.patcog.2020.107384","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087800993&doi=10.1016%2fj.patcog.2020.107384&partnerID=40&md5=82b3eeb7e0109a4518b9a75885e3f7c0","The text-to-image synthesis task aims to generate photographic images conditioned on semantic text descriptions. To ensure the sharpness and fidelity of generated images, this task tends to generate high-resolution images (e.g., 1282 or 2562). However, as the resolution increases, the network parameters and complexity increases dramatically. Recent works introduce network structures with extensive parameters and heavy computations to guarantee the production of high-resolution images. As a result, these models come across problems of the unstable training process and high training cost. To tackle these issues, in this paper, we propose an effective information compensation based approach, namely Lightweight Dynamic Conditional GAN (LD-CGAN). LD-CGAN is a compact and structured single-stream network, and it consists of one generator and two independent discriminators to regularize and generate 642 and 1282 images in one feed-forward process. Specifically, the generator of LD-CGAN is composed of three major components: (1) Conditional Embedding (CE), which is an automatically unsupervised learning process aiming at disentangling integrated semantic attributes in the text space; (2) Conditional Manipulating Modular (CM-M) in Conditional Manipulating Block (CM-B), which is designed to continuously provide the image features with the compensation information (i.e., the disentangled attribute); and (3) Pyramid Attention Refine Block (PAR-B), which is used to enrich multi-scale features by capturing spatial importance between multi-scale context. Consequently, experiments conducted under two benchmark datasets, CUB and Oxford-102, indicate that our generated 1282 images can achieve comparable performance with 2562 images generated by the state-of-the-arts on two evaluation metrics: Inception Score (IS) and Visual-semantic Similarity (VS). Compared with the current state-of-the-art HDGAN, our LD-CGAN significantly decreases the number of parameters and computation time by 86.8% and 94.9%, respectively. © 2020","Benchmarking; Parameter estimation; Photography; Semantics; Benchmark datasets; Evaluation metrics; High resolution image; Multi-scale features; Network parameters; Network structures; Photographic image; Resolution increase; Arts computing","Conditional generative adversarial network (CGAN); Disentanglement process; Entanglement process; Information compensation; Network complexity; Pyramid attentive fusion; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85087800993"
"Yurt M.; Dar S.U.; Erdem A.; Erdem E.; Oguz K.K.; Çukur T.","Yurt, Mahmut (57211187325); Dar, Salman UH (57195220338); Erdem, Aykut (13410510300); Erdem, Erkut (13410837300); Oguz, Kader K (35564489500); Çukur, Tolga (23034054800)","57211187325; 57195220338; 13410510300; 13410837300; 35564489500; 23034054800","Mustgan: Multi-stream generative adversarial networks for MR image synthesis","2021","Medical Image Analysis","70","","101944","","","","10.1016/j.media.2020.101944","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102021754&doi=10.1016%2fj.media.2020.101944&partnerID=40&md5=bd07d71e999a34c9d0fb66eb9c690e57","Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one methods take as input a single source contrast, and they learn a latent representation sensitive to unique features of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source images via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many-to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods. © 2020 Elsevier B.V.","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Magnetic resonance imaging; Adversarial networks; Complementary features; Morphological information; Multiple source; Radiological assessment; State of the art; Synthesis method; Unique features; adult; article; clinical assessment; controlled study; human; motion; nuclear magnetic resonance imaging; quantitative analysis; synthesis; image processing; Image enhancement","Fusion; Generative adversarial networks (GAN); Image synthesis; Magnetic resonance imaging (MRI); Multi-contrast; Multi-stream","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102021754"
"Meharban M.S.; Sabu M.K.; Krishnan S.","Meharban, M S (57191912889); Sabu, M K (25121997800); Krishnan, Santhana (57645389900)","57191912889; 25121997800; 57645389900","Introduction to Medical Image Synthesis Using Deep Learning:A Review","2021","2021 7th International Conference on Advanced Computing and Communication Systems, ICACCS 2021","","","9442041","414","419","5","10.1109/ICACCS51430.2021.9442041","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108029186&doi=10.1109%2fICACCS51430.2021.9442041&partnerID=40&md5=f6e7c9416fe751afe8b846c800c39fbf","Medical imaging performs a vital function in unique medical programs. But, because of multiple issues like price and radiation dose, the purchase of sure image modalities is moreover limited. Therefore, clinical photo synthesis is of excellent profit with the aid of estimating a desired imaging modality whereas no longer acquisition companion in nursing real experiment. Photograph synthesis has attracted people's attention due to its plethora of application in social media. The GANs have finished tremendous finally ends up inside the international of photo synthesis, like CycleGAN. This overview paper gives revolutionary growth in generative antagonistic networks-based clinical programs like clinical photo technology, and cross-modality synthesis. © 2021 IEEE.","Medical imaging; Purchasing; Clinical photos; Cross modality; Image modality; Image synthesis; Imaging modality; Multiple issue; Social media; Deep learning","CycleGAN:Cycle Generative Adversarial Networks; GANs:Generative Adversarial Networks","Conference paper","Final","","Scopus","2-s2.0-85108029186"
"Li G.; Liu Y.; Wei X.; Zhang Y.; Wu S.; Xu Y.; Wong H.-S.","Li, Guanyue (57211747687); Liu, Yi (57225002733); Wei, Xiwen (57345724700); Zhang, Yang (57840860900); Wu, Si (55495122900); Xu, Yong (57274194400); Wong, Hau-San (7402864844)","57211747687; 57225002733; 57345724700; 57840860900; 55495122900; 57274194400; 7402864844","Discovering Density-Preserving Latent Space Walks in GANs for Semantic Image Transformations","2021","MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia","","","","1562","1570","8","10.1145/3474085.3475293","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119380434&doi=10.1145%2f3474085.3475293&partnerID=40&md5=64f47a48516b48dc3db7b7a79e3b7649","Generative adversarial network (GAN)-based models possess superior capability of high-fidelity image synthesis. There are a wide range of semantically meaningful directions in the latent representation space of well-trained GANs, and the corresponding latent space walks are meaningful for semantic controllability in the synthesized images. To explore the underlying organization of a latent space, we propose an unsupervised Density-Preserving Latent Semantics Exploration model (DP-LaSE). The important latent directions are determined by maximizing the variations in intermediate features, while the correlation between the directions is minimized. Considering that latent codes are sampled from a prior distribution, we adopt a density-preserving regularization approach to ensure latent space walks are maintained in iso-density regions, since moving to a higher/lower density region tends to cause unexpected transformations. To further refine semantics-specific transformations, we perform subspace learning over intermediate feature channels, such that the transformations are limited to the most relevant subspaces. Extensive experiments on a variety of benchmark datasets demonstrate that DP-LaSE is able to discover interpretable latent space walks, and specific properties of synthesized images can thus be precisely controlled. © 2021 ACM.","Image processing; Semantic Web; Semantics; Density regions; Density-preserving; Exploration model; Image transformations; Latent semantics; Latent space walk; Semantic controllability; Semantic images; Space walks; Synthesized images; Generative adversarial networks","density-preserving; generative adversarial networks; latent space walks; semantic controllability","Conference paper","Final","","Scopus","2-s2.0-85119380434"
"Zhou Y.; Zhao P.; Tong W.; Zhu Y.","Zhou, Yingbo (57221722932); Zhao, Pengcheng (57220960961); Tong, Weiqin (7202450909); Zhu, Yongxin (36903371100)","57221722932; 57220960961; 7202450909; 36903371100","Cdl-gan: Contrastive distance learning generative adversarial network for image generation","2021","Applied Sciences (Switzerland)","11","4","1380","1","15","14","10.3390/app11041380","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100617394&doi=10.3390%2fapp11041380&partnerID=40&md5=4092e980f1d6a3d5f99a77f63f95a494","While Generative Adversarial Networks (GANs) have shown promising performance in image generation, they suffer from numerous issues such as mode collapse and training instability. To stabilize GAN training and improve image synthesis quality with diversity, we propose a simple yet effective approach as Contrastive Distance Learning GAN (CDL-GAN) in this paper. Specifically, we add Consistent Contrastive Distance (CoCD) and Characteristic Contrastive Distance (ChCD) into a principled framework to improve GAN performance. The CoCD explicitly maximizes the ratio of the distance between generated images and the increment between noise vectors to strengthen image feature learning for the generator. The ChCD measures the sampling distance of the encoded images in Euler space to boost feature representations for the discriminator. We model the framework by employing Siamese Network as a module into GANs without any modification on the backbone. Both qualitative and quantitative experiments conducted on three public datasets demonstrate the effectiveness of our method. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Characteristic contrastive distance; Consistent contrastive distance; Contrastive distance learning; Generative adversarial networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100617394"
"Liu X.; Xing F.; Prince J.L.; Carass A.; Stone M.; Fakhri G.E.; Woo J.","Liu, Xiaofeng (57193764777); Xing, Fangxu (39162147600); Prince, Jerry L. (56600943200); Carass, Aaron (15061054500); Stone, Maureen (7402473365); Fakhri, Georges El (7003672447); Woo, Jonghye (23986697400)","57193764777; 39162147600; 56600943200; 15061054500; 7402473365; 7003672447; 23986697400","Dual-cycle constrained bijective vae-gan for tagged-to-cine magnetic resonance image synthesis","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9433852","1448","1452","4","10.1109/ISBI48211.2021.9433852","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107180182&doi=10.1109%2fISBI48211.2021.9433852&partnerID=40&md5=a9189cbd6e93f618c6cccfbbca7886ae","Tagged magnetic resonance imaging (MRI) is a widely used imaging technique for measuring tissue deformation in moving organs. Due to tagged MRI's intrinsic low anatomical resolution, another matching set of cine MRI with higher resolution is sometimes acquired in the same scanning session to facilitate tissue segmentation, thus adding extra time and cost. To mitigate this, in this work, we propose a novel dual-cycle constrained bijective VAE-GAN approach to carry out tagged-to-cine MR image synthesis. Our method is based on a variational autoencoder backbone with cycle reconstruction constrained adversarial training to yield accurate and realistic cine MR images given tagged MR images. Our framework has been trained, validated, and tested using 1, 768, 416, and 1, 560 subject-independent paired slices of tagged and cine MRI from twenty healthy subjects, respectively, demonstrating superior performance over the comparison methods. Our method can potentially be used to reduce the extra acquisition time and cost, while maintaining the same workflow for further motion analyses.  © 2021 IEEE.","Cost benefit analysis; Medical imaging; Tissue; Acquisition time; Auto encoders; Comparison methods; Healthy subjects; Higher resolution; Tagged MRI; Tissue deformations; Tissue segmentation; Magnetic resonance imaging","Deep learning; Generative adversarial networks; Image synthesis; Tagged MRI","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107180182"
"Sindhura D.N.; Pai R.M.; Bhat S.N.; Manohara Pai M.M.","Sindhura, D.N. (57423642600); Pai, Radhika M (57194587874); Bhat, Shyamasunder N (17343579200); Manohara Pai, M.M. (57219054419)","57423642600; 57194587874; 17343579200; 57219054419","Synthetic Vertebral Column Fracture Image Generation by Deep Convolution Generative Adversarial Networks","2021","Proceedings of CONECCT 2021: 7th IEEE International Conference on Electronics, Computing and Communication Technologies","","","","","","","10.1109/CONECCT52877.2021.9622527","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123377818&doi=10.1109%2fCONECCT52877.2021.9622527&partnerID=40&md5=06b28761b217e86532f7a3878ae028c7","In the field of medical imaging, the challenging objective is to generate synthetic, realistic images which resembles the original images. The generated synthetic images would enhance the accuracy of the computer-assisted classification, Decision Support System, which aid the doctor in diagnosis of diseases. The Generative Adversarial Networks (GANs), is a method of data augmentation which can be used to generate synthetic realistic looking images, however low quality images are generated. For AI models, it is challenging tasks to do classification using this low quality images. In this work, generation of high quality synthetic medical image using Deep Convolutional Generative Adversarial Networks (DCGANs) is presented. Data augmentation method by DCGANs is illustrated on the limited dataset of CT (Computed Tomography) images of vertebral column fracture. A total of 340 CT scan images were taken for the study, which comprises of complete burst fracture scans of vertebral column. The evaluation of the generated images was done with Visual Turing Test. © 2021 IEEE.","Computer aided analysis; Computer aided diagnosis; Computerized tomography; Decision support systems; Fracture; Generative adversarial networks; Image enhancement; Medical imaging; Musculoskeletal system; AI model; Data augmentation; Deep convolutional generative adversarial network; Images synthesis; Low qualities; Quality image; Turing tests; Vertebral column; Vertebral column fracture; Visual turing test; Convolution","AI models; computed tomography; data augmentation; Deep Convolutional Generative Adversarial Network; Image synthesis; vertebral column fracture; Visual Turing Test","Conference paper","Final","","Scopus","2-s2.0-85123377818"
"Shamsolmoali P.; Zareapoor M.; Granger E.; Zhou H.; Wang R.; Celebi M.E.; Yang J.","Shamsolmoali, Pourya (56350053200); Zareapoor, Masoumeh (56349635100); Granger, Eric (7004286338); Zhou, Huiyu (23062556900); Wang, Ruili (55825442900); Celebi, M. Emre (55667346700); Yang, Jie (15039078800)","56350053200; 56349635100; 7004286338; 23062556900; 55825442900; 55667346700; 15039078800","Image synthesis with adversarial networks: A comprehensive survey and case studies","2021","Information Fusion","72","","","126","146","20","10.1016/j.inffus.2021.02.014","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101926388&doi=10.1016%2fj.inffus.2021.02.014&partnerID=40&md5=56bb72e09612e58483b608ab6e94140f","Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study. © 2021 Elsevier B.V.","Medicine; Natural language processing systems; Surveys; Adversarial networks; Data-driven methods; Evaluation metrics; Future research directions; NAtural language processing; Software implementation; Synthetic image generation; Training data sets; Image processing","Classification; GANs; Image fusion; Image synthesis; Image-to-image translation","Short survey","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101926388"
"Alarsan F.I.; Younes M.","Alarsan, Fajr Ibrahem (57210839967); Younes, Mamoon (57210841424)","57210839967; 57210841424","Best Selection of Generative Adversarial Networks Hyper-Parameters Using Genetic Algorithm","2021","SN Computer Science","2","4","283","","","","10.1007/s42979-021-00689-3","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131809985&doi=10.1007%2fs42979-021-00689-3&partnerID=40&md5=48e67d1acda4d0e24d01dbd71a7ee0d8","Generative adversarial networks (GANs) are most popular generative frameworks that have achieved compelling performance. They follow an adversarial approach where two deep models generator and discriminator compete with each other. In this paper, we propose a Generative Adversarial Network with best hyper-parameters selection to generate fake images for digit numbers 1–9 with generator and train discriminator to decide whereas the generated images are fake or true. Genetic algorithm (GA) technique was used to adapt GAN hyper-parameters, the resulted algorithm is named GANGA: generative adversarial network with genetic algorithm. The resulted algorithm has achieved high performance; it was able to get zero value of loss function for the generator and discriminator separately. Anaconda environment with tensorflow library facilitates was used; python as programming language was adapted with needed libraries. The implementation was done using MNIST dataset to validate the work. The proposed method is to let genetic algorithm choose best values of hyper-parameters depending on minimizing a cost function such as a loss function or maximizing accuracy function used to find best values of learning rate, batch normalization, number of neurons and a parameter of dropout layer. © 2021, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.","","Discriminator; Generative adversarial networks (GAN); Generator; Genetic algorithm; Image synthesis; MNIST dataset","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131809985"
"Mutepfe F.; Kalejahi B.K.; Meshgini S.; Danishvar S.","Mutepfe, Freedom (57336795800); Kalejahi, Behnam Kiani (57219496523); Meshgini, Saeed (26648943100); Danishvar, Sebelan (22134243100)","57336795800; 57219496523; 26648943100; 22134243100","Generative adversarial network image synthesis method for skin lesion generation and classification","2021","Journal of Medical Signals and Sensors","11","4","","237","252","15","10.4103/jmss.JMSS_53_20","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118972364&doi=10.4103%2fjmss.JMSS_53_20&partnerID=40&md5=ca8f7150c4b1ffcfa2987e11c2dfbf1b","Background: One of the common limitations in the treatment of cancer is in the early detection of this disease. The customary medical practice of cancer examination is a visual examination by the dermatologist followed by an invasive biopsy. Nonetheless, this symptomatic approach is time-consuming and prone to human errors. An automated machine learning model is essential to capacitate fast diagnoses and early treatment. Objective: The key objective of this study is to establish a fully automatic model that helps Dermatologists in skin cancer handling process in a way that could improve skin lesion classification accuracy. Method: The work is conducted following an implementation of a Deep Convolutional Generative Adversarial Network (DCGAN) using the Python-based deep learning library Keras. We incorporated effective image filtering and enhancement algorithms such as bilateral filter to enhance feature detection and extraction during training. The Deep Convolutional Generative Adversarial Network (DCGAN) needed slightly more fine-tuning to ripe a better return. Hyperparameter optimization was utilized for selecting the best-performed hyperparameter combinations and several network hyperparameters. In this work, we decreased the learning rate from the default 0.001 to 0.0002, and the momentum for Adam optimization algorithm from 0.9 to 0.5, in trying to reduce the instability issues related to GAN models and at each iteration the weights of the discriminative and generative network were updated to balance the loss between them. We endeavour to address a binary classification which predicts two classes present in our dataset, namely benign and malignant. More so, some well-known metrics such as the receiver operating characteristic -area under the curve and confusion matrix were incorporated for evaluating the results and classification accuracy. Results: The model generated very conceivable lesions during the early stages of the experiment and we could easily visualise a smooth transition in resolution along the way. Thus, we have achieved an overall test accuracy of 93.5% after fine-tuning most parameters of our network. Conclusion: This classification model provides spatial intelligence that could be useful in the future for cancer risk prediction. Unfortunately, it is difficult to generate high quality images that are much like the synthetic real samples and to compare different classification methods given the fact that some methods use non-public datasets for training. © 2021 Isfahan University of Medical Sciences(IUMS). All rights reserved.","Adam optimization algorithm; area under the curve; Article; binary classification; classification algorithm; comparative study; convolutional neural network; deep convolutional generative adversarial network; deep learning; dermatologist; diagnostic accuracy; feature detection; image analysis; image enhancement; image quality; malignant neoplasm; receiver operating characteristic; skin cancer; skin defect; stochastic gradient descent optimizer","DCGAN; dermoscopy; pretraining; skin lesion","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118972364"
"Park H.; Waseem T.; Teo W.Q.; Hwei Low Y.; Lim M.K.; Yong Chong C.","Park, Hyejin (57222422044); Waseem, Taaha (57222420914); Teo, Wen Qi (57222421569); Hwei Low, Ying (57222421273); Lim, Mei Kuan (24824794500); Yong Chong, Chun (55789402100)","57222422044; 57222420914; 57222421569; 57222421273; 24824794500; 55789402100","Robustness Evaluation of Stacked Generative Adversarial Networks using Metamorphic Testing","2021","Proceedings - 2021 IEEE/ACM 6th International Workshop on Metamorphic Testing, MET 2021","","","9477682","1","8","7","10.1109/MET52542.2021.00008","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114274138&doi=10.1109%2fMET52542.2021.00008&partnerID=40&md5=a1f180f3e2a0e437bac33bb87c81d1a8","Synthesising photo-realistic images from natural language is one of the challenging problems in computer vision. Over the past decade, a number of approaches have been proposed, of which the improved Stacked Generative Adversarial Network (StackGAN-v2) has proven capable of generating high resolution images that reflect the details specified in the input text descriptions. In this paper, we aim to assess the robustness and fault-tolerance capability of the StackGAN-v2 model by introducing variations in the training data. However, due to the working principle of Generative Adversarial Network (GAN), it is difficult to predict the output of the model when the training data are modified. Hence, in this work, we adopt Metamorphic Testing technique to evaluate the robustness of the model with a variety of unexpected training dataset. As such, we first implement StackGAN-v2 algorithm and test the pre-trained model provided by the original authors to establish a ground truth for our experiments. We then identify a metamorphic relation, from which test cases are generated. Further, metamorphic relations were derived successively based on the observations of prior test results. Finally, we synthesise the results from our experiment of all the metamorphic relations and found that StackGAN-v2 algorithm is susceptible to input images with obtrusive objects, even if it overlaps with the main object minimally, which was not reported by the authors and users of StackGAN-v2 model. The proposed metamorphic relations can be applied to other text-to-image synthesis models to not only verify the robustness but also to help researchers understand and interpret the results made by the machine learning models.  © 2021 IEEE.","Fault tolerance; Image enhancement; Adversarial networks; Fault-tolerance capability; High resolution image; Machine learning models; Metamorphic relations; Metamorphic testing; Photorealistic images; Robustness evaluation; Statistical tests","metamorphic relations; Metamorphic testing; robustness testing; Stacked Generative Adversarial Network","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85114274138"
"Sanaat A.; Boehringer A.; Ghavabesh A.; Shiri I.; Salimi Y.; Arabi H.; Zaidi H.","Sanaat, A. (57207449502); Boehringer, A. (57913622600); Ghavabesh, A. (57914047400); Shiri, I. (57191829519); Salimi, Y. (57191823474); Arabi, H. (57220877447); Zaidi, H. (7004977873)","57207449502; 57913622600; 57914047400; 57191829519; 57191823474; 57220877447; 7004977873","Deep-PVC: A Deep Learning Model for Synthesizing Full-Dose Partial Volume Corrected PET Images from Low-Dose Images","2021","2021 IEEE Nuclear Science Symposium and Medical Imaging Conference Record, NSS/MIC 2021 and 28th International Symposium on Room-Temperature Semiconductor Detectors, RTSD 2022","","","","","","","10.1109/NSS/MIC44867.2021.9875501","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139140078&doi=10.1109%2fNSS%2fMIC44867.2021.9875501&partnerID=40&md5=a17c7d4300ff5e251a5e3d1d0e380f17","Moderating the injected activity and/or reducing acquisition time to minimize potential radiation hazards and increase patient comfort are important trends in PET. This work aims to assess the performance of regular full-dose partial volume corrected (FD-PVC) image synthesis from fast/low-dose (LD) brain PET images using deep learning techniques for 18F-FDG, 18F-Flortaucipir, and 18F-Flutemetamol radiotracers. Clinical brain PET/CT studies of 100 patients for each radiotracer were employed. The 5% of the events were randomly selected from the FD list-mode PET data to simulate a realistic LD acquisition. A modified cycle-consistent generative adversarial network (CycleGAN) model was implemented to predict FD-PVC PET images. Quantitative analysis using established metrics, including the peak signal-to-noise ratio (PSNR), structural similarity index metric (SSIM), root mean square error (RMSE), and standardized uptake value (SUV) bias, were performed. The R2 values were 0.95, 0.90, and 0.93 for 18F-FDG, 18F-Flortaucipir, and 18F-Flutemetamol, respectively. PSNR and SSIM values of 33.62±2.35 and 31.81±3.7, 34.44±4.11 and 0.96±0.02, 0.96±0.03, and 0.95±0.04 were obtained for 18F-FDG, 18F-Flortaucipir, and 18F-Flutemetamol, respectively. The CycleGAN is able to generate PVC images similar to references from under sampled LD images. © 2021 IEEE.","Deep learning; Finite difference method; Learning systems; Mean square error; Medical imaging; Radiotherapy; Signal to noise ratio; Acquisition time; Learning models; Low dose; Partial volumes; Patient comfort; Peak signal to noise ratio; Performance; PET images; Similarity indices; Structural similarity; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85139140078"
"Li G.; Jiao Q.; Qian S.; Wu S.; Wong H.-S.","Li, Guanyue (57211747687); Jiao, Qianfen (57219596151); Qian, Sheng (57212420158); Wu, Si (55495122900); Wong, Hau-San (7402864844)","57211747687; 57219596151; 57212420158; 55495122900; 7402864844","High Fidelity GAN Inversion via Prior Multi-Subspace Feature Composition","2021","35th AAAI Conference on Artificial Intelligence, AAAI 2021","9B","","","8366","8374","8","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130631349&partnerID=40&md5=ab9b5fb2be1ddca8b77c6643b410bbd2","Generative Adversarial Networks (GANs) have shown impressive gains in image synthesis. GAN inversion was recently studied to understand and utilize the knowledge it learns, where a real image is inverted back to a latent code and can thus be reconstructed by the generator. Although increasing the number of latent codes can improve inversion quality to a certain extent, we find that important details may still be neglected when performing feature composition over all the intermediate feature channels. To address this issue, we propose a Prior multi-Subspace Feature Composition (PmSFC) approach for high-fidelity inversion. Considering that the intermediate features are highly correlated with each other, we incorporate a self-expressive layer in the generator to discover meaningful subspaces. In this case, the features at a channel can be expressed as a linear combination of those at other channels in the same subspace. We perform feature composition separately in the subspaces. The semantic differences between them benefit the inversion quality, since the inversion process is regularized based on different aspects of semantics. In the experiments, the superior performance of PmSFC demonstrates the effectiveness of prior subspaces in facilitating GAN inversion together with extended applications in visual manipulation. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Image processing; Semantics; High-fidelity; Highly-correlated; Images synthesis; Knowledge IT; Learn+; Linear combinations; Multi subspace; Network inversion; Real images; Semantic difference; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85130631349"
"Das N.; Kundu S.; Deb S.","Das, Nataraj (57230075000); Kundu, Shaona (57456873500); Deb, Suman (55003599100)","57230075000; 57456873500; 55003599100","Image Synthesis of Warli Tribal Stick figures using Generative Adversarial Networks","2021","2021 IEEE 6th International Conference on Computing, Communication and Automation, ICCCA 2021","","","","266","271","5","10.1109/ICCCA52192.2021.9666257","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124799982&doi=10.1109%2fICCCA52192.2021.9666257&partnerID=40&md5=0500c093a2cfd0368c6484bb8f29ac4e","With the advancement in deep learning methods, Generative Adversarial Neural Networks(GANs) have been successful in the application of Computer Vision, Natural Language Processing, and medical. It is observed to have great research potential of using GAN for Image Synthesis. GANs can generate meaningful samples from complex patters or distributions. We present a method to generate Asian Tribal human synthetic stick figures using GAN. With variety of applications the output are limited to low resolution and are unrealistic. The work includes image to image translation where the inputs are low resolution images and as the training progresses, addition of new layers lead to better or high resolution output. For this purpose, it was mandatory to train the generator and discriminator following the principles governing the concept of transfer learning and to fetch evidence of how the generated output resembles the original data set. We also extend additional visualizations of our approach and support our GAN model with images of our generated output. This piece of work includes customized data set as input, where the input images are constructed based on laws of coordinate geometry and spherical polar coordinates and final generated images to illustrate its resemblance with the actual input images. The generated images were quite efficient in faking the discriminator, contributing towards robustness, precision and viability of the stated architecture.  © 2021 IEEE.","Deep learning; Discriminators; Fake detection; Natural language processing systems; Data set; Deep learning; Fake image; Generative adversarial neural network; Generator; Images synthesis; Input image; Neural-networks; Real images; Warli stick figure; Generative adversarial networks","Deep Learning; discriminator; fake images; GAN; generative adversarial networks; generator; image synthesis; real images; warli stick figure","Conference paper","Final","","Scopus","2-s2.0-85124799982"
"Frolov S.; Sharma A.; Hees J.; Karayil T.; Raue F.; Dengel A.","Frolov, Stanislav (57221158423); Sharma, Avneesh (57222757839); Hees, Jörn (35229784500); Karayil, Tushar (57188723660); Raue, Federico (57141992100); Dengel, Andreas (6603764314)","57221158423; 57222757839; 35229784500; 57188723660; 57141992100; 6603764314","AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13024 LNCS","","","361","375","14","10.1007/978-3-030-92659-5_23","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124255179&doi=10.1007%2f978-3-030-92659-5_23&partnerID=40&md5=cb7ed7e55233b9069e46999df04fe2dc","Conditional image synthesis from layout has recently attracted much interest. Previous approaches condition the generator on object locations as well as class labels but lack fine-grained control over the diverse appearance aspects of individual objects. Gaining control over the image generation process is fundamental to build practical applications with a user-friendly interface. In this paper, we propose a method for attribute controlled image synthesis from layout which allows to specify the appearance of individual objects without affecting the rest of the image. We extend a state-of-the-art approach for layout-to-image generation to additionally condition individual objects on attributes. We create and experiment on a synthetic, as well as the challenging Visual Genome dataset. Our qualitative and quantitative results show that our method can successfully control the fine-grained details of individual objects when modelling complex scenes with multiple objects. Source code, dataset and pre-trained models are publicly available (https://github.com/stanifrolov/AttrLostGAN ). © 2021, Springer Nature Switzerland AG.","Computer vision; Computers; Class labels; Condition; Fine-grained control; Generation process; Image generations; Images synthesis; Individual objects; Object class; Object location; Reconfigurable; Generative adversarial networks","Generative Adversarial Networks; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85124255179"
"Guo P.; Wang P.; Yasarla R.; Zhou J.; Patel V.M.; Jiang S.","Guo, Pengfei (57215060760); Wang, Puyang (57194870401); Yasarla, Rajeev (57195373734); Zhou, Jinyuan (7405548160); Patel, Vishal M. (56660008900); Jiang, Shanshan (56495940500)","57215060760; 57194870401; 57195373734; 7405548160; 56660008900; 56495940500","Anatomic and Molecular MR Image Synthesis Using Confidence Guided CNNs","2021","IEEE Transactions on Medical Imaging","40","10","","2832","2844","12","10.1109/TMI.2020.3046460","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098792747&doi=10.1109%2fTMI.2020.3046460&partnerID=40&md5=3dc08951e02c29bcc60d3829d4a2f79f","Data-driven automatic approaches have demonstrated their great potential in resolving various clinical diagnostic dilemmas in neuro-oncology, especially with the help of standard anatomic and advanced molecular MR images. However, data quantity and quality remain a key determinant, and a significant limit of the potential applications. In our previous work, we explored the synthesis of anatomic and molecular MR image networks (SAMR) in patients with post-Treatment malignant gliomas. In this work, we extend this through a confidence-guided SAMR (CG-SAMR) that synthesizes data from lesion contour information to multi-modal MR images, including T1-weighted ( {T}-{1}\text{w} ), gadolinium enhanced {T}-{1}\text{w} (Gd-{T}-{1}\text{w} ), T2-weighted ( {T}-{2}\text{w} ), and fluid-Attenuated inversion recovery ( \textit {FLAIR} ), as well as the molecular amide proton transfer-weighted ( \textit {APT}\text{w} ) sequence. We introduce a module that guides the synthesis based on a confidence measure of the intermediate results. Furthermore, we extend the proposed architecture to allow training using unpaired data. Extensive experiments on real clinical data demonstrate that the proposed model can perform better than current the state-of-The-Art synthesis methods. Our code is available at https://github.com/guopengf/CG-SAMR. © 1982-2012 IEEE.","Glioma; Humans; Magnetic Resonance Imaging; Amides; Diagnosis; Enhanced recovery; Magnetic resonance imaging; Patient treatment; amide; gadolinium; proton; Automatic approaches; Clinical diagnostics; Confidence Measure; Contour information; Fluid attenuated inversion recoveries; Intermediate results; Proposed architectures; Synthesis method; Article; contrast enhancement; controlled study; convolutional neural network; neuroanatomy; nuclear magnetic resonance imaging; synthesis; diagnostic imaging; glioma; human; Image enhancement","confidence guidance; Generative adversarial network; glioma; multi-modal MR image synthesis; segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098792747"
"Pan Y.; Huang J.; Wang B.; Zhao P.; Liu Y.; Xia Y.","Pan, Yongsheng (56440550200); Huang, Jingyu (57275369800); Wang, Bao (57197774672); Zhao, Peng (57212043475); Liu, Yingchao (25628398600); Xia, Yong (26427407400)","56440550200; 57275369800; 57197774672; 57212043475; 25628398600; 26427407400","Cerebral Blood Volume Prediction Based on Multi-modality Magnetic Resonance Imaging","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12965 LNCS","","","121","130","9","10.1007/978-3-030-87592-3_12","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115859423&doi=10.1007%2f978-3-030-87592-3_12&partnerID=40&md5=779beda9ef2a00b4cecd3457c8e16d65","Cerebral blood volume (CBV) refers to the blood volume of a certain brain tissue per unit time, which is the most useful parameter to evaluate intracranial mass lesions. However, the current CBV measurement methods rely on blood perfusion imaging technology which has obvious shortcomings, i.e., long imaging time, high cost, and great discomfort to the patients. To address this, we attempt to utilize some techniques to synthesize the CBV maps from multiple MRI sequences, which is the least harmful imaging technology currently, so as to reduce the time and cost of clinical diagnosis as well as the patients’ discomfort. Two image synthesis techniques are investigated to synthesize the CBV maps on our collection of 103 groups of multiple MRI modalities of 70 subjects. The experimental results on various modality combinations demonstrate that our redesigned algorithms are possible to synthesize promising CBV maps, which is a good start of developing efficient and cheaper CBV prediction system. © 2021, Springer Nature Switzerland AG.","Blood; Brain; Diagnosis; Hemodynamics; Magnetic resonance imaging; Medical imaging; Blood volumes; Brain tissue; Cerebral blood volume; Images synthesis; Imaging technology; Medical image synthesis; Multi-modality; Per unit; Prediction-based; Volume predictions; Generative adversarial networks","Cerebral blood volume; Generative adversarial network; Medical image synthesis","Conference paper","Final","","Scopus","2-s2.0-85115859423"
"Wei D.; Xu X.; Shen H.; Huang K.","Wei, Dongxu (57219750324); Xu, Xiaowei (55621149400); Shen, Haibin (13309317500); Huang, Kejie (54581052800)","57219750324; 55621149400; 13309317500; 54581052800","GAC-GAN: A General Method for Appearance-Controllable Human Video Motion Transfer","2021","IEEE Transactions on Multimedia","23","","9147027","2457","2470","13","10.1109/TMM.2020.3011290","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111638733&doi=10.1109%2fTMM.2020.3011290&partnerID=40&md5=195ed6851bd222aea59813d1dfffb4d7","Human video motion transfer has a wide range of applications in multimedia, computer vision, and graphics. Recently, due to the rapid development of Generative Adversarial Networks (GANs), there has been significant progress in the field. However, almost all existing GAN-based works are prone to address the mapping from human motions to video scenes, with scene appearances encoded individually in the trained models. Therefore, each trained model can only generate videos with a specific scene appearance, and new models are required to be trained to generate new appearances. Besides, existing works lack the capability of appearance control. For example, users have to provide video records of wearing new clothes or performing in new backgrounds to enable clothes or background changing in their synthetic videos, which greatly limits the application flexibility. In this paper, we propose General Appearance-Controllable GAN (GAC-GAN), a general method for appearance-controllable human video motion transfer. To enable general-purpose appearance synthesis, we propose to include appearance information in the conditioning inputs. Thus, once trained, our model can generate new appearances by altering the input appearance information. To achieve appearance control, we first obtain the appearance-controllable conditioning inputs, and then utilize a two-stage GAC-GAN to generate the corresponding appearance-controllable outputs, where we utilize an Appearance-Consistency GAN (ACGAN) loss, and a shadow extraction module for output foreground, and background appearance control respectively. We further build a solo dance dataset containing a large number of dance videos for training, and evaluation. Experimental results on our solo dance dataset, and iPER dataset show that our proposed GAC-GAN can not only support appearance-controllable human video motion transfer but also achieve higher video quality than state-of-art methods.  © 1999-2012 IEEE.","Clothes; Large dataset; Adversarial networks; Appearance synthesis; General method; Human motions; Shadow extractions; State-of-art methods; Video motion; Video quality; Arts computing","generative adversarial networks (GANs); image synthesis; Motion transfer; video generation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111638733"
"Katsumata K.; Nakayama H.","Katsumata, Kai (57221099626); Nakayama, Hideki (35230509500)","57221099626; 35230509500","Semantic image synthesis from inaccurate and coarse masks","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","2285","2289","4","10.1109/ICASSP39728.2021.9414521","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115071466&doi=10.1109%2fICASSP39728.2021.9414521&partnerID=40&md5=bda1a82ebaca85e83b2c6ad0586bb78c","Semantic image synthesis is an image-to-image translation problem where the goal is to learn mapping from semantic segmentation masks to corresponding photorealistic images. However, conventional semantic image synthesis methods require numerous pairs of correct semantic masks and real images, and collecting these pairs is not always possible. To address this issue, we propose a smoothing method, which we call local label smoothing (LLS), that incorporates label smoothing per small patch of an input mask to learn mapping from masks to images even when semantic masks are inaccurate. Furthermore, we also propose an extended method for coarse masks. We demonstrate the advantage of the proposed methods over existing methods to deal with noisy masks on several datasets. © 2021 IEEE","Mapping; Semantics; Image translation; Photorealistic images; Real images; Semantic images; Semantic segmentation; Small patches; Smoothing methods; Image segmentation","Computational imaging; Deep learning; Generative adversarial networks; Semantic image synthesis","Conference paper","Final","","Scopus","2-s2.0-85115071466"
"Luo Y.; Nie D.; Zhan B.; Li Z.; Wu X.; Zhou J.; Wang Y.; Shen D.","Luo, Yanmei (57223424973); Nie, Dong (57188806186); Zhan, Bo (57221803799); Li, Zhiang (57223440128); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Wang, Yan (56039981100); Shen, Dinggang (7401738392)","57223424973; 57188806186; 57221803799; 57223440128; 57221065403; 21234416400; 56039981100; 7401738392","Edge-preserving MRI image synthesis via adversarial network with iterative multi-scale fusion","2021","Neurocomputing","452","","","63","77","14","10.1016/j.neucom.2021.04.060","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105886086&doi=10.1016%2fj.neucom.2021.04.060&partnerID=40&md5=3bf0dc3ddd0c791de62a6b067f575e13","Magnetic resonance imaging (MRI) is a major imaging technique for studying neuroanatomy. By applying different pulse sequences and parameters, different modalities can be generated regarding the same anatomical structure, which can provide complementary information for diagnosis. However, limited by the scanning time and related cost, multiple different modalities are often not available for the same patient in clinic. Recently, many methods have been proposed for cross-modality MRI synthesis, but most of them only consider pixel-level differences between the synthetic and ground-truth images, ignoring the edge information, which is critical to provide clinical information. In this paper, we propose a novel edge-preserving MRI image synthesis method with iterative multi-scale feature fusion based generative adversarial network (EP_IMF-GAN). Particularly, the generator consists of a shared encoder and two specific decoders to carry out different tasks: 1) a primary task aiming to generate the target modality and 2) an auxiliary task aiming to generate the corresponding edge image of target modality. We assume that infusing the auxiliary edge image generation task can help preserve edge information and learn better latent representation features through the shared encoder. Meanwhile, an iterative multi-scale fusion module is embedded in the primary decoder to fuse supplementary information of feature maps at different scales, thereby further improving quality of the synthesized target modality. Experiments on the BRATS dataset indicate that our proposed method is superior to the state-of-the-art image synthesis approaches in both qualitative and quantitative measures. Ablation study further validates the effectiveness of the proposed components. © 2021 Elsevier B.V.","Decoding; Image fusion; Iterative methods; Signal encoding; Adversarial networks; Edge image; Edge information; Edge preserving; Generative adversarial network; Images synthesis; Iterative multi-scale fusion; Magnetic resonance imaging; Multiscale fusion; Pulse sequence; article; controlled study; human; nuclear magnetic resonance imaging; quantitative analysis; synthesis; Magnetic resonance imaging","Edge-preserving; Generative Adversarial Networks (GAN); Image synthesis; Iterative multi-scale fusion (IMF); Magnetic Resonance Imaging (MRI)","Article","Final","","Scopus","2-s2.0-85105886086"
"Ye J.; Xue Y.; Liu P.; Zaino R.; Cheng K.C.; Huang X.","Ye, Jiarong (57211998352); Xue, Yuan (57202313224); Liu, Peter (57286081800); Zaino, Richard (7005347157); Cheng, Keith C. (57201532636); Huang, Xiaolei (57218604182)","57211998352; 57202313224; 57286081800; 7005347157; 57201532636; 57218604182","A Multi-attribute Controllable Generative Model for Histopathology Image Synthesis","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12908 LNCS","","","613","623","10","10.1007/978-3-030-87237-3_59","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116448955&doi=10.1007%2f978-3-030-87237-3_59&partnerID=40&md5=8166dc524aeda3b37706b3fdb646a3ac","Generative models have been applied in the medical imaging domain for various image recognition and synthesis tasks. However, a more controllable and interpretable image synthesis model is still lacking yet necessary for important applications such as assisting in medical training. In this work, we leverage the efficient self-attention and contrastive learning modules and build upon state-of-the-art generative adversarial networks (GANs) to achieve an attribute-aware image synthesis model, termed AttributeGAN, which can generate high-quality histopathology images based on multi-attribute inputs. In comparison to existing single-attribute conditional generative models, our proposed model better reflects input attributes and enables smoother interpolation among attribute values. We conduct experiments on a histopathology dataset containing stained H&E images of urothelial carcinoma and demonstrate the effectiveness of our proposed model via comprehensive quantitative and qualitative comparisons with state-of-the-art models as well as different variants of our model. © 2021, Springer Nature Switzerland AG.","Image recognition; Learning systems; Medical imaging; Attribute-aware conditional generative model; Conditional contrastive learning; Generative model; Histopathology image synthesis; Images synthesis; Learning modules; Medical training; Multi-attributes; State of the art; Synthesis models; Generative adversarial networks","Attribute-aware conditional generative model; Conditional contrastive learning; Histopathology image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116448955"
"Wentao L.; Yuehu P.","Wentao, Liao (57226647864); Yuehu, Pu (57226650607)","57226647864; 57226650607","Dose-Conditioned Synthesis of Radiotherapy dose with Auxiliary Classifier Generative Adversarial Network(February 2020)","2021","IEEE Access","","","","","","","10.1109/ACCESS.2021.3089369","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112183902&doi=10.1109%2fACCESS.2021.3089369&partnerID=40&md5=8c39495f5dde1cb60095d58dba437de1","In recent years, the research on automatic planning based on artificial intelligence technology has been increasing. Focus on dose prediction of radiotherapy planning. Because of the small sample nature of radiotherapy planning data, it is difficult to obtain large-scale training data sets. In this paper, we propose a model of generating confrontation network (ACGAN) by using auxiliary classifier, and a method of customizing and synthesizing dose distribution images of specific tumor types and beam types is considered. This method can customize and generate dose distribution images of tumor types and beam types, which can be used for data enhancement of training data sets of dose prediction methods. The reliability and accuracy of the dose prediction model are improved effectively. CCBYNCND","Artificial intelligence; Forecasting; Predictive analytics; Radiotherapy; Tumors; Adversarial networks; Artificial intelligence technologies; Automatic planning; Dose distributions; Prediction methods; Prediction model; Radiotherapy planning; Training data sets; Image enhancement","Data mining; Generative adversarial networks; Image synthesis; Licenses; Radiotherapy IMRT dose distribution Autoplanning ACGAN Data enhancement; Task analysis; Tumors; Visualization","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85112183902"
"Jin Q.; Cui H.; Sun C.; Meng Z.; Su R.","Jin, Qiangguo (57206537651); Cui, Hui (55321437000); Sun, Changming (7404247926); Meng, Zhaopeng (7201894900); Su, Ran (57193653283)","57206537651; 55321437000; 7404247926; 7201894900; 57193653283","Free-form tumor synthesis in computed tomography images via richer generative adversarial network","2021","Knowledge-Based Systems","218","","106753","","","","10.1016/j.knosys.2021.106753","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101135578&doi=10.1016%2fj.knosys.2021.106753&partnerID=40&md5=59c89315a3ccf74a406b8c0abb875115","The insufficiency of annotated medical imaging scans for cancer makes it challenging to train and validate data-hungry deep learning models in precision oncology. We propose a new richer generative adversarial network for free-form 3D tumor/lesion synthesis in computed tomography (CT) images. The network is composed of a new richer convolutional feature enhanced dilated–gated generator (RicherDG) and a hybrid loss function. The RicherDG has dilated–gated convolution layers to enable tumor-painting and to enlarge perceptive fields; and it has a novel richer convolutional feature association branch to recover multi-scale convolutional features especially from uncertain boundaries between tumor and surrounding healthy tissues. The hybrid loss function, which consists of a diverse range of losses, is designed to aggregate complementary information to improve optimization. We perform a comprehensive evaluation of the synthesis results on a wide range of public CT image datasets covering the liver, kidney tumors, and lung nodules. The qualitative and quantitative evaluations and ablation study demonstrated improved synthesizing results over advanced tumor synthesis methods. © 2021 Elsevier B.V.","Convolution; Deep learning; Function evaluation; Medical imaging; Tumors; Adversarial networks; Comprehensive evaluation; Computed tomography images; Feature association; Learning models; Quantitative evaluation; Synthesis method; Uncertain boundaries; Computerized tomography","3D free-form synthesis; dilated–gated convolution; Generative adversarial network; Medical image synthesis; Richer convolutional feature","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101135578"
"Durall R.; Frolov S.; Hees J.; Raue F.; Pfreundt F.-J.; Dengel A.; Keuper J.","Durall, Ricard (57212194172); Frolov, Stanislav (57221158423); Hees, Jörn (35229784500); Raue, Federico (57141992100); Pfreundt, Franz-Josef (6507544948); Dengel, Andreas (6603764314); Keuper, Janis (57142459800)","57212194172; 57221158423; 35229784500; 57141992100; 6507544948; 6603764314; 57142459800","Combining Transformer Generators with Convolutional Discriminators","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12873 LNAI","","","67","79","12","10.1007/978-3-030-87626-5_6","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116905959&doi=10.1007%2f978-3-030-87626-5_6&partnerID=40&md5=132ebda50a27fac715f6ed4b46bd9da0","Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator. © 2021, Springer Nature Switzerland AG.","Convolution; Convolutional neural networks; Discriminators; Image enhancement; Network architecture; Architectural element; Attention mechanisms; Convolutional neural network; Data augmentation; Hybrid model; Images synthesis; Superresolution; Transformer; Transformer generators; Transformer modeling; Generative adversarial networks","Generative adversarial networks; Hybrid models; Image synthesis; Transformers","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116905959"
"He F.; Chen S.; Li S.; Zhou L.; Zhang H.; Peng H.; Huang X.","He, Fan (57203130280); Chen, Sizhe (57210341070); Li, Shuaiyi (57224195555); Zhou, Lu (57224205484); Zhang, Haiqin (57224205817); Peng, Haixia (57206259346); Huang, Xiaolin (7410247356)","57203130280; 57210341070; 57224195555; 57224205484; 57224205817; 57206259346; 7410247356","Colonoscopic image synthesis for polyp detector enhancement via gan and adversarial training","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9434050","1887","1891","4","10.1109/ISBI48211.2021.9434050","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107194132&doi=10.1109%2fISBI48211.2021.9434050&partnerID=40&md5=f2f7bbe8313feb4ab1ca6a6d929926c1","Computer-aided polyp detection system powered by deep neural networks has achieved high performance but also suffers from data insufficiency. To address this problem, recent researches focus on synthesizing new colonoscopic images by Generative Adversarial Network (GAN). However, the synthesized images follow the same distribution as that of the training dataset, which limits the performance of the detectors re-trained on it. Recent studies show that adversarial examples can expand the data distribution and thus adversarial training can effectively improve the robustness of deep neural networks. Inspired by these two factors, this paper proposes a data augmentation framework to directly produce false negative colonoscopic images via GAN and the adversarial attack. The synthesized polyps are natural and experiments on three popular detectors show that compared with using GAN alone, producing false negative images by the adversarial attack can further improve the performance of the re-trained detectors.  © 2021 IEEE.","Deep neural networks; Medical imaging; Neural networks; Adversarial networks; Colonoscopic images; Data augmentation; Data distribution; Polyp detection systems; Recent researches; Synthesized images; Training dataset; Image enhancement","Adversarial training; Colonoscopic images; Image synthesis; Polyp detection","Conference paper","Final","","Scopus","2-s2.0-85107194132"
"Lahiani A.; Klaman I.; Navab N.; Albarqouni S.; Klaiman E.","Lahiani, Amal (57201184583); Klaman, Irina (57208414270); Navab, Nassir (7003458998); Albarqouni, Shadi (55129204800); Klaiman, Eldad (57201186358)","57201184583; 57208414270; 7003458998; 55129204800; 57201186358","Seamless Virtual Whole Slide Image Synthesis and Validation Using Perceptual Embedding Consistency","2021","IEEE Journal of Biomedical and Health Informatics","25","2","9003176","403","411","8","10.1109/JBHI.2020.2975151","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095506639&doi=10.1109%2fJBHI.2020.2975151&partnerID=40&md5=eb91eb74da8185e8aba2709c8ab71b33","Stain virtualization is an application with growing interest in digital pathology allowing simulation of stained tissue images thus saving lab and tissue resources. Thanks to the success of Generative Adversarial Networks (GANs) and the progress of unsupervised learning, unsupervised style transfer GANs have been successfully used to generate realistic, clinically meaningful and interpretable images. The large size of high resolution Whole Slide Images (WSIs) presents an additional computational challenge. This makes tilewise processing necessary during training and inference of deep learning networks. Instance normalization has a substantial positive effect in style transfer GAN applications but with tilewise inference, it has the tendency to cause a tiling artifact in reconstructed WSIs. In this paper we propose a novel perceptual embedding consistency (PEC) loss forcing the network to learn color, contrast and brightness invariant features in the latent space and hence substantially reducing the aforementioned tiling artifact. Our approach results in more seamless reconstruction of the virtual WSIs. We validate our method quantitatively by comparing the virtually generated images to their corresponding consecutive real stained images. We compare our results to state-of-the-art unsupervised style transfer methods and to the measures obtained from consecutive real stained tissue slide images. We demonstrate our hypothesis about the effect of the PEC loss by comparing model robustness to color, contrast and brightness perturbations and visualizing bottleneck embeddings. We validate the robustness of the bottleneck feature maps by measuring their sensitivity to the different perturbations and using them in a tumor segmentation task. Additionally, we propose a preliminary validation of the virtual staining application by comparing interpretation of 2 pathologists on real and virtual tiles and inter-pathologist agreement.  © 2013 IEEE.","Humans; Image Processing, Computer-Assisted; Embeddings; Luminance; Tissue; Adversarial networks; Bottleneck features; Computational challenges; Digital pathologies; Invariant features; State of the art; Tumor segmentation; Whole slide images; article; artifact; brightness; deep learning; embedding; human; human tissue; pathologist; quantitative analysis; synthesis; validation process; image processing; Deep learning","Digital pathology; GANs; inter-pathologist agreement; style transfer; virtual staining; WSI","Article","Final","","Scopus","2-s2.0-85095506639"
"Le Moing G.; Vu T.-H.; Jain H.; Pérez P.; Cord M.","Le Moing, Guillaume (57221773745); Vu, Tuan-Hung (57208015669); Jain, Himalaya (57191428215); Pérez, Patrick (55431049800); Cord, Matthieu (6701549439)","57221773745; 57208015669; 57191428215; 55431049800; 6701549439","Semantic Palette: Guiding Scene Generation with Class Proportions","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","9338","9346","8","10.1109/CVPR46437.2021.00922","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123173835&doi=10.1109%2fCVPR46437.2021.00922&partnerID=40&md5=0a09dd07e386ba8fff72c0364448dd04","Despite the recent progress of generative adversarial networks (GANs) at synthesizing photo-realistic images, producing complex urban scenes remains a challenging problem. Previous works break down scene generation into two consecutive phases: unconditional semantic layout synthesis and image synthesis conditioned on layouts. In this work, we propose to condition layout generation as well for higher semantic control: given a vector of class proportions, we generate layouts with matching composition. To this end, we introduce a conditional framework with novel architecture designs and learning objectives, which effectively accommodates class proportions to guide the scene generation process. The proposed architecture also allows partial layout editing with interesting applications. Thanks to the semantic control, we can produce layouts close to the real distribution, helping enhance the whole scene generation process. On different metrics and urban scene benchmarks, our models outperform existing baselines. Moreover, we demonstrate the merit of our approach for data augmentation: semantic segmenters trained on real layout-image pairs along with additional ones generated by our approach outperform models only trained on real pairs. © 2021 IEEE.","Computer vision; Generative adversarial networks; Network architecture; Break down; Condition; Generation process; Images synthesis; Layout generations; Layout synthesis; Photorealistic images; Recent progress; Scene generation; Urban scenes; Semantics","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123173835"
"","","","4th IEEE International Conference on Knowledge Innovation and Invention 2021, ICKII 2021","2021","4th IEEE International Conference on Knowledge Innovation and Invention 2021, ICKII 2021","","","","","","239","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118976736&partnerID=40&md5=97ff481bd9d18124d1b950ea8eb10db4","The proceedings contain 48 papers. The topics discussed include: modularized architecture of attribute generative adversarial network for image synthesis; authentication system by using HOG face recognition technique and web-based for medical dispenser machine; time-series analysis of newspaper articles for automatic event detection using LDA; a distributed simulated annealing based decision tree (DSABDT) for cancer classification; air quality forecast and evaluation based on long short-term memory network and fuzzy algorithm; phase dependent power in a transmission line circuit; the research on the best mode of the negative-ion smoke-removal helmet and the negative-ion emitters equipped in a fire scene; and design and implementation of the inverse kinematics and monitoring module for six-axis crank arm platform.","","","Conference review","Final","","Scopus","2-s2.0-85118976736"
"Park H.Y.; Bae H.-J.; Hong G.-S.; Kim M.; Yun J.; Park S.; Chung W.J.; Kim N.","Park, Ho Young (57196405081); Bae, Hyun-Jin (57221812256); Hong, Gil-Sun (56651697300); Kim, Minjee (57203241386); Yun, JiHye (57204474501); Park, Sungwon (50861768900); Chung, Won Jung (36445365200); Kim, NamKug (16550058300)","57196405081; 57221812256; 56651697300; 57203241386; 57204474501; 50861768900; 36445365200; 16550058300","Realistic high-resolution body computed tomography image synthesis by using progressive growing generative adversarial network: Visual turing test","2021","JMIR Medical Informatics","9","3","e23328","","","","10.2196/23328","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103679929&doi=10.2196%2f23328&partnerID=40&md5=7297b4e3679c4c1d114ed6aed2b15ae8","Background: Generative adversarial network (GAN)–based synthetic images can be viable solutions to current supervised deep learning challenges. However, generating highly realistic images is a prerequisite for these approaches. Objective: The aim of this study was to investigate and validate the unsupervised synthesis of highly realistic body computed tomography (CT) images by using a progressive growing GAN (PGGAN) trained to learn the probability distribution of normal data. Methods: We trained the PGGAN by using 11,755 body CT scans. Ten radiologists (4 radiologists with <5 years of experience [Group I], 4 radiologists with 5-10 years of experience [Group II], and 2 radiologists with >10 years of experience [Group III]) evaluated the results in a binary approach by using an independent validation set of 300 images (150 real and 150 synthetic) to judge the authenticity of each image. Results: The mean accuracy of the 10 readers in the entire image set was higher than random guessing (1781/3000, 59.4% vs 1500/3000, 50.0%, respectively; P<.001). However, in terms of identifying synthetic images as fake, there was no significant difference in the specificity between the visual Turing test and random guessing (779/1500, 51.9% vs 750/1500, 50.0%, respectively; P=.29). The accuracy between the 3 reader groups with different experience levels was not significantly different (Group I, 696/1200, 58.0%; Group II, 726/1200, 60.5%; and Group III, 359/600, 59.8%; P=.36). Interreader agreements were poor (κ=0.11) for the entire image set. In subgroup analysis, the discrepancies between real and synthetic CT images occurred mainly in the thoracoabdominal junction and in the anatomical details. Conclusions: The GAN can synthesize highly realistic high-resolution body CT images that are indistinguishable from real images; however, it has limitations in generating body images of the thoracoabdominal junction and lacks accuracy in the anatomical details. © Ho Young Park, Hyun-Jin Bae, Gil-Sun Hong, Minjee Kim, JiHye Yun, Sungwon Park, Won Jung Chung, NamKug Kim.","","Computed tomography; Generative adversarial network; Synthetic body images; Unsupervised deep learning; Visual turing test","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85103679929"
"Shin Y.; Yang J.; Lee Y.H.","Shin, Yirang (57219512766); Yang, Jaemoon (35243768700); Lee, Young Han (43262256000)","57219512766; 35243768700; 43262256000","Deep generative adversarial networks: Applications in musculoskeletal imaging","2021","Radiology: Artificial Intelligence","3","3","e200157","","","","10.1148/ryai.2021200157","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104920938&doi=10.1148%2fryai.2021200157&partnerID=40&md5=bf2fddf25b244c82a1ce143db1387950","In recent years, deep learning techniques have been applied in musculoskeletal radiology to increase the diagnostic potential of acquired images. Generative adversarial networks (GANs), which are deep neural networks that can generate or transform images, have the potential to aid in faster imaging by generating images with a high level of realism across multiple contrast and modalities from existing imaging protocols. This review introduces the key architectures of GANs as well as their technical background and challenges. Key research trends are highlighted, including: (a) reconstruction of high-resolution MRI; (b) image synthesis with different modalities and contrasts; (c) image enhancement that efficiently preserves high-frequency information suitable for human interpretation; (d) pixel-level segmentation with annotation sharing between domains; and (e) applications to different musculoskeletal anatomies. In addition, an overview is provided of the key issues wherein clinical applicability is challenging to capture with conventional performance metrics and expert evaluation. When clinically validated, GANs have the potential to improve musculoskeletal imaging. © RSNA, 2021.","article; deep learning; deep neural network; human; human experiment; image enhancement; nuclear magnetic resonance imaging; performance indicator; radiology; synthesis","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104920938"
"Benitez-Garcia G.; Yanai K.","Benitez-Garcia, Gibran (55334733700); Yanai, Keiji (7103290726)","55334733700; 7103290726","Ketchup GAN: A New Dataset for Realistic Synthesis of Letters on Food","2021","MMArt-ACM 2021 - Proceedings of the 2021 International Joint Workshop on Multimedia Artworks Analysis and Attractiveness Computing in Multimedia 2021","","","","8","12","4","10.1145/3463946.3469241","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114553198&doi=10.1145%2f3463946.3469241&partnerID=40&md5=477cc3090827912f78518ec704d6741f","This paper introduces a new dataset for the realistic synthesis of letters on food. Specifically, the ""Ketchup GAN""dataset consists of real-world images of egg omelettes decorated with ketchup letters. Our dataset contains sufficient size and variety to train and evaluate deep learning-based generative models. In addition, we generate a synthetic ketchup-free set, which enables us to train paired-based generative adversarial networks (GAN). The ketchup GAN dataset comprises more than two thousand images of omelette dishes collected from Twitter. Automatically generated segmentation masks of egg and ketchup are also provided as part of the dataset. Thus, we can evaluate generative models based on segmentation inputs as well. With our dataset, two state-of-the-art GAN models (Pix2Pix and SPADE) are reviewed on photorealistic ketchup letter synthesis. We finally present an automatic application of omelette decoration with ketchup text input from users.  © 2021 ACM.","Computer programming; Computer science; Adversarial networks; Automatic application; Automatically generated; Generative model; Photo-realistic; Real-world image; Segmentation masks; Text input; Deep learning","food image dataset; food image segmentation; food image synthesis; letters on food","Conference paper","Final","","Scopus","2-s2.0-85114553198"
"Li R.; Bastiani M.; Auer D.; Wagner C.; Chen X.","Li, Ruizhe (57212515721); Bastiani, Matteo (55295726500); Auer, Dorothee (7004864007); Wagner, Christian (57217431173); Chen, Xin (57133952200)","57212515721; 55295726500; 7004864007; 57217431173; 57133952200","Image Augmentation Using a Task Guided Generative Adversarial Network for Age Estimation on Brain MRI","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12722 LNCS","","","350","360","10","10.1007/978-3-030-80432-9_27","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112140309&doi=10.1007%2f978-3-030-80432-9_27&partnerID=40&md5=48c1d20af1b8556c9e6dc857ea6eb577","Brain age estimation based on magnetic resonance imaging (MRI) is an active research area in early diagnosis of some neurodegenerative diseases (e.g. Alzheimer, Parkinson, Huntington, etc.) for elderly people or brain underdevelopment for the young group. Deep learning methods have achieved the state-of-the-art performance in many medical image analysis tasks, including brain age estimation. However, the performance and generalisability of the deep learning model are highly dependent on the quantity and quality of the training data set. Both collecting and annotating brain MRI data are extremely time-consuming. In this paper, to overcome the data scarcity problem, we propose a generative adversarial network (GAN) based image synthesis method. Different from the existing GAN-based methods, we integrate a task-guided branch (a regression model for age estimation) to the end of the generator in GAN. By adding a task-guided loss to the conventional GAN loss, the learned low-dimensional latent space and the synthesised images are more task-specific. It helps to boost the performance of the down-stream task by combining the synthesised images and real images for model training. The proposed method was evaluated on a public brain MRI data set for age estimation. Our proposed method outperformed (statistically significant) a deep convolutional neural network based regression model and the GAN-based image synthesis method without the task-guided branch. More importantly, it enables the identification of age-related brain regions in the image space. The code is available on GitHub (https://github.com/ruizhe-l/tgb-gan ). © 2021, Springer Nature Switzerland AG.","Brain; Convolutional neural networks; Deep learning; Deep neural networks; Diagnosis; Image analysis; Image understanding; Learning systems; Medical imaging; Neurodegenerative diseases; Regression analysis; Adversarial networks; Early diagnosis; Image synthesis; Learning methods; Low dimensional; Regression model; State-of-the-art performance; Training data sets; Magnetic resonance imaging","Brain age regression; Data augmentation; Generative adversarial network","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112140309"
"Gheshlaghi S.H.; Nok Enoch Kan C.; Ye D.H.","Gheshlaghi, Saba Heidari (57219544954); Nok Enoch Kan, Chi (57404893300); Ye, Dong Hye (36171321600)","57219544954; 57404893300; 36171321600","Breast Cancer Histopathological Image Classification with Adversarial Image Synthesis","2021","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","","3387","3390","3","10.1109/EMBC46164.2021.9630678","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122496433&doi=10.1109%2fEMBC46164.2021.9630678&partnerID=40&md5=2918b669e4fbe1e5f22d0f85e545c492","Data limitation is one of the major challenges in applying deep learning to medical images. Data augmentation is a critical step to train robust and accurate deep learning models for medical images. In this research, we increase the size of a small dataset by using an Auxiliary Classifier Generative Adversarial Network (ACGAN) which generates realistic images along with their class labels.We evaluate the effectiveness of our ACGAN augmentation method by performing breast cancer histopathological image classification with deep convolutional neural network (dCNN) classifiers trained on our enhanced dataset. For our classifier, we use a transfer learning approach where the convolutional features are extracted from a pertained model and subsequently fed into several extreme gradient boosting (XGBoost) classifiers. Our experimental results on Breast Cancer Histopathological (BreakHis) dataset show that ACGAN data augmentation, along with our XGBoost classifier increases the classification accuracy by 9.35% for binary classification (benign vs. malignant) and 8.88% for four-class tumor sub-type classification compared with standard transfer learning approach. © 2021 IEEE.","Breast; Breast Neoplasms; Female; Humans; Neural Networks, Computer; Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Diseases; Image classification; Image enhancement; Medical imaging; Transfer learning; Breast Cancer; Critical steps; Data augmentation; Data limitations; Histopathological images; Images classification; Images synthesis; Learning approach; Learning models; Transfer learning; breast; breast tumor; diagnostic imaging; female; human; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85122496433"
"Saxena S.; Lal K.; Joshi S.","Saxena, Suraj (57209284588); Lal, Kanhaiya (54397389900); Joshi, Sharad (56119223500)","57209284588; 54397389900; 56119223500","Retinal Vessel Segmentation Using Blending-Based Conditional Generative Adversarial Networks","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13052 LNCS","","","135","144","9","10.1007/978-3-030-89128-2_13","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119356588&doi=10.1007%2f978-3-030-89128-2_13&partnerID=40&md5=5e6c5772f8c29a5db1a18fe5fa11fa1d","With a critical need for faster and more accurate diagnosis in medical image analysis, artificial intelligence plays a critical role. Precise artery segmentation and faster diagnosis in retinal blood vessel segmentation can be beneficial for the early detection of acute diseases such as diabetic retinopathy and glaucoma. Recent advancements in deep learning have led to some exciting improvements in the field of medical image segmentation. However, one common problem faced by such methods is the limited availability of labelled data to train a suitable deep learning model. The publicly available dataset for retinal vessel segmentation contains less than 50 images. On the other hand, deep learning is a data-hungry process. We propose a method to generate synthetic images to augment the training needs of the deep learning model. Specifically, we propose a blending and enhancement-based strategy to learn a conditional generative adversarial model. The network synthesizes high-quality fundus images used along with the real images to learn a convolutional neural network-based segmentation model. Experimental evaluation shows that the proposed synthetic generation method improves segmentation performance on the real test images of the vascular extraction (DRIVE) dataset achieving 97.01% segmentation accuracy. © 2021, Springer Nature Switzerland AG.","Blood vessels; Convolution; Convolutional neural networks; Deep learning; Diagnosis; Digital storage; Eye protection; Image enhancement; Image segmentation; Medical imaging; Ophthalmology; Statistical tests; Artery segmentations; Blood-vessel segmentations; Convolutional neural network; Images synthesis; Learn+; Learning models; Medical image analysis; Medical image segmentation; Retinal blood vessels; Retinal vessel segmentations; Generative adversarial networks","Convolutional neural network; Generative adversarial networks; Image synthesis; Medical image segmentation; Retinal vessel segmentation","Conference paper","Final","","Scopus","2-s2.0-85119356588"
"Xia W.; Yang Y.; Xue J.-H.","Xia, Weihao (57202788795); Yang, Yujiu (35729585000); Xue, Jing-Hao (7202881908)","57202788795; 35729585000; 7202881908","Cali-sketch: Stroke calibration and completion for high-quality face image generation from human-like sketches","2021","Neurocomputing","460","","","256","265","9","10.1016/j.neucom.2021.07.029","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106369705&doi=10.1016%2fj.neucom.2021.07.029&partnerID=40&md5=7a7ab53107cf25b7d6fb880441d7e099","Image generation has received increasing attention because of its wide application in security and entertainment. Sketch-based face generation brings more fun and better quality of image generation due to supervised interaction. However, when a sketch poorly aligned with the true face is given as input, existing supervised image-to-image translation methods often cannot generate acceptable photo-realistic face images. To address this problem, in this paper we propose Cali-Sketch, a human-like-sketch to photo-realistic-image generation method. Cali-Sketch explicitly models stroke calibration and image generation using two constituent networks: a Stroke Calibration Network (SCN), which calibrates strokes of facial features and enriches facial details while preserving the original intent features; and an Image Synthesis Network (ISN), which translates the calibrated and enriched sketches to photo-realistic face images. In this way, we manage to decouple a difficult cross-domain translation problem into two easier steps. Extensive experiments verify that the face photos generated by Cali-Sketch are both photo-realistic and faithful to the input sketches, compared with state-of-the-art methods. © 2021 Elsevier B.V.","Computer vision; Neural networks; Face generation; Face images; Face sketch-to-photo synthesis; Generative adversarial network; High quality; Human like; Image generations; Image translation; Neural-networks; Photo-realistic; Article; calibration; drawing; facies; image analysis; image quality; Image Synthesis Network; nerve cell network; painting; photography; Stroke Calibration Network; Calibration","Face sketch-to-photo synthesis; Generative adversarial network; Image translation; Neural network","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85106369705"
"Zhang Z.; Yu W.; Jiang N.; Zhou J.","Zhang, Zhiqiang (57206280843); Yu, Wenxin (36610960300); Jiang, Ning (57212426361); Zhou, Jinjia (35099640400)","57206280843; 36610960300; 57212426361; 35099640400","TEXT TO IMAGE SYNTHESIS WITH ERUDITE GENERATIVE ADVERSARIAL NETWORKS","2021","Proceedings - International Conference on Image Processing, ICIP","2021-September","","","2438","2442","4","10.1109/ICIP42928.2021.9506487","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125566676&doi=10.1109%2fICIP42928.2021.9506487&partnerID=40&md5=344dd0b93bc9e063f862cc7982928e0f","In this paper, an Erudite Generative Adversarial Networks (EruditeGAN) is proposed for the text-to-image synthesis task. By introducing additional image distribution related to the original image into the network structure, the entire network can learn more about the image distribution and become more knowledgeable. In this case, it can be more clear about the distribution of the image that needs to be synthesized and finally synthesize high-quality results. Experiments well validate our method’s effectiveness and demonstrate the different effects of different distribution situations on the final results. According to the quantitative results of Fréchet Inception Distance (FID) and R-precision, our method’s comprehensive score is the best, which reflects our results are closer to the real image effect. © 2021 IEEE","Computer vision; Deep learning; Deep learning; Different effects; High quality; Image distributions; Images synthesis; Learn+; Network structures; Original images; Synthesised; Text-to-image synthesis; Generative adversarial networks","Computer vision; Deep learning; Generative adversarial networks; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85125566676"
"Zhang M.; Li C.; Zhou Z.","Zhang, Min (57210605870); Li, Chunye (57205083887); Zhou, Zhiping (55574984200)","57210605870; 57205083887; 55574984200","Text to image synthesis using multi-generator text conditioned generative adversarial networks","2021","Multimedia Tools and Applications","80","5","","7789","7803","14","10.1007/s11042-020-09965-5","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094650406&doi=10.1007%2fs11042-020-09965-5&partnerID=40&md5=f0d2699ef01291bba3aa965b95474f5a","Recently, Generative Adversarial Network(GAN) has been the most mainstream technology in the task of Text to Image. However, the vanilla deep neural networks tend to approximate continuous mappings in real generation tasks rather than discontinuous mappings with discrete points. When training on datasets with multiple types, GAN fails to synthesize diverse images, which we call as mode collapse. To deal with it, we propose the Multi-generator Text Conditioned Generative Adversarial Network (MTC-GAN) in this paper. Textual description of real images is embedded on the noise vector as a constraint. Based on Deep Convolutional Generative Adversarial Networks(DCGAN), multiple generators are incorporated to capture high probability among the target distribution. To identify the generated fake sample from a particular generator, the discriminator must enforce multiple generators to have different identifiable modes. The method based on global constraints can make the generated images more diverse. Multiple generators can improve the particular functional shape of the discriminators indirectly, which should make the GAN more stable when trained in high dimensional spaces. The experimental results on the standard dataset demonstrate the good performance of the proposed method. The problem of mode collapse can be improved, and the generated samples can be more diverse. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep neural networks; Mapping; Probability distributions; Adversarial networks; Continuous mappings; Discontinuous mapping; Global constraints; High dimensional spaces; High probability; Image synthesis; Textual description; Image processing","Generative adversarial networks; Mode collapse; Multiple generators; Text description; Text to image","Article","Final","","Scopus","2-s2.0-85094650406"
"Li Y.; Li Y.; Lu J.; Shechtman E.; Lee Y.J.; Singh K.K.","Li, Yuheng (57402017400); Li, Yijun (57191433923); Lu, Jingwan (36022690300); Shechtman, Eli (55924548800); Lee, Yong Jae (50561495900); Singh, Krishna Kumar (57213994696)","57402017400; 57191433923; 36022690300; 55924548800; 50561495900; 57213994696","Collaging Class-specific GANs for Semantic Image Synthesis","2021","Proceedings of the IEEE International Conference on Computer Vision","","","","14398","14407","9","10.1109/ICCV48922.2021.01415","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127241406&doi=10.1109%2fICCV48922.2021.01415&partnerID=40&md5=3842a282f495add77a9b5f60ed443534","We propose a new approach for high resolution semantic image synthesis. It consists of one base image generator and multiple class-specific generators. The base generator generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has several benefits including - dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibility of object-level control by using class-specific generators. © 2021 IEEE","Computer vision; Semantic Segmentation; Semantics; Base images; High quality images; High resolution; Image generators; Image-based; Images synthesis; Multiple class; New approaches; Segmentation map; Semantic images; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127241406"
"Zhang S.; Ni J.; Hou L.; Zhou Z.; Hou J.; Gao F.","Zhang, Susu (57215657489); Ni, Jiancheng (14831535200); Hou, Lijun (57223213956); Zhou, Zili (57215660890); Hou, Jie (57705815000); Gao, Feng (57215650829)","57215657489; 14831535200; 57223213956; 57215660890; 57705815000; 57215650829","Global-Affine And Local-Specific Generative Adversarial Network For Semantic-Guided Image Generation","2021","Mathematical Foundations of Computing","4","3","","145","165","20","10.3934/mfc.2021009","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120946808&doi=10.3934%2fmfc.2021009&partnerID=40&md5=eeef0de57d353d665d2498c7f7decddf","The recent progress in learning image feature representations has opened the way for tasks such as label-to-image or text-to-image synthesis. However, one particular challenge widely observed in existing methods is the difficulty of synthesizing fine-grained textures and small-scale instances. In this paper, we propose a novel Global-Affine and Local-Specific Generative Adversarial Network (GALS-GAN) to explicitly construct global semantic lay-outs and learn distinct instance-level features. To achieve this, we adopt the graph convolutional network to calculate the instance locations and spatial relationships from scene graphs, which allows our model to obtain the high-fidelity semantic layouts. Also, a local-specific generator, where we introduce the feature filtering mechanism to separately learn semantic maps for different categories, is utilized to disentangle and generate specific visual features. Moreover, we especially apply a weight map predictor to better combine the global and local pathways considering the highly complementary between these two generation sub-networks. Extensive experiments on the COCO-Stuff and Visual Genome datasets demonstrate the superior generation performance of our model against previous methods, our approach is more capable of capturing photo-realistic local characteristics and rendering small-sized entities with more details © American Institute of Mathematical Sciences","Convolution; Image processing; Semantic Web; Semantics; Convolutional networks; Feature filtering; Feature filtering mechanism; Filtering mechanism; Graph convolutional network; Layout construction; Learn+; Semantic layout construction; Weight map predictor; Weight maps; Generative adversarial networks","feature filtering mechanism; generative adversarial network; graph convolutional network; semantic layout construction; weight map predictor","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85120946808"
"Zhou Z.; Guo H.; Guo Y.; Sheng H.","Zhou, Ziheng (57237378400); Guo, Hong (57056737900); Guo, Yuwen (57223434330); Sheng, Huanhuan (57205114595)","57237378400; 57056737900; 57223434330; 57205114595","Synthesis and Segmentation Method of Cross-Staining Style Nuclei Pathology Image Based on Adversarial Learning","2021","19th IEEE International Symposium on Parallel and Distributed Processing with Applications, 11th IEEE International Conference on Big Data and Cloud Computing, 14th IEEE International Conference on Social Computing and Networking and 11th IEEE International Conference on Sustainable Computing and Communications, ISPA/BDCloud/SocialCom/SustainCom 2021","","","","522","532","10","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00078","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124164915&doi=10.1109%2fISPA-BDCloud-SocialCom-SustainCom52081.2021.00078&partnerID=40&md5=db51d7e85524c3b4189e9b419fd33fd3","In modern clinical diagnosis, nuclei is an important basis for obtaining disease information, and rapid and accurate nuclei segmentation technology can effectively help doctors diagnose diseases. However, the difference in staining equipment or organs will cause staining differences between histopathological images, which will affect the effect of nuclei segmentation. Aiming at the problem of staining differences, we propose a cross-staining style nuclei pathology image synthesis and segmentation method based on adversarial learning. The algorithm is composed of a synthetic network and a segmentation network. The synthetic network adds a content discriminator to the cycle generative adversarial networks. Used to generate an image with the same staining style as the target domain, but the same content. The segmentation network segmented the generated image to verify the necessity of staining normalization and the performance of synthesis. In addition, the synthetic network also uses content loss function and style loss function to optimize network training. Experiments show that we propose a cross-staining style nuclei pathology image synthesis and segmentation method based on adversarial learning can effectively solve the problem of staining differences and optimize the effect of segmentation network. © 2021 IEEE.","Diagnosis; Image segmentation; Pathology; Adversarial learning; Adversarial networks; Generating adversarial network; Image; Images synthesis; Normalisation; Nucleus segmentation; Segmentation methods; Staining normalization histopathology; Synthesis method; Generative adversarial networks","Generating adversarial network; Image; Image synthesis; Nuclei segmentation; Staining normalization Histopathology","Conference paper","Final","","Scopus","2-s2.0-85124164915"
"Jiang S.; Liu H.; Wu Y.; Fu Y.","Jiang, Songyao (57195216444); Liu, Hongfu (54389423300); Wu, Yue (57209589888); Fu, Yun (7404432812)","57195216444; 54389423300; 57209589888; 7404432812","Spatially Constrained GAN for Face and Fashion Synthesis","2021","Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021","","","","","","","10.1109/FG52635.2021.9666991","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125098859&doi=10.1109%2fFG52635.2021.9666991&partnerID=40&md5=8107b571713481ce3a99b796a640db5f","Image synthesis has raised tremendous attention in both academic and industrial areas, especially for conditional and target-oriented image synthesis, such as criminal portrait and fashion design. The current studies have achieved encouraging results along this direction, but they mostly focus on class labels where spatial contents are randomly generated from latent vectors. Some recent studies have explored spatial constraints for generative models guided by semantic segmentation, but most of them are designed for scene generation and lack random variation. Such methods are not suitable for face or fashion image synthesis, where different images may share the same semantics. Different from all the current methods, we decouple the image synthesis task into three independent dimensions and propose a novel Spatially Constrained Generative Adversarial Network (SCGAN) to model it. SCGAN uses a simple yet effective way to decouple spatial constraints and attribute conditions from latent vectors, and treat them as additional controllable signals via a segmentor and a specially designed generator. Other unregulated contents are left to be generated from latent vectors. Experimentally, we provide both qualitative and quantitative results on CelebA and DeepFashion datasets to demonstrate that the proposed SCGAN is very effective in synthesizing spatially controllable and attribute-specific images with high visual quality and large variations. Our code is provided at https://github.com/jackyjsy/SCGAN. © 2021 IEEE.","Computer vision; Large dataset; Semantic Segmentation; Semantics; 'current; Class labels; Fashion design; Generative model; Images synthesis; Industrial area; Latent vectors; Semantic segmentation; Spatial constraints; Target oriented; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125098859"
"Gan Y.; Ye M.; Liu D.; Yang S.; Xiang T.","Gan, Yan (57203150814); Ye, Mao (35241431500); Liu, Dan (14016130600); Yang, Shangming (8505735800); Xiang, Tao (57213003210)","57203150814; 35241431500; 14016130600; 8505735800; 57213003210","A novel hybrid augmented loss discriminator for text-to-image synthesis","2021","International Journal of Intelligent Systems","36","2","","1085","1107","22","10.1002/int.22333","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096714620&doi=10.1002%2fint.22333&partnerID=40&md5=2f98856065809ebff5fcb30108d40cdb","For the text-to-image synthesis task, most discriminators in existing generative adversarial networks based methods tend to fall into a local suboptimal state too early in the training process, resulting in the poor quality of generated images. To address the above problems, a hybrid augmented loss discriminator is designed. In this designed discriminator, to reduce the sensitivity of the discriminator classification recognition, make it pay attention to the semantic and structural changes, we add the loss value of the fake sample to the loss value of the real sample. Moreover, to indirectly guide the generator to generate samples, the loss value of the real sample is added to the fake sample. The loss value mixed with real and fake samples actually augments signal transmission. It perturbs parameter update of the discriminator during optimization and prevents the discriminator from falling into the local suboptimal state prematurely. Whereafter, we apply the proposed discriminator to two kinds of text-to-image synthesis tasks. Experimental results show that the proposed method can help the baseline models to improve performance. © 2020 Wiley Periodicals LLC","Discriminators; Semantics; Adversarial networks; Baseline models; Image synthesis; Improve performance; Real samples; Signal transmission; Training process; Image processing","adversarial learning; hybrid augmented loss; perturbation; text-to-image synthesis","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85096714620"
"Zhou Q.; Liu Y.; Hu H.; Guan Q.; Guo Y.; Zhang F.","Zhou, Qianwei (55330189900); Liu, Yibo (57465342800); Hu, Haigen (30267591900); Guan, Qiu (55671678500); Guo, Yuan (56137584100); Zhang, Fan (57208893083)","55330189900; 57465342800; 30267591900; 55671678500; 56137584100; 57208893083","Unsupervised Multimodal MR Images Synthesizer Using Knowledge from Higher Dimension","2021","Proceedings - 2021 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021","","","","1633","1636","3","10.1109/BIBM52615.2021.9669327","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125184738&doi=10.1109%2fBIBM52615.2021.9669327&partnerID=40&md5=d77c973ae83567ab2d85797b84be3308","Magnetic Resonance Images (MRIs) of different modalities have different reference values for pathological diagnosis. But it is difficult to obtain multimodality MRIs. So, medical image synthesis has been proposed as an effective solution, with which any missing modalities are synthesized from the existing ones. To train a multimodal MRI synthesizer with limited number of unpaired MRIs, in this paper, we have proposed a novel High-dimensional Knowledge Guided Generative Adversarial Network (HKG-GAN). In our HKG-GAN, a cross-dimensional knowledge transfer network is utilized to extract features from 2D images (slices of MRIs) to measure the perceptual similarity of images of source and synthesized modalities, whose knowledge is transferred from a pre-trained 3D network without accessing its private training dataset. Nevertheless, based on code-splitting and cross-decoding, HKG-GAN is a one-for-all network that encodes MRIs into content codes and style codes, and then cross-decodes the encoding of a random image of different modality to convert MRI to target modality. The effectiveness has been proofed through comparative experiments.  © 2021 IEEE.","Decoding; Deep learning; Diagnosis; Encoding (symbols); Generative adversarial networks; Knowledge management; Magnetic resonance; Medical imaging; Network coding; Deep learning; High-dimensional; Higher dimensions; Higher-dimensional; Image codecs; Knowledge transfer; MR-images; Multi-modal; Reference values; Synthesised; Magnetic resonance imaging","Deep Learning; Generative Adversarial Network; Image Codec; Knowledge Transfer; Magnetic Resonance Images","Conference paper","Final","","Scopus","2-s2.0-85125184738"
"Wang Z.","Wang, Zi (57191986725)","57191986725","Learning fast converging, effective conditional generative adversarial networks with a mirrored auxiliary classifier","2021","Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021","","","","2565","2574","9","10.1109/WACV48630.2021.00261","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116093772&doi=10.1109%2fWACV48630.2021.00261&partnerID=40&md5=f9d994d7772e9b212966fe41e3475f37","Training conditional generative adversarial networks (GANs) has been remaining as a challenging task, though standard GANs have developed substantially and gained huge successes in recent years. In this paper, we propose a novel conditional GAN architecture with a mirrored auxiliary classifier (MAC-GAN) in its discriminator for the purpose of label conditioning. Unlike existing works, our mirrored auxiliary classifier contains both a real and a fake node for each specific class to distinguish real samples from generated samples that are assigned into the same category by previous models. Comparing with previous auxiliary classifier-based conditional GANs, our MAC-GAN learns a fast converging model for high-quality image generation, taking benefits from its robust, newly designed auxiliary classifier. Experiments on multiple benchmark datasets illustrate that our proposed model improves the quality of image synthesis compared with state-of-the-art approaches. Moreover, much better classification performance can be achieved with the mirrored auxiliary classifier, which can in turn promote the use of MAC-GAN in various transfer learning tasks. © 2021 IEEE.","Benchmarking; Computer vision; Image enhancement; Benchmark datasets; Classification performance; Fake node; High quality images; Image generations; Images synthesis; Learn+; Real samples; Specific class; State-of-the-art approach; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85116093772"
"Imtiaz S.; Taj I.A.; Nawaz R.","Imtiaz, Shehryar (57539532100); Taj, Imtiaz Ahmad (36978464900); Nawaz, Rab (57848488200)","57539532100; 36978464900; 57848488200","Visible to Thermal Image Synthesis using Light Weight Pyramid Network","2021","ICET 2021 - 16th International Conference on Emerging Technologies 2021, Proceedings","","","","","","","10.1109/ICET54505.2021.9689909","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126800880&doi=10.1109%2fICET54505.2021.9689909&partnerID=40&md5=e6109636b0fd401c39f6c34d98a7d0d1","We investigate Lightweight Pyramid Network as a general-purpose solution for image-to-image synthesis. Already employed techniques that are based upon deep convolutional neural networks (CNNs) have found reasonable success but with the trade-off of a large number of parameters that eventually result in high computational costs. In these methods, post processing of various types is incorporated as well to further refine the transformed image, thus making the whole process cumbersome and time taking. In this paper, we have made use of lightweight pyramid network (LPNet) for image synthesis that was primarily used for image deraining. We find that by using Laplacian-Gaussian image pyramid decomposition coupled with reconstruction and calculation of SSIM along with neural network, the heat signature in the resulting synthesized thermal image becomes much more enhanced and at the same time contours of various image objects stays prominent even without the use of any post processing techniques. The computations for training become less intensive due to the use of a shallow network. We further prove the efficacy of our approach by doing SSIM, PSNR and UQI Quantitative Analysis.  © 2021 IEEE.","Computer vision; Convolution; Convolutional neural networks; Deep neural networks; Economic and social effects; Image enhancement; CRN; CVC-09; CVC-14 lightweight network; Deep convolutional neural network; KAIST; Lightweight pyramid network; Mobilenet; Post-processing; Pyramid; Pyramid network; Residual learning; Resnet; Stylegan2; Thermalnet; Generative adversarial networks","CRN; CVC-09; CVC-14 lightweight networks; Deep convolutional neural network (CNN); Generative Adversarial Networks; KAIST; LPNet; MobileNet; Post Processing; pyramid; Residual Learning; ResNet; StyleGAN2; ThermalNet","Conference paper","Final","","Scopus","2-s2.0-85126800880"
"Wang Z.; Lim G.; Ng W.Y.; Keane P.A.; Campbell J.P.; Tan G.S.W.; Schmetterer L.; Wong T.Y.; Liu Y.; Ting D.S.W.","Wang, Zhaoran (57999723000); Lim, Gilbert (57188817284); Ng, Wei Yan (57701960000); Keane, Pearse A. (24480921100); Campbell, J. Peter (56544916600); Tan, Gavin Siew Wei (34972108600); Schmetterer, Leopold (7103327518); Wong, Tien Yin (57226219251); Liu, Yong (55187331400); Ting, Daniel Shu Wei (37010354600)","57999723000; 57188817284; 57701960000; 24480921100; 56544916600; 34972108600; 7103327518; 57226219251; 55187331400; 37010354600","Generative adversarial networks in ophthalmology: What are these and how can they be used?","2021","Current Opinion in Ophthalmology","32","5","","459","467","8","10.1097/ICU.0000000000000794","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114521922&doi=10.1097%2fICU.0000000000000794&partnerID=40&md5=295f472720798188eac5d19c992ee533","Purpose of reviewThe development of deep learning (DL) systems requires a large amount of data, which may be limited by costs, protection of patient information and low prevalence of some conditions. Recent developments in artificial intelligence techniques have provided an innovative alternative to this challenge via the synthesis of biomedical images within a DL framework known as generative adversarial networks (GANs). This paper aims to introduce how GANs can be deployed for image synthesis in ophthalmology and to discuss the potential applications of GANs-produced images.Recent findingsImage synthesis is the most relevant function of GANs to the medical field, and it has been widely used for generating 'new' medical images of various modalities. In ophthalmology, GANs have mainly been utilized for augmenting classification and predictive tasks, by synthesizing fundus images and optical coherence tomography images with and without pathologies such as age-related macular degeneration and diabetic retinopathy. Despite their ability to generate high-resolution images, the development of GANs remains data intensive, and there is a lack of consensus on how best to evaluate the outputs produced by GANs.SummaryAlthough the problem of artificial biomedical data generation is of great interest, image synthesis by GANs represents an innovation with yet unclear relevance for ophthalmology. © 2021 Lippincott Williams and Wilkins. All rights reserved.","Artificial Intelligence; Deep Learning; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Ophthalmology; age related macular degeneration; deep learning; diabetic retinopathy; gene network analysis; generative adversarial network; human; image analysis; ophthalmology; optical coherence tomography; outcome assessment; Review; artificial intelligence; image processing; procedures","artificial intelligence; deep learning; generative adversarial networks; medical image synthesis; ophthalmology","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85114521922"
"Yi D.; Guo C.; Bai T.","Yi, Da (57280767000); Guo, Chao (56438465700); Bai, Tianxiang (57217610359)","57280767000; 56438465700; 57217610359","Exploring painting synthesis with diffusion models","2021","Proceedings 2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence, DTPI 2021","","","","332","335","3","10.1109/DTPI52967.2021.9540115","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116120795&doi=10.1109%2fDTPI52967.2021.9540115&partnerID=40&md5=24f979b205997f7fcc99f8793eda2cbf","As a significant composition of art, fine art painting is becoming a research hotspot in machine learning community. With unique aesthetic value, paintings have quite different representations from natural images, making them irreplaceable. Meanwhile, the lack of training data is common in painting-related machine learning tasks. Therefore, the synthesis of fine art painting is meaningful and challenging work. There are two main types of generative models for image synthesis: generative adversarial networks (GANs) and likelihood-based models. GAN-based models can obtain high-quality samples but usually sacrifice diversity and training stability. Diffusion models are a class of likelihood-based models and have recently been shown to achieve state-of-the-art quality on the image synthesis tasks. In this paper, we explore generating fine art paintings by using diffusion models. We carried out the experiments on the partial impression paintings from the Wikiart dataset. The results demonstrate that the diffusion model can generate high-quality samples, and it is easy to train to cover more target distribution than the GAN-based methods. © 2021 IEEE.","Computer vision; Generative adversarial networks; Machine learning; Painting; Aesthetic value; Art paintings; Diffusion model; Fine arts; High quality; Hotspots; Image generations; Images synthesis; Machine learning communities; Painting synthesis; Diffusion","Diffusion models; Image generation; Painting synthesis","Conference paper","Final","","Scopus","2-s2.0-85116120795"
"Marzullo A.; Moccia S.; Catellani M.; Calimeri F.; Momi E.D.","Marzullo, Aldo (57194212378); Moccia, Sara (57192919602); Catellani, Michele (57193092265); Calimeri, Francesco (8685471300); Momi, Elena De (57203951877)","57194212378; 57192919602; 57193092265; 8685471300; 57203951877","Towards realistic laparoscopic image generation using image-domain translation","2021","Computer Methods and Programs in Biomedicine","200","","105834","","","","10.1016/j.cmpb.2020.105834","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096562492&doi=10.1016%2fj.cmpb.2020.105834&partnerID=40&md5=31441f3aae282257dcba517106d7f15a","Background and ObjectivesOver the last decade, Deep Learning (DL) has revolutionized data analysis in many areas, including medical imaging. However, there is a bottleneck in the advancement of DL in the surgery field, which can be seen in a shortage of large-scale data, which in turn may be attributed to the lack of a structured and standardized methodology for storing and analyzing surgical images in clinical centres. Furthermore, accurate annotations manually added are expensive and time consuming. A great help can come from the synthesis of artificial images; in this context, in the latest years, the use of Generative Adversarial Neural Networks (GANs) achieved promising results in obtaining photo-realistic images. MethodsIn this study, a method for Minimally Invasive Surgery (MIS) image synthesis is proposed. To this aim, the generative adversarial network pix2pix is trained to generate paired annotated MIS images by transforming rough segmentation of surgical instruments and tissues into realistic images. An additional regularization term was added to the original optimization problem, in order to enhance realism of surgical tools with respect to the background. Results Quantitative and qualitative (i.e., human-based) evaluations of generated images have been carried out in order to assess the effectiveness of the method. ConclusionsExperimental results show that the proposed method is actually able to translate MIS segmentations to realistic MIS images, which can in turn be used to augment existing data sets and help at overcoming the lack of useful images; this allows physicians and algorithms to take advantage from new annotated instances for their training. © 2020","Algorithms; Humans; Image Processing, Computer-Assisted; Laparoscopy; Neural Networks, Computer; Deep learning; Image segmentation; Surgical equipment; Transplantation (surgical); Adversarial networks; Image generations; Minimally invasive surgery; Optimization problems; Photorealistic images; Regularization terms; Rough segmentation; Surgical instrument; accuracy; adversarial neural network; Article; diagnostic imaging; embedding; endoscopy; human; human tissue; image quality; image segmentation; laparoscopy; machine learning; minimally invasive surgery; recall; surgeon; algorithm; image processing; Medical imaging","Data Augmentation; Generative Adversarial Networks; Image translation; Minimally Invasive Surgery","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85096562492"
"Qiao Y.; Chen Q.; Deng C.; DIng N.; Qi Y.; Tan M.; Ren X.; Wu Q.","Qiao, Yanyuan (57204979716); Chen, Qi (57717769500); Deng, Chaorui (57205539704); DIng, Ning (57616760800); Qi, Yuankai (55977742700); Tan, Mingkui (22837202600); Ren, Xincheng (55820448200); Wu, Qi (57188639124)","57204979716; 57717769500; 57205539704; 57616760800; 55977742700; 22837202600; 55820448200; 57188639124","R-GAN: Exploring Human-like Way for Reasonable Text-to-Image Synthesis via Generative Adversarial Networks","2021","MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia","","","","2085","2093","8","10.1145/3474085.3475363","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119379351&doi=10.1145%2f3474085.3475363&partnerID=40&md5=0b6c456c7dd773ad3b2253f84248c1d4","Despite recent significant progress on generative models, context-rich text-to-image synthesis depicting multiple complex objects is still non-trivial. The main challenges lie in the ambiguous semantic of a complex description and the intricate scene of an image with various objects, different positional relationship and diverse appearances. To address these challenges, we propose R-GAN, which can generate reasonable images according to the given text in a human-like way. Specifically, just like humans will first find and settle the essential elements to create a simple sketch, we first capture a monolithic-structural text representation by building a scene graph to find the essential semantic elements. Then, based on this representation, we design a bounding box generator to estimate the layout with position and size of target objects, and a following shape generator, which draws a fine-detailed shape for each object. Different from previous work only generating coarse shapes blindly, we introduce a coarse-to-fine shape generator based on a shape knowledge base. At last, to finish the final image synthesis, we propose a multi-modal geometry-aware spatially-adaptive generator conditioned on the monolithic-structural text representation and the geometry-aware map of the shapes. Extensive experiments on the real-world dataset MSCOCO show the superiority of our method in terms of both quantitative and qualitative metrics. © 2021 ACM.","Complex networks; Image processing; Knowledge based systems; Semantics; Complex objects; Generative model; Human like; Images synthesis; Model contexts; Monolithics; Non-trivial; Rich texts; Text representation; Text-to-image synthesis; Generative adversarial networks","generative adversarial networks; text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85119379351"
"Jeon E.; Kim K.; Kim D.","Jeon, Eunyeong (57271845600); Kim, Kunhee (57244263400); Kim, Daijin (24597347100)","57271845600; 57244263400; 24597347100","FA-GAN: FEATURE-AWARE GAN FOR TEXT TO IMAGE SYNTHESIS","2021","Proceedings - International Conference on Image Processing, ICIP","2021-September","","","2443","2447","4","10.1109/ICIP42928.2021.9506172","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125587343&doi=10.1109%2fICIP42928.2021.9506172&partnerID=40&md5=b1f6faa7538f882bd520030d3405b9c2","Text-to-image synthesis aims to generate a photo-realistic image from a given natural language description. Previous works have made significant progress with Generative Adversarial Networks (GANs). Nonetheless, it is still hard to generate intact objects or clear textures (Fig 1). To address this issue, we propose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a high-quality image by integrating two techniques: a self-supervised discriminator and a feature-aware loss. First, we design a self-supervised discriminator with an auxiliary decoder so that the discriminator can extract better representation. Secondly, we introduce a feature-aware loss to provide the generator more direct supervision by employing the feature representation from the self-supervised discriminator. Experiments on the MS-COCO dataset show that our proposed method significantly advances the state-of-the-art FID score from 28.92 to 24.58. © 2021 IEEE.","Computer vision; Discriminators; Textures; Feature representation; Feature-aware generative adversarial network; High quality images; Images synthesis; Language description; Natural languages; Network features; Photorealistic images; State of the art; Text-to-image synthesis; Generative adversarial networks","Feature-Aware GAN; Generative adversarial networks; Text-to-image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125587343"
"Bibas K.; Weiss-DIcker G.; Cohen D.; Cahan N.; Greenspan H.","Bibas, Koby (57211254793); Weiss-DIcker, Gili (57222072835); Cohen, Dana (57222076877); Cahan, Noa (57222069823); Greenspan, Hayit (7004965553)","57211254793; 57222072835; 57222076877; 57222069823; 7004965553","Learning rotation invariant features for cryogenic electron microscopy image reconstruction","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9433789","563","566","3","10.1109/ISBI48211.2021.9433789","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107217379&doi=10.1109%2fISBI48211.2021.9433789&partnerID=40&md5=326b2ae3544b9972b3f484658f2c2088","Cryo-Electron Microscopy (Cryo-EM) is a Nobel prizewinning technology for determining the 3D structure of particles at near-atomic resolution. A fundamental step in the recovering of the 3D single-particle structure is to align its 2D projections; thus, the construction of a canonical representation with a fixed rotation angle is required. Most approaches use discrete clustering which fails to capture the continuous nature of image rotation, others suffer from low-quality image reconstruction. We propose a novel method that leverages the recent development in the generative adversarial networks. We introduce an encoder-decoder with a rotation angle classifier. In addition, we utilize a discriminator on the decoder output to minimize the reconstruction error. We demonstrate our approach with the Cryo-EM 5HDB and the rotated MNIST datasets showing substantial improvement over recent methods.  © 2021 IEEE.","Decoding; Electron microscopes; Electron microscopy; Medical imaging; Rotation; Adversarial networks; Atomic resolution; Canonical representations; Cryo-electron microscopies (cryo- em); Electron microscopy images; Reconstruction error; Rotation invariant features; Single particle; Image reconstruction","5HDB; Cryo-EM; Deep learning; Generative adversarial networks; Image synthesis; Rotated MNIST","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107217379"
"Kawahara D.; Ozawa S.; Kimura T.; Nagata Y.","Kawahara, Daisuke (56350513700); Ozawa, Shuichi (16031706800); Kimura, Tomoki (56517341100); Nagata, Yasushi (56415732800)","56350513700; 16031706800; 56517341100; 56415732800","Image synthesis of monoenergetic CT image in dual-energy CT using kilovoltage CT with deep convolutional generative adversarial networks","2021","Journal of Applied Clinical Medical Physics","22","4","","184","192","8","10.1002/acm2.13190","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100975361&doi=10.1002%2facm2.13190&partnerID=40&md5=753a50e38513f9074f224445c9520885","Purpose: To synthesize a dual-energy computed tomography (DECT) image from an equivalent kilovoltage computed tomography (kV-CT) image using a deep convolutional adversarial network. Methods: A total of 18,084 images of 28 patients are categorized into training and test datasets. Monoenergetic CT images at 40, 70, and 140 keV and equivalent kV-CT images at 120 kVp are reconstructed via DECT and are defined as the reference images. An image prediction framework is created to generate monoenergetic computed tomography (CT) images from kV-CT images. The accuracy of the images generated by the CNN model is determined by evaluating the mean absolute error (MAE), mean square error (MSE), relative root mean square error (RMSE), peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and mutual information between the synthesized and reference monochromatic CT images. Moreover, the pixel values between the synthetic and reference images are measured and compared using a manually drawn region of interest (ROI). Results: The difference in the monoenergetic CT numbers of the ROIs between the synthetic and reference monoenergetic CT images is within the standard deviation values. The MAE, MSE, RMSE, and SSIM are the smallest for the image conversion of 120 kVp to 140 keV. The PSNR is the smallest and the MI is the largest for the synthetic 70 keV image. Conclusions: The proposed model can act as a suitable alternative to the existing methods for the reconstruction of monoenergetic CT images in DECT from single-energy CT images. © 2021 The Authors. Journal of Applied Clinical Medical Physics published by Wiley Periodicals, Inc. on behalf of American Association of Physicists in Medicine","Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Signal-To-Noise Ratio; Tomography, X-Ray Computed; human; image processing; signal noise ratio; x-ray computed tomography","artificial Intelligence; deep learning; dual-energy CT; image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85100975361"
"Cao C.; Hong Y.; Li X.; Wang C.; Xu C.; Fu Y.; Xue X.","Cao, Chenjie (57215325519); Hong, Yuxin (57224893092); Li, Xiang (57225159941); Wang, Chengrong (57211681740); Xu, Chengming (57218706387); Fu, Yanwei (36571734200); Xue, Xiangyang (7202711086)","57215325519; 57224893092; 57225159941; 57211681740; 57218706387; 36571734200; 7202711086","The Image Local Autoregressive Transformer","2021","Advances in Neural Information Processing Systems","22","","","18433","18445","12","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131950795&partnerID=40&md5=b15430aff922485e2f3b5ef3361b5ff9","Recently, AutoRegressive (AR) models for the whole image generation empowered by transformers have achieved comparable or even better performance compared to Generative Adversarial Networks (GANs). Unfortunately, directly applying such AR models to edit/change local image regions, may suffer from the problems of missing global information, slow inference speed, and information leakage of local guidance. To address these limitations, we propose a novel model - image Local Autoregressive Transformer (iLAT), to better facilitate the locally guided image synthesis. Our iLAT learns the novel local discrete representations, by the newly proposed local autoregressive (LA) transformer of the attention mask and convolution mechanism. Thus iLAT can efficiently synthesize the local image regions by key guidance information. Our iLAT is evaluated on various locally guided image syntheses, such as pose-guided person image synthesis and face editing. Both quantitative and qualitative results show the efficacy of our model. © 2021 Neural information processing systems foundation. All rights reserved.","Image processing; Auto-regressive; Autoregressive modelling; Global informations; Guided images; Image generations; Image regions; Images synthesis; Information leakage; Model images; Performance; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85131950795"
"Zhu K.; Tong P.; Kan H.; Li R.","Zhu, Kefeng (57409255200); Tong, Peilin (57219714410); Kan, Hongwei (57219711718); Li, Rengang (57210173712)","57409255200; 57219714410; 57219711718; 57210173712","You Get What You Sow: High Fidelity Image Synthesis with a Single Pretrained Network","2021","IJCAI International Joint Conference on Artificial Intelligence","","","","3477","3483","6","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125461223&partnerID=40&md5=98145ec924937343df534a67fc6949ad","State-of-the-art image synthesis methods are mostly based on generative adversarial networks and require large dataset and extensive training. Although the model-inversion-oriented branch of methods eliminate the training requirement, the quality of the resulting image tends to be limited due to the lack of sufficient natural and class-specific information. In this paper, we introduce a novel strategy for high fidelity image synthesis with a single pretrained classification network. The strategy includes a class-conditional natural regularization design and a corresponding metadata collecting procedure for different scenarios. We show that our method can synthesize high quality natural images that closely follow the features of one or more given seed images. Moreover, our method achieves surprisingly decent results in the task of sketch-based image synthesis without training. Finally, our method further improves the performance in terms of accuracy and efficiency in the data-free knowledge distillation task. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.","Distillation; Image processing; Large dataset; Arts image; Classification networks; High-fidelity images; Images synthesis; Large datasets; Model inversion; Novel strategies; State of the art; Synthesis method; Training requirement; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85125461223"
"Fei Y.; Zhan B.; Hong M.; Wu X.; Zhou J.; Wang Y.","Fei, Yuchen (57219972263); Zhan, Bo (57221803799); Hong, Mei (36099839300); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Wang, Yan (56039981100)","57219972263; 57221803799; 36099839300; 57221065403; 21234416400; 56039981100","Deep learning-based multi-modal computing with feature disentanglement for MRI image synthesis","2021","Medical Physics","48","7","","3778","3789","11","10.1002/mp.14929","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105888192&doi=10.1002%2fmp.14929&partnerID=40&md5=8bf4bb6cc84d08ab50ed49d784a7018f","Purpose: Different Magnetic resonance imaging (MRI) modalities of the same anatomical structure are required to present different pathological information from the physical level for diagnostic needs. However, it is often difficult to obtain full-sequence MRI images of patients owing to limitations such as time consumption and high cost. The purpose of this work is to develop an algorithm for target MRI sequences prediction with high accuracy, and provide more information for clinical diagnosis. Methods: We propose a deep learning-based multi-modal computing model for MRI synthesis with feature disentanglement strategy. To take full advantage of the complementary information provided by different modalities, multi-modal MRI sequences are utilized as input. Notably, the proposed approach decomposes each input modality into modality-invariant space with shared information and modality-specific space with specific information, so that features are extracted separately to effectively process the input data. Subsequently, both of them are fused through the adaptive instance normalization (AdaIN) layer in the decoder. In addition, to address the lack of specific information of the target modality in the test phase, a local adaptive fusion (LAF) module is adopted to generate a modality-like pseudo-target with specific information similar to the ground truth. Results: To evaluate the synthesis performance, we verify our method on the BRATS2015 dataset of 164 subjects. The experimental results demonstrate our approach significantly outperforms the benchmark method and other state-of-the-art medical image synthesis methods in both quantitative and qualitative measures. Compared with the pix2pixGANs method, the PSNR improves from 23.68 to 24.8. Moreover the ablation studies have also verified the effectiveness of important components of the proposed method. Conclusion: The proposed method could be effective in prediction of target MRI sequences, and useful for clinical diagnosis and treatment. © 2021 American Association of Physicists in Medicine","Algorithms; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Diagnosis; Magnetic resonance imaging; Medical imaging; Anatomical structures; Clinical diagnosis; Image synthesis; Input modalities; Shared information; Specific information; State of the art; Time consumption; adult; algorithm; article; controlled study; decomposition; deep learning; female; human; human experiment; major clinical study; male; nuclear magnetic resonance imaging; prediction; quantitative analysis; synthesis; algorithm; image processing; nuclear magnetic resonance imaging; Deep learning","deep learning; generative adversarial networks (GANs); image synthesis; magnetic resonance imaging (MRI)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105888192"
"Ke A.; Liu G.; Chen J.; Wu X.","Ke, Aihua (57558470800); Liu, Gang (57457043100); Chen, Jian (57559252800); Wu, Xinyun (56584944100)","57558470800; 57457043100; 57559252800; 56584944100","Trilateral GAN with Channel Attention Residual for Semantic Image Synthesis","2021","Proceedings - 2021 International Conference on Computer Information Science and Artificial Intelligence, CISAI 2021","","","","1123","1129","6","10.1109/CISAI54367.2021.00223","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127436345&doi=10.1109%2fCISAI54367.2021.00223&partnerID=40&md5=a259ac802242a8e32bfc3dc5cf56d0df","In this paper, a novel method for synthesizing photo-realistic images from semantic label maps using GANs is proposed, which is an ideal and challenging task in computer vision and image synthesis. Due to the sparsity of the information contained in semantic label maps, it is difficult for some existing methods to achieve satisfactory synthesis effect. This paper proposes a trilateral generative adversarial network to support multi-directional transmission between images of different resolutions, called TrilateralGAN. Compared with the traditional single-directional transmission, the design of our TrilateralGAN network can better retain the information in the original image to avoid loss of details. In addition, we further propose a new channel attention residual as the main part of the TrilateralGAN network. This part can enhance the retained information to varying degrees, which can make the image synthesized by TrilateralGAN have clearer edges and richer details. The experimental results on Cityscapes and ADE20K datasets demonstrate the advantage of TrilateralGAN over the state-of-the-art approaches, regarding both visual quality and the representative evaluating criteria.  © 2021 IEEE.","Computer vision; Image enhancement; Semantic Web; Semantics; Channel attention residual; Different resolutions; Directional transmission; Images synthesis; Label maps; Novel methods; Photorealistic images; Semantic images; Semantic labels; Synthesis effects; Generative adversarial networks","channel attention residual; Generative Adversarial Networks; image synthesis; semantic label","Conference paper","Final","","Scopus","2-s2.0-85127436345"
"Chen H.; Chen F.; He H.","Chen, Hongyou (57211268513); Chen, Fan (55531508900); He, Hongjie (14625202400)","57211268513; 55531508900; 14625202400","SSC-GAN: A Novel GAN Based on the Same Solution Constraints of First-Order ODEs","2021","International Journal of Pattern Recognition and Artificial Intelligence","35","11","2152018","","","","10.1142/S0218001421520182","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112601182&doi=10.1142%2fS0218001421520182&partnerID=40&md5=84b66f1f7154463633a44661044aa484","Generative adversarial network (GAN) is the most important natural image synthesis method, and one of GAN major challenges is the adversarial learning stability. Usually, GAN directly optimizes a single divergence via the two-player zero-sum game of generator and discriminator. Because the parameter updating of generator and discriminator is complex and changeable, directly optimizing the single divergence is easy to fall into local optimal solution, which affects the adversarial learning stability. To improve the adversarial learning stability and convergence accuracy, the same solution constraints GAN (SSC-GAN) is proposed. In this novel GAN, a first-order ordinary differential equation (ODE) is constructed by training set probability density function, which is the same solution problem of GAN optimization problem, and its constraint conditions are given to ensure the existence and uniqueness of the ODE solution. These constraints are used to guide the GAN parameter updating to improve the adversarial learning stability, thereby getting better training effects. In CELEBA and CIFAR10, the experimental results show that the novel GAN is better than the classic GAN models. In CELEBA, CIFAR10 and LSUN-BEDROOM, the proposed SSC-GAN is also better than two state-of-the-art GAN models, SAGAN and SNGAN.  © 2021 World Scientific Publishing Company.","Probability density function; Stability; Adversarial learning; Adversarial networks; Constraint conditions; Existence and uniqueness; First order ordinary differential equations; Local optimal solution; Optimization problems; Solution problems; Ordinary differential equations","adversarial learning stability; Deep learning; generative adversarial network; ordinary differential equations","Article","Final","","Scopus","2-s2.0-85112601182"
"Zhu C.; Lai R.; Bi L.; Wang X.; Du J.","Zhu, Chen (57299512400); Lai, Ru (7201986834); Bi, Luzheng (7004648542); Wang, Xuyang (57299693000); Du, Jiarong (57299512500)","57299512400; 7201986834; 7004648542; 57299693000; 57299512500","Edge-guided Adversarial Network Based on Contrastive Learning for Image-to-Image Translation","2021","Chinese Control Conference, CCC","2021-July","","","7949","7954","5","10.23919/CCC52363.2021.9549847","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117358366&doi=10.23919%2fCCC52363.2021.9549847&partnerID=40&md5=5a06ffac5475330187885de8ad1c7ac5","In recent years, generative adversarial networks have made great progress in image synthesis and image translation tasks in the field of image processing and computer vision. However, the quality of the generated image and the scalability over multiple datasets is still not satisfying. We briefly review some prior works and propose a method for image-to-image translation, which is learning a mapping between different visual domains. The network extracts edge feature from both domains of output and target, and minimizes the difference using a framework based on patchwise contrastive learning. We apply edge feature guidance in our method and select Sobel operator among several classical edge detection operators. We demonstrate that our method outperforms existing approaches in the task of unpaired image-to-image translation across datasets. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.","Image processing; Adversarial networks; Contrastive learning; Edge features; Image processing and computer vision; Image translation; Image-to-image translation; Images synthesis; Multiple data sets; Network-based; Sobel operator; Generative adversarial networks","Contrastive learning; Edge detection; Generative adversarial network; Image-to-Image translation","Conference paper","Final","","Scopus","2-s2.0-85117358366"
"Kamran S.A.; Hossain K.F.; Tavakkoli A.; Zuckerbrod S.L.; Baker S.A.","Kamran, Sharif Amit (57207775104); Hossain, Khondker Fariha (57219762852); Tavakkoli, Alireza (13908382500); Zuckerbrod, Stewart Lee (57212200552); Baker, Salah A. (7403308202)","57207775104; 57219762852; 13908382500; 57212200552; 7403308202","VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers","2021","Proceedings of the IEEE International Conference on Computer Vision","2021-October","","","3228","3238","10","10.1109/ICCVW54120.2021.00362","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123056339&doi=10.1109%2fICCVW54120.2021.00362&partnerID=40&md5=809e5408b2eef19c4fb7f41bbd187bad","In Fluorescein Angiography (FA), an exogenous dye is injected in the bloodstream to image the vascular structure of the retina. The injected dye can cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even death. In contrast, color fundus imaging is a non-invasive technique used for photographing the retina but does not have sufficient fidelity for capturing its vascular structure. The only non-invasive method for capturing retinal vasculature is optical coherence tomography-angiography (OCTA). However, OCTA equipment is quite expensive, and stable imaging is limited to small areas on the retina. In this paper, we propose a novel conditional generative adversarial network (GAN) capable of simultaneously synthesizing FA images from fundus photographs while predicting retinal degeneration. The proposed system has the benefit of addressing the problem of imaging retinal vasculature in a non-invasive manner as well as predicting the existence of retinal abnormalities. We use a semi-supervised approach to train our GAN using multiple weighted losses on different modalities of data. Our experiments validate that the proposed architecture exceeds recent state-of-the-art generative networks for fundus-to-angiography synthesis. Moreover, our vision transformer-based discriminators generalize quite well on out-of-distribution data sets for retinal disease prediction.  © 2021 IEEE.","Computer vision; Forecasting; Generative adversarial networks; Noninvasive medical procedures; Ophthalmology; Optical tomography; Fluorescein angiography; Fundus imaging; Images synthesis; Noninvasive methods; Noninvasive technique; Retinal image; Retinal vasculature; Semi-supervised; Small area; Vascular structures; Angiography","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123056339"
"Zhang X.; Cheng Z.; Zhang X.; Liu H.","Zhang, Xianchao (7410283670); Cheng, Ziyang (57420144700); Zhang, Xiaotong (56025149100); Liu, Han (56393753300)","7410283670; 57420144700; 56025149100; 56393753300","Posterior Promoted GAN with Distribution Discriminator for Unsupervised Image Synthesis","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","6515","6524","9","10.1109/CVPR46437.2021.00645","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123185082&doi=10.1109%2fCVPR46437.2021.00645&partnerID=40&md5=174033e9e239e0a9c608b118df0c97bf","Sufficient real information in generator is a critical point for the generation ability of GAN. However, GAN and its variants suffer from lack of this point, resulting in brittle training processes. In this paper, we propose a novel variant of GAN, Posterior Promoted GAN (P2GAN), which promotes generator with the real information in the posterior distribution produced by discriminator. In our framework, different from other variants of GAN, the discriminator maps images to a multivariate Gaussian distribution and extracts real information. The generator employs the real information by AdaIN and a latent code regularizer. Besides, reparameterization trick and pretraining is applied to guarantee a stable training process in practice. The convergence of P2GAN is theoretically proved. Experimental results on typical high-dimensional multi-modal datasets demonstrate that P2GAN has achieved comparable results with the state-of-the-art variants of GAN on unsupervised image synthesis. © 2021 IEEE","Computer vision; Generative adversarial networks; A-stable; High-dimensional; Images synthesis; Map image; Multivariate Gaussian Distributions; Posterior distributions; Pre-training; Regularizer; Reparameterization; Training process; Discriminators","","Conference paper","Final","","Scopus","2-s2.0-85123185082"
"Lee D.; Jeong S.W.; Kim S.J.; Cho H.; Park W.; Han Y.","Lee, Dongyeon (55698936200); Jeong, Sang Woon (57236581300); Kim, Sung Jin (57211871443); Cho, Hyosung (7403936631); Park, Won (57235735500); Han, Youngyih (55544695000)","55698936200; 57236581300; 57211871443; 7403936631; 57235735500; 55544695000","Improvement of megavoltage computed tomography image quality for adaptive helical tomotherapy using cycleGAN-based image synthesis with small datasets","2021","Medical Physics","48","10","","5593","5610","17","10.1002/mp.15182","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113765554&doi=10.1002%2fmp.15182&partnerID=40&md5=01bd9e949905d4d6100c5c18ce2be2db","Purpose: Megavoltage computed tomography (MVCT) offers an opportunity for adaptive helical tomotherapy. However, high noise and reduced contrast in the MVCT images due to a decrease in the imaging dose to patients limits its usability. Therefore, we propose an algorithm to improve the image quality of MVCT. Methods: The proposed algorithm generates kilovoltage CT (kVCT)-like images from MVCT images using a cycle-consistency generative adversarial network (cycleGAN)-based image synthesis model. Data augmentation using an affine transformation was applied to the training data to overcome the lack of data diversity in the network training. The mean absolute error (MAE), root-mean-square error (RMSE), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM) were used to quantify the correction accuracy of the images generated by the proposed algorithm. The proposed method was validated by comparing the images generated with those obtained from conventional and deep learning-based image processing method through non-augmented datasets. Results: The average MAE, RMSE, PSNR, and SSIM values were 18.91 HU, 69.35 HU, 32.73 dB, and 95.48 using the proposed method, respectively, whereas cycleGAN with non-augmented data showed inferior results (19.88 HU, 70.55 HU, 32.62 dB, 95.19, respectively). The voxel values of the image obtained by the proposed method also indicated similar distributions to those of the kVCT image. The dose-volume histogram of the proposed method was also similar to that of electron density corrected MVCT. Conclusions: The proposed algorithm generates synthetic kVCT images from MVCT images using cycleGAN with small patient datasets. The image quality achieved by the proposed method was correspondingly improved to the level of a kVCT image while maintaining the anatomical structure of an MVCT image. The evaluation of dosimetric effectiveness of the proposed method indicates the applicability of accurate treatment planning in adaptive radiation therapy. © 2021 American Association of Physicists in Medicine","Deep learning; Image enhancement; Image quality; Mean square error; Metadata; Radiotherapy; Signal to noise ratio; Adaptive radiation therapies; Affine transformations; Computed tomography images; Dose-volume histograms; Image processing - methods; Peak signal to noise ratio; Root mean square errors; Structural similarity index measures (SSIM); adaptive radiation; adult; affine transform; anatomical concepts; article; comparative effectiveness; computer assisted tomography; deep learning; dose volume histogram; human; image processing; image quality; quantitative analysis; radiotherapy; signal noise ratio; synthesis; tomotherapy; treatment planning; Computerized tomography","adaptive radiation therapy; cycleGAN; data augmentation; deep learning; image synthesis; megavoltage computed tomography","Article","Final","","Scopus","2-s2.0-85113765554"
"Asadi F.; O'reilly J.A.","Asadi, Fawad (57576036300); O'reilly, Jamie A. (57197381665)","57576036300; 57197381665","Artificial Computed Tomography Images with Progressively Growing Generative Adversarial Network","2021","BMEiCON 2021 - 13th Biomedical Engineering International Conference","","","","","","","10.1109/BMEiCON53485.2021.9745251","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128173323&doi=10.1109%2fBMEiCON53485.2021.9745251&partnerID=40&md5=e778fa11dcdc15a3929ac02b216cf2e0","Applications of artificial intelligence in medical imaging include classification, segmentation, and treatment planning. Using current deep-learning techniques, developing these systems requires large amounts of labelled training data. Obtaining this data is challenging due to costs, required expertise, inconsistency of imaging procedures and formatting, and patient privacy concerns. Generative adversarial networks (GANs) may alleviate some of these issues by supplying realistic artificial medical images. In this study, we trained progressively growing (PG)GAN to synthesize full-sized computed tomography (CT) images and succeeded. Performance of the PGGAN was evaluated using Fréchet Inception Distance (FID), Inception Score (IS), and Precision (P) and Recall (R). These metrics were calculated for generated, training, and validation images. The influence of dataset size was explored by varying the number of samples used to calculate each metric; this affected FID, P, and R, but not IS, which has obvious implications for comparing studies. The FID between artificial CT images from PGGAN and real validation images was 42; interestingly, FID between real training and validation images was 24. This suggests that a further reduction of 18 could be achieved by improving the generative model. Overall, artificial CT images generated by PGGAN were almost indistinguishable from real images to the human eye, although computational metrics could identify differences between them. In future work, GANs may be deployed to augment data for training medical AI systems. © 2021 IEEE.","Computerized tomography; Deep learning; Medical imaging; 'current; Computed tomography images; Evaluation metrics; Frechet; Generative adversarial network evaluation metric; Images synthesis; Lung computed tomography; Medical image synthesis; Network evaluation; Treatment planning; Generative adversarial networks","GAN evaluation metrics; generative adversarial networks; lung computed tomography; medical image synthesis","Conference paper","Final","","Scopus","2-s2.0-85128173323"
"Ge N.; Zhu Y.; Xiong X.; Zheng B.; Huang J.","Ge, Ning (57381503800); Zhu, Yonghua (55723795700); Xiong, Xiaoyu (57482548600); Zheng, Binghui (57382387600); Huang, Jieyu (57381325500)","57381503800; 55723795700; 57482548600; 57382387600; 57381325500","KnHiGAN: Knowledge-enhanced Hierarchical Generative Adversarial Network for Fine-grained Text-to-Image Synthesis","2021","Proceedings - 2021 14th International Symposium on Computational Intelligence and Design, ISCID 2021","","","","357","360","3","10.1109/ISCID52796.2021.00088","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126017129&doi=10.1109%2fISCID52796.2021.00088&partnerID=40&md5=6a036e23b7713e204c1cc0f24c613907","To generate fine-grained images with greater authenticity, in this paper, we propose a Knowledge-enhanced Hierarchical Generative Adversarial Network (KnHiGAN) for text-to-image synthesis. KnHiGAN sets up a Knowledge Enhancement Module to expand conditions for the limited text descriptions by combining with the knowledge graph, as a result, it can provide richer fine-grained details to the generative network. Moreover, a Hierarchical Generative Adversarial Network is designed to generate the foreground and background separately, and the two are integrated together to composite the final result. Experiments on CUB-200 and Oxford-102 datasets show that our KnHiGAN can not only generate the fine-grained images which are more like those that exist in the real world, but also can maintain a high degree of consistency with the original text input.  © 2021 IEEE.","Computer vision; Image enhancement; Knowledge graph; Condition; Fine grained; Hierarchical structures; Images synthesis; Knowledge graphs; Network knowledge; Real-world; Text input; Text-to-image synthesis; Generative adversarial networks","Generative Adversarial Network; Hierarchical structure; Knowledge graph; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85126017129"
"Moghari M.D.; Zhou L.; Yu B.; Young N.; Moore K.; Evans A.; Fulton R.R.; Kyme A.Z.","Moghari, Mahdieh Dashtbani (56522947400); Zhou, Luping (23398846800); Yu, Biting (57201496052); Young, Noel (7402413046); Moore, Krystal (57220945472); Evans, Andrew (57222650871); Fulton, Roger R (7201917864); Kyme, Andre Z (6507529846)","56522947400; 23398846800; 57201496052; 7402413046; 57220945472; 57222650871; 7201917864; 6507529846","Efficient radiation dose reduction in whole-brain CT perfusion imaging using a 3D GAN: Performance and clinical feasibility","2021","Physics in Medicine and Biology","66","7","075008","","","","10.1088/1361-6560/abe917","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103639231&doi=10.1088%2f1361-6560%2fabe917&partnerID=40&md5=14d1dae443ccd279d3b0e47e6eeb45d4","Dose reduction in cerebral CT perfusion (CTP) imaging is desirable but is accompanied by an increase in noise that can compromise the image quality and the accuracy of image-based haemodynamic modelling used for clinical decision support in acute ischaemic stroke. The few reported methods aimed at denoising low-dose CTP images lack practicality by considering only small sections of the brain or being computationally expensive. Moreover, the prediction of infarct and penumbra size and location - the chief means of decision support for treatment options - from denoised data has not been explored using these approaches. In this work, we present the first application of a 3D generative adversarial network (3D GAN) for predicting normal-dose CTP data from low-dose CTP data. Feasibility of the approach was tested using real data from 30 acute ischaemic stroke patients in conjunction with low dose simulation. The 3D GAN model was applied to 643 voxel patches extracted from two different configurations of the CTP data - frame-based and stacked. The method led to whole-brain denoised data being generated for haemodynamic modelling within 90 s. Accuracy of the method was evaluated using standard image quality metrics and the extent to which the clinical content and lesion characteristics of the denoised CTP data were preserved. Results showed an average improvement of 5.15-5.32 dB PSNR and 0.025-0.033 structural similarity index (SSIM) for CTP images and 2.66-3.95 dB PSNR and 0.036-0.067 SSIM for functional maps at 50% and 25% of normal dose using GAN model in conjunction with a stacked data regime for image synthesis. Consequently, the average lesion volumetric error reduced significantly (p-value <0.05) by 18%-29% and dice coefficient improved significantly by 15%-22%. We conclude that GAN-based denoising is a promising practical approach for reducing radiation dose in CTP studies and improving lesion characterisation.  © 2021 Institute of Physics and Engineering in Medicine.","3D modeling; Decision support systems; Image enhancement; Image quality; Adversarial networks; Clinical decision support; Clinical feasibility; Decision supports; Low-dose simulations; Radiation dose reductions; Structural similarity indices (SSIM); Volumetric errors; Computerized tomography","","Article","Final","","Scopus","2-s2.0-85103639231"
"Alqahtani H.; Kavakli-Thorne M.; Kumar G.","Alqahtani, Hamed (57194574403); Kavakli-Thorne, Manolya (6602420178); Kumar, Gulshan (35932222600)","57194574403; 6602420178; 35932222600","Applications of Generative Adversarial Networks (GANs): An Updated Review","2021","Archives of Computational Methods in Engineering","28","2","","525","552","27","10.1007/s11831-019-09388-y","73","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077091121&doi=10.1007%2fs11831-019-09388-y&partnerID=40&md5=5d2a51b74ad6a862ea25cb97896be7bf","Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This paper aims to present an overview of GANs, its different variants, and potential application in various domains. The paper attempts to identify GANs’ advantages, disadvantages and significant challenges to the successful implementation of GAN in different application areas. The main intention of this paper is to explore and present a comprehensive review of the crucial applications of GANs covering a variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects. © 2019, CIMNE, Barcelona, Spain.","Neural networks; Semantics; Supervised learning; Unsupervised learning; Adversarial networks; Annotated training data; Application area; Image super resolutions; Image synthesis; Real-world; Semantic images; Backpropagation","Generative adversarial networks; Neural networks; Supervised learning; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85077091121"
"Biswas S.; Riba P.; Lladós J.; Pal U.","Biswas, Sanket (57226196113); Riba, Pau (56728257900); Lladós, Josep (6603062543); Pal, Umapada (57200742116)","57226196113; 56728257900; 6603062543; 57200742116","DocSynth: A Layout Guided Approach for Controllable Document Image Synthesis","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12823 LNCS","","","555","568","13","10.1007/978-3-030-86334-0_36","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115298488&doi=10.1007%2f978-3-030-86334-0_36&partnerID=40&md5=29e2a6cbfc302ae39a57aab0e5536ca7","Despite significant progress on current state-of-the-art image generation models, synthesis of document images containing multiple and complex object layouts is a challenging task. This paper presents a novel approach, called DocSynth, to automatically synthesize document images based on a given layout. In this work, given a spatial layout (bounding boxes with object categories) as a reference by the user, our proposed DocSynth model learns to generate a set of realistic document images consistent with the defined layout. Also, this framework has been adapted to this work as a superior baseline model for creating synthetic document image datasets for augmenting real data during training for document layout analysis tasks. Different sets of learning objectives have been also used to improve the model performance. Quantitatively, we also compare the generated results of our model with real data using standard evaluation metrics. The results highlight that our model can successfully generate realistic and diverse document images with multiple objects. We also present a comprehensive qualitative analysis summary of the different scopes of synthetic image generation tasks. Lastly, to our knowledge this is the first work of its kind. © 2021, Springer Nature Switzerland AG.","Computer vision; Image analysis; Arts image; Document images; Document synthesis; Image generations; Images synthesis; Layout generations; Multiple objects; On currents; On-currents; State of the art; Generative adversarial networks","Document synthesis; Generative Adversarial Networks; Layout generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85115298488"
"Shen T.; Hao K.; Gou C.; Wang F.-Y.","Shen, Tianyu (57207733071); Hao, Kunkun (57222081695); Gou, Chao (56320227000); Wang, Fei-Yue (57211758869)","57207733071; 57222081695; 56320227000; 57211758869","Mass Image Synthesis in Mammogram with Contextual Information Based on GANs","2021","Computer Methods and Programs in Biomedicine","202","","106019","","","","10.1016/j.cmpb.2021.106019","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101397555&doi=10.1016%2fj.cmpb.2021.106019&partnerID=40&md5=0e402e9ee852b24756a94bf956dbb6d1","Background and Objective: In medical imaging, the scarcity of labeled lesion data has hindered the application of many deep learning algorithms. To overcome this problem, the simulation of diverse lesions in medical images is proposed. However, synthesizing labeled mass images in mammograms is still challenging due to the lack of consistent patterns in shape, margin, and contextual information. Therefore, we aim to generate various labeled medical images based on contextual information in mammograms. Methods:In this paper, we propose a novel approach based on GANs to generate various mass images and then perform contextual infilling by inserting the synthetic lesions into healthy screening mammograms. Through incorporating features of both realistic mass images and corresponding masks into the adversarial learning scheme, the generator can not only learn the distribution of the real mass images but also capture the matching shape, margin and context information. Results:To demonstrate the effectiveness of our proposed method, we conduct experiments on publicly available mammogram database of DDSM and a private database provided by Nanfang Hospital in China. Qualitative and quantitative evaluations validate the effectiveness of our approach. Additionally, through the data augmentation by image generation of the proposed method, an improvement of 5.03% in detection rate can be achieved over the same model trained on original real lesion images. Conclusions:The results show that the data augmentation based on our method increases the diversity of dataset. Our method can be viewed as one of the first steps toward generating labeled breast mass images for precise detection and can be extended in other medical imaging domains to solve similar problems. © 2021","Algorithms; China; Databases, Factual; Image Processing, Computer-Assisted; Mammography; Deep learning; Image enhancement; Learning algorithms; Mammography; Medical problems; X ray screens; Adversarial learning; Context information; Contextual information; Data augmentation; Image generations; Private database; Quantitative evaluation; Screening mammogram; affine transform; algorithm; Article; breast tumor; clinical assessment; clinical effectiveness; clinical evaluation; comparative study; controlled study; data base; deep learning; feature extraction; female; generative adversarial network; human; image analysis; image quality; mammography; medical information; outcome assessment; qualitative analysis; quantitative analysis; screening test; synthesis; China; factual database; image processing; Medical imaging","generative adversarial network; mammogram; mass detection; medical image synthesis","Article","Final","","Scopus","2-s2.0-85101397555"
"Mao F.; Ma B.; Chang H.; Shan S.; Chen X.","Mao, Fengling (57193001664); Ma, Bingpeng (13606466200); Chang, Hong (36719786700); Shan, Shiguang (22235341500); Chen, Xilin (57215374943)","57193001664; 13606466200; 36719786700; 22235341500; 57215374943","Learning efficient text-to-image synthesis via interstage cross-sample similarity distillation","2021","Science China Information Sciences","64","2","120102","","","","10.1007/s11432-020-2900-x","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096315223&doi=10.1007%2fs11432-020-2900-x&partnerID=40&md5=41bc50f5d002611dd9e254bda2c15984","For a given text, previous text-to-image synthesis methods commonly utilize a multistage generation model to produce images with high resolution in a coarse-to-fine manner. However, these methods ignore the interaction among stages, and they do not constrain the consistent cross-sample relations of images generated in different stages. These deficiencies result in inefficient generation and discrimination. In this study, we propose an interstage cross-sample similarity distillation model based on a generative adversarial network (GAN) for learning efficient text-to-image synthesis. To strengthen the interaction among stages, we achieve interstage knowledge distillation from the refined stage to the coarse stages with novel interstage cross-sample similarity distillation blocks. To enhance the constraint on the cross-sample relations of the images generated at different stages, we conduct cross-sample similarity distillation among the stages. Extensive experiments on the Oxford-102 and Caltech-UCSD Birds-200–2011 (CUB) datasets show that our model generates visually pleasing images and achieves quantitatively comparable performance with state-of-the-art methods. © 2020, Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature.","Distillation; Distilleries; Adversarial networks; Coarse to fine; Different stages; High resolution; Image synthesis; Model-based OPC; Sample similarities; State-of-the-art methods; Image enhancement","generative adversarial network (GAN); knowledge distillation; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85096315223"
"Ding S.; Zheng J.; Liu Z.; Zheng Y.; Chen Y.; Xu X.; Lu J.; Xie J.","Ding, Saisai (57219537256); Zheng, Jian (56389843700); Liu, Zhaobang (55265778400); Zheng, Yanyan (57217362926); Chen, Yanmei (57202016015); Xu, Xiaomin (57282744700); Lu, Jia (57199238859); Xie, Jing (57219535652)","57219537256; 56389843700; 55265778400; 57217362926; 57202016015; 57282744700; 57199238859; 57219535652","High-resolution dermoscopy image synthesis with conditional generative adversarial networks","2021","Biomedical Signal Processing and Control","64","","102224","","","","10.1016/j.bspc.2020.102224","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093651381&doi=10.1016%2fj.bspc.2020.102224&partnerID=40&md5=f555e5a0dc75d143914b9cd0a6332d1a","Background and objective: Due to the lack of training data, the accurate classification of skin lesions still has great challenges. Generative adversarial networks (GANs) have been used to synthesize dermoscopy images successfully. Unfortunately, previous methods usually directly feed category labels into GANs and cannot provide effective information gain for the classification model. This paper studies a specific conditional image synthesis method, which could convert semantic segmentation map to dermoscopy image. Methods: We proposed a conditional GANs (CGAN) for high-resolution dermoscopy images synthesis. First, we established an effective label mapping with pathological significance by combining the segmentation mask and category label of skin lesions. Then, a CGAN based on the image-to-image translation framework is constructed and took the previous label mapping as input to generate dermoscopy images. Especially, the shallow and deep features are combined together in the generator to avoid the loss of semantic information, and discriminator-based feature matching loss is introduced to improve the quality of generated images. Results: The proposed method is evaluated in ISIC-2017 skin dataset. Compared with several representative GANs architectures including the newest semantic image synthesis method, the proposed method has better performance in both visual effect and quantitative evaluation. Moreover, by using the generated images, the average AUC values of several skin lesion classification models can be improved effectively. Conclusions: The proposed method can generate high-realistic and high-resolution demoscopy images, leading to performance improvement of skin lesion classification models, which could also be helpful for solving data shortages and classes imbalance problems in the field of medical image analysis. © 2020 Elsevier Ltd","Classification (of information); Dermatology; Diagnosis; Image segmentation; Mapping; Medical imaging; Medical problems; Semantics; Adversarial networks; Classification models; Dermoscopy images; Imbalance problem; Quantitative evaluation; Segmentation masks; Semantic information; Semantic segmentation; Article; classification algorithm; deep learning; epiluminescence microscopy; evaluation study; feature extraction; generative adversarial network; high resolution dermoscopy; image analysis; image quality; image segmentation; information processing; machine learning; melanoma; nevus; priority journal; qualitative analysis; quantitative analysis; seborrheic keratosis; skin defect; Image enhancement","Data shortages; Dermoscopy image; Feature matching loss; Generative adversarial networks; Skin lesion","Article","Final","","Scopus","2-s2.0-85093651381"
"Desai P.; Sujatha C.; Shanbhag R.; Gotur R.; Hebbar R.; Kurtkoti P.","Desai, Padmashree (36975003800); Sujatha, C. (57192275806); Shanbhag, R. (57258529000); Gotur, R. (57258529100); Hebbar, R. (57914373200); Kurtkoti, P. (57258529200)","36975003800; 57192275806; 57258529000; 57258529100; 57914373200; 57258529200","Adversarial Network for Photographic Image Synthesis from Fine-grained Captions","2021","2021 International Conference on Intelligent Technologies, CONIT 2021","","","","","","","10.1109/CONIT51480.2021.9498513","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114891997&doi=10.1109%2fCONIT51480.2021.9498513&partnerID=40&md5=1482aa828e822834b5e843d7818c4fe7","Automatic synthesis of realistic images from a given caption of text is a fascinating research idea and useful in many applications such as image in-painting, photo-editing, computer-aided design, etc. Current AI techniques are still exploring in this direction. However, researchers are currently developing text-to-image synthesis networks focused on the learning of discriminative text and image features using continuous and generalized robust neural network architectures. Many applications use deep convolution Generative Adversarial Networks (GANs) to construct extremely persuasive representations of explicit categories like faces, album covers, birds, creatures, and room interiors, among others. With the progress of generative models, neural networks can not only recognize images but are used to generate audio and realistic images as well. In the proposed work, the authors have used GAN-CLS architecture to create images from given text descriptions/captions. The experiment uses the CUB-200 dataset, which contains 11, 788 bird images from 200 categories, as well as the Oxford-102 dataset, which contains 8, 189 flower images from 102 categories. The proposed system's performance is assessed and compared to that of other systems. © 2021 IEEE.","Birds; Computer aided design; Network architecture; Photography; Adversarial networks; Automatic synthesis; Generative model; Image features; Image synthesis; Photographic image; Realistic images; System's performance; Neural networks","captions; GAN; image synthesis; photographic; text","Conference paper","Final","","Scopus","2-s2.0-85114891997"
"Xiaomao Z.; Wei W.; Bing D.","Xiaomao, Zhou (57437183300); Wei, Wang (57437323300); Bing, Du (57437043300)","57437183300; 57437323300; 57437043300","PSG-GAN: Progressive Person Image Generation with Self-Guided Local Focuses","2021","Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","2021-November","","","763","769","6","10.1109/ICTAI52525.2021.00121","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123951892&doi=10.1109%2fICTAI52525.2021.00121&partnerID=40&md5=fb85c7ab76eb2b0e8a6555882681df5d","This paper proposes PSG-GAN, a novel Generative Adversarial Network for pose-guided person image synthesis, which can progressively generate realistic person images of desired poses together with corresponding semantic segmentation masks. Specifically, PSG-GAN consists of a sequence of Region-Focal Transfer Blocks (RFBs) where each contains two generation pathways: the appearance generation pathway and the semantic generation pathway. The former pathway is responsible for generating the target image by explicitly preserving appearance-related features within certain regions, where local region transformations are considered. The latter pathway is used to generate semantic masks which define the areas for the local transformations to attend to. These two learning pathways work together and reinforce each other to simultaneously generate the target image and semantic masks progressively. Qualitative and quantitative experimental results on two benchmark datasets demonstrate PSG-GAN's superiority over other approaches in generating realistic person images in pose transfer tasks.  © 2021 IEEE.","Computer vision; Generative adversarial networks; Semantic Segmentation; GAN; Guided images; Image generations; Images synthesis; Local region; Pose-guided image generation; Segmentation masks; Semantic generations; Semantic segmentation; Target images; Semantics","GAN; pose-guided image generation; segmentation mask","Conference paper","Final","","Scopus","2-s2.0-85123951892"
"Wang C.; Yang G.; Papanastasiou G.; Tsaftaris S.A.; Newby D.E.; Gray C.; Macnaught G.; MacGillivray T.J.","Wang, Chengjia (57196394674); Yang, Guang (57216243504); Papanastasiou, Giorgos (56539707000); Tsaftaris, Sotirios A. (6505952824); Newby, David E. (7006580760); Gray, Calum (55224398700); Macnaught, Gillian (55241784200); MacGillivray, Tom J. (57207503523)","57196394674; 57216243504; 56539707000; 6505952824; 7006580760; 55224398700; 55241784200; 57207503523","DiCyc: GAN-based deformation invariant cross-domain information fusion for medical image synthesis","2021","Information Fusion","67","","","147","160","13","10.1016/j.inffus.2020.10.015","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094323019&doi=10.1016%2fj.inffus.2020.10.015&partnerID=40&md5=64903166b7e31a9f3babe1e43f7c8ce2","Cycle-consistent generative adversarial network (CycleGAN) has been widely used for cross-domain medical image synthesis tasks particularly due to its ability to deal with unpaired data. However, most CycleGAN-based synthesis methods cannot achieve good alignment between the synthesized images and data from the source domain, even with additional image alignment losses. This is because the CycleGAN generator network can encode the relative deformations and noises associated to different domains. This can be detrimental for the downstream applications that rely on the synthesized images, such as generating pseudo-CT for PET-MR attenuation correction. In this paper, we present a deformation invariant cycle-consistency model that can filter out these domain-specific deformation. The deformation is globally parameterized by thin-plate-spline (TPS), and locally learned by modified deformable convolutional layers. Robustness to domain-specific deformations has been evaluated through experiments on multi-sequence brain MR data and multi-modality abdominal CT and MR data. Experiment results demonstrated that our method can achieve better alignment between the source and target data while maintaining superior image quality of signal compared to several state-of-the-art CycleGAN-based methods. © 2020 The Authors","Alignment; Deformation; Medical imaging; Adversarial networks; Attenuation correction; Consistency model; Different domains; Downstream applications; Relative deformation; Synthesized images; Thin-plate spline; Computerized tomography","GAN; Image synthesis; Information fusion","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85094323019"
"Matsumori S.; Abe Y.; Shingyouchi K.; Sugiura K.; Imai M.","Matsumori, Shoya (57205301318); Abe, Yuki (57205297667); Shingyouchi, Kosuke (57226005412); Sugiura, Komei (55615122900); Imai, Michita (7402062745)","57205301318; 57205297667; 57226005412; 55615122900; 7402062745","LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation","2021","IEEE Access","9","","","160521","160532","11","10.1109/ACCESS.2021.3129215","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120079066&doi=10.1109%2fACCESS.2021.3129215&partnerID=40&md5=d902172f1bfe33f485e64967348cc995","Text-guided image manipulation tasks have recently gained attention in the vision-and-language community. While most of the prior studies focused on single-turn manipulation, our goal in this paper is to address the more challenging multi-turn image manipulation (MTIM) task. Previous models for this task successfully generate images iteratively, given a sequence of instructions and a previously generated image. However, this approach suffers from under-generation and a lack of generated quality of the objects that are described in the instructions, which consequently degrades the overall performance. To overcome these problems, we present a novel architecture called a Visually Guided Language Attention GAN (LatteGAN). Here, we address the limitations of the previous approaches by introducing a Visually Guided Language Attention (Latte) module, which extracts fine-grained text representations for the generator, and a Text-Conditioned U-Net discriminator architecture, which discriminates both the global and local representations of fake or real images. Extensive experiments on two distinct MTIM datasets, CoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the proposed model. The code is available online (https://github.com/smatsumori/LatteGAN). © 2013 IEEE.","Computer vision; Job analysis; Network architecture; Semantics; Features extraction; Generative adversarial network; Generator; Guided images; Image manipulation; Images synthesis; Manipulation task; Multi-turn; Multi-turn text-conditioned image manipulation; Task analysis; Generative adversarial networks","Generative adversarial network (GAN); multi-turn text-conditioned image manipulation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120079066"
"Peng D.; Yang W.; Liu C.; Lü S.","Peng, Dunlu (8556510900); Yang, Wuchen (57222044805); Liu, Cong (57138870000); Lü, Shuairui (57222042538)","8556510900; 57222044805; 57138870000; 57222042538","SAM-GAN: Self-Attention supporting Multi-stage Generative Adversarial Networks for text-to-image synthesis","2021","Neural Networks","138","","","57","67","10","10.1016/j.neunet.2021.01.023","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101094906&doi=10.1016%2fj.neunet.2021.01.023&partnerID=40&md5=b8bf8bac52a24050b6b5555a58354b38","Synthesizing photo-realistic images based on text descriptions is a challenging task in the field of computer vision. Although generative adversarial networks have made significant breakthroughs in this task, they still face huge challenges in generating high-quality visually realistic images consistent with the semantics of text. Generally, existing text-to-image methods accomplish this task with two steps, that is, first generating an initial image with a rough outline and color, and then gradually yielding the image within high-resolution from the initial image. However, one drawback of these methods is that, if the quality of the initial image generation is not high, it is hard to generate a satisfactory high-resolution image. In this paper, we propose SAM-GAN, Self-Attention supporting Multi-stage Generative Adversarial Networks, for text-to-image synthesis. With the self-attention mechanism, the model can establish the multi-level dependence of the image and fuse the sentence- and word-level visual-semantic vectors, to improve the quality of the generated image. Furthermore, a multi-stage perceptual loss is introduced to enhance the semantic similarity between the synthesized image and the real image, thus enhancing the visual-semantic consistency between text and images. For the diversity of the generated images, a mode seeking regularization term is integrated into the model. The results of extensive experiments and ablation studies, which were conducted in the Caltech-UCSD Birds and Microsoft Common Objects in Context datasets, show that our model is superior to competitive models in text-to-image synthesis. © 2021 Elsevier Ltd","Color; Image Processing, Computer-Assisted; Neural Networks, Computer; Semantics; Semantics; Adversarial networks; Attention mechanisms; Competitive models; High resolution image; Photorealistic images; Regularization terms; Semantic similarity; Synthesized images; article; attention; bird; machine learning; nonhuman; synthesis; color; image processing; procedures; semantics; Image enhancement","Machine learning; SAM-GAN; Self-attention mechanism; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85101094906"
"Qiao X.; Han Y.; Wu Y.; Zhang Z.","Qiao, Xing (57654812100); Han, Yanghong (57392332300); Wu, Yan (57336924900); Zhang, Zili (57208133059)","57654812100; 57392332300; 57336924900; 57208133059","Progressive Text-to-Face Synthesis with Generative Adversarial Network","2021","Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021","","","","","","","10.1109/FG52635.2021.9667004","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125043591&doi=10.1109%2fFG52635.2021.9667004&partnerID=40&md5=d9dc0bb4a329dd218511326ad2e4c19d","Text-to-Face synthesis has considerable challenges and potentials in the field of public safety. Compared with the Text-to-Image synthesis models, the text descriptions of facial features are more complex and diverse. For the text embedding, most of the previous Text-to-Face synthesis models only deal with a single sentence containing several features of face images, and the generated images are vague and lack of details. In this paper, a novel Progressive Text-to-Face synthesis with Generative Adversarial Network (PFGAN) is proposed to generate natural face images from text descriptions. Firstly, a new text encoding method Convolution-Deconvolution Word Embedding LSTM (CDWE-BLSTM) is leveraged as the text encoder, which tackles more complex sentences and improves the accuracy of text encoding. Secondly, the PFGAN is composed of multiple generators and discriminators arranged in a tree-like structure. Furthermore, face images at multiple scales are progressively generated from different branches of the tree, corresponding to the same descriptions. images at multiple scales corresponding to the same scene are generated from different branches of the tree. By comparing with three existing Text-to-Face synthesis methods, extensive experiments demonstrate that the proposed PFGAN is very competitive in the IS (Inception Scores), FID (Frechet Inception Distance) and resolution of the generated face images. © 2021 IEEE.","Complex networks; Embeddings; Encoding (symbols); Long short-term memory; Signal encoding; Embeddings; Encoding methods; Face images; Face synthesis; Facial feature; Images synthesis; Multiple scale; Public safety; Synthesis models; Text encoding; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85125043591"
"Zhao Q.; Zheng Z.; Zeng H.; Yu Z.; Zheng H.; Zheng B.","Zhao, Qi (57688383000); Zheng, Ziqiang (57202612793); Zeng, Huimin (57220805373); Yu, Zhibin (36999020600); Zheng, Haiyong (24922273300); Zheng, Bing (35304360700)","57688383000; 57202612793; 57220805373; 36999020600; 24922273300; 35304360700","The Synthesis of Unpaired Underwater Images for Monocular Underwater Depth Prediction","2021","Frontiers in Marine Science","8","","690962","","","","10.3389/fmars.2021.690962","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116330369&doi=10.3389%2ffmars.2021.690962&partnerID=40&md5=60cbb397d6f6b061677da19e364f0cc6","Underwater depth prediction plays an important role in underwater vision research. Because of the complex underwater environment, it is extremely difficult and expensive to obtain underwater datasets with reliable depth annotation. Thus, underwater depth map estimation with a data-driven manner is still a challenging task. To tackle this problem, we propose an end-to-end system including two different modules for underwater image synthesis and underwater depth map estimation, respectively. The former module aims to translate the hazy in-air RGB-D images to multi-style realistic synthetic underwater images while retaining the objects and the structural information of the input images. Then we construct a semi-real RGB-D underwater dataset using the synthesized underwater images and the original corresponding depth maps. We conduct supervised learning to perform depth estimation through the pseudo paired underwater RGB-D images. Comprehensive experiments have demonstrated that the proposed method can generate multiple realistic underwater images with high fidelity, which can be applied to enhance the performance of monocular underwater image depth estimation. Furthermore, the trained depth estimation model can be applied to real underwater image depth map estimation. We will release our codes and experimental setting in https://github.com/ZHAOQIII/UW_depth. © Copyright © 2021 Zhao, Zheng, Zeng, Yu, Zheng and Zheng.","","generative adversarial network; image-to-image translation; underwater depth map estimation; underwater image translation; underwater vision","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116330369"
"Saqlain A.S.; Fang F.; Ahmad T.; Wang L.; Abidin Z.-U.","Saqlain, Ali Syed (57224473435); Fang, Fang (55624485445); Ahmad, Tanvir (57214283388); Wang, Liyun (57215302242); Abidin, Zain-Ul (57329562300)","57224473435; 55624485445; 57214283388; 57215302242; 57329562300","Evolution and effectiveness of loss functions in generative adversarial networks","2021","China Communications","18","10","","45","76","31","10.23919/JCC.2021.10.004","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118837713&doi=10.23919%2fJCC.2021.10.004&partnerID=40&md5=86e40c14e1a7c63a5203791ff5aa3a88","Recently, the evolution of Generative Adversarial Networks (GANs) has embarked on a journey of revolutionizing the field of artificial and computational intelligence. To improve the generating ability of GANs, various loss functions are introduced to measure the degree of similarity between the samples generated by the generator and the real data samples, and the effectiveness of the loss functions in improving the generating ability of GANs. In this paper, we present a detailed survey for the loss functions used in GANs, and provide a critical analysis on the pros and cons of these loss functions. First, the basic theory of GANs along with the training mechanism are introduced. Then, the most commonly used loss functions in GANs are introduced and analyzed. Third, the experimental analyses and comparison of these loss functions are presented in different GAN architectures. Finally, several suggestions on choosing suitable loss functions for image synthesis tasks are given.  © 2013 China Institute of Communications.","","deep learning; generative adversarial networks (GANs); image synthesis; loss functions; machine learning; unsupervised learning","Article","Final","","Scopus","2-s2.0-85118837713"
"Nasr A.; Mutasim R.; Imam H.","Nasr, Ammar (57221323024); Mutasim, Ruba (57224170684); Imam, Hiba (57224168490)","57221323024; 57224170684; 57224168490","SemGAN: Text to Image Synthesis from Text Semantics using Attentional Generative Adversarial Networks","2021","Proceedings of: 2020 International Conference on Computer, Control, Electrical, and Electronics Engineering, ICCCEEE 2020","","","9429602","","","","10.1109/ICCCEEE49695.2021.9429602","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107112854&doi=10.1109%2fICCCEEE49695.2021.9429602&partnerID=40&md5=149fecbbe06a05d8133072213f782eb7","Text to Image Synthesis is the procedure of automatically creating a realistic image from a particular text description. There are numerous innovative and practical applications for text to image synthesis, including image processing and compute-raided design. Using Generative Adversarial Networks (GANs) alongside the Attention mechanism has led to huge improvements lately. The fine-grained attention mechanism, although powerful, does not preserve the general description information well in the generator since it only attends to the text description at word-level (fine-grained). We propose incorporating the whole sentence semantics when generating images from captions to enhance the attention mechanism outputs. According to experiments, on our model produces more robust images with a better semantic layout. We use the Caltech birds dataset to run experiments on both models and validate the effectiveness of our proposal. Our model boosts the original AttnGAN Inception score by +4.13% and the Fréchet Inception Distance score by +13.93%. Moreover, an empirical analysis is carried out on the objective and subjective measures to: (i) address and overcome the limitations of these metrics (ii) verify that performance improvements are due to fundamental algorithmic changes rather than initialization and fine-Tuning as with GANs models. © 2021 IEEE.","Semantic Web; Semantics; Adversarial networks; Attention mechanisms; Empirical analysis; Fine grained; General description; Image synthesis; Objective and subjective measures; Realistic images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85107112854"
"Schulze H.; Yaman D.; Waibel A.","Schulze, Henning (57219305507); Yaman, Dogucan (57195221738); Waibel, Alexander (35575129700)","57219305507; 57195221738; 35575129700","CAGAN: Text-To-Image Generation with Combined Attention Generative Adversarial Networks","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13024 LNCS","","","392","404","12","10.1007/978-3-030-92659-5_25","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124254940&doi=10.1007%2f978-3-030-92659-5_25&partnerID=40&md5=052b60feaf39878059a46512bd1c82d3","Generating images according to natural language descriptions is a challenging task. Prior research has mainly focused to enhance the quality of generation by investigating the use of spatial attention and/or textual attention thereby neglecting the relationship between channels. In this work, we propose the Combined Attention Generative Adversarial Network (CAGAN) to generate photo-realistic images according to textual descriptions. The proposed CAGAN utilises two attention models: word attention to draw different sub-regions conditioned on related words; and squeeze-and-excitation attention to capture non-linear interaction among channels. With spectral normalisation to stabilise training, our proposed CAGAN achieves state-of-the-art FID and comparative IS scores on the CUB dataset and on the more challenging COCO dataset. Furthermore, we demonstrate that judging a model by a single evaluation metric can be misleading by developing an additional model adding local self-attention which scores a higher IS than our other model, but generates unrealistic images through feature repetition. © 2021, Springer Nature Switzerland AG.","Computer vision; Computers; Attention; Generative adversarial network; Image generations; Images synthesis; Language description; Natural languages; Photorealistic images; Spatial attention; Text-to-image synthesis; Textual description; Generative adversarial networks","Attention; Generative adversarial network (GAN); Text-to-image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85124254940"
"Bejiga M.B.; Hoxha G.; Melgani F.","Bejiga, Mesay Belete (57192697078); Hoxha, Genc (57213198314); Melgani, Farid (35613488300)","57192697078; 57213198314; 35613488300","Improving Text Encoding for Retro-Remote Sensing","2021","IEEE Geoscience and Remote Sensing Letters","18","4","9066830","622","626","4","10.1109/LGRS.2020.2983851","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103405336&doi=10.1109%2fLGRS.2020.2983851&partnerID=40&md5=191c3564bbabf57658d20c378cee4ded","A recent work on retro-remote sensing (converting ancient text descriptions into images) was proposed using a multilabel encoding scheme in which an input text description is represented by a binary vector indicating the presence or absence of specific objects. However, this kind of encoding disregards information such as object attributes and spatial relationship between multiple objects in a description, resulting in images that do not semantically (fully) conform to the input description. In this letter, we propose an improved text-encoding mechanism that takes into account different levels of information available from an input text. The encoded text is then used as conditional information to guide the image synthesis process using generative adversarial networks (GANs). Besides, we present a modified GAN architecture intending to improve the semantic content of the generated images. Both the qualitative and quantitative results obtained indicate that the proposed method is particularly promising. © 2004-2012 IEEE.","Encoding (symbols); Image coding; Remote sensing; Semantics; Signal encoding; Adversarial networks; Encoding schemes; Image synthesis; Multiple objects; Object attributes; Quantitative result; Semantic content; Spatial relationships; image analysis; remote sensing; satellite imagery; spatial analysis; Image enhancement","Generative adversarial networks (GANs); multimodal learning; retro-remote sensing; text-To-image synthesis","Article","Final","","Scopus","2-s2.0-85103405336"
"Lin J.; Zhang R.; Ganz F.; Han S.; Zhu J.-Y.","Lin, Ji (57200618213); Zhang, Richard (57223101349); Ganz, Frieder (38361352200); Han, Song (56697675800); Zhu, Jun-Yan (56316642900)","57200618213; 57223101349; 38361352200; 56697675800; 56316642900","Anycost GANs for interactive image synthesis and editing","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","14981","14991","10","10.1109/CVPR46437.2021.01474","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112386214&doi=10.1109%2fCVPR46437.2021.01474&partnerID=40&md5=dffa891811f5b4d9bf4c944ef3a182cb","Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing. However, due to the high computational cost of large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the results of a single edit on edge devices, prohibiting interactive user experience. In this paper, inspired by quick preview features in modern rendering software, we propose Anycost GAN for interactive natural image editing. We train the Anycost GAN to support elastic resolutions and channels for faster image generation at versatile speeds. Running subsets of the full generator produce outputs that are perceptually similar to the full generator, making them a good proxy for quick preview. By using sampling-based multi-resolution training, adaptive-channel training, and a generator-conditioned discriminator, the anycost generator can be evaluated at various configurations while achieving better image quality compared to separately trained models. Furthermore, we develop new encoder training and latent code optimization techniques to encourage consistency between the different sub-generators during image projection. Anycost GAN can be executed at various cost budgets (up to 10× computation reduction) and adapt to a wide range of hardware and latency requirements. When deployed on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12× speedup, enabling interactive image editing. The code and demo are publicly available. © 2021 IEEE","Budget control; Codes (symbols); Program processors; Computational costs; Image editing; Image generations; Images synthesis; Interactive images; Large-scales; Natural images; Photorealistic images; Sampling-based; Users' experiences; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112386214"
"Zhou R.; Jiang C.; Xu Q.","Zhou, Rui (57223435415); Jiang, Cong (57223937568); Xu, Qingyang (55482776600)","57223435415; 57223937568; 55482776600","A survey on generative adversarial network-based text-to-image synthesis","2021","Neurocomputing","451","","","316","336","20","10.1016/j.neucom.2021.04.069","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106508076&doi=10.1016%2fj.neucom.2021.04.069&partnerID=40&md5=db9e7d208684b39b79b37db854ec0ed7","The task of text-to-image synthesis is a new challenge in the field of image synthesis. In the earlier research, the task of text-to-image synthesis is mainly to achieve the alignment of words and images by the way of retrieval based on the sentences or keywords. With the development of deep learning, especially the application of deep generative models in image synthesis, image synthesis achieves promising progress. The Generative adversarial networks (GANs) are one of the most significant generative models, and GANs have been successfully applied in computer vision, natural language processing and so on. In this paper, we review and summarize the recent research in GANs-based text-to-image synthesis, and provide a summary of the development of classic and advanced models. The input of the GANs-based text-to-image synthesis is not only the general text description as earlier studies, also includes scene layout and dialog text. The typical structure of each categories is elaborated. The general text-based image synthesis is the most commonly in the text-to-image synthesis, and it is subdivided into three groups based on the improvements of text information utilization, network structure and output control conditions. Through the survey, the detailed and logical overview of the evolution of GANs-based text-to-image synthesis is presented. Finally, the challenged problems and the future development of text-to-image synthesis are discussed. © 2021 Elsevier B.V.","Deep learning; Natural language processing systems; Surveys; Adversarial networks; Deep learning; Generative adversarial network; Generative model; Images synthesis; Language processing; Natural languages; Network-based; Scene layout; Text-to-image synthesis; article; computer vision; deep learning; natural language processing; synthesis; Image enhancement","Deep learning; Generative adversarial network (GAN); Scene layout; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85106508076"
"Liu M.; Li Q.; Qin Z.; Zhang G.; Wan P.; Zheng W.","Liu, Mingcong (57322194300); Li, Qiang (57215778360); Qin, Zekui (57215187390); Zhang, Guoxin (57208444845); Wan, Pengfei (57221702931); Zheng, Wen (57215778104)","57322194300; 57215778360; 57215187390; 57208444845; 57221702931; 57215778104","BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation","2021","Advances in Neural Information Processing Systems","35","","","29710","29722","12","","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129996306&partnerID=40&md5=d0fa4734830db823c8055740a68de10f","Generative Adversarial Networks (GANs) have made a dramatic leap in high-fidelity image synthesis and stylized face generation. Recently, a layer-swapping mechanism has been developed to improve the stylization performance. However, this method is incapable of fitting arbitrary styles in a single model and requires hundreds of style-consistent training images for each style. To address the above issues, we propose BlendGAN for arbitrary stylized face generation by leveraging a flexible blending strategy and a generic artistic dataset. Specifically, we first train a self-supervised style encoder on the generic artistic dataset to extract the representations of arbitrary styles. In addition, a weighted blending module (WBM) is proposed to blend face and style representations implicitly and control the arbitrary stylization effect. By doing so, BlendGAN can gracefully fit arbitrary styles in a unified model while avoiding case-by-case preparation of style-consistent training images. To this end, we also present a novel large-scale artistic face dataset AAHQ. Extensive experiments demonstrate that BlendGAN outperforms state-of-the-art methods in terms of visual quality and style diversity for both latent-guided and reference-guided stylized face synthesis. Our project webpage is https://onionliu.github.io/BlendGAN/ © 2021 Neural information processing systems foundation. All rights reserved.","HTTP; Large dataset; Face generation; High-fidelity images; Images synthesis; Large-scales; Performance; Single models; State-of-the-art methods; Training image; Unified Modeling; Visual qualities; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85129996306"
"Cao G.; Liu S.; Mao H.; Zhang S.","Cao, Guogang (57193384068); Liu, Shunkun (57222115156); Mao, Hongdong (57222109935); Zhang, Shu (57212232484)","57193384068; 57222115156; 57222109935; 57212232484","Improved CyeleGAN for MR to CT synthesis","2021","ICIIBMS 2021 - 6th International Conference on Intelligent Informatics and Biomedical Sciences","","","","205","208","3","10.1109/ICIIBMS52876.2021.9651571","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124344968&doi=10.1109%2fICIIBMS52876.2021.9651571&partnerID=40&md5=b4d656f3677aeb9d56e2e60f6cf71419","Radiotherapy treatment planning requires CT images to accurately calculate the dose distribution, but sometimes only MR images can be obtained, therefore it is necessary to generate CT images from its corresponding MR images. In order to synthesize quality pseudo CT images, an improved cycle generative adversarial network (CycleGAN) with residual network (ResNet) and U-Net was proposed. The improved CycleGAN was trained by unpaired data to get better result, and approved by the cycle inconsistency of CycleGAN, the slow-down gradient disappearance of ResNet and the multi-scale fusion of U-Nets. Avoiding the disappearance of input information and the vanishing of gradient information, the improved network synthesized more quality CT images. Compared with the original CycleGAN method, the MAE of the proposed method was reduced by 5.12%, the SSIM increased by 0.7% and the PSNR increased by 1.29%, which was trained and tested on a dataset of 18 patients. Additional, compared with the DCNN method, the atlas-based method, and the pix2pix method, the relative error of MAE was reduced by 1.73%, 2.215% and 0.228% respectively. The proposed method synthesizes more vivid CT images owing to the advantages of deep learning model, which better meets the requirements of clinical analysis.  © 2021 IEEE.","Computerized tomography; Deep learning; Image enhancement; Magnetic resonance imaging; Medical imaging; CT Image; Cycle generative adversarial network; Dose distributions; Gradient informations; Images synthesis; MR-images; Multiscale fusion; Radiotherapy treatment planning; Residual network; U-net; Generative adversarial networks","CycleGAN; Image synthesis; ResNet; U-Net","Conference paper","Final","","Scopus","2-s2.0-85124344968"
"Ma L.; Huang K.; Wei D.; Ming Z.; Shen H.","Ma, Liyuan (57370602600); Huang, Kejie (54581052800); Wei, Dongxu (57219750324); Ming, Zhao-Yan (36447695600); Shen, Haibin (13309317500)","57370602600; 54581052800; 57219750324; 36447695600; 13309317500","FDA-GAN: Flow-based Dual Attention GAN for Human Pose Transfer","2021","IEEE Transactions on Multimedia","","","","","","","10.1109/TMM.2021.3134157","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121362211&doi=10.1109%2fTMM.2021.3134157&partnerID=40&md5=6481164d91e132e94d296c4c01cbb963","Human pose transfer aims at transferring the appearance of the source person to the target pose. Existing methods utilizing flow-based warping for non-rigid human image generation have achieved great success. However, they fail to preserve the appearance details in synthesized images since the spatial correlation between the source and target is not fully exploited. To this end, we propose the Flow-based Dual Attention GAN (FDA-GAN) to apply occlusion- and deformation-aware feature fusion for higher generation quality. Specifically, deformable local attention and flow similarity attention, constituting the dual attention mechanism, can derive the output features responsible for deformable- and occlusion-aware fusion, respectively. Besides, to maintain the pose and global position consistency in transferring, we design a pose normalization network for learning adaptive normalization from the target pose to the source person. Both qualitative and quantitative results show that our method outperforms state-of-the-art models in public iPER and DeepFashion datasets. IEEE","","Feature extraction; Generative adversarial networks; Generative Adversarial Networks(GANs); Image Synthesis; Image synthesis; Pose Transfer; Solid modeling; Strain; Task analysis; Three-dimensional displays","Article","Article in press","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85121362211"
"Chan E.R.; Monteiro M.; Kellnhofer P.; Wu J.; Wetzstein G.","Chan, Eric R. (57219751908); Monteiro, Marco (57221630271); Kellnhofer, Petr (55250016000); Wu, Jiajun (55553991400); Wetzstein, Gordon (24462821700)","57219751908; 57221630271; 55250016000; 55553991400; 24462821700","pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","5795","5805","10","10.1109/CVPR46437.2021.00574","91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123206447&doi=10.1109%2fCVPR46437.2021.00574&partnerID=40&md5=5c79f39c4b233ee0345ee43049097af0","We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches however fall short in two ways: first, they may lack an underlying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (π-GAN or pi-GAN), for high-quality 3D-aware image synthesis. π-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets. © 2021 IEEE","Computer vision; Network architecture; Rendering (computer graphics); Three dimensional computer graphics; 3d representations; Generative model; High quality; Images synthesis; Multi-views; Neural representations; Periodic activation functions; Two ways; Visual model; Volumetric rendering; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123206447"
"Cheng J.; Wang Z.; Liu Z.; Feng Z.; Wang H.; Pan X.","Cheng, Jijun (57439155700); Wang, Zimin (35216658000); Liu, Zhenbing (24080876300); Feng, Zhengyun (57222090190); Wang, Huadeng (35185576800); Pan, Xipeng (55973002800)","57439155700; 35216658000; 24080876300; 57222090190; 35185576800; 55973002800","Deep Adversarial Image Synthesis for Nuclei Segmentation of Histopathology Image","2021","Proceedings - 2021 2nd Asia Conference on Computers and Communications, ACCC 2021","","","","63","68","5","10.1109/ACCC54619.2021.00017","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125645769&doi=10.1109%2fACCC54619.2021.00017&partnerID=40&md5=22ff27f66736b499281d99d17587bf28","Nuclei segmentation is a fundamental upstream task of digital pathology image analysis. Existing nuclei segmentation methods usually require pixel-level labeled images from experienced pathologists. In this paper, we proposed an innovative data augmentation workflow for histopathology images: a) generates a set of initial central points randomly with existing human-annotated histopathology image datasets; b) generates nuclei segmentation masks based on the generated centroid points of step a); c) generates Haematoxylin and Eosin (HE)-stained histopathology images corresponding to the generated nuclei masks. In addition, we proposed a deep attention feature fusion generative adversarial network (DAFF-GAN) to improve the image quality and the photorealism of the generated image. We conducted extensive experiments on several existing nuclei segmentation methods, comparing using raw data with the augmented data by our strategy. Extensive experiments proved the effectiveness of our proposed strategy.  © 2021 IEEE.","Image enhancement; Image segmentation; Medical imaging; Data augmentation; Digital pathologies; GAN; Histopathology image; Image-analysis; Images synthesis; Labeled images; Nucleus segmentation; Pixel level; Segmentation methods; Generative adversarial networks","data augmentation; GAN; histopathology image; nuclei segmentation","Conference paper","Final","","Scopus","2-s2.0-85125645769"
"","","","43rd DAGM German Conference on Pattern Recognition, DAGM GCPR 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13024 LNCS","","","","","724","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124255688&partnerID=40&md5=80f2d2283daf27f975494fc5681a7bbf","The proceedings contain 46 papers. The special focus in this conference is on Pattern Recognition. The topics include: Sublabel-Accurate Multilabeling Meets Product Label Spaces; investigating the Consistency of Uncertainty Sampling in Deep Active Learning; scaleNet: An Unsupervised Representation Learning Method for Limited Information; a New Split for Evaluating True Zero-Shot Action Recognition; video Instance Segmentation with Recurrent Graph Neural Networks; distractor-Aware Video Object Segmentation; (SP)2 Net for Generalized Zero-Label Semantic Segmentation; contrastive Representation Learning for Hand Shape Estimation; Fusion-GCN: Multimodal Action Recognition Using Graph Convolutional Networks; FIFA: Fast Inference Approximation for Action Segmentation; Hybrid SNN-ANN: Energy-Efficient Classification and Object Detection for Event-Based Vision; infoSeg: Unsupervised Semantic Image Segmentation with Mutual Information Maximization; a Comparative Study of PnP and Learning Approaches to Super-Resolution in a Real-World Setting; Merging-ISP: Multi-exposure High Dynamic Range Image Signal Processing; spatiotemporal Outdoor Lighting Aggregation on Image Sequences; AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style; learning Conditional Invariance Through Cycle Consistency; CAGAN: Text-To-Image Generation with Combined Attention Generative Adversarial Networks; txT: Crossmodal End-to-End Learning with Transformers; diverse Image Captioning with Grounded Style; leveraging Group Annotations in Object Detection Using Graph-Based Pseudo-labeling; quantifying Uncertainty of Image Labelings Using Assignment Flows; sampling-Free Variational Inference for Neural Networks with Multiplicative Activation Noise; implicit and Explicit Attention for Zero-Shot Learning; self-supervised Learning for Object Detection in Autonomous Driving; Assignment Flows and Nonlocal PDEs on Graphs; viewpoint-Tolerant Semantic Segmentation for Aerial Logistics; t6D-Direct: Transformers for Multi-object 6D Pose Direct Regression; tetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases; detecting Slag Formations with Deep Convolutional Neural Networks; virtual Temporal Samples for Recurrent Neural Networks: Applied to Semantic Segmentation in Agriculture; weakly Supervised Segmentation Pretraining for Plant Cover Prediction; how Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?.","","","Conference review","Final","","Scopus","2-s2.0-85124255688"
"Toda R.; Teramoto A.; Tsujimoto M.; Toyama H.; Imaizumi K.; Saito K.; Fujita H.","Toda, Ryo (57218321874); Teramoto, Atsushi (15060820200); Tsujimoto, Masakazu (56985412300); Toyama, Hiroshi (55708721400); Imaizumi, Kazuyoshi (7202367813); Saito, Kuniaki (7406507233); Fujita, Hiroshi (22634241800)","57218321874; 15060820200; 56985412300; 55708721400; 7202367813; 7406507233; 22634241800","Synthetic CT image generation of shape-controlled lung cancer using semi-conditional InfoGAN and its applicability for type classification","2021","International Journal of Computer Assisted Radiology and Surgery","16","2","","241","251","10","10.1007/s11548-021-02308-1","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099383803&doi=10.1007%2fs11548-021-02308-1&partnerID=40&md5=2b495f87c4e7bd41a1f366b323c900bc","Purpose: In recent years, convolutional neural network (CNN), an artificial intelligence technology with superior image recognition, has become increasingly popular and frequently used for classification tasks in medical imaging. However, the amount of labelled data available for classifying medical images is often significantly less than that of natural images, and the handling of rare diseases is often challenging. To overcome these problems, data augmentation has been performed using generative adversarial networks (GANs). However, conventional GAN cannot effectively handle the various shapes of tumours because it randomly generates images. In this study, we introduced semi-conditional InfoGAN, which enables some labels to be added to InfoGAN, for the generation of shape-controlled tumour images. InfoGAN is a derived model of GAN, and it can represent object features in images without any label. Methods: Chest computed tomography images of 66 patients diagnosed with three histological types of lung cancer (adenocarcinoma, squamous cell carcinoma, and small cell lung cancer) were used for analysis. To investigate the applicability of the generated images, we classified the histological types of lung cancer using a CNN that was pre-trained with the generated images. Results: As a result of the training, InfoGAN was possible to generate images that controlled the diameters of each lesion and the presence or absence of the chest wall. The classification accuracy of the pre-trained CNN was 57.7%, which was higher than that of the CNN trained only with real images (34.2%), thereby suggesting the potential of image generation. Conclusion: The applicability of semi-conditional InfoGAN for feature learning and representation in medical images was demonstrated in this study. InfoGAN can perform constant feature learning and generate images with a variety of shapes using a small dataset. © 2021, CARS.","Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Lung; Lung Neoplasms; Neural Networks, Computer; Radiography, Thoracic; Tomography, X-Ray Computed; Article; clinical classification; computer assisted tomography; computer graphics; controlled study; convolutional neural network; feature extraction; histology; histopathology; human; human tissue; image analysis; image display; image processing; image reconstruction; infogenerative adversarial network; lung adenocarcinoma; lung cancer; lung nodule; major clinical study; priority journal; small cell lung cancer; squamous cell lung carcinoma; thorax radiography; thorax wall; tumor volume; artificial intelligence; diagnostic imaging; lung; lung tumor; procedures; x-ray computed tomography","Classification; CNN; CT imaging; GAN; Image synthesis; Lung cancer","Article","Final","","Scopus","2-s2.0-85099383803"
"Khine W.S.S.; Siritanawan P.; Kotani K.","Khine, Win Shwe Sin (57219971870); Siritanawan, Prarinya (36186132800); Kotani, Kazunori (56038824900)","57219971870; 36186132800; 56038824900","Wasserstein Based Emogans+","2021","2021 Joint 10th International Conference on Informatics, Electronics and Vision, ICIEV 2021 and 2021 5th International Conference on Imaging, Vision and Pattern Recognition, icIVPR 2021","","","","","","","10.1109/ICIEVICIVPR52578.2021.9564216","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126209571&doi=10.1109%2fICIEVICIVPR52578.2021.9564216&partnerID=40&md5=421a4c9ecad54b357b6997ff795089dc","Nowadays, numerous generative models are powerful and becoming popular for image synthesis because their generated images are more and more similar to the actual images, especially, Generative Adversarial Networks (GANs) and their variants. Such generative models are helpful to cover the shortage of datasets in various areas with their impressive realistic looking generated images. In this paper, we proposed the EmoGANs+ model to create the compound facial expressions images with stable adversarial training by using Wasserstein loss. The proposed methodology consists of three steps: preprocessing, image generation with proposed EmoGANs+, and lastly, evaluation. Our experiments are conducted on the Multimedia Understanding Group (MUG) facial expression dataset, Extended Cohn-Kanade dataset (CK+), and Japanese Female Facial Expressions (JAFFE) dataset. Our proposed model provides high feature similarity scores between the features of generated images and ground-truth compound facial expressions images. Contribution-Our contribution is to provide the synthetic compound facial expressions images with proposed EmoGANs+ model for the purpose of covering dataset limitation in the compound emotion study. © 2021 IEEE","Computer vision; Computers; Compound emotion; Compound facial expression; Emotion generative adversarial network; Facial Expressions; Generative model; Image generations; Image's truths; Images synthesis; Pre-processing image; Similarity scores; Generative adversarial networks","Compound Emotions; Compound Facial Expressions; Emotion Generative Adversarial Networks","Conference paper","Final","","Scopus","2-s2.0-85126209571"
"Da Silva Costa D.; Moura P.N.; Garcia A.C.B.","Da Silva Costa, Daniel (57445704700); Moura, Pedro Nuno (57444837400); Garcia, Ana Cristina Bicharra (25723096500)","57445704700; 57444837400; 25723096500","Improving human perception of GAN generated facial image synthesis by filtering the training set considering facial attributes","2021","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","","","","100","106","6","10.1109/SMC52423.2021.9659033","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124276701&doi=10.1109%2fSMC52423.2021.9659033&partnerID=40&md5=c3ff3f0d7020bb0c91da09f3f8640203","Data anonymization methods have been investigated as strategies to provide data privacy and to minimize prejudices against minorities. Recent works on machine learning have shown impressive results in generating new examples of images using Generative Adversarial Networks (GAN). Creating synthetic facial images that resembles a group, but no one in particular seem an interesting strategy to anonymize a face. Nevertheless, the image synthesis process is computationally expensive and requires a very large training dataset to produce a human-acceptable outcome. This paper presents an investigation on the trade-off between the required number of instances (size) and the images' inter-variability of a training set for using Deep Convolutional Generative Adversarial Network (DCGAN) to generate human-acceptable synthetic images. We report an experiment using the Appen crowdsourcing platform to evaluate the human acceptance of the synthetic images generated by our DCGAN trained using different samples of the CelebA public dataset, taken according to facial attributes, such as cheekbones protuberance, face shape and eyebrow thickness. The results indicate that (1) Facial attributes have a significant effect on diminishing the images' inter-variability, specially face shape; (2) smaller images' inter-variability leads to smaller required training dataset and (3) filtering the DCGAN's training datasets using oval face shape led to a required dataset almost 50% lesser than without filtering for generating images humanly acceptable. We believe this result may help individuals to anonymize their visual data maintaining certain characteristics but keeping their privacy at a lower computational cost.  © 2021 IEEE.","Computer vision; Data privacy; Economic and social effects; Image enhancement; Large dataset; Data anonymization; Face shapes; Facial Image synthesis; Facial images; Human perception; Synthesis process; Synthetic images; Training dataset; Training sets; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85124276701"
"Ul Hassan A.; Ahmed H.; Choi J.","Ul Hassan, Ammar (57210801858); Ahmed, Hammad (57211585581); Choi, Jaeyoung (56812522400)","57210801858; 57211585581; 56812522400","Unpaired font family synthesis using conditional generative adversarial networks[Formula presented]","2021","Knowledge-Based Systems","229","","107304","","","","10.1016/j.knosys.2021.107304","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111172977&doi=10.1016%2fj.knosys.2021.107304&partnerID=40&md5=9995ba92007d7fc5067366a0619dac9f","Automatic font image synthesis has been an extremely active topic in recent years. Various deep learning-based approaches have been proposed to tackle this font synthesis task by considering it as an image-to-image translation problem in a supervised setting. However, all such approaches mainly focus on one-to-one font mapping, i.e., synthesizing a single font style, making it difficult to handle more practical problems such as the font family synthesis, which is a one-to-many mapping problem. Moreover, this font family synthesis is more challenging because it is an unsupervised image-to-image translation problem, i.e., no paired dataset is available during training. To address this font family synthesis problem, we propose a method that utilizes a single generator to conditionally produce various font family styles to form a font family. To the best of our knowledge, our proposed method is the first to synthesize a font family (multiple font styles belonging to a font), instead of synthesizing a single font style. More specifically, our method is trained to learn a font family by conditioning on various styles, e.g., normal, bold, italic, bold-italic, etc. After training, given an unobserved single font style (normal style font as an input), our method can successfully synthesize the remaining styles (e.g., bold, italic, bold-italic, etc.) to complete the font family. Qualitative and quantitative experiments were conducted to demonstrate the effectiveness of our proposed method. © 2021 Elsevier B.V.","Deep learning; Image processing; Adversarial networks; Font generation; Font images; Generative adversarial network; Image translation; Images synthesis; Learning-based approach; Practical problems; Style transfer; Unsupervised image-to-image translation; Mapping","Font generation; Generative adversarial networks; Style transfer; Unsupervised image-to-image translation","Article","Final","","Scopus","2-s2.0-85111172977"
"Xu Y.; Shen Y.; Zhu J.; Yang C.; Zhou B.","Xu, Yinghao (57219692967); Shen, Yujun (57207766466); Zhu, Jiapeng (57215047797); Yang, Ceyuan (57204283344); Zhou, Bolei (36697366200)","57219692967; 57207766466; 57215047797; 57204283344; 36697366200","Generative Hierarchical Features from Synthesizing Images","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","4430","4440","10","10.1109/CVPR46437.2021.00441","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123214800&doi=10.1109%2fCVPR46437.2021.00441&partnerID=40&md5=4b3309970afbb5a471cb47fab336115e","Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying distribution of the observed data. However, how the features learned from solving the task of image generation are applicable to other vision tasks remains seldom explored. In this work, we show that learning to synthesize images can bring remarkable hierarchical visual features that are generalizable across a wide range of applications. Specifically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise representation to train a novel hierarchical encoder. The visual feature produced by our encoder, termed as Generative Hierarchical Feature (GH-Feat), has strong transferability to both generative and discriminative tasks, including image editing, image harmonization, image classification, face verification, landmark detection, and layout prediction. Extensive qualitative and quantitative experimental results demonstrate the appealing performance of GH-Feat. © 2021 IEEE","Computer vision; Image representation; Signal encoding; Harmonisation; Hierarchical features; Image editing; Image generations; Images synthesis; Layer-wise; Loss functions; Observed data; Underlying distribution; Visual feature; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123214800"
"Li T.; Zhang W.; Song R.; Li Z.; Liu J.; Li X.; Lu S.","Li, Tianjiao (57220804744); Zhang, Wei (56119460300); Song, Ran (36716771200); Li, Zhiheng (57221053711); Liu, Jun (55264328500); Li, Xiaolei (57192490838); Lu, Shijian (8439329200)","57220804744; 56119460300; 36716771200; 57221053711; 55264328500; 57192490838; 8439329200","PoT-GAN: Pose Transform GAN for Person Image Synthesis","2021","IEEE Transactions on Image Processing","30","","9524559","7677","7688","11","10.1109/TIP.2021.3104183","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114640380&doi=10.1109%2fTIP.2021.3104183&partnerID=40&md5=8b60cc2dae76eaba22da0de506d0632c","Pose-based person image synthesis aims to generate a new image containing a person with a target pose conditioned on a source image containing a person with a specified pose. It is challenging as the target pose is arbitrary and often significantly differs from the specified source pose, which leads to large appearance discrepancy between the source and the target images. This paper presents the Pose Transform Generative Adversarial Network (PoT-GAN) for person image synthesis where the generator explicitly learns the transform between the two poses by manipulating the corresponding multi-scale feature maps. By incorporating the learned pose transform information into the multi-scale feature maps of the source image in a GAN architecture, our method reliably transfers the appearance of the person in the source image to the target pose with no need for any hard-coded spatial information depicting the change of pose. According to both qualitative and quantitative results, the proposed PoT-GAN demonstrates a state-of-the-art performance on three publicly available datasets for person image synthesis.  © 1992-2012 IEEE.","Algorithms; Humans; Image Processing, Computer-Assisted; Image processing; Feature map; Images synthesis; Learn+; Multi-scale features; Pose transform; Quantitative result; Source images; Spatial informations; State-of-the-art performance; Target images; algorithm; human; image processing; Generative adversarial networks","generative adversarial network; Image synthesis; pose transform","Article","Final","","Scopus","2-s2.0-85114640380"
"Zhou Y.","Zhou, Yutong (57215652189)","57215652189","Generative Adversarial Network for Text-to-Face Synthesis and Manipulation","2021","MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia","","","","2940","2944","4","10.1145/3474085.3481026","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119375597&doi=10.1145%2f3474085.3481026&partnerID=40&md5=d3639ff0d5abf7760c2026d05323119c","Over the past few years, several studies have been conducted on text-to-image synthesis techniques, which transfer input textual descriptions into realistic images. However, facial image synthesis and manipulation from input sentences have not been widely explored due to the lack of datasets. My research interests center around the development of multi-modality technology and facial image generation with Generative Adversarial Networks. Towards that end, we propose an approach for facial image generation and manipulation from text descriptions. We also introduce the first Text-to-Face synthesis dataset with large-scale facial attributes. In this extended abstract, we first present the existing condition and further direction of my Ph.D. research that I have followed during the first year. Then, we introduce the proposed method (accepted by IEEE FG2021), annotated novel dataset and experimental results. Finally, the future outlook on other challenges, proposed dataset and expected impact are discussed. Codes and paper lists studied in text-to-image synthesis are summarized on https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image. © 2021 ACM.","Abstracting; Computer vision; Large dataset; Dataset; Face synthesis; Facial images; Image generations; Image manipulation; Images synthesis; Synthesis techniques; Text-to-image synthesis; Textual description; Vision and language; Generative adversarial networks","datasets; generative adversarial network; text-to-image synthesis; vision and language","Conference paper","Final","","Scopus","2-s2.0-85119375597"
"Deng K.; Zhang K.; Yao P.; Cheng S.; He P.","Deng, Kai (57217296176); Zhang, Kun (57214938091); Yao, Ping (57206347584); Cheng, Siyuan (57261961400); He, Peng (57192956908)","57217296176; 57214938091; 57206347584; 57261961400; 57192956908","Skip attention GaN for remote sensing image synthesis","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","2305","2309","4","10.1109/ICASSP39728.2021.9414701","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115066093&doi=10.1109%2fICASSP39728.2021.9414701&partnerID=40&md5=41f0d63d2d07c053c46253fa30d878df","High-quality remote sensing images are difficult to obtain due to limited conditions and high cost for data acquisition. With the development of machine vision and deep learning, some image generation methods (e.g., GANs) are introduced into this field, but it's still hard to generate images with good texture details and structural dependencies. We establish Skip Attention Mechanism to deal with this problem, which learns dependencies between local points on low-resolution feature maps, and then upsample the attention map and combine it with high-resolution feature maps. With this method, long-range dependencies learned from low-resolution are used for generating remote sensing images with more structural details. We name this method as Skip Attention GAN, which is the first method applying cross-scale attention mechanism for unsupervised remote sensing image generation. Experiments show that our method outperforms previous methods under several metrics. Visual and ablation results of attention layers show that Skip Attention has learned long-distance structural dependencies between similar targets. © 2021 IEEE","Data acquisition; Deep learning; Gallium nitride; III-V semiconductors; Signal processing; Textures; Attention mechanisms; High quality; High resolution; Image generations; Long-range dependencies; Low resolution; Remote sensing images; Structural details; Remote sensing","Attention mechanism; Generative adversarial network; Remote sensing image synthesis","Conference paper","Final","","Scopus","2-s2.0-85115066093"
"Kalantar R.; Messiou C.; Winfield J.M.; Renn A.; Latifoltojar A.; Downey K.; Sohaib A.; Lalondrelle S.; Koh D.-M.; Blackledge M.D.","Kalantar, Reza (57216935371); Messiou, Christina (24067209000); Winfield, Jessica M. (56115716700); Renn, Alexandra (56054166800); Latifoltojar, Arash (55961544100); Downey, Kate (55586216200); Sohaib, Aslam (23104280100); Lalondrelle, Susan (26436394500); Koh, Dow-Mu (57211256625); Blackledge, Matthew D. (35336046600)","57216935371; 24067209000; 56115716700; 56054166800; 55961544100; 55586216200; 23104280100; 26436394500; 57211256625; 35336046600","CT-Based Pelvic T1-Weighted MR Image Synthesis Using UNet, UNet++ and Cycle-Consistent Generative Adversarial Network (Cycle-GAN)","2021","Frontiers in Oncology","11","","665807","","","","10.3389/fonc.2021.665807","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112466201&doi=10.3389%2ffonc.2021.665807&partnerID=40&md5=e5cce8c2dde004f39e25b1524866244b","Background: Computed tomography (CT) and magnetic resonance imaging (MRI) are the mainstay imaging modalities in radiotherapy planning. In MR-Linac treatment, manual annotation of organs-at-risk (OARs) and clinical volumes requires a significant clinician interaction and is a major challenge. Currently, there is a lack of available pre-annotated MRI data for training supervised segmentation algorithms. This study aimed to develop a deep learning (DL)-based framework to synthesize pelvic T1-weighted MRI from a pre-existing repository of clinical planning CTs. Methods: MRI synthesis was performed using UNet++ and cycle-consistent generative adversarial network (Cycle-GAN), and the predictions were compared qualitatively and quantitatively against a baseline UNet model using pixel-wise and perceptual loss functions. Additionally, the Cycle-GAN predictions were evaluated through qualitative expert testing (4 radiologists), and a pelvic bone segmentation routine based on a UNet architecture was trained on synthetic MRI using CT-propagated contours and subsequently tested on real pelvic T1 weighted MRI scans. Results: In our experiments, Cycle-GAN generated sharp images for all pelvic slices whilst UNet and UNet++ predictions suffered from poorer spatial resolution within deformable soft-tissues (e.g. bladder, bowel). Qualitative radiologist assessment showed inter-expert variabilities in the test scores; each of the four radiologists correctly identified images as acquired/synthetic with 67%, 100%, 86% and 94% accuracy. Unsupervised segmentation of pelvic bone on T1-weighted images was successful in a number of test cases Conclusion: Pelvic MRI synthesis is a challenging task due to the absence of soft-tissue contrast on CT. Our study showed the potential of deep learning models for synthesizing realistic MR images from CT, and transferring cross-domain knowledge which may help to expand training datasets for 21 development of MR-only segmentation models. © Copyright © 2021 Kalantar, Messiou, Winfield, Renn, Latifoltojar, Downey, Sohaib, Lalondrelle, Koh and Blackledge.","article; bladder; computer assisted tomography; controlled study; convolutional neural network; deep learning; human; intestine; loss of function mutation; nuclear magnetic resonance imaging; pelvic girdle; prediction; quantitative analysis; radiologist; radiotherapy; soft tissue; synthesis","computed tomography (CT); convolutional neural network (CNN); generative adversarial network (GAN); magnetic resonance imaging (MRI); medical image synthesis; radiotherapy planning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85112466201"
"Yang C.; Shen Y.; Xu Y.; Zhou B.","Yang, Ceyuan (57204283344); Shen, Yujun (57207766466); Xu, Yinghao (57219692967); Zhou, Bolei (36697366200)","57204283344; 57207766466; 57219692967; 36697366200","Data-Efficient Instance Generation from Instance Discrimination","2021","Advances in Neural Information Processing Systems","12","","","9378","9390","12","","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131830466&partnerID=40&md5=42bbf24aeaa9bf3adb7dbcd18bb61383","Generative Adversarial Networks (GANs) have significantly advanced image synthesis, however, the synthesis quality drops significantly given a limited amount of training data. To improve the data efficiency of GAN training, prior work typically employs data augmentation to mitigate the overfitting of the discriminator yet still learn the discriminator with a bi-classification (i.e., real vs. fake) task. In this work, we propose a data-efficient Instance Generation (InsGen) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can benefit from the infinite synthesized samples for training, alleviating the overfitting problem caused by insufficient training data. A noise perturbation strategy is further introduced to improve its discriminative power. Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of 2K training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID improvement. © 2021 Neural information processing systems foundation. All rights reserved.","Data augmentation; Generation method; Images synthesis; Learn+; Network training; Over fitting problem; Overfitting; Synthesised; Training data; Training sets; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85131830466"
"Mizginov V.A.; Kniaz V.V.; Fomin N.A.","Mizginov, V.A. (57192159517); Kniaz, V.V. (56540022100); Fomin, N.A. (57202441244)","57192159517; 56540022100; 57202441244","A method for synthesizing thermal images using gan multi-layered approach","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","54","2/W1","","155","162","7","","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108460695&partnerID=40&md5=8c3432a628f1295e75932e92243dd1a8","The active development of neural network technologies and optoelectronic systems has led to the introduction of computer vision technologies in various fields of science and technology. Deep learning made it possible to solve complex problems that a person had not been able to solve before. The use of multi-spectral optical systems has significantly expanded the field of application of video systems. Tasks such as image recognition, object re-identification, video surveillance require high accuracy, speed and reliability. These qualities are provided by algorithms based on deep convolutional neural networks. However, they require to have large databases of multi-spectral images of various objects to achieve state-of-the-art results. While large and various databases of color images of different objects are widely available in public domain, then similar databases of thermal images are either not available, or they represent a small number of types of objects. The quality of three-dimensional modeling for the thermal imaging spectral range remains at an insufficient level for solving a number of important tasks, which require high precision and reliability. The realistic synthesis of thermal images is especially important due to the complexity and high cost of obtaining real data. This paper is focused on the development of a method for synthesizing thermal imaging images based on generative adversarial neural networks. We developed an algorithm for a multi-spectral image-to-image translation. We have changed to the original GAN architecture and converted the loss function. We presented a new learning approach. For this, we prepared a special training dataset including about 2000 image tensors. The evaluation of the results obtained showed that the proposed method can be used to expand the available databases of thermal images. © Authors 2021. CC BY 4.0 License.","Complex networks; Computer vision; Convolutional neural networks; Database systems; Deep neural networks; Image recognition; Infrared imaging; Optoelectronic devices; Security systems; Spectroscopy; Computer vision technology; Multi-layered approach; Multispectral images; Network technologies; Optoelectronic systems; Re identifications; Science and Technology; Three-dimensional model; Deep learning","Generative adversarial networks; Image synthesis; Infrared image; Object recognition","Conference paper","Final","","Scopus","2-s2.0-85108460695"
"Jiang Y.; Chen H.; Loew M.; Ko H.","Jiang, Yifan (57207334917); Chen, Han (57219791907); Loew, Murray (7004977045); Ko, Hanseok (35069749800)","57207334917; 57219791907; 7004977045; 35069749800","COVID-19 CT Image Synthesis with a Conditional Generative Adversarial Network","2021","IEEE Journal of Biomedical and Health Informatics","25","2","9281375","441","452","11","10.1109/JBHI.2020.3042523","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097960653&doi=10.1109%2fJBHI.2020.3042523&partnerID=40&md5=c5c9c5d65ce8aa119586a5591cc031fe","Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has spread rapidly since December 2019. Real-time reverse transcription polymerase chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits of quick reporting, a low cost, and high sensitivity for the detection of pulmonary infection. Recently, deep-learning-based computer vision methods have demonstrated great promise for use in medical imaging applications, including X-rays, magnetic resonance imaging, and CT imaging. However, training a deep-learning model requires large volumes of data, and medical staff faces a high risk when collecting COVID-19 CT data due to the high infectivity of the disease. Another issue is the lack of experts available for data labeling. In order to meet the data requirements for COVID-19 CT imaging, we propose a CT image synthesis approach based on a conditional generative adversarial network that can effectively generate high-quality and realistic COVID-19 CT images for use in deep-learning-based medical imaging tasks. Experimental results show that the proposed method outperforms other state-of-the-art image synthesis methods with the generated COVID-19 CT images and indicates promising for various machine learning applications including semantic segmentation and classification.  © 2013 IEEE.","COVID-19; Deep Learning; Humans; Lung; Radiography, Thoracic; SARS-CoV-2; Tomography, X-Ray Computed; Deep learning; Diagnosis; Image segmentation; Learning systems; Magnetic resonance imaging; Medical imaging; Personnel training; Polymerase chain reaction; Semantics; Adversarial networks; Data requirements; Imaging applications; Machine learning applications; Pulmonary infections; Reverse transcription-polymerase chain reaction; Semantic segmentation; State of the art; Article; computer assisted tomography; computer vision; coronavirus disease 2019; deep learning; diagnostic imaging; Generative Adversarial Network; ground glass opacity; human; image analysis; image quality; image segmentation; machine learning; medical staff; nuclear magnetic resonance imaging; real time reverse transcription polymerase chain reaction; semantics; topography; training; lung; thorax radiography; x-ray computed tomography; Computerized tomography","computed topography; conditional generative adversarial network; COVID-19; image synthesis","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85097960653"
"Lin W.; Lin W.; Chen G.; Zhang H.; Gao Q.; Huang Y.; Tong T.; Du M.","Lin, Wanyun (57220953051); Lin, Weiming (57204792632); Chen, Gang (57208691754); Zhang, Hejun (56006910200); Gao, Qinquan (55598017400); Huang, Yechong (57209740744); Tong, Tong (55625986800); Du, Min (36463370900)","57220953051; 57204792632; 57208691754; 56006910200; 55598017400; 57209740744; 55625986800; 36463370900","Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer’s Disease","2021","Frontiers in Neuroscience","15","","646013","","","","10.3389/fnins.2021.646013","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104938401&doi=10.3389%2ffnins.2021.646013&partnerID=40&md5=1d415add3bcc0ee9bdf2264a5ea172b4","Combining multi-modality data for brain disease diagnosis such as Alzheimer’s disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient’s body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer’s disease, demonstrating the effectiveness of the proposed framework. © Copyright © 2021 Lin, Lin, Chen, Zhang, Gao, Huang, Tong, Du and the Alzheimer’s Disease Neuroimaging Initiative.","Alzheimer disease; Article; brain mapping; brain radiography; brain tissue; clinical effectiveness; clinical evaluation; controlled study; convolutional neural network; human; image analysis; image processing; information processing; mathematical computing; mathematical model; neuroimaging; neurologic disease assessment; nuclear magnetic resonance imaging; positron emission tomography; radiological parameters; Reversible Generative Adversarial Network; task performance; three-dimensional imaging","3D CNN; Alzheimer’s disease; image synthesis; multi-modality; reversible GAN","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85104938401"
"Rusak F.; Cruz R.S.; Smith E.; Fripp J.; Fookes C.; Bourgeat P.; Bradley A.","Rusak, Filip (57219314843); Cruz, Rodrigo Santa (57201377991); Smith, Elliot (57688524200); Fripp, Jurgen (13605436500); Fookes, Clinton (7003338437); Bourgeat, Pierrick (8576247900); Bradley, Andrew (7202846882)","57219314843; 57201377991; 57688524200; 13605436500; 7003338437; 8576247900; 7202846882","Detail Matters: High-Frequency Content for Realistic Synthetic MRI Generation","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12965 LNCS","","","3","13","10","10.1007/978-3-030-87592-3_1","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115833938&doi=10.1007%2f978-3-030-87592-3_1&partnerID=40&md5=01836851905334802401b8be4a228ce7","Deep Learning (DL)-based segmentation methods have been quite successful in various medical imaging applications. The main bottleneck of these methods is the scarcity of quality-labelled samples needed for their training. The lack of labelled training data is often addressed by augmentation methods, which aim to synthesise realistic samples with corresponding labels. While the synthesis of realistic samples remains a challenging task, little is known about the impact of fine detail in synthetic data on the performance of DL-based segmentation models. In this work, we investigate whether, and to what extent, the high-frequency (HF) detail in synthetic brain MR images (MRIs) impacts the performance of DL-based segmentation methods. To assess the impact of HF detail, we generate two synthetic datasets, with and without HF detail and train corresponding segmentation models to evaluate the impact on their performance. The results obtained demonstrate that the presence of HF detail in synthetic brain MRIs, used during training, significantly improve the Dice score up to 1.73% for Gray Matter (GM), 1.34% for White Matter (WM) and 4.41% for Cerebrospinal Fluid (CSF); and therefore justify the need for synthesising realistic-looking MRIs. © 2021, Springer Nature Switzerland AG.","Cerebrospinal fluid; Deep learning; Image segmentation; Magnetic resonance imaging; Medical imaging; Brain MR images; Data augmentation; High frequency HF; Images synthesis; Learning-based segmentation; MR-images; Performance; Realistic brain MR image synthesis; Segmentation methods; Generative adversarial networks","Brain MRI; Data augmentation; Generative adversarial network; Realistic brain MRI synthesis","Conference paper","Final","","Scopus","2-s2.0-85115833938"
"E Sousa M.Q.; Pedrosa J.; Rocha J.; Pereira S.C.; Mendonca A.M.; Campilho A.","E Sousa, Martim Quintas (57465334000); Pedrosa, Joao (57159246600); Rocha, Joana (57211180638); Pereira, Sofia Cardoso (57465043600); Mendonca, Ana Maria (56252793600); Campilho, Aurelio (57200232252)","57465334000; 57159246600; 57211180638; 57465043600; 56252793600; 57200232252","Chest Radiography Few-Shot Image Synthesis for Automated Pathology Screening Applications","2021","Proceedings - 2021 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021","","","","1791","1798","7","10.1109/BIBM52615.2021.9669859","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125190889&doi=10.1109%2fBIBM52615.2021.9669859&partnerID=40&md5=1da9dea8d991dc9aabf0897d680baa2e","Chest radiography is one of the most ubiquitous imaging modalities, playing an essential role in screening, diagnosis and disease management. However, chest radiography interpretation is a time-consuming and complex task, requiring the availability of experienced radiologists. As such, automated diagnosis systems for pathology detection have been proposed aiming to reduce the burden on radiologists and reduce variability in image interpretation. While promising results have been obtained, particularly since the advent of deep learning, there are significant limitations in the developed solutions, namely the lack of representative data for less frequent pathologies and the learning of biases from the training data, such as patient position, medical devices and other markers as proxies for certain pathologies. The lack of explainability is also a challenge for the adoption of these solutions in clinical practice.Generative adversarial networks could play a significant role as a solution for these challenges as they allow to artificially create new realistic images. This way, new synthetic chest radiography images could be used to increase the prevalence of less represented pathology classes and decrease model biases as well as improving the explainability of automatic decisions by generating samples that serve as examples or counter-examples to the image being analysed, ensuring patient privacy.In this study, a few-shot generative adversarial network is used to generate synthetic chest radiography images. A minimum Fréchet Inception Distance score of 17.83 was obtained, allowing to generate convincing synthetic images. Perceptual validation was then performed by asking multiple readers to classify a mixed set of synthetic and real images. An average accuracy of 83.5% was obtained but a strong dependency on reader experience level was observed. While synthetic images showed structural irregularities, the overall image sharpness was a major factor in the decision of readers. The synthetic images were then validated using a MobileNet abnormality classifier and it was shown that over 99% of images were classified correctly, indicating that the generated images were correctly interpreted by the classifier. Finally, the use of the synthetic images during training of a YOLOv5 pathology detector showed that the addition of the synthetic images led to an improvement of mean average precision of 0.05 across 14 pathologies.In conclusion, the usage of few-shot generative adversarial networks for chest radiography image generation was shown and tested in multiple scenarios, establishing a baseline for future experiments to increase the applicability of generative models in clinical scenarios of automatic CXR screening and diagnosis tools.  © 2021 IEEE.","Deep learning; Diagnosis; Generative adversarial networks; Image enhancement; Medical imaging; Radiography; Automated diagnosis system; Chest radiography; Complex task; Disease management; Images synthesis; Imaging modality; Radiography images; Screening application; Synthetic images; Time-consuming tasks; Pathology","","Conference paper","Final","","Scopus","2-s2.0-85125190889"
"Xiong J.; Chen J.","Xiong, Jiecheng (57194205644); Chen, Jun (57215375448)","57194205644; 57215375448","Crowd jumping load simulation with generative adversarial networks","2021","Smart Structures and Systems","27","4","","607","621","14","10.12989/sss.2021.27.4.607","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107490586&doi=10.12989%2fsss.2021.27.4.607&partnerID=40&md5=28227af997b06c9d95fce8b934c6cab5","To mathematically represent crowd jumping loads, the features of the jumping load of each person, including pulse curve patterns, pulse interval sequences, and pulse energy sequences are considered. These features are essentially highdimensional random variables. However, they have to be represented in a practically simplified model due to the lack of mathematical tools. The recently emerged generative adversarial networks (GANs) can model high-dimensional random variables well, as demonstrated in image synthesis and text generation. Therefore, this study adopts GANs as a new method for modelling crowd jumping loads. Conditional GANs (CGANs) combined with Wasserstein GANs with gradient penalty (WGANs-GP) are used in pulse curve pattern modelling, where a multi-layer perceptron and convolutional neural network are selected as the discriminator and generator, respectively. For the pulse energy sequence and pulse interval sequence modelling, similar GANs are used, where recurrent neural networks are selected as both the generator and discriminator. Finally, crowd jumping loads can be simulated by connected the pulse samples based on the pulse energy sequence samples and interval sequence samples, generated by the three proposed GANs. The experimental individual and crowd jumping load records are utilized in training GANs to ensure their output can simulate real load records well. Finally, the feasibility of the proposed GANs was verified by comparing the measured structural responses of an existing floor to the predicted structural responses. © 2021 Techno Press. All rights reserved.","Convolutional neural networks; Discriminators; Random variables; Recurrent neural networks; Adversarial networks; High-dimensional; Interval sequences; Load simulation; Mathematical tools; Multi layer perceptron; Structural response; Text generations; Multilayer neural networks","Crowd jumping; Deep learning; Generative adversarial networks; Human-induced vibration","Article","Final","","Scopus","2-s2.0-85107490586"
"Guo J.; Wang C.; Feng Y.","Guo, Jiongyu (57381873500); Wang, Can (56017252400); Feng, Yan (57204938737)","57381873500; 56017252400; 57204938737","Online Adversarial Knowledge Distillation for Image Synthesis of Bridge Defect","2021","ACM International Conference Proceeding Series","","","3487171","","","","10.1145/3487075.3487171","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121565329&doi=10.1145%2f3487075.3487171&partnerID=40&md5=8f25800a9ce9fe165e556881f55c5269","Bridge defect detection is an essential task of its daily maintenance, which aims to protect people's life and property safety. However, for a variety of reasons, research institutions have been faced with the scarcity of anomaly samples. One solution is using generative adversarial network (GAN) to generate extra samples for data augmentation. In this paper,we draw on the idea from online knowledge distillation to improve the self-attention GAN, and propose a new framework called Online Knowledge Distillation -Self Attention Generative Adversarial Network (OKD-SAGAN).We introduce a new module called connector which has the same structure with discriminator to train multiple groups of SAGAN together. The role of the connector is to control the output distribution of the corresponding generator to be consistent with the surrounding generators in order to achieve the purpose of mutual learning. We have conducted experiments on the CODEBRIM dataset and in order to further illustrate the effectiveness of OKD structure, we also applied OKD on ACGAN for experiments. The results show that the performance of some generators has exceeded a single set of SAGAN and ACGAN. Compared with SAGAN, OKD-SAGAN G2's FID score decreases by 15.4% and the average FID score decreases by 5.5%. As for ACGAN, OKD-ACGAN G1's FID score decreases by 7.6% and the average FID score decreases by 3.8%, which proves the validity of OKD structure.. © 2021 Association for Computing Machinery. All rights reserved.","Defects; Generative adversarial networks; Data augmentation; Defect detection; Image generations; Images synthesis; Multiple-group; Mutual learning; Online knowledge distillation; Output distribution; Property; Research institutions; Distillation","Generative adversarial network; Image generation; Online knowledge distillation","Conference paper","Final","","Scopus","2-s2.0-85121565329"
"Viertel P.; Konig M.; Rexilius J.","Viertel, Philipp (57207314637); Konig, Matthias (56970765600); Rexilius, Jan (57478971600)","57207314637; 56970765600; 57478971600","PollenGAN: Synthetic Pollen Grain Image Generation for Data Augmentation","2021","Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021","","","","44","49","5","10.1109/ICMLA52953.2021.00015","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123028252&doi=10.1109%2fICMLA52953.2021.00015&partnerID=40&md5=3c306f875915bd0c959d669f7c21bb19","Palynology, the study of pollen, is becoming the focus of attention in computer vision in recent years. Various proposed automated classification and segmentation methods have been evaluated on a number of data sets. However, as of 2021 most data sets are sparse; they either contain only a small number of pollen classes, images in total or are imbalanced overall. In this work, we explore the possibility of creating synthetic pollen grain images from less than 2, 000 images per pollen class via a Generative Adversarial Network (GAN). For that purpose, we selected two distinct pollen classes from a state of the art pollen data set and evaluated the data set with and without synthetic data on a Convolutional Neural Network (CNN). The enriched data set performed better overall (+1.4%) and specifically for the two pollen classes (+2%). We also drastically reduced the no. of real images and were still able to achieve a score of 60% to 80%. The experiments show, that our synthesized pollen images are visually close to real-life pollen grains and can be used to enrich imbalanced data sets as an addition to traditional data augmentation methods. © 2021 IEEE.","Classification (of information); Computer vision; Convolutional neural networks; Data visualization; Deep learning; Automated classification; Data augmentation; Data set; Deep learning; Focus of Attention; Image generations; Images synthesis; Machine-learning; Palynology; Pollen grains; Generative adversarial networks","Computer vision; Data augmentation; Deep learning; Generative adversarial networks; Image synthesis; Machine Learning; Palynology","Conference paper","Final","","Scopus","2-s2.0-85123028252"
"Goetschalckx L.; Andonian A.; Wagemans J.","Goetschalckx, Lore (57195966497); Andonian, Alex (57204293810); Wagemans, Johan (7006148651)","57195966497; 57204293810; 7006148651","Generative adversarial networks unlock new methods for cognitive science","2021","Trends in Cognitive Sciences","25","9","","788","801","13","10.1016/j.tics.2021.06.006","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112399376&doi=10.1016%2fj.tics.2021.06.006&partnerID=40&md5=3b7ec85f512b622e8ac21e885ca10536","Generative adversarial networks (GANs) enable computers to learn complex data distributions and sample from these distributions. When applied to the visual domain, this allows artificial, yet photorealistic images to be synthesized. Their success at this very challenging task triggered an explosion of research within the field of artificial intelligence (AI), yielding various new GAN findings and applications. After explaining the core principles behind GANs and reviewing recent GAN innovations, we illustrate how they can be applied to tackle thorny theoretical and methodological problems in cognitive science. We focus on how GANs can reveal hidden structure in internal representations and how they offer a valuable new compromise in the trade-off between experimental control and ecological validity. © 2021 Elsevier Ltd","Artificial Intelligence; Cognitive Science; Computers; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Economic and social effects; Adversarial networks; Complex data distributions; Ecological validity; Experimental control; Hidden structures; Internal representation; Methodological problems; Photorealistic images; artificial intelligence; data science; ecological validity; generative adversarial network; human; hypothesis; machine learning; mental representation; psychology; Review; artificial intelligence; computer; image processing; psychology; Artificial intelligence","experimental control; generative adversarial networks; image synthesis; natural images; visual stimuli","Review","Final","","Scopus","2-s2.0-85112399376"
"Hu Z.; Liu H.; Li Z.; Yu Z.","Hu, Zebin (57316039900); Liu, Hao (56275670700); Li, Zhendong (57204674009); Yu, Zekuan (57218290944)","57316039900; 56275670700; 57204674009; 57218290944","Cross-Model Transformer Method for Medical Image Synthesis","2021","Complexity","2021","","5624909","","","","10.1155/2021/5624909","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122978885&doi=10.1155%2f2021%2f5624909&partnerID=40&md5=9e4851a02d5a20c44973016a1eeb89ff","Acquiring complementary information about tissue morphology from multimodal medical images is beneficial to clinical disease diagnosis, but it cannot be widely used due to the cost of scans. In such cases, medical image synthesis has become a popular area. Recently, generative adversarial network (GAN) models are applied to many medical image synthesis tasks and show prior performance, since they enable to capture structural details clearly. However, GAN still builds the main framework based on convolutional neural network (CNN) that exhibits a strong locality bias and spatial invariance through the use of shared weights across all positions. Therefore, the long-range dependencies have been destroyed in this processing. To address this issue, we introduce a double-scale deep learning method for cross-modal medical image synthesis. More specifically, the proposed method captures locality feature via local discriminator based on CNN and utilizes long-range dependencies to learning global feature through global discriminator based on transformer architecture. To evaluate the effectiveness of double-scale GAN, we conduct folds of experiments on the standard benchmark IXI dataset and experimental results demonstrate the effectiveness of our method. Copyright © 2021 Zebin Hu et al.","Convolutional neural networks; Deep learning; Diagnosis; Medical imaging; Clinical disease; Convolutional neural network; Cross model; Disease diagnosis; Double-scale; Images synthesis; Long-range dependencies; Model transformers; Multimodal medical images; Tissue morphology; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122978885"
"Xia G.; Ma F.; Liu Q.; Zhang D.","Xia, Guiyu (56625526600); Ma, Furong (57222314487); Liu, Qingshan (36063739200); Zhang, Du (57193483289)","56625526600; 57222314487; 36063739200; 57193483289","Pose-Driven Realistic 2-D Motion Synthesis","2021","IEEE Transactions on Cybernetics","","","","","","","10.1109/TCYB.2021.3120010","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118567484&doi=10.1109%2fTCYB.2021.3120010&partnerID=40&md5=e4da6ec14f5aae8fd5ce666624afe99f","A realistic 2-D motion can be treated as a deforming process of an individual appearance texture driven by a sequence of human poses. In this article, we thereby propose to transform the 2-D motion synthesis into a pose conditioned realistic motion image generation task considering the promising performance of pose estimation technology and generative adversarial nets (GANs). However, the problem is that GAN is only suitable to do the region-aligned image translation task while motion synthesis involves a large number of spatial deformations. To avoid this drawback, we design a two-step and multistream network architecture. First, we train a special GAN to generate the body segment images with given poses in step-I. Then in step-II, we input the body segment images as well as the poses into the multistream network so that it only needs to generate the textures in each aligned body region. Besides, we provide a real face as another input of the network to improve the face details of the generated motion image. The synthesized results with realism and sharp details on four training sets demonstrate the effectiveness of the proposed model. IEEE","Computer vision; Deformation; Image enhancement; Image segmentation; Job analysis; Network architecture; Body segment; Conditional generative adversarial net; Face; Image generations; Images segmentations; Images synthesis; Motion segmentation; Motion synthesis; Person image generation; Pose.; Task analysis; Generative adversarial networks","Body segment; conditional generative adversarial net (GAN); Faces; Generative adversarial networks; Image segmentation; Image synthesis; Motion segmentation; motion synthesis; person image generation; pose.; Task analysis; Training","Article","Article in press","All Open Access; Bronze Open Access","Scopus","2-s2.0-85118567484"
"Bahr D.; Eschweiler D.; Bhattacharyya A.; Moreno-Andres D.; Antonin W.; Stegmaier J.","Bahr, Dennis (57224199522); Eschweiler, Dennis (57194396472); Bhattacharyya, Anuk (57224201885); Moreno-Andres, Daniel (56515474000); Antonin, Wolfram (6602107022); Stegmaier, Johannes (55584699900)","57224199522; 57194396472; 57224201885; 56515474000; 6602107022; 55584699900","Cellcyclegan: Spatiotemporal microscopy image synthesis of cell populations using statistical shape models and conditional gans","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9433896","1915","1919","4","10.1109/ISBI48211.2021.9433896","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107226286&doi=10.1109%2fISBI48211.2021.9433896&partnerID=40&md5=da95548ef391d3815263cc41277f93ad","Automatic analysis of spatio-temporal microscopy images is inevitable for state-of-the-art research in the life sciences. Recent developments in deep learning provide powerful tools for automatic analyses of such image data, but heavily depend on the amount and quality of provided training data to perform well. To this end, we developed a new method for realistic generation of synthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei. The method combines spatiotemporal statistical shape models of different cell cycle stages with a conditional GAN to generate time series of cell populations and provides instance-level control of cell cycle stage and the fluorescence intensity of generated cells. We show the effect of the GAN conditioning and create a set of synthetic images that can be readily used for training and benchmarking of cell segmentation and tracking approaches.  © 2021 IEEE.","Cell culture; Cell proliferation; Cells; Deep learning; Image analysis; Image segmentation; Medical imaging; Automatic analysis; Cell populations; Cell segmentation; Fluorescence intensities; Microscopy images; State of the art; Statistical shape model; Synthetic images; Population statistics","Cell Biology; Data Synthesis; Generative Adversarial Networks; Microscopy; Statistical Shape Models","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107226286"
"Soni R.K.; Nair B.B.","Soni, Rajat Kumar (57238252600); Nair, Binoy B (35386974400)","57238252600; 35386974400","Deep Learning Based Approach to Generate Realistic Data for ADAS Applications","2021","2021 5th International Conference on Computer, Communication, and Signal Processing, ICCCSP 2021","","","9465529","181","185","4","10.1109/ICCCSP52374.2021.9465529","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113834571&doi=10.1109%2fICCCSP52374.2021.9465529&partnerID=40&md5=3bfe5efcd50606d500b1ce53d778ef5a","Quantity, quality and diversity of datasets are prerequisites for training the deep-learning autonomous driving models. One major issue identified from the literature is the lack of realistic training data which eventually leads to a less robust model. Simulators can help in dealing with reducing reality gaps, however, commonly available simulators generate data that are far removed from the real world scenarios. The model proposed in this study is based on video-To-video synthesis and image synthesis methods using Generative Adversarial Networks (GANs). The results indicate improved realism. Kanade-Lucas-Tomasi (KLT) and Fr'echet Inception Distance (FID) based temporal coherence evaluation metrics have also been proposed as a possible alternative to human perception driven evaluations.  © 2021 IEEE.","Signal processing; Adversarial networks; Autonomous driving; Evaluation metrics; Human perception; Learning-based approach; Real-world scenario; Temporal coherence; Video to videos; Deep learning","Advanced driver assistance systems (ADAS); Fr'echet Inception Distance (FID); Generative adversarial network (GAN); Kanade-Lucas-Tomasi (KLT)","Conference paper","Final","","Scopus","2-s2.0-85113834571"
"Zhao L.; Li X.; Huang P.; Chen Z.; Dai Y.; Li T.","Zhao, Liang (57199018520); Li, Xinwei (57388385300); Huang, Pingda (57388303600); Chen, Zhikui (56020772800); Dai, Yanqi (57388864100); Li, Tianyu (57388385400)","57199018520; 57388385300; 57388303600; 56020772800; 57388864100; 57388385400","TRGAN: Text to Image Generation Through Optimizing Initial Image","2021","Communications in Computer and Information Science","1516 CCIS","","","651","658","7","10.1007/978-3-030-92307-5_76","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121904248&doi=10.1007%2f978-3-030-92307-5_76&partnerID=40&md5=22e844fd92847a7cb790882bae5329b9","Generative Adversarial Networks (GANs) have shown success in text-to-image generation tasks. Most of the current methods use multi-stages to generate images, but the quality of the final images is largely dependent on the quality of the initial generated images, thus it is difficult to generate high-quality images in the end if the initial images in the first stage are of low quality, low resolution, irregular shape, strange color, and unrealistic entity relations. Therefore, in this paper, we propose to design a multi-stage generation model, and we address this problem by developing a novel generation model called Text-representation Generative Adversarial Network (TRGAN). TRGAN contains two modules: Joint attention stacked generation module (JASGM) and Text generation in the opposite direction and correction module (TGOCM). In the JASGM module, the detailed feature is extracted from word-level information and the images are generated based on the global sentence attention. In the TGOCM module, the text descriptions are generated reversely, which can improve the quality of the initial images by matching the word-level feature vector. Experimental results present that our proposed model TRGAN outperforms the compared state-of-the-art text-to-image generation methods on CUB and COCO datasets. © 2021, Springer Nature Switzerland AG.","Image enhancement; Semantic Web; Semantics; Image generations; Image semantics; Images synthesis; Semantics understanding; Text generation reversely; Text generations; Text images; Text representation; Text-image semantic understanding; Text-to-image synthesis; Generative adversarial networks","Generative Adversarial Network; Text generation reversely; Text-image semantic understanding; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85121904248"
"Li W.","Li, Wanwan (57221638937)","57221638937","Image synthesis and editing with generative adversarial networks (GANs): A review","2021","Proceedings of the 2021 5th World Conference on Smart Trends in Systems Security and Sustainability, WorldS4 2021","","","","65","70","5","10.1109/WorldS451998.2021.9514052","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114449593&doi=10.1109%2fWorldS451998.2021.9514052&partnerID=40&md5=b8423b10be5059a4f1666a6481a60398","Recently, as many deep learning models are emerging, deep learning has achieved great success in the field of artificial intelligence(AI). Especially, the Generative adversarial networks (GANs) based on zero-sum game theory has become a new research hot spot in the field of deep learning. The significance of the GAN model is that it can generate realistic data through unsupervised learning. Based on the conceptual and theoretical framework of the generative adversarial network, GANs models and their application result in tremendous success among different areas, especially in image synthesis and editing. This paper visualizes the data structures of various kinds of GANs models in 3D and discusses the variational GAN models with respect to their improvements in the applications. As the GANs have superior learning ability, strong plasticity, great potential for improvement, and a wide application range, this paper prospects the possible applications of the GANs in the near future. © 2021 IEEE.","Deep learning; Game theory; Image processing; Sustainable development; Three dimensional computer graphics; Adversarial networks; Application range; Hot spot; Image synthesis; Learning abilities; Learning models; Theoretical framework; Zero-sum game theory; Learning systems","Deep Learning; Generative Adversarial Networks; Image Editing; Image Synthesis; Neural Networks","Conference paper","Final","","Scopus","2-s2.0-85114449593"
"Dong Y.; Zhang Y.; Ma L.; Wang Z.; Luo J.","Dong, Yanlong (57205425168); Zhang, Ying (56685932500); Ma, Lin (56377428300); Wang, Zhi (55913248200); Luo, Jiebo (7404182441)","57205425168; 56685932500; 56377428300; 55913248200; 7404182441","Unsupervised text-to-image synthesis","2021","Pattern Recognition","110","","107573","","","","10.1016/j.patcog.2020.107573","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090826812&doi=10.1016%2fj.patcog.2020.107573&partnerID=40&md5=5c5bbf6978344780f620e0d38146e65e","Recently, text-to-image synthesis has achieved great progresses with the advancement of the Generative Adversarial Network (GAN). However, training the GAN models requires a large amount of pairwise image-text data, which is extremely labor-intensive to collect. In this paper, we make the first attempt to train a text-to-image synthesis model in an unsupervised manner, which does not require any human labeled image-text pair data. Specifically, we first rely on the visual concepts to bridge two independent image and sentence sets and thereby yield the pseudo image-text pair data, based on which one GAN model can thereby be initialized. One novel visual concept discrimination loss is proposed to train both generator and discriminator, which not only encourages the image expressing the true local visual concepts but also ensures the noisy visual concepts contained in the pseudo sentence being suppressed. Afterwards, one global semantic consistency regarding to the real sentence is used to adapt the pretrained GAN model to real sentences. Experimental results demonstrate that our proposed unsupervised training strategy is able to generate favorable images for given sentences, which even outperforms some existing models trained in the supervised manner. The code of this paper is available at https://github.com/dylls/Unsupervised_Text-to-Image_Synthesis. © 2020","Semantics; Adversarial networks; Image synthesis; Labeled images; Labor intensive; Pseudo-sentence; Semantic consistency; Unsupervised training; Visual concept; Image processing","Generative adversarial network (GAN); Text-to-image synthesis; Unsupervised training","Article","Final","","Scopus","2-s2.0-85090826812"
"Ivgi M.; Benny Y.; Ben-David A.; Berant J.; Wolf L.","Ivgi, Maor (57221150585); Benny, Yaniv (57219509614); Ben-David, Avichai (57221144188); Berant, Jonathan (52163188700); Wolf, Lior (57203078732)","57221150585; 57219509614; 57221144188; 52163188700; 57203078732","SCENE GRAPH TO IMAGE GENERATION WITH CONTEXTUALIZED OBJECT LAYOUT REFINEMENT","2021","Proceedings - International Conference on Image Processing, ICIP","2021-September","","","2428","2432","4","10.1109/ICIP42928.2021.9506651","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125580583&doi=10.1109%2fICIP42928.2021.9506651&partnerID=40&md5=d1bce1a3174cc2867b48ed9a541036b6","Generating images from scene graphs is a challenging task that attracted substantial interest recently. Prior works have approached this task by generating an intermediate layout description of the target image. However, the representation of each object in the layout was generated independently, which resulted in high overlap, low coverage, and an overall blurry layout. We propose a novel method that alleviates these issues by generating the entire layout description gradually to improve inter-object dependency. We empirically show on the COCO-STUFF dataset that our approach improves the quality of both the intermediate layout and the final image. Our approach improves the layout coverage by almost 20 points, and drops object overlap to negligible amounts. Our code is available at github.com/yanivbenny/COLoR. © 2021 IEEE","Computer vision; Generative adversarial networks; GAN; Image generations; Images synthesis; Novel methods; Object layouts; Scene-graphs; Target images; Image enhancement","GAN; Image synthesis; Scene graph","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125580583"
"Hu J.; Ren Y.; Yuan Y.; Li Y.; Chen L.","Hu, Jinyu (57232548300); Ren, Yuchen (57233010300); Yuan, Yuan (57203237779); Li, Yin (57202161660); Chen, Lei (57192609774)","57232548300; 57233010300; 57203237779; 57202161660; 57192609774","PathosisGAN: Sick Face Image Synthesis with Generative Adversarial Network","2021","ACM International Conference Proceeding Series","","","3470691","","","","10.1145/3469213.3470691","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113468301&doi=10.1145%2f3469213.3470691&partnerID=40&md5=b84e173b252c7126db8d7b97ae5eceb2","The image-to-image translation method based on Generative Adversarial Networks (GAN) realizes the conversion of the image from the source data domain to the target data domain by learning the joint distribution of the two data domains. However, there are still some challenges in applying GAN directly to sick face synthesis. Firstly, most existing image-to-image translation methods realize global style feature transfer, which is difficult to extract the subtle sick features (e.g., dark circles) in the local area of human face. Secondly, the number of images generated by image translation is limited, which is not conducive to a large-scale expansion of training data. In order to solve these problems, we build a novel Generative Adversarial Network model called PathosisGAN based on the CycleGAN framework. The model uses a mask control module to transfer the sick features in the local area of the human face to the source images and retain the source images subject information. Meanwhile, we add a feature extraction module to the GAN model to synthesize face images with different degrees of sickness, enhancing the data augmentation effect. Experimental results show that PathosisGAN achieves the synthesis of sick face images under unpaired data. Compared with other methods, the synthesized face images have clear sick features and natural visual effects, which provide enough sample data for medical analysis tasks based on human face images. © 2021 Association for Computing Machinery. All rights reserved.","Artificial intelligence; Information systems; Information use; Medical imaging; Adversarial networks; Data augmentation; Face image synthesis; Feature transfers; Human face image; Image translation; Joint distributions; Medical analysis; Image enhancement","augmentation; face synthesis; Generative adversarial networks; image-to-image translation","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85113468301"
"Kumar R.; Malik R.","Kumar, Robin (57423838200); Malik, Rahul (57220699318)","57423838200; 57220699318","A Review on Generative Adversarial Networks used for Image Reconstruction in Medical imaging","2021","2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions), ICRITO 2021","","","","","","","10.1109/ICRITO51393.2021.9596487","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123387454&doi=10.1109%2fICRITO51393.2021.9596487&partnerID=40&md5=f6b19548254443b71e40414c6dc7d7a3","In computer vision, Because of its capacity to generate data, generative adversarial networks have gotten much interest. Generative Adversarial Network (GANs) opens different ways to overcome difficulties in medical image investigations like de-noising of the captured image, reconstruction of captured medical image, segmentation of the captured medical image, synthesis of the captured image, detection and classification of processed medical image. GANs great potentials in image restoration or reconstruction for different anatomy open a new research area. However, image reconstruction for specific anatomy is still facing the challenge. This article consists of a recent literature review on the applications of GAN for the reconstruction of medical images. Different reconstruction methods in the field of medical imaging were explored in this paper thoroughly. We have studied the foremost relevant publications. © 2021 IEEE.","Image reconstruction; Image segmentation; Medical imaging; De-noising; Generative adversarial network model; Image detection; Images classification; Images reconstruction; Images synthesis; Medical image segmentation; Medical imaging anatomy; Network models; Reconstruction; Generative adversarial networks","GAN model; Generative adversarial network; Medical imaging Anatomy; Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85123387454"
"Jiang G.; Wei J.; Xu Y.; He Z.; Zeng H.; Wu J.; Qin G.; Chen W.; Lu Y.","Jiang, Gongfa (57203758821); Wei, Jun (57201973656); Xu, Yuesheng (57203363105); He, Zilong (57054536200); Zeng, Hui (57193255233); Wu, Jiefang (57212317156); Qin, Genggeng (35785080200); Chen, Weiguo (55568522646); Lu, Yao (57190289782)","57203758821; 57201973656; 57203363105; 57054536200; 57193255233; 57212317156; 35785080200; 55568522646; 57190289782","Synthesis of Mammogram from Digital Breast Tomosynthesis Using Deep Convolutional Neural Network with Gradient Guided cGANs","2021","IEEE Transactions on Medical Imaging","40","8","9398655","2080","2091","11","10.1109/TMI.2021.3071544","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103919208&doi=10.1109%2fTMI.2021.3071544&partnerID=40&md5=0bc3c14d0ff137ae17d9eb1a1dd78a2e","Synthetic digital mammography (SDM), a 2D image generated from digital breast tomosynthesis (DBT), is used as a potential substitute for full-field digital mammography (FFDM) in clinic to reduce the radiation dose for breast cancer screening. Previous studies exploited projection geometry and fused projection data and DBT volume, with different post-processing techniques applied on re-projection data which may generate different image appearance compared to FFDM. To alleviate this issue, one possible solution to generate an SDM image is using a learning-based method to model the transformation from the DBT volume to the FFDM image using current DBT/FFDM combo images. In this study, we proposed to use a deep convolutional neural network (DCNN) to learn the transformation to generate SDM using current DBT/FFDM combo images. Gradient guided conditional generative adversarial networks (GGGAN) objective function was designed to preserve subtle MCs and the perceptual loss was exploited to improve the performance of the proposed DCNN on perceptual quality. We used various image quality criteria for evaluation, including preserving masses and MCs which are important in mammogram. Experiment results demonstrated progressive performance improvement of network using different objective functions in terms of those image quality criteria. The methodology we exploited in the SDM generation task to analyze and progressively improve image quality by designing objective functions may be helpful to other image generation tasks.  © 1982-2012 IEEE.","Early Detection of Cancer; Mammography; Neural Networks, Computer; Radiographic Image Enhancement; Convolution; Convolutional neural networks; Deep neural networks; Image quality; Mammography; Medical imaging; Space division multiple access; Tomography; X ray screens; Adversarial networks; Breast cancer screening; Digital breast tomosynthesis; Digital breast tomosynthesis (DBT); Full field digital mammography; Image quality criteria; Learning-based methods; Post-processing techniques; Article; breast cancer; breast density; breast tissue; cancer screening; cluster analysis; comparative study; convolutional neural network; deep learning; digital breast tomosynthesis; digital mammography; edge detection; female; gradient guided conditional generative adversarial network; human; image analysis; image processing; image quality; image reconstruction; image segmentation; major clinical study; synthetic digital mammography; early cancer diagnosis; image enhancement; mammography; Image enhancement","Breast cancer; deep learning; digital breast tomosynthesis; generative adversarial networks; image synthesis","Article","Final","","Scopus","2-s2.0-85103919208"
"Abu-Srhan A.; Almallahi I.; Abushariah M.A.M.; Mahafza W.; Al-Kadi O.S.","Abu-Srhan, Alaa (57196038395); Almallahi, Israa (57233239900); Abushariah, Mohammad A.M. (36547417700); Mahafza, Waleed (7801681414); Al-Kadi, Omar S. (24398656700)","57196038395; 57233239900; 36547417700; 7801681414; 24398656700","Paired-unpaired Unsupervised Attention Guided GAN with transfer learning for bidirectional brain MR-CT synthesis","2021","Computers in Biology and Medicine","136","","104763","","","","10.1016/j.compbiomed.2021.104763","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113700498&doi=10.1016%2fj.compbiomed.2021.104763&partnerID=40&md5=68c05521be5e1e5cf1deb0a09c52e039","Medical image acquisition plays a significant role in the diagnosis and management of diseases. Magnetic Resonance (MR) and Computed Tomography (CT) are considered two of the most popular modalities for medical image acquisition. Some considerations, such as cost and radiation dose, may limit the acquisition of certain image modalities. Therefore, medical image synthesis can be used to generate required medical images without actual acquisition. In this paper, we propose a paired–unpaired Unsupervised Attention Guided Generative Adversarial Network (uagGAN) model to translate MR images to CT images and vice versa. The uagGAN model is pre-trained with a paired dataset for initialization and then retrained on an unpaired dataset using a cascading process. In the paired pre-training stage, we enhance the loss function of our model by combining the Wasserstein GAN adversarial loss function with a new combination of non-adversarial losses (content loss and L1) to generate fine structure images. This will ensure global consistency, and better capture of the high and low frequency details of the generated images. The uagGAN model is employed as it generates more accurate and sharper images through the production of attention masks. Knowledge from a non-medical pre-trained model is also transferred to the uagGAN model for improved learning and better image translation performance. Quantitative evaluation and qualitative perceptual analysis by radiologists indicate that employing transfer learning with the proposed paired-unpaired uagGAN model can achieve better performance as compared to other rival image-to-image translation models. © 2021 Elsevier Ltd","Attention; Brain; Image Processing, Computer-Assisted; Machine Learning; Magnetic Resonance Spectroscopy; Tomography, X-Ray Computed; Computerized tomography; Data transfer; Diagnosis; Image acquisition; Image enhancement; Learning systems; Magnetic resonance imaging; Medical imaging; Adversarial networks; Generative adversarial network; Image translation; Loss functions; Magnetic resonance-to-computed tomography image synthesis; Network models; Paired and unpaired data; Performance; Transfer learning; Unsupervised image translation; adult; aged; Article; brain radiation; brain tumor; clinical article; cloud computing; comparative study; computer assisted tomography; digital imaging and communications in medicine; human; image analysis; image artifact; image quality; neuroimaging; nuclear magnetic resonance imaging; radiologist; signal noise ratio; transfer of learning; treatment planning; unsupervised attention guided generative adversarial network; unsupervised machine learning; visual information; attention; brain; diagnostic imaging; image processing; machine learning; nuclear magnetic resonance spectroscopy; x-ray computed tomography; Magnetic resonance","Generative adversarial networks; MR-to-CT Image synthesis; Paired and unpaired data; Transfer learning; Unsupervised image translation","Article","Final","","Scopus","2-s2.0-85113700498"
"Wang R.; Liu L.","Wang, Rongzhao (57562598100); Liu, Libo (55715263400)","57562598100; 55715263400","Paint to Better Describe: Learning Image Caption by Using Text-to-Image Synthesis","2021","Proceedings - 2021 IEEE International Conference on Dependable, Autonomic and Secure Computing, International Conference on Pervasive Intelligence and Computing, International Conference on Cloud and Big Data Computing and International Conference on Cyber Science and Technology Congress, DASC/PiCom/CBDCom/CyberSciTech 2021","","","","958","964","6","10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00160","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127586185&doi=10.1109%2fDASC-PICom-CBDCom-CyberSciTech52372.2021.00160&partnerID=40&md5=424cf38ecd67e1e248fc36a981d1c6a9","Image caption aims to generate accurate and informative description according to given image. Although numerous existing methods have been proposed to generate high-quality and semantically real text description by using Encoder-Decoder architecture, how to guarantee whether the generated description is informative enough remains very challenging. In this paper, we propose a symmetrical structure named 'Paint to better describe' (PTBD) by learning text-to-image synthesis to address this problem. Our PTBD approach consists of three modules, an image caption module (ICM), an image regeneration module (IREM), and a visual semantic alignment module (VSAM). ICM uses transformer-based method to generate image caption, leveraging both visual contents and object tags from the image. IREM uses generative adversarial network to regenerate the image according to the generated description, which semantically aligns with it. VSAM keeps network to generate accurate and informative description by measuring whether the regenerated image has enough semantics and visual information. Through experiments on MSCOCO dataset demonstrate the effectiveness of PTBD over other representative methods.  © 2021 IEEE.","Paint; Semantics; Encoder-decoder architecture; High quality; Image caption; Images synthesis; Learning text; Semantic alignments; Symmetrical structure; Visual content; Visual objects; Visual semantics; Generative adversarial networks","Generative Adversarial Network; Image Caption; Symmetrical Structure","Conference paper","Final","","Scopus","2-s2.0-85127586185"
"Liu Y.; Shu Z.; Li Y.; Lin Z.; Perazzi F.; Kung S.Y.","Liu, Yuchen (57209885905); Shu, Zhixin (56564871400); Li, Yijun (57191433923); Lin, Zhe (7404230086); Perazzi, Federico (55365483100); Kung, S.Y. (7102989364)","57209885905; 56564871400; 57191433923; 7404230086; 55365483100; 7102989364","Content-aware GaN compression","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","12151","12161","10","10.1109/CVPR46437.2021.01198","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120762588&doi=10.1109%2fCVPR46437.2021.01198&partnerID=40&md5=6ce15fd4178aac8ae8508b788d8107a1","Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in various image generation and synthesis tasks, yet their notoriously high computational cost hinders their efficient deployment on edge devices. Directly applying generic compression approaches yields poor results on GANs, which motivates a number of recent GAN compression works. While prior works mainly accelerate conditional GANs, e.g., pix2pix and CycleGAN, compressing state-of-the-art unconditional GANs has rarely been explored and is more challenging. In this paper, we propose novel approaches for unconditional GAN compression. We first introduce effective channel pruning and knowledge distillation schemes specialized for unconditional GANs. We then propose a novel content-aware method to guide the processes of both pruning and distillation. With content-awareness, we can effectively prune channels that are unimportant to the contents of interest, e.g., human faces, and focus our distillation on these regions, which significantly enhances the distillation quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2 by 11× with visually negligible image quality loss compared to the full-size model. More interestingly, when applied to various image manipulation tasks, our compressed model forms a smoother and better disentangled latent manifold, making it more effective for image editing. © 2021 IEEE","Computer vision; Distillation; Compression approach; Compression methods; Compression work; Computational costs; Content-aware; Human faces; Image generations; Images synthesis; Network compression; State of the art; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85120762588"
"Srichandan A.; Gupta B.; Prakasam P.; Mohammed V.N.","Srichandan, Aditya (57232854100); Gupta, Bhart (57490073200); Prakasam, P. (19933934400); Mohammed, V. Noor (57490192700)","57232854100; 57490073200; 19933934400; 57490192700","Imagination with Generative Adversarial Networks for Object Detection","2021","3rd IEEE International Virtual Conference on Innovations in Power and Advanced Computing Technologies, i-PACT 2021","","","","","","","10.1109/i-PACT52855.2021.9696929","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126456359&doi=10.1109%2fi-PACT52855.2021.9696929&partnerID=40&md5=47c5cd720e0ff5e41fe6e3862e41f1c4","Assistive and home service robots have been an important as well as lucrative applications in the burgeoning robotics technology. In this paper we introduce an approach for home service mobile robots to assist and fetch objects based on speech commands given by the user, from its surroundings. We have tried to make the robot learn to fetch using imaginative learning (TI) by making use of text to image GANs in the object detection pipeline. The robot will fetch the object from different rooms in the environment by imagining the object based on the user's speech command and compare it with real ones. We use deep learning techniques and models such as speech to text, text to image deep learning, deep image similarity, classical computer vision, SLAM and path planning of robot to achieve the overall fetching object pipeline and finally compare similarity score of generated images and detected images using Deep Image Similarity by the robot. © 2021 IEEE.","Deep learning; Generative adversarial networks; Motion planning; Object detection; Object recognition; Pipelines; Robot programming; Assistive; Home service robot; Image similarity; Images synthesis; Object based; Objects detection; Objects-based; Robotic technologies; Speech commands; Text to image synthesis; Mobile robots","Generative Adversarial Networks; Mobile robots; Object Detection; Text to image synthesis","Conference paper","Final","","Scopus","2-s2.0-85126456359"
"Wang M.; Lang C.; Liang L.; Feng S.; Wang T.; Gao Y.","Wang, Min (57221235394); Lang, Congyan (7402002472); Liang, Liqian (57218477192); Feng, Songhe (7402531247); Wang, Tao (56135273700); Gao, Yutong (57218478378)","57221235394; 7402002472; 57218477192; 7402531247; 56135273700; 57218478378","Fine-Grained Semantic Image Synthesis with Object-Attention Generative Adversarial Network","2021","ACM Transactions on Intelligent Systems and Technology","12","5","3470008","","","","10.1145/3470008","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122065997&doi=10.1145%2f3470008&partnerID=40&md5=3e520b1f65ff5fe3740c4ab135e35306","Semantic image synthesis is a new rising and challenging vision problem accompanied by the recent promising advances in generative adversarial networks. The existing semantic image synthesis methods only consider the global information provided by the semantic segmentation mask, such as class label, global layout, and location, so the generative models cannot capture the rich local fine-grained information of the images (e.g., object structure, contour, and texture). To address this issue, we adopt a multi-scale feature fusion algorithm to refine the generated images by learning the fine-grained information of the local objects. We propose OA-GAN, a novel object-attention generative adversarial network that allows attention-driven, multi-fusion refinement for fine-grained semantic image synthesis. Specifically, the proposed model first generates multi-scale global image features and local object features, respectively, then the local object features are fused into the global image features to improve the correlation between the local and the global. In the process of feature fusion, the global image features and the local object features are fused through the channel-spatial-wise fusion block to learn gwhat' and gwhere' to attend in the channel and spatial axes, respectively. The fused features are used to construct correlation filters to obtain feature response maps to determine the locations, contours, and textures of the objects. Extensive quantitative and qualitative experiments on COCO-Stuff, ADE20K and Cityscapes datasets demonstrate that our OA-GAN significantly outperforms the state-of-the-art methods.  © 2021 Association for Computing Machinery.","Image enhancement; Image fusion; Semantic Segmentation; Semantic Web; Semantics; Textures; Attention-driven; Channel-spatial-wise fusion; Features fusions; Fine grained; GAN; Image features; Images synthesis; Local object; Semantic image synthesis; Semantic images; Generative adversarial networks","attention-driven; channel-spatial-wise fusion; GAN; Semantic image synthesis","Article","Final","","Scopus","2-s2.0-85122065997"
"Fu X.; Chen C.; Li D.; Chen Z.","Fu, Xue (57218570677); Chen, Chunxiao (8657255600); Li, Dongsheng (57212313956); Chen, Zhiying (57243892600)","57218570677; 8657255600; 57212313956; 57243892600","MR Image Generation Method Based on Dense Connection Self⁃inverse Generative Adversarial Network; [基于密集连接自逆生成对抗网络的MR图像生成方法]","2021","Shuju Caiji Yu Chuli/Journal of Data Acquisition and Processing","36","4","","739","745","6","10.16337/j.1004⁃9037.2021.04.012","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114162479&doi=10.16337%2fj.1004%e2%81%839037.2021.04.012&partnerID=40&md5=8b239149861a63881c5ded94461215ee","With the rapid development of medical imaging technology, medical images have been widely used in clinical detection and scientific research. In view of the insufficient clinical image data set, this paper proposes a generation model based on dense connection self-inverse generative adversarial network (GAN) to realize the mutual generation of T1- and T2-weighted MR images. Especially, the dense block is introduced into the generator module of self-inverse GAN model, and the multi-scale fusion framework of U-net is adopted to realize the mutual generation of T1 and T2 weighted MR images. The BraTS 2018 data set is used for validation and the peak signal-to-noise ratio and structure similarity of the generated images could reach 22.78 and 0.8, respectively. Contrast experimental results of different generators show that the model with the generator based on dense block has better performance than the model with the generator based on U-net or ResNet. The MR image generation method based on dense connection self-inverse GAN proposed in this paper can reduce the negative influence brought from missing T1 or T2 weighted images and provide more information for clinical judgment. © 2021 by Journal of Data Acquisition and Processing.","","Generative adversarial network; Image synthesis; Image-to-image translation; Virtual samples","Article","Final","","Scopus","2-s2.0-85114162479"
"Gan Y.; Xiang T.; Liu H.; Ye M.; Liu D.","Gan, Yan (57203150814); Xiang, Tao (57213003210); Liu, Hangcheng (57219900424); Ye, Mao (35241431500); Liu, Dan (57749316400)","57203150814; 57213003210; 57219900424; 35241431500; 57749316400","TEACHER-SUPERVISED GENERATIVE ADVERSARIAL NETWORKS","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428290","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126473128&doi=10.1109%2fICME51207.2021.9428290&partnerID=40&md5=6a200cd8fcf27fbb5f6644ef584d851a","Although generative adversarial networks (GANs) show impressive effects on image generation, existing GANs suffer an unstable training process, and thus result in poor image quality sometimes. To solve this problem, we first introduce a supervision mechanism into GANs and propose a teacher-supervised GAN (GAN-T) model. Specifically, we design a teacher supervision mechanism to inspect whether the features of generated images are as real as those of real images. If not, we add action into the generator. The action takes the encoding of the real image as prior knowledge to guide the generation of samples. We then apply our proposed method to existing GANs to show its compatibility with them. Finally, we conduct extensive experiments on the tasks of noise-to-image generation and image translation, and experimental results show that our proposed method can significantly stabilize the training process of the generator and improve the quality of generated images. © 2021 IEEE Computer Society. All rights reserved.","Computer vision; Image enhancement; Image generations; Image translation; Images synthesis; Prior-knowledge; Real images; Supervision mechanisms; T-model; Teacher supervision mechanism; Teachers'; Training process; Generative adversarial networks","GANs; image synthesis; prior knowledge; teacher supervision mechanism","Conference paper","Final","","Scopus","2-s2.0-85126473128"
"Kim S.; Jang H.; Hong S.; Hong Y.S.; Bae W.C.; Kim S.; Hwang D.","Kim, Sewon (57193012541); Jang, Hanbyol (57200415444); Hong, Seokjun (57226758548); Hong, Yeong Sang (57226748292); Bae, Won C. (7005750885); Kim, Sungjun (57197815473); Hwang, Dosik (10039447700)","57193012541; 57200415444; 57226758548; 57226748292; 7005750885; 57197815473; 10039447700","Fat-saturated image generation from multi-contrast MRIs using generative adversarial networks with Bloch equation-based autoencoder regularization","2021","Medical Image Analysis","73","","102198","","","","10.1016/j.media.2021.102198","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112514362&doi=10.1016%2fj.media.2021.102198&partnerID=40&md5=d30c3fca35e3df5d2c7516432d7716d7","Obtaining multiple series of magnetic resonance (MR) images with different contrasts is useful for accurate diagnosis of human spinal conditions. However, this can be time consuming and a burden on both the patient and the hospital. We propose a Bloch equation-based autoencoder regularization generative adversarial network (BlochGAN) to generate a fat saturation T2-weighted (T2 FS) image from T1-weighted (T1-w) and T2-weighted (T2-w) images of human spine. To achieve this, our approach was to utilize the relationship between the contrasts using Bloch equation since it is a fundamental principle of MR physics and serves as a physical basis of each contrasts. BlochGAN properly generated the target-contrast images using the autoencoder regularization based on the Bloch equation to identify the physical basis of the contrasts. BlochGAN consists of four sub-networks: an encoder, a decoder, a generator, and a discriminator. The encoder extracts features from the multi-contrast input images, and the generator creates target T2 FS images using the features extracted from the encoder. The discriminator assists network learning by providing adversarial loss, and the decoder reconstructs the input multi-contrast images and regularizes the learning process by providing reconstruction loss. The discriminator and the decoder are only used in the training process. Our results demonstrate that BlochGAN achieved quantitatively and qualitatively superior performance compared to conventional medical image synthesis methods in generating spine T2 FS images from T1-w, and T2-w images. © 2021","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Decoding; Diagnosis; Discriminators; Learning systems; Magnetic resonance; Signal encoding; Adversarial networks; Bloch equation; Fundamental principles; Image generations; Image synthesis; Learning process; Network learning; Training process; adult; article; autoencoder; human; network learning; nuclear magnetic resonance imaging; physics; quantitative analysis; spine; synthesis; image processing; Medical imaging","Autoencoder regularization; Bloch equation; Geneartive adversarial networks; Image synthesis; Magnetic resonance image; Multi-contrast imaging","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112514362"
"Selim M.; Zhang J.; Fei B.; Zhang G.-Q.; Chen J.","Selim, Md (57219739473); Zhang, Jie (57150866600); Fei, Baowei (7005499116); Zhang, Guo-Qiang (7405271191); Chen, Jin (57203334776)","57219739473; 57150866600; 7005499116; 7405271191; 57203334776","CT Image Harmonization for Enhancing Radiomics Studies","2021","Proceedings - 2021 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021","","","","1057","1062","5","10.1109/BIBM52615.2021.9669448","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125194113&doi=10.1109%2fBIBM52615.2021.9669448&partnerID=40&md5=ffccd9320a10c8a0e4186dcb46970ecb","While remarkable advances have been made in Computed Tomography (CT), most of the existing efforts focus on imaging enhancement while reducing radiation dose. How to normalize CT images acquired using non-standard protocols is vital for decision-making in cross-center large-scale radiomics studies but remains the boundary to explore. We develop a novel GAN-based image standardization algorithm called RadiomicGAN to mitigate the discrepancy caused by using non-standard acquisition protocols. In RadiomicGAN, a pre-trained U-Net has been adopted as part of the generator to learn radiomic feature distributions efficiently, and a novel training approach, called Window Training, has been developed to smoothly transform the pre-trained model to the medical imaging domain. In the experiments, we compared RadiomicGAN with four state-of-the-art CT image standardization approaches on both patient and phantom CT images acquired using three different reconstruction kernels. We objectively evaluated model performance based on more than 1,000 radiomic features. The results show that RadiomicGAN clearly outperforms the compared models. The source code, manual, and sample data are available at https://github.con selim-iitdu/radiomicGAN.  © 2021 IEEE.","Computer vision; Decision making; Generative adversarial networks; Image acquisition; Image enhancement; Medical imaging; Standardization; Acquisition protocols; Computed tomography images; Decisions makings; Harmonisation; Image standardization; Images synthesis; Large-scales; Learn+; Non-standard protocol; Radiomic; Computerized tomography","Computed Tomography; Generative Adversarial Network; Image Synthesis; Radiomics; Standardization","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125194113"
"Makhmudkhujaev F.; Park I.K.","Makhmudkhujaev, Farkhod (57200140017); Park, In Kyu (8612277600)","57200140017; 8612277600","Generative Adversarial Networks with Attention Mechanisms at Every Scale","2021","IEEE Access","9","","","168404","168414","10","10.1109/ACCESS.2021.3135637","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121835419&doi=10.1109%2fACCESS.2021.3135637&partnerID=40&md5=a1ff8b4931cddd09ce2ad878514f0cb4","Existing works in image synthesis have shown the efficiency of applying attention mechanisms in generating natural-looking images. Despite the great informativeness, current works utilize such mechanisms at a certain scale of generative and discriminative networks. Intuitively, the increased use of attention should lead to better performance. However, due to memory constraints, even moving a single attention mechanism to a higher scale of the network is infeasible. Motivated by the importance of attention in image generation, we tackle this limitation by proposing a generative adversarial network-based framework that readily incorporates attention mechanisms at every scale of its networks. A straightforward structure of attention mechanism enables direct plugging in a scale-wise manner and trains jointly with adversarial networks. As a result, networks are forced to focus on relevant regions of feature maps learned at every scale, thus improving their own image representation power. In addition, we exploit and show the usage of multiscale attention features as a complementary feature set in discriminator training. We demonstrate qualitatively and quantitatively that the introduction of scale-wise attention mechanisms benefits competitive networks, thus improving the performance compared with those of current works.  © 2013 IEEE.","Image enhancement; Image representation; Job analysis; 'current; Attention; Attention mechanisms; Game; Generator; Images synthesis; Multiscale; Performance; Task analysis; Generative adversarial networks","attention; generative adversarial networks; Image synthesis; multiscale","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121835419"
"Sarkar K.; Liu L.; Golyanik V.; Theobalt C.","Sarkar, Kripasindhu (57202320173); Liu, Lingjie (57195997876); Golyanik, Vladislav (57190134145); Theobalt, Christian (6507027272)","57202320173; 57195997876; 57190134145; 6507027272","HumanGAN: A Generative Model of Human Images","2021","Proceedings - 2021 International Conference on 3D Vision, 3DV 2021","","","","258","267","9","10.1109/3DV53792.2021.00036","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119325238&doi=10.1109%2f3DV53792.2021.00036&partnerID=40&md5=8a7cb19f62d61ce7731e7f1c88227ef0","Generative adversarial networks achieve great performance in photorealistic image synthesis in various domains,including human images. However,they usually employ latent vectors that encode the sampled outputs globally. This does not allow convenient control of semantically relevant individual parts of the image,and cannot draw samples that only differ in partial aspects,such as clothing style. We address these limitations and present a generative model for images of dressed humans offering control over pose,local body part appearance and garment style. This is the first method to solve various aspects of human image generation,such as global appearance sampling,pose transfer,parts and garment transfer,and part sampling jointly in a unified framework. As our model encodes part-based latent appearance vectors in a normalized pose-independent space and warps them to different poses,it preserves body and clothing appearance under varying posture. Experiments show that our flexible and general generative method outperforms task-specific baselines for pose-conditioned image generation,pose transfer and part sampling in terms of realism and output resolution.  © 2021 IEEE.","Computer vision; Generative adversarial networks; Vector spaces; Body parts; Clothing styles; Generative model; Global appearances; Image generations; Images synthesis; Latent vectors; Partial aspects; Performance; Photorealistic images; Encoding (symbols)","","Conference paper","Final","","Scopus","2-s2.0-85119325238"
"Pan Y.; Liu M.; Xia Y.; Shen D.","Pan, Yongsheng (56440550200); Liu, Mingxia (36677833300); Xia, Yong (26427407400); Shen, Dinggang (7401738392)","56440550200; 36677833300; 26427407400; 7401738392","Disease-image-specific Learning for Diagnosis-oriented Neuroimage Synthesis with Incomplete Multi-Modality Data","2021","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","","","","10.1109/TPAMI.2021.3091214","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112447183&doi=10.1109%2fTPAMI.2021.3091214&partnerID=40&md5=3b8458a6f67e8f7b6b7d6aca1d1bf06e","Incomplete data problem is commonly existing in disease diagnosis with multi-modality neuroimages, to track which, some methods have been proposed to utilize all available subjects by imputing missing neuroimages. However, these methods usually treat image synthesis and disease diagnosis as two standalone tasks, thus ignoring the specificity conveyed in different modalities, i.e., different modalities may highlight different disease-relevant regions in the brain. To this end, we propose a disease-image-specific deep learning (DSDL) framework for joint neuroimage synthesis and disease diagnosis using incomplete multi-modality neuroimages. Specifically, with each whole-brain scan as input, we first design a Disease-image-Specific Network (DSNet) with a spatial cosine module to implicitly model the disease-image specificity. We then develop a Feature-consistency Generative Adversarial Network (FGAN) to impute missing neuroimages, where feature maps (generated by DSNet) of a synthetic image and its respective real image are encouraged to be consistent while preserving the disease-image-specific information. Since our FGAN is correlated with DSNet, missing neuroimages can be synthesized in a diagnosis-oriented manner. Experimental results on three datasets suggest that our method can not only generate reasonable neuroimages, but also achieve state-of-the-art performance in both tasks of Alzheimer's disease identification and mild cognitive impairment conversion prediction. IEEE","Algorithms; Alzheimer Disease; Brain; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Neuroimaging; Deep learning; Image processing; Neurodegenerative diseases; Adversarial networks; Alzheimer's disease; Disease diagnosis; Feature consistency; Mild cognitive impairments; Specific information; Specific learning; State-of-the-art performance; algorithm; Alzheimer disease; brain; computer assisted diagnosis; diagnostic imaging; human; neuroimaging; nuclear magnetic resonance imaging; procedures; Diagnosis","Diseases; Generative adversarial networks; Image synthesis; Magnetic resonance imaging; Medical diagnosis; Neuroimaging; Task analysis","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85112447183"
"Wu C.; Zheng R.; Zang H.; Liu M.; Xu J.; Zhan S.","Wu, Congzhong (57193276871); Zheng, Rongsheng (57205688484); Zang, Huaijuan (56425381200); Liu, Mingwei (57223438610); Xu, Jiajia (57223033982); Zhan, Shu (24765737400)","57193276871; 57205688484; 56425381200; 57223438610; 57223033982; 24765737400","Face pose correction based on morphable model and image inpainting; [结合形变模型与图像修复的人脸姿态矫正]","2021","Journal of Image and Graphics","26","4","","828","836","8","10.11834/jig.200011","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105854416&doi=10.11834%2fjig.200011&partnerID=40&md5=59c544d66fca78a5ae42b8dd06a05b9b","Objective: Face recognition has been a widely studied topic in the field of computer vision for a long time. In the past few decades, great progress in face recognition has been achieved due to the capacity and wide application of convolutional neural networks. However, pose variations still remain a great challenge and warrant further studies. To the best of our knowledge, the existing methods that address this problem can be generally categorized into two classes: feature-based methods and deep learning-based methods. Feature-based methods attempt to obtain pose-invariant representations directly from non-frontal faces or design handcrafted local feature descriptors, which are robust to face poses. However, it is often too difficult to obtain robust representation of the face pose using these handcrafted local feature descriptors. Thus, these methods cannot produce satisfactory results, especially when the face pose is too large. In recent years, convolutional neural networks have been introduced in face recognition problems due to their outstanding performance in image classification tasks. Different from traditional methods, convolutional neural networks do not require the manual extraction of local feature descriptors. They try to directly rotate the face image of arbitrary pose and illuminate into the target pose, which maintains the face identity feature well. In addition, due to the powerful ability of image generation, generative adversarial network is also used in frontal face image synthesis and has achieved great progress. Compared with traditional methods, deep learning-based methods can obtain a higher face recognition rate. However, the disadvantage of deep learning-based methods is that the face images synthesized from the large face pose have low credibility, which lead to poor face recognition accuracy. To deal with the limitations of these two kinds of methods, we present a face pose correction algorithm based on 3D morphable model (3DMM) and image inpainting. Method: In this study, we propose a face frontalization method by combining deep learning model and a 3DMM, which can generate a photorealistic frontal view of the face image. In detail, we first detect facial landmarks by using a well-known facial landmark detector, which is robust to large pose variations. We detect a total of 68 facial landmarks to fit the face image more accurately. Then, we perform accurate 3DMM fitting for face image with facial landmark weighting. Next, we estimate the depth information of the face image and rotate the 3D face model into frontal view using 3D transformation. Finally, we employ image inpainting for the irregular facial invisible region caused by self-occlusion by utilizing deep learning model. We fine-tune the pre-trained model to train our image inpainting model. In the training process, all of the convolutional layers are replaced with partial convolutional layers. Our training set consists of 13 223 face images that are selected from the labeled faces in the wild (LFW) dataset. Our image inpainting network is implemented in Keras. The batch size is set to 4, the learning rate is set to 10-4, and the weight decay is 0.000 5. The network training procedure is accelerated using NVIDIA GTX 1080 Ti GPU devices, which takes approximately 10 days in total. Result: We compare our method with state-of-the-art methods, including the traditional method and deep learning method, on two public face datasets, namely, LFW dataset and StirlingESRC 3D face dataset. The quantitative evaluation metric is face recognition rate under different face poses, and we provide several synthesized frontal face images by our method. The synthesized frontal face images show that our method can produce more photorealistic results than other methods in the LFW dataset. We achieve 96.57% face recognition accuracy on the LFW face dataset. In addition, the quantitative experiment results show that our method outperforms all other methods in StirlingESRC 3D face dataset. The experimental results show that the face recognition accuracy of our method is improved under different face poses. Compared with the other two methods in the StirlingESRC 3D face dataset, the face recognition accuracy increased by 5.195% and 2.265% under the face pose of 22° and by 5.875% and 11.095% under the face pose of 45°, respectively. Moreover, the average face recognition rate increased by 5.53% and 7.13%, respectively. The experimental results show that the proposed multi-pose face recognition algorithm improves the accuracy of face recognition. Conclusion: In this study, we propose a face pose correction algorithm for multi-pose face recognition by combining 3DMM with deep learning model. The qualitative and quantitative experiment results show that our method can synthesize a more photorealistic frontal face image than other methods and can improve the accuracy performance of multi-pose face recognition. © 2021, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","3D morphable model (3DMM); Convolutional neural network(CNN); Deep learning; Image inpainting; Multi-pose face recognition","Article","Final","","Scopus","2-s2.0-85105854416"
"Demergis D.","Demergis, Dimitri (57213520914)","57213520914","Comparative Analysis of Machine Learning Techniques for Island Heightmap Generation","2021","Proceedings of the International Joint Conference on Neural Networks","2021-July","","","","","","10.1109/IJCNN52387.2021.9533580","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116485844&doi=10.1109%2fIJCNN52387.2021.9533580&partnerID=40&md5=5b00e6620372ec29ee03e15b20a5941c","Traditional approaches to terrain heightmap generation rely on geometric methods to generate a matrix of elevation values using noise functions. More advanced approaches attempt to model the natural processes that shape landmasses in the real world, such as wind, moisture, and rainfall. This survey leverages recent advancements in image generation using generative machine learning models in order to evaluate their effectiveness in this problem space. A variational autoencoder (VAE), generative adversarial network (GAN), and PixelCNN network are trained on real-world island heightmap data and produce realistic island terrain. The author compares the results in terms of image quality and 'closeness' to the original real images, and evaluates the trade-off between quality and training/generation speed. © 2021 IEEE.","Deep learning; Economic and social effects; Landforms; Quality control; Variational techniques; Auto encoders; Deep learning; Generative model; Heightmap; Images synthesis; Landscape generation; Pixelcnn; Pixelrnn; Terrain generations; Variational autoencoder; Generative adversarial networks","deep learning; generative adversarial network; generative models; heightmap; image synthesis; landscape generation; machine learning; PixelCNN; PixelRNN; terrain generation; variational autoencoder","Conference paper","Final","","Scopus","2-s2.0-85116485844"
"Zhi R.; Guo Z.; Zhang W.; Wang B.; Kaiser V.; Wiederer J.; Flohr F.B.","Zhi, Rong (57221705477); Guo, Zijie (57221704643); Zhang, Wuqiang (57221705237); Wang, Baofeng (57211025882); Kaiser, Vitali (57193017746); Wiederer, Julian (57189849695); Flohr, Fabian B. (55263777300)","57221705477; 57221704643; 57221705237; 57211025882; 57193017746; 57189849695; 55263777300","Pose-guided person image synthesis for data augmentation in pedestrian detection","2021","IEEE Intelligent Vehicles Symposium, Proceedings","2021-July","","","1493","1500","7","10.1109/IV48863.2021.9575574","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118865128&doi=10.1109%2fIV48863.2021.9575574&partnerID=40&md5=0368ed67cd118c72af8d722b2fc5143a","In this paper, we present a data augmentation framework for pedestrian detection using a pose-guided person image synthesis model. The proposed framework can boost the performance of state-of-the-art pedestrian detectors by generating new and unseen pedestrian training samples with controllable appearances and poses. This is achieved by a new latent-consistent adversarial variational auto-encoder (LAVAE) model, leveraging the advantages of conditional variational auto-encoders and conditional generative adversarial networks to disengage and reconstruct person images conditioned on target poses. An additional latent regression path is introduced to preserve appearance information and to guarantee a spatial alignment during transfer. LAVAE goes beyond existing works in restoring structural information and perceptual details with limited annotations and can further benefit the pedestrian detection task in automated driving scenarios. Extensive pedestrian detection and person image synthesis experiments are performed on the EuroCity Person dataset. We show that data augmentation using LAVAE improves the accuracy of state-of-the-art pedestrian detectors significantly. Furthermore, a competitive performance can be observed when we compare LAVAE with other generative models for person image synthesis. © 2021 IEEE.","Computer vision; Learning systems; Auto encoders; Data augmentation; Images synthesis; Pedestrian detection; Performance; Spatial alignment; State of the art; Structural information; Synthesis models; Training sample; Signal encoding","","Conference paper","Final","","Scopus","2-s2.0-85118865128"
"Sumi P.; Sindhuja S.; Sureshkumar S.","Sumi, Philo (57226834477); Sindhuja, S. (57226833318); Sureshkumar, S. (57226830923)","57226834477; 57226833318; 57226830923","A comparison between AttnGAN and DF GAN: Text to image synthesis","2021","2021 3rd International Conference on Signal Processing and Communication, ICPSC 2021","","","9451789","615","619","4","10.1109/ICSPC51351.2021.9451789","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112847788&doi=10.1109%2fICSPC51351.2021.9451789&partnerID=40&md5=6706b0c7f0a8e801d0441a5f7708f963","Nowadays conversion from text to high resolution image is a challenging task due to its wide variety of application area. For text to image conversion almost all systems use Generative Adversarial Networks as the basic part of the system and GAN guarantees semantic consistency between the text input and the generated image output. In this paper we are comparing two algorithms that is used for generating image from text. The first algorithm is the AttnGAN and the second one is the DF-GAN. AttnGAN builds on top of StackGAN by using attention network which allows it to capture word level information along with the broader sentence level information. The second algorithm is the DF-GAN, which uses single generator and discriminator model to synthesize high resolution images and also uses Matching-Aware Gradient Penalty (MA-GP) to get real images with real description. The model contains a Deep text-image Fusion Block (DFBlock) to generate image features from text. Both algorithms work efficiently for image generation from text but DF-GAN generates the perfect output than AttnGAN. The AttnGAN always focus on the textual part to generate output image but DF-GAN also focuses on background of image. © 2021 IEEE.","Image fusion; Semantics; Adversarial networks; Application area; High resolution image; Image conversion; Image generations; Image synthesis; Semantic consistency; Sentence level; Image processing","AttnGAN; BLSTM; CNN; DF GAN","Conference paper","Final","","Scopus","2-s2.0-85112847788"
"Chen K.-H.; Lin B.-S.","Chen, Kung-Hao (57336915000); Lin, Bor-Shen (55466840000)","57336915000; 55466840000","Modularized Architecture of Attribute Generative Adversarial Network for Image Synthesis","2021","4th IEEE International Conference on Knowledge Innovation and Invention 2021, ICKII 2021","","","","140","145","5","10.1109/ICKII51822.2021.9574660","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118982107&doi=10.1109%2fICKII51822.2021.9574660&partnerID=40&md5=3ead765ce444350ef5fd710169fdf217","In recent years, neural network models have achieved great success in the application of image-to-image translation based on conditional or cyclic generative adversarial networks. A variety of extended models have been developed to perform the multi-task image translation to synthesize output images according to the target attributes. In this paper, we propose a novel network architecture with multiple attribute discriminators that are pre-trained independently with attribute-relevant data, instead of being trained as a part of the generative adversarial network. Discriminators of extra attributes are added into the network incrementally, and the network is trained much faster since it is unnecessary to train the whole network for all attributes from scratch. This new design methodology makes the network modularized and extensible for new attributes in image translation tasks after the network has been trained with given attributes. We verified the proposed approach on an image-translation task that changes the attributes of the emoji images. Experimental result shows that the quality of image with multiple attributes generated by this approach is compatible with that by the conventional network. © 2021 IEEE.","Computer vision; Network architecture; CGAN; Design Methodology; Extended model; GAN; Image translation; Images synthesis; Modularized architecture; Multi tasks; Multiple attributes; Neural network model; Generative adversarial networks","CGAN; GAN; Image translation; Modularized architecture","Conference paper","Final","","Scopus","2-s2.0-85118982107"
"Marnerides D.; Bashford-Rogers T.; Debattista K.","Marnerides, Demetris (57203394055); Bashford-Rogers, Thomas (36052335600); Debattista, Kurt (10240295400)","57203394055; 36052335600; 10240295400","Deep hdr hallucination for inverse tone mapping","2021","Sensors","21","12","4032","","","","10.3390/s21124032","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107499018&doi=10.3390%2fs21124032&partnerID=40&md5=f14ca8108f1669f5525c068e1c868059","Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range (HDR) information from Low Dynamic Range (LDR) image content. The dynamic range of well-exposed areas must be expanded and any missing information due to over/under-exposure must be recovered (hallucinated). The majority of methods focus on the former and are relatively successful, while most attempts on the latter are not of sufficient quality, even ones based on Convolutional Neural Networks (CNNs). A major factor for the reduced inpainting quality in some works is the choice of loss function. Work based on Generative Adversarial Networks (GANs) shows promising results for image synthesis and LDR inpainting, suggesting that GAN losses can improve inverse tone mapping results. This work presents a GAN-based method that hallucinates missing information from badly exposed areas in LDR images and compares its efficacy with alternative variations. The proposed method is quantitatively competitive with state-of-the-art inverse tone mapping methods, providing good dynamic range expansion for well-exposed areas and plausible hallucinations for saturated and under-exposed areas. A density-based normalisation method, targeted for HDR content, is also proposed, as well as an HDR data augmentation method targeted for HDR hallucination. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Hallucinations; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Convolutional neural networks; Image enhancement; Mapping; Adversarial networks; Data augmentation; Dynamic range expansions; High dynamic range; Inverse tone mappings; Low dynamic range; Missing information; State of the art; hallucination; human; image processing; Inverse problems","Deep learning; High dynamic range; Inverse tone mapping","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85107499018"
"Denck J.; Guehring J.; Maier A.; Rothgang E.","Denck, Jonas (57209571095); Guehring, Jens (13907262000); Maier, Andreas (23392966100); Rothgang, Eva (51864404400)","57209571095; 13907262000; 23392966100; 51864404400","Enhanced magnetic resonance image synthesis with contrast-aware generative adversarial networks","2021","Journal of Imaging","7","8","133","","","","10.3390/jimaging7080133","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113779132&doi=10.3390%2fjimaging7080133&partnerID=40&md5=ba2220c1351ad3c764e0b784251ef2e6","A magnetic resonance imaging (MRI) exam typically consists of the acquisition of multiple MR pulse sequences, which are required for a reliable diagnosis. With the rise of generative deep learning models, approaches for the synthesis of MR images are developed to either synthesize addi-tional MR contrasts, generate synthetic data, or augment existing data for AI training. While current generative approaches allow only the synthesis of specific sets of MR contrasts, we developed a method to generate synthetic MR images with adjustable image contrast. Therefore, we trained a generative adversarial network (GAN) with a separate auxiliary classifier (AC) network to generate synthetic MR knee images conditioned on various acquisition parameters (repetition time, echo time, and image orientation). The AC determined the repetition time with a mean absolute error (MAE) of 239.6 ms, the echo time with an MAE of 1.6 ms, and the image orientation with an accuracy of 100%. Therefore, it can properly condition the generator network during training. Moreover, in a visual Turing test, two experts mislabeled 40.5% of real and synthetic MR images, demonstrating that the image quality of the generated synthetic and real MR images is comparable. This work can support radiologists and technologists during the parameterization of MR sequences by previewing the yielded MR contrast, can serve as a valuable tool for radiology training, and can be used for customized data generation to support AI training. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Adversarial learning; Deep learning; Image synthesis; Magnetic resonance imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113779132"
"Yu P.; Shi Q.; Wang H.","Yu, Peilun (57221044096); Shi, Quan (36941479000); Wang, Han (57195290300)","57221044096; 36941479000; 57195290300","Infrared-to-visible image translation based on parallel generator network; [并行生成网络的红外-可见光图像转换]","2021","Journal of Image and Graphics","26","10","","2346","2356","10","10.11834/jig.200113","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117693635&doi=10.11834%2fjig.200113&partnerID=40&md5=452a624e864f2a330da32637ba390f20","Objective: Image-to-image translation involves the automated conversion of input data into a corresponding output image, which differs in characteristics such as color and style. Examples include converting a photograph to a sketch or a visible image to a semantic label map. Translation has various applications in the field of computer vision such facial recognition, person identification, and image dehazing. In 2014, Goodfellow proposed an image generation model based on generative adversarial networks (GANs). This algorithm uses a loss function to classify output images as authentic or fabricated while simultaneously training a generative model to minimize loss. GANs have achieved impressive image generation results using adversarial loss specifically. For example, the image-to-image translation framework Pix2Pix was developed using a GAN architecture. Pix2Pix operates by learning a conditional generative model from input-output image pairs, which is more suitable for translation tasks. In addition, U-Net has often been used as generator networks in place of conventional decoders. While Pix2Pix provides a robust framework for image translation, acquiring sufficient quantities of paired input-output training data can be challenging. In order to solve this problem, cycle-consistent adversarial networks (CycleGANs) were developed by adding an inverse mapping and cycle consistency loss to enforce the relationship between generated and input images. In addition, ResNets have been used as generators to enhance translated image quality. Pix2PixHD offers high-resolution (2 048 × 1 024 pixels) output using a modified multiscale generator network that includes an instance map in the training step. Although these algorithms have effectively been used for image-to-image translation and a variety of related applications, they typically adopt U-Net or ResNet generators. These single-structure networks struggle to keep high performance across multiple evaluation indicators. As such, this study presents a novel parallel stream-based generator network to increase the robustness across multiple evaluation indicators. Unlike in previous studies, this model consists of two entirely different convolutional neural network (CNN) structures. The output translated visible image of each stream is fused with a linear interpolation-based fusion method to allow for simultaneous optimization of parameters in each model. Method The proposed parallel generator network consists of one ResNet processing stream and one DenseNet processing stream, which are fused in parallel. The ResNet stream includes down-sampling and nine Res-Unit feature extraction networks. Each Res-Unit consists of a feedforward neural network exhibiting elementwise addition. Two convolution layers are skipped. Similarly, the DenseNet stream includes down-sampling and nine Den-Unit feature extraction networks. Every Den-Unit is composed of three convolutional layers and two concatenation layers. As a result, the Den-Units output a concatenation of deep feature maps produced in all three convolutional layers. To utilize the advantages of both ResNet and DenseNet streams, two generated images are segmented into low-and high-intensity image parts with an optimal intensity threshold. Then, a linear interpolation method is proposed to fuse the segmented output images of two generator streams in the R, G, B channel respectively. We also design an intensity threshold objective function to obtain optimal parameters in the generator raining process. In addition, to avoid overfitting during training under a small dataset, we modify the discriminator structure by including four convolution-dropout pairs and a convolution layer. Result We compared our model with six state-of-the-art saliency models, including CRN(cascaded refinement networks), SIMS(semi-parametric image synthesis), Pix2Pix(pixel to pixel), CycleGAN(cycle generative adversarial networks), MUNIT(multimodal unsupervised image-to-image translation) and GauGAN(group adaptive normalization generative adversarial networks), on a public dataset named ""AAU(Aalborg University) RainSnow Traffic Surveillance Dataset"". The experimental dataset, which was composed of 22 5-min video sequences acquired from traffic intersections in the Danish cities of Aalborg and Viborg, was used for testing purposes. This dataset was collected at seven different locations with a conventional RGB camera and a thermal camera, each with a resolution of 640 × 480 pixels, at 20 frames per second. The total experimental dataset consisted of 2 100 RGB-IR image pairs, and each scene was then randomly divided into training and test datasets by 80%-20%. In this study, multi-perspective evaluation results were acquired using the mean square error (MSE), structural similarity index (SSIM), gray intensity histogram correlation, and Bhattacharyya distance. The advantages of a parallel stream-based generator network were assessed by comparing the proposed parallel generator with a ResNet, DenseNet, and residual dense block (RDN)-based hybrid network. We evaluated the average MSE and SSIM values for the test data, produced using four different generators (ParaNet, ResNet, DenseNet, and RDN). The proposed method achieved an average MSE of 34.835 8, which was lower than that of ResNet, DenseNet, and hybrid RDN network. Simultaneously, the average SSIM value produced with the proposed method was 0.747 7, which was also higher than that of DenseNet, ResNet, and RDN. This result shows that the proposed parallel structure-based network produced more effective fusion results than RDB-based hybrid network structure. Moreover, comparative experiments demonstrated that parallel generator structure improves the robustness performance across multi-perspective evaluations for infrared-to-visible image translation. Compared with the six conventional methods, the MSE performance (lower is better) increased by at least 22.30%, and the SSIM (higher is better) decreased by at least 8.55%. The experimental results show that the proposed parallel generator network-based infrared-to-visible image translation deep learning model achieves high performance in terms of MSE or SSIM compared with conventional deep learning models such as CRN, SIMS, Pix2Pix, CycleGAN, MUNIT, and GauGAN. Conclusion: A novel parallel stream architecture-based generator network was proposed for infrared-to-visible image translation. Unlike conventional models, the proposed parallel generator structure consists of two different network architectures: a ResNet and a DenseNet. Parallel linear combination-based fusion allowed the model to incorporate benefits from both networks simultaneously. The structure of discriminator networks used in the conditional GAN framework was also improved for training and identifying optimal ParaNet parameters. The experimental results showed that the inclusion of different networks led to increases in common assessment metrics. The MSE, SSIM, and intensity histogram similarity for the proposed parallel generator network were higher than those of existing models. In the future, this algorithm will be applied to image dehazing. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","DenseNet; Linear interpolation fusion; Modal translation; Parallel generator network; ResNet","Article","Final","","Scopus","2-s2.0-85117693635"
"Long J.; Lu H.","Long, Jia (57222172775); Lu, Hongtao (8943716200)","57222172775; 8943716200","Generative adversarial networks with Bi-directional normalization for semantic image synthesis","2021","ICMR 2021 - Proceedings of the 2021 International Conference on Multimedia Retrieval","","","","219","226","7","10.1145/3460426.3463651","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114897669&doi=10.1145%2f3460426.3463651&partnerID=40&md5=72cd90da0be9db67ae8ca2938c2c7835","Semantic image synthesis aims at translating semantic label maps to photo-realistic images. However, most of previous methods easily generate blurred regions and artifacts, and the quality of these images is far from realistic. There are two unresolved problems existing: first, these methods directly feed the semantic label as input to the deep network, through convolution operation to produce the normalization parameters γand β, we find that the semantic labels are different from real scene images, they are not able to provide detailed structural information, making it difficult to synthesize local details and structures; second, there are no bi-directional information flow between the semantic labels and the real scene images, this leads to inefficiently utilize the semantic information and maintain semantic constrains to preserve the semantic information in the process of semantic image synthesis. We propose Bi-directional Normalization (BDN) in our generative adversarial networks to solve these problems, which allows semantic label information and real scene image feature representation to be effectively utilized by a bi-directional way for generating high quality images. Extensive experiments on several challenging datasets demonstrate significantly better than that results of existing methods in both visual fidelity and quantitative metrics. © 2021 ACM.","Semantic Web; Semantics; Adversarial networks; High quality images; Photorealistic images; Quantitative metrics; Semantic images; Semantic information; Structural information; Visual fidelity; Image processing","Generative adversarial network; Normalization; Semantic image synthesis","Conference paper","Final","","Scopus","2-s2.0-85114897669"
"Toikkanen M.; Kwon D.; Lee M.","Toikkanen, Miika (57286454000); Kwon, Doyoung (57212002790); Lee, Minho (57191730119)","57286454000; 57212002790; 57191730119","ReSGAN: Intracranial Hemorrhage Segmentation with Residuals of Synthetic Brain CT Scans","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12901 LNCS","","","400","409","9","10.1007/978-3-030-87193-2_38","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116493465&doi=10.1007%2f978-3-030-87193-2_38&partnerID=40&md5=f20b6ea70925039f9494eff964cb5c64","Intracranial hemorrhage (ICH) is a dangerous condition of bleeding within the skull that calls for rapid and precise diagnosis due to potentially fatal consequences. In this paper, we propose Residual Segmentation with Generative Adversarial Networks (ReSGAN) to accurately localize the hemorrhage from computerized tomography (CT) scans with a GAN-based model. Although convolutional neural networks have shown success in the ICH segmentation task, precise localization remains challenging due to in-balance and scarcity of labeled training data. Synthetic samples from generative models, and aligned templates as reference from brain atlas have been demonstrated to alleviate the issues. We consider synthetic templates as another candidate and solve the problem by directly applying a generative model to segmentation. Our ReSGAN learns a distribution of pseudo-normal brain CT scans, that through residuals, reliably delineates the hemorrhaging areas. We perform experiments on two datasets and compare our model against a well established baseline, that consistently shows significant improvements, therefore demonstrating the validity of our novel method. © 2021, Springer Nature Switzerland AG.","Computerized tomography; Convolutional neural networks; Diagnosis; Image segmentation; Medical imaging; Semantics; Bleedings; Computerized tomography scan; Condition; Generative model; Images synthesis; Intracranial hemorrhages; Non-contrast computerized tomography; Segmentation; Semantic image synthesis; Semantic images; Generative adversarial networks","Brain; Intracranial hemorrhage; Non-contrast CT; Segmentation; Semantic image synthesis","Conference paper","Final","","Scopus","2-s2.0-85116493465"
"Gu X.; Yu J.; Wong Y.; Kankanhalli M.S.","Gu, Xiaoling (56006961700); Yu, Jun (56145221000); Wong, Yongkang (35085446500); Kankanhalli, Mohan S. (7003629165)","56006961700; 56145221000; 35085446500; 7003629165","Toward Multi-Modal Conditioned Fashion Image Translation","2021","IEEE Transactions on Multimedia","23","","9141513","2361","2371","10","10.1109/TMM.2020.3009500","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111695095&doi=10.1109%2fTMM.2020.3009500&partnerID=40&md5=4dff108bede951efdab699746d4e2a79","Having the capability to synthesize photo-realistic fashion product images conditioned on multiple attributes or modalities would bring many new exciting applications. In this work, we propose an end-to-end network architecture that built upon a new generative adversarial network for automatically synthesizing photo-realistic images of fashion products under multiple conditions. Given an input pose image that consists of a 2D skeleton pose and a sentence description of products, our model synthesizes a fashion image preserving the same pose and wearing the fashion products described as the text. Specifically, the generator G tries to generate realistic-looking fashion images based on a langle mathsf {pose}, mathsf {text} rangle pair condition to fool the discriminator. An attention network is added for enhancing the generator, which predicts a probability map indicating which part of the image needs to be attended for translation. In contrast, the discriminator D distinguishes real images from the translated ones based on the input pose image and text description. The discriminator is divided into two multi-scale sub-discriminators for improving image distinguishing task. Quantitative and qualitative analysis demonstrates that our method is capable of synthesizing realistic images that retain the poses of given images while matching the semantics of provided sentence descriptions.  © 1999-2012 IEEE.","Network architecture; Semantics; Adversarial networks; Image distinguishing; Image translation; Multiple attributes; Photorealistic images; Probability maps; Quantitative and qualitative analysis; Realistic images; Image enhancement","fashion image synthesis; Generative adversarial network; image-to-image translation","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85111695095"
"Qu Y.; Su W.; Lv X.; Deng C.; Wang Y.; Lu Y.; Chen Z.; Xiao N.","Qu, Yili (57202442730); Su, Wanqi (57217050591); Lv, Xuan (57221005039); Deng, Chufu (57217042232); Wang, Ying (56374854200); Lu, Yutong (56174806000); Chen, Zhiguang (55705765300); Xiao, Nong (7006517717)","57202442730; 57217050591; 57221005039; 57217042232; 56374854200; 56174806000; 55705765300; 7006517717","Synthesis of Registered Multimodal Medical Images with Lesions","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12396 LNCS","","","774","786","12","10.1007/978-3-030-61609-0_61","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096601538&doi=10.1007%2f978-3-030-61609-0_61&partnerID=40&md5=3a84a12faa3b56f933401015f3b4aff7","The collection and annotation of medical images data have always been a challenge in many data-driven medical image processing tasks, especially for registered multimodal medical images data. This can be effectively alleviated by utilizing the image synthesis technology. However, directly-synthesized medical images generated by current methods usually have unreasonable structures or contours and uncontrollable lesions. In this paper, we proposed a new method to synthesize registered multimodal medical images from a random normal distribution matrix based on the Generative Adversarial Networks. Besides, the corresponding lesions can be generated efficiently based on the selected lesion labels. We performed validation experiments on multiple public datasets to verify the effectiveness of synthetic lesions and the availability of synthetic data. The results show that our synthetic data can be used as pre-trained data or enhanced data in medical image intelligent processing tasks to greatly improve the generalization ability of the model. © 2020, Springer Nature Switzerland AG.","Image enhancement; Neural networks; Normal distribution; Adversarial networks; Data driven; Generalization ability; Image synthesis; Intelligent processing; Medical images datum; Multimodal medical images; Synthetic data; Medical image processing","Image synthesis; Lesions; Medical images; Multimodal","Conference paper","Final","","Scopus","2-s2.0-85096601538"
"Ko D.H.; Ul Muhammad A.U.H.; Majeed S.; Choi J.","Ko, Debbie Honghee (57222588400); Ul Muhammad, Ammar Ul Hassan (57338911800); Majeed, Saima (57210803355); Choi, Jaeyoung (56812522400)","57222588400; 57338911800; 57210803355; 56812522400","Font2Fonts: A modified image-to-image translation framework for font generation","2020","ACM International Conference Proceeding Series","","","","288","294","6","10.1145/3426020.3426094","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119096475&doi=10.1145%2f3426020.3426094&partnerID=40&md5=a06899b10a9d7d741a0c5489c7c741a5","Generating a font from scratch requires font domain knowledge additionally, it's a labor intensive and time-consuming task. With the remarkable success of deep learning methods for image synthesis, many researchers are focusing on generating fonts by utilizing these methods. In order to utilize these deep learning methods for font generation, language specific font image datasets are manually prepared which is a cumbersome and time-consuming task. Additionally, existing supervised image-to-image translation methods like pix2pix are able to do only one-to-one domain translation therefore they cannot be applied to font generation task which is multi-domain. In this paper, we propose a model, Font2Fonts, a conditional generative adversarial network (GAN) for font synthesis in a supervised setting. Unlike pix2pix which can only translate from one font domain to the other, Font2Fonts is a multi-domain translation model. The proposed method can synthesize high quality diverse font images using a single end-to-end network. By our qualitative and quantitative experiments, we verify the effectiveness of our proposed model. Moreover, we also propose a Unicode-based module for automatically generating font image dataset. Our proposed Unicode-based method can be easily applied for preparing font dataset of various language characters.  © 2020 ACM.","Deep learning; Domain Knowledge; Translation (languages); Font generation; Font images; Image datasets; Image translation; Image-to-image translation; Learning methods; Multi-domains; Style transfer; Time-consuming tasks; Unicodes; Generative adversarial networks","Font generation; Generative adversarial network; Image-to-Image translation; Style transfer","Conference paper","Final","","Scopus","2-s2.0-85119096475"
"Behera A.P.; Godage S.; Verma S.; Kumar M.","Behera, Adarsh Prasad (57223436044); Godage, Sayli (57224413053); Verma, Shekhar (57199473632); Kumar, Manish (57713983100)","57223436044; 57224413053; 57199473632; 57713983100","Regularized Deep Convolutional Generative Adversarial Network","2021","Communications in Computer and Information Science","1378 CCIS","","","452","464","12","10.1007/978-981-16-1103-2_38","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107470526&doi=10.1007%2f978-981-16-1103-2_38&partnerID=40&md5=cf20a75f66ee3cc1f645f89d913f16d0","The unique adversarial approach in training deep models of Generative Adversarial Networks (GANs) results in high-quality image synthesis. With minor tweaks in architecture, training methods, learning types, many variants of GANs are proposed. Despite the development of different variants of GANs, there is an absence of work on the comparison of GAN and its regularized variant. This work focuses on the analysis of DCGAN and Regularized DCGAN. First, we define two unique methods, such as direct and indirect classifications for performance evaluation of GANs using image classification through CNN. We compare both the models according to different parameters such as architecture, training methods, accuracy with CNN classifier, distribution of real and synthesized data, discriminator loss, and generator loss on the benchmark MNIST and CIFAR10 data sets. Comparison results show significant improvement in generated image quality, higher direct and indirect classification accuracy, and better learning of distributions in the case of Regularized DCGAN. © 2021, Springer Nature Singapore Pte Ltd.","Classification (of information); Computer vision; Image enhancement; Network architecture; Adversarial networks; Classification accuracy; Comparison result; High quality images; Training methods; Convolutional neural networks","Adversarial; CIFAR10; CNN classifier; DCGAN; Direct classification; GAN; Indirect classification; MNIST; Regularization","Conference paper","Final","","Scopus","2-s2.0-85107470526"
"Wang Y.; Zhou L.; Wang M.; Shao C.; Shi L.; Yang S.; Zhang Z.; Feng M.; Shan F.; Liu L.","Wang, Yunpeng (57768362200); Zhou, Lingxiao (57209853700); Wang, Mingming (57188999481); Shao, Cheng (57218220182); Shi, Lili (57218220944); Yang, Shuyi (57216224359); Zhang, Zhiyong (56119440000); Feng, Mingxiang (34771287500); Shan, Fei (16556633000); Liu, Lei (57214941859)","57768362200; 57209853700; 57188999481; 57218220182; 57218220944; 57216224359; 56119440000; 34771287500; 16556633000; 57214941859","Combination of generative adversarial network and convolutional neural network for automatic subcentimeter pulmonary adenocarcinoma classification","2020","Quantitative Imaging in Medicine and Surgery","10","6","","1249","1264","15","10.21037/QIMS-19-982","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088366213&doi=10.21037%2fQIMS-19-982&partnerID=40&md5=31184eafe826ba2729c6ae53cd240ad1","Background: The efficient and accurate diagnosis of pulmonary adenocarcinoma before surgery is of considerable significance to clinicians. Although computed tomography (CT) examinations are widely used in practice, it is still challenging and time-consuming for radiologists to distinguish between different types of subcentimeter pulmonary nodules. Although there have been many deep learning algorithms proposed, their performance largely depends on vast amounts of data, which is difficult to collect in the medical imaging area. Therefore, we propose an automatic classification system for subcentimeter pulmonary adenocarcinoma, combining a convolutional neural network (CNN) and a generative adversarial network (GAN) to optimize clinical decision-making and to provide small dataset algorithm design ideas. Methods: A total of 206 nodules with postoperative pathological labels were analyzed. Among them were 30 adenocarcinomas in situ (AISs), 119 minimally invasive adenocarcinomas (MIAs), and 57 invasive adenocarcinomas (IACs). Our system consisted of two parts, a GAN-based image synthesis, and a CNN classification. First, several popular existing GAN techniques were employed to augment the datasets, and comprehensive experiments were conducted to evaluate the quality of the GAN synthesis. Additionally, our classification system processes were based on two-dimensional (2D) nodule-centered CT patches without the need of manual labeling information. Results: For GAN-based image synthesis, the visual Turing test showed that even radiologists could not tell the GAN-synthesized from the raw images (accuracy: primary radiologist 56%, senior radiologist 65%). For CNN classification, our progressive growing wGAN improved the performance of CNN most effectively (area under the curve =0.83). The experiments indicated that the proposed GAN augmentation method improved the classification accuracy by 23.5% (from 37.0% to 60.5%) and 7.3% (from 53.2% to 60.5%) in comparison with training methods using raw and common augmented images respectively. The performance of this combined GAN and CNN method (accuracy: 60.5%±2.6%) was comparable to the state-of-the-art methods, and our CNN was also more lightweight. Conclusions: The experiments revealed that GAN synthesis techniques could effectively alleviate the problem of insufficient data in medical imaging. The proposed GAN plus CNN framework can be generalized for use in building other computer-aided detection (CADx) algorithms and thus assist in diagnosis. © Quantitative Imaging in Medicine and Surgery. All rights reserved.","","Computed tomography; Data augmentation; Deep convolutional neural networks; Generative adversarial network (GAN); Subcentimeter pulmonary adenocarcinoma diagnosis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088366213"
"Schwarz K.; Liao Y.; Niemeyer M.; Geiger A.","Schwarz, Katja (57219584146); Liao, Yiyi (56742605600); Niemeyer, Michael (57214465239); Geiger, Andreas (55822335800)","57219584146; 56742605600; 57214465239; 55822335800","GRAF: Generative radiance fields for 3D-aware image synthesis","2020","Advances in Neural Information Processing Systems","2020-December","","","","","","","105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108453161&partnerID=40&md5=4c2aad8a3a2717184b205dab0827a368","While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity. © 2020 Neural information processing systems foundation. All rights reserved.","Cameras; Image resolution; Rendering (computer graphics); Adversarial networks; Generative model; High resolution image; Image formation process; Novel view synthesis; Object identity; Precise control; Real-world datasets; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85108453161"
"Cheng K.; Tahir R.; Eric L.K.; Li M.","Cheng, Keyang (8878638900); Tahir, Rabia (57195996194); Eric, Lubamba Kasangu (57212381465); Li, Maozhen (7405263968)","8878638900; 57195996194; 57212381465; 7405263968","An analysis of generative adversarial networks and variants for image synthesis on MNIST dataset","2020","Multimedia Tools and Applications","79","19-20","","13725","13752","27","10.1007/s11042-019-08600-2","30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085234977&doi=10.1007%2fs11042-019-08600-2&partnerID=40&md5=583fd1267f7a31c1c67ac8ec58c77271","Generative Adversarial Networks (GANs) are most popular generative frameworks that have achieved compelling performance. They follow an adversarial approach where two deep models generator and discriminator compete with each other. They have been used for many applications especially for image synthesis because of their capability to generate high quality images. In past few years, different variants of GAN have proposed and they produced high quality results for image generation. This paper conducts an analysis of working and architecture of GAN and its popular variants for image generation in detail. In addition, we summarize and compare these models according to different parameters such as architecture, training method, learning type, benefits and performance metrics. Finally, we apply all these methods on a benchmark MNIST dataset, which contains handwritten digits and compare qualitative and quantitative results. The evaluation is based on quality of generated images, classification accuracy, discriminator loss, generator loss and computational time of these models. The aim of this study is to provide a comprehensive information about GAN and its various models in the field of image synthesis. Our main contribution in this work is critical comparison of popular GAN variants for image generation on MNIST dataset. Moreover, this paper gives insights regarding existing limitations and challenges faced by GAN and discusses associated future research work. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Discriminators; Network architecture; Quality control; Adversarial networks; Classification accuracy; Comprehensive information; Generator; High quality images; Image synthesis; MNIST dataset; Performance metrics; Image analysis","Discriminator; GAN; Generator; Image synthesis; MNIST dataset","Article","Final","","Scopus","2-s2.0-85085234977"
"Karnewar A.; Wang O.","Karnewar, Animesh (57195068846); Wang, Oliver (24170210400)","57195068846; 24170210400","MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156324","7796","7805","9","10.1109/CVPR42600.2020.00782","85","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094347605&doi=10.1109%2fCVPR42600.2020.00782&partnerID=40&md5=080d9c4fe7e388cc1840e759d8eccff6","While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to adapt to different datasets, in part due to instability during training and sensitivity to hyperparameters. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator become uninformative when there isn't enough overlap in the supports of the real and fake distributions. In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this by allowing the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique. We show that MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains, as well as different types of loss functions and architectures, all with the same set of fixed hyperparameters. When compared to state-of-the-art GANs, our approach matches or exceeds the performance in most of the cases we tried. © 2020 IEEE.","Pattern recognition; Adversarial networks; Different sizes; High resolution image; Hyperparameters; Image synthesis; Loss functions; Multiple scale; State of the art; Image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094347605"
"Yang Y.; Wang L.; Xie D.; Deng C.; Tao D.","Yang, Yanhua (56967230200); Wang, Lei (57070633700); Xie, De (57215009284); Deng, Cheng (57208019993); Tao, Dacheng (7102600334)","56967230200; 57070633700; 57215009284; 57208019993; 7102600334","Multi-Sentence Auxiliary Adversarial Networks for Fine-Grained Text-to-Image Synthesis","2021","IEEE Transactions on Image Processing","30","","9345477","2798","2809","11","10.1109/TIP.2021.3055062","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100766253&doi=10.1109%2fTIP.2021.3055062&partnerID=40&md5=7354b3f63b1efe87844f631fbc68dbbb","Due to the development of Generative Adversarial Networks (GANs), significant progress has been achieved in text-to-image synthesis task. However, most previous works have only focus on learning the semantic consistency between paired images and sentences, without exploring the semantic correlation between different yet related sentences that describe the same image, which leads to significant visual variation among the synthesized images. Accordingly, in this article, we propose a new method for text-to-image synthesis, dubbed Multi-sentence Auxiliary Generative Adversarial Networks (MA-GAN); this approach not only improves the generation quality but also guarantees the generation similarity of related sentences by exploring the semantic correlation between different sentences describing the same image. More specifically, we propose a Single-sentence Generation and Multi-sentence Discrimination (SGMD) module that explores the semantic correlation between multiple related sentences in order to reduce the variation between their generated images and enhance the reliability of the generated results. Moreover, a Progressive Negative Sample Selection mechanism (PNSS) is designed to mine more suitable negative samples for training, which can effectively promote detailed discrimination ability in the generative model and facilitate the generation of more fine-grained results. Extensive experiments on Oxford-102 and CUB datasets reveal that our MA-GAN significantly outperforms the state-of-the-art methods. © 1992-2012 IEEE.","Semantics; Adversarial networks; Discrimination ability; Generative model; Image synthesis; Negative samples; Semantic consistency; State-of-the-art methods; Synthesized images; article; reliability; synthesis; Image enhancement","attention mechanism; Conditional image synthesis; generative adversarial networks; negative sample learning","Article","Final","","Scopus","2-s2.0-85100766253"
"Fetty L.; Bylund M.; Kuess P.; Heilemann G.; Nyholm T.; Georg D.; Löfstedt T.","Fetty, Lukas (57193197801); Bylund, Mikael (57200436828); Kuess, Peter (55037342200); Heilemann, Gerd (55618450900); Nyholm, Tufve (13007151100); Georg, Dietmar (7005029017); Löfstedt, Tommy (36145247100)","57193197801; 57200436828; 55037342200; 55618450900; 13007151100; 7005029017; 36145247100","Latent space manipulation for high-resolution medical image synthesis via the StyleGAN","2020","Zeitschrift fur Medizinische Physik","30","4","","305","314","9","10.1016/j.zemedi.2020.05.001","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086664703&doi=10.1016%2fj.zemedi.2020.05.001&partnerID=40&md5=020d16d56587d47e5eff8a3766472dee","Introduction: This paper explores the potential of the StyleGAN model as an high-resolution image generator for synthetic medical images. The possibility to generate sample patient images of different modalities can be helpful for training deep learning algorithms as e.g. a data augmentation technique. Methods: The StyleGAN model was trained on Computed Tomography (CT) and T2- weighted Magnetic Resonance (MR) images from 100 patients with pelvic malignancies. The resulting model was investigated with regards to three features: Image Modality, Sex, and Longitudinal Slice Position. Further, the style transfer feature of the StyleGAN was used to move images between the modalities. The root-mean-squard error (RMSE) and the Mean Absolute Error (MAE) were used to quantify errors for MR and CT, respectively. Results: We demonstrate how these features can be transformed by manipulating the latent style vectors, and attempt to quantify how the errors change as we move through the latent style space. The best results were achieved by using the style transfer feature of the StyleGAN (58.7 HU MAE for MR to CT and 0.339 RMSE for CT to MR). Slices below and above an initial central slice can be predicted with an error below 75 HU MAE and 0.3 RMSE within 4 cm for CT and MR, respectively. Discussion: The StyleGAN is a promising model to use for generating synthetic medical images for MR and CT modalities as well as for 3D volumes. © 2020","Algorithms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Signal-To-Noise Ratio; Tomography, X-Ray Computed; aged; analytical error; analytical parameters; Article; computer assisted tomography; controlled study; deep learning; female; generative adversarial network; human; image processing; image quality; image synthesis; latent space manipulation; longitudinal slice position; machine learning; major clinical study; male; nuclear magnetic resonance imaging; pelvis cancer; radiological parameters; sex difference; three-dimensional imaging; algorithm; image processing; nuclear magnetic resonance imaging; procedures; signal noise ratio; x-ray computed tomography","StyleGAN, Image synthesis, Latent space","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85086664703"
"Agnese J.; Herrera J.; Tao H.; Zhu X.","Agnese, Jorge (57215078034); Herrera, Jonathan (57215093135); Tao, Haicheng (55827133000); Zhu, Xingquan (8393069200)","57215078034; 57215093135; 55827133000; 8393069200","A survey and taxonomy of adversarial neural networks for text-to-image synthesis","2020","Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery","10","4","e1345","","","","10.1002/widm.1345","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079880646&doi=10.1002%2fwidm.1345&partnerID=40&md5=40420d8788d38661f340ba8fca972fc3","Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to the text. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to find best alignment of the visual content matching to the text. Recent progress in deep learning (DL) has brought a new set of unsupervised DL methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. The change of direction from the computer vision-based approaches to artificial intelligence (AI)-driven methods ignited the intense interest in industry, such as virtual reality, recreational & professional (eSports) gaming, and computer-aided design, and so on, to automatically generate compelling images from text-based natural language descriptions. In this paper, we review the most recent development in the text-to-image synthesis research domain. Our goal is to provide value by delivering a comparative review of the state-of-the-art models in terms of their architecture and design. The survey first introduces image synthesis and its challenges, and then reviews key concepts such as generative adversarial networks (GANs) and deep convolutional encoder-decoder neural networks (DCNNs). After that, we propose a taxonomy to summarize GAN-based text-to-image synthesis into four major categories: semantic enhancement GANs, resolution enhancement GANs, diversity enhancement GANS, and motion enhancement GANs. We elaborate on the main objective of each group, and further review typical GAN architectures in each group. The taxonomy and the review outline the techniques and the evolution of different approaches, and eventually provide a roadmap to summarize the list of contemporaneous solutions that utilize GANs and DCNNs to generate enthralling results in categories such as human faces, birds, flowers, room interiors, object reconstruction from edge maps (games), and so on. The survey concludes with a comparison of the proposed solutions, challenges that remain unresolved, and future developments in the text-to-image synthesis domain. This article is categorized under: Algorithmic Development > Multimedia Technologies > Machine Learning. © 2020 Wiley Periodicals, Inc.","Computer aided design; Convolutional neural networks; Deep learning; Learning systems; Network architecture; Semantics; Surveys; Taxonomies; Virtual reality; Adversarial networks; Convolutional encoders; Image synthesis; Object reconstruction; Resolution enhancement; Semantic enhancements; Trained neural networks; Vision-based approaches; Image enhancement","deep learning; generative adversarial network (GAN); machine learning; text-to-image synthesis","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85079880646"
"Li Z.; Deng C.; Yang E.; Tao D.","Li, Zeyu (57192706525); Deng, Cheng (57208019993); Yang, Erkun (57193533988); Tao, Dacheng (7102600334)","57192706525; 57208019993; 57193533988; 7102600334","Staged Sketch-to-Image Synthesis via Semi-supervised Generative Adversarial Networks","2021","IEEE Transactions on Multimedia","23","","9163292","2694","2705","11","10.1109/TMM.2020.3015015","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099601802&doi=10.1109%2fTMM.2020.3015015&partnerID=40&md5=a68b64141225e1df38863c18b5b6e550","Sketch-based image synthesis is a challenging problem in computer graphics and vision. Existing approaches either require exact edge maps or rely on the retrieval of existing photographs, which limits their applications in real-world scenarios. Accordingly in this work, we propose a staged semi-supervised generative adversarial networks based method for sketch-to-image synthesis, which can directly generate realistic images from novice sketches. More specifically, we first adopt a conditional generative adversarial network (CGAN) to extract class-wise representations from unpaired images. These class-wise representations are then exploited and incorporated with another CGAN, which are used to generate realistic images from sketches. By incorporating the class-wise representations, our method can leverage both the general class information from unpaired images and the targeted object information from input sketches. Additionally, this network architecture also enables us to take full advantage of widely available unpaired images and learn more accurate class representations. Extensive experiments demonstrate, compared with state-of-the-art image translation methods, our approach can achieve more promising results and synthesize images with significantly better Inception Scores and Fréchet Inception Distance.  © 1999-2012 IEEE.","Computer graphics; Network architecture; Adversarial networks; General class; Image synthesis; Real-world scenario; Realistic images; Semi-supervised; State of the art; Targeted objects; Image processing","Gan; image generation; sketch","Article","Final","","Scopus","2-s2.0-85099601802"
"Cheng Y.; Gan Z.; Li Y.; Liu J.; Gao J.","Cheng, Yu (55421399200); Gan, Zhe (57057041200); Li, Yitong (57194688330); Liu, Jingjing (57189311527); Gao, Jianfeng (55702627000)","55421399200; 57057041200; 57194688330; 57189311527; 55702627000","Sequential Attention GAN for Interactive Image Editing","2020","MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia","","","","4383","4391","8","10.1145/3394171.3413551","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098853982&doi=10.1145%2f3394171.3413551&partnerID=40&md5=544d5b32ba752413b28f25b14626a32a","Most existing text-to-image synthesis tasks are static single-turn generation, based on pre-defined textual descriptions of images. To explore more practical and interactive real-life applications, we introduce a new task - Interactive Image Editing, where users can guide an agent to edit images via multi-turn textual commands on-the-fly. In each session, the agent takes a natural language description from the user as the input, and modifies the image generated in previous turn to a new design, following the user description. The main challenges in this sequential and interactive image generation task are two-fold: 1) contextual consistency between a generated image and the provided textual description; 2) step-by-step region-level modification to maintain visual consistency across the generated image sequence in each session. To address these challenges, we propose a novel Sequential Attention Generative Adversarial Network (SeqAttnGAN), which applies a neural state tracker to encode the previous image and the textual description in each turn of the sequence, and uses a GAN framework to generate a modified version of the image that is consistent with the preceding images and coherent with the description. To achieve better region-specific refinement, we also introduce a sequential attention mechanism into the model. To benchmark on the new task, we introduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain multi-turn sessions with image-description sequences in the fashion domain. Experiments on both datasets show that the proposed SeqAttnGAN model outperforms state-of-the-art approaches on the interactive image editing task across all evaluation metrics including visual quality, image sequence coherence and text-image consistency. © 2020 ACM.","Adversarial networks; Attention mechanisms; Contextual consistency; Interactive image editing; Interactive images; Real-life applications; State-of-the-art approach; Textual description","generative adversarial network; image editing with natural language; sequential attention","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85098853982"
"Dumagpi J.K.; Jeong Y.-J.","Dumagpi, Joanna Kazzandra (57215696070); Jeong, Yong-Jin (55599021800)","57215696070; 55599021800","Evaluating gan-based image augmentation for threat detection in large-scale xray security images","2021","Applied Sciences (Switzerland)","11","1","36","1","21","20","10.3390/app11010036","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098673207&doi=10.3390%2fapp11010036&partnerID=40&md5=077f886953c655877a182fe00fbd95dc","The inherent imbalance in the data distribution of X-ray security images is one of the most challenging aspects of computer vision algorithms applied in this domain. Most of the prior studies in this field have ignored this aspect, limiting their application in the practical setting. This paper investigates the effect of employing Generative Adversarial Networks (GAN)-based image augmen-tation, or image synthesis, in improving the performance of computer vision algorithms on an imbalanced X-ray dataset. We used Deep Convolutional GAN (DCGAN) to generate new X-ray images of threat objects and Cycle-GAN to translate camera images of threat objects to X-ray images. We synthesized new X-ray security images by combining threat objects with background X-ray images, which are used to augment the dataset. Then, we trained various Faster (Region Based Convolu-tional Neural Network) R-CNN models using different augmentation approaches and evaluated their performance on a large-scale practical X-ray image dataset. Experiment results show that image synthesis is an effective approach to combating the imbalance problem by significantly reducing the false-positive rate (FPR) by up to 15.3%. The FPR is further improved by up to 19.9% by combining image synthesis and conventional image augmentation. Meanwhile, a relatively high true positive rate (TPR) of about 94% was maintained regardless of the augmentation method used. © 2020 by the author. Licensee MDPI, Basel, Switzerland.","","Deep learning; GAN; Image augmentation; Image translation; Security; Threat detection; X-ray","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85098673207"
"Yang P.-T.; Su F.-G.; Wang Y.-C.F.","Yang, Pei-Tse (57711158800); Su, Feng-Guang (57222002596); Wang, Yu-Chiang Frank (35216822800)","57711158800; 57222002596; 35216822800","Diverse Audio-to-Image Generation via Semantics and Feature Consistency","2020","2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2020 - Proceedings","","","9306323","1188","1192","4","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100938113&partnerID=40&md5=6f9148240dea0eb3ece9abe07fad52b6","Humans are capable of imagining scene images when hearing ambient sounds. Therefore, audio-to-image synthesis becomes a challenging yet practical topic for both natural language comprehension and image content understanding. In this paper, we propose an audio-to-image generation network by applying the conditional generative adversarial networks. Specifically, we utilize such generative models with the proposed feature consistency and conditional adversarial losses, so that diverse image outputs with satisfactory visual quality can be synthesized from a single audio input. Experimental results on sports audio/visual data verify that the effectiveness and practicality of the proposed method over the state-of-the-art approaches on audio-to-image synthesis.  © 2020 APSIPA.","Audition; Semantics; Adversarial networks; Feature consistency; Generative model; Image generations; Image synthesis; Natural languages; State-of-the-art approach; Visual qualities; Image processing","audio-to-image generation; conditional generative adversarial network; cross-modal generation","Conference paper","Final","","Scopus","2-s2.0-85100938113"
"Yang C.; Lv Z.","Yang, Chun (57221194253); Lv, Zhihan (55925162500)","57221194253; 55925162500","Gender based face aging with cycle-consistent adversarial networks","2020","Image and Vision Computing","100","","103945","","","","10.1016/j.imavis.2020.103945","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086454582&doi=10.1016%2fj.imavis.2020.103945&partnerID=40&md5=9f9d9a4c76dc560cca6e7fc30e9c95ee","Face aging is a task which referred to image synthesis, and the challenge comes from the training dataset, most existing face aging works require paired face images which is difficult to collect. Face images with various ages of the same person can be considered as unpaired images which come from different domains. The degree of aging effect can be influenced by age, gender, race and some other factors. In this paper, we are committed to studying the impact of gender on face aging problem, which involves the processing and modeling of face images. It has been proved that Generative Adversarial Networks(GANs) is competitive in realistic image synthesis, and many works employed Cycle-Consistent Adversarial Networks(CycleGANs) have shown the high performance in unpaired image-to-image translation. To overcome current difficulties and improve the performance of existing models on the face aging tasks, we proposed an innovative Gender-based training method using CycleGAN by pairwise training CycleGAN over several age groups which are grouped by age and gender. We build a constraint model based on gender discrimination to better simulate the expected aging effect of face images. To evaluate our works using subjective method, we have set up a quantitative evaluation mechanism with participants involved in. Compared with other similar subjective evaluation methods, our method is more objective in the demonstration of experimental results. The experimental results show that our method has a more realistic and excellent performance compared to those using CycleGAN for face age synthesis directly. © 2020 Elsevier B.V.","Computer applications; Electrical engineering; Adversarial networks; Different domains; Gender discrimination; Processing and modeling; Quantitative evaluation; Realistic image synthesis; Subjective evaluations; Subjective methods; Image processing","Age estimation; Age group classification; Cycle-Consistent Generative Adversarial Networks; Face aging; Image-to-Image translation; Subjective test","Article","Final","","Scopus","2-s2.0-85086454582"
"Parihar A.S.; Kaushik A.; Choudhary A.V.; Singh A.K.","Parihar, Anil Singh (36998900100); Kaushik, Aditya (57535684900); Choudhary, Aditya Vikram (57222371638); Singh, Amit Kumar (55726466900)","36998900100; 57535684900; 57222371638; 55726466900","A Primer on Conditional Text based Image Generation through Generative Models","2020","2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering, ICRAIE 2020 - Proceeding","","","9358343","","","","10.1109/ICRAIE51050.2020.9358343","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102512228&doi=10.1109%2fICRAIE51050.2020.9358343&partnerID=40&md5=9d11726c705539fd06e43147a0fdede5","Synthesis of Images from text descriptions has emerged as an interesting albeit a challenging task in the domain of Image Synthesis. Many promising advances have been made in the direction of text-based image generation in the recent years, with the emergence of Multi-modal Generative Adversarial Networks. In this paper, we discuss the various approaches which utilise Conditional-GANs to accomplish the task of generating photo-realistic images based on their text descriptions and compare their architectures and performance on various benchmark datasets. The performance of these approaches are evaluated using various well-known metrics. © 2020 IEEE.","Engineering; Industrial engineering; Adversarial networks; Benchmark datasets; Generative model; Image generations; Image synthesis; Multi-modal; Photorealistic images; Benchmarking","","Conference paper","Final","","Scopus","2-s2.0-85102512228"
"Wang L.; Zhang C.; Qin P.; Lin S.; Gao Y.; Dou J.","Wang, Lifang (57142669800); Zhang, Chengcheng (57225061162); Qin, Pinle (23393704900); Lin, Suzhen (7407607523); Gao, Yuan (57221270944); Dou, Jieliang (57219014538)","57142669800; 57225061162; 23393704900; 7407607523; 57221270944; 57219014538","Image registration method with residual dense relativistic average CGAN; [残差密集相对平均CGAN的脑部图像配准]","2020","Journal of Image and Graphics","25","4","","745","758","13","10.11834/jig.190116","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090995111&doi=10.11834%2fjig.190116&partnerID=40&md5=87834ef3f9118ed8f5d5c0ba36693d50","Objective: Multimodal medical image registration is a key step in medical image analysis and processing as it complements the information from different modality images and provides doctors with a variety of information about diseased tissues or organs. This method enables doctors to make accurate diagnosis and treatment plans. Image registration based on image synthesis is the main method for achieving high-precision registration. A high-quality composite image indicates good registration effect. However, current image-based registration algorithm has poor robustness in synthetic models and provides an insufficient representation of synthetic image feature information, resulting in low registration accuracy. In recent years, owing to the success of deep learning in many fields, medical image registration based on deep learning has become a focus of research. The synthetic model is trained according to the modal type of the image to be registered, and the synthetic model bidirectional synthetic image is used to guide the subsequent registration. Anatomical information is employed to guide the registration and improve the accuracy of multimodal image registration. Therefore, a multimodal brain image registration method based on residual dense relative average conditional generative adversarial network (RD-RaCGAN) is proposed in this study. Method: First, the RD-RaCGAN image synthesis model is constructed by combining the advantages of the relative average discriminator in the relativistic average generative adversarial network, which can enhance the model stability, and the advantages of the conditional generative adversarial network, which can improve the quality of the generated data, and also the ability of residual dense blocks to fully extract the characteristics of the deep network. Residual dense blocks are utilized as core components for building a generator. The purpose of this generator is to capture the law of sample distribution and generate a target image with specific significance, that is, to input a floating magnetic resonance (MR) or reference computed tomography (CT) image and generate the corresponding synthetic CT or synthetic MR image. The convolution neural network is used as a relative average discriminator, which correctly distinguishes an image generated by the generator from the real image. The generator and relative average discriminator perform confrontational training. First, the generator is fixed to train the relative average discriminator.Then, the relative average discriminator is fixed to train the generator, and the loop training is subsequently continued. During training, the least square function optimization generator and relative average discriminator, which are more stable and less saturated than the cross entropy function are selected. The ability of the generator and the relative average discriminator is enhanced, and the image generated by the generator can be falsified. At this point, the synthetic model training is completed. Subsequently, the CT image and MR image to be registered are bidirectionally synthesized into the corresponding reference MR image and floating CT image through the RD-RaCGAN synthesis model that has been trained. Four images obtained by bidirectional synthesis are registered by a region-adaptive registration algorithm. Specifically, the key points of the bone information are selected from the reference CT image and the floating CT image.The key points of the soft tissue information are selected from the floating MR image and the reference MR image, and the estimation of the deformation field is guided by the extracted key points. In other words, one deformation field is estimated from the floating CT image to the reference CT image, and a deformation field is estimated from the floating MR image to the reference MR image. At the same time, the idea of hierarchical symmetry is adopted to further guide the registration. The key points in the image are gradually increased when the reference and floating images are close to each other.Moreover, anatomical information is used to optimize the two deformation fields continuously until the difference between the two deformation fields reaches a minimum. The two deformation fields are fused to form the deformation field between the reference CT image and floating MR image. Finally, the deformation field is applied to the floating image to complete registration. Given that the synthesis of a target image from two images to be registered through the synthesis model requires time, the algorithm efficiency in this study is slightly lower than that of D.Demons(diffeomorphic demons) and ANTs-SyN(advanced normalization toolbox-symmetric normalization). Result: Given that the quality of the synthesized image directly affects registration accuracy, three sets of contrast experiments are designed to verify the effect of the algorithm in this study. Different algorithms are by MR synthesis CT, and different algorithms are compared by CT synthesis MR, and comparison of the effect of different registration algorithms. The experimental results show that the target image synthesized by the synthesis model in this study is superior to those obtained by the other methods in terms of visual effect and objective evaluation index. The target image synthesized by RD-RaCGAN is similar to the real image and has less noise than the target images generated by the other synthetic methods. As can be seen from the bones of the synthesized brain image and the area near the air interface, the synthetic model in this work visually shows realistic texture details. Compared with the Powell-optimized MI(mutual information) method, ANTs-SyN, D.Demons, Cue-Aware Net(cue-aware deep regression network), and I-SI(intensity and spatial information) image registration methods, the normalized mutual information increased by 43.71%, 12.87%, 10.59%, 0.47%, and 5.59%, respectively. In addition, the mean square root error decreased by 39.80%, 38.67%, 15.68%, 4.38%, and 2.61%, respectively. The results obtained by the registration algorithm in this study are close to the reference image. The registration effect diagram that the difference between the registration image and the reference image obtained by the algorithm in this study is smaller than that obtained by the other three methods. Small difference between the two images means good registration effect. Conclusion: This study proposes a multimodal brain image registration method based on RD-RaCGAN, which solves the problem of the poor robustness of the model synthesis algorithm based on image synthesis, leading to the inaccuracy of the synthetic image and the poor registration effect. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","CGAN (conditional generative adversarial network); Image synthesis; Least squares; Medical image registration; RaGAN (relativistic average generative adversarial network); Residual dense blocks","Article","Final","","Scopus","2-s2.0-85090995111"
"Qiao Z.; Qian Z.; Tang H.; Gong G.; Yin Y.; Huang C.; Fan W.","Qiao, Zhi (37013959700); Qian, Zhen (7201384454); Tang, Hui (57206632295); Gong, Guanzhong (39161248900); Yin, Yong (36601699700); Huang, Chao (55807635600); Fan, Wei (7401635674)","37013959700; 7201384454; 57206632295; 39161248900; 36601699700; 55807635600; 7401635674","CorGAN: Context aware Recurrent Generative Adversarial Network for Medical Image Generation","2020","Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020","","","9313470","1100","1103","3","10.1109/BIBM49941.2020.9313470","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100350422&doi=10.1109%2fBIBM49941.2020.9313470&partnerID=40&md5=60a5c764b8c08eed4fe42ad40b749966","Multi-modal imaging plays a critical role in various clinical applications. However, due to the associated high cost and potential risk, the acquisition of multi-modal images could be limited. To address this issue, many cross-modality image synthesis methods have been proposed. The state-of-the-art methods are mainly based on traditional convolutional generative adversarial networks (GANs) for generating target images. In 3D medical image synthesis, an open problem is how to efficiently exploit the spatial correlations of the 3D image sequence to resolve the inter-slice discontinuity and unevenness artifacts. In this paper, we propose a novel Context aware Residual Recurrent Generative Adversarial Network (short for CorGAN) for sequential medical image generation, which jointly exploits the spatial dependencies of the sequences as well as the peer image generation with GANs. Experimental results show the robustness and accuracy of our method, which outperforms the-state-of-the-art methods in synthesizing target 3D images from the corresponding source images. © 2020 IEEE.","Bioinformatics; Convolutional neural networks; Recurrent neural networks; Adversarial networks; Clinical application; Image generations; Multi-modal image; Multi-modal imaging; Spatial correlations; Spatial dependencies; State-of-the-art methods; Medical imaging","","Conference paper","Final","","Scopus","2-s2.0-85100350422"
"Khan M.Z.; Jabeen S.; Khan M.U.G.; Saba T.; Rehmat A.; Rehman A.; Tariq U.","Khan, Muhammad Zeeshan (57226797215); Jabeen, Saira (57202775358); Khan, Muhammad Usman Ghani (57482888000); Saba, Tanzila (36110026100); Rehmat, Asim (57212378192); Rehman, Amjad (35093155800); Tariq, Usman (14827558600)","57226797215; 57202775358; 57482888000; 36110026100; 57212378192; 35093155800; 14827558600","A Realistic Image Generation of Face from Text Description Using the Fully Trained Generative Adversarial Networks","2021","IEEE Access","9","","9163356","1250","1260","10","10.1109/ACCESS.2020.3015656","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099143366&doi=10.1109%2fACCESS.2020.3015656&partnerID=40&md5=02e30492c63e98117585a315718f0e57","Text to face generation is a sub-domain of text to image synthesis. It has a huge impact on new research areas along with the wide range of applications in the public safety domain. Due to the lack of dataset, the research work focused on the text to face generation is very limited. Most of the work for text to face generation until now is based on the partially trained generative adversarial networks, in which the pre-trained text encoder has been used to extract the semantic features of the input sentence. Later, these semantic features have been utilized to train the image decoder. In this research work, we propose a fully trained generative adversarial network to generate realistic and natural images. The proposed work trained the text encoder as well as the image decoder at the same time to generate more accurate and efficient results. In addition to the proposed methodology, another contribution is to generate the dataset by the amalgamation of LFW, CelebA and locally prepared dataset. The dataset has also been labeled according to our defined classes. Through performing different kinds of experiments, it has been proved that our proposed fully trained GAN outperformed by generating good quality images by the input sentence. Moreover, the visual results have also strengthened our experiments by generating the face images according to the given query.  © 2013 IEEE.","Decoding; Metals; Signal encoding; Adversarial networks; Face generation; Image synthesis; Natural images; Public safety; Quality image; Realistic images; Semantic features; Semantics","CNN; data augmentation; face synthesis; GAN; image generation; legal identity for all; text to face","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099143366"
"Wang Z.; Quan Z.; Wang Z.-J.; Hu X.; Chen Y.","Wang, Zixu (57221407970); Quan, Zhe (36337162300); Wang, Zhi-Jie (57221406867); Hu, Xinjian (57218834607); Chen, Yangyang (57218837246)","57221407970; 36337162300; 57221406867; 57218834607; 57218837246","Text to image synthesis with bidirectional generative adversarial network","2020","Proceedings - IEEE International Conference on Multimedia and Expo","2020-July","","9102904","","","","10.1109/ICME46284.2020.9102904","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090382123&doi=10.1109%2fICME46284.2020.9102904&partnerID=40&md5=f91961605d3bf5224326652868839ed0","Generating realistic images from text descriptions is a challenging problem in computer vision. Although previous works have shown remarkable progress, guaranteeing semantic consistency between text descriptions and images remains challenging. To generate semantically consistent images, we propose two semantics-enhanced modules and a novel Textual-Visual Bidirectional Generative Adversarial Network (TVBi-GAN). Specifically, this paper proposes a semanticsenhanced attention module and a semantics-enhanced batch normalization module. These modules improve consistency of synthesized images by involving precisely semantic features. What's more, an encoder network is proposed to extract semantic features from images. During the adversarial process, the encoder could guide our generator to explore corresponding features behind descriptions. With extensive experiments on CUB and COCO datasets, we demonstrate that our TVBi-GAN outperforms state-of-the-art methods. © 2020 IEEE.","Bismuth compounds; Semantics; Signal encoding; Adversarial networks; Image synthesis; Realistic images; Semantic consistency; Semantic features; State-of-the-art methods; Synthesized images; Image enhancement","Text-to-Image Synthesis","Conference paper","Final","","Scopus","2-s2.0-85090382123"
"Zhang H.; Zhu H.; Yang S.; Li W.","Zhang, Han (56098272800); Zhu, Hongqing (8538289200); Yang, Suyi (57222109843); Li, Wenhao (57222114728)","56098272800; 8538289200; 57222109843; 57222114728","DGattGAN: Cooperative Up-Sampling Based Dual Generator Attentional GAN on Text-to-Image Synthesis","2021","IEEE Access","9","","9352788","29584","29598","14","10.1109/ACCESS.2021.3058674","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101443143&doi=10.1109%2fACCESS.2021.3058674&partnerID=40&md5=b29b2da45619b7d0cbde7bf93ce2e163","Text-to-image synthesis task aims at generating images consistent with input text descriptions and is well developed by the Generative Adversarial Network (GAN). Although GAN based image generation approaches have achieved promising results, synthesizing quality is sometimes unsatisfied due to discursive generation of background and object. In this article, we propose a cooperative up-sampling based Dual Generator attentional GAN (DGattGAN) to generate high-quality images from text description. To achieve this, two generators with individual generation purpose are established to decouple object and background generation. In particular, we introduce a cooperative up-sampling mechanism to build cooperation between object and background generators during training. This strategy is potentially very useful as any dual generator architecture in GAN models can benefit from this mechanism. Furthermore, we propose an asymmetric information feeding scheme to distinguish two synthesis tasks, such that each generator only synthesizes based on semantic information they accept. Taking advantage of effective dual generator, the attention mechanism we incorporated on object generator could devote to fine-grained details generation on actual targeted objects. Experiments on Caltech-UCSD Bird (CUB) and Oxford-102 datasets suggest that generated images by the proposed model are more realistic and consistent with input text, and DGattGAN is competent compared to state-of-the-art methods according to Inception Score (IS) and R-precision metrics. Our codes are available at: https://github.com/ecfish/DGattGAN.  © 2013 IEEE.","Semantics; Signal sampling; Adversarial networks; Asymmetric information; Attention mechanisms; Background generations; High quality images; Object generators; Semantic information; State-of-the-art methods; Image processing","Asymmetric information feeding; cooperative up-sampling; dual generator; generative adversarial networks; text-to-image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101443143"
"Ranjan R.; Inoue S.; Shibata T.","Ranjan, Rakesh (57220640688); Inoue, Sozo (9335840200); Shibata, Tomohiro (35460767600)","57220640688; 9335840200; 35460767600","Synthesizing Cell Protein data for Human Protein Cell Profiling Using Dual Deep Generative Modeling","2020","2020 Joint 9th International Conference on Informatics, Electronics and Vision and 2020 4th International Conference on Imaging, Vision and Pattern Recognition, ICIEV and icIVPR 2020","","","9306574","","","","10.1109/ICIEVicIVPR48672.2020.9306574","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099881187&doi=10.1109%2fICIEVicIVPR48672.2020.9306574&partnerID=40&md5=c2059202abdbc359d90ba552b1c00610","To understand the biology of health, and how molecular dysfunction leads to disease, knowledge of the human cell is essential. The protein is the core unit of the human body made from trillions of cells, forming the body's various tissues. These tissues come together to create human organs. It is essential to understand the Spatio-temporal distribution of proteins in cells and to investigate human RNA-sequencing for human genes characterization. For this, it requires a massive amount of annotated data. However, due to many considerations like the high cost of data sample collection, lack of data sample availability, and lawful clauses for patient privacy, the majority of medical data is out of reach for general public research. In this study, we propose a new dual deep generative method for synthesizing human cell protein images by using the Generative Adversarial Network technique. Specifically, for that, we pair original cell protein images with their respective Cell-protein-tree. These pairs are then used to learn the mapping from a binary cell protein to a new cell protein image. For this purpose, we use an image-to-image translation technique based on adversarial learning. The generated cell protein images are expected to preserve the structural and visual quality of the training images. Visual and quantitative analysis of the experimental results demonstrates that the synthesized data are preserving the desired quality while maintaining the different forms of original data. Contribution-We have proposed a new dual deep generative model for synthesizing cell protein data. © 2020 IEEE.","Cells; Cytology; Histology; Pattern recognition; Proteins; Tissue; Adversarial learning; Adversarial networks; Generative methods; Generative model; Image translation; Patient privacies; Spatiotemporal distributions; Visual qualities; Biosynthesis","Cell protein image synthesis; Dual deep generative modeling; Generative adversarial network; Image-to-image translation","Conference paper","Final","","Scopus","2-s2.0-85099881187"
"Gu J.; Shen Y.; Zhou B.","Gu, Jinjin (57212042330); Shen, Yujun (57207766466); Zhou, Bolei (36697366200)","57212042330; 57207766466; 36697366200","Image processing using multi-code GaN prior","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9157000","3009","3018","9","10.1109/CVPR42600.2020.00308","120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094101942&doi=10.1109%2fCVPR42600.2020.00308&partnerID=40&md5=5a2c3cdd2b3066979fb422431c0959c4","Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.1 © 2020 IEEE.","Backpropagation; Codes (symbols); Gallium nitride; III-V semiconductors; Image enhancement; Pattern recognition; Semantics; Adversarial networks; Image colorizations; Image Inpainting; Image synthesis; Intermediate layers; Multiple features; Reconstruction quality; Super resolution; Image reconstruction","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094101942"
"He Y.; Wang L.; Yang F.; Clarysse P.; Zhu Y.","He, Yunlong (57193382175); Wang, Lihui (57141686200); Yang, Feng (56408792200); Clarysse, Patrick (6701664073); Zhu, Yuemin (55629946300)","57193382175; 57141686200; 56408792200; 6701664073; 55629946300","Deep Group-Wise Angular Translation of Cardiac Diffusion MRI in q-space via Manifold Regularized GAN","2020","International Conference on Signal Processing Proceedings, ICSP","2020-December","","9320925","511","515","4","10.1109/ICSP48669.2020.9320925","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100272190&doi=10.1109%2fICSP48669.2020.9320925&partnerID=40&md5=9a6b4ca7efc09d97e5c6ba5f2f9c3c99","Diffusion magnetic resonance imaging (dMRI) has become an indispensable tool for non-invasive characterization of fiber structures of tissues. Clinical applicability of dMRI is often shackled by trade-off between image quality and long acquisition time. We propose a novel group-wise image translation method to improve the angular resolution of cardiac dMRI data. It consists in using a generative adversarial network (GAN) model to estimate a sequence of images from given DW images acquired in a limited number of diffusion gradient directions. We embed a supervised manifold regularized term in the GAN loss function to exploit the correlation between multiple DW images acquired in different gradient directions. Experimental results on cardiac dMRI data demonstrated that our method can significantly improve the quality of diffusion tensor imaging (DTI) reconstruction.  © 2020 IEEE.","Diffusion; Economic and social effects; Image acquisition; Image enhancement; Magnetic resonance imaging; Tensors; Adversarial networks; Angular resolution; Characterization of fibers; Diffusion gradients; Diffusion magnetic resonance imaging; Gradient direction; Indispensable tools; Sequence of images; Diffusion tensor imaging","cardiac DTI; deep learning; diffusion MRI; image synthesis; spatial-angular information; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85100272190"
"Sun K.; Qu L.; Lian C.; Pan Y.; Hu D.; Xia B.; Li X.; Chai W.; Yan F.; Shen D.","Sun, Kun (7401518654); Qu, Liangqiong (57217038105); Lian, Chunfeng (56517715300); Pan, Yongsheng (56440550200); Hu, Dan (36161111200); Xia, Bingqing (57217871898); Li, Xinyue (57217873880); Chai, Weimin (35602805800); Yan, Fuhua (12789764400); Shen, Dinggang (7401738392)","7401518654; 57217038105; 56517715300; 56440550200; 36161111200; 57217871898; 57217873880; 35602805800; 12789764400; 7401738392","High-Resolution Breast MRI Reconstruction Using a Deep Convolutional Generative Adversarial Network","2020","Journal of Magnetic Resonance Imaging","52","6","","1852","1858","6","10.1002/jmri.27256","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087743930&doi=10.1002%2fjmri.27256&partnerID=40&md5=9f2389b9333024ceb4d192369bdf9697","Background: A generative adversarial network could be used for high-resolution (HR) medical image synthesis with reduced scan time. Purpose: To evaluate the potential of using a deep convolutional generative adversarial network (DCGAN) for generating HRpre and HRpost images based on their corresponding low-resolution (LR) images (LRpre and LRpost). Study Type: This was a retrospective analysis of a prospectively acquired cohort. Population: In all, 224 subjects were randomly divided into 200 training subjects and an independent 24 subjects testing set. Field Strength/Sequence: Dynamic contrast-enhanced (DCE) MRI with a 1.5T scanner. Assessment: Three breast radiologists independently ranked the image datasets, using the DCE images as the ground truth, and reviewed the image quality of both the original LR images and the generated HR images. The BI-RADS category and conspicuity of lesions were also ranked. The inter/intracorrelation coefficients (ICCs) of mean image quality scores, lesion conspicuity scores, and Breast Imaging Reporting and Data System (BI-RADS) categories were calculated between the three readers. Statistical Test: Wilcoxon signed-rank tests evaluated differences among the multireader ranking scores. Results: The mean overall image quality scores of the generated HRpre and HRpost were significantly higher than those of the original LRpre and LRpost (4.77 ± 0.41 vs. 3.27 ± 0.43 and 4.72 ± 0.44 vs. 3.23 ± 0.43, P < 0.0001, respectively, in the multireader study). The mean lesion conspicuity scores of the generated HRpre and HRpost were significantly higher than those of the original LRpre and LRpost (4.18 ± 0.70 vs. 3.49 ± 0.58 and 4.35 ± 0.59 vs. 3.48 ± 0.61, P < 0.001, respectively, in the multireader study). The ICCs of the image quality scores, lesion conspicuity scores, and BI-RADS categories had good agreements among the three readers (all ICCs >0.75). Data Conclusion: DCGAN was capable of generating HR of the breast from fast pre- and postcontrast LR and achieved superior quantitative and qualitative performance in a multireader study. Level of Evidence: 3. Technical Efficacy Stage: 2 J. MAGN. RESON. IMAGING 2020;52:1852–1858. © 2020 International Society for Magnetic Resonance in Medicine","Breast; Magnetic Resonance Imaging; Neural Networks, Computer; Radiography; Retrospective Studies; adult; article; breast imaging reporting and data system; cohort analysis; contrast enhancement; controlled study; dynamic contrast-enhanced magnetic resonance imaging; female; human; human experiment; image quality; major clinical study; male; radiologist; randomized controlled trial; retrospective study; Wilcoxon signed ranks test; breast; diagnostic imaging; nuclear magnetic resonance imaging; radiography","breast; generative adversarial network; MRI","Article","Final","","Scopus","2-s2.0-85087743930"
"Li Y.; Cheng Y.; Gan Z.; Yu L.; Wang L.; Liu J.","Li, Yandong (57202365760); Cheng, Yu (55421399200); Gan, Zhe (57057041200); Yu, Licheng (55609320200); Wang, Liqiang (36687714000); Liu, Jingjing (57189311527)","57202365760; 55421399200; 57057041200; 55609320200; 36687714000; 57189311527","BachGAN: High-resolution image synthesis from salient object layout","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9157573","8362","8371","9","10.1109/CVPR42600.2020.00839","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094859466&doi=10.1109%2fCVPR42600.2020.00839&partnerID=40&md5=c577ebb6840bb184ece3dc7281e69587","We propose a new task towards more practical applications for image generation - high-quality image synthesis from salient object layout. This new setting requires users to provide only the layout of salient objects (i.e., foreground bounding boxes and categories) and lets the model complete the drawing with an invented background and a matching foreground. Two main challenges spring from this new task: (i) how to generate fine-grained details and realistic textures without segmentation map input; and (ii) how to create and weave a background into standalone objects in a seamless way. To tackle this, we propose Background Hallucination Generative Adversarial Network (BachGAN), which leverages a background retrieval module to first select a set of segmentation maps from a large candidate pool, then encodes these candidate layouts via a background fusion module to hallucinate a suitable background for the given objects. By generating the hallucinated background representation dynamically, our model can synthesize high-resolution images with both photo-realistic foreground and integral background. Experiments on Cityscapes and ADE20K datasets demonstrate the advantage of BachGAN over existing approaches, measured on both visual fidelity of generated images and visual alignment between output images and input layouts. © 2020 IEEE.","Pattern recognition; Textures; Adversarial networks; High quality images; High resolution image; Image generations; Photo-realistic; Salient objects; Segmentation map; Visual fidelity; Image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094859466"
"Wu K.; Qiang Y.; Song K.; Ren X.; Yang W.K.; Zhang W.; Hussain A.; Cui Y.","Wu, Kun (57211681436); Qiang, Yan (26639724500); Song, Kai (57211684200); Ren, Xueting (57211682430); Yang, WenKai (57211025809); Zhang, Wanjun (57169366800); Hussain, Akbar (57211683833); Cui, Yanfen (53877122400)","57211681436; 26639724500; 57211684200; 57211682430; 57211025809; 57169366800; 57211683833; 53877122400","Image synthesis in contrast MRI based on super resolution reconstruction with multi-refinement cycle-consistent generative adversarial networks","2020","Journal of Intelligent Manufacturing","31","5","","1215","1228","13","10.1007/s10845-019-01507-7","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074847809&doi=10.1007%2fs10845-019-01507-7&partnerID=40&md5=d195d6dadb7a7f5210c9aa99f851fba0","In the field of medical image processing represented by magnetic resonance imaging (MRI), synthesizing the complementary target contrast of the target patient from the existing contrast has obvious medical significance for assisting doctors in making clinical diagnoses. To satisfy the image translation problem between different MRI contrasts (T1 and T2), a generative adversarial network is proposed that works in an end-to-end manner at image level. The low-frequency and high-frequency information of the image is preserved by using multi-stage optimization learning aided by adversarial loss, the loss of perceptual consistency and the loss of cyclic consistency, as it results in preserving the same contrast anatomical structure of the source domain supervisely when the perceptual pixel distribution of the target contrast is learned perfectly. To integrate different penalties (L1 and L2) organically, adaptive weights are set for the error sensitivity of the penalty function in the present total loss function, the aim being to achieve adaptive optimization of each stage of generating high-resolution images. In addition, a new net structure called multi-skip connection residual net is proposed to refine medical image details step by step with multi-stage optimization. Compared with the existing technology, the present method is more advanced. The contrast conversion of T1 and T2 in MRI is validated, which can help to shorten the imaging time, improve the imaging quality, and effectively assist doctors with diagnoses. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Diagnosis; Image reconstruction; Medical imaging; Synthesis (chemical); Adversarial networks; Anatomical structures; Cyclic consistency; High-frequency informations; Multi stage; Multi-skip; Multi-stage optimization; Super resolution reconstruction; Magnetic resonance imaging","Contrast MRI; Cyclic consistency; Generative adversarial network; Multi-skip; Multi-stage; Synthesis","Article","Final","","Scopus","2-s2.0-85074847809"
"Singh S.; Sharma R.; Smeaton A.F.","Singh, Simranjeet (57226080901); Sharma, Rajneesh (57220725645); Smeaton, Alan F. (7003631244)","57226080901; 57220725645; 7003631244","Using GANs to synthesise minimum training data for deepfake generation","2020","CEUR Workshop Proceedings","2771","","","193","204","11","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099333242&partnerID=40&md5=8842c93d94a022c5fa76f4b3f9e119f7","There are many applications of Generative Adversarial Networks (GANs) in fields like computer vision, natural language processing, speech synthesis, and more. Undoubtedly the most notable results have been in the area of image synthesis and in particular in the generation of deepfake videos. While deepfakes have received much negative media coverage, they can be a useful technology in applications like entertainment, customer relations, or even assistive care. One problem with generating deepfakes is the requirement for a lot of image training data of the subject which is not an issue if the subject is a celebrity for whom many images already exist. If there are only a small number of training images then the quality of the deepfake will be poor. Some media reports have indicated that a good deepfake can be produced with as few as 500 images but in practice, quality deepfakes require many thousands of images, one of the reasons why deepfakes of celebrities and politicians have become so popular. In this study, we exploit the property of a GAN to produce images of an individual with variable facial expressions which we then use to generate a deepfake. We observe that with such variability in facial expressions of synthetic GAN-generated training images and a reduced quantity of them, we can produce a near-realistic deepfake videos. © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Natural language processing systems; Public relations; Speech synthesis; Adversarial networks; Customer relations; Facial Expressions; Image synthesis; Image training; NAtural language processing; Negative media; Training image; Artificial intelligence","Deepfake generation; GANs; Generative Adversarial Networks; Variable face images","Conference paper","Final","","Scopus","2-s2.0-85099333242"
"Kang H.; Park J.-S.; Cho K.; Kang D.-Y.","Kang, Hyeon (57215317751); Park, Jang-Sik (49662072500); Cho, Kook (57203125431); Kang, Do-Young (7402889482)","57215317751; 49662072500; 57203125431; 7402889482","Visual and quantitative evaluation of amyloid brain PET image synthesis with generative adversarial network","2020","Applied Sciences (Switzerland)","10","7","2628","","","","10.3390/app10072628","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083424520&doi=10.3390%2fapp10072628&partnerID=40&md5=7eee67e19594c7da3a48320e65e4bde7","Conventional data augmentation (DA) techniques, which have been used to improve the performance of predictive models with a lack of balanced training data sets, entail an effort to define the proper repeating operation (e.g., rotation and mirroring) according to the target class distribution. Although DA using generative adversarial network (GAN) has the potential to overcome the disadvantages of conventional DA, there are not enough cases where this technique has been applied to medical images, and in particular, not enough cases where quantitative evaluation was used to determine whether the generated images had enough realism and diversity to be used for DA. In this study, we synthesized 18F-Florbetaben (FBB) images using CGAN. The generated images were evaluated using various measures, and we presented the state of the images and the similarity value of quantitative measurement that can be expected to successfully augment data from generated images for DA. The method includes (1) conditional WGAN-GP to learn the axial image distribution extracted from pre-processed 3D FBB images, (2) pre-trained DenseNet121 and model-agnostic metrics for visual and quantitative measurements of generated image distribution, and (3) a machine learning model for observing improvement in generalization performance by generated dataset. The Visual Turing test showed similarity in the descriptions of typical patterns of amyloid deposition for each of the generated images. However, differences in similarity and classification performance per axial level were observed, which did not agree with the visual evaluation. Experimental results demonstrated that quantitative measurements were able to detect the similarity between two distributions and observe mode collapse better than the Visual Turing test and t-SNE. © 2020 by the authors.","","Alzheimer's disease; Data augmentation; Deep learning; Generative adversarial network; Positron emission tomography","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083424520"
"Yuan M.; Peng Y.","Yuan, Mingkuan (57196123068); Peng, Yuxin (7403419173)","57196123068; 7403419173","Bridge-GAN: Interpretable Representation Learning for Text-to-Image Synthesis","2020","IEEE Transactions on Circuits and Systems for Video Technology","30","11","8902154","4258","4268","10","10.1109/TCSVT.2019.2953753","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084188404&doi=10.1109%2fTCSVT.2019.2953753&partnerID=40&md5=619378474be0eb048f94f46104fa5bfd","Text-to-image synthesis is to generate images with the consistent content as the given text description, which is a highly challenging task with two main issues: visual reality and content consistency. Recently, it is available to generate images with high visual reality due to the significant progress of generative adversarial networks. However, translating text description to image with high content consistency is still ambitious. For addressing the above issues, it is reasonable to establish a transitional space with interpretable representation as a bridge to associate text and image. So we propose a text-to-image synthesis approach named Bridge-like Generative Adversarial Networks (Bridge-GAN). Its main contributions are: (1) A transitional space is established as a bridge for improving content consistency, where the interpretable representation can be learned by guaranteeing the key visual information from given text descriptions. (2) A ternary mutual information objective is designed for optimizing the transitional space and enhancing both the visual reality and content consistency. It is proposed under the goal to disentangle the latent factors conditioned on text description for further interpretable representation learning. Comprehensive experiments on two widely-used datasets verify the effectiveness of our Bridge-GAN with the best performance. © 1991-2012 IEEE.","Networks (circuits); Video signal processing; Adversarial networks; Content consistency; Image synthesis; Interpretable representation; Mutual informations; Transitional spaces; Visual information; Visual realities; Image processing","Bridge-GAN; interpretable representation learning; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85084188404"
"Shi G.; Wang J.; Qiang Y.; Yang X.; Zhao J.; Hao R.; Yang W.; Du Q.; Kazihise N.G.-F.","Shi, Guohua (57216214788); Wang, Jiawen (57211026994); Qiang, Yan (26639724500); Yang, Xiaotang (55194893900); Zhao, Juanjuan (35097898900); Hao, Rui (52563712100); Yang, Wenkai (57211025809); Du, Qianqian (57212001911); Kazihise, Ntikurako Guy-Fernand (57211847549)","57216214788; 57211026994; 26639724500; 55194893900; 35097898900; 52563712100; 57211025809; 57212001911; 57211847549","Knowledge-guided synthetic medical image adversarial augmentation for ultrasonography thyroid nodule classification","2020","Computer Methods and Programs in Biomedicine","196","","105611","","","","10.1016/j.cmpb.2020.105611","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087513820&doi=10.1016%2fj.cmpb.2020.105611&partnerID=40&md5=4749f7eaf21b1b3aa5d995e53ff0b452","Background and objective: Image classification is an important task in many medical applications. Methods based on deep learning have made great achievements in the computer vision domain. However, they typically rely on large-scale datasets which are annotated. How to obtain such great datasets is still a serious problem in medical domain. Methods: In this paper, we propose a knowledge-guided adversarial augmentation method for synthesizing medical images. First, we design Term and Image Encoders to extract domain knowledge from radiologists, then we use domain knowledge as novel condition to constrain the Auxiliary Classifier Generative Adversarial Network (ACGAN) framework for the synthesis of high-quality thyroid nodule images. Finally, we demonstrate our method on the task of classifying ultrasonography thyroid nodule. Our method can make effective use of the high-quality diagnostic experience of advanced radiologists. In addition, we creatively choose to extract domain knowledge from standardized terms rather than ultrasound images. Results: Our novel method is demonstrated on a limited dataset of 1937 clinical thyroid ultrasound images and corresponding standardized terms. The accuracy of the proposed model for thyroid nodules is 91.46%, the sensitivity is 90.63%, the specificity is 92.65%, and the AUC is 95.32%, which is better than the current classification methods for thyroid nodules. The experimental results show the model has better generalization and robustness. Conclusions: We believe that the proposed method can alleviate the problem of insufficient data in the medical domain, and other medical problems can benefit from using synthetic augmentation. © 2020","Humans; Neural Networks, Computer; Thyroid Nodule; Ultrasonography; Data mining; Deep learning; Diagnosis; Image classification; Large dataset; Medical applications; Medical problems; Ultrasonography; Adversarial networks; Augmentation methods; Classification methods; Domain knowledge; Large-scale datasets; Medical domains; Thyroid nodule; Ultrasound images; Article; artifact reduction; artificial neural network; back propagation neural network; classifier; data extraction; diagnostic accuracy; diagnostic test accuracy study; discrete cosine transform; disease classification; echography; human; image quality; knowldge guided auxiliary classifier generative adversarial network; knowledge; multilayer perceptron; radiologist; thyroid nodule; diagnostic imaging; echography; thyroid nodule; Medical imaging","Classification; Data augmentation; Domain knowledge; Generative adversarial network; Image synthesis; Thyroid nodule","Article","Final","","Scopus","2-s2.0-85087513820"
"Santos J.S.; Frango I.S.","Santos, Jonathan Silva (57219255171); Frango, Ismar Silvera (57219256366)","57219255171; 57219256366","Increasing performance of strabismus detection models through the synthesis of photorealistic images of the eye region; [Aumento da performance de modelos de detecção de estrabismo através da síntese de imagens fotorrealistas da região dos olhos]","2020","RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao","2020","E33","","93","104","11","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094594995&partnerID=40&md5=ef41db952ef87fdd50ca315756733d44","This study, using a Deep Convolutional Generative Adversarial Networks technique, develops a generative model capable of generating photorealistic images of the eye region that can be used as objects of a data augmentation approach to increase the accuracy of strabismus detection models. This paper contains a simple experiment, a strabismus detector model, which shows a 7% accuracy gain when using synthetic images as a data augmentation. © 2020, Associacao Iberica de Sistemas e Tecnologias de Informacao. All rights reserved.","","GAN; DCGAN; Image synthesis; Strabismus","Article","Final","","Scopus","2-s2.0-85094594995"
"Li C.Y.; Cavallaro A.","Li, Chau Yi (57205422299); Cavallaro, Andrea (7103233230)","57205422299; 7103233230","Cast-Gan: Learning to Remove Colour Cast from Underwater Images","2020","Proceedings - International Conference on Image Processing, ICIP","2020-October","","9191157","1083","1087","4","10.1109/ICIP40778.2020.9191157","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098624023&doi=10.1109%2fICIP40778.2020.9191157&partnerID=40&md5=247f367d8046647c85408655e530e584","Underwater images are degraded by blur and colour cast caused by the attenuation of light in water. To remove the colour cast with neural networks, images of the scene taken under white illumination are needed as reference for training, but are generally unavailable. As an alternative, one can use surrogate reference images taken close to the water surface or degraded images synthesised from reference datasets. However, the former still suffer from colour cast and the latter generally have limited colour diversity. To address these problems, we exploit open data and typical colour distributions of objects to create a synthetic image dataset that reflects degradations naturally occurring in underwater photography. We use this dataset to train Cast-GAN, a Generative Adversarial Network whose loss function includes terms that eliminate artefacts that are typical of underwater images enhanced with neural networks. We compare the enhancement results of Cast-GAN with four state-of-the-art methods and validate the cast removal with a subjective evaluation. © 2020 IEEE.","Color; Neural networks; Open Data; Underwater photography; Adversarial networks; Degraded images; Loss functions; Naturally occurring; Reference image; State-of-the-art methods; Subjective evaluations; Synthetic image dataset; Image enhancement","Generative Adversarial Networks; Image enhancement; Image synthesis; Underwater images","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098624023"
"Sengupta S.; Athwale A.; Gulati T.; Zelek J.; Lakshminarayanan V.","Sengupta, Sourya (57212026321); Athwale, Akshay (57219402266); Gulati, Tanmay (57212246450); Zelek, John (6603746225); Lakshminarayanan, Vasudevan (7004723358)","57212026321; 57219402266; 57212246450; 6603746225; 7004723358","Funsyn-Net: Enhanced residual variational auto-encoder and image-to-image translation network for fundus image synthesis","2020","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11313","","2549869","","","","10.1117/12.2549869","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092602917&doi=10.1117%2f12.2549869&partnerID=40&md5=f5890b5842d7790d011e176a96c13305","Medical imaging datasets typically do not contain many training images and are usually not sufficient for training deep learning networks. We propose a deep residual variational auto-encoder and a generative adversarial network based approach that can generate a synthetic retinal fundus image dataset with corresponding blood vessel annotations. In terms of structural statistics comparison of real and artificial our model performed better than existing methods. The generated blood vessel structures achieved a structural similarity value of 0.74 and the artificial dataset achieved a sensitivity of 0.84 and specificity of 0.97 for the blood vessel segmentation task. The successful application of generative models for the generation of synthetic medical data will not only help to mitigate the small dataset problem but will also address the privacy concerns associated with such medical datasets. © 2020 SPIE. All rights reserved.","Blood; Blood vessels; Deep learning; Learning systems; Medical image processing; Network coding; Adversarial networks; Blood vessel segmentation; Image translation; Medical data sets; Privacy concerns; Retinal fundus images; Structural similarity; Structural statistics; Image enhancement","Fundus; GAN; Image Synthesis; Ophthalmology; Retina; RSVAE","Conference paper","Final","","Scopus","2-s2.0-85092602917"
"Wang M.","Wang, Mingxing (57219655623)","57219655623","Video Description with GAN","2020","2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology, CCET 2020","","","9213129","10","13","3","10.1109/CCET50901.2020.9213129","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094614656&doi=10.1109%2fCCET50901.2020.9213129&partnerID=40&md5=c3e54236ad7368f4da13325b13233edb","Video description is to convert rich information of video data into text information, Which has been attracting broad research attention in the Artificial Intelligence Community. Deep learning has given computers a strong understanding of one-dimensional picture data and two-dimensional video data. However, In real application scenarios, it still faces the problem of insufficient robustness. For example, the generated text information is unreasonable, the scene information and semantic information rich in video data cannot be extracted effectively. GAN (Generative Adversarial Nets) is a model that generates data using countermeasures, in recent years, which is widely used in text generation, dialogue system, image synthesis, etc. However, there has not been much effort on exploring GAN for Video description. In this paper, we design a new discriminant network on the basis of the traditional text description. In addition, we add the long-short Time memory networks into the model to minimize the loss of information in the encoding or decoding process, so as to generate more reasonable sentences. Experimental results demonstrate that our proposed model exceeds most video description methods in public datasets.  © 2020 IEEE.","Deep learning; Semantics; Speech processing; Video recording; Decoding process; Description method; Dialogue systems; Image synthesis; Real applications; Semantic information; Text generations; Text information; One dimensional","generative adversarial networks; long-short time memory networks; video description","Conference paper","Final","","Scopus","2-s2.0-85094614656"
"Ding S.; Lyu J.","Ding, Saisai (57219537256); Lyu, Jia (57199238859)","57219537256; 57199238859","High Resolution Dermoscopy Image Synthesis Method with pix2pixHD; [采用pix2pixHD的高分辨率皮肤镜图像合成方法]","2020","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","32","11","","1795","1803","8","10.3724/SP.J.1089.2020.18199","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097271008&doi=10.3724%2fSP.J.1089.2020.18199&partnerID=40&md5=4514499648227e99622ce287875fb5fd","Automated skin lesion recognition in dermoscopy images is an essential way to improve the diagnostic perfor-mance and reduce skin cancer deaths. To solve the problem of insufficient training data, a high resolution der-moscopy image synthesis method with pix2pixHD is proposed in this paper. First, a label mapping with patho-logical significance is obtained to model the knowledge of skin lesions by combining the ground truth of skin le-sions with the lesion category. Second, the label mapping is used as the constraint condition for lesion synthesis, and the constructed generator adversarial network is used to implement the spatial mapping of the label mapping. In order to avoid the loss of image details, the shallow and deep features together at each scale of the generator are combined. After that, the standard deviation matching loss is introduced to stabilize the training of generator adversarial network. Compared with the quality of images generated by different synthesis methods, the experimental results on the ISIC-2017 dataset show that this method has better visual effect and quantitative index evaluation of IS and FID, and can provide additional information gain for the supervised learning network to improve the accuracy of skin lesion classification. © 2020, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Classification (of information); Dermatology; Diagnosis; Learning systems; Mapping; Adversarial networks; Constraint conditions; Dermoscopy images; Information gain; Lesion synthesis; Quantitative indices; Standard deviation; Synthesis method; Image enhancement","Dermoscopy image; Feature matching loss; Generative adversarial networks; Lesion recognition","Article","Final","","Scopus","2-s2.0-85097271008"
"He J.; Zheng J.; Shen Y.; Guo Y.; Zhou H.","He, Jijun (57189212888); Zheng, Jinjin (8518127200); Shen, Yuan (55492339400); Guo, Yutang (56137496300); Zhou, Hongjun (7404742428)","57189212888; 8518127200; 55492339400; 56137496300; 7404742428","Facial Image Synthesis and Super-Resolution With Stacked Generative Adversarial Network","2020","Neurocomputing","402","","","359","365","6","10.1016/j.neucom.2020.03.107","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083672039&doi=10.1016%2fj.neucom.2020.03.107&partnerID=40&md5=4ee6bc2391c804fddc825286d07b2c39","Image synthesis and super-resolution (SR) have always been a hot spot for computer vision and image processing research. Since the development of Deep Learning, especially after the Deep Convolutional Generative Adversarial Network (DC-GAN) methods, facial image synthesis and SR problem had been solved in many circumstances. But most of the existing works were focused on natural-looking of the synthesized result rather than keeping facial information of the original image. Our paper presented an end-to-end method of getting high-resolution photo-realistic facial images from low-resolution (LR) in-the-wild images without losing the facial identity details. The pipeline used a flexible stacked GAN structure for the SR process with different target image resolutions on different upscaling factors. To avoid getting blur or nonsensical image output and realize the flexibility, “U-Net” architecture and upsampling layers with residual learning blocks were stacked. The stacked network structure makes applying different loss functions in different parts of the network possible, which helps to solve the two problems of keeping identical facial details of the LR input image and generating high-quality output images simultaneously. By using 3 different loss functions in different positions of the stacked network separately, through experimental comparison, we found the best stacked residual block parameters which could get the best output image quality. Experimental results also explicated that the network had a good SR ability compare to state of the art methods in different resolution and upscaling factor. © 2020","Deep learning; Image resolution; Optical resolving power; Adversarial networks; Different resolutions; Experimental comparison; Facial Image synthesis; High-resolution photos; Network structures; State-of-the-art methods; Super resolution; Article; convolutional neural network; deep learning; facial recognition; identity; image analysis; image display; image enhancement; image processing; image quality; mathematical computing; priority journal; stacked generative adversarial network; Image processing","Face hallucination; Generative Adversarial Network; Image synthesis; Super Resolution","Article","Final","","Scopus","2-s2.0-85083672039"
"Venugopal A.; Shivani A.; Neha M.; Mamatha H.R.","Venugopal, Abhijith (57216953696); Shivani, Adapa (57219483570); Neha, M. (57221077837); Mamatha, H.R. (54906688100)","57216953696; 57219483570; 57221077837; 54906688100","A Deep Learning Generative Approach for Speech-to-Scene Generation","2021","Lecture Notes in Networks and Systems","154","","","593","603","10","10.1007/978-981-15-8354-4_59","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098174928&doi=10.1007%2f978-981-15-8354-4_59&partnerID=40&md5=cbe927dadadb32f13c401bfd86a61f3b","Visualization can enhance the power of our subconscious mind. Research has proven that visualization is a very effective medium for communication since it enables humans to remember insights for a longer duration. A speech input visualizer would thus be of utmost importance because of its wide-ranging applications in areas such as Education, Engineering, Defence, Art, Game Development, Architecture, and so on. This paper introduces a real-time efficient “Speech-to-Scene” Generator using a deep learning approach. A novel combination of HTML speech recognition API and Text Conditioned Auxiliary Classifier Generative Adversarial Network has been proposed to overcome this problem. This model has been trained on a car’s data set consisting of 16 distinct classes. Additionally, this model can also adapt to various other data sets. The generated images were evaluated with Inception Score and Multi-Scale Structural Similarity Index (MS-SSIM) to compare with state-of-the-art image generation technologies. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Audio-visual systems; Deep learning; Generative approach; Image synthesis; Visualization","Conference paper","Final","","Scopus","2-s2.0-85098174928"
"Oliveira D.A.B.","Oliveira, Dario Augusto Borges (27567900100)","27567900100","Controllable Skin Lesion Synthesis Using Texture Patches, Bézier Curves and Conditional GANs","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098676","1798","1802","4","10.1109/ISBI45749.2020.9098676","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085859380&doi=10.1109%2fISBI45749.2020.9098676&partnerID=40&md5=9b6ca5889aa3182c63e9a2999f187376","Data synthesis is an important tool for improving data availability in cases where data is hard to capture or annotate. In the context of skin lesions data, data synthesis has been used for data augmentation in automated classification methods or for supporting training of dermoscopic images visual inspection. In this paper, we propose a simple yet effective approach for diverse skin lesion image synthesis using conditional generative adversarial networks. Our pipeline takes as input a random Bézier curve representing the lesion mask, and two texture patches: one for skin, and one for lesion; and synthesizes a new dermoscopic image. Our method generates images where lesions and skin reproduce the corresponding provided texture patches, and the lesion conforms to the provided Bézier mask. Our results report realistic controllable synthesis and improved performance for skin lesion segmentation task considering different semantic segmentation networks in a public challenge in comparison to classic data augmentation. © 2020 IEEE.","Dermatology; Medical imaging; Semantics; Textures; Adversarial networks; Automated classification; Controllable synthesis; Data availability; Dermoscopic images; Effective approaches; Semantic segmentation; Skin lesion images; Image texture","","Conference paper","Final","","Scopus","2-s2.0-85085859380"
"Lee H.; Kim J.; Kim E.K.; Kim S.","Lee, Hansoo (55145203500); Kim, Jonggeun (55869318800); Kim, Eun Kyeong (56488966800); Kim, Sungshin (8942065000)","55145203500; 55869318800; 56488966800; 8942065000","Wasserstein generative adversarial networks based data augmentation for radar data analysis","2020","Applied Sciences (Switzerland)","10","4","1449","","","","10.3390/app10041449","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081582169&doi=10.3390%2fapp10041449&partnerID=40&md5=314e4de4b2dc71f007298e8f6441d447","Ground-based weather radar can observe a wide range with a high spatial and temporal resolution. They are beneficial tometeorological research and services by providing valuable information. Recent weather radar data related research has focused on applying machine learning and deep learning to solve complicated problems. It is a well-known fact that an adequate amount of data is a positively necessary condition in machine learning and deep learning. Generative adversarial networks (GANs) have received extensive attention for their remarkable data generation capacity, with a fascinating competitive structure having been proposed since. Consequently, a massive number of variants have been proposed;whichmodel is adequate to solve the given problemis an inevitable concern. In this paper, we propose exploring the problemof radar image synthesis and evaluating differentGANswith authentic radar observation results. The experimental results showed that the improvedWasserstein GAN is more capable of generating similar radar images while achieving higher structural similarity results. © 2020 by the authors.","","Data augmentation; Generative adversarial networks; Structural similarity; Weather radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081582169"
"Ozgen A.C.; Aghdam O.A.; Ekenel H.K.","Ozgen, Azmi Can (57203170438); Aghdam, Omid Abdollahi (57203167516); Ekenel, Hazim Kemal (55958877400)","57203170438; 57203167516; 55958877400","Text-to-Painting on a Large Variance Dataset with Sequential Generative Adversarial Networks","2020","2020 28th Signal Processing and Communications Applications Conference, SIU 2020 - Proceedings","","","9302112","","","","10.1109/SIU49456.2020.9302112","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100305294&doi=10.1109%2fSIU49456.2020.9302112&partnerID=40&md5=06b7f0a5c6b289818190496142e43c5f","Converting text descriptions to images using Generative Adversarial Networks has become a popular research area. Visually appealing images were generated in recent years successfully. We investigated the generation of artistic images on a custom-built large variance dataset, which includes training images with variations, for example, in shape, color, and content. These variations in images provide originality, which is an important factor for artistic essence. One major characteristic of our work is that we used keywords as image descriptions, instead of sentences. As a network architecture, we proposed a sequential Generative Adversarial Network model, which utilizes several techniques like Wasserstein loss, spectral normalization, and minibatch discrimination to have stable training curves. Ultimately, we were able to generate painting images, which have a variety of styles. We evaluated the quality of generated paintings by using Fréchet Inception Distance score.  © 2020 IEEE.","Large dataset; Network architecture; Adversarial networks; Image descriptions; Spectral normalization; Training image; Signal processing","Generative Adversarial Networks (GANs); Painting generation; Sequential GANs; Text-to-Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85100305294"
"Lee W.; Kim D.; Hong S.; Lee H.","Lee, Wonkwang (57219635154); Kim, Donggyun (57219634435); Hong, Seunghoon (56118655200); Lee, Honglak (15056237200)","57219635154; 57219634435; 56118655200; 15056237200","High-Fidelity Synthesis with Disentangled Representation","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12371 LNCS","","","157","174","17","10.1007/978-3-030-58574-7_10","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097260583&doi=10.1007%2f978-3-030-58574-7_10&partnerID=40&md5=38fed8bc8bc7bda477e4fa277359c5c1","Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance (i.e., improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024 × 1024 pixels) for the first time using the disentangled representations. Our code is available at https://www.github.com/1Konny/idgan. © 2020, Springer Nature Switzerland AG.","Computer vision; Distillation; Economic and social effects; Adversarial networks; Generic frameworks; High resolution image; Image generations; Interpretability; Mutual informations; State of the art; State-of-the-art methods; Learning systems","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097260583"
"Bui T.D.; Nguyen M.; Le N.; Luu K.","Bui, Toan Duc (57210736515); Nguyen, Manh (57209163503); Le, Ngan (57214452870); Luu, Khoa (35217996900)","57210736515; 57209163503; 57214452870; 35217996900","Flow-Based Deformation Guidance for Unpaired Multi-contrast MRI Image-to-Image Translation","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12262 LNCS","","","728","737","9","10.1007/978-3-030-59713-9_70","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092729464&doi=10.1007%2f978-3-030-59713-9_70&partnerID=40&md5=1d2547ff8d4c357ceb4b728e416dae9d","Image synthesis from corrupted contrasts increases the diversity of diagnostic information available for many neurological diseases. Recently the image-to-image translation has experienced significant levels of interest within medical research, beginning with the successful use of the Generative Adversarial Network (GAN) to the introduction of cyclic constraint extended to multiple domains. However, in current approaches, there is no guarantee that the mapping between the two image domains would be unique or one-to-one. In this paper, we introduce a novel approach to unpaired image-to-image translation based on the invertible architecture. The invertible property of the flow-based architecture assures a cycle-consistency of image-to-image translation without additional loss functions. We utilize the temporal information between consecutive slices to provide more constraints to the optimization for transforming one domain to another in unpaired volumetric medical images. To capture temporal structures in the medical images, we explore the displacement between the consecutive slices using a deformation field. In our approach, the deformation field is used as a guidance to keep the translated slides realistic and consistent across the translation. The experimental results have shown that the synthesized images using our proposed approach are able to archive a competitive performance in terms of mean squared error, peak signal-to-noise ratio, and structural similarity index when compared with the existing deep learning-based methods on three standard datasets, i.e. HCP, MRBrainS13 and Brats2019. © 2020, Springer Nature Switzerland AG.","Air navigation; Constrained optimization; Deep learning; Deformation; Diagnosis; Magnetic resonance imaging; Mean square error; Medical computing; Network architecture; Signal to noise ratio; Adversarial networks; Competitive performance; Flow-based architecture; Learning-based methods; Neurological disease; Peak signal to noise ratio; Structural similarity indices; Temporal information; Medical imaging","cycleGAN; Flow-based generator; Image-to-image translation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092729464"
"Liu Y.; Meng L.; Zhong J.","Liu, Yang (57225001520); Meng, Lu (36093115100); Zhong, Jianping (57216340658)","57225001520; 36093115100; 57216340658","MAGAN: Mask Attention Generative Adversarial Network for Liver Tumor CT Image Synthesis","2021","Journal of Healthcare Engineering","2021","","6675259","","","","10.1155/2021/6675259","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100723992&doi=10.1155%2f2021%2f6675259&partnerID=40&md5=bbf3639d687b7d911f2e1918c191af68","For deep learning, the size of the dataset greatly affects the final training effect. However, in the field of computer-aided diagnosis, medical image datasets are often limited and even scarce. We aim to synthesize medical images and enlarge the size of the medical image dataset. In the present study, we synthesized the liver CT images with a tumor based on the mask attention generative adversarial network (MAGAN). We masked the pixels of the liver tumor in the image as the attention map. And both the original image and attention map were loaded into the generator network to obtain the synthesized images. Then, the original images, the attention map, and the synthesized images were all loaded into the discriminator network to determine if the synthesized images were real or fake. Finally, we can use the generator network to synthesize liver CT images with a tumor. The experiments showed that our method outperformed the other state-of-the-art methods and can achieve a mean peak signal-to-noise ratio (PSNR) of 64.72 dB. All these results indicated that our method can synthesize liver CT images with a tumor and build a large medical image dataset, which may facilitate the progress of medical image analysis and computer-aided diagnosis. An earlier version of our study has been presented as a preprint in the following link: https://www.researchsquare.com/article/rs-41685/v1. © 2021 Yang Liu et al.","Attention; Humans; Image Processing, Computer-Assisted; Liver Neoplasms; Salicylates; Tomography, X-Ray Computed; Computer aided analysis; Computer aided diagnosis; Deep learning; Large dataset; Medical imaging; Signal to noise ratio; Tumors; magnesium salicylate; salicylic acid; Adversarial networks; Image datasets; Liver tumors; Original images; Peak signal to noise ratio; State-of-the-art methods; Synthesized images; Training effects; algorithm; Article; comparative study; computer aided design; computer assisted tomography; controlled study; deep learning; diagnostic imaging; human; image analysis; image segmentation; liver tumor; mask attention generative adversarial network; oncological parameters; radiologist; receptive field; retina image; signal noise ratio; tumor diagnosis; attention; image processing; liver tumor; procedures; x-ray computed tomography; Computerized tomography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85100723992"
"Alqahtani H.; Kavakli-Thorne M.; Kumar G.","Alqahtani, Hamed (57194574403); Kavakli-Thorne, Manolya (57219506272); Kumar, Gulshan (35932222600)","57194574403; 57219506272; 35932222600","Generative adversarial networks - an introduction","2020","Image Recognition: Progress, Trends and Challenges","","","","107","134","27","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145342974&partnerID=40&md5=84df3d7970cb88a2f6e8b618de0c7ac0","Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This chapter aims to present an overview of GANs, and its different variants. The chapter attempts to identify GANs' advantages, disadvantages and significant challenges to the successful implementation of GAN in different application areas. Finally, the chapter ends with the conclusion and future aspects. © 2020 by Nova Science Publishers, Inc. All rights reserved.","","Generative adversarial networks; Neural networks; Supervised learning; Unsupervised learning","Book chapter","Final","","Scopus","2-s2.0-85145342974"
"Liu Y.; Pan Y.; Yang W.; Ning Z.; Yue L.; Liu M.; Shen D.","Liu, Yunbi (57190948977); Pan, Yongsheng (56440550200); Yang, Wei (56982069100); Ning, Zhenyuan (57194600411); Yue, Ling (55634279800); Liu, Mingxia (36677833300); Shen, Dinggang (7401738392)","57190948977; 56440550200; 56982069100; 57194600411; 55634279800; 36677833300; 7401738392","Joint Neuroimage Synthesis and Representation Learning for Conversion Prediction of Subjective Cognitive Decline","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12267 LNCS","","","583","592","9","10.1007/978-3-030-59728-3_57","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092736317&doi=10.1007%2f978-3-030-59728-3_57&partnerID=40&md5=08848d7101c2d46020e6fa2a87bcd5a7","Predicting the progression of preclinical Alzheimer’s disease (AD) such as subjective cognitive decline (SCD) is fundamental for the effective intervention of pathological cognitive decline. Even though multimodal neuroimaging has been widely used in automated AD diagnosis, there are few studies dedicated to SCD progression prediction, due to challenges of incomplete and limited data. To this end, we propose a Joint neuroimage Synthesis and Representation Learning (JSRL) framework with transfer learning for SCD conversion prediction using incomplete multimodal neuroimaging data. Specifically, JSRL consists of two major components: 1) a generative adversarial network for synthesizing missing neuroimaging data, and 2) a classification network for learning neuroimage representations and predicting the progression of SCD. These two subnetworks share the same feature encoding module, encouraging that the to-be-generated representations are prediction-oriented and also the underlying association among multimodal images can be effectively modeled for accurate prediction. To handle the limited data problem, we further leverage both image synthesis and prediction models learned from a large-scale ADNI database (with MRI and PET acquired from 863 subjects) to a small-scale SCD database (with only MRI acquired from 113 subjects) in a transfer learning manner. Experimental results show that the proposed JSRL can synthesize reasonable PET scans and is superior to several state-of-the-art methods in SCD conversion prediction. © 2020, Springer Nature Switzerland AG.","Diagnosis; Forecasting; Image coding; Magnetic resonance imaging; Medical computing; Neuroimaging; Transfer learning; Accurate prediction; Adversarial networks; Classification networks; Cognitive decline; Encoding modules; Multi-modal image; Prediction model; State-of-the-art methods; Predictive analytics","","Conference paper","Final","","Scopus","2-s2.0-85092736317"
"Yao G.; Yuan Y.; Shao T.; Zhou K.","Yao, Guangming (57219786187); Yuan, Yi (57211427476); Shao, Tianjia (36462632700); Zhou, Kun (55486916000)","57219786187; 57211427476; 36462632700; 55486916000","Mesh Guided One-shot Face Reenactment Using Graph Convolutional Networks","2020","MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia","","","","1773","1781","8","10.1145/3394171.3413865","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102053374&doi=10.1145%2f3394171.3413865&partnerID=40&md5=b60ae2a9a8385c405d0f7caea97664fe","Face reenactment aims to animate a source face image to a different pose and expression provided by a driving image. Existing approaches are either designed for a specific identity, or suffer from the identity preservation problem in the one-shot or few-shot scenarios. In this paper, we introduce a method for one-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the source mesh and driving mesh) as guidance to learn the optical flow needed for the reenacted face synthesis. Technically, we explicitly exclude the driving face's identity information in the reconstructed driving mesh. In this way, our network can focus on the motion estimation for the source face without the interference of driving face shape. We propose a motion net to learn the face motion, which is an asymmetric autoencoder. The encoder is a graph convolutional network (GCN) that learns a latent motion vector from the meshes, and the decoder serves to produce an optical flow image from the latent vector with CNNs. Compared to previous methods using sparse keypoints to guide the optical flow learning, our motion net learns the optical flow directly from 3D dense meshes, which provide the detailed shape and pose information for the optical flow, so it can achieve more accurate expression and pose on the reenacted face. Extensive experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons. © 2020 ACM.","Convolution; Convolutional neural networks; Flow graphs; Learning systems; Mesh generation; Motion estimation; Optical flows; Convolutional networks; Face synthesis; Identity information; Latent vectors; Motion Vectors; Pose information; Quantitative comparison; State-of-the-art methods; MESH networking","face reenactment; generative adversarial networks; graph convolutional networks; image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102053374"
"Seidlitz S.; Jürgens K.; Makrushin A.; Kraetzer C.; Dittmann J.","Seidlitz, Stefan (57222523152); Jürgens, Kris (57215560755); Makrushin, Andrey (35191938300); Kraetzer, Christian (15033995900); Dittmann, Jana (25821612000)","57222523152; 57215560755; 35191938300; 15033995900; 25821612000","Generation of privacy-friendly datasets of latent fingerprint images using generative adversarial networks","2021","VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","4","","","345","352","7","","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103011482&partnerID=40&md5=6d422ad5cb6cef72fb4dc0bb32a590a1","The restrictions posed by the recent trans-border regulations to the usage of biometric data force researchers in the fields of digitized forensics and biometrics to use synthetic data for development and evaluation of new algorithms. For digitized forensics, we introduce a technique for conversion of privacy-sensitive datasets of real latent fingerprints to ""privacy-friendly"" datasets of synthesized fingerprints. Privacy-friendly means in our context that the generated fingerprint images cannot be linked to a particular person who provided fingerprints to the original dataset. In contrast to the standard fingerprint generation approach that makes use of mathematical modeling for drawing ridge-line patterns, we propose applying a data-driven approach making use of generative adversarial neural networks (GAN). In our synthesis experiments the performance of three established GAN architectures is examined. The NIST Special Database 27 is exemplary used as a data source of real latent fingerprints. The set of training images is augmented by applying filters from the StirTrace benchmarking tool. The suitability of the generated fingerprint images is checked with the NIST fingerprint image quality tool (NFIQ2). The unlinkability to any original fingerprint is established by evaluating outcomes of the NIST fingerprint matching tool. Copyright © 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","Biometrics; Computer graphics; Computer vision; Forensic science; Image quality; Pattern matching; Adversarial networks; Benchmarking tools; Data-driven approach; Fingerprint image quality; Fingerprint images; Fingerprint matching; Latent fingerprint; Synthesis experiment; Privacy by design","Digitized Forensics; GAN; Generative Adversarial Networks; Image Synthesis; Latent Fingerprint; Privacy","Conference paper","Final","","Scopus","2-s2.0-85103011482"
"Chen Y.; Xia S.; Zhao J.; Jian M.; Zhou Y.; Niu Q.; Yao R.; Zhu D.","Chen, Ying (57195284871); Xia, Shixiong (55650066900); Zhao, Jiaqi (57138970300); Jian, Meng (55832971100); Zhou, Yong (35480110700); Niu, Qiang (22635551200); Yao, Rui (23567304500); Zhu, Dongjun (57204810481)","57195284871; 55650066900; 57138970300; 55832971100; 35480110700; 22635551200; 23567304500; 57204810481","Person image synthesis through siamese generative adversarial network","2020","Neurocomputing","417","","","490","500","10","10.1016/j.neucom.2020.09.004","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091790439&doi=10.1016%2fj.neucom.2020.09.004&partnerID=40&md5=53296765367a2b54a82ad54cea592b70","Photo-realistic image synthesis is an attracting idea for person re-identification (ReID) and data augmentation on human pose estimation. However, existing advances manipulating human image synthesis lack texture details for varying poses or appearances. This paper presents a person image synthesis Siamese generative adversarial network (PS2GAN), which re-synthesizes person image by changing the pose of that person to a given pose, modeled in a Siamese structure with image generative network and pair conditional discriminative networks in single-branch. For pose transfer, the proposed PS2GAN adopts Siamese structure consisting of two image generative networks and a novel contrastive-pose loss regularizing the generation process. Additionally, a nearest-neighbor loss computes the difference between fake and real images to make high-level information closer. Furthermore, the proposed PS2GAN is competitive to the state-of-the-art performance on Market-1501 and DeepFashion datasets via qualitatively and quantitatively comparing with prior works, and synthetic images of the PS2GAN can alleviate data insufficiency for person ReID. © 2020 Elsevier B.V.","Textures; Adversarial networks; Discriminative networks; Generation process; High-level information; Human pose estimations; Person re identifications; Photo realistic image synthesis; State-of-the-art performance; adult; article; case report; clinical article; female; human; human experiment; male; synthesis; Thai (people); Image processing","Generative adversarial network; Person image synthesis; Person re-identification; Siamese structure","Article","Final","","Scopus","2-s2.0-85091790439"
"Liu D.; Yang Y.; Jing X.","Liu, Dingdong (57205361579); Yang, Yang (56438087300); Jing, Xiangyi (57221337430)","57205361579; 56438087300; 57221337430","Multiple Facial Expressions Synthesis Driven by Editable Line Maps","2020","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2020-October","","9283195","1645","1650","5","10.1109/SMC42975.2020.9283195","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098858786&doi=10.1109%2fSMC42975.2020.9283195&partnerID=40&md5=a4e8ef95c8d737ec2a8a3bb4cba9fb7d","Facial expression is an important facial semantics on visual aspect. The facial expressions synthesis has a wide range of applications in human-computer interaction and virtual reality. In recent years, image synthesis base on generative adversarial networks(GANs) is developing rapidly. In the image-to-image translation work, we propose a new facial expression generation method base on the idea of conditional GANs and realize the optimization of the generated results. The main work of this paper includes: Editable facial lines map is utilized as a constraint, combining with neutral face images as inputs of generator, so that a variety of facial expression images can be generated by editing the constraints. Correntropy loss of feature matching is added, which is used to measure the intermediate representation between the real images and the generated images by improving the adversarial loss. Consequently, the generated facial expressions can be more realistic. Base on the ideas above, the proposed method needs only one generator to generate different realistic facial images with various expressions. © 2020 IEEE.","Human computer interaction; Semantics; Adversarial networks; Facial expression generation; Facial Expressions; Feature matching; Image synthesis; Image translation; Intermediate representations; Visual aspects; Image enhancement","Correntropy; Facial Expression Synthesis; Generative Adversarial Network; Image-to-Image","Conference paper","Final","","Scopus","2-s2.0-85098858786"
"Wang M.; Lang C.; Feng S.; Wang T.; Jin Y.; Li Y.","Wang, Min (57221235394); Lang, Congyan (7402002472); Feng, Songhe (7402531247); Wang, Tao (56135273700); Jin, Yi (42761595900); Li, Yidong (54955980500)","57221235394; 7402002472; 7402531247; 56135273700; 42761595900; 54955980500","Text to photo-realistic image synthesis via chained deep recurrent generative adversarial network","2021","Journal of Visual Communication and Image Representation","74","","102955","","","","10.1016/j.jvcir.2020.102955","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098950517&doi=10.1016%2fj.jvcir.2020.102955&partnerID=40&md5=5b8683da36d6268eb28ccf5ade789583","Despite the promising progress made in recent years, automatically generating high-resolution realistic images from text descriptions remains a challenging task due to semantic gap between human-written descriptions and diversities of visual appearance. Most existing approaches generate the rough images with the given text descriptions, while the relationship between sentence semantics and visual content is not holistically exploited. In this paper, we propose a novel chained deep recurrent generative adversarial network (CDRGAN) for synthesizing images from text descriptions. Our model uses carefully designed chained deep recurrent generators that simultaneously recovers global image structures and local details. Specially, our method not only considers the logic relationships of image pixels, but also removes computational bottlenecks through parameters sharing. We evaluate our method on three public benchmarks: CUB, Oxford-102 and MS COCO datasets. Experimental results show that our method significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics. © 2021 Elsevier Inc.","Computation theory; Recurrent neural networks; Semantics; Adversarial networks; Computational bottlenecks; Evaluation metrics; Photo realistic image synthesis; Realistic images; State-of-the-art approach; Visual appearance; Written description; Image processing","Computational bottlenecks; Logic relationships; Parameters sharing; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85098950517"
"Hu M.; Guo J.","Hu, Mingming (57213689472); Guo, Jingtao (57209303328)","57213689472; 57209303328","Facial attribute-controlled sketch-to-image translation with generative adversarial networks","2020","Eurasip Journal on Image and Video Processing","2020","1","2","","","","10.1186/s13640-020-0489-5","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077962286&doi=10.1186%2fs13640-020-0489-5&partnerID=40&md5=cad4e1e3059064a9347130d71d776af3","Due to the rapid development of the generative adversarial networks (GANs) and convolution neural networks (CNN), increasing attention is being paid to face synthesis. In this paper, we address the new and challenging task of facial sketch-to-image synthesis with multiple controllable attributes. To achieve this goal, first, we propose a new attribute classification loss to ensure that the synthesized face image with the facial attributes, which the users desire to have. Second, we employ the reconstruction loss to synthesize the facial texture and structure information. Third, the adversarial loss is used to encourage visual authenticity. By incorporating above losses into a unified framework, our proposed method not only can achieve high-quality sketch-to-image translation, but also allow the users control the facial attributes of synthesized image. Extensive experiments show that user-provided facial attribute information effectively controls the process of facial sketch-to-image translation. © 2020, The Author(s).","Quality control; Textures; Adversarial networks; Attribute information; Convolution neural network; Facial attribute editing; Image translation; Structure information; Synthesized images; Unified framework; Image processing","Face sketch to image translation; Facial attribute classifier; Facial attribute editing; Generative adversarial networks (GANs)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85077962286"
"Guo C.; Dai N.; Tian S.; Sun Y.; Yu Q.; Liu H.; Cheng X.","Guo, Chuang (57219566835); Dai, Ning (56365979900); Tian, Sukun (57202847579); Sun, Yuchun (54380707800); Yu, Qing (57198511103); Liu, Hao (57744177700); Cheng, Xiaosheng (8945738400)","57219566835; 56365979900; 57202847579; 54380707800; 57198511103; 57744177700; 8945738400","Morphological design of missing tooth driven by high-resolution deep generation network; [高分辨率深度生成网络的缺失牙体形态设计]","2020","Journal of Image and Graphics","25","10","","2249","2258","9","10.11834/jig.200252","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093948294&doi=10.11834%2fjig.200252&partnerID=40&md5=7f1d5ce3e847944735902869972c3448","Objective: Dental defect is a common problem in oral restoration treatment. For the symptoms of severe tooth loss, the traditional clinical treatment method is porcelain tooth restoration. This method primarily relies on the experience and technology of physicians and experts. Given the long time, this method leads to restoration precision is not easy to control. The current restoration method is to use the dental restoration computer aided design/computer aided manufacturing(CAD/CAM) system for restoration treatment. For example, in the CAD design of full-crown restorations, most methods use the standard tooth model in the standard database as the initial model to replace the defective tooth. A series of algorithms are used to properly deform the crown mesh model to reconstruct the target full-crown restoration model. This method uses the standard tooth model, lacks personalization, and requires a large number of man-machines in the design process. With regard to the interactive operation, the operator's technical experience is required, and the design contains a large degree of subjectivity. A personalized design method for the missing tooth crown based on the generative adversarial networks (HRGAN) is proposed to solve the personalized design of complex tooth morphology. Method: Given the powerful processing ability of the HRGAN on the image, the three-dimensional model is reduced to a two-dimensional image, and the network can learn it. First, the depth distance of the tooth is calculated by using the multi-direction orthogonal projection method, and the high-resolution depth images of the occlusal surface, lingual side, and buccal side were constructed. The quality of the tooth image is optimized by the method of pixel value enhancement to subsequently use the generative confrontation network and better fit the morphological distribution of the crown. Image entropy describes the richness of the information carried by the image. Image entropy is used to evaluate the constructed depth image and construct a batch of learning data sets suitable for deep convolutional networks. Then, based on the multi-scale generative and discriminant models, the structure of the missing tooth restoration network model is constructed. The generative model generates the crown data by capturing the distribution of the crown data. The discriminant model is used to determine whether a sample is from expert design data rather than generated data. The two models are trained alternately and iteratively, playing with each other and constantly optimizing. The network merges a feature-matching loss on the basis of the discriminator to better fit the morphological features of the teeth and adds a perceptual loss to enhance network's ability to reconstruct the shape of the occlusal surface of the tooth. Finally, the point cloud data of the crown are calculated on the basis of the gray value and distance value mapping, and the rough registration is performed according to the information of the missing crown in the adjacent tooth. The iterative closest point algorithm is used to precisely register the generated crown point cloud with the original point cloud and achieve the reconstruction of the missing tooth model. Result: We have constructed 500 sets of tooth depth map data, of which 400 sets are used as training sets and 100 sets are used as verification sets. The quality of the network-generated image is evaluated by the similarity between the tooth image synthesized from the HRGAN and the image obtained from the expert-designed tooth under experiments with different loss conditions. By calculating the peak signal to noise ratio(PSNR) and structrual similarity(SSIM) values of the two groups of images, the similarity between the images generated by the network and the image designed by the expert is improved after adding the loss of feature matching and perception, and the reconstructed occlusal surface of the crown has rich features of tooth pits and sulcus. The deviation analysis of the reconstructed occlusal surface and its corresponding expert design crown is carried out. The results show that compared with the CAD design method and the generative model-based design method, the standard deviation of the crown shape generated by the method in this paper is 21.2% and 7% lower than the former two methods, and the root mean square(RMS) value is 43.8% and 9.8% lower, respectively, and the shape of the occlusal surface of the crown is the closest to the shape of the expert-designed crown. Conclusion: The experimental results show that the tooth design method based on the high-resolution generation network proposed in this paper can effectively complete the morphological design of the missing tooth, and the designed crown has enough natural tooth anatomical morphological characteristics. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","Full crown restoration; Generative adversarial network(GAN); Image synthesis; Point cloud registration; Tooth shape design","Article","Final","","Scopus","2-s2.0-85093948294"
"Xue Y.; Ye J.; Zhou Q.; Long L.R.; Antani S.; Xue Z.; Cornwell C.; Zaino R.; Cheng K.C.; Huang X.","Xue, Yuan (57202313224); Ye, Jiarong (57211998352); Zhou, Qianying (57211999474); Long, L. Rodney (7201908308); Antani, Sameer (6701355570); Xue, Zhiyun (23007040400); Cornwell, Carl (15623148800); Zaino, Richard (7005347157); Cheng, Keith C. (57201532636); Huang, Xiaolei (57218604182)","57202313224; 57211998352; 57211999474; 7201908308; 6701355570; 23007040400; 15623148800; 7005347157; 57201532636; 57218604182","Selective synthetic augmentation with HistoGAN for improved histopathology image classification","2021","Medical Image Analysis","67","","101816","","","","10.1016/j.media.2020.101816","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092720332&doi=10.1016%2fj.media.2020.101816&partnerID=40&md5=8d80c4c8ec0cdfa1bce773b85efdd401","Histopathological analysis is the present gold standard for precancerous lesion diagnosis. The goal of automated histopathological classification from digital images requires supervised training, which requires a large number of expert annotations that can be expensive and time-consuming to collect. Meanwhile, accurate classification of image patches cropped from whole-slide images is essential for standard sliding window based histopathology slide classification methods. To mitigate these issues, we propose a carefully designed conditional GAN model, namely HistoGAN, for synthesizing realistic histopathology image patches conditioned on class labels. We also investigate a novel synthetic augmentation framework that selectively adds new synthetic image patches generated by our proposed HistoGAN, rather than expanding directly the training set with synthetic images. By selecting synthetic images based on the confidence of their assigned labels and their feature similarity to real labeled images, our framework provides quality assurance to synthetic augmentation. Our models are evaluated on two datasets: a cervical histopathology image dataset with limited annotations, and another dataset of lymph node histopathology images with metastatic cancer. Here, we show that leveraging HistoGAN generated images with selective augmentation results in significant and consistent improvements of classification performance (6.7% and 2.8% higher accuracy, respectively) for cervical histopathology and metastatic cancer datasets. © 2020","Humans; Neoplasms; Classification (of information); Diseases; Image classification; Quality assurance; Classification methods; Classification performance; Expert annotations; Feature similarities; Histopathological analysis; Sliding window-based; Supervised trainings; Whole slide images; Article; clinical evaluation; comparative study; controlled study; deep learning; deep neural network; diagnostic accuracy; feature extraction; histopathology; histopathology generative adversarial network; image analysis; image segmentation; lymph node; metastasis; priority journal; quality control; uterine cervix cancer; human; neoplasm; Image enhancement","Histopathology image classification; Medical image synthesis; Synthetic data augmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092720332"
"Hammami M.; Friboulet D.; Kechichian R.","Hammami, Maryam (57221264849); Friboulet, Denis (6602133133); Kechichian, Razmig (54928118000)","57221264849; 6602133133; 54928118000","Cycle GAN-Based Data Augmentation for Multi-Organ Detection in CT Images Via Yolo","2020","Proceedings - International Conference on Image Processing, ICIP","2020-October","","9191127","390","393","3","10.1109/ICIP40778.2020.9191127","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098638928&doi=10.1109%2fICIP40778.2020.9191127&partnerID=40&md5=620d0be3001302da7cf9b696f6530e11","We propose a deep learning solution to the problem of object detection in 3D CT images, i.e. the localization and classification of multiple structures. Supervised learning methods require large annotated datasets that are usually difficult to acquire. We thus develop a Cycle Generative Adversarial Network (CycleGAN) + You Only Look Once (YOLO) combined method for CT data augmentation using MRI source images to train a YOLO detector. This results in a fast and accurate detection with a mean average distance of 7. 95 \pm 6.2 mm, which is significantly better than detection without data augmentation. We show that the approach compares favorably to state-of-the-art detection methods for medical images. © 2020 IEEE.","Deep learning; Large dataset; Learning systems; Magnetic resonance imaging; Medical imaging; Object detection; Adversarial networks; Annotated datasets; Average Distance; Data augmentation; Detection methods; Multiple structures; State of the art; Supervised learning methods; Computerized tomography","data augmentation; image synthesis; medical imaging; multi-organ detection","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098638928"
"Diviš V.; Hrúz M.","Diviš, Václav (57219467263); Hrúz, Marek (25646087700)","57219467263; 25646087700","Evaluation of Image Synthesis for Automotive Purposes","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12336 LNAI","","","67","77","10","10.1007/978-3-030-60337-3_7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092929201&doi=10.1007%2f978-3-030-60337-3_7&partnerID=40&md5=603e526236dfdeb11363bbec4f9c0d68","The aim of this article is to evaluate a state of the art image synthesis carried out via Generative Adversarial Networks (conditional Wasserstein GAN and Self Attention GAN) on a traffic signs dataset. For the experiment, we focused on generating images with a 64 × 64 -pixel resolution as well as on the GAN’s ability to capture structural and geometric patterns. Four different GAN architectures were trained in order to highlight the difficulties of the training, such as collapse mode, vanishing gradient and resulting image fidelity. The Frechent Inception Distance is compared with other state of the art results. The importance of evaluating on automotive datasets as well as additional wishes for further improvements are addressed at the end of this article. © 2020, Springer Nature Switzerland AG.","Robotics; Traffic signs; Adversarial networks; Collapse mode; Geometric patterns; Image fidelity; Image synthesis; Pixel resolution; State of the art; Vanishing gradient; Image processing","BigGAN; Generative Adversarial Networks; Image dataset extension; SAGAN; Traffic signs dataset; WGAN","Conference paper","Final","","Scopus","2-s2.0-85092929201"
"Janveja I.; Nambi A.; Bannur S.; Gupta S.; Padmanabhan V.","Janveja, Ishani (57197707449); Nambi, Akshay (55834833700); Bannur, Shruthi (57204727327); Gupta, Sanchit (57220722027); Padmanabhan, Venkat (57203194657)","57197707449; 55834833700; 57204727327; 57220722027; 57203194657","InSight: Monitoring the State of the Driver in Low-Light Using Smartphones","2020","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","4","3","83","","","","10.1145/3411819","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092428684&doi=10.1145%2f3411819&partnerID=40&md5=677f30d5bd206e107bd0ed8791fb8ab7","Road safety is a major public health issue across the globe and over two-thirds of the road accidents occur at nighttime under low-light conditions or darkness. The state of the driver and her/his actions are the key factors impacting road safety. How can we monitor these in a cost-effective manner and in low-light conditions? RGB cameras present in smartphones perform poorly in low-lighting conditions due to lack of information captured. Hence, existing monitoring solutions rely upon specialized hardware such as infrared cameras or thermal cameras in low-light conditions, but are limited to only high-end vehicles owing to the cost of the hardware. We present InSight, a windshield-mounted smartphone-based system that can be retrofitted to the vehicle to monitor the state of the driver, specifically driver fatigue (based on frequent yawning and eye closure) and driver distraction (based on their direction of gaze). Challenges arise from designing an accurate, yet low-cost and non-intrusive system to continuously monitor the state of the driver. In this paper, we present two novel and practical approaches for continuous driver monitoring in low-light conditions: (i) Image synthesis: Enabling monitoring in low-light conditions using just the smartphone RGB camera by synthesizing a thermal image from RGB with a Generative Adversarial Network, and (ii) Near-IR LED: Using a low-cost near-IR (NIR) LED attachment to the smartphone, where the NIR LED acts as a light source to illuminate the driver's face, which is not visible to the human eyes, but can be captured by standard smartphone cameras without any specialized hardware. We show that the proposed techniques can capture the driver's face accurately in low-lighting conditions to monitor driver's state. Further, since NIR and thermal imagery is significantly different than RGB images, we present a systematic approach to generate labelled data, which is used to train existing computer vision models. We present an extensive evaluation of both the approaches with data collected from 15 drivers in controlled basement area and on real roads in low-light conditions. The proposed NIR LED setup has an accuracy (Fl-score) of 85% and 93.8% in detecting driver fatigue and distraction, respectively in low-light. © 2020 ACM.","Accident prevention; Cameras; Computer hardware; Cost effectiveness; Costs; Infrared devices; Light emitting diodes; Motor transportation; Roads and streets; Adversarial networks; Driver distractions; Infra-red cameras; Lighting conditions; Low light conditions; Public health issues; Smart-phone cameras; Specialized hardware; Smartphones","Driver Monitoring and Low-light; Edge Processing; Mobile Systems; Video Analytics","Article","Final","","Scopus","2-s2.0-85092428684"
"Ak K.E.; Lim J.H.; Tham J.Y.; Kassim A.A.","Ak, Kenan E. (56779817500); Lim, Joo Hwee (7403454337); Tham, Jo Yew (57217528957); Kassim, Ashraf A. (7004412916)","56779817500; 7403454337; 57217528957; 7004412916","Semantically consistent text to fashion image synthesis with an enhanced attentional generative adversarial network","2020","Pattern Recognition Letters","135","","","22","29","7","10.1016/j.patrec.2020.02.030","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083286910&doi=10.1016%2fj.patrec.2020.02.030&partnerID=40&md5=160ae183f3a890cd7170299e1792d19b","Recent advancements in Generative Adversarial Networks (GANs) have led to significant improvements in various image generation tasks including image synthesis based on text descriptions. In this paper, we present an enhanced Attentional Generative Adversarial Network (e-AttnGAN) with improved training stability for text-to-image synthesis. e-AttnGAN's integrated attention module utilizes both sentence and word context features and performs feature-wise linear modulation (FiLM) to fuse visual and natural language representations. In addition to multimodal similarity learning for text and image features of AttnGAN [1], similarity and feature matching losses between real and generated images are included while employing classification losses for “significant attributes”. In order to improve the stability of the training and solve the mode collapse issue, spectral normalization and two-time scale update rule are used for the discriminator together with instance noise. Our experiments show that e-AttnGAN outperforms state-of-the-art methods using the FashionGen and DeepFashion-Synthesis datasets in terms of inception score, R-precision and classification accuracy. A detailed ablation study has been conducted to observe the effect of each component. © 2020 Elsevier B.V.","Classification (of information); Semantic Web; Text processing; Visual languages; Adversarial networks; Classification accuracy; Image generations; Linear modulations; Natural language representation; Similarity learning; Spectral normalization; State-of-the-art methods; Image enhancement","Fashion; Generative adversarial networks; Hierarchical image generation; Image generation; Text-to-Image synthesis","Article","Final","","Scopus","2-s2.0-85083286910"
"Park S.-W.; Huh J.-H.; Kim J.-C.","Park, Sung-Wook (57205079478); Huh, Jun-Ho (56438784800); Kim, Jong-Chan (49963957400)","57205079478; 56438784800; 49963957400","BEGAN v3: Avoiding mode collapse in GANs using variational inference","2020","Electronics (Switzerland)","9","4","688","","","","10.3390/electronics9040688","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084044916&doi=10.3390%2felectronics9040688&partnerID=40&md5=b935cf2443c6e08fa041c380735b6895","In the field of deep learning, the generative model did not attract much attention until GANs (generative adversarial networks) appeared. In 2014, Google's Ian Goodfellow proposed a generative model called GANs. GANs use different structures and objective functions from the existing generative model. For example, GANs use two neural networks: a generator that creates a realistic image, and a discriminator that distinguishes whether the input is real or synthetic. If there are no problems in the training process, GANs can generate images that are difficult even for experts to distinguish in terms of authenticity. Currently, GANs are the most researched subject in the field of computer vision, which deals with the technology of image style translation, synthesis, and generation, and various models have been unveiled. The issues raised are also improving one by one. In image synthesis, BEGAN (Boundary Equilibrium Generative Adversarial Network), which outperforms the previously announced GANs, learns the latent space of the image, while balancing the generator and discriminator. Nonetheless, BEGAN also has a mode collapse wherein the generator generates only a few images or a single one. Although BEGAN-CS (Boundary Equilibrium Generative Adversarial Network with Constrained Space), which was improved in terms of loss function, was introduced, it did not solve the mode collapse. The discriminator structure of BEGAN-CS is AE (AutoEncoder), which cannot create a particularly useful or structured latent space. Compression performance is not good either. In this paper, this characteristic of AE is considered to be related to the occurrence of mode collapse. Thus, we used VAE (Variational AutoEncoder), which added statistical techniques to AE. As a result of the experiment, the proposed model did not cause mode collapse but converged to a better state than BEGAN-CS. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","","Artificial intelligence; Boundary equilibrium generative adversarial networks; Computer vision; Deep learning; Generative adversarial networks; Mode collapse; Variational inference","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084044916"
"Lee H.; Kim G.; Hur Y.; Lim H.","Lee, Hyunhee (57217057860); Kim, Gyeongmin (57215667681); Hur, Yuna (57215661914); Lim, Heuiseok (36028297500)","57217057860; 57215667681; 57215661914; 36028297500","Visual Thinking of Neural Networks: Interactive Text to Image Synthesis","2021","IEEE Access","9","","9410550","64510","64523","13","10.1109/ACCESS.2021.3074973","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104670342&doi=10.1109%2fACCESS.2021.3074973&partnerID=40&md5=6fc7a9bd492d80968f4586ce66a01070","Reasoning, a trait of cognitive intelligence, is regarded as a crucial ability that distinguishes humans from other species. However, neural networks now pose a challenge to this human ability. Text-to-image synthesis is a class of vision and linguistics, wherein the goal is to learn multimodal representations between the image and text features. Hence, it requires a high-level reasoning ability that understands the relationships between objects in the given text and generates high-quality images based on the understanding. Text-to-image translation can be termed as the visual thinking of neural networks. In this study, our model infers the complicated relationships between objects in the given text and generates the final image by leveraging the previous history. We define diverse novel adversarial loss functions and finally demonstrate the best one that elevates the reasoning ability of the text-to-image synthesis. Remarkably, most of our models possess their own reasoning ability. Quantitative and qualitative comparisons with several methods demonstrate the superiority of our approach.  © 2013 IEEE.","Image processing; Cognitive intelligence; High quality images; High-level reasoning; Human abilities; Image translation; Interactive texts; Reasoning ability; Visual thinking; Neural networks","Generative adversarial networks; image generation; multimodal learning; multimodal representation; text-to-image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85104670342"
"Gao L.; Zhu J.; Song J.; Zheng F.; Shen H.T.","Gao, Lianli (56611089900); Zhu, Junchen (57224080867); Song, Jingkuan (57205085174); Zheng, Feng (36070223900); Shen, Heng Tao (7404523209)","56611089900; 57224080867; 57205085174; 36070223900; 7404523209","Lab2Pix: Label-Adaptive Generative Adversarial Network for Unsupervised Image Synthesis","2020","MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia","","","","3734","3742","8","10.1145/3394171.3414027","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106888582&doi=10.1145%2f3394171.3414027&partnerID=40&md5=0164bfc60b3e9b6c5c6da9a6afba8e92","Lab2Pix refers to the task of generating photo-realistic images from labels, e.g., semantic labels or sketch labels. Despite inheriting from image-to-image translation, Lab2Pix develops its own characteristics due to the differences between labels and general images. This prevents Lab2Pix task from simply applying general image-to-image translation models. Therefore, we propose an unsupervised framework named Lab2Pix to adaptively synthesize images from labels by elegantly considering the particular properties of label to image synthesis task. Specifically, since the labels contain much less information than the images, we design our generator in a cumulative style which gradually renders synthesized images by fusing features in different levels. Accordingly, the verification process feeds the generated images to a segmentation component and compares the results to the original input label. Furthermore, we propose a sharp enhancement loss, an image consistency loss and a foreground enhancement mask to encourage the network to synthesize photo-realistic images. Experiments conducted on Cityscapes, Facades, Edge2shoes and Edge2handbags datasets demonstrate that our Lab2Pix significantly outperforms existing state-of-the-art unsupervised methods and is even comparable to supervised methods. The source code is available at https://github.com/RoseRollZhu/Lab2Pix.  © 2020 ACM.","Image segmentation; Semantics; Adversarial networks; Image translation; Photorealistic images; State of the art; Supervised methods; Synthesized images; Unsupervised method; Verification process; Image enhancement","generative adversarial network; image synthesis; unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85106888582"
"Liu Y.; Yang J.; Zhou Y.; Wang W.; Zhao J.; Yu W.; Zhang D.; Ding D.; Li X.; Chen Y.","Liu, Yutong (57216036621); Yang, Jingyuan (57192931561); Zhou, Yang (57830708000); Wang, Weisen (57211999177); Zhao, Jianchun (57211997977); Yu, Weihong (14920074400); Zhang, Dingding (57202716123); Ding, Dayong (57205391568); Li, Xirong (23035594000); Chen, Youxin (16303392000)","57216036621; 57192931561; 57830708000; 57211999177; 57211997977; 14920074400; 57202716123; 57205391568; 23035594000; 16303392000","Prediction of OCT images of short-term response to anti-VEGF treatment for neovascular age-related macular degeneration using generative adversarial network","2020","British Journal of Ophthalmology","104","12","","1735","1740","5","10.1136/bjophthalmol-2019-315338","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082516865&doi=10.1136%2fbjophthalmol-2019-315338&partnerID=40&md5=36e193001b6e9a4be4caf96026497589","Background/aims The aim of this study was to generate and evaluate individualised post-therapeutic optical coherence tomography (OCT) images that could predict the short-term response of antivascular endothelial growth factor therapy for typical neovascular age-related macular degeneration (nAMD) based on pretherapeutic images using generative adversarial network (GAN). Methods A total of 476 pairs of pretherapeutic and post-therapeutic OCT images of patients with nAMD were included in training set, while 50 pretherapeutic OCT images were included in the tests set retrospectively, and their corresponding post-therapeutic OCT images were used to evaluate the synthetic images. The pix2pixHD method was adopted for image synthesis. Three experiments were performed to evaluate the quality, authenticity and predictive power of the synthetic images by retinal specialists. Results We found that 92% of the synthetic OCT images had sufficient quality for further clinical interpretation. Only about 26%-30% synthetic post-therapeutic images could be accurately identified as synthetic images. The accuracy to predict macular status of wet or dry was 0.85 (95% CI 0.74 to 0.95). Conclusion Our results revealed a great potential of GAN to generate post-therapeutic OCT images with both good quality and high accuracy.  © 2020 BMJ Publishing Group. All rights reserved.","Aged; Angiogenesis Inhibitors; Bevacizumab; Female; Fluorescein Angiography; Follow-Up Studies; Fundus Oculi; Humans; Intravitreal Injections; Macula Lutea; Male; Receptors, Vascular Endothelial Growth Factor; Retrospective Studies; Time Factors; Tomography, Optical Coherence; Treatment Outcome; Wet Macular Degeneration; vasculotropin inhibitor; angiogenesis inhibitor; bevacizumab; vasculotropin receptor; accuracy; age related macular degeneration; aged; Article; clinical feature; female; human; image analysis; major clinical study; male; ophthalmologist; optical coherence tomography; prediction; priority journal; retina macula lutea; retrospective study; treatment response; drug effect; eye fundus; fluorescence angiography; follow up; intravitreal drug administration; optical coherence tomography; pathology; procedures; time factor; treatment outcome; wet macular degeneration","retina","Article","Final","","Scopus","2-s2.0-85082516865"
"Liu R.; Ge Y.; Choi C.L.; Wang X.; Li H.","Liu, Rui (56873525500); Ge, Yixiao (57188845357); Choi, Ching Lam (57222870220); Wang, Xiaogang (55736875200); Li, Hongsheng (57141098300)","56873525500; 57188845357; 57222870220; 55736875200; 57141098300","DiVCO: Diverse conditional image synthesis via contrastive generative adversarial network","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","16372","16381","9","10.1109/CVPR46437.2021.01611","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106117338&doi=10.1109%2fCVPR46437.2021.01611&partnerID=40&md5=2cb4536068ba551cc89f57d151357f72","Conditional generative adversarial networks (cGANs) target at synthesizing diverse images given the input conditions and latent codes, but unfortunately, they usually suffer from the issue of mode collapse. To solve this issue, previous works [47, 22] mainly focused on encouraging the correlation between the latent codes and their generated images, while ignoring the relations between images generated from various latent codes. The recent MSGAN [27] tried to encourage the diversity of the generated image but only considers “negative” relations between the image pairs. In this paper, we propose a novel DivCo framework to properly constrain both “positive” and “negative” relations between the generated images specified in the latent space. To the best of our knowledge, this is the first attempt to use contrastive learning for diverse conditional image synthesis. A novel latent-augmented contrastive loss is introduced, which encourages images generated from adjacent latent codes to be similar and those generated from distinct latent codes to be dissimilar. The proposed latent-augmented contrastive loss is well compatible with various cGAN architectures. Extensive experiments demonstrate that the proposed DivCo can produce more diverse images than state-of-the-art methods without sacrificing visual quality in multiple unpaired and paired image generation tasks. Training code and pretrained models are available at https://github.com/ruiliu-ai/DivCo. © 2021 IEEE","Codes (symbols); Computer vision; Condition; Image generations; Image pairs; Images synthesis; State-of-the-art methods; Training codes; Visual qualities; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85106117338"
"Nie D.; Shen D.","Nie, Dong (57188806186); Shen, Dinggang (7401738392)","57188806186; 7401738392","Adversarial Confidence Learning for Medical Image Segmentation and Synthesis","2020","International Journal of Computer Vision","128","10-11","","2494","2513","19","10.1007/s11263-020-01321-2","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082850543&doi=10.1007%2fs11263-020-01321-2&partnerID=40&md5=9c273fefe258492535eac64e32bdc244","Generative adversarial networks (GAN) are widely used in medical image analysis tasks, such as medical image segmentation and synthesis. In these works, adversarial learning is directly applied to the original supervised segmentation (synthesis) networks. The usage of adversarial learning is effective in improving visual perception performance since adversarial learning works as realistic regularization for supervised generators. However, the quantitative performance often cannot improve as much as the qualitative performance, and it can even become worse in some cases. In this paper, we explore how we can take better advantage of adversarial learning in supervised segmentation (synthesis) models and propose an adversarial confidence learning framework to better model these problems. We analyze the roles of discriminator in the classic GANs and compare them with those in supervised adversarial systems. Based on this analysis, we propose adversarial confidence learning, i.e., besides the adversarial learning for emphasizing visual perception, we use the confidence information provided by the adversarial network to enhance the design of supervised segmentation (synthesis) network. In particular, we propose using a fully convolutional adversarial network for confidence learning to provide voxel-wise and region-wise confidence information for the segmentation (synthesis) network. With these settings, we propose a difficulty-aware attention mechanism to properly handle hard samples or regions by taking structural information into consideration so that we can better deal with the irregular distribution of medical data. Furthermore, we investigate the loss functions of various GANs and propose using the binary cross entropy loss to train the proposed adversarial system so that we can retain the unlimited modeling capacity of the discriminator. Experimental results on clinical and challenge datasets show that our proposed network can achieve state-of-the-art segmentation (synthesis) accuracy. Further analysis also indicates that adversarial confidence learning can both improve the visual perception performance and the quantitative performance. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Image analysis; Image segmentation; Medical imaging; Vision; Adversarial confidence learning; Adversarial learning; Adversarial networks; Attention mechanisms; Confidence information; Image synthesis; Structural information; Supervised segmentation; Medical image processing","Adversarial confidence learning; Image synthesis; Medical image analysis; Segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85082850543"
"Ma R.; Lou J.","Ma, Ruixin (55417819800); Lou, Junying (57223008212)","55417819800; 57223008212","CPGAN: An efficient architecture designing for text-to-image generative adversarial networks based on canonical polyadic decomposition","2021","Scientific Programming","2021","","5573751","","","","10.1155/2021/5573751","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104511198&doi=10.1155%2f2021%2f5573751&partnerID=40&md5=8e7ce128780a6d486856d12284e4ab6b","Text-to-image synthesis is an important and challenging application of computer vision. Many interesting and meaningful text-to-image synthesis models have been put forward. However, most of the works pay attention to the quality of synthesis images, but rarely consider the size of these models. Large models contain many parameters and high delay, which makes it difficult to be deployed on mobile applications. To solve this problem, we propose an efficient architecture CPGAN for text-to-image generative adversarial networks (GAN) based on canonical polyadic decomposition (CPD). It is a general method to design the lightweight architecture of text-to-image GAN. To improve the stability of CPGAN, we introduce conditioning augmentation and the idea of autoencoder during the training process. Experimental results prove that our architecture CPGAN can maintain the quality of generated images and reduce at least 20% parameters and flops. Copyright © 2021 Ruixin Ma and Junying Lou.","Design; Network architecture; Adversarial networks; Canonical polyadic decompositions; Efficient architecture; General method; Image synthesis; Lightweight architecture; Mobile applications; Training process; Image processing","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85104511198"
"Wang R.; Juefei-Xu F.; Ma L.; Xie X.; Huang Y.; Wang J.; Liu Y.","Wang, Run (55939516400); Juefei-Xu, Felix (54911989900); Ma, Lei (55479591700); Xie, Xiaofei (55268560900); Huang, Yihao (57214756217); Wang, Jian (57221358273); Liu, Yang (56911879800)","55939516400; 54911989900; 55479591700; 55268560900; 57214756217; 57221358273; 56911879800","FakeSpotter: A simple yet robust baseline for spotting AI-synthesized fake faces","2020","IJCAI International Joint Conference on Artificial Intelligence","2021-January","","","3444","3451","7","","53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095172756&partnerID=40&md5=b264b71d97df13ba3cf34c3fccc6dc3c","In recent years, generative adversarial networks (GANs) and its variants have achieved unprecedented success in image synthesis. They are widely adopted in synthesizing facial images which brings potential security concerns to humans as the fakes spread and fuel the misinformation. However, robust detectors of these AI-synthesized fake faces are still in their infancy and are not ready to fully tackle this emerging challenge. In this work, we propose a novel approach, named FakeSpotter, based on monitoring neuron behaviors to spot AI-synthesized fake faces. The studies on neuron coverage and interactions have successfully shown that they can be served as testing criteria for deep learning systems, especially under the settings of being exposed to adversarial attacks. Here, we conjecture that monitoring neuron behavior can also serve as an asset in detecting fake faces since layer-by-layer neuron activation patterns may capture more subtle features that are important for the fake detector. Experimental results on detecting four types of fake faces synthesized with the state-of-the-art GANs and evading four perturbation attacks show the effectiveness and robustness of our approach. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.","Deep learning; Learning systems; Neurons; Activation patterns; Adversarial networks; Facial images; Image synthesis; Layer by layer; Robust detector; State of the art; Testing criteria; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85095172756"
"Granstedt J.L.; Kelkar V.A.; Zhou W.; Anastasio M.A.","Granstedt, Jason L. (57190435103); Kelkar, Varun A. (57204540505); Zhou, Weimin (57193382198); Anastasio, Mark A. (7006769220)","57190435103; 57204540505; 57193382198; 7006769220","SlabGAN: A method for generating efficient 3D anisotropic medical volumes using generative adversarial networks","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11596","","1159617","","","","10.1117/12.2581380","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103628174&doi=10.1117%2f12.2581380&partnerID=40&md5=e6f2edca40714be58a31ff591a2d213b","Generative adversarial networks (GANs) have proven useful for several medical imaging tasks, including image reconstruction and stochastic object model generation. Thus far, most of the work with GANs has been constrained to twodimensional images. Considering that medical imaging data are often inherently three-dimensional (3D), a 3D GAN would be a more principled way to synthesize realistic volumes. Training a 3D GAN is both computationally and memory intensive. However, prior work has not considered the anisotropic nature of many medical imaging systems. In this paper, the SlabGAN is proposed to reduce the inefficiencies associated with training a 3D GAN. The SlabGAN uses the progressive GAN architecture extended to 3D, but removes the requirement of the three dimensions being equal sizes. This permits the generation of anisotropic 3D volumes with large x and y dimensions. The SlabGAN is trained on MRI brain images from the fastMRI dataset to generate images of dimension 256×256×16. The x and y dimensions of these images are comparable to previously published results while requiring significantly fewer computational resources to generate. The trained SlabGAN is applicable to tasks such as 3D medical image reconstruction and thin-slice MR super resolution.  © 2021 SPIE.","Anisotropy; Brain mapping; Magnetic resonance imaging; Medical image processing; Stochastic models; Stochastic systems; 3D medical image; Adversarial networks; Brain images; Computational resources; Super resolution; Three dimensions; Threedimensional (3-d); Two-dimensional images; Image reconstruction","3D; Anisotropic; Deep learning; GAN; Generative adversarial network; Image synthesis; MRI","Conference paper","Final","","Scopus","2-s2.0-85103628174"
"Osahor U.; Kazemi H.; Dabouei A.; Nasrabadi N.","Osahor, Uche (57211069120); Kazemi, Hadi (56747439600); Dabouei, Ali (57203221299); Nasrabadi, Nasser (7006312852)","57211069120; 56747439600; 57203221299; 7006312852","Quality guided sketch-to-photo image synthesis","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9150741","3575","3584","9","10.1109/CVPRW50498.2020.00418","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090115318&doi=10.1109%2fCVPRW50498.2020.00418&partnerID=40&md5=8ac9c7066110e74dab63e4a9130e04d6","Facial sketches drawn by artists are widely used for visual identification applications and mostly by law enforcement agencies, but the quality of these sketches depend on the ability of the artist to clearly replicate all the key facial features that could aid in capturing the true identity of a subject. Recent works have attempted to synthesize these sketches into plausible visual images to improve visual recognition and identification. However, synthesizing photo-realistic images from sketches proves to be an even more challenging task, especially for sensitive applications such as suspect identification. In this work, we propose a novel approach that adopts a generative adversarial network that synthesizes a single sketch into multiple synthetic images with unique attributes like hair color, sex, etc. We incorporate a hybrid discriminator which performs attribute classification of multiple target attributes, a quality guided encoder that minimizes the perceptual dissimilarity of the latent space embedding of the synthesized and real image at different layers in the network and an identity preserving network that maintains the identity of the synthesised image throughout the training process. Our approach is aimed at improving the visual appeal of the synthesised images while incorporating multiple attribute assignment to the generator without compromising the identity of the synthesised image. We synthesised sketches using XDOG filter for the CelebA, WVU Multi-modal and CelebA-HQ datasets and from an auxiliary generator trained on sketches from CUHK, IIT-D and FERET datasets. Our results are impressive compared to current state of the art. © 2020 IEEE.","Computer vision; Drawing (graphics); Laws and legislation; Network layers; Adversarial networks; Law-enforcement agencies; Multiple attributes; Photorealistic images; Sensitive application; Synthetic images; Visual identification; Visual recognition; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090115318"
"Long J.; Lu H.","Long, Jia (57222172775); Lu, Hongtao (8943716200)","57222172775; 8943716200","Multi-level Gate Feature Aggregation with Spatially Adaptive Batch-Instance Normalization for Semantic Image Synthesis","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12572 LNCS","","","378","390","12","10.1007/978-3-030-67832-6_31","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101751725&doi=10.1007%2f978-3-030-67832-6_31&partnerID=40&md5=c2910412780087250e6ecf2d80214d4e","In this paper, we focus on the task of generating realistic images given an input semantic layout, which is also called semantic image synthesis. Most of previous methods are based on conditional generative adversarial networks mechanism, which is stacks of convolution, normalization, and non-linearity layers. However, these methods easily generate blurred regions and distorted structures. There are two limits existing: their normalization layers are unable to make a good balance between keeping semantic layout information and geometric changes; and cannot effectively aggregated multi-level feature. To address the above problems, we propose a novel method which incorporates multi-level gate feature aggregation mechanism (GFA) and spatially adaptive batch-instance normalization (SPAda-BIN) for semantic image synthesis. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, in terms of both visual fidelity and quantitative metrics. © 2021, Springer Nature Switzerland AG.","Semantics; Adversarial networks; Feature aggregation; Geometric changes; Layout information; Quantitative metrics; Realistic images; Spatially adaptive; Visual fidelity; Image processing","Image-to-image translation; Multi-level feature fusion; Normalization; Semantic image synthesis","Conference paper","Final","","Scopus","2-s2.0-85101751725"
"Chen J.; Luo S.; Xiong M.; Peng T.; Zhu P.; Jiang M.; Qin X.","Chen, Jia (55867991200); Luo, Shuang (57201583550); Xiong, Mingfu (57190945157); Peng, Tao (57219304637); Zhu, Ping (57199310474); Jiang, Minghua (57199169386); Qin, Xiao (8520825700)","55867991200; 57201583550; 57190945157; 57219304637; 57199310474; 57199169386; 8520825700","HybridGAN: hybrid generative adversarial networks for MR image synthesis","2020","Multimedia Tools and Applications","79","37-38","","27615","27631","16","10.1007/s11042-020-09387-3","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088948084&doi=10.1007%2fs11042-020-09387-3&partnerID=40&md5=948ee36d1676f09e7833c569d56e652a","In this paper, we propose HybridGAN – a new medical MR image synthesis methods via generative adversarial learning. Specifically, our synthesizer generates MRI data in a sequential manner: first in order to improve the robustness of image synthesis, an input full-size real MR image is divided into an array of sub-images. Then, to avoid overfitting limited MRI encodings, these sub-images and an unlimited amount of random latent noise vectors become the input of automatic encoder for learning the marginal image distributions of real images. Finally, pseudo patches with constrained noise vectors are put into RU-NET which is a component of our HybridGAN to generate a large number of synthetic MR images. In RU-NET, A SpliceLayer is then employed to fuse sub-images together in an interlaced manner into a full-size image. The experimental results show that HybridGAN can effectively synthesize a large variety of MR images and display a good visual quality. Compared to the state-of-the-art synthesis methods, our method achieves a significant improvement in terms of both visual and quantitative evaluation metrics. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Image coding; Image enhancement; Medical imaging; Adversarial learning; Adversarial networks; Image distributions; Quantitative evaluation; Sequential manners; State of the art; Synthesis method; Visual qualities; Magnetic resonance imaging","Deep learning; Generative adversarial networks; Generative models; MR image synthesis","Article","Final","","Scopus","2-s2.0-85088948084"
"Liu Y.; Deng G.; Zeng X.; Wu S.; Yu Z.; Wong H.-S.","Liu, Yi (57225002733); Deng, Guangchang (57214456773); Zeng, Xiangping (57219653990); Wu, Si (55495122900); Yu, Zhiwen (56399660300); Wong, Hau-San (7402864844)","57225002733; 57214456773; 57219653990; 55495122900; 56399660300; 7402864844","Regularizing discriminative capability of cGANs for semi-supervised generative learning","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156751","5719","5728","9","10.1109/CVPR42600.2020.00576","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094613031&doi=10.1109%2fCVPR42600.2020.00576&partnerID=40&md5=c0c9571ffaaa5c22d3f98dda12bec0fa","Semi-supervised generative learning aims to learn the underlying class-conditional distribution of partially labeled data. Generative Adversarial Networks (GANs) have led to promising progress in this task. However, it still needs to further explore the issue of imbalance between real labeled data and fake data in the adversarial learning process. To address this issue, we propose a regularization technique based on Random Regional Replacement (R3-regularization) to facilitate the generative learning process. Specifically, we construct two types of between-class instances: cross-category ones and real-fake ones. These instances could be closer to the decision boundaries and are important for regularizing the classification and discriminative networks in our class-conditional GANs, which we refer to as R3-CGAN. Better guidance from these two networks makes the generative network produce instances with class-specific information and high fidelity. We experiment with multiple standard benchmarks, and demonstrate that the R3-regularization can lead to significant improvement in both classification and class-conditional image synthesis. ©2020 IEEE.","Image enhancement; Labeled data; Learning systems; Pattern recognition; Adversarial learning; Adversarial networks; Conditional distribution; Decision boundary; Discriminative networks; Multiple standards; Regularization technique; Specific information; Semi-supervised learning","","Conference paper","Final","","Scopus","2-s2.0-85094613031"
"Tang Z.; Zhang D.; Song Y.; Wang H.; Liu D.; Zhang C.; Liu S.; Peng H.; Cai W.","Tang, Zihao (57206576585); Zhang, Donghao (57189297987); Song, Yang (55722226800); Wang, Heng (57196430737); Liu, Dongnan (57221104987); Zhang, Chaoyi (57205626812); Liu, Siqi (56479455100); Peng, Hanchuan (15061795400); Cai, Weidong (14022230800)","57206576585; 57189297987; 55722226800; 57196430737; 57221104987; 57205626812; 56479455100; 15061795400; 14022230800","3D Conditional Adversarial Learning for Synthesizing Microscopic Neuron Image Using Skeleton-to-Neuron Translation","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098345","1775","1779","4","10.1109/ISBI45749.2020.9098345","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085861422&doi=10.1109%2fISBI45749.2020.9098345&partnerID=40&md5=5ee3073687dce67bda9670aade05d953","The automatic reconstruction of single neuron cells from microscopic images is essential to establishing the research on neuron morphology. However, the performance of single neuron reconstruction algorithms is constrained by both the quantity and the quality of the annotated 3D microscopic images since annotating large-scale single neuron models is highly labour intensive. We propose a framework for synthesizing microscopy-realistic 3D neuron images from simulated single neuron skeletons using conditional Generative Adversarial Networks (cGAN). We build the generator network with multi-resolution sub-modules to improve the output fidelity. We evaluate our framework on Janelia-Fly dataset from the BigNeuron project. With both qualitative and quantitative analysis, we show that the proposed framework outperforms the other state-of-the-art methods regarding the quality of the synthetic neuron images. We also show that combining the real neuron images and the synthetic images generated from our framework can improve the performance of neuron segmentation. © 2020 IEEE.","Image reconstruction; Image segmentation; Medical imaging; Musculoskeletal system; Neurons; Quality control; Three dimensional computer graphics; Adversarial learning; Adversarial networks; Automatic reconstruction; Microscopic image; Neuron morphologies; Qualitative and quantitative analysis; Single-neuron models; State-of-the-art methods; Image enhancement","Data Augmentation; Image Synthesis; Neuron Image","Conference paper","Final","","Scopus","2-s2.0-85085861422"
"Fang Y.; Deng W.; Du J.; Hu J.","Fang, Yuke (57194854050); Deng, Weihong (8905974100); Du, Junping (55571529600); Hu, Jiani (55712130400)","57194854050; 8905974100; 55571529600; 55712130400","Identity-aware CycleGAN for face photo-sketch synthesis and recognition","2020","Pattern Recognition","102","","107249","","","","10.1016/j.patcog.2020.107249","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078969466&doi=10.1016%2fj.patcog.2020.107249&partnerID=40&md5=d68c9b0a40832c2ce5db378e4a23366c","Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-to-sketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy. © 2020 Elsevier Ltd","Face recognition; Iterative methods; Neural networks; Adversarial networks; Convolutional neural network; Digital entertainment; Identity recognition; Optimization procedures; Recognition models; Sketch recognition; State-of-the-art methods; Image enhancement","Convolutional neural network; Generative adversarial network; Identity-aware training; Photo-sketch recognition; Photo-sketch synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078969466"
"Cheema M.N.; Nazir A.; Yang P.; Sheng B.; Li P.; Li H.; Wei X.; Qin J.; Kim J.; Feng D.D.","Cheema, Muhammad Nadeem (57193728030); Nazir, Anam (57197130962); Yang, Po (57191521367); Sheng, Bin (7004699346); Li, Ping (55268425500); Li, Huating (24066677600); Wei, Xiaoer (43761775900); Qin, Jing (35339855100); Kim, Jinman (55720292700); Feng, David Dagan (57211310078)","57193728030; 57197130962; 57191521367; 7004699346; 55268425500; 24066677600; 43761775900; 35339855100; 55720292700; 57211310078","Modified GAN-cAED to Minimize Risk of Unintentional Liver Major Vessels Cutting by Controlled Segmentation Using CTA/SPET-CT","2021","IEEE Transactions on Industrial Informatics","","","","","","","10.1109/TII.2021.3064369","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102622086&doi=10.1109%2fTII.2021.3064369&partnerID=40&md5=e7bbdbac4fa1756485d3747a9633cdd0","This paper substantially advances upon state-of-the-art to enhance liver vessels segmentation accuracy by leveraging advantages of synthetic PET-CT (SPET-CT) images in addition to computed tomography angiography (CTA) volumes. Our setup makes a hybrid solution of modified GAN-cAED combining synthetic ability of generative adversarial network (GAN) to deliver SPET-CT images with generative ability of convolutional autoencoder (cAED) network in terms of latent learning to more refined segmentation of major liver vessels. We improve time complexity through a novel concept of controlled segmentation by introducing a threshold metric to stop segmentation up-to a desired level. The innovative concept of controlled vessel segmentation with a stopping criterion via variant threshold levels will help surgeons to avoid unintentional major blood vessels cutting, reducing the risk of excessive blood loss. Clinically, such solutions offer computer-aided liver surgeries and drug treatment evaluation in a CTA-only environment, shorten the requirement of radioactive and expensive fused PET-CT images. IEEE","Blood; Blood vessels; Controlled drug delivery; Image enhancement; Image segmentation; Adversarial networks; Computed tomography angiography; Latent learning; Segmentation accuracy; State of the art; Stopping criteria; Threshold levels; Vessel segmentation; Computerized tomography","Computed tomography; fused positron emission tomography-computed tomography (PET-CT); Generative adversarial networks; Image segmentation; image synthesis; Informatics; Liver; liver resection; Liver vessel segmentation; Measurement; Surgery; synthesized PET-CT (SPET-CT)","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85102622086"
"Marin I.; Gotovac S.; Russo M.","Marin, Ivana (57219838213); Gotovac, Sven (6506350581); Russo, Mladen (57192408931)","57219838213; 6506350581; 57192408931","Evaluation of Generative Adversarial Network for Human Face Image Synthesis","2020","2020 28th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2020","","","9238203","","","","10.23919/SoftCOM50211.2020.9238203","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096603651&doi=10.23919%2fSoftCOM50211.2020.9238203&partnerID=40&md5=14b1b6439ddebeadcdabc0ee712a98a9","Meaningful and objective evaluation metric for fair model comparison is crucial for further scientific progress in the field of deep generative modeling. Despite the significant progress and impressive results obtained by Generative Adversarial Networks in recent years, the problem of their objective evaluation remains open. In this paper, we give an overview of qualitative and quantitative evaluation measures most frequently used to assess the quality of generated images and learned representations of an adversarial network together with the empirical comparison of their performance on the problem of human face image synthesis. It is shown that evaluation scores of the two most widely accepted quantitative metrics, Inception Score (IS) and Fréchet Inception Distance (FID), do not correlate. The IS is not an appropriate evaluation metric for a given problem, but FID shows good performance that correlates well with a visual inspection of generated samples. The qualitative evaluation can be used to complement results obtained with quantitative evaluation - to gain further insight into the learned data representation and detect possible overfitting.  © 2020 University of Split, FESB.","Computer networks; Image processing; Petroleum reservoir evaluation; Adversarial networks; Data representations; Empirical - comparisons; Objective evaluation; Qualitative evaluations; Quantitative evaluation; Quantitative metrics; Scientific progress; Quality control","Evaluation; Fréchet Inception Distance; Generative Adversarial Networks; Inception Score; Latent Space Exploration","Conference paper","Final","","Scopus","2-s2.0-85096603651"
"Grassucci E.; Scardapane S.; Comminiello D.; Uncini A.","Grassucci, Eleonora (57218250258); Scardapane, Simone (55772102700); Comminiello, Danilo (36444807900); Uncini, Aurelio (7005621339)","57218250258; 55772102700; 36444807900; 7005621339","Flexible Generative Adversarial Networks with Non-parametric Activation Functions","2021","Smart Innovation, Systems and Technologies","184","","","67","77","10","10.1007/978-981-15-5093-5_7","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088423557&doi=10.1007%2f978-981-15-5093-5_7&partnerID=40&md5=80a834032339110d2ffb633021bc4252","Generative adversarial networks (GANs) have become widespread models for complex density estimation tasks such as image generation or image-to-image synthesis. At the same time, training of GANs can suffer from several problems, either of stability or convergence, sometimes hindering their effective deployment. In this paper we investigate whether we can improve GAN training by endowing the neural network models with more flexible activation functions compared to the commonly used rectified linear unit (or its variants). In particular, we evaluate training a deep convolutional GAN wherein all hidden activation functions are replaced with a version of the kernel activation function (KAF), a recently proposed technique for learning non-parametric nonlinearities during the optimization process. On a thorough empirical evaluation on multiple image generation benchmarks, we show that the resulting architectures learn to generate visually pleasing images in a fraction of the number of the epochs, eventually converging to a better solution, even when we equalize (or even lower) the number of free parameters. Overall, this points to the importance of investigating better and more flexible architectures in the context of GANs. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Chemical activation; Network architecture; Activation functions; Adversarial networks; Density estimation; Empirical evaluations; Flexible activation functions; Flexible architectures; Image generations; Neural network model; Function evaluation","Activation function; Generative adversarial network; Image; Neural network","Book chapter","Final","","Scopus","2-s2.0-85088423557"
"Zhang Z.; Yu W.; Zhou J.; Zhang X.; Tang J.; Li S.; Jiang N.; He G.; He G.; Yang Z.","Zhang, Zhiqiang (57206280843); Yu, Wenxin (36610960300); Zhou, Jinjia (35099640400); Zhang, Xuewen (57217279695); Tang, Jialiang (57212377052); Li, Siyuan (57214131869); Jiang, Ning (57212426361); He, Gang (56937631400); He, Gang (36630339700); Yang, Zhuo (57203791621)","57206280843; 36610960300; 35099640400; 57217279695; 57212377052; 57214131869; 57212426361; 56937631400; 36630339700; 57203791621","Customizable GAN: Customizable Image Synthesis Based on Adversarial Learning","2020","Communications in Computer and Information Science","1332","","","336","344","8","10.1007/978-3-030-63820-7_38","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097280762&doi=10.1007%2f978-3-030-63820-7_38&partnerID=40&md5=20da3f041b569c942031c7342c647ce6","In this paper, we propose a highly flexible and controllable image synthesis method based on the simple contour and text description. The contour determines the object’s basic shape, and the text describes the specific content of the object. The method is verified in the Caltech-UCSD Birds (CUB) and Oxford-102 flower datasets. The experimental results demonstrate its effectiveness and superiority. Simultaneously, our method can synthesize the high-quality image synthesis results based on artificial hand-drawing contour and text description, which demonstrates the high flexibility and customizability of our method further. © 2020, Springer Nature Switzerland AG.","Computer science; Computers; Adversarial learning; Artificial hand; Basic shapes; Customizability; Customizable; High flexibility; High quality images; Image synthesis; Image processing","Computer vision; Customizable synthesis; Deep learning; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85097280762"
"Hamghalam M.; Wang T.; Lei B.","Hamghalam, Mohammad (35145822300); Wang, Tianfu (55602702200); Lei, Baiying (26422280400)","35145822300; 55602702200; 26422280400","High tissue contrast image synthesis via multistage attention-GAN: Application to segmenting brain MR scans","2020","Neural Networks","132","","","43","52","9","10.1016/j.neunet.2020.08.014","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089800479&doi=10.1016%2fj.neunet.2020.08.014&partnerID=40&md5=d4417242ddcee270caa669f0a158ea02","Magnetic resonance imaging (MRI) presents a detailed image of the internal organs via a magnetic field. Given MRI's non-invasive advantage in repeated imaging, the low-contrast MR images in the target area make segmentation of tissue a challenging problem. This study shows the potential advantages of synthetic high tissue contrast (HTC) images through image-to-image translation techniques. Mainly, we use a novel cycle generative adversarial network (Cycle-GAN), which provides a mechanism of attention to increase the contrast within the tissue. The attention block and training on HTC images are beneficial to our model to enhance tissue visibility. We use a multistage architecture to concentrate on a single tissue as a preliminary and filter out the irrelevant context in every stage in order to increase the resolution of HTC images. The multistage architecture reduces the gap between source and target domains and alleviates synthetic images’ artefacts. We apply our HTC image synthesising method to two public datasets. In order to validate the effectiveness of these images we use HTC MR images in both end-to-end and two-stage segmentation structures. The experiments on three segmentation baselines on BraTS’18 demonstrate that joining the synthetic HTC images in the multimodal segmentation framework develops the average Dice similarity scores (DSCs) of 0.8%, 0.6%, and 0.5% respectively on the whole tumour (WT), tumour core (TC), and enhancing tumour (ET) while removing one real MRI channels from the segmentation pipeline. Moreover, segmentation of infant brain tissue in T1w MR slices through our framework improves DSCs approximately 1% in cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) compared to state-of-the-art segmentation techniques. The source code of synthesising HTC images is publicly available. © 2020 Elsevier Ltd","Attention; Brain; Brain Neoplasms; Humans; Image Enhancement; Infant; Magnetic Resonance Imaging; Neural Networks, Computer; Brain; Cerebrospinal fluid; Image enhancement; Magnetic resonance imaging; Network architecture; Tumors; Adversarial networks; Cerebro spinal fluids; Image translation; Multi-modal segmentation; Segmentation techniques; Similarity scores; State of the art; Two-stage segmentations; algorithm; Article; artifact; brain damage; brain radiography; brain slice; cerebrospinal fluid analysis; clinical article; controlled study; female; gray matter; human; image segmentation; magnetic field; male; nuclear magnetic resonance imaging; priority journal; scoring system; white matter; attention; brain; brain tumor; diagnostic imaging; image enhancement; infant; nuclear magnetic resonance imaging; procedures; Image segmentation","Attention mechanism; Cycle-GAN; Glioma tumour; Infant brain tissue; Segmentation; Synthetic MRI image","Article","Final","","Scopus","2-s2.0-85089800479"
"Huang W.; Luo M.; Liu X.; Zhang P.; Ding H.","Huang, Wei (56195325600); Luo, Mingyuan (57206482771); Liu, Xi (57209845642); Zhang, Peng (55547108553); Ding, Huijun (25932070800)","56195325600; 57206482771; 57209845642; 55547108553; 25932070800","Arterial spin labeling image synthesis from structural MRI using improved capsule-based networks","2020","IEEE Access","8","","","181137","181153","16","10.1109/ACCESS.2020.3028113","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102832282&doi=10.1109%2fACCESS.2020.3028113&partnerID=40&md5=8454ec42fc9326e5b35411b5e29bc24d","Medical image synthesis receives much popularity in recent years, and ample medical images can be synthesized by diverse deep learning models to alleviate the problem of lack of data in many medical imaging utilizations. However, most medical image synthesis methods still incorporate the well-known pooling operation in their convolutional neural networks-based / generative adversarial networks-based models, from which image details will be inevitably lost due to the pooling operation. In order to tackle the above problem, improved capsule-based networks, in which no pooling operation is executed and spatial details of images can be effectively preserved thanks to the equivariance characteristics of capsule models, are proposed in this paper to synthesize arterial spin labeling images, for the first time. Technically, three important issues in constructing improved capsule-based networks, including the depth of basic convolutions, the layer of capsules, and the capacity of capsules, are thoroughly investigated. Comprehensive experiments made up of region-based / voxel-based partial volume corrections and dementia diseases diagnosis based on two different datasets are conducted. The superiority of improved capsule-based networks introduced in this paper is substantiated from the statistical point of view. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Magnetic resonance imaging; Medical imaging; Adversarial networks; Arterial spin labeling; Equivariance; Image details; Image synthesis; Learning models; Partial volume correction; Region-based; Image enhancement","Arterial spin labeling; Capsule; Computer aided diagnosis; Deep learning; Image analysis; Image generation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102832282"
"Phusomsai W.; Limpiyakorn Y.","Phusomsai, Warintorn (57192590415); Limpiyakorn, Yachai (56032668700)","57192590415; 56032668700","Toward U-Net-based GANs for Diverse Facial Image Synthesis from Sketch","2020","ACM International Conference Proceeding Series","","","","59","63","4","10.1145/3424311.3424323","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096320017&doi=10.1145%2f3424311.3424323&partnerID=40&md5=13c08156971197c26ad9b7377c74b4f0","Face physical changes may result from aging, surgery, or disguise. The criminal suspects conceal their identity with false appearances such as wearing a wig, glasses, beard and mustache. This research benefits the generation of various fictitious appearances of the suspects or facial changes of lost persons. The technique of GANs is applied for synthesizing a color image from a sketch. The output image can be varied in five facial attributes with a toggle: bald, makeup, straight hair, wearing glasses, beard and mustache. The approach enhances the Generator of StarGan2 with the U-Net architecture. The experiments were carried out to evaluate the performance of the proposed model compared to that of StarGan2. FID scores are used for measuring the quality of the generated images. The FID scores measured on the test data reported about 40% less than that of the baseline model and the synthesized images with varied facial attributes look natural and realistic. © 2020 ACM.","Computation theory; Glass; Wear of materials; Baseline models; Color images; Facial changes; Facial Image synthesis; NET architecture; Physical changes; Synthesized images; Test data; Image processing","facial attribute editing; GANs; Generative Adversarial Networks; image synthesis; sketch-to-image translation; U-Net","Conference paper","Final","","Scopus","2-s2.0-85096320017"
"Zhao Y.; Wu R.; Dong H.","Zhao, Yihao (57210290574); Wu, Ruihai (57219500058); Dong, Hao (56547882800)","57210290574; 57219500058; 56547882800","Unpaired Image-to-Image Translation Using Adversarial Consistency Loss","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12354 LNCS","","","800","815","15","10.1007/978-3-030-58545-7_46","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097090394&doi=10.1007%2f978-3-030-58545-7_46&partnerID=40&md5=f028e8546e6e698111786619fe531c55","Unpaired image-to-image translation is a class of vision problems whose goal is to find the mapping between different image domains using unpaired training data. Cycle-consistency loss is a widely used constraint for such problems. However, due to the strict pixel-level constraint, it cannot perform shape changes, remove large objects, or ignore irrelevant texture. In this paper, we propose a novel adversarial-consistency loss for image-to-image translation. This loss does not require the translated image to be translated back to be a specific source image but can encourage the translated images to retain important features of the source images and overcome the drawbacks of cycle-consistency loss noted above. Our method achieves state-of-the-art results on three challenging tasks: glasses removal, male-to-female translation, and selfie-to-anime translation. © 2020, Springer Nature Switzerland AG.","Textures; Glasses removal; Image domain; Image translation; Important features; Source images; State of the art; Training data; Vision problems; Computer vision","Dual learning; Generative adversarial networks; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097090394"
"Donnez M.; Carton F.-X.; Le Lann F.; De Schlichting E.; Chabanas M.","Donnez, Mélanie (57223316016); Carton, François-Xavier (57209850123); Le Lann, Florian (57194586984); De Schlichting, Emmanuel (56335311800); Chabanas, Matthieu (57222519660)","57223316016; 57209850123; 57194586984; 56335311800; 57222519660","Realistic synthesis of brain tumor resection ultrasound images with a generative adversarial network","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11598","","115982F","","","","10.1117/12.2581911","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105568007&doi=10.1117%2f12.2581911&partnerID=40&md5=1a3e582ae9b0912c39780da79d845790","The simulation of realistic ultrasound (US) images has many applications in image-guided surgery such as image registration, data augmentation, or education. We simulated intraoperative US images of the brain after tumor resection surgery. A Generative Adversarial Networks first generated an US image with resection from a resection cavity map. This generated cavity texture was then blended into a real pre-resection patient-specific US image. A validation study showed that two neurosurgeons correctly labelled only 56% and 53% of the simulated images, which indicate that these synthesized images are hardly distinguishable from real post-resection US images.  © 2021 SPIE.","Robotics; Surgery; Textures; Tumors; Ultrasonics; Adversarial networks; Data augmentation; Image guided surgery; Patient specific; Simulated images; Synthesized images; Ultrasound images; Validation study; Medical imaging","Generative Adversarial Networks; Image synthesis; Intraoperative Ultrasound; Neurosurgery","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105568007"
"Kaur S.; Aggarwal H.; Rani R.","Kaur, Sukhpal (57213734504); Aggarwal, Himanshu (7003719927); Rani, Rinkle (55232407400)","57213734504; 7003719927; 55232407400","MR Image Synthesis Using Generative Adversarial Networks for Parkinson’s Disease Classification","2021","Advances in Intelligent Systems and Computing","1164","","","317","327","10","10.1007/978-981-15-4992-2_30","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088591404&doi=10.1007%2f978-981-15-4992-2_30&partnerID=40&md5=346112f6ff93b5f9c15f99c03119db07","Parkinson’s disease (PD) is one of the most common types of neurological diseases triggered by progressive degeneration of dopaminergic neurons in the brain. Although this neurodegenerative disease does not have a known cure, early detection accompanied by earlier care may help patients to have a better quality of life. Throughout recent years, Magnetic Resonance Imaging (MRI) has been one of the most common diagnostic tools because it removes harmful radiation. Deep learning approaches have shown high applicability for diverse databases of clinical objects. But, there is huge sum of data that is required to train deep networks limiting the application of some activities like medical image classification thereby lacking sufficient training samples for each class. In this paper, a solution focused on the Generative Adversarial Network (GAN) is proposed for the augmentation of training data to improve the quality of the diagnosis of Parkinson’s disease. A Generative Adversarial Network is designed to generate MR images of high quality. A mixing training methodology is then implemented through a mixture of GAN-based data augmentation strategies to further improve the performance of the Deep CNN classifier. The MR images of healthy and PD patients obtained from Parkinson Progression Markers Initiative (PPMI) are used to test the proposed approach. A total of 185 samples are collected and 95 sample images were actually applied for data augmentation. This model’s improved data set is as many as 600 samples, and it is found that the images produced are of good quality using this data set for the detection of maximum signal-to-noise ratio (PSNR), which is of revolutionary value in the real test norm. On synthetic and real MR images of healthy and PD patients, the Deep CNN Model reaches an AUC of 88.4%. The proposed approach shows promising in PD diagnosis. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Diagnosis; Image enhancement; Magnetic resonance imaging; Medical imaging; Neurodegenerative diseases; Neurons; Signal to noise ratio; Statistical tests; Adversarial networks; Data augmentation; Diagnostic tools; Disease classification; Dopaminergic neurons; Learning approach; Neurological disease; Progressive degeneration; Image classification","Convolutional neural networks; Generative adversarial networks; Parkinson’s disease","Conference paper","Final","","Scopus","2-s2.0-85088591404"
"Hertzmann A.","Hertzmann, Aaron (6601954186)","6601954186","Visual indeterminacy in GAN art","2020","ACM SIGGRAPH 2020 Art Gallery, SIGGRAPH 2020","","","","424","428","4","10.1145/3386567.3388574","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093364640&doi=10.1145%2f3386567.3388574&partnerID=40&md5=6ecc0f3de31b056e1baeb50f05c80da1","This paper explores visual indeterminacy as a description for artwork created with Generative Adversarial Networks (GANs). Visual indeterminacy describes images that appear to depict real scenes, but on closer examination, defy coherent spatial interpretation. GAN models seem to be predisposed to producing indeterminate images, and indeterminacy is a key feature of much modern representational art, as well as most GAN art. The author hypothesizes that indeterminacy is a consequence of a powerful-but-imperfect image synthesis model that must combine general classes of objects, scenes and textures.  © 2020 Owner/Author.","Interactive computer graphics; Textures; Adversarial networks; General class; Image synthesis; Key feature; Spatial interpretation; Arts computing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093364640"
"Liu W.; You J.; Lee J.","Liu, Wei (57208497891); You, Jie (57204458966); Lee, Joonwhoan (55870620900)","57208497891; 57204458966; 55870620900","HSIGAN: A conditional hyperspectral image synthesis method with auxiliary classifier","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9369834","3330","3344","14","10.1109/JSTARS.2021.3063911","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102297678&doi=10.1109%2fJSTARS.2021.3063911&partnerID=40&md5=83e206e816789c7e82b0cd6da2f4cc75","In this article, we explore a conditional hyperspectral image (HSI) synthesis method with generative adversarial networks (GAN). A new multistage and multipole generative adversarial network, which is suitable for conditional HSI generation and classification (HSIGAN), is proposed. For HSIs synthesis, it is crucial to learn a great deal of spatial-spectral distribution features from source data. The multistage progressive training makes the generator effectively imitate the real data by fully exploiting the high-dimension learning capability of GAN models. The coarse-to-fine information extraction method helps the discriminator to understand the semantic feature better while the multiscale classification prediction presents a positive impact on results. A spectral classifier joins the adversarial network, which offers a helping hand to stabilize and optimize the model. Moreover, we apply the 3-D DropBlock layer in the generator to remove semantic information in a contiguous spatial-spectral region and avoid model collapse. Experimental results of the quantitative and qualitative evaluation show that HSIGAN could generate high-fidelity, diverse hyperspectral cubes while achieving top-ranking accuracy for supervised classification. This result is encouraging for using GANs as a data augmentation strategy in the HSI vision task.  © 2008-2012 IEEE.","Image classification; Semantics; Spectroscopy; Adversarial networks; Classification prediction; Information extraction methods; Learning capabilities; Qualitative evaluations; Semantic information; Spatial spectral distribution; Supervised classification; image classification; machine learning; multispectral image; spatial distribution; Classification (of information)","Classification; generative adversarial network (GAN); hyperspectral image (HSI); synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102297678"
"Zhan H.; Yi C.; Shi B.; Lin J.; Duan L.-Y.; Kot A.C.","Zhan, Huijing (57187728800); Yi, Chenyu (57215051064); Shi, Boxin (25423015500); Lin, Jie (55617604100); Duan, Ling-Yu (7201932863); Kot, Alex C. (35588578100)","57187728800; 57215051064; 25423015500; 55617604100; 7201932863; 35588578100","Pose-Normalized and Appearance-Preserved Street-to-Shop Clothing Image Generation and Feature Learning","2021","IEEE Transactions on Multimedia","23","","9025187","133","144","11","10.1109/TMM.2020.2978669","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098168690&doi=10.1109%2fTMM.2020.2978669&partnerID=40&md5=4baf5186d80e067034046afb9e76133f","We tackle the task of street-to-shop clothing image synthesis. Given a daily person image with a particular clothing item captured in the street scenario, we aim to synthesize the frontal facing view of that item in the shop scenario. This problem has the following challenges: 1) the distinct visual discrepancy between the street and shop scenario; 2) the severe shape deformation of clothing in the presence of an arbitrary human pose; 3) the preservation of fine-grained details during the process of clothing image generation. In this paper, we jointly solve these difficulties by proposing a Pose-Normalized and Appearance-Preserved Generative Adversarial Network (PNAP-GAN). More specifically, conditioned on the clothing-agnostic representation (i.e., clothing landmarks and semantic parsing map), we disentangle the shape and appearance synthesis in a coarse-to-fine framework. Moreover, a semantic embedding loss is introduced to guide the domain transfer in the semantic level (i.e., keeping the clothing attributes). With the synthesized frontal shop image, a pose-normalized representation in complementary to the domain-invariant feature learnt from the original street image are integrated to facilitate the problem of street-to-shop clothing retrieval. Extensive experiments conducted demonstrate the effectiveness of the proposed PNAP-GAN on generating high quality frontal-view images and the excellence of the learnt pose-normalized features on the retrieval task than existing methods. In addition, we demonstrate that the pose-normalized retrieval feature benefits the cross-scenario (i.e., street-to-shop) clothing image generation in a semantic-preserved manner. © 1999-2012 IEEE.","Machine learning; Semantics; Adversarial networks; Appearance synthesis; Clothing retrievals; Domain transfers; Image generations; Invariant features; Semantic embedding; Shape deformation; Image processing","feature learning; image generation; Pose-normalization; street-to-shop","Article","Final","","Scopus","2-s2.0-85098168690"
"Wang M.; Lang C.; Liang L.; Lyu G.; Feng S.; Wang T.","Wang, Min (57221235394); Lang, Congyan (7402002472); Liang, Liqian (57218477192); Lyu, Gengyu (57208479355); Feng, Songhe (7402531247); Wang, Tao (56135273700)","57221235394; 7402002472; 57218477192; 57208479355; 7402531247; 56135273700","Class-Balanced Text to Image Synthesis With Attentive Generative Adversarial Network","2021","IEEE Multimedia","28","3","","21","31","10","10.1109/MMUL.2020.3048939","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099410887&doi=10.1109%2fMMUL.2020.3048939&partnerID=40&md5=bb0973e82d869ed6a3c5b7524f610283","Although the text-to-image synthesis task has shown significant progress, it still remains a challenge in generating high-quality images. In this article, we first propose an attention-driven, cycle-refinement generative adversarial network, AGAN-v1, to bridge the domain gap between visual contents and semantic concepts by constructing spatial configurations of objects. The generation of image contours is the core component, in which an attention mechanism is developed to refine local details of images by focusing on the objects that complement one subregion. Second, an advanced class-balanced generative adversarial network, AGAN-v2, is proposed to address the problem of long-tailed data distribution. Importantly, it is the first method to solve this problem in the text-to-image synthesis task. Our AGAN-v2 introduces a reweighting scheme, which adopts the effective number of samples for each class to rebalance the generative loss. Extensive quantitative and qualitative experiments on CUB and MS-COCO datasets demonstrate that the proposed AGAN-v2 significantly outperforms the state-of-the-art methods.  © 1994-2012 IEEE.","Computer vision; Gallium nitride; III-V semiconductors; Job analysis; Semantics; Attention-driven; Data distribution; Head; High quality images; Images synthesis; Long-tailed data distribution; Rebalance; Task analysis; Text-to-image synthesis; Generative adversarial networks","attention-driven; generative adversarial network; long-tailed data distribution; rebalance; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85099410887"
"Zeng W.; Zhao M.; Gao Y.; Zhang Z.","Zeng, Wei (57217308882); Zhao, Mingbo (55143406500); Gao, Yuan (56335673800); Zhang, Zhao (56822575600)","57217308882; 55143406500; 56335673800; 56822575600","TileGAN: category-oriented attention-based high-quality tiled clothes generation from dressed person","2020","Neural Computing and Applications","32","23","","17587","17600","13","10.1007/s00521-020-04928-1","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084428242&doi=10.1007%2fs00521-020-04928-1&partnerID=40&md5=941f66a5082d6a0b716c23e2ead37c5e","During the past decades, applying deep learning technologies on fashion industry are increasingly the mainstream. Due to the different gesture, illumination or self-occasion, it is hard to directly utilize the clothes images in real-world applications. In this paper, to handle this problem, we present a novel multi-stage, category-supervised attention-based conditional generative adversarial network by generating clear and detailed tiled clothing images from certain model images. This newly proposed method consists of two stages: in the first stage, we generate the coarse image which contains general appearance information (such as color and shape) and category of the garment, where a spatial transformation module is utilized to handle the shape changes during image synthesis and an additional classifier is employed to guide coarse image generated in a category-supervised manner; in the second stage, we propose a dual path attention-based model to generate the fine-tuned image, which combines the appearance information of the coarse result with the high-frequency information of the model image. In detail, we introduce the channel attention mechanism to assign weights to the information of different channels instead of connecting directly. Then, a self-attention module is employed to model long-range correlation making the generated image close to the target. In additional to the framework, we also create a person-to-clothing data set containing 10 categories of clothing, which includes more than 34 thousand pairs of images with category attribute. Extensive simulations are conducted, and experimental result on the data set demonstrates the feasibility and superiority of the proposed networks. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Classification (of information); Deep learning; Adversarial networks; Attention mechanisms; Extensive simulations; Fashion industry; High-frequency informations; Learning technology; Long range correlations; Spatial transformation; Clothes","Attention; Clothes generation; Generative adversarial network (GAN); Image-to-image translation","Article","Final","","Scopus","2-s2.0-85084428242"
"Li J.; Chen Z.; Zhao X.; Shao L.","Li, Jingtao (57217013418); Chen, Zhanlong (26322275400); Zhao, Xiaozhen (57217013673); Shao, Lijia (57217014471)","57217013418; 26322275400; 57217013673; 57217014471","MAPGAN: An intelligent generation model for network tile maps","2020","Sensors (Switzerland)","20","11","3119","","","","10.3390/s20113119","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729281&doi=10.3390%2fs20113119&partnerID=40&md5=11c206c75755ee4a1045444f289c8a7b","In recent years, the generative adversarial network (GAN)-based image translation model has achieved great success in image synthesis, image inpainting, image super-resolution, and other tasks. However, the images generated by these models often have problems such as insufficient details and low quality. Especially for the task of map generation, the generated electronic map cannot achieve effects comparable to industrial production in terms of accuracy and aesthetics. This paper proposes a model called Map Generative Adversarial Networks (MapGAN) for generating multitype electronic maps accurately and quickly based on both remote sensing images and render matrices. MapGAN improves the generator architecture of Pix2pixHD and adds a classifier to enhance the model, enabling it to learn the characteristics and style differences of different types of maps. Using the datasets of Google Maps, Baidu maps, and Map World maps, we compare MapGAN with some recent image translation models in the fields of one-to-one map generation and one-to-many domain map generation. The results show that the quality of the electronic maps generated by MapGAN is optimal in terms of both intuitive vision and classic evaluation indicators. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Maps; Remote sensing; Adversarial networks; Evaluation indicators; Image Inpainting; Image super resolutions; Image synthesis; Image translation; Industrial production; Remote sensing images; article; classifier; human; human experiment; remote sensing; vision; Image processing","Deep generation model; Image translation; Map generation; Network tile map","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085729281"
"Cho J.; Shimoda W.; Yanai K.","Cho, Jaehyeong (57205639683); Shimoda, Wataru (56912184900); Yanai, Keiji (7103290726)","57205639683; 56912184900; 7103290726","Mask-based style-controlled image synthesis using a mask style encoder","2020","Proceedings - International Conference on Pattern Recognition","","","9412647","5176","5183","7","10.1109/ICPR48806.2021.9412647","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110496616&doi=10.1109%2fICPR48806.2021.9412647&partnerID=40&md5=0ba5090552791c48145df7a2c05e8f68","In recent years, the advances in Generative Adversarial Networks (GANs) have shown impressive results for image generation and translation tasks. In particular, the image-to-image translation is a method of learning mapping from a source domain to a target domain and synthesizing an image. Image-to-image translation can be applied to a variety of tasks, making it possible to quickly and easily synthesize realistic images from semantic segmentation masks. However, in the existing image-to-image translation method, there is a limitation on controlling the style of the translated image, and it is not easy to synthesize an image by controlling the style of each mask element in detail. Therefore, we propose an image synthesis method that controls the style of each element by improving the existing image-to-image translation method. In the proposed method, we implement a mask style encoder that extracts style features for each mask element. The extracted style features are concatenated to the semantic mask in the normalization layer, and used the style-controlled image synthesis of each mask element. In the experiments, we performed style-controlled images synthesis using the datasets consisting of semantic segmentation masks and real images. The results show that the proposed method has excellent performance for style-controlled images synthesis for each element. © 2020 IEEE","Image segmentation; Pattern recognition; Semantics; Signal encoding; Adversarial networks; Image generations; Image synthesis; Image translation; Images synthesis; Method of learning; Realistic images; Semantic segmentation; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85110496616"
"Tang H.; Bai S.; Sebe N.","Tang, Hao (57208238003); Bai, Song (57206839797); Sebe, Nicu (57204924633)","57208238003; 57206839797; 57204924633","Dual Attention GANs for Semantic Image Synthesis","2020","MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia","","","","1994","2002","8","10.1145/3394171.3416270","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099901063&doi=10.1145%2f3394171.3416270&partnerID=40&md5=0b74189217f82764545d2d527c0b9fdf","In this paper, we focus on the semantic image synthesis task that aims at transferring semantic label maps to photo-realistic images. Existing methods lack effective semantic constraints to preserve the semantic information and ignore the structural correlations in both spatial and channel dimensions, leading to unsatisfactory blurry and artifact-prone results. To address these limitations, we propose a novel Dual Attention GAN (DAGAN) to synthesize photo-realistic and semantically-consistent images with fine details from the input layouts without imposing extra training overhead or modifying the network architectures of existing methods. We also propose two novel modules, i.e., position-wise Spatial Attention Module (SAM) and scale-wise Channel Attention Module (CAM), to capture semantic structure attention in spatial and channel dimensions, respectively. Specifically, SAM selectively correlates the pixels at each position by a spatial attention map, leading to pixels with the same semantic label being related to each other regardless of their spatial distances. Meanwhile, CAM selectively emphasizes the scale-wise features at each channel by a channel attention map, which integrates associated features among all channel maps regardless of their scales. We finally sum the outputs of SAM and CAM to further improve feature representation. Extensive experiments on four challenging datasets show that DAGAN achieves remarkably better results than state-of-the-art methods, while using fewer model parameters.  © 2020 ACM.","Network architecture; Pixels; Associated feature; Feature representation; Photorealistic images; Semantic constraints; Semantic information; Semantic structures; State-of-the-art methods; Structural correlation; Semantics","channel attention; generative adversarial networks (gans); semantic image synthesis; spatial attention","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85099901063"
"Fu Y.; Lei Y.; Zhou J.; Wang T.; Yu D.S.; Beitler J.J.; Curran W.J.; Liu T.; Yang X.","Fu, Yabo (57193681581); Lei, Yang (57202715941); Zhou, Jun (57216100590); Wang, Tonghe (57189639465); Yu, David S. (57199474700); Beitler, Jonathan J. (35398487100); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Yang, Xiaofeng (36712893800)","57193681581; 57202715941; 57216100590; 57189639465; 57199474700; 35398487100; 57203070877; 26643332700; 36712893800","Synthetic CT-aided MRI-CT image registration for head and neck radiotherapy","2020","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11317","","1131728","","","","10.1117/12.2549092","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118859409&doi=10.1117%2f12.2549092&partnerID=40&md5=6da0d2b4815ad827e7aa09ba2c601bc0","In this study, we propose a synthetic CT (sCT) aided MRI-CT deformable image registration for head and neck radiotherapy. An image synthesis network, cycle consistent generative adversarial network (CycleGAN), was first trained using 25 pre-aligned CT-MRI image pairs. Using the MR head and neck images, the trained CycleGAN then predicts sCT images, which were used as MRI's surrogate in MRI-CT registration. Demons registration algorithm was used to perform the sCT-CT registration on 5 separate datasets. For comparison, the original MRI and CT images were registered using mutual information as similarity metric. Our results showed that the target registration errors after registration were on average 1.31 mm and 1.02 mm for MRI-CT and sCT-CT registration, respectively. The mean normalized cross correlation between the sCT and CT after registration was 0.97, indicating that the proposed method is a viable way to perform MRI-CT image registration for head neck patients. © 2020 SPIE","Computerized tomography; Deep learning; Generative adversarial networks; Image registration; Medical imaging; Radiotherapy; CT Image; Deep learning; Deformable image registration; Head; Head and neck; Images registration; Images synthesis; MRI Image; Neck; Synthetic CT; Magnetic resonance imaging","Deep learning; Head; Image registration; Neck; Synthetic CT","Conference paper","Final","","Scopus","2-s2.0-85118859409"
"Kwak J.G.; Ko H.","Kwak, Jeong Gi (57216148639); Ko, Hanseok (35069749800)","57216148639; 35069749800","Unsupervised generation and synthesis of facial images via an auto-encoder-based deep generative adversarial network","2020","Applied Sciences (Switzerland)","10","6","1995","","","","10.3390/app10061995","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082657445&doi=10.3390%2fapp10061995&partnerID=40&md5=0226b7f00ace857c3053da76498ad050","The processing of facial images is an important task, because it is required for a large number of real-world applications. As deep-learning models evolve, they require a huge number of images for training. In reality, however, the number of images available is limited. Generative adversarial networks (GANs) have thus been utilized for database augmentation, but they suffer from unstable training, low visual quality, and a lack of diversity. In this paper, we propose an auto-encoder-based GAN with an enhanced network structure and training scheme for Database (DB) augmentation and image synthesis. Our generator and decoder are divided into two separate modules that each take input vectors for low-level and high-level features; these input vectors affect all layers within the generator and decoder. The effectiveness of the proposed method is demonstrated by comparing it with baseline methods. In addition, we introduce a new scheme that can combine two existing images without the need for extra networks based on the auto-encoder structure of the discriminator in our model. We add a novel double-constraint loss to make the encoded latent vectors equal to the input vectors. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","","Database augmentation; Facial image; GAN (Generative adversarial networks); Generation; Generative models; Synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082657445"
"Kozubek M.","Kozubek, Michal (56177678200)","56177678200","When Deep Learning Meets Cell Image Synthesis","2020","Cytometry Part A","97","3","","222","225","3","10.1002/cyto.a.23957","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077844556&doi=10.1002%2fcyto.a.23957&partnerID=40&md5=75f5ea3722ba4ee8a26584b5c1940994","[No abstract available]","Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; image processing","cell image synthesis; deep learning; generative adversarial networks; style transfer","Note","Final","","Scopus","2-s2.0-85077844556"
"Yang J.; Lee J.; Kim Y.; Cho H.-Y.; Kim I.","Yang, Jinhyeok (57210264358); Lee, Junmo (57219786983); Kim, Youngik (57219109481); Cho, Hoon-Young (36175399600); Kim, Injung (55477734000)","57210264358; 57219786983; 57219109481; 36175399600; 55477734000","VocGAN: A high-fidelity real-time vocoder with a hierarchically-nested adversarial network","2020","Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH","2020-October","","","200","204","4","10.21437/Interspeech.2020-1238","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098199397&doi=10.21437%2fInterspeech.2020-1238&partnerID=40&md5=ba63eb41a3cc3e1ad1bf7b8079afcc68","We present a novel high-fidelity real-time neural vocoder called VocGAN. A recently developed GAN-based vocoder, MelGAN, produces speech waveforms in real-time. However, it often produces a waveform that is insufficient in quality or inconsistent with acoustic characteristics of the input mel spectrogram. VocGAN is nearly as fast as MelGAN, but it significantly improves the quality and consistency of the output waveform. VocGAN applies a multi-scale waveform generator and a hierarchically-nested discriminator to learn multiple levels of acoustic properties in a balanced way. It also applies the joint conditional and unconditional objective, which has shown successful results in high-resolution image synthesis. In experiments, VocGAN synthesizes speech waveforms 416.7x faster on a GTX 1080Ti GPU and 3.24x faster on a CPU than real-time. Compared with MelGAN, it also exhibits significantly improved quality in multiple evaluation metrics including mean opinion score (MOS) with minimal additional overhead. Additionally, compared with Parallel WaveGAN, another recently developed high-fidelity vocoder, VocGAN is 6.98x faster on a CPU and exhibits higher MOS. Copyright © 2020 ISCA","Acoustic properties; Speech communication; Titanium compounds; Acoustic characteristic; Adversarial networks; Evaluation metrics; High resolution image; Mean opinion scores; Output waveform; Speech waveforms; Waveform generators; Vocoders","Generative adversarial network; Hierarchically-nested adversarial loss; Neural vocoder; Speech synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098199397"
"Park M.","Park, Minje (57219586023)","57219586023","JGAN: A joint formulation of GaN for synthesizing images and labels","2020","IEEE Access","8","","","188883","188888","5","10.1109/ACCESS.2020.3031292","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099730094&doi=10.1109%2fACCESS.2020.3031292&partnerID=40&md5=701a90387162d2d9315d65f59bfeca9c","Image generation with explicit condition or label generally works better than unconditional methods. In modern GAN frameworks, both generator and discriminator are formulated to model the conditional distribution of images given with labels. In this article, we provide an alternative formulation of GAN which models the joint distribution of images and labels. There are two advantages in this joint formulation over conditional approaches. The first advantage is that the joint formulation is more robust to label noises if it’s properly modeled. This alleviates the burden of making noise-free labels and allows the use of weakly-supervised labels in image generation. The second is that we can use any kinds of weak labels or image features that have correlations with the original image data to enhance unconditional image generation. We will show the effectiveness of our joint formulation on CIFAR10, CIFAR100, and STL dataset with the state-of-the-art GAN architecture. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Gallium nitride; III-V semiconductors; Conditional distribution; Image features; Image generations; Joint distributions; Original images; State of the art; Weak labels; Image enhancement","Deep learning; Generative adversarial network; Image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099730094"
"Van Truong H.","Van Truong, Hoang (57201703359)","57201703359","Motorbike Generator Using Bidirectional Generative Adversarial Networks","2020","ACM International Conference Proceeding Series","","","","40","44","4","10.1145/3418994.3418995","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093100297&doi=10.1145%2f3418994.3418995&partnerID=40&md5=2258d058b82d7f6ae1a14fd252c65e86","This paper proposes a Bidirectional Generative Adversarial Network (BiGAN) to generate fake images for Motorbike Generator Challenge. Experiment results, Motorbike Generator task demonstrate that this method not only stabilizes GAN training but also achieves significant improvements over several state-of-the-art GAN methods.  © 2020 ACM.","Information systems; Information use; Adversarial networks; State of the art; Data communication systems","BiGAN; Deep learning; Fréchet Inception distance; Generative adversarial networks; Image synthesis; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85093100297"
"Shen Y.; Gu J.; Tang X.; Zhou B.","Shen, Yujun (57207766466); Gu, Jinjin (57212042330); Tang, Xiaoou (7404101198); Zhou, Bolei (36697366200)","57207766466; 57212042330; 7404101198; 36697366200","Interpreting the latent space of GANs for semantic face editing","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9157070","9240","9249","9","10.1109/CVPR42600.2020.00926","310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094851362&doi=10.1109%2fCVPR42600.2020.00926&partnerID=40&md5=82c0b11803a5bbbbcaee59f69658fa96","Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.1 © 2020 IEEE","Codes (symbols); Linear transformations; Mathematical transformations; Pattern recognition; Vector spaces; Adversarial networks; Distributed representation; Generative model; High-fidelity images; Inversion methods; Photorealistic images; Random distribution; Subspace projection; Semantics","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094851362"
"Dai X.; Lei Y.; Fu Y.; Curran W.J.; Liu T.; Mao H.; Yang X.","Dai, Xianjin (56883634100); Lei, Yang (57202715941); Fu, Yabo (57193681581); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Mao, Hui (7201795917); Yang, Xiaofeng (36712893800)","56883634100; 57202715941; 57193681581; 57203070877; 26643332700; 7201795917; 36712893800","Multimodal MRI synthesis using unified generative adversarial networks","2020","Medical Physics","47","12","","6343","6354","11","10.1002/mp.14539","29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093941310&doi=10.1002%2fmp.14539&partnerID=40&md5=038b1b4b01512bd20ea05dd9a44ae937","Purpose: Complementary information obtained from multiple contrasts of tissue facilitates physicians assessing, diagnosing and planning treatment of a variety of diseases. However, acquiring multiple contrasts magnetic resonance images (MRI) for every patient using multiple pulse sequences is time-consuming and expensive, where, medical image synthesis has been demonstrated as an effective alternative. The purpose of this study is to develop a unified framework for multimodal MR image synthesis. Methods: A unified generative adversarial network consisting of only a single generator and a single discriminator was developed to learn the mappings among images of four different modalities. The generator took an image and its modality label as inputs and learned to synthesize the image in the target modality, while the discriminator was trained to distinguish between real and synthesized images and classify them to their corresponding modalities. The network was trained and tested using multimodal brain MRI consisting of four different contrasts which are T1-weighted (T1), T1-weighted and contrast-enhanced (T1c), T2-weighted (T2), and fluid-attenuated inversion recovery (Flair). Quantitative assessments of our proposed method were made through computing normalized mean absolute error (NMAE), peak signal-to-noise ratio (PSNR), structural similarity index measurement (SSIM), visual information fidelity (VIF), and naturalness image quality evaluator (NIQE). Results: The proposed model was trained and tested on a cohort of 274 glioma patients with well-aligned multi-types of MRI scans. After the model was trained, tests were conducted by using each of T1, T1c, T2, Flair as a single input modality to generate its respective rest modalities. Our proposed method shows high accuracy and robustness for image synthesis with arbitrary MRI modality that is available in the database as input. For example, with T1 as input modality, the NMAEs for the generated T1c, T2, Flair respectively are 0.034 ± 0.005, 0.041 ± 0.006, and 0.041 ± 0.006, the PSNRs respectively are 32.353 ± 2.525 dB, 30.016 ± 2.577 dB, and 29.091 ± 2.795 dB, the SSIMs are 0.974 ± 0.059, 0.969 ± 0.059, and 0.959 ± 0.059, the VIF are 0.750 ± 0.087, 0.706 ± 0.097, and 0.654 ± 0.062, and NIQE are 1.396 ± 0.401, 1.511 ± 0.460, and 1.259 ± 0.358, respectively. Conclusions: We proposed a novel multimodal MR image synthesis method based on a unified generative adversarial network. The network takes an image and its modality label as inputs and synthesizes multimodal images in a single forward pass. The results demonstrate that the proposed method is able to accurately synthesize multimodal MR images from a single MR image. © 2020 American Association of Physicists in Medicine","Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Signal-To-Noise Ratio; Diagnosis; Image quality; Magnetic resonance; Medical imaging; Signal to noise ratio; Adversarial networks; Fluid attenuated inversion recoveries; Magnetic resonance images (MRI); Mean absolute error; Peak signal to noise ratio; Quantitative assessments; Structural similarity indices; Visual information fidelity; Article; cancer patient; cohort analysis; contrast enhancement; data base; glioma; human; image quality; major clinical study; multimodal imaging; naturalness image quality evaluator; nerve cell network; neuroimaging; normalized mean absolute error; nuclear magnetic resonance imaging; peak signal to noise ratio; radiological parameters; signal noise ratio; structural similarity index measurement; unified generative adversarial network; visual information; visual information fidelity; brain; diagnostic imaging; image processing; Magnetic resonance imaging","deep learning; generative adversarial networks; magnetic resonance imaging; medical image synthesis; multimodal imaging","Article","Final","","Scopus","2-s2.0-85093941310"
"Zhang X.; Li S.","Zhang, Xianhong (57193845908); Li, Shusen (8294816300)","57193845908; 8294816300","Flow image generation algorithms for improving gan","2021","Journal of Flow Visualization and Image Processing","28","1","","45","59","14","10.1615/JFlowVisImageProc.2020034486","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101958884&doi=10.1615%2fJFlowVisImageProc.2020034486&partnerID=40&md5=3dc50fe6853952271db0ef77504a850c","Generative adversarial network (GAN) is widely utilized for image synthesis, but the original GAN has many drawbacks in image generation, such as unstable training, poor image quality, and insufficient diversity. This paper attempts to solve these problems by establishing a simple and effective GAN to enhance the quality of output images. This GAN is optimized using a scaled exponential linear unit (SELU) activation function in the generator, which prevents the gradient from disappearing. In addition, the loss function is adjusted to further increase the diversity of im-ages. Furthermore, mini-batch gradient descent (MBGD) is incorporated into Adam's algorithm to enhance the robustness of the network training process. The experimental results from the MNIST data set and the flow images show that the improved GAN is effective. © 2021 by Begell House, Inc.","Gradient methods; Activation functions; Adversarial networks; Gradient descent; Image generations; Image synthesis; Linear units; Loss functions; Network training; Image enhancement","Activation function; GAN; image generation; Optimizer","Article","Final","","Scopus","2-s2.0-85101958884"
"Lee M.; Tae D.; Choi J.H.; Jung H.-Y.; Seok J.","Lee, Minhyeok (57194701375); Tae, Donghyun (57194702774); Choi, Jae Hun (56126608800); Jung, Ho-Youl (57224844484); Seok, Junhee (24069490100)","57194701375; 57194702774; 56126608800; 57224844484; 24069490100","Improved recurrent generative adversarial networks with regularization techniques and a controllable framework","2020","Information Sciences","538","","","428","443","15","10.1016/j.ins.2020.05.116","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086887200&doi=10.1016%2fj.ins.2020.05.116&partnerID=40&md5=0cb4b7c0c526ad31dc45066fea1f1cd3","Generative Adversarial Network (GAN), a deep learning framework to generate synthetic but realistic samples, has produced astonishing results for image synthesis. However, because GAN is routinely used for image datasets, regularization methods for GAN have been developed for convolutional layers. In this study, to expand these methods for time-series data, which are one of the most common data types in various real datasets, modified regularization methods are proposed for Long Short-Term Memory (LSTM)-based GANs. Specifically, the spectral normalization, hinge loss, orthogonal regularization, and the truncation trick are modified and assessed for LSTM-based GANs. Furthermore, a conditional GAN architecture called Controllable GAN (ControlGAN) is applied to LSTM-based GANs to produce the desired samples. The evaluations are conducted with sine wave data, air pollution datasets, and a medical time-series dataset obtained from intensive care units. As a result, ControlGAN with the spectral normalization on gates and cell states consistently outperforms the others, including the conventional model, called Recurrent Conditional GAN (RCGAN). © 2020 Elsevier Inc.","Deep learning; Intensive care units; Time series; Adversarial networks; Conventional modeling; Learning frameworks; Medical time series; Regularization methods; Regularization technique; Spectral normalization; Time-series data; Long short-term memory","Generative adversarial network; Long short-term memory; Recurrent neural network; Sample generation; Spectral normalization","Article","Final","","Scopus","2-s2.0-85086887200"
"Li M.; Lin J.; DIng Y.; Liu Z.; Zhu J.-Y.; Han S.","Li, Muyang (57219670683); Lin, Ji (57200618213); DIng, Yaoyao (57209224323); Liu, Zhijian (57213431660); Zhu, Jun-Yan (56316642900); Han, Song (56697675800)","57219670683; 57200618213; 57209224323; 57213431660; 56316642900; 56697675800","GAN Compression: Efficient Architectures for Interactive Conditional GANs","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9157494","5283","5293","10","10.1109/CVPR42600.2020.00533","69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094327925&doi=10.1109%2fCVPR42600.2020.00533&partnerID=40&md5=862fdd264560c82121b7eab0aa26a6f5","Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many computer vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more computationally-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing CNNs compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize the GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model, and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method automatically finds efficient architectures via neural architecture search (NAS). To accelerate the search process, we decouple the model training and architecture search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings (paired and unpaired), model architectures, and learning methods (e.g., pix2pix, GauGAN, CycleGAN). Without losing image quality, we reduce the computation of CycleGAN by more than 20x and GauGAN by 9x, paving the way for interactive image synthesis. The code and demo are publicly available. © 2020 IEEE.","Computer vision; Network architecture; Adversarial networks; Compression methods; Efficient architecture; Graphics applications; Intermediate representations; Model architecture; Neural architectures; Orders of magnitude; Learning systems","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094327925"
"Daroach G.B.; Yoder J.A.; Iczkowski K.A.; LaViolette P.S.","Daroach, Gagandeep B. (57222719770); Yoder, Josiah A. (55561941300); Iczkowski, Kenneth A. (7005139638); LaViolette, Peter S. (25951451800)","57222719770; 55561941300; 7005139638; 25951451800","High-resolution controllable prostatic histology synthesis using StyleGAN","2021","BIOIMAGING 2021 - 8th International Conference on Bioimaging; Part of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2021","","","","103","114","11","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103843838&partnerID=40&md5=0ad61249a85cd15ee4e8ea92ff8941df","For use of deep learning algorithms in clinical practice, detailed justification for diagnosis is necessary. Convolutional Neural Networks (CNNs) have been demonstrated to classify prostatic histology using the same diagnostic signals as pathologists. Using the StyleGAN series of networks, we demonstrate that recent advances in high-resolution image synthesis with Generative Adversarial Networks (GANs) can be applied to prostatic histology. The trained network can produce novel histology samples indistinguishable from real histology at 1024x1024 resolution and can learn disentangled representations of histologic semantics that separates at a variety of scales. Through blending of the latent representations, users have the ability to control the projection of histologic semantics onto a reconstructed image. When applied to the medical domain without modification, StyleGAN2 is able to achieve a Fréchet Inception Distance (FID) of 3.69 and perceptual path length (PPL) of 33.25. © 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Biomedical engineering; Convolutional neural networks; Deep learning; Diagnosis; Image processing; Learning algorithms; Semantics; Adversarial networks; Clinical practices; High resolution; High resolution image; Medical domains; Path length; Reconstructed image; Histology","Deep learning; GAN; Generative adversarial networks; Gleason; Histology; Latent space; Machine learning; Medical imaging; Prostate; Prostate cancer; StyleGAN; StyleGAN2","Conference paper","Final","","Scopus","2-s2.0-85103843838"
"Hamghalam M.; Wang T.; Qin J.; Lei B.","Hamghalam, Mohammad (35145822300); Wang, Tianfu (55602702200); Qin, Jing (35339855100); Lei, Baiying (26422280400)","35145822300; 55602702200; 35339855100; 26422280400","Transforming Intensity Distribution of Brain Lesions Via Conditional Gans for Segmentation","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098347","1499","1502","3","10.1109/ISBI45749.2020.9098347","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085858350&doi=10.1109%2fISBI45749.2020.9098347&partnerID=40&md5=bd1d54ec8bb38a483fcef1ad8189024e","Brain lesion segmentation is crucial for diagnosis, surgical planning, and analysis. Owing to the fact that pixel values of brain lesions in magnetic resonance (MR) scans are distributed over the wide intensity range, there is always a considerable overlap between the class-conditional densities of lesions. Hence, an accurate automatic brain lesion segmentation is still a challenging task. We present a novel architecture based on conditional generative adversarial networks (cGANs) to improve the lesion contrast for segmentation. To this end, we propose a novel generator adaptively calibrating the input pixel values, and a Markovian discriminator to estimate the distribution of tumors. We further propose the Enhancement and Segmentation GAN (Enh-Seg-GAN) which effectively incorporates the classifier loss into the adversarial one during training to predict the central labels of the sliding input patches. Particularly, the generated synthetic MR images are a substitute for the real ones to maximize lesion contrast while suppressing the background. The potential of proposed frameworks is confirmed by quantitative evaluation compared to the state-of-the-art methods on BraTS'13 dataset. © 2020 IEEE.","Diagnosis; Magnetic resonance imaging; Medical imaging; Pixels; Adversarial networks; Conditional density; Intensity distribution; Intensity range; Novel architecture; Quantitative evaluation; State-of-the-art methods; Surgical planning; Magnetic resonance","Brain lesion segmentation; conditional GANs; distribution transformation; image synthesis; MRI","Conference paper","Final","","Scopus","2-s2.0-85085858350"
"Singh H.; Saini S.S.; Lakshminarayanan V.","Singh, Hardit (57217680569); Saini, Simarjit S. (57222556016); Lakshminarayanan, Vasudevan (7004723358)","57217680569; 57222556016; 7004723358","Real or fake? Fourier analysis of generative adversarial network fundus images","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11601","","116010H","","","","10.1117/12.2581078","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103180505&doi=10.1117%2f12.2581078&partnerID=40&md5=40d657b7bb42088c7c611f1fc6823c53","With the increasing use of deep learning methodologies in various biomedical applications, there is a need for a large number of labeled medical image datasets for training and validation purposes. However, the accumulation of labeled datasets is expensive and time consuming. Recently, generative adversarial networks (GAN) have been utilized to generate synthetic datasets. Currently, the accuracy of generative adversarial networks is calculated using a structural similarity index measure (SSIM). SSIM is not adequate for comparison of images as it underestimates the distortions near hard edges. In this paper, we compare the real DRIVE dataset to the synthetic FunSyn-Net using Fourier transform techniques and show that Fourier behavior is quite different in the two datasets, especially at high frequencies. It is observed that for real images, the amplitude of the Fourier components exponentially decreased with increasing frequency. For the synthesized images, the rate of decrease of the amplitude is much slower. If a linear function is fit to the high frequency components, the slope distributions for the two datasets are completely different with no offset. The average slope in the log scale for DRIVE dataset and FunSyn-Net were 0.0195, and 0.009 respectively. We also looked at auto correlations for the horizontal cut of the Fourier transform and again saw a statistically significant difference between the means for the two datasets. Finally, we also observed that Fourier transforms with real images have higher magnitude squared coherence as compared to the synthesized images. Fourier transform has shown great success for finding differences between real and synthesized images and can be used to improve the synthesized GAN models. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Fourier analysis; Health care; Image enhancement; Large dataset; Medical applications; Medical imaging; Medical informatics; Adversarial networks; Biomedical applications; High frequency components; Magnitude squared coherences; Slope distribution; Statistically significant difference; Structural similarity index measures (SSIM); Synthetic datasets; Fourier transforms","Fourier transform; fundus images; FunSyn-Net; Generative adversarial networks; image synthesis; magnitude squared coherence; ophthalmology; retina","Conference paper","Final","","Scopus","2-s2.0-85103180505"
"Li R.; Wang N.; Feng F.; Zhang G.; Wang X.","Li, Ruifan (13608752200); Wang, Ning (57701553900); Feng, Fangxiang (56432140300); Zhang, Guangwei (13604766600); Wang, Xiaojie (35235644100)","13608752200; 57701553900; 56432140300; 13604766600; 35235644100","Exploring Global and Local Linguistic Representations for Text-to-Image Synthesis","2020","IEEE Transactions on Multimedia","22","12","8989803","3075","3087","12","10.1109/TMM.2020.2972856","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083769219&doi=10.1109%2fTMM.2020.2972856&partnerID=40&md5=069cb8a07d316b02c40d9795d1b73f3b","The task of text-to-image synthesis is to generate photographic images conditioned on given textual descriptions. This challenging task has recently attracted considerable attention from the multimedia community due to its potential applications. Most of the up-to-date approaches are built based on generative adversarial network (GAN) models, and they synthesize images conditioned on the global linguistic representation. However, the sparsity of the global representation results in training difficulties on GANs and a shortage of fine-grained information in the generated images. To address this problem, we propose cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN) by incorporating the local linguistic representation into the GAN. In our CGL-GAN, we construct a generator to synthesize the target images and a discriminator to judge whether the generated images conform with the text description. In the discriminator, we construct the cross-modal correlation by projecting the image representations at high and low levels onto the global and local linguistic representations, respectively. We design the hinge loss function to train our CGL-GAN model. We evaluate the proposed CGL-GAN on two publicly available datasets, the CUB and the MS-COCO. The extensive experiments demonstrate that incorporating fine-grained local linguistic information with cross-modal correlation can greatly improve the performance of text-to-image synthesis, even when generating high-resolution images. © 1999-2012 IEEE.","Linguistics; Photography; Adversarial networks; Cross-modal correlation; Global representation; High resolution image; Image representations; Linguistic information; Linguistic representations; Multimedia community; Image enhancement","cross-modal; generative adversarial network (GAN); linguistic representation; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85083769219"
"Marin I.; Gotovac S.; Russo M.; Božić-Štulić D.","Marin, Ivana (57219838213); Gotovac, Sven (6506350581); Russo, Mladen (57192408931); Božić-Štulić, Dunja (57203895010)","57219838213; 6506350581; 57192408931; 57203895010","The effect of latent space dimension on the quality of synthesized human face images","2021","Journal of Communications Software and Systems","17","2","","124","133","9","10.24138/jcomss-2021-0035","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107475380&doi=10.24138%2fjcomss-2021-0035&partnerID=40&md5=66bfe5eba0594edd31ff8c60f3b60c39","In recent years Generative Adversarial Networks (GANs) have achieved remarkable results in the task of realistic image synthesis. Despite their continued success and advances, there still lacks a thorough understanding of how precisely GANs map random latent vectors to realistic-looking images and how the priors set on the latent space affect the learned mapping. In this work, we analyze the effect of the chosen latent dimension on the final quality of synthesized images of human faces and learned data representations. We show that GANs can generate images plausibly even with latent dimensions significantly smaller than the standard dimensions like 100 or 512. Although one might expect that larger latent dimensions encourage the generation of more diverse and enhanced quality images, we show that an increase of latent dimension after some point does not lead to visible improvements in perceptual image quality nor in quantitative estimates of its generalization abilities. © 2021 University of Split. All rights reserved.","","Evaluation; Fréchet Inception Distance (FID); Generative Adversarial Networks; Image synthesis; Latent dimension; Latent space exploration","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85107475380"
"Castro D.L.; Valenti C.; Tegolo D.","Castro, Dario Lo (57202448477); Valenti, Cesare (7005530157); Tegolo, Domenico (6506475631)","57202448477; 7005530157; 6506475631","Retinal image synthesis through the least action principle","2020","ICIIBMS 2020 - 5th International Conference on Intelligent Informatics and Biomedical Sciences","","","9336421","111","116","5","10.1109/ICIIBMS50712.2020.9336421","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101462510&doi=10.1109%2fICIIBMS50712.2020.9336421&partnerID=40&md5=168d1d60c83914db9f997deb700753c1","Eye fundus image analysis is a fundamental approach in medical diagnosis and follow-up ophthalmic diagnostics. Manual annotation by experts needs hard work, thus only a small set of annotated vessel structures is available. Examples such as DRIVE and STARE include small sets for training images of fundus image benchmarks. Moreover, there is no vessel structure annotation for a number of fundus image datasets. Synthetic images have been generated by using appropriate parameters for the modeling of vascular networks or by methods developing deep learning techniques and supported by performance hardware. Our methodology aims to produce high-resolution synthetic fundus images alternative to the increasing use of generative adversarial networks, to overcome the problems that arise in producing slightly modified versions of the same real images, to simulate pathologies and for the prediction of eye-related diseases. Our approach is based on the principle of the least action to place vessels on the simulated eye fundus.  © 2020 IEEE.","Deep learning; Diagnosis; Learning systems; Adversarial networks; High resolution; Learning techniques; Least action principle; Manual annotation; Synthetic images; Vascular network; Vessel structure; Medical imaging","data augmentation; fundus image analysis; predictive evaluation diseases; statistical features; synthetic retinal image","Conference paper","Final","","Scopus","2-s2.0-85101462510"
"Zheng Q.; Xiaoguang R.; Liu Y.; Qin W.","Zheng, Qibin (57208157528); Xiaoguang, Ren (57492370600); Liu, Yi (57208194180); Qin, Wei (57211256762)","57208157528; 57492370600; 57208194180; 57211256762","Generalization or Instantiation?: Estimating the Relative Abstractness between Images and Text","2020","ACM International Conference Proceeding Series","","","","275","282","7","10.1145/3404555.3404610","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092233139&doi=10.1145%2f3404555.3404610&partnerID=40&md5=051003fdc21abc2922ea7a8802b07562","Learning from multi-modal data is very often in current data mining and knowledge management applications. However, the information imbalance between modalities brings challenges for many multi-modal learning tasks, such as cross-modal retrieval, image captioning, and image synthesis. Understanding the cross-modal information gap is an important foundation for designing models and choosing the evaluating criteria of those applications. Especially for text and image data, existing researches have proposed the abstractness to measure the information imbalance. They evaluate the abstractness disparity by training a classifier using the manually annotated multi-modal sample pairs. However, these methods ignore the impact of the intra-modal relationship on the inter-modal abstractness; besides, the annotating process is very labor-intensive, and the quality cannot be guaranteed. In order to evaluate the text-image relationship more comprehensively and reduce the cost of evaluating, we propose the relative abstractness index (RAI) to measure the abstractness between multi-modal items, which measures the abstractness of a sample according to its certainty of differentiating the items of another modality. Besides, we proposed a cycled generating model to compute the RAI values between images and text. In contrast to existing works, the proposed index can better describe the image-text information disparity, and its computing process needs no annotated training samples.  © 2020 ACM.","Artificial intelligence; Data mining; Knowledge management; Modal analysis; Computing process; Evaluating criteria; Generating models; Image captioning; Knowledge management applications; Labor intensive; Multi-modal data; Multi-modal learning; Image processing","generative adversarial networks; image-text relationship; Multi-modality; relative abstractness","Conference paper","Final","","Scopus","2-s2.0-85092233139"
"Liang J.; Yang X.; Li H.; Wang Y.; Van M.T.; Dou H.; Chen C.; Fang J.; Liang X.; Mai Z.; Zhu G.; Chen Z.; Ni D.","Liang, Jiamin (57217028259); Yang, Xin (56967210500); Li, Haoming (57216642196); Wang, Yi (56163256100); Van, Manh The (57216287823); Dou, Haoran (57193993443); Chen, Chaoyu (57217029324); Fang, Jinghui (57216287322); Liang, Xiaowen (57203762872); Mai, Zixin (57216287851); Zhu, Guowen (57217028515); Chen, Zhiyi (35261712100); Ni, Dong (26023577500)","57217028259; 56967210500; 57216642196; 56163256100; 57216287823; 57193993443; 57217029324; 57216287322; 57203762872; 57216287851; 57217028515; 35261712100; 26023577500","Synthesis and Edition of Ultrasound Images via Sketch Guided Progressive Growing GANS","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098384","1793","1797","4","10.1109/ISBI45749.2020.9098384","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085865561&doi=10.1109%2fISBI45749.2020.9098384&partnerID=40&md5=cc5dae31ccf99b61ee31724a514b2092","Ultrasound (US) is widely accepted in clinic for anatomical structure inspection. However, lacking in resources to practice US scan, novices often struggle to learn the operation skills. Also, in the deep learning era, automated US image analysis is limited by the lack of annotated samples. Efficiently synthesizing realistic, editable and high resolution US images can solve the problems. The task is challenging and previous methods can only partially complete it. In this paper, we devise a new framework for US image synthesis. Particularly, we firstly adopt a sketch generative adversarial networks (Sgan) to introduce background sketch upon object mask in a conditioned generative adversarial network. With enriched sketch cues, Sgan can generate realistic US images with editable and fine-grained structure details. Although effective, Sgan is hard to generate high resolution US images. To achieve this, we further implant the Sgan into a progressive growing scheme (PGSgan). By smoothly growing both generator and discriminator, PGSgan can gradually synthesize US images from low to high resolution. By synthesizing ovary and follicle US images, our extensive perceptual evaluation, user study and segmentation results prove the promising efficacy and efficiency of the proposed PGSgan. © 2020 IEEE.","Deep learning; Medical imaging; Ultrasonic applications; Adversarial networks; Anatomical structures; Fine-grained structure; Image synthesis; Operation skills; Perceptual evaluation; Segmentation results; Ultrasound images; Image segmentation","Conditional GAN; High resolution; Image synthesis; Progressive growing; Ultrasound","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085865561"
"Ge Z.; Chen F.; Du S.; Yu Y.; Zhou Y.","Ge, Zhipeng (57216947022); Chen, Fei (57207166664); Du, Sidan (7202757877); Yu, Yao (57001042100); Zhou, Yu (57071818500)","57216947022; 57207166664; 7202757877; 57001042100; 57071818500","Learn a global appearance semi-supervisedly for synthesizing person images","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093443","1179","1188","9","10.1109/WACV45572.2020.9093443","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085515207&doi=10.1109%2fWACV45572.2020.9093443&partnerID=40&md5=7c2d300f441f3968f2e1c891dbaff995","We present a novel approach for person images synthesis in this paper, that can generate person images in arbitrary poses, shapes and views. Unlike existing methods just using keypoints' locations in heatmaps format, we propose to render SMPL model to UV maps, which can provide human structural information about poses and shapes. Thus, by varying the parameters of poses, shapes and camera in SMPL model, we can generate different person images with various attributions in a simple way, while in most cases we can only obtain new shapes of people by computer graphics methods. We train an end to end generative adversarial network with unlabeled data. As our SMPL parameters come from a pretrained model, we call our overall network semi- supervised. Our network keeps a global appearance during the fine-tuning stage of the target person, thus we can get a complete appearance of the target person, rather than the inaccurate appearance caused by inferencing without enough information. Experiments on Human3.6M Dataset and a self-collected dataset demonstrate the excellent effectiveness of our approach on person images synthesis for different applications. © 2020 IEEE.","Computer graphics; Adversarial networks; Fine tuning; Global appearances; Images synthesis; Overall networks; Semi-supervised; Structural information; Unlabeled data; Computer vision","","Conference paper","Final","","Scopus","2-s2.0-85085515207"
"Surya S.; Setlur A.; Biswas A.; Negi S.","Surya, Shiv (57191905989); Setlur, Amrith (57211439187); Biswas, Arijit (57213950137); Negi, Sumit (24825212000)","57191905989; 57211439187; 57213950137; 24825212000","ReStGAN: A step towards visually guided shopper experience via text-to-image synthesis","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093459","1189","1197","8","10.1109/WACV45572.2020.9093459","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085500866&doi=10.1109%2fWACV45572.2020.9093459&partnerID=40&md5=92c8c5b43f8993ca792b0059c4d637f4","E-commerce companies like Amazon, Alibaba and Flip-kart have an extensive catalogue comprising of billions of products. Matching customer search queries to plausible products is challenging due to the size and diversity of the catalogue. These challenges are compounded in apparel due to the semantic complexity and a large variation of fashion styles, product attributes and colours. Providing aids that can help the customer visualise the styles and colours matching their ""search queries"" will provide customers with necessary intuition about what can be done next. This helps the customer buy a product with the styles, embellishments and colours of their liking. In this work, we propose a Generative Adversarial Network (GAN) for generating images from text streams like customer search queries. Our GAN learns to incrementally generate possible images complementing the fine-grained style, colour of the apparel in the query. We incorporate a novel colour modelling approach enabling the GAN to render a wide spectrum of colours accurately. We compile a dataset from an e-commerce website to train our model. The proposed approach outperforms the baselines on qualitative and quantitative evaluations. © 2020 IEEE.","Clothes; Computer vision; Electronic commerce; Sales; Semantics; Adversarial networks; E-commerce websites; Fashion styles; Image synthesis; Product attributes; Quantitative evaluation; Search queries; Wide spectrum; Color","","Conference paper","Final","","Scopus","2-s2.0-85085500866"
"Walhazi H.; Maalej A.; Amara N.E.B.","Walhazi, Hajer (57220025086); Maalej, Ahmed (56613073900); Amara, Najoua Essoukri Ben (55667518900)","57220025086; 56613073900; 55667518900","Mask2LFP: Mask-constrained Adversarial Latent Fingerprint Synthesis","2020","Proceedings - 2020 International Conference on Cyberworlds, CW 2020","","","9240519","265","271","6","10.1109/CW49994.2020.00049","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099532792&doi=10.1109%2fCW49994.2020.00049&partnerID=40&md5=1014f1d40e8b1e14e9e6c5f7fcff7a8c","Latent fingerprints are one of the most valuable and unique biometric attributes that are extensively used in forensic and law enforcement applications. Compared to rolled/plain fingerprint, latent fingerprint is of poor quality in term of friction ridge patterns, hence a more challenging for automatic fingerprint recognition systems. Considering the difficulties of dusting, lifting, and recovery of latent fingerprint, this type of fingerprints remain expensive to develop and collect. In this paper, we present a novel approach for synthetic latent fingerprint generation using Generative Adversarial Network (GAN). Our proposed framework, named mask to latent fingerprint (Mask2LFP), uses binary mask of distorted fingerprint-like shapes as input, and outputs a realistic latent fingerprint. This work focuses on the generation of synthetic latent fingerprints. The aim is to alleviate the scarcity issue of latent fingerprint data and serve the increasing need for developing, evaluating, and enhancing fingerprint-based identification systems, especially in forensic applications. © 2020 IEEE.","Forensic science; Pattern recognition systems; Adversarial networks; Automatic fingerprint recognition system; Binary masks; Forensic applications; Latent fingerprint; Ridge patterns; Palmprint recognition","Generative Adversarial Networks; Image synthesis; latent fingerprints; Mask embedding.","Conference paper","Final","","Scopus","2-s2.0-85099532792"
"Ho T.-T.; Virtusio J.J.; Chen Y.-Y.; Hsu C.-M.; Hua K.-L.","Ho, Trang-Thi (57201583419); Virtusio, John Jethro (57208645681); Chen, Yung-Yao (36941161600); Hsu, Chih-Ming (36838891900); Hua, Kai-Lung (55223901500)","57201583419; 57208645681; 36941161600; 36838891900; 55223901500","Sketch-guided Deep Portrait Generation","2020","ACM Transactions on Multimedia Computing, Communications and Applications","16","3","3396237","","","","10.1145/3396237","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091994892&doi=10.1145%2f3396237&partnerID=40&md5=90a50815244b8ee11f0d9571316b9d63","Generating a realistic human class image from a sketch is a unique and challenging problem considering that the human body has a complex structure that must be preserved. Additionally, input sketches often lack important details that are crucial in the generation process, hence making the problem more complicated. In this article, we present an effective method for synthesizing realistic images from human sketches. Our framework incorporates human poses corresponding to locations of key semantic components (e.g., arm, eyes, nose), seeing that its a strong prior for generating human class images. Our sketch-image synthesis framework consists of three stages: Semantic keypoint extraction, coarse image generation, and image refinement. First, we extract the semantic keypoints using Part Affinity Fields (PAFs) and a convolutional autoencoder. Then, we integrate the sketch with semantic keypoints to generate a coarse image of a human. Finally, in the image refinement stage, the coarse image is enhanced by a Generative Adversarial Network (GAN) that adopts an architecture carefully designed to avoid checkerboard artifacts and to generate photo-realistic results. We evaluate our method on 6,300 sketch-image pairs and show that our proposed method generates realistic images and compares favorably against state-of-the-art image synthesis methods.  © 2020 ACM.","Semantics; Adversarial networks; Complex structure; Generation process; Image generations; Image synthesis; Realistic images; Semantic components; State of the art; Image enhancement","convolutional autoencoder; generative adversarial networks; Image synthesis; perceptual loss; semantic keypoints","Article","Final","","Scopus","2-s2.0-85091994892"
"Xin B.; Hu Y.; Zheng Y.; Liao H.","Xin, Bingyu (57217029061); Hu, Yifan (57212001376); Zheng, Yefeng (8062522600); Liao, Hongen (7201507586)","57217029061; 57212001376; 8062522600; 7201507586","Multi-Modality Generative Adversarial Networks with Tumor Consistency Loss for Brain MR Image Synthesis","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098449","1803","1807","4","10.1109/ISBI45749.2020.9098449","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085862728&doi=10.1109%2fISBI45749.2020.9098449&partnerID=40&md5=dc8c825e63139539186c6361fd4f806e","Magnetic Resonance (MR) images of different modalities can provide complementary information for clinical diagnosis, but whole modalities are often costly to access. Most existing methods only focus on synthesizing missing images between two modalities, which limits their robustness and efficiency when multiple modalities are missing. To address this problem, we propose a multi-modality generative adversarial network (MGAN) to synthesize three high-quality MR modalities (FLAIR, T1 and T1ce) from one MR modality T2 simultaneously. The experimental results show that the quality of the synthesized images by our proposed methods is better than the one synthesized by the baseline model, pix2pix. Besides, for MR brain image synthesis, it is important to preserve the critical tumor information in the generated modalities, so we further introduce a multi-modality tumor consistency loss to MGAN, called TC-MGAN. We use the synthesized modalities by TC-MGAN to boost the tumor segmentation accuracy, and the results demonstrate its effectiveness. © 2020 IEEE.","Brain mapping; Diagnosis; Magnetic resonance imaging; Tumors; Adversarial networks; Baseline models; Brain MR images; Clinical diagnosis; Multi modality; Multiple modalities; Synthesized images; Tumor segmentation; Magnetic resonance","Brain tumor segmentation; Generative Adversarial Networks; Image synthesis; Multi-modality","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085862728"
"Du W.-L.; Zhou Y.; Zhao J.; Tian X.","Du, Wen-Liang (55265123100); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Tian, Xiaolin (7202380154)","55265123100; 35480110700; 57138970300; 7202380154","K-Means Clustering Guided Generative Adversarial Networks for SAR-Optical Image Matching","2020","IEEE Access","8","","9279214","217554","217572","18","10.1109/ACCESS.2020.3042213","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097759183&doi=10.1109%2fACCESS.2020.3042213&partnerID=40&md5=5003cba703defca1b0ca7cafa9124b5e","Synthetic Aperture Radar and optical (SAR-optical) image matching is a technique of finding correspondences between SAR and optical images. SAR-optical image matching can be simplified to single-mode image matching through image synthesis. However, the existing SAR-optical image synthesis methods are unable to provide qualified images for SAR-optical image matching. In this work, we present a K-means Clustering Guide Generative Adversarial Networks (KCG-GAN) to improve the image quality of synthesizing by constraining spatial information synthesis. KCG-GAN uses k-means segmentations as one of the image generator's inputs and introduces feature matching loss, segmentation loss, and L1 loss to the objective function. Meanwhile, to provide repeatable k-means segmentations, we develop a straightforward 1D k-means algorithm. We compare KCG-GAN with a leading image synthesis method-pix2pixHD. Qualitative results illustrate that KCG-GAN preserves more spatial structures than pix2pixHD. Quantitative results show that, compared with pix2pixHD, images synthesized by KCG-GAN are more similar to original optical images, and SAR-optical image matching based on KCG-GAN obtains at most 3.15 times more qualified matchings. Robustness tests demonstrate that SAR-optical image matching based on KCG-GAN is robust to rotation and scale changing. We also test three SIFT-like algorithms on matching original SAR-optical image pairs and matching KCG-GAN synthesized optical-optical image pairs. Experimental results show that our KCG-GAN significantly improves the performances of the three algorithms on SAR-optical image matching. © 2013 IEEE.","Geometrical optics; Image enhancement; Image matching; Image segmentation; K-means clustering; Synthetic aperture radar; Adversarial networks; Feature matching; Image generators; K-means segmentations; Objective functions; Quantitative result; Spatial informations; Spatial structure; Radar imaging","generative adversarial networks (GANs); Image matching; image synthesis; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097759183"
"Zheng H.; Fu J.; Zeng Y.; Luo J.; Zha Z.-J.","Zheng, Heliang (57200613046); Fu, Jianlong (36731082400); Zeng, Yanhong (57201095055); Luo, Jiebo (7404182441); Zha, Zheng-Jun (36626639900)","57200613046; 36731082400; 57201095055; 7404182441; 36626639900","Learning semantic-aware normalization for generative adversarial networks","2020","Advances in Neural Information Processing Systems","2020-December","","","","","","","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108422491&partnerID=40&md5=cbcfbed362ff74885b61b0d7c5bce22c","The recent advances in image generation have been achieved by style-based image generators. Such approaches learn to disentangle latent factors in different image scales and encode latent factors as “style” to control image synthesis. However, existing approaches cannot further disentangle fine-grained semantics from each other, which are often conveyed from feature channels. In this paper, we propose a novel image synthesis approach by learning Semantic-aware relative importance for feature channels in Generative Adversarial Networks (SariGAN). Such a model disentangles latent factors according to the semantic of feature channels by channel-/group- wise fusion of latent codes and feature channels. Particularly, we learn to cluster feature channels by semantics and propose an adaptive group-wise Normalization (AdaGN) to independently control the styles of different channel groups. For example, we can adjust the statistics of channel groups for a human face to control the open and close of the mouth, while keeping other facial features unchanged. We propose to use adversarial training, a channel grouping loss, and a mutual information loss for joint optimization, which not only enables high-fidelity image synthesis but leads to superior interpretable properties. Extensive experiments show that our approach outperforms the SOTA style-based approaches in both unconditional image generation and conditional image inpainting tasks. © 2020 Neural information processing systems foundation. All rights reserved.","Semantic Web; Semantics; Adversarial networks; High-fidelity images; Image generations; Image generators; Image Inpainting; Joint optimization; Learning semantics; Mutual informations; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85108422491"
"Amirrajab S.; Abbasi-Sureshjani S.; Al Khalil Y.; Lorenz C.; Weese J.; Pluim J.; Breeuwer M.","Amirrajab, Sina (57211821982); Abbasi-Sureshjani, Samaneh (57189379015); Al Khalil, Yasmina (57513053600); Lorenz, Cristian (55486269900); Weese, Jürgen (7005746479); Pluim, Josien (6701614327); Breeuwer, Marcel (7004252845)","57211821982; 57189379015; 57513053600; 55486269900; 7005746479; 6701614327; 7004252845","XCAT-GAN for Synthesizing 3D Consistent Labeled Cardiac MR Images on Anatomically Variable XCAT Phantoms","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12264 LNCS","","","128","137","9","10.1007/978-3-030-59719-1_13","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092792431&doi=10.1007%2f978-3-030-59719-1_13&partnerID=40&md5=f43710a34b87f543f84ede879eaa49a9","Generative adversarial networks (GANs) have provided promising data enrichment solutions by synthesizing high-fidelity images. However, generating large sets of labeled images with new anatomical variations remains unexplored. We propose a novel method for synthesizing cardiac magnetic resonance (CMR) images on a population of virtual subjects with a large anatomical variation, introduced using the 4D eXtended Cardiac and Torso (XCAT) computerized human phantom. We investigate two conditional image synthesis approaches grounded on a semantically-consistent mask-guided image generation technique: 4-class and 8-class XCAT-GANs. The 4-class technique relies on only the annotations of the heart; while the 8-class technique employs a predicted multi-tissue label map of the heart-surrounding organs and provides better guidance for our conditional image synthesis. For both techniques, we train our conditional XCAT-GAN with real images paired with corresponding labels and subsequently at the inference time, we substitute the labels with the XCAT derived ones. Therefore, the trained network accurately transfers the tissue-specific textures to the new label maps. By creating 33 virtual subjects of synthetic CMR images at the end-diastolic and end-systolic phases, we evaluate the usefulness of such data in the downstream cardiac cavity segmentation task under different augmentation strategies. Results demonstrate that even with only 20% of real images (40 volumes) seen during training, segmentation performance is retained with the addition of synthetic CMR images. Moreover, the improvement in utilizing synthetic images for augmenting the real data is evident through the reduction of Hausdorff distance up to 28% and an increase in the Dice score up to 5%, indicating a higher similarity to the ground truth in all dimensions. © 2020, Springer Nature Switzerland AG.","Heart; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Medical computing; Medical imaging; Phantoms; Textures; Tissue; Adversarial networks; Anatomical variations; Cardiac magnetic resonance; Cardiac mr images; Hausdorff distance; High-fidelity images; Segmentation performance; Tissue specifics; Image enhancement","Cardiac Magnetic Resonance imaging; Conditional image synthesis; XCAT anatomical phantom","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092792431"
"Hammami M.; Friboulet D.; Kechichian R.","Hammami, Maryam (57221264849); Friboulet, Denis (6602133133); Kechichian, Razmig (54928118000)","57221264849; 6602133133; 54928118000","Data augmentation for multi-organ detection in medical images","2020","2020 10th International Conference on Image Processing Theory, Tools and Applications, IPTA 2020","","","9286712","","","","10.1109/IPTA50016.2020.9286712","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099280564&doi=10.1109%2fIPTA50016.2020.9286712&partnerID=40&md5=7690ff0e3f80c9b9e2b26542a4cd3dbf","We propose a deep learning solution to the problem of object detection in 3D medical images, i.e. the localization and classification of multiple structures. Supervised learning methods require large annotated datasets that are usually difficult to acquire. We thus develop a Cycle Generative Adversarial Network (CycleGAN) and You Only Look Once (YOLO) combined method for data augmentation from one modality to another via CycleGAN and organ detection from generated images via YOLO. This results in a fast and accurate detection with a mean average distance of 7.95 mm for CT modality and 16.18 mm for MRI modality, which is significantly better than detection without data augmentation. We show that the approach compares favorably to state-of-the-art detection methods for medical images on CT data. © 2020 IEEE.","Computerized tomography; Deep learning; Large dataset; Learning systems; Object detection; Adversarial networks; Annotated datasets; Average Distance; Data augmentation; Detection methods; Multiple structures; State of the art; Supervised learning methods; Medical imaging","data augmentation; image synthesis; medical imaging; multi-organ detection","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099280564"
"Tao M.; Wu S.; Zhang X.; Wang C.","Tao, Ming (57219565980); Wu, Songsong (24485676900); Zhang, Xiaofeng (57217109516); Wang, Cailing (35367319800)","57219565980; 24485676900; 57217109516; 35367319800","DCFGAN: Dynamic convolutional fusion generative adversarial network for text-to-image synthesis","2020","Proceedings of 2020 IEEE International Conference on Information Technology, Big Data and Artificial Intelligence, ICIBA 2020","","","9277299","1250","1254","4","10.1109/ICIBA50161.2020.9277299","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099320789&doi=10.1109%2fICIBA50161.2020.9277299&partnerID=40&md5=85ba94bb23f30c1ec3c60cea1b4f6821","Text-to-image synthesis is the task of synthesizing realistic and text-matching images according to given text descriptions. Most text-to-image generative networks consist of two modules: a pre-trained text-image encoder and a text-to-image generative adversarial network. In this paper, we propose a stronger text encoder which employs a text Transformer to extract semantically meaningful parts from text descriptions. With the stronger text encoder, the generator can obtain more meaningful text information to synthesize realistic and text-matching images. In addition, we propose a Dynamic Convolutional text-image Fusion Generative Adversarial Network (DCFGAN) which employs the Dynamic Convolutional Fusion Block to fuse text and image features efficiently. The Dynamic Convolutional Fusion block adjusts the parameters in the convolution layer according to different text descriptions to synthesize text-matching images. It improves the efficiency of fusing text features and image features in generator network. We evaluate the proposed DCF-GAN on two benchmark datasets, the CUB and the Oxford-102. The extensive experiments demonstrate that our proposed stronger text encoder and Dynamic Convolutional Fusion Layer can greatly promote the performance of text-to-image synthesis. © 2020 IEEE.","Artificial intelligence; Big data; Convolution; Image enhancement; Signal encoding; Adversarial networks; Benchmark datasets; Fusion layers; Image features; Image synthesis; Text feature; Text information; Text-matching; Image fusion","Cross-modal; Deep representation learning; Generative adversarial network; Image-text matching; Text-to-Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85099320789"
"Cheng J.; Wu F.; Tian Y.; Wang L.; Tao D.","Cheng, Jun (14057685600); Wu, Fuxiang (57214770266); Tian, Yanling (57205126539); Wang, Lei (57203825442); Tao, Dapeng (54411283900)","14057685600; 57214770266; 57205126539; 57203825442; 54411283900","Rifegan: Rich feature generation for text-to-image synthesis from prior knowledge","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156682","10908","10917","9","10.1109/CVPR42600.2020.01092","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094596886&doi=10.1109%2fCVPR42600.2020.01092&partnerID=40&md5=35674de611906cf8f997479396c8db4e","Text-to-image synthesis is a challenging task that generates realistic images from a textual sequence, which usually contains limited information compared with the corresponding image and so is ambiguous and abstractive. The limited textual information only describes a scene partly, which will complicate the generation with complementing the other details implicitly and lead to low-quality images. To address this problem, we propose a novel rich feature generation text-to-image synthesis, called RiFeGAN, to enrich the given description. In order to provide additional visual details and avoid conflicting, RiFeGAN exploits an attention-based caption matching model to select and refine the compatible candidate captions from prior knowledge. Given enriched captions, RiFeGAN uses self-attentional embedding mixtures to extract features across them effectually and handle the diverging features further. Then it exploits multi-captions attentional generative adversarial networks to synthesize images from those features. The experiments conducted on widely-used datasets show that the models can generate images from enriched captions effectually and improve the results significantly. ©2020 IEEE.","Pattern recognition; Adversarial networks; Image synthesis; Limited information; Low qualities; Matching models; Prior knowledge; Realistic images; Textual information; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85094596886"
"Wu C.; Li H.","Wu, Chunpeng (56892929800); Li, Hai (57204886743)","56892929800; 57204886743","Conditional Transferring Features: Scaling GANs to Thousands of Classes with 30% Less High-Quality Data for Training","2020","Proceedings of the International Joint Conference on Neural Networks","","","9207546","","","","10.1109/IJCNN48605.2020.9207546","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093821292&doi=10.1109%2fIJCNN48605.2020.9207546&partnerID=40&md5=d5a490a0a1efbd9143e489c079c5f05b","Generative adversarial network (GAN) can greatly improve the quality of unsupervised image generation. Previous GAN-based methods often require a large amount of high-quality training data. This work aims to reduce the use of high-quality data in training, meanwhile scaling up GANs to thousands of classes. We propose an image generation method based on conditional transferring features, which can capture pixel-level semantic changes when transforming low-quality images into high-quality ones. Self-supervision learning is then integrated into our GAN architecture to provide more label-free semantic supervisory information observed from the training data. As such, training our GAN architecture requires much fewer high-quality images with a small number of additional low-quality images. Experiments show that even removing 30% high-quality images from the training set, our method can still achieve better image synthesis quality on CIFAR-10, STL-10, ImageNet, and CASIA-HWDB1.0, compared to previous competitive methods. Experiments on ImageNet with 1,000 classes of images and CASIA-HWDB1.0 with 3,755 classes of Chinese handwriting characters also validate the scalability of our method on object classes. Ablation studies further validate the contribution of our conditional transferring features and self-supervision learning to the quality of our synthesized images. © 2020 IEEE.","Network architecture; Neural networks; Semantics; Adversarial networks; Chinese handwriting; High quality data; High quality images; Image generations; Image synthesis; Supervision learning; Synthesized images; Image enhancement","Conditional transferring feature; generative adversarial network; high-quality image; image generation; low-quality image; self-supervision","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093821292"
"Revanda A.R.; Fatichah C.; Suciati N.","Revanda, Aldinata Rizky (57221629470); Fatichah, Chastine (35811948800); Suciati, Nanik (24825315200)","57221629470; 35811948800; 24825315200","Utilization of Generative Adversarial Networks in Face Image Synthesis for Augmentation of Face Recognition Training Data","2020","CENIM 2020 - Proceeding: International Conference on Computer Engineering, Network, and Intelligent Multimedia 2020","","","9297899","396","401","5","10.1109/CENIM51130.2020.9297899","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099665888&doi=10.1109%2fCENIM51130.2020.9297899&partnerID=40&md5=6cdceeea6204548c7c2fb68bc23a07de","Face recognition has become a popular research field in computer vision and is widely applied in various sectors. The challenge with face recognition is that if the training data is limited, the face recognition rate will be less effective. Generative Adversarial Networks (GANs) is a deep learning method that can create synthesis images with high quality. This research aims to utilize GANs in synthesizing face images as a form of augmentation in face recognition training data. Initially, the latent space representation of the face image will be made using GANs, then adding styles to the face image using the latent direction method. In the experiment of making latent space representation, the loss value was able to reach 0.15. In the experiment of face recognition, the addition of face image synthesis was able to increase the accuracy of the face recognition classifier model from 0.74 to 0.89. © 2020 IEEE.","Deep learning; Generative adversarial networks; Classifier models; Face image synthesis; Face images; Face recognition rates; High quality; Learning methods; Research fields; Training data; Face recognition","face image synthesis; face recognition; generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85099665888"
"Kang B.; Tripathi S.; Nguyen T.Q.","Kang, Byeongkeun (57031161000); Tripathi, Subarna (24823634700); Nguyen, Truong Q. (55843958400)","57031161000; 24823634700; 55843958400","Generating images in compressed domain using generative adversarial networks","2020","IEEE Access","8","","","180977","180991","14","10.1109/ACCESS.2020.3027800","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102760056&doi=10.1109%2fACCESS.2020.3027800&partnerID=40&md5=d92a7320eb62096ec259c0b2f817e8d8","In this article, we present a generative adversarial network framework that generates compressed images instead of synthesizing raw RGB images and compressing them separately. In the real world, most images and videos are stored and transferred in a compressed format to save storage capacity and data transfer bandwidth. However, since typical generative adversarial networks generate raw RGB images, those generated images need to be compressed by a post-processing stage to reduce the data size. Among image compression methods, JPEG has been one of the most commonly used lossy compression methods for still images. Hence, we propose a novel framework that generates JPEG compressed images using generative adversarial networks. The novel generator consists of the proposed locally connected layers, chroma subsampling layers, quantization layers, residual blocks, and convolution layers. The locally connected layer is proposed to enable block-based operations. We also discuss training strategies for the proposed architecture including the loss function and the decoding between its generator and its discriminator. The proposed method is evaluated using the publicly available CIFAR-10 dataset and LSUN bedroom dataset. The results demonstrate that the proposed method is able to generate compressed data with competitive qualities. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Data transfer; Digital storage; Adversarial networks; Compressed images; Image compression methods; JPEG compressed images; Lossy compression methods; Post-processing stages; Proposed architectures; Subsampling layers; Image compression","Generative adversarial networks; Image generation; Image synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102760056"
"Niu T.; Feng F.; Li L.; Wang X.","Niu, Tianrui (57217247447); Feng, Fangxiang (56432140300); Li, Lingxuan (57217248483); Wang, Xiaojie (35235644100)","57217247447; 56432140300; 57217248483; 35235644100","Image synthesis from locally related texts","2020","ICMR 2020 - Proceedings of the 2020 International Conference on Multimedia Retrieval","","","","145","153","8","10.1145/3372278.3390684","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086905090&doi=10.1145%2f3372278.3390684&partnerID=40&md5=895a5894b0b8049203b49cbcc7e7d105","Text-to-image synthesis refers to generating photo-realistic images from text descriptions. Recent works focus on generating images with complex scenes and multiple objects. However, the text inputs to these models are the only captions that always describe the most apparent object or feature of the image and detailed information (e.g. visual attributes) for regions and objects are often missing. Quantitative evaluation of generation performances is still an unsolved problem, where traditional image classification- or retrieval-based metrics fail at evaluating complex images. To address these problems, we propose to generate images conditioned on locally-related texts, i.e., descriptions of local image regions or objects instead of the whole image. Specifically, questions and answers (QAs) are chosen as locally-related texts, which makes it possible to use VQA accuracy as a new evaluation metric. The intuition is simple: higher image quality and image-text consistency (both globally and locally) can help a VQA model answer questions more correctly. We purposed VQA-GAN model with three key modules: hierarchical QA encoder, QA-conditional GAN and external VQA loss. These modules help leverage the new inputs effectively. Thorough experiments on two public VQA datasets demonstrate the effectiveness of the model and the newly proposed metric. © 2020 ACM.","Complex scenes; Evaluation metrics; Image synthesis; Multiple objects; Photorealistic images; Quantitative evaluation; Unsolved problems; Visual attributes; Image processing","Generative adversarial networks; Text-to-image synthesis; Visual question answering","Conference paper","Final","","Scopus","2-s2.0-85086905090"
"Sanaat A.; Shiri I.; Arabi H.; Mainta I.; Nkoulou R.; Zaidi H.","Sanaat, Amirhossein (57207449502); Shiri, Isaac (57191829519); Arabi, Hossein (57220877447); Mainta, Ismini (56527354000); Nkoulou, René (8246720200); Zaidi, Habib (7004977873)","57207449502; 57191829519; 57220877447; 56527354000; 8246720200; 7004977873","Whole-body PET Image Synthesis from Low-Dose Images Using Cycle-consistent Generative Adversarial Networks","2020","2020 IEEE Nuclear Science Symposium and Medical Imaging Conference, NSS/MIC 2020","","","","","","","10.1109/NSS/MIC42677.2020.9507947","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110441797&doi=10.1109%2fNSS%2fMIC42677.2020.9507947&partnerID=40&md5=375e69141c66cb3f5524aaf9e19b19cd","This work sets out to investigate the performance of full-dose (FD) PET prediction from fast or low-dose (LD) whole-body (WB) PET scans using convolutional neural networks. One hundred patients who underwent WB PET/CT examinations were retrospectively used to develop LD to FD PET conversion models. The patients underwent two separate WB examinations lasting ~27 min (regular scan) and ~3 min (fast or LD scan) acquisition times. The fast (3 min) WB PET examinations are equivalent to 1/8th of the full-dose PET scan. A residual neural network (ResNet) and a modified cycle-consistent generative adversarial network (CycleGAN) architecture were employed to model the LD to FD PET conversion. The quality of synthetic PET images produced by ResNet and CycleGAN models, referred to as RNET and CGAN, respectively, were evaluated by two nuclear medicine physicians using a five-point scoring. Quantitative metrics, including structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), mean square error (MSE) and standardized uptake value (SUV) bias were calculated within the left/right lung, brain, liver, and one hundred hot spots (malignant lesions) in PET images predicted using CGAN and RNET models. The physicians assigned scores of 3.88 and 4.92 (out of 5) to the CGAN-predicted FD PET images for the neck & trunk and brain regions, respectively. Considering the PSNR and SSIM metrics, the CGAN model exhibited superior performance with PSNR=39.08±3.56 and SSIM=0.97±0.02 compared to the RNET model with PSNR=34.91±1.50 and SSIM=0.93±0.04. Overall, the CGAN model outperformed the RNET model exhibiting lower SUV bias and higher image quality in the predicted FD PET images. © 2020 IEEE","Brain; Convolutional neural networks; Finite difference method; Generative adversarial networks; Image quality; Image segmentation; Mean square error; Medical imaging; Nuclear medicine; Positron emission tomography; Low dose; Neural-networks; Peak signal to noise ratio; Performance; PET images; PET Scan; Similarity indices; Standardized uptake values; Structural similarity; Whole-body; Signal to noise ratio","","Conference paper","Final","","Scopus","2-s2.0-85110441797"
"Zheng Z.; Wu Y.; Han X.; Shi J.","Zheng, Ziqiang (57202612793); Wu, Yang (55647067100); Han, Xinran (57220899625); Shi, Jianbo (55252537400)","57202612793; 55647067100; 57220899625; 55252537400","ForkGAN: Seeing into the Rainy Night","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12348 LNCS","","","155","170","15","10.1007/978-3-030-58580-8_10","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097824359&doi=10.1007%2f978-3-030-58580-8_10&partnerID=40&md5=a35194ee2ed5a3d01347a9383e8e5932","We present a ForkGAN for task-agnostic image translation that can boost multiple vision tasks in adverse weather conditions. Three tasks of image localization/retrieval, semantic image segmentation, and object detection are evaluated. The key challenge is achieving high-quality image translation without any explicit supervision, or task awareness. Our innovation is a fork-shape generator with one encoder and two decoders that disentangles the domain-specific and domain-invariant information. We force the cyclic translation between the weather conditions to go through a common encoding space, and make sure the encoding features reveal no information about the domains. Experimental results show our algorithm produces state-of-the-art image synthesis results and boost three vision tasks’ performances in adverse weathers. © 2020, Springer Nature Switzerland AG.","Encoding (symbols); Image segmentation; Meteorology; Object detection; Semantics; Signal encoding; Adverse weather; Domain specific; High quality images; Image localization; Image translation; Semantic image segmentations; State of the art; Task awareness; Computer vision","Generative adversarial networks; Image synthesis; Image-to-image translation; Light illumination","Conference paper","Final","","Scopus","2-s2.0-85097824359"
"Bejiga M.B.; Hoxha G.; Melgani F.","Bejiga, Mesay Belete (57192697078); Hoxha, Genc (57213198314); Melgani, Farid (35613488300)","57192697078; 57213198314; 35613488300","Retro-Remote Sensing with Doc2Vec Encoding","2020","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium, M2GARSS 2020 - Proceedings","","","9105139","89","92","3","10.1109/M2GARSS47143.2020.9105139","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086743107&doi=10.1109%2fM2GARSS47143.2020.9105139&partnerID=40&md5=922348b5f0dd3c64fbbfe33476e00728","In this work, we attempt to address the issue of developing a sophisticated text encoder for retro-remote sensing application. The encoder converts ancient landscape descriptions into a fixed-size vector that, adequately, represents the available information. This vector is then used as a conditioning data to a Generative adversarial network (GAN) that synthesizes the equivalent image. We propose using a pre-trained Doc2Vec encoder for text encoding and train a Wasserstein GAN (a variant of GAN) to convert landscape descriptions written by travelers and geographers into the equivalent image. Qualitative and quantitative analysis of the generated images signify usefulness of the proposed method. © 2020 IEEE.","Encoding (symbols); Geology; Signal encoding; Adversarial networks; Fixed size; Qualitative and quantitative analysis; Remote sensing applications; Text encoding; Remote sensing","Deep learning; Generative adversarial networks; Retro-remote sensing; Text embedding; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85086743107"
"Karmakar A.; Mishra D.","Karmakar, Arnab (57207770192); Mishra, Deepak (56689241300)","57207770192; 56689241300","A Robust Pose Transformational GAN for Pose Guided Person Image Synthesis","2020","Communications in Computer and Information Science","1249","","","89","99","10","10.1007/978-981-15-8697-2_8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097289651&doi=10.1007%2f978-981-15-8697-2_8&partnerID=40&md5=c99ebe724c382ccf4fb8962aebb952f2","Generating photorealistic images of human subjects in any unseen pose have crucial applications in generating a complete appearance model of the subject. However, from a computer vision perspective, this task becomes significantly challenging due to the inability of modelling the data distribution conditioned on pose. Existing works use a complicated pose transformation model with various additional features such as foreground segmentation, human body parsing etc. to achieve robustness that leads to computational overhead. In this work, we propose a simple yet effective pose transformation GAN by utilizing the Residual Learning method without any additional feature learning to generate a given human image in any arbitrary pose. Using effective data augmentation techniques and cleverly tuning the model, we achieve robustness in terms of illumination, occlusion, distortion and scale. We present a detailed study, both qualitative and quantitative, to demonstrate the superiority of our model over the existing methods on two large datasets. © 2020, Springer Nature Singapore Pte Ltd.","Computer vision; Large dataset; Appearance modeling; Computational overheads; Data augmentation; Data distribution; Feature learning; Foreground segmentation; Photorealistic images; Transformation model; Learning systems","Generative adversarial networks; Image synthesis; Pose guided person image generation; Pose transformation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097289651"
"Cheng Q.; Gu X.","Cheng, Qingrong (57193678501); Gu, Xiaodong (7403204205)","57193678501; 7403204205","Cross-modal Feature Alignment based Hybrid Attentional Generative Adversarial Networks for text-to-image synthesis","2020","Digital Signal Processing: A Review Journal","107","","102866","","","","10.1016/j.dsp.2020.102866","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092148222&doi=10.1016%2fj.dsp.2020.102866&partnerID=40&md5=b2ef11c42a0308d6aa70eb315a6561d5","With the development of the generative model, image synthesis has become a research hotspot. This paper presents a novel Cross-modal Feature Alignment based Hybrid Attentional Generative Adversarial Networks (CFA-HAGAN) for text-to-image synthesis. It mainly consists of two steps, text-image encoding and text-to-image synthesis. Text-image encoding learns a Cross-modal Feature Alignment Model (CFAM), which adopts a fine-grained attentional network to learn the original multi-modalities' aligned features. The feature alignment space is viewed as the transitional space in the whole process. Then, the Hybrid Attentional Generative Adversarial Networks (HAGAN) learns the inverse mapping from the encoded text feature to the original image. Specifically, the hybrid attention block consists of text-image cross-modal attention mechanism and self-attention mechanism of an image. Cross-modal attention makes the synthesized image fine-grained by adding word-level information as additional supervision. Self-attention can solve the long-distance reliance problem of image sub-region features when synthesizes images from the hidden feature. Although excellent performance in an ocean of tasks, GANs are well-known for the difficulty of training. Adopting spectral normalization, the discriminators are satisfied with 1-Lipschitz constraint, which makes their training process more stable than original GANs. During quantitative and non-quantitative comparison with many state-of-the-art methods, the experimental results show that the proposed method achieves better performance on evaluation metric and visual effect. Besides, the experimental section presents attention visualization, ablation study, and generalization ability analysis to show the effectiveness of the proposed method. © 2020 Elsevier Inc.","Alignment; Encoding (symbols); Inverse problems; Signal encoding; Adversarial networks; Attention mechanisms; Experimental section; Generalization ability; Quantitative comparison; Spectral normalization; State-of-the-art methods; Transitional spaces; Image processing","Cross-modal attention; Generative Adversarial Networks; Self-attention mechanism; Spectral normalization; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85092148222"
"Wang G.; Gong E.; Banerjee S.; Martin D.; Tong E.; Choi J.; Chen H.; Wintermark M.; Pauly J.M.; Zaharchuk G.","Wang, Guanhua (57212214103); Gong, Enhao (56046651600); Banerjee, Suchandrima (8442706400); Martin, Dann (56603517700); Tong, Elizabeth (57222527858); Choi, Jay (57211131780); Chen, Huijun (57212853092); Wintermark, Max (7003404861); Pauly, John M. (7101724924); Zaharchuk, Greg (6602464023)","57212214103; 56046651600; 8442706400; 56603517700; 57222527858; 57211131780; 57212853092; 7003404861; 7101724924; 6602464023","Synthesize High-Quality Multi-Contrast Magnetic Resonance Imaging from Multi-Echo Acquisition Using Multi-Task Deep Generative Model","2020","IEEE Transactions on Medical Imaging","39","10","9063444","3089","3099","10","10.1109/TMI.2020.2987026","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092680711&doi=10.1109%2fTMI.2020.2987026&partnerID=40&md5=d4334ba1877979ea8afa0d1f1fea9f68","Multi-echo saturation recovery sequence can provide redundant information to synthesizemulti-contrast magnetic resonance imaging. Traditional synthesis methods, such as GE's MAGiC platform, employ a model-fitting approach to generate parameter-weighted contrasts. However, models' over-simplification, as well as imperfections in the acquisition, can lead to undesirable reconstruction artifacts, especially in T2-FLAIR contrast. To improve the image quality, in this study, a multi-task deep learning model is developed to synthesize multi-contrast neuroimaging jointly using both signal relaxation relationships and spatial information. Compared with previous deep learning-based synthesis, the correlation between different destination contrast is utilized to enhance reconstruction quality. To improvemodel generalizabilityand evaluate clinical significance, the proposedmodelwas trained and tested on a large multi-center dataset, including healthy subjects and patients with pathology. Results from both quantitative comparison and clinical reader study demonstrate that the multi-task formulation leads to more efficient and accurate contrast synthesis than previous methods. © 2020 IEEE.","Artifacts; Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neuroimaging; Image enhancement; Image quality; Large dataset; Magnetic resonance imaging; Multi-task learning; Neuroimaging; Generative model; Healthy subjects; Model-fitting approach; Quantitative comparison; Reconstruction artifacts; Reconstruction quality; Spatial informations; Synthesis method; adult; article; clinical evaluation; controlled study; deep learning; human; image quality; leisure; neuroimaging; nuclear magnetic resonance imaging; quantitative analysis; synthesis; artifact; brain; diagnostic imaging; image processing; Deep learning","Deep learning (dl); Generative adversarial network (gan); Image fusion; Image synthesis; Magnetic resonance imaging (mri)","Article","Final","","Scopus","2-s2.0-85092680711"
"Qin Z.; Liu Z.; Zhu P.; Xue Y.","Qin, Zhiwei (57201011097); Liu, Zhao (56706864200); Zhu, Ping (50263790700); Xue, Yongbo (57217078796)","57201011097; 56706864200; 50263790700; 57217078796","A GAN-based image synthesis method for skin lesion classification","2020","Computer Methods and Programs in Biomedicine","195","","105568","","","","10.1016/j.cmpb.2020.105568","88","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086068778&doi=10.1016%2fj.cmpb.2020.105568&partnerID=40&md5=966fca81984b19020791b2b06ce6a42e","Background and Objective: There are many types of skin cancer, and melanoma is the most lethal one. Dermoscopy is an important imaging technique to screen melanoma and other skin lesions. However, Skin lesion classification based on computer-aided diagnostic techniques is a challenging task owing to the scarcity of labeled data and class-imbalanced dataset. It is necessary to apply data augmentation technique based on generative adversarial networks (GANs) to skin lesion classification for helping dermatologists in more accurate diagnostic decisions. Methods: A whole process of using GAN-based data augmentation technology to improve the skin lesion classification performance has been established in this article. First of all, the skin lesion style-based GANs is proposed according to the basic architecture of style-based GANs. The proposed model modifies the structure of style control and noise input in the original generator, adjusts both the generator and discriminator to efficiently synthesize high-quality skin lesion images. As for image classification, the classifier is constructed on the pretrained deep neural network using transfer learning method. The synthetic images from the proposed skin lesion style-based GANs are finally added to the training set to help train the classifier for better classification performance. Results: The proposed skin lesion style-based GAN has been evaluated by Inception Score (IS), Fréchet Inception Distance (FID), Precision and Recall, and is superior to other compared GAN models in these quantitative evaluation metrics. By adding the synthesized images to the training set, the main classification indicators like accuracy, sensitivity, specificity, average precision and balanced multiclass accuracy are 95.2%, 83.2%, 74.3%, 96.6% and 83.1% on the dataset of International Skin Imaging Collaboration (ISIC) 2018 Challenge, which have been improved by 1.6%, 24.4%, 3.6%, 23.2% and 5.6% respectively compared to the CNN model. Conclusions: The proposed skin lesion style-based GANs can generate high-quality skin lesion images efficiently, leading to the performance improvement of the classification model. This work provides a valuable reference for medical image analysis based on deep learning. © 2020 Elsevier B.V.","Humans; Image Processing, Computer-Assisted; Melanoma; Neural Networks, Computer; Skin Diseases; Skin Neoplasms; Classification (of information); Deep learning; Deep neural networks; Diagnosis; Image classification; Image enhancement; Learning systems; Medical imaging; Oncology; Quality control; Transfer learning; Adversarial networks; Classification models; Classification performance; Computer aided diagnostics; Diagnostic decisions; Precision and recall; Quantitative evaluation; Transfer learning methods; actinic keratosis; affine transform; Article; artificial neural network; basal cell carcinoma; classifier; deep learning; deep neural network; dermatofibroma; disease classification; epiluminescence microscopy; generative adversarial network; image processing; keratosis; pigmented nevus; qualitative analysis; recall; sensitivity and specificity; skin defect; transfer of learning; vascular lesion; diagnostic imaging; human; image processing; melanoma; skin disease; skin tumor; Dermatology","Data augmentation; Generative adversarial networks; Image synthesis; Skin lesion classification; Transfer learning","Article","Final","","Scopus","2-s2.0-85086068778"
"Wang Q.; Zhang X.; Chen W.; Wang K.; Zhang X.","Wang, Qiuli (57207794366); Zhang, Xingpeng (56659324900); Chen, Wei (57212514483); Wang, Kun (57213024990); Zhang, Xiaohong (55276997400)","57207794366; 56659324900; 57212514483; 57213024990; 55276997400","Class-Aware Multi-window Adversarial Lung Nodule Synthesis Conditioned on Semantic Features","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12266 LNCS","","","589","598","9","10.1007/978-3-030-59725-2_57","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092775546&doi=10.1007%2f978-3-030-59725-2_57&partnerID=40&md5=e8d7da9267a804047e5ad1bb1b27eebc","Nodule CT image synthesis is effective as a data augmentation method for deep learning tasks about lung nodules. To advance the realistic malignant/benign lung nodule synthesis, the conditional Generative Adversarial Networks have been widely adopted. In this paper, we argue about an issue in the existing technique for class-aware nodule synthesis: the class-aware controllability of semantic features. To address this issue, we propose a adversarial lung nodule synthesis framework based on conditional Generative Adversarial Networks and class-aware multi-window semantic feature learning. By learning semantic features from multi-window CT images, our framework can generate realistic nodule CT images, and has better controllability of class-aware nodule features. Our framework provides a new perspective for nodule CT image synthesis that has never been noticed before. We train our framework on the public dataset LIDC-IDRI. Our framework improves the malignancy prediction F1 score by more than 3% and shows promising results as a solution for lung nodule augmentation. The source code can be found at https://github.com/qiuliwang/CA-MW-Adversarial-Synthesis. © 2020, Springer Nature Switzerland AG.","Biological organs; Deep learning; Medical computing; Medical imaging; Semantics; Adversarial networks; Data augmentation; Learning semantics; Learning tasks; Multi-Windows; Public dataset; Semantic features; Source codes; Computerized tomography","Computed Tomography; Generative Adversarial Networks; Lung nodule synthesis; Multi-window","Conference paper","Final","","Scopus","2-s2.0-85092775546"
"Liu X.; Ma Z.; Guo X.; Hou J.; Schaefer G.; Wang L.; Wang V.; Fang H.","Liu, Xiyao (57095810500); Ma, Ziping (57221331081); Guo, Xingbei (57221338196); Hou, Jialu (57221338056); Schaefer, Gerald (7102506029); Wang, Lei (57070545400); Wang, Victoria (37076454200); Fang, Hui (55470721600)","57095810500; 57221331081; 57221338196; 57221338056; 7102506029; 57070545400; 37076454200; 55470721600","Camouflage Generative Adversarial Network: Coverless Full-image-to-image Hiding","2020","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2020-October","","9283054","166","172","6","10.1109/SMC42975.2020.9283054","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098873500&doi=10.1109%2fSMC42975.2020.9283054&partnerID=40&md5=0d84b0230a34c17f92516196de255cbd","Image hiding, one of the most important data hiding techniques, is widely used to enhance cybersecurity when transmitting multimedia data. In recent years, deep learning-based image hiding algorithms have been designed to improve the embedding capacity whilst maintaining sufficient imperceptibility to malicious eavesdroppers. These methods can hide a full-size secret image into a cover image, thus allowing full-image-to-image hiding. However, these methods suffer from a trade-off challenge to balance the possibility of detection from the container image against the recovery quality of secret image. In this paper, we propose Camouflage Generative Adversarial Network (Cam-GAN), a novel two-stage coverless full-image-to-image hiding method named, to tackle this problem. Our method offers a hiding solution through image synthesis to avoid using a modified cover image as the image hiding container and thus enhancing both image hiding imperceptibility and recovery quality of secret images. Our experimental results demonstrate that Cam-GAN outperforms state-of-the-art full-image-to-image hiding algorithms on both aspects. © 2020 IEEE.","Cams; Computer system recovery; Containers; Deep learning; Economic and social effects; Security of data; Adversarial networks; Cyber security; Embedding capacity; Image hiding method; Image synthesis; Multimedia data; Recovery quality; State of the art; Image enhancement","deep learning; generative adversarial network; image hiding; image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098873500"
"Liao Y.; Schwarz K.; Mescheder L.; Geiger A.","Liao, Yiyi (56742605600); Schwarz, Katja (57219584146); Mescheder, Lars (57194766823); Geiger, Andreas (55822335800)","56742605600; 57219584146; 57194766823; 55822335800","Towards unsupervised learning of generative models for 3D controllable image synthesis","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156436","5870","5879","9","10.1109/CVPR42600.2020.00591","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094322294&doi=10.1109%2fCVPR42600.2020.00591&partnerID=40&md5=bca82409217f6dac51d82dd7fdcc0091","In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task. © 2020 IEEE","Pattern recognition; Three dimensional computer graphics; 3d representations; Adversarial networks; Generative model; Image generations; Image synthesis; Photorealistic images; Physical world; Rendering pipelines; Image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094322294"
"Wang C.; Sun X.; Zhang B.; Lai G.; Yu D.; Su K.","Wang, Che (57222196545); Sun, Xiaoyu (57222197163); Zhang, Bin (57222195573); Lai, Guanjun (57222195005); Yu, Dan (55512406700); Su, Kang (57222196348)","57222196545; 57222197163; 57222195573; 57222195005; 55512406700; 57222196348","Brain CT Image with Motion Artifact Augmentation Based on PGGAN and FBP for Artifact Detection","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12447 LNAI","","","370","378","8","10.1007/978-3-030-65390-3_29","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101872245&doi=10.1007%2f978-3-030-65390-3_29&partnerID=40&md5=64767d53c3fa4be95f25cc515f9a4714","Synthesis of computed tomographic images with motion artifact has many applications in assisted medical diagnosis such as artifact detection and removal. However, one of the challenges is on how to synthesize high-resolution images with motion artifact while artifact features and tissues are naturally presented within image. In this paper, we propose a new method to solve the problem by combing filtered back-projection (FBP) and progressive growing of generative adversarial networks (PGGAN), while FBP is for artifact generation and feature extraction and PGGAN is for feature augmentation. Finally, we superimpose artifact features onto artifact-free data, so to obtain a set of pre-demanded and diversified images with all kinds of motion artifacts. We quantitatively evaluate the synthetic images by training models with synthetic data for artifact detection. Our extensive experiments demonstrated that the performance of our proposed method is superior over the state-of-the-art methods. © 2020, Springer Nature Switzerland AG.","Data mining; Diagnosis; Medical imaging; Adversarial networks; Artifact detection; Computed tomographic; Filtered back projection; High resolution image; Motion artifact; State-of-the-art methods; Synthetic images; Computerized tomography","Generative adversarial networks; Medical image synthesis; Motion artifact generation","Conference paper","Final","","Scopus","2-s2.0-85101872245"
"Kawahara D.; Saito A.; Ozawa S.; Nagata Y.","Kawahara, Daisuke (56350513700); Saito, Akito (8904573300); Ozawa, Shuichi (16031706800); Nagata, Yasushi (56415732800)","56350513700; 8904573300; 16031706800; 56415732800","Image synthesis with deep convolutional generative adversarial networks for material decomposition in dual-energy CT from a kilovoltage CT","2021","Computers in Biology and Medicine","128","","104111","","","","10.1016/j.compbiomed.2020.104111","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097328753&doi=10.1016%2fj.compbiomed.2020.104111&partnerID=40&md5=ad25b2320ce4c6b6ba16dcd24299f8c8","Generative Adversarial Networks (GANs) have been widely used and it is expected to use for the clinical examination and image. The objective of the current study was to synthesize material decomposition images of bone-water (bone(water)) and fat-water (fat(water)) reconstructed from dual-energy computed tomography (DECT) using an equivalent kilovoltage-CT (kV-CT) image and a deep conditional GAN. The effective atomic number images were reconstructed using DECT. We used 18,084 images of 28 patients divided into two datasets: the training data for the model included 16,146 images (20 patients) and the test data for evaluation included 1938 images (8 patients). Image prediction frameworks of the equivalent single energy CT images at 120 kVp to the effective atomic number images were created. The image-synthesis framework was based on a CNN with a generator and discriminator. The mean absolute error (MAE), relative mean square error (MSE), relative root mean square error (RMSE), peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and mutual information (MI) were evaluated. The Hounsfield unit (HU) difference between the synthesized and reference material decomposition images of bone(water) and fat(water) were within 5.3 HU and 20.3 HU, respectively. The average MAE, MSE, RMSE, SSIM, and MI of the synthesized and reference material decomposition of the bone(water) images were 0.8, 1.3, 0.9, 0.9, 55.3, and 0.8, respectively. The average MAE, MSE, RMSE, SSIM, and MI of the synthesized and reference material decomposition of the fat(water) images were 0.0, 0.0, 0.1, 0.9, 72.1, and 1.4, respectively. The proposed model can act as a suitable alternative to the existing methods for the reconstruction of material decomposition images of bone(water) and fat(water) reconstructed via DECT from kV-CT. © 2020 Elsevier Ltd","Atoms; Convolutional neural networks; Errors; Image reconstruction; Mean square error; Signal to noise ratio; Clinical examination; Dual energy computed tomography (DECT); Effective atomic number; Material decomposition; Peak signal to noise ratio; Relative mean square errors; Root mean square errors; Structural similarity indices (SSIM); adult; Article; artificial intelligence; clinical examination; cone beam computed tomography; controlled study; convolutional neural network; deep learning; digital imaging and communications in medicine; dual energy computed tomography; electric potential; histogram; human; information processing; priority journal; signal noise ratio; synthesis; Computerized tomography","Artificial intelligence; Deep learning; Dual-energy CT; Material decomposition; Medical imaging","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097328753"
"Hossain M.Z.; Sohel F.; Shiratuddin M.F.; Laga H.; Bennamoun M.","Hossain, Md. Zakir (57214230875); Sohel, Ferdous (9273356400); Shiratuddin, Mohd Fairuz (6505710759); Laga, Hamid (8717100800); Bennamoun, Mohammed (7004376121)","57214230875; 9273356400; 6505710759; 8717100800; 7004376121","Text to Image Synthesis for Improved Image Captioning","2021","IEEE Access","9","","9416431","64918","64928","10","10.1109/ACCESS.2021.3075579","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105047593&doi=10.1109%2fACCESS.2021.3075579&partnerID=40&md5=be679ec13c3e48fda8ce940986304d7d","Generating textual descriptions of images has been an important topic in computer vision and natural language processing. A number of techniques based on deep learning have been proposed on this topic. These techniques use human-annotated images for training and testing the models. These models require a large number of training data to perform at their full potential. Collecting human generated images with associative captions is expensive and time-consuming. In this paper, we propose an image captioning method that uses both real and synthetic data for training and testing the model. We use a Generative Adversarial Network (GAN) based text to image generator to generate synthetic images. We use an attention-based image captioning method trained on both real and synthetic images to generate the captions. We demonstrate the results of our models using both qualitative and quantitative analysis on popularly used evaluation metrics. We show that our experimental results achieve two fold benefits of our proposed work: i) it demonstrates the effectiveness of image captioning for synthetic images, and ii) it further improves the quality of the generated captions for real images, understandably because we use additional images for training.  © 2013 IEEE.","Deep learning; Natural language processing systems; Adversarial networks; Evaluation metrics; Image captioning; Image generators; NAtural language processing; Qualitative and quantitative analysis; Textual description; Training and testing; Image enhancement","attention; generative adversarial network; Image captioning; synthetic images","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85105047593"
"Zhu P.; Abdal R.; Qin Y.; Wonka P.","Zhu, Peihao (57220739950); Abdal, Rameen (57200697007); Qin, Yipeng (57219470495); Wonka, Peter (12765329500)","57220739950; 57200697007; 57219470495; 12765329500","SEAN: Image synthesis with semantic region-adaptive normalization","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156510","5103","5112","9","10.1109/CVPR42600.2020.00515","153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093672513&doi=10.1109%2fCVPR42600.2020.00515&partnerID=40&md5=4e511925bc333565deb73b27143294c6","We propose semantic region-adaptive normalization (SEAN), a simple but effective building block for Generative Adversarial Networks conditioned on segmentation masks that describe the semantic regions in the desired output image. Using SEAN normalization, we can build a network architecture that can control the style of each semantic region individually, e.g., we can specify one style reference image per region. SEAN is better suited to encode, transfer, and synthesize style than the best previous method in terms of reconstruction quality, variability, and visual quality. We evaluate SEAN on multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than the current state of the art. SEAN also pushes the frontier of interactive image editing. We can interactively edit images by changing segmentation masks or the style for any given region. We can also interpolate styles from two reference images per region. Code: https://github.com/ZPdesu/SEAN. © 2020 IEEE","Network architecture; Pattern recognition; Semantics; Adversarial networks; Building blockes; Interactive image editing; Multiple data sets; Quantitative metrics; Reconstruction quality; Segmentation masks; Synthesize styles; Image segmentation","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093672513"
"Chu W.-T.; Huang P.-S.","Chu, Wei-Ta (35075752600); Huang, Ping-Shen (57222163108)","35075752600; 57222163108","Thermal Face Recognition Based on Multi-scale Image Synthesis","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12572 LNCS","","","99","110","11","10.1007/978-3-030-67832-6_9","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101749936&doi=10.1007%2f978-3-030-67832-6_9&partnerID=40&md5=67b8fce49e142401a1f6dcb6b6daba76","We present a transformation-based method to achieve thermal face recognition. Given a thermal face, the proposed model transforms the input to a synthesized visible face, which is then used as a probe to compare with visible faces in the database. This transformation model is built on the basis of a generative adversarial network, mainly with the ideas of multi-scale discrimination and various loss functions like feature embedding, identity preservation, and facial landmark-guided texture synthesis. The evaluation results show that the proposed method outperforms the state of the art. © 2021, Springer Nature Switzerland AG.","Textures; Adversarial networks; Evaluation results; Feature embedding; Model transforms; State of the art; Texture synthesis; Transformation based; Transformation model; Face recognition","Image synthesis; Multi-scale; Thermal face recognition","Conference paper","Final","","Scopus","2-s2.0-85101749936"
"Baydoun A.; Xu K.; Heo J.U.; Yang H.; Zhou F.; Bethell L.A.; Fredman E.T.; Ellis R.J.; Podder T.K.; Traughber M.S.; Paspulati R.M.; Qian P.; Traughber B.J.; Muzic R.F.","Baydoun, Atallah (55362014500); Xu, Ke (57220999031); Heo, Jin Uk (57216269321); Yang, Huan (57207732435); Zhou, Feifei (57216272965); Bethell, Latoya A. (57221602366); Fredman, Elisha T. (55341720900); Ellis, Rodney J. (7401985120); Podder, Tarun K. (14054613500); Traughber, Melanie S. (55515560400); Paspulati, Raj M. (8857378100); Qian, Pengjiang (36598989000); Traughber, Bryan J. (57189451433); Muzic, Raymond F. (6603808571)","55362014500; 57220999031; 57216269321; 57207732435; 57216272965; 57221602366; 55341720900; 7401985120; 14054613500; 55515560400; 8857378100; 36598989000; 57189451433; 6603808571","Synthetic CT Generation of the Pelvis in Patients with Cervical Cancer: A Single Input Approach Using Generative Adversarial Network","2021","IEEE Access","9","","9316666","17208","17221","13","10.1109/ACCESS.2021.3049781","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099546726&doi=10.1109%2fACCESS.2021.3049781&partnerID=40&md5=74722f8e584395dc5af9ee0392720f12","Multi-modality imaging constitutes a foundation of precision medicine, especially in oncology where reliable and rapid imaging techniques are needed in order to insure adequate diagnosis and treatment. In cervical cancer, precision oncology requires the acquisition of 18F-labelled 2-fluoro-2-deoxy-D-glucose (FDG) positron emission tomography (PET), magnetic resonance (MR), and computed tomography (CT) images. Thereafter, images are co-registered to derive electron density attributes required for FDG-PET attenuation correction and radiation therapy planning. Nevertheless, this traditional approach is subject to MR-CT registration defects, expands treatment expenses, and increases the patient's radiation exposure. To overcome these disadvantages, we propose a new framework for cross-modality image synthesis which we apply on MR-CT image translation for cervical cancer diagnosis and treatment. The framework is based on a conditional generative adversarial network (cGAN) and illustrates a novel tactic that addresses, simplistically but efficiently, the paradigm of vanishing gradient vs. feature extraction in deep learning. Its contributions are summarized as follows: 1) The approach-termed sU-cGAN- uses, for the first time, a shallow U-Net (sU-Net) with an encoder/decoder depth of 2 as generator; 2) sU-cGAN's input is the same MR sequence that is used for radiological diagnosis, i.e. T2-weighted, Turbo Spin Echo Single Shot (TSE-SSH) MR images; 3) Despite limited training data and a single input channel approach, sU-cGAN outperforms other state of the art deep learning methods and enables accurate synthetic CT (sCT) generation. In conclusion, the suggested framework should be studied further in the clinical settings. Moreover, the sU-Net model is worth exploring in other computer vision tasks. © 2013 IEEE.","Channel coding; Deep learning; Diagnosis; Diseases; Learning systems; Magnetic resonance; Magnetic resonance imaging; Oncology; Positron emission tomography; Radiotherapy; Adversarial networks; Clinical settings; Limited training data; Multi-modality imaging; Positron emission tomography (PET); Radiation Exposure; Traditional approaches; Vanishing gradient; Computerized tomography","Cervical cancer; computed tomography; deep learning; generative adversarial network; magnetic resonance imaging; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099546726"
"Ning Z.; Zhang Y.; Pan Y.; Zhong T.; Liu M.; Shen D.","Ning, Zhenyuan (57194600411); Zhang, Yu (57224621211); Pan, Yongsheng (56440550200); Zhong, Tao (57204287917); Liu, Mingxia (36677833300); Shen, Dinggang (7401738392)","57194600411; 57224621211; 56440550200; 57204287917; 36677833300; 7401738392","LDGAN: Longitudinal-Diagnostic Generative Adversarial Network for Disease Progression Prediction with Missing Structural MRI","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12436 LNCS","","","170","179","9","10.1007/978-3-030-59861-7_18","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092702976&doi=10.1007%2f978-3-030-59861-7_18&partnerID=40&md5=ca074d40575b7f3a42be7db2d471ed6f","Predicting future progression of brain disorders is fundamental for effective intervention of pathological cognitive decline. Structural MRI provides a non-invasive solution to examine brain pathology and has been widely used for longitudinal analysis of brain disorders. Previous studies typically use only complete baseline MRI scans to predict future disease status due to the lack of MRI data at one or more future time points. Since temporal changes of each brain MRI are ignored, these methods would result in sub-optimal performance. To this end, we propose a longitudinal-diagnostic generative adversarial network (LDGAN) to predict multiple clinical scores at future time points using incomplete longitudinal MRI data. Specifically, LDGAN imputes MR images by learning a bi-directional mapping between MRIs of two adjacent time points and performing clinical score prediction jointly, thereby explicitly encouraging task-oriented image synthesis. The proposed LDGAN is further armed with a temporal constraint and an output constraint to model the temporal regularity of MRIs at adjacent time points and encourage the diagnostic consistency, respectively. We also design a weighted loss function to make use of those subjects without ground-truth scores at certain time points. The major advantage of the proposed LDGAN is that it can impute those missing scans in a task-oriented manner and can explicitly capture the temporal characteristics of brain changes for accurate prediction. Experimental results on both ADNI-1 and ADNI-2 datasets demonstrate that, compared with the state-of-the-art methods, LDGAN can generate more reasonable MRI scans and efficiently predict longitudinal clinical measures. © 2020, Springer Nature Switzerland AG.","Computer aided instruction; Forecasting; Machine learning; Magnetic resonance imaging; Medical imaging; Accurate prediction; Adversarial networks; Longitudinal analysis; State-of-the-art methods; Sub-optimal performance; Temporal characteristics; Temporal constraints; Weighted loss function; Diagnosis","","Conference paper","Final","","Scopus","2-s2.0-85092702976"
"Tang W.; Li G.; Bao X.; Nian F.; Li T.","Tang, Wei (57219183280); Li, Gui (57219175351); Bao, Xinyuan (57202360375); Nian, Fudong (56182914200); Li, Teng (35976315200)","57219183280; 57219175351; 57202360375; 56182914200; 35976315200","MsCGAN: Multi-scale Conditional Generative Adversarial Networks for Person Image Generation","2020","Proceedings of the 32nd Chinese Control and Decision Conference, CCDC 2020","","","9164755","1440","1445","5","10.1109/CCDC49329.2020.9164755","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091581891&doi=10.1109%2fCCDC49329.2020.9164755&partnerID=40&md5=33999cf28e44e8e4cde55af3930a5698","To synthesize high-quality person images with arbitrary poses is challenging. In this paper, we propose a novel Multi-scale Conditional Generative Adversarial Networks (MsCGAN), aiming to convert the input conditional person image to a synthetic image of any given target pose, whose appearance and the texture are consistent with the input image. MsCGAN is a multi-scale adversarial network consisting of two generators and two discriminators. One generator transforms the conditional person image into a coarse image of the target pose globally, and the other is to enhance the detailed quality of the synthetic person image through a local reinforcement network. The outputs of the two generators are then merged into a synthetic, discriminant and high-resolution image. On the other hand, the synthetic image is downsampled to multiple resolutions as the input to multi-scale discriminator networks. The proposed multi-scale generators and discriminators handling different levels of visual features can benefit to synthesizing high-resolution person images with realistic appearance and texture. Experiments are conducted on the Market-1501 and DeepFashion datasets to evaluate the proposed model, and both qualitative and quantitative results demonstrate the superior performance of the proposed MsCGAN. © 2020 IEEE.","Discriminators; Image enhancement; Textures; Adversarial networks; High resolution; High resolution image; Image generations; Local reinforcements; Multiple resolutions; Quantitative result; Synthetic images; Image texture","generative adversarial networks; image synthesis; multi-scale discriminators; person image generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85091581891"
"Cai J.; Xie H.; Li J.; Li S.","Cai, Jianmin (57220211901); Xie, Hongliang (57220211777); Li, Jianfeng (57203736355); Li, Shigang (16202805500)","57220211901; 57220211777; 57203736355; 16202805500","Facial Expression Recognition with an Attention Network Using a Single Depth Image","2020","Communications in Computer and Information Science","1332","","","222","231","9","10.1007/978-3-030-63820-7_25","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097257473&doi=10.1007%2f978-3-030-63820-7_25&partnerID=40&md5=47af18ee67b2987158a0eb2d5afe3694","In the facial expression recognition field, RGB image-involved models have always achieved the best performance. Since RGB images are easily influenced by illumination, skin color, and cross-databases, the effect of these methods decreases accordingly. To avoid these issues, we propose a novel facial expression recognition framework in which the input only relies on a single depth image since depth image performs very stable in cross-situations. In our framework, we pretrain an RGB face image synthesis model by a generative adversarial network (GAN) using a public database. This pretrained model can synthesize an RGB face image under a unified imaging situation from a depth face image input. Then, introducing the attention mechanism based on facial landmarks into a convolutional neural network (CNN) for recognition, this attention mechanism can strengthen the weights of the key parts. Thus, our framework has a stable input (depth face image) while retaining the natural merits of RGB face images for recognition. Experiments conducted on public databases demonstrate that the recognition rate of our framework is better than that of the state-of-the-art methods, which are also based on depth images. © 2020, Springer Nature Switzerland AG.","Convolutional neural networks; Database systems; Adversarial networks; Attention mechanisms; Face image synthesis; Face images; Facial expression recognition; Facial landmark; Public database; State-of-the-art methods; Face recognition","Depth face image; Facial expression recognition; Synthesized RGB face image","Conference paper","Final","","Scopus","2-s2.0-85097257473"
"Ntavelis E.; Romero A.; Kastanis I.; Van Gool L.; Timofte R.","Ntavelis, Evangelos (57219725885); Romero, Andrés (57205197421); Kastanis, Iason (8339669800); Van Gool, Luc (22735702300); Timofte, Radu (35812778400)","57219725885; 57205197421; 8339669800; 22735702300; 35812778400","SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12367 LNCS","","","394","411","17","10.1007/978-3-030-58542-6_24","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097297291&doi=10.1007%2f978-3-030-58542-6_24&partnerID=40&md5=82f27bd3fe94392eabcf3f526fde61ab","Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels. © 2020, Springer Nature Switzerland AG.","Computer vision; Editing operations; Image generations; Image manipulation; Input streams; Semantic concept; Semantic images; Semantic labels; State-of-the-art performance; Semantics","Generative adversarial networks; Image synthesis; Interactive image editing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097297291"
"Li B.; Yu Y.; Wang M.","Li, Benyuan (57221313303); Yu, Yue (55817105900); Wang, Menglan (57221306279)","57221313303; 55817105900; 57221306279","Semantic Image Synthesis with Trilateral Generative Adversarial Networks","2020","ACM International Conference Proceeding Series","PartF168342","","","218","224","6","10.1145/3447450.3447484","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104174367&doi=10.1145%2f3447450.3447484&partnerID=40&md5=d7f3d54ff1fbc583383d094726a5ebbb","We present a new method that can synthesize photorealistic images from semantic label maps using GANs with a trilateral generator. Due to the limitation of network capacity, it is difficult forscene image generation networks to consider resolution and image fidelity at the same time. The solution is to use multi-path structure instead of the traditional single-path structure. In this work, we propose a novel trilateral Generative Adversarial Network (trilateral GAN), which has fewer parameters than other recent methods to synthesize 512∗1024 images with high fidelity. Moreover, we improve the semantic consistency loss and feature matching loss by using the features before activation, which can make the synthetic images have sharper edges and richer textures. Finally, we replace all the Batch-Normalization (BN) layers with Spectral Normalization (SN) in the network anduse Conditional Normalization Blocks (CNB) to avoid difficult convergence during training and make the synthesized images more realistic. The proposed network has a better performance and gets higher mIoU scores on COCO-Stuff, ADE20K and Cityscapes than the state-of-the-art methods. © 2020 ACM.","Semantic Web; Semantics; Textures; Video signal processing; Adversarial networks; Feature matching; Image generations; Photorealistic images; Semantic consistency; Spectral normalization; State-of-the-art methods; Synthesized images; Image enhancement","GAN; Image generation; semantic label maps","Conference paper","Final","","Scopus","2-s2.0-85104174367"
"Li L.; Sun Y.; Hu F.; Zhou T.; Xi X.; Ren J.","Li, Linyan (57211233434); Sun, Yu (57206701110); Hu, Fuyuan (23469669100); Zhou, Tao (57020593700); Xi, Xuefeng (36509542500); Ren, Jinchang (23398632100)","57211233434; 57206701110; 23469669100; 57020593700; 36509542500; 23398632100","Text to Realistic Image Generation with Attentional Concatenation Generative Adversarial Networks","2020","Discrete Dynamics in Nature and Society","2020","","6452536","","","","10.1155/2020/6452536","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096024300&doi=10.1155%2f2020%2f6452536&partnerID=40&md5=75306aadbedc5b7e84f1f32c8d57a635","In this paper, we propose an Attentional Concatenation Generative Adversarial Network (ACGAN) aiming at generating 1024 × 1024 high-resolution images. First, we propose a multilevel cascade structure, for text-to-image synthesis. During training progress, we gradually add new layers and, at the same time, use the results and word vectors from the previous layer as inputs to the next layer to generate high-resolution images with photo-realistic details. Second, the deep attentional multimodal similarity model is introduced into the network, and we match word vectors with images in a common semantic space to compute a fine-grained matching loss for training the generator. In this way, we can pay attention to the fine-grained information of the word level in the semantics. Finally, the measure of diversity is added to the discriminator, which enables the generator to obtain more diverse gradient directions and improve the diversity of generated samples. The experimental results show that the inception scores of the proposed model on the CUB and Oxford-102 datasets have reached 4.48 and 4.16, improved by 2.75% and 6.42% compared to Attentional Generative Adversarial Networks (AttenGAN). The ACGAN model has a better effect on text-generated images, and the resulting image is closer to the real image. © 2020 Linyan Li et al.","","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096024300"
"Yang Z.-K.; Bu L.-P.; Wang T.; Ouyang J.-N.","Yang, Zhi-Kai (57206894662); Bu, Le-Ping (8266284100); Wang, Teng (57189235790); Ouyang, Ji-Neng (57206905148)","57206894662; 8266284100; 57189235790; 57206905148","Scenemigration of indoor flame image based on Cycle-Consistent adversarial networks; [基于循环一致性对抗网络的室内火焰图像场景迁移]","2020","Guangxue Jingmi Gongcheng/Optics and Precision Engineering","28","3","","745","758","13","10.3788/OPE.20202803.0745","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083155145&doi=10.3788%2fOPE.20202803.0745&partnerID=40&md5=4309686172a99094c5dd8ac94263e0b3","The training of video fire detection models based on deep learning relies on a large number of positive and negative samples, namely, fire videos and scenario videos involving other disturbances similar to fires. In some instances, the fire video sampled from a scene is insufficient owing to the prohibition of ignition. In this thesis, it was proposed that the flames recorded in other similar scenarios be migrated into the specified scene to increase the data of the fire video in such restricted situations. To complete the content information, a flame kernel was previously implanted into the specified scene, and then style information, such as smoke and ground reflection, were added to fuse the scene and the flame. Our method eliminated the background distortion caused by the loss of information during image translation via the existing multimodal image translation. In addition, Cycle-Consistent adversarial networks were adopted to decrease the dataset quantity and remove the restriction of strict matching of the training images. Compared with other multimodal image-to-image translations, our method ensured that the fire in a scene was diverse, and the migration scene was more visually realistic. The minimum values of FID and LPIPS are achieved, which are 119.6 and 0.134 2, respectively. © 2020, Science Press. All right reserved.","Deep learning; Smoke; Adversarial networks; Background distortions; Content information; Fire detection; Ground reflection; Image translation; Multi-modal image; Negative samples; Fires","Cycle Generative Adversarial Network(CycleGAN); Fire image synthesis; Generative adversarial network; Image translation","Article","Final","","Scopus","2-s2.0-85083155145"
"","","","International Conference on Artificial Intelligence and Applications, ICAIA 2020","2021","Advances in Intelligent Systems and Computing","1164","","","","","615","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088566660&partnerID=40&md5=aac9b09eb05a2a6e1fdc3a5ddfc6de11","The proceedings contain 57 papers. The special focus in this conference is on Artificial Intelligence and Applications. The topics include: Cryptosystem Based on Hybrid Chaotic Structured Phase Mask and Hybrid Mask Using Gyrator Transform; PE File-Based Malware Detection Using Machine Learning; intelligence Graphs for Threat Intelligence and Security Policy Validation of Cyber Systems; anomaly Detection Using Federated Learning; enhanced Digital Image Encryption Using Sine Transformed Complex Chaotic Sequence; A Low-Power Ring Voltage-Controlled Oscillator with MOS Resistor Tuning for Wireless Application; Fuzzy Logic Control D-STATCOM Technique; Comparative Study on Machine Learning Classifiers for Epileptic Seizure Detection in Reference to EEG Signals; Design Fundamentals: Iris Waveguide Filters Versus Substrate Integrated Waveguide (SIW) Bandpass Filters; Software Cost Estimation Using LSTM-RNN; FPGA Implementation of Recursive Algorithm of DCT; Classification of EEG Signals for Hand Gripping Motor Imagery and Hardware Representation of Neural States Using Arduino-Based LED Sensors; Bandwidth and Gain Enhancement Techniques of DRA Antenna; TODD: Time-Aware Opinion Dynamics Diffusion Model for Online Social Networks; spectral Graph Theory-Based Spatio-spectral Filters for Motor Imagery Brain–Computer Interface; Discovering Mutated Motifs in DNA Sequences: A Comparative Analysis; classification of S&P 500 Stocks Based on Correlating Market Trends; blockchain and Industrial Internet of Things: Applications for Industry 4.0; opinion Mining to Aid User Acceptance Testing for Open Beta Versions; a Genesis of an Effective Clustering-Based Fusion Descriptor for an Image Retrieval System; Artificial Neural Network (ANN) to Design Microstrip Transmission Line; MR Image Synthesis Using Generative Adversarial Networks for Parkinson’s Disease Classification; repulsion-Based Grey Wolf Optimizer.","","","Conference review","Final","","Scopus","2-s2.0-85088566660"
"Sindhu N.; Mamatha H.R.","Sindhu, N. (57212673818); Mamatha, H.R. (54906688100)","57212673818; 54906688100","Generating automobile images dynamically from text description","2021","Lecture Notes on Data Engineering and Communications Technologies","53","","","197","211","14","10.1007/978-981-15-5258-8_21","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090373470&doi=10.1007%2f978-981-15-5258-8_21&partnerID=40&md5=24ef7416b9fd77bcbd9929d456b530be","Synthesis of a realistic image from matching visual descriptions provided in the textual format is a challenge that has attracted attention in the recent research community in the field of artificial intelligence. Generation of the image from given text input is a problem, where given a text input, an image which matches text description must be generated. However, a relatively new class of convolutional neural networks referred to as generative adversarial networks (GANs) has provided compelling results in understanding textual features and generating high-resolution images. In this work, the main aim is to generate an automobile image from the given text input using generative adversarial networks and manipulate automobile colour using text-adaptive discriminator. This work involves creating a detailed text description of each image of a car to train the GAN model to produce images. © Springer Nature Singapore Pte Ltd 2021.","Automobiles; Adversarial networks; High resolution image; Realistic images; Recent researches; Text input; Textual features; Textual format; Convolutional neural networks","Discriminator; GANS; Generative adversarial networks; Generator; Text to image synthesis","Book chapter","Final","","Scopus","2-s2.0-85090373470"
"Hertzmann A.","Hertzmann, Aaron (6601954186)","6601954186","Visual indeterminacy in GAN art","2020","Leonardo","53","4","","424","428","4","10.1162/LEON_a_01930","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090646284&doi=10.1162%2fLEON_a_01930&partnerID=40&md5=8da87165e3b079d65874948e392acff4","This paper explores visual indeterminacy as a description for artwork created with Generative Adversarial Networks (GANs). Visual indeterminacy describes images that appear to depict real scenes, but on closer examination, defy coherent spatial interpretation. GAN models seem to be predisposed to producing indeterminate images, and indeterminacy is a key feature of much modern representational art, as well as most GAN art. The author hypothesizes that indeterminacy is a consequence of a powerful-but-imperfect image synthesis model that must combine general classes of objects, scenes and textures. © 2020 ISAST.","","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090646284"
"Yu B.; Zhou L.; Wang L.; Shi Y.; Fripp J.; Bourgeat P.","Yu, Biting (57201496052); Zhou, Luping (23398846800); Wang, Lei (54958774700); Shi, Yinghuan (35241386100); Fripp, Jurgen (13605436500); Bourgeat, Pierrick (8576247900)","57201496052; 23398846800; 54958774700; 35241386100; 13605436500; 8576247900","Sample-Adaptive GANs: Linking Global and Local Mappings for Cross-Modality MR Image Synthesis","2020","IEEE Transactions on Medical Imaging","39","7","8970559","2339","2350","11","10.1109/TMI.2020.2969630","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087468999&doi=10.1109%2fTMI.2020.2969630&partnerID=40&md5=c04148ccdf076d0cb835c37b18891a15","Generative adversarial network (GAN) has been widely explored for cross-modality medical image synthesis. The existing GAN models usually adversarially learn a global sample space mapping from the source-modality to the target-modality and then indiscriminately apply this mapping to all samples in the whole space for prediction. However, due to the scarcity of training samples in contrast to the complicated nature of medical image synthesis, learning a single global sample space mapping that is 'optimal' to all samples is very challenging, if not intractable. To address this issue, this paper proposes sample-adaptive GAN models, which not only cater for the global sample space mapping between the source- and the target-modalities but also explore the local space around each given sample to extract its unique characteristic. Specifically, the proposed sample-adaptive GANs decompose the entire learning model into two cooperative paths. The baseline path learns a common GAN model by fitting all the training samples as usual for the global sample space mapping. The new sample-adaptive path additionally models each sample by learning its relationship with its neighboring training samples and using the target-modality features of these training samples as auxiliary information for synthesis. Enhanced by this sample-adaptive path, the proposed sample-adaptive GANs are able to flexibly adjust themselves to different samples, and therefore optimize the synthesis performance. Our models have been verified on three cross-modality MR image synthesis tasks from two public datasets, and they significantly outperform the state-of-the-art methods in comparison. Moreover, the experiment also indicates that our sample-adaptive strategy could be utilized to improve various backbone GAN models. It complements the existing GANs models and can be readily integrated when needed.  © 1982-2012 IEEE.","Image Processing, Computer-Assisted; Learning systems; Magnetic resonance imaging; Medical imaging; Sampling; Adaptive strategy; Adversarial networks; Auxiliary information; Cross-modality medical images; Image synthesis; Learning models; State-of-the-art methods; Training sample; article; artificial neural network; brain; decomposition; intermethod comparison; machine learning; nuclear magnetic resonance imaging; synthesis; image processing; Mapping","brain; machine learning; magnetic resonance imaging (MRI); Neural networks","Article","Final","","Scopus","2-s2.0-85087468999"
"Zhang Y.; Zhang X.; Zhang Z.; Yu W.; Jiang N.; He G.","Zhang, Yunye (57211359203); Zhang, Xuewen (57217279695); Zhang, Zhiqiang (57206280843); Yu, Wenxin (36610960300); Jiang, Ning (57212426361); He, Gang (36630339700)","57211359203; 57217279695; 57206280843; 36610960300; 57212426361; 36630339700","No-Reference Quality Assessment Based on Spatial Statistic for Generated Images","2020","Communications in Computer and Information Science","1332","","","497","506","9","10.1007/978-3-030-63820-7_57","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097259349&doi=10.1007%2f978-3-030-63820-7_57&partnerID=40&md5=27828f7608c3b67f3f12c0cf1ed31f29","In recent years, generative adversarial networks has made remarkable progress in the field of text-to-image synthesis whose task is to obtain high-quality generated images. Current evaluation metrics in this field mainly evaluate the quality distribution of the generated image dataset rather than the quality of single image itself. With the deepening research of text-to-image synthesis, the quality and quantity of generated images will be greatly improved. There will be a higher demand for generated image evaluation. Therefore, this paper proposes a blind generated image evaluator(BGIE) based on BRISQUE model and sparse neighborhood co-occurrence matrix, which is specially used to evaluate the quality of single generated image. Through experiments, BGIE surpasses all no-reference methods proposed in the past. Compared to VSS method, the surpassing ratio: SRCC is 8.8%, PLCC is 8.8%. By the “One-to-Multi” high-score image screening experiment, it is proved that the BGIE model can screen out best image from multiple images. © 2020, Springer Nature Switzerland AG.","Image enhancement; Quality control; Adversarial networks; Co-occurrence-matrix; Evaluation metrics; No reference methods; Quality assessment; Quality distribution; Screening experiments; Spatial statistics; Image quality","Generated images; Generating adversarial networks; Natural scene statistics; No-Reference assessment; “One-to-multi” screening","Conference paper","Final","","Scopus","2-s2.0-85097259349"
"Wang J.; Zhou W.; Qi G.-J.; Fu Z.; Tian Q.; Li H.","Wang, Jiayu (57204980951); Zhou, Wengang (8979446000); Qi, Guo-Jun (15521333300); Fu, Zhongqian (8710475500); Tian, Qi (7102891959); Li, Houqiang (35956273100)","57204980951; 8979446000; 15521333300; 8710475500; 7102891959; 35956273100","Transformation GaN for unsupervised image synthesis and representation learning","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156940","469","478","9","10.1109/CVPR42600.2020.00055","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094670054&doi=10.1109%2fCVPR42600.2020.00055&partnerID=40&md5=fe0ca528ead685a1f32bee2fdc0c7fd0","Generative Adversarial Networks (GAN) have shown promising performance in image generation and unsupervised learning (USL). In most cases, however, the representations extracted from unsupervised GAN are usually unsatisfactory in other computer vision tasks. By using conditional GAN (CGAN), this problem could be solved to some extent, but the main shortcoming of conditional GAN is the necessity for labeled data. To improve both image synthesis quality and representation learning performance under the unsupervised setting, in this paper, we propose a simple yet effective Transformation Generative Adversarial Networks (TrGAN). In our approach, instead of capturing the joint distribution of image-label pairs p(x, y) as in conditional GAN, we try to estimate the joint distribution of transformed image t(x) and transformation t. Specifically, given a randomly sampled transformation t, we train the discriminator to give an estimate of input transformation, while following the adversarial training scheme of the original GAN. In addition, intermediate feature matching as well as feature-transformation matching methods are introduced to strengthen the regularization on the generated features. To evaluate the quality of both generated samples and extracted representations, extensive experiments are conducted on four public datasets. The experimental results on the quality of both the synthesized images and the extracted representations demonstrate the effectiveness of our method. © 2020 IEEE","Computer vision; Gallium nitride; III-V semiconductors; Adversarial networks; Feature matching; Feature transformations; Image generations; Input transformation; Joint distributions; Learning performance; Synthesized images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85094670054"
"Yang X.; Lin Y.; Wang Z.; Li X.; Cheng K.-T.","Yang, Xin (57203539516); Lin, Yi (57208593735); Wang, Zhiwei (57216175393); Li, Xin (55892214800); Cheng, Kwang-Ting (7402997957)","57203539516; 57208593735; 57216175393; 55892214800; 7402997957","Bi-Modality Medical Image Synthesis Using Semi-Supervised Sequential Generative Adversarial Networks","2020","IEEE Journal of Biomedical and Health Informatics","24","3","8736809","855","865","10","10.1109/JBHI.2019.2922986","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081724042&doi=10.1109%2fJBHI.2019.2922986&partnerID=40&md5=49d4a2358e0b77818e345fb2275843b7","In this paper, we propose a bi-modality medical image synthesis approach based on sequential generative adversarial network (GAN) and semi-supervised learning. Our approach consists of two generative modules that synthesize images of the two modalities in a sequential order. A method for measuring the synthesis complexity is proposed to automatically determine the synthesis order in our sequential GAN. Images of the modality with a lower complexity are synthesized first, and the counterparts with a higher complexity are generated later. Our sequential GAN is trained end-to-end in a semi-supervised manner. In supervised training, the joint distribution of bi-modality images are learned from real paired images of the two modalities by explicitly minimizing the reconstruction losses between the real and synthetic images. To avoid overfitting limited training images, in unsupervised training, the marginal distribution of each modality is learned based on unpaired images by minimizing the Wasserstein distance between the distributions of real and fake images. We comprehensively evaluate the proposed model using two synthesis tasks based on three types of evaluate metrics and user studies. Visual and quantitative results demonstrate the superiority of our method to the state-of-the-art methods, and reasonable visual quality and clinical significance. Code is made publicly available at https://github.com/hust- linyi/Multimodal-Medical-Image-Synthesis. © 2013 IEEE.","Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Multimodal Imaging; Neural Networks, Computer; Prostate; Retina; Supervised Machine Learning; Complex networks; Semi-supervised learning; Adversarial learning; Adversarial networks; Image synthesis; Marginal distribution; State-of-the-art methods; Supervised trainings; Unsupervised training; Wasserstein distance; article; learning; quantitative analysis; synthesis; brain; diagnostic imaging; human; image processing; male; multimodal imaging; nuclear magnetic resonance imaging; pathology; procedures; prostate; retina; supervised machine learning; Medical imaging","Bi-modality; generative adversarial learning; medical image synthesis; semi-supervised learning","Article","Final","","Scopus","2-s2.0-85081724042"
"Chen Z.; Wang C.; Wu H.; Shang K.; Wang J.","Chen, Zhangling (57196393251); Wang, Ce (57209884793); Wu, Huaming (55605704300); Shang, Kun (56583806100); Wang, Jun (57163959600)","57196393251; 57209884793; 55605704300; 56583806100; 57163959600","DMGAN: Discriminative Metric-based Generative Adversarial Networks","2020","Knowledge-Based Systems","192","","105370","","","","10.1016/j.knosys.2019.105370","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077171986&doi=10.1016%2fj.knosys.2019.105370&partnerID=40&md5=708a84f07f33efffec328279f7803e92","With the proposed of Generative Adversarial Networks (GANs), the generative adversarial models have been extensively studied in recent years. Although probability-based methods have achieved remarkable results in image synthesis tasks, there are still some unsolved challenges that are difficult to overcome. In this paper, we propose a novel model, called Discriminative Metric-based Generative Adversarial Networks (DMGANs), for generating real-like samples from the perspective of deep metric learning. To be specific, the generator is trained to generate realistic samples by reducing the distance between real and generated samples. Instead of outputting probability, the discriminator in our model is conducted as a feature extractor, which is well constrained by introducing a combination of identity preserving loss and discriminative loss. Meanwhile, to reduce the identity preserving loss, we calculate the distance between samples and their corresponding center and update these centers during training to improve the stability of our model. In addition, a data-dependent strategy of weight adaption is proposed to further improve the quality of generated samples. Experiments on several datasets illustrate the potential of our model. © 2019 Elsevier B.V.","Artificial intelligence; Knowledge based systems; Software engineering; Adversarial networks; Data dependent; Feature extractor; Image generations; Image synthesis; Metric learning; Weight adaption; Deep learning","Deep metric learning; Generative Adversarial Networks; Image generation; Weight adaption","Article","Final","","Scopus","2-s2.0-85077171986"
"Newson A.; Almansa A.; Gousseau Y.; Ladjal S.","Newson, Alasdair (55549728700); Almansa, Andrés (6602169029); Gousseau, Yann (6507849028); Ladjal, Saïd (24781147400)","55549728700; 6602169029; 6507849028; 24781147400","Processing Simple Geometric Attributes with Autoencoders","2020","Journal of Mathematical Imaging and Vision","62","3","","293","312","19","10.1007/s10851-019-00924-w","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075125386&doi=10.1007%2fs10851-019-00924-w&partnerID=40&md5=c31011ee5d957047fc1789c8097c800d","Image synthesis is a core problem in modern deep learning, and many recent architectures such as autoencoders and generative adversarial networks produce spectacular results on highly complex data, such as images of faces or landscapes. While these results open up a wide range of new, advanced synthesis applications, there is also a severe lack of theoretical understanding of how these networks work. This results in a wide range of practical problems, such as difficulties in training, the tendency to sample images with little or no variability and generalization problems. In this paper, we propose to analyze the ability of the simplest generative network, the autoencoder, to encode and decode two simple geometric attributes: size and position. We believe that, in order to understand more complicated tasks, it is necessary to first understand how these networks process simple attributes. For the first property, we analyze the case of images of centered disks with variable radii. We explain how the autoencoder projects these images to and from a latent space of smallest possible dimension, a scalar. In particular, we describe both the encoding process and a closed-form solution to the decoding training problem in a network without biases and shows that during training, the network indeed finds this solution. We then investigate the best regularization approaches which yield networks that generalize well. For the second property, position, we look at the encoding and decoding of Dirac delta functions, also known as “one-hot” vectors. We describe a handcrafted filter that achieves encoding perfectly and show that the network naturally finds this filter during training. We also show experimentally that the decoding can be achieved if the dataset is sampled in an appropriate manner. We hope that the insights given here will provide better understanding of the precise mechanisms used by generative networks and will ultimately contribute to producing more robust and generalizable networks. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Decoding; Deep learning; Delta functions; Encoding (symbols); Signal encoding; Adversarial networks; Autoencoders; Closed form solutions; Dirac delta function; Encoding and decoding; Generative model; Image synthesis; Regularization approach; Image processing","Autoencoders; Deep learning; Generative models; Image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075125386"
"Coiffier G.; Renard P.; Lefebvre S.","Coiffier, Guillaume (57225904972); Renard, Philippe (7102645765); Lefebvre, Sylvain (24779875700)","57225904972; 7102645765; 24779875700","3D Geological Image Synthesis From 2D Examples Using Generative Adversarial Networks","2020","Frontiers in Water","2","","560598","","","","10.3389/frwa.2020.560598","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112636802&doi=10.3389%2ffrwa.2020.560598&partnerID=40&md5=fae8e0c977425660ec4ca18152f7dbbb","Generative Adversarial Networks (GAN) are becoming an alternative to Multiple-point Statistics (MPS) techniques to generate stochastic fields from training images. But a difficulty for all the training image based techniques (including GAN and MPS) is to generate 3D fields when only 2D training data sets are available. In this paper, we introduce a novel approach called Dimension Augmenter GAN (DiAGAN) enabling GANs to generate 3D fields from 2D examples. The method is simple to implement and is based on the introduction of a random cut sampling step between the generator and the discriminator of a standard GAN. Numerical experiments show that the proposed approach provides an efficient solution to this long lasting problem. Copyright © 2020 Coiffier, Renard and Lefebvre.","","deep learning; generative adversarial network; geology; groundwater; heterogeneity; stochastic model","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112636802"
"Osakabe T.; Tanaka M.; Kinoshita Y.; Kiya H.","Osakabe, Takayuki (57221534642); Tanaka, Miki (57221773880); Kinoshita, Yuma (57191907376); Kiya, Hitoshi (7006251528)","57221534642; 57221773880; 57191907376; 7006251528","CycleGAN without checkerboard artifacts for counter-forensics of fake-image detection","2021","Proceedings of SPIE - The International Society for Optical Engineering","11766","","1176609","","","","10.1117/12.2590977","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103267297&doi=10.1117%2f12.2590977&partnerID=40&md5=55f33911a518aad15e034662a785330c","In this paper, we propose a novel CycleGAN without checkerboard artifacts for counter-forensics of fake-image detection. Recent rapid advances in image manipulation tools and deep image synthesis techniques, such as Generative Adversarial Networks (GANs) have easily generated fake images, so detecting manipulated images has become an urgent issue. Most state-of-the-art forgery detection methods assume that images include checkerboard artifacts which are generated by using DNNs. Accordingly, we propose a novel CycleGAN without any checkerboard artifacts for counter-forensics of fake-mage detection methods for the first time, as an example of GANs without checkerboard artifacts.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Imaging techniques; Adversarial networks; Counter-Forensics; Detection methods; Forgery detections; Image detection; Image manipulation; Image synthesis; State of the art; Digital forensics","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85103267297"
"Saha M.; Guo X.; Sharma A.","Saha, Monjoy (56823066900); Guo, Xiaoyuan (57204680598); Sharma, Ashish (57201794355)","56823066900; 57204680598; 57201794355","TilGAN: GAN for Facilitating Tumor-Infiltrating Lymphocyte Pathology Image Synthesis with Improved Image Classification","2021","IEEE Access","9","","9443091","79829","79840","11","10.1109/ACCESS.2021.3084597","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107236074&doi=10.1109%2fACCESS.2021.3084597&partnerID=40&md5=27a7e27300c6cbd37615e2f379d04010","Tumor-infiltrating lymphocytes (TILs) act as immune cells against cancer tissues. The manual assessment of TILs is usually erroneous, tedious, costly and subject to inter- and intraobserver variability. Machine learning approaches can solve these issues, but they require a large amount of labeled data for model training, which is expensive and not readily available. In this study, we present an efficient generative adversarial network, TilGAN, to generate high-quality synthetic pathology images followed by classification of TIL and non-TIL regions. Our proposed architecture is constructed with a generator network and a discriminator network. The novelty exists in the TilGAN architecture, loss functions, and evaluation techniques. Our TilGAN-generated images achieved a higher Inception score than the real images (2.90 vs. 2.32, respectively). They also achieved a lower kernel Inception distance (1.44) and a lower Fréchet Inception distance (0.312). It also passed the Turing test performed by experienced pathologists and clinicians. We further extended our evaluation studies and used almost one million synthetic data, generated by TilGAN, to train a classification model. Our proposed classification model achieved a 97.83% accuracy, a 97.37% F1-score, and a 97% area under the curve. Our extensive experiments and superior outcomes show the efficiency and effectiveness of our proposed TilGAN architecture. This architecture can also be used for other types of images for image synthesis. © 2013 IEEE.","Image classification; Lymphocytes; Network architecture; Pathology; Tumors; Adversarial networks; Area under the curves; Classification models; Evaluation study; Image synthesis; Intra-observer variability; Machine learning approaches; Proposed architectures; Image enhancement","artificial intelligence; deep learning; Digital pathology; generative adversarial network; lung cancer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85107236074"
"Ding H.; Wu S.; Tang H.; Wu F.; Gao G.; Jing X.-Y.","Ding, Hao (57217103780); Wu, Songsong (24485676900); Tang, Hao (57208238003); Wu, Fei (57220616735); Gao, Guangwei (55352183600); Jing, Xiao-Yuan (7202420489)","57217103780; 24485676900; 57208238003; 57220616735; 55352183600; 7202420489","Cross-View Image Synthesis with Deformable Convolution and Attention Mechanism","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12305 LNCS","","","386","397","11","10.1007/978-3-030-60633-6_32","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093834685&doi=10.1007%2f978-3-030-60633-6_32&partnerID=40&md5=95ff523578e24113adf6dcb6e8047726","Learning to generate natural scenes has always been a daunting task in computer vision. This is even more laborious when generating images with very different views. When the views are very different, the view fields have little overlap or objects are occluded, leading the task very challenging. In this paper, we propose to use Generative Adversarial Networks (GANs) based on a deformable convolution and attention mechanism to solve the problem of cross-view image synthesis (see Fig. 1). It is difficult to understand and transform scenes appearance and semantic information from another view, thus we use deformed convolution in the U-net network to improve the network’s ability to extract features of objects at different scales. Moreover, to better learn the correspondence between images from different views, we apply an attention mechanism to refine the intermediate feature map thus generating more realistic images. A large number of experiments on different size images on the Dayton dataset [1] show that our model can produce better results than state-of-the-art methods. © 2020, Springer Nature Switzerland AG.","Convolution; Deformation; Large dataset; Semantics; Adversarial networks; Attention mechanisms; Different sizes; Image synthesis; Natural scenes; Realistic images; Semantic information; State-of-the-art methods; Computer vision","Attention mechanism; Cross-view image synthesis; Deformable convolution; GANs","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093834685"
"Yu H.; Zhang X.","Yu, Houqiang (57190013980); Zhang, Xuming (55916582900)","57190013980; 55916582900","Synthesis of prostate MR images for classification using capsule network-based GAN model","2020","Sensors (Switzerland)","20","20","5736","1","17","16","10.3390/s20205736","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092389317&doi=10.3390%2fs20205736&partnerID=40&md5=240d6f8b5644eaca83af289974e487cb","Prostate cancer remains a major health concern among elderly men. Deep learning is a state-of-the-art technique for MR image-based prostate cancer diagnosis, but one of major bottlenecks is the severe lack of annotated MR images. The traditional and Generative Adversarial Network (GAN)-based data augmentation methods cannot ensure the quality and the diversity of generated training samples. In this paper, we have proposed a novel GAN model for synthesis of MR images by utilizing its powerful ability in modeling the complex data distributions. The proposed model is designed based on the architecture of deep convolutional GAN. To learn the more equivariant representation of images that is robust to the changes in the pose and spatial relationship of objects in the images, the capsule network is applied to replace CNN used in the discriminator of regular GAN. Meanwhile, the least squares loss has been adopted for both the generator and discriminator in the proposed GAN to address the vanishing gradient problem of sigmoid cross entropy loss function in regular GAN. Extensive experiments are conducted on the simulated and real MR images. The results demonstrate that the proposed capsule network-based GAN model can generate more realistic and higher quality MR images than the compared GANs. The quantitative comparisons show that among all evaluated models, the proposed GAN generally achieves the smallest Kullback–Leibler divergence values for image generation task and provides the best classification performance when it is introduced into the deep learning method for image classification task. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Aged; Humans; Image Processing, Computer-Assisted; Male; Prostate; Deep learning; Diseases; Learning systems; Magnetic resonance imaging; Urology; Adversarial networks; Classification performance; Complex data distributions; Image generations; Quantitative comparison; Spatial relationships; State-of-the-art techniques; Vanishing gradient; aged; diagnostic imaging; human; image processing; male; prostate; Image classification","Capsule network; GAN; Image synthesis; MR image; Prostate cancer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092389317"
"Chen Y.; Xia S.; Zhao J.; Zhou Y.; Niu Q.; Yao R.; Zhu D.","Chen, Ying (57195284871); Xia, Shixiong (55650066900); Zhao, Jiaqi (57138970300); Zhou, Yong (35480110700); Niu, Qiang (22635551200); Yao, Rui (23567304500); Zhu, Dongjun (57204810481)","57195284871; 55650066900; 57138970300; 35480110700; 22635551200; 23567304500; 57204810481","Appearance and shape based image synthesis by conditional variational generative adversarial network","2020","Knowledge-Based Systems","193","","105450","","","","10.1016/j.knosys.2019.105450","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077752454&doi=10.1016%2fj.knosys.2019.105450&partnerID=40&md5=c7008508911cd0cbce096e8d33572889","Person image synthesis based on shape and appearance using deep generative models opens the door in mickle applications, such as person re-identification (ReID) and movie industry. The methods of image synthesis are driven by producing the image of an object directly, which fail to recover spatial deformations when images are generated. In this paper, we present a conditional variational generative adversarial network (CVGAN) to synthesize desired images guided by target shape by modeling the inherent interplay between shape and appearance. Firstly, the shape and appearance of the given images are disentangled by adopting variational inference, which enables us to generate person images with arbitrary shapes. Secondly, to preserve the details and generate photo-realistic images, the Kullback–Leibler (KL) loss is adopted to reduce the gap between the condition image and generated image. Thirdly, to prevent partly gradient vanishing problem for training our framework stably, we propose combined general learning method, where the discriminative network leverages least squares loss. In addition, we experiment on COCO, DeepFashion and Market-1501 datasets, and results demonstrate that VGAN significantly improves the synthesis of images on discriminability, diversity and quality over the existing methods. © 2019 Elsevier B.V.","Learning systems; Least squares approximations; Adversarial networks; Discriminative networks; Generative model; Image synthesis; Person re identifications; Photorealistic images; Spatial deformation; Variational inference; Image enhancement","Deep generative models; Generative adversarial network; Image synthesis; Variational inference","Article","Final","","Scopus","2-s2.0-85077752454"
"Hu M.; Li J.; Hu M.; Hu T.","Hu, Mengxiao (57219491996); Li, Jinlong (57192272098); Hu, Maolin (57219685323); Hu, Tao (57219689701)","57219491996; 57192272098; 57219685323; 57219689701","Hierarchical modes exploring in generative adversarial networks","2020","AAAI 2020 - 34th AAAI Conference on Artificial Intelligence","","","","10981","10988","7","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106399067&partnerID=40&md5=59527ae60aa8985b01c6b7fb99ebacaf","In conditional Generative Adversarial Networks (cGANs), when two different initial noises are concatenated with the same conditional information, the distance between their outputs is relatively smaller, which makes minor modes likely to collapse into large modes. To prevent this happen, we proposed a hierarchical mode exploring method to alleviate mode collapse in cGANs by introducing a diversity measurement into the objective function as the regularization term. We also introduced the Expected Ratios of Expansion (ERE) into the regularization term, by minimizing the sum of differences between the real change of distance and ERE, we can control the diversity of generated images w.r.t specific-level features. We validated the proposed algorithm on four conditional image synthesis tasks including categorical generation, paired and un-paired image translation and text-to-image generation. Both qualitative and quantitative results show that the proposed method is effective in alleviating the mode collapse problem in cGANs, and can control the diversity of output images w.r.t specific-level features.  © AAAI 2020 - 34th AAAI Conference on Artificial Intelligence. All Rights Reserved.","Adversarial networks; Diversity measurement; Hierarchical mode; Image generations; Image translation; Objective functions; Quantitative result; Regularization terms; Artificial intelligence","","Conference paper","Final","","Scopus","2-s2.0-85106399067"
"Ma B.; Zhao Y.; Yang Y.; Zhang X.; Dong X.; Zeng D.; Ma S.; Li S.","Ma, Baoqiang (57217078153); Zhao, Yan (56305783700); Yang, Yujing (57208670210); Zhang, Xiaohui (57225101058); Dong, Xiaoxi (57219624970); Zeng, Debin (57217077161); Ma, Siyu (57219626742); Li, Shuyu (8781450200)","57217078153; 56305783700; 57208670210; 57225101058; 57219624970; 57217077161; 57219626742; 8781450200","MRI image synthesis with dual discriminator adversarial learning and difficulty-aware attention mechanism for hippocampal subfields segmentation","2020","Computerized Medical Imaging and Graphics","86","","101800","","","","10.1016/j.compmedimag.2020.101800","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094326545&doi=10.1016%2fj.compmedimag.2020.101800&partnerID=40&md5=45095a44ef52255bab9cdd79f0969ab7","Background and objective: Hippocampal subfields (HS) segmentation accuracy on high resolution (HR) MRI images is higher than that on low resolution (LR) MRI images. However, HR MRI data collection is more expensive and time-consuming. Thus, we intend to generate HR MRI images from the corresponding LR MRI images for HS segmentation. Methods and results: To generate high-quality HR MRI hippocampus region images, we use a dual discriminator adversarial learning model with difficulty-aware attention mechanism in hippocampus regions (da-GAN). A local discriminator is applied in da-GAN to evaluate the visual quality of hippocampus region voxels of the synthetic images. And the difficulty-aware attention mechanism based on the local discriminator can better model the generation of hard-to-synthesis voxels in hippocampus regions. Additionally, we design a SemiDenseNet model with 3D Dense CRF postprocessing and an Unet-based model to perform HS segmentation. The experiments are implemented on Kulaga-Yoskovitz dataset. Compared with conditional generative adversarial network (c-GAN), the PSNR of generated HR T2w images acquired by our da-GAN achieves 0.406 and 0.347 improvement in left and right hippocampus regions. When using two segmentation models to segment HS, the DSC values achieved on the generated HR T1w and T2w images are both improved than that on LR T1w images. Conclusion: Experimental results show that da-GAN model can generate higher-quality MRI images, especially in hippocampus regions, and the generated MRI images can improve HS segmentation accuracy. © 2020 Elsevier Ltd","Cerebral Cortex; Hippocampus; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; 3D modeling; Brain; Discriminators; Image segmentation; Magnetic resonance imaging; Adversarial learning; Adversarial networks; Attention mechanisms; High resolution; Segmentation accuracy; Segmentation models; Synthetic images; Visual qualities; adult; Article; artificial neural network; attention; brain region; conditional generative adversarial network; controlled study; data synthesis; diagnostic accuracy; discrimination learning; female; human; human experiment; image analysis; image processing; image quality; image segmentation; machine learning; male; neuroimaging; nuclear magnetic resonance imaging; priority journal; right hippocampus; SemiDenseNet model; three-dimensional imaging; Unet model; brain cortex; diagnostic imaging; hippocampus; image processing; Image enhancement","Adversarial learning; Difficulty-aware attention; Dual discriminator; Hippocampal subfields segmentation; MRI image synthesis","Article","Final","","Scopus","2-s2.0-85094326545"
"Balakrishnan G.; Xiong Y.; Xia W.; Perona P.","Balakrishnan, Guha (57198394458); Xiong, Yuanjun (56119215300); Xia, Wei (57219635416); Perona, Pietro (57217508533)","57198394458; 56119215300; 57219635416; 57217508533","Towards Causal Benchmarking of Bias in Face Analysis Algorithms","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12363 LNCS","","","547","563","16","10.1007/978-3-030-58523-5_32","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097840937&doi=10.1007%2f978-3-030-58523-5_32&partnerID=40&md5=a34aed9d9129a1ac464e934dd23dfbb6","Measuring algorithmic bias is crucial both to assess algorithmic fairness, and to guide the improvement of algorithms. Current bias measurement methods in computer vision are based on observational datasets, and so conflate algorithmic bias with dataset bias. To address this problem we develop an experimental method for measuring algorithmic bias of face analysis algorithms, which directly manipulates the attributes of interest, e.g., gender and skin tone, in order to reveal causal links between attribute variation and performance change. Our method is based on generating synthetic image grids that differ along specific attributes while leaving other attributes constant. Crucially, we rely on the perception of human observers to control for synthesis inaccuracies when measuring algorithmic bias. We validate our method by comparing it to a traditional observational bias analysis study in gender classification algorithms. The two methods reach different conclusions. While the observational method reports gender and skin color biases, the experimental method reveals biases due to gender, hair length, age, and facial hair. We also show that our synthetic transects allow for more straightforward bias analysis on minority and intersectional groups. © 2020, Springer Nature Switzerland AG.","Artificial intelligence; Computer science; Computers; Experimental methods; Face analysis; Gender classification; Hair length; Human observers; Measurement methods; Observational method; Synthetic images; Computer vision","Bias; Causality; Counterfactuals; Faces; Fairness; Generative adversarial networks (GANs); Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097840937"
"Fontanini T.; Iotti E.; Donati L.; Prati A.","Fontanini, Tomaso (57193277601); Iotti, Eleonora (57188548251); Donati, Luca (57201861769); Prati, Andrea (7003595956)","57193277601; 57188548251; 57201861769; 7003595956","MetalGAN: Multi-domain label-less image synthesis using cGANs and meta-learning","2020","Neural Networks","131","","","185","200","15","10.1016/j.neunet.2020.07.031","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089213260&doi=10.1016%2fj.neunet.2020.07.031&partnerID=40&md5=54f7b22a30a7efc5b9e2dce492026949","Image synthesis is currently one of the most addressed image processing topic in computer vision and deep learning fields of study. Researchers have tackled this problem focusing their efforts on its several challenging problems, e.g. image quality and size, domain and pose changing, architecture of the networks, and so on. Above all, producing images belonging to different domains by using a single architecture is a very relevant goal for image generation. In fact, a single multi-domain network would allow greater flexibility and robustness in the image synthesis task than other approaches. This paper proposes a novel architecture and a training algorithm, which are able to produce multi-domain outputs using a single network. A small portion of a dataset is intentionally used, and there are no hard-coded labels (or classes). This is achieved by combining a conditional Generative Adversarial Network (cGAN) for image generation and a Meta-Learning algorithm for domain switch, and we called our approach MetalGAN. The approach has proved to be appropriate for solving the multi-domain label-less problem and it is validated on facial attribute transfer, using CelebA dataset. © 2020 Elsevier Ltd","Automated Facial Recognition; Humans; Image Processing, Computer-Assisted; Machine Learning; Deep learning; Image processing; Network architecture; Adversarial networks; Different domains; Image generations; Image synthesis; Learning fields; Multidomain networks; Novel architecture; Training algorithms; article; face; learning algorithm; synthesis; human; image processing; machine learning; procedures; Learning algorithms","Few-shots; Generative adversarial networks; Image-to-image translation; Meta-learning; Multi-domain","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85089213260"
"Tan H.; Liu X.; Liu M.; Yin B.; Li X.","Tan, Hongchen (57209272265); Liu, Xiuping (36910875600); Liu, Meng (56571781700); Yin, Baocai (8616230700); Li, Xin (57201432287)","57209272265; 36910875600; 56571781700; 8616230700; 57201432287","KT-GAN: Knowledge-Transfer Generative Adversarial Network for Text-to-Image Synthesis","2021","IEEE Transactions on Image Processing","30","","9210842","1275","1290","15","10.1109/TIP.2020.3026728","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098106749&doi=10.1109%2fTIP.2020.3026728&partnerID=40&md5=35d9addfa1170d9d6028dd324747bc5b","This paper presents a new framework, Knowledge-Transfer Generative Adversarial Network (KT-GAN), for fine-grained text-to-image generation. We introduce two novel mechanisms: an Alternate Attention-Transfer Mechanism (AATM) and a Semantic Distillation Mechanism (SDM), to help generator better bridge the cross-domain gap between text and image. The AATM updates word attention weights and attention weights of image sub-regions alternately, to progressively highlight important word information and enrich details of synthesized images. The SDM uses the image encoder trained in the Image-to-Image task to guide training of the text encoder in the Text-to-Image task, for generating better text features and higher-quality images. With extensive experimental validation on two public datasets, our KT-GAN outperforms the baseline method significantly, and also achieves the competive results over different evaluation metrics.  © 1992-2012 IEEE.","Distillation; Knowledge management; Semantics; Signal encoding; Space division multiple access; Adversarial networks; Baseline methods; Evaluation metrics; Experimental validations; Image generations; Knowledge transfer; Synthesized images; Transfer mechanisms; article; attention; distillation; synthesis; Image processing","alternate attention-transfer mechanism; Generative adversarial network; knowledge distillation; Text-to-Image Generation","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85098106749"
"Gencoglu S.; Keles H.Y.","Gencoglu, Sinan (57215327111); Keles, Hacer Yalim (36185569500)","57215327111; 36185569500","Sign Language Video Synthesis using Skeleton Sequence; [Skelet Sekans ile aret Dili Videosu Sentezleme]","2020","2020 28th Signal Processing and Communications Applications Conference, SIU 2020 - Proceedings","","","9302436","","","","10.1109/SIU49456.2020.9302436","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100307171&doi=10.1109%2fSIU49456.2020.9302436&partnerID=40&md5=9b50b733024d538cd81164244436bd54","Generative Adversarial Networks (GANs) enable generating realistic synthetic images. However, majority of the research in this domain focus on image-to-image synthesis problem. The aim of this study is to develop a model that encodes high quality video frames, with true motion dynamics, using only a reference image frame and a skeleton sequence. In this context, Ankara University Turkish Sign Language dataset is used to synthesize new sign videos using a given signer frame as a reference and a skeleton stream. To solve this challenging problem, a conditional generative adversarial network (GAN) is designed, where skeletal data is used as a condition. Using the trained model, we are able to generate sign video streams with the given signer, where the motion dynamics are successfully and fluently encoded in the video. Moreover, we evaluated the quality of the generated images using Fr chet Inception Distance (FID) metric; the FID score is 26. © 2020 IEEE.","Computer hardware description languages; Musculoskeletal system; Adversarial networks; High quality video; Image synthesis; Motion dynamics; Reference image; Sign language; Synthetic images; Video synthesis; Signal processing","conditional generative adversarial networks; convolutional neural networks; Generative adversarial networks; video to video synthesis.","Conference paper","Final","","Scopus","2-s2.0-85100307171"
"Attia M.; Hossny M.; Zhou H.; Nahavandi S.; Asadi H.; Yazdabadi A.","Attia, Mohamed (57198059142); Hossny, Mohammed (23667683300); Zhou, Hailing (57198957382); Nahavandi, Saeid (55992860000); Asadi, Hamed (55246754200); Yazdabadi, Anousha (25653208600)","57198059142; 23667683300; 57198957382; 55992860000; 55246754200; 25653208600","Realistic hair simulator for skin lesion images: A novel benchemarking tool","2020","Artificial Intelligence in Medicine","108","","101933","","","","10.1016/j.artmed.2020.101933","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087868427&doi=10.1016%2fj.artmed.2020.101933&partnerID=40&md5=7ed88cca7ede5f9bf472e91c984ee6ae","Automated skin lesion analysis is one of the trending fields that has gained attention among the dermatologists and health care practitioners. Skin lesion restoration is an essential pre-processing step for lesion enhancements for accurate automated analysis and diagnosis by both dermatologists and computer-aided diagnosis tools. Hair occlusion is one of the most popular artifacts in dermatoscopic images. It can negatively impact the skin lesions diagnosis by both dermatologists and automated computer diagnostic tools. Digital hair removal is a non-invasive method for image enhancement for decrease the hair-occlusion artifact in previously captured images. Several hair removal methods were proposed for skin delineation and removal without standardized benchmarking techniques. Manual annotation is one of the main challenges that hinder the validation of these proposed methods on a large number of images or against benchmarking datasets for comparison purposes. In the presented work, we propose a photo-realistic hair simulator based on context-aware image synthesis using image-to-image translation techniques via conditional adversarial generative networks for generation of different hair occlusions in skin images, along with ground-truth mask for hair location. Hair-occluded image is synthesized using the latent structure of any input hair-free image by deep encoding the input image into a latent vector of features. The locations of required hair are highlighted using white pixels on the input image. Then, these deep encoded features are used to reconstruct the synthetic highly realistic hair-occluded image. Besides, we explored using three loss functions including L1-norm, L2-norm and structural similarity index (SSIM) to maximize the image synthesis visual quality. For the evaluation of the generated samples, the t-SNE feature mapping and Bland–Altman test are used as visualization tools for the experimental results. The results show the superior performance of our proposed method compared to previous methods for hair synthesis with plausible colours and preserving the integrity of the lesion texture. The proposed method can be used to generate benchmarking datasets for comparing the performance of digital hair removal methods. The code is available online at: https://github.com/attiamohammed/realhair. © 2020 Elsevier B.V.","Artifacts; Diagnosis, Computer-Assisted; Humans; Image Processing, Computer-Assisted; Melanoma; Automation; Benchmarking; Computer aided analysis; Computer aided diagnosis; Dermatology; Large dataset; Noninvasive medical procedures; Textures; Automated analysis; Benchmarking techniques; Manual annotation; Noninvasive methods; Pre-processing step; Skin lesion images; Structural similarity indices (SSIM); Visualization tools; Article; hair color; hair removal; image enhancement; image quality; limit of agreement; melanoma; priority journal; qualitative analysis; quantitative analysis; simulation; skin defect; artifact; computer assisted diagnosis; human; image processing; melanoma; Image enhancement","Computer-aided diagnosis; Dermatology; Generative adversarial networks; Melanoma; Synthetic","Article","Final","","Scopus","2-s2.0-85087868427"
"Ruffino C.; Hérault R.; Laloy E.; Gasso G.","Ruffino, Cyprien (57210726275); Hérault, Romain (15057961600); Laloy, Eric (23100239900); Gasso, Gilles (23392497400)","57210726275; 15057961600; 23100239900; 23392497400","Pixel-wise conditioned generative adversarial networks for image synthesis and completion","2020","Neurocomputing","416","","","218","230","12","10.1016/j.neucom.2019.11.116","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083883613&doi=10.1016%2fj.neucom.2019.11.116&partnerID=40&md5=c9682a7cd0617088cd1a6dd381e6259f","Generative Adversarial Networks (GANs) have proven successful for unsupervised image generation. Several works have extended GANs to image inpainting by conditioning the generation with parts of the image to be reconstructed. Despite their success, these methods have limitations in settings where only a small subset of the image pixels is known beforehand. In this paper we investigate the effectiveness of conditioning GANs when very few pixel values are provided. We propose a modelling framework which results in adding an explicit cost term to the GAN objective function to enforce pixel-wise conditioning. We investigate the influence of this regularization term on the quality of the generated images and the fulfillment of the given pixel constraints. Using the recent PacGAN technique, we ensure that we keep diversity in the generated samples. Conducted experiments on FashionMNIST show that the regularization term effectively controls the trade-off between quality of the generated images and the conditioning. Experimental evaluation on the CIFAR-10 and CelebA datasets evidences that our method achieves accurate results both visually and quantitatively in term of Fréchet Inception Distance, while still enforcing the pixel conditioning. We also evaluate our method on a texture image generation task using fully-convolutional networks. As a final contribution, we apply the method to a classical geological simulation application. © 2020 Elsevier B.V.","Convolutional neural networks; Economic and social effects; Quality control; Textures; Adversarial networks; Convolutional networks; Experimental evaluation; Image generations; Modelling framework; Objective functions; Regularization terms; Simulation applications; article; quantitative analysis; simulation; synthesis; Pixels","Conditional GAN; Deep generative models; Generative adversarial networks","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85083883613"
"Shi J.; Wang L.; Wang S.; Chen Y.; Wang Q.; Wei D.; Liang S.; Peng J.; Yi J.; Liu S.; Ni D.; Wang M.; Zhang D.; Shen D.","Shi, Jun (7404495816); Wang, Linlin (57215963826); Wang, Shanshan (36761739900); Chen, Yanxia (57212010273); Wang, Qian (57192157811); Wei, Dongming (57211428968); Liang, Shujun (57197835138); Peng, Jialin (55265406900); Yi, Jiajin (57217028521); Liu, Shengfeng (57207199099); Ni, Dong (26023577500); Wang, Mingliang (57195685852); Zhang, Daoqiang (7405356869); Shen, Dinggang (7401738392)","7404495816; 57215963826; 36761739900; 57212010273; 57192157811; 57211428968; 57197835138; 55265406900; 57217028521; 57207199099; 26023577500; 57195685852; 7405356869; 7401738392","Applications of deep learning in medical imaging: a survey; [深度学习在医学影像中的应用综述]","2020","Journal of Image and Graphics","25","10","","1953","1981","28","10.11834/jig.200255","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093986045&doi=10.11834%2fjig.200255&partnerID=40&md5=c4454f4e3a3ade717502b5f4e92d8b48","Deep learning can automatically learn from a large amount of data to obtain effective feature representations, thereby effectively improving the performance of various machine learning tasks. It has been widely used in various fields of medical imaging. Smart healthcare has become an important application area of deep learning, which is an effective approach to solve the following clinical problems: 1) given the limited medical resources, the experienced radiologists are not fully available, which cannot satisfy the fast development of the clinical requirement; 2) the lack of experienced radiologists, which cannot satisfy the fast increase of medical demand. At present, deep learning-based intelligent medical imaging systems are the typical scenarios in smart healthcare. This paper primarily reviews the applications of deep learning methods in various applications using four major clinical imaging techniques (i.e., X-ray, ultrasound, computed tomography(CT), and magnetic resonance imaging(MRI)). These works cover the whole pipeline of medical imaging, including reconstruction, detection, segmentation, registration, and computer-aided diagnosis (CAD). The reviews on medical image reconstruction focus on both MRI reconstruction and low-dose CT reconstruction on the basis of deep learning. Deep learning methods for MRI reconstruction can be divided into two categories: 1) data-driven end-to-end methods, 2) model-based methods. The low-dose CT reconstruction primarily introduces methods on the basis of convolutional neural networks and generative adversarial networks. In addition, deep learning methods for ultrasound imaging, medical image synthesis, and medical image super-resolution are reviewed. The reviews on lesion detection primarily focuses on the deep learning methods for lung lesions detection using CT, the deep learning detection model for tumor lesions, and the deep learning methods for the general lesion area detection. At present, deep learning has been widely used in medical image segmentation tasks, and its performance is significantly improved compared with traditional image segmentation methods. Most deep learning segmentation methods are typical data-driven machine learning models. We review supervised models, semi-supervised models, and self-supervised models with regard to the amount of labeled data and annotation. Medical images contain rich anatomical information, which enhances the performance of deep learning models with different supervision. Deep learning models incorporating prior knowledge are also reviewed. Medical image registration consistency is a difficult task in the field of medical image analysis. Deep learning has become a breakthrough to improve the performance of medical image registration. The end-to-end network structures produce high-precision registration results and have become a hotspot in the field of image registration. Compared with the conventional methods, the deep learning methods for medical image registration have a significant improvement in registration performance. According to the different supervision in the training procedure, this paper divides the deep learning methods for medical image registration into three modes: fully supervised methods, unsupervised methods, and weakly supervised methods. Computer-aided diagnosis is another application of deep learning in the field of medical imaging. This paper summarizes the deep learning methods on CAD with different supervision and the CAD works on the basis of multi-modality medical images. Notably, although deep learning methods have been applied in medical imaging, several challenges are still identified. For example, the small-sample size problem is common in medical imaging analysis. Advanced machine learning methods, including weakly supervised learning, transfer learning, few-shot learning, self-supervised learning, and increase learning, can help alleviate this problem. In addition, the data annotation of medical images is a problem that seriously restricts the extensive and in-depth application of deep learning, and extensive research on automatic data labeling must be carried out. Interpretability of the deep neural networks is also important in medical image analysis. Improving the interpretability of a deep neural network has always been a difficult point, and in-depth research must be carried out in this area. Furthermore, carrying out human-computer collaboration in medical care is important. The lightweight deep neural network is easy to deploy into portable medical devices, giving portable devices more powerful functions, which is also an important research direction. Deep learning has been successful in various tasks in medical imaging analysis. New methods must be developed for its further application in intelligent medical products. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","Computer-aided diagnosis(CAD); Deep learning; Image reconstruction; Image registration; Image segmentation; Lesion detection; Medical imaging","Review","Final","","Scopus","2-s2.0-85093986045"
"Sheng X.; Xu L.; Guo J.; Liu J.; Zhao R.; Xu Y.","Sheng, Xin (57218925308); Xu, Linli (23669631700); Guo, Junliang (57202367651); Liu, Jingchang (57208600960); Zhao, Ruoyu (57223961539); Xu, Yinlong (7406447070)","57218925308; 23669631700; 57202367651; 57208600960; 57223961539; 7406447070","Introvnmt: An introspective model for variational neural machine translation","2020","AAAI 2020 - 34th AAAI Conference on Artificial Intelligence","","","","8830","8837","7","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106616433&partnerID=40&md5=e1518a5e84b6c92032aa6ac29b5d76f4","We propose a novel introspective model for variational neural machine translation (IntroVNMT) in this paper, inspired by the recent successful application of introspective variational autoencoder (IntroVAE) in high quality image synthesis. Different from the vanilla variational NMT model, IntroVNMT is capable of improving itself introspectively by evaluating the quality of the generated target sentences according to the high-level latent variables of the real and generated target sentences. As a consequence of introspective training, the proposed model is able to discriminate between the generated and real sentences of the target language via the latent variables generated by the encoder of the model. In this way, IntroVNMT is able to generate more realistic target sentences in practice. In the meantime, IntroVNMT inherits the advantages of the variational autoencoders (VAEs), and the model training process is more stable than the generative adversarial network (GAN) based models. Experimental results on different translation tasks demonstrate that the proposed model can achieve significant improvements over the vanilla variational NMT model. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Computational linguistics; Computer aided language translation; Learning systems; Adversarial networks; Autoencoders; High quality images; Latent variable; Machine translations; Model training; Target language; Artificial intelligence","","Conference paper","Final","","Scopus","2-s2.0-85106616433"
"Zeno B.; Kalinovskiy I.; Matveev Y.; Alkhatib B.","Zeno, Bassel (57191858933); Kalinovskiy, Ilya (57205629516); Matveev, Yuri (7006613471); Alkhatib, Bassel (55568960400)","57191858933; 57205629516; 7006613471; 55568960400","CtrlFaceNet: Framework for geometric-driven face image synthesis","2020","Pattern Recognition Letters","138","","","527","533","6","10.1016/j.patrec.2020.08.026","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090243492&doi=10.1016%2fj.patrec.2020.08.026&partnerID=40&md5=b1047657dcba2900d39b9da82fe33bc2","In this work, we introduce a novel framework based on Generative Adversarial Networks to control the pose, expression and facial features of a given face image using another face image. It can then be used for data augmentation, pose invariant face identification, face verification, and lightweight image editing. Generating new realistic face images with controllable poses, facial features, and expressions is a challenging generative learning problem due to skin tone variations, the identity preservation problem, necessity to deal with unseen large poses, and the absence of ground truth images in the training process. We make the following contributions. First, we present a network, CtrlFaceNet that can control a source face image while preserving the identity and skin tone. Second, we introduce a method for training the framework in fully self-supervised mode using a large-scale dataset of unconstrained face images. Third, we show that the style loss function can be used to preserve the skin tone of the source image. The experimental results show that our approach outperforms all other baselines. Furthermore, to the best of our knowledge, we are the first to train such a model using large-scale dataset of unconstrained face images. © 2020","Large dataset; Adversarial networks; Data augmentation; Face identification; Face image synthesis; Large-scale dataset; Learning problem; Pose invariant; Training process; Face recognition","Generative adversarial networks; Image editing; Self-supervised","Article","Final","","Scopus","2-s2.0-85090243492"
"Wu L.; Sun R.; Kan J.; Gao J.","Wu, Liuwei (57214317862); Sun, Rui (35774555500); Kan, Junsong (57214322410); Gao, Jun (57189890073)","57214317862; 35774555500; 57214322410; 57189890073","Double dual generative adversarial networks for cross-age sketch-to-photo translation; [双重对偶生成对抗网络的跨年龄素描-照片转换]","2020","Journal of Image and Graphics","25","4","","732","744","12","10.11834/jig.190329","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091003434&doi=10.11834%2fjig.190329&partnerID=40&md5=c8dcc623ef0f1d85b6ec1f4cf6c5c299","Objective: Sketch-to-photo translation has a wide range of applications in the public safety and digital entertainment area. For example, it can help the police find fugitives and missing children or generate an avatar of social account. The existing algorithm of sketch-to-photo translation can only translate sketches into photos under the same age group. However, it does not solve the problem of cross-age sketch-to-photo translation. Cross-age sketch-to-photo translation characters also have a wide range of applications. For example, when the sketch image of the police at hand is out of date after a long time, the task can generate an aging photo based on outdated sketches to help the police find the suspect. Given that paired cross-age sketches and photo images are difficult to obtain, no data sets are available. To solve this problem, this study combines dual generative adversarial networks (DualGANs) and identity-preserved conditional generative adversarial networks (IPCGANs) to propose double dual generative adversarial networks (D-DualGANs). Method: DualGANs have the advantage of two-way conversion without the need to pair samples. However, it can only achieve a two-way conversion of an attribute and cannot achieve the conversion of two attributes at the same time. IPCGANs can complete the aging or rejuvenation of the face while retaining the personalized features of the person's face, but it cannot complete the two-way change between different age groups. This article considers the span of age as a domain conversion problem and considers the cross-age sketch-to-photo translation task as a problem of style and age conversion. We combined the characteristics of the above network to build D-DualGANs by setting up four generators and four discriminators to combat training. The method not only learns the mapping of the sketch domain to the photo domain and the mapping of the photo domain to the sketch domain but also learns the mapping of the source age group to the target age group and the mapping of the target age group to the original age group. In D-DualGANs, the original sketch image or the original photo image is successively completed by the four generators to achieve the four-domain conversion to obtain cross-age photo images or cross-age sketch images and reconstructed same-age sketch images or reconstructed same-age photo images. The generator is optimized by measuring the distance between the generated cross-age image and the reconstructed image of the same age by full reconstruction loss. We also used the identity retention module to introduce reconstructed identity loss to maintain the personalized features of the face. Eventually, the input sketch images and photo images from the different age groups are converted into photos and sketches of the other age group. This method does not require paired samples, currently overcoming the problem of lack of paired samples of cross-age sketches and photos. Result: The experiments combine the images of the CUFS(CUHK(Chinese University of Hong Kong)-face sketeh database) and CUSFS(CUHK face sketch face recognition technology database) sketch photo datasets and produces corresponding age labels for each image based on the results of the age estimation software. According to the age label, the sketch and photo images in the datasets are divided into three groups of 1130, 3150, and 50+, and each age group is evenly distributed. Six D-DualGAN models were trained to realize the two-two conversion between sketches and photographic images of the three age groups, namely, the 1130 sketch and the 3150 photo, the 1130 sketch and the 50+ photo, the 3150 sketch and the 1130 photo, the 3150 sketch and the 50+ photo, the 50+ sketch and the 3150 photo, the 50+ sketch and the 1130 photo. As there is little research on cross-age sketch-to-photo translation. To illustrate the effectiveness of the method, the generated image obtained by this method is compared with the generated image obtained by DualGANs and then by IPCGANs. Our images are of good quality with less distortion and noise. Using an age estimate CNN to judge the age accuracy of the generated image, the mean absolute error (MAE) of our method is lower than the direct addition of DualGANs and IPCGANs. To evaluate the similarity between the generated image and the original image, we invite volunteers unrelated to this study to determine whether the generated image is the same as the original image. The results show that the resulting aging image is similar, and the resulting younger image is poor. Among them, the 3150 photos generated by 1130 sketches are the same as the original image. Conclusion: D-DualGANs proposed in this study provides knowledge on mapping and inverse mapping between the sketch domain and the photo domain and the mapping and inverse mapping between the different age groups. It also converts both the age and style properties of the input image. Photo images of different ages can be generated from a given sketch image. Through the introduced reconstructed identity loss and complete identity loss, the generated image effectively retains the identity features of the original image. Thus, the problem of image cross-style and cross-age translation is solved effectively. D-DualGANs can be used as a general framework to solve other computer vision tasks that need to complete two attribute conversions at the same time. However, some shortcomings are still observed in this method. For example, conversion between the different age groups requires training different models, such as to achieve 1130 sketches to 3150 photos and 1130 sketches to 50+ photos. To train two D-DualGAN models separately is necessary. This work is cumbersome in practical applications and can be used as an improvement direction in the future so that training a network model can achieve conversion between all age groups. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","Face aging; Face sketch synthesis; Generative adversarial networks(GAN); Heterogeneous image synthesis; Image translation","Article","Final","","Scopus","2-s2.0-85091003434"
"Jin W.; Sadiqbatcha S.; Sun Z.; Zhou H.; Tan S.X.-D.","Jin, Wentian (57217589154); Sadiqbatcha, Sheriff (57192573441); Sun, Zeyu (57190065302); Zhou, Han (57197824839); Tan, Sheldon X.-D. (57220556915)","57217589154; 57192573441; 57190065302; 57197824839; 57220556915","EM-GAN: Data-Driven Fast Stress Analysis for Multi-Segment Interconnects","2020","Proceedings - IEEE International Conference on Computer Design: VLSI in Computers and Processors","2020-October","","9283508","296","303","7","10.1109/ICCD50377.2020.00057","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098854761&doi=10.1109%2fICCD50377.2020.00057&partnerID=40&md5=f8fe40612815a13e24c2e5ab0a62c276","Electromigration (EM) analysis for complicated interconnects requires the solving of partial differential equations, which is expensive. In this paper, we propose a fast transient hydrostatic stress analysis for EM failure assessment for multisegment interconnects using generative adversarial networks (GANs). Our work is inspired by the image synthesis and feature of generative deep neural networks. The stress evaluation of multi-segment interconnects, modeled by partial differential equations, can be viewed as time-varying 2D-images-to-image problem where the input is the multi-segment interconnects topology with current densities and the output is the EM stress distribution in those wire segments at the given aging time. We show that the conditional GAN can be exploited to attend the temporal dynamics for modeling the time-varying dynamic systems like stress evolution over time. The resulting algorithm, called EM-GAN, can quickly give accurate stress distribution of a general multi-segment wire tree for a given aging time, which is important for full-chip fast EM failure assessment. Our experimental results show that the EM-GAN shows 6.6% averaged error compared to COMSOL simulation results with orders of magnitude speedup. It also delivers 8.3× speedup over state-of-the-art analytic based EM analysis solver. © 2020 IEEE.","Deep neural networks; Integrated circuit interconnects; Partial differential equations; Stress analysis; Stress concentration; Trees (mathematics); Adversarial networks; Failure assessment; Hydrostatic stress; Orders of magnitude; Stress evaluations; Stress evolution; Temporal dynamics; Time-varying dynamics; Image segmentation","electromigration; generative adversarial networks; hydrostatic stress analysis","Conference paper","Final","","Scopus","2-s2.0-85098854761"
"Dong J.; Liu C.; Man P.; Zhao G.; Wu Y.; Lin Y.","Dong, Jiale (57221934021); Liu, Caiwei (57221931426); Man, Panpan (57218295264); Zhao, Guohua (57208839855); Wu, Yaping (57195319814); Lin, Yusong (52264164500)","57221934021; 57221931426; 57218295264; 57208839855; 57195319814; 52264164500","Fproi-GAN with Fused Regional Features for the Synthesis of High-Quality Paired Medical Images","2021","Journal of Healthcare Engineering","2021","","6678031","","","","10.1155/2021/6678031","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105732452&doi=10.1155%2f2021%2f6678031&partnerID=40&md5=7ba770d430d97fe73d6e2d7648a29e14","The use of medical image synthesis with generative adversarial networks (GAN) is effective for expanding medical samples. The structural consistency between the synthesized and actual image is a key indicator of the quality of the synthesized image, and the region of interest (ROI) of the synthesized image is related to its usability, and these parameters are the two key issues in image synthesis. In this paper, the fusion-ROI patch GAN (Fproi-GAN) model was constructed by incorporating a priori regional feature based on the two-stage cycle consistency mechanism of cycleGAN. This model has improved the tissue contrast of ROI and achieved the pairwise synthesis of high-quality medical images and their corresponding ROIs. The quantitative evaluation results in two publicly available datasets, INbreast and BRATS 2017, show that the synthesized ROI images have a DICE coefficient of 0.981 ± 0.11 and a Hausdorff distance of 4.21 ± 2.84 relative to the original images. The classification experimental results show that the synthesized images can effectively assist in the training of machine learning models, improve the generalization performance of prediction models, and improve the classification accuracy by 4% and sensitivity by 5.3% compared with the cycleGAN method. Hence, the paired medical images synthesized using Fproi-GAN have high quality and structural consistency with real medical images.  © 2021 Jiale Dong et al.","Humans; Image Processing, Computer-Assisted; Machine Learning; Image segmentation; Medical imaging; Predictive analytics; Adversarial networks; Classification accuracy; Generalization performance; Hausdorff distance; Machine learning models; Quantitative evaluation; Synthesized images; The region of interest (ROI); article; machine learning; prediction; quantitative analysis; synthesis; human; image processing; machine learning; procedures; Image enhancement","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85105732452"
"Wadhawan R.; Drall T.; Singh S.; Chakraverty S.","Wadhawan, Rohan (57223773738); Drall, Tanuj (57243845800); Singh, Shubham (57244661300); Chakraverty, Shampa (7005011462)","57223773738; 57243845800; 57244661300; 7005011462","Multi-attributed and structured text-to-face synthesis","2020","Proceedings of 2020 IEEE International Conference on Technology, Engineering, Management for Societal Impact Using Marketing, Entrepreneurship and Talent, TEMSMET 2020","","","","","","","10.1109/TEMSMET51618.2020.9557583","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117482304&doi=10.1109%2fTEMSMET51618.2020.9557583&partnerID=40&md5=00e6ede9d5b25f509c5c3a22856bc473","Generative Adversarial Networks (GANs) have revolutionized image synthesis through many applications like face generation, photograph editing, and image super-resolution. Image synthesis using GANs has predominantly been uni-modal, with few approaches that can synthesize images from text or other data modes. Text-to-image synthesis, especially text-to-face synthesis, has promising use cases of robust face-generation from eye witness accounts and augmentation of the reading experience with visual cues. However, only a couple of datasets provide consolidated face data and textual descriptions for text-to-face synthesis. Moreover, these textual annotations are less extensive and descriptive, which reduces the diversity of faces generated from it. This paper empirically proves that increasing the number of facial attributes in each textual description helps GANs generate more diverse and real-looking faces. To prove this, we propose a new methodology that focuses on using structured textual descriptions. We also consolidate a Multi-Attributed and Structured Text-to-face (MAST) dataset consisting of high-quality images with structured textual annotations and make it available to researchers to experiment and build upon. Lastly, we report benchmark Fréchet's Inception Distance (FID), Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for the MAST dataset.  © 2020 IEEE.","Computer vision; Crowdsourcing; Machine learning; Semantics; Face generation; Face synthesis; Frechet; Frechet inception distance; Images synthesis; Machine-learning; Multi-attributed and structured text-to-face dataset; Structured text; Text-to-face synthesis; Textual description; Generative adversarial networks","Crowdsourcing; Face Generation; Frechet's Inception Distance; Generative Adversarial Network; Machine Learning; MAST dataset; Text-to-face synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117482304"
"Yi Z.; Chen Z.; Cai H.; Mao W.; Gong M.; Zhang H.","Yi, Zili (57204817695); Chen, Zhiqin (57214778958); Cai, Hao (56520804900); Mao, Wendong (57205563204); Gong, Minglun (7201566763); Zhang, Hao (55685656900)","57204817695; 57214778958; 56520804900; 57205563204; 7201566763; 55685656900","BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled Representation Learning and Image Synthesis","2020","IEEE Transactions on Image Processing","29","","9165961","9073","9083","10","10.1109/TIP.2020.3014608","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094148160&doi=10.1109%2fTIP.2020.3014608&partnerID=40&md5=a9aeebcc9f777e6ad96cfaeec774a979","We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively 'de-freeze' the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales. Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN.  © 1992-2012 IEEE.","Knowledge representation; Network layers; Vectors; Adversarial networks; High resolution image; Higher resolution images; Image generations; Image representations; Image synthesis; Multiple scale; Training methods; Image enhancement","Image Generation; Image Processing; Visual Effects","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094148160"
"Fujioka T.; Kubota K.; Mori M.; Katsuta L.; Kikuchi Y.; Kimura K.; Kimura M.; Adachi M.; Oda G.; Nakagawa T.; Kitazume Y.; Tateishi U.","Fujioka, Tomoyuki (55653670000); Kubota, Kazunori (7402692166); Mori, Mio (57207256127); Katsuta, Leona (57207908894); Kikuchi, Yuka (57204193949); Kimura, Koichiro (57211942722); Kimura, Mizuki (57218197515); Adachi, Mio (57212479820); Oda, Goshi (35604215300); Nakagawa, Tsuyoshi (57203947531); Kitazume, Yoshio (24528785600); Tateishi, Ukihide (7003533919)","55653670000; 7402692166; 57207256127; 57207908894; 57204193949; 57211942722; 57218197515; 57212479820; 35604215300; 57203947531; 24528785600; 7003533919","Virtual Interpolation Images of Tumor Development and Growth on Breast Ultrasound Image Synthesis With Deep Convolutional Generative Adversarial Networks","2021","Journal of Ultrasound in Medicine","40","1","","61","69","8","10.1002/jum.15376","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088255695&doi=10.1002%2fjum.15376&partnerID=40&md5=14ae5638cf8bcde42bafc5a14fddac42","Objectives: We sought to generate realistic synthetic breast ultrasound images and express virtual interpolation images of tumors using a deep convolutional generative adversarial network (DCGAN). Methods: After retrospective selection of breast ultrasound images of 528 benign masses, 529 malignant masses, and 583 normal breasts, 20 synthesized images of each were generated by the DCGAN. Fifteen virtual interpolation images of tumors were generated by changing the value of the input vector. A total of 60 synthesized images and 20 virtual interpolation images were evaluated by 2 readers, who scored them on a 5-point scale (1, very good; to 5, very poor) and then answered whether the synthesized image was benign, malignant, or normal. Results: The mean score of overall quality for synthesized images was 3.05, and that of the reality of virtual interpolation images was 2.53. The readers classified the generated images with a correct answer rate of 92.5%. Conclusions: A DCGAN can generate high-quality synthetic breast ultrasound images of each pathologic tissue and has the potential to create realistic virtual interpolation images of tumor development. © 2020 American Institute of Ultrasound in Medicine","Breast Neoplasms; Female; Growth and Development; Humans; Neural Networks, Computer; Retrospective Studies; Ultrasonography, Mammary; Convolution; Convolutional neural networks; Interpolation; Tumors; Ultrasonics; Adversarial networks; Breast ultrasound images; Malignant mass; Overall quality; Pathologic tissues; Synthesized images; Tumor development; Virtual interpolation; breast tumor; diagnostic imaging; echomammography; female; growth, development and aging; human; retrospective study; Medical imaging","breast cancer; convolutional neural network; deep learning; generative adversarial networks; ultrasound imaging","Article","Final","","Scopus","2-s2.0-85088255695"
"Shen T.; Gou C.; Wang J.; Wang F.-Y.","Shen, Tianyu (57207733071); Gou, Chao (56320227000); Wang, Jiangong (57207737632); Wang, Fei-Yue (57211758869)","57207733071; 56320227000; 57207737632; 57211758869","Collaborative Adversarial Networks for Joint Synthesis and Segmentation of X-ray Breast Mass Images","2020","Proceedings - 2020 Chinese Automation Congress, CAC 2020","","","9326848","1743","1747","4","10.1109/CAC51589.2020.9326848","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100930805&doi=10.1109%2fCAC51589.2020.9326848&partnerID=40&md5=68ce4fec69167dbc78334b1edc25c317","In this paper, we propose Collaborative Adversarial Networks (CAN) to enable simultaneous forward synthesis and backward segmentation of X-ray breast mass image. The proposed CAN consists of a generator (G), an inverter (I) and a discriminator (D). G aims to reconstruct mass images from corresponding annotated masks, while I is trained for mapping images back to accurate segmentation masks. All the obtained mask-image pairs are fed to D trained in an adversarial learning scheme. Through the collaborative adversarial training using a joint loss function, G synthesizes realistic mass images consistent with provided masks and I effectively segments the tumor regions from the images. Qualitative and quantitative evaluations on publicly available INbreast database demonstrate the effectiveness of our model. Furthermore, different from conventional GANs-based methods that can only perform either image synthesis or segmentation, the proposed model can be generalized to other bidirectional image-to-image translation of multimodal medical data. © 2020 IEEE.","Medical imaging; X rays; Adversarial learning; Adversarial networks; Image synthesis; Image translation; Loss functions; Medical data; Quantitative evaluation; Segmentation masks; Image segmentation","generative adversarial network; mass segmentation; medical image synthesis; X-ray breast mass","Conference paper","Final","","Scopus","2-s2.0-85100930805"
"Han F.; Guerrero R.; Pavlovic V.","Han, Fangda (57216953183); Guerrero, Ricardo (57215269019); Pavlovic, Vladimir (13606159500)","57216953183; 57215269019; 13606159500","CookGAN: Meal image synthesis from ingredients","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093463","1439","1447","8","10.1109/WACV45572.2020.9093463","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085517043&doi=10.1109%2fWACV45572.2020.9093463&partnerID=40&md5=4d4d94582374b18671abbc002b88f64b","In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual list of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by generative neural networks (GAN) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers, but meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. To generate real-like meal images from ingredients, we propose Cook Generative Adversarial Networks (CookGAN), CookGAN first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Experiments show our model is able to generate meal images corresponding to the ingredients. © 2020 IEEE.","Computer vision; Cooking; Adversarial networks; Association models; Computational framework; Consistent constraint; Cooking methods; Photo-realistic; Realistic images; Spatial quality; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085517043"
"Pandey S.; Singh P.R.; Tian J.","Pandey, Siddharth (57211189299); Singh, Pranshu Ranjan (57217568251); Tian, Jing (35281347600)","57211189299; 57217568251; 35281347600","An image augmentation approach using two-stage generative adversarial network for nuclei image segmentation","2020","Biomedical Signal Processing and Control","57","","101782","","","","10.1016/j.bspc.2019.101782","38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076173608&doi=10.1016%2fj.bspc.2019.101782&partnerID=40&md5=85fd99740a8fe350dfd087ede16223da","The major challenge in applying deep neural network techniques in the medical imaging domain is how to cope with small datasets and the limited amount of annotated samples. Data augmentation procedures that include conventional geometrical transformation based augmentation techniques and the recent image synthesis techniques using generative adversarial networks (GANs) can be employed to artificially increase the number of training images. This paper is focused on data augmentation for image segmentation task, which has an inherent challenge when compared to the conventional image classification task, due to its requirement to produce a corresponding mask for each generated image. To tackle the challenge of image-mask pair augmentation for image segmentation, this paper proposes a novel two-stage generative adversarial network. The proposed approach first employs a GAN to generate a synthesized binary mask, then incorporates this synthesized mask into the second GAN to perform a conditional generation of the synthesized image. Thus, these two GANs collaborate to generate the synthesized image-mask pairs, which are used to improve the performance of the conventional image segmentation approaches. The proposed approach is evaluated using the cell nuclei image segmentation task and demonstrates the superior performance to outperform both the traditional augmentation methods and the existing GAN-based augmentation methods in extensive results conducted using the benchmark Kaggle cell nuclei image segmentation dataset. © 2019 Elsevier Ltd","Benchmarking; Deep neural networks; Image enhancement; Mathematical transformations; Medical imaging; Metadata; Adversarial networks; Augmentation methods; Augmentation techniques; Conditional generation; Geometrical transformation; Image augmentation; Neural network techniques; Nuclei detections; article; cell nucleus; image segmentation; Image segmentation","Generative adversarial network; Image augmentation; Image segmentation; Nuclei detection in image","Article","Final","","Scopus","2-s2.0-85076173608"
"Wang Z.; She Q.; Smeaton A.F.; Ward T.E.; Healy G.","Wang, Zhengwei (57191625959); She, Qi (57038364600); Smeaton, Alan F. (7003631244); Ward, Tomás E. (7402100229); Healy, Graham (35069552200)","57191625959; 57038364600; 7003631244; 7402100229; 35069552200","Synthetic-Neuroscore: Using a neuro-AI interface for evaluating generative adversarial networks","2020","Neurocomputing","405","","","26","36","10","10.1016/j.neucom.2020.04.069","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084960494&doi=10.1016%2fj.neucom.2020.04.069&partnerID=40&md5=9190f66300c67fcec74f9a838271208c","Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. Arguably the most striking results have been in the area of image synthesis. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we describe an evaluation metric we call Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Materials related to this work are provided at https://github.com/villawang/Neuro-AI-Interface. © 2020 Elsevier B.V.","Convolutional neural networks; Natural language processing systems; Quality control; Speech synthesis; Adversarial networks; Evaluation metrics; Human judgments; Human perception; Image synthesis; NAtural language processing; Neural response; Prediction capability; Article; artificial intelligence; controlled study; convolutional neural network; decision making; electroencephalogram; generative adversarial network; human; image quality; machine learning; nerve potential; perception; prediction; priority journal; Image quality","Brain-computer interface; Generative adversarial networks; Neuro-AI interface; Neuroscore","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85084960494"
"Mauduit V.; Abry P.; Leonarduzzi R.; Roux S.G.; Quemener E.","Mauduit, V. (57220006277); Abry, P. (6701426529); Leonarduzzi, R. (36718012900); Roux, S.G. (57196937621); Quemener, E. (57220004964)","57220006277; 6701426529; 36718012900; 57196937621; 57220004964","Dcgan for the synthesis of multivariate multifractal textures: How do we know it works?","2020","IEEE International Workshop on Machine Learning for Signal Processing, MLSP","2020-September","","9231828","","","","10.1109/MLSP49062.2020.9231828","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096496901&doi=10.1109%2fMLSP49062.2020.9231828&partnerID=40&md5=f68269387c757a7ad8322a1aaf89c563","Deep Learning is nowadays widely used for several tasks in image processing. Notably, it has been massively used for image synthesis, mostly however with strong geometrical contents. Focused on a race for better performance via more complex architectures, research on Deep Learning, however, left behind the critical issue of assessing quantitatively and in a reproducible manner the quality of the synthesized images, notably for the case of pure textures. The present work aims to study the ability of Deep Convolutional Generative Adversarial Networks to synthesize multivariate textures characterized by rich multiscale multivariate statistics (multifractals). The focus is thus on quantifying the quality of the synthesized textures, on assessing the reproducibility of the learning procedure and on studying the impact of loss functions and of training dataset sizes, rather than on proposing yet another architecture.  © 2020 IEEE.","Convolutional neural networks; Deep learning; Fractals; Learning systems; Multivariant analysis; Network architecture; Textures; Adversarial networks; Complex architectures; Learning procedures; Multivariate statistics; Multivariate texture; Reproducibilities; Synthesized images; Synthesized texture; Image processing","Generative Adversarial Network; Multifractals; Multivariate Texture Synthesis; Quality Assessment","Conference paper","Final","","Scopus","2-s2.0-85096496901"
"Mori Y.; Inoue K.; Yoshioka M.","Mori, Yuto (57219841389); Inoue, Katsufumi (56723194900); Yoshioka, Michifumi (7402480508)","57219841389; 56723194900; 7402480508","Speech synthesis based on speaker impression with hierarchical discriminator gan","2020","IEEJ Transactions on Electronics, Information and Systems","140","11","","1207","1212","5","10.1541/ieejeiss.140.1207","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095716452&doi=10.1541%2fieejeiss.140.1207&partnerID=40&md5=addc6b34229e9345d5652b1517c27e0b","Recently, speech synthesis has been spotlighted as a key technology for broadcasting original movie with character on YouTube. To make a natural speech in the methods based on GAN(Generative Adversarial Network), the following unsolved problems are remained: Impression of synthesized speech such as warm, cool, etc., and long-term optimization of speech synthesis. In the former problem, since the conventional methods have focused on natural intonation of speech, they have not discussed the impression sufficiently. In this research, to deal with the impression, we proposed a new GAN based speech synthesis method using impression vector digitized the speaker impression. On the other hand, for the latter problem, since conventional methods optimize the relationship among frames insufficiently, the synthesized speech is still not natural. To solve this problem, inspired by an image synthesis technology such as HDGAN, we proposed a new GAN based network structure. The characteristic point is hierarchically nested discriminators at intermediate layers of the generator. In experiments with 15 speeches synthesized by the proposed method and 14 impression items, we estimated impression recognition accuracy by 11 listeners as subjective evaluation. From the experimental results, we have achieved 40.61% of subjective accuracy. © 2020 Institute of Electrical Engineers of Japan. All rights reserved.","Discriminators; Adversarial networks; Characteristic point; Conventional methods; Impression vectors; Intermediate layers; Network structures; Recognition accuracy; Subjective evaluations; Speech synthesis","Generalive adversarial network (gan); Impression vector; Text-to-specch","Article","Final","","Scopus","2-s2.0-85095716452"
"Wang M.; Lang C.; Liang L.; Lyu G.; Feng S.; Wang T.","Wang, Min (57221235394); Lang, Congyan (7402002472); Liang, Liqian (57218477192); Lyu, Gengyu (57208479355); Feng, Songhe (7402531247); Wang, Tao (56135273700)","57221235394; 7402002472; 57218477192; 57208479355; 7402531247; 56135273700","Attentive generative adversarial network to bridge multi-domain gap for image synthesis","2020","Proceedings - IEEE International Conference on Multimedia and Expo","2020-July","","9102761","","","","10.1109/ICME46284.2020.9102761","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090391870&doi=10.1109%2fICME46284.2020.9102761&partnerID=40&md5=66c9195823c8e672c9f9cb29585ad6e9","Despite the significant progress on text-to-image synthesis, automatically generating realistic images remains a challenging task since the location and specific shape of object are not given in the text descriptions. To address these problems, we propose a novel attentive generative adversarial network with contextual loss (AGAN-CL) algorithm. More specifically, the generative network consists of two sub-networks: a contextual network for generating image contours, and a cycle transformation autoencoder for converting contours to realistic images. Our core idea is the injection of image contours into the generative network, which is the most critical part of our network, since it will guide the whole generative network to focus on object regions. In addition, we also apply contextual loss and cycle-consistent loss to bridge multi-domain gap. Comprehensive results on several challenging datasets demonstrate the advantage of the proposed method over the leading approaches, regarding both visual fidelity and alignment with input descriptions. © 2020 IEEE.","Adversarial networks; Contextual network; Image contour; Image synthesis; Multi domains; Object region; Realistic images; Visual fidelity; Image processing","Attentive generative adversarial network; Contextual loss; Image contours; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85090391870"
"Miyoshi T.; Higaki A.; Kawakami H.; Yamaguchi O.","Miyoshi, Toru (56967580300); Higaki, Akinori (56688637600); Kawakami, Hideo (7202484084); Yamaguchi, Osamu (55712028800)","56967580300; 56688637600; 7202484084; 55712028800","Automated interpretation of the coronary angioscopy with deep convolutional neural networks","2020","Open Heart","7","1","openhrt-2019-001177","","","","10.1136/openhrt-2019-001177","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085158897&doi=10.1136%2fopenhrt-2019-001177&partnerID=40&md5=77e45bb982db2b09422a2ec20f4de319","Background Coronary angioscopy (CAS) is a useful modality to assess atherosclerotic changes, but interpretation of the images requires expert knowledge. Deep convolutional neural networks (DCNN) can be used for diagnostic prediction and image synthesis. Methods 107 images from 47 patients, who underwent CAS in our hospital between 2014 and 2017, and 864 images, selected from 142 MEDLINE-indexed articles published between 2000 and 2019, were analysed. First, we developed a prediction model for the angioscopic findings. Next, we made a generative adversarial networks (GAN) model to simulate the CAS images. Finally, we tried to control the output images according to the angioscopic findings with conditional GAN architecture. Results For both yellow colour (YC) grade and neointimal coverage (NC) grade, we could observe strong correlations between the true grades and the predicted values (YC grade, average r=0.80±0.02, p<0.001; NC grade, average r=0.73±0.02, p<0.001). The binary classification model for the red thrombus yielded 0.71±0.03 F 1 -score and the area under the receiver operator characteristic curve was 0.91±0.02. The standard GAN model could generate realistic CAS images (average Inception score=3.57±0.06). GAN-based data augmentation improved the performance of the prediction models. In the conditional GAN model, there were significant correlations between given values and the expert's diagnosis in YC grade but not in NC grade. Conclusion DCNN is useful in both predictive and generative modelling that can help develop the diagnostic support system for CAS. © Author(s) (or their employer(s)) 2020. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.","angioscopy; Article; binary classification; clinical article; convolutional neural network; human; image analysis; laboratory automation; neointima; predictive value; priority journal; simulation; support vector machine; thrombus","coronary angioscopy; coronary artery disease; imaging and diagnostics","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085158897"
"Shen Z.; Chen Y.; Huang T.S.; Zhou S.K.; Georgescu B.; Liu X.","Shen, Zengming (55872710300); Chen, Yifan (57216945659); Huang, Thomas S. (35513984600); Zhou, S.Kevin (57307954200); Georgescu, Bogdan (6603044053); Liu, Xuqi (57216955501)","55872710300; 57216945659; 35513984600; 57307954200; 6603044053; 57216955501","One-to-one mapping for unpaired image-to-image translation","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093622","1159","1168","9","10.1109/WACV45572.2020.9093622","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085484885&doi=10.1109%2fWACV45572.2020.9093622&partnerID=40&md5=e201d119ffe72780c3fe7a469a50cdcf","Recently image-to-image translation has attracted significant interests in the literature, starting from the successful use of the generative adversarial network (GAN), to the introduction of cyclic constraint, to extensions to multiple domains. However, in existing approaches, there is no guarantee that the mapping between two image domains is unique or one-to-one. Here we propose a self-inverse network learning approach for unpaired image-to-image translation. Building on top of CycleGAN, we learn a self-inverse function by simply augmenting the training samples by swapping inputs and outputs during training and with separated cycle consistency loss for each mapping direction. The outcome of such learning is a proven one-to-one mapping function. Our extensive experiments on a variety of datasets, including cross-modal medical image synthesis, object transfiguration, and semantic labeling, consistently demonstrate clear improvement over the CycleGAN method both qualitatively and quantitatively. Especially our proposed method reaches the state-of-the-art result on the cityscapes benchmark dataset for the label to photo un-paired directional image translation. © 2020 IEEE.","Computer vision; Inverse problems; Mapping; Medical imaging; Semantics; Adversarial networks; Benchmark datasets; Directional images; Image translation; Multiple domains; One-to-one mappings; Semantic labeling; State of the art; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085484885"
"Sun L.; Wang J.; Huang Y.; Ding X.; Greenspan H.; Paisley J.","Sun, Liyan (57161337400); Wang, Jiexiang (57200605852); Huang, Yue (57204367647); Ding, Xinghao (57204367131); Greenspan, Hayit (7004965553); Paisley, John (35810949000)","57161337400; 57200605852; 57204367647; 57204367131; 7004965553; 35810949000","An adversarial learning approach to medical image synthesis for lesion detection","2020","IEEE Journal of Biomedical and Health Informatics","24","8","8950113","2303","2314","11","10.1109/JBHI.2020.2964016","47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089206502&doi=10.1109%2fJBHI.2020.2964016&partnerID=40&md5=cfb76e9b2e299e24b92e62da33dd60dc","The identification of lesion within medical image data is necessary for diagnosis, treatment and prognosis. Segmentation and classification approaches are mainly based on supervised learning with well-paired image-level or voxel-level labels. However, labeling the lesion in medical images is laborious requiring highly specialized knowledge. We propose a medical image synthesis model named abnormal-to-normal translation generative adversarial network (ANT-GAN) to generate a normal-looking medical image based on its abnormal-looking counterpart without the need for paired training data. Unlike typical GANs, whose aim is to generate realistic samples with variations, our more restrictive model aims at producing a normal-looking image corresponding to one containing lesions, and thus requires a special design. Being able to provide a 'normal' counterpart to a medical image can provide useful side information for medical imaging tasks like lesion segmentation or classification validated by our experiments. In the other aspect, the ANT-GAN model is also capable of producing highly realistic lesion-containing image corresponding to the healthy one, which shows the potential in data augmentation verified in our experiments. © 2013 IEEE.","Brain; Humans; Image Interpretation, Computer-Assisted; Unsupervised Machine Learning; Classification (of information); Diagnosis; Image segmentation; Adversarial learning; Adversarial networks; Classification approach; Data augmentation; Lesion detection; Lesion segmentations; Side information; Specialized knowledge; article; controlled study; diagnostic imaging; human; human experiment; learning; synthesis; brain; computer assisted diagnosis; procedures; unsupervised machine learning; Medical imaging","generative adversarial network; Medical image synthesis; unsupervised learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85089206502"
"Wolterink J.M.; Mukhopadhyay A.; Leiner T.; Vogl T.J.; Bucher A.M.; Išgum I.","Wolterink, Jelmer M. (56198388700); Mukhopadhyay, Anirban (56241021800); Leiner, Tim (7004171845); Vogl, Thomas J. (35444602000); Bucher, Andreas M. (55962296500); Išgum, Ivana (6507874503)","56198388700; 56241021800; 7004171845; 35444602000; 55962296500; 6507874503","Generative adversarial networks: A primer for radiologists","2021","Radiographics","41","3","","840","857","17","10.1148/rg.2021200151","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105906248&doi=10.1148%2frg.2021200151&partnerID=40&md5=ba361cd9c648e0cd671b02131b3e48f4","Artificial intelligence techniques involving the use of artificial neural networks—that is, deep learning techniques—are expected to have a major effect on radiology. Some of the most exciting applications of deep learning in radiology make use of generative adversarial networks (GANs). GANs consist of two artificial neural networks that are jointly optimized but with opposing goals. One neural network, the generator, aims to synthesize images that cannot be distinguished from real images. The second neural network, the discriminator, aims to distinguish these synthetic images from real images. These deep learning models allow, among other applications, the synthesis of new images, acceleration of image acquisitions, reduction of imaging artifacts, efficient and accurate conversion between medical images acquired with different modalities, and identification of abnormalities depicted on images. The authors provide an introduction to GANs and adversarial deep learning methods. In addition, the different ways in which GANs can be used for image synthesis and image-to-image translation tasks, as well as the principles underlying conditional GANs and cycle-consistent GANs, are described. Illustrated examples of GAN applications in radiologic image analysis for different imaging modalities and different tasks are provided. The clinical potential of GANs, future clinical GAN applications, and potential pitfalls and caveats that radiologists should be aware of also are discussed in this review. © RSNA, 2021.","Artificial Intelligence; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Radiologists; artificial intelligence; human; image processing; radiologist","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105906248"
"Du W.-L.; Zhou Y.; Zhao J.; Tian X.; Yang Z.; Bian F.","Du, Wen-Liang (55265123100); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Tian, Xiaolin (7202380154); Yang, Zhi (57284929100); Bian, Fuqiang (57202867285)","55265123100; 35480110700; 57138970300; 7202380154; 57284929100; 57202867285","Exploring the Potential of Unsupervised Image Synthesis for SAR-Optical Image Matching","2021","IEEE Access","9","","9427486","71022","71033","11","10.1109/ACCESS.2021.3079327","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105853208&doi=10.1109%2fACCESS.2021.3079327&partnerID=40&md5=3ebbd56086511abe8160d0a847102a35","We consider SAR-optical image matching problems, where correspondences are acquired from a pair of SAR and optical images. Recent methods for such a problem typically simplify the SAR-optical image matching to the SAR-SAR or optical-optical image matchings using supervised-image-synthesis methods. However, training supervised-image-synthesis needs plenty of aligned SAR-optical image pairs while gathering sufficient amounts of aligned multi-modal image pairs is challenging in remote sensing. In this work, we investigate the applicability of unsupervised-image-synthesis for SAR-optical image matching such that the unaligned SAR-optical images could be used. To this end, we apply feature matching loss to a well known unsupervised-image-synthesis method, i.e., CycleGAN, to enforce the feature matching consistency. Moreover, we develop a shared-matching-strategy to improve the results of SAR-optical image matching further. Qualitative comparisons against CycleGAN, StarGAN, and DualGAN demonstrate the superiority of our approach. Quantitative results show that, compared with CycleGAN, StarGAN, and DualGAN, our method obtains at least 2.6 times more qualified SAR-optical matchings.  © 2013 IEEE.","Geometrical optics; Image enhancement; Image matching; Remote sensing; Feature matching; Image synthesis; Matching problems; Matchings; Multi-modal image; Optical image; Quantitative result; Radar imaging","generative adversarial networks (GANs); Image matching; synthetic aperture radar (SAR); unsupervised-image-synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105853208"
"Alqahtani H.; Kavakli-Thorne M.; Kumar G.","Alqahtani, Hamed (57194574403); Kavakli-Thorne, Manolya (57219506272); Kumar, Gulshan (35932222600)","57194574403; 57219506272; 35932222600","Generative adversarial networks - application domains","2020","Image Recognition: Progress, Trends and Challenges","","","","227","272","45","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145342380&partnerID=40&md5=6a14d606c8774956491cbae1afcf7f48","Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This chapter aims to present an overview of GANs, and potential application in various domains. The main intention of this chapter is to explore and present a comprehensive review of the crucial applications of GANs covering a variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the chapter ends with the conclusion and future aspects. © 2020 by Nova Science Publishers, Inc. All rights reserved.","","Generative adversarial networks; Neural networks; Supervised learning; Unsupervised learning","Book chapter","Final","","Scopus","2-s2.0-85145342380"
"Ozbey M.; Cukur T.","Ozbey, Muzaffer (57213816253); Cukur, Tolga (23034054800)","57213816253; 23034054800","T1-Weighted Contrast-Enhanced Synthesis for Multi-Contrast MRI Segmentation; [Coklu Kontrast MRG Segmentasyonu Icin T1-Agirlikli Artirilmis-Kontrast Sentezi]","2020","2020 28th Signal Processing and Communications Applications Conference, SIU 2020 - Proceedings","","","9302109","","","","10.1109/SIU49456.2020.9302109","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100314103&doi=10.1109%2fSIU49456.2020.9302109&partnerID=40&md5=b18febfbd41ab3b3e4de1e94f0e74cba","In recent years, deep learning techniques have been used in computer science as well as in many other disciplines. Successful detection of complex relationships and connections within big data enables effective use of deep learning in many areas. Deep learning with the detection of patterns and abnormalities in images is also a promising method for the eld of Radiology. Detection of abnormalities in MR images enables detection of brain tumor and can automate this process. However, the deep learning models developed for brain tumor detection are sensitive to missing MR images in the input data and therefore the model is not robust enough. One of the models used for brain tumor detection requires a combined MR image in 4 different contrast and sequence; T1, T2, Flair and T1c images. In this study, it is proposed to synthesize the missing contrast image in the input data with another deep learning technique. Incomplete T1c MR image was synthesized by the generative adversarial networks (GAN) method and brain tumor detection performance was examined.  © 2020 IEEE.","Brain; Input output programs; Learning systems; Magnetic resonance imaging; Pattern recognition; Tumors; Adversarial networks; Brain tumors; Complex relationships; Contrast-enhanced; Learning models; Learning techniques; MRI segmentation; T1-weighted; Deep learning","Deep learning; Generative adversarial network; MR image synthesis; Tumor segmentation","Conference paper","Final","","Scopus","2-s2.0-85100314103"
"Härkönen E.; Hertzmann A.; Lehtinen J.; Paris S.","Härkönen, Erik (57211426418); Hertzmann, Aaron (6601954186); Lehtinen, Jaakko (7005279290); Paris, Sylvain (14036202700)","57211426418; 6601954186; 7005279290; 14036202700","GANSpace: Discovering interpretable GAN controls","2020","Advances in Neural Information Processing Systems","2020-December","","","","","","","122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107368569&partnerID=40&md5=7428d82e3ecd1c1ddebe8be3f1b0fd08","This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Component Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. We show results on different GANs trained on various datasets, and demonstrate good qualitative matches to edit directions found through earlier supervised approaches. © 2020 Neural information processing systems foundation. All rights reserved.","Adversarial networks; Feature space; Image synthesis; Layer-wise; Principal directions; Time of day","","Conference paper","Final","","Scopus","2-s2.0-85107368569"
"Fang F.; Luo F.; Zhang H.-P.; Zhou H.-J.; Chow A.L.H.; Xiao C.-X.","Fang, Fei (56119103900); Luo, Fei (57203391498); Zhang, Hong-Pan (57217089992); Zhou, Hua-Jian (57217088892); Chow, Alix L. H. (57217088832); Xiao, Chun-Xia (8663722500)","56119103900; 57203391498; 57217089992; 57217088892; 57217088832; 8663722500","A Comprehensive Pipeline for Complex Text-to-Image Synthesis","2020","Journal of Computer Science and Technology","35","3","","522","537","15","10.1007/s11390-020-0305-9","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086138147&doi=10.1007%2fs11390-020-0305-9&partnerID=40&md5=aba6c1dd53344a24163c0d0cbc368f3e","Synthesizing a complex scene image with multiple objects and background according to text description is a challenging problem. It needs to solve several difficult tasks across the fields of natural language processing and computer vision. We model it as a combination of semantic entity recognition, object retrieval and recombination, and objects’ status optimization. To reach a satisfactory result, we propose a comprehensive pipeline to convert the input text to its visual counterpart. The pipeline includes text processing, foreground objects and background scene retrieval, image synthesis using constrained MCMC, and post-processing. Firstly, we roughly divide the objects parsed from the input text into foreground objects and background scenes. Secondly, we retrieve the required foreground objects from the foreground object dataset segmented from Microsoft COCO dataset, and retrieve an appropriate background scene image from the background image dataset extracted from the Internet. Thirdly, in order to ensure the rationality of foreground objects’ positions and sizes in the image synthesis step, we design a cost function and use the Markov Chain Monte Carlo (MCMC) method as the optimizer to solve this constrained layout problem. Finally, to make the image look natural and harmonious, we further use Poisson-based and relighting-based methods to blend foreground objects and background scene image in the post-processing step. The synthesized results and comparison results based on Microsoft COCO dataset prove that our method outperforms some of the state-of-the-art methods based on generative adversarial networks (GANs) in visual quality of generated scene images. © 2020, Institute of Computing Technology, Chinese Academy of Sciences.","Complex networks; Constrained optimization; Cost functions; Markov chains; Monte Carlo methods; Natural language processing systems; Pipeline processing systems; Pipelines; Semantics; Text processing; Adversarial networks; Background image; Background scenes; Comparison result; Foreground objects; Markov chain Monte Carlo method; NAtural language processing; State-of-the-art methods; Image processing","image synthesis; Markov Chain Monte Carlo (MCMC); scene generation; text-to-image conversion","Article","Final","","Scopus","2-s2.0-85086138147"
"Situ Z.; Teng S.; Liu H.; Luo J.; Zhou Q.","Situ, Zuxiang (57223046482); Teng, Shuai (57210558798); Liu, Hanlin (57224200277); Luo, Jinhua (57214915522); Zhou, Qianqian (54416814000)","57223046482; 57210558798; 57224200277; 57214915522; 54416814000","Automated Sewer Defects Detection Using Style-Based Generative Adversarial Networks and Fine-Tuned Well-Known CNN Classifier","2021","IEEE Access","9","","9406802","59498","59507","9","10.1109/ACCESS.2021.3073915","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107236421&doi=10.1109%2fACCESS.2021.3073915&partnerID=40&md5=4ab35ea83fb03fe5515cb7da018380e8","Automated sewer defects detection has become an important trend for better management and maintenance of urban sewer systems. Deep learning technology has developed rapidly and offers an innovative solution for automated detection in engineering applications. However, insufficient data and unbalanced samples have proposed a big challenge to deep learning model training. This study adopts the state-of-the-art Style-based Generative Adversarial Networks (StyleGANs) model and compares the performances of its two variants in producing high-quality synthetic sewer defects images. Seven well-known CNN models are further fine-tuned and trained using the synthetic images for automated sewer defects detection to examine the effects of StyleGANs on augmenting the detection performance. Results show that both StyleGANs are efficient in producing high-quality images with various styles and high-level details for multiple types of sewer defects. Specifically, the StyleGAN2-Adaptive Discriminator Augmentation (StyleGAN2-ADA) with the aid of Freeze Discriminator (Freeze-D) yields the best model performance. Among the adopted CNN classifiers, Inception_v3 achieves the highest detection accuracy. The mean detection accuracy is 94% (with a specific accuracy of 99.7%, 97%, 95.3% and 84% for tree root, residential wall, disjoint and obstacle, respectively) and confirms the reliability of the StyleGANs' performance. The study shows that StyleGANs provide a promising method to alleviate the limited and uneven dataset problem and can improve the deep learning model performance. © 2013 IEEE.","Automation; Deep learning; Defects; Engineering education; Sewers; Adversarial networks; Automated detection; Detection performance; Engineering applications; High quality images; Innovative solutions; Learning technology; Urban sewer system; Learning systems","Automated detection; image synthesis; sewer defects; StyleGANs; well-known CNN classifiers","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85107236421"
"Kuriakose B.; Thomas T.; Thomas N.E.; Varghese S.J.; Kumar V.A.","Kuriakose, Bipin (57218938823); Thomas, Theres (57218938164); Thomas, Nikitha Elsa (57218936983); Varghese, Sharon John (57211468950); Kumar, Veena A. (57211156136)","57218938823; 57218938164; 57218936983; 57211468950; 57211156136","Synthesizing Images from Hand-Drawn Sketches using Conditional Generative Adversarial Networks","2020","Proceedings of the International Conference on Electronics and Sustainable Communication Systems, ICESC 2020","","","9155550","774","778","4","10.1109/ICESC48915.2020.9155550","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090864632&doi=10.1109%2fICESC48915.2020.9155550&partnerID=40&md5=af1f3f942348410c814b0961272c25f3","Today, Technology have remarkable charm in the area of Computer Graphics and Vision. Producing absolute images from the poor hand-drawn sketches is a very demanding and laborious task in this area. Hand-drawn sketch recognition is widely used in sketch based image and video retrieval, manipulations and reorganizations. In S ketch to image synthesis, the sketches are translated to realistic images with the use of a generative model. An image is put forward to image translation network that involves in producing a synthesized image from the input sketch via an adversarial process. A novel Conditional Generative Adversarial Network (cGANs) which is an extension of Generative Adversarial Networks (GANs) is used to produce the images with some sort of conditions or attributes. In this work, the implementation of cGANs for synthesizing the images from hand-drawn sketches gives a remarkable output. The performance of the proposed sketch to image translation network was excellent and appreciable. © 2020 IEEE.","Computer graphics; Drawing (graphics); Adversarial networks; Generative model; Hand-drawn sketches; Image synthesis; Image translation; Realistic images; Synthesized images; Video retrieval; Image processing","Adversarial process; Generative Adversarial Networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85090864632"
"Kawahara D.; Nagata Y.","Kawahara, Daisuke (56350513700); Nagata, Yasushi (56415732800)","56350513700; 56415732800","T1-weighted and T2-weighted MRI image synthesis with convolutional generative adversarial networks","2021","Reports of Practical Oncology and Radiotherapy","26","1","","35","42","7","10.5603/RPOR.a2021.0005","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107537596&doi=10.5603%2fRPOR.a2021.0005&partnerID=40&md5=914a0064381a86376c8839fc1d00efa5","background: The objective of this study was to propose an optimal input image quality for a conditional generative adversarial network (GaN) in T1-weighted and T2-weighted magnetic resonance imaging (MrI) images. Materials and methods: a total of 2,024 images scanned from 2017 to 2018 in 104 patients were used. The prediction framework of T1-weighted to T2-weighted MrI images and T2-weighted to T1-weighted MrI images were created with GaN. Two image sizes (512 × 512 and 256 × 256) and two grayscale level conversion method (simple and adaptive) were used for the input images. The images were converted from 16-bit to 8-bit by dividing with 256 levels in a simple conversion method. For the adaptive conversion method, the unused levels were eliminated in 16-bit images, which were converted to 8-bit images by dividing with the value obtained after dividing the maximum pixel value with 256. results: The relative mean absolute error (rMae) was 0.15 for T1-weighted to T2-weighted MrI images and 0.17 for T2-weighted to T1-weighted MrI images with an adaptive conversion method, which was the smallest. Moreover, the adaptive conversion method has a smallest mean square error (rMse) and root mean square error (rrMse), and the largest peak signal-to-noise ratio (psNr) and mutual information (MI). The computation time depended on the image size. conclusions: Input resolution and image size affect the accuracy of prediction. The proposed model and approach of prediction framework can help improve the versatility and quality of multi-contrast MrI tests without the need for prolonged examinations. © 2021 Urban and Partner. All rights reserved.","adult; article; diagnostic test accuracy study; female; human; image quality; major clinical study; male; nuclear magnetic resonance imaging; prediction; signal noise ratio; synthesis","Convolutional generative adversarial networks; Image synthesis; MrI","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85107537596"
"Liang J.; Pei W.; Lu F.","Liang, Jiadong (57201286955); Pei, Wenjie (57193153361); Lu, Feng (54956194300)","57201286955; 57193153361; 54956194300","CPGAN: Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12349 LNCS","","","491","508","17","10.1007/978-3-030-58548-8_29","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097441435&doi=10.1007%2f978-3-030-58548-8_29&partnerID=40&md5=31486733b59687348e49664a579444a9","Typical methods for text-to-image synthesis seek to design effective generative architecture to model the text-to-image mapping directly. It is fairly arduous due to the cross-modality translation. In this paper we circumvent this problem by focusing on parsing the content of both the input text and the synthesized image thoroughly to model the text-to-image consistency in the semantic level. Particularly, we design a memory structure to parse the textual content by exploring semantic correspondence between each word in the vocabulary to its various visual contexts across relevant images during text encoding. Meanwhile, the synthesized image is parsed to learn its semantics in an object-aware manner. Moreover, we customize a conditional discriminator to model the fine-grained correlations between words and image sub-regions to push for the text-image semantic alignment. Extensive experiments on COCO dataset manifest that our model advances the state-of-the-art performance significantly (from 35.69 to 52.73 in Inception Score). © 2020, Springer Nature Switzerland AG.","Computer vision; Image coding; Adversarial networks; Image synthesis; Memory structure; Semantic correspondence; Semantic levels; State-of-the-art performance; Synthesized images; Textual content; Semantics","Content-Parsing; Cross-modality; Generative Adversarial Networks; Memory structure; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85097441435"
"Gou C.; Zhang H.; Wang K.; Wang F.-Y.; Ji Q.","Gou, Chao (56320227000); Zhang, Hui (57216240519); Wang, Kunfeng (55901133200); Wang, Fei-Yue (57211758869); Ji, Qiang (18935108400)","56320227000; 57216240519; 55901133200; 57211758869; 18935108400","Cascade learning from adversarial synthetic images for accurate pupil detection","2019","Pattern Recognition","88","","","584","594","10","10.1016/j.patcog.2018.12.014","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058798116&doi=10.1016%2fj.patcog.2018.12.014&partnerID=40&md5=707fea2a2c77f5b038391829bc47f665","Image-based pupil detection, which aims to find the pupil location in an image, has been an active research topic in computer vision community. Learning-based approaches can achieve preferable results given large amounts of training data with eye center annotations. However, there are limited publicly available datasets with accurate eye center annotations and it is unreliable and time-consuming for manually labeling large amounts of training data. In this paper, inspired by learning from synthetic data in Parallel Vision framework, we introduce a step of parallel imaging built upon Generative Adversarial Networks (GANs) to generate adversarial synthetic images. In particular, we refine the synthetic eye images by the improved SimGAN using adversarial training scheme. For the computational experiments, we further propose a coarse-to-fine pupil detection framework based on shape augmented cascade regression models learning from the adversarial synthetic images. Experiments on benchmark databases of BioID, GI4E, and LFW show that the proposed work performs significantly better over other state-of-the-art methods by leveraging the power of cascade regression and adversarial image synthesis. © 2018 Elsevier Ltd","Regression analysis; Adversarial networks; Benchmark database; Computational experiment; GANs; Learning-based approach; Pupil detection; State-of-the-art methods; Vision communities; Image enhancement","Cascade regression; GANs; Pupil detection","Article","Final","","Scopus","2-s2.0-85058798116"
"Di X.; Riggan B.S.; Hu S.; Short N.J.; Patel V.M.","Di, Xing (57193029816); Riggan, Benjamin S. (56406510300); Hu, Shuowen (34881794700); Short, Nathaniel J. (56940280300); Patel, Vishal M. (56660008900)","57193029816; 56406510300; 34881794700; 56940280300; 56660008900","Polarimetric Thermal to Visible Face Verification via Self-Attention Guided Synthesis","2019","2019 International Conference on Biometrics, ICB 2019","","","8987329","","","","10.1109/ICB45273.2019.8987329","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081067965&doi=10.1109%2fICB45273.2019.8987329&partnerID=40&md5=8e208947536ea9f11ef4e514a6e7625d","Polarimetric thermal to visible face verification entails matching two images that contain significant domain differences. Several recent approaches have attempted to synthesize visible faces from thermal images for cross-modal matching. In this paper, we take a different approach in which rather than focusing only on synthesizing visible faces from thermal faces, we also propose to synthesize thermal faces from visible faces. Our intuition is based on the fact that thermal images also contain some discriminative information about the person for verification. Deep features from a pre-trained Convolutional Neural Network (CNN) are extracted from the original as well as the synthesized images. These features are then fused to generate a template which is then used for verification. The proposed synthesis network is based on the self-attention generative adversarial network (SAGAN) which essentially allows efficient attention-guided image synthesis. Extensive experiments on the ARL polarimetric thermal face dataset demonstrate that the proposed method achieves state-of-the-art performance. © 2019 IEEE.","Biometrics; Convolutional neural networks; Polarimeters; Adversarial networks; Cross-modal; Domain differences; Face Verification; Guided images; State-of-the-art performance; Synthesized images; Thermal images; Face recognition","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081067965"
"Heljakka A.; Solin A.; Kannala J.","Heljakka, Ari (55645509800); Solin, Arno (57077059600); Kannala, Juho (8928446400)","55645509800; 57077059600; 8928446400","Pioneer Networks: Progressively Growing Generative Autoencoder","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11361 LNCS","","","22","38","16","10.1007/978-3-030-20887-5_2","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066805190&doi=10.1007%2f978-3-030-20887-5_2&partnerID=40&md5=7a04f1d072ae2aeae8461323a0b4f684","We introduce a novel generative autoencoder network model that learns to encode and reconstruct images with high quality and resolution, and supports smooth random sampling from the latent space of the encoder. Generative adversarial networks (GANs) are known for their ability to simulate random high-quality images, but they cannot reconstruct existing images. Previous works have attempted to extend GANs to support such inference but, so far, have not delivered satisfactory high-quality results. Instead, we propose the Progressively Growing Generative Autoencoder (Pioneer) network which achieves high-quality reconstruction with images without requiring a GAN discriminator. We merge recent techniques for progressively building up the parts of the network with the recently introduced adversarial encoder–generator network. The ability to reconstruct input images is crucial in many real-world applications, and allows for precise intelligent manipulation of existing images. We show promising results in image synthesis and inference, with state-of-the-art results in CelebA inference tasks. © 2019, Springer Nature Switzerland AG.","Computer vision; Learning systems; Signal encoding; Adversarial networks; Auto encoders; Generative model; High quality images; High quality reconstruction; Intelligent manipulation; Network modeling; State of the art; Image reconstruction","Autoencoder; Computer vision; Generative models","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85066805190"
"Cao J.; Hu Y.; Yu B.; He R.; Sun Z.","Cao, Jie (57197831202); Hu, Yibo (57203223443); Yu, Bing (57207868336); He, Ran (35764463900); Sun, Zhenan (8081773300)","57197831202; 57203223443; 57207868336; 35764463900; 8081773300","3D Aided Duet GANs for Multi-View Face Image Synthesis","2019","IEEE Transactions on Information Forensics and Security","14","8","8603840","2028","2042","14","10.1109/TIFS.2019.2891116","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065620120&doi=10.1109%2fTIFS.2019.2891116&partnerID=40&md5=b86771a240d6bf8a115400939c7ef688","Multi-view face synthesis from a single image is an ill-posed computer vision problem. It often suffers from appearance distortions if it is not well-defined. Producing photo-realistic and identity preserving multi-view results is still a not well-defined synthesis problem. This paper proposes 3D aided duet generative adversarial networks (AD-GAN) to precisely rotate the yaw angle of an input face image to any specified angle. AD-GAN decomposes the challenging synthesis problem into two well-constrained subtasks that correspond to a face normalizer and a face editor. The normalizer first frontalizes an input image, and then the editor rotates the frontalized image to a desired pose guided by a remote code. In the meantime, the face normalizer is designed to estimate a novel dense UV correspondence field, making our model aware of 3D face geometry information. In order to generate photo-realistic local details and accelerate convergence process, the normalizer and the editor are trained in a two-stage manner and regulated by a conditional self-cycle loss and a perceptual loss. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only improves the visual realism of multi-view synthetic images but also preserves identity information well. © 2005-2012 IEEE.","3D modeling; Image enhancement; Adversarial networks; Computer vision problems; Face image synthesis; Face reconstruction; Face rotation; Face synthesis; Identity information; Pose-invariant face recognition; Face recognition","face reconstruction; Face rotation and frontalization; multi-view face synthesis; pose-invariant face recognition","Article","Final","","Scopus","2-s2.0-85065620120"
"Wu M.; Cai X.; Chen Q.; Ji Z.; Niu S.; Leng T.; Rubin D.L.; Park H.","Wu, Menglin (55873505400); Cai, Xinxin (57211180968); Chen, Qiang (55420651600); Ji, Zexuan (34971196100); Niu, Sijie (56281272500); Leng, Theodore (37051202000); Rubin, Daniel L. (7202307112); Park, Hyunjin (56512679000)","55873505400; 57211180968; 55420651600; 34971196100; 56281272500; 37051202000; 7202307112; 56512679000","Geographic atrophy segmentation in SD-OCT images using synthesized fundus autofluorescence imaging","2019","Computer Methods and Programs in Biomedicine","182","","105101","","","","10.1016/j.cmpb.2019.105101","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072884923&doi=10.1016%2fj.cmpb.2019.105101&partnerID=40&md5=bbc7cdc3aceef1211d291ddd90404021","Background and objective: Accurate assessment of geographic atrophy (GA) is critical for diagnosis and therapy of non-exudative age-related macular degeneration (AMD). Herein, we propose a novel GA segmentation framework for spectral-domain optical coherence tomography (SD-OCT) images that employs synthesized fundus autofluorescence (FAF) images. Methods: An en-face OCT image is created via the restricted sub-volume projection of three-dimensional OCT data. A GA region-aware conditional generative adversarial network is employed to generate a plausible FAF image from the en-face OCT image. The network balances the consistency between the entire synthesize FAF image and the lesion. We use a fully convolutional deep network architecture to segment the GA region using the multimodal images, where the features of the en-face OCT and synthesized FAF images are fused on the front-end of the network. Results: Experimental results for 56 SD-OCT scans with GA indicate that our synthesis algorithm can generate high-quality synthesized FAF images and that the proposed segmentation network achieves a dice similarity coefficient, an overlap ratio, and an absolute area difference of 87.2%, 77.9%, and 11.0%, respectively. Conclusion: We report an automatic GA segmentation method utilizing synthesized FAF images. Significance: Our method is effective for multimodal segmentation of the GA region and can improve AMD treatment. © 2019","Automation; Fundus Oculi; Geographic Atrophy; Humans; Optical Imaging; Tomography, Optical Coherence; Coherent light; Genetic algorithms; Network architecture; Ophthalmology; Optical tomography; Tomography; Age-related macular degeneration; Autofluorescence imaging; Biomedical image segmentation; Geographic atrophy; Image synthesis; Multi-modal segmentation; Retinal image analysis; Spectral domain optical coherence tomographies; Article; autofluorescence imaging; convolutional neural network; geographic atrophy; human; image analysis; image segmentation; major clinical study; multimodal imaging; optic nerve; spectral domain optical coherence tomography; automation; diagnostic imaging; eye fundus; fluorescence imaging; geographic atrophy; optical coherence tomography; procedures; Image segmentation","Biomedical image segmentation; Geographic atrophy; Image synthesis; Optical coherence tomography; Retinal image analysis","Article","Final","","Scopus","2-s2.0-85072884923"
"Fujioka T.; Mori M.; Kubota K.; Kikuchi Y.; Katsuta L.; Adachi M.; Oda G.; Nakagawa T.; Kitazume Y.; Tateishi U.","Fujioka, Tomoyuki (55653670000); Mori, Mio (57207256127); Kubota, Kazunori (7402692166); Kikuchi, Yuka (57204193949); Katsuta, Leona (57207908894); Adachi, Mio (57212479820); Oda, Goshi (35604215300); Nakagawa, Tsuyoshi (57203947531); Kitazume, Yoshio (24528785600); Tateishi, Ukihide (7003533919)","55653670000; 57207256127; 7402692166; 57204193949; 57207908894; 57212479820; 35604215300; 57203947531; 24528785600; 7003533919","Breast ultrasound image synthesis using deep convolutional generative adversarial networks","2019","Diagnostics","9","4","176","","","","10.3390/diagnostics9040176","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076798171&doi=10.3390%2fdiagnostics9040176&partnerID=40&md5=1ffa05ceba403580e648140aeac06a22","Deep convolutional generative adversarial networks (DCGANs) are newly developed tools for generating synthesized images. To determine the clinical utility of synthesized images, we generated breast ultrasound images and assessed their quality and clinical value. After retrospectively collecting 528 images of 144 benign masses and 529 images of 216 malignant masses in the breasts, synthesized images were generated using a DCGAN with 50, 100, 200, 500, and 1000 epochs. The synthesized (n = 20) and original (n = 40) images were evaluated by two radiologists, who scored them for overall quality, definition of anatomic structures, and visualization of the masses on a five-point scale. They also scored the possibility of images being original. Although there was no significant difference between the images synthesized with 1000 and 500 epochs, the latter were evaluated as being of higher quality than all other images. Moreover, 2.5%, 0%, 12.5%, 37.5%, and 22.5% of the images synthesized with 50, 100, 200, 500, and 1000 epochs, respectively, and 14% of the original images were indistinguishable from one another. Interobserver agreement was very good (|r| = 0.708-0.825, p < 0.001). Therefore, DCGAN can generate high-quality and realistic synthesized breast ultrasound images that are indistinguishable from the original images. © 2019 by the authors.","article; artificial intelligence; cancer size; clinical article; controlled study; convolutional neural network; deep learning; echomammography; human; interrater reliability; radiologist; synthesis","Artificial intelligence; Breast imaging; Convolutional neural network; Deep learning; Generative adversarial networks; Ultrasound","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85076798171"
"Peng G.; Wang S.","Peng, Guozhu (57195589783); Wang, Shangfei (55711853800)","57195589783; 55711853800","Dual semi-supervised learning for facial action unit recognition","2019","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","","","","8827","8834","7","","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087639165&partnerID=40&md5=fe0792d6aef5c7e5f1f79b3dd41bd3a8","Current works on facial action unit (AU) recognition typically require fully AU-labeled training samples. To reduce the reliance on time-consuming manual AU annotations, we propose a novel semi-supervised AU recognition method leveraging two kinds of readily available auxiliary information. The method leverages the dependencies between AUs and expressions as well as the dependencies among AUs, which are caused by facial anatomy and therefore embedded in all facial images, independent on their AU annotation status. The other auxiliary information is facial image synthesis given AUs, the dual task of AU recognition from facial images, and therefore has intrinsic probabilistic connections with AU recognition, regardless of AU annotations. Specifically, we propose a dual semi-supervised generative adversarial network for AU recognition from partially AU-labeled and fully expression-labeled facial images. The proposed network consists of an AU classifier C, an image generator G, and a discriminator D. In addition to minimize the supervised losses of the AU classifier and the face generator for labeled training data, we explore the probabilistic duality between the tasks using adversary learning to force the convergence of the face-AU-expression tuples generated from the AU classifier and the face generator, and the ground-truth distribution in labeled data for all training data. This joint distribution also includes the inherent AU dependencies. Furthermore, we reconstruct the facial image using the output of the AU classifier as the input of the face generator, and create AU labels by feeding the output of the face generator to the AU classifier. We minimize reconstruction losses for all training data, thus exploiting the informative feedback provided by the dual tasks. Within-database and cross-database experiments on three benchmark databases demonstrate the superiority of our method in both AU recognition and face synthesis compared to state-of-the-art works. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Classification (of information); Database systems; Probability distributions; Semi-supervised learning; Adversarial networks; Auxiliary information; Benchmark database; Facial action unit recognition; Facial Image synthesis; Joint distributions; Labeled training data; Recognition methods; Face recognition","","Conference paper","Final","","Scopus","2-s2.0-85087639165"
"Ullah A.; Yu X.; Majid A.; Rahman H.U.; Mughal M.F.","Ullah, Anwar (57212312538); Yu, Xinguo (7404114721); Majid, Abdul (57209365589); Rahman, Hafiz Ur (57212197446); Mughal, M. Farhan (57529536600)","57212312538; 7404114721; 57209365589; 57212197446; 57529536600","High-Resolution Realistic Image Synthesis from Text Using Iterative Generative Adversarial Network","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11854 LNCS","","","211","224","13","10.1007/978-3-030-34879-3_17","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076427909&doi=10.1007%2f978-3-030-34879-3_17&partnerID=40&md5=c7b252de6ace998db3c69a57e4b4f22a","Synthesizing high-resolution realistic images from text description using one iteration Generative Adversarial Network (GAN) is difficult without using any additional techniques because mostly the blurry artifacts and mode collapse problems are occurring. To reduce these problems, this paper proposes an Iterative Generative Adversarial Network (iGAN) which takes three iterations to synthesize high-resolution realistic image from their text description. In the iteration, GAN synthesizes a low-resolution pixels basic shape and basic color image from the text description with less mode collapse and blurry artifacts problems. In the iteration, GAN takes the result of the iteration and text description again and synthesizes a better resolution pixels better shape and well color image with very less mode collapse and blurry artifacts problems. In the last iteration, GAN takes the result of the iteration and text description as well and synthesizes a high-resolution well shape and clear image with almost no mode collapse and blurry artifacts problems. Our proposed iGAN shows a significant performance on CUB birds and Oxford-102 flowers datasets. Moreover, iGAN improves the inception score and human rank as compare to the other state-of-the-art methods. © 2019, Springer Nature Switzerland AG.","Pixels; Adversarial networks; CUB dataset; Human rank; Image synthesis; Inception score; Iterative GAN; Oxford-102 dataset; Iterative methods","CUB dataset; Generative Adversarial Network (GAN); Human rank; Inception score; Iterative GAN; Oxford-102 dataset; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85076427909"
"Lin S.; Chen L.; Zou Q.; Tian W.","Lin, Shaobo (57210585215); Chen, Long (57201430734); Zou, Qin (55628590470); Tian, Wei (57199663141)","57210585215; 57201430734; 55628590470; 57199663141","High-resolution driving scene synthesis using stacked conditional gans and spectral normalization","2019","Proceedings - IEEE International Conference on Multimedia and Expo","2019-July","","8784886","1330","1335","5","10.1109/ICME.2019.00231","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070939788&doi=10.1109%2fICME.2019.00231&partnerID=40&md5=067211ce5a62b8f2e649a6c9208c479b","Large-scale dataset plays a key role in the driving scene understanding for deep learning based-autonomous driving tasks. Due to the fact that the annotation for a large number of images is extremely labor-intensive and time-consuming, many researchers turn to using image-synthesis techniques for automatic construction of training data. However, traditional methods often have difficulties in producing high-definition driving scene images. To tackle this problem, in this paper, we propose a novel deep model - hdCGAN - for high-definition image-to-image translation. The hdCGAN is built on a conditional GAN in combination with a spectral normalization. Moreover, we improve the hdCGAN by using a stacked network architecture and the enhanced model is called stack-hdCGAN. With the guidance of multi-scale discriminators and the constraint of spectral normalization in the training procedure, the learned models can generate high-resolution and high-quality driving scene images from corresponding semantic segmentation maps. Quantitative and qualitative evaluations on the Cityscapes dataset demonstrate the effectiveness of the proposed models. © 2019 IEEE.","Deep learning; Large dataset; Network architecture; Semantics; Adversarial networks; Automatic construction; Image translation; Qualitative evaluations; Scene understanding; Semantic segmentation; Spectral normalization; Stack structure; Image segmentation","Conditional generative adversarial networks; Image to image translation; Spectral normalization; Stack structure","Conference paper","Final","","Scopus","2-s2.0-85070939788"
"Kotera M.; Togo R.; Ogawa T.; Haseyama M.","Kotera, Megumi (57215820985); Togo, Ren (57190438320); Ogawa, Takahiro (35332753900); Haseyama, Miki (56238860900)","57215820985; 57190438320; 35332753900; 56238860900","Aesthetic style transfer through text-to-image synthesis and image-to-image translation","2019","2019 IEEE 8th Global Conference on Consumer Electronics, GCCE 2019","","","9015508","483","484","1","10.1109/GCCE46687.2019.9015508","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081957348&doi=10.1109%2fGCCE46687.2019.9015508&partnerID=40&md5=63b4cd98b6c509edf1477aa53d549832","This paper presents a style transfer method combining generative adversarial networks and style transfer networks. In the previous style transfer methods, transformation from one image to another has been proposed. On the other hand, our method enables style transfer from a text to an image. This will be helpful when there are no images that represent the desired style. Experimental results show the effectiveness of our method. © 2019 IEEE.","Electronics industry; Adversarial networks; Image synthesis; Image translation; Transfer method; Transfer network; Electronics engineering","","Conference paper","Final","","Scopus","2-s2.0-85081957348"
"Agarwal P.; Poddar S.; Hazarika A.; Rahaman H.","Agarwal, Pranav (57215118841); Poddar, Soumyajit (55321684000); Hazarika, Anakhi (57209273202); Rahaman, Hafizur (57207594144)","57215118841; 55321684000; 57209273202; 57207594144","Learning to synthesize faces using voice clips for Cross-Modal biometric matching","2019","Proceedings of 2019 IEEE Region 10 Symposium, TENSYMP 2019","","","8971330","397","402","5","10.1109/TENSYMP46218.2019.8971330","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079285736&doi=10.1109%2fTENSYMP46218.2019.8971330&partnerID=40&md5=364cedafdf290c437409062bd0319fac","Cross-Modal biometric matching has been a scarcely explored field but carries several important applications and aims to further secure the currently existing security systems. In this paper, a framework for cross-modal biometric matching is presented, where faces of an individual are generated using his/her voice clips and further the synthesized faces are tested using a face classification network. Generative Adversarial Network (GAN) has become a recent trend in deep learning and has been widely used for image synthesis. We explore the advancements of Convolutional Neural Network (CNN) for feature extraction and generative networks for image synthesis. In the experiment, we compare the performance of Variational Autoencoders(VAE), Conditional Generative Adversarial Networks(C-GAN) and Regularized Conditional Generative Adversarial Networks(RC-GAN) and show that RC-GAN that is C-GAN with a regularization factor added to its loss is able to generate faces corresponding to the true identity of the voice clips with the best accuracy of 84.52% while VAE generates a less noise prone image with the highest PSNR of 28.276 decibels but with an accuracy of 72.61%. © 2019 IEEE.","Biometrics; Convolution; Deep learning; Deep neural networks; Image processing; Adversarial networks; Autoencoders; Biometric matching; Cross-modal; Face classification; Image synthesis; Recent trends; True identity; Convolutional neural networks","Biometric matching; Convolutional Neural Network; Deep Learning; Generative Adversarial Network","Conference paper","Final","","Scopus","2-s2.0-85079285736"
"Wang S.; Gao H.; Zhu Y.; Zhang W.; Chen Y.","Wang, Su (57212575903); Gao, Honghao (36442463200); Zhu, Yonghua (55723795700); Zhang, Weilin (57204922006); Chen, Yihai (8241850600)","57212575903; 36442463200; 55723795700; 57204922006; 8241850600","A Food Dish Image Generation Framework Based on Progressive Growing GANs","2019","Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST","292","","","323","333","10","10.1007/978-3-030-30146-0_22","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077124787&doi=10.1007%2f978-3-030-30146-0_22&partnerID=40&md5=ea25670cdb3839c0a7dd90e2cf792670","The generative adversarial networks (GANs) have demonstrated the ability to synthesize realistic images. However, there are few researches applying GANs into the field of food image synthesis. In this paper, we propose an extension to GANs for generating more realistic food dish images with rich detail, which adds a food condition that contains taste and other information. That makes the model generate images with rich details. To improve the quality of the generated image, the taste information condition is added to each stage of the generator and discriminator. First, the model learns embedding conditions of food information, including ingredients, cooking methods, tastes and cuisines. Secondly, the training model grows progressively, and the model learns details increasingly during the training process, which allows the model to generate images with rich details. To demonstrate the effectiveness of our proposed model, we collect a dataset called Food-121, which includes the names of the food, ingredients, cooking methods, tastes, and cuisines. The results of experiment show that our model can produce complex details of food dish image and obtain high inception score on the Food-121 dataset compared with other models. © 2019, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","Cooking; Adversarial networks; Cooking methods; GANs; Image generations; Image synthesis; Realistic images; Training model; Training process; Image enhancement","Food dataset; Food dish image synthesis; GANs","Conference paper","Final","","Scopus","2-s2.0-85077124787"
"Zheng N.; Hu L.; Song X.; Cao D.; Chen Z.; Nie L.","Zheng, Na (57215300452); Hu, Linmei (56181376700); Song, Xuemeng (57001779600); Cao, Da (36951628300); Chen, Zhaozheng (57215288742); Nie, Liqiang (57205067955)","57215300452; 56181376700; 57001779600; 36951628300; 57215288742; 57205067955","Virtually trying on new clothing with arbitrary poses","2019","MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia","","","","266","274","8","10.1145/3343031.3350946","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074858387&doi=10.1145%2f3343031.3350946&partnerID=40&md5=b8f2119e54cd5a68c2ce293400ec8dcf","Thanks to the recent advance in the multimedia techniques, increasing research attention has been paid to the virtual try-on task, especially with the 2D image modeling. The traditional try-on task aims to align the target clothing item naturally to the given person's body and hence present a try-on look of the person. However, in practice, people may also be interested in their try-on looks with different poses. Therefore, in this work, we introduce a new try-on setting, which enables the changes of both the clothing item and the person's pose. Towards this end, we propose a pose-guided virtual try-on scheme based on the generative adversarial networks (GANs) with a bi-stage strategy. In particular, in the first stage, we propose a shape enhanced clothing deformation model for deforming the clothing item, where the user body shape is incorporated as the intermediate guidance. For the second stage, we present an attentive bidirectional GAN, which jointly models the attentive clothing-person alignment and bidirectional generation consistency. For evaluation, we create a large-scale dataset, FashionTryOn, comprising 28, 714 triplets with each consisting of a clothing item image and two model images in different poses. Extensive experiments on FashionTryOn validate the superiority of our model over the state-of-the-art methods. © 2019 Association for Computing Machinery.","Deformation; Adversarial networks; Deformation modeling; Image synthesis; Large-scale dataset; Multimedia techniques; Pose Transformation; State-of-the-art methods; Virtual try-on; Large dataset","Generative Adversarial Networks; Person Image Synthesis; Pose Transformation; Virtual Try-On System","Conference paper","Final","","Scopus","2-s2.0-85074858387"
"Cai Y.; Wang X.; Yu Z.; Li F.; Xu P.; Li Y.; Li L.","Cai, Yali (57213825966); Wang, Xiaoru (55736762500); Yu, Zhihong (56299044600); Li, Fu (16550324600); Xu, Peirong (57210131182); Li, Yueli (57213841558); Li, Lixian (57213826845)","57213825966; 55736762500; 56299044600; 16550324600; 57210131182; 57213841558; 57213826845","Dualattn-GAN: Text to Image Synthesis with Dual Attentional Generative Adversarial Network","2019","IEEE Access","7","","8930532","183706","183716","10","10.1109/ACCESS.2019.2958864","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078072673&doi=10.1109%2fACCESS.2019.2958864&partnerID=40&md5=6af0768b8f021d15d9afad6a9021a70e","Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance. © 2019 IEEE.","Behavioral research; Neural networks; Semantics; Adversarial networks; Residual structure; Spectral normalization; textual attention; Visual Attention; Image processing","Generative adversarial network; inverted residual structure; spectral normalization; textual attention; visual attention","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078072673"
"Ullah I.; Chikontwe P.; Park S.H.","Ullah, Ihsan (57610951000); Chikontwe, Philip (57195637251); Park, Sang Hyun (57188954175)","57610951000; 57195637251; 57188954175","Catheter synthesis in X-Ray fluoroscopy with generative adversarial networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11843 LNCS","","","125","133","8","10.1007/978-3-030-32281-6_13","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075659797&doi=10.1007%2f978-3-030-32281-6_13&partnerID=40&md5=6ab22ee000c4664793a50b2138cae25e","Accurate localization of catheters or guidewires in fluoroscopy images is important to improve the stability of intervention procedures as well as the development of surgical navigation systems. Recently, deep learning methods have been proposed to improve performance, however these techniques require extensive pixel-wise annotations. Moreover, the human annotation effort is equally expensive. In this study, we mitigate this labeling effort using generative adversarial networks (cycleGAN) wherein we synthesize realistic catheters in flouroscopy from localized guidewires in camera images whose annotations are cheaper to acquire. Our approach is motivated by the fact that catheters are tubular structures with varying profiles, thus given a guidewire in a camera image, we can obtain the centerline that follows the profile of a catheter in an X-ray image and create plausible X-ray images composited with such a centerline. In order to generate an image similar to the actual X-ray image, we propose a loss term that includes perceptual loss alongside the standard cycle loss. Experimental results show that the proposed method has better performance than the conventional GAN and generates images with consistent quality. Further, we provide evidence to the development of methods that leverage such synthetic composite images in supervised settings. © Springer Nature Switzerland AG 2019.","Cameras; Catheters; Deep learning; Fluorescent screens; Medical imaging; Navigation systems; Neural networks; Robots; Surgical equipment; Adversarial learning; Adversarial networks; Convolutional neural network; Image synthesis; Image translation; Improve performance; Surgical navigation systems; Synthetic composites; Image enhancement","Adversarial learning; Catheter robot; Convolutional neural networks; Image synthesis; Image translation","Conference paper","Final","","Scopus","2-s2.0-85075659797"
"Cheng J.; Chen Y.-P.P.; Li M.; Jiang Y.-G.","Cheng, Juntong (57211680843); Chen, Yi-Ping Phoebe (57195375471); Li, Minjun (57201318659); Jiang, Yu-Gang (14054081900)","57211680843; 57195375471; 57201318659; 14054081900","TC-GAN: Triangle cycle-consistent GANs for face frontalization with facial features preserved","2019","MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia","","","","220","228","8","10.1145/3343031.3351031","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074871242&doi=10.1145%2f3343031.3351031&partnerID=40&md5=81a4898d7268e790ccea9016f2e3cd21","Face frontalization has always been an important field. Recently, with the introduction of generative adversarial networks (GANs), face frontalization has achieved remarkable success. A critical challenge during face frontalization is to ensure the features of the original profile image are retained. Even though some state-of-the-art methods can preserve identity features while rotating the face to the frontal view, they still have difficulty preserving facial expression features. Therefore, we propose the novel triangle cycle-consistent generative adversarial networks for the face frontalization task, termed TC-GAN. Our networks contain two generators and one discriminator. One of the generators generates the frontal contour, and the other generates the facial features. They work together to generate a photo-realistic frontal view of the face. We also introduce cycle-consistent loss to retain feature information effectively. To validate the advantages of TC-GAN, we apply it to the face frontalization task on two datasets. The experimental results demonstrate that our method can perform large-pose face frontalization while preserving the facial features (both identity and expression). To the best of our knowledge, TC-GAN outperforms the state-of-the-art methods in the preservation of facial identity and expression features during face frontalization. © 2019 Association for Computing Machinery.","Adversarial networks; Critical challenges; Face frontalization; Facial Expressions; Feature information; GANs; Image synthesis; State-of-the-art methods","Face frontalization; GANs; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85074871242"
"Bellemo V.; Burlina P.; Yong L.; Wong T.Y.; Ting D.S.W.","Bellemo, Valentina (57209682213); Burlina, Philippe (6603713214); Yong, Liu (57209691868); Wong, Tien Yin (7403147159); Ting, Daniel Shu Wei (37010354600)","57209682213; 6603713214; 57209691868; 7403147159; 37010354600","Generative Adversarial Networks (GANs) for Retinal Fundus Image Synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11367 LNCS","","","289","302","13","10.1007/978-3-030-21074-8_24","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068487313&doi=10.1007%2f978-3-030-21074-8_24&partnerID=40&md5=26d88128aa096613ad2c6f0ad40520ff","The lack of access to large annotated datasets and legal concerns regarding patient privacy are limiting factors for many applications of deep learning in the retinal image analysis domain. Therefore the idea of generating synthetic retinal images, indiscernible from real data, has gained more interest. Generative adversarial networks (GANs) have proven to be a valuable framework for producing synthetic databases of anatomically consistent retinal fundus images. In Ophthalmology, GANs in particular have shown increased interest. We discuss here the potential advantages and limitations that need to be addressed before GANs can be widely adopted for retinal imaging. © 2019, Springer Nature Switzerland AG.","Computer vision; Deep learning; Large dataset; Ophthalmology; Surveying; Adversarial networks; Annotated datasets; Legal concern; Patient privacies; Retinal fundus images; Retinal image analysis; Retinal imaging; Synthetic database; Medical imaging","Deep learning; Generative adversarial networks; Medical imaging; Retinal fundus images; Survey","Conference paper","Final","","Scopus","2-s2.0-85068487313"
"Ganesan P.; Rajaraman S.; Long R.; Ghoraani B.; Antani S.","Ganesan, Prasanth (56966315000); Rajaraman, Sivaramakrishnan (51764361200); Long, Rodney (7401716102); Ghoraani, Behnaz (16244827900); Antani, Sameer (6701355570)","56966315000; 51764361200; 7401716102; 16244827900; 6701355570","Assessment of Data Augmentation Strategies Toward Performance Improvement of Abnormality Classification in Chest Radiographs","2019","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","8857516","841","844","3","10.1109/EMBC.2019.8857516","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077875780&doi=10.1109%2fEMBC.2019.8857516&partnerID=40&md5=f9ea3c81f889dc491a6197ae6058c552","Image augmentation is a commonly performed technique to prevent class imbalance in datasets to compensate for insufficient training samples, or to prevent model overfitting. Traditional augmentation (TA) techniques include various image transformations, such as rotation, translation, channel splitting, etc. Alternatively, Generative Adversarial Network (GAN), due to its proven ability to synthesize convincingly-realistic images, has been used to perform image augmentation as well. However, it is unclear whether GAN augmentation (GA) strategy provides an advantage over TA for medical image classification tasks. In this paper, we study the usefulness of TA and GA for classifying abnormal chest X-ray (CXR) images. We first trained a progressive-growing GAN (PG-GAN) to synthesize high-resolution CXRs for performing GA. Then, we trained an abnormality classifier using three training sets individually - training set with TA, with GA and with no augmentation (NA). Finally, we analyzed the abnormality classifier's performance for the three training cases, which led to the following conclusions: (1) GAN strategy is not always superior to TA for improving the classifier's performance; (2) in comparison to NA, however, both TA and GA leads to a significant performance improvement; and, (3) increasing the quantity of images in TA and GA strategies also improves the classifier's performance. © 2019 IEEE.","Radiography; Thorax; Classification (of information); Deep learning; Medical imaging; X rays; Adversarial networks; Chest radiographs; Chest x-rays; Classifier's performance; Data augmentation; Image synthesis; Image transformations; Progressive-growing GAN; diagnostic imaging; radiography; thorax; Image enhancement","Abnormality classification; Chest X-ray; Deep learning; Generative adversarial network; Medical image synthesis; Progressive-growing GAN","Conference paper","Final","","Scopus","2-s2.0-85077875780"
"Wu J.; Huang Z.; Acharya D.; Li W.; Thoma J.; Paudel D.P.; Van Gool L.","Wu, Jiqing (56566943500); Huang, Zhiwu (55643660000); Acharya, DInesh (57192573752); Li, Wen (54380409600); Thoma, Janine (57195062496); Paudel, Danda Pani (56422108100); Van Gool, Luc (22735702300)","56566943500; 55643660000; 57192573752; 54380409600; 57195062496; 56422108100; 22735702300","Sliced wasserstein generative models","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8953921","3708","3717","9","10.1109/CVPR.2019.00383","42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078728914&doi=10.1109%2fCVPR.2019.00383&partnerID=40&md5=5c479c5429d87a838970aa2a6ff20cd7","In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks--Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner. © 2019 IEEE.","Benchmarking; Computer vision; Deep learning; One dimensional; Concrete applications; High resolution image; Marginal distribution; Optimization method; Orthogonal projection; Sliced wasserstein distances; State-of-the-art performance; Video synthesis; Learning systems","Deep Learning; Image and Video Synthesis; Optimization Methods","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078728914"
"Feng Y.; Chai X.; Ba Q.; Yang G.","Feng, Yile (57212481572); Chai, Xiaoqi (57194972844); Ba, Qinle (57045730200); Yang, Ge (56903300600)","57212481572; 57194972844; 57045730200; 56903300600","Quality Assessment of Synthetic Fluorescence Microscopy Images for Image Segmentation","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8802971","814","818","4","10.1109/ICIP.2019.8802971","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076819500&doi=10.1109%2fICIP.2019.8802971&partnerID=40&md5=ecc71244abe84e4b36c4d15126eb08f5","Synthetic images are widely used in image segmentation for algorithm training and performance assessment. Recently, advances in image synthesis techniques, especially generative adversarial networks (GANs), have made it possible to generate fluorescence microscopy images with remarkably realistic appearance. However, intuitive and specific metrics to assess the quality of these images remain lacking. Here, we propose three quality metrics that quantify the fidelity of the foreground signal, the background noise, and blurring, respectively, of synthesized fluorescence microscopy images. Using these metrics, we examine images of mitochondria synthesized by two representative GANs: pix2pix, which requires paired training data, and CycleGAN, which does not require paired training data. We find that both networks generate realistic images and achieve similar fidelity in reproducing background noise and blurring of real images. However, CycleGAN achieves significantly higher fidelity than pix2pix in reproducing intensity patterns of real mitochondria. When used to train the U-Net for segmentation, images synthesized by both networks achieve performance on par with real images. Overall, we have developed a method to assess quality of synthetic fluorescence microscopy images and to evaluate their training performance in image segmentation. The quality metrics proposed are general and can be used to assess fluorescence microscopy images synthesized by different methods. © 2019 IEEE.","","fluorescence microscopy; generative adversarial network; image segmentation; Quality assessment; synthetic image","Conference paper","Final","","Scopus","2-s2.0-85076819500"
"Amalia A.N.; Huda A.F.; Ramdania D.R.; Irfan M.","Amalia, Aifa Nur (57216783074); Huda, Arief Fatchul (57195075394); Ramdania, Diena Rauda (57212210161); Irfan, Mohamad (57191849732)","57216783074; 57195075394; 57212210161; 57191849732","Making a Batik Dataset for Text to Image Synthesis Using Generative Adversarial Networks","2019","Proceeding of 2019 5th International Conference on Wireless and Telematics, ICWT 2019","","","8978233","","","","10.1109/ICWT47785.2019.8978233","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084669654&doi=10.1109%2fICWT47785.2019.8978233&partnerID=40&md5=913953f6254049c910bb09567e054dc9","Batik is a cultural heritage as well as the identity of the Indonesian nation that needs to be preserved. The use of deep learning allows the process of making batik patterns done by computer through the mechanism of text-to-image synthesis without humans needing to make it directly. The main contribution of this research is to produce a synthetic batik pattern that is similar to the original without removing the characteristics possessed by each batik pattern. This process of text synthesis to images uses the Generative Adversarial Networks (GAN) by first creating a system that can learn from a datasets. A varied and structured dataset can make it easier for the system to learn faster. In this study, a batik dataset was created for the synthesis of text into images. © 2019 IEEE.","Deep learning; Adversarial networks; Cultural heritages; Image synthesis; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85084669654"
"Zhao C.; Chen C.; He Z.; Wu Z.","Zhao, Caidan (23669845300); Chen, Caiyun (57200192577); He, Zeping (57205128728); Wu, Zhiqiang (7501414539)","23669845300; 57200192577; 57205128728; 7501414539","Application of auxiliary classifier wasserstein generative adversarial networks in wireless signal classification of illegal unmanned aerial vehicles","2018","Applied Sciences (Switzerland)","8","12","2664","","","","10.3390/app8122664","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058713313&doi=10.3390%2fapp8122664&partnerID=40&md5=77e80ad058049a135ab41cb49b38258f","Recently, many studies have reported on image synthesis based on Generative Adversarial Networks (GAN). However, the use of GAN does not provide much attention on the signal classification problem. In the context of using wireless signals to classify illegal Unmanned Aerial Vehicles (UAVs), this paper explores the feasibility of using GAN to improve the training datasets and obtain a better classification model, thereby improving the accuracy of classification. First, we use the generative model of GAN to generate a large datasets, which does not need manual annotation. At the same time, the discriminative model of GAN is improved to classify the types of signals based on the loss function of the discriminative model. Finally, this model can be used to the outdoor environment and obtain a real-time illegal UAVs signal classification system. Our experiments confirmed that the improvements on the Auxiliary Classifier Generative Adversarial Networks (AC-GANs) by limited datasets achieve excellent results. The recognition rate can reach more than 95% in the indoor environment, and this method is also applicable in the outdoor environment. Moreover, based on the theory of Wasserstein GANs (WGAN) and AC-GANs, a more robust Auxiliary Classifier Wasserstein GANs (AC-WGANs) model is obtained, which is suitable for multi-class UAVs. Through the combination of AC-WGANs and Universal Software Radio Peripheral (USRP) B210 software defined radio (SDR) platform, a real-time UAVs signal classification system is also implemented. © 2018 by the authors.","","AC-WGANs; Classify model; GAN; USRP; Wireless signals","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85058713313"
"Zheng C.; Xie X.; Zhou K.; Chen B.; Chen J.; Ye H.; Li W.; Qiao T.; Gao S.; Yang J.; Liu J.","Zheng, Ce (15752410500); Xie, Xiaolin (57212277116); Zhou, Kang (57203987052); Chen, Bang (57217016199); Chen, Jili (57200177644); Ye, Haiyun (35219384300); Li, Wen (57221637605); Qiao, Tong (57002786900); Gao, Shenghua (35224747100); Yang, Jianlong (57212326066); Liu, Jiang (23389932700)","15752410500; 57212277116; 57203987052; 57217016199; 57200177644; 35219384300; 57221637605; 57002786900; 35224747100; 57212326066; 23389932700","Assessment of generative adversarial networks model for synthetic optical coherence tomography images of retinal disorders","2020","Translational Vision Science and Technology","9","2","29","1","9","8","10.1167/tvst.9.2.29","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088699544&doi=10.1167%2ftvst.9.2.29&partnerID=40&md5=8468ecea9866ffb5c996ae1ba75fb506","Purpose: To assess whether a generative adversarial network (GAN) could synthesize realistic optical coherence tomography (OCT) images that satisfactorily serve as the educational images for retinal specialists, and the training datasets for the classification of various retinal disorders using deep learning (DL). Methods: The GANs architecture was adopted to synthesize high-resolution OCT images trained on a publicly available OCT dataset, including urgent referrals (37,206 OCT images from eyes with choroidal neovascularization, and 11,349 OCT images from eyes with diabetic macular edema) and nonurgent referrals (8617 OCT images from eyes with drusen, and 51,140 OCT images from normal eyes). Four hundred real and synthetic OCT images were evaluated by two retinal specialists (with over 10 years of clinical retinal experience) to assess image quality. We further trained two DL models on either real or synthetic datasets and compared the performance of urgent versus nonurgent referrals diagnosis tested on a local (1000 images from the public dataset) and clinical validation dataset (278 images from Shanghai Shibei Hospital). Results: The image quality of real versus synthetic OCT images was similar as assessed by two retinal specialists. The accuracy of discrimination of real versus synthetic OCT images was 59.50% for retinal specialist 1 and 53.67% for retinal specialist 2. For the local dataset, the DL model trained on real (DL_Model_R) and synthetic OCT images (DL_Model_S) had an area under the curve (AUC) of 0.99, and 0.98, respectively. For the clinical dataset, the AUC was 0.94 for DL_Model_R and 0.90 for DL_Model_S. Conclusions: The GAN synthetic OCT images can be used by clinicians for educational purposes and for developing DL algorithms. Translational Relevance: The medical image synthesis based on GANs is promising in humans and machines to fulfill clinical tasks. © 2020 The Authors.","algorithm; area under the curve; Article; deep learning; diabetic retinopathy; diagnostic accuracy; diagnostic test accuracy study; entropy; eye disease; generative adversarial network; human; image quality; learning algorithm; machine learning; optical coherence tomography; predictive value; receiver operating characteristic; retrospective study; sensitivity and specificity; subretinal neovascularization; training; validation process","Deep learning; Generative adversarial networks; Optical coherence tomography; Retinal disorders","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088699544"
"Mei L.; Ran X.; Hu J.","Mei, Lingrui (57215416318); Ran, Xuming (57215411315); Hu, Jin (55499481800)","57215416318; 57215411315; 55499481800","Weakly Supervised Attention Inference Generative Adversarial Network for Text-to-Image","2019","2019 IEEE Symposium Series on Computational Intelligence, SSCI 2019","","","9002666","1574","1578","4","10.1109/SSCI44817.2019.9002666","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080911926&doi=10.1109%2fSSCI44817.2019.9002666&partnerID=40&md5=e2e979645710c2bea8206edae70471c8","Text-to-Image is a significant problem in computer vision. Recently, there are some problems in the quality and semantic consistency of the generated image. In this paper we propose an approach for Text-to-Image synthesis by focusing on the perception. We use text embeddings to generate semantic feature maps before target images synthesis instead of generating target images directly. The ground truth semantic layouts are calculated by interpretable classification network, and we will learn to generate semantic layouts before inferring target images from them. We have trained our approach on the CUB2011 dataset and verified the quality of its generation and the interpretability of the network in simple background and small scale feature generation. © 2019 IEEE.","Artificial intelligence; Semantics; Adversarial networks; attention inference; Classification networks; Interpretability; Semantic consistency; Semantic features; Small-scale features; Text-to-Image; Image processing","attention inference; generative adversarial network; Text-to-Image.","Conference paper","Final","","Scopus","2-s2.0-85080911926"
"Yang F.; Lu Z.; Qiu G.; Lin J.; Zhang Q.","Yang, Fei (57201912129); Lu, Zheng (55186082900); Qiu, Guoping (7103292111); Lin, Jing (57209198474); Zhang, Qian (57221143411)","57201912129; 55186082900; 7103292111; 57209198474; 57221143411","Capsule based image synthesis for interior design effect rendering","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11365 LNCS","","","183","198","15","10.1007/978-3-030-20873-8_12","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066790296&doi=10.1007%2f978-3-030-20873-8_12&partnerID=40&md5=6066945ecda14414ea48a512227bacad","Effect rendering that renders 3D model to 2D images with various coloring and lighting effects, is an important step in home interior design. Traditional way of manual rendering using professional software is very labor intensive and time consuming. In this paper, we present a novel capsule based conditional generative adversarial network that can automatically synthesize an indoor image with realistic and aesthetically pleasing rendering effect from a given plain image rendered without any effects from a interior designed 3D model. By adapting capsule blocks in both generator and discriminator and a novel multi-way loss function inside discriminator, our framework is able to generate more realistic rendering effect at both detail and global levels. In addition, a novel line preservation loss is introduced not only to help preserve the properties that are independent of lighting effect, but also improves the lighting effect along those lines. We apply our technique on a dataset specially prepared for interior design effect rendering and systematically compare our approach with multiple state-of-the-art methods. © Springer Nature Switzerland AG 2019.","3D modeling; Architectural design; Computer vision; Interiors (building); Lighting; Three dimensional computer graphics; Adversarial networks; Capsule; Image synthesize; Interior designs; Lighting effects; Professional software; Realistic rendering; Rendering; Rendering (computer graphics)","Capsule; GAN; Image synthesize; Rendering","Conference paper","Final","","Scopus","2-s2.0-85066790296"
"Guo Y.; Chen Q.; Chen J.; Wu Q.; Shi Q.; Tan M.","Guo, Yong (57190176914); Chen, Qi (57211763557); Chen, Jian (55954396500); Wu, Qingyao (36603693400); Shi, Qinfeng (24829507300); Tan, Mingkui (22837202600)","57190176914; 57211763557; 55954396500; 36603693400; 24829507300; 22837202600","Auto-Embedding Generative Adversarial Networks for High Resolution Image Synthesis","2019","IEEE Transactions on Multimedia","21","11","8676365","2726","2737","11","10.1109/TMM.2019.2908352","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074449899&doi=10.1109%2fTMM.2019.2908352&partnerID=40&md5=3dca3efd39e3a269f30b25b2465e27a1","Generating images via a generative adversarial network (GAN) has attracted much attention recently. However, most of the existing GAN-based methods can only produce low-resolution images of limited quality. Directly generating high-resolution images using GANs is nontrivial, and often produces problematic images with incomplete objects. To address this issue, we develop a novel GAN called auto-embedding generative adversarial network, which simultaneously encodes the global structure features and captures the fine-grained details. In our network, we use an autoencoder to learn the intrinsic high-level structure of real images and design a novel denoiser network to provide photo-realistic details for the generated images. In the experiments, we are able to produce 512 × 512 images of promising quality directly from the input noise. The resultant images exhibit better perceptual photo-realism, that is, with sharper structure and richer details, than other baselines on several datasets, including Oxford-102 Flowers, Caltech-UCSD Birds (CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale Scene Understanding (LSUN), and ImageNet. © 1999-2012 IEEE.","Embeddings; Large dataset; Adversarial learning; Adversarial networks; Auto encoders; Generative model; High resolution image; High-level structure; Low dimensional embedding; Low resolution images; Learning systems","adversarial learning; autoencoder; Generative models; low-dimensional embedding","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85074449899"
"Liu J.; Tian Y.; Ağıldere A.M.; Haberal K.M.; Coşkun M.; Duzgol C.; Akin O.","Liu, Jingya (57203969127); Tian, Yingli (16556710700); Ağıldere, A. Muhteşem (6603903474); Haberal, K. Murat (57196347621); Coşkun, Mehmet (57210101069); Duzgol, Cihan (57202549574); Akin, Oguz (7005769802)","57203969127; 16556710700; 6603903474; 57196347621; 57210101069; 57202549574; 7005769802","Dyefreenet: Deep virtual contrast ct synthesis","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12417 LNCS","","","80","89","9","10.1007/978-3-030-59520-3_9","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092162112&doi=10.1007%2f978-3-030-59520-3_9&partnerID=40&md5=05943f332669b3277a202474e9d0d113","To highlight structures such as blood vessels and tissues for clinical diagnosis, veins are often infused with contrast agents to obtain contrast-enhanced CT scans. In this paper, the use of a deep learning-based framework, DyeFreeNet, to generate virtual contrast abdominal and pelvic CT images based on the original non-contrast CT images is presented. First, to solve the overfitting issue for a deep learning-based method on small datasets, a pretrained model is obtained through a novel self-supervised feature learning network, whereby the network extracted intensity features from a large-scale, publicly available dataset without the use of annotations and classified four transformed intensity levels. Second, an enhanced high-resolution “primary learning generative adversarial network (GAN)” is then used to learn intensity variations between contrast and non-contrast CT images as well as retain high-resolution representations to yield virtual contrast CT images. Then, to reduce GAN training instability, an “intensity refinement GAN” using a novel cascade intensity refinement strategy is applied to obtain more detailed and accurate intensity variations to yield the final predicted virtual contrast CT images. The generated virtual contrast CTs by the proposed framework directly from non-contrast CTs are quite realistic with the virtual enhancement of the major arterial structures. To the best of our knowledge, this is the first work to synthesize virtual contrast-enhanced abdominal and pelvic CT images from non-contrast CT scans. © Springer Nature Switzerland AG 2020.","Blood vessels; Deep learning; Diagnosis; Image enhancement; Large dataset; Learning systems; Medical imaging; Adversarial networks; Clinical diagnosis; Contrast-enhanced; Contrast-enhanced CT; Intensity features; Intensity variations; Learning-based methods; Refinement strategy; Computerized tomography","Deep learning; Image synthesis; Self-supervised learning; Virtual contrast CT","Conference paper","Final","","Scopus","2-s2.0-85092162112"
"Uzunova H.; Ehrhardt J.; Jacob F.; Frydrychowicz A.; Handels H.","Uzunova, Hristina (57194398082); Ehrhardt, Jan (56985527400); Jacob, Fabian (57200034831); Frydrychowicz, Alex (12798815600); Handels, Heinz (6701686055)","57194398082; 56985527400; 57200034831; 12798815600; 6701686055","Abstract: Multi-scale GANs for memory-effcient generation of high resolution medical images","2020","Informatik aktuell","","","","286","","","10.1007/978-3-658-29267-6_63","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083077278&doi=10.1007%2f978-3-658-29267-6_63&partnerID=40&md5=eab2eedb7e5252e96a3bed065b24b8b8","Generative adversarial networks (GANs) have shown impressive results for photo-realistic image synthesis in the last couple of years. They also offer numerous applications in medical image analysis, such as generating images for data augmentation, image reconstruction and image synthesis for domain adaptation. Despite the undeniable success and the large variety of applications, GANs still struggle to generate images of high resolution. © Springer Fachmedien Wiesbaden GmbH, ein Teil von Springer Nature 2020.","Image reconstruction; Adversarial networks; Data augmentation; Domain adaptation; High resolution; Image synthesis; Photo realistic image synthesis; Medical imaging","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85083077278"
"Štepec D.; Skočaj D.","Štepec, Dejan (57195223615); Skočaj, Danijel (6508184644)","57195223615; 6508184644","Image synthesis as a pretext for unsupervised histopathological diagnosis","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12417 LNCS","","","174","183","9","10.1007/978-3-030-59520-3_18","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092177084&doi=10.1007%2f978-3-030-59520-3_18&partnerID=40&md5=f96a4887c9a5cbcf5299507cd0522024","Anomaly detection in visual data refers to the problem of differentiating abnormal appearances from normal cases. Supervised approaches have been successfully applied to different domains, but require abundance of labeled data. Due to the nature of how anomalies occur and their underlying generating processes, it is hard to characterize and label them. Recent advances in deep generative based models have sparked interest towards applying such methods for unsupervised anomaly detection and have shown promising results in medical and industrial inspection domains. In this work we evaluate a crucial part of the unsupervised visual anomaly detection pipeline, that is needed for normal appearance modelling, as well as the ability to reconstruct closest looking normal and tumor samples. We adapt and evaluate different high-resolution state-of-the-art generative models from the face synthesis domain and demonstrate their superiority over currently used approaches on a challenging domain of digital pathology. Multifold improvement in image synthesis is demonstrated in terms of the quality and resolution of the generated images, validated also against the supervised model. © Springer Nature Switzerland AG 2020.","Anomaly detection; Diagnosis; Image enhancement; Different domains; Digital pathologies; Generative model; High resolution; Histopathological diagnosis; Industrial inspections; State of the art; Unsupervised anomaly detection; Medical imaging","Anomaly detection; Deep-learning; Digital pathology; Generative adversarial networks; Image synthesis; Unsupervised","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092177084"
"Hong Y.; Hwang U.; Yoo J.; Yoon S.","Hong, Yongjun (57207309842); Hwang, Uiwon (57207310001); Yoo, Jaeyoon (57195417223); Yoon, Sungroh (7404035832)","57207309842; 57207310001; 57195417223; 7404035832","How generative adversarial networks and their variants work: An overview","2019","ACM Computing Surveys","52","1","10","","","","10.1145/3301282","175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062420736&doi=10.1145%2f3301282&partnerID=40&md5=f4c10eb248d4cf7c73c111825df08d7e","Generative Adversarial Networks (GANs) have received wide attention in the machine learning field for their potential to learn high-dimensional, complex real data distribution. Specifically, they do not rely on any assumptions about the distribution and can generate real-like samples from latent space in a simple manner. This powerful property allows GANs to be applied to various applications such as image synthesis, image attribute editing, image translation, domain adaptation, and other academic fields. In this article, we discuss the details of GANs for those readers who are familiar with, but do not comprehend GANs deeply or who wish to view GANs from various perspectives. In addition, we explain how GANs operates and the fundamental meaning of various objective functions that have been suggested recently. We then focus on how the GAN can be combined with an autoencoder framework. Finally, we enumerate the GAN variants that are applied to various tasks and other fields for those who are interested in exploiting GANs for their research. © 2019 Association for Computing Machinery.","Learning algorithms; Supervised learning; Adversarial networks; Auto encoders; Domain adaptation; Integral probability metric; Mode collapse; Semi- supervised learning; Machine learning","Domain adaptation; Generative adversarial networks; Integral probability metric; Mode collapse; Semi-supervised learning; Variational auto-encoder","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062420736"
"Zhang Z.; Yang L.; Zheng Y.","Zhang, Zizhao (57020905500); Yang, Lin (55771607100); Zheng, Yefeng (8062522600)","57020905500; 55771607100; 8062522600","Translating and Segmenting Multimodal Medical Volumes with Cycle- and Shape-Consistency Generative Adversarial Network","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8579061","9242","9251","9","10.1109/CVPR.2018.00963","237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058783644&doi=10.1109%2fCVPR.2018.00963&partnerID=40&md5=a0749e6db0e9fc14f591a72c5c3fd996","Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized computed tomography (CT) data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 3D images using unpaired training data, 2) ensuring consistent anatomical structures, which could be changed by geometric distortion in cross-modality synthesis and 3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss, which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4,496 CT and magnetic resonance imaging (MRI) cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively. © 2018 IEEE.","Computer vision; Image enhancement; Image segmentation; Magnetic resonance imaging; Medical imaging; Neural networks; Sampling; Adversarial networks; Anatomical structures; Convolutional neural network; Generalization capability; Geometric distortion; Image synthesis; Volume segmentation; X-ray attenuation; Computerized tomography","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85058783644"
"Frid-Adar M.; Diamant I.; Klang E.; Amitai M.; Goldberger J.; Greenspan H.","Frid-Adar, Maayan (57195642611); Diamant, Idit (13409806100); Klang, Eyal (56080228800); Amitai, Michal (6701865585); Goldberger, Jacob (7005511043); Greenspan, Hayit (7004965553)","57195642611; 13409806100; 56080228800; 6701865585; 7005511043; 7004965553","GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification","2018","Neurocomputing","321","","","321","331","10","10.1016/j.neucom.2018.09.013","789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054131811&doi=10.1016%2fj.neucom.2018.09.013&partnerID=40&md5=31327732d757144493dcab6eb1474f08","Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results increased to 85.7% sensitivity and 92.4% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists’ efforts to improve diagnosis. © 2018 Elsevier B.V.","Computer aided diagnosis; Computerized tomography; Convolution; Deep learning; Image enhancement; Medical imaging; Neural networks; Adversarial networks; Convolutional neural network; Data augmentation; Image synthesis; Lesion classification; Liver lesions; Article; artificial neural network; computer assisted tomography; controlled study; convolutional neural network; diagnostic radiologist; diagnostic test accuracy study; generative adversarial network; human; image analysis; image augmentation; image quality; liver cyst; liver hemangioma; liver metastasis; liver tumor; major clinical study; priority journal; sensitivity and specificity; tumor classification; Image classification","Convolutional neural networks; Data augmentation; Deep learning; Generative adversarial network; Image synthesis; Lesion classification; Liver lesions","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054131811"
"Bailo O.; Ham D.; Shin Y.M.","Bailo, Oleksandr (57194457458); Ham, Dongshik (57216362029); Shin, Young Min (57216374541)","57194457458; 57216362029; 57216374541","Red blood cell image generation for data augmentation using conditional generative adversarial networks","2019","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2019-June","","9025635","1039","1048","9","10.1109/CVPRW.2019.00136","30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083305024&doi=10.1109%2fCVPRW.2019.00136&partnerID=40&md5=659aba339ec54da4c0115fc7aa236fbf","In this paper, we describe how to apply image-to-image translation techniques to medical blood smear data to generate new data samples and meaningfully increase small datasets. Specifically, given the segmentation mask of the microscopy image, we are able to generate photorealistic images of blood cells which are further used alongside real data during the network training for segmentation and object detection tasks. This image data generation approach is based on conditional generative adversarial networks which have proven capabilities to high-quality image synthesis. In addition to synthesizing blood images, we synthesize segmentation mask as well which leads to a diverse variety of generated samples. The effectiveness of the technique is thoroughly analyzed and quantified through a number of experiments on a manually collected and annotated dataset of blood smear taken under a microscope. © 2019 IEEE.","Cells; Computer vision; Image segmentation; Medical imaging; Object detection; Adversarial networks; Data augmentation; High quality images; Image translation; Microscopy images; Network training; Photorealistic images; Segmentation masks; Blood","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85083305024"
"Wang Z.; Healy G.; Smeaton A.F.; Ward T.E.","Wang, Zhengwei (57191625959); Healy, Graham (35069552200); Smeaton, Alan F. (7003631244); Ward, Tomás E. (7402100229)","57191625959; 35069552200; 7003631244; 7402100229","Use of Neural Signals to Evaluate the Quality of Generative Adversarial Network Performance in Facial Image Generation","2020","Cognitive Computation","12","1","","13","24","11","10.1007/s12559-019-09670-y","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070320422&doi=10.1007%2fs12559-019-09670-y&partnerID=40&md5=76f917e7041db4313005f16a2615fcd1","There is a growing interest in using generative adversarial networks (GANs) to produce image content that is indistinguishable from real images as judged by a typical person. A number of GAN variants for this purpose have been proposed; however, evaluating GAN performance is inherently difficult because current methods for measuring the quality of their output are not always consistent with what a human perceives. We propose a novel approach that combines a brain-computer interface (BCI) with GANs to generate a measure we call Neuroscore, which closely mirrors the behavioral ground truth measured from participants tasked with discerning real from synthetic images. This technique we call a neuro-AI interface, as it provides an interface between a human’s neural systems and an AI process. In this paper, we first compare the three most widely used metrics in the literature for evaluating GANs in terms of visual quality and compare their outputs with human judgments. Secondly, we propose and demonstrate a novel approach using neural signals and rapid serial visual presentation (RSVP) that directly measures a human perceptual response to facial production quality, independent of a behavioral response measurement. The correlation between our proposed Neuroscore and human perceptual judgments has Pearson correlation statistics: r(48) = − 0.767, p = 2.089e − 10. We also present the bootstrap result for the correlation i.e., p ≤ 0.0001. Results show that our Neuroscore is more consistent with human judgment compared with the conventional metrics we evaluated. We conclude that neural signals have potential applications for high-quality, rapid evaluation of GANs in the context of visual image synthesis. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Brain computer interface; Correlation methods; Quality control; Adversarial networks; Behavioral response; Human judgments; Pearson correlation; Production quality; Rapid serial visual presentations; Synthetic images; Visual qualities; Image quality","Brain-computer interface; Generative adversarial networks; Human judgments; Neuro-AI interface; Rapid serial visual presentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85070320422"
"Yu Z.; Xiang Q.; Meng J.; Kou C.; Ren Q.; Lu Y.","Yu, Zekuan (57218290944); Xiang, Qing (57209096263); Meng, Jiahao (57661589900); Kou, Caixia (55979025200); Ren, Qiushi (9736022400); Lu, Yanye (55542239400)","57218290944; 57209096263; 57661589900; 55979025200; 9736022400; 55542239400","Retinal image synthesis from multiple-landmarks input with generative adversarial networks","2019","BioMedical Engineering Online","18","1","62","","","","10.1186/s12938-019-0682-x","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066488449&doi=10.1186%2fs12938-019-0682-x&partnerID=40&md5=f2ab2413d05239fc0cdc25fb877a9649","Background: Medical datasets, especially medical images, are often imbalanced due to the different incidences of various diseases. To address this problem, many methods have been proposed to synthesize medical images using generative adversarial networks (GANs) to enlarge training datasets for facilitating medical image analysis. For instance, conventional methods such as image-to-image translation techniques are used to synthesize fundus images with their respective vessel trees in the field of fundus image. Methods: In order to improve the image quality and details of the synthetic images, three key aspects of the pipeline are mainly elaborated: the input mask, architecture of GANs, and the resolution of paired images. We propose a new preprocessing pipeline named multiple-channels-multiple-landmarks (MCML), aiming to synthesize color fundus images from a combination of vessel tree, optic disc, and optic cup images. We compared both single vessel mask input and MCML mask input on two public fundus image datasets (DRIVE and DRISHTI-GS) with different kinds of Pix2pix and Cycle-GAN architectures. A new Pix2pix structure with ResU-net generator is also designed, which has been compared with the other models. Results and conclusion: As shown in the results, the proposed MCML method outperforms the single vessel-based methods for each architecture of GANs. Furthermore, we find that our Pix2pix model with ResU-net generator achieves superior PSNR and SSIM performance than the other GANs. High-resolution paired images are also beneficial for improving the performance of each GAN in this work. Finally, a Pix2pix network with ResU-net generator using MCML and high-resolution paired images are able to generate good and realistic fundus images in this work, indicating that our MCML method has great potential in the field of glaucoma computer-aided diagnosis based on fundus image. © 2019 The Author(s).","Image Processing, Computer-Assisted; Neural Networks (Computer); Retina; Computer aided diagnosis; Forestry; Medical imaging; Network architecture; Ophthalmology; Pipelines; Adversarial networks; Conventional methods; Image translation; Medical data sets; Multiple channels; Multiple landmarks; Retinal image; Training data sets; Article; generative adversarial network; image analysis; image processing; image quality; image resolution; image segmentation; image synthesis; imaging and display; multiple landmark input; optic disk; priority journal; retina; artificial neural network; diagnostic imaging; physiology; procedures; retina; Image enhancement","Generative adversarial networks; Multiple landmarks; Retinal image synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85066488449"
"Sohail M.; Riaz M.N.; Wu J.; Long C.; Li S.","Sohail, Muhammad (57214597776); Riaz, Muhammad Naveed (57208423709); Wu, Jing (56428178500); Long, Chengnian (8718008500); Li, Shaoyuan (7409238141)","57214597776; 57208423709; 56428178500; 8718008500; 7409238141","Unpaired multi-contrast MR image synthesis using generative adversarial networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11827 LNCS","","","22","31","9","10.1007/978-3-030-32778-1_3","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075650515&doi=10.1007%2f978-3-030-32778-1_3&partnerID=40&md5=62ba695f47957febcaa461e7b64d68b9","Magnetic Resonance Imaging (MRI) has been established as an important diagnostic tool for research and clinical purposes. Multi-contrast scans can enhance the accuracy for many deep learning algorithms. However, these scans may not be available in some situations. Thus, it is valuable to synthetically generate non-existent contrasts from the available one. Existing methods based on Generative Adversarial Networks (GANs) lack the freedom to map one image to multiple contrasts using only a single generator and discriminator, hence, requiring training of multiple models for multi-contrast MR synthesis. We present a novel method for multi-contrast MR image synthesis with unpaired data using GANs. Our method leverages the strength of Star-GAN to translate a given image to n contrasts using a single generator and discriminator. We also introduce a new generation loss function, which enforces the generator to produce high-quality images which are perceptually closer to the real ones and exhibit high structural similarity as well. We experiment on IXI dataset to learn all possible mappings among T1 -weighted, T2 -weighted, Proton Density (PD) weighted and Magnetic Resonance Angiography (MRA) images. Qualitative and quantitative comparison against baseline method shows the superiority of our approach. © Springer Nature Switzerland AG 2019.","Clinical research; Deep learning; Diagnosis; Medical imaging; Adversarial networks; Baseline methods; Diagnostic tools; High quality images; Magnetic resonance Angiography; MR imaging; Quantitative comparison; Structural similarity; Magnetic resonance imaging","Generative Adversarial Networks; MR Imaging; Multi-contrast synthesis","Conference paper","Final","","Scopus","2-s2.0-85075650515"
"Teng L.; Fu Z.; Yao Y.","Teng, Long (57216179411); Fu, Zhongliang (56729694600); Yao, Yu (36071006800)","57216179411; 56729694600; 36071006800","Interactive Translation in Echocardiography Training System with Enhanced Cycle-GAN","2020","IEEE Access","8","","9110573","106147","106156","9","10.1109/ACCESS.2020.3000666","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086733719&doi=10.1109%2fACCESS.2020.3000666&partnerID=40&md5=ee0f5024fc3d6ab95a0abc9ef833250b","Interactive translation in echocardiography training system refers to the pixel-wise translation between ultrasound cardiac and theoretical sketch images in the course of hand-on operation. It is capable of efficiently gaining more insights into clinical ultrasound anatomy. However, major studies on the synthesis of ultrasound cardiac image primarily discuss the physical model simulation, while studies on cardiac image segmentation place an emphasis on image processing. Thus, they cannot be easily integrated into one pipeline for interactive translation. This paper presents an enhanced Cycle-GAN for interactive translation. Perceptual loss is introduced to enhance the quality of synthetic ultrasound texture, while Cycle-GAN translates between two modalities. The proposed method is trained on 300 pair images and tested on 68 pair images. As revealed from the experiment results, the proposed method is feasible in interactive translation, and it is superior over Cycle-GAN for ultrasound image synthesis. © 2013 IEEE.","Echocardiography; Textures; Ultrasonics; Cardiac image segmentation; Hand-on; Physical model simulations; Training Systems; Ultrasound cardiac images; Ultrasound images; Image segmentation","computer vision; Echocardiography; generative adversarial network (GAN); interactive translation; medical image analysis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086733719"
"Yue K.; Li Y.; Li H.","Yue, Ke (57215130497); Li, Yidong (54955980500); Li, Huifang (57191956543)","57215130497; 54955980500; 57191956543","Progressive semantic image synthesis via generative adversarial network","2019","2019 IEEE International Conference on Visual Communications and Image Processing, VCIP 2019","","","8966069","","","","10.1109/VCIP47243.2019.8966069","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079222736&doi=10.1109%2fVCIP47243.2019.8966069&partnerID=40&md5=b75e8fd26f03addf1de580814aab83cc","Semantic image synthesis via text description is a desirable and challenging task, which requires more protection of the text irrelevant content in the original image. Existing methods directly modify the original image, which become more difficult when encountering high resolution image, and the generated images are also blurred and lack in detail. This paper presents a novel network architecture to progressively manipulate an image starting from low-resolution, while introducing the original image of corresponding size at different stages with our proposed union module to avoid losing of detail. And the progressive design of the network allows us to modify the image from coarse into fine. Compared with the previous methods, our new method can successfully manipulate a high resolution image and generate a new image with background protection and fine details. The experimental results on CUB-200-2011 dataset show that the proposed approach outperforms existing methods in terms of image detail, background protection and high resolution generation. © 2019 IEEE.","Network architecture; Semantic Web; Semantics; Visual communication; Adversarial networks; Different stages; High resolution; High resolution image; Low resolution; Original images; Progressive; Semantic images; Image processing","Generative adversarial network; Progressive; Semantic image synthesis","Conference paper","Final","","Scopus","2-s2.0-85079222736"
"Zhang H.; Xu T.; Li H.; Zhang S.; Wang X.; Huang X.; Metaxas D.N.","Zhang, Han (56098272800); Xu, Tao (56465290800); Li, Hongsheng (57141098300); Zhang, Shaoting (13605200100); Wang, Xiaogang (55736875200); Huang, Xiaolei (57218604182); Metaxas, Dimitris N. (7006359060)","56098272800; 56465290800; 57141098300; 13605200100; 55736875200; 57218604182; 7006359060","StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks","2019","IEEE Transactions on Pattern Analysis and Machine Intelligence","41","8","8411144","1947","1962","15","10.1109/TPAMI.2018.2856256","430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049955346&doi=10.1109%2fTPAMI.2018.2856256&partnerID=40&md5=db42376049610a57b6e10c7af05bc281","Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images. © 2018 IEEE.","Gallium nitride; Gas generators; III-V semiconductors; Image processing; Image resolution; Job analysis; Personnel training; Adversarial networks; Computational model; Generative model; Image generations; Image synthesis; Multi stage; multi-distribution approximation; Photorealistic images; Task analysis; article; synthesis; Network architecture","generative adversarial networks (GANs); Generative models; multi-distribution approximation; multi-stage GANs; photo-realistic image generation; text-to-image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85049955346"
"Kerfoot E.; Puyol-Antón E.; Ruijsink B.; Ariga R.; Zacur E.; Lamata P.; Schnabel J.","Kerfoot, Eric (54408706600); Puyol-Antón, Esther (57190216600); Ruijsink, Bram (57160365500); Ariga, Rina (36674671600); Zacur, Ernesto (25229462100); Lamata, Pablo (12806240900); Schnabel, Julia (7003492772)","54408706600; 57190216600; 57160365500; 36674671600; 25229462100; 12806240900; 7003492772","Synthesising images and labels between mr sequence types with cycleGAN","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11795 LNCS","","","45","53","8","10.1007/978-3-030-33391-1_6","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075699776&doi=10.1007%2f978-3-030-33391-1_6&partnerID=40&md5=cf29f0dbbf8000d6ba8803156a6e57f3","Real-time (RT) sequences for cardiac magnetic resonance imaging (CMR) have recently been proposed as alternatives to standard cine CMR sequences for subjects unable to hold the breath or suffering from arrhythmia. RT image acquisitions during free breathing produce comparatively poor quality images, a trade-off necessary to achieve the high temporal resolution needed for RT imaging and hence are less suitable in the clinical assessment of cardiac function. We demonstrate the application of a CycleGAN architecture to train autoencoder networks for synthesising cine-like images from RT images and vice versa. Applying this conversion to real-time data produces clearer images with sharper distinctions between myocardial and surrounding tissues, giving clinicians a more precise means of visually inspecting subjects. Furthermore, applying the transformation to segmented cine data to produce pseudo-real-time images allows this label information to be transferred to the real-time image domain. We demonstrate the feasibility of this approach by training a U-net based architecture using these pseudo-real-time images which can effectively segment actual real-time images. © Springer Nature Switzerland AG 2019.","Computer aided instruction; Data visualization; Economic and social effects; Heart; Image segmentation; Magnetic resonance imaging; Metadata; Network architecture; Neural networks; Adversarial networks; Cardiac MR; Cardiac quantification; Convolutional neural network; Image synthesis; Medical imaging","Cardiac MR; Cardiac quantification; Convolutional neural networks; Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85075699776"
"Sun W.; Wu T.","Sun, Wei (57210921364); Wu, Tianfu (55476641200)","57210921364; 55476641200","Image synthesis from reconfigurable layout and style","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9008768","10530","10539","9","10.1109/ICCV.2019.01063","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081615773&doi=10.1109%2fICCV.2019.01063&partnerID=40&md5=cde897d7a7ca971b4e06852f9fa545d9","Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a long- standing problem to learn generative models that are capable of synthesizing realistic and sharp images from re- configurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout- and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at https://github.com/iVMCL/LostGANs. © 2019 IEEE.","Codes (symbols); Computer vision; Adversarial networks; Feature normalization; Generative model; High resolution; Learning objects; One-to-many mapping; Standing problems; State-of-the-art performance; Reconfigurable architectures","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081615773"
"Wan C.-H.; Chuang S.-P.; Lee H.-Y.","Wan, Chia-Hung (57203116946); Chuang, Shun-Po (57203115951); Lee, Hung-Yi (34969292900)","57203116946; 57203115951; 34969292900","Towards Audio to Scene Image Synthesis Using Generative Adversarial Network","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8682383","496","500","4","10.1109/ICASSP.2019.8682383","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068987741&doi=10.1109%2fICASSP.2019.8682383&partnerID=40&md5=23831c2b5a8ad4edf2d13151f2eaaf13","Humans can imagine a scene from a sound. We want machines to do so by using conditional generative adversarial networks (GANs). By applying the techniques including spectral norm, projection discriminator and auxiliary classifier, compared with naive conditional GAN, the model can generate images with better quality in terms of both subjective and objective evaluations. Almost three-fourth of people agree that our model have the ability to generate images related to sounds. By inputting different volumes of the same sound, our model output different scales of changes based on the volumes, showing that our model truly knows the relationship between sounds and images to some extent. © 2019 IEEE.","Speech communication; Adversarial networks; Audio-visual; conditional GANs; Cross-modal; Model outputs; Scene image; Spectral norms; Subjective and objective evaluations; Audio signal processing","audio-visual; conditional GANs; cross-modal generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068987741"
"Lei Y.; Fu Y.; Mao H.; Curran W.J.; Liu T.; Yang X.","Lei, Yang (57202715941); Fu, Yabo (57193681581); Mao, Hui (7201795917); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Yang, Xiaofeng (36712893800)","57202715941; 57193681581; 7201795917; 57203070877; 26643332700; 36712893800","Multi-modality MRI arbitrary transformation using unified generative adversarial networks","2020","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11313","","2549794","","","","10.1117/12.2549794","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092556509&doi=10.1117%2f12.2549794&partnerID=40&md5=60f0d322a7179f2e116a3f1e6d57e5e2","We propose a deep learning-based method to perform arbitrary image-to-image translations among four types of MRI scans, including T1-weighted, T1c (T1-weighted with contrast enhancement), Flair and T2-weighted. The goal is to rapidly generate different contrast weighted images which provide comprehensive diagnostic information. The proposed method employs a unified generative adversarial network (unified GAN) which translates any randomly selected MRI scan to the rest scan types. Compared to traditional GAN which takes only images as input, the proposed unified GAN takes both the original image and target domain label as input. The proposed method was evaluated using 50 patients' brain datasets with well-aligned multi-types of MRI scans. Normalized mean absolute error (NMAE) and peak signal-to-noise ratio (PSNR) were used to quantify the synthesis accuracy of the proposed method. With T2 scan as input, the average NMAE was 0.018±0.003, 0.014±0.002, and 0.022±0.005 for T1, T1c and Flair MRI scans, respectively. The average PSNR was 30.1±3.7 dB, 36.3±3.5 dB, and 30.4±4.7 dB for T1, T1c and Flair MRI scans, respectively. Image quality of the synthesized MRI scans are comparable to original MRI scans. © 2020 SPIE. All rights reserved.","Deep learning; Diagnosis; Image enhancement; Image quality; Medical image processing; Signal to noise ratio; Adversarial networks; Contrast Enhancement; Image translation; Learning-based methods; Mean absolute error; Multi modality; Original images; Peak signal to noise ratio; Magnetic resonance imaging","Cross-modality transformation; Magnetic resonance imaging; Medical image synthesis; Multi-modality transformation; Unified generative adversarial network","Conference paper","Final","","Scopus","2-s2.0-85092556509"
"Tang Y.-B.; Oh S.; Tang Y.-X.; Xiao J.; Summers R.M.","Tang, You-Bao (36554909300); Oh, Sooyoun (57209578945); Tang, Yu-Xing (55842098600); Xiao, Jing (57190986110); Summers, Ronald M. (57200247340)","36554909300; 57209578945; 55842098600; 57190986110; 57200247340","CT-realistic data augmentation using generative adversarial network for robust lymph node segmentation","2019","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","10950","","109503V","","","","10.1117/12.2512004","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068106942&doi=10.1117%2f12.2512004&partnerID=40&md5=0d79fd1de1fb4aa0616c4f48e47b792e","As an important task in medical imaging analysis, automatic lymph node segmentation from computed tomography (CT) scans has been studied well in recent years, but it is still very challenging due to the lack of adequately-labeled training data. Manually annotating a large number of lymph node segmentations is expensive and time-consuming. For this reason, data augmentation can be considered as a surrogate of enriching the data. However, most of the traditional augmentation methods use a combination of affine transformations to manipulate the data, which cannot increase the diversity of the data's contextual information. To mitigate this problem, this paper proposes a data augmentation approach based on generative adversarial network (GAN) to synthesize a large number of CT-realistic images from customized lymph node masks. In this work, the pix2pix GAN model is used due to its strength for image generation, which can learn the structural and contextual information of lymph nodes and their surrounding tissues from CT scans. With these additional augmented images, a robust U-Net model is learned for lymph node segmentation. Experimental results on NIH lymph node dataset demonstrate that the proposed data augmentation approach can produce realistic CT images and the lymph node segmentation performance is improved effectively using the additional augmented data, e.g. the Dice score increased about 2.2% (from 80.3% to 82.5%). © 2019 SPIE.","Body fluids; Computer aided diagnosis; Image enhancement; Image segmentation; Medical imaging; Affine transformations; Computed tomography scan; Contextual information; Data augmentation; Image synthesis; Labeled training data; Lymph node; Segmentation performance; Computerized tomography","CT; Data augmentation; GAN; Image synthesis; Lymph node segmentation","Conference paper","Final","","Scopus","2-s2.0-85068106942"
"Wu S.; Zhai W.; Cao Y.","Wu, Shilian (57313744700); Zhai, Wei (57200649163); Cao, Yang (57022583200)","57313744700; 57200649163; 57022583200","PixtextGan: Structure aware text image synthesis for license plate recognition","2019","IET Image Processing","13","14","","2744","2752","8","10.1049/iet-ipr.2018.6588","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077460176&doi=10.1049%2fiet-ipr.2018.6588&partnerID=40&md5=e8e3e7aef094a2342a8491788430bb28","Rapid progress on text image recognition has been achieved with the development of deep-learning techniques. However, it is still a great challenge to achieve a comprehensive license plate recognition in the real scenes, since there are no publicly available large diverse datasets for the training of deep learning models. This paper aims at synthesising of license plate images with generative adversarial networks (GAN), refraining from collecting a vast amount of labelled data. The authors thus propose a novel PixTextGAN that leverages a controllable architecture that generates specific character structures for different text regions to generate synthetic license plate images with reasonable text details. Specifically, a comprehensive structure-aware loss function is presented to preserve the key characteristic of each character region and thus to achieve appearance adaption for better recognition. Qualitative and quantitative experiments demonstrate the superiority of authors’ proposed method in text image synthetisation over state-of-the-art GANs. Further experimental results of license plate recognition on ReId and CCPD dataset demonstrate that using the synthesised images by PixTextGAN can greatly improve the recognition accuracy. © The Institution of Engineering and Technology 2019.","Deep learning; Image enhancement; Image recognition; Large dataset; License plates (automobile); Plates (structural components); Adversarial networks; Key characteristics; Learning techniques; License plate images; License plate recognition; Quantitative experiments; Recognition accuracy; State of the art; Optical character recognition","","Article","Final","","Scopus","2-s2.0-85077460176"
"Baniukiewicz P.; Lutton E.J.; Collier S.; Bretschneider T.","Baniukiewicz, Piotr (55955546900); Lutton, E. Josiah (57202305357); Collier, Sharon (57195234877); Bretschneider, Till (7003316282)","55955546900; 57202305357; 57195234877; 7003316282","Generative Adversarial Networks for Augmenting Training Data of Microscopic Cell Images","2019","Frontiers in Computer Science","1","","10","","","","10.3389/fcomp.2019.00010","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096800557&doi=10.3389%2ffcomp.2019.00010&partnerID=40&md5=e738206c185a6a2e88ac239c08dc76cb","Generative adversarial networks (GANs) have recently been successfully used to create realistic synthetic microscopy cell images in 2D and predict intermediate cell stages. In the current paper we highlight that GANs can not only be used for creating synthetic cell images optimized for different fluorescent molecular labels, but that by using GANs for augmentation of training data involving scaling or other transformations the inherent length scale of biological structures is retained. In addition, GANs make it possible to create synthetic cells with specific shape features, which can be used, for example, to validate different methods for feature extraction. Here, we apply GANs to create 2D distributions of fluorescent markers for F-actin in the cell cortex of Dictyostelium cells (ABD), a membrane receptor (cAR1), and a cortex-membrane linker protein (TalA). The recent more widespread use of 3D lightsheet microscopy, where obtaining sufficient training data is considerably more difficult than in 2D, creates significant demand for novel approaches to data augmentation. We show that it is possible to directly generate synthetic 3D cell images using GANs, but limitations are excessive training times, dependence on high-quality segmentations of 3D images, and that the number of z-slices cannot be freely adjusted without retraining the network. We demonstrate that in the case of molecular labels that are highly correlated with cell shape, like F-actin in our example, 2D GANs can be used efficiently to create pseudo-3D synthetic cell data from individually generated 2D slices. Because high quality segmented 2D cell data are more readily available, this is an attractive alternative to using less efficient 3D networks. © Copyright © 2019 Baniukiewicz, Lutton, Collier and Bretschneider.","","cell image synthesis; data augmentation; GAN; generative models; live cell fluorescence microscopy","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096800557"
"Zhang F.; Zhang T.; Mao Q.; Xu C.","Zhang, Feifei (57138854900); Zhang, Tianzhu (55729040600); Mao, Qirong (7101735930); Xu, Changsheng (56153258200)","57138854900; 55729040600; 7101735930; 56153258200","A Unified Deep Model for Joint Facial Expression Recognition, Face Synthesis, and Face Alignment","2020","IEEE Transactions on Image Processing","29","","9090326","6574","6589","15","10.1109/TIP.2020.2991549","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087548858&doi=10.1109%2fTIP.2020.2991549&partnerID=40&md5=49126617c856c97b3ab959d81ed6f59b","Facial expression recognition, face synthesis, and face alignment are three coherently related tasks and can be solved in a joint framework. To achieve this goal, in this paper, we propose a novel end-to-end deep learning model by exploiting the expression code, geometry code and generated data jointly for simultaneous pose-invariant facial expression recognition, face image synthesis, and face alignment. The proposed deep model enjoys several merits. First, to the best of our knowledge, this is the first work to address these three tasks jointly in a unified deep model to complement and enhance each other. Second, the proposed model can effectively disentangle the global and local identity representation from different expression and geometry codes. As a result, it can automatically generate facial images with different expressions under arbitrary geometry codes. Third, these three tasks can further boost their performance for each other via our model. Extensive experimental results on three standard benchmarks demonstrate that the proposed deep model performs favorably against state-of-the-art methods on the three tasks. © 1992-2012 IEEE.","Codes (symbols); Deep learning; Geometry; Arbitrary geometry; Face alignment; Face image synthesis; Face synthesis; Facial expression recognition; Learning models; Pose invariant; State-of-the-art methods; Face recognition","Facial expression recognition; facial image synthesis; facial landmarks; generative adversarial network","Article","Final","","Scopus","2-s2.0-85087548858"
"Botha J.; Pieterse H.","Botha, Johnny (57188651410); Pieterse, Heloise (55487079900)","57188651410; 55487079900","Fake news and deepfakes: A dangerous threat for 21st century information security","2020","Proceedings of the 15th International Conference on Cyber Warfare and Security, ICCWS 2020","","","","57","66","9","10.34190/ICCWS.20.085","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083375710&doi=10.34190%2fICCWS.20.085&partnerID=40&md5=2848ce147fb73c5d20e10a8e55096133","Fake news, often referred to as junk news or pseudo-news, is a form of yellow journalism or propaganda created with the purpose of distributing deliberate disinformation or false news using traditional print or online social media. Fake news has become a significant problem globally in the past few years. It has become common to find popular individuals and even members of the state using misinformation to influence individuals' actions whether consciously or subconsciously. The latest trend is using Artificial Intelligence (AI) to create fake videos known as “deepfakes”. Deepfake, a portmanteau of “deep learning” and “fake”, is an artificial intelligence-based human image synthesis technique. It is used to combine and superimpose existing images and videos onto source images or videos using a machine learning technique called a “generative adversarial network” (GAN). The combination of the existing and source videos results in a fake video that shows a person or persons performing an action at an event that never occurred in reality. This paper provides an overview of the currently available creation and detection techniques to identify fake news and deepfakes. The outcome of this paper provides the reader with an adequate literature review that summarises the current state of fake news and deepfakes, with special attention given to the tools and technologies that can be used to both create and detect fake news or deepfake material. © 2020. the authors. All Rights Reserved.","Computer crime; Learning systems; Security of data; Social networking (online); Adversarial networks; Image synthesis; Literature reviews; Machine learning techniques; Online social medias; Source images; Tools and technologies; Deep learning","Artificial Intelligence; Deepfake; Detection; Fake news; Machine-learning","Conference paper","Final","","Scopus","2-s2.0-85083375710"
"Qiu H.; Wang C.; Zhu H.; Zhu X.; Gu J.; Han X.","Qiu, H. (57211804273); Wang, C. (57214862646); Zhu, H. (57218146385); Zhu, X. (57211986375); Gu, J. (57212042330); Han, X. (55451013500)","57211804273; 57214862646; 57218146385; 57211986375; 57212042330; 55451013500","Two-phase Hair Image Synthesis by Self-Enhancing Generative Model","2019","Computer Graphics Forum","38","7","","403","412","9","10.1111/cgf.13847","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075069513&doi=10.1111%2fcgf.13847&partnerID=40&md5=22b99d2a8cc687d4b4f4faa522c14590","Generating plausible hair image given limited guidance, such as sparse sketches or low-resolution image, has been made possible with the rise of Generative Adversarial Networks (GANs). Traditional image-to-image translation networks can generate recognizable results, but finer textures are usually lost and blur artifacts commonly exist. In this paper, we propose a two-phase generative model for high-quality hair image synthesis. The two-phase pipeline first generates a coarse image by an existing image translation model, then applies a re-generating network with self-enhancing capability to the coarse image. The self-enhancing capability is achieved by a proposed differentiable layer, which extracts the structural texture and orientation maps from a hair image. Extensive experiments on two tasks, Sketch2Hair and Hair Super-Resolution, demonstrate that our approach is able to synthesize plausible hair image with finer details, and reaches the state-of-the-art. © 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.","Image enhancement; Textures; Adversarial networks; Generative model; Image translation; Low resolution images; Orientation maps; State of the art; Structural textures; Super resolution; Image texture","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075069513"
"","","","9th Pacific-Rim Symposium on Image and Video Technology, PSIVT 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11854 LNCS","","","","","415","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076394301&partnerID=40&md5=2b0119c2eddcec0a4448f95acfe2ced6","The proceedings contain 31 papers. The special focus in this conference is on Image and Video Technology. The topics include: Efficient Self-embedding Data Hiding for Image Integrity Verification with Pixel-Wise Recovery Capability; enhanced Transfer Learning with ImageNet Trained Classification Layer; equine Welfare Assessment: Horse Motion Evaluation and Comparison to Manual Pain Measurements; exposure Correction and Local Enhancement for Backlit Image Restoration; grapevine Nutritional Disorder Detection Using Image Processing; Hierarchical Colour Image Segmentation by Leveraging RGB Channels Independently; high-Resolution Realistic Image Synthesis from Text Using Iterative Generative Adversarial Network; human Shape Reconstruction with Loose Clothes from Partially Observed Data by Pose Specific Deformation; improved Saliency-Enhanced Multi-cue Correlation-Filter-Based Visual Tracking; a Robust Face Recognition System for One Sample Problem; measuring Apple Size Distribution from a Near Top–Down Image; multi-temporal Registration of Environmental Imagery Using Affine Invariant Convolutional Features; Multimodal 3D Facade Reconstruction Using 3D LiDAR and Images; multiview Dimension Reduction Based on Sparsity Preserving Projections; non-peaked Discriminant Analysis for Image Representation; prostate Cancer Classification Based on Best First Search and Taguchi Feature Selection Method; real-Time Retinal Vessel Segmentation on High-Resolution Fundus Images Using Laplacian Pyramids; RVNet: Deep Sensor Fusion of Monocular Camera and Radar for Image-Based Obstacle Detection in Challenging Environments; semantic Segmentation of Grey-Scale Traffic Scenes; Shoeprint Extraction via GAN; analysis of Motion Patterns in Video Streams for Automatic Health Monitoring in Koi Ponds; turnstile Jumping Detection in Real-Time Video Surveillance; unsupervised Deep Features for Privacy Image Classification; attention-Guided Model for Robust Face Detection System.","","","Conference review","Final","","Scopus","2-s2.0-85076394301"
"Wu H.; Jiang X.; Jia F.","Wu, Haitao (57212006678); Jiang, Xiling (57212005947); Jia, Fucang (55784808100)","57212006678; 57212005947; 55784808100","UC-GAN for MR to CT image synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11850 LNCS","","","146","153","7","10.1007/978-3-030-32486-5_18","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075698793&doi=10.1007%2f978-3-030-32486-5_18&partnerID=40&md5=9425fb0d9431dd82c0504cfbd1dca027","Accurate MR-to-CT synthesis plays an important role in MRI-only radiotherapy treatment planning. In medical image synthesis, the cycle-generative adversarial network (CycleGAN) is becoming an influential method, however, its image quality of synthesis is not optimal yet. In this study, we proposed a new learning method named U-Net-CycleGAN (UC-GAN) to generate synthetic CT (sCT) image for MRI-only radiation treatment planning, which integrated an improved U-Net concept into the original CycleGAN framework. After experimental comparison, The MAE value and PSNR of our UC-GAN model are 76.7 ± 4.5 and 46.1 ± 1.5, respectively, which are statistics significantly better than the 94.0 ± 4.3 (MAE) and 45.1 ± 1.5 (PSNR) of the original CycleGAN model. The results of our quantitative evaluation show that the UC-GAN model can synthesize a CT image closer to the reference real CT image with better performance. © Springer Nature Switzerland AG 2019.","Artificial intelligence; Image enhancement; Learning systems; Magnetic resonance imaging; Medical imaging; Radiotherapy; Adversarial networks; CycleGAN; Experimental comparison; Image synthesis; Learning methods; Quantitative evaluation; Radiation treatment planning; Radiotherapy treatment planning; Computerized tomography","CycleGAN; Image synthesis; MR-to-CT; U-Net","Conference paper","Final","","Scopus","2-s2.0-85075698793"
"Zhu M.; Pan P.; Chen W.; Yang Y.","Zhu, Minfeng (57149322700); Pan, Pingbo (56482811900); Chen, Wei (55613230656); Yang, Yi (56159216600)","57149322700; 56482811900; 55613230656; 56159216600","DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8954283","5795","5803","8","10.1109/CVPR.2019.00595","182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078801772&doi=10.1109%2fCVPR.2019.00595&partnerID=40&md5=c200616036fa40100373ad9d0d033f6d","In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches. © 2019 IEEE.","Deep learning; Adversarial networks; High quality images; Realistic images; Refinement process; State-of-the-art approach; Text information; Text representation; Video synthesis; Computer vision","Deep Learning; Image and Video Synthesis; Vision + Language","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078801772"
"Dhondse A.; Kulkarni S.; Khadilkar K.; Kane I.; Chavan S.; Barhate R.","Dhondse, Amol (57211884007); Kulkarni, Siddhivinayak (7403057003); Khadilkar, Kunal (57208585469); Kane, Indrajeet (57211885608); Chavan, Sumit (57214570754); Barhate, Rahul (57208586275)","57211884007; 7403057003; 57208585469; 57211885608; 57214570754; 57208586275","Generative Adversarial Networks as an Advancement in 2D to 3D Reconstruction Techniques","2020","Advances in Intelligent Systems and Computing","1016","","","343","364","21","10.1007/978-981-13-9364-8_25","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075242844&doi=10.1007%2f978-981-13-9364-8_25&partnerID=40&md5=15cc2ad6380359ad24cfca0f7d39240c","Synthesizing three-dimensional objects from single or multiple two-dimensional views has been a challenging task. To combat this, several techniques involving Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), and Recurrent Neural Network (RNN) have been proposed. Since its advent in 2014, there has been a tremendous amount of research done in the area of Generative Adversarial Networks (GANs). Among the various applications of GANs, image synthesis has shown great potential due to the power of two deep neural networks—generator and discriminator, trained in a competitive way, which are able to produce reasonably realistic images. Formulation of 3D-GANs—which are able to generate three-dimensional objects from multiple two-dimensional views with impressive accuracy—has emerged as a promising solution to the aforementioned issue. This paper provides a comprehensive analysis of deep learning methods used in generating three-dimensional objects, reviews the different models and frameworks for three-dimensional object generation, and discusses some evaluation metrics and future research direction in using GANs as an alternative for simultaneous localization and environment mapping as well as leveraging the power of GANs to revolutionize the field of education and medicine. © 2020, Springer Nature Singapore Pte Ltd.","Convolution; Deep learning; Deep neural networks; Image reconstruction; Information management; Petroleum reservoir evaluation; Adversarial networks; Comprehensive analysis; Convolutional neural network; Environment mapping; Future research directions; Image synthesis; Recurrent neural network (RNN); Three-dimensional object; Recurrent neural networks","Convolutional neural network; Deep learning; Generative adversarial networks; Image synthesis; Three-dimensional object reconstruction","Conference paper","Final","","Scopus","2-s2.0-85075242844"
"Deshpande S.; Minhas F.; Rajpoot N.","Deshpande, Srijay (57206158392); Minhas, Fayyaz (24399575300); Rajpoot, Nasir (8042017200)","57206158392; 24399575300; 8042017200","Train small, generate big: Synthesis of colorectal cancer histology images","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12417 LNCS","","","164","173","9","10.1007/978-3-030-59520-3_17","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092204297&doi=10.1007%2f978-3-030-59520-3_17&partnerID=40&md5=70d9747c71626aa48d4e5ce71cdf121b","The construction of large tissue images is a challenging task in the field of generative modeling of histopathology images. Such synthetic images can be used for development and evaluation of various types of deep learning methods. However, memory and computational processing requirements limit the sizes of image constructed using neural generative models. To tackle this, we propose a conditional generative adversarial network framework that learns to generate and stitch small patches to construct large tissue image tiles while preserving global morphological characteristics. The key novelty of the proposed scheme is that it can be used to generate tiles larger than those used for training with high fidelity. Our evaluation of the Colorectal Adenocarcinoma Gland (CRAG) dataset shows that the proposed model can generate large tissue tiles that exhibit realistic morphological tissue features including glands appearance, nuclear structure, and stromal architecture. Our experimental results also show that the proposed model can be effectively used for evaluation of image segmentation models as well. © Springer Nature Switzerland AG 2020.","Deep learning; Diseases; Image segmentation; Large dataset; Learning systems; Petroleum reservoir evaluation; Tissue; Adversarial networks; Colorectal adenocarcinoma; Colorectal cancer; Computational processing; Histology images; Image segmentation model; Morphological characteristic; Nuclear structure; Medical imaging","Computational pathology; Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85092204297"
"Yang H.; Qian P.; Fan C.","Yang, Huan (57207732435); Qian, Pengjiang (36598989000); Fan, Chao (57216463865)","57207732435; 36598989000; 57216463865","An Indirect Multimodal Image Registration and Completion Method Guided by Image Synthesis","2020","Computational and Mathematical Methods in Medicine","2020","","2684851","","","","10.1155/2020/2684851","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088157124&doi=10.1155%2f2020%2f2684851&partnerID=40&md5=3c8323b98fb452cd9bc0e2d2073e5151","Multimodal registration is a challenging task due to the significant variations exhibited from images of different modalities. CT and MRI are two of the most commonly used medical images in clinical diagnosis, since MRI with multicontrast images, together with CT, can provide complementary auxiliary information. The deformable image registration between MRI and CT is essential to analyze the relationships among different modality images. Here, we proposed an indirect multimodal image registration method, i.e., sCT-guided multimodal image registration and problematic image completion method. In addition, we also designed a deep learning-based generative network, Conditional Auto-Encoder Generative Adversarial Network, called CAE-GAN, combining the idea of VAE and GAN under a conditional process to tackle the problem of synthetic CT (sCT) synthesis. Our main contributions in this work can be summarized into three aspects: (1) We designed a new generative network called CAE-GAN, which incorporates the advantages of two popular image synthesis methods, i.e., VAE and GAN, and produced high-quality synthetic images with limited training data. (2) We utilized the sCT generated from multicontrast MRI as an intermediary to transform multimodal MRI-CT registration into monomodal sCT-CT registration, which greatly reduces the registration difficulty. (3) Using normal CT as guidance and reference, we repaired the abnormal MRI while registering the MRI to the normal CT. © 2020 Huan Yang et al.","Algorithms; Brain; Computational Biology; Databases, Factual; Deep Learning; Humans; Image Interpretation, Computer-Assisted; Magnetic Resonance Imaging; Multimodal Imaging; Synthetic Biology; Tomography, X-Ray Computed; Deep learning; Diagnosis; Image registration; Magnetic resonance imaging; Medical imaging; Adversarial networks; Auxiliary information; Clinical diagnosis; Completion methods; Deformable image registration; Limited training data; Multimodal image registration; Multimodal registration; article; autoencoder; deep learning; image registration; nuclear magnetic resonance imaging; synthesis; algorithm; biology; brain; computer assisted diagnosis; diagnostic imaging; factual database; human; multimodal imaging; procedures; synthetic biology; x-ray computed tomography; Computerized tomography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088157124"
"Chen W.; Hays J.","Chen, Wengling (57207770452); Hays, James (7102191400)","57207770452; 7102191400","SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8579079","9416","9425","9","10.1109/CVPR.2018.00981","165","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062873703&doi=10.1109%2fCVPR.2018.00981&partnerID=40&md5=40abf5faabad39f6041aaba6b4a34a9c","Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores. © 2018 IEEE.","Computer graphics; Computer vision; Drawing (graphics); Adversarial networks; Data augmentation; Image synthesis; Information flows; Multiple scale; Network building blocks; Realistic images; State of the art; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062873703"
"Wang K.; Zhao R.; Ji Q.","Wang, Kang (56637259500); Zhao, Rui (56461916600); Ji, Qiang (18935108400)","56637259500; 56461916600; 18935108400","A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578151","440","448","8","10.1109/CVPR.2018.00053","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062837341&doi=10.1109%2fCVPR.2018.00053&partnerID=40&md5=619c2f3f5b07ac60f394d0c872e5fd9d","In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthesis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye geometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermediate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields. © 2018 IEEE.","Computer vision; Knowledge based systems; Adversarial networks; Benchmark datasets; Data-driven model; Generative model; Intermediate components; Knowledge-based model; Quantitative evaluation; Top-down inference; C (programming language)","","Conference paper","Final","","Scopus","2-s2.0-85062837341"
"Zhao T.; Zhang H.; Zhang Y.; Yang J.","Zhao, Tengfei (57211976151); Zhang, Haigang (57126667300); Zhang, Yutao (57211984266); Yang, Jinfeng (16178707100)","57211976151; 57126667300; 57211984266; 16178707100","X-Ray Image with Prohibited Items Synthesis Based on Generative Adversarial Network","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11818 LNCS","","","379","387","8","10.1007/978-3-030-31456-9_42","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075560688&doi=10.1007%2f978-3-030-31456-9_42&partnerID=40&md5=1b836e68edcf89cfa51379966d64e0df","Using deep learning to assist people in recognizing prohibited items in X-Ray images is crucial to improve the quality of security inspections. However, these methods require lots of data and the data collection usually takes much time and efforts. In this paper, we propose a method to synthesize X-ray image to support the training of prohibited items detectors. The proposed framework is built on the Generative Adversarial Networks (GAN) with multiple discriminators, trying to synthesize realistic X-Ray prohibited items and learn the background context simultaneously. In the other hand, a guided filter is introduced for detail preserving. The experimental results show that our model can smoothly synthesize prohibited items on background images. To quantitatively evaluate our approach, we add the generated samples into training data of the Single Shot MultiBox Detector (SSD) and show the synthetic images are able to improve the detectors’ performance. © 2019, Springer Nature Switzerland AG.","Biometrics; Deep learning; Image enhancement; Adversarial networks; Background image; Data collection; Detail preserving; Image synthesis; Prohibited items; Quality-of-Security; Synthetic images; X ray detectors","Generative Adversarial Network; Image synthesis; X-ray baggage security","Conference paper","Final","","Scopus","2-s2.0-85075560688"
"Zhang S.; Liang R.; Wang M.","Zhang, Shuyang (57208258697); Liang, Runze (57208258339); Wang, Miao (57189656696)","57208258697; 57208258339; 57189656696","ShadowGAN: Shadow synthesis for virtual objects with conditional adversarial networks","2019","Computational Visual Media","5","1","","105","115","10","10.1007/s41095-019-0136-1","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064241377&doi=10.1007%2fs41095-019-0136-1&partnerID=40&md5=c348358cf0cb0b7ec4664608c7a359ac","We introduce ShadowGAN, a generative adversarial network (GAN) for synthesizing shadows for virtual objects inserted in images. Given a target image containing several existing objects with shadows, and an input source object with a specified insertion position, the network generates a realistic shadow for the source object. The shadow is synthesized by a generator; using the proposed local adversarial and global adversarial discriminators, the synthetic shadow’s appearance is locally realistic in shape, and globally consistent with other objects’ shadows in terms of shadow direction and area. To overcome the lack of training data, we produced training samples based on public 3D models and rendering technology. Experimental results from a user study show that the synthetic shadowed results look natural and authentic. © 2019, The Author(s).","Deep learning; Three dimensional computer graphics; Adversarial networks; Image synthesis; Input sources; Insertion position; Target images; Training data; Training sample; Virtual objects; Image processing","deep learning; generative adversarial networks; image synthesis; shadow synthesis","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85064241377"
"Haarburger C.; Horst N.; Truhn D.; Broeckmann M.; Schrading S.; Kuhl C.; Merhof D.","Haarburger, Christoph (56537042300); Horst, Nicolas (57217630313); Truhn, Daniel (23010796500); Broeckmann, Mirjam (57212004640); Schrading, Simone (12764370400); Kuhl, Christiane (7006785400); Merhof, Dorit (13103575700)","56537042300; 57217630313; 23010796500; 57212004640; 12764370400; 7006785400; 13103575700","Multiparametric magnetic resonance image synthesis using generative adversarial networks","2019","Eurographics Workshop on Visual Computing for Biology and Medicine, VCBM 2019","","","","11","15","4","10.2312/vcbm.20191226","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087454616&doi=10.2312%2fvcbm.20191226&partnerID=40&md5=cab3b256df91e5302c8072f9ec0ba759","Generative adversarial networks have been shown to alleviate the problem of limited training data for supervised learning problems in medical image computing. However, most generative models for medical images focus on image-to-image translation rather than de novo image synthesis. In many clinical applications, image acquisition is multiparametric, i.e. includes contrast-enchanced or diffusion-weighted imaging. We present a generative adversarial network that synthesizes a sequence of temporally consistent contrast-enhanced breast MR image patches. Performance is evaluated quantitatively using the Fréchet Inception Distance, achieving a minimum FID of 21.03. Moreover, a qualitative human reader test shows that even a radiologist cannot differentiate between real and fake images easily. © 2019 The Author(s) Eurographics Proceedings © 2019 The Eurographics Association.","Image enhancement; Magnetic resonance imaging; Medical problems; Adversarial networks; Clinical application; Contrast-enhanced; Diffusion weighted imaging; Image translation; Limited training data; Medical image computing; Supervised learning problems; Medical imaging","","Conference paper","Final","","Scopus","2-s2.0-85087454616"
"Ma X.; Jin R.; Sohn K.-A.; Paik J.-Y.; Chung T.-S.","Ma, Xiaohan (57194789503); Jin, Rize (57606062800); Sohn, Kyung-Ah (56043382200); Paik, Joon-Young (35189419700); Chung, Tae-Sun (21033702700)","57194789503; 57606062800; 56043382200; 35189419700; 21033702700","An Adaptive Control Algorithm for Stable Training of Generative Adversarial Networks","2019","IEEE Access","7","","8936350","184103","184114","11","10.1109/ACCESS.2019.2960461","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077796572&doi=10.1109%2fACCESS.2019.2960461&partnerID=40&md5=a56464b08f0f9cdacd8f68e1c42e989e","Generative adversarial networks (GANs) have shown significant progress in generating high-quality visual samples, however they are still well known both for being unstable to train and for the problem of mode collapse, particularly when trained on data collections containing a diverse set of visual objects. In this paper, we propose an Adaptive k -step Generative Adversarial Network ( text{A}k -GAN), which is designed to mitigate the impact of instability and saturation in the original by dynamically adjusting the ratio of the training steps of both the generator and discriminator. To accomplish this, we track and analyze stable training curves of relatively narrow datasets and use them as the target fitting lines when training more diverse data collections. Furthermore, we conduct experiments on the proposed procedure using several optimization techniques (e.g., supervised guiding from previous stable learning curves with and without momentum) and compare their performance with that of state-of-the-art models on the task of image synthesis from datasets consisting of diverse images. Empirical results demonstrate that text{A}k -GAN works well in practice and exhibits more stable behavior than regular GANs during training. A quantitative evaluation has been conducted on the Inception~Score ( IS ) and the relative~inverse~Inception~Score ( RIS ); compared with regular GANs, the former has been improved by 61% and 83%, and the latter by 21% and 60%, on the CelebA and the Anime datasets, respectively. © 2013 IEEE.","Adaptive algorithms; Curve fitting; Data acquisition; Adaptive control algorithms; Adversarial networks; Data collection; Image generations; mode collapse; Optimization techniques; Quantitative evaluation; State of the art; Adaptive control systems","adaptive algorithm; Generative adversarial networks; image generation; mode collapse","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85077796572"
"Zhang Z.; Zhang Y.; Yu W.; Lu J.; Nie L.; He G.; Jiang N.; He G.; Fan Y.; Yang Z.","Zhang, Zhiqiang (57206280843); Zhang, Yunye (57211359203); Yu, Wenxin (36610960300); Lu, Jingwei (57210792096); Nie, Li (57211169596); He, Gang (56937631400); Jiang, Ning (57212426361); He, Gang (36630339700); Fan, Yibo (23466795500); Yang, Zhuo (57203791621)","57206280843; 57211359203; 36610960300; 57210792096; 57211169596; 56937631400; 57212426361; 36630339700; 23466795500; 57203791621","Text to image synthesis based on multiple discrimination","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11729 LNCS","","","578","589","11","10.1007/978-3-030-30508-6_46","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072854645&doi=10.1007%2f978-3-030-30508-6_46&partnerID=40&md5=0bc2b7035d220489b1408199757e075f","We propose a novel and simple text-to-image synthesizer (MD-GAN) using multiple discrimination. Based on the Generative Adversarial Network (GAN), we introduce segmentation images to the discriminator to ensure the improvement of discrimination ability. The improvement of discrimination ability will enhance the generator’s generating ability, thus obtaining high-resolution results. Experiments well validate the outstanding performance of our algorithm. On CUB dataset, our inception score is 27.7% and 1.7% higher than GAN-CLS-INT and GAWWN, respectively. On the flower dataset, it further outplays GAN-CLS-INT and StackGAN by 21.8% and 1.25%, respectively. At the same time, our model is more concise in structure, and its training time is only half that of StackGAN. © Springer Nature Switzerland AG 2019.","Computer vision; Deep learning; Image segmentation; Neural networks; Adversarial networks; Discrimination ability; High resolution; Image synthesis; Multiple discrimination; Segmentation images; Training time; Image enhancement","Computer vision; Deep learning; GAN; Multiple discrimination; Text to image synthesis","Conference paper","Final","","Scopus","2-s2.0-85072854645"
"Li Q.; Luo Y.","Li, Qixin (57214915616); Luo, Yaneng (57191528235)","57214915616; 57191528235","Using GAN priors for ultrahigh resolution seismic inversion","2020","SEG International Exposition and Annual Meeting 2019","","","","2453","2457","4","10.1190/segam2019-3215520.1","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079489488&doi=10.1190%2fsegam2019-3215520.1&partnerID=40&md5=d84e64a4787d5235fca6b4b54ef96992","As one of machine learning techniques, deep learning has recently achieved the state-of-the-art performances in many areas, such as computer vision, natural language processing, to name a few. A generative model called Generative Adversarial Network (GAN) was invented in 2014. This deep network model is deemed as the most interesting idea in the last 10 years by the machine learning community and outperformed the traditional methods in many tasks like image synthesis and super-resolution. Laying the heart of the GAN is its ability to model any realistically sharp data distribution. Instead of providing a “blurry” sample, the high-resolution samples can be sampled from the GAN model, no matter it is a natural image or a well log. In this abstract, we propose a novel ultrahigh resolution seismic inversion method using GAN priors. The basic workflow is described below. Firstly, a simple GAN architecture was designed. Then, we train this GAN to model the well-log data distribution. Once the GAN is properly trained, it offers the high-resolution samples as priors to the inversion algorithm. To effectively use this prior information, we adopt the projected gradient descent algorithm to iteratively fit the seismic data and projects the “blurry” sample to the high resolution set of prior samples defined by the GAN. We further use a thin-layer model to validate the feasibility and superiority of our method. Comparing with the traditional method, our result shows a higher precision and resolution. © 2019 SEG","Deep learning; Geophysical prospecting; Gradient methods; Natural language processing systems; Seismology; Well logging; Adversarial networks; Inversion algorithm; Machine learning communities; Machine learning techniques; NAtural language processing; Projected gradient; State-of-the-art performance; Ultrahigh resolution; Learning systems","","Conference paper","Final","","Scopus","2-s2.0-85079489488"
"Lee J.H.; Han I.H.; Kim D.H.; Yu S.; Lee I.S.; Song Y.S.; Joo S.; Jin C.-B.; Kim H.","Lee, Jung Hwan (57196140939); Han, In Ho (57226102226); Kim, Dong Hwan (57210111592); Yu, Seunghan (57215662973); Lee, In Sook (55705262400); Song, You Seon (36605357700); Joo, Seongsu (57207765020); Jin, Cheng-Bin (56902110800); Kim, Hakil (56091877400)","57196140939; 57226102226; 57210111592; 57215662973; 55705262400; 36605357700; 57207765020; 56902110800; 56091877400","Spine computed tomography to magnetic resonance image synthesis using generative adversarial networks: A preliminary study","2020","Journal of Korean Neurosurgical Society","63","3","","386","396","10","10.3340/jkns.2019.0084","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084520295&doi=10.3340%2fjkns.2019.0084&partnerID=40&md5=e8fcdc6d7ddd9bd01d0cc24ca61872da","Objective: To generate synthetic spine magnetic resonance (MR) images from spine computed tomography (CT) using generative adversarial networks (GANs), as well as to determine the similarities between synthesized and real MR images. Methods: GANs were trained to transform spine CT image slices into spine magnetic resonance T2 weighted (MRT2) axial image slices by combining adversarial loss and voxel-wise loss. Experiments were performed using 280 pairs of lumbar spine CT scans and MRT2 images. The MRT2 images were then synthesized from 15 other spine CT scans. To evaluate whether the synthetic MR images were realistic, two radiologists, two spine surgeons, and two residents blindly classified the real and synthetic MRT2 images. Two experienced radiologists then evaluated the similarities between subdivisions of the real and synthetic MRT2 images. Quantitative analysis of the synthetic MRT2 images was performed using the mean absolute error (MAE) and peak signal-to-noise ratio (PSNR). Results: The mean overall similarity of the synthetic MRT2 images evaluated by radiologists was 80.2%. In the blind classification of the real MRT2 images, the failure rate ranged from 0% to 40%. The MAE value of each image ranged from 13.75 to 34.24 pixels (mean, 21.19 pixels), and the PSNR of each image ranged from 61.96 to 68.16 dB (mean, 64.92 dB). Conclusion: This was the first study to apply GANs to synthesize spine MR images from CT images. Despite the small dataset of 280 pairs, the synthetic MR images were relatively well implemented. Synthesis of medical images using GANs is a new paradigm of artificial intelligence application in medical imaging. We expect that synthesis of MR images from spine CT images using GANs will improve the diagnostic usefulness of CT. To better inform the clinical applications of this technique, further studies are needed involving a large dataset, a variety of pathologies, and other MR sequence of the lumbar spine. © 2020 The Korean Neurosurgical Society.","","Computed tomography; Deep learning; Lumbar vertebrae; Magnetic resonance imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084520295"
"Yao Z.; Dong H.; Liu F.; Guo Y.","Yao, Zhongwei (57205102742); Dong, Hao (57192424168); Liu, Fangde (57194785628); Guo, Yike (12765868000)","57205102742; 57192424168; 57194785628; 12765868000","Conditional image synthesis using stacked auxiliary classifier generative adversarial networks","2019","Advances in Intelligent Systems and Computing","887","","","423","433","10","10.1007/978-3-030-03405-4_29","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059319984&doi=10.1007%2f978-3-030-03405-4_29&partnerID=40&md5=739515737ef6d99ebc913fed54c6fddf","Synthesizing photo-realistic images has been a long-standing challenge in image processing and could provide crucial approaches for dataset augmentation and balancing. Traditional methods have trouble in dealing with the rich and complicated structural information of objects resulting from the variations in colors, poses, textures and illumination. Recent advancement in Deep Learning techniques presents a new perspective to this task. The aim of our paper is to apply state-of-the-art generative models to synthesize diverse and realistic high-resolution images. Extensive experiments have been conducted on celebA dataset, a large-scale face attributes dataset with more than 200 thousand celebrity images, each with 40 attribute labels. Enlightened by existing structures, we present stacked Auxiliary Classifier Generative Adversarial Networks (Stack-ACGAN) for image synthesis given conditioning labels, which generates low resolution images (e.g. 64 × 64) that sketch basic shapes and colors in Stage-I and high resolution images (e.g. 256 × 256) with plausible details in Stage-II. Inception scores and Multi-Scale Structural Similarity (MS-SSIM) are computed for evaluation of the synthesized images. Both quantitative and qualitative analysis prove the proposed model is capable of generating diverse and realistic images. © 2019, Springer Nature Switzerland AG.","Image classification; Adversarial networks; High resolution image; Learning techniques; Low resolution images; Photorealistic images; Quantitative and qualitative analysis; Structural information; Structural similarity; Deep learning","Deep learning; Generative adversarial networks; High-resolution image synthesis","Conference paper","Final","","Scopus","2-s2.0-85059319984"
"Alfred R.; Lun C.Y.","Alfred, Rayner (24722539300); Lun, Chew Ye (57212575343)","24722539300; 57212575343","Unsupervised learning of image data using generative adversarial network","2020","Advances in Intelligent Systems and Computing","1041","","","127","135","8","10.1007/978-981-15-0637-6_10","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077110455&doi=10.1007%2f978-981-15-0637-6_10&partnerID=40&md5=6fa0726661228a668066406de3f34be2","Over the past few years, with the introduction of deep learning techniques such as convolution neural network (CNN), supervised learning with CNN had achieved a huge success in the computer vision area such as classifying digital images. However, supervised learning has a major drawback, in which it requires a large dataset for them to perform more effectively. As the data used in training grew bigger, the cost of labeling data for training becomes more expensive and impractical. In order to resolve this issue, unsupervised learning is encouraged to be used as it can draw inferences from datasets consisting of unlabeled input data. Generative adversarial network (GAN) is one of the unsupervised learning technique that has the ability to create natural-looking images, converting text description into images, recover resolution of images and last but not least, its power of representation learning from unlabeled data. Thus, this study attempts to evaluate the effectiveness of GAN algorithm in performing the supervised task and unsupervised task such as classification and clustering. Based on the results obtained, the GAN algorithm can learn the internal representation of data without labels and can act as good features extractor. Future works include applying GAN framework in other domains such as video, natural language processing and text to image synthesis. © Springer Nature Singapore Pte Ltd. 2020.","Feature extraction; Image processing; Large dataset; Learning algorithms; Machine learning; Natural language processing systems; Supervised learning; Unsupervised learning; Adversarial networks; Classification and clustering; Convolution neural network; Image synthesis; Internal representation; Learning from unlabeled data; Learning techniques; NAtural language processing; Deep learning","Feature extraction; Generative adversarial network; Supervised learning; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85077110455"
"Zhou Z.; Guo X.; Yang W.; Shi Y.; Zhou L.; Wang L.; Yang M.","Zhou, Ziqi (57212007302); Guo, Xinna (57212008876); Yang, Wanqi (55337215500); Shi, Yinghuan (35241386100); Zhou, Luping (23398846800); Wang, Lei (54958774700); Yang, Ming (56601992500)","57212007302; 57212008876; 55337215500; 35241386100; 23398846800; 54958774700; 56601992500","Cross-Modal Attention-Guided Convolutional Network for Multi-modal Cardiac Segmentation","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11861 LNCS","","","601","610","9","10.1007/978-3-030-32692-0_69","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075689732&doi=10.1007%2f978-3-030-32692-0_69&partnerID=40&md5=9974b1c3e27c39437a589e6097ba010d","To leverage the correlated information between modalities to benefit the cross-modal segmentation, we propose a novel cross-modal attention-guided convolutional network for multi-modal cardiac segmentation. In particular, we first employed the cycle-consistency generative adversarial networks to complete the bidirectional image generation (i.e., MR to CT, CT to MR) to help reduce the modal-level inconsistency. Then, with the generated and original MR and CT images, a novel convolutional network is proposed where (1) two encoders learn individual features separately and (2) a common decoder learns shareable features between modalities for a final consistent segmentation. Also, we propose a cross-modal attention module between the encoders and decoder in order to leverage the correlated information between modalities. Our model can be trained in an end-to-end manner. With extensive evaluation on the unpaired CT and MR cardiac images, our method outperforms the baselines in terms of the segmentation performance. © 2019, Springer Nature Switzerland AG.","Computer aided instruction; Computerized tomography; Convolution; Decoding; Heart; Image segmentation; Machine learning; Signal encoding; Adversarial networks; Cardiac segmentation; Convolutional networks; Cross-modal; Encoders and decoders; Image synthesis; Individual features; Segmentation performance; Medical imaging","Cross-modal attention; Cross-modal image synthesis; Multi-modal cardiac segmentation","Conference paper","Final","","Scopus","2-s2.0-85075689732"
"Kwon G.; Han C.; Kim D.-S.","Kwon, Gihyun (57211270075); Han, Chihye (57211269262); Kim, Dae-shik (26637469800)","57211270075; 57211269262; 26637469800","Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial Networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11766 LNCS","","","118","126","8","10.1007/978-3-030-32248-9_14","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075693669&doi=10.1007%2f978-3-030-32248-9_14&partnerID=40&md5=f2a844601cb366ee0125183476e45a9a","As deep learning is showing unprecedented success in medical image analysis tasks, the lack of sufficient medical data is emerging as a critical problem. While recent attempts to solve the limited data problem using Generative Adversarial Networks (GAN) have been successful in generating realistic images with diversity, most of them are based on image-to-image translation and thus require extensive datasets from different domains. Here, we propose a novel model that can successfully generate 3D brain MRI data from random vectors by learning the data distribution. Our 3D GAN model solves both image blurriness and mode collapse problems by leveraging GAN that combines the advantages of Variational Auto-Encoder (VAE) and GAN with an additional code discriminator network. We also use the Wasserstein GAN with Gradient Penalty (WGAN-GP) loss to lower the training instability. To demonstrate the effectiveness of our model, we generate new images of normal brain MRI and show that our model outperforms baseline models in both quantitative and qualitative measurements. We also train the model to synthesize brain disorder MRI data to demonstrate the wide applicability of our model. Our results suggest that the proposed model can successfully generate various types and modalities of 3D whole brain volumes from a small set of training data. © 2019, Springer Nature Switzerland AG.","3D modeling; Deep learning; Learning systems; Medical computing; Medical imaging; Network coding; Adversarial networks; Critical problems; Data augmentation; Data distribution; Different domains; Image synthesis; Image translation; Qualitative measurements; Magnetic resonance imaging","3D; Data augmentation; Generative Adversarial Networks; Image synthesis; MRI","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075693669"
"Ayinde B.O.; Nishihama K.; Zurada J.M.","Ayinde, Babajide O. (56638383600); Nishihama, Keishin (57208838091); Zurada, Jacek M. (57201784191)","56638383600; 57208838091; 57201784191","Diversity Regularized Adversarial Deep Learning","2019","IFIP Advances in Information and Communication Technology","559","","","292","306","14","10.1007/978-3-030-19823-7_24","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065916350&doi=10.1007%2f978-3-030-19823-7_24&partnerID=40&md5=d638a9a3e4588a90505ab771a0fee01a","The two key players in Generative Adversarial Networks (GANs), the discriminator and generator, are usually parameterized as deep neural networks (DNNs). On many generative tasks, GANs achieve state-of-the-art performance but are often unstable to train and sometimes miss modes. A typical failure mode is the collapse of the generator to a single parameter configuration where its outputs are identical. When this collapse occurs, the gradient of the discriminator may point in similar directions for many similar points. We hypothesize that some of these shortcomings are in part due to primitive and redundant features extracted by discriminator and this can easily make the training stuck. We present a novel approach for regularizing adversarial models by enforcing diverse feature learning. In order to do this, both generator and discriminator are regularized by penalizing both negatively and positively correlated features according to their differentiation and based on their relative cosine distances. In addition to the gradient information from the adversarial loss made available by the discriminator, diversity regularization also ensures that a more stable gradient is provided to update both the generator and discriminator. Results indicate our regularizer enforces diverse features, stabilizes training, and improves image synthesis. © 2019, IFIP International Federation for Information Processing.","Artificial intelligence; Deep learning; Image enhancement; Adversarial learning; Adversarial networks; Feature correlation; Feature redundancy; Generative model; Regularization; Deep neural networks","Adversarial learning; Deep learning; Feature correlation; Feature redundancy; Generative Adversarial Networks; Generative model; Regularization","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065916350"
"Liang G.; Fouladvand S.; Zhang J.; Brooks M.A.; Jacobs N.; Chen J.","Liang, Gongbo (57209688898); Fouladvand, Sajjad (57216704139); Zhang, Jie (57150866600); Brooks, Michael A. (20336482100); Jacobs, Nathan (15520711800); Chen, Jin (57203334776)","57209688898; 57216704139; 57150866600; 20336482100; 15520711800; 57203334776","GANai: Standardizing CT Images using Generative Adversarial Network with Alternative Improvement","2019","2019 IEEE International Conference on Healthcare Informatics, ICHI 2019","","","8904763","","","","10.1109/ICHI.2019.8904763","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075922424&doi=10.1109%2fICHI.2019.8904763&partnerID=40&md5=00cf99888c365348df4bf051604ed09a","Computed tomography (CT) is a widely-used diagnostic image modality routinely used for assessing anatomical tissue characteristics. However, non-standardized imaging protocols are commonplace, which poses a fundamental challenge in large-scale cross-center CT image analysis. One approach to address the problem is to standardize and normalize CT images using image synthesis algorithms including generative adversarial network (GAN) models. GAN learns the data distribution of training images and generate synthesized images under the same distribution. However, existing GAN models are not directly applicable to this task mainly due to the lack of constraints on the mode of data to generate. Furthermore, they treat every image equally, but in real applications, certain images are more difficult to standardize than the others. All these may lead to the lack-of-detail problem in CT image synthesis. We present a new GAN model called GANai to mitigate the differences in radiomic features across CT images captured using non-standard imaging protocols. Given source images, GANai composes new images by specifying a high-level goal that the image features of the synthesized images should be similar to those of the standard images. GANai introduces a new alternative improvement training strategy to alternatively and gradually improve GAN model performance. The new training strategy enables a series of technical improvements, including phase-specific loss functions, phase-specific training data, and the adoption of ensemble learning, leading to better model performance. The experimental results show that efficiency and stability of GAN models have been much improved in GANai and our model is significantly better than the existing state-of-the-art image synthesis algorithms on CT image standardization. © 2019 IEEE.","Computerized tomography; Health care; Adversarial networks; Ensemble learning; Image synthesis; Real applications; Specific loss function; Synthesized images; Technical improvement; Tissue characteristics; Image enhancement","Alternative training; Computed tomography; Generative adversarial network; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075922424"
"Wang Y.; Zhou L.; Yu B.; Wang L.; Zu C.; Lalush D.S.; Lin W.; Wu X.; Zhou J.; Shen D.","Wang, Yan (56039981100); Zhou, Luping (23398846800); Yu, Biting (57201496052); Wang, Lei (54958774700); Zu, Chen (55377165500); Lalush, David S. (7004144493); Lin, Weili (56999175100); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Shen, Dinggang (7401738392)","56039981100; 23398846800; 57201496052; 54958774700; 55377165500; 7004144493; 56999175100; 57221065403; 21234416400; 7401738392","3D Auto-Context-Based Locality Adaptive Multi-Modality GANs for PET Synthesis","2019","IEEE Transactions on Medical Imaging","38","6","8552676","1328","1339","11","10.1109/TMI.2018.2884053","85","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057882665&doi=10.1109%2fTMI.2018.2884053&partnerID=40&md5=e271530cf68d8523ac59224cde9ecf0b","Positron emission tomography (PET) has been substantially used recently. To minimize the potential health risk caused by the tracer radiation inherent to PET scans, it is of great interest to synthesize the high-quality PET image from the low-dose one to reduce the radiation exposure. In this paper, we propose a 3D auto-context-based locality adaptive multi-modality generative adversarial networks model (LA-GANs) to synthesize the high-quality FDG PET image from the low-dose one with the accompanying MRI images that provide anatomical information. Our work has four contributions. First, different from the traditional methods that treat each image modality as an input channel and apply the same kernel to convolve the whole image, we argue that the contributions of different modalities could vary at different image locations, and therefore a unified kernel for a whole image is not optimal. To address this issue, we propose a locality adaptive strategy for multi-modality fusion. Second, we utilize 1 × 1 × 1 kernel to learn this locality adaptive fusion so that the number of additional parameters incurred by our method is kept minimum. Third, the proposed locality adaptive fusion mechanism is learned jointly with the PET image synthesis in a 3D conditional GANs model, which generates high-quality PET images by employing large-sized image patches and hierarchical features. Fourth, we apply the auto-context strategy to our scheme and propose an auto-context LA-GANs model to further refine the quality of synthesized images. Experimental results show that our method outperforms the traditional multi-modality fusion methods used in deep networks, as well as the state-of-the-art PET estimation approaches. © 1982-2012 IEEE.","Brain; Databases, Factual; Deep Learning; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Phantoms, Imaging; Positron-Emission Tomography; Radiation Dosage; Health risks; Image fusion; Magnetic resonance imaging; Positron emission tomography; Positrons; Adaptive fusion; Adversarial networks; Image synthesis; Multi modality; Positron emission; article; low drug dose; nuclear magnetic resonance imaging; positron emission tomography; synthesis; topography; brain; diagnostic imaging; factual database; human; imaging phantom; positron emission tomography; procedures; radiation dose; three-dimensional imaging; Image processing","generative adversarial networks (GANs); Image synthesis; locality adaptive fusion; multi-modality; positron emission topography (PET)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85057882665"
"Horiuchi Y.; Iizuka S.; Simo-Serra E.; Ishikawa H.","Horiuchi, Yusuke (57210357606); Iizuka, Satoshi (57197724901); Simo-Serra, Edgar (55303749400); Ishikawa, Hiroshi (37109859900)","57210357606; 57197724901; 55303749400; 37109859900","Spectral normalization and relativistic adversarial training for conditional pose generation with self-attention","2019","Proceedings of the 16th International Conference on Machine Vision Applications, MVA 2019","","","8758013","","","","10.23919/MVA.2019.8758013","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070455014&doi=10.23919%2fMVA.2019.8758013&partnerID=40&md5=9d484cfbc1cb31e902e42f9a81ffefd6","We address the problem of conditional image generation of synthesizing a new image of an individual given a reference image and target pose. We base our approach on generative adversarial networks and leverage deformable skip connections to deal with pixel-to-pixel misalignments, self-attention to leverage complementary features in separate portions of the image, e.g., arms or legs, and spectral normalization to improve the quality of the synthesized images. We train the synthesis model with a nearest-neighbour loss in combination with a relativistic average hinge adversarial loss. We evaluate on the Market-1501 dataset and show how our proposed approach can surpass existing approaches in conditional image synthesis performance. © 2019 MVA Organization.","Computer vision; Pixels; Adversarial networks; Complementary features; Image generations; Nearest neighbour; Reference image; Spectral normalization; Synthesis models; Synthesized images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85070455014"
"","","","Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019","2019","Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019","","","","","","2224","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063598066&partnerID=40&md5=c21b7804f7f2dd1fdb35ffab976802c5","The proceedings contain 229 papers. The topics discussed include: GAN-based pose-aware regulation for video-based person re-identification; domain-specific human-inspired binarized statistical image features for iris recognition; a hierarchical grocery store image dataset with visual and semantic labels; fast geometrically-perturbed adversarial faces; warping-based stereoscopic 3D video retargeting with depth remapping; analyzing modern camera response functions; and fashion attributes-to-image synthesis using attention-based generative adversarial network. ","","","Conference review","Final","","Scopus","2-s2.0-85063598066"
"Li B.; Yuan X.; Shi M.","Li, Bo (57199786743); Yuan, Xue (7402202691); Shi, Minghan (57215542710)","57199786743; 7402202691; 57215542710","Synthetic data generation based on local-foreground generative adversarial networks for surface defect detection","2020","Journal of Electronic Imaging","29","1","013016","","","","10.1117/1.JEI.29.1.013016","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081137937&doi=10.1117%2f1.JEI.29.1.013016&partnerID=40&md5=9f830c97642b935acaa5522ed140fbe8","We propose a synthetic image method named local-foreground generative adversarial networks for surface defect detection in the industry. The method comprises three contributions: First, in order to provide more training data, an algorithm that generates full synthetic images of defects is proposed. The method may blend the defect samples stored in the different mobile terminal into existing background images of the production environment in a natural way, accounting for both geometry and appearance. Second, the diversity of defects appearance is increased due to the use of deep convolutional generative adversarial networks only for local-foreground and the defect edge details that are preserved by introducing a guided filter in the process of image synthesis. Third, the image fusion method can adapt to the various production environment of different brightness and camera angles, which has strong adaptability and expansion ability. We also discuss the experimental results of synthetic data on an end-to-end detection system based on deep learning. As we know, YOLO detector trained on synthetic data achieving average precision of 95.2% on test dataset, which is 12.5% higher than heuristic data augmentation individually. Furthermore, it can process 23 images/s on 1080Ti GPU, which meets the requirements for real-time industrial inspection. © 2020 SPIE and IS&T.","Deep learning; Defects; Image processing; Statistical tests; Surface defects; Adversarial networks; Image fusion methods; Image synthesis; Industrial inspections; Production environments; Surface defect detections; Synthetic data generations; Visual inspection; Image fusion","full image synthesis; generative adversarial network; image fusion; surface defect detection; visual inspection","Article","Final","","Scopus","2-s2.0-85081137937"
"Menardi M.; Falcon A.; Mohamed S.S.; Seidenari L.; Serra G.; Del Bimbo A.; Tasso C.","Menardi, Marco (57215855050); Falcon, Alex (57215860617); Mohamed, Saida S. (57220614858); Seidenari, Lorenzo (36015900900); Serra, Giuseppe (10140344600); Del Bimbo, Alberto (15018931800); Tasso, Carlo (6602872714)","57215855050; 57215860617; 57220614858; 36015900900; 10140344600; 15018931800; 6602872714","Text-to-Image Synthesis Based on Machine Generated Captions","2020","Communications in Computer and Information Science","1177 CCIS","","","62","74","12","10.1007/978-3-030-39905-4_7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082117548&doi=10.1007%2f978-3-030-39905-4_7&partnerID=40&md5=6b3f24f2957a87c89e32da3412971158","Text-to-Image Synthesis refers to the process of automatic generation of a photo-realistic image starting from a given text and is revolutionizing many real-world applications. In order to perform such process it is necessary to exploit datasets containing captioned images, meaning that each image is associated with one (or more) captions describing it. Despite the abundance of uncaptioned images datasets, the number of captioned datasets is limited. To address this issue, in this paper we propose an approach capable of generating images starting from a given text using conditional generative adversarial network (GAN) trained on uncaptioned images dataset. In particular, uncaptioned images are fed to an Image Captioning Module to generate the descriptions. Then, the GAN Module is trained on both the input image and the “machine-generated” caption. To evaluate the results, the performance of our solution is compared with the results obtained by the unconditional GAN. For the experiments, we chose to use the uncaptioned dataset LSUN-bedroom. The results obtained in our study are preliminary but still promising. © 2020, Springer Nature Switzerland AG.","Digital libraries; Adversarial networks; Automatic Generation; Critical sequence; Image captioning; Image synthesis; Input image; Photorealistic images; StackGAN; Image processing","Generative Adversarial Networks (GANs); Self-Critical Sequence Training (SCST); StackGAN; Text-to-Image Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85082117548"
"Shen T.; Gou C.; Wang F.-Y.; He Z.; Chen W.","Shen, Tianyu (57207733071); Gou, Chao (56320227000); Wang, Fei-Yue (57211758869); He, Zilong (57054536200); Chen, Weiguo (55568522646)","57207733071; 56320227000; 57211758869; 57054536200; 55568522646","Learning from adversarial medical images for X-ray breast mass segmentation","2019","Computer Methods and Programs in Biomedicine","180","","105012","","","","10.1016/j.cmpb.2019.105012","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070217245&doi=10.1016%2fj.cmpb.2019.105012&partnerID=40&md5=72e5b5e1548cf16aed9cb15749c8333a","Background and Objective: Simulation of diverse lesions in images is proposed and applied to overcome the scarcity of labeled data, which has hindered the application of deep learning in medical imaging. However, most of current studies focus on generating samples with class labels for classification and detection rather than segmentation, because generating images with precise masks remains a challenge. Therefore, we aim to generate realistic medical images with precise masks for improving lesion segmentation in mammagrams. Methods: In this paper, we propose a new framework for improving X-ray breast mass segmentation performance aided by generated adversarial lesion images with precise masks. Firstly, we introduce a conditional generative adversarial network (cGAN) to learn the distribution of real mass images as well as a mapping between images and corresponding segmentation masks. Subsequently, a number of lesion images are generated from various binary input masks using the generator in the trained cGAN. Then the generated adversarial samples are concatenated with original samples to produce a dataset with increased diversity. Furthermore, we introduce an improved U-net and train it on the previous augmented dataset for breast mass segmentation. Results: To demonstrate the effectiveness of our proposed method, we conduct experiments on publicly available mammogram database of INbreast and a private database provided by Nanfang Hospital in China. Experimental results show that an improvement up to 7% in Jaccard index can be achieved over the same model trained on original real lesion images. Conclusions: Our proposed method can be viewed as one of the first steps toward generating realistic X-ray breast mass images with masks for precise segmentation. © 2019 Elsevier B.V.","China; Deep Learning; Female; Humans; Image Processing, Computer-Assisted; Mammography; X-Rays; Deep learning; Image enhancement; Image segmentation; X rays; Adversarial networks; Breast mass; Generating samples; Image synthesis; Lesion segmentations; Original sample; Private database; Segmentation masks; Article; Article; breast tumor; breast tumor; China; China; Chinese; Chinese; data base; data base; deep learning; deep learning; diagnostic imaging; female; human; image analysis; image processing; image segmentation; image segmentation; major clinical study; mammography; mammography; radiography; sensitivity and specificity; procedures; X ray; Medical image processing","Generative adversarial network; Lesion segmentation; Medical image synthesis; X-ray breast mass","Article","Final","","Scopus","2-s2.0-85070217245"
"Wu Y.; Yang F.; Xu Y.; Ling H.","Wu, Yifan (57206750971); Yang, Fan (57749556000); Xu, Yong (57274194400); Ling, Haibin (57191091290)","57206750971; 57749556000; 57274194400; 57191091290","Privacy-Protective-GAN for Privacy Preserving Face De-Identification","2019","Journal of Computer Science and Technology","34","1","","47","60","13","10.1007/s11390-019-1898-8","57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060701779&doi=10.1007%2fs11390-019-1898-8&partnerID=40&md5=4c816de6929b699a53f06e8b82a39a4f","Face de-identification has become increasingly important as the image sources are explosively growing and easily accessible. The advance of new face recognition techniques also arises people’s concern regarding the privacy leakage. The mainstream pipelines of face de-identification are mostly based on the k-same framework, which bears critiques of low effectiveness and poor visual quality. In this paper, we propose a new framework called Privacy-Protective-GAN (PP-GAN) that adapts GAN (generative adversarial network) with novel verificator and regulator modules specially designed for the face de-identification problem to ensure generating de-identified output with retained structure similarity according to a single input. We evaluate the proposed approach in terms of privacy protection, utility preservation, and structure similarity. Our approach not only outperforms existing face de-identification techniques but also provides a practical framework of adapting GAN with priors of domain knowledge. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Software engineering; Adversarial networks; De-identification; Face recognition technique; Image synthesis; Privacy preserving; Privacy protection; Structure similarity; Visual qualities; Face recognition","face de-identification; generative adversarial network (GAN); image synthesis; privacy protection","Article","Final","","Scopus","2-s2.0-85060701779"
"Ak K.E.; Lim J.H.; Tham J.Y.; Kassim A.","Ak, Kenan Emir (56779817500); Lim, Joo Hwee (7403454337); Tham, Jo Yew (57217528957); Kassim, Ashraf (7004412916)","56779817500; 7403454337; 57217528957; 7004412916","Semantically consistent hierarchical text to fashion image synthesis with an enhanced-attentional generative adversarial network","2019","Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019","","","9022076","3121","3124","3","10.1109/ICCVW.2019.00379","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082462604&doi=10.1109%2fICCVW.2019.00379&partnerID=40&md5=4b6f5e88455a11e2c741df2097faaeef","In this paper, we present the enhanced Attentional Generative Adversarial Network (e-AttnGAN) with improved training stability for text-to-image synthesis. e-AttnGAN's integrated attention module utilizes both sentence and word context features and performs feature-wise linear modulation (FiLM) to fuse visual and natural language representations. In addition to multimodal similarity learning for text and image features of AttnGAN, cosine and feature matching losses of real and generated images are included while employing a classification loss for 'significant attributes'. In order to improve the stability of the training and solve the issue of model collapse, spectral normalization and two-time scale update for the discriminator are used together with instance noise. Our experiments show that e-AttnGAN outperforms state-of-the-art methods on the FashionGen and DeepFashion-Synthesis datasets. © 2019 IEEE.","Computer vision; Semantic Web; Text processing; Visual languages; Adversarial networks; Image synthesis; Linear modulations; Natural language representation; Similarity learning; Spectral normalization; State-of-the-art methods; Text to image; Image enhancement","Henerative adversarial networks; Image synthesis; Text to image","Conference paper","Final","","Scopus","2-s2.0-85082462604"
"Bejiga M.B.; Melgani F.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300)","57192697078; 35613488300","Towards Generating Remote Sensing Images of the Far Past","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899834","9502","9505","3","10.1109/IGARSS.2019.8899834","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077699173&doi=10.1109%2fIGARSS.2019.8899834&partnerID=40&md5=ca6577d1479893c8d5b6ea3d7f491233","Text-to-image synthesis is a research topic that has not yet been addressed by the remote sensing community. It consists in learning a mapping from text description to image pixels. In this paper, we propose to address this topic for the very first time. More specifically, our objective is to convert ancient text descriptions of geographic areas written by past explorers into an equivalent remote sensing image. To this effect, we rely on generative adversarial networks (GANs) to learn the mapping. GANs aim to represent the distribution of a dataset using weights of a deep neural network, which are trained as an adversarial competition between two networks. We collected ancient texts dating back to 7 BC to train our network and obtained interesting results, which form the basis to highlight future research directions to advance this new topic. © 2019 IEEE.","Deep neural networks; Geology; Image processing; Mapping; Adversarial networks; Future research directions; GANs; Geographic areas; Image pixels; Image synthesis; Remote sensing images; Research topics; Remote sensing","GANs; remote sensing; text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85077699173"
"Yang D.; Xiong T.; Xu D.; Kevin Zhou S.","Yang, Dong (56903331300); Xiong, Tao (56386883300); Xu, Daguang (57191707223); Kevin Zhou, S. (7404165802)","56903331300; 56386883300; 57191707223; 7404165802","Segmentation using adversarial image-to-image networks","2019","Handbook of Medical Image Computing and Computer Assisted Intervention","","","","165","182","17","10.1016/B978-0-12-816176-0.00012-0","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082596469&doi=10.1016%2fB978-0-12-816176-0.00012-0&partnerID=40&md5=612c0f44452833aea130b2ff5a27938b","Generative adversarial network (GAN) nowadays is widely adopted in the field of machine learning, computer vision, and medical image analysis. The deep image-to-image network is an effective and efficient baseline method for medical image segmentation. It has been designed as multiple forms in different applications recently. GAN is capable of improving segmentation performance with global shape constraints. Meanwhile, it can be applied to domain adaptation for both image synthesis and semantic segmentation. © 2020 Elsevier Inc. All rights reserved.","","Cycle GAN; Deep image-to-image network; Domain adaptation; Generative adversarial network (GAN); Semantic segmentation","Book chapter","Final","","Scopus","2-s2.0-85082596469"
"Sung T.L.; Lee H.J.","Sung, Thai Leang (57209823579); Lee, Hyo Jong (55706807400)","57209823579; 55706807400","Image-to-image translation using Identical-pair Adversarial Networks","2019","Applied Sciences (Switzerland)","9","13","2668","","","","10.3390/app9132668","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068850596&doi=10.3390%2fapp9132668&partnerID=40&md5=9b64f91438bdb1b3add8be5ee9ecc2de","We propose Identical-pair Adversarial Networks (iPANs) to solve image-to-image translation problems, such as aerial-to-map, edge-to-photo, de-raining, and night-to-daytime. Our iPANs rely mainly on the effectiveness of adversarial loss function and its network architectures. Our iPANs consist of two main networks, an image transformation network T and a discriminative network D. We use U-NET for the transformation network T and a perceptual similarity network, which has two streams of VGG16 that share the same weights for network D. Our proposed adversarial losses play a minimax game against each other based on a real identical-pair and a fake identical-pair distinguished by the discriminative network D; e.g. a discriminative network D considers two inputs as a real pair only when they are identical, otherwise a fake pair. Meanwhile, the transformation network T tries to persuade the discriminator network D that the fake pair is a real pair. We experimented on several problems of image-to-image translation and achieved results that are comparable to those of some existing approaches, such as pix2pix, and PAN. © 2019 by the authors.","","Generative adversarial network; Image synthesis; Image transformation; Image-to-image translation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85068850596"
"","","","14th Asian Conference on Computer Vision, ACCV 2018","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11364 LNCS","","","","","4380","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066826991&partnerID=40&md5=1bfa94858ce72a6e1e057189eb858f5b","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Continual Occlusion and Optical Flow Estimation; adversarial Learning for Visual Storytelling with Sense Group Partition; laser Scar Detection in Fundus Images Using Convolutional Neural Networks; Gradient-Guided DCNN for Inverse Halftoning and Image Expanding; Learning from PhotoShop Operation Videos: The PSOV Dataset; a Joint Local and Global Deep Metric Learning Method for Caricature Recognition; fast Single Shot Instance Segmentation; a Stable Algebraic Camera Pose Estimation for Minimal Configurations of 2D/3D Point and Line Correspondences; symmetry-Aware Face Completion with Generative Adversarial Networks; growBit: Incremental Hashing for Cross-Modal Retrieval; gated Hierarchical Attention for Image Captioning; region-Semantics Preserving Image Synthesis; SemiStarGAN: Semi-supervised Generative Adversarial Networks for Multi-domain Image-to-Image Translation; gated Transfer Network for Transfer Learning; detecting Anomalous Trajectories via Recurrent Neural Networks; a Binary Optimization Approach for Constrained K-Means Clustering; LS3D: Single-View Gestalt 3D Surface Reconstruction from Manhattan Line Segments; deep Supervised Hashing with Spherical Embedding; semantic Aware Attention Based Deep Object Co-segmentation; PIRC Net: Using Proposal Indexing, Relationships and Context for Phrase Grounding; Paired-D GAN for Semantic Image Synthesis; dealing with Ambiguity in Robotic Grasping via Multiple Predictions; Skeleton Transformer Networks: 3D Human Pose and Skinned Mesh from Single RGB Image; detecting Text in the Wild with Deep Character Embedding Network; design Pseudo Ground Truth with Motion Cue for Unsupervised Video Object Segmentation; identity-Enhanced Network for Facial Expression Recognition; a Novel Multi-scale Invariant Descriptor Based on Contour and Texture for Shape Recognition.","","","Conference review","Final","","Scopus","2-s2.0-85066826991"
"Li Q.; Luo Y.","Li, Qixin (57831750600); Luo, Yaneng (57191528235)","57831750600; 57191528235","Using GAN priors for ultrahigh resolution seismic inversion","2019","SEG Technical Program Expanded Abstracts","","","","2453","2457","4","10.1190/segam2019-3215520.1","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100508253&doi=10.1190%2fsegam2019-3215520.1&partnerID=40&md5=ad9566a00fc94e931bee0b3c96cb4ea0","As one of machine learning techniques, deep learning has recently achieved the state-of-the-art performances in many areas, such as computer vision, natural language processing, to name a few. A generative model called Generative Adversarial Network (GAN) was invented in 2014. This deep network model is deemed as the most interesting idea in the last 10 years by the machine learning community and outperformed the traditional methods in many tasks like image synthesis and super-resolution. Laying the heart of the GAN is its ability to model any realistically sharp data distribution. Instead of providing a “blurry” sample, the high-resolution samples can be sampled from the GAN model, no matter it is a natural image or a well log. In this abstract, we propose a novel ultrahigh resolution seismic inversion method using GAN priors. The basic workflow is described below. Firstly, a simple GAN architecture was designed. Then, we train this GAN to model the well-log data distribution. Once the GAN is properly trained, it offers the high-resolution samples as priors to the inversion algorithm. To effectively use this prior information, we adopt the projected gradient descent algorithm to iteratively fit the seismic data and projects the “blurry” sample to the high resolution set of prior samples defined by the GAN. We further use a thin-layer model to validate the feasibility and superiority of our method. Comparing with the traditional method, our result shows a higher precision and resolution. © 2019 SEG","Deep learning; Gradient methods; Learning algorithms; Natural language processing systems; Seismology; Well logging; Data distribution; Generative model; High resolution; Images synthesis; Machine learning communities; Machine learning techniques; Network models; Seismic inversion; State-of-the-art performance; Ultrahigh resolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85100508253"
"Zhang X.; DIng J.; Errapotu S.M.; Huang X.; Li P.; Pan M.","Zhang, Xinyue (57204791151); DIng, Jiahao (57207980247); Errapotu, Sai Mounika (57192114761); Huang, Xiaoxia (37057229000); Li, Pan (16205150700); Pan, Miao (23009949300)","57204791151; 57207980247; 57192114761; 37057229000; 16205150700; 23009949300","Differentially private functional mechanism for generative adversarial networks","2019","2019 IEEE Global Communications Conference, GLOBECOM 2019 - Proceedings","","","9014134","","","","10.1109/GLOBECOM38437.2019.9014134","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081957290&doi=10.1109%2fGLOBECOM38437.2019.9014134&partnerID=40&md5=182f8c8f93a19e4a51c32ec02052c321","In recent years, generative adversarial network (GAN) has attracted great attention due to its impressive performance and potential numerous applications, such as data augmentation, real-like image synthesis, image compression improvement, etc. The generator in GAN learns the density of the distribution from real data in order to generate high fidelity fake samples from latent space and deceive the discriminator. Despite its advantages, GAN can easily memorize training samples because of the high model complexity of deep neural networks. Thus, training a GAN with sensitive or private data samples may compromise the privacy of training data. To address this privacy issue, we propose a novel textit{Privacy Preserving Generative Adversarial Network} (PPGAN) that perturbs the objective function of discriminator by injecting Laplace noises based on functional mechanism to guarantee the differential privacy of training data. Since generator training is considered as a post-processing step while guaranteeing differential privacy of discriminator, the trained generator should be differentially private to effectively protect data samples. Through detailed privacy analysis, we theoretically prove that PPGAN can provide such strict differential privacy guarantee. With extensive simulation study on the benchmark dataset MNIST, we show the efficacy of the proposed PPGAN under practical privacy budgets. © 2019 IEEE.","Budget control; Deep neural networks; Discriminators; Image compression; Image enhancement; Adversarial networks; Benchmark datasets; Data augmentation; Differential privacies; Extensive simulations; Functional mechanisms; Objective functions; Privacy preserving; Data privacy","Differential privacy; Functional mechanism; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85081957290"
"Chen Z.; Luo Y.","Chen, Zhi (57210803155); Luo, Yadan (57191907315)","57210803155; 57191907315","Cycle-consistent diverse image synthesis from natural language","2019","Proceedings - 2019 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2019","","","8795022","459","464","5","10.1109/ICMEW.2019.00085","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071457637&doi=10.1109%2fICMEW.2019.00085&partnerID=40&md5=f26e8e8233750ec579f824b4efd86087","Text-to-image translation has become an attractive yet challenging task in computer vision. Previous approaches tend to generate similar, or even monotonous, images for distinctive texts and overlook the characteristics of specific sentences. In this paper, we aim to generate images from the given texts by preserving diverse appearances and modes of the objects or instances contained. To achieve that, a novel learning model named SuperGAN is proposed, which consists of two major components: an image synthesis network and a captioning model in a Cycle-GAN framework. SuperGAN adopts the cycle-consistent adversarial training strategy to learn an image generator where the feature distribution of the generated images complies with the distribution of the generic images. Meanwhile, a cycle-consistency loss is applied to constrain that the caption of the generated images is closed to the original texts. Extensive experiments on the benchmark dataset Oxford-flowers-102 demonstrate the validity and effectiveness of our proposed method. In addition, a new evaluation metric is proposed to measure the diversity of synthetic results. © 2019 IEEE.","Natural language processing systems; Adversarial networks; Benchmark datasets; Evaluation metrics; Feature distribution; Image captioning; Image synthesis; Image translation; Training strategy; Image processing","Cycle-consistency loss; Generative adversarial networks; Image captioning; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85071457637"
"Coiffier G.; Renard P.","Coiffier, G. (57225904972); Renard, P. (7102645765)","57225904972; 7102645765","3D geological image synthesis from 2D examples using generative adversarial networks","2019","4th EAGE Conference on Petroleum Geostatistics","","","ThPG01","","","","10.3997/2214-4609.201902198","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073115741&doi=10.3997%2f2214-4609.201902198&partnerID=40&md5=1694cac32648c471353988ef962cbe4f","Recently, Generative Adversarial Networks (GAN) have been proposed as a potential alternative to Multipoint Statistics (MPS) to generate stochastic fields from a large set of training images. A difficulty for all the training image based techniques (including GAN and MPS) is to generate 3D fields when only 2D training data sets are available. In this paper, we introduce a novel approach called Dimension Augmenter GAN (DiAGAN) enabling GANs to generate 3D fields from 2D examples. The method is simple to implement and is based on the introduction of a random cut sampling step between the generator and the discriminator of a standard GAN. Numerical experiments show that the proposed approach provides an efficient solution to this long lasting problem. © EAGE 2019.","Stochastic systems; Adversarial networks; Image synthesis; Multi-point statistics; Numerical experiments; Sampling steps; Stochastic field; Training data sets; Training image; Gasoline","","Conference paper","Final","","Scopus","2-s2.0-85073115741"
"Sun J.; Zhou Y.; Zhang B.","Sun, Jingcong (57205199381); Zhou, Yimin (55556256200); Zhang, Bin (55605769073)","57205199381; 55556256200; 55605769073","ResFPA-GAN: Text-to-Image Synthesis with Generative Adversarial Network Based on Residual Block Feature Pyramid Attention","2019","Proceedings of IEEE Workshop on Advanced Robotics and its Social Impacts, ARSO","2019-October","","8948717","317","322","5","10.1109/ARSO46408.2019.8948717","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078340036&doi=10.1109%2fARSO46408.2019.8948717&partnerID=40&md5=2967620aaf1f418045547f5f2588a964","Text-to-image synthesis based on generative adversarial networks (GAN) is a challenging task. The developed methods have show prominent progress on visual quality of the synthesized images, but it still face challenge in the image synthesis of details. In this paper, we introduce an image synthesis algorithm based on semantic description and propose a residual block feature pyramid attention generative adversarial network, called ResFPA-GAN. This network introduces multiscale feature fusion by embedding feature pyramid structure to achieve the fine-grained image synthesis. The quality of the image synthesis can be improved via the iterative training of GAN, while the reference of attention can enhance the network's learning of the details of image texture. Through extensive experimental comparison on the CUB dataset, our method can achieve significant improvement on the variety and authenticity for the synthesised images. © 2019 IEEE.","Economic and social effects; Image enhancement; Iterative methods; Robotics; Semantics; Textures; Adversarial networks; Experimental comparison; Feature pyramid; Image synthesis; Multi-scale features; Semantic descriptions; Synthesized images; Visual qualities; Image texture","","Conference paper","Final","","Scopus","2-s2.0-85078340036"
"Liu C.; Lu Z.; Ma L.; Wang L.; Jin X.; Si W.","Liu, Cong (57200872012); Lu, Zheming (27467442800); Ma, Longhua (7403575033); Wang, Lang (57196335365); Jin, Xiance (15044718700); Si, Wen (36515671100)","57200872012; 27467442800; 7403575033; 57196335365; 15044718700; 36515671100","A modality conversion approach to MV-DRs and KV-DRRs registration using information bottlenecked conditional generative adversarial network","2019","Medical Physics","46","10","","4575","4587","12","10.1002/mp.13770","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071854994&doi=10.1002%2fmp.13770&partnerID=40&md5=2a8d3e3287f6e96f441786f1ed96ad57","Purpose: As affordable equipment, electronic portal imaging devices (EPIDs) are wildly used in radiation therapy departments to verify patients’ positions for accurate radiotherapy. However, these devices tend to produce visually ambiguous and low-contrast planar digital radiographs under megavoltage x ray (MV-DRs), which poses a tremendous challenge for clinicians to perform multimodal registration between the MV-DRs and the kilovoltage digital reconstructed radiographs (KV-DRRs) developed from the planning computed tomography. Furthermore, the existent of strong appearance variations also makes accurate registration beyond the reach of current automatic algorithms. Methods: We propose a novel modality conversion approach to this task that first synthesizes KV images from MV-DRs, and then registers the synthesized and real KV-DRRs. We focus on the synthesis technique and develop a conditional generative adversarial network with information bottleneck extension (IB-cGAN) that takes MV-DRs and nonaligned KV-DRRs as inputs and outputs synthesized KV images. IB-cGAN is designed to address two main challenges in deep-learning-based synthesis: (a) training with a roughly aligned dataset suffering from noisy correspondences; (b) making synthesized images have real clinical meanings that faithfully reflects MV-DRs rather than nonaligned KV-DRRs. Accordingly, IB-cGAN employs (a) an adversarial loss to provide training supervision at semantic level rather than the imprecise pixel level; (b) an IB to constrain the information from the nonaligned KV-DRRs. Results: We collected 2698 patient scans to train the model and 208 scans to test its performance. The qualitative results demonstrate realistic KV images can be synthesized allowing clinicians to perform the visual registration. The quantitative results show it significantly outperforms current nonmodality conversion methods by 22.37% (P = 0.0401) in terms of registration accuracy. Conclusions: The modality conversion approach facilitates the downstream MV–KV registration for both clinicians and off-the-shelf registration algorithms. With this approach, it is possible to benefit the developing countries where inexpensive EPIDs are widely used for the image-guided radiation therapy. © 2019 American Association of Physicists in Medicine","Image Processing, Computer-Assisted; Machine Learning; Radiography; Article; body regions; clinical effectiveness; clinical evaluation; computer assisted tomography; controlled study; digital radiography; electric potential; human; image analysis; image guided radiotherapy; image reconstruction; image registration; imaging algorithm; information bottlenecked conditional generative adversarial network; information processing; kilovoltage digital reconstructed radiograph; major clinical study; megavoltage radiation; megavoltage x ray; qualitative analysis; visual information; X ray; image processing; machine learning; procedures; radiography","generative adversarial networks; image synthesis; image-guided radiation therapy; information bottleneck; multimodal image registration","Article","Final","","Scopus","2-s2.0-85071854994"
"Bozorgtabar B.; Rad M.S.; Ekenel H.K.; Thiran J.-P.","Bozorgtabar, Behzad (37109488100); Rad, Mohammad Saeed (57196089451); Ekenel, Hazım Kemal (55958877400); Thiran, Jean-Philippe (35554798200)","37109488100; 57196089451; 55958877400; 35554798200","Learn to synthesize and synthesize to learn","2019","Computer Vision and Image Understanding","185","","","1","11","10","10.1016/j.cviu.2019.04.010","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066147205&doi=10.1016%2fj.cviu.2019.04.010&partnerID=40&md5=aaa391d0bf2521a52f73599c41c56091","Attribute guided face image synthesis aims to manipulate attributes on a face image. Most existing methods for image-to-image translation can either perform a fixed translation between any two image domains using a single attribute or require training data with the attributes of interest for each subject. Therefore, these methods could only train one specific model for each pair of image domains, which limits their ability in dealing with more than two domains. Another disadvantage of these methods is that they often suffer from the common problem of mode collapse that degrades the quality of the generated images. To overcome these shortcomings, we propose attribute guided face image generation method using a single model, which is capable to synthesize multiple photo-realistic face images conditioned on the attributes of interest. In addition, we adopt the proposed model to increase the realism of the simulated face images while preserving the face characteristics. Compared to existing models, synthetic face images generated by our method present a good photorealistic quality on several face datasets. Finally, we demonstrate that generated facial images can be used for synthetic data augmentation, and improve the performance of the classifier used for facial expression recognition. © 2019","Face recognition; Human computer interaction; Adversarial networks; Face image synthesis; Facial expression recognition; Image translation; Photo-realistic; Photorealistic quality; Synthetic data; Synthetic faces; Image enhancement","Attribute guided face image synthesis; Facial expression recognition; Generative adversarial network","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85066147205"
"Huang H.; Zhang F.; Zhou Y.; Yin Q.; Hu W.","Huang, Henghua (57219442515); Zhang, Fan (56320587700); Zhou, Yongsheng (22959334700); Yin, Qiang (36959885000); Hu, Wei (56316293300)","57219442515; 56320587700; 22959334700; 36959885000; 56316293300","High resolution sar image synthesis with hierarchical generative adversarial networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900494","2782","2785","3","10.1109/IGARSS.2019.8900494","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082673599&doi=10.1109%2fIGARSS.2019.8900494&partnerID=40&md5=a10eaaed3665d0fd8aacfcae65f8bade","Generative adversarial network (GAN) is an artificial neural network based on unsupervised learning method. Due to its powerful model representation capabilities, GAN has been introduced to synthesize synthetic aperture radar (SAR) image data, for the real sample is difficult to acquire. Largescale, high-resolution SAR images play an important role in promoting SAR applications, such as automatic target recognition and image interpretation. However, on account of the difficult training problem of GAN network, especially for SAR images with speckle noise, it is difficult to obtain high-resolution SAR images by simply transfer the net from optical image. Recent studies in other image fields have shown that hierarchical structure is an effective and useful way to decompose a generation task into several smaller subtasks. How to obtain more high-resolution SAR images from limited original samples through GAN is the target of our research. Therefore, in this paper, we introduce a hierarchical GAN network model to generate SAR images, through the multi-stage network, gradually improve the quality of the generated image, and finally obtain highresolution images. The type and aspect of generated images are determined by the input of condition vectors in the last two stages. In addition, we introduce the triple loss, in which the background loss is used to imitating background clutter noise of SAR image, the condition loss is to make the generated images' type and aspect become controllable, and the global loss for getting higher image generation quality. The generated images show high similarity with the real samples.  © 2019 IEEE.","Automatic target recognition; Geometrical optics; Image enhancement; Learning systems; Radar target recognition; Remote sensing; Synthetic aperture radar; Unsupervised learning; Adversarial networks; Hierarchical structures; High resolution image; High-resolution SAR; Image interpretation; Model representation; Synthetic aperture radar (SAR) images; Unsupervised learning method; Radar imaging","Automatic target recognition (ATR); Generative adversarial network(GAN); SAR simulator; Synthetic aperture radar (SAR); Triple loss","Conference paper","Final","","Scopus","2-s2.0-85082673599"
"","","","2nd International Workshop on Machine Learning for Medical Image Reconstruction, MLMIR 2019 held in Conjunction with 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11905 LNCS","","","","","264","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076201995&partnerID=40&md5=7756405fa8d0d20a48fd30ea90563ea3","The proceedings contain 24 papers. The special focus in this conference is on Machine Learning for Medical Image Reconstruction. The topics include: Measuring CT Reconstruction Quality with Deep Convolutional Neural Networks; deep Learning Based Metal Inpainting in the Projection Domain: Initial Results; flexible Conditional Image Generation of Missing Data with Learned Mental Maps; Spatiotemporal PET Reconstruction Using ML-EM with Learned Diffeomorphic Deformation; stain Style Transfer Using Transitive Adversarial Networks; Blind Deconvolution Microscopy Using Cycle Consistent CNN with Explicit PSF Layer; Deep Learning Based Approach to Quantification of PET Tracer Uptake in Small Tumors; Task-GAN: Improving Generative Adversarial Network for Image Reconstruction; gamma Source Location Learning from Synthetic Multi-pinhole Collimator Data; Self-supervised Recurrent Neural Network for 4D Abdominal and In-utero MR Imaging; neural Denoising of Ultra-low Dose Mammography; image Reconstruction in a Manifold of Image Patches: Application to Whole-Fetus Ultrasound Imaging; image Super Resolution via Bilinear Pooling: Application to Confocal Endomicroscopy; TPSDicyc: Improved Deformation Invariant Cross-domain Medical Image Synthesis; PredictUS: A Method to Extend the Resolution-Precision Trade-Off in Quantitative Ultrasound Image Reconstruction; fast Dynamic Perfusion and Angiography Reconstruction Using an End-to-End 3D Convolutional Neural Network; APIR-Net: Autocalibrated Parallel Imaging Reconstruction Using a Neural Network; Accelerated MRI Reconstruction with Dual-Domain Generative Adversarial Network; Deep Learning for Low-Field to High-Field MR: Image Quality Transfer with Probabilistic Decimation Simulator; joint Multi-anatomy Training of a Variational Network for Reconstruction of Accelerated Magnetic Resonance Image Acquisitions; modeling and Analysis Brain Development via Discriminative Dictionary Learning; Virtual Thin Slice: 3D Conditional GAN-based Super-Resolution for CT Slice Interval.","","","Conference review","Final","","Scopus","2-s2.0-85076201995"
"Zhang T.; Fu H.; Zhao Y.; Cheng J.; Guo M.; Gu Z.; Yang B.; Xiao Y.; Gao S.; Liu J.","Zhang, Tianyang (57211191800); Fu, Huazhu (35317209500); Zhao, Yitian (56583188900); Cheng, Jun (57535555300); Guo, Mengjie (57212003737); Gu, Zaiwang (57202116090); Yang, Bing (57212003336); Xiao, Yuting (57212002326); Gao, Shenghua (35224747100); Liu, Jiang (23389932700)","57211191800; 35317209500; 56583188900; 57535555300; 57212003737; 57202116090; 57212003336; 57212002326; 35224747100; 23389932700","SkrGAN: Sketching-rendering unconditional generative adversarial networks for medical image synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11767 LNCS","","","777","785","8","10.1007/978-3-030-32251-9_85","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075665995&doi=10.1007%2f978-3-030-32251-9_85&partnerID=40&md5=d633612e6e8170e24f331d9334ef7d5f","Generative Adversarial Networks (GANs) have the capability of synthesizing images, which have been successfully applied to medical image synthesis tasks. However, most of existing methods merely consider the global contextual information and ignore the fine foreground structures, e.g., vessel, skeleton, which may contain diagnostic indicators for medical image analysis. Inspired by human painting procedure, which is composed of stroking and color rendering steps, we propose a Sketching-rendering Unconditional Generative Adversarial Network (SkrGAN) to introduce a sketch prior constraint to guide the medical image generation. In our SkrGAN, a sketch guidance module is utilized to generate a high quality structural sketch from random noise, then a color render mapping is used to embed the sketch-based representations and resemble the background appearances. Experimental results show that the proposed SkrGAN achieves the state-of-the-art results in synthesizing images for various image modalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). In addition, we also show that the performances of medical image segmentation method has been improved by using our synthesized images as data augmentation. © Springer Nature Switzerland AG 2019.","Color; Computerized tomography; Deep learning; Diagnosis; Image enhancement; Image segmentation; Magnetic resonance imaging; Medical computing; Medical imaging; Adversarial networks; Contextual information; Data augmentation; Diagnostic indicators; Image generations; Image synthesis; Structural sketch; Synthesized images; Rendering (computer graphics)","Deep learning; Generative Adversarial Networks; Medical image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075665995"
"Yu B.; Wang Y.; Wang L.; Shen D.; Zhou L.","Yu, Biting (57201496052); Wang, Yan (56039981100); Wang, Lei (54958774700); Shen, Dinggang (7401738392); Zhou, Luping (23398846800)","57201496052; 56039981100; 54958774700; 7401738392; 23398846800","Medical Image Synthesis via Deep Learning","2020","Advances in Experimental Medicine and Biology","1213","","","23","44","21","10.1007/978-3-030-33128-3_2","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079082706&doi=10.1007%2f978-3-030-33128-3_2&partnerID=40&md5=73ba201ce20d7016a28501b4e563569b","Medical images have been widely used in clinics, providing visual representations of under-skin tissues in human body. By applying different imaging protocols, diverse modalities of medical images with unique characteristics of visualization can be produced. Considering the cost of scanning high-quality single modality images or homogeneous multiple modalities of images, medical image synthesis methods have been extensively explored for clinical applications. Among them, deep learning approaches, especially convolutional neural networks (CNNs) and generative adversarial networks (GANs), have rapidly become dominating for medical image synthesis in recent years. In this chapter, based on a general review of the medical image synthesis methods, we will focus on introducing typical CNNs and GANs models for medical image synthesis. Especially, we will elaborate our recent work about low-dose to high-dose PET image synthesis, and cross-modality MR image synthesis, using these models. © 2020, Springer Nature Switzerland AG.","Deep Learning; Humans; Image Processing, Computer-Assisted; brain; convolutional neural network; deep learning; drug megadose; human; human experiment; low drug dose; nuclear magnetic resonance imaging; positron emission tomography; skin; synthesis; image processing; procedures","Brain; Convolutional neural networks (CNNs); Deep learning; Generative adversarial networks (GANs); Machine learning; Magnetic resonance imaging (MRI); Medical image synthesis; Positron emission tomography (PET)","Book chapter","Final","","Scopus","2-s2.0-85079082706"
"Chen Y.; Zhu X.; Gong S.","Chen, Yanbei (57222344538); Zhu, Xiatian (56050744800); Gong, Shaogang (7203001358)","57222344538; 56050744800; 7203001358","Instance-guided context rendering for cross-domain person re-identification","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9008370","232","242","10","10.1109/ICCV.2019.00032","107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078551097&doi=10.1109%2fICCV.2019.00032&partnerID=40&md5=75559ba73d76c2dfcc28b7c5ffbfacb4","Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning. © 2019 IEEE.","Benchmarking; Computer vision; Data visualization; Learning systems; Transfer learning; Adversarial networks; Identity labels; Image generations; Image synthesis; Model learning; Person re identifications; Synthetic data; Synthetic training data; Rendering (computer graphics)","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078551097"
"Zhang F.; Zhang T.; Mao Q.; Xu C.","Zhang, Feifei (57138854900); Zhang, Tianzhu (55729040600); Mao, Qirong (7101735930); Xu, Changsheng (56153258200)","57138854900; 55729040600; 7101735930; 56153258200","Joint Pose and Expression Modeling for Facial Expression Recognition","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578452","3359","3368","9","10.1109/CVPR.2018.00354","167","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058228451&doi=10.1109%2fCVPR.2018.00354&partnerID=40&md5=e8ad814a0af47f33a5b12d9eec2b021d","Facial expression recognition (FER) is a challenging task due to different expressions under arbitrary poses. Most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifiers for each pose. Different from existing methods, in this paper, we propose an end-to-end deep learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, the encoder-decoder structure of the generator can learn a generative and discriminative identity representation for face images. Second, the identity representation is explicitly disentangled from both expression and pose variations through the expression and pose codes. Third, our model can automatically generate face images with different expressions under arbitrary poses to enlarge and enrich the training set for FER. Quantitative and qualitative evaluations on both controlled and in-the-wild datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods. © 2018 IEEE.","Computer vision; Deep learning; Adversarial networks; Conventional approach; Expression modeling; Facial expression recognition; Facial Image synthesis; Learning models; Qualitative evaluations; State-of-the-art methods; Face recognition","","Conference paper","Final","","Scopus","2-s2.0-85058228451"
"Ye L.; Zhang B.; Yang M.; Lian W.","Ye, Linbin (57208923593); Zhang, Bob (57217861698); Yang, Meng (55703267800); Lian, Wei (57203753305)","57208923593; 57217861698; 55703267800; 57203753305","Triple-translation GAN with multi-layer sparse representation for face image synthesis","2019","Neurocomputing","358","","","294","308","14","10.1016/j.neucom.2019.04.074","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066153345&doi=10.1016%2fj.neucom.2019.04.074&partnerID=40&md5=8558ef105cffc4bd0bc3542fd8631b06","Face image synthesis with facial feature and identity preserving is one of the key challenges in computer vision. Recently, outstanding performances in image translation and synthesis have been reported in CycleGAN. However, for the task of face image synthesis, there are still several remaining issues (e.g., poor-visual-quality facial feature, changed face identity, unstable model optimization). In order to solve the above issues, in this paper we propose a novel model of triple translation GAN (TTGAN) with multi-layer sparse representation. We design a multi-layer sparse representation model, in which L1-norm representation constraint is integrated into the image generation to enhance the ability of identity preserving and the robustness of the generated facial images to reconstruction error. Moreover, in order to improve the stability of the model optimization, we propose a triple translation consistence loss, including a designed third image translation from a reconstructed original input to a desired output. The face synthesis experimental results on the benchmark face databases clearly shows the superior performance over the competing methods. © 2019 Elsevier B.V.","Benchmarking; Image reconstruction; Adversarial networks; Face image synthesis; Face synthesis; Image generations; Image translation; Model optimization; Reconstruction error; Sparse representation; article; face; identity; synthesis; Image enhancement","Face synthesis; Generative Adversarial Networks (GANs); Triple translation","Article","Final","","Scopus","2-s2.0-85066153345"
"Su L.; Xu X.; Lu Q.; Zhang W.","Su, Lei (57214659566); Xu, Xiangyi (57208795860); Lu, Qiyu (57208796426); Zhang, Wancai (57208800906)","57214659566; 57208795860; 57208796426; 57208800906","General image classification method based on semi-supervised generative adversarial networks","2019","High Technology Letters","25","1","","35","41","6","10.3772/j.issn.1006-6748.2019.01.005","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065813567&doi=10.3772%2fj.issn.1006-6748.2019.01.005&partnerID=40&md5=ee408b41b8ea7de95be9ea3a806e74c0","Generative adversarial networks (GANs) have become a competitive method among computer vision tasks. There have been many studies devoted to utilizing generative network to do generative tasks, such as images synthesis. In this paper, a semi-supervised learning scheme is incorporated with generative adversarial network on image classification tasks to improve the image classification accuracy. Two applications of GANs are mainly focused on: semi-supervised learning and generation of images which can be as real as possible. The whole process is divided into two sections. First, only a small part of the dataset is utilized as labeled training data. And then a huge amount of samples generated from the generator is added into the training samples to improve the generalization of the discriminator. Through the semi-supervised learning scheme, full use of the unlabeled data is made which may contain potential information. Thus, the classification accuracy of the discriminator can be improved. Experimental results demonstrate the improvement of the classification accuracy of discriminator among different datasets, such as MNIST, CIFAR-10. Copyright © by HIGH TECHNOLOGY LETTERS PRESS.","Classification (of information); Image enhancement; Learning algorithms; Machine learning; Supervised learning; Adversarial networks; Classification accuracy; Classification methods; Images synthesis; Labeled training data; Semi- supervised learning; Semi-supervised; Training sample; Image classification","Generative adversarial network (GAN); Image classification; Semi-supervised","Article","Final","","Scopus","2-s2.0-85065813567"
"Wang Z.-H.; Wang N.; Shi J.; Li J.-J.; Yang H.","Wang, Zhi-Hui (55719820900); Wang, Ning (57702345200); Shi, Jian (57203747863); Li, Jian-Jun (57206962714); Yang, Hairui (55794183300)","55719820900; 57702345200; 57203747863; 57206962714; 55794183300","Multi-Instance Sketch to Image Synthesis with Progressive Generative Adversarial Networks","2019","IEEE Access","7","","8698864","56683","56693","10","10.1109/ACCESS.2019.2913178","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065876215&doi=10.1109%2fACCESS.2019.2913178&partnerID=40&md5=b51d93a11697af4116769c344da8df1a","Real-world images usually contain multiple objects, as a result, generating an image from a multi-instance sketch is an attractive research topic. However, existing generative networks usually produce a similar texture on different instances for those methods focus on learning the distribution of the whole image. To address this problem, we propose a progressive instance texture reserved generative approach to generate more convincible images by decoupling the generation of the instances and the whole image. Specifically, we create an instance generator to synthesize the primitive color distribution and the detailed texture for each instance. Then, an image generator is designed to combine all of these instances to synthesize an image retaining texture and color. Besides, to generate more significant details, such as eyes, ears, and so on, we propose a novel technique called discriminative sketch augmentation, which can provide structural constraint by obtaining the sketch of the discriminative region. The extensive experiments demonstrate that our model not only generates convincing images but also achieves higher inception score and lower Fréchet Inception Distance on the MS-COCO dataset. © 2013 IEEE.","Image processing; Textures; Adversarial networks; Color distribution; Image generations; Instance generator; Multi-instance sketch to image; Novel techniques; Real-world image; Structural constraints; Image texture","image generation; image processing; Multi-instance sketch to image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85065876215"
"Dong H.; Liang X.; Shen X.; Wu B.; Chen B.-C.; Yin J.","Dong, Haoye (57208444380); Liang, Xiaodan (55926362100); Shen, Xiaohui (7402721428); Wu, Bowen (57215775583); Chen, Bing-Cheng (57215770728); Yin, Jian (35316639800)","57208444380; 55926362100; 7402721428; 57215775583; 57215770728; 35316639800","FW-GAN: Flow-navigated warping GAN for video virtual try-on","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9009020","1161","1170","9","10.1109/ICCV.2019.00125","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081895851&doi=10.1109%2fICCV.2019.00125&partnerID=40&md5=92e3d271a3d1b3aafa9426b424477187","Beyond current image-based virtual try-on systems that have attracted increasing attention, we move a step forward to developing a video virtual try-on system that precisely transfers clothes onto the person and generates visually realistic videos conditioned on arbitrary poses. Besides the challenges in image-based virtual try-on (e.g., clothes fidelity, image synthesis), video virtual try-on further requires spatiotemporal consistency. Directly adopting existing image-based approaches often fails to generate coherent video with natural and realistic textures. In this work, we propose Flow-navigated Warping Generative Adversarial Network (FW-GAN), a novel framework that learns to synthesize the video of virtual try-on based on a person image, the desired clothes image, and a series of target poses. FW-GAN aims to synthesize the coherent and natural video while manipulating the pose and clothes. It consists of: (i) a flow-guided fusion module that warps the past frames to assist synthesis, which is also adopted in the discriminator to help enhance the coherence and quality of the synthesized video; (ii) a warping net that is designed to warp clothes image for the refinement of clothes textures; (iii) a parsing constraint loss that alleviates the problem caused by the misalignment of segmentation maps from images with different poses and various clothes. Experiments on our newly collected dataset show that FW-GAN can synthesize high-quality video of virtual try-on and significantly outperforms other methods both qualitatively and quantitatively. © 2019 IEEE.","Computer vision; Image enhancement; Image segmentation; Navigation; Textures; Adversarial networks; Constraint loss; Fusion modules; High quality video; Image synthesis; Segmentation map; Spatio-temporal consistencies; Virtual try-on; Image texture","","Conference paper","Final","","Scopus","2-s2.0-85081895851"
"Wang G.; Dong G.; Li H.; Han L.; Tao X.; Ren P.","Wang, Guangxing (57213191777); Dong, Guoshuai (57208247636); Li, Hui (57207879663); Han, Lirong (57203552486); Tao, Xuanwen (57205509060); Ren, Peng (25960361900)","57213191777; 57208247636; 57207879663; 57203552486; 57205509060; 25960361900","Remote Sensing Image Synthesis via Graphical Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898915","10027","10030","3","10.1109/IGARSS.2019.8898915","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077676346&doi=10.1109%2fIGARSS.2019.8898915&partnerID=40&md5=3895a5ecd138121cbdc7d04b7c58f859","We explore the use of graphical generative adversarial networks (Graphical-GAN) for synthesizing remote sensing images. The model is probabilistic graphical based generative adversarial networks (GAN). It pairs a generative network G with a recognition network R. Both of them are adversarially trained with a discriminative network D. Particularly, R is employed to infer the underlying causal relationships among both observed and latent variables from real remote sensing images. The advantages of the Graphical-GAN for synthesizing multiple categories of remote sensing images are two fold. Firstly, it considers the underlying causal relationships and captures the true data distribution of remote sensing images. Secondly, the adversarial learning generates synthetic sensing images that are similar to real ones with slight differences. Our remote sensing image synthesis scheme paves a promising way for remote sensing dataset augmentation, which is an effective means of improving the accuracy of learning models. Experimental results with high Inception Scores (IS) validate the effectiveness of the Graphical-GAN for remote sensing image synthesis. © 2019 IEEE.","Geology; Image enhancement; Adversarial learning; Adversarial networks; Causal relationships; Data distribution; Discriminative networks; Latent variable; Learning models; Remote sensing images; Remote sensing","Graphical Generative Adversarial Networks; Remote Sensing Image Synthesis","Conference paper","Final","","Scopus","2-s2.0-85077676346"
"Lee H.; Lee S.-G.","Lee, Hanbit (56911272000); Lee, Sang-goo (27168587800)","56911272000; 27168587800","Fashion attributes-to-image synthesis using attention-based generative adversarial network","2019","Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019","","","8658244","462","470","8","10.1109/WACV.2019.00055","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063584037&doi=10.1109%2fWACV.2019.00055&partnerID=40&md5=1e2f6da2ca58d33514fd0fee174f6c80","In this paper, we present a method to generate fashion product images those are consistent with a given set of fashion attributes. Since distinct fashion attributes are related to different local sub-regions of a product image, we propose to use generative adversarial network with attentional discriminator. The attribute-attended loss signal from discriminator leads generator to generate more consistent images with given attributes. In addition, we present a generator based on Product-of-Gaussian to encode the composition of fashion attributes in effective way. To verify the proposed model whether it generates consistent image, an oracle attribute classifier is trained and judge the consistency of given attributes and the generated images. Our model significantly outperforms the baseline model in terms of correctness measured by the pre-trained oracle classifier. We show not only qualitative performance but also synthesized images with various combinations of attributes, so we can compare them with baseline model. © 2019 IEEE","Computer science; Computers; Adversarial networks; Baseline models; Gaussians; Image synthesis; Product images; Sub-regions; Synthesized images; Computer vision","","Conference paper","Final","","Scopus","2-s2.0-85063584037"
"Park H.; Yoo Y.; Kwak N.","Park, Hyojin (57215362857); Yoo, Youngjoon (56047856200); Kwak, Nojun (7005248772)","57215362857; 56047856200; 7005248772","MC-GAN: Multi-conditional generative adversarial network for image synthesis","2019","British Machine Vision Conference 2018, BMVC 2018","","","","","","","","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084013662&partnerID=40&md5=ea0a9ed2f5b47c967ae6edcdd078c6c3","In this paper, we introduce a new method for generating an object image from text attributes on a desired location, when the base image is given. One step further to the existing studies on text-to-image generation mainly focusing on the object's appearance, the proposed method aims to generate an object image preserving the given background information, which is the first attempt in this field. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which controls both the object and background information jointly. As a core component of MC-GAN, we propose a synthesis block which disentangles the object and background information in the training stage. This block enables MC-GAN to generate a realistic object image with the desired background by controlling the amount of the background information from the given base image using the foreground information from the text attributes. From the experiments with Caltech-200 bird and Oxford-102 flower datasets, we show that our model is able to generate photo-realistic images with a resolution of 128  128. © 2018. The copyright of this document resides with its authors.","Computer vision; Adversarial networks; Background information; Core components; Foreground information; Image generations; Image synthesis; Photorealistic images; Text attributes; Image segmentation","","Conference paper","Final","","Scopus","2-s2.0-85084013662"
"Chin J.; Mehmood A.","Chin, Jonathan (57211081108); Mehmood, Asif (36133731600)","57211081108; 36133731600","Generative adversarial networks based super resolution of satellite aircraft imagery","2019","Proceedings of SPIE - The International Society for Optical Engineering","10995","","109950W","","","","10.1117/12.2524720","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070258802&doi=10.1117%2f12.2524720&partnerID=40&md5=75e4bc319f16b44aeac3d4bc6d3c1527","Generative Adversarial Networks (GANs) are one of the most popular Machine Learning algorithms developed in recent times, and are a class of neural networks that are used in unsupervised machine learning. The advantage of unsupervised machine learning approaches such as GANs is that they do not need a large amount of labeled data, which is costly and time consuming. GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. In this work, GANs are utilized to solve the single image super-resolution problem. This approach in literature is referred to as super resolution GANs (SRGAN), and employs a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes the solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and the original photo-realistic images, and the content loss is motivated by the perceptual similarity and not the similarity in the pixel space. This paper presents implementation of SRGAN using Deep convolution network applied to both the aerial and satellite imagery of the aircrafts. The results thus obtained are compared with traditional super resolution methods. The resulting estimates of SRGAN are compared against the traditional methods using peak signal to noise ratio (PSNR) and structure similarity index metric (SSIM). The PSNR and SSIM of SRGAN estimates are similar to traditional method such as Bicubic interpolation but traditional methods are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. © 2019 SPIE. Downloading of the abstract is permitted for personal use only.","Aircraft; Antennas; Convolution; Deep learning; Deep neural networks; Learning algorithms; Neural networks; Optical resolving power; Pattern recognition; Satellite imagery; Semantics; Signal to noise ratio; Convolutional neural network; Image super resolutions; Peak signal to noise ratio; Perceptual similarity; Single images; Super resolution; Superresolution methods; Unsupervised machine learning; Machine learning","Convolutional Neural Network; Deep Learning; Satellite Imagery; Single Image Super Resolution; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85070258802"
"Jin C.-B.; Kim H.; Liu M.; Jung W.; Joo S.; Park E.; Ahn Y.S.; Han I.H.; Lee J.I.; Cui X.","Jin, Cheng-Bin (56902110800); Kim, Hakil (56091877400); Liu, Mingjie (57197853883); Jung, Wonmo (55574727900); Joo, Seongsu (57207765020); Park, Eunsik (57207760606); Ahn, Young Saem (57207777474); Han, In Ho (57226102226); Lee, Jae Il (55870655900); Cui, Xuenan (35301678900)","56902110800; 56091877400; 57197853883; 55574727900; 57207765020; 57207760606; 57207777474; 57226102226; 55870655900; 35301678900","Deep CT to MR synthesis using paired and unpaired data","2019","Sensors (Switzerland)","19","10","2361","","","","10.3390/s19102361","81","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066831634&doi=10.3390%2fs19102361&partnerID=40&md5=db942956e5cce8f43d9cc715b6451d5b","Magnetic resonance (MR) imaging plays a highly important role in radiotherapy treatment planning for the segmentation of tumor volumes and organs. However, the use of MR is limited, owing to its high cost and the increased use of metal implants for patients. This study is aimed towards patients who are contraindicated owing to claustrophobia and cardiac pacemakers, and many scenarios in which only computed tomography (CT) images are available, such as emergencies, situations lacking an MR scanner, and situations in which the cost of obtaining an MR scan is prohibitive. From medical practice, our approach can be adopted as a screening method by radiologists to observe abnormal anatomical lesions in certain diseases that are difficult to diagnose by CT. The proposed approach can estimate an MR image based on a CT image using paired and unpaired training data. In contrast to existing synthetic methods for medical imaging, which depend on sparse pairwise-aligned data or plentiful unpaired data, the proposed approach alleviates the rigid registration of paired training, and overcomes the context-misalignment problem of unpaired training. A generative adversarial network was trained to transform two-dimensional (2D) brain CT image slices into 2D brain MR image slices, combining the adversarial, dual cycle-consistent, and voxel-wise losses. Qualitative and quantitative comparisons against independent paired and unpaired training methods demonstrated the superiority of our approach. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Diagnosis; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Pacemakers; Radiotherapy; Adversarial networks; Cardiac pacemakers; MR images; Quantitative comparison; Radiotherapy treatment planning; Rigid registration; Synthetic methods; Two Dimensional (2 D); Computerized tomography","CT-based radiotherapy; Dual cycle-consistent loss; Generative adversarial networks; MR image synthesis; Paired and unpaired training","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85066831634"
"Zhou Y.; He X.; Cui S.; Zhu F.; Liu L.; Shao L.","Zhou, Yi (57195420298); He, Xiaodong (37085932700); Cui, Shanshan (57211998025); Zhu, Fan (58062706600); Liu, Li (56102788900); Shao, Ling (55643855000)","57195420298; 37085932700; 57211998025; 58062706600; 56102788900; 55643855000","High-resolution diabetic retinopathy image synthesis manipulated by grading and lesions","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11764 LNCS","","","505","513","8","10.1007/978-3-030-32239-7_56","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075637009&doi=10.1007%2f978-3-030-32239-7_56&partnerID=40&md5=3f51f6595502c34ee2d0e6be7f3c64e6","Diabetic retinopathy (DR) is a complication of diabetes that severely affects eyes, and can be graded into five levels according to international protocol. However, optimizing a grading model with strong generalization ability requires large balanced training data, which is difficult to collect in general but particularly for the high severity levels. Typical data augmentation methods, including flip and rotation cannot generate data with high diversity. In this paper, we propose a diabetic retinopathy generative adversarial network (DR-GAN) to synthesize high-resolution fundus images, which can be manipulated with arbitrary grading and lesion information. Thus, large-scale generated data can be used for more meaningful augmentation to train a DR grading model. The proposed retina generator is conditioned on vessel and lesion masks, and adaptive grading vectors sampled from the latent grading space, which can be adopted to control the synthesized grading severity. Moreover, multi-scale discriminators are designed to operate from large to small receptive fields, and joint adversarial losses are adopted to optimize the whole network in an end-to-end manner. With extensive experiments evaluated on the EyePACS dataset connected to Kaggle, we validate the effectiveness of our method, which can both synthesize highly realistic (1280 × 1280) controllable fundus images and contribute to the DR grading task. © 2019, Springer Nature Switzerland AG.","Eye protection; Medical computing; Medical imaging; Vector spaces; Adversarial networks; Data augmentation; Diabetic retinopathy; Generalization ability; High resolution; Image synthesis; Receptive fields; Training data; Grading","","Conference paper","Final","","Scopus","2-s2.0-85075637009"
"Zhang Z.; Zhang Y.; Yu W.; He G.; Jiang N.; He G.; Fan Y.; Yang Z.","Zhang, Zhiqiang (57206280843); Zhang, Yunye (57211359203); Yu, Wenxin (36610960300); He, Gang (56937631400); Jiang, Ning (57212426361); He, Gang (36630339700); Fan, Yibo (23466795500); Yang, Zhuo (57203791621)","57206280843; 57211359203; 36610960300; 56937631400; 57212426361; 36630339700; 23466795500; 57203791621","Text to Image Synthesis Using Two-Stage Generation and Two-Stage Discrimination","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11776 LNAI","","","110","114","4","10.1007/978-3-030-29563-9_12","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077120227&doi=10.1007%2f978-3-030-29563-9_12&partnerID=40&md5=c5da1ffd2367c7ef4b86d05132d365ad","In this paper, the method of two-stage generation and two-stage discrimination (2G2D) is proposed to generate high-resolution and more realistic images. It is a simple but effective way to synthesize images based on text descriptions. Our method generates the refined foreground image in the first stage, and then combines the text description to generate the final high-resolution image in second stage. We demonstrate the performance of the proposed method on the Caltech-UCSD Birds (CUB) dataset. Through the experimental results, our model can improve the resolution and the authenticity of content of the synthetic image better than the existing state-of-the-art methods. © 2019, Springer Nature Switzerland AG.","Computer vision; Deep learning; Adversarial networks; Foreground images; High resolution; High resolution image; Image synthesis; Realistic images; State-of-the-art methods; Synthetic images; Image enhancement","Computer vision; Deep learning; Generative Adversarial Networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85077120227"
"Spick R.; Walker J.A.","Spick, Ryan (57204808077); Walker, James Alfred (57214383954)","57204808077; 57214383954","Realistic and textured terrain generation using GANs","2019","Proceedings - CVMP 2019: 16th ACM SIGGRAPH European Conference on Visual Media Production","","","a3","","","","10.1145/3359998.3369407","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077331936&doi=10.1145%2f3359998.3369407&partnerID=40&md5=f6352e86cbbf7d72ef6f3a2dd23c54bf","In computer graphics and virtual environment development, a large portion of time is spent creating assets - one of these being the terrain environment, which usually forms the basis of many large graphical worlds. The texturing of height maps is usually performed as a post-processing step - with software requiring access to the height and gradient of the terrain in order to generate a set of conditions for colouring slopes, flats, mountains etc. With further additions such as biomes specifying which predominant texturing the region should exhibit such as grass, snow, dirt etc. much like the real-world. These methods combined with a height map generation algorithm can create impressive terrain renders which look visually stunning - however can appear somewhat repetitive. Previous work has explored the use of variants of Generative Adversarial Networks for the learning of elevation data through real-world data sets of world height data. In this paper, a method is proposed for learning not only the height map values but also the corresponding satellite image of a specific region. This data is trained through a non-spatially dependant generative adversarial network, which can produce an endless amount of variants of a specific region. The textured outputs are measured using existing similarity metrics and compared to the original region, which yields strong results. Additionally, a visual and statistical comparison of other deep learning image synthesis techniques is performed. The network outputs are also rendered in a 3D graphics engine and visualised in the paper. This method produces powerful outputs when compared directly with the training region, creating a tool that can produce many different variants of the target terrain. This is ideally suited for the use of a developer wanting a large number of specific structures of terrain. © 2019 Copyright held by the owner/author(s).","Deep learning; Interactive computer graphics; Rendering (computer graphics); Textures; Virtual reality; Adversarial networks; Image synthesis; Post processing; Procedural content generations; Satellite images; Similarity metrics; Statistical comparisons; Terrain generations; Landforms","Deep Learning; Generative Adversarial Networks; Procedural Content Generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077331936"
"Ali M.B.; Gu I.Y.-H.; Jakola A.S.","Ali, Muhaddisa Barat (57191335051); Gu, Irene Yu-Hua (36724563600); Jakola, Asgeir Store (16312689600)","57191335051; 36724563600; 16312689600","Multi-stream Convolutional Autoencoder and 2D Generative Adversarial Network for Glioma Classification","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11678 LNCS","","","234","245","11","10.1007/978-3-030-29888-3_19","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072867074&doi=10.1007%2f978-3-030-29888-3_19&partnerID=40&md5=2fbd94f23a29c36e915cc55514599934","Diagnosis and timely treatment play an important role in preventing brain tumor growth. Deep learning methods have gained much attention lately. Obtaining a large amount of annotated medical data remains a challenging issue. Furthermore, high dimensional features of brain images could lead to over-fitting. In this paper, we address the above issues. Firstly, we propose an architecture for Generative Adversarial Networks to generate good quality synthetic 2D MRIs from multi-modality MRIs (T1 contrast-enhanced, T2, FLAIR). Secondly, we propose a deep learning scheme based on 3-streams of Convolutional Autoencoders (CAEs) followed by sensor information fusion. The rational behind using CAEs is that it may improve glioma classification performance (as comparing with conventional CNNs), since CAEs offer noise robustness and also efficient feature reduction hence possibly reduce the over-fitting. A two-round training strategy is also applied by pre-training on GAN augmented synthetic MRIs followed by refined-training on original MRIs. Experiments on BraTS 2017 dataset have demonstrated the effectiveness of the proposed scheme (test accuracy 92.04%). Comparison with several exiting schemes has provided further support to the proposed scheme. © 2019, Springer Nature Switzerland AG.","Brain; Brain mapping; Classification (of information); Convolution; Diagnosis; Grading; Image analysis; Information fusion; Statistical tests; Tumors; Adversarial networks; Autoencoders; Brain tumor classifications; Classification performance; Feature reduction; High dimensional feature; Image synthesis; Sensor information fusions; Deep learning","Brain tumor classification; Deep learning; Generative Adversarial Networks; Glioma grading; Image synthesis; Information fusion; Multi-stream Convolutional Autoencoders","Conference paper","Final","","Scopus","2-s2.0-85072867074"
"Bazrafkan S.; Javidnia H.; Corcoran P.","Bazrafkan, Shabab (56403389300); Javidnia, Hossein (56113112200); Corcoran, Peter (57190839462)","56403389300; 56113112200; 57190839462","Latent space mapping for generation of object elements with corresponding data annotation","2018","Pattern Recognition Letters","116","","","179","186","7","10.1016/j.patrec.2018.10.025","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055342404&doi=10.1016%2fj.patrec.2018.10.025&partnerID=40&md5=58d7775ac7cb86d1a9b7025ccf9cc68a","Deep neural generative models such as Variational Auto-Encoders (VAE) and Generative Adversarial Networks (GAN) give promising results in estimating the data distribution across a range of machine learning fields of application. Recent results have been especially impressive in image synthesis where learning the spatial appearance information is a key goal. This enables the generation of intermediate spatial data that corresponds to the original dataset. In the training stage, these models learn to decrease the distance of their output distribution to the actual data and, in the test phase, they map a latent space to the data space. Since these models have already learned their latent space mapping, one question is whether there is a function mapping the latent space to any aspect of the database for the given generator. In this work, it has been shown that this mapping is relatively straightforward using small neural network models and by minimizing the mean square error. As a demonstration of this technique, two example use cases have been implemented: firstly, the idea to generate facial images with corresponding landmark data and secondly, generation of low-quality iris images (as would be captured with a smartphone user-facing camera) with a corresponding ground-truth segmentation contour. © 2018","Deep neural networks; Image segmentation; Mean square error; Adversarial networks; Data distribution; Function mapping; Generative model; Minimizing the mean square errors; Neural network model; Output distribution; Space mappings; Mapping","Deep neural networks; Generative models; Latent space mapping","Article","Final","","Scopus","2-s2.0-85055342404"
"Wang L.; Chen W.; Yang W.; Bi F.; Yu F.R.","Wang, Lei (57196333972); Chen, Wei (57171121800); Yang, Wenjia (57216500474); Bi, Fangming (23479256400); Yu, Fei Richard (57213980384)","57196333972; 57171121800; 57216500474; 23479256400; 57213980384","A State-of-the-Art Review on Image Synthesis with Generative Adversarial Networks","2020","IEEE Access","8","","9043519","63514","63537","23","10.1109/ACCESS.2020.2982224","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083740108&doi=10.1109%2fACCESS.2020.2982224&partnerID=40&md5=0dc500e541f68a8d9a8ddefa9556e84f","Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios. © 2013 IEEE.","Semantics; Three dimensional computer graphics; 3D face reconstruction; Adversarial networks; Future research directions; Image generations; Image super resolutions; Image translation; Recent researches; State-of-the art reviews; Image processing","cartoon generation; Generative adversarial networks; image editing; image synthesis; image-to-image translation","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083740108"
"Sun Y.; Tang J.; Sun Z.; Tistarelli M.","Sun, Yunlian (55866644500); Tang, Jinhui (56364850900); Sun, Zhenan (8081773300); Tistarelli, Massimo (7003853982)","55866644500; 56364850900; 8081773300; 7003853982","Facial Age and Expression Synthesis Using Ordinal Ranking Adversarial Networks","2020","IEEE Transactions on Information Forensics and Security","15","","9036898","2960","2972","12","10.1109/TIFS.2020.2980792","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081558599&doi=10.1109%2fTIFS.2020.2980792&partnerID=40&md5=6df0d0173431c31bbdd2e6cd011e92d0","Facial image synthesis has been extensively studied, for a long time, in both computer graphics and computer vision. Particularly, the synthesis of face images with varying ages, expressions and poses has received an increasing attention owing to several real-world applications. In this paper, facial age and expression synthesis are addressed. While previous and current research papers on facial age synthesis mostly adopt an age span of 10 years, this paper investigates face aging with a shorter time span. For expression synthesis, given a neutral face, we work on synthesizing faces with varying expression intensities (e.g., from zero to high). Note that both human ages and expression intensities are inherently ordinal. To fully exploit this ordinal nature, we devise ordinal ranking generative adversarial networks (ranking GAN). For each face, a one-hot label is assigned to define its age range/expression intensity. By exploiting the relative order information among age ranges/expression intensities, a binary ranking vector is further computed for each face. In ranking GAN, one-hot labels are used as the condition of the generator for synthesizing faces with target age groups/expression intensities. Moreover, we add a sequence of cost-sensitive ordinal rankers on top of several multi-scale discriminators, with the aim of minimizing age/intensity rank estimation loss when optimizing both the generator and discriminators. In order to evaluate the proposed ranking GAN, extensive experiments are carried out on several public face databases. As demonstrated by the experimental testing, this ranking scheme performs well even when the amount of available labeled training data is limited. The reported experimental results well demonstrate the effectiveness of ranking GAN on synthesizing face aging sequences and faces with varying expression intensities. © 2020 IEEE.","Computer graphics; Labeled data; Well testing; Adversarial networks; Expression intensities; Face images; Facial expression synthesis; Facial Image synthesis; Labeled training data; Ordinal ranking; Synthesis of face images; Image processing","Face image aging; facial expression synthesis; generative adversarial networks; ordinal ranking","Article","Final","","Scopus","2-s2.0-85081558599"
"Dar S.U.H.; Yurt M.; Karacan L.; Erdem A.; Erdem E.; Cukur T.","Dar, Salman U.H. (57195220338); Yurt, Mahmut (57211187325); Karacan, Levent (55931892300); Erdem, Aykut (13410510300); Erdem, Erkut (13410837300); Cukur, Tolga (23034054800)","57195220338; 57211187325; 55931892300; 13410510300; 13410837300; 23034054800","Image Synthesis in Multi-Contrast MRI with Conditional Generative Adversarial Networks","2019","IEEE Transactions on Medical Imaging","38","10","8653423","2375","2388","13","10.1109/TMI.2019.2901750","227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068854811&doi=10.1109%2fTMI.2019.2901750&partnerID=40&md5=3dd831858e7d4e93d1bd16d76d29ef98","Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1- A nd T2-weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations. © 2019 IEEE.","Brain; Brain Neoplasms; Glioma; Humans; Image Interpretation, Computer-Assisted; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Diagnosis; Magnetic resonance imaging; Mathematical transformations; Pixels; Adversarial networks; Healthy subjects; High frequency HF; Image synthesis; Non-linear regression; State-of-the-art methods; Structural details; Synthesized images; adult; article; body weight; case report; clinical article; diagnostic value; female; human; male; nuclear magnetic resonance imaging; synthesis; brain; brain tumor; computer assisted diagnosis; diagnostic imaging; glioma; image processing; nuclear magnetic resonance imaging; procedures; Image enhancement","cycleconsistency loss; Generative adversarial network; image synthesis; multi-contrast MRI; pixel-wise loss","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068854811"
"Regmi K.; Borji A.","Regmi, Krishna (57210319031); Borji, Ali (23395793600)","57210319031; 23395793600","Cross-view image synthesis using geometry-guided conditional GANs","2019","Computer Vision and Image Understanding","187","","102788","","","","10.1016/j.cviu.2019.07.008","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070368114&doi=10.1016%2fj.cviu.2019.07.008&partnerID=40&md5=b48c07f33401991a07339ba8669ddfe6","We address the problem of generating images across two drastically different views, namely ground (street) and aerial (overhead) views. Image synthesis by itself is a very challenging computer vision task and is even more so when generation is conditioned on an image in another view. Due to the difference in viewpoints, there is small overlapping field of view and little common content between these two views. Here, we try to preserve the pixel information between the views so that the generated image is a realistic representation of cross view input image. For this, we resort to homography as a guide to map the images between the views based on the common field of view to preserve the details in the input image. We then use generative adversarial networks to inpaint the missing regions in the transformed image and add realism to it. Our exhaustive evaluation and model comparison demonstrate that utilizing geometry constraints adds fine details to the generated images and can be a better approach for cross view image synthesis than purely pixel based synthesis methods. © 2019 Elsevier Inc.","Pixels; Synthesis (chemical); Adversarial networks; Cross-view; GANs; Geometry constraints; Homographies; Model comparison; Pixel information; Synthesis method; Antennas","Cross-view; GANs; Homography; Synthesis; View-translation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85070368114"
"Tran L.; Yin X.; Liu X.","Tran, Luan (57189323204); Yin, Xi (56288395400); Liu, Xiaoming (35793096800)","57189323204; 56288395400; 35793096800","Representation Learning by Rotating Your Faces","2019","IEEE Transactions on Pattern Analysis and Machine Intelligence","41","12","8453880","3007","3021","14","10.1109/TPAMI.2018.2868350","70","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052787764&doi=10.1109%2fTPAMI.2018.2868350&partnerID=40&md5=f8d73a191e7bfb7c6fcefb46ad94ca31","The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images. © 2018 IEEE.","Algorithms; Biometric Identification; Face; Female; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Pattern Recognition, Automated; Rotation; Decoding; Gallium nitride; Gas generators; Gesture recognition; III-V semiconductors; Image quality; Job analysis; Quality control; Adversarial networks; Face rotation; Image generations; Pose-invariant face recognition; representation learning; Task analysis; algorithm; anatomy and histology; automated pattern recognition; biometry; face; female; human; image processing; machine learning; male; procedures; rotation; Face recognition","face rotation and frontalization; generative adversarial network; pose-invariant face recognition; Representation learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85052787764"
"Wang M.; Yang G.-Y.; Li R.; Liang R.-Z.; Zhang S.-H.; Hall P.M.; Hu S.-M.","Wang, Miao (57189656696); Yang, Guo-Ye (57203137225); Li, Ruilong (57200286712); Liang, Run-Ze (57208258339); Zhang, Song-Hai (36968379500); Hall, Peter M. (56274791100); Hu, Shi-Min (34769829200)","57189656696; 57203137225; 57200286712; 57208258339; 36968379500; 56274791100; 34769829200","Example-guided style-consistent image synthesis from semantic labeling","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8953682","1495","1504","9","10.1109/CVPR.2019.00159","55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078345072&doi=10.1109%2fCVPR.2019.00159&partnerID=40&md5=08b8bd9481622a73482e3bec52c113f6","Example-guided image synthesis aims to synthesize an image from a semantic label map and an exemplary image indicating style. We use the term 'style' in this problem to refer to implicit characteristics of images, for example: In portraits 'style' includes gender, racial identity, age, hairstyle; in full body pictures it includes clothing; in street scenes it refers to weather and time of day and such like. A semantic label map in these cases indicates facial expression, full body pose, or scene segmentation. We propose a solution to the example-guided image synthesis problem using conditional generative adversarial networks with style consistency. Our key contributions are (i) a novel style consistency discriminator to determine whether a pair of images are consistent in style; (ii) an adaptive semantic consistency loss; and (iii) a training data sampling strategy, for synthesizing style-consistent results to the exemplar. We demonstrate the efficiency of our method on face, dance and street view synthesis tasks. © 2019 IEEE.","Computer vision; Deep learning; Adversarial networks; Facial Expressions; Scene segmentation; Semantic consistency; Semantic labeling; Semantic labels; Style consistency; Video synthesis; Semantics","Deep Learning; Image and Video Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078345072"
"","","","14th Asian Conference on Computer Vision, ACCV 2018","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11367 LNCS","","","","","4380","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068434299&partnerID=40&md5=baba8b8953d30a7450cabd5bcb90e067","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Paying Attention to Style: Recognizing Photo Styles with Convolutional Attentional Units; E2E-MLT - An Unconstrained End-to-End Method for Multi-language Scene Text; an Invoice Reading System Using a Graph Convolutional Network; reading Industrial Inspection Sheets by Inferring Visual Relations; Learning to Clean: A GAN Perspective; deep Reader: Information Extraction from Document Images via Relation Extraction and Natural Language; simultaneous Recognition of Horizontal and Vertical Text in Natural Images; Automatic Retinal and Choroidal Boundary Segmentation in OCT Images Using Patch-Based Supervised Machine Learning Methods; Discrimination Ability of Glaucoma via DCNNs Models from Ultra-Wide Angle Fundus Images Comparing Either Full or Confined to the Optic Disc; synthesizing New Retinal Symptom Images by Multiple Generative Models; localizing the Gaze Target of a Crowd of People; retinal Detachment Screening with Ensembles of Neural Network Models; Recent Developments of Retinal Image Analysis in Alzheimer’s Disease and Potential AI Applications; intermediate Goals in Deep Learning for Retinal Image Analysis; Enhanced Detection of Referable Diabetic Retinopathy via DCNNs and Transfer Learning; Generative Adversarial Networks (GANs) for Retinal Fundus Image Synthesis; AI-based AMD Analysis: A Review of Recent Progress; artificial Intelligence Using Deep Learning in Classifying Side of the Eyes and Width of Field for Retinal Fundus Photographs; OCT Segmentation via Deep Learning: A Review of Recent Work; auto-classification of Retinal Diseases in the Limit of Sparse Data Using a Two-Streams Machine Learning Model; unconstrained Iris Segmentation Using Convolutional Neural Networks.","","","Conference review","Final","","Scopus","2-s2.0-85068434299"
"Kaneko T.; Hiramatsu K.; Kashino K.","Kaneko, Takuhiro (57191894644); Hiramatsu, Kaoru (7202186220); Kashino, Kunio (6701924954)","57191894644; 7202186220; 6701924954","Generative Adversarial Image Synthesis with Decision Tree Latent Controller","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578789","6606","6615","9","10.1109/CVPR.2018.00691","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062844466&doi=10.1109%2fCVPR.2018.00691&partnerID=40&md5=021154e70aaa7b4afecbd565b9184cf2","This paper proposes the decision tree latent controller generative adversarial network (DTLC-GAN), an extension of a GAN that can learn hierarchically interpretable representations without relying on detailed supervision. To impose a hierarchical inclusion structure on latent variables, we incorporate a new architecture called the DTLC into the generator input. The DTLC has a multiple-layer tree structure in which the ON or OFF of the child node codes is controlled by the parent node codes. By using this architecture hierarchically, we can obtain the latent space in which the lower layer codes are selectively used depending on the higher layer ones. To make the latent codes capture salient semantic features of images in a hierarchically disentangled manner in the DTLC, we also propose a hierarchical conditional mutual information regularization and optimize it with a newly defined curriculum learning method that we propose as well. This makes it possible to discover hierarchically interpretable representations in a layer-by-layer manner on the basis of information gain by only using a single DTLC-GAN model. We evaluated the DTLC-GAN on various datasets, i.e., MNIST, CIFAR-10, Tiny ImageNet, 3D Faces, and CelebA, and confirmed that the DTLC-GAN can learn hierarchically interpretable representations with either unsupervised or weakly supervised settings. Furthermore, we applied the DTLC-GAN to image-retrieval tasks and showed its effectiveness in representation learning. © 2018 IEEE.","Codes (symbols); Computer vision; Data mining; Decision trees; Image coding; Network architecture; Semantics; Trees (mathematics); Adversarial networks; Conditional mutual information; Image synthesis; Inclusion structure; Information gain; Interpretable representation; Learning methods; Semantic features; Image retrieval","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062844466"
"","","","10th International Workshop on Machine Learning in Medical Imaging, MLMI 2019 held in conjunction with the 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11861 LNCS","","","","","689","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075650451&partnerID=40&md5=d417709758303c70ec1c63ee5dffee8b","The proceedings contain 78 papers. The special focus in this conference is on International Workshop on Machine Learning in Medical Imaging, held in conjunction with the International Conference on Medical Image Computing and Computer-Assisted Intervention. The topics include: GFD Faster R-CNN: Gabor Fractal DenseNet Faster R-CNN for Automatic Detection of Esophageal Abnormalities in Endoscopic Images; deep Active Lesion Segmentation; infant Brain Deformable Registration Using Global and Local Label-Driven Deep Regression Learning; a Relation Hashing Network Embedded with Prior Features for Skin Lesion Classification; end-to-End Adversarial Shape Learning for Abdomen Organ Deep Segmentation; privacy-Preserving Federated Brain Tumour Segmentation; residual Attention Generative Adversarial Networks for Nuclei Detection on Routine Colon Cancer Histology Images; semi-supervised Multi-task Learning with Chest X-Ray Images; Novel Bi-directional Images Synthesis Based on WGAN-GP with GMM-Based Noise Generation; spatial Regularized Classification Network for Spinal Dislocation Diagnosis; pseudo-labeled Bootstrapping and Multi-stage Transfer Learning for the Classification and Localization of Dysplasia in Barrett’s Esophagus; Anatomy-Aware Self-supervised Fetal MRI Synthesis from Unpaired Ultrasound Images; end-to-End Boundary Aware Networks for Medical Image Segmentation; Automatic Rodent Brain MRI Lesion Segmentation with Fully Convolutional Networks; Morphological Simplification of Brain MR Images by Deep Learning for Facilitating Deformable Registration; Joint Shape Representation and Classification for Detecting PDAC; FusionNet: Incorporating Shape and Texture for Abnormality Detection in 3D Abdominal CT Scans; ultrasound Liver Fibrosis Diagnosis Using Multi-indicator Guided Deep Neural Networks; weakly Supervised Segmentation by a Deep Geodesic Prior; correspondence-Steered Volumetric Descriptor Learning Using Deep Functional Maps; globally-Aware Multiple Instance Classifier for Breast Cancer Screening; confounder-Aware Visualization of ConvNets.","","","Conference review","Final","","Scopus","2-s2.0-85075650451"
"Akbir K.; Mahmoud M.","Akbir, Khan (57207841405); Mahmoud, Marwa (39861993700)","57207841405; 39861993700","Considering race a problem of transfer learning","2019","Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision Workshops, WACVW 2019","","","8638327","100","106","6","10.1109/WACVW.2019.00022","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063058135&doi=10.1109%2fWACVW.2019.00022&partnerID=40&md5=a243c6a8051d26a2627dee34e0d28302","As biometric applications are fielded to serve large population groups, issues of performance differences between individual sub-groups are becoming increasingly important. In this paper we examine cases where we believe race is one such factor. We look in particular at two forms of problem; facial classification and image synthesis. We take the novel approach of considering race as a boundary for transfer learning in both the task (facial classification) and the domain (synthesis over distinct datasets). We demonstrate a series of techniques to improve transfer learning of facial classification; outperforming similar models trained in the target's own domain. We conduct a study to evaluate the performance drop of Generative Adversarial Networks trained to conduct image synthesis, in this process, we produce a new annotation for the Celeb-A dataset by race. These networks are trained solely on one race and tested on another - demonstrating the subsets of the CelebA to be distinct domains for this task. © 2019 IEEE.","Computer vision; Adversarial networks; Biometric applications; Image synthesis; Large population; Similar models; Sub-groups; Transfer learning; Classification (of information)","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85063058135"
"Huang W.; Luo M.; Liu X.; Zhang P.; Ding H.; Ni D.","Huang, Wei (56195325600); Luo, Mingyuan (57206482771); Liu, Xi (57209845642); Zhang, Peng (55547108553); Ding, Huijun (25932070800); Ni, Dong (26023577500)","56195325600; 57206482771; 57209845642; 55547108553; 25932070800; 26023577500","Novel Bi-directional Images Synthesis Based on WGAN-GP with GMM-Based Noise Generation","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11861 LNCS","","","160","168","8","10.1007/978-3-030-32692-0_19","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075672345&doi=10.1007%2f978-3-030-32692-0_19&partnerID=40&md5=2a932560d2b99452f8fc82b58878a3d2","A novel WGAN-GP-based model is proposed in this study to fulfill bi-directional synthesis of medical images for the first time. GMM-based noise generated from the Glow model is newly incorporated into the WGAN-GP-based model to better reflect the characteristics of heterogeneity commonly seen in medical images, which is beneficial to produce high-quality synthesized medical images. Both the conventional “down-sampling”-like synthesis and the more challenging “up-sampling”-like synthesis are realized through the newly introduced model, which is thoroughly evaluated with comparisons towards several popular deep learning-based models both qualitatively and quantitatively. The superiority of the new model is substantiated based on a series of rigorous experiments using a multi-modal MRI database composed of 355 real demented patients in this study, from the statistical perspective. © 2019, Springer Nature Switzerland AG.","Computer aided instruction; Deep learning; Diagnosis; Machine learning; Signal sampling; Adversarial networks; Bi-directional; Bi-directional synthesis; Down sampling; High quality; Images synthesis; Learning Based Models; Noise generation; Medical imaging","Dementia diseases diagnosis; Generative adversarial network; Medical images synthesis","Conference paper","Final","","Scopus","2-s2.0-85075672345"
"Hosangadi R.; Adiga D.; Vyeth V.","Hosangadi, Raunak (57194612114); Adiga, Darshan (57212340740); Vyeth, Viveka (57212341467)","57194612114; 57212340740; 57212341467","OCR-friendly image synthesis using generative adversarial networks","2019","ACM International Conference Proceeding Series","","","","226","234","8","10.1145/3373509.3373580","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082718556&doi=10.1145%2f3373509.3373580&partnerID=40&md5=828121f6a0a3481f3dd02a09a014ab4b","With quality being a deciding factor in the accuracy of OCR predictions for a given source image, there comes the need of preprocessing methods to improve the quality of an image before undergoing this process. Towards that, we present a GAN-based method targeted at improving the quality of source image in the fields of image resolution, blur and noise. The model uses an encoder trained to generate latent image representations for a lowquality image, the representations corresponding to blur and noise types present in the image. These representations act as inputs to the constructed conditional GAN. Besides these labels, the generator takes a low-quality image as input and is trained to generate a high-quality image as represented by the target images. The discriminator makes use of a standard OCR score to measure the performance of the generator in order to achieve images that are more OCR-friendly than the given target image. The model uses perceptual losses for training to produce clear images and improve convergence. Improving of image resolution is made by the use of super-resolution by means of sub-pixel convolution. Finally, the paper describes a series of experiments run on the model, as well as the results to show improvements in OCR performance using a single mode; for images suffering a variety of distortions. The results are quantified based on multiple performance measures and the final analysis is presented. © 2019 Association for Computing Machinery.","Convolution; Image resolution; Optical character recognition; Pixels; Adversarial networks; Autoencoders; High quality images; Image synthesis; Multiple performance measures; Pre-processing method; Sub pixels; Super resolution; Image enhancement","Autoencoders; Generative adversarial networks; Optical character recognition; Sub-pixel convolution.","Conference paper","Final","","Scopus","2-s2.0-85082718556"
"Lai K.; Yanushkevich S.N.","Lai, Kenneth (56332896100); Yanushkevich, Svetlana N. (6701583388)","56332896100; 6701583388","Multi-metric evaluation of thermal-to-visual face recognition","2019","2019 8th International Conference on Emerging Security Technologies, EST 2019","","","8806202","","","","10.1109/EST.2019.8806202","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072045126&doi=10.1109%2fEST.2019.8806202&partnerID=40&md5=8019934b329220b7303255f4e7eca2f1","In this paper, we aim to address the problem of heterogeneous or cross-spectral face recognition using machine learning to synthesize visual spectrum face from infrared images. The synthesis of visual-band face images allows for more optimal extraction of facial features to be used for face identification and/or verification. We explore the ability to use Generative Adversarial Networks (GANs) for face image synthesis, and examine the performance of these images using pre-trained Convolutional Neural Networks (CNNs). The features extracted using CNNs are applied in face identification and verification. We explore the performance in terms of acceptance rate when using various similarity measures for face verification. © 2019 IEEE.","Infrared imaging; Neural networks; Adversarial networks; Convolutional neural network; Face identification; Face image synthesis; Face Verification; Metric evaluation; Similarity measure; Visual spectrum; Face recognition","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85072045126"
"Khan F.H.; de Silva A.; Yetukuri J.; Norouzi N.","Khan, Fahim Hasan (57210804729); de Silva, Akila (57210804917); Yetukuri, Jayanth (57210803341); Norouzi, Narges (57210786343)","57210804729; 57210804917; 57210803341; 57210786343","Sequential image synthesis for human activity video generation","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11663 LNCS","","","129","133","4","10.1007/978-3-030-27272-2_11","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071468185&doi=10.1007%2f978-3-030-27272-2_11&partnerID=40&md5=bf6cd4d0f508228c261e826e6bd0da46","In the field of computer graphics and multimedia, automatic synthesis of a new set of image sequences from another different set of image sequences for creating realistic video or animation of some human activity performed is a research challenge. Traditionally, creating such animation or similar visual media contents is done manually, which is a tedious task. Recent advancements in deep learning have made some promising progress for automating this type of media creation process. This work is motivated by the idea to synthesize a temporally coherent sequence of images (e.g., a video) of a person performing some activity by using a video or set of images of a different person performing a similar activity. To achieve that, our approach utilized the cycle-consistent adversarial network (CycleGAN). We present a new approach for learning to transfer a human activity from a source domain to a target domain without using any complicated pose detection or extraction method. Our objective in this work is to learn a mapping between two consecutive sequences of images from two domains representing two different activities and use that mapping to transfer the activity from one domain to another for synthesizing an entirely new consecutive sequence of images, which can be combined to make a video of new human activity. We also present and analyze some qualitative results generated by our method. © Springer Nature Switzerland AG 2019.","Animation; Computer graphics; Deep learning; Mapping; Adversarial networks; Automatic synthesis; Graphics and multimedias; Image synthesis; Media creation process; Research challenges; Sequence of images; Sequential images; Image analysis","Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85071468185"
"","","","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","","","9519","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062861149&partnerID=40&md5=e44fe934b37bf888619f0c158311b8d7","The proceedings contain 979 papers. The topics discussed include: embodied question answering; learning by asking questions; finding tiny faces in the wild with generative adversarial network; paired CycleGAN: asymmetric style transfer for applying and removing makeup; learning pose specific representations by predicting different views; weakly and semi supervised human body part parsing via pose-guided knowledge transfer; person transfer GAN to bridge domain gap for person re-identification; cross-modal deep variational hand pose estimation; disentangled person image generation; super-fan: integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs; multistage adversarial losses for pose-based human image synthesis; and a certifiably globally optimal solution to the non-minimal relative pose problem.","","","Conference review","Final","","Scopus","2-s2.0-85062861149"
"Zhu K.; Liu X.; Yang H.","Zhu, Kongtao (35281018800); Liu, Xiwei (55193285400); Yang, Hongxue (57207845255)","35281018800; 55193285400; 57207845255","A Survey of Generative Adversarial Networks","2019","Proceedings 2018 Chinese Automation Congress, CAC 2018","","","8623645","2768","2773","5","10.1109/CAC.2018.8623645","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062785032&doi=10.1109%2fCAC.2018.8623645&partnerID=40&md5=74800cb342d92191b731ee8d1ce237cd","Generative adversarial networks(GANs) coming from the game theory allow machines to learn deep representations without extra training data. By training two adversarial networks, including a generator and a discriminator, GANs could get the distribution of the real samples. This capability makes it a prospect learning method in image synthesis, image recognition, image translation etc. In this paper, we survey the state of the art of GANs by categorizing the GANs into four classifications on the basis of GANs' functions and list two application domains: vision computing natural language processing(NLP) regarding to GANs' applications. © 2018 IEEE.","Computation theory; Game theory; Image recognition; List processing languages; Surveys; Adversarial networks; Image synthesis; Image translation; Learning methods; NAtural language processing; State of the art; Training data; Vision computing; Natural language processing systems","generative adversarial networks; natural language processing(NLP); vision computing","Conference paper","Final","","Scopus","2-s2.0-85062785032"
"Morooka K.; Zhang X.; Miyauchi S.; Kurazume R.; Ohno E.","Morooka, Ken’ich (7004183686); Zhang, Xueru (57215436653); Miyauchi, Shoko (56027207200); Kurazume, Ryo (6602093068); Ohno, Eiji (57215409350)","7004183686; 57215436653; 56027207200; 6602093068; 57215409350","GAN-Based Method for Synthesizing Multi-focus Cell Images","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11994 LNCS","","","100","107","7","10.1007/978-3-030-39770-8_8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080961258&doi=10.1007%2f978-3-030-39770-8_8&partnerID=40&md5=95e2893a46a8cba29558cef2b139006d","This paper presents a method for synthesizing multi-focus cell images by using generative adversarial networks (GANs). The proposed method, called multi-focus image GAN (MI-GAN), consists of two generators. A base image generator synthesizes a 2D base cell image from random noise. Using the generated base image, a multi-focus cell image generator produces 11 realistic multi-focus images of the cell while considering the relationships between the images acquired at successive focus points. From experimental results, MI-GAN achieves the good performance to generate realistic multi-focus cell images. © Springer Nature Switzerland AG 2020.","Cells; Cytology; Adversarial networks; Base images; Focus points; Image synthesis; Multi-focus; Multifocus images; Pathological images; Random noise; Image processing","GAN; Image synthesis; Multi-focus pathological images","Conference paper","Final","","Scopus","2-s2.0-85080961258"
"Liu X.; Shao J.; Yin G.; Wang X.; Li H.","Liu, Xihui (57200621033); Shao, Jing (57200616653); Yin, Guojun (57204286291); Wang, Xiaogang (55736875200); Li, Hongsheng (57141098300)","57200621033; 57200616653; 57204286291; 55736875200; 57141098300","Learning to predict layout-to-image conditional convolutions for semantic image synthesis","2019","Advances in Neural Information Processing Systems","32","","","","","","","67","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088122806&partnerID=40&md5=51e7c29ff4ffde5d991518641a1326d8","Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach. © 2019 Neural information processing systems foundation. All rights reserved.","Convolution; Noise pollution; Semantics; Adversarial networks; Affine transformations; Convolutional kernel; Photorealistic images; Quantitative metrics; Semantic segmentation; State-of-the-art performance; Subjective evaluations; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85088122806"
"Wang C.; Papanastasiou G.; Tsaftaris S.; Yang G.; Gray C.; Newby D.; Macnaught G.; MacGillivray T.","Wang, Chengjia (57196394674); Papanastasiou, Giorgos (56539707000); Tsaftaris, Sotirios (6505952824); Yang, Guang (57216243504); Gray, Calum (55224398700); Newby, David (7006580760); Macnaught, Gillian (55241784200); MacGillivray, Tom (57207503523)","57196394674; 56539707000; 6505952824; 57216243504; 55224398700; 7006580760; 55241784200; 57207503523","TPSDicyc: Improved Deformation Invariant Cross-domain Medical Image Synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11905 LNCS","","","245","254","9","10.1007/978-3-030-33843-5_23","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076205008&doi=10.1007%2f978-3-030-33843-5_23&partnerID=40&md5=e5d4232dcd33996e9fd112b5e2102bf2","Cycle-consistent generative adversarial network (CycleGAN) has been widely used for cross-domain medical image systhesis tasks particularly due to its ability to deal with unpaired data. However, most CycleGAN-based synthesis methods can not achieve good alignment between the synthesized images and data from the source domain, even with additional image alignment losses. This is because the CycleGAN generator network can encode the relative deformations and noises associated to different domains. This can be detrimental for the downstream applications that rely on the synthesized images, such as generating pseudo-CT for PET-MR attenuation correction. In this paper, we present a deformation invariant model based on the deformation-invariant CycleGAN (DicycleGAN) architecture and the spatial transformation network (STN) using thin-plate-spline (TPS). The proposed method can be trained with unpaired and unaligned data, and generate synthesised images aligned with the source data. Robustness to the presence of relative deformations between data from the source and target domain has been evaluated through experiments on multi-sequence brain MR data and multi-modality abdominal CT and MR data. Experiment results demonstrated that our method can achieve better alignment between the source and target data while maintaining superior image quality of signal compared to several state-of-the-art CycleGAN-based methods. © Springer Nature Switzerland AG 2019.","Alignment; Computer aided instruction; Deformation; Image enhancement; Image reconstruction; Machine learning; Medical image processing; Adversarial networks; Attenuation correction; Different domains; Downstream applications; Relative deformation; Spatial transformation; Synthesized images; Thin-plate spline; Computerized tomography","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076205008"
"Oeldorf C.; Spanakis G.","Oeldorf, Cedric (57215411368); Spanakis, Gerasimos (35749092700)","57215411368; 35749092700","LoGANv2: Conditional style-based logo generation with generative adversarial networks","2019","Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019","","","8999346","462","468","6","10.1109/ICMLA.2019.00086","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080910546&doi=10.1109%2fICMLA.2019.00086&partnerID=40&md5=a2966d35a78096b825e0c918bac89ba7","Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored, where the challenge lies in the fact that, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output. © 2019 IEEE.","Deep learning; Deep neural networks; Learning systems; Adversarial networks; Feature separation; Higher-dimensional problems; Image synthesis; Low resolution; Multi modality; Network extensions; Recent researches; Network architecture","Deep Learning; Generative Adversarial Neural Networks; Image Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85080910546"
"Guan S.; Loew M.","Guan, Shuyue (54904821700); Loew, Murray (7004977045)","54904821700; 7004977045","Breast cancer detection using synthetic mammograms from generative adversarial networks in convolutional neural networks","2019","Journal of Medical Imaging","6","3","031411","","","","10.1117/1.JMI.6.3.031411","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064149562&doi=10.1117%2f1.JMI.6.3.031411&partnerID=40&md5=ce579ba6a798057f52d7a981cfa25808","The convolutional neural network (CNN) is a promising technique to detect breast cancer based on mammograms. Training the CNN from scratch, however, requires a large amount of labeled data. Such a requirement usually is infeasible for some kinds of medical image data such as mammographic tumor images. Because improvement of the performance of a CNN classifier requires more training data, the creation of new training images, image augmentation, is one solution to this problem. We applied the generative adversarial network (GAN) to generate synthetic mammographic images from the digital database for screening mammography (DDSM). From the DDSM, we cropped two sets of regions of interest (ROIs) from the images: normal and abnormal (cancer/tumor). Those ROIs were used to train the GAN, and the GAN then generated synthetic images. For comparison with the affine transformation augmentation methods, such as rotation, shifting, scaling, etc., we used six groups of ROIs [three simple groups: affine augmented, GAN synthetic, real (original), and three mixture groups of any two of the three simple groups] for each to train a CNN classifier from scratch. And, we used real ROIs that were not used in training to validate classification outcomes. Our results show that, to classify the normal ROIs and abnormal ROIs from DDSM, adding GAN-generated ROIs in the training data can help the classifier prevent overfitting, and on validation accuracy, the GAN performs about 3.6% better than affine transformations for image augmentation. Therefore, GAN could be an ideal augmentation approach. The images augmented by GAN or affine transformation cannot substitute for real images to train CNN classifiers because the absence of real images in the training set will cause over-fitting. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Computer aided diagnosis; Computer aided instruction; Convolution; Deep learning; Diseases; Image enhancement; Mammography; Medical imaging; Neural networks; X ray screens; Adversarial networks; Breast mass; Convolutional neural network; image augmentation; Image synthesis; mammogram; Article; artificial neural network; breast cancer; cancer diagnosis; classification; classifier; convolutional neural network; data base; digital imaging; mammography; measurement accuracy; rotation; Classification (of information)","breast mass classification; computer-aided diagnosis; convolutional neural networks; deep learning; generative adversarial networks; image augmentation; image synthesis; mammogram","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85064149562"
"Wang T.-C.; Liu M.-Y.; Zhu J.-Y.; Tao A.; Kautz J.; Catanzaro B.","Wang, Ting-Chun (56612466200); Liu, Ming-Yu (22835742800); Zhu, Jun-Yan (56316642900); Tao, Andrew (57204284349); Kautz, Jan (7006458237); Catanzaro, Bryan (56000048900)","56612466200; 22835742800; 56316642900; 57204284349; 7006458237; 56000048900","High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8579015","8798","8807","9","10.1109/CVPR.2018.00917","1943","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055134349&doi=10.1109%2fCVPR.2018.00917&partnerID=40&md5=96d1f3a25617f38ac4f1b13a3c93d719","We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 Ã - 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing. © 2018 IEEE.","Semantics; Adversarial networks; High resolution image; High-resolution photos; Object appearance; Object categories; Object manipulation; Segmentation informations; Semantic labels; Computer vision","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055134349"
"Regmi K.; Borji A.","Regmi, Krishna (57210319031); Borji, Ali (23395793600)","57210319031; 23395793600","Cross-View Image Synthesis Using Conditional GANs","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578467","3501","3510","9","10.1109/CVPR.2018.00369","95","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061808022&doi=10.1109%2fCVPR.2018.00369&partnerID=40&md5=3a739216237c72d6cd53aebf704734b6","Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64Ã - 64 and 256Ã - 256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views. © 2018 IEEE.","Antennas; Computer vision; Network architecture; Semantics; Adversarial networks; Image synthesis; Image translation; Proposed architectures; Quantitative evaluation; Semantic information; Semantic segmentation; Visual appearance; Image segmentation","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85061808022"
"Zeng J.; Ma X.; Zhou K.","Zeng, Jiangfeng (57192074761); Ma, Xiao (57199215120); Zhou, Ke (7202915095)","57192074761; 57199215120; 7202915095","Photo-realistic face age progression/regression using a single generative adversarial network","2019","Neurocomputing","366","","","295","304","9","10.1016/j.neucom.2019.07.085","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070370552&doi=10.1016%2fj.neucom.2019.07.085&partnerID=40&md5=219a2d8050dd14241a6e32699068b00b","Face age progression/regression is enjoying renewed interest due to the remarkable improvements in image synthesis achieved by the deep generative models (e.g. the Generative Adversarial Networks (GANs)) and its tremendous impact on a wide-range of practical applications like finding back missing individuals with photos of childhood, entertainment, etc. Most existing approaches are focusing on face age progression and have proven to be successful and effective in learning the transformation between age groups with the aid of paired samples, i.e., face images of the same person at different ages. Although some signs of aging are synthesized by these approaches, they heavily rely on the availability of paired samples which are difficult and costly to collect. Inspired by the significant success achieved by using GANs in unsupervised image transduction, in this paper, we formulate this task as an unsupervised multi-domain image-to-image translation problem, and devise a novel generative framework using only a single generative adversarial network, dubbed FaceGAN which is capable of synthesizing photo-realistic face images with aging effects without paired samples and achieves face age progression and regression in a holistic framework. Experimental results show the superiority of our proposed method in terms of visual fidelity. We further empirically demonstrate the broad application capability of our approach on a facial attribute transfer and a facial expression synthesis tasks. © 2019","Computer applications; Neural networks; Adversarial networks; Age progression; Broad application; Facial expression synthesis; Generative model; Holistic frameworks; Image synthesis; Image translation; aging; article; facial expression; human; human experiment; synthesis; Image enhancement","Age progression/regression; Generative adversarial networks; Image-to-image translation","Article","Final","","Scopus","2-s2.0-85070370552"
"Zeng G.; Li Z.; Zhang Y.","Zeng, Gangyan (57215816277); Li, Zhaohui (57188984524); Zhang, Yuan (7601312884)","57215816277; 57188984524; 7601312884","PororoGAN: An improved story visualization model on pororo-SV dataset","2019","ACM International Conference Proceeding Series","","","","155","159","4","10.1145/3374587.3374649","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081953171&doi=10.1145%2f3374587.3374649&partnerID=40&md5=aeee4147bbddcaf71ddeedd54f5f1602","Generating a sequence of images from a multi-sentence paragraph is a recently proposed task called Story-Visualization. In this task, how to keep the global consistency across dynamic scenes and characters in the story flow is the distinct difference from other single-image works, which is also a significant challenge. However, the visual quality and semantic relevance of existing results are not satisfying when handling datasets with high semantic complexity, such as Pororo-SV cartoon dataset. To address this issue, we propose a new story visualization model named PororoGAN, which jointly considers story-to-imagesequence, sentence-to-image and word-to-image-patch alignment. In particular, we introduce ASE (aligned sentence encoder) and AWE (attentional word encoder) to improve global and local relevance, respectively. Additionally, we add an image patches discriminator to improve the reality of results. Both quantitative and qualitative studies show that PororoGAN outperforms the state-of-the-art models. © 2019 Association for Computing Machinery.","Artificial intelligence; Multimedia systems; Semantics; Signal encoding; Visualization; Adversarial networks; Attention; Global consistency; Image synthesis; Qualitative study; Semantic relevance; Sequence of images; Visualization modeling; Image enhancement","Attention; Generative adversarial network; Story visualization; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85081953171"
"Zeng Z.; Yu Y.K.; Hong Wong K.","Zeng, Zhiliang (57194850727); Yu, Ying Kin (25928883300); Hong Wong, Kin (57207909451)","57194850727; 25928883300; 57207909451","Adversarial network for edge detection","2019","2018 Joint 7th International Conference on Informatics, Electronics and Vision and 2nd International Conference on Imaging, Vision and Pattern Recognition, ICIEV-IVPR 2018","","","8641005","19","23","4","10.1109/ICIEV.2018.8641005","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063229212&doi=10.1109%2fICIEV.2018.8641005&partnerID=40&md5=e06b064d6406b5f0be6ade865d036ff1","Edge detection is a fundamental problem in computer vision and has been explored for many decades. Due to the rapid development of machine learning techniques and their applications to image processing, there is a proliferation of neural network-based approaches to solve the edge detection problem. These methods, such as richer convolutional features for edge detection(RCF), have good performance and even outperform human beings. Most of the existing neural network-based systems use the convolutional network or its variant. They usually produce thick edges and the application of non-maximum-suppression to suppress the edge is necessary. In this paper, we explore another type of neural network called the conditional generative adversarial network (cGAN) to address the edge detection problem. cGAN is an innovative framework to do the image synthesis task. It can generate an image close to the real one. After training, our network can produce an edge map that contains more detailed information and thinner edges compared to the state-of-the-art methods that require the well-known non-maximum-suppression for post-processing. The proposed approach is able to produce a high quality edge map directly without further processing. Our solution is computation efficient. It can achieve a speed of 59 and 26 frames per second (fps) for an image resolution of [256×256×3] and [512×512×3], respectively. © 2018 IEEE.","Convolution; Edge detection; Image resolution; Learning systems; Adversarial networks; Convolutional networks; Frames per seconds; Machine learning techniques; Network based systems; Network-based approach; Non-maximum suppression; State-of-the-art methods; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85063229212"
"Xiao J.-S.; Shen M.-Y.; Lei J.-F.; Xiong W.-X.; Jiao C.-K.","Xiao, Jin-Sheng (8526666500); Shen, Meng-Yao (57214448035); Lei, Jun-Feng (24537493100); Xiong, Wen-Xin (57214590378); Jiao, Chen-Kun (57215896166)","8526666500; 57214448035; 24537493100; 57214590378; 57215896166","Image Conversion Algorithm for Haze Scene Based on Generative Adversarial Networks; [基于生成对抗网络的雾霾场景图像转换算法]","2020","Jisuanji Xuebao/Chinese Journal of Computers","43","1","","165","176","11","10.11897/SP.J.1016.2020.00165","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082307621&doi=10.11897%2fSP.J.1016.2020.00165&partnerID=40&md5=a7eeaa1470bc6bedc8e38dc59fa0c7a1","In this paper, synthesis of hazy image based on generative adversarial net is described in detail. As an unsupervised learning method, the generative adversarial networks cannot learn the mapping between image pixels, which means the generation of images is uncontrollable. Therefore, because of the uncertainty of the parameters of the algorithms and the limitation of the application scenarios, a new application of the new method is proposed in this paper. The networks can learn the mapping between input and output image, and learn a loss function to train this mapping. The algorithm is based on GAN, and the improved generator and discriminator are proposed. And supervised learning is carried out to train the mapping between pixels in hazy images and haze-free images. Taking synthesis of hazy image as an example, we propose two networks: the one is generative network, which is used to generate hazy images, and the other one is discriminative network, which is used to identify the images. Here, considering the corresponding effect of image conversion, a hourglass-shaped fast link generator network is designed, which uses the fog-free image as the input. Specifically, the generated network is divided into two parts, encoding and decoding, and the underlying texture information of the image is preserved by adding corresponding convolution layers. Then, a funnel-shaped global convolutional judger network is designed to test the results of smog image synthesis. The composite image and the target image are respectively identified by the decider. The global convolutional neural network is used for multi-level downsampling to achieve classification and discerning image style. Meanwhile, the loss function of network is also modified. By calculating the sum of GAN loss and ABS loss, better results can be gotten. The loss function of GAN is to make the training of GAN model more accurate. However, the hazy image synthesis algorithm is actually a regression problem rather than a classification problem. And the generator is tasked not only to fool the discriminator but also to be near the ground truth Output. So in order to learn the mapping accurately and avoid deviation and distortion, the using of the ABS loss function is helpful. Finally, we analyze and contrast the experimental results carefully. This paper evaluates the transformation of different haze scenes, tests the effect of our hazing and dehazing algorithm, and compares it with other algorithms. For hazing effect, in synthetic scene and virtual scene, compared with the effect of software, the effect of our algorithm is obviously better than that of others, and there is no color distortion. In real scenes, the results of our algorithm are very similar to those of the real foggy and hazy scene. What's more, compared with other GAN image conversion algorithms, our algorithm has obvious advantages. Similarly, it can be seen that the effect of our haze removal task is also very obvious. Experiments demonstrate that the proposed algorithm has better performance than state-of-the-art methods on both synthetic and real-world images qualitatively and quantitatively. © 2020, Science Press. All right reserved.","Convolution; Deep learning; Demulsification; Image processing; Learning systems; Mapping; Pixels; Textures; Unsupervised learning; Adversarial networks; Discriminative networks; Encoding and decoding; Generative adversarial net; Haze scene; Image conversion; State-of-the-art methods; Unsupervised learning method; Convolutional neural networks","Deep learning; Generative adversarial net; Haze scene; Image conversion; Image processing","Article","Final","","Scopus","2-s2.0-85082307621"
"Gao Y.; Guo Y.; Lian Z.; Tang Y.; Xiao J.","Gao, Yue (57214681996); Guo, Yuan (57214783612); Lian, Zhouhui (23493219700); Tang, Yingmin (36609495000); Xiao, Jianguo (35520554800)","57214681996; 57214783612; 23493219700; 36609495000; 35520554800","Artistic glyph image synthesis via one-stage few-shot learning","2019","ACM Transactions on Graphics","38","6","185","","","","10.1145/3355089.3356574","45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078944798&doi=10.1145%2f3355089.3356574&partnerID=40&md5=2e175490918f5544f91fe773169dae35","Automatic generation of artistic glyph images is a challenging task that attracts many research interests. Previous methods either are specifically designed for shape synthesis or focus on texture transfer. In this paper, we propose a novel model, AGIS-Net, to transfer both shape and texture styles in one-stage with only a few stylized samples. To achieve this goal, we first disentangle the representations for content and style by using two encoders, ensuring the multi-content and multi-style generation. Then we utilize two collaboratively working decoders to generate the glyph shape image and its texture image simultaneously. In addition, we introduce a local texture refinement loss to further improve the quality of the synthesized textures. In this manner, our one-stage model is much more efficient and effective than other multi-stage stacked methods. We also propose a large-scale dataset with Chinese glyph images in various shape and texture styles, rendered from 35 professional-designed artistic fonts with 7,326 characters and 2,460 synthetic artistic fonts with 639 characters, to validate the effectiveness and extendability of our method. Extensive experiments on both English and Chinese artistic glyph image datasets demonstrate the superiority of our model in generating high-quality stylized glyph images against other state-of-the-art methods. © 2019 Association for Computing Machinery.","Deep learning; Large dataset; Textures; Adversarial networks; Automatic Generation; Font genration; Image translation; Large-scale dataset; State-of-the-art methods; Style transfer; Synthesized texture; Image texture","Deep learning; Font genration; Generative adversarial networks; Image-to-image translation; Style transfer","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078944798"
"Cheng Q.; Gu X.","Cheng, Qingrong (57193678501); Gu, Xiaodong (7403204205)","57193678501; 7403204205","Hybrid Attention Driven Text-to-Image Synthesis via Generative Adversarial Networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11731 LNCS","","","483","495","12","10.1007/978-3-030-30493-5_47","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072971868&doi=10.1007%2f978-3-030-30493-5_47&partnerID=40&md5=03ca1a5ac34b11fa3e3502f9d02e1712","With the development of generative models, image synthesis conditioned on the specific variable becomes an important research theme gradually. This paper presents a novel spectral normalization based Hybrid Attentional Generative Adversarial Networks (HAGAN) for text to image synthesis. The hybrid attentional mechanism is composed of text-image cross-modal attention and self-attention of image sub regions. Cross-modal attention mechanism contributes to synthesize more fine-grained and text-related image by introducing word-level semantic information in generative model. The self-attention solves the long distance reliance of image local-region features when generate image. With spectral normalization, the training of GANs become more stable than traditional GANs, which conduces to avoid model collapse and gradient vanishing or explosion. We conduct experiments on widely used Oxford-102 flower dataset and CUB bird dataset to validate our proposed method. During quantitative and non-quantitative experimental comparison, the results indicate that the proposed method achieves the best performance on Inception score (IS), Fréchet Inception Distance (FID) and visual effect. © Springer Nature Switzerland AG 2019.","Neural networks; Semantics; Adversarial networks; Cross-modal; Image synthesis; Self-attention; Spectral normalization; Image processing","Cross-modal attention; Generative Adversarial Networks; Self-attention; Spectral normalization; Text to image synthesis","Conference paper","Final","","Scopus","2-s2.0-85072971868"
"Rusak F.; Santa Cruz R.; Bourgeat P.; Fookes C.; Fripp J.; Bradley A.; Salvado O.","Rusak, Filip (57219314843); Santa Cruz, Rodrigo (57219319549); Bourgeat, Pierrick (8576247900); Fookes, Clinton (7003338437); Fripp, Jurgen (13605436500); Bradley, Andrew (7202846882); Salvado, Olivier (6508030900)","57219314843; 57219319549; 8576247900; 7003338437; 13605436500; 7202846882; 6508030900","3d brain mri gan-based synthesis conditioned on partial volume maps","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12417 LNCS","","","11","20","9","10.1007/978-3-030-59520-3_2","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092191503&doi=10.1007%2f978-3-030-59520-3_2&partnerID=40&md5=a014076a531da638da8f944b273ca755","In this paper, we propose a framework for synthesising 3D brain T1-weighted (T1-w) MRI images from Partial Volume (PV) maps for the purpose of generating synthetic MRI volumes with more accurate tissue borders. Synthetic MRIs are required to enlarge and enrich very limited data sets available for training of brain segmentation and related models. In comparison to current state-of-the-art methods, our framework exploits PV-map properties in order to guide a Generative Adversarial Network (GAN) towards the generation of more accurate and realistic synthetic MRI volumes. We demonstrate that conditioning a GAN on PV-maps instead of Binary-maps results in 58.96% more accurate tissue borders in synthetic MRIs. Furthermore, our results indicate an improvement in the representation of the Deep Gray Matter region in synthetic MRI volumes. Finally, we show that fine changes introduced into PV-maps are reflected in the synthetic images, while preserving accurate tissue borders, thus enabling better control during the data synthesis of novel synthetic MRI volumes. © Springer Nature Switzerland AG 2020.","Medical imaging; Tissue; Adversarial networks; Brain segmentation; Data synthesis; Limited data sets; Partial volumes; State-of-the-art methods; Synthetic images; T1-weighted; Magnetic resonance imaging","3D image synthesis; Generative Adversarial Network; Partial volume maps; Synthetic MRIs","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092191503"
"Han J.; Zhang Z.; Mao A.; Zhou Y.","Han, Jian (57210809852); Zhang, Zijie (57210803277); Mao, Ailing (57202466601); Zhou, Yuan (57191652248)","57210809852; 57210803277; 57202466601; 57191652248","Semantics Images Synthesis and Resolution Refinement Using Generative Adversarial Networks","2020","Lecture Notes in Electrical Engineering","516","","","612","620","8","10.1007/978-981-13-6504-1_74","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071466315&doi=10.1007%2f978-981-13-6504-1_74&partnerID=40&md5=ee8f6d27ca06c29bb670fe2a2c1ddabb","In this paper, we proposed a method to synthesizing a super-resolution image with the given image and text descriptions. Our work contains two parts. Wasserstein GAN is used to generate low-level resolution image under the guidance of a novel loss function. Then, a convolution net is followed to refine the resolution. This is an end-to-end network architecture. We have validated our model on Caltech-200 bird dataset, Oxford-102 flower dataset, and BSD300 dataset. The experiments show that the generated images not only match the given descriptions well but also maintain detailed features of original images with a higher resolution. © 2020, Springer Nature Singapore Pte Ltd.","Network architecture; Semantic Web; Semantics; Adversarial networks; Caltech; Higher resolution; Images synthesis; Loss functions; Original images; Resolution images; Super resolution; Image processing","Generative Adversarial Networks (GANs); Resolution refinement; Semantics images synthesis","Conference paper","Final","","Scopus","2-s2.0-85071466315"
"Diaz-Pinto A.; Colomer A.; Naranjo V.; Morales S.; Xu Y.; Frangi A.F.","Diaz-Pinto, Andres (57204781309); Colomer, Adrian (56769842500); Naranjo, Valery (55208614400); Morales, Sandra (55245697500); Xu, Yanwu (55516961100); Frangi, Alejandro F. (7005249248)","57204781309; 56769842500; 55208614400; 55245697500; 55516961100; 7005249248","Retinal Image Synthesis and Semi-Supervised Learning for Glaucoma Assessment","2019","IEEE transactions on medical imaging","38","9","","2211","2218","7","10.1109/TMI.2019.2903434","85","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068823099&doi=10.1109%2fTMI.2019.2903434&partnerID=40&md5=029bb75c0386baeee0d0d9dc56986f31","Recent works show that generative adversarial networks (GANs) can be successfully applied to image synthesis and semi-supervised learning, where, given a small labeled database and a large unlabeled database, the goal is to train a powerful classifier. In this paper, we trained a retinal image synthesizer and a semi-supervised learning method for automatic glaucoma assessment using an adversarial model on a small glaucoma-labeled database and a large unlabeled database. Various studies have shown that glaucoma can be monitored by analyzing the optic disc and its surroundings, and for that reason, the images used in this paper were automatically cropped around the optic disc. The novelty of this paper is to propose a new retinal image synthesizer and a semi-supervised learning method for glaucoma assessment based on the deep convolutional GANs. In addition, and to the best of our knowledge, this system is trained on an unprecedented number of publicly available images (86926 images). This system, hence, is not only able to generate images synthetically but to provide labels automatically. Synthetic images were qualitatively evaluated using t-SNE plots of features associated with the images and their anatomical consistency was estimated by measuring the proportion of pixels corresponding to the anatomical structures around the optic disc. The resulting image synthesizer is able to generate realistic (cropped) retinal images, and subsequently, the glaucoma classifier is able to classify them into glaucomatous and normal with high accuracy (AUC = 0.9017). The obtained retinal image synthesizer and the glaucoma classifier could then be used to generate an unlimited number of cropped retinal images with glaucoma labels.","Algorithms; Databases, Factual; Diagnostic Techniques, Ophthalmological; Glaucoma; Humans; Image Interpretation, Computer-Assisted; Retina; Supervised Machine Learning; algorithm; computer assisted diagnosis; diagnostic imaging; factual database; glaucoma; human; procedures; retina; supervised machine learning; visual system examination","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068823099"
"Alyafi B.; Diaz O.; Elangovan P.; Vilanova J.C.; Del Riego J.; Marti R.","Alyafi, Basel (57211473139); Diaz, Oliver (36172316800); Elangovan, Premkumar (19640003600); Vilanova, Joan C. (17836985700); Del Riego, Javier (37064464700); Marti, Robert (14048757500)","57211473139; 36172316800; 19640003600; 17836985700; 37064464700; 14048757500","Quality analysis of DCGAN-generated mammography lesions","2020","Proceedings of SPIE - The International Society for Optical Engineering","11513","","115130B","","","","10.1117/12.2560473","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086139155&doi=10.1117%2f12.2560473&partnerID=40&md5=053029122f57f8d4bce89b51d35da48f","Medical image synthesis has gained a great focus recently, especially after the introduction of Generative Adversarial Networks (GANs). GANs have been used widely to provide anatomically-plausible and diverse samples for augmentation and other applications, including segmentation and super resolution. In our previous work, Deep Convolutional GANs were used to generate synthetic mammogram lesions, masses mainly, that could enhance the classification performance in imbalanced datasets. In this new work, a deeper investigation was carried out to explore other aspects of the generated images evaluation, i.e., realism, feature space distribution, and observer studies. t-Stochastic Neighbor Embedding (t-SNE) was used to reduce the dimensionality of real and fake images to enable 2D visualisations. Additionally, two expert radiologists performed a realism-evaluation study. Visualisations showed that the generated images have a similar feature distribution of the real ones, avoiding outliers. Moreover, the Receiver Operating Characteristic (ROC) study showed that the radiologists could not, in many cases, distinguish between synthetic and real lesions, giving accuracies between 51% and 59% using a balanced sample set. © 2020 SPIE.","Classification (of information); Medical imaging; Stochastic systems; Visualization; Adversarial networks; Classification performance; Evaluation study; Feature distribution; Imbalanced Data-sets; Mammogram lesions; Receiver operating characteristics; Stochastic neighbor embedding; Quality control","Breast lesions; GANs; Image synthesis; Observer study; ROC curve; T-SNE","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086139155"
"","","","4th International Workshop on Simulation and Synthesis in Medical Imaging, SASHIMI 2019, held in conjunction with the 22nd International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11827 LNCS","","","","","160","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075685543&partnerID=40&md5=d02e23c98fb2596fdfe145b0997971e3","The proceedings contain 16 papers. The special focus in this conference is on International Workshop on Simulation and Synthesis in Medical Imaging, held in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention. The topics include: Physics-informed brain MRI segmentation; 3D medical image synthesis by factorised representation and deformable model learning; cycle-consistent training for reducing negative jacobian determinant in deep registration networks; ISMORE: An iterative self super-resolution algorithm; an optical model of whole blood for detecting platelets in lens-free images; Evaluation of the realism of an MRI simulator for stroke lesion prediction using convolutional neural network; Improved MR to CT synthesis for pet/mr attenuation correction using imitation learning; Unpaired multi-contrast MR image synthesis using generative adversarial networks; unsupervised retina image synthesis via disentangled representation learning; Pseudo-normal PET synthesis with generative adversarial networks for localising hypometabolism in epilepsies; breast mass detection in mammograms via blending adversarial learning; Tunable CT lung nodule synthesis conditioned on background image and semantic features; mask2Lesion: Mask-constrained adversarial skin lesion image synthesis; towards annotation-free segmentation of fluorescently labeled cell membranes in confocal microscopy images.","","","Conference review","Final","","Scopus","2-s2.0-85075685543"
"Abhishek K.; Hamarneh G.","Abhishek, Kumar (57212009862); Hamarneh, Ghassan (6603568967)","57212009862; 6603568967","Mask2Lesion: Mask-constrained adversarial skin lesion image synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11827 LNCS","","","71","80","9","10.1007/978-3-030-32778-1_8","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075658193&doi=10.1007%2f978-3-030-32778-1_8&partnerID=40&md5=eb0323986748da8060688bc937577c98","Skin lesion segmentation is a vital task in skin cancer diagnosis and further treatment. Although deep learning based approaches have significantly improved the segmentation accuracy, these algorithms are still reliant on having a large enough dataset in order to achieve adequate results. Inspired by the immense success of generative adversarial networks (GANs), we propose a GAN-based augmentation of the original dataset in order to improve the segmentation performance. In particular, we use the segmentation masks available in the training dataset to train the Mask2Lesion model, and use the model to generate new lesion images given any arbitrary mask, which are then used to augment the original training dataset. We test Mask2Lesion augmentation on the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset and achieve an improvement of 5.17% in the mean Dice score as compared to a model trained with only classical data augmentation techniques. © Springer Nature Switzerland AG 2019.","Deep learning; Dermatology; Diagnosis; Image segmentation; Large dataset; Statistical tests; Adversarial networks; Further treatments; Learning-based approach; Segmentation accuracy; Segmentation masks; Segmentation performance; Skin lesion; Skin lesion images; Medical imaging","Generative adversarial networks; Image segmentation; Skin lesion","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075658193"
"","","","2019 IEEE International Conference on Advanced Robotics and its Social Impacts, ARSO 2019","2019","Proceedings of IEEE Workshop on Advanced Robotics and its Social Impacts, ARSO","2019-October","","","","","436","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078340518&partnerID=40&md5=c10a6546d7d57e150b38b0621dfff4de","The proceedings contain 74 papers. The topics discussed include: simulation of kinematic accuracy of a 6-DOF serial robotic mechanism based on screw theory; fire fighting tactics with aerial hose-type robot 'dragon firefighter'; ResFPA-GAN: text-to-image synthesis with generative adversarial network based on residual block feature pyramid attention; multi-object grasp planning in high distribution density of service robot using inverse reachability map and base repositioning; foot trajectory planning of bipedal walking robot based on a uniform acceleration method; a method of recognizing obstacles for a small-sized autonomous underwater vehicle X4-AUV; a short-term motion prediction approach for guaranteed collision-free planning; and scientific mapping of 35 years of human factors research in occupational human-robot-interaction.","","","Conference review","Final","","Scopus","2-s2.0-85078340518"
"Mizginov V.A.; Danilov S.Yu.","Mizginov, V.A. (57192159517); Danilov, S.Yu. (57204968958)","57192159517; 57204968958","Synthetic thermal background and object texture generation using geometric information and GaN","2019","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2/W12","","149","154","5","10.5194/isprs-archives-XLII-2-W12-149-2019","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066459425&doi=10.5194%2fisprs-archives-XLII-2-W12-149-2019&partnerID=40&md5=4ca2a3a1f090e6c8fe43f3b227e074d1","Nowadays methods based on deep neural networks show the best performance among image recognition and object detection algorithms. Nevertheless, such methods require to have large databases of multispectral images of various objects to achieve state-of-the-art results. Therefore the dataset generation is one of the major challenges for the successful training of a deep neural network. However, infrared image datasets that are large enough for successful training of a deep neural network are not available in the public domain. Generation of synthetic datasets using 3D models of various scenes is a time-consuming method that requires long computation time and is not very realistic. This paper is focused on the development of the method for thermal image synthesis using a GAN (generative adversarial network). The aim of the presented work is to expand and complement the existing datasets of real thermal images. Today, deep convolutional networks are increasingly used for the goal of synthesizing various images. Recently a new generation of such algorithms commonly called GAN has become a promising tool for synthesizing images of various spectral ranges. These networks show effective results for image-to-image translations. While it is possible to generate a thermal texture for a single object, generation of environment textures is extremely difficult due to the presence of a large number of objects with different emission sources. The proposed method is based on a joint approach that uses 3D modeling and deep learning. Synthesis of background textures and objects textures is performed using a generative-adversarial neural network and semantic and geometric information about objects generated using 3D modeling. The developed approach significantly improves the realism of the synthetic images, especially in terms of the quality of background textures. © Authors 2019. CC BY 4.0 License.","3D modeling; Augmented reality; Deep neural networks; Gallium nitride; III-V semiconductors; Image recognition; Information use; Infrared imaging; Large dataset; Object detection; Object recognition; Security systems; Semantics; Textures; Adversarial networks; Background textures; Convolutional networks; Geometric information; Multispectral images; Object detection algorithms; Synthetic datasets; Thermal background; Image enhancement","Augmented reality; Generative adversarial network; Infrared images; Object recognition","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85066459425"
"Karoly A.I.; Takacs M.; Galambos P.","Karoly, Artur Istvan (57203227639); Takacs, Marta (22036707800); Galambos, Peter (26666702500)","57203227639; 22036707800; 26666702500","OCSVM-based Evaluation Method for Generative Neural Networks","2019","Proceedings of the International Joint Conference on Neural Networks","2019-July","","8852095","","","","10.1109/IJCNN.2019.8852095","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073248920&doi=10.1109%2fIJCNN.2019.8852095&partnerID=40&md5=053e29616bebe9797d139f2d2698b475","Humanity has desired to create machines that can comprehend fine arts for a long while. Recently, several articles report neural network models that managed to create something which can be considered as art, such as paintings or music. Naturally, these proposals are also leveraged in the field of robotics and autonomous vehicles for tasks such as training environment generation and efficient exploration in reinforcement learning. The common feature of all these approaches is the utilization of generative neural networks. These generative models are a trendy and actively researched area of machine learning methods. However, it is hard to find a general and objective metric for the evaluation of such models. In this paper, a One-Class Support Vector Machine-based solution is proposed. The basic concept is demonstrated via experiments on a set of GANs. The presented method can be extended and refined in further, more exhaustive studies to serve as an alternative for the current state-of-the-art procedures. © 2019 IEEE.","Arts computing; Machine learning; Support vector machines; Adversarial networks; Common features; Generative model; Image synthesis; Machine learning methods; Neural network model; One-class support vector machine; State-of-the-art procedures; Reinforcement learning","Generative Adversarial Networks; Image Synthesis; One-Class Support Vector Machine","Conference paper","Final","","Scopus","2-s2.0-85073248920"
"Brock A.; Donahue J.; Simonyan K.","Brock, Andrew (57192712827); Donahue, Jeff (54956247200); Simonyan, Karen (34870482800)","57192712827; 54956247200; 34870482800","Large scale GaN training for high fidelity natural image synthesis","2019","7th International Conference on Learning Representations, ICLR 2019","","","","","","","","1604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953405&partnerID=40&md5=cfbf9450ccfd5595bc15ff38bd640dcf","Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple “truncation trick,” allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fréchet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65. © 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.","Economic and social effects; Gallium nitride; III-V semiconductors; Adversarial networks; Complex datasets; High resolution; Image modeling; Image synthesis; Natural images; Recent progress; State of the art; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85083953405"
"Zhang Z.; Yang L.; Zheng Y.","Zhang, Zizhao (57020905500); Yang, Lin (55771607100); Zheng, Yefeng (8062522600)","57020905500; 55771607100; 8062522600","Multimodal medical volumes translation and segmentation with generative adversarial network","2019","Handbook of Medical Image Computing and Computer Assisted Intervention","","","","183","204","21","10.1016/B978-0-12-816176-0.00013-2","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082616626&doi=10.1016%2fB978-0-12-816176-0.00013-2&partnerID=40&md5=21cfc89414450ca38207cbb90205952a","Synthesized medical images have several important applications, e.g., as an intermedium in cross-modality image registration and as supplementary training samples to boost the generalization capability of a classifier. Especially, synthesized computed tomography (CT) data can provide X-ray attenuation map for radiation therapy planning. In this work, we propose a generic cross-modality synthesis approach with the following targets: (1) synthesizing realistic looking 3D images using unpaired training data, (2) ensuring consistent anatomical structures, which might be changed by geometric distortion in cross-modality synthesis, and (3) improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss, which is supervised by segmentors, to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. With extensive experiments on a dataset including a total of 4496 CT and magnetic resonance imaging (MRI) cardiovascular volumes, we show both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively. © 2020 Elsevier Inc. All rights reserved.","","Convolutional neural networks; Generative networks; Medical image synthesis; Segmentation; Volume tomography structure","Book chapter","Final","","Scopus","2-s2.0-85082616626"
"Kaplan S.; Lensu L.; Laaksonen L.; Uusitalo H.","Kaplan, Sinan (56814206000); Lensu, Lasse (6508033274); Laaksonen, Lauri (38361600700); Uusitalo, Hannu (56284366000)","56814206000; 6508033274; 38361600700; 56284366000","Evaluation of Unconditioned Deep Generative Synthesis of Retinal Images","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12002 LNCS","","","262","273","11","10.1007/978-3-030-40605-9_23","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080929810&doi=10.1007%2f978-3-030-40605-9_23&partnerID=40&md5=2b495514ee2a048c6f1a11155961ae00","Retinal images have been increasingly important in clinical diagnostics of several eye and systemic diseases. To help the medical doctors in this work, automatic and semi-automatic diagnosis methods can be used to increase the efficiency of diagnostic and follow-up processes, as well as enable wider disease screening programs. However, the training of advanced machine learning methods for improved retinal image analysis typically requires large and representative retinal image data sets. Even when large data sets of retinal images are available, the occurrence of different medical conditions is unbalanced in them. Hence, there is a need to enrich the existing data sets by data augmentation and introducing noise that is essential to build robust and reliable machine learning models. One way to overcome these shortcomings relies on generative models for synthesizing images. To study the limits of retinal image synthesis, this paper focuses on the deep generative models including a generative adversarial network and a variational autoencoder to synthesize images from noise without conditioning on any information regarding to the retina. The models are trained with the Kaggle EyePACS retinal image set, and for quantifying the image quality in a no-reference manner, the generated images are compared with the retinal images of the DiaRetDB1 database using common similarity metrics. © 2020, Springer Nature Switzerland AG.","Computer vision; Diagnosis; Image quality; Machine learning; Medical imaging; Ophthalmology; Program diagnostics; Adversarial networks; Auto encoders; Clinical diagnostics; Generative model; Machine learning methods; Machine learning models; Retinal image; Retinal image analysis; Image enhancement","Deep generative model; Generative adversarial network; Retinal image; Variational autoencoder","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85080929810"
"Hanif M.A.; Zuhaib Akbar M.; Ahmed R.; Rehman S.; Jantsch A.; Shafique M.","Hanif, Muhammad Abdullah (57194855888); Zuhaib Akbar, Muhammad (57211107859); Ahmed, Rehan (57213088796); Rehman, Semeen (7005331385); Jantsch, Axel (6701769322); Shafique, Muhammad (17435669500)","57194855888; 57211107859; 57213088796; 7005331385; 6701769322; 17435669500","MemGANs: Memory Management for Energy-Efficient Acceleration of Complex Computations in Hardware Architectures for Generative Adversarial Networks","2019","Proceedings of the International Symposium on Low Power Electronics and Design","2019-July","","8824833","","","","10.1109/ISLPED.2019.8824833","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072664153&doi=10.1109%2fISLPED.2019.8824833&partnerID=40&md5=d57510b236f6566e76f93f4ed2713b09","Generative Adversarial Networks (GANs) have gained importance because of their tremendous unsupervised learning capability and enormous applications in data generation, for example, text to image synthesis, synthetic medical data generation, video generation, and artwork generation. Hardware acceleration for GANs become challenging due to the intrinsic complex computational phases, which require efficient data management during the training and inference. In this work, we propose a distributed on-chip memory architecture, which aims at efficiently handling the data for complex computations involved in GANs, such as strided convolution or transposed convolution. We also propose a controller that improves the computational efficiency by pre-arranging the data from either the off-chip memory or the computational units before storing it in the on-chip memory. Our architectural enhancement supports to achieve 3.65x performance improvement in state-of-the-art, and reduces the number of read accesses and write accesses by 85% and 75%, respectively. © 2019 IEEE.","Complex networks; Convolution; Data handling; Energy efficiency; Information management; Low power electronics; Medical imaging; Memory architecture; Network architecture; Adversarial networks; Architectural enhancement; Complex computation; Computational units; DCGAN; Hardware acceleration; Hardware accelerators; Hardware architecture; Computational efficiency","DCGAN; DNN; GAN; Generative Adversarial Networks; Hardware Accelerator; Memory Architecture","Conference paper","Final","","Scopus","2-s2.0-85072664153"
"Lee H.; Jo J.; Lim H.; Lee S.","Lee, Hyunhee (57217057860); Jo, Jaechoon (42661424600); Lim, Heuiseok (36028297500); Lee, Sanghyuk (57217060364)","57217057860; 42661424600; 36028297500; 57217060364","Study on Optimal Generative Network for Synthesizing Brain Tumor-Segmented MR Images","2020","Mathematical Problems in Engineering","2020","","8273173","","","","10.1155/2020/8273173","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085992741&doi=10.1155%2f2020%2f8273173&partnerID=40&md5=9127fdb1fb7896352985d1103c059602","Due to institutional and privacy issues, medical imaging researches are confronted with serious data scarcity. Image synthesis using generative adversarial networks provides a generic solution to the lack of medical imaging data. We synthesize high-quality brain tumor-segmented MR images, which consists of two tasks: synthesis and segmentation. We performed experiments with two different generative networks, the first using the ResNet model, which has significant advantages of style transfer, and the second, the U-Net model, one of the most powerful models for segmentation. We compare the performance of each model and propose a more robust model for synthesizing brain tumor-segmented MR images. Although ResNet produced better-quality images than did U-Net for the same samples, it used a great deal of memory and took much longer to train. U-Net, meanwhile, segmented the brain tumors more accurately than did ResNet. © 2020 Hyunhee Lee et al.","Brain; Image segmentation; Magnetic resonance imaging; Tumors; Adversarial networks; Data scarcity; Generic solutions; High quality; Image synthesis; Privacy issue; Quality image; Robust modeling; Medical imaging","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85085992741"
"Yang H.; Xia K.; Anqi B.; Qian P.; Khosravi M.R.","Yang, Huan (57207732435); Xia, Kaijian (35779668800); Anqi, Bi (56266944900); Qian, Pengjiang (36598989000); Khosravi, Mohammad R. (57189501555)","57207732435; 35779668800; 56266944900; 36598989000; 57189501555","Abdomen MRI synthesis based on conditional GAN","2019","Proceedings - 6th Annual Conference on Computational Science and Computational Intelligence, CSCI 2019","","","9071043","1021","1025","4","10.1109/CSCI49370.2019.00195","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084759796&doi=10.1109%2fCSCI49370.2019.00195&partnerID=40&md5=c772d936b3ae13912b2afc74f25d5e8d","Obtaining different contrast images of the same anatomical location of the same subject can increase the diversity of diagnostic information. However, due to the cost constraints, it is often not possible to obtain sufficient different contrast images, and images in some contrasts may be corrupted by noise and artifacts. In this case, if these images with different contrasts can be generated from existing images, the diagnostic accuracy will be greatly improved. Here, we explore an end-to-end deep convolutional neural networks for multi-contrast MRI synthesis based on conditional generative adversarial networks (cGAN). The proposed approach preserves high-frequency details via an adversarial loss and a pixel-wise loss, to transform abdomen CT into abdomen MR. Paired MR-CT volumes of 10 patients are analyzed. A quantitative evaluation showed that the approach is capable to synthesize MR images that closely similar to reference MR images. © 2019 IEEE.","Computerized tomography; Convolutional neural networks; Deep neural networks; Diagnosis; Magnetic resonance imaging; Adversarial networks; Anatomical locations; Cost constraints; CT volume; Diagnostic accuracy; End to end; High frequency HF; Quantitative evaluation; Image enhancement","Conditional generative adversarial networks; Image synthesis; Multi-contrast","Conference paper","Final","","Scopus","2-s2.0-85084759796"
"Mahmood F.; Xu W.; Durr N.J.; Johnson J.W.; Yuille A.","Mahmood, Faisal (56647751100); Xu, Wenhao (57209576734); Durr, Nicholas J. (16303771500); Johnson, Jeremiah W. (56327099200); Yuille, Alan (7006372632)","56647751100; 57209576734; 16303771500; 56327099200; 7006372632","Structured prediction using cGANs with fusion discriminator","2019","Deep Generative Models for Highly Structured Data, DGS@ICLR 2019 Workshop","","","","","","","","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083952682&partnerID=40&md5=5ee4161743668428ae0fd72449ff9e7d","We propose the fusion discriminator, a single unified framework for incorporating conditional information into a generative adversarial network (GAN) for a variety of distinct structured prediction tasks, including image synthesis, semantic segmentation, and depth estimation. Much like commonly used convolutional neural network - conditional Markov random field (CNN-CRF) models, the proposed method is able to enforce higher-order consistency in the model, but without being limited to a very specific class of potentials. The method is conceptually simple and flexible, and our experimental results demonstrate improvement on several diverse structured prediction tasks. © Deep Generative Models for Highly Structured Data, DGS@ICLR 2019 Workshop.All right reserved.","Discriminators; Image segmentation; Markov processes; Neural networks; Semantics; Adversarial networks; Convolutional neural network; Depth Estimation; Image synthesis; Markov Random Fields; Semantic segmentation; Structured prediction; Unified framework; Forecasting","","Conference paper","Final","","Scopus","2-s2.0-85083952682"
"Yu B.; Zhou L.; Wang L.; Shi Y.; Fripp J.; Bourgeat P.","Yu, Biting (57201496052); Zhou, Luping (23398846800); Wang, Lei (54958774700); Shi, Yinghuan (35241386100); Fripp, Jurgen (13605436500); Bourgeat, Pierrick (8576247900)","57201496052; 23398846800; 54958774700; 35241386100; 13605436500; 8576247900","Ea-GANs: Edge-Aware Generative Adversarial Networks for Cross-Modality MR Image Synthesis","2019","IEEE Transactions on Medical Imaging","38","7","8629301","1750","1762","12","10.1109/TMI.2019.2895894","116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068443097&doi=10.1109%2fTMI.2019.2895894&partnerID=40&md5=503af010c43c5b2dc827cf300203aec7","Magnetic resonance (MR) imaging is a widely used medical imaging protocol that can be configured to provide different contrasts between the tissues in human body. By setting different scanning parameters, each MR imaging modality reflects the unique visual characteristic of scanned body part, benefiting the subsequent analysis from multiple perspectives. To utilize the complementary information from multiple imaging modalities, cross-modality MR image synthesis has aroused increasing research interest recently. However, most existing methods only focus on minimizing pixel/voxel-wise intensity difference but ignore the textural details of image content structure, which affects the quality of synthesized images. In this paper, we propose edge-aware generative adversarial networks (Ea-GANs) for cross-modality MR image synthesis. Specifically, we integrate edge information, which reflects the textural structure of image content and depicts the boundaries of different objects in images, to reduce this gap. Corresponding to different learning strategies, two frameworks are proposed, i.e., a generator-induced Ea-GAN (gEa-GAN) and a discriminator-induced Ea-GAN (dEa-GAN). The gEa-GAN incorporates the edge information via its generator, while the dEa-GAN further does this from both the generator and the discriminator so that the edge similarity is also adversarially learned. In addition, the proposed Ea-GANs are 3D-based and utilize hierarchical features to capture contextual information. The experimental results demonstrate that the proposed Ea-GANs, especially the dEa-GAN, outperform multiple state-of-the-art methods for cross-modality MR image synthesis in both qualitative and quantitative measures. Moreover, the dEa-GAN also shows excellent generality to generic image synthesis tasks on benchmark datasets about facades, maps, and cityscapes. © 1982-2012 IEEE.","Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Brain; Learning systems; Medical imaging; Neural networks; Adversarial networks; Contextual information; Hierarchical features; Intensity difference; Multiple imaging modality; Quantitative measures; Scanning parameters; Synthesized images; article; brain; human; human experiment; machine learning; nuclear magnetic resonance imaging; quantitative analysis; synthesis; brain; diagnostic imaging; image processing; nuclear magnetic resonance imaging; procedures; Magnetic resonance imaging","brain; machine learning; magnetic resonance imaging (MRI); Neural networks","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068443097"
"Zhang F.; Zhang T.; Mao Q.; Xu C.","Zhang, Feifei (57138854900); Zhang, Tianzhu (55729040600); Mao, Qirong (7101735930); Xu, Changsheng (56153258200)","57138854900; 55729040600; 7101735930; 56153258200","Geometry Guided Pose-Invariant Facial Expression Recognition","2020","IEEE Transactions on Image Processing","29","","8995782","4445","4460","15","10.1109/TIP.2020.2972114","60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081045969&doi=10.1109%2fTIP.2020.2972114&partnerID=40&md5=216e96dcd66f1f9c17a62cf3415d5a49","Driven by recent advances in human-centered computing, Facial Expression Recognition (FER) has attracted significant attention in many applications. However, most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifier for each pose. Different from existing methods, this paper proposes an end-to-end deep learning model that allows to simultaneous facial image synthesis and pose-invariant facial expression recognition by exploiting shape geometry of the face image. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, given an input face and a target pose and expression designated by a set of facial landmarks, an identity-preserving face can be generated through guiding by the target pose and expression. Second, the identity representation is explicitly disentangled from both expression and pose variations through the shape geometry delivered by facial landmarks. Third, our model can automatically generate face images with different expressions and poses in a continuous way to enlarge and enrich the training set for the FER task. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on both controlled and in-the-wild benchmark datasets including Multi-PIE, BU-3DFE, and SFEW. The code is included in the supplementary material. © 2020 IEEE.","Deep learning; Geometry; Adversarial networks; Benchmark datasets; Conventional approach; Facial expression recognition; Facial Image synthesis; Facial landmark; Human-centered computing; State-of-the-art algorithms; Face recognition","Facial expression recognition; facial image synthesis; facial landmarks; generative adversarial network","Article","Final","","Scopus","2-s2.0-85081045969"
"Mao Q.; Lee H.-Y.; Tseng H.-Y.; Ma S.; Yang M.-H.","Mao, Qi (57193140237); Lee, Hsin-Ying (57207324989); Tseng, Hung-Yu (57204283094); Ma, Siwei (34872761500); Yang, Ming-Hsuan (7404927015)","57193140237; 57207324989; 57204283094; 34872761500; 7404927015","Mode seeking generative adversarial networks for diverse image synthesis","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8953230","1429","1437","8","10.1109/CVPR.2019.00152","177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078733070&doi=10.1109%2fCVPR.2019.00152&partnerID=40&md5=45361860e3b493468d1589877f714315","Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality. © 2019 IEEE.","Deep learning; Adversarial networks; Conditional generation; Network structures; Quantitative result; Regularization methods; Regularization terms; Training overhead; Video synthesis; Computer vision","Deep Learning; Image and Video Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078733070"
"Liu Z.; Gao F.; Wang Y.","Liu, Zhibo (57208718322); Gao, Feng (55842372100); Wang, Yizhou (57194437986)","57208718322; 55842372100; 57194437986","A Generative Adversarial Network for AI-Aided Chair Design","2019","Proceedings - 2nd International Conference on Multimedia Information Processing and Retrieval, MIPR 2019","","","8695313","486","490","4","10.1109/MIPR.2019.00098","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065627800&doi=10.1109%2fMIPR.2019.00098&partnerID=40&md5=b14ca9c4f38785c6fbe86b3e16ad910b","We present a method for improving human design of chairs. The goal of the method is generating enormous chair candidates in order to facilitate human designer by creating sketches and 3d models accordingly based on the generated chair design. It consists of an image synthesis module, which learns the underlying distribution of training dataset, a super-resolution module, which improve quality of generated image and human involvements. Finally, we manually pick one of the generated candidates to create a real life chair for illustration. © 2019 IEEE.","Deep learning; Design; Image enhancement; Adversarial networks; Aided designs; Human design; Image synthesis; Super resolution; Training dataset; Underlying distribution; Seats","AI-aided Design; deep learning; GAN","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065627800"
"Zhang D.; Khoreva A.","Zhang, Dan (57221171748); Khoreva, Anna (56406498500)","57221171748; 56406498500","Progressive augmentation of GANs","2019","Advances in Neural Information Processing Systems","32","","","","","","","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090171098&partnerID=40&md5=1cc570b59e4bee8024ce70935ab8d950","Training of Generative Adversarial Networks (GANs) is notoriously fragile, requiring to maintain a careful balance between the generator and the discriminator in order to perform well. To mitigate this issue we introduce a new regularization technique - progressive augmentation of GANs (PA-GAN). The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not compromise the discriminator's optimality and encourages a healthy competition between the generator and discriminator, leading to the better-performing generator. We experimentally demonstrate the effectiveness of PA-GAN across different architectures and on multiple benchmarks for the image synthesis task, on average achieving ~ 3 point improvement of the FID score. © 2019 Neural information processing systems foundation. All rights reserved.","Adversarial networks; Continuous learning; Feature space; Image synthesis; Optimality; Regularization technique; Task difficulty; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85090171098"
"Campello V.M.; Martín-Isla C.; Izquierdo C.; Petersen S.E.; Ballester M.A.G.; Lekadir K.","Campello, Víctor M. (57195319712); Martín-Isla, Carlos (57214879059); Izquierdo, Cristian (57214883869); Petersen, Steffen E. (57212301202); Ballester, Miguel A. González (11639980600); Lekadir, Karim (15042517700)","57195319712; 57214879059; 57214883869; 57212301202; 11639980600; 15042517700","Combining Multi-Sequence and Synthetic Images for Improved Segmentation of Late Gadolinium Enhancement Cardiac MRI","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12009 LNCS","","","290","299","9","10.1007/978-3-030-39074-7_31","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081911828&doi=10.1007%2f978-3-030-39074-7_31&partnerID=40&md5=cd72dfc240cc4bb45e3158e41f3cf179","Accurate segmentation of the cardiac boundaries in late gadolinium enhancement magnetic resonance images (LGE-MRI) is a fundamental step for accurate quantification of scar tissue. However, while there are many solutions for automatic cardiac segmentation of cine images, the presence of scar tissue can make the correct delineation of the myocardium in LGE-MRI challenging even for human experts. As part of the Multi-Sequence Cardiac MR Segmentation Challenge, we propose a solution for LGE-MRI segmentation based on two components. First, a generative adversarial network is trained for the task of modality-to-modality translation between cine and LGE-MRI sequences to obtain extra synthetic images for both modalities. Second, a deep learning model is trained for segmentation with different combinations of original, augmented and synthetic sequences. Our results based on three magnetic resonance sequences (LGE, bSSFP and T2) from 45 different patients show that the multi-sequence model training integrating synthetic images and data augmentation improves in the segmentation over conventional training with real datasets. In conclusion, the accuracy of the segmentation of LGE-MRI images can be improved by using complementary information provided by non-contrast MRI sequences. © Springer Nature Switzerland AG 2020.","Cardiac resynchronization therapy; Computation theory; Computational methods; Deep learning; Gadolinium; Heart; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Tissue; Accurate quantifications; Adversarial networks; Cardiac MRI; Cardiac segmentation; Data augmentation; Image synthesis; Synthetic images; Synthetic sequence; Image enhancement","Deep learning; Image segmentation; Image synthesis; Late gadolinium enhancement MRI; Multi-sequence cardiac MRI","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081911828"
"Cai J.; Zhang Z.; Cui L.; Zheng Y.; Yang L.","Cai, Jinzheng (57191430539); Zhang, Zizhao (57020905500); Cui, Lei (56925934300); Zheng, Yefeng (8062522600); Yang, Lin (55771607100)","57191430539; 57020905500; 56925934300; 8062522600; 55771607100","Towards cross-modal organ translation and segmentation: A cycle- and shape-consistent generative adversarial network","2019","Medical Image Analysis","52","","","174","184","10","10.1016/j.media.2018.12.002","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058820733&doi=10.1016%2fj.media.2018.12.002&partnerID=40&md5=1c09ba30fd56a1bf180766f6bc93e7be","Synthesized medical images have several important applications. For instance, they can be used as an intermedium in cross-modality image registration or used as augmented training samples to boost the generalization capability of a classifier. In this work, we propose a generic cross-modality synthesis approach with the following targets: 1) synthesizing realistic looking 2D/3D images without needing paired training data, 2) ensuring consistent anatomical structures, which could be changed by geometric distortion in cross-modality synthesis and 3) more importantly, improving volume segmentation by using synthetic data for modalities with limited training samples. We show that these goals can be achieved with an end-to-end 2D/3D convolutional neural network (CNN) composed of mutually-beneficial generators and segmentors for image synthesis and segmentation tasks. The generators are trained with an adversarial loss, a cycle-consistency loss, and also a shape-consistency loss (supervised by segmentors) to reduce the geometric distortion. From the segmentation view, the segmentors are boosted by synthetic data from generators in an online manner. Generators and segmentors prompt each other alternatively in an end-to-end training fashion. We validate our proposed method on three datasets, including cardiovascular CT and magnetic resonance imaging (MRI), abdominal CT and MRI, and mammography X-rays from different data domains, showing both tasks are beneficial to each other and coupling these two tasks results in better performance than solving them exclusively. © 2018","Humans; Image Interpretation, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Mammography; Neural Networks, Computer; Tomography, X-Ray Computed; Diagnostic radiography; Image enhancement; Image segmentation; Magnetic resonance imaging; Neural networks; Sampling; X rays; Adversarial networks; Anatomical structures; Convolutional Neural Networks (CNN); Generalization capability; Geometric distortion; Image synthesis; Magnetic Resonance Imaging (MRI); Organ segmentation; article; computer assisted tomography; mammography; nuclear magnetic resonance imaging; synthesis; X ray; computer assisted diagnosis; human; mammography; nuclear magnetic resonance imaging; procedures; three dimensional imaging; x-ray computed tomography; Computerized tomography","Computed tomography (CT); Generative adversarial network (GAN); Magnetic resonance imaging (MRI); Mammography X-ray; Medical image synthesis; Organ segmentation","Article","Final","","Scopus","2-s2.0-85058820733"
"Gong Y.; Karanam S.; Wu Z.; Peng K.-C.; Ernst J.; Doerschuk P.C.","Gong, Yunye (56341335400); Karanam, Srikrishna (38561629100); Wu, Ziyan (55500641100); Peng, Kuan-Chuan (56119328700); Ernst, Jan (55365390500); Doerschuk, Peter C. (7004237673)","56341335400; 38561629100; 55500641100; 56119328700; 55365390500; 7004237673","Learning Compositional Visual Concepts with Mutual Consistency","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8579001","8659","8668","9","10.1109/CVPR.2018.00903","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062868778&doi=10.1109%2fCVPR.2018.00903&partnerID=40&md5=48a652262f636fc80fc04d09f41738f0","Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application. © 2018 IEEE.","Semantics; Adversarial networks; Cyclic consistency; Data augmentation; Face Verification; Geometric changes; Illumination changes; Quantitative evaluation; Training and testing; Computer vision","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062868778"
"Lee D.; Moon W.-J.; Ye J.C.","Lee, Dongwook (57212667417); Moon, Won-Jin (13308689100); Ye, Jong Chul (57223117835)","57212667417; 13308689100; 57223117835","Assessing the importance of magnetic resonance contrasts using collaborative generative adversarial networks","2020","Nature Machine Intelligence","2","1","","34","42","8","10.1038/s42256-019-0137-x","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089899241&doi=10.1038%2fs42256-019-0137-x&partnerID=40&md5=9e4a971a61ea6d650bd36e2897efc4b7","A unique advantage of magnetic resonance imaging (MRI) is its mechanism for generating various image contrasts depending on tissue-specific parameters, which provides useful clinical information. Unfortunately, a complete set of MR contrasts is often difficult to obtain in a real clinical environment. Recently, there have been claims that generative models such as generative adversarial networks (GANs) can synthesize MR contrasts that are not acquired. However, the poor scalability of existing GAN-based image synthesis poses a fundamental challenge to understanding the nature of MR contrasts: which contrasts matter, and which cannot be synthesized by generative models? Here, we show that these questions can be addressed systematically by learning the joint manifold of multiple MR contrasts using collaborative generative adversarial networks. Our experimental results show that the exogenous contrast provided by contrast agents is not replaceable, but endogenous contrasts such as T1 and T2 can be synthesized from other contrasts. These findings provide important guidance for the acquisition-protocol design of MR in clinical environments. © 2020, The Author(s), under exclusive licence to Springer Nature Limited.","Acquisition protocols; Adversarial networks; Clinical environments; Clinical information; Generative model; Image contrasts; Image synthesis; Tissue specifics; Magnetic resonance imaging","","Article","Final","","Scopus","2-s2.0-85089899241"
"Huang X.; Wang M.; Gong M.","Huang, Xin (57221178908); Wang, Mingjie (57208015942); Gong, Minglun (7201566763)","57221178908; 57208015942; 7201566763","Hierarchically-fused generative adversarial network for text to realistic image synthesis","2019","Proceedings - 2019 16th Conference on Computer and Robot Vision, CRV 2019","","","8781630","73","80","7","10.1109/CRV.2019.00018","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070954912&doi=10.1109%2fCRV.2019.00018&partnerID=40&md5=ff2684b9bb1028f5d039e885292d8d54","In this paper, we present a novel Hierarchically-fused Generative Adversarial Network (HfGAN) for synthesizing realistic images from text descriptions. While existing approaches on this topic have achieved impressive success, to generate 256X256 images from captions, they commonly resort to coarse-to-fine scheme and associate multiple discriminators in different stages of the networks. Such a strategy is both inefficient and prone to artifacts. Motivated by the above findings, we propose an end-to-end network that can generate 256X256 photo-realistic images with only one discriminator. We fully exploit the hierarchical information from different layers and directly generate the fine-scale images by adaptively fusing features from multi-hierarchical layers. We quantitatively evaluate the synthesized images with Inception Score, Visual-semantic Similarity and average training time on the CUB birds, Oxford-102 flowers, and COCO datasets. The results show that our model is more efficient and noticeably outperforms the previous state-of-the-art methods. © 2019 IEEE.","Agricultural robots; Computer vision; Robots; Semantics; Adversarial networks; Hierarchical features; Hierarchical information; Image synthesis; Photorealistic images; Realistic image synthesis; State-of-the-art methods; Synthesized images; Discriminators","Generative Adversarial Networks; Hierarchical features fusion; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85070954912"
"Zhan F.; Zhu H.; Lu S.","Zhan, Fangneng (57204285913); Zhu, Hongyuan (55843019200); Lu, Shijian (8439329200)","57204285913; 55843019200; 8439329200","Spatial fusion gan for image synthesis","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8953967","3648","3657","9","10.1109/CVPR.2019.00377","64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077648594&doi=10.1109%2fCVPR.2019.00377&partnerID=40&md5=6fe670b592a081520dd253c0bbe96ade","Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjust the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is incorporated for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end with little supervision. The SF-GAN has been evaluated in two tasks: (1) realistic scene text image synthesis for training better recognition models; (2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN. © 2019 IEEE.","Character recognition; Computer vision; Deep learning; Glass; Adversarial networks; Detail preserving; Document analysis; Foreground objects; Quantitative comparison; Realistic image synthesis; Recognition models; Video synthesis; Geometry","Deep Learning; Document Analysis; Image and Video Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077648594"
"Zhang Z.; Zhang Y.; Liu W.; Yu W.; He G.; Jiang N.; Yang Z.","Zhang, Zhiqiang (57206280843); Zhang, Yunye (57211359203); Liu, Wenfa (57216628358); Yu, Wenxin (36610960300); He, Gang (56937631400); Jiang, Ning (57212426361); Yang, Zhuo (57203791621)","57206280843; 57211359203; 57216628358; 36610960300; 56937631400; 57212426361; 57203791621","Text to Complicated Image Synthesis with Segmentation Information Guidance","2020","Smart Innovation, Systems and Technologies","180","","","273","282","9","10.1007/978-981-15-3867-4_32","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084115401&doi=10.1007%2f978-981-15-3867-4_32&partnerID=40&md5=b022eafc2a820c9976b346fa07631fa7","In this paper, we propose a novel method called Segmentation Information Guidance (SIG). In this method, additional segmentation information is added to guide the process of text to complicated image synthesis. We demonstrate the effectiveness of SIG model on Microsoft Common Objects in Common (MSCOCO) dataset. It proves that the image results generated by directly using the segmentation image are more authentic and coherent than that without background. © 2020, Springer Nature Singapore Pte Ltd.","Imaging techniques; Three dimensional computer graphics; Image synthesis; MicroSoft; Segmentation images; Segmentation informations; Image segmentation","Computer vision; Deep learning; Generative adversarial Networks (GAN); Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85084115401"
"Kim H.; Jung S.H.","Kim, HoJoong (57208821240); Jung, Sung Hoon (56300482900)","57208821240; 56300482900","Sogn: Novel generative model using SOM","2019","Electronics Letters","55","10","","597","599","2","10.1049/el.2019.0202","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065892045&doi=10.1049%2fel.2019.0202&partnerID=40&md5=2cc35847a010814ae07295849812d293","Generative models such as variational autoencoder (VAE) and generative adversarial network (GAN) have been widely applied to many areas including image synthesis and voice generation. However, they have some problems that VAE makes blur images and GAN is difficult to learn due to mode collapsing. A novel generative model is proposed using a self-organising map (SOM) termed a self-organising generative network (SOGN). In the SOGN, training images are first mapped to SOM and then the output space of SOM is transformed into 2D vector spaces. These vector values are used as latent vectors to train the generative network such as artificial neural networks or convolutional neural networks. Experimental results with MNIST and CIFAR-10 datasets showed that their generative model was easy to train without mode collapsing and made more clean images than VAE. It was also confirmed that the manifold is well-observed without generating by the average effect of multiple images. © The Institution of Engineering and Technology 2019","Neural networks; Adversarial networks; Convolutional neural network; Generative model; Image synthesis; Latent vectors; Multiple image; Self-organising; Training image; Vector spaces","","Article","Final","","Scopus","2-s2.0-85065892045"
"Ben-Cohen A.; Klang E.; Raskin S.P.; Soffer S.; Ben-Haim S.; Konen E.; Amitai M.M.; Greenspan H.","Ben-Cohen, Avi (56198986300); Klang, Eyal (56080228800); Raskin, Stephen P. (55389995700); Soffer, Shelly (57190729426); Ben-Haim, Simona (55579699800); Konen, Eli (6701640190); Amitai, Michal Marianne (6701865585); Greenspan, Hayit (7004965553)","56198986300; 56080228800; 55389995700; 57190729426; 55579699800; 6701640190; 6701865585; 7004965553","Cross-modality synthesis from CT to PET using FCN and GAN networks for improved automated lesion detection","2019","Engineering Applications of Artificial Intelligence","78","","","186","194","8","10.1016/j.engappai.2018.11.013","90","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057755952&doi=10.1016%2fj.engappai.2018.11.013&partnerID=40&md5=974889515bcf5461b0d22cb5cc736a93","In this work we present a novel system for generation of virtual PET images using CT scans. We combine a fully convolutional network (FCN) with a conditional generative adversarial network (GAN) to generate simulated PET data from given input CT data. The synthesized PET can be used for false-positive reduction in lesion detection solutions. Clinically, such solutions may enable lesion detection and drug treatment evaluation in a CT-only environment, thus reducing the need for the more expensive and radioactive PET/CT scan. Our dataset includes 60 PET/CT scans from Sheba Medical center. We used 23 scans for training and 37 for testing. Different schemes to achieve the synthesized output were qualitatively compared. Quantitative evaluation was conducted using an existing lesion detection software, combining the synthesized PET as a false positive reduction layer for the detection of malignant lesions in the liver. Current results look promising showing a 28% reduction in the average false positive per case from 2.9 to 2.1. The suggested solution is comprehensive and can be expanded to additional body organs, and different modalities. © 2018 Elsevier Ltd","Deep learning; Drug therapy; Polyethylene terephthalates; Adversarial networks; Convolutional networks; False-positive reduction; Image synthesis; Lesion detection; Liver lesions; Malignant lesion; Quantitative evaluation; Computerized tomography","CT; Deep learning; GAN; Image synthesis; Liver lesion; PET","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85057755952"
"Vo D.M.; Sugimoto A.","Vo, Duc Minh (56015722800); Sugimoto, Akihiro (23391348700)","56015722800; 23391348700","Paired-D GAN for Semantic Image Synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11364 LNCS","","","468","484","16","10.1007/978-3-030-20870-7_29","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066846955&doi=10.1007%2f978-3-030-20870-7_29&partnerID=40&md5=77ba024873a67f6b4d3c93dac606f9b6","Semantic image synthesis is to render foreground (object) given as a text description into a given source image. This has a wide range of applications such as intelligent image manipulation, and is helpful to those who are not good at painting. We propose a generative adversarial network having a pair of discriminators with different architectures, called Paired-D GAN, for semantic image synthesis where the two discriminators make different judgments: one for foreground synthesis and the other for background synthesis. The generator of paired-D GAN has the encoder-decoder architecture with skip-connections and synthesizes an image matching the given text description while preserving other parts of the source image. The two discriminators judge foreground and background of the synthesized image separately to meet an input text description and a source image. The paired-D GAN is trained using the effective adversarial learning process in a simultaneous three-player minimax game. Experimental results on the Caltech-200 bird dataset and the Oxford-102 flower dataset show that Paired-GAN is capable of semantically synthesizing images to match an input text description while retaining the background in a source image against the state-of-the-art methods. © 2019, Springer Nature Switzerland AG.","Network architecture; Semantics; Adversarial learning; Adversarial networks; Encoder-decoder architecture; Image manipulation; Minimax games; Semantic images; State-of-the-art methods; Synthesized images; Computer vision","","Conference paper","Final","","Scopus","2-s2.0-85066846955"
"Huang P.; Li D.; Jiao Z.; Wei D.; Li G.; Wang Q.; Zhang H.; Shen D.","Huang, Pu (57193329557); Li, Dengwang (36463794100); Jiao, Zhicheng (57189091634); Wei, Dongming (57211428968); Li, Guoshi (16417306600); Wang, Qian (57192157811); Zhang, Han (36117322900); Shen, Dinggang (7401738392)","57193329557; 36463794100; 57189091634; 57211428968; 16417306600; 57192157811; 36117322900; 7401738392","CoCa-GAN: Common-Feature-Learning-Based Context-Aware Generative Adversarial Network for Glioma Grading","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11766 LNCS","","","155","163","8","10.1007/978-3-030-32248-9_18","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075698370&doi=10.1007%2f978-3-030-32248-9_18&partnerID=40&md5=1f56e0b6ed3ca78b71fd3ef1bf56fcf9","Multi-modal structural MRI has been widely used for presurgical glioma grading for treatment planning. Despite providing complementary information, a complete set of high-resolution multi-modality data is costly and often impossible to acquire in clinical settings (although T1 MRI is more commonly acquired). To leverage more comprehensive multimodality information for better glioma grading instead of doing so with T1 MRI data only, we introduce a three-dimensional common feature learning-based context-aware generative adversarial network (CoCa-GAN) for multimodal MRI data synthesis based on T1 MRI and use the comprehensive features from a common feature space to achieve a clinically feasible glioma grading with limited imaging modality. The common feature space is first learned by simultaneously utilizing four MRI modalities with the adversarial learning and context-aware learning, where the inter-modality relationships and lesion-specific features can be explicitly encoded. Then, the domain (modality) invariant information represented in the common space is leveraged to synthesize the missing modalities for a joint prediction of glioma grades (high- vs. low-grade). Furthermore, Gradient-weighted Class Activation Mapping (GradCAM) is utilized to provide interpretability to the factors that contribute to the grading, for potential clinical usage. Results demonstrate that the common feature learning achieves more accurate glioma grading than simply using single modality data and leads to a comparable performance to that with complete modalities as inputs. Our method offers a highly feasible solution to clinical practice where multi-modality data is often unavailable. © 2019, Springer Nature Switzerland AG.","Machine learning; Magnetic resonance imaging; Medical computing; Medical imaging; Tumors; Activation mapping; Adversarial learning; Adversarial networks; Clinical practices; Clinical settings; Context-aware learning; Image synthesis; Treatment planning; Grading","Generative adversarial network; Glioma grading; Image synthesis; MRI","Conference paper","Final","","Scopus","2-s2.0-85075698370"
"Ni J.; Zhang S.; Zhou Z.; Hou J.; Gao F.","Ni, Jiancheng (14831535200); Zhang, Susu (57215657489); Zhou, Zili (57215660890); Hou, Jie (57705815000); Gao, Feng (57215650829)","14831535200; 57215657489; 57215660890; 57705815000; 57215650829","Instance Mask Embedding and Attribute-Adaptive Generative Adversarial Network for Text-to-Image Synthesis","2020","IEEE Access","8","","9007390","37697","37711","14","10.1109/ACCESS.2020.2975841","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081596780&doi=10.1109%2fACCESS.2020.2975841&partnerID=40&md5=813bcd7db2631be8989eb10b6c063634","Existing image generation models have achieved the synthesis of reasonable individuals and complex but low-resolution images. Directly from complicated text to high-resolution image generation still remains a challenge. To this end, we propose the instance mask embedding and attribute-adaptive generative adversarial network (IMEAA-GAN). Firstly, we use the box regression network to compute a global layout containing the class labels and locations for each instance. Then the global generator encodes the layout, combines the whole text embedding and noise to preliminarily generate a low-resolution image; the instance embedding mechanism is used firstly to guide local refinement generators obtain fine-grained local features and generate a more realistic image. Finally, in order to synthesize the exact visual attributes, we introduce the multi-scale attribute-adaptive discriminator, which provides local refinement generators with the specific training signals to explicitly generate instance-level features. Extensive experiments based on the MS-COCO dataset and the Caltech-UCSD Birds-200-2011 dataset show that our model can obtain globally consistent attributes and generate complex images with local texture details. © 2013 IEEE.","Complex networks; Embeddings; Noise generators; Textures; Adversarial networks; global generator; High resolution image; Image generations; instance mask embedding; Local refinement; Low resolution images; Visual attributes; Image processing","attribute-adaptive discriminator; Generative adversarial network; global generator; instance mask embedding; local refinement generator","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081596780"
"Eghbal-Zadeh H.; Fischer L.; Hoch T.","Eghbal-Zadeh, Hamid (37080509500); Fischer, Lukas (36339154700); Hoch, Thomas (56964543000)","37080509500; 36339154700; 56964543000","On conditioning gans to hierarchical ontologies","2019","Communications in Computer and Information Science","1062","","","182","186","4","10.1007/978-3-030-27684-3_23","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071890537&doi=10.1007%2f978-3-030-27684-3_23&partnerID=40&md5=3e1f9d07e60594e3212ff61dbd5dc041","The recent success of Generative Adversarial Networks (GAN) is a result of their ability to generate high quality images given samples from a latent space. One of the applications of GANs is to generate images from a text description, where the text is first encoded and further used for the conditioning in the generative model. In addition to text, conditional generative models often use label information for conditioning. Hence, the structure of the meta-data and the ontology of the labels is important for such models. In this paper, we propose Ontology Generative Adversarial Networks (O-GANs) to handle the complexities of the data with label ontology. We evaluate our model on a dataset of fashion images with hierarchical label structure. Our results suggest that the incorporation of the ontology, leads to better image quality as measured by Fréchet Inception Distance and Inception Score. Additionally, we show that the O-GAN better matches the generated images to their conditioning text, compared to models that do not incorporate the label ontology. © Springer Nature Switzerland AG 2019.","Deep learning; Adversarial networks; Generative model; High quality images; Image synthesis; Label information; Ontology","Generative Adversarial Networks; Ontology-driven deep learning; Text-to-image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85071890537"
"Tan H.; Liu X.; Li X.; Zhang Y.; Yin B.","Tan, Hongchen (57209272265); Liu, Xiuping (36910875600); Li, Xin (57201432287); Zhang, Yi (57214254174); Yin, Baocai (8616230700)","57209272265; 36910875600; 57201432287; 57214254174; 8616230700","Semantics-enhanced adversarial nets for text-to-image synthesis","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9010053","10500","10509","9","10.1109/ICCV.2019.01060","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081898787&doi=10.1109%2fICCV.2019.01060&partnerID=40&md5=238a87de2ffd9c3467acd718feba47b9","This paper presents a new model, Semantics-enhanced Generative Adversarial Network (SEGAN), for fine-grained text-to-image generation. We introduce two modules, a Semantic Consistency Module (SCM) and an Attention Competition Module (ACM), to our SEGAN. The SCM incorporates image-level semantic consistency into the training of the Generative Adversarial Network (GAN), and can diversify the generated images and improve their structural coherence. A Siamese network and two types of semantic similarities are designed to map the synthesized image and the groundtruth image to nearby points in the latent semantic feature space. The ACM constructs adaptive attention weights to differentiate keywords from unimportant words, and improves the stability and accuracy of SEGAN. Extensive experiments demonstrate that our SEGAN significantly outperforms existing state-of-the-art methods in generating photo-realistic images. All source codes and models will be released for comparative study. © 2019 IEEE.","Computer vision; Semantics; Adversarial networks; Comparative studies; Photorealistic images; Semantic consistency; Semantic similarity; State-of-the-art methods; Structural coherence; Synthesized images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85081898787"
"Qin Y.; Zheng H.; Huang X.; Yang J.; Zhu Y.-M.","Qin, Yulei (57204019819); Zheng, Hao (57202279922); Huang, Xiaolin (7410247356); Yang, Jie (15039078800); Zhu, Yue-Min (55629946300)","57204019819; 57202279922; 7410247356; 15039078800; 55629946300","Pulmonary nodule segmentation with CT sample synthesis using adversarial networks","2019","Medical Physics","46","3","","1218","1229","11","10.1002/mp.13349","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060908978&doi=10.1002%2fmp.13349&partnerID=40&md5=af2b0dd4ae5c109cc4c99e5e21a38851","Purpose: Segmentation of pulmonary nodules is critical for the analysis of nodules and lung cancer diagnosis. We present a novel framework of segmentation for various types of nodules using convolutional neural networks (CNNs). Methods: The proposed framework is composed of two major parts. The first part is to increase the variety of samples and build a more balanced dataset. A conditional generative adversarial network (cGAN) is employed to produce synthetic CT images. Semantic labels are generated to impart spatial contextual knowledge to the network. Nine attribute scoring labels are combined as well to preserve nodule features. To refine the realism of synthesized samples, reconstruction error loss is introduced into cGAN. The second part is to train a nodule segmentation network on the extended dataset. We build a three-dimensional (3D) CNN model that exploits heterogeneous maps including edge maps and local binary pattern maps. The incorporation of these maps informs the model of texture patterns and boundary information of nodules, which assists high-level feature learning for segmentation. Residual unit, which learns to reduce residual error, is adopted to accelerate training and improve accuracy. Results: Validation on LIDC-IDRI dataset demonstrates that the generated samples are realistic. The mean squared error and average cosine similarity between real and synthesized samples are 1.55 × 10-2 and 0.9534, respectively. The Dice coefficient, positive predicted value, sensitivity, and accuracy are, respectively, 0.8483, 0.8895, 0.8511, and 0.9904 for the segmentation results. Conclusions: The proposed 3D CNN segmentation framework, based on the use of synthesized samples and multiple maps with residual learning, achieves more accurate nodule segmentation compared to existing state-of-the-art methods. The proposed CT image synthesis method can not only output samples close to real images but also allow for stochastic variation in image diversity. © 2018 American Association of Physicists in Medicine","Databases, Factual; Diagnosis, Computer-Assisted; Humans; Image Processing, Computer-Assisted; Multiple Pulmonary Nodules; Neural Networks (Computer); Tomography, X-Ray Computed; Computer aided diagnosis; Convolution; Convolutional neural networks; Errors; Learning systems; Mean square error; Positron emission tomography; Semantics; Stochastic systems; Textures; Adversarial networks; Boundary information; Contextual knowledge; Local binary patterns; Lung cancer diagnosis; Pulmonary nodules; State-of-the-art methods; Threedimensional (3-d); analytical error; Article; clinical effectiveness; clinical evaluation; clinical feature; computer assisted tomography; convolutional neural network; correlation coefficient; diagnostic accuracy; human; image analysis; image segmentation; learning; lung nodule; medical education; nerve cell network; sensitivity analysis; three dimensional imaging; validation process; artificial neural network; computer assisted diagnosis; diagnostic imaging; factual database; image processing; multiple pulmonary nodules; pathology; procedures; x-ray computed tomography; Computerized tomography","computer-aided diagnosis; convolutional neural networks; generative adversarial networks; pulmonary nodule segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85060908978"
"Doman K.; Konishi T.; Mekada Y.","Doman, Keisuke (35931846700); Konishi, Takaaki (57214774765); Mekada, Yoshito (6602322161)","35931846700; 57214774765; 6602322161","Lesion Image Synthesis Using DCGANs for Metastatic Liver Cancer Detection","2020","Advances in Experimental Medicine and Biology","1213","","","95","106","11","10.1007/978-3-030-33128-3_6","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079084295&doi=10.1007%2f978-3-030-33128-3_6&partnerID=40&md5=de6e1713fcf1e568e0f120f457630791","This chapter proposes a method to detect metastatic liver cancer from X-ray CT images using a convolutional neural network (CNN). The proposed method generates various lesion images by the combination of three kinds of generation methods: (1) synthesis using Poisson Blending, (2) generation based on CT value distributions, and (3) generation using deep convolutional generative adversarial networks (DCGANs). The proposed method constructs two kinds of detectors by using synthetic (fake) lesion images generated by the methods as well as real ones. One of the detectors is a 2D CNN for detecting candidate regions in a CT image, and the other is a 3D CNN for validating the candidate regions. Experimental results showed that the proposed method gave 0.30 improvement from 0.65 to 0.95 in terms of the detection rate, and 0.70 improvement from 0.90 to 0.20 in terms of the number of false detections per case. From the results, we confirmed the effectiveness of the proposed method. © 2020, Springer Nature Switzerland AG.","Deep Learning; Humans; Image Processing, Computer-Assisted; Liver Neoplasms; Tomography, X-Ray Computed; cancer diagnosis; clinical effectiveness; computer assisted tomography; convolutional neural network; human; image analysis; liver metastasis; machine learning; major clinical study; optical resolution; priority journal; simulation; validation process; diagnostic imaging; image processing; liver tumor; x-ray computed tomography","Cancer detection; Cancer diagnosis; CNN; CT image; DCGAN; Lesion image synthesis; Metastatic liver cancer; Poisson Blending","Book chapter","Final","","Scopus","2-s2.0-85079084295"
"Zhang Y.; Zhang Z.; Yu W.; Jiang N.","Zhang, Yunye (57211359203); Zhang, Zhiqiang (57206280843); Yu, Wenxin (36610960300); Jiang, Ning (57212426361)","57211359203; 57206280843; 36610960300; 57212426361","Cscore: A novel no-reference evaluation metric for generated images","2019","ACM International Conference Proceeding Series","","","","277","281","4","10.1145/3373509.3373546","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082674853&doi=10.1145%2f3373509.3373546&partnerID=40&md5=4e8a2827f2630d76d948b740cb219458","The development of deep learning advances the field of image processing. In recent years, lots of methods have made outstanding achievements in the domain of text-to-image synthesis, like Generative Adversarial Networks (GANs). Until now, although some evaluation metrics has been proposed to measure the performance of GANs in text-to-image synthesis, the quality of these evaluation metrics has always been controversial. At present, there is no widely used evaluation metric to judge the quality of generated image. In this paper, a novel No-Reference image quality evaluation metric is proposed, which can be used to get a score for each generated image produced by deep learning without referring to the real image. This evaluation metric can provide a new way to verify the quality of complex networks by judging the quality of generated images retroactively. © 2019 Association for Computing Machinery.","Complex networks; Deep learning; Pattern recognition; Quality control; Adversarial networks; Evaluation metrics; Human version; Image synthesis; No references; No-reference images; Quality assessment; Real images; Image quality","Evaluation metrics; Generative adversarial networks; Human version.; No-reference quality assessment; Parameters fusion","Conference paper","Final","","Scopus","2-s2.0-85082674853"
"Bejiga M.B.; Melgani F.; Vascotto A.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300); Vascotto, Antonio (57208124510)","57192697078; 35613488300; 57208124510","Retro-Remote Sensing: Generating Images from Ancient Texts","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","3","8660422","950","960","10","10.1109/JSTARS.2019.2895693","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063894914&doi=10.1109%2fJSTARS.2019.2895693&partnerID=40&md5=300d238b0b3891057325948999c8e038","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area. © 2008-2012 IEEE.","Deep learning; Deep neural networks; Image processing; Neural networks; Adversarial networks; Convolutional neural network; Geographical area; Image synthesis; Multi-modal learning; Multimodal sources; Remote sensing images; Statistical properties; artificial neural network; civilization; geographical variation; image analysis; learning; pixel; remote sensing; Remote sensing","Convolutional neural networks (CNN); deep learning; generative adversarial networks (GAN); multimodal learning; remote sensing; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85063894914"
"Jain P.; Jayaswal T.","Jain, Pranjal (57218828223); Jayaswal, Tanmay (57218827002)","57218828223; 57218827002","Generating images using descriptive captions via adversarial training","2020","Journal of Critical Reviews","7","8","","1684","1689","5","10.31838/jcr.07.08.328","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090338914&doi=10.31838%2fjcr.07.08.328&partnerID=40&md5=53543798498999adca1b3534eb233bf2","We present an approach to tackle one of the most popular problems in the deep learning community of Text to Image synthesis by using the highly effective generative adversarial networks (GANs). We modified the original GAN architecture to take inputs of descriptive caption and render images that are visualized form of the caption. The architecture utilizes multiple stages of deep attention networks to get high detailing for the various sub-regions of the image based on the usage of words and its association with other parts of the image. The system also uses a multistage communication methodology that helps in increasing structural coherence and render better word association. Our approach trains on and experiments on the Caltech USCD Birds-200 dataset and gets an inception score of 4.21 ± 0.05 with faster training and execution time. © 2020 by Advance Scientific Research.","","Generative adversarial networks; Text to image synthesis","Article","Final","","Scopus","2-s2.0-85090338914"
"Li T.; Zhang S.; Xia J.","Li, Tong (57221125303); Zhang, Shibin (55788282300); Xia, Jinyue (57209545549)","57221125303; 55788282300; 57209545549","Quantum generative adversarial network: A survey","2020","Computers, Materials and Continua","64","1","","401","438","37","10.32604/CMC.2020.010551","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090893167&doi=10.32604%2fCMC.2020.010551&partnerID=40&md5=8eaaf6f60994b9bc3021b56a6208f7c5","Generative adversarial network (GAN) is one of the most promising methods for unsupervised learning in recent years. GAN works via adversarial training concept and has shown excellent performance in the fields image synthesis, image super-resolution, video generation, image translation, etc. Compared with classical algorithms, quantum algorithms have their unique advantages in dealing with complex tasks, quantum machine learning (QML) is one of the most promising quantum algorithms with the rapid development of quantum technology. Specifically, Quantum generative adversarial network (QGAN) has shown the potential exponential quantum speedups in terms of performance. Meanwhile, QGAN also exhibits some problems, such as barren plateaus, unstable gradient, model collapse, absent complete scientific evaluation system, etc. How to improve the theory of QGAN and apply it that have attracted some researcher. In this paper, we comprehensively and deeply review recently proposed GAN and QAGN models and their applications, and we discuss the existing problems and future research trends of QGAN. © 2020 Tech Science Press. All rights reserved.","Quantum theory; Adversarial networks; Existing problems; Image super resolutions; Image translation; Quantum algorithms; Quantum machines; Quantum technologies; Scientific evaluations; Machine learning","Generative adversarial network; Mode collapse; Quantum generative adversarial network; Quantum machine learning","Review","Final","","Scopus","2-s2.0-85090893167"
"Pan Y.; Liu M.; Lian C.; Xia Y.; Shen D.","Pan, Yongsheng (56440550200); Liu, Mingxia (36677833300); Lian, Chunfeng (56517715300); Xia, Yong (26427407400); Shen, Dinggang (7401738392)","56440550200; 36677833300; 56517715300; 26427407400; 7401738392","Disease-Image Specific Generative Adversarial Network for Brain Disease Diagnosis with Incomplete Multi-modal Neuroimages","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11766 LNCS","","","137","145","8","10.1007/978-3-030-32248-9_16","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075697955&doi=10.1007%2f978-3-030-32248-9_16&partnerID=40&md5=6a7abd8a99f5d3f6e5aa2d983262d134","Incomplete data problem is unavoidable in automated brain disease diagnosis using multi-modal neuroimages (e.g., MRI and PET). To utilize all available subjects to train diagnostic models, deep networks have been proposed to directly impute missing neuroimages by treating all voxels in a 3D volume equally. These methods are not diagnosis-oriented, as they ignore the disease-image specific information conveyed in multi-modal neuroimages, i.e., (1) disease may cause abnormalities only at local brain regions, and (2) different modalities may highlight different disease-associated regions. In this paper, we propose a unified disease-image specific deep learning framework for joint image synthesis and disease diagnosis using incomplete multi-modal neuroimaging data. Specifically, by using the whole-brain images as input, we design a disease-image specific neural network (DSNN) to implicitly model disease-image specificity in MRI/PET scans using the spatial cosine kernel. Moreover, we develop a feature-consistent generative adversarial network (FGAN) to synthesize missing images, encouraging DSNN feature maps of synthetic images and their respective real images to be consistent. Our DSNN and FGAN can be jointly trained, by which missing images are imputed in a task-oriented manner for brain disease diagnosis. Experimental results on 1, 466 subjects suggest that our method not only generates reasonable neuroimages, but also achieves the state-of-the-art performance in both tasks of Alzheimer’s disease (AD) identification and mild cognitive impairment (MCI) conversion prediction. © 2019, Springer Nature Switzerland AG.","Brain; Brain mapping; Deep learning; Magnetic resonance imaging; Medical computing; Adversarial networks; Diagnostic model; Disease diagnosis; Learning frameworks; Mild cognitive impairments (MCI); Specific information; State-of-the-art performance; Synthetic images; Diagnosis","","Conference paper","Final","","Scopus","2-s2.0-85075697955"
"Engelhardt S.; Sharan L.; Karck M.; Simone R.D.; Wolf I.","Engelhardt, Sandy (56200795700); Sharan, Lalith (57212008360); Karck, Matthias (7006019086); Simone, Raffaele De (55750307900); Wolf, Ivo (7004536808)","56200795700; 57212008360; 7006019086; 55750307900; 7004536808","Cross-Domain Conditional Generative Adversarial Networks for Stereoscopic Hyperrealism in Surgical Training","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11768 LNCS","","","155","163","8","10.1007/978-3-030-32254-0_18","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075655210&doi=10.1007%2f978-3-030-32254-0_18&partnerID=40&md5=78f4a43fad2d45b3c6d8258ee6386106","Phantoms for surgical training are able to mimic cutting and suturing properties and patient-individual shape of organs, but lack a realistic visual appearance that captures the heterogeneity of surgical scenes. In order to overcome this in endoscopic approaches, hyperrealistic concepts have been proposed to be used in an augmented reality-setting, which are based on deep image-to-image transformation methods. Such concepts are able to generate realistic representations of phantoms learned from real intraoperative endoscopic sequences. Conditioned on frames from the surgical training process, the learned models are able to generate impressive results by transforming unrealistic parts of the image (e.g. the uniform phantom texture is replaced by the more heterogeneous texture of the tissue). Image-to-image synthesis usually learns a mapping such that the distribution of images from G(X) is indistinguishable from the distribution Y. However, it does not necessarily force the generated images to be consistent and without artifacts. In the endoscopic image domain this can affect depth cues and stereo consistency of a stereo image pair, which ultimately impairs surgical vision. We propose a cross-domain conditional generative adversarial network approach (GAN) that aims to generate more consistent stereo pairs. The results show substantial improvements in depth perception and realism evaluated by 3 domain experts and 3 medical students on a 3D monitor over the baseline method. In 84 of 90 instances our proposed method was preferred or rated equal to the baseline. © 2019, Springer Nature Switzerland AG.","Augmented reality; Depth perception; Endoscopy; Laparoscopy; Medical computing; Phantoms; Stereo image processing; Stereo vision; Surgery; Textures; Adversarial networks; Baseline methods; Image transformations; Minimally invasive surgical; Mitral valves; Stereo image pairs; Surgical training; Visual appearance; Medical imaging","Augmented reality; Generative adversarial networks; Laparoscopy; Minimally-invasive surgical training; Mitral valve simulator","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075655210"
"Yang B.; Chen X.; Hong R.; Chen Z.; Li Y.; Zha Z.-J.","Yang, Binxin (57214243551); Chen, Xuejin (36092503500); Hong, Richang (14010456800); Chen, Zihan (57214246627); Li, Yuhang (57195525820); Zha, Zheng-Jun (36626639900)","57214243551; 36092503500; 14010456800; 57214246627; 57195525820; 36626639900","Joint sketch-attribute learning for fine-grained face synthesis","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11961 LNCS","","","790","801","11","10.1007/978-3-030-37731-1_64","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078529058&doi=10.1007%2f978-3-030-37731-1_64&partnerID=40&md5=3b69954c64d5f6e2ed0f408ec7362f60","The photorealism of synthetic face images has been significantly improved by generative adversarial networks (GANs). Besides of the realism, more accurate control on the properties of face images. While sketches convey the desired shapes, attributes describe appearance. However, it remains challenging to jointly exploit sketches and attributes, which are in different modalities, to generate high-resolution photorealistic face images. In this paper, we propose a novel joint sketch-attribute learning approach to synthesize photo-realistic face images with conditional GANs. A hybrid generator is proposed to learn a unified embedding of shape from sketches and appearance from attributes for synthesizing images. We propose an attribute modulation module, which transfers user-preferred attributes to reinforce sketch representation with appearance details. Using the proposed approach, users could flexibly manipulate the desired shape and appearance of synthesized face images with fine-grained control. We conducted extensive experiments on the CelebA-HQ dataset [16]. The experimental results have demonstrated the effectiveness of the proposed approach. © 2020, Springer Nature Switzerland AG.","Artificial intelligence; Computer science; Computers; Conditional GANs; Face attributes; Image synthesis; Joint learning; Sketch; Image enhancement","Conditional GANs; Face attributes; Image synthesis; Joint learning; Sketch","Conference paper","Final","","Scopus","2-s2.0-85078529058"
"Kurupathi S.R.; Murthy P.; Stricker D.","Kurupathi, Sheela Raju (57208385923); Murthy, Pramod (57204393975); Stricker, Didier (57479589400)","57208385923; 57204393975; 57479589400","Generation of human images with clothing using advanced conditional generative adversarial networks","2020","DeLTA 2020 - Proceedings of the 1st International Conference on Deep Learning Theory and Applications","","","","30","41","11","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092361543&partnerID=40&md5=5416c6e57c58b89bf48636f9abcc3772","One of the main challenges of human-image generation is generating a person along with pose and clothing details. However, it is still a difficult task due to challenging backgrounds and appearance variance. Recently, various deep learning models like Stacked Hourglass networks, Variational Auto Encoders (VAE), and Generative Adversarial Networks (GANs) have been used to solve this problem. However, still, they do not generalize well to the real-world human-image generation task qualitatively. The main goal is to use the Spectral Normalization (SN) technique for training GAN to synthesize the human-image along with the perfect pose and appearance details of the person. In this paper, we have investigated how Conditional GANs, along with Spectral Normalization (SN), could synthesize the new image of the target person given the image of the person and the target (novel) pose desired. The model uses 2D keypoints to represent human poses. We also use adversarial hinge loss and present an ablation study. The proposed model variants have generated promising results on both the Market-1501 and DeepFashion Datasets. We supported our claims by benchmarking the proposed model with recent state-of-the-art models. Finally, we show how the Spectral Normalization (SN) technique influences the process of human-image synthesis. © 2020 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved","Deep learning; Learning systems; Adversarial networks; Auto encoders; Human pose; Image generations; Image synthesis; Learning models; Recent state; Spectral normalization; Image processing","Conditional GANs; DeepFashion; Human Pose; Market-1501","Conference paper","Final","","Scopus","2-s2.0-85092361543"
"Ren Y.; Zhu Z.; Li Y.; Kong D.; Hou R.; Grimm L.J.; Marks J.R.; Lo J.Y.","Ren, Yinhao (57201856085); Zhu, Zhe (55932104300); Li, Yingzhou (56727682200); Kong, Dehan (57209574307); Hou, Rui (57206472176); Grimm, Lars J. (35423860500); Marks, Jeffery R. (7402796968); Lo, Joseph Y. (7201650993)","57201856085; 55932104300; 56727682200; 57209574307; 57206472176; 35423860500; 7402796968; 7201650993","Mask Embedding for Realistic High-Resolution Medical Image Synthesis","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11769 LNCS","","","422","430","8","10.1007/978-3-030-32226-7_47","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075839169&doi=10.1007%2f978-3-030-32226-7_47&partnerID=40&md5=38fb6a3edea432b1d4b22aaf0aa10bb5","Generative Adversarial Networks (GANs) have found applications in natural image synthesis and begin to show promises generating synthetic medical images. In many cases, the ability to perform controlled image synthesis using masked priors such as shape and size of organs is desired. However, mask-guided image synthesis is challenging due to the pixel level mask constraint. While the few existing mask-guided image generation approaches suffer from the lack of fine-grained texture details, we tackle the issue of mask-guided stochastic image synthesis via mask embedding. Our novel architecture first encodes the input mask as an embedding vector and then inject these embedding into the random latent vector input. The intuition is to classify semantic masks into partitions before feature up-sampling for improved sample space mapping stability. We validate our approach on a large dataset containing 39,778 patients with 443,556 negative screening Full Field Digital Mammography (FFDM) images. Experimental results show that our approach can generate realistic high-resolution (256 × 512 ) images with pixel-level mask constraints, and outperform other state-of-the-art approaches. © 2019, Springer Nature Switzerland AG.","Embeddings; Large dataset; Mammography; Medical computing; Pixels; Semantics; Stochastic systems; Textures; Adversarial networks; Full field digital mammography; High resolution; Image synthesis; Mammogram; Mask constraints; Novel architecture; State-of-the-art approach; Medical imaging","Generative Adversarial Networks; Image synthesis; Mammogram; Mask embedding","Conference paper","Final","","Scopus","2-s2.0-85075839169"
"Zuo L.; Dewey B.E.; Carass A.; He Y.; Shao M.; Reinhold J.C.; Prince J.L.","Zuo, Lianrui (57202855256); Dewey, Blake E. (56963777300); Carass, Aaron (15061054500); He, Yufan (57195739376); Shao, Muhan (57195642140); Reinhold, Jacob C. (57194467370); Prince, Jerry L. (56600943200)","57202855256; 56963777300; 15061054500; 57195739376; 57195642140; 57194467370; 56600943200","Synthesizing realistic brain mr images with noise control","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12417 LNCS","","","21","31","10","10.1007/978-3-030-59520-3_3","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092184593&doi=10.1007%2f978-3-030-59520-3_3&partnerID=40&md5=2b826d397ec1c12605a5746704f384df","Image synthesis in magnetic resonance (MR) imaging has been an active area of research for more than ten years. MR image synthesis can be used to create images that were not acquired or replace images that are corrupted by artifacts, which can be of great benefit in automatic image analysis. Although synthetic images have been used with success in many applications, it is quite often true that they do not look like real images. In practice, an expert can usually distinguish synthetic images from real ones. Generative adversarial networks (GANs) have significantly improved the realism of synthetic images. However, we argue that further improvements can be made through the introduction of noise in the synthesis process, which better models the actual imaging process. Accordingly, we propose a novel approach that incorporates randomness into the model in order to better approximate the distribution of real MR images. Results show that the proposed method has comparable accuracy with the state-of-the-art approaches as measured by multiple similarity measurements while also being able to control the noise level in synthetic images. To further demonstrate the superiority of this model, we present results from a human observer study on synthetic images, which shows that our results capture the essential features of real MR images. © 2020, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.","Image acquisition; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Adversarial networks; Automatic image analysis; Brain MR images; Essential features; Similarity measurements; State-of-the-art approach; Synthesis process; Synthetic images; Image enhancement","Deep learning; MRI; Noise control; Randomness; Synthesis","Conference paper","Final","","Scopus","2-s2.0-85092184593"
"Xu T.; Wang Z.","Xu, Tianyu (57212881026); Wang, Zhi (55913248200)","57212881026; 55913248200","Text-to-image synthesis optimization based on aesthetic assessment; [基于美学评判的文本生成图像优化]","2019","Beijing Hangkong Hangtian Daxue Xuebao/Journal of Beijing University of Aeronautics and Astronautics","45","12","","2438","2448","10","10.13700/j.bh.1001-5965.2019.0366","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077450458&doi=10.13700%2fj.bh.1001-5965.2019.0366&partnerID=40&md5=8c1c7c8fd2326fe438ce30f592b5930c","Due to the development of generative adversarial network (GAN), much progress has been achieved in the research of text-to-image synthesis. However, most of the researches focus on improving the stability and resolution of generated images rather than aesthetic quality. On the other hand, image aesthetic assessment research is also a classic task in computer vision field, and currently there exists several state-of-the-art models on image aesthetic assessment. In this work, we propose to improve the aesthetic quality of images generated by text-to-image GAN by incorporating an image aesthetic assessment model into a conditional GAN. We choose StackGAN++, a state-of-the-art text-to-image synthesis model, assess the aesthetic quality of images generated by it with a chosen aesthetic assessment model, then define a new loss function: aesthetic loss, and use it to improve StackGAN++. Compared with the original model, the total aesthetic score of generated images is improved by 3.17% and the inception score is improved by 2.68%, indicating that the proposed optimization is effective but still has several weaknesses that can be improved in future work. © 2019, Editorial Board of JBUAA. All right reserved.","Image enhancement; Adversarial networks; Aesthetic assessment; Aesthetic qualities; Assessment models; Image Aesthetics; Image synthesis; StackGAN; State of the art; Image quality","Aesthetic assessment; Aesthetic loss; Generative adversarial networks (GAN); StackGAN++; Text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85077450458"
"Ngxande M.; Tapamo J.-R.; Burke M.","Ngxande, Mkhuseli (57202813365); Tapamo, Jules-Raymond (55916165800); Burke, Michael (36099859700)","57202813365; 55916165800; 36099859700","DepthwiseGANs: Fast Training Generative Adversarial Networks for Realistic Image Synthesis","2019","Proceedings - 2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa, SAUPEC/RobMech/PRASA 2019","","","8704766","111","116","5","10.1109/RoboMech.2019.8704766","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065815116&doi=10.1109%2fRoboMech.2019.8704766&partnerID=40&md5=082f0d0256b2a79c22a766db249fdfb8","Recent work has shown significant progress in the direction of synthetic data generation using Generative Adversarial Networks (GANs). GANs have been applied in many fields of computer vision including text-to-image conversion, domain transfer, super-resolution, and image-to-video applications. In computer vision, traditional GANs are based on deep convolutional neural networks. However, deep convolutional neural networks can require extensive computational resources because they are based on multiple operations performed by convolutional layers, which can consist of millions of trainable parameters. Training a GAN model can be difficult and it takes a significant amount of time to reach an equilibrium point In this paper, we investigate the use of depthwise separable convolutions to reduce training time while maintaining data generation performance. Our results show that a DepthwiseGAN architecture can generate realistic images in shorter training periods when compared to a StarGan architecture, but that model capacity still plays a significant role in generative modelling. In addition, we show that depthwise separable convolutions perform best when only applied to the generator. For quality evaluation of generated images, we use the Fréchet Inception Distance (FID), which compares the similarity between the generated image distribution and that of the training dataset. © 2019 IEEE.","Computer vision; Convolution; Deep neural networks; Neural networks; Adversarial networks; Computational resources; Convolutional neural network; GANs; Multiple operations; Realistic image synthesis; Synthetic data; Synthetic data generations; Network architecture","Depthwise Separable Convolution; FID; GANs; Synthetic Data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065815116"
"Gupta P.; Shukla S.","Gupta, Param (57216417295); Shukla, Shipra (57210015477)","57216417295; 57210015477","Image Synthesis Using Machine Learning Techniques","2020","Lecture Notes on Data Engineering and Communications Technologies","38","","","311","318","7","10.1007/978-3-030-34080-3_35","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083453038&doi=10.1007%2f978-3-030-34080-3_35&partnerID=40&md5=6e9b953d5849a42d65e8f72e2fcca250","Image synthesis is the generation of realistic images using a computer algorithm. This can be difficult and time-consuming. Image synthesis using machine learning aims to make this process easier and more accessible. The most prominent machine learning model for generating content is known as generative adversarial networks. This paper reviews and evaluates various generative model based on GANs. These various models are evaluated using inception score and Fréchet inception distance. These are common metrics for the evaluation of generative adversarial networks. © 2020, Springer Nature Switzerland AG.","Machine learning; Adversarial networks; Generative model; Image synthesis; Machine learning models; Machine learning techniques; Realistic images; Image processing","Generative adversarial networks; Image; Machine learning","Book chapter","Final","","Scopus","2-s2.0-85083453038"
"Yang M.; Zhao W.; Xu W.; Feng Y.; Zhao Z.; Chen X.; Lei K.","Yang, Min (56349712700); Zhao, Wei (57198598163); Xu, Wei (57212335167); Feng, Yabing (57226001234); Zhao, Zhou (55959624600); Chen, Xiaojun (55739099100); Lei, Kai (55440390400)","56349712700; 57198598163; 57212335167; 57226001234; 55959624600; 55739099100; 55440390400","Multitask learning for cross-domain image captioning","2019","IEEE Transactions on Multimedia","21","4","8457273","1047","1061","14","10.1109/TMM.2018.2869276","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053111731&doi=10.1109%2fTMM.2018.2869276&partnerID=40&md5=08ba38374d5abbe84429dc82471df348","Recent artificial intelligence research has witnessed great interest in automatically generating text descriptions of images, which are known as the image captioning task. Remarkable success has been achieved on domains where a large number of paired data in multimedia are available. Nevertheless, annotating sufficient data is labor-intensive and time-consuming, establishing significant barriers for adapting the image captioning systems to new domains. In this study, we introduc a novel Multitask Learning Algorithm for cross-Domain Image Captioning (MLADIC). MLADIC is a multitask system that simultaneously optimizes two coupled objectives via a dual learning mechanism: image captioning and text-To-image synthesis, with the hope that by leveraging the correlation of the two dual tasks, we are able to enhance the image captioning performance in the target domain. Concretely, the image captioning task is trained with an encoder-decoder model (i.e., CNN-LSTM) to generate textual descriptions of the input images. The image synthesis task employs the conditional generative adversarial network (C-GAN) to synthesize plausible images based on text descriptions. In C-GAN, a generative model G synthesizes plausible images given text descriptions, and a discriminative model.","Data structures; Image enhancement; Job analysis; Learning algorithms; Maximum likelihood estimation; Neural networks; Personnel training; Reinforcement learning; dual learning; Image captioning; Image generations; Image synthesis; Multitask learning; Task analysis; Training data; Long short-term memory","dual learning; image captioning; image synthesis; Multitask learning; reinforcement learning","Article","Final","","Scopus","2-s2.0-85053111731"
"Dong H.; Liang X.; Zhou C.; Lai H.; Zhu J.; Yin J.","Dong, Haoye (57208444380); Liang, Xiaodan (55926362100); Zhou, Chenxing (57210585933); Lai, Hanjiang (35867865300); Zhu, Jia (55355019800); Yin, Jian (35316639800)","57208444380; 55926362100; 57210585933; 35867865300; 55355019800; 35316639800","Part-preserving pose manipulation for person image synthesis","2019","Proceedings - IEEE International Conference on Multimedia and Expo","2019-July","","8785023","1234","1239","5","10.1109/ICME.2019.00215","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070958019&doi=10.1109%2fICME.2019.00215&partnerID=40&md5=97c25ab1a31548c380bf749847a01207","Manipulating person images under diverse poses, which transfers a person from one pose to another desired pose, is an interesting yet challenging task due to large non-rigid spatial deformation. Most existing works fail to preserve the fine-grained appearance consistency along with the pose changes due to the lack of explicit constraints and spatial modeling, leading to unrealistic results with severe artifacts. In this paper, we propose a novel Part-Preserving Generative Adversarial Network (PP-GAN) to achieve good manipulation quality by explicitly enforcing rich structure constraints over generative modeling. PP-GAN is proposed to decompose the challenging spatial transformation of the whole body into fine-grained part-level transformations, which are then integrated via human joint structure constraint. Given arbitrary poses, PP-GAN integrates human joint structure and region-level part cues as inputs to perform explicit generative modeling. Besides, we introduce a parsing-consistent loss to enforce semantic consistency among images with diverse poses, which guides the image synthesis from a semantic perspective. Extensive qualitative and quantitative evaluations on two benchmarks show that our PP-GAN significantly outperforms the state-of-the-art baselines in generating more realistic and plausible image synthesis results. PP-GAN successfully preserves part-level characteristics even for most challenging pose changes while prior works are easy to fail. © 2019 IEEE.","Semantics; Adversarial networks; Human parsing; Image synthesis; Quantitative evaluation; Semantic consistency; Spatial deformation; Spatial transformation; State of the art; Image processing","Generative adversarial network; Human parsing; Person image synthesis","Conference paper","Final","","Scopus","2-s2.0-85070958019"
"Liu X.; Yu A.; Wei X.; Pan Z.; Tang J.","Liu, Xiaoming (56912622400); Yu, Aihui (57207768907); Wei, Xiangkai (57207772641); Pan, Zhifang (55177247000); Tang, Jinshan (7404638208)","56912622400; 57207768907; 57207772641; 55177247000; 7404638208","Multimodal MR Image Synthesis Using Gradient Prior and Adversarial Learning","2020","IEEE Journal on Selected Topics in Signal Processing","","","","","","","10.1109/JSTSP.2020.3013418","34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089393455&doi=10.1109%2fJSTSP.2020.3013418&partnerID=40&md5=217e5989dcb4cef9f92ebf0d81a82694","In magnetic resonance imaging (MRI), several images can be obtained using different imaging settings (e.g. T1, T2, DWI, Flair). These images have similar anatomical structures but with different contrasts, which provide a wealth of information for diagnosis. However, the images under specific imaging settings may not be available due to the limitation of scanning time or corruption caused by noises. It is attractive to derive missing images with some settings from the available MR images. In this paper, we propose a novel end-to-end multi-setting MR image synthesis method. The proposed method is based on generative adversarial networks (GANs) - a deep learning model. In the proposed method, different MR images obtained by different settings are used as the inputs of a GANs and each image is encoded by an encoder. Each encoder includes a refinement structure which is used to extract a multi-scale feature map from an input image. The multi-scale feature maps from different input images are then fused to generate several desired target images under specific settings. Because the resultant images obtained with GANs have blur edges, we fuse gradient prior information in the model to protect high frequency information such as important tissue textures of medical images. In the proposed model, the multi-scale information is also adopted in the adversarial learning (not just in the generator or discriminator) so that we can produce high quality synthesized images. We evaluated the proposed method on two public datasets: BRATS and ISLES. Experimental results demonstrate that the proposed approach is superior to current state-of-the-art methods. IEEE","Deep learning; Diagnosis; Learning systems; Medical imaging; Signal encoding; Textures; Adversarial learning; Adversarial networks; Anatomical structures; High-frequency informations; Multi-scale features; Multi-scale informations; State-of-the-art methods; Wealth of information; Magnetic resonance imaging","Gallium nitride; Generative Adversarial Networks; gradient prior; Image generation; Image segmentation; image synthesis; Magnetic resonance imaging; magnetic resonance imaging; Medical diagnostic imaging","Article","Article in press","","Scopus","2-s2.0-85089393455"
"Tan W.R.; Chan C.S.; Aguirre H.E.; Tanaka K.","Tan, Wei Ren (57192559815); Chan, Chee Seng (57194450557); Aguirre, Hernan E. (6603898641); Tanaka, Kiyoshi (55430510400)","57192559815; 57194450557; 6603898641; 55430510400","Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork","2019","IEEE Transactions on Image Processing","28","1","8444471","394","409","15","10.1109/TIP.2018.2866698","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052685502&doi=10.1109%2fTIP.2018.2866698&partnerID=40&md5=2350c4a72b52faa686960183ad075108","This paper proposes a series of new approaches to improve generative adversarial network (GAN) for conditional image synthesis and we name the proposed model as 'ArtGAN. ' One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images on Oxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN. © 1992-2012 IEEE.","Deep learning; Adversarial networks; ArtGAN; Image synthesis; Label information; Natural images; New approaches; Novel strategies; State of the art; Image enhancement","ArtGAN; artwork synthesis; deep learning; Generative adversarial networks; image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85052685502"
"Liu L.; Xu W.; Zollhöfer M.; Kim H.; Bernard F.; Habermann M.; Wang W.; Theobalt C.","Liu, Lingjie (57195997876); Xu, Weipeng (57195993759); Zollhöfer, Michael (36245738500); Kim, Hyeongwoo (57193155212); Bernard, Florian (57203081952); Habermann, Marc (57207993529); Wang, Wenping (35147101600); Theobalt, Christian (6507027272)","57195997876; 57195993759; 36245738500; 57193155212; 57203081952; 57207993529; 35147101600; 6507027272","Neural rendering and reenactment of human actor videos","2019","ACM Transactions on Graphics","38","5","139","","","","10.1145/3333002","58","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074443058&doi=10.1145%2f3333002&partnerID=40&md5=fb63098c516777035f7b4e2f4b73e94f","We propose a method for generating video-realistic animations of real humans under user control. In contrast to conventional human character rendering, we do not require the availability of a production-quality photo-realistic three-dimensional (3D) model of the human but instead rely on a video sequence in conjunction with a (medium-quality) controllable 3D template model of the person. With that, our approach significantly reduces production cost compared to conventional rendering approaches based on production-quality 3D models and can also be used to realistically edit existing videos. Technically, this is achieved by training a neural network that translates simple synthetic images of a human character into realistic imagery. For training our networks, we first track the 3D motion of the person in the video using the template model and subsequently generate a synthetically rendered version of the video. These images are then used to train a conditional generative adversarial network that translates synthetic images of the 3D model into realistic imagery of the human. We evaluate our method for the reenactment of another person that is tracked to obtain the motion data, and show video results generated from artist-designed skeleton motion. Our results outperform the state of the art in learning-based human image synthesis. © 2019 Association for Computing Machinery.","3D modeling; Deep learning; Three dimensional computer graphics; 3D template-models; Adversarial networks; Conditional GAN; Neural rendering; Production quality; Rendering approach; Three dimensional (3-D) modeling; Video-based characters; Rendering (computer graphics)","Conditional GAN; Deep learning; Neural rendering; Rendering-to-video translation; Video-based characters","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85074443058"
"Li Y.; Li B.; Gao Z.; Wang J.","Li, Yuelong (36739342400); Li, Bowen (57221075246); Gao, Zengbin (57208223095); Wang, Jianming (56448807900)","36739342400; 57221075246; 57208223095; 56448807900","Antimode collapse generative adversarial networks","2019","Journal of Electronic Imaging","28","2","023020","","","","10.1117/1.JEI.28.2.023020","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064184101&doi=10.1117%2f1.JEI.28.2.023020&partnerID=40&md5=aec2a9f70fb8df50c54bc1809b169e60","Generative adversarial networks (GANs) are a class of techniques widely applied in image synthesis, recovery, compensation, and other related fields. We propose and introduce a novel, improved conditional model antimode collapse GANs (AMCGANs). Through a newly designed network architecture and optimizing strategy, the function of the class label information is moderately constrained and it no longer directly influences the discriminator, and hence the synthesized images belonging to the same classes will not be excessively concentrated due to the attraction of the same labels. Thus, the mode collapse problem, namely generating homogeneity, which always hinders image synthesization approaches can be effectively restrained. On the other hand, by sharing the feature extraction part and only updating its weights during the training of discriminator, AMCGANs achieves relatively efficient computing performance. In addition, it works well both for supervised and for semisupervised learning circumstances. Extensive experiments have been conducted on the Fashion-MNIST and CIFAR10 data sets to verify the effectiveness of the proposed approach. © 2019 SPIE and IS&T.","Constrained optimization; Network architecture; Supervised learning; Adversarial networks; Class label informations; Computing performance; Conditional models; Image synthesization; mode collapse; Semi- supervised learning; Synthesized images; Image processing","generative adversarial networks; image synthesization; mode collapse; semisupervised learning; supervised learning","Article","Final","","Scopus","2-s2.0-85064184101"
"Tian M.; Xue Y.; Tian C.; Wang L.; Deng D.; Wei W.","Tian, Ming (57209735455); Xue, Yuting (57212533229); Tian, Chunna (12762595600); Wang, Lei (57070633700); Deng, Donghu (55143320000); Wei, Wei (56421092200)","57209735455; 57212533229; 12762595600; 57070633700; 55143320000; 56421092200","SS-GANs: Text-to-image via stage by stage generative adversarial networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11858 LNCS","","","475","486","11","10.1007/978-3-030-31723-2_40","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076999853&doi=10.1007%2f978-3-030-31723-2_40&partnerID=40&md5=843c08dd1b1cf83920fc19287a169b3d","Realistic text-to-image synthesis has achieved great improvements in recent years. However, most work ignores the relationship between low and high resolution and prefers to adopt identical module in different stages. It is obviously inappropriate because the differences in various generation stages are huge. Therefore, we propose a novel structure of network named SS-GANs, in which specific modules are added in different stages to satisfy the unique requirements. In addition, we also explore an effective training way named coordinated train and a simple negative sample selection mechanism. Lastly, we train our model on Oxford-102 dataset, which outperforms the state-of-the-art models. © Springer Nature Switzerland AG 2019.","Computer vision; Adversarial networks; Coordinated train; Different stages; Image synthesis; Negative samples; Novel structures; State of the art; Text-to-image; Image enhancement","Coordinated train; Different stages; Negative samples; Text-to-image","Conference paper","Final","","Scopus","2-s2.0-85076999853"
"Chen J.; Konrad J.; Ishwar P.","Chen, Jiawei (57189458230); Konrad, Janusz (7102355164); Ishwar, Prakash (6602873044)","57189458230; 7102355164; 6602873044","VGAN-based image representation learning for privacy-preserving facial expression recognition","2018","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June","","8575368","1651","1660","9","10.1109/CVPRW.2018.00207","54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060882503&doi=10.1109%2fCVPRW.2018.00207&partnerID=40&md5=3082c6686ce5177ee7ac6842d3402e43","Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion. © 2018 IEEE.","Computer vision; Human computer interaction; Adversarial networks; Expression morphing; Face image synthesis; Facial expression analysis; Facial expression recognition; Human machine interaction; Identity information; Image representations; Face recognition","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85060882503"
"Chen S.S.-C.; Cui H.; Du M.-H.; Fu T.-M.; Sun X.-H.; Ji Y.; Duh H.","Chen, Steven Szu-Chi (57203642467); Cui, Hui (55321437000); Du, Ming-han (57203550575); Fu, Tie-ming (57210159811); Sun, Xiao-hong (57210162978); Ji, Yi (57190273757); Duh, Henry (6603986391)","57203642467; 55321437000; 57203550575; 57210159811; 57210162978; 57190273757; 6603986391","Cantonese porcelain classification and image synthesis by ensemble learning and generative adversarial network","2019","Frontiers of Information Technology and Electronic Engineering","20","12","","1632","1643","11","10.1631/FITEE.1900399","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079119784&doi=10.1631%2fFITEE.1900399&partnerID=40&md5=6c49358e7adf47fb240dc3f65984c93d","Accurate recognition of modern and traditional porcelain styles is a challenging issue in Cantonese porcelain management due to the large variety and complex elements and patterns. We propose a hybrid system with porcelain style identification and image recreation modules. In the identification module, prediction of an unknown porcelain sample is obtained by logistic regression of ensembled neural networks of top-ranked design signatures, which are obtained by discriminative analysis and transformed features in principal components. The synthesis module is developed based on a conditional generative adversarial network, which enables users to provide a designed mask with porcelain elements to generate synthesized images of Cantonese porcelain. Experimental results of 603 Cantonese porcelain images demonstrate that the proposed model outperforms other methods relative to precision, recall, area under curve of receiver operating characteristic, and confusion matrix. Case studies on image creation indicate that the proposed system has the potential to engage the community in understanding Cantonese porcelain and promote this intangible cultural heritage. © 2019, Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature.","Arts computing; Classification (of information); Hybrid systems; Image classification; Logistic regression; Principal component analysis; Adversarial networks; Cantonese; Creative arts; Intangible cultural heritages; Receiver operating characteristics; Style identifications; TP751; Traditional porcelains; Porcelain","Cantonese porcelain; Classification; Creative arts; Generative adversarial network; TP751","Article","Final","","Scopus","2-s2.0-85079119784"
"Hoshen Y.; Li K.; Malik J.","Hoshen, Yedid (55973097800); Li, Ke (57199453570); Malik, Jitendra (7101991704)","55973097800; 57199453570; 7101991704","Non-adversarial image synthesis with generative latent nearest neighbors","2019","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2019-June","","8953233","5804","5812","8","10.1109/CVPR.2019.00596","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078721871&doi=10.1109%2fCVPR.2019.00596&partnerID=40&md5=3245f375ec5718378a8d05b7962a2f77","Unconditional image generation has recently been dominated by generative adversarial networks (GANs). GAN methods train a generator which regresses images from random noise vectors, as well as a discriminator that attempts to differentiate between the generated images and a training set of real images. GANs have shown amazing results at generating realistic looking images. Despite their success, GANs suffer from critical drawbacks including: Unstable training and mode-dropping. The weaknesses in GANs have motivated research into alternatives including: Variational auto-encoders (VAEs), latent embedding learning methods (e.g. GLO) and nearest-neighbor based implicit maximum likelihood estimation (IMLE). Unfortunately at the moment, GANs still significantly outperform the alternative methods for image generation. In this work, we present a novel method-Generative Latent Nearest Neighbors (GLANN)-for training generative models without adversarial training. GLANN combines the strengths of IMLE and GLO in a way that overcomes the main drawbacks of each method. Consequently, GLANN generates images that are far better than GLO and IMLE. Our method does not suffer from mode collapse which plagues GAN training and is much more stable. Qualitative results show that GLANN outperforms a baseline consisting of 800 GANs and VAEs on commonly used datasets. Our models are also shown to be effective for training truly non-adversarial unsupervised image translation. © 2019 IEEE.","Computer vision; Embeddings; Maximum likelihood estimation; Adversarial networks; Generative model; Image generations; Image synthesis; Image translation; Learning methods; Nearest neighbors; Video synthesis; Learning systems","Image and Video Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078721871"
"Madsen S.L.; Dyrmann M.; Jørgensen R.N.; Karstoft H.","Madsen, Simon L. (57205578681); Dyrmann, Mads (56964314100); Jørgensen, Rasmus N. (35292305600); Karstoft, Henrik (36771565700)","57205578681; 56964314100; 35292305600; 36771565700","Generating artificial images of plant seedlings using generative adversarial networks","2019","Biosystems Engineering","187","","","147","159","12","10.1016/j.biosystemseng.2019.09.005","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072256511&doi=10.1016%2fj.biosystemseng.2019.09.005&partnerID=40&md5=9982c51ebdb62168092c67ffb2c9d26f","Plants seedlings are a part of a domain with low inter-class and relatively high intra-class variance with respect to visual appearance. This paper presents an approach for generating artificial image samples of plant seedlings using generative adversarial networks (GAN) to alleviate for the lack of training data for deep learning systems in this domain. We show that it is possible to use GAN to produce samples that are visually distinct across nine different plants species and maintain a high amount variance within each species. The generated samples resemble the intended species with an average recognition accuracy of 58.9±9.2%, evaluated using a state-of-the-art classification model. The observed errors are related to samples representing species which are relatively anonymous at the dicotyledonous growth stage and to the model's incapability to reproduce small shape details. The artificial plant samples are also used for pretraining a classification model, which is finetuned using real data. The pretrained model achieves 62.0±5.3% accuracy on classifying real plant seedlings prior to any finetuning, thus providing a strong basis for further training. However, finetuning the pretrained models show no performance increase compared to models trained without finetuning, as both approaches are capable of achieving near perfect classification on the dataset applied in this work. © 2019 IAgrE","Classification (of information); Deep learning; Adversarial networks; Classification models; Image synthesis; Intra class; Plant classification; Plant seedlings; Recognition accuracy; Without fine-tuning; Seed","GAN; Image synthesis; Low inter-class and high intra-class variance; Plant classification; Plant seedlings","Article","Final","","Scopus","2-s2.0-85072256511"
"Deshmukh A.; Sivaswamy J.","Deshmukh, Anurag (57211429600); Sivaswamy, Jayanthi (6602360103)","57211429600; 6602360103","Synthesis of optical nerve head region of fundus image","2019","Proceedings - International Symposium on Biomedical Imaging","2019-April","","8759414","583","586","3","10.1109/ISBI.2019.8759414","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073895516&doi=10.1109%2fISBI.2019.8759414&partnerID=40&md5=4b1d52a8dcbb169e182071ccab0824d5","The Optic Disc (OD) and Optic Cup (OC) boundaries play a critical role in the detection of glaucoma. However, very few annotated datasets are available for both OD and OC that are required for segmentation. Recently, Convolutional Neural Networks have shown significant improvements in segmentation performance. However, the full potential of CNNs is hindered by the lack of a large amount of annotated training images. To address this issue, we explore a method to generate synthetic images which can be used to augment the training data. Given the segmentation masks of OD, OC and vessels from arbitrarily different fundus images, the proposed method employs a combination of B-spline registration and GAN to generate high quality images that ensure that the vessels bend at the edge of the OC in a realistic manner. In contrast, the existing GAN based methods for fundus image synthesis fail to capture the local details and vasculature in the Optic Nerve Head (ONH) region. The utility of the proposed method in training deep networks for the challenging problem of OC segmentation is explored and an improvement in the dice score from 0.85 to 0.902 is seen with the inclusion of the synthetic images in the training set. © 2019 IEEE.","Eye protection; Image segmentation; Medical imaging; Neural networks; Ophthalmology; Adversarial networks; Annotated datasets; Convolutional neural network; Cup segmentations; High quality images; Segmentation masks; Segmentation performance; Synthetic images; Image enhancement","Cup segmentation; Generative adversarial networks; Synthetic images","Conference paper","Final","","Scopus","2-s2.0-85073895516"
"Yu B.; Zhou L.; Wang L.; Fripp J.; Bourgeat P.","Yu, Biting (57201496052); Zhou, Luping (23398846800); Wang, Lei (54958774700); Fripp, Jurgen (13605436500); Bourgeat, Pierrick (8576247900)","57201496052; 23398846800; 54958774700; 13605436500; 8576247900","3D cGAN based cross-modality MR image synthesis for brain tumor segmentation","2018","Proceedings - International Symposium on Biomedical Imaging","2018-April","","","626","630","4","10.1109/ISBI.2018.8363653","54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048105885&doi=10.1109%2fISBI.2018.8363653&partnerID=40&md5=a1d870167cabc988a548341d770448e9","Different modalities of magnetic resonance imaging (MRI) can indicate tumor-induced tissue changes from different perspectives, thus benefit brain tumor segmentation when they are considered together. Meanwhile, it is always interesting to examine the diagnosis potential from single modality, considering the cost of acquiring multi-modality images. Clinically, T1-weighted MRI is the most commonly used MR imaging modality, although it may not be the best option for contouring brain tumor. In this paper, we investigate whether synthesizing FLAIR images from T1 could help improve brain tumor segmentation from the single modality of T1. This is achieved by designing a 3D conditional Generative Adversarial Network (cGAN) for FLAIR image synthesis and a local adaptive fusion method to better depict the details of the synthesized FLAIR images. The proposed method can effectively handle the segmentation task of brain tumors that vary in appearance, size and location across samples. © 2018 IEEE.","Brain; Diagnosis; Image enhancement; Image fusion; Image segmentation; Medical imaging; Tumors; Adversarial networks; Brain tumor segmentation; Cross modality; Image synthesis; Local-adaptive; Magnetic Resonance Imaging (MRI); Multi modality image; Tissue changes; Magnetic resonance imaging","Brain tumor segmentation; Generative Adversarial Network; Image synthesis; Local adaptive fusion","Conference paper","Final","","Scopus","2-s2.0-85048105885"
"Luo G.; Zeng W.; Xie W.; Lei H.; Xian C.","Luo, Guoliang (55681696300); Zeng, Wei (57210536360); Xie, Wenqiang (57202451672); Lei, Haopeng (52063488600); Xian, Chuhua (24480202900)","55681696300; 57210536360; 57202451672; 52063488600; 24480202900","An image representation for the 3D face synthesis","2018","ACM International Conference Proceeding Series","","","","27","31","4","10.1145/3205326.3205351","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048404695&doi=10.1145%2f3205326.3205351&partnerID=40&md5=73a35fd4780495a5bfd4e0b006c55409","With the rapid development of the display technologies, 3D shape data is becoming another important media kind. However, most of the existing 3D shape acquisition methods are either expensive or expertise-dependent. In this paper, we present an image representation for the 3D faces to bridge the gap between the feature-lacking 3D shapes and the powerful deep neural network learning tools. To achieve this, with the training set, we first extract the radial curves for each 3D face, and reform the curves into an image matrix, which enable to apply the classical Generative Adversarial Network model for the image synthesis. Finally, we propose a refining process to transform the output images into 3D synthetic faces. Our experimental results demonstrate the capability of our method which can correctly reflect the affinities among the different facial expressions and can generate the 3D faces. © 2018 Copyright held by the owner/author(s).","Animation; Deep neural networks; 3-D face synthesis; 3D faces; Adversarial networks; Display technologies; Facial Expressions; Image representations; Neural network learning; Smoothing; Image processing","3D face; Generative adversarial network; Image representation; Smoothing","Conference paper","Final","","Scopus","2-s2.0-85048404695"
"Zhao T.; Wang Y.; Fu X.","Zhao, Tongtong (57192707963); Wang, Yafei (55211773900); Fu, Xianping (7402204912)","57192707963; 55211773900; 7402204912","Refining eye synthetic images via coarse-to-fine adversarial networks for appearance-based gaze estimation","2018","Communications in Computer and Information Science","819","","","419","428","9","10.1007/978-981-10-8530-7_41","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043366675&doi=10.1007%2f978-981-10-8530-7_41&partnerID=40&md5=4e6fb13f8556f72404391f6bc1a071b6","Recently, several models have achieved great success in terms of reducing the gap between synthetic and real image distributions with large unlabeled real data. However, collecting such large amounts of real data costs a lot of labouring and training them requires high memory. To reduce the gap with less real data, we propose a coarse-to-fine refine eye image method combining coarse model net and fine model net through adversarial training. Coarse model net is a feed-forward convolutional neural network aiming to transform synthetic eye images into coarse images. Fine model net is a modified Generative Adversarial Networks (GANs) which add realism to coarse images using unlabeled real data. Experimental results show that the proposed method achieves similar distributions as recent work but decreasing real data at least one order of magnitude. In addition, a significant accuracy improvement for gaze estimation with refined synthetic eye images is observed. © Springer Nature Singapore Pte Ltd. 2018.","Convolution; Neural networks; Accuracy Improvement; Adversarial networks; Appearance based; Coarse to fine; Convolutional neural network; Gaze estimation; Image synthesis; Synthetic images; Image enhancement","Feed-forward convolutional neural network; Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85043366675"
"Bi L.; Kim J.; Kumar A.; Feng D.; Fulham M.","Bi, Lei (55516428600); Kim, Jinman (55720292700); Kumar, Ashnil (57198890813); Feng, Dagan (7401981167); Fulham, Michael (7005082387)","55516428600; 55720292700; 57198890813; 7401981167; 7005082387","Synthesis of positron emission tomography (PET) images via multi-channel generative adversarial networks (GANs)","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10555 LNCS","","","43","51","8","10.1007/978-3-319-67564-0_5","87","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029582683&doi=10.1007%2f978-3-319-67564-0_5&partnerID=40&md5=c6257ae24808dbda58503dbf5206b58c","Positron emission tomography (PET) imaging is widely used for staging and monitoring treatment in a variety of cancers including the lymphomas and lung cancer. Recently, there has been a marked increase in the accuracy and robustness of machine learning methods and their application to computer-aided diagnosis (CAD) systems, e.g., the automated detection and quantification of abnormalities in medical images. Successful machine learning methods require large amounts of training data and hence, synthesis of PET images could play an important role in enhancing training data and ultimately improve the accuracy of PET-based CAD systems. Existing approaches such as atlas-based or methods that are based on simulated or physical phantoms have problems in synthesizing the low resolution and low signal-to-noise ratios inherent in PET images. In addition, these methods usually have limited capacity to produce a variety of synthetic PET images with large anatomical and functional differences. Hence, we propose a new method to synthesize PET data via multi-channel generative adversarial networks (M-GAN) to address these limitations. Our M-GAN approach, in contrast to the existing medical image synthetic methods that rely on using low-level features, has the ability to capture feature representations with a high-level of semantic information based on the adversarial learning concept. Our M-GAN is also able to take the input from the annotation (label) to synthesize regions of high uptake e.g., tumors and from the computed tomography (CT) images to constrain the appearance consistency based on the CT derived anatomical information in a single framework and output the synthetic PET images directly. Our experimental data from 50 lung cancer PET-CT studies show that our method provides more realistic PET images compared to conventional GAN methods. Further, the PET tumor detection model, trained with our synthetic PET data, performed competitively when compared to the detection model trained with real PET data (2.79% lower in terms of recall). We suggest that our approach when used in combination with real and synthetic images, boosts the training data for machine learning methods. © 2017, Springer International Publishing AG.","Artificial intelligence; Biological organs; Computer aided analysis; Computer aided diagnosis; Computer aided instruction; Diagnosis; Diseases; Electrons; Image processing; Image reconstruction; Learning systems; Medical imaging; Molecular imaging; Positron emission tomography; Positrons; Semantics; Signal to noise ratio; Tumors; Adversarial networks; Anatomical information; Computer Aided Diagnosis(CAD); Feature representation; Image synthesis; Low signal-to-noise ratio; Machine learning methods; Positron emission tomography (PET); Computerized tomography","Generative Adversarial Networks (GANs); Image synthesis; Positron Emission Tomography (PET)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85029582683"
"Spurr A.; Aksan E.; Hilliges O.","Spurr, Adrian (57200213697); Aksan, Emre (57073715000); Hilliges, Otmar (14041644100)","57200213697; 57073715000; 14041644100","Guiding InfoGAN with Semi-supervision","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10534 LNAI","","","119","134","15","10.1007/978-3-319-71249-9_8","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040258574&doi=10.1007%2f978-3-319-71249-9_8&partnerID=40&md5=bf9fd3daaed212604ed03957772a93d1","In this paper we propose a new semi-supervised GAN architecture (ss-InfoGAN) for image synthesis that leverages information from few labels (as little as 0.22%, max. 10% of the dataset) to learn semantically meaningful and controllable data representations where latent variables correspond to label categories. The architecture builds on Information Maximizing Generative Adversarial Networks (InfoGAN) and is shown to learn both continuous and categorical codes and achieves higher quality of synthetic samples compared to fully unsupervised settings. Furthermore, we show that using small amounts of labeled data speeds-up training convergence. The architecture maintains the ability to disentangle latent variables for which no labels are available. Finally, we contribute an information-theoretic reasoning on how introducing semi-supervision increases mutual information between synthetic and real data. Code related to this chapter is available at: https://github.com/spurra/ss-infogan. © 2017, Springer International Publishing AG.","Architecture; Artificial intelligence; Information theory; Learning systems; Adversarial networks; Data representations; Image synthesis; Latent variable; Mutual informations; Semi supervisions; Semi-supervised; Synthetic and real data; Network architecture","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85040258574"
"Zheng Z.; Wang C.; Yu Z.; Zheng H.; Zheng B.","Zheng, Ziqiang (57202612793); Wang, Chao (57207369410); Yu, Zhibin (36999020600); Zheng, Haiyong (24922273300); Zheng, Bing (35304360700)","57202612793; 57207369410; 36999020600; 24922273300; 35304360700","Instance Map Based Image Synthesis with a Denoising Generative Adversarial Network","2018","IEEE Access","6","","","33654","33665","11","10.1109/ACCESS.2018.2849108","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048897194&doi=10.1109%2fACCESS.2018.2849108&partnerID=40&md5=fa304382a1ea1f4fab54f08f3016eebb","Semantic layout-based image synthesizing, which has benefited from the success of generative adversarial networks (GANs), has received a substantial amount of attention recently. How to enhance the synthesis image equality while maintaining the stochasticity of the GAN remains a challenge. We propose a novel denoising framework to handle this problem. The generation of overlapping objects is another challenging task when synthesizing images from a semantic layout to a realistic RGB photograph. To overcome this deficiency, we include a one-hot semantic label map to force the generator to pay more attention to the generation of overlapping objects. Furthermore, we improve the loss function of the discriminator by considering the perturbed loss and cascade layer loss to guide the generation process. We applied our methods to the Cityscapes, photo-sketch, day-night, facades, and NYU datasets to demonstrate the image generation ability of our model. © 2013 IEEE.","Convolution; Gallium nitride; Gas generators; III-V semiconductors; Image processing; Job analysis; Neural networks; Semantics; Adversarial networks; Generation process; Image generations; Image synthesis; Layout; Semantic labels; Task analysis; Underwater technology; Image enhancement","artificial neural networks; image processing; Underwater technology","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048897194"
"Olut S.; Sahin Y.H.; Demir U.; Unal G.","Olut, Sahin (57212927145); Sahin, Yusuf H. (57195216061); Demir, Ugur (57810490700); Unal, Gozde (57220534209)","57212927145; 57195216061; 57810490700; 57220534209","Generative Adversarial Training for MRA Image Synthesis Using Multi-contrast MRI","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11121 LNCS","","","147","154","7","10.1007/978-3-030-00320-3_18","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145780179&doi=10.1007%2f978-3-030-00320-3_18&partnerID=40&md5=4ad3613cb8ab7aeddcb68a6df0b26e5c","Magnetic Resonance Angiography (MRA) has become an essential MR contrast for imaging and evaluation of vascular anatomy and related diseases. MRA acquisitions are typically ordered for vascular interventions, whereas in typical scenarios, MRA sequences can be absent in the patient scans. This motivates the need for a technique that generates inexistent MRA from existing MR multi-contrast, which could be a valuable tool in retrospective subject evaluations and imaging studies. We present a generative adversarial network (GAN) based technique to generate MRA from T1- and T2-weighted MRI images, for the first time to our knowledge. To better model the representation of vessels which the MRA inherently highlights, we design a loss term dedicated to a faithful reproduction of vascularities. To that end, we incorporate steerable filter responses of the generated and reference images as a loss term. Extending the well-established generator-discriminator architecture based on the recent PatchGAN model with the addition of steerable filter loss, the proposed steerable GAN (sGAN) method is evaluated on the large public database IXI. Experimental results show that the sGAN outperforms the baseline GAN method in terms of an overlap score with similar PSNR values, while it leads to improved visual perceptual quality. © Springer Nature Switzerland AG 2018.","Angiography; Cell proliferation; Magnetic resonance; Magnetic resonance imaging; Patient treatment; Angiography images; GAN; Images synthesis; Magnetic resonance Angiography; MR angiography; Network methods; Network-based; Steerable filters; Vascular anatomy; Vascular interventions; Generative adversarial networks","GANs; Image synthesis; MR angiography; Steerable filters","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85145780179"
"Wu X.; Xu K.; Hall P.","Wu, Xian (55607614200); Xu, Kun (56587223300); Hall, Peter (56274791100)","55607614200; 56587223300; 56274791100","A survey of image synthesis and editing with generative adversarial networks","2017","Tsinghua Science and Technology","22","6","8195348","660","674","14","10.23919/TST.2017.8195348","93","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039441654&doi=10.23919%2fTST.2017.8195348&partnerID=40&md5=f3330ae9b58c1f67d71e956a7d54351d","This paper presents a survey of image synthesis and editing with Generative Adversarial Networks (GANs). GANs consist of two deep networks, a generator and a discriminator, which are trained in a competitive way. Due to the power of deep networks and the competitive training manner, GANs are capable of producing reasonable and realistic images, and have shown great capability in many image synthesis and editing applications. This paper surveys recent GAN papers regarding topics including, but not limited to, texture synthesis, image inpainting, image-to-image translation, and image editing. © 1996-2012 Tsinghua University Press.","Surveys; Adversarial networks; Image editing; Image Inpainting; Image synthesis; Image translation; Paper surveys; Realistic images; Texture synthesis; Image processing","constrained image synthesis; generative adversarial networks; image editing; image synthesis; image-to-image translation","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85039441654"
"Guan S.; Loew M.","Guan, Shuyue (54904821700); Loew, Murray (7004977045)","54904821700; 7004977045","Breast cancer detection using synthetic mammograms from generative adversarial networks in convolutional neural networks","2018","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","10718","","107180X","","","","10.1117/12.2318100","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050185427&doi=10.1117%2f12.2318100&partnerID=40&md5=6822ff9b99313b1738421eeb8067c3cf","The Convolutional Neural Network (CNN) is a promising technique to detect breast cancer based on mammograms. Training the CNN from scratch, however, requires a large amount of labeled data. Such a requirement usually is infeasible for some kinds of medical image data such as mammographic tumor images. Because improvement of the performance of a CNN classifier requires more training data, the creation of new training images-image augmentation-could be one solution to this problem. In this study, we applied the Generative Adversarial Network (GAN) to generate synthetic mammographic images from the Digital Database for Screening Mammography (DDSM). From the DDSM, we cropped two sets of regions of interest (ROIs) from the images: Normal and abnormal (cancer/tumor) Those ROIs were used to train the GAN, and the GAN then generated synthetic images. To compare the GAN with the affine transformation augmentation methods, such as rotation, shifting, scaling, etc., we used six groups of ROIs (three simple groups: Affine augmented, GAN synthetic, real (original), and three mixture groups of each pair of the three simple groups) for each to train a CNN classifier from scratch. And, we used real ROIs that were not used in training to validate classification outcomes. Our results show that, to classify the normal ROIs and abnormal (tumor) ROIs from DDSM, adding GAN-generated ROIs to the training data can reduce overfitting of the classifier. But the affine transformations performed slightly better than GAN. Therefore, GAN could be an optional augmentation approach. The images augmented by GAN or affine transformation cannot substitute entirely for real images to train CNN classifiers because the absence of real images in the training set will cause serious over-fitting with more training. © 2018 SPIE.","Computer aided diagnosis; Computer aided instruction; Convolution; Deep learning; Diseases; Image enhancement; Mammography; Medical imaging; Neural networks; Tumors; X ray screens; Adversarial networks; Breast mass; Convolutional neural network; image augmentation; Image synthesis; mammogram; Classification (of information)","breast mass classification; computer-aided diagnosis; convolutional neural networks; deep learning; generative adversarial networks; image augmentation; image synthesis; mammogram","Conference paper","Final","","Scopus","2-s2.0-85050185427"
"Warde-Farley D.; Bengio Y.","Warde-Farley, David (24472176000); Bengio, Yoshua (7003958245)","24472176000; 7003958245","Improving generative adversarial networks with denoising feature matching","2017","5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings","","","","","","","","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048385339&partnerID=40&md5=cdd8f17981d94b8424e3b858b92e6ebe","We propose an augmented training procedure for generative adversarial networks designed to address shortcomings of the original by directing the generator towards probable configurations of abstract discriminator features. We estimate and track the distribution of these features, as computed from data, with a denoising auto-encoder, and use it to propose high-level targets for the generator. We combine this new loss with the original and evaluate the hybrid criterion on the task of unsupervised image synthesis from datasets comprising a diverse set of visual categories, noting a qualitative and quantitative improvement in the “objectness” of the resulting samples. © ICLR 2019 - Conference Track Proceedings. All rights reserved.","Adversarial networks; Auto encoders; De-noising; Feature matching; Image synthesis; Training procedures; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85048385339"
"Sharma A.; Jindal N.; Thakur A.","Sharma, Akanksha (57188975329); Jindal, Neeru (8571819900); Thakur, Abhishek (57055028500)","57188975329; 8571819900; 57055028500","Comparison on Generative Adversarial Networks - A Study","2018","ICSCCC 2018 - 1st International Conference on Secure Cyber Computing and Communications","","","8703267","391","396","5","10.1109/ICSCCC.2018.8703267","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065640687&doi=10.1109%2fICSCCC.2018.8703267&partnerID=40&md5=58155d051aa1abe8791a3d2595bfc668","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks. © 2018 IEEE.","Deep learning; Learning systems; Remote sensing; Adversarial networks; Diverse fields; Image synthesis; Learning models; Video generation; Image processing","Generative Adversarial Networks; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85065640687"
"Appan K P.; Sivaswamy J.","Appan K, Pujitha (57194827962); Sivaswamy, Jayanthi (6602360103)","57194827962; 6602360103","Retinal Image Synthesis for CAD Development","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10882 LNCS","","","613","621","8","10.1007/978-3-319-93000-8_70","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049437304&doi=10.1007%2f978-3-319-93000-8_70&partnerID=40&md5=c7cf71cb2e6ed1268ba2a7ef17ecd598","Automatic disease detection and classification have been attracting much interest. High performance is critical in adoption of such systems, which generally rely on training with a wide variety of annotated data. Availability of such varied annotated data in medical imaging is very scarce. Synthetic data generation is a promising solution to address this problem. We propose a novel method, based on generative adversarial networks (GAN), to generate images with lesions such that the overall severity level can be controlled. We demonstrate the reliability of the generated synthetic images independently as well as by training a computer aided diagnosis (CAD) system with the generated data. We showcase this approach for heamorrhage detection in retinal images with 4 levels of severity. Quantitative assessment results show that the generated synthetic images are very close to the real data. Haemorrhage detection was found to improve with inclusion of synthetic data in the training set with improvements in sensitivity ranging from 20% to 27% over training with just expert marked data. © 2018, Springer International Publishing AG, part of Springer Nature.","Computer aided diagnosis; Deep neural networks; Medical imaging; Ophthalmology; Adversarial networks; Computer Aided Diagnosis(CAD); Disease detection; Over trainings; Quantitative assessments; Synthetic data; Synthetic data generations; Synthetic images; Image analysis","Deep neural net; Generative adversarial networks; Synthetic images","Conference paper","Final","","Scopus","2-s2.0-85049437304"
"Bermudez C.; Plassard A.J.; Davis L.T.; Newton A.T.; Resnick S.M.; Landman B.A.","Bermudez, Camilo (56995787800); Plassard, Andrew J. (56538464700); Davis, Larry T. (55121543500); Newton, Allen T. (7201391829); Resnick, Susan M. (7102831381); Landman, Bennett A. (16679175200)","56995787800; 56538464700; 55121543500; 7201391829; 7102831381; 16679175200","Learning implicit brain MRI manifolds with deep learning","2018","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","10574","","105741L","","","","10.1117/12.2293515","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047362117&doi=10.1117%2f12.2293515&partnerID=40&md5=6436fb8b60ad06c45c4426f32d8f89a1","An important task in image processing and neuroimaging is to extract quantitative information from the acquired images in order to make observations about the presence of disease or markers of development in populations. Having a low-dimensional manifold of an image allows for easier statistical comparisons between groups and the synthesis of group representatives. Previous studies have sought to identify the best mapping of brain MRI to a low-dimensional manifold, but have been limited by assumptions of explicit similarity measures. In this work, we use deep learning techniques to investigate implicit manifolds of normal brains and generate new, high-quality images. We explore implicit manifolds by addressing the problems of image synthesis and image denoising as important tools in manifold learning. First, we propose the unsupervised synthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN) by learning from 528 examples of 2D axial slices of brain MRI. Synthesized images were first shown to be unique by performing a cross-correlation with the training set. Real and synthesized images were then assessed in a blinded manner by two imaging experts providing an image quality score of 1-5. The quality score of the synthetic image showed substantial overlap with that of the real images. Moreover, we use an autoencoder with skip connections for image denoising, showing that the proposed method results in higher PSNR than FSL SUSAN after denoising. This work shows the power of artificial networks to synthesize realistic imaging data, which can be used to improve image processing techniques and provide a quantitative framework to structural changes in the brain. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Brain mapping; Deep neural networks; Image denoising; Image enhancement; Medical image processing; Quality control; Adversarial networks; Brain MRI; Image processing technique; Image synthesis; Low-dimensional manifolds; Manifold learning; Quantitative information; Statistical comparisons; Magnetic resonance imaging","brain MRI; deep neural networks; generative adversarial networks; image synthesis; Manifold learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85047362117"
"Zhang F.; Zhang T.; Mao Q.; Duan L.; Xu C.","Zhang, Feifei (57138854900); Zhang, Tianzhu (55729040600); Mao, Qirong (7101735930); Duan, Lingyu (7201932863); Xu, Changsheng (56153258200)","57138854900; 55729040600; 7101735930; 7201932863; 56153258200","Facial expression recognition in the wild: A cycle-consistent adversarial attention transfer approach","2018","MM 2018 - Proceedings of the 2018 ACM Multimedia Conference","","","","126","135","9","10.1145/3240508.3240574","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058214690&doi=10.1145%2f3240508.3240574&partnerID=40&md5=485c2c5518c21b8db418555596d39d04","Facial expression recognition (FER) is a very challenging problem due to different expressions under arbitrary poses. Most conventional approaches mainly perform FER under laboratory controlled environment. Different from existing methods, in this paper, we formulate the FER in the wild as a domain adaptation problem, and propose a novel auxiliary domain guided Cycle-consistent adversarial Attention Transfer model (CycleAT) for simultaneous facial image synthesis and facial expression recognition in the wild. The proposed model utilizes large-scale unlabeled web facial images as an auxiliary domain to reduce the gap between source domain and target domain based on generative adversarial networks (GAN) embedded with an effective attention transfer module, which enjoys several merits. First, the GAN-based method can automatically generate labeled facial images in the wild through harnessing information from labeled facial images in source domain and unlabeled web facial images in auxiliary domain. Second, the class-discriminative spatial attention maps from the classifier in source domain are leveraged to boost the performance of the classifier in target domain. Third, it can effectively preserve the structural consistency of local pixels and global attributes in the synthesized facial images through pixel cycle-consistency and discriminative loss. Quantitative and qualitative evaluations on two challenging in-the-wild datasets demonstrate that the proposed model performs favorably against state-of-the-art methods. © 2018 Association for Computing Machinery.","Pixels; Adversarial networks; Attention transfer; Controlled environment; Domain adaptation; Facial expression recognition; Facial Image synthesis; Qualitative evaluations; State-of-the-art methods; Face recognition","Attention transfer; Domain adaptation; Emotional cue extraction; Facial expression recognition; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85058214690"
"","","","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","","","178","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060419385&partnerID=40&md5=9c12bd0dc33bbf219d0df980041bcf20","The proceedings contain 22 papers. The topics discussed include: SAR to optical image synthesis for cloud removal with generative adversarial networks; development of a portable high performance mobile mapping system using the robot operating system; data processing and recording using a versatile multi-sensor vehicle; semantic segmentation of aerial imagery via multi-scale shuffling convolutional neural networks with deep supervision; Copernicus sentinel-2 data for the determination of groundwater withdrawal in the maghreb region; calibration study of a trimble ACX4 system for direct georeferencing mapping applications; infrared measurements and estimation of temperature in the restrictive scope of an industrial cement plant; extraction of solar cells from UAV-based thermal image sequences; disparity refinement of building edges using robustly matched straight lines for stereo matching; and pilot study on the retrieval of DBH and diameter distribution of deciduous forest stands using cast shadows in UAV-based orthomosaics.","","","Conference review","Final","","Scopus","2-s2.0-85060419385"
"Reed S.; Akata Z.; Yan X.; Logeswaran L.; Schiele B.; Lee H.","Reed, Scott (57210562801); Akata, Zeynep (55367079500); Yan, Xinchen (57037472000); Logeswaran, Lajanugen (56460242600); Schiele, Bernt (55267534700); Lee, Honglak (15056237200)","57210562801; 55367079500; 57037472000; 56460242600; 55267534700; 15056237200","Generative adversarial text to image synthesis","2016","33rd International Conference on Machine Learning, ICML 2016","3","","","1681","1690","9","","717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998636515&partnerID=40&md5=4fa70f0448ea3d65e7d0a8c7b347d387","Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.","Artificial intelligence; Learning systems; Network architecture; Neural networks; Recurrent neural networks; Adversarial networks; Automatic synthesis; Deep architectures; Image modeling; Image synthesis; Realistic images; Text feature; Visual concept; Image processing","","Conference paper","Final","","Scopus","2-s2.0-84998636515"
"Zhang R.; Liu X.; Guo Y.; Hao S.","Zhang, Rongjie (57202504453); Liu, Xueliang (36721472000); Guo, Yanrong (24764663200); Hao, Shijie (57206038253)","57202504453; 36721472000; 24764663200; 57206038253","Image synthesis with aesthetics-aware generative adversarial network","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11165 LNCS","","","169","179","10","10.1007/978-3-030-00767-6_16","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057230474&doi=10.1007%2f978-3-030-00767-6_16&partnerID=40&md5=ee4c7549fdf73ca2db153adf6277efe0","With the advance of Generative Adversarial Networks (GANs), image generation has achieved rapid development. Nevertheless, the synthetic images produced by the existing GANs are still not visually plausible in terms of semantics and aesthetics. To address this issue, we propose a novel GAN model that is both aware of visual aesthetics and content semantics. Specifically, we add two types of loss functions. The first one is the aesthetics loss function, which tries to maximize the visual aesthetics of an image. The second one is the visual content loss function, which minimizes the similarity between the generated images and real images in terms of high-level visual contents. In experiments, we validate our method on two standard benchmark datasets. Qualitative and quantitative results demonstrate the effectiveness of the two loss functions. © Springer Nature Switzerland AG 2018.","Semantics; Adversarial networks; Benchmark datasets; Content semantics; Image Aesthetics; Image generations; Image synthesis; Quantitative result; Visual Aesthetics; Image processing","Generative Adversarial Network; Image aesthetics; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85057230474"
"Ge Z.; Demyanov S.; Chen Z.; Garnavi R.","Ge, Zongyuan (56287884800); Demyanov, Sergey (57190217554); Chen, Zetao (55865362200); Garnavi, Rahil (15845331200)","56287884800; 57190217554; 55865362200; 15845331200","Generative OpenMax for multi-class open set classification","2017","British Machine Vision Conference 2017, BMVC 2017","","","","","","","10.5244/c.31.42","121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086689967&doi=10.5244%2fc.31.42&partnerID=40&md5=93b8c713109d778579e6b17795d56b0e","We present a conceptually new and flexible method for multi-class open set classification. Unlike previous methods where unknown classes are inferred with respect to the feature or decision distance to the known classes, our approach is able to provide explicit modelling and decision score for unknown classes. The proposed method, called Generative OpenMax (G-OpenMax), extends OpenMax by employing generative adversarial networks (GANs) for novel category image synthesis. We validate the proposed method on two datasets of handwritten digits and characters, resulting in superior results over previous deep learning based method OpenMax Moreover, G-OpenMax provides a way to visualize samples representing the unknown classes from open space. Our simple and effective approach could serve as a new direction to tackle the challenging multi-class open set classification problem. © 2017. The copyright of this document resides with its authors.","Computer vision; Deep learning; Adversarial networks; Effective approaches; Handwritten digit; Image synthesis; Learning-based methods; Unknown class; Classification (of information)","","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85086689967"
"Liu Y.; Qin Z.; Wan T.; Luo Z.","Liu, Yifan (57857117400); Qin, Zengchang (8935532300); Wan, Tao (57201584590); Luo, Zhenbo (56489163400)","57857117400; 8935532300; 57201584590; 56489163400","Auto-painter: Cartoon image generation from sketch by using conditional Wasserstein generative adversarial networks","2018","Neurocomputing","311","","","78","87","9","10.1016/j.neucom.2018.05.045","82","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048541722&doi=10.1016%2fj.neucom.2018.05.045&partnerID=40&md5=40fac2c9c7e394651641da88dd971334","Recently, realistic image generation using deep neural networks has become a hot topic in machine learning and computer vision. Such an image can be generated at pixel level by learning from a large collection of images. Learning to generate colorful cartoon images from black-and-white sketches is not only an interesting research problem, but also a useful application in digital entertainment. In this paper, we investigate the sketch-to-image synthesis problem by using conditional generative adversarial networks (cGAN). We propose a model called auto-painter which can automatically generate compatible colors given a sketch. Wasserstein distance is used in training cGAN to overcome model collapse and enable the model converged much better. The new model is not only capable of painting hand-draw sketch with compatible colors, but also allowing users to indicate preferred colors. Experimental results on different sketch datasets show that the auto-painter performs better than other existing image-to-image methods. © 2018 Elsevier B.V.","Color; Deep learning; Deep neural networks; Neural networks; Adversarial networks; Auto-painter; Digital entertainment; Image synthesis; Realistic images; Research problems; Wasserstein distance; WGAN; algorithm; art; Article; artificial neural network; auto painter; conditional Wasserstein generative adversarial network; controlled study; image analysis; image quality; image synthesis; limit of quantitation; machine learning; measurement accuracy; priority journal; supervised machine learning; Drawing (graphics)","Auto-painter; Deep learning; GAN; Neural networks; Wasserstein distance; WGAN","Article","Final","","Scopus","2-s2.0-85048541722"
"Frid-Adar M.; Klang E.; Amitai M.; Goldberger J.; Greenspan H.","Frid-Adar, Maayan (57195642611); Klang, Eyal (56080228800); Amitai, Michal (6701865585); Goldberger, Jacob (7005511043); Greenspan, Hayit (7004965553)","57195642611; 56080228800; 6701865585; 7005511043; 7004965553","Synthetic data augmentation using GAN for improved liver lesion classification","2018","Proceedings - International Symposium on Biomedical Imaging","2018-April","","","289","293","4","10.1109/ISBI.2018.8363576","364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048075788&doi=10.1109%2fISBI.2018.8363576&partnerID=40&md5=2241537aa7e33303b11c474f04cccb9f","In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity. © 2018 IEEE.","Medical imaging; Adversarial networks; Data augmentation; Image synthesis; Lesion classification; Liver lesions; Computerized tomography","Data augmentation; Generative adversarial network; Image synthesis; Lesion classification; Liver lesions","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85048075788"
"Bermudez J.D.; Happ P.N.; Oliveira D.A.B.; Feitosa R.Q.","Bermudez, J.D. (57200270007); Happ, P.N. (55768214000); Oliveira, D.A.B. (27567900100); Feitosa, R.Q. (6602453684)","57200270007; 55768214000; 27567900100; 6602453684","SAR to OPTICAL IMAGE SYNTHESIS for CLOUD REMOVAL with GENERATIVE ADVERSARIAL NETWORKS","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","5","11","6","10.5194/isprs-annals-IV-1-5-2018","34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056341243&doi=10.5194%2fisprs-annals-IV-1-5-2018&partnerID=40&md5=d4212f7c1e4135f41d0f4ccf9889527c","Optical imagery is often affected by the presence of clouds. Aiming to reduce their effects, different reconstruction techniques have been proposed in the last years. A common alternative is to extract data from active sensors, like Synthetic Aperture Radar (SAR), because they are almost independent on the atmospheric conditions and solar illumination. On the other hand, SAR images are more complex to interpret than optical images requiring particular handling. Recently, Conditional Generative Adversarial Networks (cGANs) have been widely used in different image generation tasks presenting state-of-the-art results. One application of cGANs is learning a nonlinear mapping function from two images of different domains. In this work, we combine the fact that SAR images are hardly affected by clouds with the ability of cGANS for image translation in order to map optical images from SAR ones so as to recover regions that are covered by clouds. Experimental results indicate that the proposed solution achieves better classification accuracy than SAR based classification. © Authors 2018.","","Cloud Removal; Conditional Generative Adversarial Networks; Deep Learning; Multispectral Images; SAR","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056341243"
"Yousef A.M.; Omar Y.M.K.; Fakharany E.","Yousef, Ahmed Mohammed (57203141484); Omar, Yasser M.K. (57191157997); Fakharany, Essam (57194456195)","57203141484; 57191157997; 57194456195","Deep generative image model using a hybrid system of generative adversarial nets (GANs)","2018","ACCS/PEIT 2017 - 2017 Intl Conf on Advanced Control Circuits Systems and 2017 Intl Conf on New Paradigms in Electronics and Information Technology","2018-February","","","278","285","7","10.1109/ACCS-PEIT.2017.8303052","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050676437&doi=10.1109%2fACCS-PEIT.2017.8303052&partnerID=40&md5=221944cf7683d54139e49339c6b37803","Synthesizing realistic images has been a challenge in machine learning, due to images being complex and high dimensional, thus making them hard to model well. Building on the recent progress made in both, generative multi adversarial nets (GMAN) and conditional generative adversarial nets (CGAN), this research aims at introducing a new method to improve image synthesis in generative adversarial networks (GAN). The research benefits from combining the best of both techniques to build a model (Hybrid-GAN) that produces higher images quality, which is hardly distinguished from real images. Furthermore, this model significantly enhances log-likelihood of test data under the conditional distributions. To validate the results, we have conducted a detailed comparison between images generated by our new model, Hybrid-GAN and those images produced by standard GANs. We execute the new model using MNIST dataset and demonstrated the results obtained from the generating task. © 2017 IEEE.","Hybrid systems; Learning systems; Adversarial networks; Conditional distribution; High-dimensional; Image modeling; Image synthesis; Log likelihood; Realistic images; Recent progress; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85050676437"
"Tran L.; Yin X.; Liu X.","Tran, Luan (57189323204); Yin, Xi (56288395400); Liu, Xiaoming (35793096800)","57189323204; 56288395400; 35793096800","Disentangled representation learning GAN for pose-invariant face recognition","2017","Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017","2017-January","","","1283","1292","9","10.1109/CVPR.2017.141","587","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041925714&doi=10.1109%2fCVPR.2017.141&partnerID=40&md5=ea972275854bcbf7459ac0c3dba532c0","The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art. © 2017 IEEE.","Computer vision; Decoding; Gesture recognition; Pattern recognition; Adversarial networks; Arbitrary number; Conventional approach; Encoder-decoder; Pose-invariant face recognition; Qualitative evaluations; State of the art; Synthetic images; Face recognition","","Conference paper","Final","","Scopus","2-s2.0-85041925714"
"Costa P.; Galdran A.; Meyer M.I.; Niemeijer M.; Abràmoff M.; Mendonça A.M.; Campilho A.","Costa, Pedro (57214448535); Galdran, Adrian (56451007600); Meyer, Maria Ines (57194789130); Niemeijer, Meindert (57205785926); Abràmoff, Michael (6602158360); Mendonça, Ana Maria (56252793600); Campilho, Aurélio (57200232252)","57214448535; 56451007600; 57194789130; 57205785926; 6602158360; 56252793600; 57200232252","End-to-End Adversarial Retinal Image Synthesis","2018","IEEE Transactions on Medical Imaging","37","3","","781","791","10","10.1109/TMI.2017.2759102","221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031776330&doi=10.1109%2fTMI.2017.2759102&partnerID=40&md5=2015915a3f2b1444c5e1dd767b335b29","In medical image analysis applications, the availability of the large amounts of annotated data is becoming increasingly critical. However, annotated medical data is often scarce and costly to obtain. In this paper, we address the problem of synthesizing retinal color images by applying recent techniques based on adversarial learning. In this setting, a generative model is trained to maximize a loss function provided by a second model attempting to classify its output into real or synthetic. In particular, we propose to implement an adversarial autoencoder for the task of retinal vessel network synthesis. We use the generated vessel trees as an intermediate stage for the generation of color retinal images, which is accomplished with a generative adversarial network. Both models require the optimization of almost everywhere differentiable loss functions, which allows us to train them jointly. The resulting model offers an end-to-end retinal image synthesis system capable of generating as many retinal images as the user requires, with their corresponding vessel networks, by sampling from a simple probability distribution that we impose to the associated latent space. We show that the learned latent space contains a well-defined semantic structure, implying that we can perform calculations in the space of retinal images, e.g., smoothly interpolating new data points between two retinal images. Visual and quantitative results demonstrate that the synthesized images are substantially different from those in the training set, while being also anatomically consistent and displaying a reasonable visual quality. © 2017 IEEE.","Algorithms; Diagnostic Techniques, Ophthalmological; Humans; Image Processing, Computer-Assisted; Neural Networks (Computer); Retina; Retinal Vessels; Eye protection; Image analysis; Learning systems; Mathematical models; Ophthalmology; Personnel training; Probability distributions; Semantics; Adversarial networks; Autoencoders; Biomedical imaging; Image generations; Retinal image; Retinal image analysis; Retinal vessels; calculation; image analysis; learning; loss of function mutation; retina blood vessel; retina image; sampling; synthesis; algorithm; artificial neural network; diagnostic imaging; human; image processing; procedures; retina; retina blood vessel; visual system examination; Medical imaging","adversarial autoencoders; generative adversarial networks; retinal image analysis; Retinal image synthesis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85031776330"
"Mauricio A.; López J.; Huauya R.; Diaz J.","Mauricio, Antoni (56602560400); López, Jorge (57191707067); Huauya, Roger (57204188012); Diaz, Jose (35785677400)","56602560400; 57191707067; 57204188012; 35785677400","High-resolution generative adversarial neural networks applied to histological images generation","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11140 LNCS","","","195","202","7","10.1007/978-3-030-01421-6_20","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054798854&doi=10.1007%2f978-3-030-01421-6_20&partnerID=40&md5=e55a2f7ce4ae4e89c576a276ec1cc424","For many years, synthesizing photo-realistic images has been a highly relevant task due to its multiple applications from aesthetic or artistic [19] to medical purposes [1, 6, 21]. Related to the medical area, this application has had greater impact because most classification or diagnostic algorithms require a significant amount of highly specialized images for their training yet obtaining them is not easy at all. To solve this problem, many works analyze and interpret images of a specific topic in order to obtain a statistical correlation between the variables that define it. By this way, any set of variables close to the map generated in the previous analysis represents a similar image. Deep learning based methods have allowed the automatic extraction of feature maps which has helped in the design of more robust models photo-realistic image synthesis. This work focuses on obtaining the best feature maps for automatic generation of synthetic histological images. To do so, we propose a Generative Adversarial Networks (GANs) [8] to generate the new sample distribution using the feature maps obtained by an autoencoder [14, 20] as latent space instead of a completely random one. To corroborate our results, we present the generated images against the real ones and their respective results using different types of autoencoder to obtain the feature maps. © Springer Nature Switzerland AG 2018.","Deep learning; Diagnosis; Medical imaging; Neural networks; Diagnostic algorithms; Generative Adversarial Nets; High resolution; Histological images; Learning-based methods; Photo realistic image synthesis; Photorealistic images; Statistical correlation; Image analysis","Generative Adversarial Nets; High-resolution generated images; Histological images","Conference paper","Final","","Scopus","2-s2.0-85054798854"
"Dong H.; Zhang J.; McIlwraith D.; Guo Y.","Dong, Hao (57192424168); Zhang, Jingqing (57201582643); McIlwraith, Douglas (24829830100); Guo, Yike (12765868000)","57192424168; 57201582643; 24829830100; 12765868000","I2T2I: Learning text to image synthesis with textual data augmentation","2018","Proceedings - International Conference on Image Processing, ICIP","2017-September","","","2015","2019","4","10.1109/ICIP.2017.8296635","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045293704&doi=10.1109%2fICIP.2017.8296635&partnerID=40&md5=5ba6b4572f46dd93c4bc1ab455597c94","Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose dataset (MHP) without using sentence annotation. © 2017 IEEE.","Complex networks; Deep learning; Degrees of freedom (mechanics); Natural language processing systems; Recurrent neural networks; Adversarial networks; Image captioning; Image generations; Image synthesis; Recurrent neural network (RNN); State of the art; Training methods; Transfer learning; Image enhancement","Deep learning; GAN; Image Synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85045293704"
"Sun Q.; Tewari A.; Xu W.; Fritz M.; Theobalt C.; Schiele B.","Sun, Qianru (55643637700); Tewari, Ayush (57200618272); Xu, Weipeng (57195993759); Fritz, Mario (14035495500); Theobalt, Christian (6507027272); Schiele, Bernt (55267534700)","55643637700; 57200618272; 57195993759; 14035495500; 6507027272; 55267534700","A Hybrid Model for Identity Obfuscation by Face Replacement","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11205 LNCS","","","570","586","16","10.1007/978-3-030-01246-5_34","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055107955&doi=10.1007%2f978-3-030-01246-5_34&partnerID=40&md5=9cd0fcf2dd33783cec006f8fc2019145","As more and more personal photos are shared and tagged in social media, avoiding privacy risks such as unintended recognition, becomes increasingly challenging. We propose a new hybrid approach to obfuscate identities in photos by head replacement. Our approach combines state of the art parametric face synthesis with latest advances in Generative Adversarial Networks (GAN) for data-driven image synthesis. On the one hand, the parametric part of our method gives us control over the facial parameters and allows for explicit manipulation of the identity. On the other hand, the data-driven aspects allow for adding fine details and overall realism as well as seamless blending into the scene context. In our experiments we show highly realistic output of our system that improves over the previous state of the art in obfuscation rate while preserving a higher similarity to the original image content. © 2018, Springer Nature Switzerland AG.","Computer vision; Adversarial networks; Face replacement; Face synthesis; Hybrid approach; Image synthesis; Original images; Privacy risks; State of the art; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055107955"
"Shin H.-C.; Tenenholtz N.A.; Rogers J.K.; Schwarz C.G.; Senjem M.L.; Gunter J.L.; Andriole K.P.; Michalski M.","Shin, Hoo-Chang (56903407200); Tenenholtz, Neil A. (53867080500); Rogers, Jameson K. (57203991995); Schwarz, Christopher G. (34880774400); Senjem, Matthew L. (57226033215); Gunter, Jeffrey L. (36899097500); Andriole, Katherine P. (7004435253); Michalski, Mark (57203036685)","56903407200; 53867080500; 57203991995; 34880774400; 57226033215; 36899097500; 7004435253; 57203036685","Medical image synthesis for data augmentation and anonymization using generative adversarial networks","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11037 LNCS","","","1","11","10","10.1007/978-3-030-00536-8_1","254","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053935181&doi=10.1007%2f978-3-030-00536-8_1&partnerID=40&md5=35a7e5cacb3f1c872ed260b765e6a863","Data diversity is critical to success when training deep learning models. Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models. In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI. We demonstrate two unique benefits that the synthetic images provide. First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation. Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data. Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data. © 2018, Springer Nature Switzerland AG.","Brain; Deep learning; Hospital data processing; Image enhancement; Image segmentation; Magnetic resonance imaging; Tumors; Adversarial networks; Brain tumors; Data augmentation; Generative model; Image synthesis; Learning models; Synthetic images; Tumor segmentation; Medical imaging","Brain tumor; Deep learning; GAN; Generative models; Image synthesis; Magnetic resonance imaging; MRI; Segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85053935181"
"Sharma S.; Namboodiri V.P.","Sharma, Shashank (57205544106); Namboodiri, Vinay P. (8724086000)","57205544106; 8724086000","No modes left behind: Capturing the data distribution effectively using GANS","2018","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","","","","4042","4049","7","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060442254&partnerID=40&md5=dbaeee5f5d2478239485cc04052ec10b","Generative adversarial networks (GANs) while being very versatile in realistic image synthesis, still are sensitive to the input distribution. Given a set of data that has an imbalance in the distribution, the networks are susceptible to missing modes and not capturing the data distribution. While various methods have been tried to improve training of GANs, these have not addressed the challenges of covering the full data distribution. Specifically, a generator is not penalized for missing a mode. We show that these are therefore still susceptible to not capturing the full data distribution. In this paper, we propose a simple approach that combines an encoder based objective with novel loss functions for generator and discriminator that improves the solution in terms of capturing missing modes. We validate that the proposed method results in substantial improvements through its detailed analysis on toy and real datasets. The quantitative and qualitative results demonstrate that the proposed method improves the solution for the problem of missing modes and improves training of GANs. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Adversarial networks; Data distribution; Input distributions; Loss functions; Real data sets; Realistic image synthesis; Simple approach; Artificial intelligence","","Conference paper","Final","","Scopus","2-s2.0-85060442254"
"Reed S.; Akata Z.; Mohan S.; Tenka S.; Schiele B.; Lee H.","Reed, Scott (57210562801); Akata, Zeynep (55367079500); Mohan, Santosh (57194142141); Tenka, Samuel (57194151094); Schiele, Bernt (55267534700); Lee, Honglak (15056237200)","57210562801; 55367079500; 57194142141; 57194151094; 55267534700; 15056237200","Learning what and where to draw","2016","Advances in Neural Information Processing Systems","","","","217","225","8","","367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018890661&partnerID=40&md5=05453ec1b181b4c192459a61ed421143","Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 × 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations. © 2016 NIPS Foundation - All Rights Reserved.","Birds; Location; Stereo vision; Adversarial networks; Arbitrary subsets; Bounding box; Conditional distribution; Global constraints; Image synthesis; Object location; Real-world image; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85018890661"
"Tan W.R.; Chan C.S.; Aguirre H.E.; Tanaka K.","Tan, Wei Ren (57192559815); Chan, Chee Seng (57194450557); Aguirre, Hernan E. (6603898641); Tanaka, Kiyoshi (55430510400)","57192559815; 57194450557; 6603898641; 55430510400","ArtGAN: Artwork synthesis with conditional categorical GANs","2018","Proceedings - International Conference on Image Processing, ICIP","2017-September","","","3760","3764","4","10.1109/ICIP.2017.8296985","49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045318097&doi=10.1109%2fICIP.2017.8296985&partnerID=40&md5=5904b23eea1c608bb76ce96faee77ff2","This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ArtGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ArtGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10. © 2017 IEEE.","Backpropagation; Deep learning; Adversarial networks; Complex image; Image synthesis; Label information; Loss functions; Natural images; Real-world image; Image processing","Deep learning; Generative adversarial networks; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85045318097"
"Shin Y.; Qadir H.A.; Balasingham I.","Shin, Younghak (44061719700); Qadir, Hemin Ali (56331081100); Balasingham, Ilangko (6602773063)","44061719700; 56331081100; 6602773063","Abnormal colon polyp image synthesis using conditional adversarial networks for improved detection performance","2018","IEEE Access","6","","8478237","56007","56017","10","10.1109/ACCESS.2018.2872717","44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054398601&doi=10.1109%2fACCESS.2018.2872717&partnerID=40&md5=4cd027138d75fc988bfbf731b10d9431","One of the major obstacles in automatic polyp detection during colonoscopy is the lack of labeled polyp training images. In this paper, we propose a framework of conditional adversarial networks to increase the number of training samples by generating synthetic polyp images. Using a normal binary form of polyp mask which represents only the polyp position as an input conditioned image, realistic polyp image generation is a difficult task in a generative adversarial networks approach. We propose an edge filtering-based combined input conditioned image to train our proposed networks. This enables realistic polyp image generations while maintaining the original structures of the colonoscopy image frames. More importantly, our proposed framework generates synthetic polyp images from normal colonoscopy images which have the advantage of being relatively easy to obtain. The network architecture is based on the use of multiple dilated convolutions in each encoding part of our generator network to consider large receptive fields and avoid much contractions of a feature map size. An image resizing with convolution for upsampling in the decoding layers is considered to prevent artifacts on generated images. We show that the generated polyp images are not only qualitatively realistic, but also help to improve polyp detection performance. © 2013 IEEE.","Convolution; Endoscopy; Network architecture; Neural networks; Adversarial networks; Automatic polyp detection; Colonoscopy; Convolutional neural network; Detection performance; Image generations; Original structures; Polyp detection; Image enhancement","Colonoscopy; Convolutional neural network; Dilated convolution; Generative adversarial networks; Polyp detection","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85054398601"
"Purbaya M.E.; Setiawan N.A.; Adji T.B.","Purbaya, Muhammad Eka (57203090940); Setiawan, Noor Akhmad (57215833019); Adji, Teguh Bharata (24734043700)","57203090940; 57215833019; 24734043700","Leaves image synthesis using generative adversarial networks with regularization improvement","2018","2018 International Conference on Information and Communications Technology, ICOIACT 2018","2018-January","","","360","365","5","10.1109/ICOIACT.2018.8350780","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050487385&doi=10.1109%2fICOIACT.2018.8350780&partnerID=40&md5=6f0795a4b570674ba4973d11f947b993","Diversity of leaves form a special feature in a plant that can be done research such as image segmentation. However, the thing that is the main issue is the quantity of labeled data. Through image synthesis or image segmentation we are able to add leaf shape needed to use Generative Adversarial Networks (GAN). To train the GAN requires the choice of architecture, initialization parameters and more accurate selection as it often becomes a GAN challenge. Therefore appropriate regularization techniques are needed to address the problem. In the end we were able to segment the 3 leaf shape images using conventional GANs that we have modified using attention to the optimal regularizer parameters. Elastic Net or a combination of L1 and L2 regularizer that we tested on the second model gives error rate 0,105% for discriminator and 20,95% for generator. © 2018 IEEE.","Deep learning; Image enhancement; Plants (botany); Adversarial networks; Elastic net; Error rate; Image synthesis; Labeled data; leaves; Regularization technique; Regularizer; Image segmentation","deep learning; GAN; Generative Adversarial Network; image segmentation; leaves; regularizer","Conference paper","Final","","Scopus","2-s2.0-85050487385"
"Zhang G.; Tu E.; Cui D.","Zhang, Guanghao (57194474566); Tu, Enmei (54411244300); Cui, Dongshun (57192665955)","57194474566; 54411244300; 57192665955","Stable and improved generative adversarial nets (GANS): A constructive survey","2018","Proceedings - International Conference on Image Processing, ICIP","2017-September","","","1871","1875","4","10.1109/ICIP.2017.8296606","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045331971&doi=10.1109%2fICIP.2017.8296606&partnerID=40&md5=c16a7ec9b220c58781cd9686b98e0392","In this paper, we present a general and applicable adversarial training framework based on a comprehensive survey, not limited to straightforward GANs related works, also including shallow neural networks and reinforcement learning. Concentrating on challenging face synthesis task, we summarize a stable training pipeline: 1) booting training procedure with noise injection; 2) fixing weights of fully connected layer in generator to improve performance further; 3) involving Markov decision module to dynamically choose learning rates of discriminator and generator respectively. Finally in experiments, we highlight a mutual evaluation criterion over entropy score based on a pre-trained classifier and manual voting. © 2017 IEEE.","Reinforcement learning; Surveys; Adversarial networks; Fully-connected layers; Image synthesis; Improve performance; Mutual evaluations; Stable GAN; Training framework; Training procedures; Image processing","GAN; Generative adversarial networks; Image synthesis; Stable GAN","Conference paper","Final","","Scopus","2-s2.0-85045331971"
"Wang Y.; Zhou L.; Wang L.; Yu B.; Zu C.; Lalush D.S.; Lin W.; Wu X.; Zhou J.; Shen D.","Wang, Yan (56039981100); Zhou, Luping (23398846800); Wang, Lei (54958774700); Yu, Biting (57201496052); Zu, Chen (55377165500); Lalush, David S. (7004144493); Lin, Weili (56999175100); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Shen, Dinggang (7401738392)","56039981100; 23398846800; 54958774700; 57201496052; 55377165500; 7004144493; 56999175100; 57221065403; 21234416400; 7401738392","Locality adaptive multi-modality GANs for high-quality PET image synthesis","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11070 LNCS","","","329","337","8","10.1007/978-3-030-00928-1_38","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054055593&doi=10.1007%2f978-3-030-00928-1_38&partnerID=40&md5=badbbb289ca019a0a11467563e16d28c","Positron emission topography (PET) has been substantially used in recent years. To minimize the potential health risks caused by the tracer radiation inherent to PET scans, it is of great interest to synthesize the high-quality full-dose PET image from the low-dose one to reduce the radiation exposure while maintaining the image quality. In this paper, we propose a locality adaptive multi-modality generative adversarial networks model (LA-GANs) to synthesize the full-dose PET image from both the low-dose one and the accompanying T1-weighted MRI to incorporate anatomical information for better PET image synthesis. This paper has the following contributions. First, we propose a new mechanism to fuse multi-modality information in deep neural networks. Different from the traditional methods that treat each image modality as an input channel and apply the same kernel to convolute the whole image, we argue that the contributions of different modalities could vary at different image locations, and therefore a unified kernel for a whole image is not appropriate. To address this issue, we propose a method that is locality adaptive for multi-modality fusion. Second, to learn this locality adaptive fusion, we utilize 1 × 1 × 1 kernel so that the number of additional parameters incurred by our method is kept minimum. This also naturally produces a fused image which acts as a pseudo input for the subsequent learning stages. Third, the proposed locality adaptive fusion mechanism is learned jointly with the PET image synthesis in an end-to-end trained 3D conditional GANs model developed by us. Our 3D GANs model generates high quality PET images by employing large-sized image patches and hierarchical features. Experimental results show that our method outperforms the traditional multi-modality fusion methods used in deep networks, as well as the state-of-the-art PET estimation approaches. © Springer Nature Switzerland AG 2018.","Deep neural networks; Health risks; Magnetic resonance imaging; Medical computing; Medical imaging; Adversarial networks; Anatomical information; Estimation approaches; Hierarchical features; Multi-modality fusion; Positron emission; Potential health risks; Radiation Exposure; Image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054055593"
"Han L.; Murphy R.F.; Ramanan D.","Han, Ligong (57191699123); Murphy, Robert F. (7403470459); Ramanan, Deva (6506835622)","57191699123; 7403470459; 6506835622","Learning generative models of tissue organization with supervised GANs","2018","Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January","","","682","690","8","10.1109/WACV.2018.00080","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050951605&doi=10.1109%2fWACV.2018.00080&partnerID=40&md5=a1a79b6243573779e86792804572b949","A key step in understanding the spatial organization of cells and tissues is the ability to construct generative models that accurately reflect that organization. In this paper, we focus on building generative models of electron microscope (EM) images in which the positions of cell membranes and mitochondria have been densely annotated, and propose a two-stage procedure that produces realistic images using Generative Adversarial Networks (or GANs) in a supervised way. In the first stage, we synthesize a label 'image' given a noise 'image' as input, which then provides supervision for EM image synthesis in the second stage. The full model naturally generates label-image pairs. We show that accurate synthetic EM images are produced using assessment via (1) shape features and global statistics, (2) segmentation accuracies, and (3) user studies. We also demonstrate further improvements by enforcing a reconstruction loss on intermediate synthetic labels and thus unifying the two stages into one single end-to-end framework. © 2018 IEEE.","Computer vision; Cytology; Tissue; Adversarial networks; Generative model; Global statistics; Image synthesis; Realistic images; Segmentation accuracy; Spatial organization; Two stage procedure; Image segmentation","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85050951605"
"Nie D.; Trullo R.; Lian J.; Petitjean C.; Ruan S.; Wang Q.; Shen D.","Nie, Dong (57188806186); Trullo, Roger (56938706800); Lian, Jun (7202008269); Petitjean, Caroline (22734728700); Ruan, Su (7102191561); Wang, Qian (56098714800); Shen, Dinggang (7401738392)","57188806186; 56938706800; 7202008269; 22734728700; 7102191561; 56098714800; 7401738392","Medical image synthesis with context-aware generative adversarial networks","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10435 LNCS","","","417","425","8","10.1007/978-3-319-66179-7_48","379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029498722&doi=10.1007%2f978-3-319-66179-7_48&partnerID=40&md5=0290e9c49284fc1873eb8919e42e8116","Computed tomography (CT) is critical for various clinical applications, e.g., radiation treatment planning and also PET attenuation correction in MRI/PET scanner. However, CT exposes radiation during acquisition, which may cause side effects to patients. Compared to CT, magnetic resonance imaging (MRI) is much safer and does not involve radiations. Therefore, recently researchers are greatly motivated to estimate CT image from its corresponding MR image of the same subject for the case of radiation planning. In this paper, we propose a data-driven approach to address this challenging problem. Specifically, we train a fully convolutional network (FCN) to generate CT given the MR image. To better model the nonlinear mapping from MRI to CT and produce more realistic images, we propose to use the adversarial training strategy to train the FCN. Moreover, we propose an image-gradient-difference based loss function to alleviate the blurriness of the generated CT. We further apply Auto-Context Model (ACM) to implement a context-aware generative adversarial network. Experimental results show that our method is accurate and robust for predicting CT images from MR images, and also outperforms three state-of-the-art methods under comparison. © Springer International Publishing AG 2017.","Deep learning; Image processing; Magnetic levitation vehicles; Magnetic resonance imaging; Medical computing; Medical imaging; Radiation effects; Auto-context; Clinical application; Convolutional networks; Data-driven approach; Generative model; Image synthesis; Radiation treatment planning; State-of-the-art methods; Computerized tomography","Auto-context; Deep learning; GAN; Generative models; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85029498722"
"Liu Y.-C.; Chiu W.-C.; Wang S.-D.; Wang Y.-C.F.","Liu, Yen-Cheng (57206819334); Chiu, Wei-Chen (24449911200); Wang, Sheng-De (7410337696); Wang, Yu-Chiang Frank (35216822800)","57206819334; 24449911200; 7410337696; 35216822800","Domain-Adaptive generative adversarial networks for sketch-to-photo inversion","2017","IEEE International Workshop on Machine Learning for Signal Processing, MLSP","2017-September","","","1","6","5","10.1109/MLSP.2017.8168181","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042275790&doi=10.1109%2fMLSP.2017.8168181&partnerID=40&md5=73e9259a9b656ca96cb05e31c829d01a","Generating photo-realistic images from multiple style sketches is one of challenging tasks in image synthesis with important applications such as facial composite for suspects. While machine learning techniques have been applied for solving this problem, the requirement of collecting sketch and face photo image pairs would limit the use of the learned model for rendering sketches of different styles. In this paper, we propose a novel deep learning model of Domain-adaptive Generative Adversarial Networks (DA-GAN). The design of DA-GAN performs cross-style sketch-to-photo inversion, which mitigates the difference across input sketch styles without the need to collect a large number of sketch and face image pairs for training purposes. In experiments, we show that our method is able to produce satisfactory results as well as performing favorably against state-of-the-art approaches. © 2017 IEEE.","Artificial intelligence; Learning systems; Neural networks; Signal processing; Adversarial networks; Convolutional neural network; Facial composites; Image inversions; Machine learning techniques; Photorealistic images; State-of-the-art approach; Training purpose; Deep learning","Convolutional Neural Network; Deep Learning; Generative Adversarial Network; Image Inversion","Conference paper","Final","","Scopus","2-s2.0-85042275790"
"Canas K.; Liu X.; Ubiera B.; Liu Y.","Canas, Karen (57203369157); Liu, Xinlian (37045888900); Ubiera, Brandon (57203373005); Liu, Yanling (57203373310)","57203369157; 37045888900; 57203373005; 57203373310","Scalable biomedical image synthesis with GAN","2018","ACM International Conference Proceeding Series","","","a95","","","","10.1145/3219104.3229261","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051427317&doi=10.1145%2f3219104.3229261&partnerID=40&md5=99c8dd1b93b8b0ddc1cf86076c164a77","Despite the fast-paced progress in imaging techniques made possible by ubiquitous applications of convolutional neural networks, biomedical imaging has yet to benefit from the full potential of deep learning. An unresolved bottleneck is the lack of training set data. Some experimentally obtained data are kept and preserved by individual research groups where they were produced, out of the reach of the public; more often, high cost and rare occurrences simply mean not enough such images have been made. We propose to develop deep learning based workflow to overcome this barrier. Leveraging the largest radiology data (chest X-Ray) recently published by the NIH, we train a generative adversarial network (GAN) and use it to produce photorealistic images that retain pathological quality. We also explore porting our models to a range of supercomputing platforms and systems that we have access to, including XSEDE, NERSC, OLCF, Blue Waters, NIH Biowulf etc., to investigate and compare their performance. In addition to the obvious benefits of biomedical research, our work will help understand how current supercomputing infrastructure embraces machine learning demands. Our code and enhanced data set are available through GitHub/Binder. © 2018 Copyright held by the owner/author(s).","Convolution; Data flow analysis; Deep learning; Medical imaging; Neural networks; Adversarial networks; Biomedical images; Biomedical imaging; Biomedical research; Convolutional neural network; Image synthesis; Photorealistic images; Ubiquitous application; Image processing","Biomedical Imaging; Convolutional Neural Network; Deep Learning; Generative Adversarial Network; Image Synthesis","Conference paper","Final","","Scopus","2-s2.0-85051427317"
"Wang C.; Macnaught G.; Papanastasiou G.; MacGillivray T.; Newby D.","Wang, Chengjia (57196394674); Macnaught, Gillian (55241784200); Papanastasiou, Giorgos (56539707000); MacGillivray, Tom (57207503523); Newby, David (7006580760)","57196394674; 55241784200; 56539707000; 57207503523; 7006580760","Unsupervised learning for cross-domain medical image synthesis using deformation invariant cycle consistency networks","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11037 LNCS","","","52","60","8","10.1007/978-3-030-00536-8_6","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053888519&doi=10.1007%2f978-3-030-00536-8_6&partnerID=40&md5=6663ed1c21928e044af98b0a754d8abe","Recently, the cycle-consistent generative adversarial networks (CycleGAN) has been widely used for synthesis of multi-domain medical images. The domain-specific nonlinear deformations captured by CycleGAN make the synthesized images difficult to be used for some applications, for example, generating pseudo-CT for PET-MR attenuation correction. This paper presents a deformation-invariant CycleGAN (DicycleGAN) method using deformable convolutional layers and new cycle-consistency losses. Its robustness dealing with data that suffer from domain-specific nonlinear deformations has been evaluated through comparison experiments performed on a multi-sequence brain MR dataset and a multi-modality abdominal dataset. Our method has displayed its ability to generate synthesized data that is aligned with the source while maintaining a proper quality of signal compared to CycleGAN-generated data. The proposed model also obtained comparable performance with CycleGAN when data from the source and target domains are alignable through simple affine transformations. © 2018, Springer Nature Switzerland AG.","Deep learning; Deformation; Image processing; Medical imaging; Synthesis (chemical); Unsupervised learning; Adversarial networks; Affine transformations; Attenuation correction; Domain specific; Image synthesis; Multi-sequences; Nonlinear deformations; Synthesized images; Computerized tomography","Deep learning; GAN; Synthesis; Unsupervised learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85053888519"
"Zhang H.; Xu T.; Li H.; Zhang S.; Wang X.; Huang X.; Metaxas D.","Zhang, Han (56098272800); Xu, Tao (56465290800); Li, Hongsheng (57141098300); Zhang, Shaoting (13605200100); Wang, Xiaogang (55736875200); Huang, Xiaolei (57218604182); Metaxas, Dimitris (7006359060)","56098272800; 56465290800; 57141098300; 13605200100; 55736875200; 57218604182; 7006359060","StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks","2017","Proceedings of the IEEE International Conference on Computer Vision","2017-October","","8237891","5908","5916","8","10.1109/ICCV.2017.629","1291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040306596&doi=10.1109%2fICCV.2017.629&partnerID=40&md5=19f592951fc620e711d0f4db147645b6","Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions. © 2017 IEEE.","Computer vision; Image processing; Adversarial networks; Augmentation techniques; Benchmark datasets; High quality images; High resolution image; Low resolution images; Photo realistic image synthesis; Photorealistic images; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85040306596"
"Huo Y.; Xu Z.; Bao S.; Assad A.; Abramson R.G.; Landman B.A.","Huo, Yuankai (56830058500); Xu, Zhoubing (55210723700); Bao, Shunxing (57189987093); Assad, Albert (57193204561); Abramson, Richard G. (55218605000); Landman, Bennett A. (16679175200)","56830058500; 55210723700; 57189987093; 57193204561; 55218605000; 16679175200","Adversarial synthesis learning enables segmentation without target modality ground truth","2018","Proceedings - International Symposium on Biomedical Imaging","2018-April","","","1217","1220","3","10.1109/ISBI.2018.8363790","75","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048125799&doi=10.1109%2fISBI.2018.8363790&partnerID=40&md5=81788338eda8c606dce3bf561632739d","A lack of generalizability is one key limitation of deep learning based segmentation. Typically, one manually labels new training images when segmenting organs in different imaging modalities or segmenting abnormal organs from distinct disease cohorts. The manual efforts can be alleviated if one is able to reuse manual labels from one modality (e.g., MRI) to train a segmentation network for a new modality (e.g., CT). Previously, two stage methods have been proposed to use cycle generative adversarial networks (CycleGAN) to synthesize training images for a target modality. Then, these efforts trained a segmentation network independently using synthetic images. However, these two independent stages did not use the complementary information between synthesis and segmentation. Herein, we proposed a novel end-to-end synthesis and segmentation network (EssNet) to achieve the unpaired MRI to CT image synthesis and CT splenomegaly segmentation simultaneously without using manual labels on CT. The end-to-end EssNet achieved significantly higher median Dice similarity coefficient (0.9188) than the two stages strategy (0.8801), and even higher than canonical multi-atlas segmentation (0.9125) and ResNet method (0.9107), which used the CT manual labels. © 2018 IEEE.","Deep learning; Image segmentation; Magnetic resonance imaging; Medical imaging; Adversarial networks; Imaging modality; Learning-based segmentation; New modality; Similarity coefficients; Synthetic images; Training image; Two-stage methods; Computerized tomography","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85048125799"
"Li S.; Liu W.; Ma H.; Zhu S.","Li, Shuangqun (57192989695); Liu, Wu (55145372400); Ma, Huadong (7403096223); Zhu, Shaopeng (57220918723)","57192989695; 55145372400; 7403096223; 57220918723","Beyond View Transformation: Cycle-Consistent Global and Partial Perception Gan for View-Invariant Gait Recognition","2018","Proceedings - IEEE International Conference on Multimedia and Expo","2018-July","","8486484","","","","10.1109/ICME.2018.8486484","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057119266&doi=10.1109%2fICME.2018.8486484&partnerID=40&md5=96371e1fbf0040a6c5ef85f13b1cf667","Cross-view gait recognition is a challenging problem when view-interval and pose variation are relatively large. In this paper, we propose Cycle-consistent Attentive Generative Adversarial Networks (CA-GAN) to map different views' gait images to view-consistent and photorealistic gait images for cross-view gait recognition. In CA-GAN, the generative network is composed of two branches, which simultaneously perceives human's global contexts and local body parts information respectively. Moreover, we design a novel Attentive Adversarial Network (AAN) to adaptively learn different weights for the discriminator's receptive fields with attention mechanism. Furthermore, as it is hard to collect the pose-aligned gait image pairs from different views for training CA-GAN' we combine forward cycle-consistency loss and adver-sarial loss to learn the transformation relationship from source views to target view. The combined loss function can also preserve the discriminative gait structures of different identities at the training stage. Finally, we directly exploit the synthesized view-consistent gait images for cross-view gait recognition task. Experimental results on CASIA-B demonstrate that our method not only outperforms the state-of-the-art methods in cross-view gait recognition, but also presents compelling perceptual results even across the large view-interval. © 2018 IEEE.","Gait analysis; Image processing; Adversarial networks; Attention mechanisms; Gait recognition; Image synthesis; Receptive fields; State-of-the-art methods; View invariants; View transformations; Pattern recognition","attention mechanism; cycle-consistency loss; Gait recognition; generative adversarial network; image synthesis","Conference paper","Final","","Scopus","2-s2.0-85057119266"
"Souza D.M.; Ruiz D.D.","Souza, Douglas M. (57204639868); Ruiz, Duncan D. (56187800200)","57204639868; 56187800200","Towards High-Resolution Face Pose Synthesis","2018","Proceedings of the International Joint Conference on Neural Networks","2018-July","","8488993","","","","10.1109/IJCNN.2018.8488993","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056524946&doi=10.1109%2fIJCNN.2018.8488993&partnerID=40&md5=c09ed4c3743fa8c1f1f0d2506a32dd12","Synthesizing different views of a face image is a challenging task that can potentially help in several computer graphics and computer vision applications. In this work, we present a novel approach to address this task. We leverage the power of Generative Adversarial Networks (GANs) to synthesize face poses in a high-resolution and realistic fashion. We control the rotation of synthesized faces along the three axes of space (roll, pitch, yaw). We start by estimating the pose of each face in the training set and storing a vector containing the rotation angles. Then, we use the images along with the angles to train a conditioned version of a state-of-the-art GAN. Our experiments show image synthesis with a high-realistic finish, plus the absolute control of the pose of synthesized face images. © 2018 IEEE.","Computer graphics; Adversarial networks; Computer vision applications; Face images; High resolution; Image synthesis; Rotation angles; State of the art; Training sets; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85056524946"
"Luo H.; Kong Q.; Wu F.","Luo, Hengliang (56723304700); Kong, Qingqun (55390783400); Wu, Fuchao (55984485700)","56723304700; 55390783400; 55984485700","Traffic Sign Image Synthesis with Generative Adversarial Networks","2018","Proceedings - International Conference on Pattern Recognition","2018-August","","8545787","2540","2545","5","10.1109/ICPR.2018.8545787","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059743959&doi=10.1109%2fICPR.2018.8545787&partnerID=40&md5=137e37be3c31d790659790f74bd33c6e","Deep convolutional neural networks (CNN) has achieved state-of-the-art result on traffic sign classification, which plays a key role in intelligent transportation system. However, it usually requires a large number of labeled training data, which is not always available, to guarantee a good performance. In this paper, we propose to synthesize traffic sign images by generative adversarial networks (GANs). It takes a standard traffic sign template and a background image as input to the generative network in GANs, where the template defines which class of traffic sign to include and the background image controls the visual appearance of the synthetic images. Experiments show that our method could generate more realistic traffic sign images than the conventional image synthesis method. Meanwhile, by adding the synthesis images to train a typical CNN for traffic sign classification, we obtained a better accuracy. © 2018 IEEE.","Deep neural networks; Intelligent systems; Neural networks; Pattern recognition; Adversarial networks; Background image; Deep convolutional neural networks; Intelligent transportation systems; Labeled training data; Realistic traffics; Synthetic images; Visual appearance; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85059743959"
"Lee D.; Yun S.; Choi S.; Yoo H.; Yang M.-H.; Oh S.","Lee, Donghoon (57223779537); Yun, Sangdoo (55581546300); Choi, Sungjoon (55954115000); Yoo, Hwiyeon (57200754731); Yang, Ming-Hsuan (7404927015); Oh, Songhwai (8703374300)","57223779537; 55581546300; 55954115000; 57200754731; 7404927015; 8703374300","Unsupervised Holistic Image Generation from Key Local Patches","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11209 LNCS","","","21","37","16","10.1007/978-3-030-01228-1_2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055084000&doi=10.1007%2f978-3-030-01228-1_2&partnerID=40&md5=0fe3673b1af6a76bc721f2849975ada3","We introduce a new problem of generating an image based on a small number of key local patches without any geometric prior. In this work, key local patches are defined as informative regions of the target object or scene. This is a challenging problem since it requires generating realistic images and predicting locations of parts at the same time. We construct adversarial networks to tackle this problem. A generator network generates a fake image as well as a mask based on the encoder-decoder framework. On the other hand, a discriminator network aims to detect fake images. The network is trained with three losses to consider spatial, appearance, and adversarial information. The spatial loss determines whether the locations of predicted parts are correct. Input patches are restored in the output image without much modification due to the appearance loss. The adversarial loss ensures output images are realistic. The proposed network is trained without supervisory signals since no labels of key parts are required. Experimental results on seven datasets demonstrate that the proposed algorithm performs favorably on challenging objects and scenes. © 2018, Springer Nature Switzerland AG.","Artificial intelligence; Computer science; Computers; Adversarial networks; Encoder-decoder; Geometric priors; Image generations; Image synthesis; Image-based; Realistic images; Target object; Computer vision","Generative adversarial networks; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055084000"
"Li Y.; Turner R.E.","Li, Yingzhen (57189093229); Turner, Richard E. (57214257840)","57189093229; 57214257840","Gradient estimators for implicit models","2018","6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings","","","","","","","","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083952966&partnerID=40&md5=9e436b0513c5b06c502d26c641a99f22","Implicit models, which allow for the generation of samples but not for point-wise evaluation of probabilities, are omnipresent in real-world problems tackled by machine learning and a hot topic of current research. Some examples include data simulators that are widely used in engineering and scientific research, generative adversarial networks (GANs) for image synthesis, and hot-off-the-press approximate inference techniques relying on implicit distributions. The majority of existing approaches to learning implicit models rely on approximating the intractable distribution or optimisation objective for gradient-based optimisation, which is liable to produce inaccurate updates and thus poor models. This paper alleviates the need for such approximations by proposing the Stein gradient estimator, which directly estimates the score function of the implicitly defined distribution. The efficacy of the proposed estimator is empirically demonstrated by examples that include gradient-free MCMC, meta-learning for approximate inference and entropy regularised GANs that provide improved sample diversity. © Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.","Adversarial networks; Approaches to learning; Approximate inference; Data simulators; Gradient estimator; Intractable distributions; Real-world problem; Scientific researches; Learning systems","","Conference paper","Final","","Scopus","2-s2.0-85083952966"
"Peleg I.; Wolf L.","Peleg, Irad (56765757400); Wolf, Lior (57203078732)","56765757400; 57203078732","Structured GANs","2018","Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January","","","719","728","9","10.1109/WACV.2018.00084","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050954207&doi=10.1109%2fWACV.2018.00084&partnerID=40&md5=e4ffeefc3ebff1d358f6a4a3f2807f06","We present Generative Adversarial Networks (GANs), in which the symmetric property of the generated images is controlled. This is obtained through the generator network's architecture, while the training procedure and the loss remain the same. The symmetric GANs are applied to face image synthesis in order to generate novel faces with a varying amount of symmetry. We also present an unsupervised face rotation capability, which is based on the novel notion of one-shot fine tuning. © 2018 IEEE.","Computer science; Computers; Adversarial networks; Face image synthesis; Face rotation; Fine tuning; Training procedures; Computer vision","","Conference paper","Final","","Scopus","2-s2.0-85050954207"
"Zhang Q.; Wang H.; Lu H.; Won D.; Yoon S.W.","Zhang, Qianqian (57196026486); Wang, Haifeng (57189446223); Lu, Hongya (57196019806); Won, Daehan (57192717855); Yoon, Sang Won (56154998800)","57196026486; 57189446223; 57196019806; 57192717855; 56154998800","Medical image synthesis with generative adversarial networks for tissue recognition","2018","Proceedings - 2018 IEEE International Conference on Healthcare Informatics, ICHI 2018","","","8419363","199","207","8","10.1109/ICHI.2018.00030","34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051125442&doi=10.1109%2fICHI.2018.00030&partnerID=40&md5=7c9887301278badd6b836e0431ecd348","This paper presents an adversarial learning-based approach to synthesize medical images for medical image tissue recognition. The performance of medical image recognition models highly depends on the representativeness and sufficiency of training samples. The high expense of collecting large amounts of practical medical images leads to a demand of synthesizing image samples. In this research, generative adversarial networks (GANs), which consist of a generative network and a discriminative network, are applied to develop a medical image synthesis model. Specifically, deep convolutional GANs (DCGANs), Wasserstein GANs (WGANs), and boundary equilibrium GANs (BEGANs) are implemented and compared to synthesize medical images in this research. Convolutional neural networks (CNNs) are applied in the GAN models, which can capture feature representations that describe a high level of image semantic information. Then synthetic images are generated by employing the generative network mapping from random noise. The effectiveness of the generative network is validated by a discriminative network, which is trained to detect the synthetic images from real images. Through a minimax two-player game, the generative and discriminative networks can train each other. The generated synthetic images are used to train a CNN classification model for tissue recognition. Through the experiments with the synthetic images, the tissue recognition accuracy achieves 98.83%, which reveals the effectiveness and applicability of synthesizing medical images through the GAN models. © 2018 IEEE.","Convolution; Game theory; Health care; Image processing; Image recognition; Neural networks; Semantics; Tissue; Adversarial networks; Boundary equilibrium; Classification models; Convolutional neural network; Discriminative networks; Feature representation; Image synthesis; Wasserstein distance; Medical imaging","Generative Adversarial Networks; Image Synthesis; Tissue Recognition; Wasserstein Distance","Conference paper","Final","","Scopus","2-s2.0-85051125442"
"Wu J.; Huang Z.; Thoma J.; Acharya D.; Van Gool L.","Wu, Jiqing (56566943500); Huang, Zhiwu (55643660000); Thoma, Janine (57195062496); Acharya, Dinesh (57192573752); Van Gool, Luc (22735702300)","56566943500; 55643660000; 57195062496; 57192573752; 22735702300","Wasserstein Divergence for GANs","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11209 LNCS","","","673","688","15","10.1007/978-3-030-01228-1_40","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055109763&doi=10.1007%2f978-3-030-01228-1_40&partnerID=40&md5=42f66c918a83ca84cda6980a72fec5f3","In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the family of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the k-Lipschitz constraint required by the Wasserstein-1 metric (W-met). In this paper, we propose a novel Wasserstein divergence (W-div), which is a relaxed version of W-met and does not require the k-Lipschitz constraint. As a concrete application, we introduce a Wasserstein divergence objective for GANs (WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks, showing the superior performance of WGAN-div compared to the state-of-the-art methods. © 2018, Springer Nature Switzerland AG.","Benchmarking; Adversarial networks; Concrete applications; GANs; Lipschitz constraints; State-of-the-art methods; Visual performance; Wasserstein divergence; Wasserstein metric; Computer vision","GANs; Progressive growing; Wasserstein divergence; Wasserstein metric","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055109763"
"Cha M.; Gwon Y.; Kung H.T.","Cha, Miriam (47061133900); Gwon, Youngjune (57219545224); Kung, H.T. (7402514205)","47061133900; 57219545224; 7402514205","Adversarial nets with perceptual losses for text-to-image synthesis","2017","IEEE International Workshop on Machine Learning for Signal Processing, MLSP","2017-September","","","1","6","5","10.1109/MLSP.2017.8168140","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042281588&doi=10.1109%2fMLSP.2017.8168140&partnerID=40&md5=61186d28121c0b0f379885103a02f4dc","Recent approaches in generative adversarial networks (GANs) can automatically synthesize realistic images from descriptive text. Despite the overall fair quality, the generated images often expose visible flaws that lack structural definition for an object of interest. In this paper, we aim to extend state of the art for GAN-based text-to-image synthesis by improving perceptual quality of generated images. Differentiated from previous work, our synthetic image generator optimizes on perceptual loss functions that measure pixel, feature activation, and texture differences against a natural image. We present visually more compelling synthetic images of birds and flowers generated from text descriptions in comparison to some of the most prominent existing work. © 2017 IEEE.","Artificial intelligence; Image enhancement; Learning systems; Signal processing; Adversarial networks; Conditional generative adversarial nets; Generative adversarial nets; Image synthesis; Perceptual quality; Realistic images; State of the art; Synthetic images; Image processing","Conditional generative adversarial nets; Generative adversarial nets; Text-to-image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85042281588"
"Ma X.; Jin R.; Sohn K.-A.; Paik J.Y.; Sun J.; Chung T.-S.","Ma, Xiaohan (57194789503); Jin, Rize (57606062800); Sohn, Kyung-Ah (56043382200); Paik, Joon Young (35189419700); Sun, Jing (57208648881); Chung, Tae-Sun (21033702700)","57194789503; 57606062800; 56043382200; 35189419700; 57208648881; 21033702700","Improving Generative Adversarial Networks with Adaptive Control Learning","2018","VCIP 2018 - IEEE International Conference on Visual Communications and Image Processing","","","8698669","","","","10.1109/VCIP.2018.8698669","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065424488&doi=10.1109%2fVCIP.2018.8698669&partnerID=40&md5=0417611964f32622528a6cd43ea48abc","Generative adversarial networks (GANs) are well known both for being unstable to train and for the problem of mode collapse, particularly when trained on data collections containing a diverse set of visual objects. This study introduces an adaptive hyper-parameter learning procedure for GANs as an alternative to the existing static approach. The proposed procedure is designed to mitigate the impact of instability and saturation in the original by dynamically adjusting the ratio of the training steps of both the generator and discriminator. To accomplish this, we track and analyze stable training curves of relatively narrow datasets and use them as the target fitting lines when training more diverse data collections. Experimental results show that the proposed model improves the stability and generates more realistic images. © 2018 IEEE.","Adaptive algorithms; Curve fitting; Data acquisition; Image enhancement; Visual communication; Adaptive Control; Adversarial networks; Data collection; Hyper-parameter; Image synthesis; Realistic images; Static approach; Visual objects; Adaptive control systems","Adaptive algorithm; Generative adversarial networks; Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85065424488"
"Creswell A.; White T.; Dumoulin V.; Arulkumaran K.; Sengupta B.; Bharath A.A.","Creswell, Antonia (57191416280); White, Tom (57219522961); Dumoulin, Vincent (55237034500); Arulkumaran, Kai (57191492242); Sengupta, Biswa (35422801900); Bharath, Anil A. (6701719807)","57191416280; 57219522961; 55237034500; 57191492242; 35422801900; 6701719807","Generative Adversarial Networks: An Overview","2018","IEEE Signal Processing Magazine","35","1","8253599","53","65","12","10.1109/MSP.2017.2765202","1063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040689867&doi=10.1109%2fMSP.2017.2765202&partnerID=40&md5=7056f331445310374e5fd7451d199fe8","Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application. © 1991-2012 IEEE.","Semantics; Adversarial networks; Annotated training data; Image super-resolution; Image synthesis; Semantic images; Signal processing","","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85040689867"
"Odena A.; Olah C.; Shlens J.","Odena, Augustus (57202500011); Olah, Christopher (57202497744); Shlens, Jonathon (8632398500)","57202500011; 57202497744; 8632398500","Conditional image synthesis with auxiliary classifier gans","2017","34th International Conference on Machine Learning, ICML 2017","6","","","4043","4055","12","","546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041893734&partnerID=40&md5=0f493d7c042137f471ff81dd09eb82e0","In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 x 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 x 128 samples are more than twice as discriminable as artificially resized 32 x 32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real Image Net data. Copyright © 2017 by the author(s).","Artificial intelligence; Image classification; Learning systems; Adversarial networks; Class information; Discriminability; Global coherence; High resolution; Image quality assessment; Image synthesis; Resolution images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85041893734"
"Dong H.; Liang X.; Gong K.; Lai H.; Zhu J.; Yin J.","Dong, Haoye (57208444380); Liang, Xiaodan (55926362100); Gong, Ke (57201377057); Lai, Hanjiang (35867865300); Zhu, Jia (55355019800); Yin, Jian (35316639800)","57208444380; 55926362100; 57201377057; 35867865300; 55355019800; 35316639800","Soft-gated warping-GaN for pose-guided person image synthesis","2018","Advances in Neural Information Processing Systems","2018-December","","","474","484","10","","93","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064834223&partnerID=40&md5=e163dd47c6c4ef99b3b2e9a590140ad7","Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is lightweight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our Warping-GAN that significantly outperforms all existing methods on two large datasets. © 2018 Curran Associates Inc..All rights reserved.","Gallium nitride; Geometry; III-V semiconductors; Large dataset; Mathematical transformations; Textures; Adversarial networks; Generative model; Geometric changes; Geometric transformations; Heavy occlusion; Quantitative evaluation; Segmentation map; Spatial displacement; Image segmentation","","Conference paper","Final","","Scopus","2-s2.0-85064834223"
"Xiang L.; Li Y.; Lin W.; Wang Q.; Shen D.","Xiang, Lei (57191518204); Li, Yang (56075073900); Lin, Weili (56999175100); Wang, Qian (57192157811); Shen, Dinggang (7401738392)","57191518204; 56075073900; 56999175100; 57192157811; 7401738392","Unpaired deep cross-modality synthesis with fast training","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11045 LNCS","","","155","164","9","10.1007/978-3-030-00889-5_18","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057231018&doi=10.1007%2f978-3-030-00889-5_18&partnerID=40&md5=5d617e01badef212c5fb0a03b70ef3d9","Cross-modality synthesis can convert the input image of one modality to the output of another modality. It is thus very valuable for both scientific research and clinical applications. Most existing cross-modality synthesis methods require large dataset of paired data for training, while it is often non-trivial to acquire perfectly aligned images of different modalities for the same subject. Even tiny misalignment (i.e., due patient/organ motion) between the cross-modality paired images may place adverse impact to training and corrupt the synthesized images. In this paper, we present a novel method for cross-modality image synthesis by training with the unpaired data. Specifically, we adopt the generative adversarial networks and conduct the fast training in cyclic way. A new structural dissimilarity loss, which captures the detailed anatomies, is introduced to enhance the quality of the synthesized images. We validate our proposed algorithm on three popular image synthesis tasks, including brain MR-to-CT, prostate MR-to-CT, and brain 3T-to-7T. The experimental results demonstrate that our proposed method can achieve good synthesis performance by using the unpaired data only. © Springer Nature Switzerland AG 2018.","Clinical research; Computerized tomography; Generative adversarial networks; Image enhancement; Medical imaging; Clinical application; Cross modality; Images synthesis; Input image; Large datasets; Non-trivial; Research applications; Scientific researches; Synthesis method; Synthesized images; Large dataset","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85057231018"
"Souza D.M.; Ruiz D.D.","Souza, Douglas M. (57204639868); Ruiz, Duncan D. (56187800200)","57204639868; 56187800200","GaN-based realistic face pose synthesis with continuous latent code","2018","Proceedings of the 31st International Florida Artificial Intelligence Research Society Conference, FLAIRS 2018","","","","110","115","5","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071907897&partnerID=40&md5=5b33e6f1fc4731df5ada494699a2e68f","We present a novel approach for face pose synthesis. We leverage the power of Generative Adversarial Networks to synthesize face poses in a realistic fashion. We apply a conditioning method to control the rotation of synthesized faces along the three axes of space (roll, pitch, yaw). We start by estimating the pose of each face in the training set and storing a vector containing the rotation angles. Then, we use the images along with the angles to train a conditioned version of a state-of-the-art Generative Adversarial Network. Our experiments show image synthesis with state-of-the-art quality, plus the absolute control of the pose of synthesized face images. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Artificial intelligence; Gallium nitride; III-V semiconductors; Adversarial networks; Face images; GaN based; Image synthesis; Rotation angles; State of the art; Three axes; Training sets; Quality control","","Conference paper","Final","","Scopus","2-s2.0-85071907897"
"Liu X.; Meng G.; Xiang S.; Pan C.","Liu, Xiyan (57205361759); Meng, Gaofeng (16317032400); Xiang, Shiming (8938807200); Pan, Chunhong (8558023500)","57205361759; 16317032400; 8938807200; 8558023500","Semantic Image Synthesis via Conditional Cycle-Generative Adversarial Networks","2018","Proceedings - International Conference on Pattern Recognition","2018-August","","8545383","988","993","5","10.1109/ICPR.2018.8545383","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059749058&doi=10.1109%2fICPR.2018.8545383&partnerID=40&md5=4df3b324985c2b93d9805cc15c9005d5","Traditional approaches for semantic image synthesis mainly focus on text descriptions while ignoring the related structures and attributes in the original images. Therefore, some critical information, e.g., the style, backgrounds, objects shapes and pose, is missed in the generated images. In this paper, we propose a novel framework called Conditional Cycle-Generative Adversarial Network (CCGAN) to address this issue. Our model can generate photo-realistic images conditioned on the given text descriptions, while maintaining the attributes of the original images. The framework mainly consists of two coupled conditional adversarial networks, which are able to learn a desirable image mapping that can keep the structures and attributes in the images. We introduce a conditional cycle consistency loss to prevent the contradiction between two generators. This loss allows the generated images to retain most of the features of the original image, so as to improve the stability of network training. Moreover, benefiting from the mechanism of circular training, the proposed networks can learn the semantic information of the text much accurately. Experiments on Caltech-UCSD Bird dataset and Oxford-102 flower dataset demonstrate that the proposed method significantly outperforms the existing methods in terms of image details reconstruction and semantic information expression. © 2018 IEEE.","Pattern recognition; Semantic Web; Semantics; Adversarial networks; Image mapping; Network training; Original images; Photorealistic images; Semantic images; Semantic information; Traditional approaches; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85059749058"
"Li C.; Wang Z.; Qi H.","Li, Chengcheng (56374728100); Wang, Zi (57191986725); Qi, Hairong (7202348750)","56374728100; 57191986725; 7202348750","Fast-Converging Conditional Generative Adversarial Networks for Image Synthesis","2018","Proceedings - International Conference on Image Processing, ICIP","","","8451161","2132","2136","4","10.1109/ICIP.2018.8451161","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062901120&doi=10.1109%2fICIP.2018.8451161&partnerID=40&md5=d02534957735a7569e92d28e28ffbd0b","Building on top of the success of generative adversarial networks (GANs), conditional GANs attempt to better direct the data generation process by conditioning with certain additional information. Inspired by the most recent AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In addition to the real/fake classifier used in vanilla GANs, our discriminator has an advanced auxiliary classifier which distinguishes each real class from an extra 'fake' class. The 'fake' class avoids mixing generated data with real data, which can potentially confuse the classification of real data as AC-GAN does, and makes the advanced auxiliary classifier behave as another real/fake classifier. As a result, FC-GAN can accelerate the process of differentiation of all classes, thus boost the convergence speed. Experimental results on image synthesis demonstrate our model is competitive in the quality of images generated while achieving a faster convergence rate. © 2018 IEEE.","Natural gas conditioning; Adversarial networks; Convergence speed; Data generation; Fast convergence; Faster convergence; Image synthesis; Image processing","Conditioning; Fast convergence; Generative adversarial networks; Image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062901120"
"Osokin A.; Chessel A.; Salas R.E.C.; Vaggi F.","Osokin, Anton (57210766875); Chessel, Anatole (14035217500); Salas, Rafael E. Carazo (6603440015); Vaggi, Federico (48261428700)","57210766875; 14035217500; 6603440015; 48261428700","GANs for Biological Image Synthesis","2017","Proceedings of the IEEE International Conference on Computer Vision","2017-October","","8237507","2252","2261","9","10.1109/ICCV.2017.245","60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041915494&doi=10.1109%2fICCV.2017.245&partnerID=40&md5=a27ed959e73f745c2949125ca84d3027","In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multichannel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images. © 2017 IEEE.","Biology; Biosynthesis; Computer vision; Fluorescence; Fluorescence microscopy; Proteins; Adversarial networks; Biological applications; Biological functions; Fluorescent protein; Multichannel images; Novel applications; Protein localization; Synthesized images; Image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85041915494"
"Bowles C.; Gunn R.; Hammers A.; Rueckert D.","Bowles, Christopher (56893321000); Gunn, Roger (7101894687); Hammers, Alexander (8229046700); Rueckert, Daniel (7004895812)","56893321000; 7101894687; 8229046700; 7004895812","Modelling the progression of Alzheimer's disease in MRI using generative adversarial networks","2018","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","10574","","105741K","","","","10.1117/12.2293256","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047341082&doi=10.1117%2f12.2293256&partnerID=40&md5=d4d834022082261fe36b07748cf7136a","Being able to accurately model the progression of Alzheimer's disease (AD) is important for the diagnosis and prognosis of the disease, as well as to evaluate the effect of disease modifying treatments. Whilst there has been success in modeling the progression of AD related clinical biomarkers and image derived features over the course of the disease, modeling the expected progression as observed by magnetic resonance (MR) images directly remains a challenge. Here, we apply some recently developed ideas from the field of generative adversarial networks (GANs) which provide a powerful way to model and manipulate MR images directly though the technique of image arithmetic. This allows for synthetic images based upon an individual subject's MR image to be produced expressing different levels of the features associated with AD. We demonstrate how the model can be used to both introduce and remove AD-like features from two regions in the brain, and show that these predicted changes correspond well to the observed changes over a longitudinal examination. We also propose a modification to the GAN training procedure to encourage the model to better represent the more extreme cases of AD present in the dataset. We show the benefit of this modification by comparing the ability of the resulting models to encode and reconstruct real images with high atrophy and other unusual features. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Diagnosis; Magnetic levitation vehicles; Magnetic resonance; Magnetic resonance imaging; Medical image processing; Medical imaging; Neurodegenerative diseases; Adversarial networks; Alzheimer's disease; Image synthesis; Latent images; MR images; Shape Modelling; Image processing","Alzheimer's Disease; Generative Adversarial Networks; Image Synthesis; Latent Image Arithmetic; MR Image Modelling; Shape Modelling","Conference paper","Final","","Scopus","2-s2.0-85047341082"
