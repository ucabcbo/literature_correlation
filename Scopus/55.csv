"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Karaca A.C.; Kara O.; Güllü M.K.","Karaca, Ali Can (55292760600); Kara, Ozan (57221815490); Güllü, Mehmet Kemal (55666247200)","55292760600; 57221815490; 55666247200","MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks","2021","Journal of Visual Communication and Image Representation","81","","103385","","","","10.1016/j.jvcir.2021.103385","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119176940&doi=10.1016%2fj.jvcir.2021.103385&partnerID=40&md5=0dbc36c8a377487bfb873208d19d64b7","Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications. © 2021 Elsevier Inc.","Big data; Deep learning; Generative adversarial networks; Geographical regions; Image compression; Mean square error; Signal to noise ratio; Change detection; Compression methods; Multi-spectral; Multi-temporal; Multi-temporal image; Multispectral images; Multispectral-image compression; Reflected energy; Remote-sensing; Spectral correlation; Remote sensing","Big data; Generative adversarial networks; Multispectral image compression; Multitemporal images; Remote sensing","Article","Final","","Scopus","2-s2.0-85119176940"
"Holgado Alvarez J.L.; Ravanbakhsh M.; Demir B.","Holgado Alvarez, Jose Luis (57222243160); Ravanbakhsh, Mahdyar (57192545758); Demir, Begum (15131434800)","57222243160; 57192545758; 15131434800","S2-CGAN: Self-Supervised Adversarial Representation Learning for Binary Change Detection in Multispectral Images","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324345","2515","2518","3","10.1109/IGARSS39084.2020.9324345","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101992272&doi=10.1109%2fIGARSS39084.2020.9324345&partnerID=40&md5=171559755fde98baaedcb935473c67c0","Deep Neural Networks have recently demonstrated promising performance in binary change detection (CD) problems in remote sensing (RS), requiring a large amount of labeled multitemporal training samples. Since collecting such data is time-consuming and costly, most of the existing methods rely on pre-trained networks on publicly available computer vision (CV) datasets. However, because of the differences in image characteristics in CV and RS, this approach limits the performance of the existing CD methods. To address this problem, we propose a self-supervised conditional Generative Adversarial Network (S2-cGAN). The proposed S2-cGAN is trained to generate only the distribution of unchanged samples. To this end, the proposed method consists of two main steps: 1) Generating a reconstructed version of the input image as an unchanged image 2) Learning the distribution of unchanged samples through an adversarial game. Unlike the existing GAN based methods (which only use the discriminator during the adversarial training to supervise the generator), the S2-cGAN directly exploits the discriminator likelihood to solve the binary CD task. Experimental results show the effectiveness of the proposed S2-cGAN when compared to the state of the art CD methods. © 2020 IEEE.","Deep learning; Deep neural networks; Geology; Adversarial networks; Change detection; Image characteristics; Large amounts; Multi-temporal; Multispectral images; State of the art; Training sample; Remote sensing","binary change detection; Generative adversarial networks; multitemporal images; remote sensing; self-supervised learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101992272"
"","","","9th Pacific-Rim Symposium on Image and Video Technology, PSIVT 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11854 LNCS","","","","","415","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076394301&partnerID=40&md5=2b0119c2eddcec0a4448f95acfe2ced6","The proceedings contain 31 papers. The special focus in this conference is on Image and Video Technology. The topics include: Efficient Self-embedding Data Hiding for Image Integrity Verification with Pixel-Wise Recovery Capability; enhanced Transfer Learning with ImageNet Trained Classification Layer; equine Welfare Assessment: Horse Motion Evaluation and Comparison to Manual Pain Measurements; exposure Correction and Local Enhancement for Backlit Image Restoration; grapevine Nutritional Disorder Detection Using Image Processing; Hierarchical Colour Image Segmentation by Leveraging RGB Channels Independently; high-Resolution Realistic Image Synthesis from Text Using Iterative Generative Adversarial Network; human Shape Reconstruction with Loose Clothes from Partially Observed Data by Pose Specific Deformation; improved Saliency-Enhanced Multi-cue Correlation-Filter-Based Visual Tracking; a Robust Face Recognition System for One Sample Problem; measuring Apple Size Distribution from a Near Top–Down Image; multi-temporal Registration of Environmental Imagery Using Affine Invariant Convolutional Features; Multimodal 3D Facade Reconstruction Using 3D LiDAR and Images; multiview Dimension Reduction Based on Sparsity Preserving Projections; non-peaked Discriminant Analysis for Image Representation; prostate Cancer Classification Based on Best First Search and Taguchi Feature Selection Method; real-Time Retinal Vessel Segmentation on High-Resolution Fundus Images Using Laplacian Pyramids; RVNet: Deep Sensor Fusion of Monocular Camera and Radar for Image-Based Obstacle Detection in Challenging Environments; semantic Segmentation of Grey-Scale Traffic Scenes; Shoeprint Extraction via GAN; analysis of Motion Patterns in Video Streams for Automatic Health Monitoring in Koi Ponds; turnstile Jumping Detection in Real-Time Video Surveillance; unsupervised Deep Features for Privacy Image Classification; attention-Guided Model for Robust Face Detection System.","","","Conference review","Final","","Scopus","2-s2.0-85076394301"
"Zhang X.; Pun M.-O.; Liu M.","Zhang, Xiaokang (56703557200); Pun, Man-On (6603645302); Liu, Ming (57216968027)","56703557200; 6603645302; 57216968027","Semi‐supervised multi‐temporal deep representation fusion network for landslide mapping from aerial orthophotos","2021","Remote Sensing","13","4","548","1","22","21","10.3390/rs13040548","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100661538&doi=10.3390%2frs13040548&partnerID=40&md5=8255545a48d1424b03ae2b6033f93a03","Using remote sensing techniques to monitor landslides and their resultant land cover changes is fundamentally important for risk assessment and hazard prevention. Despite enormous efforts in developing intelligent landslide mapping (LM) approaches, LM remains challenging ow-ing to high spectral heterogeneity of very‐high‐resolution (VHR) images and the daunting labeling efforts. To this end, a deep learning model based on semi‐supervised multi‐temporal deep representation fusion network, namely SMDRF‐Net, is proposed for reliable and efficient LM. In comparison with previous methods, the SMDRF‐Net possesses three distinct properties. 1) Unsupervised deep representation learning at the pixel‐ and object‐level is performed by transfer learning using the Wasserstein generative adversarial network with gradient penalty to learn discriminative deep features and retain precise outlines of landslide objects in the high‐level feature space. 2) At-tention‐based adaptive fusion of multi‐temporal and multi‐level deep representations is developed to exploit the spatio‐temporal dependencies of deep representations and enhance the feature representation capability of the network. 3) The network is optimized using limited samples with pseudo‐labels that are automatically generated based on a comprehensive uncertainty index. Ex-perimental results from the analysis of VHR aerial orthophotos demonstrate the reliability and ro-bustness of the proposed approach for LM in comparison with state‐of‐the‐art methods. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Antennas; Landslides; Photomapping; Reliability analysis; Remote sensing; Risk assessment; Transfer learning; Adversarial networks; Aerial orthophotos; Automatically generated; Feature representation; Hazard prevention; Landslide mapping; Remote sensing techniques; Spectral heterogeneity; Deep learning","Attention; Deep representation learning; Landslide mapping; Multi‐temporal fusion; Semi‐supervised; WGAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100661538"
"Jiang F.; Gong M.; Zhan T.; Fan X.","Jiang, Fenlong (57210730944); Gong, Maoguo (8933846400); Zhan, Tao (57052462500); Fan, Xiaolong (57209827950)","57210730944; 8933846400; 57052462500; 57209827950","A Semisupervised GAN-Based Multiple Change Detection Framework in Multi-Spectral Images","2020","IEEE Geoscience and Remote Sensing Letters","17","7","8854295","1223","1227","4","10.1109/LGRS.2019.2941318","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090386350&doi=10.1109%2fLGRS.2019.2941318&partnerID=40&md5=9996c3aeb4484448b04822ecaf9650ad","Effectively highlighting multiple changes in the earth surface from multi-temporal remote sensing images is a meaningful but challenging task. In order to reduce costs and ensure the performance, it is advisable to employ a semisupervised strategy to achieve this goal. As a discriminative joint classification task, semisupervised change detection aims to extract useful and discriminative features from a large amount of unlabeled data in addition to limited labeled samples. The discriminator of a well-trained generative adversarial network (GAN) is just right for this. Therefore, in this letter, we proposed a semisupervised GAN-based multiple change detection framework for multi-spectral images. First, the GAN is trained by all data without any prior information. Then, we combine two identical trained discriminators to construct a dual-pipeline joint classifier. Finally, the classifier is fine-tuned by a very small amount of labeled data to detect multiple changes. The superior performance of the proposed model over both real multi-spectral data sets demonstrates its robustness and effectiveness.  © 2004-2012 IEEE.","Pipelines; Spectroscopy; Adversarial networks; Change detection; Classification tasks; Discriminative features; Multi-spectral data; Multi-temporal remote sensing; Multispectral images; Prior information; algorithm; detection method; image analysis; spectral analysis; supervised learning; Remote sensing","Generative adversarial network (GAN); multi-spectral images; multiple change detection; semisupervised","Article","Final","","Scopus","2-s2.0-85090386350"
"Liu J.; Chen K.; Xu G.; Li H.; Yan M.; DIao W.; Sun X.","Liu, Junfu (57213195524); Chen, Keming (55683543900); Xu, Guangluan (56420820800); Li, Hao (57196362306); Yan, Menglong (57200852160); DIao, Wenhui (56816620400); Sun, Xian (34875643000)","57213195524; 55683543900; 56420820800; 57196362306; 57200852160; 56816620400; 34875643000","Semi-Supervised Change Detection Based on Graphs with Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898913","74","77","3","10.1109/IGARSS.2019.8898913","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077722752&doi=10.1109%2fIGARSS.2019.8898913&partnerID=40&md5=48946b2c31785f5179da3c7ed8e39908","In this paper, we present a semi-supervised remote sensing change detection method based on graph model with Generative Adversarial Networks (GANs). Firstly, the multi-temporal remote sensing change detection problem is converted as a problem of semi-supervised learning on graph where a majority of unlabeled nodes and a few labeled nodes are contained. Then, GANs are adopted to generate samples in a competitive manner and help improve the classification accuracy. Finally, a binary change map is produced by classifying the unlabeled nodes to a certain class with the help of both the labeled nodes and the unlabeled nodes on graph. Experimental results carried on several very high resolution remote sensing image data sets demonstrate the effectiveness of our method. © 2019 IEEE.","Geology; Graph theory; Semi-supervised learning; Adversarial networks; Change detection; Classification accuracy; Graph model; Multi-temporal remote sensing; Semi-supervised; Very high resolution; Remote sensing","Change Detection; Generative Adversarial Network; Graph Model; Semi-supervised Learning","Conference paper","Final","","Scopus","2-s2.0-85077722752"
"He W.; Yokoya N.","He, Wei (55908554500); Yokoya, Naoto (36440631200)","55908554500; 36440631200","Multi-temporal sentinel-1 and -2 data fusion for optical Image Simulation","2018","ISPRS International Journal of Geo-Information","7","10","389","","","","10.3390/ijgi7100389","59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056671515&doi=10.3390%2fijgi7100389&partnerID=40&md5=af751b20fb7f715d36e2ed504c44d7ed","In this paper, we present the optical image simulation from synthetic aperture radar (SAR) data using deep learning based methods. Two models, i.e., optical image simulation directly from the SAR data and from multi-temporal SAR-optical data, are proposed to testify the possibilities. The deep learning based methods that we chose to achieve the models are a convolutional neural network (CNN) with a residual architecture and a conditional generative adversarial network (cGAN). We validate our models using the Sentinel-1 and -2 datasets. The experiments demonstrate that the model with multi-temporal SAR-optical data can successfully simulate the optical image; meanwhile, the state-of-the-art model with simple SAR data as input failed. The optical image simulation results indicate the possibility of SAR-optical information blending for the subsequent applications such as large-scale cloud removal, and optical data temporal super-resolution. We also investigate the sensitivity of the proposed models against the training samples, and reveal possible future directions. © 2018 by the authors.","","Convolutional neural network; Data simulation; Generative adversarial network; Optical; Sentinel; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056671515"
"Bermudez J.D.; Happ P.N.; Feitosa R.Q.; Oliveira D.A.B.","Bermudez, Jose D. (57200270007); Happ, Patrick N. (55768214000); Feitosa, Raul Q. (6602453684); Oliveira, Dario A.B. (27567900100)","57200270007; 55768214000; 6602453684; 27567900100","Synthesis of Multispectral Optical Images from SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks","2019","IEEE Geoscience and Remote Sensing Letters","16","8","8637007","1220","1224","4","10.1109/LGRS.2019.2894734","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069501813&doi=10.1109%2fLGRS.2019.2894734&partnerID=40&md5=273712603dde1a4c2477a5546fab9439","The synthesis of realistic data using deep learning techniques has greatly improved the performance of classifiers in handling incomplete data. Remote sensing applications that have profited from those techniques include translating images of different sensors, improving the image resolution and completing missing temporal or spatial data such as in cloudy optical images. In this context, this letter proposes a new deep-learning-based framework to synthesize missing or corrupted multispectral optical images using multimodal/multitemporal data. Specifically, we use conditional generative adversarial networks (cGANs) to generate the missing optical image by exploiting the correspondent synthetic aperture radar (SAR) data with a SAR-optical data from the same area at a different acquisition date. The proposed framework was evaluated in two land-cover applications over tropical regions, where cloud coverage is a major problem: crop recognition and wildfire detection. In both applications, our proposal was superior to alternative approaches tested in our experiments. In particular, our approach outperformed recent cGAN-based proposals for cloud removal, on average, by 7.7% and 8.6% in terms of overall accuracy and F1-score, respectively. © 2019 IEEE.","Crops; Data handling; Deep learning; Fires; Geometrical optics; Image resolution; Radar imaging; Remote sensing; Synthetic aperture radar; Adversarial networks; Crop recognition; Land cover applications; Learning techniques; Multi-temporal data; Performance of classifier; Remote sensing applications; Wildfire detection; artificial neural network; data acquisition; experimental study; image analysis; image resolution; optical method; pattern recognition; remote sensing; satellite data; synthetic aperture radar; wildfire; Image enhancement","Conditional generative adversarial networks (cGANs); crop recognition; deep learning; remote sensing; wildfire detection","Article","Final","","Scopus","2-s2.0-85069501813"
"Saha S.; Solano-Correa Y.T.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Solano-Correa, Yady Tatiana (57156565000); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 57156565000; 9943212600; 7006892410","Unsupervised Deep Transfer Learning-Based Change Detection for HR Multispectral Images","2021","IEEE Geoscience and Remote Sensing Letters","18","5","9089195","856","860","4","10.1109/LGRS.2020.2990284","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104852204&doi=10.1109%2fLGRS.2020.2990284&partnerID=40&md5=c51d7664d31828a2b0bd499c41222563","To overcome the limited capability of most state-of-the-art change detection (CD) methods in modeling spatial context of multispectral high spatial resolution (HR) images and exploiting all spectral bands jointly, this letter presents a novel unsupervised deep-learning-based CD method that can effectively model contextual information and handle the large number of bands in multispectral HR images. This is achieved by exploiting all spectral bands after grouping them into spectral-dedicated band groups. To eliminate the necessity of multitemporal training data, the proposed method exploits a data set targeted for image classification to train spectral-dedicated Auxiliary Classifier Generative Adversarial Networks (ACGANs). They are used to obtain pixelwise deep change hypervector from multitemporal images. Each feature in deep change hypervector is analyzed based on the magnitude to identify changed pixels. An ensemble decision fusion strategy is used to combine change information from different features. Experimental results on the urban, Alpine, and agricultural Sentinel-2 data sets confirm the effectiveness of the proposed method. © 2004-2012 IEEE.","Agricultural robots; Classification (of information); Transfer learning; Adversarial networks; Change detection; Contextual information; High spatial resolution; Multi-temporal image; Multispectral images; Spatial context; State of the art; data set; detection method; image analysis; image classification; machine learning; numerical model; pixel; spatial resolution; spectral analysis; unsupervised classification; Deep learning","Change detection (CD); deep learning; generative adversarial network; high resolution; Sentinel-2","Article","Final","","Scopus","2-s2.0-85104852204"
"Li X.; Zhang L.; Wang Q.; Ai H.","Li, Xue (49663402500); Zhang, Li (55943300700); Wang, Qingdong (57220167155); Ai, Haibin (36974932200)","49663402500; 55943300700; 57220167155; 36974932200","Multi-temporal remote sensing imagery semantic segmentation color consistency adversarial network; [多时相遥感影像语义分割色彩一致性对抗网络]","2020","Cehui Xuebao/Acta Geodaetica et Cartographica Sinica","49","11","","1473","1484","11","10.11947/j.AGCS.2020.20190439","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097042530&doi=10.11947%2fj.AGCS.2020.20190439&partnerID=40&md5=885d5a731b8d3eec0c337c81d1def3fd","Using deep convolutional neural network (CNN) to intelligently extract buildings from remote sensing images is of great significance for digital city construction, disaster detection and land management. The color difference between multi-temporal remote sensing images will lead to the decrease of generalization ability of building semantic segmentation model. In view of this, this paper proposes the attention-guided color consistency adversarial network (ACGAN). The algorithm takes the reference color style images and the images to be corrected in the same area and different phases as the training set and adopts the consistency adversarial network with the U-shaped attention mechanism to train the color consistency model. In the prediction stage, this model converts the hue of the images to that of the reference color style image, which is based on the reasoning ability of the deep learning model, instead of the corresponding reference color style image. This model transforms the hue of the images to be corrected into that of the reference color style images. This stage is based on the reasoning ability of the deep learning model, and the corresponding reference color style image is no longer needed. In order to verify the effectiveness of the algorithm, firstly, we compare the algorithm of this paper with the traditional image processing algorithm and other consistency adversarial network. The results show that the images after ACGAN color consistency processing are more similar to that of the reference color style images. Secondly, we carried out the building semantic segmentation experiment on the images processed by the above different color consistency algorithms, which proved that the method in this paper is more conducive to the impro-vement of the generalization ability of multi-temporal remote sensing image semantic segmentation model. © 2020, Surveying and Mapping Press. All right reserved.","Color; Colorimetry; Convolutional neural networks; Deep learning; Deep neural networks; Image segmentation; Learning systems; Remote sensing; Semantic Web; Semantics; Adversarial networks; Attention mechanisms; Generalization ability; Image processing algorithm; Multi-temporal remote sensing; Reasoning ability; Remote sensing images; Semantic segmentation; algorithm; artificial neural network; color; experimental study; image processing; remote sensing; satellite imagery; segmentation; training; Color image processing","Attention mechanism; Color consistency; Generative adversarial networks; Multi-temporal remote sensing imagery; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85097042530"
"Huang G.-L.; Wu P.-Y.","Huang, Gi-Luen (58075611900); Wu, Pei-Yuan (55413171800)","58075611900; 55413171800","CTGAN: CLOUD TRANSFORMER GENERATIVE ADVERSARIAL NETWORK","2022","Proceedings - International Conference on Image Processing, ICIP","","","","511","515","4","10.1109/ICIP46576.2022.9897229","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146693577&doi=10.1109%2fICIP46576.2022.9897229&partnerID=40&md5=1fd3edd319b5b3b6136aedd14c984e3e","Cloud occlusions obstruct some applications of remote sensing imagery, such as environment monitoring, land cover classification, and poverty prediction. In this paper, we propose the Cloud Transformer Generative Adversarial Network (CTGAN), taking three temporal cloudy images as input and generating a corresponding cloud-free image. Unlike previous work using generative networks, we design the feature extractor to maintain the weight of the cloudless region while reducing the weight of the cloudy region, and we pass the extracted features to a conformer module to find the most critical representations. Meanwhile, to address the lack of datasets, we collected a new dataset named Sen2 MTC from the Sentinel-2 satellite and manually labeled each cloudy and cloud-free image. Finally, we conducted extensive experiments on FS-2, the STGAN dataset, and Sen2 MTC. Our proposed CTGAN demonstrates higher qualitative and quantitative performance than the previous work and achieves state-of-the-art performance on these three datasets. The code is available at https://github.com/come880412/CTGAN. © 2022 IEEE.","Remote sensing; Satellites; Cloud removal; Cloud removal for multi-temporal cloudy image; Conformer; Environment monitoring; Formosat-2; Formosat-2 satellite; Land cover classification; Multi-temporal; Remote sensing imagery; Sentinel-2 satellite; Generative adversarial networks","Cloud removal for multi-temporal cloudy images; conformer; FormoSat-2 satellite; generative adversarial network; Sentinel-2 satellite","Conference paper","Final","","Scopus","2-s2.0-85146693577"
"Haoyang T.; Jiaxin X.; Yang L.; Dongfang Y.","Haoyang, Tang (57217945713); Jiaxin, Xiao (57439193800); Yang, Liu (57203122217); Dongfang, Yang (57220117110)","57217945713; 57439193800; 57203122217; 57220117110","An Edge Feature Extraction Method for Remote Sensing Image Edge Based on Generative Adversarial Strategy","2021","10th International Conference on Control, Automation and Information Sciences, ICCAIS 2021 - Proceedings","","","","524","531","7","10.1109/ICCAIS52680.2021.9624520","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124034369&doi=10.1109%2fICCAIS52680.2021.9624520&partnerID=40&md5=6887d9a28fffdcd2b493f96a1f32bca1","Affected by changes in illumination, weather, and surface activities, multi-temporal remote sensing image data changes slowly all the time. How to extract edge feature is a key common problem that needs to be resolved in the field of remote sensing image intelligent processing. This paper proposes an edge feature extraction method for remote sensing image edges based on a generative confrontation strategy. This method first designs a CycleGAN network based on the Smooth L1 loss function to meet the requirements of robust feature extraction for edges with semantic information in remote sensing images. The network transfers the style of remote sensing images, and finally extracts edge feature for the images after the style transfer, which is used as the description of the robust features on the surface. Experimental results show that the edge extraction model designed in this paper improves the accuracy of robust feature extraction of remote sensing images by 8.1%. In addition, the method is less affected by environmental factors such as illumination, and can realize the intelligent extraction of robust features of the remote sensing image. The relevant code of this article has been published on the research group's home page. © 2021 IEEE.","Extraction; Feature extraction; Image enhancement; Remote sensing; Semantics; Edge features; Edge-based; Feature extraction methods; Image edge; Multi-temporal remote sensing; Remote sensing image processing; Remote sensing images; Robust feature extractions; Style migration; Surface activities; Generative adversarial networks","Generative adversarial network; Remote sensing image processing; Robust feature extraction; Style migration","Conference paper","Final","","Scopus","2-s2.0-85124034369"
"Wu P.; Su Y.; Duan S.-B.; Li X.; Yang H.; Zeng C.; Ma X.; Wu Y.; Shen H.","Wu, Penghai (55644317800); Su, Yang (57812006400); Duan, Si-bo (26429409700); Li, Xinghua (55626987300); Yang, Hui (36835970800); Zeng, Chao (37056513600); Ma, Xiaoshuang (55768476100); Wu, Yanlan (12141267000); Shen, Huanfeng (8359721100)","55644317800; 57812006400; 26429409700; 55626987300; 36835970800; 37056513600; 55768476100; 12141267000; 8359721100","A two-step deep learning framework for mapping gapless all-weather land surface temperature using thermal infrared and passive microwave data","2022","Remote Sensing of Environment","277","","113070","","","","10.1016/j.rse.2022.113070","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134589072&doi=10.1016%2fj.rse.2022.113070&partnerID=40&md5=a697e9a8dbf883609480a924d74fce26","Blending data from thermal infrared (TIR) and passive microwave (PMW) measurements is a promising solution for generating the all-weather land surface temperature (LST). However, owing to swath gaps in PMW data and the resolution inconsistence between TIR and PWM data, spatial details are often incomplete or considerable losses are generated in the all-weather LST using traditional methods. This study was conducted to develop a two-step deep learning framework (TDLF) for mapping gapless all-weather LST over the China's landmass using MODIS and AMSR-E LST data. In the TDLF, a multi-temporal feature connected convolutional neural network bidirectional reconstruction model was developed to obtain the spatially complete AMSR-E LST. A multi-scale multi-temporal feature connected generative adversarial network model was then designed to blend spatially complete AMSR-E LST and cloudy-sky MODIS LST, and generate gapless all-weather LST data. Gapless all-weather LST data were evaluated using six in-situ LST data from the Tibetan Plateau (TP) and the Heihe River Basin (HRB). The root mean squared errors (RMSEs) of the gapless all-weather LST were 1.71–2.0 K with determination coefficients (R2) of 0.94–0.98 under clear conditions, and RMSEs of 3.41–3.87 K and R2 of 0.88–0.94 were obtained under cloudy conditions. Compared to the existing PMW-based all-weather LSTs, the validation accuracy and image quality (such as spatial detail) of the generated gapless all-weather LSTs were superior. The TDLF does not require the use of any additional data and can potentially be implemented with other satellite TIR and PWM sensors to produce long-term, gapless, all-weather MODIS LST records on a global scale. Such a capability is beneficial for generating further gapless all-weather soil moisture and evapotranspiration datasets that can all be applied in global climate change research. © 2022","China; Gansu; Hei River; Qinghai-Xizang Plateau; Atmospheric temperature; Climate change; Generative adversarial networks; Infrared radiation; Land surface temperature; Mapping; Mean square error; Pulse width modulation; Radiometers; Remote sensing; Soil moisture; Surface measurement; Surface properties; All-weather; Deep learning; Gapless; Land surface temperature; Learning frameworks; Multi-temporal; Passive microwave data; Satellite remote sensing; Temperature data; Thermal-infrared; AMSR-E; climate change; evapotranspiration; global climate; image analysis; land surface; machine learning; mapping method; MODIS; reconstruction; satellite altimetry; satellite data; weather; Deep learning","All-weather; Deep learning; Gapless; Land surface temperature; Satellite remote sensing","Article","Final","","Scopus","2-s2.0-85134589072"
"Sebastianelli A.; Puglisi E.; Del Rosso M.P.; Mifdal J.; Nowakowski A.; Mathieu P.P.; Pirri F.; Ullo S.L.","Sebastianelli, Alessandro (57202958220); Puglisi, Erika (57226605784); Del Rosso, Maria Pia (57221180323); Mifdal, Jamila (57200605283); Nowakowski, Artur (57527552900); Mathieu, Pierre Philippe (12241655700); Pirri, Fiora (56990245000); Ullo, Silvia Liberata (6503914067)","57202958220; 57226605784; 57221180323; 57200605283; 57527552900; 12241655700; 56990245000; 6503914067","PLFM: Pixel-Level Merging of Intermediate Feature Maps by Disentangling and Fusing Spatial and Temporal Data for Cloud Removal","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5412216","","","","10.1109/TGRS.2022.3208694","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139437659&doi=10.1109%2fTGRS.2022.3208694&partnerID=40&md5=e1abd7c8853e146aa31e7b4680ee5bdf","Cloud removal is a relevant topic in remote sensing, fostering medium- and high-resolution optical (OPT) image usability for Earth monitoring and study. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps: the amount of cloud coverage, the landscape temporal changes, and the density and thickness of clouds need further investigation. We fill some of these gaps in this work by introducing an innovative deep model. The proposed model is multimodal, relying on both spatial and temporal sources of information to restore the whole optical scene of interest. We use the outcomes of both temporal-sequence blending and direct translation from synthetic aperture radar (SAR) to optical images to obtain a pixel-wise restoration of the whole scene. The reconstructed images preserve scene details without resorting to a considerable portion of a clean image. Our approach's advantage is demonstrated across various atmospheric conditions tested on different datasets. Quantitative and qualitative results prove that the proposed method obtains cloud-free images coping with landscape changes.  © 1980-2012 IEEE.","Data fusion; Generative adversarial networks; Geometrical optics; Hierarchical systems; Image reconstruction; Optical remote sensing; Pixels; Radar imaging; Restoration; Adaptation models; Cloud removal; Conditional generative adversarial network; Convolutional long short-term memory; Deep hierarchical model; Hierarchical model; Images reconstruction; Multi-temporal remote sensing; Multitemporal remote sensing image; Optical data; Optical imaging; Radar polarimetry; Remote sensing images; Synthetic aperture radar-optical data fusion; hierarchical system; map; pixel; remote sensing; spatial data; synthetic aperture radar; temporal analysis; Synthetic aperture radar","Cloud removal (CR); conditional generative adversarial networks (cGANs); convolutional long short-term memory (ConvLSTM); deep hierarchical model; multitemporal remote sensing (RS) images; synthetic aperture radar (SAR)-optical (OPT) data fusion","Article","Final","","Scopus","2-s2.0-85139437659"
"Li J.; Wu Z.; Sheng Q.; Wang B.; Hu Z.; Zheng S.; Camps-Valls G.; Molinier M.","Li, Jun (57202722259); Wu, Zhaocong (15023850900); Sheng, Qinghong (36562635800); Wang, Bo (57190495006); Hu, Zhongwen (55630272400); Zheng, Shaobo (57852261300); Camps-Valls, Gustau (6603888005); Molinier, Matthieu (22234853700)","57202722259; 15023850900; 36562635800; 57190495006; 55630272400; 57852261300; 6603888005; 22234853700","A hybrid generative adversarial network for weakly-supervised cloud detection in multispectral images","2022","Remote Sensing of Environment","280","","113197","","","","10.1016/j.rse.2022.113197","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136277692&doi=10.1016%2fj.rse.2022.113197&partnerID=40&md5=58da582698f1afcd1e7983df21b2c735","Cloud detection is a crucial step in the optical satellite image processing pipeline for Earth observation. Clouds in optical remote sensing images seriously affect the visibility of the background and greatly reduce the usability of images for land applications. Traditional methods based on thresholding, multi-temporal or multi-spectral information are often specific to a particular satellite sensor. Convolutional Neural Networks for cloud detection often require labeled cloud masks for training that are very time-consuming and expensive to obtain. To overcome these challenges, this paper presents a hybrid cloud detection method based on the synergistic combination of generative adversarial networks (GAN) and a physics-based cloud distortion model (CDM). The proposed weakly-supervised GAN-CDM method (available online https://github.com/Neooolee/GANCDM) only requires patch-level labels for training, and can produce cloud masks at pixel-level in both training and testing stages. GAN-CDM is trained on a new globally distributed Landsat 8 dataset (WHUL8-CDb, available online doi:https://doi.org/10.5281/zenodo.6420027) including image blocks and corresponding block-level labels. Experimental results show that the proposed GAN-CDM method trained on Landsat 8 image blocks achieves much higher cloud detection accuracy than baseline deep learning-based methods, not only in Landsat 8 images (L8 Biome dataset, 90.20% versus 72.09%) but also in Sentinel-2 images (“S2 Cloud Mask Catalogue” dataset, 92.54% versus 77.00%). This suggests that the proposed method provides accurate cloud detection in Landsat images, has good transferability to Sentinel-2 images, and can quickly be adapted for different optical satellite sensors. © 2022 The Authors","Deep learning; HTTP; Image processing; Landsat; Learning systems; Optical data processing; Optical remote sensing; Cloud detection; Cloud distortion model; Cloud masks; Deep learning; Distortion model; Generative adversarial network; LANDSAT; Model method; Remote-sensing; Satellite sensors; artificial neural network; detection method; image processing; Landsat; remote sensing; satellite imagery; Sentinel; spectral analysis; supervised learning; Generative adversarial networks","Cloud detection; Cloud distortion model; Deep learning; Generative adversarial networks (GAN); Remote sensing","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85136277692"
"Romero L.S.; Marcello J.; Vilaplana V.","Romero, Luis Salgueiro (57218455911); Marcello, Javier (6602158797); Vilaplana, Verónica (23394280500)","57218455911; 6602158797; 23394280500","Super-resolution of Sentinel-2 imagery using generative adversarial networks","2020","Remote Sensing","12","15","2424","","","","10.3390/RS12152424","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089853089&doi=10.3390%2fRS12152424&partnerID=40&md5=6bb6fc362a861fc8952daf3ca71d5b36","Sentinel-2 satellites provide multi-spectral optical remote sensing images with four bands at 10 m of spatial resolution. These images, due to the open data distribution policy, are becoming an important resource for several applications. However, for small scale studies, the spatial detail of these images might not be sufficient. On the other hand, WorldView commercial satellites offer multi-spectral images with a very high spatial resolution, typically less than 2 m, but their use can be impractical for large areas or multi-temporal analysis due to their high cost. To exploit the free availability of Sentinel imagery, it is worth considering deep learning techniques for single-image super-resolution tasks, allowing the spatial enhancement of low-resolution (LR) images by recovering high-frequency details to produce high-resolution (HR) super-resolved images. In this work, we implement and train a model based on the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) with pairs of WorldView-Sentinel images to generate a super-resolved multispectral Sentinel-2 output with a scaling factor of 5. Our model, named RS-ESRGAN, removes the upsampling layers of the network to make it feasible to train with co-registered remote sensing images. Results obtained outperform state-of-the-art models using standard metrics like PSNR, SSIM, ERGAS, SAM and CC. Moreover, qualitative visual analysis shows spatial improvements as well as the preservation of the spectral information, allowing the super-resolved Sentinel-2 imagery to be used in studies requiring very high spatial resolution. © 2020 by the authors.","Deep learning; Image analysis; Image resolution; Network layers; Open Data; Optical resolving power; Remote sensing; Spectroscopy; Commercial satellites; Data distribution policies; Low resolution images; Multi-temporal analysis; Optical remote sensing; Remote sensing images; Spectral information; Very high spatial resolutions; Image enhancement","Deep learning; Generative adversarial network; Sentinel-2; Super-resolution; WorldView","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089853089"
"Adriano B.; Yokoya N.; Xia J.; Baier G.; Koshimura S.","Adriano, Bruno (55613844500); Yokoya, Naoto (36440631200); Xia, Junshi (35099662000); Baier, Gerald (57188720676); Koshimura, Shunichi (6701566049)","55613844500; 36440631200; 35099662000; 57188720676; 6701566049","Cross-Domain-Classification of Tsunami Damage Via Data Simulation and Residual-Network-Derived Features from Multi-Source Images","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899155","4947","4950","3","10.1109/IGARSS.2019.8899155","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077693062&doi=10.1109%2fIGARSS.2019.8899155&partnerID=40&md5=478436287b8ed494d247b123a879fd8d","This paper presents a novel application of remote sensing data and machine learning technologies for damage classification in a real-world cross-domain application. The proposed methodology trains models to learn the building damage characteristics recorded in the 2011 Tohoku Tsunami from multi-sensor and multi-temporal remote sensing images. Then, the trained models are tested in the recent 2018 Sulawesi Tsunami. Additionally, a simulation of high-resolution SAR image was carried to deal with missing data modality. Our initial results show that the ResNet-derived features from optical images acquired after the disaster together with moderate- and high-resolution synthetic aperture radar (SAR) post-event intensity data showed significant accuracy in classifying two levels of tsunami-induced damage, with an average f-score of approximately 0.72. Taking into account that no training data from the 2018 Sulawesi Tsunami was used, our methodology shows excellent potential for future implementation of a rapid response system based on a database of building damage constructed from previous majors disasters. © 2019 IEEE.","Chemical sensors; Disasters; Geology; Geometrical optics; Image classification; Radar imaging; Remote sensing; Synthetic aperture radar; Tsunamis; Adversarial networks; Cross-domain; Damage classification; High resolution synthetic aperture radar; High-resolution SAR; Induced damage; Machine learning technology; Multi-temporal remote sensing; Classification (of information)","Conditional generative adversarial network; cross-domain classification; residual networks; tsunami-induced damage.","Conference paper","Final","","Scopus","2-s2.0-85077693062"
"Wu C.; Du B.; Zhang L.","Wu, Chen (55585973400); Du, Bo (57217375214); Zhang, Liangpei (8359720900)","55585973400; 57217375214; 8359720900","Fully Convolutional Change Detection Framework with Generative Adversarial Network for Unsupervised, Weakly Supervised and Regional Supervised Change Detection","2023","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","15","14","10.1109/TPAMI.2023.3237896","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147293048&doi=10.1109%2fTPAMI.2023.3237896&partnerID=40&md5=f598c847c53a78f69fb79041f23fd594","Deep learning for change detection is one of the current hot topics in the field of remote sensing. However, most end-to-end networks are proposed for supervised change detection, and unsupervised change detection models depend on traditional pre-detection methods. Therefore, we proposed a fully convolutional change detection framework with generative adversarial network, to unify unsupervised, weakly supervised, regional supervised, and fully supervised change detection tasks into one end-to-end framework. A basic Unet segmentor is used to obtain change detection map, an image-to-image generator is implemented to model the spectral and spatial variation between multi-temporal images, and a discriminator for changed and unchanged is proposed for modeling the semantic changes in weakly and regional supervised change detection task. The iterative optimization of segmentor and generator can build an end-to-end network for unsupervised change detection, the adversarial process between segmentor and discriminator can provide the solutions for weakly and regional supervised change detection, the segmentor itself can be trained for fully supervised task. The experiments indicate the effectiveness of the propsed framework in unsupervised, weakly supervised and regional supervised change detection. This paper provides new theorical definitions for unsupervised, weakly supervised and regional supervised change detection tasks with the proposed framework, and shows great potentials in exploring end-to-end network for remote sensing change detection. IEEE","Change detection; Convolution; Deep learning; Generative adversarial networks; Image segmentation; Iterative methods; Semantics; Change detection; Fully covolutional network; Generator; Images segmentations; Predictive models; Remote-sensing; Supervised segmentation; Task analysis; Weakly supervised segmentation; Remote sensing","Change Detection; Fully Covolutional Network; Generative Adversarial Network; Generative adversarial networks; Generators; Image segmentation; Predictive models; Remote Sensing; Remote sensing; Task analysis; Training; Weakly Supervised Segmentation","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85147293048"
"Zerrouki N.; Dairi A.; Harrou F.; Zerrouki Y.; Sun Y.","Zerrouki, Nabil (56039050100); Dairi, Abdelkader (57200449072); Harrou, Fouzi (26433417000); Zerrouki, Yacine (57220962220); Sun, Ying (55737745300)","56039050100; 57200449072; 26433417000; 57220962220; 55737745300","Efficient land desertification detection using a deep learning-driven generative adversarial network approach: A case study","2022","Concurrency and Computation: Practice and Experience","34","4","e6604","","","","10.1002/cpe.6604","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114689809&doi=10.1002%2fcpe.6604&partnerID=40&md5=0d6e4aeaffcaab1b9310234942464ec1","Precisely detecting land cover changes aids in improving the analysis of the dynamics of the landscape and plays an essential role in mitigating the effects of desertification. Mainly, sensing desertification is challenging due to the high correlation between desertification and like-desertification events (e.g., deforestation). An efficient and flexible deep learning approach is introduced to address desertification detection through Landsat imagery. Essentially, a generative adversarial network (GAN)-based desertification detector is designed and for uncovering the pixels influenced by land cover changes. In this study, the adopted features have been derived from multi-temporal images and incorporate multispectral information without considering image segmentation preprocessing. Furthermore, to address desertification detection challenges, the GAN-based detector is constructed based on desertification-free features and then employed to identify atypical events associated with desertification changes. The GAN-detection algorithm flexibly learns relevant information from linear and nonlinear processes without prior assumption on data distribution and significantly enhances the detection's accuracy. The GAN-based desertification detector's performance has been assessed via multi-temporal Landsat optical images from the arid area nearby Biskra in Algeria. This region is selected in this work because desertification phenomena heavily impact it. Compared to some state-of-the-art methods, including deep Boltzmann machine (DBM), deep belief network (DBN), convolutional neural network (CNN), as well as two ensemble models, namely, random forests and AdaBoost, the proposed GAN-based detector offers superior discrimination performance of deserted regions. Results show the promising potential of the proposed GAN-based method for the analysis and detection of desertification changes. Results also revealed that the GAN-driven desertification detection approach outperforms the state-of-the-art methods. © 2021 John Wiley & Sons Ltd.","Adaptive boosting; Climatology; Convolutional neural networks; Decision trees; Deforestation; Geometrical optics; Image segmentation; Adversarial networks; Deep belief network (DBN); Deep boltzmann machines; Detection algorithm; Detection approach; Land desertification; Multi-temporal image; State-of-the-art methods; Deep learning","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85114689809"
"Saha S.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 9943212600; 7006892410","Unsupervised multiple-change detection in vhr multisensor images via deep-learning based adaptation","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900173","5033","5036","3","10.1109/IGARSS.2019.8900173","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091066032&doi=10.1109%2fIGARSS.2019.8900173&partnerID=40&md5=b0391853cc5285db6a03366210e2ea21","Change Detection (CD) using multitemporal satellite images is an important application of remote sensing. In this work, we propose a Convolutional-Neural-Network (CNN) based unsupervised multiple-change detection approach that simultaneously accounts for the high spatial correlation among pixels in Very High spatial Resolution (VHR) images and the differences in multisensor images. We accomplish this by learning in an unsupervised way a transcoding between multisensor multitemporal data by exploiting a cycle-consistent Generative Adversarial Network (CycleGAN) that consists of two generator CNN networks. After unsupervised training, one generator of the CycleGAN is used to mitigate multisensor differences, while the other is used as a feature extractor that enables the computation of multitemporal deep features. These features are then compared pixelwise to generate a change detection map. Changed pixels are then further analyzed based on multitemporal deep features for identifying different kind of changes (multiple-change detection). Results obtained on multisensor multitemporal dataset consisting of Quickbird and Pleiades images confirm the effectiveness of the proposed approach. ©2019 IEEE","Convolutional neural networks; Feature extraction; Pixels; Remote sensing; Adversarial networks; Feature extractor; Multi sensor images; Multi-temporal data; Multi-temporal satellite images; Spatial correlations; Unsupervised training; Very-high spatial resolutions; Deep learning","Change detection; Deep Change Vector Analysis; Deep learning; Generative Adversarial Network; Multisensor images; Multitemporal images; Very High Resolution","Conference paper","Final","","Scopus","2-s2.0-85091066032"
"","","","2021 6th International Conference on Signal and Image Processing, ICSIP 2021","2021","2021 6th International Conference on Signal and Image Processing, ICSIP 2021","","","","","","1285","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125194577&partnerID=40&md5=61b3bc34d85e49d537ff4dd09c9f9a3e","The proceedings contain 239 papers. The topics discussed include: single-channel speech enhancement using multi-task learning and attention mechanism; KL divergence based objective state selection method for EW conflict; semi-supervised anomaly detection and location based on generative adversarial network; high resolution imaging of GEO SAR under the influence of terrain elevation; research on hand soft rehabilitation system based on brain-computer interface and virtual reality; use of depth completion and SLAM algorithm to build dense maps of large scenes; efficient FFT based multi-source DOA estimation for ULA; Pivot-V: an optimized algorithm for password generation in PCFGs model; a co-simulation method of polarization imaging for temporary birefringent materials; and a two-stage line matching method for multi-temporal remote sensing images.","","","Conference review","Final","","Scopus","2-s2.0-85125194577"
"Wang Z.; Zhang Y.; Luo L.; Wang N.","Wang, Zhixue (57214753076); Zhang, Yu (57214252359); Luo, Lin (57865848800); Wang, Nan (57701306200)","57214753076; 57214252359; 57865848800; 57701306200","CSA-CDGAN: channel self-attention-based generative adversarial network for change detection of remote sensing images","2022","Neural Computing and Applications","34","24","","21999","22013","14","10.1007/s00521-022-07637-z","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136984579&doi=10.1007%2fs00521-022-07637-z&partnerID=40&md5=24b987427aaabe37bbbd3db388a759c9","Remote sensing images change detection (RSICD) is a task to identify desired significant differences between multi-temporal images acquired at different times. From the existing methods, most of them solved this issue with a Siamese network, focusing on how to utilize the comparison between two image features to generate an initial difference map. However, Siamese network-based methods have three drawbacks: (1) complex architecture; (2) rough change map; (3) cumbersome detecting procedure: including feature extraction and feature comparison. To overcome the above drawbacks, we devoted our work to design a general framework which has a simple architecture, integrated detecting procedure, and good capacity of detecting subtle changes. In this paper, we proposed a channel self-attention network based on the generative adversarial network for change detection of remote sensing images. The network used an encoder–decoder network to directly produce a change map from two input images. It was better to detect small punctate and slim linear changes than Siamese-based networks. By regarding RSICD as an image translation problem, we used a Generative Adversarial Network to detect changes. In addition, a channel self-attention module was proposed to further improve the performance of this network. Experimental results on three public remote sensing RGB-image datasets, including change detection dataset, Wuhan University building change detection dataset and LEVIR building Change Detection dataset demonstrated that our method outperformed other state-of-the-art methods. In terms of the F1 score, the proposed method achieved maximum improvements of 5.1%, 3.1%, and 1.7% on the above datasets, respectively. Models and codes will be available at https://github.com/wangle53/CSA-CDGAN. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","College buildings; Feature extraction; Generative adversarial networks; Network architecture; Remote sensing; Adversarial learning; Building change detection; Change detection; Channel attention; Complex architectures; Image change detection; Image features; Multi-temporal image; Network-based; Remote sensing images; Change detection","Adversarial learning; Change detection; Channel attention","Article","Final","","Scopus","2-s2.0-85136984579"
"Wei Z.; Li C.-L.; Shen Y.-A.; Liu Y.-F.; Zhou P.-C.","Wei, Zhe (55900977300); Li, Cong-Li (56404814600); Shen, Yan-An (57194940455); Liu, Yong-Feng (57219556153); Zhou, Pu-Cheng (7401848718)","55900977300; 56404814600; 57194940455; 57219556153; 7401848718","Thick Cloud Region Content Generation of UAV Image Based on Two-Stage Model; [基于两阶段模型的无人机图像厚云区域内容生成]","2021","Jisuanji Xuebao/Chinese Journal of Computers","44","11","","2233","2247","14","10.11897/SP.J.1016.2021.02233","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118560876&doi=10.11897%2fSP.J.1016.2021.02233&partnerID=40&md5=473e68e02950d6da1fb590a3ba2c4860","Cloud covering often causes the loss of information on the underlying surface of the image during UAV flight. However, existing cloud region estimating methods based on multi-spectral and multi-temporal mainly orient to satellite remote sensing images, and cannot be applied directly to UAV images. How to use the available information to reasonably infer the content covered by the thick cloud, so as to improve the image availability, remains an urgent problem to be solved. With image inpainting theory, which regards the covered regions as the missing or damaged parts of the image and devotes to reconstruct their consistency, a two-stage thick cloud region content generating method based on DCGAN is proposed for the characteristics of single spectrum, short flight time and random flight path of UAV imaging. The two-stage model consists of a first stage DCGAN, an image retrieval module, an affine transformation net and a second stage DCGAN from front to back sequentially. The first stage DCGAN takes the masked image in, and generates preliminary completion result. In order to make the most of the homogeneous samples in dataset, an image retrieval module and an affine transformation is added. BoW retrieval algorithm is used to search for top N homogeneous samples of the image completed in the first stage, and the affine transform network is designed to align them with attention mechanisms for the second stage. The second stage DCGAN, which has the same structure as the first, takes the preliminary completion result and the output of the affine transform network, and generates the refined result in the end. The 4 parts constitute a complete forward form. This model makes image generation easier to utilize the information of the known image with identical distribution, and solves the difficulty of feature extraction with multiple distributions in UAV images, and addresses the limitation that existing inpainting methods rely heavily on single image. This paper also improves the structure of classical DCGAN, and designs a new joint loss function, combining local and global adversarial loss with the perceptual loss and the total variation loss, which not only prevent blurry result, but also generate pixels that approximate the true semantic distribution with less noise. In the training phase, image retrieval module is trained firstly to get the whole bags of visual word and clusters. Then affine transform network is trained by affined samples with manual random setting. 2-stage DCGANs are trained end-to-end using Adam optimization alternately with samples generated by the first 2 module. In the testing phase, these modules are cascaded and worked at fixed parameters. Simulation experiments with masks and real cloud-containing image are carried out respectively. On the central 1/4 mask of the simulation experiments, PSNR and SSIM are improved by 0.3214~3.6793 and 0.0005~0.0543, and average pixel L1 loss, NIQE and BLIINDS are decreased by 0.0171~4.1120, 0.0565~4.7440 and 0.8841~4.2586, compared with other classical methods, respectively. In the real cloud-containing image experiments, NIQE and BLIINDS indexes are decreased by 0.1062~1.8992 and 1.0903~5.6495. Visual effects under the same conditions are shown and analyzed. The subjective and objective experimental results show that compared with the classical method, the proposed method has certain advantages in semantic rationality, information accuracy and visual naturalness, and provides a better solution for single spectral image pixel value prediction against thick cloud covering. © 2021, Science Press. All right reserved.","Affine transforms; Antennas; Deep learning; Feature extraction; Image enhancement; Image retrieval; Remote sensing; Semantics; Unmanned aerial vehicles (UAV); Affine transformations; Classical methods; Deep convolutional generative adversarial net; Deep learning; Homogeneous samples; Image generations; Image-based; Two stage model; Unmanned aerial vehicle image; Vehicle images; Generative adversarial networks","Deep convolutional generative adversarial net; Deep learning; Image generation; Two-stage model; Unmanned aerial vehicle image","Article","Final","","Scopus","2-s2.0-85118560876"
