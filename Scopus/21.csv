"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Liu X.; Li M.; Wang X.","Liu, Xuanguang (58003340000); Li, Mengmeng (48761446900); Wang, Xiaoqin (55736763000)","58003340000; 48761446900; 55736763000","The use of Siamese multi-task neural network for building change detection from VHR remote sensing Images","2022","International Conference on Geoinformatics","2022-August","","","","","","10.1109/Geoinformatics57846.2022.9963829","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143907686&doi=10.1109%2fGeoinformatics57846.2022.9963829&partnerID=40&md5=965ff621ae6a4be01bb8d8b9490d1a0f","Building change detection is essential to many applications such as monitoring of urban areas, land use management, and illegal building detection. This paper proposes a Siamese multi-task neural network, labeled as SMTNet, to detect building changes from high-resolution remote sensing images. We combine the advantages of the multi-task learning method and Siamese neural networks to improve the geometric accuracies of detected boundaries. We applied the proposed method to a very high-resolution (VHR) remote sensing dataset that is a GF2 image-pair in Fuzhou City. We also compared the proposed method with two other existing methods, i.e., Obj-SiamNet, STANet. Our results show that the proposed method using SMTNet performed better than the existing method from the perspective of geometric and attribute accuracies. We conclude that the proposed SMTNet method has a high potential for extracting changed boundaries of buildings from high-resolution remote sensing images.  © 2022 IEEE.","Buildings; Change detection; Generative adversarial networks; Learning systems; Remote sensing; Change detection; Change detection of remote sensing; High-resolution remote sensing images; Multi tasks; Multitask learning; Neural-networks; Remote-sensing; Siamese neural network; Very high resolution; Very high resolution remote sensing image; Land use","change detection of remote sensing; Generative Adversarial Network; multi-task learning; Siamese Neural Network; Very High Resolution remote sensing images","Conference paper","Final","","Scopus","2-s2.0-85143907686"
"Wang J.; Nie J.; Chen H.; Xie H.; Zheng C.; Ye M.; Wei Z.","Wang, Jingyu (58026577400); Nie, Jie (56379566800); Chen, Hao (57840143300); Xie, Huaxin (57883366700); Zheng, Chengyu (57221979496); Ye, Min (57245588400); Wei, Zhiqiang (7402259116)","58026577400; 56379566800; 57840143300; 57883366700; 57221979496; 57245588400; 7402259116","Remote Sensing Image Colorization Based on Joint Stream Deep Convolutional Generative Adversarial Networks","2022","Proceedings of the 4th ACM International Conference on Multimedia in Asia, MMAsia 2022","","","21","","","","10.1145/3551626.3564951","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145777167&doi=10.1145%2f3551626.3564951&partnerID=40&md5=1e11d5ef3bec67c59490de1f4e8108a1","With the development of deep neural networks, especially generation networks, gray image coloring technology has made great progress. As one of the fields, remote sensing image colorization needs to be solved urgently. This is because remote sensing images cannot obtain clear color images due to the limitations of shooting equipment and transmission equipment. Compared with ordinary images, remote sensing images are characterized by the uneven spatial distribution of objects, therefore, it is a great challenge to ensure the spatial consistency of coloring. To embrace this challenge, we propose a new joint stream DCGAN including a micro stream and a macro stream, in which the latter is set as a prior to constrain the former for colorization. In addition, the Low-level Correlation Feature Extraction (LCFE) module is proposed to obtain the salient shallow detail feature with global correlation, which is used to enhance the global constraints as well as supplement the low-level information to the micro stream. What's more, we propose the Gated Selection (GSM) module by selecting useful information using a gated scheme to fuse features from two streams appropriately. Comprehensive comparison and ablation experiments are implemented and verify the proposed method performs surpasses other methods in both qualitative and quantitative metrics.  © 2022 ACM.","Convolutional neural networks; Feature extraction; Generative adversarial networks; Global system for mobile communications; Information use; Remote sensing; Adaptive fusion; Colour image; DCGAN; Gray image; Image colorizations; Multi-scales; Remote sensing image colorization; Remote sensing images; Transmission equipments; U-net; Deep neural networks","adaptive fusion; DCGAN; multi-scale; remote sensing image colorization; U-net","Conference paper","Final","","Scopus","2-s2.0-85145777167"
"Huang Y.; Li T.; Liu S.; Mei W.","Huang, Ying (56593902000); Li, Tangsheng (57271502300); Liu, Su (55887406000); Mei, Wenhao (57937955600)","56593902000; 57271502300; 55887406000; 57937955600","Adversarial Self-Training Unsupervised Domain Adaptation for Remote Sensing Scene Classification","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1572","1575","3","10.1109/IGARSS46834.2022.9884074","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140384571&doi=10.1109%2fIGARSS46834.2022.9884074&partnerID=40&md5=bbd43cc10a775b1bed8447db8574573f","In recent years, deep learning have achieved outstanding performance in field of remote sensing scenes classification. At the same time, most neural network models also exposed many shortcomings when dealing with different imaging conditions, insufficient sample size, etc. By analyzing the characteristics of source domain and target domain, reducing the gap between these domains is the main key for better recognition performance. In this paper, a novel adversarial self-training unsupervised domain adaptation (AST) framework is proposed to deal with the domain migration issues. To enforcing the gap reducing and the alignment of feature distribution of domains, we first implement the domain-adversarial learning mechanism. And then the self-training modules is also applied for better decision boundary generation. The experimental results show that our proposed method outperforms state of art in remote sensing scene classification. © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Domain adaptation; In-field; Neural network model; Performance; Remote sensing; Remote-sensing; Scene classification; Self-training; Unsupervised domain adaptation; Remote sensing","generative adversarial network; remote sensing (RS); Self-training (ST); unsupervised domain adaptation (UDA)","Conference paper","Final","","Scopus","2-s2.0-85140384571"
"Huang G.-L.; Wu P.-Y.","Huang, Gi-Luen (58075611900); Wu, Pei-Yuan (55413171800)","58075611900; 55413171800","CTGAN: CLOUD TRANSFORMER GENERATIVE ADVERSARIAL NETWORK","2022","Proceedings - International Conference on Image Processing, ICIP","","","","511","515","4","10.1109/ICIP46576.2022.9897229","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146693577&doi=10.1109%2fICIP46576.2022.9897229&partnerID=40&md5=1fd3edd319b5b3b6136aedd14c984e3e","Cloud occlusions obstruct some applications of remote sensing imagery, such as environment monitoring, land cover classification, and poverty prediction. In this paper, we propose the Cloud Transformer Generative Adversarial Network (CTGAN), taking three temporal cloudy images as input and generating a corresponding cloud-free image. Unlike previous work using generative networks, we design the feature extractor to maintain the weight of the cloudless region while reducing the weight of the cloudy region, and we pass the extracted features to a conformer module to find the most critical representations. Meanwhile, to address the lack of datasets, we collected a new dataset named Sen2 MTC from the Sentinel-2 satellite and manually labeled each cloudy and cloud-free image. Finally, we conducted extensive experiments on FS-2, the STGAN dataset, and Sen2 MTC. Our proposed CTGAN demonstrates higher qualitative and quantitative performance than the previous work and achieves state-of-the-art performance on these three datasets. The code is available at https://github.com/come880412/CTGAN. © 2022 IEEE.","Remote sensing; Satellites; Cloud removal; Cloud removal for multi-temporal cloudy image; Conformer; Environment monitoring; Formosat-2; Formosat-2 satellite; Land cover classification; Multi-temporal; Remote sensing imagery; Sentinel-2 satellite; Generative adversarial networks","Cloud removal for multi-temporal cloudy images; conformer; FormoSat-2 satellite; generative adversarial network; Sentinel-2 satellite","Conference paper","Final","","Scopus","2-s2.0-85146693577"
"Kumar S.; Setty S.L.N.","Kumar, Sandeep (57234304400); Setty, Suresh Lakshmi Narasimha (57211395435)","57234304400; 57211395435","UFS-LSTM: unsupervised feature selection with long short-term memory network for remote sensing scene classification","2023","Evolutionary Intelligence","16","1","","299","315","16","10.1007/s12065-021-00660-4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113844851&doi=10.1007%2fs12065-021-00660-4&partnerID=40&md5=e5bb6685646489a644b944015a3ef3c8","The aim of this research is to perform remote sensing scene classification, because it supports numerous strategic research fields like land use and land cover monitoring. However, classifying an enormous amount of remote sensing data is a challenging task in scene classification. In this research work, a new model is introduced to improve the feature extraction ability for better scene classification. A multiscale Retinex technique is employed for color restoration, and contrast enhancement in the aerial images that are collected from UC Merced, aerial image dataset, and RESISC45. Further, the feature extraction is carried out using steerable pyramid transform, gray level co-occurrence matrix features, and local ternary pattern. The feature extraction mechanism reduces overfitting risks, improves training process, and data visualization ability. Generally, the extracted features are high dimension, so an unsupervised feature selection based on multi subspace randomization and collaboration with state transition algorithm is proposed for selecting active features for better multiclass classification. The selected features are fed to long short term memory network for scene type classification. The experimental results showed that the proposed model achieved 99.14 %, 98.09%, and 99.25% of overall classification accuracy on UC Merced, RESISC45 and aerial image dataset. The proposed model showed a minimum of 0.03 % and maximum of 18.6 % improvement in classification accuracy compared to the existing models like self-attention based deep feature fusion, multitask learning system with convolutional neural network, multilayer feature fusion Wasserstein generative adversarial networks, and transfer learning model on UC Merced, RESISC45 and aerial dataset, respectively. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Antennas; Brain; Classification (of information); Convolution; Convolutional neural networks; Data visualization; Extraction; Image enhancement; Land use; Learning systems; Long short-term memory; Matrix algebra; Personnel training; Remote sensing; Aerial images; Features extraction; Gray-level co-occurrence matrix; Gray-level co-occurrence matrix feature; Grey-level co-occurrence matrixes; Local ternary pattern; Remote sensing scene classification; Remote-sensing; Scene classification; Steerable pyramid transforms; Feature extraction","Gray-level co-occurrence matrix features; Local ternary pattern; Long short-term memory; Remote sensing scene classification; Steerable pyramid transform","Article","Final","","Scopus","2-s2.0-85113844851"
"Dong W.; Hou S.; Xiao S.; Qu J.; Du Q.; Li Y.","Dong, Wenqian (57196087026); Hou, Shaoxiong (57355999500); Xiao, Song (7402022818); Qu, Jiahui (57196097630); Du, Qian (57254219600); Li, Yunsong (55986546100)","57196087026; 57355999500; 7402022818; 57196097630; 57254219600; 55986546100","Generative Dual-Adversarial Network With Spectral Fidelity and Spatial Enhancement for Hyperspectral Pansharpening","2022","IEEE Transactions on Neural Networks and Learning Systems","33","12","","7303","7317","14","10.1109/TNNLS.2021.3084745","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143180461&doi=10.1109%2fTNNLS.2021.3084745&partnerID=40&md5=942f7963e319575ce1b9e39a4cbc5cbf","Hyperspectral (HS) pansharpening is of great importance in improving the spatial resolution of HS images for remote sensing tasks. HS image comprises abundant spectral contents, whereas panchromatic (PAN) image provides spatial information. HS pansharpening constitutes the possibility for providing the pansharpened image with both high spatial and spectral resolution. This article develops a specific pansharpening framework based on a generative dual-adversarial network (called PS-GDANet). Specifically, the pansharpening problem is formulated as a dual task that can be solved by a generative adversarial network (GAN) with two discriminators. The spatial discriminator forces the intensity component of the pansharpened image to be as consistent as possible with the PAN image, and the spectral discriminator helps to preserve spectral information of the original HS image. Instead of designing a deep network, PS-GDANet extends GANs to two discriminators and provides a high-resolution pansharpened image in a fraction of iterations. The experimental results demonstrate that PS-GDANet outperforms several widely accepted state-of-the-art pansharpening methods in terms of qualitative and quantitative assessment.  © 2012 IEEE.","Generative adversarial networks; Image enhancement; Remote sensing; Adversarial networks; Generative dual-adversarial network; HyperSpectral; Hyperspectral  pansharpening; Pan-sharpening; Pansharpened images; Spatial discriminator; Spatial enhancement; Spectral discriminator; Spectral fidelity; article; quantitative analysis; remote sensing; Discriminators","Generative dual-adversarial network; hyperspectral (HS) pansharpening; spatial discriminator; spectral discriminator","Article","Final","","Scopus","2-s2.0-85143180461"
"Wang Y.; Bashir S.M.A.; Khan M.; Ullah Q.; Wang R.; Song Y.; Guo Z.; Niu Y.","Wang, Yi (12763268500); Bashir, Syed Muhammad Arsalan (56385198200); Khan, Mahrukh (57197808732); Ullah, Qudrat (57212812840); Wang, Rui (57226734727); Song, Yilin (57339519000); Guo, Zhe (36026786800); Niu, Yilong (18234173100)","12763268500; 56385198200; 57197808732; 57212812840; 57226734727; 57339519000; 36026786800; 18234173100","Remote sensing image super-resolution and object detection: Benchmark and state of the art","2022","Expert Systems with Applications","197","","116793","","","","10.1016/j.eswa.2022.116793","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125859223&doi=10.1016%2fj.eswa.2022.116793&partnerID=40&md5=151a05325bb6827420a29e481c57df4f","For the past two decades, there have been significant efforts to develop methods for object detection in Remote Sensing (RS) images. In most cases, the datasets for small object detection in remote sensing images are inadequate. Many researchers used scene classification datasets for object detection, which has its limitations; for example, the large-sized objects outnumber the small objects in object categories. Thus, they lack diversity; this further affects the detection performance of small object detectors in RS images. This paper reviews current datasets and object detection methods (deep learning-based) for remote sensing images. We also propose a large-scale, publicly available benchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The RSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of very high-resolution (VHR) images with a spatial resolution of ∼ 0.05 m. There are five classes with varying frequencies of labels per class; the images are annotated in You Only Look Once (YOLO) and Common Objects in Context (COCO) format. The image patches are extracted from satellite images, including real image distortions such as tangential scale distortion and skew distortion. The proposed RSSOD dataset will help researchers benchmark the state-of-the-art object detection methods across various classes, especially for small objects using image super-resolution. We also propose a novel Multi-class Cyclic super-resolution Generative adversarial network with Residual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark image super-resolution-based object detection and compare with the existing state-of-the-art methods based on image super-resolution (SR). The proposed MCGR achieved state-of-the-art performance for image SR with an improvement of 1.2 dB in peak signal-to-noise ratio (PSNR) compared to the current state-of-the-art non-local sparse network (NLSN). MCGR achieved best object detection mean average precisions (mAPs) of 0.758, 0.881, 0.841, and 0.983, respectively, for five-class, four-class, two-class, and single classes, respectively surpassing the performance of the state-of-the-art object detectors YOLOv5, EfficientDet, Faster RCNN, SSD, and RetinaNet. © 2022 Elsevier Ltd","Benchmarking; Classification (of information); Deep learning; Generative adversarial networks; Image enhancement; Image resolution; Image segmentation; Large dataset; Object recognition; Remote sensing; Signal to noise ratio; Deep learning object detection; Image super resolutions; MCGR; Multiclass GAN; Object detection in remote sensing; Remote sensing benchmark; Remote sensing images; Remote-sensing; Small object detection; Object detection","Deep learning object detection; MCGR; multiclass GAN; Object detection in remote sensing; Remote sensing benchmark; Small object detection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125859223"
"Niroshan L.; Carswell J.D.","Niroshan, Lasith (57195742111); Carswell, James D. (7005802453)","57195742111; 7005802453","OSM-GAN: Using Generative Adversarial Networks for Detecting Change in High-Resolution Spatial Images","2022","Lecture Notes on Data Engineering and Communications Technologies","143","","","95","105","10","10.1007/978-3-031-08017-3_9","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131146400&doi=10.1007%2f978-3-031-08017-3_9&partnerID=40&md5=c02a00445e808c20cce76b208f2cd653","Detecting changes to built environment objects such as buildings/roads/etc. in aerial/satellite (spatial) imagery is necessary to keep online maps and various value-added LBS applications up-to-date. However, recognising such changes automatically is not a trivial task, and there are many different approaches to this problem in the literature. This paper proposes an automated end-to-end workflow to address this problem by combining OpenStreetMap (OSM) vectors of building footprints with a machine learning Generative Adversarial Network (GAN) model - where two neural networks compete to become more accurate at predicting changes to building objects in spatial imagery. Notably, our proposed OSM-GAN architecture achieved over 88% accuracy predicting/detecting building object changes in high-resolution spatial imagery of Dublin city centre. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Antennas; Computer software maintenance; Generative adversarial networks; Object detection; Remote sensing; Built environment; Change detection; End to end; High resolution; Online maps; Openstreetmap; Remote-sensing; Spatial imagery; Spatial images; Work-flows; Change detection","Change detection; Generative Adversarial Networks; GIS; OpenStreetMap; Remote sensing","Book chapter","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131146400"
"Li J.; Wu Z.; Sheng Q.; Wang B.; Hu Z.; Zheng S.; Camps-Valls G.; Molinier M.","Li, Jun (57202722259); Wu, Zhaocong (15023850900); Sheng, Qinghong (36562635800); Wang, Bo (57190495006); Hu, Zhongwen (55630272400); Zheng, Shaobo (57852261300); Camps-Valls, Gustau (6603888005); Molinier, Matthieu (22234853700)","57202722259; 15023850900; 36562635800; 57190495006; 55630272400; 57852261300; 6603888005; 22234853700","A hybrid generative adversarial network for weakly-supervised cloud detection in multispectral images","2022","Remote Sensing of Environment","280","","113197","","","","10.1016/j.rse.2022.113197","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136277692&doi=10.1016%2fj.rse.2022.113197&partnerID=40&md5=58da582698f1afcd1e7983df21b2c735","Cloud detection is a crucial step in the optical satellite image processing pipeline for Earth observation. Clouds in optical remote sensing images seriously affect the visibility of the background and greatly reduce the usability of images for land applications. Traditional methods based on thresholding, multi-temporal or multi-spectral information are often specific to a particular satellite sensor. Convolutional Neural Networks for cloud detection often require labeled cloud masks for training that are very time-consuming and expensive to obtain. To overcome these challenges, this paper presents a hybrid cloud detection method based on the synergistic combination of generative adversarial networks (GAN) and a physics-based cloud distortion model (CDM). The proposed weakly-supervised GAN-CDM method (available online https://github.com/Neooolee/GANCDM) only requires patch-level labels for training, and can produce cloud masks at pixel-level in both training and testing stages. GAN-CDM is trained on a new globally distributed Landsat 8 dataset (WHUL8-CDb, available online doi:https://doi.org/10.5281/zenodo.6420027) including image blocks and corresponding block-level labels. Experimental results show that the proposed GAN-CDM method trained on Landsat 8 image blocks achieves much higher cloud detection accuracy than baseline deep learning-based methods, not only in Landsat 8 images (L8 Biome dataset, 90.20% versus 72.09%) but also in Sentinel-2 images (“S2 Cloud Mask Catalogue” dataset, 92.54% versus 77.00%). This suggests that the proposed method provides accurate cloud detection in Landsat images, has good transferability to Sentinel-2 images, and can quickly be adapted for different optical satellite sensors. © 2022 The Authors","Deep learning; HTTP; Image processing; Landsat; Learning systems; Optical data processing; Optical remote sensing; Cloud detection; Cloud distortion model; Cloud masks; Deep learning; Distortion model; Generative adversarial network; LANDSAT; Model method; Remote-sensing; Satellite sensors; artificial neural network; detection method; image processing; Landsat; remote sensing; satellite imagery; Sentinel; spectral analysis; supervised learning; Generative adversarial networks","Cloud detection; Cloud distortion model; Deep learning; Generative adversarial networks (GAN); Remote sensing","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85136277692"
"Qiu D.; Hu X.; Liang P.; Liu X.; Jiang J.","Qiu, Defen (58085676600); Hu, Xingyu (57279534600); Liang, Pengwei (57201500677); Liu, Xianming (57204313011); Jiang, Junjun (54902306100)","58085676600; 57279534600; 57201500677; 57204313011; 54902306100","A deep progressive infrared and visible image fusion network; [红外与可见光图像渐进融合深度网络]","2023","Journal of Image and Graphics","28","1","","156","165","9","10.11834/jig.220319","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147183420&doi=10.11834%2fjig.220319&partnerID=40&md5=76da56962094a406fab714557438b5cf","Objective Multi-modal images have been developed based on multiple imaging techniques. The infrared image collects the radiation information of the target in the infrared band. The visible image is more suitable to human visual perception in terms of higher spatial resolution, richer effective information and lower noise. Infrared and visible image fusion (IVIF) can integrate the configurable information of multi-sensors to alleviate the limitations of hardware equipment and obtain more low-cost information for high-quality images. The IVIF can be used for a wide range of applications like surveillance, remote sensing and agriculture. However, there are several challenges to be solved in multi-modal image fusion. For instance, effective information extraction issue from different modalities and the problem-solving for fusion rule of the complementary information of different modalities. Current researches can be roughly divided into two categories: 1) traditional methods and 2) deep learning based methods. The traditional methods decompose the infrared image and the visible image into the transform domain to make the decomposed representation have special properties that are benefit to fusion, then perform fusion in the transform domain, which can depress information loss and avoid the artifacts caused by direct pixel manipulation, and finally reconstruct the fused image. Traditional methods are based on the assumptions on the source image pair and manual-based image decomposition methods to extract features. However, these hand-crafted features are not comprehensive and may cause the sensitivity to high-frequency or primary components and generate image distortion and artifacts. In recent years, data-driven deep learning-based image fusion methods have been developing. Most of the deep learning based fusion methods have been oriented for the infrared and visible image fusion in the deep feature space. Deep learning-based fusion methods can be divided into two categories: 1) convolutional neural network (CNN) for fusion, and 2) generative adversarial network (GAN) to generate fusion images. CNN-based information extraction is not fully utilized by the intermediate layers. The GAN-based methods are challenged to preserving image details in adequately. Method We develop a novel progressive infrared and visible image fusion framework (ProFuse), which extracts multi-scale features with U-Net as our backbone, merges the multi-scale features and reconstructs the fused image layer by layer. Our network has composed of three parts: 1) encoder; 2) fusion module; and 3) decoder. First, a series of multi-scale feature maps are generated from the infrared image and the visible image via the encoder. Next, the multi-scale features of the infrared and visible image pair are fused in the fusion layer to obtain fused features. At last, the fused features pass through the decoder to construct the fused image. The network architecture of the encoder and decoder is designed based on U-Net. The encoder consists of the replicable applications of recurrent residual convolutional unit (RRCU) and the max pooling operation. Each down-sampling step can be doubled the number of feature channels, so that more features can be extracted. The decoder aims to reconstruct the final fused image. Every step in the decoder consists of an up-sampling of the feature map followed by a 3 × 3 convolution that halves the number of feature channels, a concatenation with the corresponding feature maps from the encoder, and a RRCU. At the fusion layer, our spatial attention-based fusion method is used to deal with image fusion tasks. This method has the following two advantages. First, it can perform fusion on global information-contained high-level features (at bottleneck semantic layer), and details-related low-level features (at shallow layers). Second, our method not only perform fusion on the original scale (maintaining more details), but also perform fusion on other smaller scales (maintaining semantic information). Therefore, the design of progressive fusion is mainly specified in the following two aspects: 1) we conduct image fusion progressively from high-level to low-level and 2) from small-scale to large-scale progressively. Result In order to evaluate the fusion performance of our method, we conduct experiments on publicly available Toegepast Natuurwetenschappelijk Onderzoek (TNO) dataset and compare it with some state-of-the-art (SOTA) fusion methods including DenseFuse, discrete wavelet transform (DWT), Fusion-GAN, ratio of low-pass pyramid (RP), generative adversarial network with multiclassification constraints for infrared and visible image fusion (GANMcC), curvelet transform (CVT). All these competitors are implemented according to public code, and the parameters are set by referring to their original papers. Our method is evaluated with other methods in subjective evaluation, and some quality metrics are used to evaluate the fusion performance objectively. Generally speaking, the fusion results of our method obviously have higher contrast, more details and clearer targets. Compared with other methods, our method preserves the detailed information of visible and infrared radiation in maximization. At the same time, very little noise and artifacts are introduced in the results. We evaluate the performances of different fusion methods quantitatively via using six metrics, i. e., entropy (EN), structure similarity (SSIM), edge-based similarity measure (Qabf), mutual information (MI), standard deviation (STD), sum of the correlations of differences (SCD). Our method has achieved a larger value on EN, Qabf, MI and STD. The maximum EN value indicates that our method retains richer information than other competitors. The Qabf is a novel objective quality evaluation metric for fused images. The higher the value of Qabf is, the better the quality of the fusion images are. STD is an objective evaluation index that measures the richness of image information. The larger the value, the more scattered the gray-level distribution of the image, the more information the image carries, and the better the quality of the fused image. The larger the value of MI, the more information obtained from the source images, and the better the fusion effect. Our method has an improvement of 115. 64% in the MI index compared with the generative adversarial network for infrared and visible image fusion (FusionGAN) method, 19. 93% in the STD index compared with the GANMcC method, 1. 91% in the edge preservation (Qabf) index compared with the DWT method and 1. 30% in the EN index compared with the GANMcC method. This indicates that our method is effective for IVIF task. Conclusion Extensive experiments demonstrate the effectiveness and generalization of our method. It shows better results on the evaluations in qualitative and quantitative both. © 2023 Editorial and Publishing Board of JIG. All rights reserved.","","deep learning; image fusion; infrared image; unsupervised learning; visible image","Article","Final","","Scopus","2-s2.0-85147183420"
"Qin J.; Liu Z.; Ran L.; Xie R.; Tang J.; Guo Z.","Qin, Jikai (57865177800); Liu, Zheng (56460597000); Ran, Lei (56666952500); Xie, Rong (35725013400); Tang, Junkui (57864281300); Guo, Zekun (57223267343)","57865177800; 56460597000; 56666952500; 35725013400; 57864281300; 57223267343","A Target SAR Image Expansion Method Based on Conditional Wasserstein Deep Convolutional GAN for Automatic Target Recognition","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","7153","7170","17","10.1109/JSTARS.2022.3199091","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136854934&doi=10.1109%2fJSTARS.2022.3199091&partnerID=40&md5=181e83981f33784e2df92fdbe3fd9322","For the automatic target recognition (ATR) based on synthetic aperture radar (SAR) images, enough training data are required to effectively characterize target features and obtain good recognition performance. However, in practical applications, it is difficult to collect sufficient training data. To tackle the limitation, a novel end-to-end expansion method, called conditional Wasserstein deep convolutional generative adversarial network with gradient penalty (CWDCGAN), is proposed to achieve SAR image expansion with specified category. To be specific, the CWDCGAN innovatively designed a generative adversarial network architecture based on convolutional and deconvolution networks to improve the quality of generated images. At the same time, conditional information is introduced to control the categories of generated images, and Wasserstein distance and gradient penalty are used to modify the loss function, which makes the network training more stable. Besides, feature extraction and classifier design in a typical ATR system often rely heavily on subjective expert knowledge, which seriously affects its generalization performance. Therefore, a joint recognition method of Resnet18 and support vector machine (Renset18-SVM) is adopted to improve the generalization capacity and the recognition performance. Experimental results with public measured data show that the CWDCGAN can generate higher quality SAR images, and by feeding expanded data to Renset18-SVM, the recognition accuracy is improved under different proportions of training samples.  © 2022 IEEE.","Automatic target recognition; Convolution; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Network architecture; Radar imaging; Radar target recognition; Support vector machines; Expansion methods; Features extraction; Image expansions; Radar polarimetry; Resnet18; Support vectors machine; Synthetic aperture radar image expansion; Synthetic aperture radar images; Target recognition; Training data; accuracy assessment; artificial neural network; experimental study; remote sensing; satellite imagery; support vector machine; synthetic aperture radar; Synthetic aperture radar","Automatic target recognition (ATR); generative adversarial network (GAN); resnet18; SAR image expansion; support vector machine; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85136854934"
"Jamali A.; Mahdianpari M.; Mohammadimanesh F.; Homayouni S.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939); Mohammadimanesh, Fariba (56541784200); Homayouni, Saeid (24070293900)","56909712300; 57190371939; 56541784200; 24070293900","A deep learning framework based on generative adversarial networks and vision transformer for complex wetland classification using limited training samples","2022","International Journal of Applied Earth Observation and Geoinformation","115","","103095","","","","10.1016/j.jag.2022.103095","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141519900&doi=10.1016%2fj.jag.2022.103095&partnerID=40&md5=d705bcbc2474023599e7331d161879d0","Wetlands have long been recognized among the most critical ecosystems globally, yet their numbers quickly diminish due to human activities and climate change. Thus, large-scale wetland monitoring is essential to provide efficient spatial and temporal insights for resource management and conservation plans. However, the main challenge is the lack of enough reference data for accurate large-scale wetland mapping. As such, the main objective of this study was to investigate the efficient deep-learning models for generating high-resolution and temporally rich training datasets for wetland mapping. The Sentinel-1 and Sentinel-2 satellites from the European Copernicus program deliver radar and optical data at a high temporal and spatial resolution. These Earth observations provide a unique source of information for more precise wetland mapping from space. The second objective was to investigate the efficiency of vision transformers for complex landscape mapping. As such, we proposed a 3D Generative Adversarial Network (3D GAN) to best achieve these two objectives of synthesizing training data and a Vision Transformer model for large-scale wetland classification. The proposed approach was tested in three different study areas of Saint John, Sussex, and Fredericton, New Brunswick, Canada. The results showed the ability of the 3D GAN to stimulate and increase the number of training data and, as a result, increase the accuracy of wetland classification. The quantitative results also demonstrated the capability of jointly using data augmentation, 3D GAN, and Vision Transformer models with overall accuracy, average accuracy, and Kappa index of 75.61%, 73.4%, and 71.87%, respectively, using a disjoint data sampling strategy. Therefore, the proposed deep learning method opens a new window for large-scale remote sensing wetland classification. © 2022","artificial neural network; classification; climate change; conservation planning; human activity; learning; radar; remote sensing; resource management; Sentinel; wetland","Convolutional neural network; Deep learning; Generative adversarial network; New Brunswick; Vision Transformer (ViT); Wetland classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141519900"
"Rage K.","Rage, Kiranmai (57320646500)","57320646500","A Study on Different Deep Learning Architectures on Image Captioning","2022","8th International Conference on Smart Structures and Systems, ICSSS 2022","","","","","","","10.1109/ICSSS54381.2022.9782260","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132294301&doi=10.1109%2fICSSS54381.2022.9782260&partnerID=40&md5=31979128bc072660796b55c7ef4463fc","In the recent era Deep learning is showing exemplary results in many application areas. Researches from different disciplines have incorporated deep learning in to their research to solve different interdisciplinary problems. Deep learning applications areas include Speech recognition, Natural Language Processing, Computer vision, Networking, Healthcare, IoT, Robotics, Agriculture, Remote sensing and many other areas. Thus, this paper contributes a review on various deep learning approaches which include Convolution Neural Network (CNN), Deep Neural Network (DNN), Auto-Encoder (AE), Recurrent Neural Network (RNN) enclosed with Gated Recurrent Units (GRU), Long Short-Term Memory (LSTM) and ConvLSTM, Deep Reinforcement Learning (DRL), Generative based Adversarial Network (GAN) and Deep Belief Network (DBN). For extraction of the most important features of an image various data extraction procedures have designed. CNN's tremendous learning capacity is based on the utilisation of several feature extraction stages that continuously learn from data. Some of the popular CNN architectures name as LeNet, AlexNet, ZFNet / Clarifai, VGGNET, GoogLeNet, ResNet, DenseNet, FractalNet and CapsuleNet. Recent works extended to a combination of two CNN models like Inception, ResNetV2 to attained different existing approaches consequences in deep learning. Identification of the significant objects, their parameters and relationships of an object images are required for image captioning. Syntactically and semantically correct sentences need to be generated. The intricacies and problems of picture captioning may be handled using deep learning approaches. A comparison between these models was also presented. We also discussed about different standard datasets which are utilized for executing and estimating the deep learning approaches. © 2022 IEEE.","Convolutional neural networks; Deep neural networks; Extraction; Generative adversarial networks; Image processing; Learning algorithms; Learning systems; Natural language processing systems; Network architecture; Neural network models; Reinforcement learning; Remote sensing; Speech recognition; Application area; Auto encoders; Convolution neural network; Image captioning; Language processing; Learning approach; Learning architectures; Natural languages; Reinforcement learnings; Remote-sensing; Long short-term memory","CNN; LSTM; RNN","Conference paper","Final","","Scopus","2-s2.0-85132294301"
"Shen R.; Zhang X.; Xiang Y.","Shen, Runhan (57651727000); Zhang, Xiaofeng (57217109516); Xiang, Yonggang (57655595800)","57651727000; 57217109516; 57655595800","AFFNet: Attention Mechanism Network Based on Fusion Feature for Image Cloud Removal","2022","International Journal of Pattern Recognition and Artificial Intelligence","36","8","2254014","","","","10.1142/S0218001422540143","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128994119&doi=10.1142%2fS0218001422540143&partnerID=40&md5=06a7bb2d8c87e501d5a83ddeb9c10b16","Clouds frequently affect optical remote sensing pictures throughout the gathering process, resulting in low-resolution images that affect judgment and subsequent use of ground data. Because of the thick cloud cover, the ground surface information below is entirely incorrect. This kind of end-to-end image problem should not be dismissed as a simple task of image inpainting or image translation. Therefore, this paper proposes a multi-head self-attention module based on the encoding-decoding generative adversarial network, considering the redundant information of the deep network, furthermore this paper introduces Ghost convolution to effectively solve the influence of redundant feature maps in the network on the increase of time consumption and parameters. The method in this paper can solve the problem of cloud occlusion. By considering spatial information, it can better complete the prediction of cloud removal. It can reduce the amount of network calculations and parameters while maintaining the effect. In addition, Feature Fusion Module is proposed to integrate high-level features with low-level features, so that the network can extract enough feature information and better supplement the details to complete the cloud removal. The method in this paper has achieved excellent results on the RICE1 and RICE2 datasets.  © 2022 World Scientific Publishing Company.","Image processing; Remote sensing; Attention mechanisms; Cloud removal; Fusion features; Ground data; Image cloud removal; Low resolution images; Network-based; Optical remote sensing; Optical remote sensing image; Remote sensing images; Generative adversarial networks","image cloud removal; Optical remote sensing image","Article","Final","","Scopus","2-s2.0-85128994119"
"Zhou J.; Luo X.; Rong W.; Xu H.","Zhou, Jianjun (57724336300); Luo, Xiaobo (36562124600); Rong, Wentao (57724414200); Xu, Hao (57427838100)","57724336300; 36562124600; 57724414200; 57427838100","Cloud Removal for Optical Remote Sensing Imagery Using Distortion Coding Network Combined with Compound Loss Functions","2022","Remote Sensing","14","14","3452","","","","10.3390/rs14143452","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137262049&doi=10.3390%2frs14143452&partnerID=40&md5=7a16dcc956b383ff62e50bbb8777d068","Optical remote sensing (RS) satellites perform imaging in the visible and infrared electromagnetic spectrum to collect data and analyze information on the optical characteristics of the objects of interest. However, optical RS is sensitive to illumination and atmospheric conditions, especially clouds, and multiple acquisitions are typically required to obtain an image of sufficient quality. To accurately reproduce surface information that has been contaminated by clouds, this work proposes a generative adversarial network (GAN)-based cloud removal framework using a distortion coding network combined with compound loss functions (DC-GAN-CL). A novel generator embedded with distortion coding and feature refinement mechanisms is applied to focus on cloudy regions and enhance the transmission of optical information. In addition, to achieve feature and pixel consistency, both coherent semantics and local adaptive reconstruction factors are considered in our loss functions. Extensive numerical evaluations on RICE1, RICE2, and Paris datasets are performed to validate the good performance achieved by the proposed DC-GAN-CL in both peak signal-to-noise ratio (PSNR) and visual perception. This system can thus restore images to obtain similar quality to cloud-free reference images, in a dynamic range of over 30 dB. The restoration effect on the coherence of image semantics produced by this technique is competitive compared with other methods. © 2022 by the authors.","Generative adversarial networks; Image coding; Image reconstruction; Light transmission; Network coding; Restoration; Semantics; Signal to noise ratio; Adaptive reconstruction; Cloud removal; Distortion coding; Local adaptive reconstruction; Local-adaptive; Loss functions; Optical remote sensing; Optical remote-sensing imagery; Remote sensing images; Remote sensing satellites; Optical remote sensing","cloud removal; distortion coding; generative adversarial network; local adaptive reconstruction; remote sensing images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137262049"
"Maggiolo L.; Solarna D.; Moser G.; Serpico S.B.","Maggiolo, Luca (57207878137); Solarna, David (57192703421); Moser, Gabriele (7101795745); Serpico, Sebastiano Bruno (7005306316)","57207878137; 57192703421; 7101795745; 7005306316","Registration of Multisensor Images through a Conditional Generative Adversarial Network and a Correlation-Type Similarity Measure","2022","Remote Sensing","14","12","2811","","","","10.3390/rs14122811","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132264566&doi=10.3390%2frs14122811&partnerID=40&md5=638d2a5fde8b10c72eef7aecc278ae3a","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural characteristics of the input data. Information-theoretic measures are often used to favor comparing local intensity distributions in the images. In this paper, a novel method based on the combination of a deep learning architecture and a correlation-type area-based functional is proposed for the registration of a multisensor pair of images, including an optical image and a synthetic aperture radar (SAR) image. The method makes use of a conditional generative adversarial network (cGAN) in order to address image-to-image translation across the optical and SAR data sources. Then, once the optical and SAR data are brought to a common domain, an area-based ℓ2 similarity measure is used together with the COBYLA constrained maximization algorithm for registration purposes. While correlation-type functionals are usually ineffective in the application to multisensor registration, exploiting the image-to-image translation capabilities of cGAN architectures allows moving the complexity of the comparison to the domain adaptation step, thus enabling the use of a simple ℓ2 similarity measure, favoring high computational efficiency, and opening the possibility to process a large amount of data at runtime. Experiments with multispectral and panchromatic optical data combined with SAR images suggest the effectiveness of this strategy and the capability of the proposed method to achieve more accurate registration as compared to state-of-the-art approaches. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Computational efficiency; Deep learning; Geometrical optics; Image registration; Network architecture; Optical correlation; Optical remote sensing; Radar imaging; Statistical mechanics; Synthetic aperture radar; Area-based; Area-based registration; COBYLA; Conditional generative adversarial network; Deep learning; Image translation; Image-to-image translation; Multisensor image registration; Similarity measure; ℓ2 similarity; Generative adversarial networks","area-based registration; COBYLA; conditional generative adversarial networks (cGANs); deep learning; image-to-image translation; multisensor image registration; ℓ<sup>2</sup> similarity","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132264566"
"Nagle-Mcnaughton T.P.; Scuderi L.A.; Erickson N.","Nagle-Mcnaughton, Timothy Paul (57208753971); Scuderi, Louis Anthony (6602124129); Erickson, Nicholas (57762966700)","57208753971; 6602124129; 57762966700","Squeezing Data from a Rock: Machine Learning for Martian Science","2022","Geosciences (Switzerland)","12","6","248","","","","10.3390/geosciences12060248","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132701082&doi=10.3390%2fgeosciences12060248&partnerID=40&md5=d398f36c9f95c59a157ac15df3076d93","Data analysis methods have scarcely kept pace with the rapid increase in Earth observations, spurring the development of novel algorithms, storage methods, and computational techniques. For scientists interested in Mars, the problem is always the same: there is simultaneously never enough of the right data and an overwhelming amount of data in total. Finding sufficient data needles in a haystack to test a hypothesis requires hours of manual data screening, and more needles and hay are added constantly. To date, the vast majority of Martian research has been focused on either one-off local/regional studies or on hugely time-consuming manual global studies. Machine learning in its numerous forms can be helpful for future such work. Machine learning has the potential to help map and classify a large variety of both features and properties on the surface of Mars and to aid in the planning and execution of future missions. Here, we outline the current extent of machine learning as applied to Mars, summarize why machine learning should be an important tool for planetary geomorphology in particular, and suggest numerous research avenues and funding priorities for future efforts. We conclude that: (1) moving toward methods that require less human input (i.e., self-or semi-supervised) is an important paradigm shift for Martian applications, (2) new robust methods using generative adversarial networks to generate synthetic high-resolution digital terrain models represent an exciting new avenue for Martian geomorphologists, (3) more effort and money must be directed toward developing standardized datasets and benchmark tests, and (4) the community needs a large-scale, generalized, and programmatically accessible geographic information system (GIS). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","geomorphology; machine learning; Mars; planetary science; remote sensing; surface processes","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132701082"
"Zhao Y.; Celik T.; Liu N.; Li H.-C.","Zhao, Yitao (57211375150); Celik, Turgay (35101499300); Liu, Nanqing (57221303417); Li, Heng-Chao (24174798500)","57211375150; 35101499300; 57221303417; 24174798500","A Comparative Analysis of GAN-Based Methods for SAR-to-Optical Image Translation","2022","IEEE Geoscience and Remote Sensing Letters","19","","3512605","","","","10.1109/LGRS.2022.3177001","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130799341&doi=10.1109%2fLGRS.2022.3177001&partnerID=40&md5=e1faced3a99d069a2e8e07de3fb2576e","Unlike optical sensors, synthetic aperture radar (SAR) sensors acquire images of the Earth's surface with all-weather and all-time capabilities, which is vital in a situation such as a disaster assessment. However, SAR sensors do not offer as rich visual information as optical sensors. SAR-to-optical image-to-image translation generates optical images from SAR images to benefit from what both imaging modalities have to offer. It also enables multisensor image analysis of the same scene for applications such as heterogeneous change detection. Various architectures of generative adversarial networks (GANs) have achieved remarkable image-to-image translation results in different domains. Still, their performances in SAR-to-optical image translation have not been analyzed in the remote-sensing domain. This letter compares and analyzes the state-of-the-art GAN-based translation methods with open-source implementations for SAR-to-optical image translation. The results show that GAN-based SAR-to-optical image translation methods achieve satisfactory results. However, their performances depend on the structural complexity of the observed scene and the spatial resolution of the data. We also introduce a new dataset with a higher resolution than the existing SAR-to-optical image datasets and release implementations of GAN-based methods considered in this letter to support the reproducible research in remote sensing.  © 2004-2012 IEEE.","Generative adversarial networks; Geometrical optics; Image analysis; Image sensors; Optical sensors; Radar imaging; Remote sensing; Generator; Image translation; Image-to-image translation; Multi sensor images; Optical image; Optical imaging; Optical-; Remote-sensing; Synthetic aperture radar-to-optical image translation; comparative study; optical method; remote sensing; synthetic aperture radar; Synthetic aperture radar","Generative adversarial network (GAN); image-to-image translation; multisensor images; optical; remote sensing; SAR-to-optical image translation; synthetic aperture radar (SAR)","Article","Final","","Scopus","2-s2.0-85130799341"
"Kapilaratne R.G.C.J.; Kakuta S.; Kaneta S.","Kapilaratne, R.G.C.J. (57194537565); Kakuta, S. (56536327700); Kaneta, S. (57219049570)","57194537565; 56536327700; 57219049570","Enhanced super resolution for remote sensing imageries","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","53","60","7","10.5194/isprs-Annals-V-3-2022-53-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132043853&doi=10.5194%2fisprs-Annals-V-3-2022-53-2022&partnerID=40&md5=e739cd48d213e5e5de51b63d8bcee7db","Single image super resolution (SISR) technology has been attracted much attention from remote sensing community due to its proven potentials in remote sensing applications. Existing SISR techniques varying from conventional interpolation methods to different network architectures. Generative adversarial networks (GANs) are one of the latest network architectures proven a greater potential as a SISR method whereas least attention has been given by the remote sensing community. Several studies have already been carried out on this context. However, yet there is no generalized GAN based approach to super resolve remote sensing imageries. Therefore, this study investigated the potentials of enhanced super resolution generative adversarial (ESRGAN) model to super resolve very high to medium resolution images from high to coarse resolution images for remote sensing applications. Two models were trained and Worldview-3 (WV3) images used as for very high resolution images. Whereas, down sampled WV3 and Sentinel-2(S2) were used as low resolution counterparts. Model performances were qualitatively and quantitatively analysed using standard metrics such as PSNR, SSIM, UIQI, CC, SAM, SID. Evaluation results emphasised super resolved images were preserved the original quality of the satellite images to a greater extent while improving its ground resolution.  © Authors 2022.","Deep learning; Generative adversarial networks; Image enhancement; Network architecture; Optical resolving power; Deep learning; ESRGAN; Image super resolutions; Remote sensing applications; Remote sensing imagery; Remote-sensing; Sentinel-2.; Single images; Superresolution; Worldview-3; Remote sensing","Deep Learning; ESRGAN; Remote Sensing; Sentinel-2.; Super Resolution; WorldView-3","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132043853"
"Wang H.; Zhang Z.; Hu Z.; Dong Q.","Wang, Haixia (57881055100); Zhang, Zhigang (57881055200); Hu, Zhanyi (7404211366); Dong, Qiulei (14008900200)","57881055100; 57881055200; 7404211366; 14008900200","SAR-to-Optical Image Translation with Hierarchical Latent Features","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5233812","","","","10.1109/TGRS.2022.3200996","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137574586&doi=10.1109%2fTGRS.2022.3200996&partnerID=40&md5=31e1a964d79cb32f507ce8174581ac94","Due to the all-weather and all-time imaging capability of synthetic aperture radar (SAR), SAR remote sensing analysis has attracted much attention recently. However, compared with the optical images, SAR images are more difficult to be interpreted. If an SAR image could be translated into its corresponding optical image, then the generated optical image would be helpful for assisting the interpretation. Addressing this issue, we investigate how to translate SAR images into optical ones in this work and propose a parallel generative adversarial model for SAR-to-optical image translation, called parallel generative adversarial network (Parallel-GAN), consisting of a backbone image translation subnetwork and an adjoint optical image reconstruction subnetwork. Under the proposed model, the backbone image translation subnetwork is designed to translate SAR images into optical ones, and simultaneously some of its intermediate layers are required to output similar latent features to those from the corresponding layers of the adjoint image reconstruction subnetwork. Thanks to the imposed hierarchical latent optical features, the proposed Parallel-GAN could achieve the SAR-to-optical image translation effectively. Extensive experimental results on three public datasets demonstrate that the proposed method outperforms ten state-of-the-art methods for SAR-to-optical image translation.  © 1980-2012 IEEE.","Adaptive optics; Generative adversarial networks; Geometrical optics; Image reconstruction; Optical design; Optical remote sensing; Radar imaging; Image translation; Images reconstruction; Optical image; Optical imaging; Parallel generative adversarial model; Radar polarimetry; Subnetworks; Synthetic aperture radar; Synthetic aperture radar images; Synthetic aperture radar-to-optical image translation; analytical hierarchy process; image analysis; numerical model; remote sensing; satellite imagery; synthetic aperture radar; Synthetic aperture radar","Parallel generative adversarial model; SAR; synthetic aperture radar (SAR)-to-optical image translation","Article","Final","","Scopus","2-s2.0-85137574586"
"Yu M.; Wang H.; Liu C.; Lin D.","Yu, Mengbei (57223211542); Wang, Hongjuan (57210918053); Liu, Chang (57240736300); Lin, Deping (55608464000)","57223211542; 57210918053; 57240736300; 55608464000","Super-resolution reconstruction of remote sensing images based on SRGAN","2022","Proceedings of SPIE - The International Society for Optical Engineering","12473","","124730U","","","","10.1117/12.2653847","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144046247&doi=10.1117%2f12.2653847&partnerID=40&md5=cd5db18d4b97c3d4861777f4b69804a5","In the field of remote sensing images, due to the limitations of hardware equipment, image transmission, natural environment and other reasons, the resolution of the obtained remote sensing images cannot reach the desired resolution. The emergence of image super-resolution reconstruction technology can improve the resolution of remote sensing images without increasing the high cost. Image super-resolution reconstruction refers to the fact that low-resolution images can obtain high-resolution images through certain algorithmic techniques. With the rapid development of deep learning ideas, researchers have applied it to the field of image super-resolution reconstruction and achieved good results. Image super-resolution reconstruction also shifts from traditional reconstruction methods to deep learning-based methods. The emergence of the idea of Generative Adversarial Networks has further advanced the field of image super-resolution reconstruction. By using the idea of Generative Adversarial Network (GAN), researchers can obtain more realistic high-resolution images. This paper mainly uses the SRGAN model, the image dataset DIV2K for super-resolution reconstruction, and uses a dense residual structure in the generator network to obtain more image information, so that the effect of image reconstruction is more realistic. Through the experimental verification on the SIRI-WHU remote sensing test data set, the two evaluation indicators of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) are compared, and the effect is improved. Better generation results can also be observed through subjective human vision. © 2022 SPIE.","Deep learning; Generative adversarial networks; Image enhancement; Optical resolving power; Remote sensing; Signal to noise ratio; Statistical tests; Algorithmic techniques; High costs; High-resolution images; Image super-resolution reconstruction; Image-based; Low resolution images; Natural environments; Reconstruction method; Remote sensing images; Super-resolution reconstruction; Image reconstruction","Generative Adversarial Networks; Image Super-Resolution Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85144046247"
"Wang Z.; Ma Y.; Zhang Y.","Wang, Zhaobin (23669866600); Ma, Yikun (57556806300); Zhang, Yaonan (36633454400)","23669866600; 57556806300; 36633454400","Hybrid cGAN: Coupling Global and Local Features for SAR-to-Optical Image Translation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5236016","","","","10.1109/TGRS.2022.3212208","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139819638&doi=10.1109%2fTGRS.2022.3212208&partnerID=40&md5=5c798b97f5c08ce2ad1c33db8aa20ec3","Synthetic aperture radar (SAR) has the advantage of all-weather observation, but its imaging principle based on the backscattering of electromagnetic waves makes its information less interpretable. One feasible approach is to convert SAR images into optical images, which not only improves the interpretability of SAR images but also fills the gaps in information captured by optical sensors due to weather and light limitations. Since conditional generative adversarial network (cGAN) has the powerful ability to generate images, many studies have started to apply it to image translation tasks. For SAR-to-optical translation, some specialized cGAN models have been proposed, but most of them struggle to process SAR images with widely varying styles, often generating images with poor quality. To this end, we propose a hybrid cGAN that combines the advantages of convolutional neural network (CNN) and vision transformer (ViT). With the advantage of ViT to capture long-distance feature dependencies, the global features can be extracted and then fused with the local features extracted by CNN to improve the representation capabilities of our generator. Moreover, we expand the receptive field of the residual blocks in CNN by hierarchical convolution. Perceptual loss and classification loss are added for training to further improve the fidelity of the generated images. Finally, we introduce the multiscale strategy into the discriminator to balance its learning ability with that of the generator. Both visual and quantitative experiments are conducted with other state-of-the-art methods. The results show that our method not only achieves the optimal results in all the evaluation metrics but also generates images that are more consistent with the human visual system. In addition, the potential of our method to process multitype SAR images with significant style differences is also experimentally demonstrated.  © 1980-2012 IEEE.","Adaptive optics; Generative adversarial networks; Geometrical optics; Image enhancement; Neural networks; Optical remote sensing; Optical sensors; Radar imaging; Conditional generative adversarial network; Features extraction; Generator; Image translation; Optical imaging; Remote-sensing; Synthetic aperture radar images; Task analysis; Transformer; coupling; image analysis; remote sensing; synthetic aperture radar; Synthetic aperture radar","Conditional generative adversarial networks (cGANs); image translation; remote sensing; synthetic aperture radar (SAR); transformer","Article","Final","","Scopus","2-s2.0-85139819638"
"Zhu B.; Lv Q.; Yang Y.; Sui X.; Zhang Y.; Tang Y.; Tan Z.","Zhu, Baoyu (57880361000); Lv, Qunbo (55513035000); Yang, Yuanbo (57879780700); Sui, Xuefu (57879780600); Zhang, Yu (57840649800); Tang, Yinhui (57930339800); Tan, Zheng (57188729245)","57880361000; 55513035000; 57879780700; 57879780600; 57840649800; 57930339800; 57188729245","Blind Deblurring of Remote-Sensing Single Images Based on Feature Alignment","2022","Sensors","22","20","7894","","","","10.3390/s22207894","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140824919&doi=10.3390%2fs22207894&partnerID=40&md5=8897d84ac7fac9c6e01be9264b955d08","Motion blur recovery is a common method in the field of remote sensing image processing that can effectively improve the accuracy of detection and recognition. Among the existing motion blur recovery methods, the algorithms based on deep learning do not rely on a priori knowledge and, thus, have better generalizability. However, the existing deep learning algorithms usually suffer from feature misalignment, resulting in a high probability of missing details or errors in the recovered images. This paper proposes an end-to-end generative adversarial network (SDD-GAN) for single-image motion deblurring to address this problem and to optimize the recovery of blurred remote sensing images. Firstly, this paper applies a feature alignment module (FAFM) in the generator to learn the offset between feature maps to adjust the position of each sample in the convolution kernel and to align the feature maps according to the context; secondly, a feature importance selection module is introduced in the generator to adaptively filter the feature maps in the spatial and channel domains, preserving reliable details in the feature maps and improving the performance of the algorithm. In addition, this paper constructs a self-constructed remote sensing dataset (RSDATA) based on the mechanism of image blurring caused by the high-speed orbital motion of satellites. Comparative experiments are conducted on self-built remote sensing datasets and public datasets as well as on real remote sensing blurred images taken by an in-orbit satellite (CX-6(02)). The results show that the algorithm in this paper outperforms the comparison algorithm in terms of both quantitative evaluation and visual effects. © 2022 by the authors.","Algorithms; Image Processing, Computer-Assisted; Motion; Alignment; Computer system recovery; Deep learning; Feature extraction; Generative adversarial networks; Image enhancement; Learning algorithms; Orbits; Recovery; Blind deblurring; Deep learning; Feature alignment; Feature map; Features selection; Image deblurring; Image-based; Motion blur; Remote-sensing; Single images; algorithm; image processing; motion; procedures; Remote sensing","deep learning; feature alignment; feature selection; generative adversarial networks; image deblurring; remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140824919"
"Abduljawad M.; Alsalmani A.","Abduljawad, Mohamed (58067643600); Alsalmani, Abdullah (57554356500)","58067643600; 57554356500","Towards Creating Exotic Remote Sensing Datasets using Image Generating AI","2022","2022 International Conference on Electrical and Computing Technologies and Applications, ICECTA 2022","","","","84","88","4","10.1109/ICECTA57148.2022.9990245","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146372111&doi=10.1109%2fICECTA57148.2022.9990245&partnerID=40&md5=c9b7a7d99817a1936107810c1d9a8225","Over the past few years, neural networks have been used more often to solve long lasting challenges. Remote sensing and data classification were some of the fields that have widely depended on this continuously developing technology. In this context, remote sensing data related to places with harsh conditions have been missing, especially the ones related to SAR imagery. Such conditions include deserts, glaciers, and icebergs, where lots of people have lost their lives in, due to the lack of efficient methods of searching and finding these people in such critical timing. Training AI models on similar scenarios to fasten the process can be beneficial, but the lack of data is an obstacle in the way of development such models. In this paper, we propose using image generating AI systems to generate remote sensing datasets that are difficult to collect using normal imagery, thus creating more efficient image classification systems that can be used in scenarios such as locating missing people. Several AI models are discussed in this paper: Dall-E 2, Stable Diffusion and Midjourney, where they are found to vary a lot in terms of the generated images, that could be because of the architecture of the model, and the data they trained on. The overall performance of the AI models is promising. Dall-E 2 performed the best in our tests, followed by Stable Diffusion, and finally Midjourney. This research could open the door to using such models in generating lots of datasets, which might solve crucial problems.  © 2022 IEEE.","Arid regions; Classification (of information); Diffusion; Generative adversarial networks; Radar imaging; Sea ice; Synthetic aperture radar; Condition; Data classification; Dataset; Diffusion model; Long lasting; Neural-networks; Remote data; Remote sensing classification; Remote sensing data; Remote-sensing; Remote sensing","Datasets; Diffusion Models; Generative Adversarial Networks; Remote Sensing","Conference paper","Final","","Scopus","2-s2.0-85146372111"
"Belderbos I.; de Jong T.; Popa M.","Belderbos, Itzel (57746011600); de Jong, Tim (57211005630); Popa, Mirela (36458869500)","57746011600; 57211005630; 36458869500","GANs Based Conditional Aerial Images Generation for Imbalanced Learning","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13364 LNCS","","","330","342","12","10.1007/978-3-031-09282-4_28","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132105615&doi=10.1007%2f978-3-031-09282-4_28&partnerID=40&md5=d4f6ca89e517b2c97919c467c47c3572","In this paper, we examine whether we can use Generative Adversarial Networks as an oversampling technique for a largely imbalanced remote sensing dataset containing solar panels, endeavoring a better generalization ability on another geographical location. To this cause, we first analyze the image data by using several clustering methods on latent feature information extracted by a fine-tuned VGG16 network. After that, we use the cluster assignments as auxiliary input for training the GANs. In our experiments we have used three types of GANs: (1) conditional vanilla GANs, (2) conditional Wasserstein GANs, and (3) conditional Self-Attention GANs. The synthetic data generated by each of these GANs is evaluated by both the Fréchet Inception Distance and a comparison of a VGG11-based classification model with and without adding the generated positive images to the original source set. We show that all models are able to generate realistic outputs as well as improving the target performance. Furthermore, using the clusters as a GAN input showed to give a more diversified feature representation, improving stability of learning and lowering the risk of mode collapse. © 2022, Springer Nature Switzerland AG.","Antennas; Deep learning; Remote sensing; Aerial images; Deep learning; Generalization ability; Geographical locations; Image data; Image generations; Imbalanced Learning; Oversampling technique; Remote-sensing; Solar panels; Generative adversarial networks","Deep learning; Generative adversarial networks; Imbalanced learning","Conference paper","Final","","Scopus","2-s2.0-85132105615"
"Shi H.; Zhang B.; Wang Y.; Cui Z.; Chen L.","Shi, Hao (56047288600); Zhang, Bocheng (57216438951); Wang, Yupei (57201309202); Cui, Zihan (57610187400); Chen, Liang (57037284200)","56047288600; 57216438951; 57201309202; 57610187400; 57037284200","SAR-to-Optical Image Translating Through Generate-Validate Adversarial Networks","2022","IEEE Geoscience and Remote Sensing Letters","19","","4506905","","","","10.1109/LGRS.2022.3168391","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128683491&doi=10.1109%2fLGRS.2022.3168391&partnerID=40&md5=e5c74e47f7161aaee9f9cf918d525da5","Synthetic aperture radar (SAR) has the advantages of high resolution in all-weather and all-day. However, SAR images are hard to be understood, due to their unique imaging mechanism. The SAR to optical image translation can assist in interpreting and has become a topic of growing interest in the field of remote sensing. In this letter, a SAR to optical image translation network is proposed, called generate-validate adversarial networks (GVANs). More specifically, there are two Pix2Pix networks form the cyclic structure. The validate module is employed to increase the training process and improve the edge retention ability. In order to improve multidomain images adaptability, the embedded layer is proposed. Additionally, the dilation convolution layer is employed in the generator, which is more suitable for the characteristics of SAR images. The proposed method has experimented on the SEN1-2 dataset. The result demonstrates the superiority of the proposed method over state-of-the-art methods.  © 2004-2012 IEEE.","Convolution; Edge detection; Generative adversarial networks; Geometrical optics; Image enhancement; Radar imaging; Remote sensing; Tracking radar; Adversarial networks; Generator; High resolution; Image edge detection; Optical image; Optical imaging; Radar polarimetry; Synthetic aperture radar images; Synthetic aperture radar-to-optical image translating; U-net; artificial neural network; backscatter; image analysis; model validation; synthetic aperture radar; Synthetic aperture radar","Generative adversarial networks (GVANs); SAR-to-optical image translating; synthetic aperture radar (SAR); U-Net","Article","Final","","Scopus","2-s2.0-85128683491"
"Zheng Y.; Su J.; Zhang S.; Tao M.; Wang L.","Zheng, Yitong (57218834803); Su, Jia (57887304900); Zhang, Shun (57888219200); Tao, Mingliang (57887992300); Wang, Ling (57887992400)","57218834803; 57887304900; 57888219200; 57887992300; 57887992400","Dehaze-AGGAN: Unpaired Remote Sensing Image Dehazing Using Enhanced Attention-Guide Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5630413","","","","10.1109/TGRS.2022.3204890","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137916262&doi=10.1109%2fTGRS.2022.3204890&partnerID=40&md5=529d74dd0db53f0592198490c9ff34b1","Remote sensing image dehazing is of great scientific interest and application value in both military and civil fields. In this article, we propose an enhanced attention-guide generative adversarial network (GAN) network, Dehaze-AGGAN, to solve the remote sensing images dehazing problem, which does not require paired training data. Since haze images have a great influence on remote sensing object detection, the dehazing of remote sensing images has become significantly important. Typical image dehazing methods require a hazy input image and its ground truth in a paired manner, while paired training data are usually not available in the field of remote sensing. To solve this problem, we propose the Dehaze-AGGAN network and train it by feeding unpaired clean and hazy images into the model. We present a novel total variation loss combined with the cycle consistency loss to eliminate wave noise and improve the target edge quality in the test dataset. Moreover, we present a new dehazing dataset called remote sensing dehazing dataset (RSD), which contains 7000 simulate and real hazy images including 3500 warship images and 3500 civilian ship images, and evaluate our method in the dataset. We conduct experiments on RSD. Extensive experiments demonstrate that the proposed Dehaze-AGGAN is effective and has strong robustness and adaptability in different settings.  © 1980-2012 IEEE.","Generative adversarial networks; Image enhancement; Marine pollution; Military photography; Military vehicles; Object detection; Oil spills; Statistical tests; Attention-guided; Dehaze; GAN; Generator; Marine vehicles; Remote-sensing; Task analysis; Total variation loss; Total-variation; Training data; artificial neural network; data set; remote sensing; satellite imagery; simulation; Remote sensing","Attention guided; dehaze; generative adversarial networks (GANs); total variation loss","Article","Final","","Scopus","2-s2.0-85137916262"
"Tu J.; Mei G.; Ma Z.; Piccialli F.","Tu, Jingzhi (57217008206); Mei, Gang (57806891200); Ma, Zhengjing (57219977775); Piccialli, Francesco (42762051900)","57217008206; 57806891200; 57219977775; 42762051900","SWCGAN: Generative Adversarial Network Combining Swin Transformer and CNN for Remote Sensing Image Super-Resolution","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","5662","5673","11","10.1109/JSTARS.2022.3190322","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134297666&doi=10.1109%2fJSTARS.2022.3190322&partnerID=40&md5=7c6271eb172cba28091080f8603bf88a","Easy and efficient acquisition of high-resolution remote sensing images is of importance in geographic information systems. Previously, deep neural networks composed of convolutional layers have achieved impressive progress in super-resolution reconstruction. However, the inherent problems of the convolutional layer, including the difficulty of modeling the long-range dependency, limit the performance of these networks on super-resolution reconstruction. To address the abovementioned problems, we propose a generative adversarial network (GAN) by combining the advantages of the swin transformer and convolutional layers, called SWCGAN. It is different from the previous super-resolution models, which are composed of pure convolutional blocks. The essential idea behind the proposed method is to generate high-resolution images by a generator network with a hybrid of convolutional and swin transformer layers and then to use a pure swin transformer discriminator network for adversarial training. In the proposed method, first, we employ a convolutional layer for shallow feature extraction that can be adapted to flexible input sizes; second, we further propose the residual dense swin transformer block to extract deep features for upsampling to generate high-resolution images; and third, we use a simplified swin transformer as the discriminator for adversarial training. To evaluate the performance of the proposed method, we compare the proposed method with other state-of-the-art methods by utilizing the UCMerced benchmark dataset, and we apply the proposed method to real-world remote sensing images. The results demonstrate that the reconstruction performance of the proposed method outperforms other state-of-the-art methods in most metrics.  © 2008-2012 IEEE.","Benchmarking; Convolution; Deep neural networks; Image reconstruction; Optical resolving power; Remote sensing; Convolutional layer; Generative adversarial network; High-resolution images; High-resolution remote sensing images; Image super resolutions; Performance; Remote sensing images; State-of-the-art methods; Super-resolution reconstruction; Swin transformer; input-output analysis; numerical model; remote sensing; sampling; satellite data; satellite imagery; spatial resolution; Generative adversarial networks","Convolutional layers; generative adversarial network (GAN); remote sensing images; super-resolution reconstruction; swin transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134297666"
"Yu N.; Ma A.; Zhong Y.; Gong X.","Yu, Ning (57457393900); Ma, Ailong (55972916000); Zhong, Yanfei (12039673900); Gong, Xiaodong (57937220400)","57457393900; 55972916000; 12039673900; 57937220400","HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2864","2867","3","10.1109/IGARSS46834.2022.9883519","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140359276&doi=10.1109%2fIGARSS46834.2022.9883519&partnerID=40&md5=a349a82425091fd96f9ab0317f8e4fc2","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation. © 2022 IEEE.","Aberrations; Color; Colorimetry; Generative adversarial networks; Geometrical optics; Image fusion; Optical remote sensing; Radar imaging; Textures; Color information; Expert knowledge; Image tranlation; Image translation; Imaging mechanism; Optical image; Optical remote sensing; Remote-sensing; SAR Images; Spatial informations; Synthetic aperture radar","Generative Adversarial Network; Image tranlation; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85140359276"
"Lu T.; Zhao K.; Wu Y.; Wang Z.; Zhang Y.","Lu, Tao (56406646300); Zhao, Kanghui (57220898298); Wu, Yuntao (55993578900); Wang, Zhongyuan (34973569100); Zhang, Yanduo (55993581700)","56406646300; 57220898298; 55993578900; 34973569100; 55993581700","Structure-Texture Parallel Embedding for Remote Sensing Image Super-Resolution","2022","IEEE Geoscience and Remote Sensing Letters","19","","6516105","","","","10.1109/LGRS.2022.3206348","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139380653&doi=10.1109%2fLGRS.2022.3206348&partnerID=40&md5=3bdfd90776645e24ea1815eb0d85b7c3","The structure and texture of images are crucial for remote sensing image super-resolution (SR). Generative adversarial networks (GANs) recover image details through adversarial training. However, the recovered images always have structural distortions, on the one hand, and GANs are difficult to train, on the other hand. In addition, some methods assist reconstruction by introducing prior information of the image, but this brings additional computational cost. To address this issue, we propose a novel structure-texture parallel embedding (SPE) method for SR of remote sensing images. Our method does not require additional image priors to reconstruct high-quality images. Specifically, we use the global structure information and local texture information of the image in the ascending space to guide the reconstruction result of the image. First, we design a structure preserving block (SPB) to extract global structural features in the ascending space of the image, so as to obtain global structure information for a priori representation. Then, we design a local texture attention module (LTAM) to restore richer texture details. We have conducted lots of experiments on Draper public dataset. Experimental results show that our proposed method not only achieves a better tradeoff between computational cost and performance, but also outperforms the existing several SR methods in terms of objective index evaluation and subjective visual effects.  © 2004-2012 IEEE.","Economic and social effects; Embeddings; Generative adversarial networks; Image texture; Optical resolving power; Remote sensing; Space optics; Textures; Attention mechanisms; Features extraction; Image super resolutions; Images reconstruction; Remote sensing image super-resolution; Remote sensing images; Remote-sensing; Structure-preserving; Superresolution; Texture attention mechanism; data set; image resolution; performance assessment; remote sensing; Image reconstruction","Remote sensing image super-resolution (SR); structure preserving; texture attention mechanism","Article","Final","","Scopus","2-s2.0-85139380653"
"Chen S.; Liu X.; Meng S.","Chen, Siqi (57870097500); Liu, Xiao (57219490472); Meng, Shilong (57869951700)","57870097500; 57219490472; 57869951700","Generative Adversarial Network-Based Methods in Super Resolution","2022","ISCTT 2021 - 6th International Conference on Information Science, Computer Technology and Transportation","","","","375","381","6","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137114222&partnerID=40&md5=e1c785d61b87b993ae88d4ff346fa2c1","Super-resolution (SR) is a crucial image processing technique to optimize the resolution of images and videos. Recent years have witnessed significant development of SR approaches using Generative Adversarial Nets (GAN). Herein, a thorough overview on the latest achievements of SR approaches using GAN are given. Specifically, we firstly define the SR problem. Then, we introduce traditional and deep-learning (DL)-based SR techniques including Convolutional Neural Network (CNN) and GAN methods. Afterward, we explore the background of GAN and pay special attention to GAN-based approaches such as SRGAN, GMGAN, and Cycle-GAN. In addition, we also cover the applications of GAN-based SR approaches in real life, including medical diagnosis and remote sensing. © VDE VERLAG GMBH · Berlin · Offenbach.","Deep learning; Diagnosis; Image processing; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; Image processing technique; Learning-based super-resolution; Network-based; Remote-sensing; Resolution techniques; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85137114222"
"Wang H.; Wu Y.; Ni Q.; Liu W.","Wang, Hancong (57807054000); Wu, Yin (56093628700); Ni, Qiang (57614625100); Liu, Wenbo (55723547200)","57807054000; 56093628700; 57614625100; 55723547200","A Novel Wireless Leaf Area Index Sensor Based on a Combined U-Net Deep Learning Model","2022","IEEE Sensors Journal","22","16","","16573","16585","12","10.1109/JSEN.2022.3188697","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134328841&doi=10.1109%2fJSEN.2022.3188697&partnerID=40&md5=8ad0e55d3bf3341ff686cb5f1b34da75","Leaf area index (LAI) is an important parameter for forestry vegetation canopy structure investigation and ecological environment model study. Traditional ground direct measuring method is too time and labor consuming, while the remote sensing technique lacks of adequate validation and comparative analysis. Here, a novel wireless LAI sensor based on a lightweight deep learning model (LAINET) has been designed with a Raspberry Pi microcomputer and a LoRa transceiver. The mainly metering pattern of sensor system is the digital hemispherical photo-graphy (DHP) methodology based on Beer-Lambert law: firstly, the crown canopy's image is captured and segmented by LAINET, then the vegetation gap fraction can be extracted to calculate the LAI value. Our proposed LAINET consists of a lightweight convolutional neural network (CNN) and a generative adversarial network (GAN). The average accuracy of semantic segmentation (i.e. CNN part) could reach 0.978, and the combination of GAN for image super-resolution reconstruction can improve the accuracy of gap fraction measurement more by 5.5%. In addition, LAINET effectively solves the problem of low segmentation accuracy brought by environmental effects, the separation accuracy in direct sunlight or clear weather has been improved significantly. So the ultimate LAI value can be calculated precisely and stably. Experiment results show that the proposed sensor obtains a fine measuring error of less than 4% when comparing with the commercial plant canopy analyzer HM-G20. Combined with Uninterruptible Power Supply module of 5200 mAh, the sensor can work effectively for about 8 months, principally meeting the deployment and measurement criteria of forestry LAI. Therefore, the wireless sensor presented in this paper has a great application prospect.  © 2001-2012 IEEE.","Deep learning; Forestry; Generative adversarial networks; Image enhancement; Neural networks; Radio transceivers; Remote sensing; Semantic Segmentation; Semantics; Timber; Vegetation mapping; Canopy fisheye image; Deep learning; Fisheye images; Images segmentations; Leaf Area Index; Learning models; Raspberry pi; Vegetation mapping; Wireless communications; Wireless sensor; Wireless sensor networks","canopy fisheye image; deep learning; Leaf area index; Raspberry Pi; wireless sensor","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134328841"
"Jiang M.; Shen H.; Li J.","Jiang, Menghui (57210173702); Shen, Huanfeng (8359721100); Li, Jie (57214207213)","57210173702; 8359721100; 57214207213","Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5410915","","","","10.1109/TGRS.2022.3188998","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135232679&doi=10.1109%2fTGRS.2022.3188998&partnerID=40&md5=8337c148bae6361ca03536bbf9d398c0","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal-spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.  © 1980-2012 IEEE.","Deep learning; Feature extraction; Feedback; Generative adversarial networks; Optical remote sensing; Radiometers; Satellite imagery; Cloud cover; Deep residual cycle generative adversarial network; Features extraction; Generator; Heterogeneous integrated framework; Integrated frameworks; Land-cover change; Remote-sensing; Spatial resolution; Thick cloud cover; cloud cover; integrated approach; Landsat; MODIS; network analysis; remote sensing; satellite imagery; Sentinel; spatiotemporal analysis; Image fusion","Deep residual cycle generative adversarial network (GAN); heterogeneous integrated framework; land-cover change; thick cloud cover","Article","Final","","Scopus","2-s2.0-85135232679"
"","","","12th IFIP TC 12 International Conference on Intelligent Information Processing, IIP 2022","2022","IFIP Advances in Information and Communication Technology","643 IFIP","","","","","548","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132016256&partnerID=40&md5=745a66aeb0f9052a07ef3856ed5aaaec","The proceedings contain 43 papers. The special focus in this conference is on Intelligent Information Processing. The topics include: A Game-Theoretic Analysis of Impulse Purchase; research on Blockchain Privacy Protection Mechanism in Financial Transaction Services Based on Zero-Knowledge Proof and Federal Learning; a Hybrid Parallel Algorithm With Multiple Improved Strategies; resource Scheduling for Human-Machine Collaboration in Multiagent Systems; pre-loaded Deep-Q Learning; using Multi-level Attention Based on Concept Embedding Enrichen Short Text to Classification; does Large Pretrained Dataset Always Help? On the Effect of Dataset Size on Big Transfer Model; augmenting Context Representation with Triggers Knowledge for Relation Extraction; high-Resolution Remote Sensing Image Semantic Segmentation Method Based on Improved Encoder-Decoder Convolutional Neural Network; preface; classification Between Rumors and Explanations of Rumors Based on Common and Difference Subsequences of Sentences; augmenting Convolution Neural Networks by Utilizing Attention Mechanism for Knowledge Tracing; a Hybrid Multi-objective Optimization Algorithm with Improved Neighborhood Rough Sets for Feature Selection; predicting Student Performance in Online Learning Using a Highly Efficient Gradient Boosting Decision Tree; A Method for AGV Double-Cycling Scheduling at Automated Container Terminals; inductive Light Graph Convolution Network for Text Classification Based on Word-Label Graph; super-Resolution of Defocus Thread Image Based on Cycle Generative Adversarial Networks; a Method on Online Learning Video Recommendation Method Based on Knowledge Graph; A HEp-2 Cell Image Classification Model Based on Deep Residual Shrinkage Network Combined with Dilated Convolution; attention Adaptive Chinese Named Entity Recognition Based on Vocabulary Enhancement; software Defect Prediction Method Based on Cost-Sensitive Random Forest; a Pear Leaf Diseases Image Recognition Model Based on Capsule Network.","","","Conference review","Final","","Scopus","2-s2.0-85132016256"
"Luo Y.; Feng A.; Li H.; Li D.; Wu X.; Liao J.; Zhang C.; Zheng X.; Pu H.","Luo, Yuanjiang (57224686626); Feng, Ao (57221536626); Li, Hongxiang (57218441760); Li, Danyang (57281018200); Wu, Xuan (57767877900); Liao, Jie (57751433300); Zhang, Chengwu (57835327800); Zheng, Xingqiang (57835391600); Pu, Haibo (14519797000)","57224686626; 57221536626; 57218441760; 57281018200; 57767877900; 57751433300; 57835327800; 57835391600; 14519797000","New deep learning method for efficient extraction of small water from remote sensing images","2022","PLoS ONE","17","8 August","e0272317","","","","10.1371/journal.pone.0272317","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135598458&doi=10.1371%2fjournal.pone.0272317&partnerID=40&md5=0a0e7b77312766fe5486d9491f5c36cc","Extracting water bodies from remote sensing images is important in many fields, such as in water resources information acquisition and analysis. Conventional methods of water body extraction enhance the differences between water bodies and other interfering water bodies to improve the accuracy of water body boundary extraction. Multiple methods must be used alternately to extract water body boundaries more accurately. Water body extraction methods combined with neural networks struggle to improve the extraction accuracy of fine water bodies while ensuring an overall extraction effect. In this study, false color processing and a generative adversarial network (GAN) were added to reconstruct remote sensing images and enhance the features of tiny water bodies. In addition, a multi-scale input strategy was designed to reduce the training cost. We input the processed data into a new water body extraction method based on strip pooling for remote sensing images, which is an improvement of DeepLabv3+. Strip pooling was introduced in the DeepLabv3+ network to better extract water bodies with a discrete distribution at long distances using different strip kernels. The experiments and tests show that the proposed method can improve the accuracy of water body extraction and is effective in fine water body extraction. Compared with seven other traditional remote sensing water body extraction methods and deep learning semantic segmentation methods, the prediction accuracy of the proposed method reaches 94.72%. In summary, the proposed method performs water body extraction better than existing methods. © 2022 Luo et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Remote Sensing Technology; Water; water; water; accuracy; algorithm; anisotropy; Article; back propagation; data processing; deep learning; extraction; image segmentation; mathematical model; nonhuman; prediction; qualitative analysis; quantitative analysis; remote sensing; training; image processing; procedures","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85135598458"
"Jin X.; Li Y.; Wan J.; Lyu X.; Ren P.; Shang J.","Jin, Xifang (36625703400); Li, Yun (57735469900); Wan, Jianhua (18937805700); Lyu, Xinrong (56096489800); Ren, Peng (57736297300); Shang, Jie (57735470000)","36625703400; 57735469900; 18937805700; 56096489800; 57736297300; 57735470000","MODIS Green-Tide Detection With a Squeeze and Excitation Oriented Generative Adversarial Network","2022","IEEE Access","10","","","60294","60305","11","10.1109/ACCESS.2022.3180331","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131728699&doi=10.1109%2fACCESS.2022.3180331&partnerID=40&md5=2b3edd11f58dfa185d71bc1d08a908fd","This paper presents a novel framework combining spectral analysis and machine learning for green-tide detection. The framework incorporates a squeeze and excitation (SE) attention module into a U-shaped generator of a generative adversarial network (GAN), and is referred to as squeeze and excitation oriented generative adversarial network (SE-GAN). In the SE-GAN, the normalized differential vegetation index (NDVI) is used as the preprocessing filter, which enhances the information associated with green-tide. The SE attention module recalibrates the feature maps so as to enhance the useful features conveyed from the generator's convolution layer while suppressing less useful ones. Overall, the generator attempts to render images that contain green-tide in a way that highly approximates the reference images, while the discriminator characterizes the difference between the generated images and the reference images. The training scheme, which is adversarial, minimizes the f- divergence between the generator and discriminator. Consequently, compared to other green-tide detection algorithms only applicable in small-area scenes, SE-GAN can automatically detect green-tide in MODIS images of any size. Experiments with both large- and small-format MODIS imagery confirm that SE-GAN's detection results are superior to those of five other commonly used methods.  © 2013 IEEE.","Generative adversarial networks; Photomapping; Remote sensing; Spectrum analysis; Vegetation; Green products; Green tides; Green-tide detection; Index; Normalized differential vegetation indices; Reference image; Sea-land separation; Squeeze and excitation; Squeeze and excitation-generative adversarial network; Vegetation mapping; Radiometers","GAN; green-tide detection; NDVI; SE; SE-GAN; sea-land separation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131728699"
"Li Y.; Wang Y.; Li B.; Wu S.","Li, Yunhe (55647591200); Wang, Yi (57204548320); Li, Bo (57777715900); Wu, Shaohua (57189245768)","55647591200; 57204548320; 57777715900; 57189245768","Super-Resolution of Remote Sensing Images for ×4 Resolution without Reference Images","2022","Electronics (Switzerland)","11","21","3474","","","","10.3390/electronics11213474","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141875470&doi=10.3390%2felectronics11213474&partnerID=40&md5=2b0b794fd72a9dfbed4cf586766ec1e8","Sentinel-2 satellites can provide free optical remote-sensing images with a spatial resolution of up to 10 M, but the spatial details provided are not enough for many applications, so it is worth considering improving the spatial resolution of Sentinel-2 satellites images through super-resolution (SR). Currently, the most effective SR models are mainly based on deep learning, especially the generative adversarial network (GAN). Models based on GAN need to be trained on LR–HR image pairs. In this paper, a two-step super-resolution generative adversarial network (TS-SRGAN) model is proposed. The first step is having the GAN train the degraded models. Without supervised HR images, only the 10 m resolution images provided by Sentinel-2 satellites are used to generate the degraded images, which are in the same domain as the real LR images, and then to construct the near-natural LR–HR image pairs. The second step is to design a super-resolution generative adversarial network with strengthened perceptual features, to enhance the perceptual effects of the generated images. Through experiments, the proposed method obtained an average NIQE as low as 2.54, and outperformed state-of-the-art models according to other two NR-IQA metrics, such as BRISQUE and PIQE. At the same time, the comparison of the intuitive visual effects of the generated images also proved the effectiveness of TS-SRGAN. © 2022 by the authors.","","generative adversarial network; remote-sensing image; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141875470"
"Li J.-T.; Bian Z.; Guo L.-X.","Li, Jiang-Ting (37006755000); Bian, Zheng (57208108900); Guo, Li-Xin (56498191400)","37006755000; 57208108900; 56498191400","Optimized complex object classification model: reconstructing the ISAR image of a hypersonic vehicle covered with a plasma sheath using a U-WGAN-GP framework","2022","International Journal of Remote Sensing","43","14","","5306","5323","17","10.1080/01431161.2022.2133578","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140244358&doi=10.1080%2f01431161.2022.2133578&partnerID=40&md5=8d670dc17d22a30cfa02d8a00b7ce710","This study aims to apply generative adversarial networks (GANs) to the effective classification of high/low resolution (HR/LR) image pairs obtained via inverse synthetic aperture radar (ISAR) for hypersonic objects covered with a plasma sheath. We propose a classification training model based on a Wasserstein GAN with a gradient penalty (U-WGAN-GP) framework, wherein a U-Net with an excellent jump connection structure is used as a generator, and a VGG-Net with high robustness is used as a discriminator, to support the reliable classification of HR/LR ISAR image pairs for the enhancement of ISAR image resolution. The WGAN-GP provides a shortcut for the gradient propagation of network parameters during the training stage through the stable encoder and decoder structure of U-Net. It inherits the echo intensity variation and position distribution characteristics of the scattering points between hypersonic objects in LR images and effectively transplants them into the generated super-resolution ISAR images, and it establishes end-to-end mapping between the HR/LR ISAR images. Moreover, the VGG-Net ensures that the generated super-resolution images are stable, controllable, and undistorted. In addition, the WGAN-GP structure combines the content and adversarial losses, optimizes the generator loss function, and uses the GP method to provide a stable and continuous training process. Finally, a traditional VGG-16 network classifier is added at the end of the training model. The proposed model is applied to an HR/LR image pair dataset of hypersonic objects. Compared with existing methods, the proposed method improved the classification and recognition accuracy from 54.4% to 81.8%. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Generative adversarial networks; Hypersonic aerodynamics; Hypersonic vehicles; Image enhancement; Image resolution; Inverse synthetic aperture radar; Plasma sheaths; Radar imaging; High-low; High/low resolution (HR/LR) image pair; Hypersonic object; Image pairs; Inverse synthetic aperture radar; Inverse synthetic aperture radar images; Lower resolution; Training model; U-net; Wasserstein generative adversarial network with gradient penalty; conceptual framework; image analysis; image resolution; inverse problem; remote sensing; synthetic aperture radar; Inverse problems","High/low resolution (HR/LR) image pairs; hypersonic objects; inverse synthetic aperture radar (ISAR); plasma sheath; U-Net; Wasserstein generative adversarial networks with gradient penalty (WGAN-GP)","Article","Final","","Scopus","2-s2.0-85140244358"
"Wang S.; Guo J.; Zhang Y.; Wu Y.","Wang, Shihong (57221762372); Guo, Jiayi (57194143247); Zhang, Yueting (57218470544); Wu, Yirong (8403430500)","57221762372; 57194143247; 57218470544; 8403430500","Multi-baseline SAR 3D reconstruction of vehicle from very sparse aspects: A generative adversarial network based approach","2023","ISPRS Journal of Photogrammetry and Remote Sensing","197","","","36","55","19","10.1016/j.isprsjprs.2023.01.022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147196612&doi=10.1016%2fj.isprsjprs.2023.01.022&partnerID=40&md5=b48f60c62982d112fd8cbc403133970d","Reconstruction of anisotropic targets using SAR from multiple observation aspects is an important research direction. Conventionally, to obtain reconstruction of high quality using compressed sensing (CS) or back-projection (BP) methods, it is essential to acquire signals from plenty of aspects. However, limited by the flight trajectory and other factors, only few aspects are available. Besides, massive signal data from abundant aspects brings huge burden in computation. Thus, a new framework to generate 3D reconstruction of anisotropic targets from few aspects with low computational cost is necessary. To tackle this problem, we propose a new framework, which combines conventional CS algorithm and neural network. The CS algorithm is an imaging module to transform data from frequency domain into spatial domain with high resolution in each aspect to generate incomplete outputs. The sparse-aspects-completion network (SACNet) based on GAN principle is originally introduced to predict integral structures from the incomplete results. To evaluate the effectiveness and robustness of our framework, the simulation data (Civilian Vehicle Demo) is used to train while the measured data (GOTCHA) is for validating. Extensive experiments are conducted under condition of {4,6,8,10} aspects to estimate the performance with respect to the number of aspects. The proposed framework achieves the highest IoU and the lowest BCE metrics compared with the conventional algorithms and the SOTA networks used in similar optical tasks in both simulation and measured datasets under various number of aspects, which proves the effectiveness and robustness of our framework. The source code is available at: https://gitee.com/WshongCola/sar_3D_multi_aspect. © 2023 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Anisotropy; Frequency domain analysis; Generative adversarial networks; Image reconstruction; Remote sensing; Vehicles; 3D reconstruction; Anisotropic target; Back-projection methods; Compressed-Sensing; High quality; Multi-baseline; Network-based approach; Sensing algorithms; Vehicle targets; Very sparse aspect; algorithm; artificial neural network; reconstruction; signal; synthetic aperture radar; three-dimensional modeling; Compressed sensing","3D reconstruction; Multi-baseline; SAR; Vehicle target; Very sparse aspects","Article","Final","","Scopus","2-s2.0-85147196612"
"Alsughayer R.; Hussain M.; Saeed F.; AboalSamh H.","Alsughayer, Rawan (57923925600); Hussain, Muhammad (36476370800); Saeed, Fahman (57188848302); AboalSamh, Hatim (6508339935)","57923925600; 36476370800; 57188848302; 6508339935","Detection and localization of splicing on remote sensing images using image-to-image transformation","2022","Applied Intelligence","","","","","","","10.1007/s10489-022-04126-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139611468&doi=10.1007%2fs10489-022-04126-7&partnerID=40&md5=8400c4ee865047b2ab24d47aea5b3ecf","Remote sensing images can be easily tampered with user-friendly tools to hide important information. It necessitated the development of automatic splicing detection methods. The existing few methods concentrate on the semantic content in images for tamper detection and are not robust. On the contrary, we hypothesize that residual noise is independent of the semantic content and embeds the tampering traces; it is helpful for splice detection. In view of this, we focus on residual noise and formulate the problem as an image-to-image transformation, and model it using a U-net architecture. To suppress semantic content and extract the residual noise, we introduce a constrained convolutional layer in the U-net model. The model processes the input image and yields a map that localizes tampering in case of splicing. The model is trained using the conditional generative adversarial network (cGAN) framework. The loss function is composed of the cross-entropy, the Jaccard, and the end-point error (EPE) loss functions to enhance the detection and localization of tampered regions. To evaluate the proposed method, we develop a new dataset containing remote sensing images from different satellites and aerial sensors. The model detects splicing at pixel and image levels with high accuracy. It shows good robustness against well-known post-processing operations, including Gaussian blurring (GB) and white additive Gaussian noise (WAGN). © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Antennas; Convolution; Convolutional neural networks; Gaussian noise (electronic); Remote sensing; Semantics; Conditional generative adversarial network; Convolutional neural network; Detection and localization; Image splicing detection; Image transformations; Loss functions; Remote sensing images; Residual noise; Semantic content; User-friendly tool; Generative adversarial networks","cGAN; Convolutional neural network; Image splicing detection","Article","Article in press","","Scopus","2-s2.0-85139611468"
"Aljarrah I.A.; Alshare E.M.","Aljarrah, Inad A. (12799816300); Alshare, Eman M. (57641058000)","12799816300; 57641058000","Improved Residual Dense Network for Large Scale Super-Resolution via Generative Adversarial Network","2022","International Journal of Communication Networks and Information Security","14","1","","118","125","7","10.54039/ijcnis.v14i1.5221","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128840584&doi=10.54039%2fijcnis.v14i1.5221&partnerID=40&md5=96e40e00eaf60ee414c18fd5ee27c8cd","Recent single image super resolution (SISR) studies were conducted extensively on small upscaling factors such as x2 and x4 on remote sensing images, while less work was conducted on large factors such as the factor x8 and x16. Owing to the high performance of the generative adversarial networks (GANs), in this paper, two GAN’s frameworks are implemented to study the SISR on the residual remote sensing image with large magnification under x8 scale factor, which is still lacking acceptable results. This work proposes a modified version of the residual dense network (RDN) and then it been implemented within GAN framework which named RDGAN. The second GAN framework has been built based on the densely sampled super resolution network (DSSR) and we named DSGAN. The used loss function for the training employs the adversarial, mean squared error (MSE) and the perceptual loss derived from the VGG19 model. We optimize the training by using Adam for number of epochs then switching to the SGD optimizer. We validate the frameworks on the proposed dataset of this work and other three remote sensing datasets: the UC Merced, WHURS19 and RSSCN7. To validate the frameworks, we use the following image quality assessment metrics: the PSNR and the SSIM on the RGB and the Y channel and the MSE. The RDGAN evaluation values on the proposed dataset were 26.02, 0.704, and 257.70 for PSNR, SSIM and the MSE, respectively, and the DSGAN evaluation on the same dataset yielded 26.13, 0.708 and 251.89 for the PSNR, the SSIM, and the MSE. © 2022 International Journal of Communication Networks and Information Security","","Generative adversarial network; Remote sensing; Residual dense generative adversarial network.; Residual dense network; Single image super-resolution","Article","Final","","Scopus","2-s2.0-85128840584"
"Qin H.; Xie W.; Li Y.; Jiang K.; Lei J.; Du Q.","Qin, Haonan (57209330434); Xie, Weiying (56768656200); Li, Yunsong (57956995400); Jiang, Kai (57192700864); Lei, Jie (36663710700); Du, Qian (57254219600)","57209330434; 56768656200; 57956995400; 57192700864; 36663710700; 57254219600","Weakly supervised adversarial learning via latent space for hyperspectral target detection","2023","Pattern Recognition","135","","109125","","","","10.1016/j.patcog.2022.109125","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141546256&doi=10.1016%2fj.patcog.2022.109125&partnerID=40&md5=dffa7e5fbd683fc64b8fced020635fc8","As an advanced technique in remote sensing, hyperspectral target detection (HTD) is widely concerned in civilian and military applications. However, the limitation of prior and mixed pixels phenomenon makes HTD models sensitive to data corruption under various interference from environment. In this work, a novel two-stage detection framework based on adversarial learning is proposed, which extracts spectral features in latent space through background reconstruction under weak supervision. To address the issues of insufficient utilization of both background information and limited prior knowledge, the generative adversarial network (GAN) is applied to estimate background in a weakly supervised manner with target-based constraints and channel-wise attention, which produces the detection proposal in the first stage. Then, a refined result is produced in the second stage, in which the input data consists of the refined data and refined feature map based on previous detection proposal. To provide samples for weakly supervised learning (WSL), the pseudo datasets are produced by a coarse sample selection procedure, which makes full use of limited prior information. Finally, an exponential constrained nonlinear function is adopted to acquire pixel-level prediction via suppressing the background and combining features from different stages. Experiments on real hyperspectral images (HSIs) captured by different sensors at various scenes verify the effectiveness of the proposed framework. © 2022","Feature extraction; Generative adversarial networks; Military photography; Pixels; Radar target recognition; Remote sensing; Spectroscopy; Supervised learning; Adversarial learning; Detection models; HyperSpectral; Hyperspectral image; Hyperspectral target detection; Latent space; Mixed pixel; Remote-sensing; Targets detection; Weakly supervised learning; Military applications","Adversarial learning; Hyperspectral image; Latent space; Target detection; Weakly supervised learning","Article","Final","","Scopus","2-s2.0-85141546256"
"Wu C.; Du B.; Zhang L.","Wu, Chen (55585973400); Du, Bo (57217375214); Zhang, Liangpei (8359720900)","55585973400; 57217375214; 8359720900","Fully Convolutional Change Detection Framework with Generative Adversarial Network for Unsupervised, Weakly Supervised and Regional Supervised Change Detection","2023","IEEE Transactions on Pattern Analysis and Machine Intelligence","","","","1","15","14","10.1109/TPAMI.2023.3237896","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147293048&doi=10.1109%2fTPAMI.2023.3237896&partnerID=40&md5=f598c847c53a78f69fb79041f23fd594","Deep learning for change detection is one of the current hot topics in the field of remote sensing. However, most end-to-end networks are proposed for supervised change detection, and unsupervised change detection models depend on traditional pre-detection methods. Therefore, we proposed a fully convolutional change detection framework with generative adversarial network, to unify unsupervised, weakly supervised, regional supervised, and fully supervised change detection tasks into one end-to-end framework. A basic Unet segmentor is used to obtain change detection map, an image-to-image generator is implemented to model the spectral and spatial variation between multi-temporal images, and a discriminator for changed and unchanged is proposed for modeling the semantic changes in weakly and regional supervised change detection task. The iterative optimization of segmentor and generator can build an end-to-end network for unsupervised change detection, the adversarial process between segmentor and discriminator can provide the solutions for weakly and regional supervised change detection, the segmentor itself can be trained for fully supervised task. The experiments indicate the effectiveness of the propsed framework in unsupervised, weakly supervised and regional supervised change detection. This paper provides new theorical definitions for unsupervised, weakly supervised and regional supervised change detection tasks with the proposed framework, and shows great potentials in exploring end-to-end network for remote sensing change detection. IEEE","Change detection; Convolution; Deep learning; Generative adversarial networks; Image segmentation; Iterative methods; Semantics; Change detection; Fully covolutional network; Generator; Images segmentations; Predictive models; Remote-sensing; Supervised segmentation; Task analysis; Weakly supervised segmentation; Remote sensing","Change Detection; Fully Covolutional Network; Generative Adversarial Network; Generative adversarial networks; Generators; Image segmentation; Predictive models; Remote Sensing; Remote sensing; Task analysis; Training; Weakly Supervised Segmentation","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85147293048"
"Ren T.; Xu H.; Jiang G.; Yu M.; Zhang X.; Wang B.; Luo T.","Ren, Tingdi (57605422500); Xu, Haiyong (55462879200); Jiang, Gangyi (7401706697); Yu, Mei (57762346100); Zhang, Xuan (57605927600); Wang, Biao (57888440900); Luo, Ting (54397301800)","57605422500; 55462879200; 7401706697; 57762346100; 57605927600; 57888440900; 54397301800","Reinforced Swin-Convs Transformer for Simultaneous Underwater Sensing Scene Image Enhancement and Super-resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4209616","","","","10.1109/TGRS.2022.3205061","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137878445&doi=10.1109%2fTGRS.2022.3205061&partnerID=40&md5=55fdb6560fa249229efcd66661ed54a6","Underwater image enhancement (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. Meanwhile, the ever-increasing requirement for higher resolution images from a lower resolution in the underwater domain cannot be overlooked. To address these problems, a novel U-Net-based reinforced Swin-Convs Transformer for simultaneous enhancement and superresolution (URSCT-SESR) method is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, the Swin Transformer is embedded into U-Net for improving the ability to capture the global dependence. Then, given the inadequacy of the Swin Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, an ingenious manner is presented for the fusion of convolutions and the core attention mechanism to build a reinforced Swin-Convs Transformer block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the Swin Transformer. Finally, experimental results on available datasets demonstrate that the proposed URSCT-SESR achieves the state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code is publicly available at https://github.com/TingdiRen/URSCT-SESR.  © 1980-2012 IEEE.","Convolution; Generative adversarial networks; Image reconstruction; Image resolution; Light absorption; Neural networks; Reinforcement; Atmospheric modeling; Convolutional neural network; Scene image; Superresolution; Swin-convs transformer; Transformer; U-net; Underwater image enhancements; Underwater image enhancemnet; image classification; image resolution; instrumentation; remote sensing; satellite imagery; Image enhancement","Super-resolution (SR); Swin-Convs Transformer; U-Net; underwater image enhancement (UIE)","Article","Final","","Scopus","2-s2.0-85137878445"
"Nie H.; Fu Z.; Tang B.-H.; Li Z.; Chen S.; Wang L.","Nie, Han (57751405100); Fu, Zhitao (57217523670); Tang, Bo-Hui (14009374600); Li, Ziqian (57750383900); Chen, Sijing (57750894600); Wang, Leiguang (57196339250)","57751405100; 57217523670; 14009374600; 57750383900; 57750894600; 57196339250","A Dual-Generator Translation Network Fusing Texture and Structure Features for SAR and Optical Image Matching","2022","Remote Sensing","14","12","2946","","","","10.3390/rs14122946","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132877338&doi=10.3390%2frs14122946&partnerID=40&md5=bcee3adc36bde4fd295ea4a3942fc331","The matching problem for heterologous remote sensing images can be simplified to the matching problem for pseudo homologous remote sensing images via image translation to improve the matching performance. Among such applications, the translation of synthetic aperture radar (SAR) and optical images is the current focus of research. However, the existing methods for SAR-tooptical translation have two main drawbacks. First, single generators usually sacrifice either structure or texture features to balance the model performance and complexity, which often results in textural or structural distortion; second, due to large nonlinear radiation distortions (NRDs) in SAR images, there are still visual differences between the pseudo-optical images generated by current generative adversarial networks (GANs) and real optical images. Therefore, we propose a dual-generator translation network for fusing structure and texture features. On the one hand, the proposed network has dual generators, a texture generator, and a structure generator, with good cross-coupling to obtain high-accuracy structure and texture features; on the other hand, frequency-domain and spatialdomain loss functions are introduced to reduce the differences between pseudo-optical images and real optical images. Extensive quantitative and qualitative experiments show that our method achieves state-of-the-art performance on publicly available optical and SAR datasets. Our method improves the peak signal-to-noise ratio (PSNR) by 21.0%, the chromatic feature similarity (FSIMc) by 6.9%, and the structural similarity (SSIM) by 161.7% in terms of the average metric values on all test images compared with the next best results. In addition, we present a before-and-after translation comparison experiment to show that our method improves the average keypoint repeatability by approximately 111.7% and the matching accuracy by approximately 5.25%. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Frequency domain analysis; Generative adversarial networks; Geometrical optics; Image enhancement; Image matching; Image texture; Optical remote sensing; Radar imaging; Signal to noise ratio; Textures; Dual-generator; Image translation; Optical image; Structure features; Synthetic aperture radar and optical image matching; Synthetic aperture radar images; Synthetic aperture radar-to-optical image translation; Texture and structure fusing; Texture features; Synthetic aperture radar","dual-generator; SAR and optical image matching; SAR-to-optical image translation; texture and structure fusing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132877338"
"Tang H.; Xiao J.; Zhai Y.; Yang D.","Tang, Haoyang (56190818200); Xiao, Jiaxin (57875450900); Zhai, Yuxiang (57447250000); Yang, Dongfang (55888542900)","56190818200; 57875450900; 57447250000; 55888542900","Remote sensing image matching algorithm based on cycle generative adversarial strategy; [基于循环生成对抗策略的遥感图像匹配算法]","2022","Guangdianzi Jiguang/Journal of Optoelectronics Laser","33","8","","824","830","6","10.16136/j.joel.2022.08.0811","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137295321&doi=10.16136%2fj.joel.2022.08.0811&partnerID=40&md5=2e46f0a7814719dc46aef809be17edc6","Aming at the difficulty of image matching caused by different imaging modes, time phases and resolutions of heterogeneous remote sensing images, a remote sensing image matching algorithm based on cycle generative adversarial strategy is proposed. A cross-data domain image feature migration cycle generative adversarial network (GAN) was constructed, a SmoothL1 loss function was designed to optimize the network, the accuracy of remote sensing image feature extraction was improved, and based on the result of image feature migration, triple margin ranking loss function (TMRL) was established to reduce remote sensing image mismatched points, to achieve accurate matching of heterogeneous remote sensing images. The test results show that the method in this paper improves the average accuracy of heterogeneous remote sensing image matching by 33.51%, and has a better remote sensing image matching effect than the cross modality matching net (CMM-Net) method. In addition, this method not require the annotation information of the target domain image, and the matching time is shortened by 0.073 s, which can quickly and accurately achieve heterogeneous remote image matching. © 2022, The Editorial Department of Journal of Optoelectronics•Laser. All right reserved.","","Feature extraction; Generative adversarial; Image matching; Style migration","Article","Final","","Scopus","2-s2.0-85137295321"
"Nie W.; Gou P.; Liu Y.; Shrestha B.; Zhou T.; Xu N.; Wang P.; Du Q.","Nie, Wei (57854867800); Gou, Peng (57853888900); Liu, Yang (57859610000); Shrestha, Bhaskar (57970970500); Zhou, Tianyu (57854169000); Xu, Nuo (57200211778); Wang, Peng (57309244300); Du, QiQi (57854316300)","57854867800; 57853888900; 57859610000; 57970970500; 57854169000; 57200211778; 57309244300; 57854316300","Semi Supervised Change Detection Method of Remote Sensing Image","2022","IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","2022-October","","","1013","1019","6","10.1109/IAEAC54830.2022.9930050","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142214031&doi=10.1109%2fIAEAC54830.2022.9930050&partnerID=40&md5=c4b8bfa76af2a5ade66a61538778fe86","Change detection based on deep learning is an important research direction in intelligent interpretation of remote sensing images. It has developed rapidly in recent years, but it is also a long-term challenge in remote sensing applications. This is mainly because the production of labeled data for training requires a lot of labor costs, and the currently available change detection labeled data is relatively small. While the complexity of high-resolution remote sensing imagery greatly increases the difficulty for deep learning models to learn robust and discriminative representations from scenes and objects, in this case, training deep learning models with a small amount of labeled data is still a huge challenge. To address this issue, this paper proposes a semi-supervised learning change detection method based on Generative Adversarial Networks (GAN). Compared with previous techniques, this paper combines a typical GAN framework with a Siamese network and applies it to change detection in remote sensing images. We introduce residual networks and atrous convolutions into Siamese networks, and employ a flow alignment module (FAM) to learn semantic flow between adjacent hierarchical feature maps. The connected discriminator formulates the training of the generator as a min-max optimization problem. Comprehensive quantitative and qualitative evaluations of multiple models show that our proposed method outperforms state-of-the-art change detection algorithms.  © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Learning systems; Remote sensing; Semantics; Supervised learning; Wages; Change detection; Detection methods; Intelligent interpretation; Labeled data; Learn+; Learning models; Remote sensing images; Remote-sensing; Semi-supervised; Supervised change detection; Change detection","change detection; generative adversarial network; remote sensing; semi-supervised","Conference paper","Final","","Scopus","2-s2.0-85142214031"
"Zheng Z.; Yu S.; Zhu M.; Jiang S.; He Y.; Peng Q.; Liu Q.; Jiang L.; Li P.","Zheng, Zezhong (18439055300); Yu, Shuang (58086889700); Zhu, Mingcang (57192698093); Jiang, Shaobin (57207874393); He, Yong (57116735300); Peng, Qingjun (58086374800); Liu, Qiang (57001314100); Jiang, Ling (55197801300); Li, Pengshan (57056660200)","18439055300; 58086889700; 57192698093; 57207874393; 57116735300; 58086374800; 57001314100; 55197801300; 57056660200","A Domain Adaptation Method for Land Use Classification Based on Improved HR-Net","2023","IEEE Transactions on Geoscience and Remote Sensing","","","","1","1","0","10.1109/TGRS.2023.3235050","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147208459&doi=10.1109%2fTGRS.2023.3235050&partnerID=40&md5=92fcf21c65cbdfffe8e393cdb72e6f54","In recent years, the recognition accuracy of semantic segmentation model on natural images can yield a very high level. Thus, it is of great significance to utilize semantic segmentation algorithm to obtain land use classification with remote sensing images. However, due to the large differences between natural images and remote sensing images, the standard semantic segmentation algorithm is not effective for land use classification of remote sensing images. In this paper, the structure of high-resolution network (HR-Net) algorithm is improved according to the difference between the two kinds of images to make it more suitable for remote sensing images. Furthermore, in order to overcome the dependence of the semantic segmentation algorithm on a large number of high-quality prior data sets, some research experiments are conducted with the improved HR-Net domain adaptation model, and both of the adversarial domain adaptation model and the fusion domain adaptation model based on improved HR-Net and CycleGAN are designed to reduce the workload of manually labeling data. The extensive experimental results show that the classification of our improved HR-Net algorithm and the two domain adaptation models outperform other algorithms, that demonstrates the effectiveness and superiority of our algorithms. IEEE","","Adaptation models; Classification algorithms; Domain adaptation; generative adversarial network; improved HR-Net; land use classification; Remote sensing; Semantic segmentation; Spatial resolution; Standards; Training","Article","Article in press","","Scopus","2-s2.0-85147208459"
"Zhu F.; Wang C.; Zhu B.; Sun C.; Qi C.","Zhu, Fuzhen (12780819500); Wang, Chen (57980554000); Zhu, Bing (57199801240); Sun, Ce (58059130100); Qi, Chengxiao (57282587700)","12780819500; 57980554000; 57199801240; 58059130100; 57282587700","An improved generative adversarial networks for remote sensing image super-resolution reconstruction via multi-scale residual block","2023","Egyptian Journal of Remote Sensing and Space Science","26","1","","151","160","9","10.1016/j.ejrs.2022.12.008","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146055217&doi=10.1016%2fj.ejrs.2022.12.008&partnerID=40&md5=0718459def821c5b8689d5680f7a98b8","Existing image super-resolution algorithms still suffer from the problems of not extracting rich image features and losing realistic high-frequency details. In order to solve these problems, this paper proposes an improved generative adversarial network algorithm for super-resolution reconstruction of remote sensing images by multi-scale residual blocks. The original generative adversarial network (GAN) structure is improved and multi-scale residual blocks are introduced in the generator to fuse features at different scales. After extracting the parallel information of multi-scale features, information is exchanged between multi-resolution information streams to obtain contextual information through spatial and channel attention mechanisms, and multi-scale features are fused according to the attention mechanism. In the discriminator, the concept of relative average GAN (RaGAN) is introduced, and the loss function of the network is redesigned so that the discriminator can predict relative probabilities instead of absolute probabilities thus enabling clear learning of edge and texture details. Experimental results show that the proposed method in this paper significantly outperforms state-of-the-art (SOTA) methods in terms of both subjective and objective metrics.In three test datasets, compared with SOTA methods, the Peak Signal to Noise Ratio(PSNR) is improved by a maximum of 1.18 dB, 0.84 dB and 1.29 dB respectively, and the Structural Similarity Index (SSIM) is improved by 0.0264, 0.0077 and 0.0109 respectively in scale of 2, 3 and 4 times images super-resolution.The model proposed in this paper effectively improved the super-resolution re-construction results of remote sensing images. © 2022 National Authority of Remote Sensing & Space Science","Image enhancement; Image reconstruction; Optical resolving power; Remote sensing; Signal to noise ratio; Textures; Attention mechanisms; Image super resolutions; Image super-resolution reconstruction; Multi-scale features; Multi-scale residual block; Multi-scales; Relative average GAN; Remote sensing images; State-of-the-art methods; Super-resolution reconstruction; algorithm; artificial neural network; image processing; image resolution; reconstruction; remote sensing; signal-to-noise ratio; Generative adversarial networks","GAN; Multi-scale residual block; RaGAN; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85146055217"
"Jamali A.; Mahdianpari M.; Mohammadimanesh F.; Brisco B.; Salehi B.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939); Mohammadimanesh, Fariba (56541784200); Brisco, Brian (7003505161); Salehi, Bahram (36610817400)","56909712300; 57190371939; 56541784200; 7003505161; 36610817400","3-D Hybrid CNN Combined With 3-D Generative Adversarial Network for Wetland Classification With Limited Training Data","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8095","8108","13","10.1109/JSTARS.2022.3206143","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139549121&doi=10.1109%2fJSTARS.2022.3206143&partnerID=40&md5=1f2a1469abdfa2bbf5bbf11030aed776","Recently, deep learning algorithms, specifically convolutional neural networks (CNNs), have played an important role in remote sensing image classification, including wetland mapping. However, one limitation of deep CNN for classification is its requirement for a great number of training samples. This limitation is particularly enhanced when the classes of interest are spectrally similar, such as that of wetland types, and the training samples are limited. This article presents a novel approach named 3-D hybrid generative adversarial network (3-D hybrid GAN) that addresses the limited training sample issue in the classification of remote sensing imagery with a focus on complex wetland classification. We used a conditional map unit that generates synthetic training samples for only classes with a lower number of training samples to improve the per-class accuracy of wetlands. This procedure overcomes the issue of imbalanced data in conventional wetland mapping. Based on the achieved results, better classification accuracy is obtained by integrating a 3-D generative adversarial network (3-D GAN) and the CNN network of a 3-D hybrid CNN using both 3-D and 2-D convolutional filters. Experimental results on the avalon pilot site located in eastern Newfoundland, Canada, and covering five wetland types of bog, fen, marsh, swamp, and shallow water demonstrate that our model significantly outperforms other CNN models, including the HybridSN, SpectralNet, MLP-mixer, as well as a conventional algorithm of random forest for complex wetland classification by approximately 1% to 51% in terms of F-1 score.  © 2008-2012 IEEE.","Classification (of information); Complex networks; Convolution; Decision trees; Deep neural networks; Generative adversarial networks; Image classification; Remote sensing; Sampling; Wetlands; Convolutional neural network; Deep convolutional neural network; Generative adversarial network; Random forest; Random forests; Training sample; Wetland classification; Wetland mapping; artificial neural network; computer simulation; digital mapping; image classification; numerical model; remote sensing; three-dimensional modeling; wetland management; Mapping","Convolutional neural network (CNN); deep convolutional neural network (DCNN); generative adversarial network (GAN); random forest (RF); wetland mapping","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139549121"
"Margarita F.; Nishchhal N.","Margarita, Favorskaya (57953387900); Nishchhal, Nishchhal (57953619000)","57953387900; 57953619000","VERIFICATION OF MARINE OIL SPILLS USING AERIAL IMAGES BASED ON DEEP LEARNING METHODS; [ВЕРИФИКАЦИЯ РАЗЛИВОВ НЕФТИ НА ВОДНЫХ ПОВЕРХНОСТЯХ ПО АЭРОФОТОСНИМКАМ НА ОСНОВЕ МЕТОДОВ ГЛУБОКОГО ОБУЧЕНИЯ]","2022","Informatics and Automation","21","5","","937","962","25","10.15622/ia.21.5.4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141341293&doi=10.15622%2fia.21.5.4&partnerID=40&md5=ec52e0afd32bba2e8e636607c698c538","The article solves the problem of verifying oil spills on the water surfaces of rivers, seas and oceans using optical aerial photographs, which are obtained from cameras of unmanned aerial vehicles, based on deep learning methods. The specificity of this problem is the presence of areas visually similar to oil spills on water surfaces caused by blooms of specific algae, substances that do not cause environmental damage (for example, palm oil), or glare when shooting (so-called look-alikes). Many studies in this area are based on the analysis of synthetic aperture radars (SAR) images, which do not provide accurate classification and segmentation. Follow-up verification contributes to reducing environmental and property damage, and oil spill size monitoring is used to make further response decisions. A new approach to the verification of optical images as a binary classification problem based on the Siamese network is proposed, when a fragment of the original image is repeatedly compared with representative examples from the class of marine oil slicks. The Siamese network is based on the lightweight VGG16 network. When the threshold value of the output function is exceeded, a decision is made about the presence of an oil spill. To train the networks, we collected and labeled our own dataset from open Internet resources. A significant problem is an imbalance of classes in the dataset, which required the use of augmentation methods based not only on geometric and color manipulations, but also on the application of a Generative Adversarial Network (GAN). Experiments have shown that the classification accuracy of oil spills and look-alikes on the test set reaches values of 0.91 and 0.834, respectively. Further, an additional problem of accurate semantic segmentation of an oil spill is solved using convolutional neural networks (CNN) of the encoder-decoder type. Three deep network architectures U-Net, SegNet, and Poly-YOLOv3 have been explored for segmentation. The Poly-YOLOv3 network demonstrated the best results, reaching an accuracy of 0.97 and an average image processing time of 385 s with the Google Colab web service. A database was also designed to store both original and verified images with problem areas. © 2022 China Surfactant Detergent and Cosmetics.","","aerial photographs; deep learning; Earth remote sensing; oil spill detection; segmentation; verification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141341293"
"Cai F.; Wu K.-Y.; Wang F.","Cai, Feng (57888175100); Wu, Ke-Yu (57324654500); Wang, Feng (56459216100)","57888175100; 57324654500; 56459216100","Remote Sensing Image Super-Resolution Via Attentional Feature Aggregation Generative Adversarial Network","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2598","2601","3","10.1109/IGARSS46834.2022.9884863","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141821238&doi=10.1109%2fIGARSS46834.2022.9884863&partnerID=40&md5=39a0f2d24636b6be3ed4ff7137c98de3","The extraction of high-frequency details is generally neglected in single image super-resolution (SISR) for remote sensing images. In this paper, we propose an attentional feature aggregation generative adversarial network (AFA-GAN) with the capability of strong feature extraction and attentional feature fusion to generate high-resolution remote sensing images. We adopt the residual feature aggregation framework for the feature extraction to make full use of the hierarchical features on the residual branches. To better fuse global and local features with inconsistent scales, an attentional feature fusion mechanism is utilized in residual feature aggregation modules. The comprehensive experiments with state-of-the-art SISR methods on the UC Merced dataset demonstrate the effectiveness and superiority of our AFA-GAN. © 2022 IEEE.","Computer vision; Extraction; Feature extraction; Optical resolving power; Remote sensing; Attentional feature aggregation; Feature aggregation; Features extraction; Features fusions; Generative adversarial network; High frequency HF; Image super resolutions; Remote sensing images; Single image super-resolution; Single images; Generative adversarial networks","attentional feature aggregation (AFA); generative adversarial network (GAN); Remote sensing images; single image super-resolution (SISR)","Conference paper","Final","","Scopus","2-s2.0-85141821238"
"Manocha A.; Afaq Y.","Manocha, Ankush (57207915903); Afaq, Yasir (57223431081)","57207915903; 57223431081","Optical and SAR images-based image translation for change detection using generative adversarial network (GAN)","2023","Multimedia Tools and Applications","","","","","","","10.1007/s11042-023-14331-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146930772&doi=10.1007%2fs11042-023-14331-2&partnerID=40&md5=d3f89b79c17033e07093df56dcccf19f","Monitoring a specific area to analyze a continuous change has become more accessible by using optical images in remote sensing technology. However, several natural and artificial aspects such as fog and air pollution make it difficult to extract correct geometric information. To overcome the limitation of optical images, Synthetic Aperture Radar (SAR) images can be used to access more accurate information with respect to the targeted area. In this manner, optical and SAR images can be utilized together to detect the scale of change even in bad weather conditions. To process optical and SAR images, an image translation process-oriented Deep Adaptation-based Change Detection Technique (DACDT) is proposed. An optimized U-Net++ model is proposed that helps to improve the global and regional impacts of the images. Moreover, a multi-scale loss function is utilized to access the features of different dimensions. In this manner, the final change maps are generated by transferring the features of optical images to the SAR images for better change analysis. The prediction performance of the proposed approach is evaluated on four different datasets such as Gloucester I, Shuguang Village, Gloucester-II, and California. The calculated outcomes define the prediction performance of the proposed solution by registering the accuracy of 98.67%, 99.77%, 97.68%, and 98.87%, respectively. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Change detection; Deep learning; Generative adversarial networks; Geometrical optics; Image analysis; Image enhancement; Optical remote sensing; Radar imaging; Change detection; Deep adaptation; Deep learning; Gloucester; Image translation; Optical image; Optical-; Prediction performance; Synthetic aperture radar and optical; Synthetic aperture radar images; Synthetic aperture radar","Change detection; Deep adaptation; Deep learning; GAN; SAR and optical","Article","Article in press","","Scopus","2-s2.0-85146930772"
"Liu Z.; Zhu H.; Chen Z.","Liu, Ziyu (58087753100); Zhu, Han (57221180146); Chen, Zhenzhong (57985265500)","58087753100; 57221180146; 57985265500","Adversarial Spectral Super-Resolution for Multispectral Imagery Using Spatial Spectral Feature Attention Module","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","","","1","14","13","10.1109/JSTARS.2023.3238853","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147300696&doi=10.1109%2fJSTARS.2023.3238853&partnerID=40&md5=9c1fcd62e9714f69543d9e53282a0dbf","Acquiring high-quality hyperspectral imagery with high spatial and spectral resolution plays an important role in remote sensing. Due to the limited capacity of sensors, providing high spatial and spectral resolution is still a challenging issue. Spectral super-resolution (SSR) increases the spectral dimensionality of multispectral images to achieve resolution enhancement. In this paper, we propose a spectral resolution enhancement method based on the generative adversarial network (GAN) framework without introducing additional spectral responses prior. In order to adaptively rescale informative features for capturing interdependencies in the spectral and spatial dimensions, a spatial spectral feature attention module (SSFAM) is introduced. The proposed method jointly exploits spatio-spectral distribution in the hyperspectral manifold to increase spectral resolution while maintaining spatial content consistency. Experiments are conducted on both synthetic Landsat 8 and Sentinel-2 radiance data and real co-registered ALI and Hyperion (MS and HS) images, which indicates the superiority of the proposed method compared to other state-of-the-art methods. Author","Generative adversarial networks; Hyperspectral imaging; Image enhancement; Remote sensing; Spectral resolution; Adversarial learning; Attention mechanisms; Correlation; Hyper-spectral imageries; Images reconstruction; Spatial resolution; Spectral feature; Spectral super-resolution; Superresolution; Image reconstruction","adversarial learning; attention Mechanism; Cameras; Correlation; Hyperspectral imagery; Hyperspectral imaging; Image reconstruction; Sensors; Spatial resolution; spectral super-resolution; Superresolution","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85147300696"
"Shi C.; Zhang T.; Liao D.; Jin Z.; Wang L.","Shi, Cuiping (9734198100); Zhang, Tianyu (57307236400); Liao, Diling (57254701200); Jin, Zhan (57950879000); Wang, Liguo (55745497100)","9734198100; 57307236400; 57254701200; 57950879000; 55745497100","Dual hybrid convolutional generative adversarial network for hyperspectral image classification","2022","International Journal of Remote Sensing","43","14","","5452","5479","27","10.1080/01431161.2022.2135412","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141033363&doi=10.1080%2f01431161.2022.2135412&partnerID=40&md5=58a82c59a6ccabcf084071c561ec1a14","Generative adversarial networks (GANs) have effectively promoted the development of hyperspectral image classification technology in generating samples. Many GAN-based models for hyperspectral image classification use deconvolution to generate fake samples, which will cause chequerboard artefacts and affect classification performance. Furthermore, the training of GANs still faces the problem of mode collapse. Aiming at the above problems, we proposed a dual hybrid convolutional generative adversarial network (DHCGAN) for hyperspectral image classification. Firstly, the combination of nearest neighbour upsampling and sub-pixel convolution is employed in the generator, which avoids the overlap of convolution domain and effectively suppresses the chequerboard artefacts caused by deconvolution. Secondly, the traditional convolution and dilated convolution are fused in the discriminator, which expands the receptive field without increasing parameters and achieves more effective feature extraction. In addition, some adaptive drop blocks are embedded into the generator and discriminator to effectively alleviate the problem of mode collapse. Experiments were performed on four hyperspectral datasets (including three classical datasets–Indian Pines, University of Pavia and Houston, a new dataset–WHU-Hi-HanChuan). Experimental results show that the proposed method can provide a certain performance improvement over some competing methods, such as the accuracy has been increased by more than 1% on the three classical datasets, and even got over 3% improvement on WHU_Hi_HanChaun dataset. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Italy; Lombardy; Pavia; Fake detection; Generative adversarial networks; Image classification; Pixels; Spectroscopy; Classification performance; Classification technology; Deconvolutions; Generating samples; Generative adversarial network; Hybrid convolution; Hyperspectral image classification; Network-based modeling; Sub-pixel convolution; Sub-pixels; deconvolution; image classification; image resolution; pixel; remote sensing; satellite data; satellite imagery; Convolution","generative adversarial network (GAN); hybrid convolution; hyperspectral image (HSI) classification; sub-pixel convolution","Article","Final","","Scopus","2-s2.0-85141033363"
"Zhao Z.; Ren C.; Teng Q.; He X.","Zhao, Zhibo (57419279700); Ren, Chao (57207076112); Teng, Qizhi (7005503530); He, Xiaohai (9237988800)","57419279700; 57207076112; 7005503530; 9237988800","A practical super-resolution method for multi-degradation remote sensing images with deep convolutional neural networks","2022","Journal of Real-Time Image Processing","19","6","","1139","1154","15","10.1007/s11554-022-01245-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138184640&doi=10.1007%2fs11554-022-01245-9&partnerID=40&md5=1fd64565de0db2ce864c451a6ebcfb74","Recent studies have proved that convolutional neural networks (CNNs) have great potential for image super-resolution (SR) tasks. However, most existing methods rely on paired high-resolution (HR) and low-resolution (LR) images to train the CNN, where the LR images are routinely synthesized by applying predefined degradation operations (e.g., bicubic). Because the degradation process of LR images is usually unknown and more complex than those predefined, these methods suffer a significant performance decrease when applied to real-world SR problems. In addition, a deeper and wider network structure enables superior performance while increasing the network parameters and inference time, making it difficult to process real-time data. Inspired by the above motivations, we present an efficient two-step SR method for multi-degradation remote sensing images. Specifically, we first present a novel kernel estimation framework based on generative adversarial networks that can accurately extract the latent blur kernel from the input LR image without any image priors. We then train an efficient SR deep neural network with paired HR and corresponding LR images degraded with the generated kernels. To better balance network parameters and network performance, the densely connected attention mechanism and multi-scale feature extract blocks are introduced in the SR network by increasing the flow of feature information within the network. Extensive experiments indicate that the proposed method outperforms current methods with desired network parameters and complexity, making it feasible to enable real-time image processing. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Complex networks; Convolution; Convolutional neural networks; Deep neural networks; Image processing; Optical resolving power; Remote sensing; Convolutional neural network; High resolution; Kernel models; Low resolution images; Multi-degradation; Network parameters; Performance; Remote sensing images; Superresolution; Superresolution methods; Generative adversarial networks","Convolutional neural network; Kernel modeling; Multi-degradation; Remote sensing image; Super-resolution","Article","Final","","Scopus","2-s2.0-85138184640"
"Wu S.; Xu M.; Wu M.; Zhang C.; Shen H.","Wu, Sibo (58087649500); Xu, Mengqiu (57219573819); Wu, Ming (57209361563); Zhang, Chuang (55925736800); Shen, Hua (58087606600)","58087649500; 57219573819; 57209361563; 55925736800; 58087606600","Identify, Guess and Reconstruct: Three Principles for Cloud Removal Task","2022","2022 IEEE International Conference on Visual Communications and Image Processing, VCIP 2022","","","","","","","10.1109/VCIP56404.2022.10008852","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147250385&doi=10.1109%2fVCIP56404.2022.10008852&partnerID=40&md5=1e0b6acaee624b02edce82cd72684f2a","Remote sensing images serve a significant role in earth observation to tackle climate change and post-disaster reconstruction concerns. However, optical images are obscured by clouds or haze, preventing precise earth observation; hence, cloud removal has been a hot topic among concerned scholars. The objective of this article is to make cloud removal more efficient and explicable by proposing three principles: identifying clouds, guessing objects beneath the clouds, and reconstructing the cloudy area. In addition, a modified dual contrastive learning Generative Adversarial Network is proposed based on these three principles by adding cloud detection and weight sharing strategy to obtain cloud semantics. In particular, we align two datasets by forming a quaternary sample pair that includes not only optical pictures and SAR images, but also region information for a more precise reconstruction. Our experiment results on the integrated dataset reveal the superiority of proposed method over previous cloud removal methods and the effectiveness of added modules through ablation experiments, with PSNR and SSIM values of 26.2 and 0.728, respectively. © 2022 IEEE.","Climate change; Deep learning; Earth (planet); Geometrical optics; Image reconstruction; Observatories; Optical remote sensing; Semantics; Synthetic aperture radar; Cloud detection; Cloud removal; Deep learning; Dual contrastive learning GAN; Earth observations; Hot topics; Optical image; Post-disaster reconstruction; Remote sensing images; Remote-sensing; Generative adversarial networks","Cloud Removal; Deep Learning; Dual Contrastive Learning GAN; Remote Sensing","Conference paper","Final","","Scopus","2-s2.0-85147250385"
"Shi C.; Dang Y.; Fang L.; Zhao M.; Lv Z.; Miao Q.; Pun C.-M.","Shi, Cheng (57195378613); Dang, Yenan (57324651800); Fang, Li (57917902000); Zhao, Minghua (57726381200); Lv, Zhiyong (23111268400); Miao, Qiguang (9133503300); Pun, Chi-Man (7003931852)","57195378613; 57324651800; 57917902000; 57726381200; 23111268400; 9133503300; 7003931852","Multifeature Collaborative Adversarial Attack in Multimodal Remote Sensing Image Classification","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5631815","","","","10.1109/TGRS.2022.3208337","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139432193&doi=10.1109%2fTGRS.2022.3208337&partnerID=40&md5=5c599d3e4247711874cb68451c99b472","Deep neural networks have strong feature learning ability, but their vulnerability cannot be ignored. Current research shows that deep learning models are threatened by adversarial examples in remote sensing (RS) classification tasks, and their robustness drops sharply in the face of adversarial attacks. Therefore, many adversarial attack methods have been studied to predict the risks faced by a network. However, the existing adversarial attack methods mainly focus on single-modal image classification networks, and the rapid growth of RS data makes multimodal RS image classification a research hotspot. Generating multimodal adversarial examples needs to consider a high attack success rate, subtle perturbation, and collaborative attack ability between different modalities. In this article, we investigate the vulnerability of multimodal RS classification networks and propose a multifeature collaborative adversarial network (MFCANet) for generating multimodal adversarial examples. Two modality-specific generators are designed to generate the multimodal collaborative perturbations with strong attack ability, and two modality-specific discriminators make the generated multimodal adversarial examples closer to the real instances. In addition, a modality-specific generative loss and a modality-specific discriminative loss are proposed, and an alternating optimization strategy is designed for training the proposed MFCANet. Extensive experiments are carried out on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen 2D dataset and ISPRS Potsdam 2D dataset. The results show that the attack performance of the proposed method is stronger than that of the fast gradient sign method (FGSM), project gradient descent (PGD), and Carlini and Wagner (C&W) attack methods.  © 1980-2012 IEEE.","Deep neural networks; Generative adversarial networks; Network security; Perturbation techniques; Remote sensing; Collaboration; Deep learning; Generator; Multi-modal; Multimodal adversarial attack; Multimodal remote sensing image classification; Perturbation method; Remote sensing image classification; Task analysis; artificial neural network; data set; image classification; numerical model; remote sensing; satellite data; Image classification","Generative adversarial networks (GANs); multimodal adversarial attack; multimodal remote sensing (RS) image classification","Article","Final","","Scopus","2-s2.0-85139432193"
"Cheng S.; Yao P.; Deng K.; Fu L.","Cheng, Siyuan (57261961400); Yao, Ping (57206347584); Deng, Kai (57217296176); Fu, Li (57215326325)","57261961400; 57206347584; 57217296176; 57215326325","DETGAN: GAN for Arbitrary-oriented Object Detection in Remote Sensing Images","2022","Proceedings - 2022 Asia Conference on Algorithms, Computing and Machine Learning, CACML 2022","","","","337","341","4","10.1109/CACML55074.2022.00063","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138003479&doi=10.1109%2fCACML55074.2022.00063&partnerID=40&md5=1a4f60482299477d796c6bb8b0c54ce6","Object detection in remote sensing images has be-come a research focus in recent years with the development of deep learning. However, due to objective reasons such as weather, cost, etc., we can hardly obtain abundant high-quality remote sensing images, especially for specific targets, which severely limits the training of the object detector, leading to poor detection performance. Thus for the first time, this paper introduces the Generative Adversarial Networks(GANs) for arbitrary-oriented object detection in remote sensing images, by augmenting the dataset to improve the performance of detectors. We construct DETGAN with two-layer self-attention modules to capture long-distance dependence for high-quality image generation. To solve the mismatch between generated slices and the samples for detectors, we propose the GAN-to-Detection transfer strategy, in which the slices are inserted into a background with the same size as the samples for detectors and then added to the training set. Experiments show that the performance of ship detectors is successfully improved with the transfer strategy, and demonstrate that GAN is an effective way to alleviate the problem of data insufficiency in remote sensing image object detection.  © 2022 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Object detection; Object recognition; Arbitrary-oriented object detection; Detection performance; High quality; Object detectors; Objects detection; Remote sensing images; Remote-sensing; Research focus; Transfer strategies; Remote sensing","arbitrary-oriented object detection; Generative Adversarial Network; remote sensing; transfer strategy","Conference paper","Final","","Scopus","2-s2.0-85138003479"
"Li T.; Zuo R.; Zhao X.; Zhao K.","Li, Tong (57221125107); Zuo, Renguang (15833309600); Zhao, Xinfu (57212618484); Zhao, Kuidong (7202071855)","57221125107; 15833309600; 57212618484; 7202071855","Mapping prospectivity for regolith-hosted REE deposits via convolutional neural network with generative adversarial network augmented data","2022","Ore Geology Reviews","142","","104693","","","","10.1016/j.oregeorev.2022.104693","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122618833&doi=10.1016%2fj.oregeorev.2022.104693&partnerID=40&md5=876e1d4321b2f6a982530cc83429f303","The regolith-hosted rare earth elements (REE) deposits are the dominant source of the global heavy REE resources. This study proposed a convolutional neural network (CNN) architecture to integrate the multi-source data (e.g., geological, geochemical and geomorphological data) to map mineral prospectivity of regolith-hosted REE deposits. To solve the lack of labelled data for the training of the supervised CNN, a generative adversarial network (GAN) was applied for data augmentation. The proposed GAN is trained in an unsupervised way, which regards the random downscaled real samples as the inputs. The GAN-based augmented data is validated by comparison with real data and the peak signal-to-noise ratio value, respectively. A case study of mapping prospectivity for regolith-hosted rare earth elements deposits in Southern Jiangxi Province of China further illustrates and validates the procedure. The final mineral prospectivity map is obtained by the CNN with all geological, geochemical and geomorphological data augmented by GAN. The CNN reaches 99.7% training accuracy and 98.9% validation accuracy. All the known mineral deposits are located in the prospective areas delineated by the CNN, which only occupy 2.36% of the study area. The obtained results indicate that the proposed framework, integrating CNN and the GAN-based augmentation method is an effective way for the application of the supervised deep learning algorithms in mineral prospectivity mapping of the regolith-hosted REE deposits and the prospective areas could be used for guiding further exploration in the study area. © 2022 The Author(s)","China; Jiangxi; Convolution; Convolutional neural networks; Deep learning; Deposits; Exploratory geochemistry; Geology; Learning algorithms; Mapping; Mineral exploration; Mineral resources; Rare earth elements; Rare earths; Remote sensing; Signal to noise ratio; Convolutional neural network; Data augmentation; Deep learning; Geochemicals; Mineral prospectivity mappings; Network-based; Prospectives; Prospectivity; Rare earth elements deposits; Regolith-hosted rare earth element deposit; artificial neural network; mapping method; ore deposit; rare earth element; regolith; signal-to-noise ratio; Generative adversarial networks","Convolutional neural network; Data augmentation; Deep learning; Generative adversarial network; Mineral prospectivity mapping; Regolith-hosted REE deposits","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122618833"
"Zhang J.; Bao W.","Zhang, Jian (56637434200); Bao, Wenxing (35770357700)","56637434200; 35770357700","Research on classification method of hyperspectral remote sensing image based on Generative Adversarial Network; [生成式对抗网络的高光谱遥感图像分类方法研究]","2022","National Remote Sensing Bulletin","26","2","","416","430","14","10.11834/jrs.20219192","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135788024&doi=10.11834%2fjrs.20219192&partnerID=40&md5=b6d81ea5d2cd2dd4dbfef6db7b94bb1e","Deep learning has strong learning ability and has become a widely studied method in the hyperspectral image classification community. However, the deep learning-based classification model requires a large number of training samples to train a good model. Overfitting will occur when the training sample is small. The accuracy of the model on the test set is lower than the accuracy on the training set. Researchers have proposed overfitting suppression methods such as weight decay and dropout to suppress overfitting. However, these methods need to work in a specific environment and have limited suppression effect on overfitting. Thus, this study proposes an overfitting suppression algorithm based on generative adversarial networks to suppress the overfitting phenomenon of the model.First, a spatial neighborhood block for the standard dataset is constructed, and the dataset is divided into labeled, unlabeled, and test samples. Then, the labeled and unlabeled samples are sent to the generative adversarial networks for training. During input, the pixels in the neighborhood block are independently fed into the fully connected network discriminator to extract the spectral features of each pixel. Finally, the spectral features of each pixel are fused by the average pooling, and they connected to the output layer to obtain the classification result. The overfitting is caused by the large value and variance of the network parameters. Thus, the large parameter values enable the model to fit more samples. Therefore, the network is first fitted to the data by labeled samples in each iteration, and then, the optimizer is used to minimize the mean of the high-dimensional features. This process will re-update the network parameters, reduce the value and variance of the parameters, and thus suppress the overfitting.The algorithm was applied to two standard datasets, namely, Indian Pines and Pavia University datasets. The 1% labeled samples were randomly selected for training. The overall classification accuracy rates were 89.61% and 98.79%, which were better than those of several algorithms. Compared with several commonly used overfitting suppression methods such as batch normalization, L2 regularization, and dropout, the proposed overfitting suppression algorithm obtains 5.60% and 3.20% higher results on randomly selected 1% labeled samples from the Indian Pines dataset and randomly selected 0.1% labeled samples from Pavia University dataset.The model of generative adversarial networks designed for the characteristics of hyperspectral data can fully utilize the spectral and spatial features of hyperspectral images. The proposed overfitting suppression algorithm can significantly improve the classification performance of the model. However, the overfitting suppression effect of the algorithm is not obvious when the number of labeled samples is large. Thus, further research is needed. © 2022, Science Press. All right reserved.","Deep learning; Feature extraction; Generative adversarial networks; Image classification; Iterative methods; Pixels; Sampling; Spectroscopy; Statistical tests; Features extraction; Hyperspectral image classification; Overfitting; Remote-sensing; Small training; Small training sample; Spatial features; Spectral-spatial feature; Suppression algorithm; Training sample; Remote sensing","Feature extraction; Generative adversarial network; Hyperspectral image classification; Overfitting; Remote sensing; Small training samples; Spectral-spatial feature","Article","Final","","Scopus","2-s2.0-85135788024"
"Shi N.; Wang P.; Li F.","Shi, Nan (57224351612); Wang, Ping (55575877700); Li, Fan (55737186600)","57224351612; 55575877700; 55737186600","Domain-specific knowledge-driven pan-sharpening algorithm","2023","Neurocomputing","520","","","129","140","11","10.1016/j.neucom.2022.11.068","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143500492&doi=10.1016%2fj.neucom.2022.11.068&partnerID=40&md5=7cfa7dc59486decfd15ea8d3d7cabdc6","Pan-sharpening can provide multispectral images with high spatial resolutions, which are useful for many remote sensing image applications. Currently, deep learning technology has been widely used in pan-sharpening. Most of these deep learning-based methods ignore domain-specific knowledge that can improve spatial and spectral performance. Some improved methods adopt an injection structure which injects initial details obtained from panchromatic images into multispectral images through detail mapping. However, the initial details lack frequency-domain information. Moreover, the detail mapping is completed by convolutional neural networks, which lack sufficient nonlinearity to generate rich and diverse details. To solve the above problems, a domain-specific knowledge-driven pan-sharpening framework based on a detail injection structure is proposed, which includes two stages of knowledge-driven initial detail acquisition and data-driven detail mapping. In the first stage, in order to perform better feature reconstruction in the frequency domain, the PAN-MS method is introduced to provide initial details containing frequency-domain information. In the second stage, a newly designed detail-mapping generative adversarial network (GAN) maps initial details to more various output details. Experiments conducted on three public datasets has proven that the proposed algorithm outperforms some state-of-the-art methods in terms of spatial and spectral performance. © 2022 Elsevier B.V.","Convolutional neural networks; Deep learning; Domain Knowledge; Frequency domain analysis; Generative adversarial networks; Image enhancement; Remote sensing; Detail injection; Domain-specific knowledge; Features fusions; Frequency domains; Multispectral images; Pan-sharpening; Spatial feature fusion; Spatial features; Spatial performance; Spectral performance; article; convolutional neural network; deep learning; nonlinear system; remote sensing; Mapping","Detail injection; GAN; Pan-sharpening; Spatial feature fusion","Article","Final","","Scopus","2-s2.0-85143500492"
"Wang Z.; Ma Y.; Zhang Y.","Wang, Zhaobin (23669866600); Ma, Yikun (57556806300); Zhang, Yaonan (36633454400)","23669866600; 57556806300; 36633454400","Review of pixel-level remote sensing image fusion based on deep learning","2023","Information Fusion","90","","","36","58","22","10.1016/j.inffus.2022.09.008","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138457315&doi=10.1016%2fj.inffus.2022.09.008&partnerID=40&md5=d67c4dd95a0516903495153d998da707","The booming development of remote sensing images in many visual tasks has led to an increasing demand for obtaining images with more precise details. However, it is impractical to directly supply images that are simultaneously rich in spatial, spectral, and temporal information. One feasible solution is to fuse the information from multiple images. Since deep learning has achieved impressive achievements in image processing recently, this paper aims to provide a comprehensive review of deep learning-based methods for fusing remote sensing images at pixel-level. Specifically, we first introduce some traditional methods with their main limitations. Meanwhile, a brief presentation is made on four basic deep learning models commonly used in the field. On this basis, the research progress of these models in spatial information fusion and spatio-temporal fusion are reviewed. The current status on these models is further discussed with some coarse quantitative comparisons using several image quality metrics. After that, we find that deep learning models have not achieved overwhelming superiority over traditional methods but show great potential, especially the generative adversarial networks with its great capabilities in image generation and unsupervised learning should become a hot topic for future research. The joint use of different models should also be considered to fully extract multi-modal information. In addition, there is a lack of valuable research on pixel-level fusion of radar and optical images, requiring more attention in future work. © 2022 Elsevier B.V.","Deep learning; Geometrical optics; Information fusion; Learning systems; Optical data processing; Optical remote sensing; Pixels; Vision; Deep learning; Learning models; Pixel level; Remote sensing images; Remote-sensing; Spatial informations; Spatial temporals; Spectral information; Temporal information; Visual tasks; Image fusion","Deep learning; Image fusion; Remote sensing","Short survey","Final","","Scopus","2-s2.0-85138457315"
"Peng L.; Na Z.; Guoliang Z.; Zhenhua W.; Zongsheng Z.","Peng, Lu (57782209900); Na, Zhang (57782479900); Guoliang, Zou (57781682300); Zhenhua, Wang (7801685454); Zongsheng, Zheng (57781414600)","57782209900; 57782479900; 57781682300; 7801685454; 57781414600","CycleGAN Coastline Automatic Extraction Method Based on Dual Attention Mechanism; [基于双重注意力机制的 CycleGAN 海岸线自动提取方法]","2022","Laser and Optoelectronics Progress","59","12","1210005","","","","10.3788/LOP202259.1210005","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133476238&doi=10.3788%2fLOP202259.1210005&partnerID=40&md5=961514b6dfd1c107fb40c4dcd67aecfe","The pixel-level sea-land segmentation of remote sensing images is a basic work for coastline extraction. Owing to the dynamic changes in the coastline, obtaining accurate coastline marker datasets is difficult. In this study, Google Aerial Photo-Maps-paired samples were used to construct a paired dataset after the sea-land binarization processing of Google Maps. Thus, we proposed the dual attention mechanism-cycle generative adversarial network (CycleGAN) based on the CycleGAN model to solve the problem of fewer samples in the new dataset. The new model fully considers the structural similarity between remote sensing images and sea-land binarized images, improves cycle consistency loss, and designs both channel and spatial attention modules to highlight salient features and regions to enhance the model’s performance in small feature learning ability under sample training. Furthermore, we applied three evaluation indicators, i. e., mean square error, mean pixel accuracy, and mean intersection over union (MIoU), and compared our experimental results to those of the full convolutional neural network and DeepLab models under multiple-scale dataset training. Results show that the improved model conversion of the sea-land binarized images is more consistent with the true value images and the MIoU values are increased by at least 7% and 6%, respectively, verifying the effectiveness and feasibility of the proposed method. © 2022 Universitat zu Koln. All rights reserved.","","attention mechanism; cycle consistency loss; cycle generative adversarial network; image processing; remote sensing; small sample","Article","Final","","Scopus","2-s2.0-85133476238"
"Wu P.; Su Y.; Duan S.-B.; Li X.; Yang H.; Zeng C.; Ma X.; Wu Y.; Shen H.","Wu, Penghai (55644317800); Su, Yang (57812006400); Duan, Si-bo (26429409700); Li, Xinghua (55626987300); Yang, Hui (36835970800); Zeng, Chao (37056513600); Ma, Xiaoshuang (55768476100); Wu, Yanlan (12141267000); Shen, Huanfeng (8359721100)","55644317800; 57812006400; 26429409700; 55626987300; 36835970800; 37056513600; 55768476100; 12141267000; 8359721100","A two-step deep learning framework for mapping gapless all-weather land surface temperature using thermal infrared and passive microwave data","2022","Remote Sensing of Environment","277","","113070","","","","10.1016/j.rse.2022.113070","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134589072&doi=10.1016%2fj.rse.2022.113070&partnerID=40&md5=a697e9a8dbf883609480a924d74fce26","Blending data from thermal infrared (TIR) and passive microwave (PMW) measurements is a promising solution for generating the all-weather land surface temperature (LST). However, owing to swath gaps in PMW data and the resolution inconsistence between TIR and PWM data, spatial details are often incomplete or considerable losses are generated in the all-weather LST using traditional methods. This study was conducted to develop a two-step deep learning framework (TDLF) for mapping gapless all-weather LST over the China's landmass using MODIS and AMSR-E LST data. In the TDLF, a multi-temporal feature connected convolutional neural network bidirectional reconstruction model was developed to obtain the spatially complete AMSR-E LST. A multi-scale multi-temporal feature connected generative adversarial network model was then designed to blend spatially complete AMSR-E LST and cloudy-sky MODIS LST, and generate gapless all-weather LST data. Gapless all-weather LST data were evaluated using six in-situ LST data from the Tibetan Plateau (TP) and the Heihe River Basin (HRB). The root mean squared errors (RMSEs) of the gapless all-weather LST were 1.71–2.0 K with determination coefficients (R2) of 0.94–0.98 under clear conditions, and RMSEs of 3.41–3.87 K and R2 of 0.88–0.94 were obtained under cloudy conditions. Compared to the existing PMW-based all-weather LSTs, the validation accuracy and image quality (such as spatial detail) of the generated gapless all-weather LSTs were superior. The TDLF does not require the use of any additional data and can potentially be implemented with other satellite TIR and PWM sensors to produce long-term, gapless, all-weather MODIS LST records on a global scale. Such a capability is beneficial for generating further gapless all-weather soil moisture and evapotranspiration datasets that can all be applied in global climate change research. © 2022","China; Gansu; Hei River; Qinghai-Xizang Plateau; Atmospheric temperature; Climate change; Generative adversarial networks; Infrared radiation; Land surface temperature; Mapping; Mean square error; Pulse width modulation; Radiometers; Remote sensing; Soil moisture; Surface measurement; Surface properties; All-weather; Deep learning; Gapless; Land surface temperature; Learning frameworks; Multi-temporal; Passive microwave data; Satellite remote sensing; Temperature data; Thermal-infrared; AMSR-E; climate change; evapotranspiration; global climate; image analysis; land surface; machine learning; mapping method; MODIS; reconstruction; satellite altimetry; satellite data; weather; Deep learning","All-weather; Deep learning; Gapless; Land surface temperature; Satellite remote sensing","Article","Final","","Scopus","2-s2.0-85134589072"
"Xu C.; Zheng X.; Lu X.","Xu, Chujie (57710978100); Zheng, Xiangtao (56022876500); Lu, Xiaoqiang (35180125200)","57710978100; 56022876500; 35180125200","Multi-Level Alignment Network for Cross-Domain Ship Detection","2022","Remote Sensing","14","10","2389","","","","10.3390/rs14102389","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130743068&doi=10.3390%2frs14102389&partnerID=40&md5=aa98794a51c0240389718d8c4589ad1a","Ship detection is an important research topic in the field of remote sensing. Compared with optical detection methods, Synthetic Aperture Radar (SAR) ship detection can penetrate clouds to detect hidden ships in all-day and all-weather. Currently, the state-of-the-art methods exploit convolutional neural networks to train ship detectors, which require a considerable labeled dataset. However, it is difficult to label the SAR images because of expensive labor and well-trained experts. To address the above limitations, this paper explores a cross-domain ship detection task, which adapts the detector from labeled optical images to unlabeled SAR images. There is a significant visual difference between SAR images and optical images. To achieve cross-domain detection, the multi-level alignment network, which includes image-level, convolution-level, and instance-level, is proposed to reduce the large domain shift. First, image-level alignment exploits generative adversarial networks to generate SAR images from the optical images. Then, the generated SAR images and the real SAR images are used to train the detector. To further minimize domain distribution shift, the detector integrates convolution-level alignment and instance-level alignment. Convolution-level alignment trains the domain classifier on each activation of the convolutional features, which minimizes the domain distance to learn domain-invariant features. Instance-level alignment reduces domain distribution shift on the features extracted from the region proposals. The entire multi-level alignment network is trained end-to-end and its effectiveness is proved on multiple cross-domain ship detection datasets. © 2022 by the authors. Licensee MDPI, Basel, Switzerland. otv.","Convolution; Convolutional neural networks; Generative adversarial networks; Geometrical optics; Radar imaging; Remote sensing; Ships; Tracking radar; Convolutional neural network; Cross-domain; Domain adaptation; Domain distribution; Multilevels; Optical image; Remote-sensing; Research topics; Ship detection; Synthetic aperture radar images; Synthetic aperture radar","convolutional neural network; domain adaptation; ship detection; synthetic aperture radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130743068"
"Shang C.; Jiang S.; Ling F.; Li X.; Zhou Y.; Du Y.","Shang, Cheng (57209801137); Jiang, Shan (57198704721); Ling, Feng (56278268300); Li, Xiaodong (55878368700); Zhou, Yadong (57207472820); Du, Yun (56420121700)","57209801137; 57198704721; 56278268300; 55878368700; 57207472820; 56420121700","Spectral-Spatial Generative Adversarial Network for Super-Resolution Land Cover Mapping With Multispectral Remotely Sensed Imagery","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","522","537","15","10.1109/JSTARS.2022.3228741","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144774026&doi=10.1109%2fJSTARS.2022.3228741&partnerID=40&md5=3284eca3eb21b450a28c16cc8502de49","Super-resolution mapping (SRM) can effectively predict the spatial distribution of land cover classes within mixed pixels at a higher spatial resolution than the original remotely sensed imagery. The uncertainty of land cover fraction errors within mixed pixels is one of the most important factors affecting SRM accuracy. Studies have shown that SRM methods using deep learning techniques have significantly improved land cover mapping accuracy but have not coped well with spectral-spatial errors. This study proposes an end-to-end SRM model using a spectral-spatial generative adversarial network (SGS) with the direct input of multispectral remotely sensed imagery, which deals with spectral-spatial error. The proposed SGS comprises the following three parts: first, cube-based convolution for spectral unmixing is adopted to generate land cover fraction images. Second, a residual-in-residual dense block fully and jointly considers spectral and spatial information and reduces spectral errors. Third, a relativistic average GAN is designed as a backbone to further improve the super-resolution performance and reduce spectral-spatial errors. SGS was tested in one synthetic and two realistic experiments with multi/hyperspectral remotely sensed imagery as the input, comparing the results with those of hard classification and several classic SRM methods. The results showed that SGS performed well at reducing land cover fraction errors, reconstructing spatial details, removing unpleasant and unrealistic land cover artifacts, and eliminating false recognition.  © 2008-2012 IEEE.","Deep learning; Errors; Generative adversarial networks; Image resolution; Photomapping; Pixels; Remote sensing; Deep learning; Distribution-functions; GraphicaL model; Land cover; Land cover fraction; Layout; Remote-sensing; Spatial errors; Spatial resolution; Spectral-spatial error; Superresolution; Superresolution mapping; image resolution; land cover; machine learning; mapping; multispectral image; remote sensing; spatial analysis; spectral analysis; Distribution functions","Deep learning (DL); generative adversarial network (GAN); land cover fractions; spectral-spatial errors; super-resolution mapping (SRM)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144774026"
"Shao M.; Wang C.; Zuo W.; Meng D.","Shao, Mingwen (8306286100); Wang, Chao (57218503977); Zuo, Wangmeng (56888903800); Meng, Deyu (23393058400)","8306286100; 57218503977; 56888903800; 23393058400","Efficient Pyramidal GAN for Versatile Missing Data Reconstruction in Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5626014","","","","10.1109/TGRS.2022.3188913","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134237103&doi=10.1109%2fTGRS.2022.3188913&partnerID=40&md5=47e91f95971e0acd730ee6334fcc7b05","Missing data reconstruction is a classical, yet challenging problem in remote sensing (RS) image processing due to the complex atmospheric environment and variability of satellite sensors. Most of the contemporary reconstruction methods either handle only one specific task or require supplementary data, while the single input for multitask reconstruction has not been explored yet. In this article, we propose a novel generative adversarial network-based unified framework for missing RS image reconstruction, which is capable of various reconstruction tasks given only single-source data as input. Specifically, we first propose a mask extraction network (MEN) to obtain a united soft mask, which represents the intrinsic prior under various scenarios and indicates not only location, but also context information. The versatility of mask extraction enables the multitask reconstruction of RS images. Besides, we propose a unified inpainting network (UIN) to repair diverse degraded images. Being specifically tailored for RS images, dilated pyramidal convolutions (DPCs) and an attention fusion mechanism (AFM) are introduced to further improve the feature extraction ability and thus exhaustly leveraging the single-input information. Extensive experiments demonstrate the uncompromising performance of the proposed method against state-of-the-art multiinput methods on diverse missing restoration. Moreover, further exploration shows the potential of the proposed method to utilize joint spatial-spectral-temporal information, which is evaluated to outperform existing competitors on remote sense images.  © 1980-2012 IEEE.","Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Job analysis; Remote sensing; Cloud removal; Data reconstruction; Features extraction; Images reconstruction; Missing data; Reconstruction algorithms; Remote sensing images; Remote-sensing; Task analysis; image processing; reconstruction; remote sensing; satellite sensor; Image reconstruction","Cloud removal; generative adversarial network (GAN); image reconstruction; remote sensing (RS) images","Article","Final","","Scopus","2-s2.0-85134237103"
"Xia Y.; Li J.; Guo D.","Xia, Ying (36626744600); Li, Junyao (57554180000); Guo, Dongen (36720716400)","36626744600; 57554180000; 36720716400","Semi-supervised Scene Classification of Remote Sensing Images Based on GAN; [基于GAN的半监督遥感图像场景分类]","2022","Guangzi Xuebao/Acta Photonica Sinica","51","3","0310003","","","","10.3788/gzxb20225103.0310003","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127232288&doi=10.3788%2fgzxb20225103.0310003&partnerID=40&md5=e9fd3ecc5721e6ad56f5df99527ef33d","Remote sensing image scene classification is an important and challenging problem of remote sensing image interpretation. With the generation of a large number of scene-rich high-resolution remote sensing images, scene classification of remote sensing images is widely used in many fields such as smart city construction, natural disaster monitoring and land resource utilization. Due to the advancement of deep learning techniques and the establishment of large-scale scene classification datasets, scene classification methods have been significantly improved. Although the classification methods based on deep learning have achieved high classification accuracy, the supervised methods require a large number of training samples, while the unsupervised classification methods are difficult to meet the practical needs and have low classification accuracy. Meanwhile, the annotation of remote sensing images requires rich engineering skills and expert knowledge, and in remote sensing applications, only a small amount of labeled remote sensing images exist for supervised training in most cases, and a large amount of unlabeled images cannot be fully utilized. Therefore, a semi-supervised learning method that extracts effective features from a large amount of unlabeled data by learning a small amount of labeled data becomes a potential way to solve such problems. To address the problems of complex background of remote sensing images and the inability of supervised scene classification algorithms to utilize unlabeled data, a semi-supervised remote sensing image scene classification method based on generative adversarial networks, namely, residual attention generative adversarial networks, is proposed. First, to enhance the stability of training, the residual blocks with jump structure are introduced in the deep neural network. At the same time, the spectral normalization constrains the spectral norm of the weight matrix in each convolutional layer of the residual block to ensure that the input and output of each batch of data satisfy the 1-Lipschitz continuity, which makes the generative adversarial training always smooth, not only improves the training stability, but also avoids network degradation. Secondly, since the shallow features extracted by the bottom convolution contain mostly local information and low semantics, while the deep features extracted by the top convolution contain more global information but lose part of the detail information. Therefore, the shallow features are fused with the deep features extracted from the multi-layer spectral normalized residual blocks to reduce the loss of features and allow the model to learn the complementary relationships between different features, thus improving the model's representational ability. Finally, to guide the model to focus more purposefully on important features and suppress unnecessary features, an attention module that mimics the signal processing of the human brain is used. Meanwhile, in order to obtain stronger feature representation ability and capture the dependency relationship between features, a gating mechanism is introduced to form an attention module combined with gating. To verify the superiority of the method, experiments were conducted on two high-resolution remote sensing image datasets, EuroSAT and UC Merced. In the EuroSAT dataset, the highest classification accuracy reached 93.3% and 97.4% when the number of labeled features was 2 000 and 21 600, respectively. In the UC Merced dataset, the classification accuracies reached 85.7% and 91.0% when the number of labeled was 400 and 1 680, respectively. To further validate the degree of contribution of each module, ablation experiments were also conducted in the EuroSAT and UCM public datasets, and it can be concluded from the validation that the spectral normalization residual module has the largest contribution, with improvement for different number of labeled samples. The reason is that the spectral normalization ensures that the gradient of the network is limited to a certain range during backpropagation, improving the stability of the generative adversarial network, and also does not destroy the network structure in the process. The next is the attention module combined with gating, especially when the labeled sample size is greater than 10%, the classification effect is improved more because the sample size is sufficient to learn more comprehensive features. The smallest contribution is the feature fusion module, because when the sample size is very small, the network is not sufficiently trained and learned, and a part of redundant or invalid features are extracted, resulting in lower classification accuracy. The above experimental results show that the proposed residual attention generation adversarial network classification method can effectively extract more discriminative features and improve the semi-supervised classification performance for the problem of small sample size of labeled high-resolution remote sensing images, which makes it difficult to extract discriminative features. © 2022, Science Press. All right reserved.","Classification (of information); Convolutional neural networks; Data mining; Deep neural networks; Disasters; Generative adversarial networks; Image classification; Large dataset; Remote sensing; Semantics; Supervised learning; Attention mechanisms; Classification accuracy; Classification methods; High-resolution remote sensing images; Image scene classification; Remote sensing images; Sample sizes; Scene classification; Semi-supervised; Spectral normalization; Convolution","Attention mechanism; Generative Adversarial network; Remote sensing image; Scene classification; Semi-supervised","Article","Final","","Scopus","2-s2.0-85127232288"
"Zhou Y.; Wang H.; Yang R.; Yao G.; Xu Q.; Zhang X.","Zhou, Yongxiu (57804308100); Wang, Honghui (16644207700); Yang, Ronghao (35192462600); Yao, Guangle (57192202385); Xu, Qiang (56506033600); Zhang, Xiaojuan (57199704991)","57804308100; 16644207700; 35192462600; 57192202385; 56506033600; 57199704991","A Novel Weakly Supervised Remote Sensing Landslide Semantic Segmentation Method: Combining CAM and cycleGAN Algorithms","2022","Remote Sensing","14","15","3650","","","","10.3390/rs14153650","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137086172&doi=10.3390%2frs14153650&partnerID=40&md5=224cbcb3806e6c42e1fc45bee150594a","With the development of deep learning algorithms, more and more deep learning algorithms are being applied to remote sensing image classification, detection, and semantic segmentation. The landslide semantic segmentation of a remote sensing image based on deep learning mainly uses supervised learning, the accuracy of which depends on a large number of training data and high-quality data annotation. At this stage, high-quality data annotation often requires the investment of significant human effort. Therefore, the high cost of remote sensing landslide image data annotation greatly restricts the development of a landslide semantic segmentation algorithm. Aiming to resolve the problem of the high labeling cost of landslide semantic segmentation with a supervised learning method, we proposed a remote sensing landslide semantic segmentation with weakly supervised learning method combing class activation maps (CAMs) and cycle generative adversarial network (cycleGAN). In this method, we used the image level annotation data to replace pixel level annotation data as the training data. Firstly, the CAM method was used to determine the approximate position of the landslide area. Then, the cycleGAN method was used to generate the fake image without a landslide, and to make the difference with the real image to obtain the accurate segmentation of the landslide area. Finally, the pixel-level segmentation of the landslide area on remote sensing image was realized. We used mean intersection-over-union (mIOU) to evaluate the proposed method, and compared it with the method based on CAM, whose mIOU was 0.157, and we obtain better result with mIOU 0.237 on the same test dataset. Furthermore, we made a comparative experiment using the supervised learning method of a u-net network, and the mIOU result was 0.408. The experimental results show that it is feasible to realize landslide semantic segmentation in a remote sensing image by using weakly supervised learning. This method can greatly reduce the workload of data annotation. © 2022 by the authors.","Deep learning; Image analysis; Learning algorithms; Learning systems; Pixels; Remote sensing; Semantic Segmentation; Semantics; Statistical tests; Supervised learning; Activation maps; Class activation map; Cycle generative adversarial network; Data annotation; Landslide semantic segmentation; Remote sensing images; Remote-sensing; Semantic segmentation; Supervised learning methods; Weakly supervised learning; Landslides","CAM; cycleGAN; landslide semantic segmentation; remote sensing; weakly supervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137086172"
"Parusinski M.; Ibrahim S.; Lampert T.","Parusinski, Michal (57937499200); Ibrahim, Saleh (57937499300); Lampert, Thomas (36098005400)","57937499200; 57937499300; 36098005400","A STUDY ON GENERALIZING BUILDING EXTRACTION MODELS TO UNSEEN DATASETS USING SOURCE DOMAIN TRANSFER","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1352","1355","3","10.1109/IGARSS46834.2022.9883354","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140397388&doi=10.1109%2fIGARSS46834.2022.9883354&partnerID=40&md5=027cf7b7268dba0fd840dc66fe402b82","Building footprint detection from remote sensing imagery remains an activate field of research. In particular, training models that generalise well remains challenging. A common approach to building generic models is to leverage domain adaptation approaches based on style transfer to adapt labelled datasets to an unlabelled domain. These assume the availability of labelled and unlabelled data during training. In applications such as emergency mapping however, the target domain is not always known in advance, and there is therefore a need to build models that generalise to domains unseen at training time. Using SpaceNet data various domain adaptation approaches are evaluated to improve model gen-eralisation. This article demonstrates that domain adaptation improves model generalisation, and the choice of source and target domain can be more significant than the choice of style transfer algorithm. © 2022 IEEE.","Deep learning; Extraction; Optical remote sensing; Semantic Segmentation; Semantics; Building extraction; Building footprint; Data augmentation; Deep learning; Domain adaptation; Domain transfers; Extraction modeling; Optical satellite images; Semantic segmentation; Target domain; Generative adversarial networks","Building extraction; data augmentation; deep learning; domain adaptation; generative adversarial networks; optical satellite images; semantic segmentation","Conference paper","Final","","Scopus","2-s2.0-85140397388"
"Ma S.; Liu C.; Li Z.; Yang W.","Ma, Suqiang (57905083800); Liu, Chun (55680754700); Li, Zheng (57210125953); Yang, Wei (57198593282)","57905083800; 55680754700; 57210125953; 57198593282","Integrating Adversarial Generative Network with Variational Autoencoders towards Cross-Modal Alignment for Zero-Shot Remote Sensing Image Scene Classification","2022","Remote Sensing","14","18","4533","","","","10.3390/rs14184533","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138753489&doi=10.3390%2frs14184533&partnerID=40&md5=33909d6e5857059daff28fe2b9dec603","Remote sensing image scene classification takes image blocks as classification units and predicts their semantic descriptors. Because it is difficult to obtain enough labeled samples for all classes of remote sensing image scenes, zero-shot classification methods which can recognize image scenes that are not seen in the training stage are of great significance. By projecting the image visual features and the class semantic features into the latent space and ensuring their alignment, the variational autoencoder (VAE) generative model has been applied to address remote-sensing image scene classification under a zero-shot setting. However, the VAE model takes the element-wise square error as the reconstruction loss, which may not be suitable for measuring the reconstruction quality of the visual and semantic features. Therefore, this paper proposes to augment the VAE models with the generative adversarial network (GAN) to make use of the GAN’s discriminator in order to learn a suitable reconstruction quality metric for VAE. To promote feature alignment in the latent space, we have also proposed cross-modal feature-matching loss to make sure that the visual features of one class are aligned with the semantic features of the class and not those of other classes. Based on a public dataset, our experiments have shown the effects of the proposed improvements. Moreover, taking the ResNet models of ResNet18, extracting 512-dimensional visual features, and ResNet50 and ResNet101, both extracting 2048-dimensional visual features for testing, the impact of the different visual feature extractors has also been investigated. The experimental results show that better performance is achieved by ResNet18. This indicates that more layers of the extractors and larger dimensions of the extracted features may not contribute to the image scene classification under a zero-shot setting. © 2022 by the authors.","Generative adversarial networks; Image classification; Learning systems; Remote sensing; Semantics; Space optics; Auto encoders; Cross-modal; Cross-modal feature alignment; Feature alignment; Image scene classification; Remote sensing image scene classification; Remote sensing images; Semantic features; Variational autoencoder; Visual feature; Zero-shot learning","cross-modal feature alignment; generative adversarial network; remote sensing image scene classification; variational autoencoder; zero-shot learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138753489"
"Kong Y.; Liu S.; Peng X.","Kong, Yingying (35186206400); Liu, Siyuan (56379030500); Peng, Xiangyang (57214935616)","35186206400; 56379030500; 57214935616","Multi-Scale translation method from SAR to optical remote sensing images based on conditional generative adversarial network","2022","International Journal of Remote Sensing","43","8","","2837","2860","23","10.1080/01431161.2022.2072179","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130645048&doi=10.1080%2f01431161.2022.2072179&partnerID=40&md5=bf45c7e0b73f2fdab7f48f413032beec","Synthetic aperture radar (SAR) has all-weather and all-day observation capabilities and a certain ability to penetrate the surface; therefore, it has unique advantages in many aspects, which other remote sensing methods cannot achieve. However, owing to imaging principles, the interpretation of SAR images is complicated. Therefore, converting SAR images into optical remote sensing images is one method of SAR image interpretation. Therefore, this paper proposes an improved conditional generative adversarial network (cGAN) that includes a generator based on the encoder-decoder structure of the swin transformer feature extraction module. The generator uses a spatial pyramid structure and multi-scale depth feature extraction structure to extract the SAR image features at different scales. A multi-scale discriminator was used to discriminate images on different scales. The introduction of feature matching loss makes the discriminator’s feedback to the generator richer and uses perceptual loss based on VGGNet to evaluate the image quality in depth. A series of experiments verified the effectiveness of the method in this study and proved the potential of the transformer structure in the field of SAR image translation. The method presented in this paper has obvious advantages and can translate SAR images into optical remote sensing images. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Extraction; Feature extraction; Radar imaging; Remote sensing; Synthetic aperture radar; Features extraction; Image interpretation; Image-based; Imaging principle; Multi-scales; Optical remote sensing; Remote sensing images; Remote-sensing; Synthetic aperture radar images; Translation method; data interpretation; experimental study; image analysis; optical property; remote sensing; synthetic aperture radar; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85130645048"
"Chen S.; Lan J.; Liu H.; Chen C.; Wang X.","Chen, Shuai (57195431910); Lan, Jinhui (7102691503); Liu, Haoting (34882031200); Chen, Chengkai (58031863300); Wang, Xiaohan (57915571700)","57195431910; 7102691503; 34882031200; 58031863300; 57915571700","Helmet Wearing Detection of Motorcycle Drivers Using Deep Learning Network with Residual Transformer-Spatial Attention","2022","Drones","6","12","415","","","","10.3390/drones6120415","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144877413&doi=10.3390%2fdrones6120415&partnerID=40&md5=472c7b966720d61569b417814b73ea13","Aiming at the existing problem of unmanned aerial vehicle (UAV) aerial photography for riders’ helmet wearing detection, a novel aerial remote sensing detection paradigm is proposed by combining super-resolution reconstruction, residual transformer-spatial attention, and you only look once version 5 (YOLOv5) image classifier. Due to its small target size, significant size change, and strong motion blur in UAV aerial images, the helmet detection model for riders has weak generalization ability and low accuracy. First, a ladder-type multi-attention network (LMNet) for target detection is designed to conquer these difficulties. The LMNet enables information interaction and fusion at each stage, fully extracts image features, and minimizes information loss. Second, the Residual Transformer 3D-spatial Attention Module (RT3DsAM) is proposed in this work, which digests information from global data that is important for feature representation and final classification detection. It also builds self-attention and enhances correlation between information. Third, the rider images detected by LMNet are cropped out and reconstructed by the enhanced super-resolution generative adversarial networks (ESRGAN) to restore more realistic texture information and sharp edges. Finally, the reconstructed images of riders are classified by the YOLOv5 classifier. The results of the experiment show that, when compared with the existing methods, our method improves the detection accuracy of riders’ helmets in aerial photography scenes, with the target detection mean average precision (mAP) evaluation indicator reaching 91.67%, and the image classification top1 accuracy (TOP1 ACC) gaining 94.23%. © 2022 by the authors.","","helmet wearing detection; LMNet; residual transformer-spatial attention; super-resolution reconstruction; UAV","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144877413"
"Liu M.; Ren D.; Sun H.; Yang S.X.; Shao P.","Liu, Ming (57222007940); Ren, Dong (57919426100); Sun, Hang (57189387242); Yang, Simon X. (57868439000); Shao, Pan (56001238700)","57222007940; 57919426100; 57189387242; 57868439000; 56001238700","Orchard Areas Segmentation in Remote Sensing Images via Class Feature Aggregate Discriminator","2022","IEEE Geoscience and Remote Sensing Letters","19","","2507305","","","","10.1109/LGRS.2022.3213679","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139854790&doi=10.1109%2fLGRS.2022.3213679&partnerID=40&md5=95b71cb97cb81ceaf21609f1dcfde09b","Accurate evaluation of orchard areas from remote sensing images is of great importance in economic and ecological aspects. In practice, the differences in distributions between remote sensing images and the lack of data labels make the semantic segmentation model impossible to use in new data. Unsupervised domain adaptation (UDA) methods can improve the performance of the model in the target domain by aligning the source domain and the target domain. However, due to the class mismatch problem and the interference of high-dimensional feature complexity, most UDA methods cannot achieve satisfactory results in orchard areas segmentation task. To address these issues, we propose an UDA model for orchard areas segmentation by developing a class feature aggregate discriminator (CFUDA). The class feature aggregate discriminator is designed to distinguish intradomain classes and align interdomain classes, and class feature aggregate can represent class information of different domains, which helps the model to avoid the interference of complex information. In addition, adversarial loss reweighting is introduced to the novel model, which makes the segmentation model pay more attention to the orchard areas. To verify the effectiveness of the proposed method, we conducted extensive experiments in three different remote sensing images around Yichang City. Compared to the baseline model, the proposed approach improves intersection over union (IoU) by 27.68%, and we achieve high gains of 6.07% in IoU over other UDA methods. The larger gain indicates that our proposed method has great potential in cross-domain orchard areas segmentation.  © 2004-2012 IEEE.","China; Hubei; Yichang; Aggregates; Complex networks; Discriminators; Generative adversarial networks; Image resolution; Remote sensing; Semantic Segmentation; Semantic Web; Adaptation methods; Adaptation models; Domain adaptation; Remote sensing images; Remote-sensing; Segmentation models; Semantic segmentation; Task analysis; Unsupervised domain adaptation; network analysis; orchard; remote sensing; satellite imagery; segmentation; Semantics","Generative adversarial networks; remote sensing; semantic segmentation; unsupervised domain adaptation (UDA)","Article","Final","","Scopus","2-s2.0-85139854790"
"Khoo J.J.D.; Lim K.H.; Pang P.K.","Khoo, John Julius Danker (57221834507); Lim, King Hann (25031784300); Pang, Po Ken (57209463435)","57221834507; 25031784300; 57209463435","Deep Learning Super Resolution of Sea Surface Temperature on South China Sea","2022","2022 International Conference on Green Energy, Computing and Sustainable Technology, GECOST 2022","","","","176","180","4","10.1109/GECOST55694.2022.10010371","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147041003&doi=10.1109%2fGECOST55694.2022.10010371&partnerID=40&md5=b0456d18133bc085bfce74991040fd90","Surface temperature is one of the key observations to analyse the greenhouse effect on the Earth. The surface of the ocean can be captured using satellite sensors and transmitted to a meteorological center for real-time analysis. The use of the deep learning paradigm in super resolution has its potential in geoscience applications to increase the data transmission latency and enhance low-quality observation from remote sensing data. In this paper, the deployment of Generative Adversarial Network (GAN) architecture is studied to apply resolution reconstruction using the South China Sea sea surface temperature data. In addition, the development of spectral normalization is added to the Enhanced Super Resolution Generative Adversarial Network (ESRGAN) architecture to improve the training mechanism of generator and discriminator. This improved ESRGAN is compared with its super resolution performance against peak signal-to-noise ratio and structural similarity index evaluation metrics. The experiment shows that the low resolution of South China Sea data can be inferred to obtain a higher resolution with a more realistic resolution as compared to the conventional upsampling approaches.  © 2022 IEEE.","Atmospheric temperature; Deep learning; Generative adversarial networks; Greenhouse effect; Network architecture; Oceanography; Optical resolving power; Remote sensing; Signal to noise ratio; Submarine geophysics; Surface waters; Deep learning; Geoscience applications; Learning paradigms; Real time analysis; Satellite sensors; Sea surface temperature; Sea surfaces; South China sea; Superresolution; Surface temperatures; Surface temperature","Deep Learning; Generative Adversarial Networks; Sea Surface Temperature; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85147041003"
"Jiang M.; Shen H.; Li J.","Jiang, Menghui (57210173702); Shen, Huanfeng (57915880100); Li, Jie (57214207213)","57210173702; 57915880100; 57214207213","Cycle GAN Based Heterogeneous Spatial-Spectral Fusion for Soil Moisture Downscaling","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","4819","4822","3","10.1109/IGARSS46834.2022.9884702","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141895647&doi=10.1109%2fIGARSS46834.2022.9884702&partnerID=40&md5=cb895a7ca82cfe8854fca03a6b9b6b0e","Soil moisture (SM) downscaling aims to solve the coarse resolution problem of passive microwave SM products. On the basis of SMAP SM products and related MODIS products, this study develops a deep residual cycle generative adversarial network (GAN) based heterogeneous spatial-spectral fusion method to downscale SMAP SM from 36km to 9km. On the one hand, the proposed method creatively regards the MODIS products that can reflect the SM state as the spectral features of SM in a broad sense and performs the heterogeneous spatial-spectral fusion between the low-resolution (LR) SM product and high-resolution (HR) MODIS products. On the other hand, considering the spatial correlation of SM, the proposed method utilizes a deep residual cycle generative adversarial network (GAN) to extract and fuse features of heterogeneous images through convolutions. Both qualitative and quantitative evaluation of experimental results shows that the proposed method can generate high accuracy SM products. © 2022 IEEE.","Radiometers; Remote sensing; Soil moisture; Coarser resolution; Cycle generative adversarial network; Down-scaling; Downscale; Fusion methods; Heterogeneous spatial-spectral fusion; Moisture state; Network-based; Passive microwaves; Spectral feature; Generative adversarial networks","cycle GAN; downscale; heterogeneous spatial-spectral fusion; Soil moisture","Conference paper","Final","","Scopus","2-s2.0-85141895647"
"Guo D.; Wu Z.; Zhang Y.; Shen Z.","Guo, Dongen (36720716400); Wu, Zechen (57832208500); Zhang, Yuanzheng (57934381100); Shen, Zhen (57831388700)","36720716400; 57832208500; 57934381100; 57831388700","Semi-supervised Remote Sensing Image Scene Classification Based on Generative Adversarial Networks","2022","International Journal of Computational Intelligence Systems","15","1","87","","","","10.1007/s44196-022-00150-0","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140263727&doi=10.1007%2fs44196-022-00150-0&partnerID=40&md5=84010cd6378ea4273d3ac439f10b5d88","With the availability of numerous high-resolution remote sensing images, remote sensing image scene classification has been widely used in various fields. Compared with the field of natural images, the insufficient number of labeled remote sensing images limits the performance of supervised scene classification, while unsupervised methods are difficult to meet the practical applications. Therefore, this paper proposes a semi-supervised remote sensing image scene classification method using generative adversarial networks. The proposed method introduces dense residual block, pre-trained Inception V3 networks, gating unit, pyramidal convolution, and spectral normalization into GANs to promote the semi-supervised classification performance. To be specific, the pre-trained Inception V3 network is introduced to extract semantic features to enhance the feature discriminant capability. The gating unit is utilized to capture the relationships among features. The pyramidal convolution is integrated into dense residual block to capture different levels of details to strengthen the feature representation capability. The spectral normalization is introduced to stabilize the GANs training to improve semi-supervised classification accuracy. Extensive experimental results on publicly available EuroSAT and UC Merced datasets show that the proposed method gains the highest overall accuracy, especially when only a few labeled samples are available. © 2022, The Author(s).","Convolution; Image classification; Remote sensing; Semantics; Supervised learning; Gating unit; Generative adversarial network; Image scene classification; Pyramidal convolution; Remote sensing image scene classification; Remote sensing images; Semi-supervised; Semi-supervised learning; Semisupervised classification (SSC); Spectral normalization; Generative adversarial networks","Gating unit; Generative adversarial networks (GANs); Pyramidal convolution; Remote sensing image scene classification; Semi-supervised learning; Spectral normalization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140263727"
"Boktor M.; Ecclestone B.R.; Pekar V.; Dinakaran D.; Mackey J.R.; Fieguth P.; Haji Reza P.","Boktor, Marian (57546768200); Ecclestone, Benjamin R. (57219132521); Pekar, Vlad (57547917600); Dinakaran, Deepak (57194147623); Mackey, John R. (56062267400); Fieguth, Paul (7004156748); Haji Reza, Parsin (57222737826)","57546768200; 57219132521; 57547917600; 57194147623; 56062267400; 7004156748; 57222737826","Virtual histological staining of label-free total absorption photoacoustic remote sensing (TA-PARS)","2022","Scientific Reports","12","1","10296","","","","10.1038/s41598-022-14042-y","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132144819&doi=10.1038%2fs41598-022-14042-y&partnerID=40&md5=575cf253d79336ed00eea37bc9a22079","Histopathological visualizations are a pillar of modern medicine and biological research. Surgical oncology relies exclusively on post-operative histology to determine definitive surgical success and guide adjuvant treatments. The current histology workflow is based on bright-field microscopic assessment of histochemical stained tissues and has some major limitations. For example, the preparation of stained specimens for brightfield assessment requires lengthy sample processing, delaying interventions for days or even weeks. Therefore, there is a pressing need for improved histopathology methods. In this paper, we present a deep-learning-based approach for virtual label-free histochemical staining of total-absorption photoacoustic remote sensing (TA-PARS) images of unstained tissue. TA-PARS provides an array of directly measured label-free contrasts such as scattering and total absorption (radiative and non-radiative), ideal for developing H&E colorizations without the need to infer arbitrary tissue structures. We use a Pix2Pix generative adversarial network to develop visualizations analogous to H&E staining from label-free TA-PARS images. Thin sections of human skin tissue were first virtually stained with the TA-PARS, then were chemically stained with H&E producing a one-to-one comparison between the virtual and chemical staining. The one-to-one matched virtually- and chemically- stained images exhibit high concordance validating the digital colorization of the TA-PARS images against the gold standard H&E. TA-PARS images were reviewed by four dermatologic pathologists who confirmed they are of diagnostic quality, and that resolution, contrast, and color permitted interpretation as if they were H&E. The presented approach paves the way for the development of TA-PARS slide-free histological imaging, which promises to dramatically reduce the time from specimen resection to histological imaging. © 2022, The Author(s).","Humans; Microscopy; Microtomy; Remote Sensing Technology; Staining and Labeling; Workflow; article; deep learning; gold standard; histology; histopathology; human; human tissue; pathologist; remote sensing; skin; tissue structure; microscopy; microtomy; procedures; staining; workflow","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132144819"
"Xin L.; Li Z.; Wang S.","Xin, L. (57742773600); Li, Z. (57884534800); Wang, S. (57852799900)","57742773600; 57884534800; 57852799900","Super-resolution research on remote sensing images in the megacity based on improved srgan","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","603","609","6","10.5194/isprs-Annals-V-3-2022-603-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132009410&doi=10.5194%2fisprs-Annals-V-3-2022-603-2022&partnerID=40&md5=e08573da31b29a3d21c06ab392dd8075","Remote sensing images of Earth observation with high spatial resolution and high temporal resolution are critical for the application of remote sensing technology in Megacities.With the development of Smart City,more demands which are still difficult to be perfectly satisfied on the spatial resolution and temporal resolution of remote sensing images have been put forward.This paper studies the use of SRGAN which means Super-Resolution using a Generative Adversarial Network (a network structure that uses the loss function considering the perceptual loss and the adversarial loss to improve the spatial resolution of remote sensing images) for super-resolution reconstruction of single remote sensing image.It is able to enhance the spatial resolution of remote sensing images and improve the depth and breadth of remote sensing images.We adjust the reasonable parameters and network structure for our research by analysing the SRGAN in the network architecture, the perceptual loss and the adversarial loss.A super-resolution model is obtained by training with aerial photogrammetry images whose spatial resolution are 0.1 meter in Shanghai.We find the improved SRGAN has a good performance in in remote sensing image super-resolution by comparing the super-resoved images with real high-resolution images in visual perception, spatial position mapping accuracy and chromaticity spatial information. In addition, it is proved that the trained model is also effective to deal with Worldview-2 and SuperView-1 satellite images whose spatial resolution are 0.5 m. Our research shows that our method which can effectively realize the super-resolution of remote sensing images has great potential in the application of remote sensing technology such as urban mapping and changes monitoring.  © Authors 2022.","Antennas; Generative adversarial networks; Image enhancement; Mapping; Network architecture; Remote sensing; Satellite imagery; Deeplearning; High resolution satellite imagery; Megacities; Network structures; Remote sensing images; Remote sensing technology; Spatial resolution; SRGAN; Superresolution; Urban remote sensing; Image resolution","Deeplearning; High resolution satellite imagery; SRGAN; Super resolution; Urban Remote Sensing","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132009410"
"Chen H.; Sun W.","Chen, Hongshun (56810282500); Sun, Wenjing (57918275600)","56810282500; 57918275600","Building Extraction from Remote Sensing Images with Conditional Generative Adversarial Networks","2022","2022 7th International Conference on Signal and Image Processing, ICSIP 2022","","","","655","658","3","10.1109/ICSIP55141.2022.9886096","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139442069&doi=10.1109%2fICSIP55141.2022.9886096&partnerID=40&md5=7ea4ca330ec382a4f4b01b73d002d67e","Automated building extraction from remote sensing images is one of the most challenging problems. In order to automatically extract buildings from remote sensing images by Conditional Generative Adversarial Networks(CGAN), the pix2pix model was adopted and tested on the aerial imagery dataset of WHU Building Dataset. Results show that the pix2pix model performances well and achieves a high precision.  © 2022 IEEE.","Aerial photography; Antennas; Buildings; Extraction; Remote sensing; Aerial imagery; Automated buildings; Building extraction; High-precision; Modeling performance; Pix2pix; Remote sensing images; Remote-sensing; Generative adversarial networks","building extraction; generative adversarial networks; pix2pix; remote sensing","Conference paper","Final","","Scopus","2-s2.0-85139442069"
"Kalita I.; Mugganawar N.; Roy M.","Kalita, Indrajit (56652252700); Mugganawar, Nikhil (57937583800); Roy, Moumita (36875564800)","56652252700; 57937583800; 36875564800","Unsupervised cross-sensor domain adaptation using adversarial network for land cover classification","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","5724","5727","3","10.1109/IGARSS46834.2022.9884404","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140361421&doi=10.1109%2fIGARSS46834.2022.9884404&partnerID=40&md5=eb4ad2c5143a1728a4422bff31e73c05","Remote sensing data gathered by various satellites (cross-sensor data having) are used to have a significant impact to lower down the performance of the stand-alone land cover classification model. The collected data from source and target regions have different probability distributions due to the different resolution of images and different geographical locations. To deal with this problem, an adversarial network-based unsupervised cross-sensor domain network has been investigated by considering two source → target scenarios using hyperspectral and aerial image datasets. Initially, an unsupervised generative adversarial network (GAN) has been implemented to minimize the distribution between both domains. Following that, the transformed target images are obtained using the trained GAN architecture. Thereafter, a deep convolutional neural network (DCNN) has been trained using the source images and finally, the trained DCNN is used to predict the land cover classes under a multi-sensor framework. The effectiveness of the proposed scheme has been compared with the state-of-the-art techniques, and the results are found to be promising to handle the issues under an unsupervised cross-sensor environment. © 2022 IEEE.","Antennas; Convolution; Convolutional neural networks; Deep neural networks; Geographical regions; Probability distributions; Remote sensing; Active Learning; Adversarial networks; Convolutional neural network; Cross-sensor domain adaptation; Domain adaptation; Land cover classification; Performance; Remote sensing data; Sensor domains; Sensors data; Generative adversarial networks","Active learning; Convolutional neural network; Cross-sensor domain adaptation; Land cover classification","Conference paper","Final","","Scopus","2-s2.0-85140361421"
"Xu X.; Zhao B.; Tong X.; Xie H.; Feng Y.; Wang C.; Xiao C.; Ke X.; Du J.","Xu, Xiong (55520591300); Zhao, Beibei (57796530700); Tong, Xiaohua (55500134600); Xie, Huan (36117406500); Feng, Yongjiu (57760693900); Wang, Chao (57218503727); Xiao, Changjiang (57859165700); Ke, Xiaoxue (57919600700); Du, Jinhuan (57919948500)","55520591300; 57796530700; 55500134600; 36117406500; 57760693900; 57218503727; 57859165700; 57919600700; 57919948500","A Data Augmentation Strategy Combining a Modified pix2pix Model and the Copy-Paste Operator for Solid Waste Detection with Remote Sensing Images","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8484","8491","7","10.1109/JSTARS.2022.3209967","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139481780&doi=10.1109%2fJSTARS.2022.3209967&partnerID=40&md5=1d4fcbe3f0668f3b65d2e8ae61c5a586","Solid waste detection is of great significance for environmental protection. In recent years, object detection methods based on deep learning have progressed rapidly. However, it is often extremely difficult to collect sufficient data to train a model with a good performance. In this article, a data augmentation strategy was introduced to generate sufficient synthetic high-quality images for solid waste detection. First, a modified pix2pix model was proposed, in which a local-global discriminator was designed to improve the detailed and global information of the generated images, which are commonly fuzzy with the original pix2pix model. Second, a copy-paste operator was utilized, which simply pastes the bounding box of the generated objects into different images to enhance the diversity of the samples. In this manner, the expanded dataset can be utilized to train different object detection models, for which FPN and Yolo-v4 were introduced as the validation models in this article. The experimental results show that the proposed strategy outperforms the traditional pix2pix method and the generated synthetic images can effectively improve the performance of object detection methods.  © 2008-2012 IEEE.","Deep learning; Discriminators; Generative adversarial networks; Image enhancement; Object detection; Object recognition; Remote sensing; Solid wastes; Copy-paste; Data augmentation; Generator; Local-global discriminator; Object detection method; Objects detection; Performance; Pix2pix; Remote-sensing; image analysis; image classification; numerical model; remote sensing; satellite data; satellite imagery; Personnel training","Copy-paste; data augmentation; local-global discriminator (LGD); object detection; pix2pix","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139481780"
"Wang J.; Shao Z.; Huang X.; Lu T.; Zhang R.; Li Y.","Wang, Jiaming (57206676342); Shao, Zhenfeng (7202244409); Huang, Xiao (57201292422); Lu, Tao (56406646300); Zhang, Ruiqian (57190385256); Li, Yong (57223768875)","57206676342; 7202244409; 57201292422; 56406646300; 57190385256; 57223768875","From Artifact Removal to Super-Resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5627715","","","","10.1109/TGRS.2022.3196709","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135754017&doi=10.1109%2fTGRS.2022.3196709&partnerID=40&md5=2ab1d77f49794834081a3f5fa02221d3","Deep-learning-based super-resolution (SR) methods have been extensively studied and have achieved significant performance with deep convolutional neural networks. However, the results still suffer from the ringing effect, especially in satellite image SR tasks, due to the loss of image details in the satellite degradation process. In this article, we build a novel satellite SR framework by decomposing a high-resolution image into three components, i.e., low-resolution (LR), artifact, and high-frequency information. Specifically, we propose an artifact removal network with a self-adaption difference convolution (SDC) to fully exploit the structure prior in the LR image and predict the artifact map. Considering that the artifact map and the high-frequency map share a similar pattern, we introduce the supervised structure correction (SSC) block that establishes a bridge between the high-frequency generation process and the artifact removal process. Experimental results on satellite images demonstrate that the proposed method owns an improved tradeoff between the performance and the computational cost compared to existing state-of-the-art satellite and natural SR methods. The source code is available at https://github.com/jiaming-wang/ARSRN.  © 1980-2012 IEEE.","Convolution; Deep neural networks; Edge detection; Generative adversarial networks; Image enhancement; Job analysis; Optical resolving power; Remote sensing; Satellites; Artifact removal; Difference convolution; Image edge detection; Images reconstruction; Performance; Remote-sensing; Superresolution; Superresolution methods; Task analysis; artificial neural network; image analysis; image resolution; remote sensing; satellite data; satellite imagery; Image reconstruction","Artifact removal; difference convolution; remote sensing; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85135754017"
"Rodríguez-Suárez B.; Quesada-Barriuso P.; Argüello F.","Rodríguez-Suárez, Brais (57450951800); Quesada-Barriuso, Pablo (55953738200); Argüello, Francisco (55932997900)","57450951800; 55953738200; 55932997900","Design of CGAN Models for Multispectral Reconstruction in Remote Sensing","2022","Remote Sensing","14","4","816","","","","10.3390/rs14040816","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124543215&doi=10.3390%2frs14040816&partnerID=40&md5=435c9c4747a8d04ac324e272f9139144","Multispectral imaging methods typically require cameras with dedicated sensors that make them expensive. In some cases, these sensors are not available or existing images are RGB, so the advantages of multispectral processing cannot be exploited. To solve this drawback, several techniques have been proposed to reconstruct the spectral reflectance of a scene from a single RGB image captured by a camera. Deep learning methods can already solve this problem with good spectral accuracy. Recently, a new type of deep learning network, the Conditional Generative Adversarial Network (CGAN), has been proposed. It is a deep learning architecture that simultaneously trains two networks (generator and discriminator) with the additional feature that both networks are conditioned on some sort of auxiliary information. This paper focuses the use of CGANs to achieve the reconstruction of multispectral images from RGB images. Different regression network models (convolutional neuronal networks, U-Net, and ResNet) have been adapted and integrated as generators in the CGAN, and compared in performance for multispectral reconstruction. Experiments with the BigEarthNet database show that CGAN with ResNet as a generator provides better results than other deep learning networks with a root mean square error of 316 measured over a range from 0 to 16,384. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Cameras; Deep learning; Image reconstruction; Mean square error; Neurons; Remote sensing; Conditional generative adversarial network; Deep learning; Learning network; Multi-spectral; Multispectral images; Multispectral imaging; Network models; Remote-sensing; RGB images; Spectral reconstruction; Generative adversarial networks","CGAN; Deep learning; Multispectral image; Spectral reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124543215"
"Fang S.; Guo Q.; Cao Y.; Zhang J.","Fang, Shuai (7402422537); Guo, Qing (57938173300); Cao, Yang (57022583200); Zhang, Jing (57211055913)","7402422537; 57938173300; 57022583200; 57211055913","A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","891","894","3","10.1109/IGARSS46834.2022.9883547","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140364937&doi=10.1109%2fIGARSS46834.2022.9883547&partnerID=40&md5=ca6a8cca3ffe4be9f310ce7f1b26e2b0","Remote sensing image spatiotemporal fusion (STF) algorism plays an important role by supplementing the lack of original high-resolution remote sensing satellite images in the study scenarios of dense time-series data. In recent years, the deep-learning-based STF algorithm has become a research hotspot with comparatively higher accuracy and robustness. However, due to the lack of sufficient high-quality images for training and the huge resolution gap between low-resolution images and high-resolution images, it is difficult to recover detailed information, especially for areas of land-cover change. In this paper, we propose a two-layers super-resolution based generation adversarial spatiotemporal fusion model(TLSRSTF) using smaller inputs to reduce pressure on data requirements and a mutual affine convolution to reduce model parameters. Specifically, we only use a pair of high-resolution and low-resolution images and a high-resolution image at any time. A spatial degradation consistency is constructed to adaptively determine the ratio of two layers of the super-resolution STF model. The quantitative and qualitative experimental results on public spatiotemporal fusion datasets demonstrate our superiority over the state-of-the-art methods. © 2022 IEEE.","Convolution; Deep learning; Image fusion; Optical resolving power; Remote sensing; Fusion model; High resolution remote sensing; High-resolution images; Low resolution images; Mutual affine convolution; Remote sensing images; Remote sensing satellites; Spatio-temporal fusions; Superresolution; Two-layer; Generative adversarial networks","Generative Adversarial Networks(GAN); Mutual Affine Convolution; Spatiotemporal Fusion","Conference paper","Final","","Scopus","2-s2.0-85140364937"
"Meng Y.; Li W.; Lei S.; Zou Z.; Shi Z.","Meng, Yapeng (57984756800); Li, Wenyuan (57204784272); Lei, Sen (57195618353); Zou, Zhengxia (56073977200); Shi, Zhenwei (23398841900)","57984756800; 57204784272; 57195618353; 56073977200; 23398841900","Large-Factor Super-Resolution of Remote Sensing Images With Spectra-Guided Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5634111","","","","10.1109/TGRS.2022.3222360","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142837106&doi=10.1109%2fTGRS.2022.3222360&partnerID=40&md5=7e95902a868ae82cb77d9c293a163791","Large-factor image super-resolution (SR) is a challenging task due to the high uncertainty and incompleteness of the missing details to be recovered. In remote sensing images, the subpixel spectral mixing and semantic ambiguity of ground objects make this task even more challenging. In this article, we propose a novel method for large-factor SR of remote sensing images named spectra-guided generative adversarial networks (SpecGANs). In response to the above problems, we explore whether introducing additional hyperspectral images (HSIs) to GAN as conditional input can be the key to solving the problems. Different from previous approaches that mainly focus on improving the feature representation of a single source input, we propose a dual-branch network architecture to effectively fuse low-resolution (LR) red, green, blue (RGB) images and corresponding HSIs, which fully exploit the rich hyperspectral information as conditional semantic guidance. Due to the spectral specificity of ground objects, the semantic accuracy of the generated images is guaranteed. To further improve the visual fidelity of the generated output, we also introduce the Latent Code Bank with rich visual priors under a generative adversarial training framework so that high-resolution, detailed, and realistic images can be progressively generated. Extensive experiments show the superiority of our method over the state-of-art image SR methods in terms of both quantitative evaluation metrics and visual quality. Ablation experiments also suggest the necessity of adding spectral information and the effectiveness of our designed fusion module. To our best knowledge, we are the first to achieve up to 32x SR of remote sensing images with high visual fidelity under the premise of accurate ground object semantics. Our code can be publicly available at https://github.com/YapengMeng/SpecGAN. © 1980-2012 IEEE.","Computer vision; Deep neural networks; Generative adversarial networks; Hyperspectral imaging; Image enhancement; Network architecture; Optical resolving power; Quality control; Remote sensing; Semantics; Spectroscopy; Uncertainty analysis; Convolutional neural network; Deep convolutional neural network; Ground objects; HyperSpectral; Hyperspectral image; Images reconstruction; Remote sensing images; Superresolution; Task analysis; artificial neural network; image resolution; remote sensing; satellite imagery; spectral analysis; Image reconstruction","Deep convolutional neural networks (CNNs); generative adversarial networks (GANs); hyperspectral image (HSI); remote sensing image; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85142837106"
"","","","ISPRS GeoSpatial Conference 2022, Joint 6th Sensors and Models in Photogrammetry and Remote Sensing, SMPR 2022 and 4th Geospatial Information Research, GIResearch 2022 Conferences","2023","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","48","4/W2-2022","","","","115","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146925775&partnerID=40&md5=bf221f15f85d64c5dfcacc1ff284f58a","The proceedings contain 16 papers. The topics discussed include: on the mean sea surface data in the GDR files of the TOPEX/Poseidon, Jason-1, 2, and 3 missions; designing a commercial location-based system to serve customers based on GIS; castle: a context-aware spatial-temporal location embedding pre-training model for next location prediction; building change detection by W-shape RESUNET++ network with triple attention mechanism; retrospective study of vertical ground deformation in Como, Northern Italy: integration of levelling and psi measurements; rapid automatic detection of COVID-19 in chest CT images using VGG-16 and transfer learning; improving semantic segmentation of high-resolution remote sensing images using Wasserstein generative adversarial network; google earth engine based approach for coastal water monitoring: a case study of the southern shore of Caspian Sea; road network accounting when estimating settlement field potential; and determining the effectiveness of different indicators in identifying informal settlements using the membership function.","","","Conference review","Final","","Scopus","2-s2.0-85146925775"
"Wang J.; Cai M.; Gu Y.; Liu Z.; Li X.; Han Y.","Wang, Junshu (57222581353); Cai, Mingrui (57895280600); Gu, Yifan (57396726500); Liu, Zhen (57219133156); Li, Xiaoxin (57895280700); Han, Yuxing (57895280800)","57222581353; 57895280600; 57396726500; 57219133156; 57895280700; 57895280800","Cropland encroachment detection via dual attention and multi-loss based building extraction in remote sensing images","2022","Frontiers in Plant Science","13","","993961","","","","10.3389/fpls.2022.993961","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138231571&doi=10.3389%2ffpls.2022.993961&partnerID=40&md5=b4a6739c16565fa0ce40bda7432e490d","The United Nations predicts that by 2050, the world’s total population will increase to 9.15 billion, but the per capita cropland will drop to 0.151°hm2. The acceleration of urbanization often comes at the expense of the encroachment of cropland, the unplanned expansion of urban area has adversely affected cultivation. Therefore, the automatic extraction of buildings, which are the main carriers of urban population activities, in remote sensing images has become a more meaningful cropland observation task. To solve the shortcomings of traditional building extraction methods such as insufficient utilization of image information, relying on manual characterization, etc. A U-Net based deep learning building extraction model is proposed and named AttsegGAN. This study proposes an adversarial loss based on the Generative Adversarial Network in terms of training strategy, and the additionally trained learnable discriminator is used as a distance measurer for the two probability distributions of ground truth Pdata and prediction Pg. In addition, for the sharpness of the building edge, the Sobel edge loss based on the Sobel operator is weighted and jointly participated in the training. In WHU building dataset, this study applies the components and strategies step by step, and verifies their effectiveness. Furthermore, the addition of the attention module is also subjected to ablation experiments and the final framework is determined. Compared with the original, AttsegGAN improved by 0.0062, 0.0027, and 0.0055 on Acc, F1, and IoU respectively after adopting all improvements. In the comparative experiment. AttsegGAN is compared with state-of-the-arts including U-Net, DeeplabV3+, PSPNet, and DANet on both WHU and Massachusetts building dataset. In WHU dataset, AttsegGAN achieved 0.9875, 0.9435, and 0.8907 on Acc, F1, and IoU, surpassed U-Net by 0.0260, 0.1183, and 0.1883, respectively, demonstrated the effectiveness of the proposed components in a similar hourglass structure. In Massachusetts dataset, AttsegGAN also surpassed state-of-the-arts, achieved 0.9395, 0.8328, and 0.7130 on Acc, F1, and IoU, respectively, it improved IoU by 0.0412 over the second-ranked PSPNet, and it was 0.0025 and 0.0101 higher than the second place in Acc and F1. Copyright © 2022 Wang, Cai, Gu, Liu, Li and Han.","","building extraction; cropland observation; dual attention; Massachusetts building dataset; multi-loss; Sobel edge loss; UAV; WHU building dataset","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85138231571"
"Li W.; Abrashitova K.; Osnabrugge G.; Amitonova L.V.","Li, Wei (57407084700); Abrashitova, Ksenia (57193745903); Osnabrugge, Gerwin (57190004921); Amitonova, Lyubov V. (57208316205)","57407084700; 57193745903; 57190004921; 57208316205","Generative Adversarial Network for Superresolution Imaging through a Fiber","2022","Physical Review Applied","18","3","034075","","","","10.1103/PhysRevApplied.18.034075","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139278179&doi=10.1103%2fPhysRevApplied.18.034075&partnerID=40&md5=2e6eb182e6752354858101547fbdcc76","A multimode fiber represents the ultimate limit in miniaturization of imaging endoscopes. However, such a miniaturization usually comes as a cost of a low spatial resolution and a long acquisition time. Here we propose a fast superresolution-fiber-imaging technique employing compressive sensing through a multimode fiber with a data-driven machine-learning framework. We implement a generative adversarial network (GAN) to explore the sparsity inherent to the model and provide compressive reconstruction images that are not sparse in a representation basis. The proposed method outperforms other widespread compressive imaging algorithms in terms of both image quality and noise robustness. We experimentally demonstrate machine-learning ghost imaging below the diffraction limit at a sub-Nyquist speed through a thin multimode fiber probe. We believe that this work has great potential in applications in various fields ranging from biomedical imaging to remote sensing.  © 2022 authors. Published by the American Physical Society. Published by the American Physical Society under the terms of the ""https://creativecommons.org/licenses/by/4.0/""Creative Commons Attribution 4.0 International license. Further distribution of this work must maintain attribution to the author(s) and the published article's title, journal citation, and DOI.","Diffraction; Medical imaging; Miniature instruments; Multimode fibers; Optical resolving power; A.Fibres; Acquisition time; Compressive sensing; Data driven; Learning frameworks; Machine-learning; Miniaturisation; Spatial resolution; Super resolution imaging; Superresolution; Generative adversarial networks","","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139278179"
"Duan Y.; Liu X.; Jatowt A.; Yu H.-T.; Lynden S.; Kim K.-S.; Matono A.","Duan, Yijun (57195233018); Liu, Xin (56948501900); Jatowt, Adam (14826985000); Yu, Hai-Tao (57188723616); Lynden, Steven (7801635042); Kim, Kyoung-Sook (57184508400); Matono, Akiyoshi (23397696500)","57195233018; 56948501900; 14826985000; 57188723616; 7801635042; 57184508400; 23397696500","SORAG: Synthetic Data Over-Sampling Strategy on Multi-Label Graphs","2022","Remote Sensing","14","18","4479","","","","10.3390/rs14184479","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138835930&doi=10.3390%2frs14184479&partnerID=40&md5=5ec17e6225cef89354a9566c4921d061","In many real-world networks of interest in the field of remote sensing (e.g., public transport networks), nodes are associated with multiple labels, and node classes are imbalanced; that is, some classes have significantly fewer samples than others. However, the research problem of imbalanced multi-label graph node classification remains unexplored. This non-trivial task challenges the existing graph neural networks (GNNs) because the majority class can dominate the loss functions of GNNs and result in the overfitting of the majority class features and label correlations. On non-graph data, minority over-sampling methods (such as the synthetic minority over-sampling technique and its variants) have been demonstrated to be effective for the imbalanced data classification problem. This study proposes and validates a new hypothesis with unlabeled data over-sampling, which is meaningless for imbalanced non-graph data; however, feature propagation and topological interplay mechanisms between graph nodes can facilitate the representation learning of imbalanced graphs. Furthermore, we determine empirically that ensemble data synthesis through the creation of virtual minority samples in the central region of a minority and generation of virtual unlabeled samples in the boundary region between a minority and majority is the best practice for the imbalanced multi-label graph node classification task. Our proposed novel data over-sampling framework is evaluated using multiple real-world network datasets, and it outperforms diverse, strong benchmark models by a large margin. © 2022 by the authors.","Backpropagation; Classification (of information); Convolutional neural networks; Generative adversarial networks; Graph algorithms; Graph neural networks; Graph structures; Large dataset; Semi-supervised learning; Convolutional networks; Data classification; Data over-sampling; Graph convolutional network; Imbalanced data; Imbalanced data classification; Multi-labels; Over sampling; Remote-sensing; Semi-supervised learning; Remote sensing","data over-sampling; generative adversarial network; graph convolutional network; imbalanced data classification; remote sensing; semi-supervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138835930"
"Fu Y.; Zhang X.; Wang M.","Fu, Yujia (57776361900); Zhang, Xiangrong (37076469400); Wang, Mingyang (57919947700)","57776361900; 37076469400; 57919947700","Super-Resolution Reconstruction of Remote Sensing Images Using Generative Adversarial Network with Shallow Information Enhancement","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8529","8540","11","10.1109/JSTARS.2022.3209819","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139491442&doi=10.1109%2fJSTARS.2022.3209819&partnerID=40&md5=f967887b97beff71766faa594138489f","The super-resolution (SR) reconstruction method based on deep learning can significantly improve the spatial SR of remote sensing images. However, the current methods make insufficient use of the remote context information and channel information in shallow feature extraction, resulting in the limited effect of SR reconstruction. This article proposed a new SR reconstruction model, SIEGAN, which uses generative adversarial network with shallow information enhancement to improve the effect of SR reconstruction of remote sensing images. Similar to other generative adversarial models, SIEGAN is composed of generator and discriminator. But SIEGAN enhances the generator's ability to extract shallow information by using three different scale convolution operations. Specifically, a depthwise convolution is used to extract the local context information of each band of the image. A depthwise dilation convolution is used to capture the remote context information in the image. Finally, a 1×1 convolution is used to extract the correlation features between different channels in remote sensing images. In addition, SIEGAN uses U-Net network as its discriminator to provide detailed feedback per pixel to the generator to improve the model's ability to identify image details. And the spectral-spatial total variation loss function is introduced to ensure the spectral-spatial reliability of the reconstructed images. The experimental results on Gaofen-1 data proved that compared with the state-of-the-art models, SIEGAN has achieved better SR reconstruction performance. Furthermore, the reconstructed images by SIEGAN demonstrate better performance in land cover classification.  © 2008-2012 IEEE.","Convolution; Data mining; Deep learning; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Information use; Optical resolving power; Remote sensing; Semantics; Brain modeling; Context information; Features extraction; Images reconstruction; Multi-scale shallow information; Multi-scales; Remote sensing images; Remote-sensing; Super-resolution reconstruction; Superresolution; image classification; image resolution; network analysis; numerical model; remote sensing; satellite data; satellite imagery; spatial resolution; Image reconstruction","Generative adversarial network (GAN); multiscale shallow information; remote sensing images; super-resolution (SR) reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139491442"
"Radoi A.","Radoi, Anamaria (55544717700)","55544717700","Multimodal Satellite Image Time Series Analysis Using GAN-Based Domain Translation and Matrix Profile","2022","Remote Sensing","14","15","3734","","","","10.3390/rs14153734","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137105668&doi=10.3390%2frs14153734&partnerID=40&md5=49040114ac629c520d0c8c9d5ca43ea8","The technological development of the remote sensing domain led to the acquisition of satellite image time series (SITS) for Earth Observation (EO) by a variety of sensors. The variability in terms of the characteristics of the satellite sensors requires the existence of algorithms that allow the integration of multiple modalities and the identification of anomalous spatio-temporal evolutions caused by natural hazards. The unsupervised analysis of multimodal SITS proposed in this paper follows a two-step methodology: (i) inter-modality translation and (ii) the identification of anomalies in a change-detection framework. Inter-modality translation is achieved by means of a Generative Adversarial Network (GAN) architecture, whereas, for the identification of anomalies caused by natural hazards, we adapt the task to a similarity search in SITS. In this regard, we provide an extension of the matrix profile concept, which represents an answer to identifying differences and to discovering novelties in time series. Furthermore, the proposed inter-modality translation allows the usage of standard unsupervised clustering approaches (e.g., K-means using the Dynamic Time Warping measure) for mono-modal SITS analysis. The effectiveness of the proposed methodology is shown in two use-case scenarios, namely flooding and landslide events, for which a joint acquisition of Sentinel-1 and Sentinel-2 images is performed. © 2022 by the author.","Change detection; Disasters; Hazards; K-means clustering; Modal analysis; Remote sensing; Satellites; Time series analysis; Change detection; Dynamic time warping; Image time-series; Inter-modality translation; Intermodality; matrix; Matrix profile; Multi-modal; Multimodal satellite image time series analyse; Satellite images; Time-series analysis; Generative adversarial networks","change detection; dynamic time warping; generative adversarial networks; inter-modality translation; matrix profile; multimodal SITS analysis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137105668"
"Park J.-E.; Choi Y.-J.; Jeong J.; Hong S.","Park, Jeong-Eun (57209229613); Choi, Yun-Jeong (57450795100); Jeong, Jaehoon (58017246600); Hong, Sungwook (55817600100)","57209229613; 57450795100; 58017246600; 55817600100","Hypothetical Cirrus Band Generation for Advanced Himawari Imager Sensor Using Data-to-Data Translation With Advanced Meteorological Imager Observations","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","356","368","12","10.1109/JSTARS.2022.3224911","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144070958&doi=10.1109%2fJSTARS.2022.3224911&partnerID=40&md5=5a99b26e3a08bce11a9ef3bbf218916a","Cirrus cloud contributes significantly to earth's radiation budget and the greenhouse effect. The Advanced Himawari Imager (AHI) onboard the Himawari-8 satellite lacks a 1.37 μm band, sensitive to monitoring cirrus clouds. This article proposed a conditional generative adversarial network-based data-to-data translation (D2D) model to generate a hypothetical AHI 1.37 μm band. For training and testing the D2D model, the Geo-Kompsat-2A Advanced Meteorological Imager (AMI) 1.37 μm bands and other highly correlated bands to cirrus from July 24, 2019 to July 31, 2020, were used. The D2D model exhibited a high level of agreement (mean of statistics: correlation coefficient (CC) = 0.9827, bias = 0.0004, and root-mean-square error (RMSE) = 0.0086 in albedo units) between the observed and D2D-generated AMI 1.37 μm bands from validation datasets. The application of the D2D model to the AHI sensor showed that the D2D-generated AHI 1.37 μm band was qualitatively analogous to the observed AMI 1.37 μm band (average of statistics: bias = 0.0026, RMSE = 0.0191 in albedo units, and CC = 0.9158) on the 1st, 15th, and 28th of each month of 2020 in the common observing regions between Korea and Japan. The validation results with the CALIPSO data also showed that the D2D-generated AHI 1.37 μm band performed similarly to the observed AMI 1.37 μm band. Consequently, this article can significantly contribute to cirrus detection and its application to climatology.  © 2008-2012 IEEE.","Japan; Korea; Budget control; Clouds; Greenhouse effect; Remote sensing; Solar radiation; 1.37 μm; CGAN; Cirrus; Cirrus clouds; Correlation coefficient; Data translations; Data-to-data translation; Geo-kompsat-2a; Himawari; Satellite remote sensing; CALIPSO; cirrus; climatology; cloud radiative forcing; data processing; error analysis; greenhouse effect; image analysis; model test; model validation; radiation budget; remote sensing; satellite data; satellite imagery; Mean square error","1.37 μm; CGAN; cirrus; data-to-data translation; geo-kompsat-2A; Himawari; satellite remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144070958"
"Chen T.; Wang S.; Gao T.; Liu M.; Chen Y.","Chen, Ting (55687638000); Wang, Songtao (57287308700); Gao, Tao (57202469695); Liu, Mengni (57216646395); Chen, Youjing (57560759200)","55687638000; 57287308700; 57202469695; 57216646395; 57560759200","A Relativistic Average Generative Adversarial Network for Pan-Sharpening; [用于全色锐化的相对平均生成对抗网络]","2022","Hsi-An Chiao Tung Ta Hsueh/Journal of Xi'an Jiaotong University","56","3","","54","64","10","10.7652/xjtuxb202203006","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127499207&doi=10.7652%2fxjtuxb202203006&partnerID=40&md5=e4234d8eab146196a5a3fd0d2ecdc6dc","A relativistic average generative adversarial network for pan-sharpening (Pan-RaGAN) based on a deep learning algorithm is proposed to solve the problems that details of fusion result are easy to be lost due to insufficient feature extraction from original image in the pan-sharpening process and information redundancy is caused by ignoring spatial feature difference of different regions in image fusion process. Firstly, the improved dense block structure is utilized to extract the features of the original image in the generator, so as to make full use of the features from different layers of the original image and obtain better fusion results with more details. Secondly, a feature refinement module based on spatial attention mechanism is proposed for feature selection, which can make a better trade-off between retaining effective high-frequency information and eliminating redundant information. Furthermore, the image reconstruction module is utilized to fuse the refined features with the up-sampled low resolution multispectral images to preserve the spectral information. Finally, the relativistic average discriminator is utilized to improve the loss function of the network, and further optimize the fusion effect. Experimental results on Gao Fen-2 dataset and Quick Bird dataset and a comparison with the existing generative adversarial network for remote sensing image pan-sharpening show that the spectral angle mapper index of the proposed Pan-RaGAN network is reduced by 0.075 on average, which verifies the effectiveness of the proposed Pan-RaGAN network. © 2022, Editorial Office of Journal of Xi'an Jiaotong University. All right reserved.","Deep learning; Economic and social effects; Feature extraction; Image enhancement; Image fusion; Image reconstruction; Learning algorithms; Remote sensing; Attention mechanisms; Deep learning; Features extraction; Original images; Pan-sharpening; Process redundancy; Relativistic average generative adversarial network; Relativistics; Spatial attention; Spatial attention mechanism; Generative adversarial networks","Deep learning; Image fusion; Pan-sharpening; Relativistic average generative adversarial network; Spatial attention mechanism","Article","Final","","Scopus","2-s2.0-85127499207"
"Miranda M.; Drees L.; Roscher R.","Miranda, Miro (57763934800); Drees, Lukas (57201377002); Roscher, Ribana (37113871300)","57763934800; 57201377002; 37113871300","Controlled Multi-modal Image Generation for Plant Growth Modeling","2022","Proceedings - International Conference on Pattern Recognition","2022-August","","","5118","5124","6","10.1109/ICPR56361.2022.9956115","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143629403&doi=10.1109%2fICPR56361.2022.9956115&partnerID=40&md5=bf708b9a13188bbb0e1caa784da1f073","Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage. © 2022 IEEE.","Data fusion; Decision making; Farms; Forecasting; Generative adversarial networks; Remote sensing; Decisions makings; Generative model; Image generations; Latent vectors; Modelling techniques; Multimodal images; Plant development; Plant growth; Plant growth modeling; Precision-farming; Vector spaces","","Conference paper","Final","","Scopus","2-s2.0-85143629403"
"","","","3rd International Conference on Pattern Recognition and Artificial Intelligence, ICPRAI 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13363 LNCS","","","","","1216","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131957409&partnerID=40&md5=a4a39089ea2e34bc9dca38ee001fdf89","The proceedings contain 98 papers. The special focus in this conference is on Pattern Recognition and Artificial Intelligence. The topics include: Controlling the Quality of GAN-Based Generated Images for Predictions Tasks; deep Learning for Fast Segmentation of E-waste Devices’ Inner Parts in a Recycling Scenario; improving Semantic Segmentation with Graph-Based Structural Knowledge; application of Rail Segmentation in the Monitoring of Autonomous Train’s Frontal Environment; on the Feasibility and Generality of Patch-Based Adversarial Attacks on Semantic Segmentation Problems; structure-Aware Photorealistic Style Transfer Using Ghost Bottlenecks; unsupervised Cell Segmentation in Fluorescence Microscopy Images via Self-supervised Learning; hypercomplex Generative Adversarial Networks for Lightweight Semantic Labeling; learning Document Graphs with Attention for Image Manipulation Detection; is On-Line Handwriting Gender-Sensitive? What Tells us a Combination of Statistical and Machine Learning Approaches; using Convolutional Neural Network to Handle Word Shape Similarities in Handwritten Cursive Arabic Scripts of Pashto Language; discriminatory Expressions to Improve Model Comprehensibility in Short Documents; hologram Detection for Identity Document Authentication; multi-view Monocular Depth and Uncertainty Prediction with Deep SfM in Dynamic Environments; seeking Attention: Using Full Context Transformers for Better Disparity Estimation; inpainting Applied to Facade Images: A Comparison of Algorithms; personalized Frame-Level Facial Expression Recognition in Video; lateral Ego-Vehicle Control Without Supervision Using Point Clouds; visual Microfossil Identification via Deep Metric Learning; RDMMLND: A New Robust Deep Model for Multiple License Plate Number Detection in Video; from Synthetic to One-Shot Regression of Camera-Agnostic Human Performances; compositing Foreground and Background Using Variational Autoencoders; Remote Sensing Scene Classification Based on Covariance Pooling of Multi-layer CNN Features Guided by Saliency Maps; multi Layered Feature Explanation Method for Convolutional Neural Networks; QAP Optimisation with Reinforcement Learning for Faster Graph Matching in Sequential Semantic Image Analysis; bayesian Gate Mechanism for Multi-task Scale Learning; preface.","","","Conference review","Final","","Scopus","2-s2.0-85131957409"
"Liu Z.; Guan R.; Hu J.; Chen W.; Li X.","Liu, Zhaoyang (57640602500); Guan, Renxiang (57638563800); Hu, Jingyu (57639591300); Chen, Weitao (55822747800); Li, Xianju (41261664800)","57640602500; 57638563800; 57639591300; 55822747800; 41261664800","Remote Sensing Scene Data Generation Using Element Geometric Transformation and GAN-Based Texture Synthesis","2022","Applied Sciences (Switzerland)","12","8","3972","","","","10.3390/app12083972","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128825602&doi=10.3390%2fapp12083972&partnerID=40&md5=5d3f5df969d617098750533823d6ed51","Classification of remote sensing scene image (RSSI) has been broadly applied and has attracted increasing attention. However, scene classification methods based on convolutional neural networks (CNNs) require a large number of manually labeled samples as training data, which is time-consuming and costly. Therefore, generating labeled data becomes a practical approach. However, conventional scene generation based on generative adversarial networks (GANs) involve some significant limitations, such as distortion and limited size. To solve the mentioned problems, herein, we propose a method of RSSI generation using element geometric transformation and GAN-based texture synthesis. Firstly, we segment the RSSI, extracting the element information in the RSSI. Then, we perform geometric transformations on the elements and extract the texture information in them. After that, we use the GAN-based method to model and generate the texture. Finally, we fuse the transformed elements with the generated texture to obtain the generated RSSI. The geometric transformation increases the complexity of the scene. The GAN-based texture synthesis ensures the generated scene image is not distorted. Experimental results demonstrate that the RSSI generated by our method achieved a better visual effect than a GAN model. In addition, the performance of CNN classifiers was reduced by 0.44–3.41% on the enhanced data set, which is partly attributed to the complexity of the generated samples. The proposed method was able to generate diverse scene data with sufficient fidelity under conditions of small sample size and solve the accuracy saturation issues of the public scene data sets. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","classification; CNN; data generation; GAN; texture synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128825602"
"Singh A.; Bruzzone L.","Singh, Abhishek (57221148913); Bruzzone, Lorenzo (7006892410)","57221148913; 7006892410","Data Augmentation Through Spectrally Controlled Adversarial Networks for Classification of Multispectral Remote Sensing Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","651","654","3","10.1109/IGARSS46834.2022.9884928","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141896986&doi=10.1109%2fIGARSS46834.2022.9884928&partnerID=40&md5=8ceaa41e70af1117da230fb1f9c124f2","Availability of limited training remote sensing datasets is one of the problems in deep learning, as deep architectures require a large number of training samples for proper training. In this paper, we present a technique for data augmentation based on a spectral indexed generative adversarial network to train deep convolutional neural networks. This technique uses the spectral characteristic of multispectral (MS) images to support data augmentation in order to generate realistic training samples with respect to each land-use and land-cover class. The impact of multispectral remote sensing data generated through the spectral indexed GAN are evaluated through classification experiments. Experimental results obtained on the classification of the Sentinel-2 Eurosatallband datasets show that data augmentation through spectral indexed GAN enhances the main accuracy metrics. © 2022 IEEE.","Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image classification; Land use; Large dataset; Remote sensing; Sampling; Adversarial networks; Classification accuracy; Data augmentation; Generative model; Image-analysis; Multispectral remote sensing image; Remote sensing image analyse; Remote sensing images; Training data; Training sample; Classification (of information)","Classification Accuracy; Data Augmentation; Generative Model; Remote Sensing Image Analysis; Training Data","Conference paper","Final","","Scopus","2-s2.0-85141896986"
"Shaik A.L.H.P.; Manoharan M.K.; Pani A.K.; Avala R.R.; Chen C.-M.","Shaik, Abdul Lateef Haroon Phulara (58028368600); Manoharan, Monica Komala (56586072300); Pani, Alok Kumar (57194506472); Avala, Raji Reddy (58028527000); Chen, Chien-Ming (35072405000)","58028368600; 56586072300; 57194506472; 58028527000; 35072405000","Gaussian Mutation–Spider Monkey Optimization (GM-SMO) Model for Remote Sensing Scene Classification","2022","Remote Sensing","14","24","6279","","","","10.3390/rs14246279","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144615476&doi=10.3390%2frs14246279&partnerID=40&md5=d3585dfd43d21a344cc74a8bbb3087e6","Scene classification aims to classify various objects and land use classes such as farms, highways, rivers, and airplanes in the remote sensing images. In recent times, the Convolutional Neural Network (CNN) based models have been widely applied in scene classification, due to their efficiency in feature representation. The CNN based models have the limitation of overfitting problems, due to the generation of more features in the convolutional layer and imbalanced data problems. This study proposed Gaussian Mutation–Spider Monkey Optimization (GM-SMO) model for feature selection to solve overfitting and imbalanced data problems in scene classification. The Gaussian mutation changes the position of the solution after exploration to increase the exploitation in feature selection. The GM-SMO model maintains better tradeoff between exploration and exploitation to select relevant features for superior classification. The GM-SMO model selects unique features to overcome overfitting and imbalanced data problems. In this manuscript, the Generative Adversarial Network (GAN) is used for generating the augmented images, and the AlexNet and Visual Geometry Group (VGG) 19 models are applied to extract the features from the augmented images. Then, the GM-SMO model selects unique features, which are given to the Long Short-Term Memory (LSTM) network for classification. In the resulting phase, the GM-SMO model achieves 99.46% of accuracy, where the existing transformer-CNN has achieved only 98.76% on the UCM dataset. © 2022 by the authors.","Convolution; Convolutional neural networks; Feature Selection; Gaussian distribution; Generative adversarial networks; Land use; Long short-term memory; Remote sensing; Alexnet; Convolutional neural network; Gaussian mutation; Gaussian mutation–spider monkey optimization; Imbalanced data problems; Network-based modeling; Optimisations; Optimization models; Scene classification; VGG19; Classification (of information)","AlexNet; Gaussian Mutation–Spider Monkey Optimization; generative adversarial network; scene classification; VGG19","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144615476"
"Tang L.; Zhang H.; Xu H.; Ma J.","Tang, Linfeng (57223158028); Zhang, Hao (57215014270); Xu, Han (57201056465); Ma, Jiayi (26638975600)","57223158028; 57215014270; 57201056465; 26638975600","Deep learning-based image fusion: a survey; [基于深度学习的图像融合方法综述]","2023","Journal of Image and Graphics","28","1","","3","36","33","10.11834/jig.220422","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147204739&doi=10.11834%2fjig.220422&partnerID=40&md5=ef8fb9eb40bb95e22d8f40f5b432bba4","Image fusion aims to integrate complementary information from multi-source images into a single fused image to characterize the imaging scene and facilitate the subsequent vision tasks further. In recent years, it has been a concern in the field of image processing, especially in artificial intelligence-related industries like intelligent medical service, autonomous driving, smart photography, security surveillance, and military monitoring. Moreover, the growth of deep learning has been promoting deep learning-based image fusion algorithms. In particular, the emergence of advanced techniques, such as the auto-encoder, generative adversarial network, and Transformer, has led to a qualitative leap in image fusion performance. However, a comprehensive review and analysis of state-of-the-art deep learning-based image fusion algorithms for different fusion scenarios are required to be realized. Thus, we develop a systematic and critical review to explore the developments of image fusion in recent years. First, a comprehensive and systematic introduction of the image fusion field is presented from the following three aspects: 1) the development of image fusion technology, 2) the prevailing datasets, and 3) the common evaluation metrics. Then, more extensive qualitative experiments, quantitative experiments, and running efficiency evaluations of representative image fusion methods are conducted on the public datasets to compare their performance. Finally, the summary and challenges in the image fusion community are highlighted. In particular, some prospects are recommended further in the field of image fusion. First of all, from the perspective of fusion scenarios, the existing image fusion methods can be divided into three categories, i. e., multi-modal image fusion, digital photography image fusion, and remote sensing image fusion. Specifically, multi-modal image fusion is composed of infrared and visible image fusion as well as medical image fusion, and digital photography image fusion consists of multi-exposure image fusion as well as multi-focus image fusion. Multi-spectral and panchromatic image fusion can be as one of the key aspects for remote sensing image fusion. In addition, the domain of deep learning-based image fusion algorithms can be classified into the auto-encoder based (AE-based) fusion framework, convolutional neural network based (CNN-based) fusion framework, and generative adversarial network based (GAN-based) fusion framework from the aspect of network architectures. The AE-based fusion framework achieves the feature extraction and image reconstruction by a pre-trained auto-encoder, and accomplishes deep feature fusion via manual fusion strategies. To clarify feature extraction, fusion, and image reconstruction, the CNN-based fusion framework is originated from detailed network structures and loss functions. The GAN-based framework defines the image fusion problem as an adversarial game between the generators and discriminators. From the perspective of the supervision paradigm, the deep learning fusion methods can also be categorized into three classes, i. e., unsupervised, self-supervised, and supervised. The supervised methods leverage ground truth value to guide the training processes, and the unsupervised approaches construct loss function via constraining the similarity between the fusion result and source images. The self-supervised algorithms are associated with the AE-based framework in common. Our critical review is focused on the main concepts and discussions of the characteristics of each method for different fusion scenarios from the perspectives of the network architecture and supervision paradigm. Especially, we summarize the limitations of different fusion algorithms and provide some recommendations for further research. Secondly, we briefly introduce the popular public datasets and provide the interfaces-related to download them for each specific image fusion scenario. Then, we present the common evaluation metrics in the image fusion field from two aspects: regular-based evaluation metrics and specific-based metrics designed for pan-sharpening. The generic metrics can be utilized to evaluate multi-modal and digital photography image fusion algorithms of those are entropy-based, correlation-based, image feature-based, image structure-based, and human visual perception-based metrics in total. Some of the generic metrics, such as peak signal-to-noise ratio (PSNR), correlation coefficient (CC), structural similarity index measure (SSIM), and visual information fidelity (VIF), are also used for the quantitative assessment of pan-sharpening. The specific metrics designed for pan-sharpening consist of no-reference metrics and full-reference metrics that employ the full-resolution image as the reference image, i. e., ground truth. Thirdly, we present the qualitative/quantitative results, and average running times of representative alternatives for various fusion missions. Finally, this review has critically analyzed the conclusion, highlights the challenges in the image fusion community, and carried out forecasting analysis, such as non-registered image fusion, high-level vision task-driven image fusion, cross-resolution image fusion, real-time image fusion, color image fusion, image fusion based on physical imaging principles, image fusion under extreme conditions, and comprehensive evaluation metrics, etc. The methods, datasets, and evaluation metrics mentioned are linked at: https://github.com/Linfeng-Tang/Image-Fusion. © 2023 SAE-China. All rights reserved.","","deep learning; digital photography; image fusion; multi-modal; remote sensing imagery","Review","Final","","Scopus","2-s2.0-85147204739"
"Kussul N.; Shelestov A.; Yailymova H.; Shumilo L.; Drozd S.","Kussul, Nataliia (6602485938); Shelestov, Andrii (6507365226); Yailymova, Hanna (57202424721); Shumilo, Leonid (57208256914); Drozd, Sophia (57456870300)","6602485938; 6507365226; 57202424721; 57208256914; 57456870300","Agriculture Land Appraisal with Use of Remote Sensing and Infrastructure Data","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2785","2788","3","10.1109/IGARSS46834.2022.9884045","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140357841&doi=10.1109%2fIGARSS46834.2022.9884045&partnerID=40&md5=eb8b51ef384d51205e0438cb2db12049","1st July 2021 the law on the creation of land market start effect in Ukraine. As a result, land appraisal became cornerstone task in Ukrainian agriculture sector. The official methodology on land appraisal includes use of soil fertility characteristics combined with coefficients related to the distance to the infrastructure objects or settlements and placing of field in specific functional areas, like recreational, or areas with high level of radiation pollution. In this study we collected open source infostructure geospatial information and characteristics of fields obtained from remote sensing data-crop types and Normalized Difference Vegetation Index to build land price predictive model trained on the official land market information. This work designed to investigate potential of geo-informational technologies and remote sensing in the land appraisal use. We separated all available ground truth land price data into three groups by fields size-very small, small, medium and big. We found different relationships between field characteristics and prices. For very small fields the most important features are area, altitude, slope, bonitet and distances to elevators, villages and roads. For small fields the most important are bonitet, altitude, area and distances to cities and roads. For medium and big field's area, slope, distance to cities, roads and historical NDVI. © 2022 IEEE.","Agriculture; Commerce; Deep learning; Generative adversarial networks; Agriculture sectors; Deep learning; GAN; Land markets; Land prices; Remote-sensing; Sentinel-2; Soil fertility; Superresolution; Ukraine; Remote sensing","deep learning; GAN; Generative Adversarial Networks; Sentinel-2; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140357841"
"Xu M.; Deng F.; Jia S.; Jia X.; Plaza A.J.","Xu, Meng (57206827164); Deng, Furong (57226857111); Jia, Sen (7202859948); Jia, Xiuping (7201933692); Plaza, Antonio J. (7006613644)","57206827164; 57226857111; 7202859948; 7201933692; 7006613644","Attention mechanism-based generative adversarial networks for cloud removal in Landsat images","2022","Remote Sensing of Environment","271","","112902","","","","10.1016/j.rse.2022.112902","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123216163&doi=10.1016%2fj.rse.2022.112902&partnerID=40&md5=d96f3ff8fa2c337ce2014ba1e8c3dfde","The existence of clouds affects the quality of optical remote sensing images. Cloud removal is an important preprocessing procedure to effectively improve the utilization of optical remote sensing images. Thin clouds partly obscure the land surfaces beneath them, making it possible to correct the cloudy scenes according to the available information. In this research, we introduce the attention mechanism-based generative adversarial networks for cloud removal (AMGAN-CR) method for Landsat images. First, attention maps of the input cloudy images are generated to extract the cloud distributions and features through an attentive recurrent network. Second, clouds are removed by an attentive residual network under the guidance of the attention maps. Finally, the generated feature maps are fed to a reconstruction network to restore the final cloud-free images. The networks are trained by cloudy and cloud-free Landsat image pairs, and the cloudy images are tested to validate the effectiveness of AMGAN-CR. Both simulated and real cloud experimental results show that the proposed method is more outstanding than the other five state-of-the-art traditional and deep learning methods in removing cloud. © 2022 Elsevier Inc.","Deep learning; Image enhancement; Remote sensing; Attention mechanisms; Cloud distributions; Cloud removal; Generative adversarial network; Land surface; Landsat images; Mechanism-based; Optical remote sensing; Remote sensing images; Removal method; cloud; imagery; Landsat; machine learning; reconstruction; remote sensing; Generative adversarial networks","Attention mechanism; Cloud removal; Generative adversarial networks (GANs); Landsat images","Article","Final","","Scopus","2-s2.0-85123216163"
"Adamiak M.; Bȩdkowski K.; Bielecki A.","Adamiak, MacIej (57216301071); Bȩdkowski, Krzysztof (12780042300); Bielecki, Adam (58086530300)","57216301071; 12780042300; 58086530300","Generative Adversarial Approach to Urban Areas' NDVI Estimation: A Case Study of Łódź, Poland","2023","Quaestiones Geographicae","","","","87","106","19","10.14746/quageo-2023-0007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147221731&doi=10.14746%2fquageo-2023-0007&partnerID=40&md5=8af1528295e35c4ab36e6c47a4d76d0d","Generative adversarial networks (GAN) opened new possibilities for image processing and analysis. Inpainting, dataset augmentation using artificial samples, or increasing spatial resolution of aerial imagery are only a few notable examples of utilising GANs in remote sensing (RS). The normalised difference vegetation index (NDVI) ground-truth labels were prepared by combining RGB and NIR orthophotos. The dataset was then utilised as input for a conditional generative adversarial network (cGAN) to perform an image-to-image translation. The main goal of the neural network was to generate an artificial NDVI image for each processed 256 px × 256 px patch using only information available in the panchromatic input. The network achieved a structural similarity index measure (SSIM) of 0.7569 ± 0.1083, a peak signal-to-noise ratio (PSNR) of 26.6459 ± 3.6577 and a root-mean-square error (RSME) of 0.0504 ± 0.0193 on the test set, which should be considered high. The perceptual evaluation was performed to verify the method's usability when working with a real-life scenario. The research confirms that the structure and texture of the panchromatic aerial RS image contain sufficient information for NDVI estimation for various objects of urban space. Even though these results can highlight areas rich in vegetation and distinguish them from the urban background, there is still room for improvement regarding the accuracy of the estimated values. The research aims to explore the possibility of utilising GAN to enhance panchromatic images (PAN) with information related to vegetation. This opens exciting opportunities for historical RS imagery processing and analysis.  © 2023 Maciej Adamiak et al., published by Sciendo 2023.","","artificial datasets; generative adversarial networks; green areas; NDVI; orthophoto","Article","Article in press","","Scopus","2-s2.0-85147221731"
"Feng Q.; Chen B.; Li G.; Yao X.; Gao B.; Zhang L.","Feng, Quanlong (56417016300); Chen, Boan (57363506300); Li, Guoqing (56199992700); Yao, Xiaochuang (56085211600); Gao, Bingbo (36246121200); Zhang, Lianchong (57191676269)","56417016300; 57363506300; 56199992700; 56085211600; 36246121200; 57191676269","A review for sample datasets of remote sensing imagery; [遥感影像样本数据集研究综述]","2022","National Remote Sensing Bulletin","26","4","","589","605","16","10.11834/jrs.20221162","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135845410&doi=10.11834%2fjrs.20221162&partnerID=40&md5=bfb3287c5409e3a96cc2de37b0070ea5","With the rapid development of artificial intelligence technology such as machine learning and deep learning in remote sensing, data-driven models have become a new research paradigm for automatic information retrieval from remote sensing imagery, calling for higher requirements for the quantity, quality, and diversity of sample datasets. Before the era of deep learning, because classical machine learning methods (e.‍g., support vector machine and random forest) do not require huge numbers of samples for model training, the previously published sample datasets usually have a relatively small size (i.e., less than 100). In recent years, with the rapid development of technologies such as big data, parallel computing, and deep learning, many scholars and research institutions have issued a series of sample datasets, laying a solid foundation for a wide range of research and applications such as scene understanding, semantic segmentation, and object detection from remote sensing images. However, comprehensive review of the recently published sample datasets for remote sensing image analysis under the context of big data and deep learning remains lacking. Therefore, the objective of this study is to summarize and analyze these datasets to provide a valuable data reference for relevant researchers.On the basis of literature retrieval and analysis, this paper summarized a total of 124 widely used, open access, and influential remote sensing image sample datasets that were published between 2001 and 2020.We reviewed and summarized the development of recently published sample datasets for remote sensing imagery based on metadata analysis from the following aspects, such as data sources, application fields, keywords, and data size. Afterward, we analyzed these sample datasets from the perspective of spatial, spectral, and temporal resolutions. We listed the commonly used deep learning models (e.g., convolutional neural networks, recurrent neural networks, and generative adversarial networks) in the remote sensing field to show how these sample datasets could be used. We also divided the remote sensing image sample datasets into eight categories based on the following application fields: scene recognition, land cover/land use classification, thematic information extraction, change detection, ground-object detection, semantic segmentation, quantitative remote sensing, and other applications. The typical datasets and related research progress were carefully reviewed for each application field. In addition, because deep learning models are data-hungry, how to train a model with good generalization capability under limited labeled data has become a significant issue, especially for remote sensing applications given that obtaining sufficient labeled samples is time-consuming. To address this issue, we discussed several methods that could increase the model's generalization capability, including sample transfer between spatio-temporal domains, few-shot learning, and zero-shot learning, active learning, and semi-supervised learning for sample discovery, as well as sample generation through generative adversarial networks.By means of multi-dimensional analysis, we give a comprehensive overview of remote sensing image sample datasets. To the best of our knowledge, this paper is the first review of remote sensing image sample datasets for deep learning, potentially providing data reference for researchers in related fields. © 2022, Science Press. All right reserved.","Big data; Classification (of information); Convolutional neural networks; Decision trees; Generative adversarial networks; Information retrieval; Learning systems; Metadata; Object recognition; Recurrent neural networks; Remote sensing; Semantic Segmentation; Semantics; Support vector machines; Volcanoes; Application fields; Deep learning; Generalization capability; Learning models; Machine-learning; Objects detection; Remote sensing imagery; Remote sensing images; Sample dataset; Semantic segmentation; Object detection","Deep learning; Machine learning; Remote sensing imagery; Sample datasets","Article","Final","","Scopus","2-s2.0-85135845410"
"Newton D.W.","Newton, David William (57204544406)","57204544406","Identifying correlations between depression and urban morphology through generative deep learning","2022","International Journal of Architectural Computing","","","","","","","10.1177/14780771221089885","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130877160&doi=10.1177%2f14780771221089885&partnerID=40&md5=1a00bee75664d813575d4b8a4d1a7504","Mental health disorders, such as depression, have been estimated to account for the largest proportion of global disease burden. Existing research has established significant correlations between the built environment and mental health. This research, however, has relied on traditional statistical methods that are not amenable to working with large remote sensing image-based datasets. This research addresses this challenge and contributes new knowledge and a novel method for using generative deep learning for urban analysis and synthesis tasks involving mental health. The research specifically investigates three mental state measures: depression, anxiety, and the perception of safety. The experimental results demonstrate the efficacy of the process—providing a new method to find correlational signals, while providing insights on the correlation between specific urban design features and the incidence of depression. © The Author(s) 2022.","Deep learning; Generative adversarial networks; Health; Large dataset; Morphology; Remote sensing; Built environment; Depression; Disease burdens; Generative deep learning; Health disorders; Image-based; Mental health; Novel methods; Remote sensing images; Urban morphology; Urban planning","depression; generative adversarial network; generative deep learning; urban planning","Article","Article in press","","Scopus","2-s2.0-85130877160"
"Hoque M.R.U.; Wu J.; Kwan C.; Koperski K.; Li J.","Hoque, Md Reshad Ul (57215344852); Wu, Jian (57193141747); Kwan, Chiman (7201421216); Koperski, Krzysztof (6603540174); Li, Jiang (56226550100)","57215344852; 57193141747; 7201421216; 6603540174; 56226550100","ArithFusion: An Arithmetic Deep Model for Temporal Remote Sensing Image Fusion","2022","Remote Sensing","14","23","6160","","","","10.3390/rs14236160","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143761796&doi=10.3390%2frs14236160&partnerID=40&md5=6d162c4fc285294f57b7c7e89e83f5f4","Different satellite images may consist of variable numbers of channels which have different resolutions, and each satellite has a unique revisit period. For example, the Landsat-8 satellite images have 30 m resolution in their multispectral channels, the Sentinel-2 satellite images have 10 m resolution in the pan-sharp channel, and the National Agriculture Imagery Program (NAIP) aerial images have 1 m resolution. In this study, we propose a simple yet effective arithmetic deep model for multimodal temporal remote sensing image fusion. The proposed model takes both low- and high-resolution remote sensing images at (Formula presented.) together with low-resolution images at a future time (Formula presented.) from the same location as inputs and fuses them to generate high-resolution images for the same location at (Formula presented.). We propose an arithmetic operation applied to the low-resolution images at the two time points in feature space to take care of temporal changes. We evaluated the proposed model on three modality pairs for multimodal temporal image fusion, including downsampled WorldView-2/original WorldView-2, Landsat-8/Sentinel-2, and Sentinel-2/NAIP. Experimental results show that our model outperforms traditional algorithms and recent deep learning-based models by large margins in most scenarios, achieving sharp fused images while appropriately addressing temporal changes. © 2022 by the authors.","Antennas; Deep learning; Generative adversarial networks; Image fusion; Satellite imagery; Space optics; Deep learning; Generative adversarial network; HRNet; LANDSAT; Neural-networks; Remote sensing images; Remote-sensing; Satellite images; Superresolution; U-net; Remote sensing","deep learning; generative adversarial network (GAN); HRNet; image fusion; neural networks; remote sensing; super-resolution; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143761796"
"Niroshan L.; Carswell J.D.","Niroshan, Lasith (57195742111); Carswell, James D. (7005802453)","57195742111; 7005802453","Post-analysis of OSM-GAN Spatial Change Detection","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13238 LNCS","","","28","42","14","10.1007/978-3-031-06245-2_3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131141206&doi=10.1007%2f978-3-031-06245-2_3&partnerID=40&md5=eb200c2e2d2f45505439b0a7c7d887bb","Keeping crowdsourced maps up-to-date is important for a wide range of location-based applications (route planning, urban planning, navigation, tourism, etc.). We propose a novel map updating mechanism that combines the latest freely available remote sensing data with the current state of online vector map data to train a Deep Learning (DL) neural network. It uses a Generative Adversarial Network (GAN) to perform image-to-image translation, followed by segmentation and raster-vector comparison processes to identify changes to map features (e.g. buildings, roads, etc.) when compared to existing map data. This paper evaluates various GAN models trained with sixteen different datasets designed for use by our change detection/map updating procedure. Each GAN model is evaluated quantitatively and qualitatively to select the most accurate DL model for use in future spatial change detection applications. © 2022, Springer Nature Switzerland AG.","Computer software maintenance; Deep learning; Generative adversarial networks; Geographic information systems; Image segmentation; Change detection; Location-based applications; Map updating; Network models; Openstreetmap; Post analysis; Remote-sensing; Route planning; Spatial change detection; Spatial changes; Remote sensing","Generative Adversarial Networks; OpenStreetMap; Remote sensing; Spatial change detection","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131141206"
"Kang X.; Zhang X.","Kang, Xu (57670876000); Zhang, Xiaofeng (57774127200)","57670876000; 57774127200","Radar Remote Sensing Data Augmentation Method Based on Generative Adversarial Network; [基于生成对抗神经网络的雷达遥感数据增广方法]","2022","Xitong Fangzhen Xuebao / Journal of System Simulation","34","4","","920","927","7","10.16182/j.issn1004731x.joss.20-0953","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129617337&doi=10.16182%2fj.issn1004731x.joss.20-0953&partnerID=40&md5=00ee2474358566af584afc2e1ffa51c6","In the research field of radar remote sensing, both the completeness and diversity of radar data samples cannot meet the requirement of effective training of deep learning models, and the models are prone to over-fitting, which significantly limits the wide application of deep learning techniques in this field. Targeting on the needs of intelligent application in radar remote sensing, a microwave imaging radar suited data augmentation method is proposed to solve the issue of insufficient radar data samples by leveraging the general framework of generative adversarial network. Aiming at the features of radar samples being not obvious, the label smoothing regularization technique is utilized to automatically classify the augmentated radar samples. The augmentated samples together with the real samples are collaboratively used to implement the robust training of deep learning models. The proposed method is verified by the experiments based on the extensive open-sourse radar remote sensing data. © 2022, The Editorial Board of Journal of System Simulation. All right reserved.","","Data augmentation; Deep learning; Generative adversarial network; Radar remote sensing","Article","Final","","Scopus","2-s2.0-85129617337"
"Li J.; Zhang Y.; Sheng Q.; Wu Z.; Wang B.; Hu Z.; Shen G.; Schmitt M.; Molinier M.","Li, Jun (57790004000); Zhang, Yuejie (57927825700); Sheng, Qinghong (36562635800); Wu, Zhaocong (15023850900); Wang, Bo (57836499500); Hu, Zhongwen (55630272400); Shen, Guanting (57928295100); Schmitt, Michael (7401931279); Molinier, Matthieu (22234853700)","57790004000; 57927825700; 36562635800; 15023850900; 57836499500; 55630272400; 57928295100; 7401931279; 22234853700","Thin Cloud Removal Fusing Full Spectral and Spatial Features for Sentinel-2 Imagery","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8759","8775","16","10.1109/JSTARS.2022.3211857","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139828521&doi=10.1109%2fJSTARS.2022.3211857&partnerID=40&md5=caab2fe27943e37e465f7432d43cfda4","Multispectral remote sensing images are widely used for monitoring the globe. Although thin clouds can affect all optical bands, the influences of thin clouds differ with band wavelength. When processing multispectral bands at different resolutions, many methods only remove thin clouds in visible/near-infrared bands or rescale multiresolution bands to the same resolution and then process them together. The former cannot make full use of multispectral information, and in the latter, the rescaling process will introduce noise. In this article, a deep-learning-based thin cloud removal method that fuses full spectral and spatial features in original Sentinel-2 bands is proposed, named CR4S2. A multi-input and output architecture is designed for better fusing information in all bands and reconstructing the background at original resolutions. In addition, two parallel downsampling residual blocks are designed to transfer features extracted from different depths to the bottom of the network. Experiments were conducted on a new globally distributed Sentinel-2 thin cloud removal dataset called WHUS2-CRv. The results show that the best averaged peak signal-to-noise ratio, structural similarity index measurement, normalized root-mean-square error, and spectral angle mapper of the proposed method over 12 bands in all 20 testing images were 39.55, 0.9443, 0.0245, and 2.5676°, respectively. Compared with baseline methods, the proposed CR4S2 method can better restore not only the spatial features but also spectral features. This indicates that the proposed method is very promising for removing thin clouds in multispectral remote sensing images at different resolutions.  © 2008-2012 IEEE.","Distributed computer systems; Generative adversarial networks; Infrared devices; Optical remote sensing; Cloud removal; Cloud-computing; Deep learning; Features extraction; Multi-feature fusion; Parallel down-sample residual block; Remote-sensing; Sentinel-2; Spatial resolution; Thin cloud removal; Vegetation mapping; machine learning; multispectral image; satellite imagery; Sentinel; spatial analysis; spectral analysis; Deep learning","Deep learning (DL); multifeature fusion; parallel downsample residual block (PDRB); Sentinel-2; thin cloud removal","Article","Final","","Scopus","2-s2.0-85139828521"
"La Grassa R.; Gallo I.; Re C.; Cremonese G.; Landro N.; Pernechele C.; Simioni E.; Gatti M.","La Grassa, Riccardo (57204648786); Gallo, Ignazio (7003336792); Re, Cristina (6603585550); Cremonese, Gabriele (56240953900); Landro, Nicola (57214364871); Pernechele, Claudio (6701418383); Simioni, Emanuele (55598125000); Gatti, Mattia (57903868600)","57204648786; 7003336792; 6603585550; 56240953900; 57214364871; 6701418383; 55598125000; 57903868600","An Adversarial Generative Network Designed for High-Resolution Monocular Depth Estimation from 2D HiRISE Images of Mars","2022","Remote Sensing","14","18","4619","","","","10.3390/rs14184619","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138727611&doi=10.3390%2frs14184619&partnerID=40&md5=1278e98bd7b4ca15a47aceec717c5f90","In computer vision, stereoscopy allows the three-dimensional reconstruction of a scene using two 2D images taken from two slightly different points of view, to extract spatial information on the depth of the scene in the form of a map of disparities. In stereophotogrammetry, the disparity map is essential in extracting the digital terrain model (DTM) and thus obtaining a 3D spatial mapping, which is necessary for a better analysis of planetary surfaces. However, the entire reconstruction process performed with the stereo-matching algorithm can be time consuming and can generate many artifacts. Coupled with the lack of adequate stereo coverage, it can pose a significant obstacle to 3D planetary mapping. Recently, many deep learning architectures have been proposed for monocular depth estimation, which aspires to predict the third dimension given a single 2D image, with considerable advantages thanks to the simplification of the reconstruction problem, leading to a significant increase in interest in deep models for the generation of super-resolution images and DTM estimation. In this paper, we combine these last two concepts into a single end-to-end model and introduce a new generative adversarial network solution that estimates the DTM at 4× resolution from a single monocular image, called SRDiNet (super-resolution depth image network). Furthermore, we introduce a sub-network able to apply a refinement using interpolated input images to better enhance the fine details of the final product, and we demonstrate the effectiveness of its benefits through three different versions of the proposal: SRDiNet with GAN approach, SRDiNet without adversarial network, and SRDiNet without the refinement learned network plus GAN approach. The results of Oxia Planum (the landing site of the European Space Agency’s Rosalind Franklin ExoMars rover 2023) are reported, applying the best model along all Oxia Planum tiles and releasing a 3D product enhanced by (Formula presented.). © 2022 by the authors.","Deep learning; E-learning; Generative adversarial networks; Image enhancement; Image reconstruction; Mapping; Optical resolving power; Remote sensing; Three dimensional computer graphics; 2D images; 3-D mapping; Deep learning; Depth Estimation; Depth image; Digital terrain model; Mars; Remote-sensing; Satellite images; Superresolution; Stereo image processing","3D mapping; deep learning; digital terrain model; Mars; remote sensing; satellite images; super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138727611"
"Fu X.; Kouyama T.; Yang H.; Nakamura R.; Yoshikawa I.","Fu, Xuanchao (57937799100); Kouyama, Toru (16162149600); Yang, Hang (57482386400); Nakamura, Ryosuke (56711594400); Yoshikawa, Ichiro (35235779300)","57937799100; 16162149600; 57482386400; 56711594400; 35235779300","Toward Faster and Accurate Post-Disaster Damage Assessment: Development of End-to-End Building Damage Detection Framework with Super-Resolution Architecture","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1588","1591","3","10.1109/IGARSS46834.2022.9883317","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140394652&doi=10.1109%2fIGARSS46834.2022.9883317&partnerID=40&md5=88b4139b5055b2af62dffafb2b08454d","Building damage detection (BDD) with satellite images has been frequently adopted as an essential reference for post-disaster rescue, whereas its timeliness is significantly impacted by the long revisit time of high-resolution remote sensing satellites. Therefore, a reliable super-resolution method which is optimized for accurate and detail BDD is fundamental one for advancing the BDD analysis even when we can use only low-resolution images after a disaster. Based on Super-Resolution Generative Adversarial Network (SRGAN) and U-Net convolutional network, an efficient and novel BDD framework is proposed in this paper for obtaining upsampled BDD results from low-resolution post-disaster images. We trained the framework using two disasters from the xBD dataset and tested three different structures. The results show that our training structure based on an end-to-end framework successfully generated super-resolution BDD maps from low-resolution images, which performed significantly better than those from a two-stage training structure. © 2022 IEEE.","Boolean functions; Deep learning; Disasters; Generative adversarial networks; Optical resolving power; Remote sensing; Building damage; Damage assessments; Deep learning; Detection framework; End to end; End-to-end network; Low resolution images; Post disasters; Superresolution; XBD dataset; Damage detection","building damage; deep learning; end-to-end network; Super-resolution; xBD dataset","Conference paper","Final","","Scopus","2-s2.0-85140394652"
"Chen H.; Peng S.; Du C.; Li J.; Wu S.","Chen, Hao (57056710000); Peng, Shuang (57161760400); Du, Chun (50061333500); Li, Jun (57171589600); Wu, Songbing (57210638979)","57056710000; 57161760400; 50061333500; 57171589600; 57210638979","SW-GAN: Road Extraction from Remote Sensing Imagery Using Semi-Weakly Supervised Adversarial Learning","2022","Remote Sensing","14","17","4145","","","","10.3390/rs14174145","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137902235&doi=10.3390%2frs14174145&partnerID=40&md5=905d1a8f091f0ced55dcdc42ced20af8","Road networks play a fundamental role in our daily life. It is of importance to extract the road structure in a timely and precise manner with the rapid evolution of urban road structure. Recently, road network extraction using deep learning has become an effective and popular method. The main shortcoming of the road extraction using deep learning methods lies in the fact that there is a need for a large amount of training datasets. Additionally, the datasets need to be elaborately annotated, which is usually labor-intensive and time-consuming; thus, lots of weak annotations (such as the centerline from OpenStreetMap) have accumulated over the past a few decades. To make full use of the weak annotations, we propose a novel semi-weakly supervised method based on adversarial learning to extract road networks from remote sensing imagery. Our method uses a small set of pixel-wise annotated data and a large amount of weakly annotated data for training. The experimental results show that the proposed approach can achieve a maintained performance compared with the methods that use a large number of full pixel-wise annotations while using less fully annotated data. © 2022 by the authors.","Deep learning; Extraction; Feature extraction; Generative adversarial networks; Large dataset; Learning systems; Remote sensing; Roads and streets; Supervised learning; Adversarial learning; Daily lives; Large amounts; Openstreetmap; Remote sensing imagery; Road extraction; Road network; Road structures; Semi-weakly supervised learning; Weakly supervised learning; Pixels","Generative Adversarial Network; OpenStreetMap; road extraction; semi-weakly supervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137902235"
"Wu Y.; Feng S.; Huang M.","Wu, Yuanyuan (57255358200); Feng, Siling (55770169400); Huang, Mengxing (25623006400)","57255358200; 55770169400; 25623006400","An enhanced spatiotemporal fusion model with degraded fine-resolution images via relativistic generative adversarial networks","2022","Geocarto International","","","2153931","","","","10.1080/10106049.2022.2153931","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143628245&doi=10.1080%2f10106049.2022.2153931&partnerID=40&md5=d57d8174aeeca3ccb53b5e0e4f54d848","Limited by the sensor technology and budget, spatiotemporal fusion (STF) of remote sensing images (RSIs) is an effective strategy to obtain products with dense time and high spatial resolution. The STF methods generally require at least three observed images, and the predicted results depend on the reference date. The lack of high-quality fine-resolution images in practice makes the STF model difficult to apply, and few models consider the bias caused by different sensors. To solve the above problems, an enhanced STF model with degraded fine-resolution images via relativistic generative adversarial networks is proposed, called EDRGAN-STF, which is an end-to-end network with all bands trained simultaneously. To reduce the bias caused by different sensors, a degraded resolution version of the Landsat image with an arbitrary date is introduced into the STF model. The inputs of EDRGAN-STF only contain a MODIS image of the predicted date, a reference Landsat image and degraded version with an arbitrary date. EDRGAN-STF consists of a generator and a relativistic average least squares discriminator (RaLSD). A dual-stream residual dense block is designed to fully obtain the local and global spatial details and low-frequency information in the generator. A multihierarchical feature fusion block is designed to fuse global information. A spectral-spatial attention mechanism is employed in the generator, which focuses on important spectral bands and spatial features and enhances the reconstruction of critical regions. A new composite loss function is introduced to better optimize the designed STF model. To verify the capability of the EDRGAN-STF, extensive experiments are conducted on two typical Landsat-MODIS datasets, and results illustrate that EDRGAN-STF improves the STF accuracy and has great prospects for practical applications.Key policy highlights: The proposed EDRGAN-STF improves spatiotemporal fusion accuracy and has great prospects for practical applications. A degraded resolution version of the Landsat image with an arbitrary date is introduced into the spatiotemporal fusion model. A dual-stream residual dense block is designed to fully obtain the local and global spatial details and low-frequency information. A spectral-spatial attention mechanism focuses on important spectral bands and spatial features and enhances the reconstruction of critical regions. A multihierarchical feature fusion block fuses global information and a new composite loss function is introduced to better optimize the model. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","","dual-stream residual dense block; multihierarchical feature fusion; Relativistic average least squares generative adversarial networks (RaLSGAN); remote sensing images; spatiotemporal fusion","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85143628245"
"Zhao J.; Ni W.; Zhou Y.; Chen Y.; Yang Z.; Bian F.","Zhao, Jiaqi (57138970300); Ni, Wenxin (57658565900); Zhou, Yong (35480110700); Chen, Ying (57195284871); Yang, Zhi (57284929100); Bian, Fuqiang (57202867285)","57138970300; 57658565900; 35480110700; 57195284871; 57284929100; 57202867285","SAR-to-optical image translation by a variational generative adversarial network","2022","Remote Sensing Letters","13","7","","672","682","10","10.1080/2150704X.2022.2068986","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129334057&doi=10.1080%2f2150704X.2022.2068986&partnerID=40&md5=b6c0d4e8855f513209cfa182de91dd40","Due to all-weather and all-time work characteristics, synthetic aperture radar (SAR) images have been widely used in remote sensing. There is great difficulty in understanding SAR images because they are quite different from optical images in imaging mechanism, geometric characteristics, and radiation characteristics. It can greatly improve the readability of SAR images if we can translate them into optical image styles. In this paper, we propose a variational generative adversarial network for SAR images to optical images translation (S2O-VGAN). To demonstrate the validity of the proposed model, a new large-scale dataset called SARGB is proposed. Experimental results based on the proposed dataset with several evaluations show the superiority of the proposed model over the existing methods on SAR-optical image translation. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Generative adversarial networks; Geometrical optics; Image enhancement; Large dataset; Radar imaging; Remote sensing; Geometric characteristics; Image translation; Imaging mechanism; Large-scale datasets; Optical image; Radiation characteristics; Remote-sensing; Synthetic aperture radar images; image analysis; network analysis; remote sensing; satellite imagery; synthetic aperture radar; Synthetic aperture radar","generative adversarial network; image translation; optical images; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85129334057"
"Wu A.N.; Stouffs R.; Biljecki F.","Wu, Abraham Noah (57221912439); Stouffs, Rudi (6506322664); Biljecki, Filip (55609188900)","57221912439; 6506322664; 55609188900","Generative Adversarial Networks in the built environment: A comprehensive review of the application of GANs across data types and scales","2022","Building and Environment","223","","109477","","","","10.1016/j.buildenv.2022.109477","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136502357&doi=10.1016%2fj.buildenv.2022.109477&partnerID=40&md5=3f0020802352e664dc0cd638da286195","Generative Adversarial Networks (GANs) are a type of deep neural network that have achieved many state-of-the-art results for generative tasks. GANs can be useful in the built environment, from processing large-scale urban mobility data and remote sensing images at the regional level, to performance analysis and design generation at the building level. We analyzed 100 articles to provide a comprehensive state-of-the-art review on how GANs are currently applied to solve challenging tasks in the built environment. Our results show that: (i) GANs are replacing older methods in some problems and setting state-of-the-art performances; (ii) GANs are opening new frontiers in previously overlooked problems, such as automatically generating spatially accurate floorplan layouts; (iii) GANs can be applied to different scales in the built environment, from entire cities to neighborhoods and buildings; and (iv) GANs are being used in a variety of problems and data types, from remote sensing data augmentation, vector data generation, spatio-temporal data privacy protection, to building design generation. In total, there are 26 unique application domains enabled by GANs; (v) however, one common challenge in this field currently is the lack of high-quality datasets curated specifically for problems in the built environment. With more data in the future, GANs could potentially produce even better results than today. © 2022 Elsevier Ltd","Architectural design; Data privacy; Deep neural networks; Remote sensing; Built environment; Datatypes; Design generation; Generative design; GeoAI; Large-scales; Machine-learning; State of the art; Urban AI; Urban mobility; Generative adversarial networks","Generative design; GeoAI; Machine learning; Urban AI; Urban planning","Review","Final","","Scopus","2-s2.0-85136502357"
"Liu Y.; Al-Shehari H.; Zhang H.","Liu, Yan (57231599500); Al-Shehari, Hassan (57246896200); Zhang, Hongying (56070999800)","57231599500; 57246896200; 56070999800","Attention mechanism enhancement algorithm based on cycle consistent generative adversarial networks for single image dehazing","2022","Journal of Visual Communication and Image Representation","83","","103434","","","","10.1016/j.jvcir.2021.103434","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123022571&doi=10.1016%2fj.jvcir.2021.103434&partnerID=40&md5=54670742084df0cbd9a5bb6967b35f30","This paper proposes AMEA-GAN, an attention mechanism enhancement algorithm. It is cycle consistency-based generative adversarial networks for single image dehazing, which follows the mechanism of the human retina and to a great extent guarantees the color authenticity of enhanced images. To address the color distortion and fog artifacts in real-world images caused by most image dehazing methods, we refer to the human visual neurons and use the attention mechanism of similar Horizontal cell and Amazon cell in the retina to improve the structure of the generator adversarial networks. By introducing our proposed attention mechanism, the effect of haze removal becomes more natural without leaving any artifacts, especially in the dense fog area. We also use an improved symmetrical structure of FUNIE-GAN to improve the visual color perception or the color authenticity of the enhanced image and to produce a better visual effect. Experimental results show that our proposed model generates satisfactory results, that is, the output image of AMEA-GAN bears a strong sense of reality. Compared with state-of-the-art methods, AMEA-GAN not only dehazes images taken in daytime scenes but also can enhance images taken in nighttime scenes and even optical remote sensing imagery. © 2022 The Authors","Authentication; Color; Color vision; Deep learning; Demulsification; Image enhancement; Remote sensing; Attention mechanisms; Color distortions; Deep learning; Dehazing; Enhancement algorithms; Human retina; Human visual; Image dehazing; Real-world image; Single image dehazing; Generative adversarial networks","Attention mechanism; Deep learning; Generative adversarial networks; Image dehazing","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123022571"
"Jing H.; Shi J.; Qiu M.; Qi Y.; Zhu W.","Jing, Haizhao (57759652300); Shi, Jianglin (57207270235); Qiu, Mengzhe (57915825600); Qi, Yong (57760216500); Zhu, Wenxiao (57936774800)","57759652300; 57207270235; 57915825600; 57760216500; 57936774800","Super-resolution reconstruction method for space target images based on dense residual block-based GAN; [基于密集残差块生成对抗网络的空间目标图像超分辨率重建]","2022","Guangxue Jingmi Gongcheng/Optics and Precision Engineering","30","17","","2155","2165","10","10.37188/OPE.20223017.2155","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140292188&doi=10.37188%2fOPE.20223017.2155&partnerID=40&md5=5b447d74027034a3effeb28e9b256c36","To obtain the optical images of space targets with higher resolution and clarity， it is necessary to perform super-resolution reconstruction on the degraded images corrected by ground-based adaptive optics （AO） imaging telescopes. The image super-resolution reconstruction method based on deep learning has a fast operation speed and provides rich high-frequency detail information of the image； it has been widely used in natural， medical， and remote sensing images， among other applications. Aiming at the characteristics of spatial target AO images with a single background， limited resolution， motion blur， turbulent blur， and overexposure， this study proposes using a deep learning-based generative adversarial network （GAN） method to realize the super-resolution of spatial target AO images. For resolution reconstruction， a training set of spatial target AO simulation images is first constructed for neural network training， and a GAN super-resolution reconstruction method based on dense residual blocks is then proposed. By changing the traditional residual network to dense residual blocks， improving the network depth， and introducing a relative average loss function into the discriminator network， the discriminator becomes more robust， and the training of the generative adversarial network becomes more stable. Experiments show that the proposed method improves the peak-to-noise ratio （PSNR） and structural similarity index measure （SSIM） by more than 11.6% and 10.3%， respectively， compared with traditional interpolation super-resolution methods. In addition， it improves the PSNR and SSIM by 6.5% and 4.9% on average， respectively， compared with the deep learning-based blind image super-resolution method. The proposed method effectively realizes the clear reconstruction of a spatial target AO image， reduces the artifacts of the reconstructed image， enriches image details， and achieves a better reconstruction effect. © 2022 Guangxue Jingmi Gongcheng/Optics and Precision Engineering. All rights reserved.","Adaptive optics; Deep learning; Geometrical optics; Image enhancement; Image reconstruction; Learning systems; Medical imaging; Neural networks; Optical remote sensing; Optical resolving power; Dense residual block; Generative adversarial network（GAN）; Reconstruction method; Space object image; Space objects; Space targets; Super-resolution reconstruction; Superresolution; Superresolution methods; Target images; Generative adversarial networks","dense residual blocks; Generative Adversarial Network（GAN）; space object images; super resolution","Article","Final","","Scopus","2-s2.0-85140292188"
"Zhang C.; Zhang X.; Yu Q.; Ma C.","Zhang, Chengyao (57223862536); Zhang, Xuqing (36026110800); Yu, Qunfei (57965593700); Ma, Chenhao (57889659400)","57223862536; 36026110800; 57965593700; 57889659400","An Improved Method for Removal of Thin Clouds in Remote Sensing Images by Generative Adversarial Network","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","6706","6709","3","10.1109/IGARSS46834.2022.9884704","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141897887&doi=10.1109%2fIGARSS46834.2022.9884704&partnerID=40&md5=87250539d64536e85199ed697f545b50","Thin clouds will attenuate surface radiation, causing color distortion in remote sensing images and loss of detailed information. Aiming at the color cast phenomenon of the existing thin cloud removal methods, this paper proposes a remote sensing image thin cloud removal method based on generative adversarial networks with color consistency constraints. The RICE1 thin cloud dataset was used to verify the effectiveness of the method. CIEDE2000 was added to the evaluation index to measure the consistency of the thin cloud removal results and the color of the cloud-free image, and the reasonable value of the color constraint parameter lambda was explored. The experimental results show that the method in this paper is significantly better than homomorphic filtering, dark channel prior method and non-color-constrained generative adversarial network cloud removal method. © 2022 IEEE.","Color; Image enhancement; Remote sensing; CIEDE2000; Cloud removal; Color consistency; Color consistency constraint; Consistency constraints; Gan; Remote sensing images; Removal method; Surface radiation; Thin cloud removal; Generative adversarial networks","CIEDE2000; Color consistency constraint; Gan; Thin cloud removal","Conference paper","Final","","Scopus","2-s2.0-85141897887"
"Si L.; Huang T.; Wang X.; Yao Y.; Ma H.","Si, Lu (57216199813); Huang, Tongyu (57222518094); Wang, Xingjian (57220191157); Yao, Yue (57202003982); Ma, Hui (24321542200)","57216199813; 57222518094; 57220191157; 57202003982; 24321542200","Deep learning-based polarization feature retrieval from a single Stokes vector","2022","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11963","","1196307","","","","10.1117/12.2609583","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129136577&doi=10.1117%2f12.2609583&partnerID=40&md5=c891fbabdad41954e428ed4b01539620","Polarization is capable of probing microstructures and has unique sensitivity to fibrous anisotropic structure. Polarimetric imaging has demonstrated promising potential in diverse applications ranging from biomedicine, material science, and atmospheric remote sensing. The polarization properties of samples can be comprehensively described by a Mueller matrix (MM). However, the relationship between individual MM elements and properties of the sample is often not clear. There have been consistent efforts to derive polarization parameters from MM based on certain assumptions for better description of the samples, e.g., MM polar decomposition (MMPD), MM transformation (MMT) and MM differential decomposition. Usually, the MM imaging requires sequential measurements with different polarization states of incident light and the imaging process is time consuming. In addition, for movable samples, we cannot guarantee the consistency during the imaging. This may cause precision issues since the images cannot be well-registered. In this work, we built a statistical translation model to generate polarization parameters from a single Stokes vector which can be obtained by one-shot imaging. This will improve the imaging efficiency, simplify the optical system and avoid introducing errors by the image registration. In the model design, we adopted the generative adversarial network (GAN) where the generator is based on a U-net architecture. We demonstrated the effectiveness of our approach on liver tissue, blood smear and porous anodic alumina (PAA) film, and quantitatively evaluated the results by similarity assessment methods. The model can generate a parameter image within 0.1 second on a desktop computer, which shows the potential to achieve real-Time performance. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Alumina; Aluminum oxide; Deep learning; Generative adversarial networks; Image enhancement; Incident light; Matrix algebra; Optical systems; Polarimeters; Remote sensing; Anisotropic structure; Atmospheric remote sensing; Deep learning; Diverse applications; Feature retrieval; Material science; Mueller's matrices; Polarimetric imaging; Polarization parameters; Stokes vector; Polarization","Deep learning; Mueller matrix; Polarimetric imaging; Stokes vector","Conference paper","Final","","Scopus","2-s2.0-85129136577"
"","","","3rd International Conference on Pattern Recognition and Artificial Intelligence, ICPRAI 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13364 LNCS","","","","","1216","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132118666&partnerID=40&md5=76a6a694cfcb4226841aea48d235c274","The proceedings contain 98 papers. The special focus in this conference is on Pattern Recognition and Artificial Intelligence. The topics include: Controlling the Quality of GAN-Based Generated Images for Predictions Tasks; deep Learning for Fast Segmentation of E-waste Devices’ Inner Parts in a Recycling Scenario; improving Semantic Segmentation with Graph-Based Structural Knowledge; application of Rail Segmentation in the Monitoring of Autonomous Train’s Frontal Environment; on the Feasibility and Generality of Patch-Based Adversarial Attacks on Semantic Segmentation Problems; structure-Aware Photorealistic Style Transfer Using Ghost Bottlenecks; unsupervised Cell Segmentation in Fluorescence Microscopy Images via Self-supervised Learning; hypercomplex Generative Adversarial Networks for Lightweight Semantic Labeling; learning Document Graphs with Attention for Image Manipulation Detection; is On-Line Handwriting Gender-Sensitive? What Tells us a Combination of Statistical and Machine Learning Approaches; using Convolutional Neural Network to Handle Word Shape Similarities in Handwritten Cursive Arabic Scripts of Pashto Language; discriminatory Expressions to Improve Model Comprehensibility in Short Documents; hologram Detection for Identity Document Authentication; multi-view Monocular Depth and Uncertainty Prediction with Deep SfM in Dynamic Environments; seeking Attention: Using Full Context Transformers for Better Disparity Estimation; inpainting Applied to Facade Images: A Comparison of Algorithms; personalized Frame-Level Facial Expression Recognition in Video; lateral Ego-Vehicle Control Without Supervision Using Point Clouds; visual Microfossil Identification via Deep Metric Learning; RDMMLND: A New Robust Deep Model for Multiple License Plate Number Detection in Video; from Synthetic to One-Shot Regression of Camera-Agnostic Human Performances; compositing Foreground and Background Using Variational Autoencoders; Remote Sensing Scene Classification Based on Covariance Pooling of Multi-layer CNN Features Guided by Saliency Maps; multi Layered Feature Explanation Method for Convolutional Neural Networks; QAP Optimisation with Reinforcement Learning for Faster Graph Matching in Sequential Semantic Image Analysis; bayesian Gate Mechanism for Multi-task Scale Learning; preface.","","","Conference review","Final","","Scopus","2-s2.0-85132118666"
"Çolak E.; Sunar F.","Çolak, E. (57201377169); Sunar, F. (6603011829)","57201377169; 6603011829","A COMPARISON BETWEEN CYCLE-GAN BASED FEATURE TRANSLATION AND OPTICAL-SAR VEGETATION INDICES","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2022","","583","588","5","10.5194/isprs-archives-XLIII-B3-2022-583-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131951421&doi=10.5194%2fisprs-archives-XLIII-B3-2022-583-2022&partnerID=40&md5=f16cf94c6df99366775fc1ce81faa6bb","Optical and microwave remote sensing technologies have become key tools for local and global change detection applications. Generally, optical data has been the focus of remote sensing for change detection because of the varied spatial and temporal resolutions that allow for reliable information. However, the dependence of optical data on weather conditions prevents continuous and up-to-date information. On the other hand, Synthetic Aperture Radar (SAR) data can record all-weather and all-time polarization information which is critical for change detection in poor weather conditions; nonetheless, SAR is not precise as optical data for forestry change detection applications as it cannot provide the spectral features of interest. The combined processing of optical and SAR images allow for the retrieval of information of interest with a precision that none of them could achieve alone. In this context, Cycle-Consistent Generative Adversarial Networks (CycleGAN) based deep feature translation method was proposed in this study for change detection. The CycleGAN transfers images from one domain (optical) to another domain (SAR) into the same feature space using a cyclic structure. Thus, it can provide continuous and up-to-date information for change detection while keeping its spectral features. The accuracy of the fake images generated from CycleGAN was evaluated by correlating them with spectral indices (e.g., Normalized Difference Vegetation Index (NDVI), Modified Radar Vegetation Index (mRVI), and Modified Radar Forest Degradation Index (mRFDI)) directly obtained from optical and SAR data. As a result, the best correlation coefficients (R) were found between real NDVI (optical data) and fake NDVI (CycleGAN) with 0.98 and 0.97 for two different dated datasets.  © Authors 2022","Deep learning; Fake detection; Feature extraction; Forestry; Image enhancement; Meteorology; Optical correlation; Remote sensing; Space optics; Synthetic aperture radar; Vegetation; Change detection; Cycle-GAN; Deep learning; Feature translation; Normalized difference vegetation index; Optical data; Optical-; Radar data; Spectral feature; Vegetation index; Generative adversarial networks","Change Detection; Cycle-GAN; Deep Learning; Feature Translation; Vegetation Indices","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131951421"
"Guo M.; Zhang Z.; Liu H.; Huang Y.","Guo, Mingqiang (34872040100); Zhang, Zeyuan (57567072200); Liu, Heng (57022065500); Huang, Ying (57013696500)","34872040100; 57567072200; 57022065500; 57013696500","NDSRGAN: A Novel Dense Generative Adversarial Network for Real Aerial Imagery Super-Resolution Reconstruction","2022","Remote Sensing","14","7","1574","","","","10.3390/rs14071574","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127852163&doi=10.3390%2frs14071574&partnerID=40&md5=3101b5c6877bde898c09b50c7a40375b","In recent years, more and more researchers have used deep learning methods for super-resolution reconstruction and have made good progress. However, most of the existing super-resolution reconstruction models generate low-resolution images for training by downsampling high-resolution images through bicubic interpolation, and the models trained from these data have poor reconstruction results on real-world low-resolution images. In the field of unmanned aerial vehicle (UAV) aerial photography, the use of existing super-resolution reconstruction models in reconstructing real-world low-resolution aerial images captured by UAVs is prone to producing some artifacts, texture detail distortion and other problems, due to compression and fusion processing of the aerial images, thereby resulting in serious loss of texture detail in the obtained low-resolution aerial images. To address this problem, this paper proposes a novel dense generative adversarial network for real aerial imagery super-resolution reconstruction (NDSRGAN), and we produce image datasets with paired high-and low-resolution real aerial remote sensing images. In the generative network, we use a multilevel dense network to connect the dense connections in a residual dense block. In the discriminative network, we use a matrix mean discriminator that can discriminate the generated images locally, no longer discriminating the whole input image using a single value but instead in chunks of regions. We also use smoothL1 loss instead of the L1 loss used in most existing super-resolution models, to accelerate the model convergence and reach the global optimum faster. Compared with traditional models, our model can better utilise the feature information in the original image and discriminate the image in patches. A series of experiments is conducted with real aerial imagery datasets, and the results show that our model achieves good performance on quantitative metrics and visual perception. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Aerial photography; Antennas; Deep learning; Generative adversarial networks; Image reconstruction; Optical resolving power; Textures; Unmanned aerial vehicles (UAV); Aerial imagery; Aerial images; Deep learning; Down sampling; Learning methods; Low resolution images; Lower resolution; Real-world; Remote-sensing; Super-resolution reconstruction; Remote sensing","aerial imagery; deep learning; generative adversarial network; remote sensing; super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127852163"
"Shi Y.; Du L.; Guo Y.; Du Y.","Shi, Yu (57226794115); Du, Lan (56430356700); Guo, Yuchen (57226823334); Du, Yuang (57226186149)","57226794115; 56430356700; 57226823334; 57226186149","Unsupervised Domain Adaptation Based on Progressive Transfer for Ship Detection: From Optical to SAR Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5230317","","","","10.1109/TGRS.2022.3185298","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133619259&doi=10.1109%2fTGRS.2022.3185298&partnerID=40&md5=030809f18f488d518dbb417e31bb2e5d","In recent years, synthetic aperture radar (SAR) ship detection methods based on convolutional neural networks (CNNs) have attracted wide attention in remote sensing fields. However, these methods require a large number of labeled SAR images to train the network, where labeling for SAR images is more expensive and time-consuming than that for optical images. To address the problem of lacking labeled SAR images, in this article, we proposed an unsupervised domain adaptation (UDA) framework based on progressive transfer for SAR ship detection by transferring knowledge from the optical domain to the SAR domain. Due to the prominent difference between the optical and SAR images, our approach progressively transfers knowledge from three levels: pixel level, feature level, and prediction level. At the pixel level, considering the difference in imaging mechanism, we propose a special data augmentation method for ship targets and build the generator with skip connection based on generative adversarial networks (GANs) to generate transition domain, which can reduce the appearance discrepancy between the optical and SAR images. At the feature level, the detector is trained to learn the domain-invariant features with adversarial alignment. At the prediction level, we further use the predicted pseudo-labels obtained by the feature-aligned detector to learn more discriminative features of the SAR images directly and propose the robust self-training (RST) method to reduce the influence of noisy pseudo-labels on detector training. Specially, RST is formulated as a loss minimization problem for object detection. The experimental results based on the domain adaptation from optical dataset to SAR dataset demonstrate that our approach achieves superior SAR ship detection performance with unlabeled SAR images.  © 1980-2012 IEEE.","Adaptive optics; Feature extraction; Generative adversarial networks; Geometrical optics; Neural networks; Object detection; Object recognition; Optical remote sensing; Pixels; Radar imaging; Ships; Space-based radar; Tracking radar; Domain adaptation; Marine vehicles; Optical detectors; Optical imaging; Optical to synthetic aperture radar; Optical-; Radar polarimetry; Robust self-training; Self-training; Ship detection; Synthetic aperture radar; Unsupervised domain adaptation; artificial neural network; detection method; multispectral image; remote sensing; synthetic aperture radar; Synthetic aperture radar","Generative adversarial networks (GANs); optical to synthetic aperture radar (SAR); robust self-training (RST); SAR; ship detection; unsupervised domain adaptation (UDA)","Article","Final","","Scopus","2-s2.0-85133619259"
"Karatsiolis S.; Padubidri C.; Kamilaris A.","Karatsiolis, Savvas (55569345400); Padubidri, Chirag (57222362784); Kamilaris, Andreas (36189564000)","55569345400; 57222362784; 36189564000","Exploiting Digital Surface Models for Inferring Super-Resolution for Remotely Sensed Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4414213","","","","10.1109/TGRS.2022.3209340","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139526038&doi=10.1109%2fTGRS.2022.3209340&partnerID=40&md5=4a6e18e6e3566978fe6a4d1b304ca49f","Despite the plethora of successful super-resolution (SR) reconstruction (SRR) models applied to natural images, their application to remote sensing imagery tends to produce poor results. Remote sensing imagery is often more complicated than natural images, has its peculiarities such as being of lower resolution, contains noise, and often depicts large textured surfaces. As a result, applying nonspecialized SRR models like the enhanced SR generative adversarial network (ESRGAN) on remote sensing imagery results in artifacts and poor reconstructions. To address these problems, we propose a novel strategy for enabling an SRR model to output realistic remote sensing images: Instead of relying on feature-space similarities as a perceptual loss, the model considers pixel-level information inferred from the normalized digital surface model (nDSM) of the image. This allows the application of better-informed updates during the training of the model which sources from a task (elevation map inference) that is closely related to remote sensing. Nonetheless, the nDSM auxiliary information is not required during production, i.e., the model infers an SR image without additional data. We assess our model on two remotely sensed datasets of different spatial resolutions that also contain the DSMs of the images: The Data Fusion 2018 Contest (DFC2018) dataset and the dataset containing the national LiDAR flyby of Luxembourg. We compare our model with ESRGAN, and we show that it achieves better performance and does not introduce any artifacts in the results. In particular, the results for the high-resolution DFC2018 dataset are realistic and almost indistinguishable from the ground-truth images.  © 1980-2012 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; Job analysis; Optical resolving power; Personnel training; Deep learning; Digital surface models; Images reconstruction; Loss measurement; Normalized digital surface model; Perceptual loss; Remote-sensing; Spatial resolution; Super-resolution reconstruction; Superresolution; Task analysis; pixel; remote sensing; satellite imagery; spatial resolution; Remote sensing","Deep learning (DL); normalized digital surface model (nDSM); perceptual loss; remote sensing; super-resolution (SR) reconstruction (SRR)","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139526038"
"Wang D.; Zhao F.; Yi H.; Li Y.; Chen X.","Wang, Decheng (57209655296); Zhao, Feng (57225866170); Yi, Hui (57762807400); Li, Yinan (57762565600); Chen, Xiangning (24079871800)","57209655296; 57225866170; 57762807400; 57762565600; 24079871800","An unsupervised heterogeneous change detection method based on image translation network and post-processing algorithm","2022","International Journal of Digital Earth","15","1","","1056","1080","24","10.1080/17538947.2022.2092658","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132742577&doi=10.1080%2f17538947.2022.2092658&partnerID=40&md5=a4fb1113f95d4fbe4af286e28ac967d0","The change detection (CD) of heterogeneous remote sensing images is an important but challenging task. The difficulty is to obtain the change information by directly comparing the different statistical characteristics of the images acquired by different sensors. This paper proposes an unsupervised method for heterogeneous image CD based on an image domain transfer network. First, an attention mechanism is added to the Cycle-generative adversarial networks (Cycle-GANs) to obtain a more consistent feature expression by transferring bi-temporal heterogeneous images to the common domain. The Euclidean distance of the corresponding pixels is calculated in the common domain to form a difference map, and a threshold algorithm is applied to get a rough change map. Finally, the proposed adaptive Discrete Cosine Transform (DCT) algorithm reduces the noise introduced by false detection, and the final change map is obtained. The proposed method is verified on three real heterogeneous CD datasets and compared with the current state-of-the-art methods. The results show that the proposed method is accurate and robust for performing heterogeneous CD tasks. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Change detection; Discrete cosine transforms; Remote sensing; Attention mechanisms; Change detection; Cycle-generative adversarial network; Detection methods; Domain transfers; Heterogeneous image; Image translation; Network processing; Postprocessing algorithms; Unsupervised change detection; algorithm; detection method; heterogeneous medium; image analysis; image classification; remote sensing; unsupervised classification; Generative adversarial networks","attention mechanism; cycle-generative adversarial networks (Cycle-GANs); domain transfer; heterogeneous images; Unsupervised change detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132742577"
"Manuel C.; Zehnder P.; Kaya S.; Sullivan R.; Hu F.","Manuel, Cyrus (57925258900); Zehnder, Philip (57490388600); Kaya, Sertan (57202700520); Sullivan, Ruth (57754724200); Hu, Fangyao (46061200400)","57925258900; 57490388600; 57202700520; 57754724200; 46061200400","Impact of color augmentation and tissue type in deep learning for hematoxylin and eosin image super resolution","2022","Journal of Pathology Informatics","13","","100148","","","","10.1016/j.jpi.2022.100148","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139727952&doi=10.1016%2fj.jpi.2022.100148&partnerID=40&md5=a55bc91769a4518a653eab7f701a12d6","Single image super-resolution is an important computer vision task with applications including remote sensing, medical imaging, and surveillance. Modern work on super-resolution utilizes deep learning to synthesize high resolution (HR) images from low resolution images (LR). With the increased utilization of digitized whole slide images (WSI) in pathology workflows, digital pathology has emerged as a promising domain for super-resolution. Despite extensive existing research into super-resolution, there remain challenges specific to digital pathology. Here, we investigated image augmentation techniques for hematoxylin and eosin (H&E) WSI super-resolution and model generalizability across diverse tissue types. In addition, we investigated shortcomings with common quality metrics (peak signal-to-noise ratio (PSNR), structure similarity index (SSIM)) by conducting a perceptual quality survey for super-resolved pathology images. High performing deep super-resolution models were used to generate 20X HR images from LR images (5X or 10X equivalent) for 11 different tissues and 30 human evaluators were asked to score the quality of the generated versus the ground truth 20X HR images. The scores given by a human rater and the PSNR or the SSIM were compared to investigate the correlation between model training parameters. We found that models trained on multiple tissues generalized better than those trained on a single tissue type. We also found that PSNR correlated with perceptual quality (R = 0.26) less accurately than did SSIM (R = 0.64), suggesting that the SSIM quality metric is insufficient. The methods proposed in this study can be used to virtually magnify H&E images with better perceptual quality than interpolation methods (i.e., bicubic interpolation) commonly implemented in digital pathology software. The impact of deep SISR methods is more notable when scaling to 4X is needed, such as in the case of super-resolving a low magnification WSI from 10X to 40X. © 2022 The Authors","","Artificial intelligence; Deep learning; Digital pathology; Generative adversarial networks; Histopathology; Image processing; Whole slide imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139727952"
"Gao H.; Zhao Y.; Guo P.; Sun Z.; Chen X.; Tang Y.","Gao, Han (56328820300); Zhao, Yang (57440639900); Guo, Peng (57221684706); Sun, Zihao (57440624400); Chen, Xiuwan (8505956600); Tang, Yunwei (35197211600)","56328820300; 57440639900; 57221684706; 57440624400; 8505956600; 35197211600","Cycle and Self-Supervised Consistency Training for Adapting Semantic Segmentation of Aerial Images","2022","Remote Sensing","14","7","1527","","","","10.3390/rs14071527","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127552318&doi=10.3390%2frs14071527&partnerID=40&md5=033b2de52eef0ae16c2d47f4a7a720e8","Semantic segmentation is a critical problem for many remote sensing (RS) image applications. Benefiting from large-scale pixel-level labeled data and the continuous evolution of deep neural network architectures, the performance of semantic segmentation approaches has been constantly improved. However, deploying a well-trained model on unseen and diverse testing environments remains a major challenge: a large gap between data distributions in train and test domains results in severe performance loss, while manual dense labeling is costly and not scalable. To this end, we proposed an unsupervised domain adaptation framework for RS image semantic segmentation that is both practical and effective. The framework is supported by the consistency principle, including the cycle consistency in the input space and self-supervised consistency in the training stage. Specifically, we introduce cycle-consistent generative adversarial networks to reduce the discrepancy between source and target distributions by translating one into the other. The translated source data then drive a pipeline of supervised semantic segmentation model training. We enforce consistency of model predictions across target image transformations in order to provide self-supervision for the unlabeled target data. Experiments and extensive ablation studies demonstrate the effectiveness of the proposed approach on two challenging benchmarks, on which we achieve up to 9.95% and 7.53% improvements in accuracy over the state-of-the-art methods, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Antennas; Deep neural networks; Digital storage; Generative adversarial networks; Network architecture; Remote sensing; Semantics; Space optics; Well testing; Aerial images; Critical problems; Domain adaptation; Image applications; Large-scales; Pixel level; Remote sensing images; Self-supervision; Semantic segmentation; Unsupervised domain adaptation; Semantic Segmentation","remote sensing image; self-supervision; semantic segmentation; unsupervised domain adaptation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127552318"
"Song Y.; Zhang H.; Huang H.; Zhang L.","Song, Yiyao (57221595735); Zhang, Hongyan (54954032600); Huang, He (57670565200); Zhang, Liangpei (8359720900)","57221595735; 54954032600; 57670565200; 8359720900","Remote Sensing Image Spatiotemporal Fusion via a Generative Adversarial Network With One Prior Image Pair","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5528117","","","","10.1109/TGRS.2022.3171331","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129574414&doi=10.1109%2fTGRS.2022.3171331&partnerID=40&md5=9e727c21ea45cae7ba89360b31f69013","Spatiotemporal fusion (STF) is an effective solution to promote the application of remote sensing images, given that the tradeoff between the temporal resolution and the spatial resolution is ubiquitous in the production of remote sensing images. However, cloud coverage makes it difficult to obtain dense cloud-free Landsat-Moderate Resolution Imaging Spectroradiometer (MODIS) image pairs on the timeline, which limits the application of existing STF methods. Considering the lack of prior image pairs and the huge spatial resolution gap between Landsat and MODIS images, this article presents a novel remote sensing image STF method based on a generative adversarial network to handle one Landsat-MODIS prior image pair case (OPGAN), which contains a generator and a discriminator simultaneously trained in a min-max game. OPGAN is built based on the STF observation model that learns the base information from the prior Landsat image and then captures temporal change (TC) information from a difference image constructed from MODIS images collected at times 1 and 2 and sensor difference information from the difference image between Landsat and MODIS images at time 1. They are combined together to reconstruct the Landsat image at time 2 at both high spatial and high temporal resolution. Moreover, a change loss is proposed to further improve the accuracy of TC prediction. Extensive experiments on the STF dataset illustrate that the proposed OPGAN method can obtain more accurate prediction of spatial information and TCs in the case of insufficient prior information.  © 1980-2012 IEEE.","Generative adversarial networks; Image fusion; Radiometers; Remote sensing; Satellite imagery; Change loss; Fusion methods; Image pairs; LANDSAT; One prior image pair; Remote sensing images; Remote-sensing; Spatial resolution; Spatio-temporal fusions; Temporal change; accuracy assessment; artificial neural network; Landsat; MODIS; remote sensing; satellite imagery; spatiotemporal analysis; Image resolution","Change loss; generative adversarial network (GAN); one prior image pair; spatiotemporal fusion (STF)","Article","Final","","Scopus","2-s2.0-85129574414"
"Li H.; Gu C.; Wu D.; Cheng G.; Guo L.; Liu H.","Li, Huihui (43561539300); Gu, Cang (57927481600); Wu, Dongqing (57433549100); Cheng, Gong (36801169800); Guo, Lei (56428255600); Liu, Hang (57191739243)","43561539300; 57927481600; 57433549100; 36801169800; 56428255600; 57191739243","Multiscale Generative Adversarial Network Based on Wavelet Feature Learning for SAR-to-Optical Image Translation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5236115","","","","10.1109/TGRS.2022.3211415","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139833567&doi=10.1109%2fTGRS.2022.3211415&partnerID=40&md5=84d096bfd0f9ba16f0f4ef0699545cd6","The synthetic aperture radar (SAR) system is a kind of active remote sensing, which can be carried on a variety of flight platforms and can observe the Earth under all-day and all-weather conditions, so it has a wide range of applications. However, the interpretation of SAR images is quite challenging and not suitable for nonexperts. In order to enhance the visual effect of SAR images, this article proposes a multiscale generative adversarial network based on wavelet feature learning (WFLM-GAN) to implement the translation from SAR images to optical images; the translated images not only retain the key content of SAR images but also have the style of optical images. The main advantages of this method over the previous SAR-to-optical image translation (S2OIT) methods are given as follows. First, the generator does not learn the mapping from SAR images to optical images directly but learns the mapping from SAR images to wavelet features and then reconstructs the gray-scale images to optimize the content, increasing the mapping relationships and helping to learn more effective features. Second, a multiscale coloring network based on detail learning and style learning is designed to further translate the gray-scale images into optical images, which makes the generated images have an excellent visual effect with details closer to real images. Extensive experiments on SAR image datasets in different regions and seasons demonstrate the superior performance of WFLM-GAN over the baseline algorithms in terms of structural similarity (SSIM), the peak signal-to-noise ratio (PSNR), the Frechet inception distance (FID), and the kernel inception distance (KID). Comprehensive ablation studies are also carried out to isolate the validity of each proposed component. Our codes will be available at https://github.com/G2022G/WFLM-GAN.  © 1980-2012 IEEE.","Geometrical optics; Image enhancement; Mapping; Optical remote sensing; Radar imaging; Synthetic aperture radar; Feature learning; Generative adversarial network; Image translation; Learn+; Multi-scales; Network-based; Optical image; SAR Images; SAR-to-optical image translation (S2OIT); Wavelet features; algorithm; remote sensing; satellite imagery; segmentation; signal-to-noise ratio; synthetic aperture radar; wavelet analysis; Generative adversarial networks","Generative adversarial network (GAN); multiscale; synthetic aperture radar (SAR)-to-optical image translation (S2OIT); wavelet feature","Article","Final","","Scopus","2-s2.0-85139833567"
"Ambudkar S.; Raj R.; Billa K.; Hukumchand R.","Ambudkar, Shravan (57937361900); Raj, Rahul (57937215100); Billa, Karthik (57937362000); Hukumchand, Richa (57937953900)","57937361900; 57937215100; 57937362000; 57937953900","Super-Resolution for Cross-Sensor Optical Remote Sensing Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1880","1883","3","10.1109/IGARSS46834.2022.9883182","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140403342&doi=10.1109%2fIGARSS46834.2022.9883182&partnerID=40&md5=cb1f1f61246b1ec07e09992c1e85c92f","Generative adversarial network (GAN) models are becoming popular in the field of remote sensing for generating high spatial resolution images from their low resolution versions. In this study, four models including two basic Super-resolution GAN models and two non-GAN Deep Learning models were trained and tested to achieve 2.5m, and 5m spatial resolution from their 10m spatial resolution satellite data. The comparison of results showed that the SRGAN model performed better than the other deep learning models. The performance metrics were also found to be consistent with available literature. © 2022 IEEE.","Deep learning; Image enhancement; Image resolution; Learning systems; Optical remote sensing; High spatial resolution images; Learning models; Lower resolution; Network models; Optical remote sensing; Remote sensing images; Remote-sensing; Resolution enhancement; Spatial resolution; Superresolution; Generative adversarial networks","generative adversarial network; resolution enhancement; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85140403342"
"Liang M.; Wang X.-L.","Liang, Min (57226533656); Wang, Xi-Li (36761950100)","57226533656; 36761950100","Semantic Segmentation Model for Remote Sensing Images Combining Super Resolution and Domain Adaption; [结合超分辨率和域适应的遥感图像语义分割方法]","2022","Jisuanji Xuebao/Chinese Journal of Computers","45","12","","2619","2636","17","10.11897/SP.J.1016.2022.02619","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144541942&doi=10.11897%2fSP.J.1016.2022.02619&partnerID=40&md5=c45068813edecd34a2af74c74190f28d","The semantic segmentation method based on convolutional neural network rely on supervised learning with ground truth, but it cannot be well generalized to unlabeled datasets with different sources. Unsupervised domain adaptation can solve the problem of inconsistent feature distribution between unlabeled target domain data and labeled source domain data. This is due to that remote sensing images are often come from different sources, they are variable in their spatial resolution and are influenced by different imaging regions, imaging conditions and imaging times. Even images from the same region may have large differences in spectral features. The generalization of the semantic segmentation model relies on the reduction of these inter-domain differences mentioned above. Therefore, unsupervised domain adaptation methods for remote sensing image should not only reduce the differences in features between domains, but also address the problem of different spatial resolutions. This paper designs a new end-to-end semantic segmentation deep network combined with image super resolution-Semantic Segmentation Model Combining Super Resolution and Domain Adaption, which can reduce the spatial resolution difference and feature distribution difference between the low spatial resolution source domain and high spatial resolution target domain remote sensing images, and accomplish the super-resolution task for the source domain and the domain adaptation semantic segmentation task for the target domain. The SSM-SRDA model consists of three parts one is Semantic Segmentation Network with Super Resolution (SSNSR), the second is Pixel-level Domain Discriminator (PDD), and the third is Output-space Domain Discriminator (ODD). SSNSR consists of a semantic segmentation network and a super-resolution network, which share the same feature extraction network. The super-resolution network generates a high-resolution synthetic image with target image style from a low-resolution source domain image, which can eliminate spatial resolution differences and style differences to help the feature extraction module learn the same features between the source and target domains. The Feature Affinity-Loss module enhances the learning of the semantic segmentation deep network by the feature maps with more detailed structural information obtained by super-resolution. The pixel-level domain discriminator is used to reduce the pixel-level feature differences between the high-resolution target domain and the synthetic image of the source domain. High-resolution source domain synthetic images with target domain style are generated by generative adversarial learning with the super-resolution network and participate in the training of the model as additional training data. The output-space domain discriminator reduces the feature differences between source and target domain images in the output space of the semantic segmentation network. Through the adversarial learning of the semantic segmentation network and the two discriminators, SSM-SRDA aligns the feature distribution of the source domain and the target domain at the input and output stages of the segmentation network, and can be applied to the target domain datas of more different sources through domain adaptation. It is a practical and more popular model. Experiments show that SSM-SRDA is superior to the existing domain adaptive semantic segmentation methods on four sets of remote sensing image data sets, and the intersection ratio is improved by 0.7%, 1.7%, 2.2% and 3.3%, respectively. © 2022, Science Press. All right reserved.","Convolutional neural networks; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Image resolution; Learning systems; Pixels; Remote sensing; Semantics; Space optics; Adversarial learning; Domain adaptation; Feature distribution; Pixel level; Remote sensing images; Segmentation models; Semantic segmentation; Spatial resolution; Superresolution; Target domain; Semantic Segmentation","Adversarial learning; Domain adaptation; Remote sensing image; Semantic segmentation; Super resolution","Article","Final","","Scopus","2-s2.0-85144541942"
"Cai F.; Wu K.; Jia H.; Wang F.","Cai, Feng (57888175100); Wu, Keyu (57324654500); Jia, Hecheng (57226296459); Wang, Feng (56459216100)","57888175100; 57324654500; 57226296459; 56459216100","Super Resolution of Airplane Target in Remote Sensing Images via A Multi-Degradation Model","2022","2022 IEEE 14th International Conference on Advanced Infocomm Technology, ICAIT 2022","","","","330","333","3","10.1109/ICAIT56197.2022.9862621","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137903222&doi=10.1109%2fICAIT56197.2022.9862621&partnerID=40&md5=c958b4d073294c3a993303cfbb590c6f","In this paper, we argue that the degradation model assumed by existing work on super-resolution of remote sensing images deviates from those in real-world images. To address the above problem, this article proposes a super-resolution reconstruction method for unpaired airplanes in remote sensing images, which consists of a model that simulates the multiple degradations in real-world scenarios and a super-resolution generative adversarial network that generates high-resolution images. Specifically, the novel degradation model can cover a wide range of degradations in natural scenes, which contains diverse factors of additive Gaussian noise, Poisson noise, Brown Gaussian noise, Gaussian blur, etc. Experiments are conducted on the airplane targets in the Gaofen challenge dataset, and the results demonstrate the superiority of our method compared to state-of-the-art methods, especially in raw remote sensing images with multiple noise and blur, which may be preferred in other practical satellite applications with harsh circumstances.  © 2022 IEEE.","Computer vision; Gaussian distribution; Gaussian noise (electronic); Generative adversarial networks; Optical resolving power; Remote sensing; Aircraft target in remote sensing image; Aircraft targets; Degradation model; Image super resolutions; Multiple degradation model; Multiple degradations; Remote sensing image super-resolution; Remote sensing images; Superresolution; Aircraft","aircraft targets in remote sensing images; multiple degradation model; remote sensing image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85137903222"
"Zhong J.; Li Y.; Xie W.; Lei J.; Jia X.","Zhong, Jiaping (57211922206); Li, Yunsong (55986546100); Xie, Weiying (56768656200); Lei, Jie (36663710700); Jia, Xiuping (7201933692)","57211922206; 55986546100; 56768656200; 36663710700; 7201933692","Multi-Prior Twin Least-Square Network for Anomaly Detection of Hyperspectral Imagery","2022","Remote Sensing","14","12","2859","","","","10.3390/rs14122859","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132783339&doi=10.3390%2frs14122859&partnerID=40&md5=870a00b9fc2a4081eb1b039d36789e81","Anomaly detection of hyperspectral imagery (HSI) identifies the very few samples that do not conform to an intricate background without priors. Despite the extensive success of hyperspectral interpretation techniques based on generative adversarial networks (GANs), applying trained GAN models to hyperspectral anomaly detection remains promising but challenging. Previous generative models can accurately learn the complex background distribution of HSI and typically convert the high-dimensional data back to the latent space to extract features to detect anomalies. However, both background modeling and feature-extraction methods can be improved to become ideal in terms of the modeling power and reconstruction consistency capability. In this work, we present a multi-prior-based network (MPN) to incorporate the well-trained GANs as effective priors to a general anomaly-detection task. In particular, we introduce multi-scale covariance maps (MCMs) of precise second-order statistics to construct multi-scale priors. The MCM strategy implicitly bridges the spectral-and spatial-specific information and fully represents multi-scale, enhanced information. Thus, we reliably and adaptively estimate the HSI label to alleviate the problem of insufficient priors. Moreover, the twin least-square loss is imposed to improve the generative ability and training stability in feature and image domains, as well as to overcome the gradient vanishing problem. Last but not least, the network, enforced with a new anomaly rejection loss, establishes a pure and discriminative background estimation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Clustering algorithms; Feature extraction; Generative adversarial networks; Image enhancement; Remote sensing; Spectroscopy; Unsupervised learning; Adversarial learning; Anomaly detection; Hyper-spectral imageries; HyperSpectral; Hyperspectral anomaly detection; Least Square; Multi-scale covariance map; Multi-scales; Network models; Square networks; Anomaly detection","adversarial learning; anomaly detection; hyperspectral imagery; least-square; multi-scale covariance map; unsupervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132783339"
"Jamali A.; Mahdianpari M.; Brisco B.; Mao D.; Salehi B.; Mohammadimanesh F.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939); Brisco, Brian (7003505161); Mao, Dehua (36483152900); Salehi, Bahram (36610817400); Mohammadimanesh, Fariba (56541784200)","56909712300; 57190371939; 7003505161; 36483152900; 36610817400; 56541784200","3DUNetGSFormer: A deep learning pipeline for complex wetland mapping using generative adversarial networks and Swin transformer","2022","Ecological Informatics","72","","101904","","","","10.1016/j.ecoinf.2022.101904","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141962954&doi=10.1016%2fj.ecoinf.2022.101904&partnerID=40&md5=fc8fdbb71f0f4c841e94888e1c23dc5d","Many ecosystems, particularly wetlands, are significantly degraded or lost as a result of climate change and anthropogenic activities. Simultaneously, developments in machine learning, particularly deep learning methods, have greatly improved wetland mapping, which is a critical step in ecosystem monitoring. Yet, present deep and very deep models necessitate a greater number of training data, which are costly, logistically challenging, and time-consuming to acquire. Thus, we explore and address the potential and possible limitations caused by the availability of limited ground-truth data for large-scale wetland mapping. To overcome this persistent problem for remote sensing data classification using deep learning models, we propose 3D UNet Generative Adversarial Network Swin Transformer (3DUNetGSFormer) to adaptively synthesize wetland training data based on each class's data availability. Both real and synthesized training data are then imported to a novel deep learning architecture consisting of cutting-edge Convolutional Neural Networks and vision transformers for wetland mapping. Results demonstrated that the developed wetland classifier obtained a high level of kappa coefficient, average accuracy, and overall accuracy of 96.99%, 97.13%, and 97.39%, respectively, for the data in three pilot sites in and around Grand Falls-Windsor, Avalon, and Gros Morne National Park located in Canada. The results show that the proposed methodology opens a new window for future high-quality wetland data generation and classification. The developed codes are available at https://github.com/aj1365/3DUNetGSFormer. © 2022 The Authors","Canada; Gros Morne National Park; Newfoundland; Newfoundland and Labrador; artificial neural network; climate change; machine learning; remote sensing; wetland","Convolutional neural networks; Deep learning; Generative adversarial network; Swin transformer; Vision transformers; Wetland mapping","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85141962954"
"Zhang Q.; Jia R.-S.; Li Z.-H.; Li Y.-C.; Sun H.-M.","Zhang, Qi (57367179500); Jia, Rui-Sheng (25927894300); Li, Zeng-Hu (57563441600); Li, Yong-Chao (57411683300); Sun, Hong-Mei (55729286100)","57367179500; 25927894300; 57563441600; 57411683300; 55729286100","Superresolution reconstruction of optical remote sensing images based on a multiscale attention adversarial network","2022","Applied Intelligence","52","15","","17896","17911","15","10.1007/s10489-022-03548-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127706645&doi=10.1007%2fs10489-022-03548-7&partnerID=40&md5=82bde35960d7e8ee7f087ef8569b915c","Due to the influence of imaging equipment and environmental conditions on optical remote sensing image acquisition, image resolution is generally low. Superresolution reconstruction technology is an important way to improve image quality. However, the existing optical remote sensing image superresolution reconstruction methods have some problems, such as insufficient feature extraction, blurred texture details of reconstructed images, and excessive network accumulation. To solve the above problems, a superresolution reconstruction method for optical remote sensing images based on a multiscale attention adversarial network is proposed in this paper. The method takes a generative adversarial network (GAN) as the basic framework. The generator uses four multiscale attention residual blocks (MSARBs) to extract image multiscale feature information and carries out feature fusion through a binary feature fusion structure (BFFS) to generate more realistic images. The discriminator uses a depth convolution network to distinguish the differences between real images and superresolution images. In the aspect of loss function construction, the perceptual loss and adversarial loss are combined to improve the perceptual quality of the images. Experimental results show that this method is superior to the compared algorithm in regard to the objective evaluation metrics of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), and its reconstructed images have better visual effect. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image acquisition; Image enhancement; Image fusion; Image quality; Image reconstruction; Image resolution; Optical remote sensing; Signal to noise ratio; Textures; Adversarial networks; Features fusions; Image-based; Multiscale attention residual network; Optical remote sensing; Optical remote sensing image; Reconstructed image; Reconstruction method; Remote sensing images; Super-resolution reconstruction; Generative adversarial networks","Generative adversarial network; Multiscale attention residual network; Optical remote sensing images; Superresolution reconstruction","Article","Final","","Scopus","2-s2.0-85127706645"
"Hosseinpour H.R.; Samadzadegan F.; Javan F.D.; Motayyeb S.","Hosseinpour, H.R. (57388603800); Samadzadegan, F. (55898340800); Javan, F. Dadrass (58080784400); Motayyeb, S. (57741132200)","57388603800; 55898340800; 58080784400; 57741132200","IMPROVING SEMANTIC SEGMENTATION OF HIGH-RESOLUTION REMOTE SENSING IMAGES USING WASSERSTEIN GENERATIVE ADVERSARIAL NETWORK","2023","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","48","4/W2-2022","","45","51","6","10.5194/isprs-archives-XLVIII-4-W2-2022-45-2023","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146946387&doi=10.5194%2fisprs-archives-XLVIII-4-W2-2022-45-2023&partnerID=40&md5=0bddfee803d6589222188338b497ef78","Semantic segmentation of remote sensing images with high spatial resolution has many applications in a wide range of problems in this field. In recent years, the use of advanced techniques based on fully convolutional neural networks have achieved high and impressive accuracies. However, the labels of different classes are estimated independently in this method. In general, the segmentation effect is too coarse to take the relationship between pixels into account. On the other hand, due to the use of convolution filters and limitations of calculations, the field of view information of these filters will be limited in deep layers. In this study, a method based on generative adversarial network (GAN) is proposed to strengthen spatial vicinity in the output segmentation map. The segmentation model receive assistance from the GAN model in the form of a higher order potential loss. Furthermore, for better stability and performance in model training the Wasserstein GAN is used for optimization of the model. We successfully show an increase in semantic segmentation accuracy using the challenging ISPRS Vaihingen benchmark dataset. © Author(s) 2023.","Convolution; Deep learning; Generative adversarial networks; Image enhancement; Remote sensing; Semantic Web; Semantics; Convolution filters; Convolutional neural network; Deep learning; Different class; Field of views; High spatial resolution; High-resolution remote sensing images; Remote sensing images; Semantic segmentation; Wasserstein generative adversarial network; Semantic Segmentation","Deep Learning; Generative Adversarial Network; Semantic Segmentation; Wasserstein GAN","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146946387"
"Kumar A.; Tamboli D.; Pande S.; Banerjee B.","Kumar, Advait (57226075045); Tamboli, Dipesh (57215861192); Pande, Shivam (57212166658); Banerjee, Biplab (55568183500)","57226075045; 57215861192; 57212166658; 55568183500","RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","143","146","3","10.1109/IGARSS46834.2022.9884330","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133381473&doi=10.1109%2fIGARSS46834.2022.9884330&partnerID=40&md5=c441a6d80a21548b27563e38e047909f","We tackle the problem of image inpainting in the remote sensing domain. Remote sensing images possess high resolution and geographical variations, that render the conventional inpainting methods less effective. This further entails the requirement of models with high complexity to sufficiently capture the spectral, spatial and textural nuances within an image, emerging from its high spatial variability. To this end, we propose a novel inpainting method that individually focuses on each aspect of an image such as edges, colour and texture using a task specific GAN. Moreover, each individual GAN also incorporates the attention mechanism that explicitly extracts the spectral and spatial features. To ensure consistent gradient flow, the model uses residual learning paradigm, thus simultaneously working with high and low level features. We evaluate our model, alongwith previous state of the art models, on the two well known remote sensing datasets, Open Cities AI and Earth on Canvas, and achieve competitive performance. The code can be referred here: https://github.com/advaitkumar3107/RSINet. © 2022 IEEE.","Computer vision; Remote sensing; Textures; Geographical variations; High complexity; High resolution; Image Inpainting; Inpainting; Inpainting method; Remote sensing images; Remote-sensing; Remotely sensed images; Resolution variations; Generative adversarial networks","generative adversarial networks; Image inpainting; remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133381473"
"","","","15th International Conference on Advanced Computer Theory and Engineering, ICACTE 2022","2022","15th International Conference on Advanced Computer Theory and Engineering, ICACTE 2022","","","","","","112","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143063668&partnerID=40&md5=329041e223543fc5f99d1e6b13cad216","The proceedings contain 19 papers. The topics discussed include: efficient computation offloading with energy consumption constraint for multi-cloud system; experimental implementation of quantum prisoner dilemma on IBM quantum computers; trash classification network based on attention mechanism; unsupervised image dehazing based on improved generative adversarial networks; few-shot remote sensing scene classification with multi-metric fusion; image extraction of thangka line drawings with transformer; textual adversarial attacks on named entity recognition in a hard label black box setting; matrix-based genetic algorithm for mobile robot global path planning; Chinese named entity recognition based on location information and word frequency; and a modular reasoning approach to knowledge graph.","","","Conference review","Final","","Scopus","2-s2.0-85143063668"
"Hu X.; Zhang P.; Ban Y.","Hu, Xikun (57223380183); Zhang, Puzhao (57190619273); Ban, Yifang (7202222338)","57223380183; 57190619273; 7202222338","Gan-based SAR to Optical Image Translation in Fire-Disturbed Regions","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1456","1459","3","10.1109/IGARSS46834.2022.9884234","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140393425&doi=10.1109%2fIGARSS46834.2022.9884234&partnerID=40&md5=c4d70c65dbb8e7913a15c0ab02a9331d","Climate change by anthropogenic warming leads to increases in dry fuels and promotes forest fires. Multispectral images' quality is easily affected by poor atmospheric conditions. SAR satellite sensors can penetrate through clouds and image day and night. However, the burned area mapping methods widely used for optical data are not feasible to be applied for SAR data owing to the differences in imaging mechanisms. Recent advances in deep image translation can fill this gap by using Generative Adversarial Networks (GAN). In this research, we apply a GAN-based model for SAR to optical image translation over fire-disturbed regions. Specifically, Sentinel-1 SAR images are translated into Sentinel-2 images using the ResNet-based Pix2Pix model, which is trained on 281 large fire events and tested on the other 23 events in Canada. The generated images preserve the spectral characteristics well and show high similarity to the real images with Structure Similarity Index Measure (SSIM) over 0.59. © 2022 IEEE.","Climate change; Deforestation; Fire hazards; Fires; Generative adversarial networks; Geology; Geometrical optics; Radar imaging; Remote sensing; Anthropogenic warming; C-bands; Dry fuels; GaN based; Generative adversarial network; Image translation; Optical image; Sentinel-1; Sentinel-1 C band SAR; Sentinel-2 MSI; Synthetic aperture radar","Generative Adversarial Network (GAN); image Translation; Sentinel-1 C band SAR; Sentinel-2 MSI","Conference paper","Final","","Scopus","2-s2.0-85140393425"
"Yang Y.; Lv Q.; Zhu B.; Sui X.; Zhang Y.; Tan Z.","Yang, Yuanbo (57879780700); Lv, Qunbo (55513035000); Zhu, Baoyu (57880361000); Sui, Xuefu (57879780600); Zhang, Yu (57211360027); Tan, Zheng (57188729245)","57879780700; 55513035000; 57880361000; 57879780600; 57211360027; 57188729245","One-Sided Unsupervised Image Dehazing Network Based on Feature Fusion and Multi-Scale Skip Connection","2022","Applied Sciences (Switzerland)","12","23","12366","","","","10.3390/app122312366","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143719352&doi=10.3390%2fapp122312366&partnerID=40&md5=f22c742cffb12043085e0c52a6157df3","Haze and mist caused by air quality, weather, and other factors can reduce the clarity and contrast of images captured by cameras, which limits the applications of automatic driving, satellite remote sensing, traffic monitoring, etc. Therefore, the study of image dehazing is of great significance. Most existing unsupervised image-dehazing algorithms rely on a priori knowledge and simplified atmospheric scattering models, but the physical causes of haze in the real world are complex, resulting in inaccurate atmospheric scattering models that affect the dehazing effect. Unsupervised generative adversarial networks can be used for image-dehazing algorithm research; however, due to the information inequality between haze and haze-free images, the existing bi-directional mapping domain translation model often used in unsupervised generative adversarial networks is not suitable for image-dehazing tasks, and it also does not make good use of extracted features, which results in distortion, loss of image details, and poor retention of image features in the haze-free images. To address these problems, this paper proposes an end-to-end one-sided unsupervised image-dehazing network based on a generative adversarial network that directly learns the mapping between haze and haze-free images. The proposed feature-fusion module and multi-scale skip connection based on residual network consider the loss of feature information caused by convolution operation and the fusion of different scale features, and achieve adaptive fusion between low-level features and high-level features, to better preserve the features of the original image. Meanwhile, multiple loss functions are used to train the network, where the adversarial loss ensures that the network generates more realistic images and the contrastive loss ensures a meaningful one-sided mapping from the haze image to the haze-free image, resulting in haze-free images with good quantitative metrics and visual effects. The experiments demonstrate that, compared with existing dehazing algorithms, our method achieved better quantitative metrics and better visual effects on both synthetic haze image datasets and real-world haze image datasets. © 2022 by the authors.","","contrastive learning; generative adversarial network; image dehazing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143719352"
"He L.; Zhang W.; Shi J.; Li F.","He, Lijun (57887558200); Zhang, Wanyue (57219787617); Shi, Jiankang (57887558300); Li, Fan (57888468700)","57887558200; 57219787617; 57887558300; 57888468700","Cross-Domain Association Mining Based Generative Adversarial Network for Pansharpening","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","7770","7783","13","10.1109/JSTARS.2022.3204824","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137849856&doi=10.1109%2fJSTARS.2022.3204824&partnerID=40&md5=461cb4f06f8186ace43da392250266df","Multispectral (MS) pansharpening can improve the spatial resolution of MS images, which plays an increasingly important role in agriculture and environmental monitoring. Existing neural network-based methods tend to focus on global features of images, without considering the inherent relationships between similar substances in MS images. However, there is a high probability that different substances at the junction mix with each other, which leads to spectral distortion in the final pansharpened image. In this article, we propose a cross-domain association mining-based generative adversarial network for pansharpening, which consists of a spectral fidelity generator and dual discriminators. In our spectral fidelity generator, the cross-region similarity attention module is designed to establish dependencies between similar substances at different positions in the image, thereby leveraging the similar spectral features to generate pansharpened images with better spectral preservation. To mine the potential relationship between the MS image domain and the panchromatic image domain, we pretrain a spatial information extraction network. The network is then transferred to the dual-discriminator architecture to obtain the spatial information of the pansharpened images more accurately and prevent the loss of spatial details. The experimental results show that our method outperforms several state-of-the-art pansharpening methods in both quantitative and qualitative evaluations. © 2008-2012 IEEE.","Deep learning; Discriminators; Image enhancement; Image resolution; Deep learning; Dual discriminator; Features extraction; Generator; Image association; Junction; Multi-spectral; Multispectral  pansharpening; Pan-sharpening; Spatial resolution; Superresolution; image classification; machine learning; network analysis; qualitative analysis; quantitative analysis; remote sensing; satellite data; satellite imagery; Generative adversarial networks","Deep learning; dual discriminators; image association; multispectral (MS) pansharpening","Article","Final","","Scopus","2-s2.0-85137849856"
"Buyukdemircioglu M.; Kocaman S.; Kada M.","Buyukdemircioglu, M. (57203863758); Kocaman, S. (22134793700); Kada, M. (14052416300)","57203863758; 22134793700; 14052416300","DEEP LEARNING FOR 3D BUILDING RECONSTRUCTION: A REVIEW","2022","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B2-2022","","359","366","7","10.5194/isprs-archives-XLIII-B2-2022-359-2022","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132027882&doi=10.5194%2fisprs-archives-XLIII-B2-2022-359-2022&partnerID=40&md5=559f5e257f73b05aeb89d15842bad11f","3D building reconstruction using Earth Observation (EO) data (aerial and satellite imagery, point clouds, etc.) is an important and active research topic in different fields, such as photogrammetry, remote sensing, computer vision and Geographic Information Systems (GIS). Nowadays 3D city models have become an essential part of 3D GIS environments and they can be used in many applications and analyses in urban areas. The conventional 3D building reconstruction methods depend heavily on the data quality and source; and manual efforts are still needed for generating the object models. Several tasks in photogrammetry and remote sensing have been revolutionized by using deep learning (DL) methods, such as image segmentation, classification, and 3D reconstruction. In this study, we provide a review on the state-of-the-art machine learning and in particular the DL methods for 3D building reconstruction for the purpose of city modelling using EO data. This is the first review with a focus on object model generation based on the DL methods and EO data. A brief overview of the recent building reconstruction studies with DL is also given. We have investigated the different DL architectures, such as convolutional neural networks (CNNs), generative adversarial networks (GANs), and the combinations of conventional approaches with DL in this paper and reported their advantages and disadvantages. An outlook on the future developments of 3D building modelling based on DL is also presented.  © 2022. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved. ","3D modeling; Antennas; Buildings; Convolutional neural networks; Deep learning; Generative adversarial networks; Geographic information systems; Image reconstruction; Image segmentation; Maintenance; Photogrammetry; Satellite imagery; Three dimensional computer graphics; 3-d building reconstruction; 3d city model; 3D reconstruction; City model; Deep learning; Earth observation data; Learning methods; LoD; Machine-learning; Remote-sensing; Remote sensing","3D City Models; 3D Reconstruction; Deep Learning; LoD; Machine Learning","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132027882"
"Liu P.; Li J.; Wang L.; He G.","Liu, Peng (57075315400); Li, Jun (24481713500); Wang, Lizhe (23029267900); He, Guojin (14028364400)","57075315400; 24481713500; 23029267900; 14028364400","Remote Sensing Data Fusion with Generative Adversarial Networks: State-of-the-art methods and future research directions","2022","IEEE Geoscience and Remote Sensing Magazine","10","2","","295","328","33","10.1109/MGRS.2022.3165967","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130701827&doi=10.1109%2fMGRS.2022.3165967&partnerID=40&md5=ccc91ff1d29589805aa9e756e5bf1185","In the past decades, remote sensing (RS) data fusion has always been an active research community. A large number of algorithms and models have been developed. Generative adversarial networks (GANs), as an important branch of deep learning, show promising performances in a variety of RS image fusions. This review provides an introduction to GANs for RS data fusion. We briefly review the frequently used architecture and characteristics of GANs in data fusion and comprehensively discuss how to use GANs to realize fusion for homogeneous RS, heterogeneous RS, and RS and ground observation (GO) data. We also analyze some typical applications with GAN-based RS image fusion. This review provides insight into how to make GANs adapt to different types of fusion tasks and summarizes the advantages and disadvantages of GAN-based RS data fusion. Finally, we discuss promising future research directions and make a prediction on their trends.  © 2013 IEEE.","Deep learning; Image fusion; Remote sensing; Future research directions; Network state; Network-based; Performance; Remote grounds; Remote sensing data fusion; Remote sensing images; Remote-sensing; Research communities; State-of-the-art methods; algorithm; artificial neural network; data; remote sensing; research; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85130701827"
"Song D.; Tang Y.; Wang B.; Zhang J.; Yang C.","Song, Dongmei (56427407300); Tang, Yunhe (57223142077); Wang, Bin (57188632194); Zhang, Jie (57834708400); Yang, Changlong (57223130409)","56427407300; 57223142077; 57188632194; 57834708400; 57223130409","Two-Branch Generative Adversarial Network With Multiscale Connections for Hyperspectral Image Classification","2023","IEEE Access","11","","","7336","7347","11","10.1109/ACCESS.2022.3232152","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146256177&doi=10.1109%2fACCESS.2022.3232152&partnerID=40&md5=3b83526d23aec2e390c9d39e2ce2bd0a","Hyperspectral image (HSI) classification has always drawn great attention in the field of remote sensing. Various deep learning models are in the ascendant and gradually applied to HSI classification. Nevertheless, limited-labeled and class-imbalanced datasets largely make the classifier prone to overfitting. To address the above problem, this article proposes a two-branch generative adversarial network with multiscale connections (TBGAN), which includes two generators to produce the spectral and spatial samples, respectively. Thereinto, the spectral generator is imbued with the self-attention mechanism to maximumly capture the long-term dependencies across the spectral bands. And meanwhile, an elaborated discriminator with two branches is devised in TBGAN for extracting the joint spectral-spatial features. Besides, the multiscale connections are placed between the discriminator and two generators to alleviate the instability problems caused by the inherently backward propagation of gradients in GAN. Furthermore, a feature-matching term is added to the loss function to prevent the generators from overtraining upon the current discriminator, thereby further improving the stability of the network. Experiments upon three benchmark datasets demonstrate that TBGAN achieves an extremely competitive classification accuracy and exerts lower sensitivity to the training sample size compared with several state-of-the-art methods.  © 2013 IEEE.","Classification (of information); Deep learning; Feature extraction; Hyperspectral imaging; Image classification; Job analysis; Neural networks; Remote sensing; Convolutional neural network; Features extraction; Generator; Hyperspectral image classification; Joint spectral-spatial feature; Learning models; Multiscale connection; Remote-sensing; Spatial features; Task analysis; Generative adversarial networks","generative adversarial network; Hyperspectral image classification; joint spectral-spatial features; multiscale connections","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146256177"
"Kim J.-H.; Hwang Y.","Kim, Jun-Hyung (57287626800); Hwang, Youngbae (7402311392)","57287626800; 7402311392","GAN-Based Synthetic Data Augmentation for Infrared Small Target Detection","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5002512","","","","10.1109/TGRS.2022.3179891","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131755236&doi=10.1109%2fTGRS.2022.3179891&partnerID=40&md5=5ae3a9630e6aa486ce7541e8f556b451","Recently, convolutional neural networks (CNNs) have achieved state-of-The-Art performance in infrared small target detection. However, the limited number of public training data restricts the performance improvement of CNN-based methods. To handle the scarcity of training data, we propose a method that can generate synthetic training data for infrared small target detection. We adopt the generative adversarial network framework where synthetic background images and infrared small targets are generated in two independent processes. In the first stage, we synthesize infrared images by transforming visible images into infrared ones. In the second stage, target masks are implanted on the transformed images. Then, the proposed intensity modulation network synthesizes realistic target objects that can be diversely generated from further image processing. Experimental results on the recent public dataset show that, when we train various detection networks using the dataset composed of both real and synthetic images, detection networks yield better performance than using real data only.  © 1980-2012 IEEE.","Convolution; Image segmentation; Infrared imaging; Neural networks; Object detection; Convolutional neural network; Data augmentation; Generator; Image translation; Image-to-image translation; Images segmentations; Infrared small targets; Synthetic data; Synthetic data augmentation; Training data; artificial neural network; bioaugmentation; detection method; remote sensing; Generative adversarial networks","Convolutional neural network (CNN); generative adversarial network (GAN); image-To-image translation; infrared small target; synthetic data augmentation","Article","Final","","Scopus","2-s2.0-85131755236"
"Kong G.; Fan H.; Lobaccaro G.","Kong, Gefei (57205417221); Fan, Hongchao (55368902500); Lobaccaro, Gabriele (55979502000)","57205417221; 55368902500; 55979502000","Automatic building outline extraction from ALS point cloud data using generative adversarial network","2022","Geocarto International","","","","","","","10.1080/10106049.2022.2102246","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135810409&doi=10.1080%2f10106049.2022.2102246&partnerID=40&md5=93cb74131ba4734367b4bd9472d416ea","The generation of building footprints from laser scanning point clouds or remote sensing images involves three steps: segmentation, outline extraction and boundary regularization/generalization. Currently, existing approaches mainly focus on the first and third steps, while only few studies have been conducted for the second step. However, the extraction result of the building outlines directly determines the regularization performance. Therefore, high-quality building outlines are important to be delivered for the regularization. Determining parameters, such as point distance and neighborhood radius, is the primary challenge in the process of extracting building outlines. In this study, a parameter-free method is proposed by using an improved generative adversarial network (GAN). It extracts building outlines from gridded binary images with default resolution and no other input of parameters. Hence, the parameter selection problem is overcome. The experimental results on segmented point cloud datasets of building roofs reveal that our method achieves the mean intersection over union of 93.52%, the Hausdorff distance of 0.640 m and the PoLiS of 0.165 m. The comparison with α-shape method shows that our method can improve the extraction performance of concave shapes and provide a more regularized outline result. The method reduces the difficulty and complexity of the next regularization task, and contributes to the accuracy of point cloud-based building footprint generation. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","","deep learning; extraction of building outlines; GAN; point cloud data","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85135810409"
"Qin P.; Huang H.; Tang H.; Wang J.; Liu C.","Qin, Peng (57215550489); Huang, Huabing (57216494536); Tang, Hailong (57210148473); Wang, Jie (56050004800); Liu, Chong (57970016500)","57215550489; 57216494536; 57210148473; 56050004800; 57970016500","MUSTFN: A spatiotemporal fusion method for multi-scale and multi-sensor remote sensing images based on a convolutional neural network","2022","International Journal of Applied Earth Observation and Geoinformation","115","","103113","","","","10.1016/j.jag.2022.103113","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142150477&doi=10.1016%2fj.jag.2022.103113&partnerID=40&md5=397559d93ab6afae2c76f93f62f85af1","Spatiotemporal data fusion is a commonly-used and well-proven technique to enhance the application potential of multi-source remote sensing images. However, most existing methods have trouble in generating quality fusion results when areas covered by the images undergoes rapid land cover changes or images have substantial registration errors. While deep learning algorithms have demonstrated their capabilities for imagery fusion, it is challenging to apply deep-learning-based fusion methods in regions that experiences persistent cloud covers and have limited cloud-free imagery observations. To address these challenges, we developed a Multi-scene Spatiotemporal Fusion Network (MUSTFN) algorithm based on a Convolutional Neural Network (CNN). Our approach uses multi-level features to fuse images at different resolutions acquired by multiple sensors. Furthermore, MUSTFN uses the multi-scale features to overcome the effects of geometric registration errors between different images. Additionally, a multi-constrained loss function is proposed to improve the accuracy of imagery fusion over large areas and solve fusion and gap-filling problems simultaneously by utilizing cloud-contaminated images with the fine-tuning method. Compared with several commonly-used methods, our proposed MUSTFN performs better in fusing the 30-m Landsat-7 images and 500-m MODIS images over a small area that has undergone large changes (the average relative Mean Absolute Errors (rMAE) of the first four bands are 6.8% by MUSTFN as compared to 14.1% by the Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model (ESTARFM), 12.8% by the Flexible Spatiotemporal Data Fusion (FSDAF), 8.4% by the Extended Super-Resolution Convolutional Neural Network (ESRCNN), 8.1% by the Spatiotemporal Fusion Using a Generative Adversarial Network (STFGAN)). In particularly for images at different resolutions with different registration accuracies (e.g., 16-m Chinese GaoFen-1 and 500-m MODIS), MUSTFN achieved fusion results of good quality with an average rMAE of 9.3% in spectral reflectance at the first four bands. Finally, we demonstrated the applicability of MUSTFN (average rMAE of 9.18%) when fusing long-term Landsat-8 composite images and MODIS images over a large region (830 km × 600 km). Overall, our results suggest the effectiveness of MUSTFN to address the challenges in imagery fusion, including rapid land cover changes between image acquisition dates, geometric misregistration between images and limited availabilities of cloud-free images. The program of MUSTFN is freely available at: https://github.com/qpyeah/MUSTFN. © 2022 The Authors","algorithm; artificial neural network; image processing; remote sensing; satellite data; spatiotemporal analysis","CNN; Large-area image fusion; Multi-scale fusion scenarios; Multi-sensor satellite data; Spatiotemporal fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142150477"
"Gashnikov M.V.; Kuznetsov A.V.","Gashnikov, Mikhael Valeryevich (12644970700); Kuznetsov, Andrey Vladimirovich. (57220713929)","12644970700; 57220713929","Detection of artificial fragments embedded in remote sensing images by adversarial neural networks","2022","Computer Optics","46","4","","643","649","6","10.18287/2412-6179-CO-1064","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134204821&doi=10.18287%2f2412-6179-CO-1064&partnerID=40&md5=4c36a226a1539a211985ade8d6a188de","We investigate algorithms for detecting artificial fragments of remote sensing images generat-ed by adversarial neural networks. We consider a detector of artificial images based on the detection of a spectral artifact of generative-adversarial neural networks that is caused by a layer for en-hancing the resolution. We use the detecting algorithm to detect artificial fragments embedded in natural remote sensing images using an adversarial neural network that includes a contour genera-tor. We use remote sensing images of various types and resolutions, whereas the substituted areas, some being not simply connected, have different sizes and shapes. We experimentally prove that the investigated spectral neural network detector has high efficiency in detecting artificial fragments of remote sensing images. © 2022, Institution of Russian Academy of Sciences. All rights reserved.","Generative adversarial networks; Multilayer neural networks; Artificial image; Cycle neural network; Detection of artificial fragment of image; Generative adversarial neural network; Image redefinition; Image-based; Neural-networks; Remote sensing images; Remote sensing","cycle neural networks; detection of artificial fragments of images; generative adversarial neural networks; image redefinition; neural networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134204821"
"Li Z.; Zhang D.; Wang Y.; Lin D.; Zhang J.","Li, Zihao (57222130646); Zhang, Daobing (56163515100); Wang, Yang (57821937600); Lin, Daoyu (57196095251); Zhang, Jinghua (57225122034)","57222130646; 56163515100; 57821937600; 57196095251; 57225122034","Generative Adversarial Networks for Zero-Shot Remote Sensing Scene Classification","2022","Applied Sciences (Switzerland)","12","8","3760","","","","10.3390/app12083760","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128779924&doi=10.3390%2fapp12083760&partnerID=40&md5=82c3f50049aed29e2ec1fdf184f6721b","Deep learning-based methods succeed in remote sensing scene classification (RSSC). However, current methods require training on a large dataset, and if a class does not appear in the training set, it does not work well. Zero-shot classification methods are designed to address the classification for unseen category images in which the generative adversarial network (GAN) is a popular method. Thus, our approach aims to achieve the zero-shot RSSC based on GAN. We employed the conditional Wasserstein generative adversarial network (WGAN) to generate image features. Since remote sensing images have inter-class similarity and intra-class diversity, we introduced classification loss, semantic regression module, and class-prototype loss to constrain the generator. The classification loss was used to preserve inter-class discrimination. We used the semantic regression module to ensure that the image features generated by the generator can represent the semantic features. We introduced class-prototype loss to ensure the intra-class diversity of the synthesized image features and avoid generating too homogeneous image features. We studied the effect of different semantic embeddings for zero-shot RSSC. We performed experiments on three datasets, and the experimental results show that our method performs better than the state-of-the-art methods in zero-shot RSSC in most cases. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","generative adversarial network; remote sensing scene classification; zero-shot learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128779924"
"Zhao W.; Persello C.; Stein A.","Zhao, Wufan (57219025608); Persello, Claudio (23493587700); Stein, Alfred (7401758587)","57219025608; 23493587700; 7401758587","Semantic-aware unsupervised domain adaptation for height estimation from single-view aerial images","2023","ISPRS Journal of Photogrammetry and Remote Sensing","196","","","372","385","13","10.1016/j.isprsjprs.2023.01.003","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146430031&doi=10.1016%2fj.isprsjprs.2023.01.003&partnerID=40&md5=905966d664fa161e287b7e7da984aefe","Traditional acquisition of height data to generate normalized digital surface models (nDSMs) of very high spatial resolution is time-consuming and expensive. Height estimation by means of optical remote sensing images is a more efficient and timely way to do so. Recent studies employed supervised learning methods. State-of-the-art computer vision methods, however, overlook semantic consistency during remote sensing image translation and neglect multi-task correlations for specific task learning. To address these problems, this paper proposes a semantic-aware unsupervised domain adaptation method for height estimation. The method consists of image translation and multitask representation learning for height estimation. We tested the transferability of our method from the ISPRS Postdam data set to the Vaihingen data set and a custom dataset of Enschede. In the image translation task, our method improved the Fréchet Inception Distance (FID) metric by at least 12.8% and 12.1% on the two datasets, respectively. In the height estimation task, our method achieved RMSEs of 3.257 m and 3.875 m, which are at least 0.603 m and 0.072 m lower than the compared unsupervised algorithm, and achieved competitive results with the supervised learning algorithm. Our results show the advantages of the proposed method in height estimation and image translation as compared to alternative strategies. We conclude that adding semantic supervision improves height estimation from single-view orthophoto under unsupervised domain adaptation. It also alleviates the problem of limited access to nDSM data for training the method. © 2023 The Author(s)","Antennas; Generative adversarial networks; Image enhancement; Learning systems; Optical remote sensing; Semantic Web; Semantics; Supervised learning; Data set; Digital surface models; Domain adaptation; Height estimation; Image translation; Optical remote-sensing imagery; Remote sensing images; Semantic consistency; Semantic-aware; Unsupervised domain adaptation; aerial survey; image analysis; network analysis; satellite imagery; segmentation; spatial resolution; Learning algorithms","Generative adversarial network; Height estimation; Optical remote sensing imagery; Semantic consistency; Unsupervised domain adaptation","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85146430031"
"Gashnikov M.V.; Kuznetsov A.V.","Gashnikov, M.V. (12644970700); Kuznetsov, A.V. (57220713929)","12644970700; 57220713929","Detection of Fake Remote-Sensing Data","2022","Optical Memory and Neural Networks (Information Optics)","31","1","","16","21","5","10.3103/S1060992X22010052","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128305191&doi=10.3103%2fS1060992X22010052&partnerID=40&md5=8019c0ae4ef2643aa04739780287ae6d","Abstract: The paper deals with the detection of counterfeit remote-sensing data. A remote sensing data detector is considered that uses the neural-net identification of spectral artifacts typical of adversarial generators. The detector is employed to spot the remote-sensing image inpainting produced by an edge-generator-aided generative adversarial neural nets. Not only images with solid inpainted regions, but also images with separate inpainted areas are processed. In the process we use different types of remote-sensing images of various resolutions, inpainting portions being of different sizes and shapes. The experiment proved that the spectral neural-net detector is an effective tool for detecting fake remote-sensing data. © 2022, Allerton Press, Inc.","Crime; Fake detection; Generative adversarial networks; Image reconstruction; Remote sensing; Counterfeit image generation; Data detectors; Fake image detection; Image completion; Image detection; Image generations; Neural-networks; Remote sensing data; Remote sensing images; Spectral artifacts; Neural networks","counterfeit image generation; fake image detection; image completion; neural networks; remote-sensing data","Article","Final","","Scopus","2-s2.0-85128305191"
"Min J.; Zhang Y.; Yu Y.; Lv K.; Wang Z.; Zhang L.","Min, Jie (57226783874); Zhang, Yongsheng (56414442400); Yu, Ying (55731485300); Lv, Kefeng (57730290400); Wang, Ziquan (57196351372); Zhang, Lei (57196133151)","57226783874; 56414442400; 55731485300; 57730290400; 57196351372; 57196133151","Enhanced Remote Sensing Image SRGAN Algorithm and Its Application in Improving the Accuracy of 3D Reconstruction; [增强型遥感影像SRGAN算法及其在三维重建精度提升中的应用]","2022","Journal of Geo-Information Science","24","8","","1631","1644","13","10.12082/dqxxkx.2022.210766","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137638286&doi=10.12082%2fdqxxkx.2022.210766&partnerID=40&md5=47a95d9b46e8eded52331a3e5d81ba44","Remote sensing images are important data sources for terrain mapping, 3D reconstruction, and other tasks. The spatial resolution of remote sensing images determines the representation ability of the measured object on the image and plays an important role in the positioning accuracy and reconstruction effect of 3D model in the later stage. In view of the characteristics of high resolution remote sensing images including large scale, complex target features, and rich details, an enhanced SRGAN algorithm for remote sensing image reconstruction is proposed to meet the needs of 3D model reconstruction. The proposed algorithm overcomes the problems of edge effect and fuzzy reconstruction using traditional methods for super-resolution reconstruction. In traditional methods, there is limitation that simple convolutional networks can only extract the shallow feature information of the image and cannot retain the rich details of the image with the increasing resolution. The proposed algorithm is based on the generative adversarial networks using deep learning, in which dense residual blocks are used to extract deep features, and multi-scale discrimination is introduced into the discriminant model. In the training, the generation model and the discrimination model learn features together and are optimized to finally obtain a super-resolution reconstruction model suitable for remote sensing image application. This model can improve the resolution and image quality of remote sensing images, and ensure the integrity and accuracy of feature texture, detail information, and high-frequency target. In our study, the proposed algorithm is compared with the Bicubic, SRGAN, and ESRGAN algorithms. Our results show that the PSNR of the proposed algorithm is improved by about three units, the Penetration Index (PI) is stable and closer to one, and the SSIM and clarity index Q are also improved. In 3D reconstruction, the number of image dense matching points is increased, and the error is reduced. The measured point values of the model are closer to the measured point values from the field. The visual perception of the model is also more real and delicate, which indicates that the precision and positioning accuracy of the 3D model can be significantly improved using the remote sensing images constructed by the proposed algorithm. The results demonstrate the proposed algorithm that considers the characteristics of remote sensing images performs better than other algorithms for the super-resolution reconstruction, and the geometric accuracy and visual accuracy of the real 3D models based on the constructed images are also significantly improved. © 2022, Science Press. All right reserved.","3D modeling; Convolution; Convolutional neural networks; Deep learning; Generative adversarial networks; Image enhancement; Image quality; Image resolution; Mapping; Remote sensing; Textures; Three dimensional computer graphics; 3D reconstruction; Deep learning; High resolution remote sensing; Images processing; Multi-scale relative discrimination; Multi-scales; Positioning accuracy; Remote sensing images; SRGAN; Super-resolution reconstruction; accuracy assessment; algorithm; image processing; image resolution; machine learning; remote sensing; satellite imagery; three-dimensional modeling; Image reconstruction","3D reconstruction; Deep learning; High-resolution remote sensing; Image processing; Multi-scale relative discrimination; Positioning accuracy; SRGAN; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85137638286"
"Yu M.; Zhang W.; Chen X.; Liu Y.; Niu J.","Yu, Mingyang (55475784300); Zhang, Wenzhuo (57481906000); Chen, Xiaoxian (57482704000); Liu, Yaohui (57206823558); Niu, Jingge (57226284238)","55475784300; 57481906000; 57482704000; 57206823558; 57226284238","An End-to-End Atrous Spatial Pyramid Pooling and Skip-Connections Generative Adversarial Segmentation Network for Building Extraction from High-Resolution Aerial Images","2022","Applied Sciences (Switzerland)","12","10","5151","","","","10.3390/app12105151","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130943111&doi=10.3390%2fapp12105151&partnerID=40&md5=de35b48137523f738db461615ffd746a","Automatic building extraction based on high-resolution aerial imagery is an important challenge with a wide range of practical applications. One of the mainstream methods for extracting buildings from high-resolution images is deep learning because of its excellent deep feature extraction capability. However, existing models suffer from the problems of hollow interiors of some buildings and blurred boundaries. Furthermore, the increase in remote sensing image resolution has also led to rough segmentation results. To address these issues, we propose a generative adversarial segmentation network (ASGASN) for pixel-level extraction of buildings. The segmentation network of this framework adopts an asymmetric encoder–decoder structure. It captures and aggregates multiscale contextual information using the ASPP module and improves the classification and localization accuracy of the network using the global convolutional block. The discriminator network is an adversarial network that correctly discriminates the output of the generator and ground truth maps and computes multiscale L1 loss by fusing multiscale feature mappings. The segmentation network and the discriminator network are trained alternately on the WHU building dataset and the China typical cities building dataset. Experimental results show that the proposed ASGASN can accurately identify different types of buildings and achieve pixel-level high accuracy extraction of buildings. Additionally, compared to available deep learning models, ASGASN also achieved the highest accuracy performance (89.4% and 83.6% IoU on these two datasets, respectively). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","China typical cities building dataset; deep learning; generative adversarial network; high-resolution aerial images; semantic segmentation; WHU building dataset","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130943111"
"Han K.-H.; Jang J.-C.; Ryu S.; Sohn E.-H.; Hong S.","Han, Kyung-Hoon (57919948400); Jang, Jae-Cheol (57222815023); Ryu, Sumin (57217271715); Sohn, Eun-Ha (35216006700); Hong, Sungwook (55817600100)","57919948400; 57222815023; 57217271715; 35216006700; 55817600100","Hypothetical Visible Bands of Advanced Meteorological Imager Onboard the Geostationary Korea Multi-Purpose Satellite -2A Using Data-To-Data Translation","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8378","8388","10","10.1109/JSTARS.2022.3210143","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139461698&doi=10.1109%2fJSTARS.2022.3210143&partnerID=40&md5=fc5e2cec7feb05545a089211dafc2701","True-color imagery of satellites provides various atmospheric and surface information for intuitive understanding and visualization. This article presents a conditional generative adversarial network method for generating daytime and nighttime hypothetical visible (VIS) bands of the advanced meteorological imager (AMI) sensor onboard the geostationary Korea multipurpose satellite. The AMI datasets in the form of albedo and brightness temperature (${{\boldsymbol{T}}}_{\boldsymbol{B}}$) were normalized and denormalized between 0 and 1 data-to-data (D2D) translation. The D2D model was trained and tested using data pair of the albedos at AMI VIS bands and AMI infrared (IR) bands or ${{\boldsymbol{T}}}_{\boldsymbol{B}}$ differences between two AMI IR bands. The constructed D2D model showed that the statistical results of bias, root-mean-square-error, and correlation coefficient between the observed and D2D-generated AMI VIS bands during daytime were -0.006 and 0.047 in albedo, and 0.941 for the blue band; -0.007 and 0.05 in albedo and 0.939 for the green band; -0.01 and 0.061 in albedo, and 0.917 for the red band, respectively. The proposed D2D method is being officially used by the Korea meteorological administration. Except for simulating desert areas as clouds at night, the D2D model demonstrated excellent performance, generating hypothetical AMI VIS bands at day and night. Consequently, this article could significantly contribute to the monitoring and understanding of meteorological phenomena over one-third of the Earth. Additionally, the method can be extended to other geostationary weather satellites, including Himawari-8, Fengyun-4A, meteosat third generation, and geostationary operational environmental satellites.  © 2008-2012 IEEE.","Atmospheric temperature; Clouds; Data visualization; Geostationary satellites; Meteorology; Oceanography; Remote sensing; Solar radiation; Surface waters; Advanced meteorological imager; Adversarial; CGAN; Cloud-computing; Device-to-Device communications; Geo-kompsat-2a; Hypothetical visible band; Ocean temperature; Satellite remote sensing; Sea surfaces; Spatial resolution; Visible band; artificial neural network; geostationary satellite; remote sensing; satellite imagery; sensor; visible spectrum; Mean square error","advanced meteorological imager (AMI); Adversarial; conditional generative adversarial network (CGAN); geo-kompsat-2a; hypothetical visible band; satellite remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139461698"
"Zhao L.; Zhang Y.; Cui Y.","Zhao, Liquan (25959464800); Zhang, Yupeng (57432227100); Cui, Ying (57665702900)","25959464800; 57432227100; 57665702900","An Attention Encoder-Decoder Network Based on Generative Adversarial Network for Remote Sensing Image Dehazing","2022","IEEE Sensors Journal","22","11","","10890","10900","10","10.1109/JSEN.2022.3172132","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129349078&doi=10.1109%2fJSEN.2022.3172132&partnerID=40&md5=b021c5bd1d322d04dcb40fb289fcd05a","Remote sensing image dehazing is a difficult problem for its complex characteristics. It can be regarded as the preprocessing of high-level tasks of remote sensing images. To remove haze from the hazy remote sensing image, an encoder-decoder based on generative adversarial network is proposed. It first learns the low-frequency information of the image, and then learns the high-frequency information of the image. The skip connection is also added in the network to avoid losing information. To further improve the ability of learning more useful information, a multi-scale attention module is proposed. Meanwhile, a CBlock module is also designed to extract more feature information. It can capture different size of receptive fields. In order to reduce the computational pressure of the network, a distillation module is used in the network. Inspired by multi-scale network, an enhance module is designed and introduced it in the end of the network to further improve the dehazing ability of the network by integrating context information on multi-scale. We compared with five methods and our proposed method on RICE dataset. Experimental results show that our method achieves the best effect, both qualitatively and quantitatively. © 2001-2012 IEEE.","Decoding; Demulsification; Distillation; Information use; Network coding; Remote sensing; Atmospheric modeling; Dehazing; Encoder-decoder; Features extraction; Learn+; Multi-scale attention module; Multi-scales; Remote sensing image dehazing; Remote sensing images; Remote-sensing; Generative adversarial networks","encoder-decoder; generative adversarial network; multi-scale attention module; Remote sensing image dehazing","Article","Final","","Scopus","2-s2.0-85129349078"
"Sebastianelli A.; Puglisi E.; Del Rosso M.P.; Mifdal J.; Nowakowski A.; Mathieu P.P.; Pirri F.; Ullo S.L.","Sebastianelli, Alessandro (57202958220); Puglisi, Erika (57226605784); Del Rosso, Maria Pia (57221180323); Mifdal, Jamila (57200605283); Nowakowski, Artur (57527552900); Mathieu, Pierre Philippe (12241655700); Pirri, Fiora (56990245000); Ullo, Silvia Liberata (6503914067)","57202958220; 57226605784; 57221180323; 57200605283; 57527552900; 12241655700; 56990245000; 6503914067","PLFM: Pixel-Level Merging of Intermediate Feature Maps by Disentangling and Fusing Spatial and Temporal Data for Cloud Removal","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5412216","","","","10.1109/TGRS.2022.3208694","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139437659&doi=10.1109%2fTGRS.2022.3208694&partnerID=40&md5=e1abd7c8853e146aa31e7b4680ee5bdf","Cloud removal is a relevant topic in remote sensing, fostering medium- and high-resolution optical (OPT) image usability for Earth monitoring and study. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps: the amount of cloud coverage, the landscape temporal changes, and the density and thickness of clouds need further investigation. We fill some of these gaps in this work by introducing an innovative deep model. The proposed model is multimodal, relying on both spatial and temporal sources of information to restore the whole optical scene of interest. We use the outcomes of both temporal-sequence blending and direct translation from synthetic aperture radar (SAR) to optical images to obtain a pixel-wise restoration of the whole scene. The reconstructed images preserve scene details without resorting to a considerable portion of a clean image. Our approach's advantage is demonstrated across various atmospheric conditions tested on different datasets. Quantitative and qualitative results prove that the proposed method obtains cloud-free images coping with landscape changes.  © 1980-2012 IEEE.","Data fusion; Generative adversarial networks; Geometrical optics; Hierarchical systems; Image reconstruction; Optical remote sensing; Pixels; Radar imaging; Restoration; Adaptation models; Cloud removal; Conditional generative adversarial network; Convolutional long short-term memory; Deep hierarchical model; Hierarchical model; Images reconstruction; Multi-temporal remote sensing; Multitemporal remote sensing image; Optical data; Optical imaging; Radar polarimetry; Remote sensing images; Synthetic aperture radar-optical data fusion; hierarchical system; map; pixel; remote sensing; spatial data; synthetic aperture radar; temporal analysis; Synthetic aperture radar","Cloud removal (CR); conditional generative adversarial networks (cGANs); convolutional long short-term memory (ConvLSTM); deep hierarchical model; multitemporal remote sensing (RS) images; synthetic aperture radar (SAR)-optical (OPT) data fusion","Article","Final","","Scopus","2-s2.0-85139437659"
"Liu L.; Li W.; Shi Z.; Zou Z.","Liu, Liqin (57215536317); Li, Wenyuan (57204784272); Shi, Zhenwei (23398841900); Zou, Zhengxia (56073977200)","57215536317; 57204784272; 23398841900; 56073977200","Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5528215","","","","10.1109/TGRS.2022.3173532","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130678301&doi=10.1109%2fTGRS.2022.3173532&partnerID=40&md5=d0c26ffea5447d5804eb35757065b4df","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as 'Physics-informed Deep Adversarial Spectral Synthesis (PDASS).' On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.  © 1980-2012 IEEE.","Generative adversarial networks; Hyperspectral imaging; Military applications; Optical resolving power; Pixels; Reflection; Remote sensing; Spectroscopy; Adversarial networks; Atmospheric modeling; Generation adversarial network; Images reconstruction; Imaging modeling; Remote-sensing; Spatial resolution; Spectral super-resolution; Superresolution; artificial neural network; image analysis; image resolution; imaging method; physics; pixel; remote sensing; satellite imagery; spectral analysis; Image reconstruction","Generation adversarial networks (GANs); hyperspectral image; imaging model; remote sensing; spectral super-resolution (SSR)","Article","Final","","Scopus","2-s2.0-85130678301"
"Radoi A.","Radoi, Anamaria (55544717700)","55544717700","Generative Adversarial Networks Under CutMix Transformations for Multimodal Change Detection","2022","IEEE Geoscience and Remote Sensing Letters","19","","2506905","","","","10.1109/LGRS.2022.3201003","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137586485&doi=10.1109%2fLGRS.2022.3201003&partnerID=40&md5=ad8d90eb14fd911850b6567fb00cc15a","The current technological developments lead to increased heterogeneity and variability in remote sensing imagery. In this context, unsupervised multimodal change detection techniques are mandatory to perform a continuous monitoring and rapid damage assessment by means of heterogeneous remote sensing data. Taking advantage of the latest advances in deep learning, we address multimodal change detection from an intermodality image translation perspective. Intermodality translation is achieved by means of generative adversarial networks (GANs) built over U-Net architectures at both generator and discriminator levels and trained under CutMix transformations. A change prior is used to guide the learning process of the neural network framework and to reduce the impact of changed locations over the learned model. The change prior is derived in an unsupervised manner from comparisons between the postevent locations and k nearest neighbor ( k NN) locations determined in the preevent image. The experiments were conducted over several pairs of heterogeneous remote sensing images, and the comparisons with current state-of-the-art approaches show the effectiveness of the proposed multimodal change detection framework.  © 2022 IEEE.","Automation; Chemical detection; Damage detection; Deep learning; Generative adversarial networks; Location; Remote sensing; 'current; Change detection; Cutmix; Deep learning; Image translation; Intermodality; Multi-modal; Multimodal change detection; Remote-sensing; U-net; artificial neural network; detection method; nearest neighbor analysis; remote sensing; Change detection","CutMix; generative adversarial networks (GANs); image translation; multimodal change detection; U-Net","Article","Final","","Scopus","2-s2.0-85137586485"
"Luo H.; Zhu H.; Liu S.; Liu Y.; Zhu X.; Lai J.","Luo, Huanlin (57714061200); Zhu, Haowen (57701228500); Liu, Shengyang (57701228600); Liu, Yichuan (57702280400); Zhu, Xinzhong (57203681230); Lai, Jinmei (57701765900)","57714061200; 57701228500; 57701228600; 57702280400; 57203681230; 57701765900","3-D Auxiliary Classifier GAN for Hyperspectral Anomaly Detection via Weakly Supervised Learning","2022","IEEE Geoscience and Remote Sensing Letters","19","","6009805","","","","10.1109/LGRS.2022.3175836","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130425200&doi=10.1109%2fLGRS.2022.3175836&partnerID=40&md5=030402e44606363785fecfeba597be65","Hyperspectral anomaly detection (AD) is important in Earth observation and remote sensing. However, the low spatial resolution of hyperspectral images (HSIs), insufficient samples, and lack of prior information limit the detection accuracy. To solve these problems, in this letter, we propose an auxiliary classifier generative adversarial network model based on a 3-D convolutional neural network named 3-D Auxiliary Classifier generative adversarial network (AC-GAN). First, the model is based on a 3-D convolutional neural network design, with 3-D tensors as samples. The network maintains valuable image spatial spectrum joint features to achieve good detection results. It can also generate sufficient samples to achieve dataset augmentation, solving the overfitting problem in GAN training. Second, we train the model with a weakly supervised method. The label of the samples is obtained through the coarse scanning method. Then, the AC-GAN is trained with the bootstrapping method to mitigate the impact of noise labels. The experimental results show that our proposed algorithm outperforms state-of-the-art AD algorithms.  © 2004-2012 IEEE.","Convolution; Hyperspectral imaging; Image classification; Neural networks; Remote sensing; Spectroscopy; Supervised learning; Three dimensional displays; Anomaly detection; Auxiliary classifier GAN; Classification algorithm; Hyperspectral anomaly detection; Hyperspectral image; Solid modelling; Three-dimensional display; Weakly supervised learning; anomaly; remote sensing; spatial resolution; supervised learning; Generative adversarial networks","Anomaly detection (AD); auxiliary classifier generative adversarial network (AC-GAN); hyperspectral images (HSIs); weakly supervised learning (WSL)","Article","Final","","Scopus","2-s2.0-85130425200"
"Jozdani S.; Chen D.; Pouliot D.; Alan Johnson B.","Jozdani, Shahab (57188989068); Chen, Dongmei (57203235632); Pouliot, Darren (8212745300); Alan Johnson, Brian (57493655200)","57188989068; 57203235632; 8212745300; 57493655200","A review and meta-analysis of generative adversarial networks and their applications in remote sensing","2022","International Journal of Applied Earth Observation and Geoinformation","108","","102734","","","","10.1016/j.jag.2022.102734","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126636731&doi=10.1016%2fj.jag.2022.102734&partnerID=40&md5=477a218f5f179f495a2631341aad3e8a","Generative Adversarial Networks (GANs) are one of the most creative advances in Deep Learning (DL) in recent years. The Remote Sensing (RS) community has adopted GANs quickly, and reported successful use in a wide variety of applications. Given a sharp increase in research on GANs in the field of RS, there is a need for an in-depth review of the major technological/methodological advances and new applications. In this regard, we conducted a comprehensive review and meta-analysis of GAN-related RS papers, with the goals of familiarizing the RS community with the potential of GANs and helping researchers further explore RS applications of GANs by untangling challenges common in this field. Our review is based on 231 journal papers that were retrieved and selected through the Web of Science (WoS) database. We reviewed the theories, applications, and challenges of GANs, and highlighted the gaps to explore in future studies. Through the meta-analysis conducted in this study, we observed that image classification (especially urban mapping) has been the most popular application of GANs, potentially due to the wide availability of benchmark datasets. One the other hand, we found that relatively few studies have explored the potential of GANs for analyzing medium spatial-resolution multi-spectral images (e.g., Landsat or Sentinel-2), even though such images are often freely available and useful for a wide range of applications (e.g., urban expansion analysis, vegetation mapping, etc.). In spite of the applications of GANs for different RS processing tasks, there are still several gaps/questions in this field such as: 1) which GAN models/configurations are more suitable for different applications?) 2) to what degree can GANs replace real RS data in different applications? Such gaps/questions can be appropriately addressed by, for example, conducting experimental studies on evaluating different GAN models for various RS applications to provide better insights into how/which GAN models can be best deployed. The meta-analysis results presented in this study could be helpful for RS researchers to know the opportunities of using GANs and understand how GANs contribute to the current challenges in different RS applications. © 2022 The Authors","database; image analysis; machine learning; meta-analysis; remote sensing; spatial resolution","Deep learning; GANs; Generative adversarial networks; Remote sensing","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126636731"
"He C.; Zhao Y.; Dong J.; Xiang Y.","He, Chenyang (57963642300); Zhao, Yindi (8359721000); Dong, Jihong (57963825200); Xiang, Yang (57215096476)","57963642300; 8359721000; 57963825200; 57215096476","Use of GAN to Help Networks to Detect Urban Change Accurately","2022","Remote Sensing","14","21","5448","","","","10.3390/rs14215448","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141848721&doi=10.3390%2frs14215448&partnerID=40&md5=8f65d11022b00bad126676168df8d83a","Mastering urban change information is of great importance and significance in practical areas such as urban development planning, land management, and vegetation cover. At present, high-resolution remote sensing images and deep learning techniques have been widely used in the detection of urban information changes. However, most of the existing change detection networks are Siamese networks based on encoder–decoder architectures, which tend to ignore the pixel-to-pixel relationships and affect the change detection results. To solve this problem, we introduced a generative adversarial network (GAN). The change detection network based on the encoder–decoder architecture was used as the generator of the GAN, and the Jensen-Shannon(JS) scatter in the GAN model was replaced by the Wasserstein distance. An urban scene change detection dataset named XI’AN-CDD was produced to verify the effectiveness of the algorithm. Compared with the baseline model of the change detection network, our generator outperformed it significantly and had higher feature integrity. When the GAN was added, the detected feature integrity was better, and the F1-score increased by 4.4%. © 2022 by the authors.","Change detection; Decoding; Deep learning; Feature extraction; Network architecture; Pixels; Remote sensing; Urban growth; Change detection; Detection networks; Development planning; Encoder-decoder architecture; Land managements; Network-based; Siamese network; Urban changes; Urban development; Urban scenes; Generative adversarial networks","change detection; GAN; Siamese network; urban scene","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141848721"
"Li X.; Dong Y.; Zhu Y.; Huang W.","Li, Xueling (57216211286); Dong, Yingying (41661346100); Zhu, Yining (36672150100); Huang, Wenjiang (9040267000)","57216211286; 41661346100; 36672150100; 9040267000","Enhanced Leaf Area Index Estimation with CROP-DualGAN Network","2022","IEEE Transactions on Geoscience and Remote Sensing","","","","1","1","0","10.1109/TGRS.2022.3230354","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146254136&doi=10.1109%2fTGRS.2022.3230354&partnerID=40&md5=cd0b4433d471639d920906dbf38f46b3","Quantitative estimation of regional leaf area index (LAI) is an important basis for large-scale crop growth monitoring and yield estimation. With the development of deep learning, theoretically, the use of neural networks can effectively improve the accuracy of LAI estimation, but sufficient training samples are often required due to a large number of network parameters. In an actual regional LAI quantitative estimation, there are only a few samples, which is difficult to train in networks. Therefore, a crop dual-learning generative adversarial network (CROP-DualGAN) was proposed in this article for data enhancement of small samples to estimate regional LAI. The method uses dual learning to generate hyperspectral reflectance and corresponding LAI, including two groups of generative adversarial networks, in which the generator is used to generate data that conforms to the distribution of the training set, and the discriminator is used to judge the true or false generated samples. The generators and discriminators are constantly optimized in the confrontation so that the distribution of generated data is closer to that of training samples. In single crop type experiments, 30 training samples with enhanced in VGG16 achieved the R2 of cereal, maize and rape seed as 0.921, 0.990 and 0.956, and in SSLLAI-Net achieved the R2 of cereal, maize and rape seed as 0.971, 0.991 and 0.962. In multiple crop types experiments, the result is lower than individual crop estimation, but higher than that of without enhancement. Finally, non-parametric test is used to prove that most improvement in LAI estimation is significant, and the accuracy won&#x2019;t decrease when improvement is not significant. In all, proposed method is universal and can effectively help benchmark models to improve regional LAI estimation accuracy with neural networks. Author","Biological systems; Crops; Deep learning; Generative adversarial networks; Hyperspectral imaging; Parameter estimation; Remote sensing; Sampling; Biological system modeling; Crop dual-learning generative adversarial network; Data enhancement; Generator; HyperSpectral; Leaf Area Index; Neural-networks; Quantitative estimation; Remote-sensing; Training sample; Reflection","Biological system modeling; CROP-DualGAN; Crops; data enhancement; Estimation; Generators; Hyperspectral; Hyperspectral imaging; Leaf Area Index; Reflectivity; Remote Sensing; Training","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85146254136"
"Ghildiyal S.; Goel N.; Saini M.","Ghildiyal, Sushil (57214727585); Goel, Neeraj (57201699364); Saini, Mukesh (25422670600)","57214727585; 57201699364; 25422670600","Cloud Removal in Satellite Imagery Using Adversarial Network and RGB-Optical Data Fusion","2022","Proceedings - 5th International Conference on Multimedia Information Processing and Retrieval, MIPR 2022","","","","407","412","5","10.1109/MIPR54900.2022.00080","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139024546&doi=10.1109%2fMIPR54900.2022.00080&partnerID=40&md5=cec3e04378639f51a8828332385b814d","Many earth observation activities rely on optical remote sensing data. The optical remote sensing imagery is ex-ploited in various applications like farmland monitoring, land use, land cover, 3D city models, vegetation growth, and disaster mitigation. Despite all, cloud cover significantly impacts on spatial and temporal accessibility of the earth observation. Since the first observation, it has been one persistent difficulty for optical remote sensing. For decades, researchers have been studying to remove clouds from optical images. The procedure of clearing the clouds becomes more difficult as they thicken. In such instances, it is customary to reconstruct utilizing additional images such as synthetic aperture radar (SAR) or near-infrared (NIR). In this paper, we propose a two-stage architecture-based cloud removal framework. The first stage of our network translates SAR and optical cloudy images to synthetic optical (RGB) image using the conditional Generative Adversarial Network (cGAN) and the second stage reconstructs the cloud-free image by fusing the synthetic optical (RGB) and cloudy optical image. The network was tested on the real cloudy images and the proposed method was compared with the state-of-the-art models and showed better results for cloud removal.  © 2022 IEEE.","Data fusion; Generative adversarial networks; Geometrical optics; Image reconstruction; Infrared devices; Land use; Observatories; Optical remote sensing; Radar imaging; Satellite imagery; Three dimensional computer graphics; Adversarial networks; Cloud removal; Conditional generative adversarial network; Earth observations; Optical data; Optical image; Optical remote sensing data; Optical remote-sensing imagery; Optical-; Remote-sensing; Synthetic aperture radar","cGAN; cloud removal; remote sensing; SAR","Conference paper","Final","","Scopus","2-s2.0-85139024546"
"Benzenati T.; Kessentini Y.; Kallel A.","Benzenati, Tayeb (57216240538); Kessentini, Yousri (16052461400); Kallel, Abdelaziz (22233967900)","57216240538; 16052461400; 22233967900","Pansharpening approach via two-stream detail injection based on relativistic generative adversarial networks","2022","Expert Systems with Applications","188","","115996","","","","10.1016/j.eswa.2021.115996","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117394775&doi=10.1016%2fj.eswa.2021.115996&partnerID=40&md5=f898950c62dad03fc0195dd969133168","The goal of pansharpening technique is to reconstruct a high resolution spectral image (HRMS) from a low spatial resolution multispectral image (MS) and a high spatial resolution single band panchromatic (PAN) image. As the efficacy of such a technique relies upon its capability to reinforce the spatial information of the MS image while conserving its spectral signature, in this paper, we propose DI-GAN, an effective Detail Injection Generative Adversarial Network for remote sensing image pansharpening. DI-GAN employs a deep two-stream Convolutional Neural Network (CNN) generator model to combine the spatial information available on the PAN image and the spectral characteristic belonging to the MS image, in order to predict the high-frequency detail to be accurately injected into the MS image to reconstruct the desired HRMS. An original loss function is particularly proposed to encourage the network to predict only high-frequency detail. DI-GAN incorporates in parallel a Relativistic Discriminator that allows the pansharpening products to be more realistic. Experimental results at degraded and full scale on three different datasets were showed that the proposed fusion approach can produce superior pansharpening performances in terms of spatial and spectral fidelities with respect to the literature techniques, including CNN- and GAN-based methods. © 2021 Elsevier Ltd","Convolutional neural networks; Gallium nitride; Image reconstruction; Image resolution; Remote sensing; Spectroscopy; Convolutional neural network; Detail injection; High frequency HF; High resolution; Multispectral images; Pan-sharpening; Relativistic GAN; Relativistics; Spatial informations; Two-stream; Generative adversarial networks","CNN; Detail injection; Pansharpening; Relativistic GAN","Article","Final","","Scopus","2-s2.0-85117394775"
"Huang F.; Jia Y.; Yang Y.","Huang, Feiyang (58087712700); Jia, Yamin (58087419200); Yang, Yuxuan (58087504900)","58087712700; 58087419200; 58087504900","Research Advanced in Image Denoising Based on Deep Learning","2022","2022 IEEE Conference on Telecommunications, Optics and Computer Science, TOCS 2022","","","","1472","1476","4","10.1109/TOCS56154.2022.10016170","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147249559&doi=10.1109%2fTOCS56154.2022.10016170&partnerID=40&md5=b3c91abeb424092ade0847caf0afe066","Image denoising has always been a hot research issue in the computer vision community, which aims to reduce the noise in digital images to improve image quality. As a key step in many downstream computer vision tasks, image denoising has been widely used in many fields such as medical agricultural images, satellite images, remote sensing images, face recognition, vehicle detection and many other fields. However, limited by problems such as insufficient actual image data sets, uneven noise reduction ability of different models, uneven noise reduction performance of the same model on different data sets and uneven noise reduction results of the same model for different scales of noise, the existing denoising algorithms still can not fully meet the needs of practical applications. In order to explore the optimal denoising method under the influence of the above variables, we first introduce three representative denoising frameworks based on self-supervised, residual network and generative countermeasure network. We then choose five best noise reduction models: CBM3D, DnCNN, IRCNN, FFDNet, and Fdncnn, and classify and compare the real noise and synthetic noise on five data sets, such as CBSD68, Kodak24, McMaster, DND, PolyU. In addition, in the noise reduction experiment of synthetic noise, we divide the noise into five noise levels, and discuss the change trend of noise reduction effect when the noise level changes. This paper has done a lot of experiments, but it still has high scalability. In the future, more independent variables can be introduced on the basis of this experiment, so as to draw more accurate conclusions.  © 2022 IEEE.","Classification (of information); Computer vision; Deep learning; Face recognition; Generative adversarial networks; Image enhancement; Medical imaging; Noise abatement; Remote sensing; Data set; Deep learning; Digital image; Neural-networks; Noise levels; Research issues; Residual neural network; Self-supervised learning; Synthetic noise; Vision communities; Image denoising","deep learning; generative adversarial network; Image denoising; residual neural network; self-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85147249559"
"Xu Y.; Luo W.; Hu A.; Xie Z.; Xie X.; Tao L.","Xu, Yongyang (57095192900); Luo, Wei (55584802653); Hu, Anna (57205419850); Xie, Zhong (36164790400); Xie, Xuejing (57212576401); Tao, Liufeng (57210018064)","57095192900; 55584802653; 57205419850; 36164790400; 57212576401; 57210018064","TE-SAGAN: An Improved Generative Adversarial Network for Remote Sensing Super-Resolution Images","2022","Remote Sensing","14","10","2425","","","","10.3390/rs14102425","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130994887&doi=10.3390%2frs14102425&partnerID=40&md5=28149b448945d41d9c05e4fe052bb09d","Resolution is a comprehensive reflection and evaluation index for the visual quality of remote sensing images. Super-resolution processing has been widely applied for extracting information from remote sensing images. Recently, deep learning methods have found increasing application in the super-resolution processing of remote sensing images. However, issues such as blurry object edges and existing artifacts persist. To overcome these issues, this study proposes an improved generative adversarial network with self-attention and texture enhancement (TE-SAGAN) for remote sensing super-resolution images. We first designed an improved generator based on the residual dense block with a self-attention mechanism and weight normalization. The generator gains the feature extraction capability and enhances the training model stability to improve edge contour and texture. Subsequently, a joint loss, which is a combination of L1-norm, perceptual, and texture losses, is designed to optimize the training process and remove artifacts. The L1-norm loss is designed to ensure the consistency of low-frequency pixels; perceptual loss is used to entrench medium-and high-frequency details; and texture loss provides the local features for the super-resolution process. The results of experiments using a publicly available dataset (UC Merced Land Use dataset) and our dataset show that the proposed TE-SAGAN yields clear edges and textures in the super-resolution reconstruction of remote sensing images. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image enhancement; Image texture; Land use; Optical resolving power; Remote sensing; Textures; Attention mechanisms; Joint loss; Normalisation; Remote sensing images; Remote-sensing; Resolution images; Self-attention mechanism; Super-resolution image; Superresolution; Weight normalization; Generative adversarial networks","generative adversarial network; joint loss; self-attention mechanism; super-resolution image; weight normalization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130994887"
"Sun C.; Huang C.; Zhang H.; Chen B.; An F.; Wang L.; Yun T.","Sun, Chenxin (57783059600); Huang, Chengwei (57783713400); Zhang, Huaiqing (57865265400); Chen, Bangqian (48261068400); An, Feng (55848021900); Wang, Liwen (57218273243); Yun, Ting (55273559900)","57783059600; 57783713400; 57865265400; 48261068400; 55848021900; 57218273243; 55273559900","Individual Tree Crown Segmentation and Crown Width Extraction From a Heightmap Derived From Aerial Laser Scanning Data Using a Deep Learning Framework","2022","Frontiers in Plant Science","13","","914974","","","","10.3389/fpls.2022.914974","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133527210&doi=10.3389%2ffpls.2022.914974&partnerID=40&md5=954a0eb3d2d71bbd06354a5aad9818d0","Deriving individual tree crown (ITC) information from light detection and ranging (LiDAR) data is of great significance to forest resource assessment and smart management. After proof-of-concept studies, advanced deep learning methods have been shown to have high efficiency and accuracy in remote sensing data analysis and geoscience problem solving. This study proposes a novel concept for synergetic use of the YOLO-v4 deep learning network based on heightmaps directly generated from airborne LiDAR data for ITC segmentation and a computer graphics algorithm for refinement of the segmentation results involving overlapping tree crowns. This concept overcomes the limitations experienced by existing ITC segmentation methods that use aerial photographs to obtain texture and crown appearance information and commonly encounter interference due to heterogeneous solar illumination intensities or interlacing branches and leaves. Three generative adversarial networks (WGAN, CycleGAN, and SinGAN) were employed to generate synthetic images. These images were coupled with manually labeled training samples to train the network. Three forest plots, namely, a tree nursery, forest landscape and mixed tree plantation, were used to verify the effectiveness of our approach. The results showed that the overall recall of our method for detecting ITCs in the three forest plot types reached 83.6%, with an overall precision of 81.4%. Compared with reference field measurement data, the coefficient of determination R2 was ≥ 79.93% for tree crown width estimation, and the accuracy of our deep learning method was not influenced by the values of key parameters, yielding 3.9% greater accuracy than the traditional watershed method. The results demonstrate an enhancement of tree crown segmentation in the form of a heightmap for different forest plot types using the concept of deep learning, and our method bypasses the visual complications arising from aerial images featuring diverse textures and unordered scanned points with irregular geometrical properties. Copyright © 2022 Sun, Huang, Zhang, Chen, An, Wang and Yun.","","airborne LiDAR; deep learning; forest parameter retrieval; heightmap; individual tree crown segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85133527210"
"Xu Z.; Bai X.","Xu, Zhijing (7405427967); Bai, Xue (57219423991)","7405427967; 57219423991","Small Ship Target Detection Method for Remote Sensing Images Based on Dual Feature Enhancement; [基于双重特征增强的遥感舰船小目标检测]","2022","Guangxue Xuebao/Acta Optica Sinica","42","18","1828002","","","","10.3788/AOS202242.1828002","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138780913&doi=10.3788%2fAOS202242.1828002&partnerID=40&md5=c110c51f13992503092c90cd08b4d10f","In terms of the large proportion and multidirectional rotation of small ship targets in remote sensing images, a small ship target detection method based on texture and color enhancement is proposed. Firstly, a generative adversarial network is designed to enhance the texture features of small ship targets and generate high-resolution ship images. Secondly, the deep reinforcement learning algorithm is used to improve the image color, which solves the problem of the low contrast between the ship target and the background color. Thirdly, an adaptive transform feature pyramid network is designed to enhance the global receptive field and effectively deal with the hard extraction of small target features, which is caused by the lack of spatial information in the deep network. Finally, the feature refinement module and circular smooth label are utilized to align feature points and achieve angle regression in a ship target bounding box, which effectively increases the accuracy of detecting ship targets with multidirectional rotation. In addition, related tests are carried out on the public data sets of HRSC2016 and DOTA, and results show that the proposed method achieves an mean average precision of 72.87% and 89.91%, respectively, which is better than the existing mainstream small ship target detection methods. © 2022, Chinese Lasers Press. All right reserved.","Color; Deep learning; Feature extraction; Generative adversarial networks; Image enhancement; Image texture; Reinforcement learning; Ships; Textures; Deep Q-network; Detection methods; Dual feature enhancement; Feature enhancement; Remote sensing images; Remote-sensing; Ship detection; Ship targets; Small targets; Targets detection; Remote sensing","Deep Q-network; Dual feature enhancement; Remote sensing; Remote sensing images; Ship detection; Small target","Article","Final","","Scopus","2-s2.0-85138780913"
"Bergamasco L.; Bovolo F.","Bergamasco, Luca (57214091122); Bovolo, Francesca (9943212600)","57214091122; 9943212600","Unsupervised Change Detection in Multi-Modal SAR Images using CycleGAN","2022","Proceedings of SPIE - The International Society for Optical Engineering","12267","","122670F","","","","10.1117/12.2637433","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142529145&doi=10.1117%2f12.2637433&partnerID=40&md5=debb818a3f8dd6de240fa22ef4f3c938","Many Change Detection (CD) methods exploit the bi-temporal multi-modal data derived by multiple sensors to find the changes effectively. State-of-the-Art CD methods define features with a common domain between the multi-modal data by normalizing input images or ad hoc feature extraction/selection methods. Deep Learning (DL) CD methods automatically learn features with a common domain during the training or adapt the features derived by multi-modal data. However, CD methods focusing on multi-sensor multi-frequency SAR data are still poorly investigated. We propose a DL CD method that exploits a Cycle Generative Adversarial Network (CycleGAN) to automatically learn and extract multi-scale feature maps in a domain common to the input multi-frequency multi-sensor SAR data. The feature maps are learned, during unsupervised training, by generators that aim to transform the input data domain into the target one while preserving the semantic information and aligning the feature domain. We process the multi-sensor multi-frequency SAR data with the trained generators to produce bi-temporal multi-scale feature maps that are compared to enhance changes. A standard-deviation-based feature selection is applied to keep only the most informative comparisons and reject the ones with poor change information. The multi-scale comparisons are used for a detail preserving CD. Preliminary experimental results conducted on bi-temporal SAR data acquired by Cosmo-SkyMed and SAOCOM on the urban area of Milan, Italy, in January 2020 and August 2021 provided promising results. © 2022 SPIE.","Data mining; Deep learning; Feature extraction; Generative adversarial networks; Modal analysis; Radar imaging; Remote sensing; Semantics; Synthetic aperture radar; Change detection; Cycle generative adversarial network; Detection methods; Multi sensor; Multi sensor images; Multi-modal data; Remote-sensing; SAR data; SAR Images; Unsupervised change detection; Change detection","CycleGAN; multi-modal data; multi-sensor images; Remote Sensing; SAR images; Unsupervised Change Detection","Conference paper","Final","","Scopus","2-s2.0-85142529145"
"Li Y.; Li J.; Du X.; Huang Y.; Lei J.","Li, Yan (57488735700); Li, Jianmin (57195612843); Du, Xiaofeng (29367449500); Huang, Yibo (57920502200); Lei, Jian (57920502300)","57488735700; 57195612843; 29367449500; 57920502200; 57920502300","An Improved Method for Pan-Sharpening Based on Pan-GAN","2022","2022 7th International Conference on Image, Vision and Computing, ICIVC 2022","","","","282","286","4","10.1109/ICIVC55077.2022.9887169","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139529228&doi=10.1109%2fICIVC55077.2022.9887169&partnerID=40&md5=6de64abf9be4a779fc6a50a5827ba5ad","Pan-sharpening refers to the fusion of pan images and low-resolution multispectral remote sensing images to obtain high-resolution multispectral images. Generative adversarial network (GAN)-based pan-sharpening methods have recently became popular due to the lack of ground-truth data during training. However, GAN-based methods suffer from training instability and convergence difficulties. To deal the issues, we propose a novel GAN-based pan-sharpening method using additional constraint. First, we adopt the geometric consistency constraint to enforce the network to preserve the spatial structure of image. Second, we introduce an attention mechanism in the generator to extract useful information through the features of pan image and multispectral images and pay more attention to meaningful regions. Experimental results show the effectiveness of our method in terms of the quantitative and visual results.  © 2022 IEEE.","Computer vision; Remote sensing; Attention mechanisms; Convergence difficulty; Geometric consistency; Ground truth data; High resolution; Lower resolution; Multispectral images; Multispectral remote sensing image; Network-based; Pan-sharpening; Generative adversarial networks","attention mechanism; Generative adversarial network; geometric consistency; pan-sharpening","Conference paper","Final","","Scopus","2-s2.0-85139529228"
"Li J.; Liao Y.; Zhang J.; Zeng D.; Qian X.","Li, Jia (57202721460); Liao, Yujia (57887530100); Zhang, Junjie (57201892853); Zeng, Dan (10538969100); Qian, Xiaoliang (36465575400)","57202721460; 57887530100; 57201892853; 10538969100; 36465575400","Semi-Supervised DEGAN for Optical High-Resolution Remote Sensing Image Scene Classification","2022","Remote Sensing","14","17","4418","","","","10.3390/rs14174418","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137861471&doi=10.3390%2frs14174418&partnerID=40&md5=9976c47fdb1bac8f5d018b16ff686d9a","Semi-supervised methods have made remarkable achievements via utilizing unlabeled samples for optical high-resolution remote sensing scene classification. However, the labeled data cannot be effectively combined with unlabeled data in the existing semi-supervised methods during model training. To address this issue, we present a semi-supervised optical high-resolution remote sensing scene classification method based on Diversity Enhanced Generative Adversarial Network (DEGAN), in which the supervised and unsupervised stages are deeply combined in the DEGAN training. Based on the unsupervised characteristic of the Generative Adversarial Network (GAN), a large number of unlabeled and labeled images are jointly employed to guide the generator to obtain a complete and accurate probability density space of fake images. The Diversity Enhanced Network (DEN) is designed to increase the diversity of generated images based on massive unlabeled data. Therefore, the discriminator is promoted to provide discriminative features by enhancing the generator given the game relationship between two models in DEGAN. Moreover, the conditional entropy is adopted to make full use of the information of unlabeled data during the discriminator training. Finally, the features extracted from the discriminator and VGGNet-16 are employed for scene classification. Experimental results on three large datasets demonstrate that the proposed scene classification method yields a superior classification performance compared with other semi-supervised methods. © 2022 by the authors.","Classification (of information); Deep learning; Discriminators; Image classification; Image enhancement; Large dataset; Optical remote sensing; Supervised learning; Diversity enhanced network; Features fusions; Fisher kernels; High-resolution remote sensing images; Improved fisher kernel; Optical high-resolution remote sensing image; Optical-; Scene classification; Semi-supervised; Semi-supervised scene classification; Generative adversarial networks","Diversity Enhanced Network; feature fusion; Generative Adversarial Network; Improved Fisher Kernel; optical high-resolution remote sensing image; semi-supervised scene classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137861471"
"Wang J.; Zhao J.; Sun H.; Lu X.; Huang J.; Wang S.; Fang G.","Wang, Jiahao (58001613400); Zhao, Junhao (58001842600); Sun, Hong (57224349226); Lu, Xiao (57348054300); Huang, Jixia (56105134800); Wang, Shaohua (57920125600); Fang, Guofei (45760990600)","58001613400; 58001842600; 57224349226; 57348054300; 56105134800; 57920125600; 45760990600","Satellite Remote Sensing Identification of Discolored Standing Trees for Pine Wilt Disease Based on Semi-Supervised Deep Learning","2022","Remote Sensing","14","23","5936","","","","10.3390/rs14235936","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143774916&doi=10.3390%2frs14235936&partnerID=40&md5=04cd3735232cf35c1ed51ee423b56f9f","Pine wilt disease (PWD) is the most dangerous biohazard of pine species and poses a serious threat to forest resources. Coupling satellite remote sensing technology and deep learning technology for the accurate monitoring of PWD is an important tool for the efficient prevention and control of PWD. We used Gaofen-2 remote sensing images to construct a dataset of discolored standing tree samples of PWD and selected three semantic segmentation models—DeepLabv3+, HRNet, and DANet—for training and to compare their performance. To build a GAN-based semi-supervised semantic segmentation model for semi-supervised learning training, the best model was chosen as the generator of generative adversarial networks (GANs). The model was then optimized for structural adjustment and hyperparameter adjustment. Aimed at the characteristics of Gaofen-2 images and discolored standing trees with PWD, this paper adopts three strategies—swelling prediction, raster vectorization, and forest floor mask extraction—to optimize the image identification process and results and conducts an application demonstration study in Nanping city, Fujian Province. The results show that among the three semantic segmentation models, HRNet was the optimal conventional semantic segmentation model for identifying discolored standing trees of PWD based on Gaofen-2 images and that its MIoU value was 68.36%. Additionally, the GAN-based semi-supervised semantic segmentation model GAN_HRNet_Semi improved the MIoU value by 3.10%, and its recognition segmentation accuracy was better than the traditional semantic segmentation model. The recall rate of PWD discolored standing tree monitoring in the demonstration area reached 80.09%. The combination of semi-supervised semantic segmentation technology and high-resolution satellite remote sensing technology provides new technical methods for the accurate wide-scale monitoring, prevention, and control of PWD. © 2022 by the authors.","Engineering education; Forestry; Generative adversarial networks; Remote sensing; Satellites; Semantic Segmentation; Semantics; Accurate monitoring; Pine wilt disease; Prevention and controls; Remote sensing technology; Satellite remote sensing; Segmentation models; Semantic segmentation; Semi-supervised; Standing tree; Deep learning","accurate monitoring; pine wilt disease (PWD); satellite remote sensing; semantic segmentation; semi-supervised","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143774916"
"Jia S.; Wang Z.; Li Q.; Jia X.; Xu M.","Jia, Sen (57681305300); Wang, Zhihao (57253694800); Li, Qingquan (55831292900); Jia, Xiuping (7201933692); Xu, Meng (57206827164)","57681305300; 57253694800; 55831292900; 7201933692; 57206827164","Multiattention Generative Adversarial Network for Remote Sensing Image Super-Resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5624715","","","","10.1109/TGRS.2022.3180068","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131761657&doi=10.1109%2fTGRS.2022.3180068&partnerID=40&md5=3ff02c05e0246c450feae23e75918e27","Image super-resolution (SR) methods can generate remote sensing images with high spatial resolution without increasing the cost of acquisition equipment, thereby providing a feasible way to improve the quality of remote sensing images. Clearly, image SR is a severe ill-posed problem. With the development of deep learning, the powerful fitting ability of deep neural networks has solved this problem to some extent. Since the texture information of various remote sensing images are totally different from each other, in this article, we proposed a network based on generative adversarial network (GAN) to achieve high-resolution remote sensing images, named multiattention GAN (MA-GAN). The main body of the generator in MA-GAN contains three blocks: pyramid convolutional residual dense (PCRD) block, attention-based upsampling (AUP) block, and attention-based fusion (AF) block. Specifically, the developed attention pyramid convolutional (AttPConv) operator in the PCRD block combines multiscale convolution and channel attention (CA) to automatically learn and adjust the scale of residuals for better representation. The established AUP block uses pixel attention (PA) to perform arbitrary scales of upsampling. The AF block uses branch attention (BA) to integrate upsampled low-resolution images with high-level features. Besides, the loss function takes both adversarial loss and feature loss into consideration to guide the learning procedure of the generator. We have compared our MA-GAN approach with several state-of-the-art methods on a number of remote sensing scenes, and the experimental results consistently demonstrate the effectiveness of the proposed MA-GAN. For study replication, the source code will be released at: https://github.com/ZhihaoWang1997/MA-GAN.  © 1980-2012 IEEE.","Convolution; Deep neural networks; Image enhancement; Image resolution; Remote sensing; Signal sampling; Generative adversarial network; Generator; Image super resolutions; Remote sensing images; Remote-sensing; Spatial resolution; Super resolution; Superresolution; Task analysis; Upsampling; artificial neural network; experimental study; remote sensing; satellite data; Generative adversarial networks","Generative adversarial network (GAN); remote sensing image; super-resolution (SR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131761657"
"Zhu W.; Zhang L.","Zhu, Wanning (57221264507); Zhang, Libao (35325855000)","57221264507; 35325855000","Pan-Sharpening Based on Joint Visual Saliency Analysis and Parallel Bidirectional Network","2022","IEEE Geoscience and Remote Sensing Letters","19","","6516805","","","","10.1109/LGRS.2022.3209787","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139530693&doi=10.1109%2fLGRS.2022.3209787&partnerID=40&md5=185037328fafed47da660821ccc35a07","In remote sensing (RS) images, the demands for spectral and spatial quality of different regions are different, which means that the unified fusion strategy on the whole image is not suitable for pan-sharpening task. Saliency, derived from visual attention mechanism, provides an effective way to satisfy these demands. Inspired by this, we propose a novel pan-sharpening method based on joint visual saliency analysis and parallel bidirectional network (JSPBN). First, considering the complex scenes and uneven distribution of targets in RS images, we develop a Bayesian optimization-based joint visual saliency analysis (B-JVSA) method that integrates prior saliency based on global color contrast with likelihood saliency based on joint co-occurrence histogram, which can highlight common salient regions while suppressing individual ones and irrelevant background by exploring the correlation among multiple RS images. Second, we construct a parallel bidirectional feature pyramid (PBFP) network to obtain coarse fusion features, fully considering individual characteristics of panchromatic (PAN) images and multispectral (MS) images. Finally, we design a saliency-aware layer (SAL) according to B-JVSA to further refine the fusion effect in salient regions and nonsalient regions. With the help of SAL, diverse strategies for certain regions are learned through two independent residual dense networks (RDNs) and thereby generating accurate fusion results. Experimental results show that our proposal performs better than the competing methods in both spatial quality enhancement and spectral fidelity preservation.  © 2004-2012 IEEE.","Behavioral research; Generative adversarial networks; Image analysis; Image fusion; Quality control; Visualization; Bayes method; Bidirectional networks; Features extraction; Histogram; Image color analysis; Joint visual saliency analyse; Pan-sharpening; Parallel bidirectional network; Remote-sensing; Saliency analysis; Visual saliency; Bayesian analysis; complexity; demand analysis; histogram; maximum likelihood analysis; remote sensing; satellite imagery; spatial analysis; strategic approach; visual analysis; Remote sensing","Joint visual saliency analysis; pan-sharpening; parallel bidirectional network; remote sensing (RS)","Article","Final","","Scopus","2-s2.0-85139530693"
"Salaudeen H.; Çelebi E.","Salaudeen, Habeeb (57481579000); Çelebi, Erbuğ (35101468100)","57481579000; 35101468100","Pothole Detection Using Image Enhancement GAN and Object Detection Network","2022","Electronics (Switzerland)","11","12","1882","","","","10.3390/electronics11121882","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131893482&doi=10.3390%2felectronics11121882&partnerID=40&md5=e4f337cd6b2d8168d35f0dbff1484921","Many datasets used to train artificial intelligence systems to recognize potholes, such as the challenging sequences for autonomous driving (CCSAD) and the Pacific Northwest road (PNW) datasets, do not produce satisfactory results. This is due to the fact that these datasets present complex but realistic scenarios of pothole detection tasks than popularly used datasets that achieve better results but do not effectively represents realistic pothole detection task. In remote sensing, super-resolution generative adversarial networks (GAN), such as enhanced super-resolution generative adversarial networks (ESRGAN), have been employed to mitigate the issues of small-object detection, which has shown remarkable performance in detecting small objects from low-quality images. Inspired by this success in remote sensing, we apply similar techniques with an ESRGAN super-resolution network to improve the image quality of road surfaces, and we use different object detection networks in the same pipeline to detect instances of potholes in the images. The architecture we propose consists of two main components: ESRGAN and a detection network. For the detection network, we employ both you only look once (YOLOv5) and EfficientDet networks. Comprehensive experiments on different pothole detection datasets show better performance for our method compared to similar state-of-the-art methods for pothole detection. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","deep learning; GAN; object detection; pothole detection; small object detection; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131893482"
"Liu J.; Hou W.; Luo X.; Su J.; Hou Y.; Wang Z.","Liu, Juntao (57927941800); Hou, Weimin (57207877783); Luo, Xin (57196018157); Su, Jia (57887304900); Hou, Yanli (57927309500); Wang, Zhenzhou (9737204400)","57927941800; 57207877783; 57196018157; 57887304900; 57927309500; 9737204400","SI-SA GAN: A Generative Adversarial Network Combined With Spatial Information and Self-Attention for Removing Thin Cloud in Optical Remote Sensing Images","2022","IEEE Access","10","","","114318","114330","12","10.1109/ACCESS.2022.3213354","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139864763&doi=10.1109%2fACCESS.2022.3213354&partnerID=40&md5=639d995153760818dd685d874570bb3c","In agricultural remote sensing monitoring, climate often affects the quality of optical remote sensing image data acquisition. The acquired satellite imagery results usually contain cloud information, leading to a lack of ground data information. Unlike thick clouds, the semi-transparent nature of thin clouds prevents thin clouds from completely obscuring the ground scene. In order to remove thin clouds in the cultivated land and restore the actual ground information as much as possible, we proposed a cloud removal method of spatial information fusion self-attention generative adversarial network (SI-SA GAN) based on multi-directional perceptual attention and self-attention mechanism. The proposed method identifies and focuses on cloud regions using spatial attention, channel attention, and self-attention mechanism, which can enhance image information. The modules of the discriminator utilize residual networks and self-attention non-local neural networks to guide image information output. The generative adversarial network (GAN) is applied to remove clouds and restore the corresponding irregular occlusion area according to the depth characteristics of the input information. A gradient penalty is applied to improve the robustness of the generative network. In this paper, we compared the evaluation indexes of other advanced models. The qualitative and quantitative results of Sentinel-2A and public RICE datasets confirmed that the proposed method could enhance image quality effectively after cloud removal. The model has excellent thin cloud removal performance with small-scale training data.  © 2013 IEEE.","Data acquisition; Generative adversarial networks; Image enhancement; Image reconstruction; Optical remote sensing; Restoration; Satellite imagery; Cloud removal; Features extraction; Optical remote sensing; Remote-sensing; Self-attention; Spatial informations; Spatial multi-directional perception; Spatial resolution; Thin cloud removal; Agriculture","GAN; Remote sensing; self-attention; spatial multi-directional perception; thin cloud removal","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139864763"
"Wang Z.; Xu N.; Wang B.; Liu Y.; Zhang S.","Wang, Zhen (57226052226); Xu, Nan (57195337717); Wang, Buhong (14042909700); Liu, Yaohui (57206823558); Zhang, Shanwen (57431944600)","57226052226; 57195337717; 14042909700; 57206823558; 57431944600","Urban building extraction from high-resolution remote sensing imagery based on multi-scale recurrent conditional generative adversarial network","2022","GIScience and Remote Sensing","59","1","","861","884","23","10.1080/15481603.2022.2076382","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130721135&doi=10.1080%2f15481603.2022.2076382&partnerID=40&md5=d5dddc115f49b59375fb811a5c9e863f","Urban building extraction from high-resolution remote sensing imagery is important for urban planning, population statistics, and disaster assessment. However, the high density and slight boundary differences of urban building regions pose a great challenge for accurate building extraction. Although existing building extraction methods have achieved better results in urban building extraction, there are still some problems, such as boundary information loss, poor extraction effect for dense regions, and serious interference by building shadows. To accurately extract building regions from high-resolution remote sensing images, in this study, we propose a practical method for building extraction based on convolution neural networks (CNNs). Firstly, the multi-scale recurrent residual convolution is introduced into the generative network to extract the multi-scale and multi-resolution features of remote sensing images. Secondly, the attention gates skip connection (AGs) is used to enhance the information interaction between different scale features. Finally, the adversarial network with parallel architecture is used to decrease the difference between the extracted results and the ground truths. Moreover, the conditional information constraint is introduced in the training process to improve robustness and generalization ability of the proposed method. The qualitative and quantitative analyses are performed on IAILD and Massachusetts datasets. The experimental results show that the proposed method can accurately and effectively extract building regions from remote sensing images. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","accuracy assessment; artificial neural network; building; data set; efficiency measurement; image processing; image resolution; quantitative analysis; remote sensing; satellite imagery; urban area","conditional information constraint; convolutional neural network (CNN); generative adversarial network (GAN); Remote sensing imagery; urban building extraction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130721135"
"Song B.; Liu P.; Li J.; Wang L.; Zhang L.; He G.; Chen L.; Liu J.","Song, Bingze (57274635700); Liu, Peng (57075315400); Li, Jun (24481713500); Wang, Lizhe (23029267900); Zhang, Luo (57712604600); He, Guojin (14028364400); Chen, Lajiao (55647804800); Liu, Jianbo (56055157000)","57274635700; 57075315400; 24481713500; 23029267900; 57712604600; 14028364400; 55647804800; 56055157000","MLFF-GAN: A Multilevel Feature Fusion With GAN for Spatiotemporal Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4410816","","","","10.1109/TGRS.2022.3169916","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130860119&doi=10.1109%2fTGRS.2022.3169916&partnerID=40&md5=54886bdbfe431356842d4961a6fbee4e","Due to the limitation of technology and budget, it is often difficult for sensors of a single remote sensing satellite to have both high-temporal and high-spatial (HTHS) resolution at the same time. In this article, we proposed a new multilevel feature fusion with generative adversarial network (MLFF-GAN) for generating fusion HTHS images. The MLFF-GAN mainly uses U-net-like architecture, and its generator is composed of three stages: feature extraction, feature fusion, and image reconstruction. In the feature extraction and reconstruction stage, the generator employs the encoding and decoding structure to extract three groups of multilevel features (MLFs), which can cope with the huge difference of resolution between high-resolution images and low-resolution images. In the feature fusion stage, adaptive instance normalization (AdaIN) block is designed to learn the global distribution relationship between multitemporal images, and an attention module (AM) is used to learn the local information weights for the change of small areas. The proposed MLFF-GAN was tested on two Landsat and Moderate-Resolution Imaging Spectroradiometer (MODIS) datasets. Some state-of-the-art algorithms are comprehensively compared with MLFF-GAN. We also carried on the ablation experiment to test the effectiveness of different submodules in the MLFF-GAN. The experiment results and ablation analysis show the better performances of the proposed method when compared with other methods. The code is available at https://github.com/songbingze/MLFF-GAN. © 1980-2012 IEEE.","Ablation; Budget control; Extraction; Generative adversarial networks; Image fusion; Image reconstruction; Image resolution; Remote sensing; Adaptive instance normalization; Attention mechanisms; Features extraction; Generative adversarial network; Generator; Images reconstruction; Normalisation; Spatial attention; Spatial attention mechanism; Spatial resolution; Spatiotemporal phenomenon; U-net; artificial neural network; Landsat; MODIS; reconstruction; remote sensing; satellite sensor; spatial resolution; Feature extraction","Adaptive instance normalization (AdaIN); generative adversarial network (GAN); spatial attention mechanism; U-net","Article","Final","","Scopus","2-s2.0-85130860119"
"Chen Y.-H.; Ha T.D.; Erricolo D.; Chen P.-Y.","Chen, Yi-Huan (57218452501); Ha, Trung Dung (57455303300); Erricolo, Danilo (6602988232); Chen, Pai-Yen (36087152100)","57218452501; 57455303300; 6602988232; 36087152100","Restoration of Antenna Radiation Pattern Using Conditional Generative Adversarial Network","2022","2022 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting, AP-S/URSI 2022 - Proceedings","","","","481","482","1","10.1109/AP-S/USNC-URSI47032.2022.9887123","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139787770&doi=10.1109%2fAP-S%2fUSNC-URSI47032.2022.9887123&partnerID=40&md5=c2387ce5dd52c8bfab7555c70d8d8be7","A deep learning model based on the conditional generative adversarial network (c-GAN) is proposed to accelerate the measurement of radiation and scattering patterns. Recently, GAN has been proposed to effectively restore an image from its fragments and blank areas. Here, we further leverage the GAN model to fully restore the antenna radiation pattern with sparse solid angles, thus greatly saving the characterization time and cost. We envision that the proposed GAN-assisted measurement method may be beneficial for various applications, including radar, remote sensing, and optimal deployment of 5G/6G networks. © 2022 IEEE.","5G mobile communication systems; Deep learning; Directional patterns (antenna); Image reconstruction; Radar antennas; Radar measurement; Remote sensing; Restoration; Antennas radiation patterns; Learning models; Measurement methods; Measurements of; Model-based OPC; Optimal deployment; Radar remote sensing; Scattering pattern; Solid angle; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85139787770"
"Cai Y.; Yang Y.; Shang Y.; Chen Z.; Shen Z.; Yin J.","Cai, Yuxiang (57392976200); Yang, Yingchun (57881064400); Shang, Yongheng (57881064500); Chen, Zhenqian (57397967400); Shen, Zhengwei (57272299200); Yin, Jianwei (8249720800)","57392976200; 57881064400; 57881064500; 57397967400; 57272299200; 8249720800","IterDANet: Iterative Intra-Domain Adaptation for Semantic Segmentation of Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5629517","","","","10.1109/TGRS.2022.3203040","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137580817&doi=10.1109%2fTGRS.2022.3203040&partnerID=40&md5=05dfd42845b7d3ee6652f6372aaee965","When segmenting the continuous proliferation of unlabeled remotely sensed images, unsupervised domain adaptation (UDA) has become one of the most critical techniques and achieved significant performance. But, in fact, there still exists a large performance gap between the existing UDA frameworks and supervised learning methods, for the majority of UDA frameworks do not consider the intra-domain gap in the target domain. In this article, to further minimize the complex intra-domain shift within the target domain in remote sensing, we propose a novel iterative intra-domain adaptation framework (IterDANet), which conducts inter-domain adaptation (InterDA), entropy-based ranking (ER), and iterative intra-domain adaptation (IntraDA). Specifically, first, to enhance the performance of InterDA built upon generative adversarial network (GAN)-based image-to-image (I2I) translation, we propose a new generator selection strategy (GSS) to assess and choose a well-trained generator for the inter-domain classifier. Then, to produce more accurate pseudolabels for IntraDA, we propose a new pseudolabel generation strategy (PLGS) to remove both high-entropy and low-confident pixels in predicted maps of inter-domain classifier. Finally, to better reduce the intra-domain gap, we propose to cluster all the target images into multiple subdomains using ER and iteratively align the cleanest subdomain with other noisy subdomains. The extensive experiments on the benchmark dataset, which includes cross-city aerial images, highlight the superiority and effectiveness of our IterDANet against the state-of-the-art UDA frameworks.  © 1980-2012 IEEE.","Antennas; Entropy; Image enhancement; Iterative methods; Remote sensing; Semantics; Supervised learning; Adversarial learning; Adversarial machine learning; Domain adaptation; Features extraction; Generator; Image translation; Image-to-image translation (I2I); Images segmentations; Inter-domain; Inter-domain adaptation; Intra-domain; Intra-domain adaptation; Machine-learning; Remote sensing images; Remote-sensing; Self-supervised learning; Unsupervised domain adaptation; machine learning; remote sensing; satellite imagery; segmentation; semantic standardization; supervised learning; unsupervised classification; Image segmentation","Adversarial learning; image-to-image (I2I) translation; inter-domain adaptation (InterDA); intra-domain adaptation (IntraDA); remote sensing images; self-supervised learning (SSL); unsupervised domain adaptation (UDA)","Article","Final","","Scopus","2-s2.0-85137580817"
"Wang R.; Teng D.; Yu W.; Zhang X.; Zhu J.","Wang, Ruifu (8691633200); Teng, Dongdong (57800002100); Yu, Wenqing (57216335510); Zhang, Xi (56308898300); Zhu, Jinshan (56350699000)","8691633200; 57800002100; 57216335510; 56308898300; 56350699000","Improvement and Application of a GAN Model for Time Series Image Prediction—A Case Study of Time Series Satellite Cloud Images","2022","Remote Sensing","14","21","5518","","","","10.3390/rs14215518","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141859631&doi=10.3390%2frs14215518&partnerID=40&md5=b490cf9667766bde1b271fafb887ef22","Predicting the shape evolution and movement of remote sensing satellite cloud images is a difficult task requiring the effective monitoring and rapid prediction of thunderstorms, gales, rainstorms, and other disastrous weather conditions. We proposed a generative adversarial network (GAN) model for time series satellite cloud image prediction in this research. Taking time series information as the constraint condition and abandoning the assumption of linear and stable changes in cloud clusters in traditional methods, the GAN model is used to automatically learn the data feature distribution of satellite cloud images and predict time series cloud images in the future. Through comparative experiments and analysis, the Mish activation function is selected for integration into the model. On this basis, three improvement measures are proposed: (1) The Wasserstein distance is used to ensure the normal update of the GAN model parameters; (2) establish a multiscale network structure to improve the long-term performance of model prediction; (3) combined image gradient difference loss (GDL) to improve the sharpness of prediction cloud images. The experimental results showed that for the prediction cloud images of the next four times, compared with the unimproved Mish-GAN model, the improved GDL-GAN model improves the PSNR and SSIM by 0.44 and 0.02 on average, and decreases the MAE and RMSE by 18.84% and 7.60% on average. It is proven that the improved GDL-GAN model can maintain good visualization effects while keeping the overall changes and movement trends of the prediction cloud images relatively accurate, which is helpful to achieve more accurate weather forecast. The cooperation ability of satellite cloud images in disastrous weather forecasting and early warning is enhanced. © 2022 by the authors.","Activation analysis; Generative adversarial networks; Image enhancement; Remote sensing; Storms; Weather forecasting; Case-studies; Cloud image; Condition; Image prediction; Network models; Remote sensing satellites; Satellite cloud images; Shape evolution; Time series informations; Times series; Time series","GAN; image prediction; satellite cloud image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141859631"
"S. A.; A.A. B.","S., Ansith (57681084400); A.A., Bini (57682754200)","57681084400; 57682754200","Land use classification of high resolution remote sensing images using an encoder based modified GAN architecture","2022","Displays","74","","102229","","","","10.1016/j.displa.2022.102229","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129911354&doi=10.1016%2fj.displa.2022.102229&partnerID=40&md5=006a882a0cfcb6a307ec259e3116eb95","The development of new deep learning algorithms has brought in a significant change in land use classification. Earlier models in remote sensing image classification were mainly based on machine learning architectures. Many recent models combined deep neural network structures for feature extraction and machine learning algorithms for classification. There are also some end to end deep learning architectures used for feature extraction and classification in the area of remote sensing image classification. Anyway such models are known to take more training data to yield good classification performance and they also necessitate more extensive technical support. The proposed model uses an encoder based modified Generative Adversarial Network (GAN) architecture for land use classification which can yield better results with less amount of training samples. GAN is shown to be a superior synthesiser and classifier than other deep learning models because it can work with a limited number of dataset samples. The validation of the proposed model is carried out on UC Merced and AID land use datasets. The classification results support the hypothesis that the proposed model can outperform conventional land use classification models. © 2022 Elsevier B.V.","Classification (of information); Deep neural networks; Extraction; Feature extraction; Image classification; Land use; Learning algorithms; Network architecture; Remote sensing; Signal encoding; AID lsnd use dataset; Change in land use; Deep learning; High-resolution remote sensing images; Landuse classifications; Neural networks structure; Remote sensing image classification; Remote-sensing; Scene classification; UC merced land use dataset; Generative adversarial networks","AID lsnd use dataset; Deep learning; GAN; Land use classification; Remote sensing; Scene classification; UC Merced land use dataset","Article","Final","","Scopus","2-s2.0-85129911354"
"Chen W.; Zhou G.; Liu Z.; Li X.; Zheng X.; Wang L.","Chen, Weitao (55822747800); Zhou, Gaodian (57219604979); Liu, Zhuoyue (57803679000); Li, Xianju (41261664800); Zheng, Xiongwei (57413797000); Wang, Lizhe (23029267900)","55822747800; 57219604979; 57803679000; 41261664800; 57413797000; 23029267900","NIGAN: A Framework for Mountain Road Extraction Integrating Remote Sensing Road-Scene Neighborhood Probability Enhancements and Improved Conditional Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5626115","","","","10.1109/TGRS.2022.3188908","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134260657&doi=10.1109%2fTGRS.2022.3188908&partnerID=40&md5=33b1561462434e87b4612a59d28b536c","Mountain roads are a source of important basic geographic data used in various fields. The automatic extraction of road images through high-resolution remote sensing imagery using deep learning has attracted considerable attention. But the interference of context information limited extraction accuracy, especially for roads in mountain area. Furthermore, when pursuing research in a new district, many algorithms are difficult to train due to a lack of data. To address these issues, a framework based on remote sensing road-scene neighborhood probability enhancement and improved conditional generative adversarial network (NIGAN) is proposed in this article. This framework can be divided into two sections: 1) road scenes classification section. A remote sensing road-scene neighborhood confidence enhancement method was designed for classifying road scenes of the study area to reduce the impact of nonroad information on subsequent fine-road segmentation and 2) fine-road segmentation section. An improved dilated convolution module, which is helpful in extracting small objects such as road, was added into the conditional generative adversarial network (CGAN) to increase the receptive field and pay attention to global information, and segment roads from the results of road scenes classification section. To validate the NIGAN framework, new mountain road-scene and label datasets were constructed, and diverse comparison experiments were performed. The results indicate that the NIGAN framework can improve the integrity and accuracy of mountain road-scene extraction in diverse and complex conditions. The results further confirm the validity of the NIGAN framework in small samples. In addition, the mountain road-scene datasets can serve as benchmark datasets for studying mountain road extraction.  © 1980-2012 IEEE.","Classification (of information); Data mining; Deep learning; Extraction; Generative adversarial networks; Landforms; Remote sensing; Roads and streets; Semantic Segmentation; Semantics; Deep learning; Features extraction; Images segmentations; Remote-sensing; Road; Road extraction; Scene classification; Semantic segmentation; Ziyuan-3; extraction; neighborhood; remote sensing; road; semantic standardization; Convolution","Deep learning; generative adversarial network (GAN); remote sensing; road extraction; scene classification; semantic segmentation; ZiYuan-3","Article","Final","","Scopus","2-s2.0-85134260657"
"Yang Y.; You Q.; Jin Z.; Zuo Z.; Zhang Y.","Yang, Ye (57931608900); You, Qinglong (22982141200); Jin, Zheng (57716397100); Zuo, Zhiyan (36116390600); Zhang, Yuqing (56596869400)","57931608900; 22982141200; 57716397100; 36116390600; 56596869400","Construction of surface air temperature over the Tibetan Plateau based on generative adversarial networks","2022","International Journal of Climatology","42","16","","10107","10125","18","10.1002/joc.7886","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140091386&doi=10.1002%2fjoc.7886&partnerID=40&md5=4682bf2f70649b42d5f3e955f5fae4b7","In situ measurements are the most important basis of obtaining precise meteorological datasets. However, it is difficult to accurately extrapolate the plausible meteorological field in certain regions based on in situ measurements alone, especially in areas with complex topography like the Tibetan Plateau (TP). Gridded products, remote sensing, and data assimilation technique overcome this problem but they have their own weaknesses, such as low resolution, huge computation, and time-consuming. Here we applied the state-of-the-art generative adversarial networks (GAN) in image inpainting to construct the surface temperature over the TP combined with high-resolution China Meteorological Forcing Dataset (CMFD) air temperature product and in situ observations. In this study, the surface air temperature at 2 m dataset with grids of 0.1° over the TP from 1979 to 2020 based on GAN-based model have been generated, which can capture the spatial patterns and trends of surface air temperature over the TP. Moreover, the GAN-based daily temperature has higher correlation coefficients (0.99) and lower root-mean-square errors (1.06°C) than CMFD (0.98, 1.43°C) and reanalysis products when compared with in situ observations. Finally, the temperature fields over the TP from 2019 to 2020 is constructed, whereas CMFD is only available from 1979 to 2018. The results reflect the advantages of GAN on big-data computation and fully utilizing effective information over the TP, and GAN-based temperature dataset can be used to detect climate change over the region. This study demonstrated the potential and applicability of artificial intelligence for developing and extending practicable high-resolution meteorological dataset over regions with sparse stations and rugged topography. © 2022 Royal Meteorological Society.","China; Qinghai-Xizang Plateau; Atmospheric temperature; Climate change; Large dataset; Mean square error; Remote sensing; Topography; High resolution; In-situ measurement; In-situ observations; Meteorological datasets; Meteorological fields; Meteorological forcing; Network-based; Region-based; Surface air temperatures; Tibetan Plateau; air temperature; data set; in situ measurement; machine learning; precision; regional climate; spatial analysis; trend analysis; Generative adversarial networks","construction; generative adversarial networks; temperature; Tibetan Plateau","Article","Final","","Scopus","2-s2.0-85140091386"
"Chen J.; Li H.; Song L.; Zhang G.; Hu B.; Wang S.; Liu S.; Li S.; Chen T.; Liu J.","Chen, Junyu (57212282255); Li, Haiwei (55116636600); Song, Liyao (57213686884); Zhang, Geng (56549154000); Hu, Bingliang (26660863200); Wang, Shuang (57206898653); Liu, Song (57211684649); Li, Siyuan (55975911700); Chen, Tieqiao (56074357900); Liu, Jia (57258730500)","57212282255; 55116636600; 57213686884; 56549154000; 26660863200; 57206898653; 57211684649; 55975911700; 56074357900; 57258730500","Synthetic aircraft RS image modelling based on improved conditional GAN joint embedding network","2022","Scientific Reports","12","1","320","","","","10.1038/s41598-021-03880-x","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122848825&doi=10.1038%2fs41598-021-03880-x&partnerID=40&md5=203b3971974556565cfdfd9ed11ab4a4","Developing an efficient and quality remote sensing (RS) technology using volume and efficient modelling in different aircraft RS images is challenging. Generative models serve as a natural and convenient simulation method. Because aircraft types belong to the fine class under the rough class, the issue of feature entanglement may occur while modelling multiple aircraft classes. Our solution to this issue was a novel first-generation realistic aircraft type simulation system (ATSS-1) based on the RS images. It realised fine modelling of the seven aircraft types based on a real scene by establishing an adaptive weighted conditional attention generative adversarial network and joint geospatial embedding (GE) network. An adaptive weighted conditional batch normalisation attention block solved the subclass entanglement by reassigning the intra-class-wise characteristic responses. Subsequently, an asymmetric residual self-attention module was developed by establishing a remote region asymmetric relationship for mining the finer potential spatial representation. The mapping relationship between the input RS scene and the potential space of the generated samples was explored through the GE network construction that used the selected prior distribution z, as an intermediate representation. A public RS dataset (OPT-Aircraft_V1.0) and two public datasets (MNIST and Fashion-MNIST) were used for simulation model testing. The results demonstrated the effectiveness of ATSS-1, promoting further development of realistic automatic RS simulation. © 2022, The Author(s).","aircraft; article; attention; embedding; mining; remote sensing; simulation","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85122848825"
"Li D.; Liu J.; Liu F.; Zhang W.; Zhang A.; Gao W.; Shi J.","Li, Donghui (57770324900); Liu, Jia (56376104500); Liu, Fang (56182993400); Zhang, Wenhua (57206485064); Zhang, Andi (57266976500); Gao, Wenfei (57754996000); Shi, Jiao (55553885300)","57770324900; 56376104500; 56182993400; 57206485064; 57266976500; 57754996000; 55553885300","A Dual-Fusion Semantic Segmentation Framework with Gan for SAR Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","991","994","3","10.1109/IGARSS46834.2022.9884931","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141895126&doi=10.1109%2fIGARSS46834.2022.9884931&partnerID=40&md5=ed64ff977f503b38eb25f0b424286ecb","Deep learning based semantic segmentation is one of the popular methods in remote sensing image segmentation. In this paper, a network based on the widely used encoder-decoder architecture is proposed to accomplish the synthetic aperture radar (SAR) images segmentation. With the better representation capability of optical images, we propose to enrich SAR images with generated optical images via the generative adversative network (GAN) trained by numerous SAR and optical images. These optical images can be used as expansions of original SAR images, thus ensuring robust result of segmentation. Then the optical images generated by the GAN are stitched together with the corresponding real images. An attention module following the stitched data is used to strengthen the representation of the objects. Experiments indicate that our method is efficient compared to other commonly used methods. © 2022 IEEE.","Decoding; Deep learning; Generative adversarial networks; Geometrical optics; Optical remote sensing; Radar imaging; Semantics; Signal encoding; Synthetic aperture radar; Encoder-decoder; Encoder-decoder architecture; Encoder-decoder framework; Images segmentations; Network-based; Optical image; Remote sensing images; Semantic segmentation; Synthetic Aperture Radar image segmentations; Synthetic aperture radar images; Semantic Segmentation","encoder-decoder framework; SAR images; Semantic segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141895126"
"Zhao Z.; Wang S.; Wang X.; Zhong Y.","Zhao, Zhiruo (57894151800); Wang, Shaoyu (57192818610); Wang, Xinyu (57189647737); Zhong, Yanfei (12039673900)","57894151800; 57192818610; 57189647737; 12039673900","An Improved Deep Novel Target Detection Method for Mars Rover Multispectral Imagery; [一种改进的火星车多光谱影像深度新颖目标探测方法]","2022","Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University","47","8","","1328","1335and1348","13350020","10.13203/j.whugis20220119","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138162474&doi=10.13203%2fj.whugis20220119&partnerID=40&md5=d7ab90b3db13d6520e441eb631d24901","Objectives Mars is the main target object for deep space exploration. Mars rovers, or roving probes, are important tools for surface exploration and scientific research on Mars. For the growing amount of remote sensing data collected by Mars rovers, there is an urgent need for a method that can intelligently detect novel targets of scientific value from the massive amount of images, reduce the time cost of detection planning, and provide information for subsequent scientific analysis. The traditional novel detection methods mostly include distance-based metrics and image-based reconstruction methods, distance-based metrics calculate novel scores pixel by pixel without considering spatial contextual information, and image-based reconstruction methods focus on reconstruction of typical landscape backgrounds, and novelty is manifested by image reconstruction errors, which is not effective in extracting small novel targets such as boreholes and dust removal points. Methods To address the above problems of traditional novel detection methods in Mars rover novel target detection, this paper proposes an improved Mars rover multispectral image depth novel target detection method, uses full convolutional self-coding neural network to extract deep features for typical landscape reconstruction, and joints Mahalanobis distance for novel target and typical landscape background separation, fully exploits the spatial and spectral dimensional features to improve the accuracy of Mars rover novel target detection results. Results The experiments use the multispectral image dataset of Curiosity rover released by NASA (National Aeronautics and Space Administration), and the proposed convolution auto-encoder combined Mahalanobis distance method(CAE-M) is compared with Reed-Xiaoli detector, principal component analysis, convolution auto-encoder convolution, and generative adversarial networks on the surface of Gale crater. The results show that CAE-M outperforms previous detection methods in terms of detection accuracy and visual interpretation, and has a balanced and stable performance in different classes of novel target detection. Conclusion The proposed CAE-M method comprehensively utilizes spatial and spectral information of multispectral images, which can help Mars rover exploration missions to quickly and intelligently screen and sort novel data with scientific value in massive data, save the time and cost of route planning, improve scientific returns. © 2022 Wuhan University. All rights reserved.","Cost benefit analysis; Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; NASA; Pixels; Principal component analysis; Remote sensing; Deep learning; Deep-space exploration; Detection methods; Distance-based; Image-based; Mars Rovers; Multispectral images; Novel target detection; Scientific values; Targets detection; artificial neural network; detection method; exploration; Mars; multispectral image; reconstruction; remote sensing; spacecraft; Convolution","deep learning; deep space exploration; multispectral image; novel target detection; rover","Article","Final","","Scopus","2-s2.0-85138162474"
"Wang B.; Zhu L.; Guo X.; Wang X.; Wu J.","Wang, Biao (57205222127); Zhu, Lingxuan (57546615400); Guo, Xing (55490606700); Wang, Xiaobing (45061262200); Wu, Jiaji (8567123700)","57205222127; 57546615400; 55490606700; 45061262200; 8567123700","SDTGAN: Generation Adversarial Network for Spectral Domain Translation of Remote Sensing Images of the Earth Background Based on Shared Latent Domain","2022","Remote Sensing","14","6","1359","","","","10.3390/rs14061359","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126912132&doi=10.3390%2frs14061359&partnerID=40&md5=fe45ef2d4a1a1982a4b2a29a320024b9","The synthesis of spectral remote sensing images of the Earth’s background is affected by various factors such as the atmosphere, illumination and terrain, which makes it difficult to simulate random disturbance and real textures. Based on the shared latent domain hypothesis and generation adversarial network, this paper proposes the SDTGAN method to mine the correlation between the spectrum and directly generate target spectral remote sensing images of the Earth’s background according to the source spectral images. The introduction of shared latent domain allows multispectral domains connect to each other without the need to build a one-to-one model. Meanwhile, additional feature maps are introduced to fill in the lack of information in the spectrum and improve the geographic accuracy. Through supervised training with a paired dataset, cycle consistency loss, and perceptual loss, the uniqueness of the output result is guaranteed. Finally, the experiments on the Fengyun satellite observation data show that the proposed SDTGAN method performs better than the baseline models in remote sensing image spectrum translation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Earth atmosphere; Remote sensing; Spectroscopy; Strain measurement; Textures; Adversarial networks; Earth background; Paired translation; Random disturbances; Random reals; Remote sensing images; Spectra's; Spectral domain translation; Spectral domains; Spectral images; Generative adversarial networks","Generative adversarial network; Paired translation; Remote sensing image; Spectral domain translation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126912132"
"Nie W.; Gou P.; Liu Y.; Zhou T.; Xu N.; Wang P.; Du Q.","Nie, Wei (57854867800); Gou, Peng (57853888900); Liu, Yang (57859610000); Zhou, Tianyu (57854169000); Xu, Nuo (57200211778); Wang, Peng (57309244300); Du, QiQi (57854316300)","57854867800; 57853888900; 57859610000; 57854169000; 57200211778; 57309244300; 57854316300","A semi-supervised image segmentation method based on generative adversarial network","2022","IEEE Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","2022-June","","","1217","1223","6","10.1109/ITAIC54216.2022.9836504","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136446743&doi=10.1109%2fITAIC54216.2022.9836504&partnerID=40&md5=ef31aef70a3ebb54858d8ea1c063c366","Semantic segmentation based on deep learning is an important research direction in computer vision, and it has developed rapidly in intelligent interpretation of remote sensing images in recent years. However, semantic segmentation based on deep learning is also a long-standing challenge in remote sensing image applications. The main reason is that training a neural network requires a large amount of pixel-level labeled data, and these publicly available labeled data are usually few. It requires a lot of labor costs. To address this issue, this paper proposes a semi-supervised semantic segmentation method based on Generative adversarial networks (GAN). Compared with previous techniques, we extends typical GAN networks to pixel-level prediction and discrimination, and applies it to remote sensing image semantic segmentation. We introduce residual networks and dilated convolutions into the generator, and employ a flow alignment module (FAM) to learn the semantic flow between adjacent hierarchical feature maps. The connected discriminator formulates the training as the min-maximum optimization problem. Comprehensive quantitative and qualitative evaluations on multiple models show that our proposed method outperforms state-of-the-art semi-supervised image segmentation algorithms.  © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Pixels; Semantic Segmentation; Semantics; Wages; Image applications; Images segmentations; Intelligent interpretation; Labeled data; Pixel level; Remote sensing images; Remote-sensing; Segmentation methods; Semantic segmentation; Semi-supervised; Remote sensing","generative adversarial network; image segmentation; remote sensing; semi-supervised","Conference paper","Final","","Scopus","2-s2.0-85136446743"
"Gao P.; Tian T.; Zhao T.; Li L.; Zhang N.; Tian J.","Gao, Peng (57216120423); Tian, Tian (57206471430); Zhao, Tianming (57215549257); Li, Linfeng (57216234297); Zhang, Nan (57610201300); Tian, Jinwen (7401635999)","57216120423; 57206471430; 57215549257; 57216234297; 57610201300; 7401635999","GF-Detection: Fusion with GAN of Infrared and Visible Images for Vehicle Detection at Nighttime","2022","Remote Sensing","14","12","2771","","","","10.3390/rs14122771","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135251593&doi=10.3390%2frs14122771&partnerID=40&md5=0234a0439d61ae462c5e0b44b945b749","Vehicles are important targets in the remote sensing applications and nighttime vehicle detection has been a hot study topic in recent years. Vehicles in the visible images at nighttime have inadequate features for object detection. Infrared images retain the contours of vehicles while they lose the color information. Thus, it is valuable to fuse infrared and visible images to improve the vehicle detection performance at nighttime. However, it is still a challenge to design effective fusion models due to the complexity of visible and infrared images. In order to improve vehicle detection performance at nighttime, this paper proposes a fusion model of infrared and visible images with Generative Adversarial Networks (GAN) for vehicle detection named GF-detection. GAN is utilized in the image reconstruction and introduced in the image fusion recently. To be specific, to exploit more features for the fusion, GAN is utilized to fuse the infrared and visible images via the image reconstruction. The generator fuses the image features and detection features, and then generates the reconstructed images for the discriminator to classify. Two branches, visible and infrared branches, are designed in the GF-detection model. Different feature extraction strategies are conducted according to the variance of the visible and infrared images. Detection features and self-attention mechanism are added to the fusion model aiming to build a detection task-driven fusion model of infrared and visible images. Extensive experiments based on nighttime images are conducted to demonstrate the effectiveness of the proposed fusion model in night vehicle detection. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Feature extraction; Image enhancement; Image fusion; Image reconstruction; Infrared imaging; Object detection; Object recognition; Remote sensing; Vehicle performance; Detection features; Detection fusion; Detection performance; Fusion model; Fusion of visible and infrared image; Images reconstruction; Infrared and visible image; Remote sensing applications; Vehicles detection; Visible image; Generative adversarial networks","Fusion of visible and infrared images; Generative Adversarial Networks; Vehicle detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135251593"
"Chavda S.; Goyani M.","Chavda, Sagar (57356888600); Goyani, Mahesh (36998366300)","57356888600; 36998366300","Scene Level Image Classification: A Literature Review","2022","Neural Processing Letters","","","","","","","10.1007/s11063-022-11072-5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142276731&doi=10.1007%2fs11063-022-11072-5&partnerID=40&md5=041c368f126dd349aac7e4c9d8cf6fe9","Convolutional neural networks (CNNs) have made significant contributions to natural and remote sensing imaging since the development of deep learning. Scene-level image classification is a challenge that affects both the natural and remote sensing domains and has numerous applications. The number of possible scene entities in the image content that could match the dataset images is the main focus. Scene-level classification is significant and fascinating because of open problems like intraclass heterogeneity, interclass homogeneity, background cluttering, high spatial resolution, and different imaging conditions. Additionally, the multi-label scene dataset’s imbalance, lack of preservation of complex semantic relations, and higher label-to-label correlation are all apparent. The article discusses a meta-analysis of the state-of-the-art scene classification literature practices. We discuss CNNs, attention mechanisms, capsule networks, and generative adversarial networks. The article also delivers an overview of the various activations, losses, optimization techniques, and regularization schemes pertinent to the scene domain. The standard benchmark datasets based on single- and multi-label themes are collated. The performance metrics for scene classification are explained as well. The implementation of the multi-label scene classification utilizing several CNN models on the UC Merced multi-label dataset is also covered in the paper. The proposed MobileNet-based model performs better than the recognized cutting-edge methodologies. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Chemical activation; Classification (of information); Convolutional neural networks; Deep learning; Generative adversarial networks; Image classification; Optimization; Semantics; Attention and vits; Capsnet and GAN; Convolutional neural network; Low–mid–high level research practice; Multi-labels; Optimisations; Optimization and regularization; Regularisation; Remote-sensing; Scene classification; Remote sensing","activations; Attentions and ViTs; CapsNet and GANs; CNNs; Losses; Low–mid–high level research practices; Multi-label; optimization and regularization; Remote sensing; Scene classification","Review","Article in press","","Scopus","2-s2.0-85142276731"
"Wang Z.; Zhang Y.; Luo L.; Wang N.","Wang, Zhixue (57214753076); Zhang, Yu (57214252359); Luo, Lin (57865848800); Wang, Nan (57701306200)","57214753076; 57214252359; 57865848800; 57701306200","CSA-CDGAN: channel self-attention-based generative adversarial network for change detection of remote sensing images","2022","Neural Computing and Applications","34","24","","21999","22013","14","10.1007/s00521-022-07637-z","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136984579&doi=10.1007%2fs00521-022-07637-z&partnerID=40&md5=24b987427aaabe37bbbd3db388a759c9","Remote sensing images change detection (RSICD) is a task to identify desired significant differences between multi-temporal images acquired at different times. From the existing methods, most of them solved this issue with a Siamese network, focusing on how to utilize the comparison between two image features to generate an initial difference map. However, Siamese network-based methods have three drawbacks: (1) complex architecture; (2) rough change map; (3) cumbersome detecting procedure: including feature extraction and feature comparison. To overcome the above drawbacks, we devoted our work to design a general framework which has a simple architecture, integrated detecting procedure, and good capacity of detecting subtle changes. In this paper, we proposed a channel self-attention network based on the generative adversarial network for change detection of remote sensing images. The network used an encoder–decoder network to directly produce a change map from two input images. It was better to detect small punctate and slim linear changes than Siamese-based networks. By regarding RSICD as an image translation problem, we used a Generative Adversarial Network to detect changes. In addition, a channel self-attention module was proposed to further improve the performance of this network. Experimental results on three public remote sensing RGB-image datasets, including change detection dataset, Wuhan University building change detection dataset and LEVIR building Change Detection dataset demonstrated that our method outperformed other state-of-the-art methods. In terms of the F1 score, the proposed method achieved maximum improvements of 5.1%, 3.1%, and 1.7% on the above datasets, respectively. Models and codes will be available at https://github.com/wangle53/CSA-CDGAN. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","College buildings; Feature extraction; Generative adversarial networks; Network architecture; Remote sensing; Adversarial learning; Building change detection; Change detection; Channel attention; Complex architectures; Image change detection; Image features; Multi-temporal image; Network-based; Remote sensing images; Change detection","Adversarial learning; Change detection; Channel attention","Article","Final","","Scopus","2-s2.0-85136984579"
"Karwowska K.; Wierzbicki D.","Karwowska, Kinga (57608598400); Wierzbicki, Damian (56835658600)","57608598400; 56835658600","Improving Spatial Resolution of Satellite Imagery Using Generative Adversarial Networks and Window Functions","2022","Remote Sensing","14","24","6285","","","","10.3390/rs14246285","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144608432&doi=10.3390%2frs14246285&partnerID=40&md5=aca3bea7c96713d4cf2678e7208784dc","Dynamic technological progress has contributed to the development of systems imaging of the Earth’s surface as well as data mining methods. One such example is super-resolution (SR) techniques that allow for the improvement of the spatial resolution of satellite imagery on the basis of a low-resolution image (LR) and an algorithm using deep neural networks. The limitation of these solutions is the input size parameter, which defines the image size that is adopted by a given neural network. Unfortunately, the value of this parameter is often much smaller than the size of the images obtained by Earth Observation satellites. In this article, we presented a new methodology for improving the resolution of an entire satellite image, using a window function. In addition, we conducted research to improve the resolution of satellite images acquired with the World View 2 satellite using the ESRGAN network, we determined the number of buffer pixels that will make it possible to obtain the best image quality. The best reconstruction of the entire satellite imagery using generative neural networks was obtained using a Triangular window (for 10% coverage). The Hann-Poisson window worked best when more overlap between images was used. © 2022 by the authors.","Data mining; Deep neural networks; Generative adversarial networks; Image enhancement; Remote sensing; Satellite imagery; Images processing; Network functions; Neural network application; Neural-networks; Remote-sensing; Satellite images; Spatial resolution; Surface mining methods; Technological progress; Window functions; Image resolution","image processing; image resolution; neural network application; remote sensing; satellites","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144608432"
"Lavreniuk M.; Kussul N.; Shelestov A.; Lavrenyuk A.; Shumilo L.","Lavreniuk, Mykola (56667743100); Kussul, Nataliia (6602485938); Shelestov, Andrii (6507365226); Lavrenyuk, Alla (16444915500); Shumilo, Leonid (57208256914)","56667743100; 6602485938; 6507365226; 16444915500; 57208256914","Super Resolution Approach for the Satellite Data Based on the Generative Adversarial Networks","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1095","1098","3","10.1109/IGARSS46834.2022.9884460","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140369300&doi=10.1109%2fIGARSS46834.2022.9884460&partnerID=40&md5=9273c26a2a94b7f402c373876be0b2e1","In the past few years, medium and high-resolution data became freely available for downloading. It provides great opportunity for researchers not to select between solving the task with high-resolution data on small territory or on global scale, but with low-resolution satellite images. Due to high spectral and spatial resolution of the data, Sentinel-1 and Sentinel-2 are very popular sources of information. Nevertheless, in practice if we would like to receive final product in 10 m resolution we should use bands with 10 m resolution. Sentinel-2 has four such bands, but also has other bands, especially red-edge 20 m resolution bands that are useful for vegetation analysis and often are omitted due to lower resolution. Thus, in this study we propose methodology for enhancing resolution (super-resolution) of the existing low-resolution images to higher resolution images. The main idea is to use advanced methods of deep learning-Generative Adversarial Networks (GAN) and train it to increase the resolution for the satellite images. Experimental results for the Sentinel-2 data showed that this approach is efficient and could be used for creating high resolution products. © 2022 IEEE.","Deep learning; Image enhancement; Optical resolving power; Remote sensing; Satellites; Deep learning; Global scale; High resolution data; High spatial resolution; High spectral resolution; Lower resolution; Satellite data; Satellite images; Sentinel-2; Superresolution; Generative adversarial networks","deep learning; GAN; Generative Adversarial Networks; Sentinel-2; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140369300"
"Zhang J.; Xu S.; Sun J.; Ou D.; Wu X.; Wang M.","Zhang, Junbo (57658607600); Xu, Shifeng (58031812400); Sun, Jun (57206957779); Ou, Dinghua (57192196217); Wu, Xiaobo (57226875661); Wang, Mantao (57204240542)","57658607600; 58031812400; 57206957779; 57192196217; 57226875661; 57204240542","Unsupervised Adversarial Domain Adaptation for Agricultural Land Extraction of Remote Sensing Images","2022","Remote Sensing","14","24","6298","","","","10.3390/rs14246298","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144819623&doi=10.3390%2frs14246298&partnerID=40&md5=e9443980e8a4047796207d194fa5863c","Agricultural land extraction is an essential technical means to promote sustainable agricultural development and modernization research. Existing supervised algorithms rely on many finely annotated remote-sensing images, which is both time-consuming and expensive. One way to reduce the annotation cost approach is to migrate models trained on existing annotated data (source domain) to unannotated data (target domain). However, model generalization capability is often unsatisfactory due to the limit of the domain gap. In this work, we use an unsupervised adversarial domain adaptation method to train a neural network to close the gap between the source and target domains for unsupervised agricultural land extraction. The overall approach consists of two phases: inter-domain and intra-domain adaptation. In the inter-domain adaptation, we use a generative adversarial network (GAN) to reduce the inter-domain gap between the source domain (labeled dataset) and the target domain (unlabeled dataset). The transformer with robust long-range dependency modeling acts as the backbone of the generator. In addition, the multi-scale feature fusion (MSFF) module is designed in the generator to accommodate remote sensing datasets with different spatial resolutions. Further, we use an entropy-based approach to divide the target domain. The target domain is divided into two subdomains, easy split images and hard split images. By training against each other between the two subdomains, we reduce the intra-domain gap. Experiments results on the “DeepGlobe → LoveDA”, “GID → LoveDA” and “DeepGlobe → GID” unsupervised agricultural land extraction tasks demonstrate the effectiveness of our method and its superiority to other unsupervised domain adaptation techniques. © 2022 by the authors.","Agriculture; Deep learning; Generative adversarial networks; Image processing; Remote sensing; Agricultural land; Agricultural land extraction; Deep learning; Domain adaptation; Inter-domain; Intra-domain; Remote sensing images; Remote-sensing; Target domain; Unsupervised domain adaptation; Extraction","agricultural land extraction; deep learning; remote sensing; unsupervised domain adaptation (UDA)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144819623"
"Xing Y.; Yang S.; Zhang Y.; Zhang Y.","Xing, Yinghui (57195357426); Yang, Shuyuan (8159166000); Zhang, Yan (57218471311); Zhang, Yanning (56075029000)","57195357426; 8159166000; 57218471311; 56075029000","Learning Spectral Cues for Multispectral and Panchromatic Image Fusion","2022","IEEE Transactions on Image Processing","31","","","6964","6975","11","10.1109/TIP.2022.3215906","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141570373&doi=10.1109%2fTIP.2022.3215906&partnerID=40&md5=65dddd0edccfef56fc1ba324ca59f176","Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.  © 1992-2012 IEEE.","Deep learning; Error compensation; Extraction; Generative adversarial networks; Image fusion; Linear transformations; Mean square error; Remote sensing; Features extraction; Multi-spectral; Multispectral images; Pan-sharpening; Remote-sensing; Spatial resolution; Spectral cue; Spectral loss; Task analysis; article; clinical assessment; learning; Image enhancement","generative adversarial networks; Image fusion; pansharpening; spectral cues; spectral loss","Article","Final","","Scopus","2-s2.0-85141570373"
"Zhao J.; Yang D.; Li Y.; Xiao P.; Yang J.","Zhao, Jiawei (57848338100); Yang, Dongfang (55888542900); Li, Yongfei (57872992100); Xiao, Peng (57848338200); Yang, Jinglan (57848338300)","57848338100; 55888542900; 57872992100; 57848338200; 57848338300","Intelligent Matching Method for Heterogeneous Remote Sensing Images Based on Style Transfer","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","6723","6731","8","10.1109/JSTARS.2022.3197748","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136143402&doi=10.1109%2fJSTARS.2022.3197748&partnerID=40&md5=d51f4c49f28b28188e373774702395cc","Intelligent matching of heterogeneous remote sensing images is a common basic problem in the field of intelligent remote sensing image processing. Aiming at the difficulty of matching satellite-aerial remote sensing images, this article proposes an intelligent matching method for heterogeneous remote sensing images based on style transfer. First, based on the idea of image style transfer of a generative adversarial networks, this method improves the conversion effect of the model on heterogeneous images by constructing a new generative network loss function and converts satellite images into aerial images. Then, the advanced deep learning-based matching algorithms D2-Net and LoFTR are used to achieve matching between the generated aerial image and the original aerial image. Finally, this transformation relationship is mapped to the corresponding satellite-aerial image pair to obtain the final matching result. The image style transfer experiments and the matching experiments we carry out under different test datasets show that the smooth cycle-consistent generative adversarial networks proposed in this article can effectively reduce the complexity of the algorithm and improve the quality of image generation. In addition, combining it with deep learning-based feature-matching methods can effectively improve the accuracy and robustness of the matching algorithm. Our code and data can be found at: https://gitee.com/AZQZ/intelligent-matching.  © 2008-2012 IEEE.","Antennas; Computational complexity; Deep learning; Image enhancement; Learning algorithms; Remote sensing; Satellites; Aerial images; Features extraction; Generator; Heterogeneous remote sensing image; Image style transfer; Intelligent matching; Matching methods; Matchings; Remote sensing images; Remote-sensing; accuracy assessment; aerial photograph; algorithm; data set; image analysis; mapping method; remote sensing; satellite imagery; Generative adversarial networks","Heterogeneous remote sensing images; image style transfer; intelligent matching","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136143402"
"Liu J.; Sun Y.; Ren K.; Zhao Y.; Deng K.; Wang L.","Liu, Jia (36816241900); Sun, Yongjian (57211860991); Ren, Kaijun (35093112400); Zhao, Yanlai (45662261300); Deng, Kefeng (54583346900); Wang, Lizhe (23029267900)","36816241900; 57211860991; 35093112400; 45662261300; 54583346900; 23029267900","A Spatial Downscaling Approach for WindSat Satellite Sea Surface Wind Based on Generative Adversarial Networks and Dual Learning Scheme","2022","Remote Sensing","14","3","769","","","","10.3390/rs14030769","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124521852&doi=10.3390%2frs14030769&partnerID=40&md5=5769dd6774b23f35947860e44e4eceec","Sea surface wind (SSW) is a crucial parameter for meteorological and oceanographic research, and accurate observation of SSW is valuable for a wide range of applications. However, most existing SSW data products are at a coarse spatial resolution, which is insufficient, especially for regional or local studies. Therefore, in this paper, to derive finer-resolution estimates of SSW, we present a novel statistical downscaling approach for satellite SSW based on generative adversarial networks and dual learning scheme, taking WindSat as a typical example. The dual learning scheme performs a primal task to reconstruct high resolution SSW, and a dual task to estimate the degradation kernels, which form a closed loop and are simultaneously learned, thus introducing an additional constraint to reduce the solution space. The integration of a dual learning scheme as the generator into the generative adversarial network structure further yield better downscaling performance by fine-tuning the generated SSW closer to high-resolution SSW. Besides, a model adaptation strategy was exploited to enhance the capacity for downscaling from low-resolution SSW without high-resolution ground truth. Comprehensive experiments were conducted on both the synthetic paired and unpaired SSW data. In the study areas of the East Coast of North America and the North Indian Ocean, in this work, the downscaling results to 0.25° (high resolution on the synthetic dataset), 0.03125° (8× downscaling), and 0.015625° (16× downscaling) of the proposed approach achieve the highest accuracy in terms of root mean square error and R-Square. The downscaling resolution can be enhanced by increasing the basic blocks in the generator. The highest downscaling reconstruction quality in terms of peak signal-to-noise ratio and structural similarity index was also achieved on the synthetic dataset with high-resolution ground truth. The experimental results demonstrate the effectiveness of the proposed downscaling network and the superior performance compared with the other typical advanced downscaling methods, including bicubic interpolation, DeepSD, dual regression networks, and adversarial DeepSD. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Mean square error; Remote sensing; Satellites; Signal to noise ratio; Space optics; Surface waters; Deep learning; Down-scaling; Dual learning; High resolution; Learning schemes; Satellite remote sensing; Sea surface wind; Statistical downscaling; Superresolution; WindSat; Generative adversarial networks","Deep learning; Dual learning; Generative adversarial network; Satellite remote sensing; Sea surface wind; Statistical downscaling; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124521852"
"Ni L.; Huo C.; Zhang X.; Wang P.; Zhang L.; Guo K.; Zhou Z.","Ni, Lei (57226654625); Huo, Chunlei (24080315500); Zhang, Xin (55792938900); Wang, Peng (57434649900); Zhang, Luyang (57214762794); Guo, Kangkang (57196235327); Zhou, Zhixin (7406097158)","57226654625; 24080315500; 55792938900; 57434649900; 57214762794; 57196235327; 7406097158","NaGAN: Nadir-like Generative Adversarial Network for Off-Nadir Object Detection of Multi-View Remote Sensing Imagery","2022","Remote Sensing","14","4","975","","","","10.3390/rs14040975","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124978515&doi=10.3390%2frs14040975&partnerID=40&md5=e34fca557493cb8f90b75e6d90724722","Detecting off-nadir objects is a well-known challenge in remote sensing due to the distortion and mutable representation. Existing methods mainly focus on a narrow range of view angles, and they ignore broad-view pantoscopic remote sensing imagery. To address the off-nadir object detection problem in remote sensing, a new nadir-like generative adversarial network (NaGAN) is proposed in this paper by narrowing the representation differences between the off-nadir and nadir object. NaGAN consists of a generator and a discriminator, in which the generator learns to transform the off-nadir object to a nadir-like one so that they are difficult to discriminate by the discriminator, and the discriminator competes with the generator to learn more nadir-like features. With the progressive competition between the generator and discriminator, the performances of off-nadir object detection are improved significantly. Extensive evaluations on the challenging SpaceNet benchmark for remote sensing demonstrate the superiority of NaGAN to the well-established state-of-the-art in detecting off-nadir objects. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Generative adversarial networks; Object detection; Object recognition; Learn+; Multi-view remote sensing imagery; Multi-views; Objects detection; Off-nadir; Performance; Remote sensing imagery; Remote-sensing; Spacenet; View angles; Remote sensing","Generative adversarial network; Multi-view remote sensing imagery; Object detection; Off-nadir; SpaceNet","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124978515"
"Kong Y.; Hong F.; Leung H.; Peng X.","Kong, Yingying (35186206400); Hong, Fang (57315801600); Leung, Henry (7202811506); Peng, Xiangyang (57214935616)","35186206400; 57315801600; 7202811506; 57214935616","A fusion method of optical image and sar image based on dense-ugan and gram–schmidt transformation","2021","Remote Sensing","13","21","4274","","","","10.3390/rs13214274","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118155325&doi=10.3390%2frs13214274&partnerID=40&md5=3858ddecb0bc7adabfac80b0b9b1b717","To solve the problems such as obvious speckle noise and serious spectral distortion when existing fusion methods are applied to the fusion of optical and SAR images, this paper proposes a fusion method for optical and SAR images based on Dense-UGAN and Gram–Schmidt transformation. Firstly, dense connection with U-shaped network (Dense-UGAN) are used in GAN generator to deepen the network structure and obtain deeper source image information. Secondly, according to the particularity of SAR imaging mechanism, SGLCM loss for preserving SAR texture features and PSNR loss for reducing SAR speckle noise are introduced into the generator loss function. Meanwhile in order to keep more SAR image structure, SSIM loss is introduced to discriminator loss function to make the generated image retain more spatial features. In this way, the generated high-resolution image has both optical contour characteristics and SAR texture characteristics. Finally, the GS transformation of optical and generated image retains the necessary spectral properties. Experimental results show that the proposed method can well preserve the spectral information of optical images and texture information of SAR images, and also reduce the generation of speckle noise at the same time. The metrics are superior to other algorithms that currently perform well. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Geometrical optics; Image fusion; Image texture; Radar imaging; Remote sensing; Speckle; Synthetic aperture radar; Textures; Fusion methods; Gram-schmidt; Image information; Image-based; Loss functions; Optical image; Optical-; Remote sensing images; SAR Images; Speckle noise; Generative adversarial networks","Generative adversarial network; Gram–Schmidt; Image fusion; Loss function; Remote sensing image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118155325"
"Jia F.; Xu J.; Sun X.; Ma Y.; Ni M.","Jia, Fei (57298794000); Xu, Jindong (35176864300); Sun, Xiao (57208776868); Ma, Yongli (57219779729); Ni, Mengying (26325030000)","57298794000; 35176864300; 57208776868; 57219779729; 26325030000","Blind image separation method based on cascade generative adversarial networks","2021","Applied Sciences (Switzerland)","11","20","9416","","","","10.3390/app11209416","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117199296&doi=10.3390%2fapp11209416&partnerID=40&md5=4dcab0101086c46f4edbf9800a6c341c","To solve the challenge of single-channel blind image separation (BIS) caused by unknown prior knowledge during the separation process, we propose a BIS method based on cascaded generative adversarial networks (GANs). To ensure that the proposed method can perform well in different scenarios and to address the problem of an insufficient number of training samples, a synthetic network is added to the separation network. This method is composed of two GANs: a U-shaped GAN (UGAN), which is used to learn image synthesis, and a pixel-to-attention GAN (PAGAN), which is used to learn image separation. The two networks jointly complete the task of image separation. UGAN uses the unpaired mixed image and the unmixed image to learn the mixing style, thereby generating an image with the “true” mixing characteristics which addresses the problem of an insufficient number of training samples for the PAGAN. A self-attention mechanism is added to the PAGAN to quickly extract important features from the image data. The experimental results show that the proposed method achieves good results on both synthetic image datasets and real remote sensing image datasets. Moreover, it can be used for image separation in different scenarios which lack prior knowledge and training samples. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Blind image separation; Generative adversarial networks; Remote sensing images; Visual attention","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117199296"
"Liu M.; Zhang P.; Shi Q.; Liu M.","Liu, Mengxi (57208160778); Zhang, Pengyuan (57226037738); Shi, Qian (55286447700); Liu, Mengwei (57222732517)","57208160778; 57226037738; 55286447700; 57222732517","An Adversarial Domain Adaptation Framework with KL-Constraint for Remote Sensing Land Cover Classification","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3066004","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103910726&doi=10.1109%2fLGRS.2021.3066004&partnerID=40&md5=bc4b214fb4e34c0a0f1a556f12dc14ee","Land cover classification plays a crucial role in land resource monitoring and planning. Recently, deep learning-based methods are becoming the dominating method for precise land cover mapping. However, the large-scale application of them is deeply hindered by the domain shift between different images, which is easily caused by illumination, climate, regional divergence, and so on. With the aim to cope with the problem of domain shift, many domain adaptation (DA) methods have been provided and great achievements have been made, especially the newborn adversarial DA, which usually contains a generator and a discriminator. Among these methods, the pixel-level methods are of high memory consumption, whereas feature-level methods are found hard to decode the structured information for semantic segmentation tasks due to the lack of low-dimensional information. Therefore, we propose an adversarial domain adaptation framework with Kullback-Leibler constraint (KL-ADDA) for remote sensing land cover classification. A state-of-the-art (SOTA) semantic segmentation network is utilized as the generator, which directly outputs the segmentation results to the discriminator to retain more low-level information. Besides, a Kullback-Leibler (KL)-divergence is calculated to improve the discriminative ability of the discriminator and thus enhance the generator's performance. Experiments on the international society for photogrammetry and remote sensing (ISPRS) data set and two simulated target data sets have shown the effectiveness of KL-ADDA for DA.  © 2004-2012 IEEE.","Deep learning; Image segmentation; Remote sensing; Semantics; Discriminative ability; International society; Kullback-Leibler divergence; Land cover classification; Large-scale applications; Learning-based methods; Semantic segmentation; Structured information; algorithm; image classification; land cover; remote sensing; satellite imagery; Mapping","Domain adaptation (DA); generative adversarial network (GAN); Kullback-Leibler (KL) divergence; land cover classification; semantic segmentation","Article","Final","","Scopus","2-s2.0-85103910726"
"Meng F.; Song T.; Xu D.","Meng, Fan (57209329694); Song, Tao (53985580900); Xu, Danya (7404074170)","57209329694; 53985580900; 7404074170","Simulating Tropical Cyclone Passive Microwave Rainfall Imagery Using Infrared Imagery via Generative Adversarial Networks","2022","IEEE Geoscience and Remote Sensing Letters","19","","1005105","","","","10.1109/LGRS.2022.3152847","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125360490&doi=10.1109%2fLGRS.2022.3152847&partnerID=40&md5=edd8f2991228dd782087ef424e519db1","Tropical cyclones (TCs) generally carry large amounts of water vapor and can cause large-scale extreme rainfall. Passive microwave (PMW) rainfall (PMR) estimation of TC with high spatial and temporal resolution is crucial for disaster warning of TC, but remains a challenging problem due to low temporal resolution of microwave sensors. This study attempts to solve this problem by directly predicting PMW rainfall images (PMRIs) from satellite infrared (IR) images of TC. We develop a generative adversarial network (GAN) to simulate PMRI using IR images and establish the mapping relationship between TC cloud-top brightness temperature and PMR, and the algorithm is named tropical cyclone rainfall (TCR)-GAN. Meanwhile, a new dataset that is available as a benchmark, Dataset of TC IR-to-Rainfall Prediction (TCIRRP), was established, which is expected to advance the development of artificial intelligence in this direction. The experimental results show that the algorithm can effectively extract key features from IR. The end-to-end deep learning approach shows potential as a technique that can be applied globally and provides a new perspective TC precipitation prediction via satellite, which is expected to provide important insights for real-time visualization of TC rainfall globally in operations. © 2004-2012 IEEE.","Deep learning; Forecasting; Generative adversarial networks; Hurricanes; Infrared imaging; Microwave sensors; Microwaves; Rain; Tropical cyclone; Tropics; Deep learning; Infrared imagery; Large amounts; Large-scales; Microwave rainfall; Passive microwave rainfall; Passive microwaves; Remote-sensing; Tropical cyclone; Water vapour; artificial intelligence; brightness temperature; machine learning; prediction; satellite imagery; simulation; tropical cyclone; Remote sensing","Deep learning; generative adversarial networks (GANs); passive microwave~(PMW) rainfall; remote sensing; tropical cyclones (TCs)","Article","Final","","Scopus","2-s2.0-85125360490"
"Deng W.; Zhu Q.; Sun X.; Lin W.; Guan Q.","Deng, Weihuan (57224183913); Zhu, Qiqi (56420945500); Sun, Xiongli (57213190878); Lin, Weihua (57224195800); Guan, Qingfeng (55838509944)","57224183913; 56420945500; 57213190878; 57224195800; 55838509944","EML-GAN: GENERATIVE ADVERSARIAL NETWORK-BASED END-TO-END MULTI-TASK LEARNING ARCHITECTURE FOR SUPER-RESOLUTION RECONSTRUCTION AND SCENE CLASSIFICATION OF LOW-RESOLUTION REMOTE SENSING IMAGERY","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","5397","5400","3","10.1109/IGARSS47720.2021.9554060","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129901449&doi=10.1109%2fIGARSS47720.2021.9554060&partnerID=40&md5=0fe7a3c2eada4f8431bd4807afd84b84","High spatial resolution remote sensing images (HSR-RSIs) are critical to providing fine land cover/land use information for scene classification. The global low spatial resolution remote sensing images (LSR-RSIs) can be easily obtained at present, whereas it is still a challenge to acquire large-scale HSR-RSIs. In this paper, an algorithmic-based architecture is proposed to improve the spatial resolution of RSIs beyond the limits of imaging sensors. The generative adversarial network-based end-to-end multi-task learning architecture (EML-GAN) is proposed for LSR-RSIs super-resolution reconstruction and scene classification simultaneously. In EML-GAN, the generator network is used to recover the fine geometric structures of LSR-RSIs by fusing the deep contextual, structure, and edge information. In addition, the discriminator network is designed to predict the scene label and distinguish the real/fake of the input data. The proposed architecture is evaluated on a public dataset and two self-made dataset. The experimental results show that the proposed architecture improves the visual effect and classification performance of LSR-RSIs. © 2021 IEEE","Classification (of information); Image classification; Image reconstruction; Image resolution; Network architecture; Remote sensing; End to end; High spatial resolution; Learning architectures; Multitask learning; Network-based; Proposed architectures; Remote sensing images; Scene classification; Super-resolution reconstruction; Generative adversarial networks","generative adversarial network; remote sensing images; Scene classification; super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85129901449"
"Clabaut É.; Lemelin M.; Germain M.; Bouroubi Y.; St-Pierre T.","Clabaut, Étienne (57211641688); Lemelin, Myriam (55845571900); Germain, Mickaël (7201845387); Bouroubi, Yacine (42961054400); St-Pierre, Tony (57297977400)","57211641688; 55845571900; 7201845387; 42961054400; 57297977400","Model specialization for the use of esrgan on satellite and airborne imagery","2021","Remote Sensing","13","20","4044","","","","10.3390/rs13204044","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117227601&doi=10.3390%2frs13204044&partnerID=40&md5=0490fea79fd11b226c9e096e6bf7063e","Training a deep learning model requires highly variable data to permit reasonable generalization. If the variability in the data about to be processed is low, the interest in obtaining this generalization seems limited. Yet, it could prove interesting to specialize the model with respect to a particular theme. The use of enhanced super-resolution generative adversarial networks (ERSGAN), a specific type of deep learning architecture, allows the spatial resolution of remote sensing images to be increased by “hallucinating” non-existent details. In this study, we show that ESRGAN create better quality images when trained on thematically classified images than when trained on a wide variety of examples. All things being equal, we further show that the algorithm performs better on some themes than it does on others. Texture analysis shows that these performances are correlated with the inverse difference moment and entropy of the images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image enhancement; Optical resolving power; Remote sensing; Satellite imagery; Textures; Airborne imagery; ESRGAN; Generalisation; Haralick; Learning architectures; Learning models; Spatial resolution; Specialisation; Superresolution; Variable data; Generative adversarial networks","ESRGAN; Generative adversarial networks; Haralick; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117227601"
"Ge W.; Wang Z.; Wang G.; Tan S.; Zhang J.","Ge, Wenyi (55782079500); Wang, Zhitao (57222554407); Wang, Guigui (57190274143); Tan, Shihan (57190282499); Zhang, Jianwei (57195437554)","55782079500; 57222554407; 57190274143; 57190282499; 57195437554","Remote sensing image super-resolution for the visual system of a flight simulator: Dataset and baseline","2021","Aerospace","8","3","76","","","","10.3390/aerospace8030076","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103208331&doi=10.3390%2faerospace8030076&partnerID=40&md5=6f11f15ea40c56b725a7a9cabd7b53fb","High-resolution remote sensing images are the key data source for the visual system of a flight simulator for training a qualified pilot. However, due to hardware limitations, it is an expensive task to collect spectral and spatial images at very high resolutions. In this work, we try to tackle this issue with another perspective based on image super-resolution (SR) technology. First, we present a new ultra-high-resolution remote sensing image dataset named Airport80, which is captured from the airspace near various airports. Second, a deep learning baseline is proposed by applying the generative and adversarial mechanism, which is able to reconstruct a high-resolution image during a single image super-resolution. Experimental results for our benchmark demonstrate the effectiveness of the proposed network and show it has reached satisfactory performances. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Flight simulator; Generative adversarial network; Remote sensing image; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103208331"
"Guo D.; Xia Y.; Luo X.","Guo, Dongen (36720716400); Xia, Ying (36626744600); Luo, Xiaobo (36562124600)","36720716400; 36626744600; 36562124600","GAN-Based Semisupervised Scene Classification of Remote Sensing Image","2021","IEEE Geoscience and Remote Sensing Letters","18","12","","2067","2071","4","10.1109/LGRS.2020.3014108","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101788817&doi=10.1109%2fLGRS.2020.3014108&partnerID=40&md5=7b61736ca742d992540b4507ade55ae1","With the advent of a large number of remote sensing images (RSIs), scene classification of RSI is widely applied to many fields such as urban planning, natural disaster detection, and environmental monitoring. Compared with the natural image field, the lack of labeled RSI is a bottleneck of supervised scene classification methods based on deep learning. Meanwhile, unsupervised scene classification is difficult to meet actual needs. To this end, we propose a novel semisupervised scene classification method for RSI using generative adversarial nets (GANs), in which a gating unit, a self-attention gating (SAG) module, and a pretrained Inception V3 branch are introduced into discriminative network to enhance the feature representation capability for facilitating semisupervised classification. To be specific, the gating unit aims to learn the weights of each feature map and capture the dependence relationship between features. The SAG module aims to capture a long-range dependence for adaptively focusing on important scene regions. The Inception V3 branch aims to extract the high-level semantic representation of input images and further enhance the discriminant ability by gating unit and SAG module. Furthermore, a new optimization term is incorporated into the generator loss to indirectly drive discriminator to correctly classify scene images. To verify the effectiveness of the proposed method, extensive experimental results on UC Merced and EuroSAT data sets demonstrate that the method surpasses most of the state-of-the-art semisupervised image classification methods significantly, especially when only few samples are tagged. © 2004-2012 IEEE.","Classification (of information); Deep learning; Digital storage; Disasters; Image classification; Image enhancement; Remote sensing; Semantics; Classification methods; Gating unit; Generative adversarial net; Images classification; Remote sensing image; Remote sensing images; Scene classification; Self-attention gating  module; Semi-supervised; Semisupervised image classification; image analysis; image classification; machine learning; natural disaster; remote sensing; satellite imagery; supervised classification; supervised learning; Generative adversarial networks","Gating unit; generative adversarial nets (GANs); remote sensing image (RSI); self-attention gating (SAG) module; semisupervised image classification","Article","Final","","Scopus","2-s2.0-85101788817"
"Luo X.; Tong X.; Hu Z.","Luo, Xin (56316646000); Tong, Xiaohua (55500134600); Hu, Zhongwen (55630272400)","56316646000; 55500134600; 55630272400","Improving Satellite Image Fusion via Generative Adversarial Training","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9212572","6969","6982","13","10.1109/TGRS.2020.3025821","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111155134&doi=10.1109%2fTGRS.2020.3025821&partnerID=40&md5=52efcfa7eaddf11c1f7bf0d58f09c968","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications. © 1980-2012 IEEE.","Geometrical optics; Image fusion; Remote sensing; Satellites; Adversarial networks; Discriminative networks; Multiresolution images; Operational land imager; Remote sensing applications; Remote sensing images; Spatial informations; Spectral information; artificial neural network; optical method; satellite data; Image enhancement","Deep learning; generative adversarial networks (GANs); Landsat 8; remote sensing image fusion; residual dense blocks; Sentinel-2","Article","Final","","Scopus","2-s2.0-85111155134"
"Zhou P.; Cheng G.; Yao X.; Han J.","Zhou, Peicheng (55847670300); Cheng, Gong (36801169800); Yao, Xiwen (55813585000); Han, Junwei (24450644400)","55847670300; 36801169800; 55813585000; 24450644400","Machine learning paradigms in high-resolution remote sensing image interpretation; [高分辨率遥感影像解译中的机器学习范式]","2021","National Remote Sensing Bulletin","25","1","","182","197","15","10.11834/jrs.20210164","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103158765&doi=10.11834%2fjrs.20210164&partnerID=40&md5=16e47f80cadabf655f6f9d345525627b","High-resolution remote sensing image interpretation is a major topic in remote sensing information processing. It plays a vital role in the knowledge mining and intelligent analysis of remote sensing big data and has important application values in civil and military fields. The traditional methods of high-resolution remote sensing image interpretation generally use manual visual interpretation, which is time consuming and laborious and has low accuracy. Therefore, interpreting high-resolution remote sensing images automatically and efficiently is an urgent problem to be solved. The rapid development of artificial intelligence technology in recent years has made machine learning the mainstream research direction of high-resolution remote sensing image interpretation. In this study, we systematically review five kinds of representative machine learning paradigms on the basis of the typical tasks of high-resolution remote sensing image interpretation, such as object detection, scene classification, semantic segmentation, and hyperspectral image classification. Specifically, we introduce their definitions, typical methods, and applications. The representative machine learning paradigms include supervised learning (e.g., support vector machine, k-nearest neighbor, decision tree, random tree, and probabilistic graph model), semi-supervised learning (e.g., pure semi-supervised learning, transductive learning, and active learning), weakly supervised learning (e.g., multiple instance learning), unsupervised learning (e.g., clustering, principal component analysis, and sparse coding), and deep learning (e.g., stacked auto-encoder, deep belief network, convolutional neural network, and generative adversarial network). Then, we comprehensively analyze the strengths and limitations of the five kinds of machine learning paradigms and summarize their typical applications in remote sensing image interpretation. Finally, we summarize the development direction of high-resolution remote sensing image interpretation, such as few-shot learning, unsupervised deep learning, and reinforcement learning. © 2021, Science Press. All right reserved.","Convolutional neural networks; Data mining; Decision trees; Image segmentation; Learning systems; Nearest neighbor search; Object detection; Object recognition; Reinforcement learning; Remote sensing; Semantics; Semi-supervised learning; Spectroscopy; Support vector machines; Trees (mathematics); Artificial intelligence technologies; Development directions; High resolution remote sensing images; Multiple instance learning; Probabilistic graph models; Remote sensing image interpretations; Remote sensing information; Weakly supervised learning; Deep learning","Deep learning; Few-shot learning; Machine learning paradigm; Reinforcement learning; Remote sensing image interpretation; Weakly supervised learning","Article","Final","","Scopus","2-s2.0-85103158765"
"Shi C.; Fang L.; Lv Z.; Shen H.","Shi, Cheng (57195378613); Fang, Li (53864885800); Lv, Zhiyong (23111268400); Shen, Huifang (57218659671)","57195378613; 53864885800; 23111268400; 57218659671","Improved Generative Adversarial Networks for VHR Remote Sensing Image Classification","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3025099","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112680616&doi=10.1109%2fLGRS.2020.3025099&partnerID=40&md5=faab24428922d26dde1bb670a8401d2f","With increasing spatial resolution of remote sensing images, accurate classification of land classes depends more on the number of labeled samples. However, the acquisition of labeled samples is difficult and time-consuming. Hence, generative adversarial networks (GANs) have become a new method for collecting training samples for very-high-resolution (VHR) remote sensing image classification. A traditional GAN generates new samples with the same distribution as the labeled samples. However, the generated samples have features close to their class center, and the network cannot obtain effective discriminative ability for the samples close to the decision boundary. This letter presents an improved GAN (IGAN) for VHR remote sensing image classification. In the proposed framework, the generator aims to generate synthetic samples close to the classification boundary, and the discriminator aims to constrain the labels of the synthetic samples. The obtained synthetic samples can effectively improve the classification accuracy of the classification boundary. Experiments are conducted on two VHR remote sensing images, and the results show that the proposed method performs better than several state-of-the-art methods.  © 2004-2012 IEEE.","Image classification; Image enhancement; Remote sensing; Class Centers; Classification boundary; Discriminative ability; Generative adversarial network; High-resolution remote sensing images; Remote sensing image classification; Remote sensing images; Spatial resolution; Training sample; Very high resolution; accuracy assessment; experimental study; image classification; image resolution; remote sensing; sampling; satellite imagery; spatial resolution; Generative adversarial networks","Generative adversarial network (GAN); remote sensing image classification; semisupervised learning","Article","Final","","Scopus","2-s2.0-85112680616"
"","","","Proceedings - 2021 International Conference on Signal Processing and Machine Learning, CONF-SPML 2021","2021","Proceedings - 2021 International Conference on Signal Processing and Machine Learning, CONF-SPML 2021","","","","","","356","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126679277&partnerID=40&md5=b2157b18a5cf90f07e164ffff163c310","The proceedings contain 65 papers. The topics discussed include: coarse-to-fine loss based on Viterbi algorithm for weakly supervised action segmentation; key parameter mapping method of signal-to-noise ratio from link simulation to system simulation; research on modeling and control of closed-loop fiber optic gyroscope; design of photoelectric signal parameter test system for liquid crystal filters; an overview of recommender systems and its next generation: context-aware recommender systems; examination and categorization of keywords associated with online review sorting; performance review of generative adversarial network for a bi-directional task; an overview on remote sensing image classification methods with a focus on support vector machine; and research on the consensus problem of multi-agent systems via event-triggered mechanism.","","","Conference review","Final","","Scopus","2-s2.0-85126679277"
"Song S.; Mukerji T.; Hou J.","Song, Suihong (57203800631); Mukerji, Tapan (7003413039); Hou, Jiagen (16204810600)","57203800631; 7003413039; 16204810600","Bridging the Gap between Geophysics and Geology with Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3066975","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104203659&doi=10.1109%2fTGRS.2021.3066975&partnerID=40&md5=fd71c1ba99c3a7fc5137cd02d7915fd0","Inverse mapping from geophysics to geology is a difficult problem due to the inherent uncertainty of geophysical data and the spatially heterogeneous patterns (structure) in geology. We improve conditional facies simulation based on GANs (GANSim), a new Earth model simulation method based on generative adversarial networks (GANs), to bridge the gap between remotely sensed geophysical information and geology, by introducing a specially designed loss function and an input architecture for probability maps representing geophysical interpretation. After training, the GANSim is then used to produce multiple geological facies models conditioned to the input geophysics-interpreted probability maps alone or together with input well observations and global features. By evaluation, the generated facies models are realistic, diversified, and consistent with all input conditions. We demonstrate that the GAN learns the implicit geological pattern knowledge from the training data set and the knowledge of conditioning to inputs from human-defined explicit functions. Given the commonality of remotely sensed geophysical information, sparse measurements, and global features, GANSim should be applicable to many problems of geosciences. © 1980-2012 IEEE.","Inverse problems; Petrology; Remote sensing; Adversarial networks; Earth modeling simulations; Geological facies; Geophysical data; Geophysical information; Inverse mapping; Probability maps; Training data sets; bridge; data set; geology; geophysics; mapping; probability; Geophysics","Generative adversarial networks (GANs); Geological facies models; Geological pattern; Geomodeling; Geophysics; Probability maps","Article","Final","","Scopus","2-s2.0-85104203659"
"Yu S.; Wang X.","Yu, Shuai (57219555303); Wang, Xili (36761950100)","57219555303; 36761950100","Remote sensing building segmentation by CGAN with multilevel channel attention mechanism; [含多级通道注意力机制的CGAN遥感图像建筑物分割]","2021","Journal of Image and Graphics","26","3","","686","699","13","10.11834/jig.200059","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104624477&doi=10.11834%2fjig.200059&partnerID=40&md5=a5103b74cbb72559bd4a5c712a407024","Objective: Remote sensing building object segmentation is one of the important applications in image processing, which plays a vital role in smart city planning and urban change detection. However, building objects in remote sensing images have many complex characteristics, such as variable sizes, dense distributions, diverse topological shapes, complex backgrounds, and presence of occlusions and shadows. Traditional building segmentation algorithms are mainly based on manually designed features such as shapes, edges, and shadow features. These features are shallow features of the building target and cannot well express high-level semantic information, resulting in low recognition accuracy. By contrast, deep convolutional networks show excellent performance in pixel-level classification of natural images. Various fully convolutional network based image segmentation models have been continuously proposed. Most of these models use deconvolution or bilinear interpolation after feature extraction. Feature upsampling and pixel-by-pixel classification are used to segment the input image. The deep features of the building are extracted using highly nonlinear mapping and a large amount of data training, which overcomes the shortcomings of traditional algorithms. However, upsampling cannot completely compensate the information loss caused by repeated convolution and pooling operations in the deep convolutional network model. Therefore, the prediction results are relatively rough, such as small target misclassification, inaccurate boundaries, and other issues. In the field of remote sensing, public data sets are few. Training excellent deep convolutional networks is difficult, and the robustness of the network needs to be further improved. Aiming at the above problems, this paper proposes a conditional generative adversarial network (Ra-CGAN) with multilevel channel attention mechanism to segment remote sensing building objects. Method: A generative model with a multilevel channel attention mechanism is first built. The model is based on a coding and decoding structure that solves small target misses by fusing deep semantics and shallow details with attention. Second, a discriminative network is built and used to distinguish whether the input comes from the real label map or the segmentation map generated by the model. The segmentation result (accuracy and smoothness) is improved by correcting the difference between the two maps. The downsampling method without pooling is used in the discriminator to enhance the propagation of the gradient. Finally, the generated model and the discriminant model are alternately confronted for training through the constraint of the conditional variable of the labelled image. Learning the higher-order data distribution characteristics results in more continuity for the target space. The loss function uses a hybrid loss function, which comes from the cross-entropy loss function brought by the generated map and the real label map in the generation mode. The discriminator predicts the generated image as the loss value brought by the real label image. Experiments are performed on the WHU Building Dataset and Satellite Dataset II datasets. The first dataset has a dense building with many types and accurate labels, and can provide comprehensive, representative evaluation capabilities for the model. Another dataset with a higher segmentation difficulty is used to verify the robustness and scalability of the model. The lighting information and background information of the building are more complex than those of the first dataset. The experiment uses the PyTorch deep learning framework. The size of original image and the label image are unified to 512 × 512 pixels for training, the learning rate of Adam is set to 0.000 2, the momentum parameter is 0.5, the batch-size is 12, and the epoch is 200 times. Acceleration is performed using NVIDIA GTX TITAN Xp. Evaluation indicators include intersection over union (IOU), precision, recall, and F1-score. Result: Experiments are performed on the WHU Building Dataset and Satellite Dataset II datasets, and the methods are compared with the latest literature. Experimental results show that in the WHU dataset, the segmentation performance of the Ra-CGAN model is substantially improved compared with models without attention mechanism and adversarial training. Space continuity and integrity of the complex building and small building, and smoothness of building edges are considerably improved. Compared with U-Net, IOU value is increased by 3.75%, and F1-score is increased by 2.52%. Compared with the second-performance model, IOU value is increased by 1.1%, and F1-score is increased by 1.1%. In the Satellite Dataset II, Ra-CGAN obtains more ideal results in terms of target integrity and smoothness than other models, especially in the case of insufficient data samples. Compared with U-Net, IOU value is increased by 7.26%, and F1-score is increased by 6.68%. Compared with the second-placed model, IOU value is increased by 1.7% and F1-score is increased by 1.6%. Conclusion: A CGAN remote sensing building object segmentation model with multilevel channel attention mechanism, which combines the advantages of multilevel channel attention mechanism generation model and conditional generative adversarial networks, is proposed. Experimental results show that our model is superior to several state-of-the-art segmentation methods. Much more accurate remote sensing building object segmentation results are obtained on different datasets, proving that the model exhibits better robustness and scalability. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","Attention mechanism; Conditional generative adversarial network (CGAN); Deep convolutional neural network; Multi-scale feature fusion; Remote sensing image segmentation","Article","Final","","Scopus","2-s2.0-85104624477"
"Miao W.; Geng J.; Jiang W.","Miao, Wang (57406901900); Geng, Jie (57200589477); Jiang, Wen (36006357300)","57406901900; 57200589477; 36006357300","Semi-Supervised Remote-Sensing Image Scene Classification Using Representation Consistency Siamese Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2022.3140485","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122884122&doi=10.1109%2fTGRS.2022.3140485&partnerID=40&md5=19fc60434692e90ce95e017e0adb0cd0","Deep learning has achieved excellent performance in remote-sensing image scene classification, since a large number of datasets with annotations can be applied for training. However, in actual applications, there is just a few annotated samples and a large number of unannotated samples in remote-sensing images, which leads to overfitting of the deep model and affects the performance of scene classification. In order to address these problems, a semi-supervised representation consistency Siamese network (SS-RCSN) is proposed for remote-sensing image scene classification. First, considering intraclass diversity and interclass similarity of remote-sensing images, Involution-generative adversarial network (GAN) is utilized to extract the discriminative features from remote-sensing images via unsupervised learning. Then, Siamese network with a representation consistency loss is proposed for semi-supervised classification, which aims to reduce the differences of labeled and unlabeled data. Experimental results on UC Merced dataset, RESICS-45 dataset, aerial image dataset (AID), and RS dataset demonstrate that our method yields superior classification performance compared with other semi-supervised learning (SSL) methods. © 1980-2012 IEEE.","Classification (of information); Image classification; Image representation; Neural networks; Remote sensing; Supervised learning; Uranium compounds; Convolutional neural network; Features extraction; Image scene classification; Involution-GAN; Performance; Remote sensing images; Remote-sensing; Scene classification; Semi-supervised; Siamese network; accuracy assessment; image analysis; learning; remote sensing; satellite data; training; Generative adversarial networks","Involution-generative adversarial network (GAN); Remote-sensing image; Scene classification; Semi-supervised learning (SSL); Siamese network","Article","Final","","Scopus","2-s2.0-85122884122"
"","","","24th ISPRS Congress ""Imaging Today, Foreseeing Tomorrow"", Commission IV","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B4-2021","","","","408","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117755176&partnerID=40&md5=8fd61bfb6205b6727d01ba536e2ee5b7","The proceedings contain 59 papers. The topics discussed include: generative adversarial networks to generalize urban areas in topographic maps; 3d city model as a first step towards digital twin of Sofia city; from architectural survey to continuous monitoring: graph-based data management for cultural heritage conservation with digital twins; ontology-based data mapping to support planning in historical urban centers; determining the suitable location for the metallurgical and steel processing industries in Mongolia using GIS-based multi-criteria analysis methods; spatio temporal data cube applied to ais containerships trend analysis in the early years of the belt and road initiative – from global to local scale; impact analysis of accidents on the traffic flow based on massive floating car data; and using systems of parallel and distributed data processing to build hydrological models based on remote sensing data.","","","Conference review","Final","","Scopus","2-s2.0-85117755176"
"Ran X.; Ge L.; Zhang X.","Ran, Xinyu (57224464367); Ge, Liang (57225075442); Zhang, Xiaofeng (57217109516)","57224464367; 57225075442; 57217109516","RGAN: Rethinking generative adversarial networks for cloud removal","2021","International Journal of Intelligent Systems","36","11","","6731","6747","16","10.1002/int.22566","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109092813&doi=10.1002%2fint.22566&partnerID=40&md5=2d78db12f17a0a2a005ef219817060e4","Optical remote sensing imagery is at the core of many Earth observation activities. Many applications take use of the satellite data's regular, consistent, and global-scale characteristics, such as farmland monitoring, climate change assessment, land-cover, and land-use categorization, and catastrophe assessment. Optical remote sensing images, on the other hand, are frequently impacted by clouds during the collection process, resulting in reduced image clarity, which impairs feature assessment and future usage, and heavy cloud blockage renders the surface information below totally useless. In this paper, We propose a soft attention recurrent neural module based on an encoder-decoder network, which can solve the cloud occlusion problem. We also propose an adaptive padding convolution at the end of the decoder by taking into account the spatial information, which results in better declouding predictions, and our network achieves good results on the RICE1 and RICE2 data sets. © 2021 Wiley Periodicals LLC","Climate change; Decoding; Land use; Remote sensing; Adversarial networks; Climate change assessment; Earth observations; Occlusion problems; Optical remote sensing; Optical remote-sensing imagery; Spatial informations; Surface information; Recurrent neural networks","adaptive padding convolution; cloud removal; deep learning; optical imagery","Article","Final","","Scopus","2-s2.0-85109092813"
"Qiu T.; Liang X.; Du Q.; Ren F.; Lu P.; Wu C.","Qiu, Tianqi (57203870046); Liang, Xiaojin (57203868656); Du, Qingyun (55631817500); Ren, Fu (55121193300); Lu, Pengjie (57218645854); Wu, Chao (57221208633)","57203870046; 57203868656; 55631817500; 55121193300; 57218645854; 57221208633","Techniques for the automatic detection and hiding of sensitive targets in emergency mapping based on remote sensing data","2021","ISPRS International Journal of Geo-Information","10","2","68","","","","10.3390/ijgi10020068","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106520328&doi=10.3390%2fijgi10020068&partnerID=40&md5=bb897a00972301efd548238c0429fa92","Emergency remote sensing mapping can provide support for decision making in disaster assessment or disaster relief, and therefore plays an important role in disaster response. Traditional emergency remote sensing mapping methods use decryption algorithms based on manual retrieval and image editing tools when processing sensitive targets. Although these traditional methods can achieve target recognition, they are inefficient and cannot meet the high time efficiency requirements of disaster relief. In this paper, we combined an object detection model with a generative adversarial network model to build a two-stage deep learning model for sensitive target detection and hiding in remote sensing images, and we verified the model performance on the aircraft object processing problem in remote sensing mapping. To improve the experimental protocol, we introduced a modification to the reconstruction loss function, candidate frame optimization in the region proposal network, the PointRend algorithm, and a modified attention mechanism based on the characteristics of aircraft objects. Experiments revealed that our method is more efficient than traditional manual processing; the precision is 94.87%, the recall is 84.75% higher than that of the original mask R-CNN model, and the F1-score is 44% higher than that of the original model. In addition, our method can quickly and intelligently detect and hide sensitive targets in remote sensing images, thereby shortening the time needed for emergency mapping. © 2021 by the authors.","","Deepfill model; Emergency mapping based on remote sensing data; Mask R-CNN model; Pointrend; Sensitive object detection; Sensitive object hiding","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106520328"
"Feng X.; Zhang W.; Su X.; Xu Z.","Feng, Xubin (57201876230); Zhang, Wuxia (54390588100); Su, Xiuqin (18438569800); Xu, Zhengpu (57217677334)","57201876230; 54390588100; 18438569800; 57217677334","Optical remote sensing image denoising and super-resolution reconstructing using optimized generative network in wavelet transform domain","2021","Remote Sensing","13","9","1858","","","","10.3390/rs13091858","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106526544&doi=10.3390%2frs13091858&partnerID=40&md5=e89da686618affee80112bc1b00efb5f","High spatial quality (HQ) optical remote sensing images are very useful for target detection, target recognition and image classification. Due to the influence of imaging equipment accuracy and atmospheric environment, HQ images are difficult to acquire, while low spatial quality (LQ) remote sensing images are very easy to acquire. Hence, denoising and super-resolution (SR) reconstruction technology are the most important solutions to improve the quality of remote sensing images very effectively, which can lower the cost as much as possible. Most existing methods usually only employ denoising or SR technology to obtain HQ images. However, due to the complex structure and the large noise of remote sensing images, the quality of the remote sensing image obtained only by denoising method or SR method cannot meet the actual needs. To address these problems, a method of reconstructing HQ remote sensing images based on Generative Adversarial Network (GAN) named “Restoration Generative Adversarial Network with ResNet and DenseNet” (RRDGAN) is proposed, which can acquire better quality images by incorporating denoising and SR into a unified framework. The generative network is implemented by fusing Residual Neural Network (ResNet) and Dense Convolutional Network (DenseNet) in order to consider denoising and SR problems at the same time. Then, total variation (TV) regularization is used to furthermore enhance the edge details, and the idea of Relativistic GAN is explored to make the whole network converge better. Our RRDGAN is implemented in wavelet transform (WT) domain, since different frequency parts could be handled separately in the wavelet domain. The experimental results on three different remote sensing datasets shows the feasibility of our proposed method in acquiring remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Image acquisition; Image denoising; Image enhancement; Image reconstruction; Optical resolving power; Wavelet transforms; Adversarial networks; Atmospheric environment; Convolutional networks; Optical remote sensing; Remote sensing images; Super resolution reconstruction; Total variation regularization; Wavelet-transform domain; Remote sensing","Denoising; Densely connection network (DenseNet); Generative adversarial network (GAN); Relativistic; Remote sensing; Residual network (ResNet); Super-resolution; Total variation (TV); Wavelet transform (WT)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106526544"
"Ding L.; Tang H.; Liu Y.; Shi Y.; Zhu X.X.; Bruzzone L.","Ding, Lei (57213196518); Tang, Hao (57208238003); Liu, Yahui (57156674800); Shi, Yilei (55495784300); Zhu, Xiao Xiang (55696622200); Bruzzone, Lorenzo (7006892410)","57213196518; 57208238003; 57156674800; 55495784300; 55696622200; 7006892410","Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images","2022","IEEE Transactions on Image Processing","31","","","678","690","12","10.1109/TIP.2021.3134455","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111230809&doi=10.1109%2fTIP.2021.3134455&partnerID=40&md5=b18ed776e2a821a24eafedde5a0d295e","Building extraction in VHR RSIs remains a challenging task due to occlusion and boundary ambiguity problems. Although conventional convolutional neural networks (CNNs) based methods are capable of exploiting local texture and context information, they fail to capture the shape patterns of buildings, which is a necessary constraint in the human recognition. To address this issue, we propose an adversarial shape learning network (ASLNet) to model the building shape patterns that improve the accuracy of building segmentation. In the proposed ASLNet, we introduce the adversarial learning strategy to explicitly model the shape constraints, as well as a CNN shape regularizer to strengthen the embedding of shape features. To assess the geometric accuracy of building segmentation results, we introduced several object-based quality assessment metrics. Experiments on two open benchmark datasets show that the proposed ASLNet improves both the pixel-based accuracy and the object-based quality measurements by a large margin. The code is available at: https://github.com/ggsDing/ASLNet. © 1992-2012 IEEE.","Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Remote Sensing Technology; Benchmarking; Buildings; Convolution; Deep neural networks; Extraction; Generative adversarial networks; Remote sensing; Semantic Segmentation; Adversarial machine learning; Building extraction; Convolutional neural network; Deep learning; Features extraction; Generative adversarial network; Images segmentations; Remote-sensing; Shape; human; image processing; remote sensing; Semantics","Building extraction; Convolutional neural network; Deep learning; Generative adversarial networks (GANs); Image segmentation; Remote sensing","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111230809"
"Kumar D.; Kumar D.","Kumar, Deepak (57199657828); Kumar, Dharmender (57451398200)","57199657828; 57451398200","Hyperspectral Image Classification Using Deep Learning Models: A Review","2021","Journal of Physics: Conference Series","1950","1","012087","","","","10.1088/1742-6596/1950/1/012087","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112475704&doi=10.1088%2f1742-6596%2f1950%2f1%2f012087&partnerID=40&md5=c3edbbb2f1217ccf814e3b25ff87f3fa","Hyperspectral image (HSI) classification is one of the important topic in the field of remote sensing. In general, HSI has to deal with complex characteristics and nonlinearity among the hyperspectral data which makes the classification task very challenging for traditional machine learning (ML) models. Recently, deep learning (DL) models have been very widely used in the classification of HSIs because of their capability to deal with complexity and nonlinearity in data. The utilization of deep learning models has been very successful and demonstrated good performance in the classification of HSIs. This paper presents a comprehensive review of deep learning models utilized in HSI classification literature and a comparison of various deep learning strategies for this topic. Precisely, the authors have categorized the literature review based upon the utilization of five most popular deep learning models and summarized their main methodologies used in feature extraction. This work may provide useful guidelines for the future research work in this area. © Published under licence by IOP Publishing Ltd.","Image classification; Learning systems; Remote sensing; Spectroscopy; Classification tasks; Complex characteristics; Hyperspectral Data; Learning models; Learning strategy; Literature reviews; Deep learning","Auto Encoder; Convolutional Neural Network; Deep Belief Network; Deep Learning; Generative Adversarial Network; Hyperspectral Image Classification; Recurrent Neural Network; Remote Sensing","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112475704"
"Guo Y.; Du L.; Lyu G.","Guo, Yuchen (57226823334); Du, Lan (56430356700); Lyu, Guoxin (57316429300)","57226823334; 56430356700; 57316429300","Sar target detection based on domain adaptive faster r-cnn with small training data size","2021","Remote Sensing","13","21","4202","","","","10.3390/rs13214202","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118209849&doi=10.3390%2frs13214202&partnerID=40&md5=d11313595ff331022ba1f393fd74a6f1","It is expensive and time-consuming to obtain a large number of labeled synthetic aperture radar (SAR) images. In the task of small training data size, the results of target detection on SAR images using deep network approaches are usually not ideal. In this study, considering that optical remote sensing images are much easier to be labeled than SAR images, we assume to have a large number of labeled optical remote sensing images and a small number of labeled SAR images with the similar scenes, propose to transfer knowledge from optical remote sensing images to SAR images, and develop a domain adaptive Faster R-CNN for SAR target detection with small training data size. In the proposed method, in order to make full use of the label information and realize more accurate domain adaptation knowledge transfer, an instance level domain adaptation constraint is used rather than feature level domain adaptation constraint. Specifically, generative adversarial network (GAN) constraint is applied as the domain adaptation constraint in the adaptation module after the proposals of Faster R-CNN to achieve instance level domain adaptation and learn the transferable features. The experimental results on the measured SAR image dataset show that the proposed method has higher detection accuracy in the task of SAR target detection with small training data size than the traditional Faster R-CNN. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Knowledge management; Radar imaging; Radar target recognition; Remote sensing; Synthetic aperture radar; Tracking radar; Data size; Domain adaptation; Label information; Optical remote sensing; Radar target detection; Remote sensing images; Small training; Synthetic aperture radar images; Targets detection; Training data; Generative adversarial networks","Domain adaptation; Generative adversarial network; Synthetic aperture radar; Target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118209849"
"Cao Y.; Sui B.; Zhang W.","Cao, Yungang (18435828100); Sui, Baikai (57607540900); Zhang, Wei (57276566900)","18435828100; 57607540900; 57276566900","REL-SAGAN: Relative Generation Adversarial Network Integrated With Attention Mechanism for Scene Data Augmentation of Remote Sensing","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","3107","3119","12","10.1109/JSTARS.2022.3166927","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128676714&doi=10.1109%2fJSTARS.2022.3166927&partnerID=40&md5=bbbd44d2b87d655d29c74502cfe8354c","Deep learning shows a strong ability in target detection, scene classification, and change detection of remote sensing. However, the training process requires a large number of samples, and the production of most high-quality training samples requires a lot of time and human resources. With the resolution of remote sensing image gradually improved, and the scene information becoming more and more complicated, the sample augmentation methods at this stage have shown obvious defects, such as the loss of sample key information and the lack of diversity in augmented data sets. In order to solve these problems, this article proposes a new augmentation method for remote sensing scene data named Rel-SAGAN based on generative adversarial networks (GAN). First, combined with self-attention mechanism to improve the large-scale feature learning ability and reduce the calculation parameters of GAN. Second, using relativity adversarial loss function to improve the structural stability of GAN. Furthermore, increasing convolution kernel and deeper structural of GAN to improve the ability of global feature extraction and saving the training time. Finally, taking NWPU remote sensing image dataset as experimental data, the effectiveness of the method proposed in this article is verified through ResNet-18. The experimental results show that it can generate more diverse high-resolution remote sensing natural scene images, the overall classification accuracy of augmented training dataset of remote sensing used high-quality generated images selected based on inception score and Frechet inception distance evaluations is improved by more than 3% and the classification accuracy is generally higher than the traditional data augmentation methods. © 2008-2012 IEEE.","Classification (of information); Convolution; Deep learning; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Relativity; Stability; Attention mechanisms; Augmentation methods; Features extraction; Generative adversarial network; Kernel; Relativity loss; Remote-sensing; Sample augment; Self-attention mechanism; artificial neural network; data processing; image classification; remote sensing; sampling; Remote sensing","Generative adversarial networks (GAN); relativity loss; remote sensing; sample augment; self-attention mechanism","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128676714"
"Tang D.; Huang W.; Zha Z.; Yang J.; Zhou X.; Wang C.","Tang, D. (57191203826); Huang, W. (55709699500); Zha, Z. (56534807100); Yang, J. (57202050786); Zhou, X. (57191203666); Wang, C. (56538944600)","57191203826; 55709699500; 56534807100; 57202050786; 57191203666; 56538944600","Research on the network map service technology of remote sensing image intelligent conversion based on gan model","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2021","","285","290","5","10.5194/isprs-archives-XLIII-B3-2021-285-2021","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115869315&doi=10.5194%2fisprs-archives-XLIII-B3-2021-285-2021&partnerID=40&md5=13d342a80b7ea67c46475a60592bbe47","Based on an improved generative adversarial networks algorithm (CGAN), this paper explores a technical way to realize map transformation through autonomous learning and training of remote sensing images. Just skip the trial process vector data update and cumbersome process of mapping the basic map elements can be automatically transform, the image on the main streets and typical rules of construction material, can achieve automatic identification and transformation, greatly shorten the tile map production and update cycle, improve the efficiency of the network map service quality. The results of the test platform have proved that it can be applied to a certain extent and can basically meet the requirements of network map production. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.","Automation; Image enhancement; Metadata; Remote sensing; Autonomous learning; Data update; Intelligent conversion; Learning and training; Map service; Network algorithms; Remote sensing images; The network map; Vector data; Generative adversarial networks","GAN(Generative Adversarial Networks); Intelligent conversion; Remote sensing image; The network map","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115869315"
"Zhao W.; Chen X.; Ge X.; Chen J.","Zhao, Wenzhi (56669054000); Chen, Xi (57202621654); Ge, Xiaoshan (57221313042); Chen, Jiage (56964709100)","56669054000; 57202621654; 57221313042; 56964709100","Using Adversarial Network for Multiple Change Detection in Bitemporal Remote Sensing Imagery","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3035780","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098801741&doi=10.1109%2fLGRS.2020.3035780&partnerID=40&md5=12a5ba4d2c49be2a102953079923e7c1","Change detection by comparing two bitemporal images is one of the most challenging tasks in remote sensing. At present, most related studies focus on change area detection while neglecting multiple change type identification. In this letter, an attention gates generative adversarial adaptation network (AG-GAAN) is proposed on multiple change detection. The AG-GAAN has the following contributions: 1) this method can automatically detect multiple changes; 2) it includes attention gates mechanism for spatial constraint and accelerates change area identification with finer contours; and 3) the domain similarity loss is introduced to improve the discriminability of the model so that the model can map out real changes more accurately. To demonstrate the robustness of this approach, we used the Google Earth data sets that include seasonal variations for change detection and understanding. The experimental results demonstrated that the proposed method can accurately detect the multiple change types from bitemporal imagery. © 2004-2012 IEEE.","Electrical engineering; Geotechnical engineering; Adversarial networks; Area detection; Change detection; Discriminability; Multiple changes; Remote sensing imagery; Seasonal variation; Spatial constraints; accuracy assessment; automation; detection method; experimental study; remote sensing; satellite imagery; seasonal variation; spatial analysis; Remote sensing","Attention gates (AGs); bitemporal images; domain similarity loss; generative adversarial network (GAN); multiple-change detection","Article","Final","","Scopus","2-s2.0-85098801741"
"Al-Najjar H.A.H.; Pradhan B.; Sarkar R.; Beydoun G.; Alamri A.","Al-Najjar, Husam A. H. (57204564163); Pradhan, Biswajeet (12753037900); Sarkar, Raju (55064233100); Beydoun, Ghassan (55912707700); Alamri, Abdullah (57215408871)","57204564163; 12753037900; 55064233100; 55912707700; 57215408871","A new integrated approach for landslide data balancing and spatial prediction based on generative adversarial networks (GAN)","2021","Remote Sensing","13","19","4011","","","","10.3390/rs13194011","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116675195&doi=10.3390%2frs13194011&partnerID=40&md5=bdba0b77feb24d152395ade7f5081a09","Landslide susceptibility mapping has significantly progressed with improvements in machine learning techniques. However, the inventory / data imbalance (DI) problem remains one of the challenges in this domain. This problem exists as a good quality landslide inventory map, including a complete record of historical data, is difficult or expensive to collect. As such, this can considerably affect one’s ability to obtain a sufficient inventory or representative samples. This research developed a new approach based on generative adversarial networks (GAN) to correct imbalanced landslide datasets. The proposed method was tested at Chukha Dzongkhag, Bhutan, one of the most frequent landslide prone areas in the Himalayan region. The proposed approach was then compared with the standard methods such as the synthetic minority oversampling technique (SMOTE), dense imbalanced sampling, and sparse sampling (i.e., producing non-landslide samples as many as landslide samples). The comparisons were based on five machine learning models, including artificial neural networks (ANN), random forests (RF), decision trees (DT), k-nearest neighbours (kNN), and the support vector machine (SVM). The model evaluation was carried out based on overall accuracy (OA), Kappa Index, F1-score, and area under receiver operating characteristic curves (AUROC). The spatial database was established with a total of 269 landslides and 10 conditioning factors, including altitude, slope, aspect, total curvature, slope length, lithology, distance from the road, distance from the stream, topographic wetness index (TWI), and sediment transport index (STI). The findings of this study have shown that both GAN and SMOTE data balancing approaches have helped to improve the accuracy of machine learning models. According to AUROC, the GAN method was able to boost the models by reaching the maximum accuracy of ANN (0.918), RF (0.933), DT (0.927), kNN (0.878), and SVM (0.907) when default parameters used. With the optimum parameters, all models performed best with GAN at their highest accuracy of ANN (0.927), RF (0.943), DT (0.923) and kNN (0.889), except SVM obtained the highest accuracy of (0.906) with SMOTE. Our finding suggests that RF balanced with GAN can provide the most reasonable criterion for landslide prediction. This research indicates that landslide data balancing may substantially affect the predictive capabilities of machine learning models. Therefore, the issue of DI in the spatial prediction of landslides should not be ignored. Future studies could explore other generative models for landslide data balancing. By using state-of-the-art GAN, the proposed model can be considered in the areas where the data are limited or imbalanced. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Decision trees; Geographic information systems; Landslides; Lithology; Nearest neighbor search; Neural networks; Remote sensing; Sediment transport; Support vector machines; Bhutan; Data imbalance; Imbalanced dataset; Landslide susceptibility; Machine learning models; Receiver operating characteristic curves; Remote-sensing; Spatial prediction; Support vectors machine; Synthetic minority over-sampling techniques; Generative adversarial networks","Bhutan; Generative adversarial network; GIS; Imbalanced dataset; Landslide susceptibility; Machine learning; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116675195"
"Zhang X.; Yu W.; Pun M.-O.; Liu M.","Zhang, Xiaokang (56703557200); Yu, Weikang (57424164700); Pun, Man-On (6603645302); Liu, Ming (57216968027)","56703557200; 57424164700; 6603645302; 57216968027","STYLE TRANSFORMATION-BASED CHANGE DETECTION USING ADVERSARIAL LEARNING WITH OBJECT BOUNDARY CONSTRAINTS","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","3117","3120","3","10.1109/IGARSS47720.2021.9554645","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129819483&doi=10.1109%2fIGARSS47720.2021.9554645&partnerID=40&md5=8a1b0d86853797d86b053959e0701649","Deep learning has shown promising results on change detection (CD) from bi-temporal remote sensing imagery in recent years. However, it still remains challenging to cope with the pseudo-changes caused by seasonal differences and style variations of bi-temporal images. In this paper, an object-level boundary-preserving generative adversarial network (BPGAN) is developed for style transformation-based CD of bi-temporal images. To achieve this purpose, image objects derived in the spectral domain are incorporated into the image translation to generate object-level target-style-like images. In particular, constraints on object boundary consistency and object homogeneity are established in the adversarial learning to maintain the style and content consistency while regularizing the network training. Furthermore, the Superpixel-Based Fast Fuzzy c-Means (SF-FCM) algorithm is utilized for efficient CD from the object-level style-transformed images. Extensive experiments on SPOT5 and GF1 data confirm the effectiveness of the proposed approach. © 2021 IEEE.","Deep learning; Object detection; Remote sensing; Adversarial learning; Boundary constraints; CGAN; Change detection; Object boundaries; Object-level; Remote sensing imagery; Style transformation; Temporal images; Transformation based; Generative adversarial networks","cGAN; Change detection; object-level; style transformation","Conference paper","Final","","Scopus","2-s2.0-85129819483"
"Han Y.; Ma S.; He L.; Li C.; Zhu M.; Zhang F.","Han, Yongsai (57208760607); Ma, Shiping (15119172200); He, Linyuan (36110892200); Li, Chenghao (57211523676); Zhu, Mingming (57202312340); Zhang, Fei (57221924480)","57208760607; 15119172200; 36110892200; 57211523676; 57202312340; 57221924480","Detection of the object in the fast remote sensing airport area on the improved YOLOv3; [改进YOLOv3的快速遥感机场区域目标检测]","2021","Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University","48","5","","156","166","10","10.19665/j.issn1001-2400.2021.05.019","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118924503&doi=10.19665%2fj.issn1001-2400.2021.05.019&partnerID=40&md5=2b2c6fc586b72c6391e9dffba89c281c","The detection of remote sensing airport regional objects is of great military and civilian significance. In order to achieve fast and accurate detection results, a data set that is more mission-specific is independently constructed for the detection task. We use the representative network YOLOv3 of the one-step regression global detection method as the basic framework. For the problem of uneven distribution of categories in the data set, the use of generated data isproposed. The method uses generative adversarial networks to perform targeted data expansion to obtain data sets with domain transformation characteristics and more balanced distribution of different types of data. At the same time, the improved DWFPN detection component is used to fusion deeper distinguishable and more robust features. Experiments show that, compared with the original network, the improved network brings 4.98% increase in mean average precision (mAP) and 8.33% increase in average IOU, which reaches 89.07% mAP and 61.97% average IOU, respectively. At the same time, the average detection time of the improved network is 0.0625s, which is 7 times faster than the RetinaNet-101 network with a similar detection rate. Experiments prove the effectiveness of the network and its practicality for specific tasks. © 2021, The Editorial Board of Journal of Xidian University. All right reserved.","Airports; Metadata; Object detection; Airport area; Data expansion; Data set; Detection methods; Detection tasks; Domain transformation; Images processing; Neural-networks; Remote-sensing; Step regression; Remote sensing","Airport area; Image processing; Neural network; Object detection; Remote sensing","Article","Final","","Scopus","2-s2.0-85118924503"
"Ansith S.; Bini A.A.","Ansith, S. (57485971600); Bini, A.A. (57197416169)","57485971600; 57197416169","A modified Generative Adversarial Network (GAN) architecture for land use classification","2021","Proceedings of the IEEE Madras Section International Conference 2021, MASCON 2021","","","","","","","10.1109/MASCON51689.2021.9563609","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126241210&doi=10.1109%2fMASCON51689.2021.9563609&partnerID=40&md5=d5d293219dacee7a3fbb1fad6b518d2a","Land cover and usage classification with satellite images plays an important role in many applications such as land resource management, urban planning, precision agriculture and environmental protection. Faster and easier land cover and usage classification without the prior knowledge of the terrain and training sample assignment can be done using deep learning algorithms. Early works in land use classification are mainly focused on machine learning algorithms. In the past few years some deep learning (DL) architectures are also used in the land cover and usage classification purposes. But these DL architectures need large amounts of training samples to get higher accuracy. In this paper, a modified Generative Adversarial Network (GAN) architecture has been proposed for land use classification. This deep learning model performs physical atmospheric corrections as a pre-processing step. Moreover, the model has shown to perform an efficient classification based on the statistical qualifications performed herein with a limited training dataset acquired from UC Merced land use dataset. © 2021 IEEE.","Classification (of information); Deep learning; Land use; Learning algorithms; Network architecture; Remote sensing; Sampling; Deep learning algorithm; Land cover; Land usage; Landuse classifications; Learning architectures; Precision Agriculture; Remote-sensing; Satellite images; Training sample; UC merced land use dataset; Generative adversarial networks","Deep learning algorithm; GAN; land use classification; Remote sensing; UC Merced land use dataset","Conference paper","Final","","Scopus","2-s2.0-85126241210"
"Saha S.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 9943212600; 7006892410","Building Change Detection in VHR SAR Images via Unsupervised Deep Transcoding","2021","IEEE Transactions on Geoscience and Remote Sensing","59","3","9120230","1917","1929","12","10.1109/TGRS.2020.3000296","82","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101909352&doi=10.1109%2fTGRS.2020.3000296&partnerID=40&md5=ab466196c2979db2f99963f4d73a52ce","Building change detection (CD), important for its application in urban monitoring, can be performed in near real time by comparing prechange and postchange very-high-spatial-resolution (VHR) synthetic-aperture-radar (SAR) images. However, multitemporal VHR SAR images are complex as they show high spatial correlation, prone to shadows, and show an inhomogeneous signature. Spatial context needs to be taken into account to effectively detect a change in such images. Recently, convolutional-neural-network (CNN)-based transfer learning techniques have shown strong performance for CD in VHR multispectral images. However, its direct use for SAR CD is impeded by the absence of labeled SAR data and, thus, pretrained networks. To overcome this, we exploit the availability of paired unlabeled SAR and optical images to train for the suboptimal task of transcoding SAR images into optical images using a cycle-consistent generative adversarial network (CycleGAN). The CycleGAN consists of two generator networks: one for transcoding SAR images into the optical image domain and the other for projecting optical images into the SAR image domain. After unsupervised training, the generator transcoding SAR images into optical ones is used as a bitemporal deep feature extractor to extract optical-like features from bitemporal SAR images. Thus, deep change vector analysis (DCVA) and fuzzy rules can be applied to identify changed buildings (new/destroyed). We validate our method on two data sets made up of pairs of bitemporal VHR SAR images on the city of L'Aquila (Italy) and Trento (Italy).  © 1980-2012 IEEE.","Aquila; Convolutional neural networks; Fuzzy inference; Geometrical optics; Synthetic aperture radar; Transfer learning; Adversarial networks; Building change detection; Change vector analysis; Multispectral images; Spatial correlations; Synthetic aperture radar (SAR) images; Unsupervised training; Very-high spatial resolutions; detection method; radar imagery; remote sensing; spatial resolution; synthetic aperture radar; Radar imaging","Change detection (CD); deep change vector analysis (DCVA); generative adversarial network (GAN); multitemporal images; remote sensing; synthetic aperture radar (SAR); very high-resolution images","Article","Final","","Scopus","2-s2.0-85101909352"
"Gu S.; Zhang R.; Luo H.; Li M.; Feng H.; Tang X.","Gu, Songwei (57223606806); Zhang, Rui (56412253600); Luo, Hongxia (57213205905); Li, Mengyao (57223603582); Feng, Huamei (57222038285); Tang, Xuguang (24330441100)","57223606806; 56412253600; 57213205905; 57223603582; 57222038285; 24330441100","Improved singan integrated with an attentional mechanism for remote sensing image classification","2021","Remote Sensing","13","9","1713","","","","10.3390/rs13091713","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105965164&doi=10.3390%2frs13091713&partnerID=40&md5=43a71482bc56f1a8b9b3a962f1aa74a4","Deep learning is an important research method in the remote sensing field. However, samples of remote sensing images are relatively few in real life, and those with markers are scarce. Many neural networks represented by Generative Adversarial Networks (GANs) can learn from real samples to generate pseudosamples, rather than traditional methods that often require more time and man-power to obtain samples. However, the generated pseudosamples often have poor realism and cannot be reliably used as the basis for various analyses and applications in the field of remote sensing. To address the abovementioned problems, a pseudolabeled sample generation method is proposed in this work and applied to scene classification of remote sensing images. The improved unconditional generative model that can be learned from a single natural image (Improved SinGAN) with an attention mechanism can effectively generate enough pseudolabeled samples from a single remote sensing scene image sample. Pseudosamples generated by the improved SinGAN model have stronger realism and relatively less training time, and the extracted features are easily recognized in the classification network. The improved SinGAN can better identify sub-jects from images with complex ground scenes compared with the original network. This mechanism solves the problem of geographic errors of generated pseudosamples. This study incorporated the generated pseudosamples into training data for the classification experiment. The result showed that the SinGAN model with the integration of the attention mechanism can better guarantee feature extraction of the training data. Thus, the quality of the generated samples is improved and the classification accuracy and stability of the classification network are also enhanced. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Deep learning; Image classification; Remote sensing; Adversarial networks; Attention mechanisms; Attentional mechanism; Classification accuracy; Classification networks; Remote sensing image classification; Remote sensing images; Scene classification; Image enhancement","Attention mechanism; Deep learning; Generative adversarial network; Remote sensing image; Scene classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105965164"
"Dharejo F.A.; Deeba F.; Zhou Y.; Das B.; Jatoi M.A.; Zawish M.; Du Y.; Wang X.","Dharejo, Fayaz Ali (57195487028); Deeba, Farah (57215997317); Zhou, Yuanchun (55737417400); Das, Bhagwan (56493885700); Jatoi, Munsif Ali (55822002000); Zawish, Muhammad (57206720765); Du, Yi (35791161600); Wang, Xuezhi (56512501900)","57195487028; 57215997317; 55737417400; 56493885700; 55822002000; 57206720765; 35791161600; 56512501900","TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatiooral Single Image Super Resolution","2021","ACM Transactions on Intelligent Systems and Technology","12","6","71","","","","10.1145/3456726","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123956833&doi=10.1145%2f3456726&partnerID=40&md5=423b9bbc6a776996628949b45f5fa79d","Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from a remotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks (GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, the generated image still suffers from undesirable artifacts such as the absence of texture-feature representation and high-frequency information. We propose a frequency domain-based spatiooral remote sensing single image super-resolution technique to reconstruct the HR image combined with generative adversarial networks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporating Wavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image has been split into various frequency bands by using the WT, whereas the transfer generative adversarial network predicts high-frequency components via a proposed architecture. Finally, the inverse transfer of wavelets produces a reconstructed image with super-resolution. The model is first trained on an external DIV2 K dataset and validated with the UC Merced Landsat remote sensing dataset and Set14 with each image size of 256 × 256. Following that, transferred GANs are used to process spatiooral remote sensing images in order to minimize computation cost differences and improve texture information. The findings are compared qualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of the GPU memory during training and accelerated the execution of our simplified version by eliminating batch normalization layers. © 2021 Association for Computing Machinery.","Deep learning; Frequency domain analysis; Image compression; Image enhancement; Image reconstruction; Image resolution; Image texture; Remote sensing; Textures; Wavelet transforms; High-resolution images; Image super resolutions; Neural-networks; Remote-sensing; Remotely sensed images; Single images; Spatial resolution; Spatiooral; Superresolution; Wavelets transform; Generative adversarial networks","neural networks; spatiooral; super resolution; Wavelet transform","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123956833"
"Zhou Q.","Zhou, Qiaoliang (57553798300)","57553798300","Superresolution Reconstruction of Remote Sensing Image Based on Generative Adversarial Network","2022","Wireless Communications and Mobile Computing","2022","","9114911","","","","10.1155/2022/9114911","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127113909&doi=10.1155%2f2022%2f9114911&partnerID=40&md5=3d876711c29468b78ce7006bf16c849c","To recreate high-resolution, more detailed remote sensing images from existing low-resolution photos, this technique is known as remote sensing image superresolution reconstruction, and it has numerous uses. As an important research hotspot of neural networks, generative adversarial network (GAN) has made outstanding progress for image superresolution reconstruction. It solves the computational complexity and low reconstructed image quality of standard superresolution reconstruction algorithms. This research offers a superresolution reconstruction strategy with a self-attention generative adversarial network to improve the quality of reconstructed superresolution remote sensing images. The self-attention strategy as well as residual module is utilized to build a generator in this model that transforms low-resolution remote sensing images into superresolution ones. It aims to determine the discrepancy between a reconstructed picture and a true picture by using a deep convolutional network as a discriminator. For the purpose of enhancing the accuracy, content loss is used. This is done to obtain accurate detail reconstruction. According to the findings of the experiments, this approach is capable of regenerating higher-quality images.  © 2022 Qiaoliang Zhou.","Image enhancement; Image reconstruction; Optical resolving power; Remote sensing; High resolution; Hotspots; Image super-resolution reconstruction; Image-based; Lower resolution; Neural-networks; Reconstructed image; Remote sensing images; Super-resolution reconstruction; Superresolution; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127113909"
"Chen J.; Zhu J.; Sun G.; Li J.; Deng M.","Chen, Jie (56075077300); Zhu, Jingru (57216241574); Sun, Geng (57215896562); Li, Jianhui (57208131827); Deng, Min (27267549700)","56075077300; 57216241574; 57215896562; 57208131827; 27267549700","SMAF-Net: Sharing Multiscale Adversarial Feature for High-Resolution Remote Sensing Imagery Semantic Segmentation","2021","IEEE Geoscience and Remote Sensing Letters","18","11","","1921","1925","4","10.1109/LGRS.2020.3011151","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101176583&doi=10.1109%2fLGRS.2020.3011151&partnerID=40&md5=fd4f8bbfba58de4778d4ee288a00f85d","Semantic segmentation of high-resolution remote sensing imagery (HRSI) is a major task in remote sensing analysis. Although deep convolutional neural network (DCNN)-based semantic segmentation models have powerful capacity in pixel-wise classification, they still face challenge in obtaining intersemantic continuity and extraboundary accuracy because of the geo-object's characteristic feature of diverse scales and various distributions in HRSI. Inspired by the transfer learning, in this study, we propose an efficient semantic segmentation framework named SMAF-Net, which shares multiscale adversarial features into a U-shaped semantic segmentation model. Specifically, it uses multiscale adversarial feature representation obtained from a well-trained generative adversarial network to grasp the pixel correlation and further improve the boundary accuracy of multiscale geo-objects. Comparison experiments on the Potsdam and Vaihingen data sets demonstrate that the proposed framework can achieve considerable improvement in the semantic segmentation of HRSI. © 2004-2012 IEEE.","Baden-Wurttemberg; Brandenburg [Germany]; Germany; Potsdam; Vaihingen an der Enz; Deep neural networks; Image segmentation; Pixels; Remote sensing; Semantic Web; Semantics; Generative adversarial network; GEO objects; High resolution remote sensing imagery; High-resolution remote sensing imagery; Multi-scale features; Network-based; Object characteristics; Remote sensing analysis; Segmentation models; Semantic segmentation; correlation; data set; image resolution; pixel; remote sensing; satellite imagery; Generative adversarial networks; Semantic Segmentation","Generative adversarial network (GAN); high-resolution remote sensing imagery (HRSI); multiscale feature; semantic segmentation","Article","Final","","Scopus","2-s2.0-85101176583"
"Zhang L.; Lan M.; Zhang J.; Tao D.","Zhang, Lefei (48663190100); Lan, Meng (57207761764); Zhang, Jing (57211055913); Tao, Dacheng (7102600334)","48663190100; 57207761764; 57211055913; 7102600334","Stagewise Unsupervised Domain Adaptation With Adversarial Self-Training for Road Segmentation of Remote-Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3104032","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113246889&doi=10.1109%2fTGRS.2021.3104032&partnerID=40&md5=a318787700d142eccaf9f6777c032154","Road segmentation from remote-sensing images is a challenging task with wide ranges of application potentials. Deep neural networks have advanced this field by leveraging the power of large-scale labeled data, which, however, are extremely expensive and time-consuming to acquire. One solution is to use cheap available data to train a model and deploy it to directly process the data from a specific application domain. Nevertheless, the well-known domain shift (DS) issue prevents the trained model from generalizing well on the target domain. In this article, we propose a novel stagewise domain adaptation model called RoadDA to address the DS issue in this field. In the first stage, RoadDA adapts the target domain features to align with the source ones via generative adversarial networks (GANs)-based interdomain adaptation. Specifically, a feature pyramid fusion module is devised to avoid information loss of long and thin roads and learn discriminative and robust features. Besides, to address the intradomain discrepancy in the target domain, in the second stage, we propose an adversarial self-training method. We generate the pseudo labels of the target domain using the trained generator and divide it to labeled easy split and unlabeled hard split based on the road confidence scores. The features of hard split are adapted to align with the easy ones using adversarial learning and the intradomain adaptation process is repeated to progressively improve the segmentation performance. Experiment results on two benchmarks demonstrate that RoadDA can efficiently reduce the domain gap and outperforms state-of-the-art methods. The code is available at https://github.com/LANMNG/RoadDA. © 2021 IEEE.","Deep neural networks; Grid computing; Image segmentation; Labeled data; Roads and streets; Adaptation process; Adversarial learning; Adversarial networks; Domain adaptation; Remote sensing images; Road segmentation; Segmentation performance; State-of-the-art methods; adaptation; learning; remote sensing; segmentation; Remote sensing","Adaptation models; Data models; Feature extraction; Image segmentation; Predictive models; Roads; Task analysis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85113246889"
"Zhao Y.; Shen S.; Hu J.; Li Y.; Pan J.","Zhao, Yunpu (57218577322); Shen, Shikun (57219160335); Hu, Jiarui (57226187629); Li, Yinglong (57872671400); Pan, Jun (55459046800)","57218577322; 57219160335; 57226187629; 57872671400; 55459046800","Cloud Removal Using Multimodal GAN with Adversarial Consistency Loss","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3093887","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110838141&doi=10.1109%2fLGRS.2021.3093887&partnerID=40&md5=456ba9a295ea90a942bb4e53d258dc54","In the field of remote sensing image processing, clouds heavily affect the quality of the remote sensing images and their application potential. Thus, in recent years, with the prevalence of deep learning techniques used in the field of image processing, many methods have been proposed for cloud removal using single remote sensing images. The existing single-image cloud removal methods suffer from poor generalization capabilities that prevent them from being applied to diverse remote sensing images. Thus, a novel method using a multimodal architecture is proposed which provides multiple most likely outputs for the image and selects the best one through perception-based image quality evaluator (PIQE). In addition, adversarial consistency loss is used to replace cycle consistency loss, which encourages the model to retain more texture information of the original image, and thus the quality of the generated image increases. Experiments demonstrate that the presented method can easily achieve a considerable increase in the peak signal-to-noise ratio and the structural similarity index compared with other methods.  © 2004-2012 IEEE.","Deep learning; Image quality; Learning systems; Signal to noise ratio; Textures; Generalization capability; Learning techniques; Multimodal architectures; Peak signal to noise ratio; Remote sensing image processing; Remote sensing images; Structural similarity indices; Texture information; architecture; cloud; image processing; remote sensing; signal-to-noise ratio; Remote sensing","Cloud removal; generative adversarial network (GAN); multimodal; remote sensing","Article","Final","","Scopus","2-s2.0-85110838141"
"Wang S.; Li Y.; Wu N.; Zhao Y.; Yao H.","Wang, Shengnan (57211700878); Li, Yue (55878683800); Wu, Ning (56443296900); Zhao, Yuxing (57204925238); Yao, Haiyang (57215427576)","57211700878; 55878683800; 56443296900; 57204925238; 57215427576","Attribute-Based Double Constraint Denoising Network for Seismic Data","2021","IEEE Transactions on Geoscience and Remote Sensing","59","6","9199416","5304","5316","12","10.1109/TGRS.2020.3021492","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106711320&doi=10.1109%2fTGRS.2020.3021492&partnerID=40&md5=5d059747bb9a8bc175aaf62ad53f7a75","At present, most of the seismic data denoising methods based on deep learning attempt to establish a synthetic seismic data set as the network training set to train network parameters. However, the synthetic data set cannot completely reflect the structural characteristics of the field seismic data, resulting in some false seismic reflections in field denoised results. For this reason, this article proposes an attribute-based denoising algorithm for seismic data called attribute-based double constraint denoising network (Att-DCDN). This method applies encoder-decoder and attribute classifier to constitute the generative adversarial network (GAN) and attenuates seismic noise by controlling with/without target attributes (noise attribute and signal attribute). Compared with the noise-free field seismic data, attribute vectors of the field data are easier to obtain. Therefore, our training set includes not only the synthetic seismic data but also the field seismic data, so as to reduce accuracy requirement of the synthetic noise-free data. In addition, we propose a double-constraint training way to reduce the losses of effective reflections during the denoising process. Specifically, we consider both noise attenuation and signal retention, i.e., reconstruction loss and residual loss are introduced to constrain recovery of effective reflections, and attribute classification loss and adversarial loss are applied to constrain the attenuation of seismic noise. Both the experimental results of synthetic and field seismic records show that our algorithm can effectively suppress the seismic noise and recover the effective reflections almost completely, even the weak signal areas that are seriously polluted by the seismic noise.  © 1980-2012 IEEE.","Deep learning; Geophysical prospecting; Learning systems; Seismic response; Adversarial networks; Attribute vectors; De-noising algorithm; Network parameters; Noise attenuation; Seismic reflections; Structural characteristics; Synthetic datasets; network analysis; numerical model; remote sensing; satellite data; seismic data; Seismic waves","Attribute training set; attribute-based denoising; double constraint network; seismic data","Article","Final","","Scopus","2-s2.0-85106711320"
"Li Y.; Mavromatis S.; Zhang F.; Du Z.; Sequeira J.; Wang Z.; Zhao X.; Liu R.","Li, Yadong (57226097505); Mavromatis, Sebastien (22433774300); Zhang, Feng (56434720200); Du, Zhenhong (25929119800); Sequeira, Jean (56231992200); Wang, Zhongyi (57216175636); Zhao, Xianwei (57193860330); Liu, Renyi (55809641900)","57226097505; 22433774300; 56434720200; 25929119800; 56231992200; 57216175636; 57193860330; 55809641900","Single-Image Super-Resolution for Remote Sensing Images Using a Deep Generative Adversarial Network with Local and Global Attention Mechanisms","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3093043","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110893958&doi=10.1109%2fTGRS.2021.3093043&partnerID=40&md5=9fc5b609b878823e950bf2c70f6490f1","Super-resolution (SR) technology is an important way to improve spatial resolution under the condition of sensor hardware limitations. With the development of deep learning (DL), some DL-based SR models have achieved state-of-the-art performance, especially the convolutional neural network (CNN). However, considering that remote sensing images usually contain a variety of ground scenes and objects with different scales, orientations, and spectral characteristics, previous works usually treat important and unnecessary features equally or only apply different weights in the local receptive field, which ignores long-range dependencies; it is still a challenging task to exploit features on different levels and reconstruct images with realistic details. To address these problems, an attention-based generative adversarial network (SRAGAN) is proposed in this article, which applies both local and global attention mechanisms. Specifically, we apply local attention in the SR model to focus on structural components of the earth's surface that require more attention, and global attention is used to capture long-range interdependencies in the channel and spatial dimensions to further refine details. To optimize the adversarial learning process, we also use local and global attentions in the discriminator model to enhance the discriminative ability and apply the gradient penalty in the form of hinge loss and loss function that combines $L1$ pixel loss, $L1$ perceptual loss, and relativistic adversarial loss to promote rich details. The experiments show that SRAGAN can achieve performance improvements and reconstruct better details compared with current state-of-the-art SR methods. A series of ablation investigations and model analyses validate the efficiency and effectiveness of our method. © 1980-2012 IEEE.","Convolutional neural networks; Deep learning; Learning systems; Optical resolving power; Remote sensing; Scales (weighing instruments); Adversarial learning; Adversarial networks; Discriminative ability; Long-range dependencies; Remote sensing images; Spectral characteristics; State-of-the-art performance; Structural component; artificial neural network; image resolution; remote sensing; spatial resolution; Image processing","Convolutional neural networks (CNNs); generative adversarial network (GAN); local and global attention module; remote sensing; single-image super super-resolution (SISR)","Article","Final","","Scopus","2-s2.0-85110893958"
"Jozdani S.; Chen D.; Chen W.; Leblanc S.G.; Lovitt J.; He L.; Fraser R.H.; Johnson B.A.","Jozdani, Shahab (57188989068); Chen, Dongmei (55724967900); Chen, Wenjun (55597863700); Leblanc, Sylvain G. (7006985218); Lovitt, Julie (57194797446); He, Liming (37115581900); Fraser, Robert H. (7402280263); Johnson, Brian Alan (55481350500)","57188989068; 55724967900; 55597863700; 7006985218; 57194797446; 37115581900; 7402280263; 55481350500","Evaluating image normalization via GANs for environmental mapping: A case study of lichen mapping using high-resolution satellite imagery","2021","Remote Sensing","13","24","5035","","","","10.3390/rs13245035","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121337074&doi=10.3390%2frs13245035&partnerID=40&md5=9de56052f9230aac0f8421a044dabcdf","Illumination variations in non-atmospherically corrected high-resolution satellite (HRS) images acquired at different dates/times/locations pose a major challenge for large-area environmental mapping and monitoring. This problem is exacerbated in cases where a classification model is trained only on one image (and often limited training data) but applied to other scenes without collecting additional samples from these new images. In this research, by focusing on caribou lichen mapping, we evaluated the potential of using conditional Generative Adversarial Networks (cGANs) for the normalization of WorldView-2 (WV2) images of one area to a source WV2 image of another area on which a lichen detector model was trained. In this regard, we considered an extreme case where the classifier was not fine-tuned on the normalized images. We tested two main scenarios to normalize four target WV2 images to a source 50 cm pansharpened WV2 image: (1) normalizing based only on the WV2 panchromatic band, and (2) normalizing based on the WV2 panchromatic band and Sentinel-2 surface reflectance (SR) imagery. Our experiments showed that normalizing even based only on the WV2 panchromatic band led to a significant lichen-detection accuracy improvement compared to the use of original pansharpened target images. However, we found that conditioning the cGAN on both the WV2 panchromatic band and auxiliary information (in this case, Sentinel-2 SR imagery) further improved normalization and the subsequent classification results due to adding a more invariant source of information. Our experiments showed that, using only the panchromatic band, F1-score values ranged from 54% to 88%, while using the fused panchromatic and SR, F1-score values ranged from 75% to 91%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Deep learning; Fungi; Generative adversarial networks; Image enhancement; Remote sensing; Satellite imagery; Deep learning; Environmental mapping; GAN; Image normalization; Lichen mapping; Normalisation; Panchromatic bands; Remote-sensing; Surface reflectance; Worldview-2; Mapping","Deep learning; Environmental mapping; GANs; Image normalization; Lichen mapping; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121337074"
"Ajayi O.G.; Ojima A.","Ajayi, Oluibukun Gbenga (57193160439); Ojima, Amos (57432260600)","57193160439; 57432260600","Performance evaluation of selected cloud occlusion removal algorithms on remote sensing imagery","2022","Remote Sensing Applications: Society and Environment","25","","100700","","","","10.1016/j.rsase.2022.100700","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123712143&doi=10.1016%2fj.rsase.2022.100700&partnerID=40&md5=0f4fadd7b24c14d9a5bb84dd31ad4fad","Removal of cloud cover from remote sensing satellite images is crucial to many optical remote sensing image data users because cloud cover can conceal important spatial information on an image data. This underscores the importance of making an informed choice in the selection of appropriate cloud cover detection and removal algorithms. For the purpose of large-scale training data, neural networks have been successful in many image processing tasks, but the use of neural networks to remove cloud occlusion in remote sensing imagery is still relatively evolving. The aim of this study is to evaluate the performance of two image restoration algorithms (The spatial attentive generative adversarial network and Convolutional autoencoder with symmetrical skip connection) used for the removal of cloud cover on remote sensing images. An open-source RICE dataset was used for the training and prediction of the models as each of them were implemented for the cloud cover removal. The evaluation metrics used to compare the models’ performance are the Structural similarity index ratio (SSIM), peak signal to noise ratio (PSNR), and the time taken for each model to complete its network training. After a successful completion of the network training using 80% of the data and the remaining 20% to test the networks, the spatial attentive generative adversarial network achieved the best performance on both the peak signal to noise ratio with a value of 26.3447 and the SSIM with a value of 0.8949 while the convolutional autoencoder generates a peak signal to noise ratio of 25.8257 and a SSIM of 0.6307. The result proves that SpaGAN is more effective for automatic cloud cover removal on remote sensing images and the improvement of the quality of the restored image when compared to CNN autoencoder. © 2022 Elsevier B.V.","","Cloud cover removal; Convolutional autoencoder; Generative adversarial network; Spatial attention; Symmetrical skip connection","Article","Final","","Scopus","2-s2.0-85123712143"
"Christovam L.E.; Shimabukuro M.H.; Galo M.L.B.T.; Honkavaara E.","Christovam, L.E. (57204813121); Shimabukuro, M.H. (7004876502); Galo, M.L.B.T. (57195519405); Honkavaara, E. (55927897800)","57204813121; 7004876502; 57195519405; 55927897800","Evaluation of sar to optical image translation using conditional generative adversarial network for cloud removal in a crop dataset","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3-2021","","823","828","5","10.5194/isprs-archives-XLIII-B3-2021-823-2021","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115884850&doi=10.5194%2fisprs-archives-XLIII-B3-2021-823-2021&partnerID=40&md5=fef6a726fb4f09f52dfe6f5eb7e09528","Most methods developed to map crop fields with high-quality are based on optical image time-series. However, often accuracy of these approaches is deteriorated due to clouds and cloud shadows, which can decrease the availably of optical data required to represent crop phenological stages. In this sense, the objective of this study was to implement and evaluate the conditional Generative Adversarial Network (cGAN) that has been indicated as a potential tool to address the cloud and cloud shadow removal; we also compared it with the Witthaker Smother (WS), which is a well-known data cleaning algorithm. The dataset used to train and assess the methods was the Luis Eduardo Magalhães benchmark for tropical agricultural remote sensing applications. We selected one MSI/Sentinel-2 and C-SAR/Sentinel-1 image pair taken in days as close as possible. A total of 5000 image pair patches were generated to train the cGAN model, which was used to derive synthetic optical pixels for a testing area. Visual analysis, spectral behaviour comparison, and classification were used to evaluate and compare the pixels generated with the cGAN and WS against the pixel values from the real image. The cGAN provided consistent pixel values for most crop types in comparison to the real pixel values and outperformed the WS significantly. The results indicated that the cGAN has potential to fill cloud and cloud shadow gaps in optical image time-series. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.","Benchmarking; Crops; Geometrical optics; Pixels; Radar imaging; Remote sensing; Synthetic aperture radar; Time series; CGAN; Image translation; Image-to-image; Optical image; Optical-; Pix2pix; Remote-sensing; Sar-to-optical; Sentinel-2; Synthetic images; Generative adversarial networks","CGAN; Image Translation; Image-to-Image; Pix2Pix; Remote Sensing; Sar-to-Optical; Sentinel-2; Synthetic Images","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115884850"
"Aslantaş N.; Bayram B.; Bakirman T.","Aslantaş, Nuran (57557728500); Bayram, Bülent (15130508500); Bakirman, Tolga (56247265800)","57557728500; 15130508500; 56247265800","Building Segmentation from VHR Aerial Imagery using DeepLabv3+ Architecture","2021","42nd Asian Conference on Remote Sensing, ACRS 2021","","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127386863&partnerID=40&md5=cd5c4377f135e03c8ee89f08ef0b37f3","Up-to-date building footprint maps are of high demand for geographical applications such as sustainable urban planning and management, smart city applications, urban sprawl monitoring, population estimation and disaster management. Unplanned growth and settlement cause many problems such as deterioration of ecological balance, increase in damage from natural disasters, destruction of fertile lands, and drought. Analysis and monitoring of urban growth is an important issue for urban planning, environmental management, and sustainable development in areas of rapid urbanization. Recent advancements on remote sensing and artificial intelligence technologies provide great opportunities to obtain rapid, reliable and accurate building footprint maps. Very high-resolution satellite images and aerial images are rich data sources of spatial information for obtaining building footprints with automated approaches for cities with a variety of building types. Convolutional neural networks (CNN) have recently been used to successfully recover building footprints from satellite images. However, building segmentation from high resolution data is still a challenging task due to complex backgrounds and heterogeneous data structure. To overcome these problems, deep learning (DL) techniques became useful approaches. Different DL models have been proposed for building segmentation such as U-Net with VGG11 encoder pre-trained on ImageNet, Conditional Random Fields (CRF) with FCN, end-to-end self-cascaded network approach and generative adversarial networks (GAN). Additionally, there are existing open datasets for building segmentation issues such as Massachusetts building, ISPRS Vaihingen and Potsdam, Inria and WHU dataset. In this study, we aimed to investigate the performance of the DeepLabv3+ architecture for building segmentation. The hyper parameters of the architecture have been tested and selected empirically to obtain accurate results. The Wuhan University (WHU) Aerial Building Dataset with a spatial resolution of 0.075 to 0.3 m is used for training and testing. The dataset is split as 80%, 10% and 10% for train, validation and test, respectively. Input images are cropped to 512 x 512 pixels. The accuracy assessment results on the test dataset show that mean intersection over union (IoU) reached 98.23%. The obtained results show that DeepLabv3+ architecture is highly capable for building segmentation from very high resolution aerial imagery. © ACRS 2021.All right reserved.","Aerial photography; Buildings; Convolution; Convolutional neural networks; Deep learning; Deterioration; Disaster prevention; Disasters; Environmental management; Generative adversarial networks; Image segmentation; Network architecture; Random processes; Remote sensing; Space applications; Space optics; Statistical tests; Urban growth; Aerial imagery; Building extraction; Building footprint; Deep learning; Deeplabv3+; High demand; High-resolution aerial images; Population estimations; Urban sprawl; Wuhan university dataset; Antennas","Building extraction; deep learning; deeplabv3+; high resolution aerial images; WHU dataset","Conference paper","Final","","Scopus","2-s2.0-85127386863"
"Ma A.; Yu N.; Zheng Z.; Zhong Y.; Zhang L.","Ma, Ailong (55972916000); Yu, Ning (57457393900); Zheng, Zhuo (57208120305); Zhong, Yanfei (12039673900); Zhang, Liangpei (8359720900)","55972916000; 57457393900; 57208120305; 12039673900; 8359720900","A Supervised Progressive Growing Generative Adversarial Network for Remote Sensing Image Scene Classification","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5618818","","","","10.1109/TGRS.2022.3151405","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124815630&doi=10.1109%2fTGRS.2022.3151405&partnerID=40&md5=4d4bdf709b922c0c7973d702b0fa474e","Remote sensing image scene classification is a challenging task. With the development of deep learning, methods based on convolutional neural networks (CNNs) have made great achievements in remote sensing image scene classification. Since the training of a CNN requires a large number of labeled samples, a generative adversarial network (GAN) for sample generation represents a new opportunity to solve the problem of the limited samples. However, most of the existing GAN-based sample generation methods can only generate unlabeled samples, instead of samples labeled with the corresponding scene category. In this article, to solve the problem, a supervised progressive growing generative adversarial network (SPG-GAN) is proposed for remote sensing image scene classification. The proposed method can generate labeled samples for the remote sensing image scene classification, significantly improving the classification accuracy in the case of limited samples. The SPG-GAN method has two main improvements. First, a conditional generative framework for labeled samples is proposed, in which the label information is added in the channel dimension as the input. By considering the constraints of the label information in the loss function, the network can be trained in the direction of a specific category. As a result, the network can generate remote sensing image scene classification samples with label categories. Second, a progressive growing sample generation method is introduced. In order to ensure that the generated samples have more spatial details, they are generated by progressively adding modules to the generator and discriminator, thereby ensuring that the generated sample is of better quality. After testing on two benchmark datasets and carrying out a large-scale experiment in the central area of the city of Wuhan in China, it was found that the proposed method can obtain a superior scene classification accuracy in the case of limited samples. © 1980-2012 IEEE.","China; Hubei; Wuhan; Benchmarking; Classification (of information); Deep learning; Image classification; Image enhancement; Neural networks; Remote sensing; Deep learning; Features extraction; Generative adversarial network; Generator; Image scene classification; Progressive growing; Remote sensing image scene classification; Remote sensing images; Remote-sensing; Sample generations; accuracy assessment; data set; image classification; remote sensing; Generative adversarial networks","Generative adversarial network (GAN); progressive growing; remote sensing image scene classification; sample generation","Article","Final","","Scopus","2-s2.0-85124815630"
"Cheng G.; Sun X.; Li K.; Guo L.; Han J.","Cheng, Gong (36801169800); Sun, Xuxiang (57840691500); Li, Ke (56451924500); Guo, Lei (56428255600); Han, Junwei (24450644400)","36801169800; 57840691500; 56451924500; 56428255600; 24450644400","Perturbation-Seeking Generative Adversarial Networks: A Defense Framework for Remote Sensing Image Scene Classification","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3081421","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107198276&doi=10.1109%2fTGRS.2021.3081421&partnerID=40&md5=9e98b144aa9ab96e112a2064d117f2e4","The methods for remote sensing image (RSI) scene classification based on deep convolutional neural networks (DCNNs) have achieved prominent success. However, confronted with adversarial examples obtained by adding imperceptible perturbations to clean images, the great vulnerability of DCNNs makes it worth exploring effective defense methods. To date, numerous countermeasures for adversarial examples have been proposed, but how to improve the defensive ability for unknown attacks still to be answered. To address this issue, in this article, we propose an effective defense framework specified for RSI scene classification, named perturbation-seeking generative adversarial networks (PSGANs). In brief, a new training framework is designed to train the classifier by introducing the examples generated during the image reconstruction process, in addition to clean examples and adversarial ones. These generated examples can be random kinds of unknown attacks during training and thus are utilized to eliminate the blind spots of a classifier. To assist the proposed training framework, a reconstruction method is developed. First, instead of modeling the distribution of clean examples, we model the distributions of the perturbations added in adversarial examples. Second, to make a tradeoff between the diversity of the reconstructed examples and the optimization of PSGAN, a scale factor named seeking radius is introduced to scale the generated perturbations before they are subtracted by the given adversarial examples. Comprehensive and extensive experimental results on three widely used benchmarks for RSI scene classification demonstrate the great effectiveness of PSGAN when faced with both known and unknown attacks. Our source code is available at https://github.com/xuxiangsun/PSGAN.  © 1980-2012 IEEE.","Convolutional neural networks; Deep neural networks; Image classification; Image reconstruction; Network security; Adversarial networks; Reconstruction method; Reconstruction process; Remote sensing images; Scale Factor; Scene classification; Training framework; Unknown attacks; image classification; perturbation; reconstruction; remote sensing; training; vulnerability; Remote sensing","Adversarial defense; Generative adversarial networks (GANs); Image reconstruction; Remote sensing image (RSI) scene classification","Article","Final","","Scopus","2-s2.0-85107198276"
"Huang S.; Jiang Q.; Jin X.; Lee S.; Feng J.; Yao S.","Huang, Shanshan (57214939600); Jiang, Qian (57194699462); Jin, Xin (56991832300); Lee, Shinjye (34877262700); Feng, Jianan (57221777999); Yao, Shaowen (24473851600)","57214939600; 57194699462; 56991832300; 34877262700; 57221777999; 24473851600","Semi-Supervised Remote Sensing Image Fusion Method Combining Siamese Structure with Generative Adversarial Networks; [结合双胞胎结构与生成对抗网络的半监督遥感图像融合]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","1","","92","105","13","10.3724/SP.J.1089.2021.18227","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100159308&doi=10.3724%2fSP.J.1089.2021.18227&partnerID=40&md5=377d862236b18292e2dbd0a2820abab3","To solve the problems of acquisition of label image and spectral distortion in the current remote sensing image fusion, a semi-supervised remote sensing image fusion method using Siamese structure is proposed. This method adopted a generative adversarial network structure composed of generator and discriminator, in which the generator contains two encoders and a decoder. First, the multispectral image is amplified and converted into HSV color space. Then, the V channel of the multispectral image and panchromatic images are respectively input into the Siamese network of the encoder, and the image features are extracted through the convolutional layer and the multi-skip connection layer model. Third, the obtained feature map is input to the decoder for image reconstruction. And the fused V channel image is identified by the discriminator, so as to obtain the optimal fusion result. Finally, the fused V channel is concatenated with the H and S channels of the multispectral image to obtain the final fused image. In addition, a compound loss function is designed. Experiments on QuickBird satellite remote sensing image dataset show that this method can effectively improve spatial details and color information in fused images. Compared with the contrast algorithms, the fusion images have certain advantages in subjective visual quality and objective evaluation index. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Channel coding; Color; Decoding; Image enhancement; Image reconstruction; Remote sensing; Signal encoding; Space optics; Adversarial networks; Color information; Multispectral images; Objective evaluation; Panchromatic images; QuickBird satellite; Remote sensing images; Spectral distortions; Image fusion","Conditional generative adversarial networks; Multispectral image; Panchromatic image; Remote sensing image fusion; Siamese networks","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85100159308"
"Chen S.; Zhong H.; Li Y.; Wang S.; Yang J.","Chen, Shuo (57842468800); Zhong, Huicai (55421049700); Li, Yongzhou (57872688000); Wang, Shizheng (57226888558); Yang, Jiangang (57216636912)","57842468800; 55421049700; 57872688000; 57226888558; 57216636912","Haze image recognition based on multi-scale feature and multi-adversarial networks; [基于多尺度特征多对抗网络的雾天图像识别]","2021","Journal of Image and Graphics","26","11","","2680","2690","10","10.11834/jig.200491","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119277813&doi=10.11834%2fjig.200491&partnerID=40&md5=dc31aae428607530ffb1614d65faaadd","Objective: While dealing with high-resolution remote sensing image scene recognition, classical supervised machine learning algorithms are considered effective on two conditions, namely, 1) test samples should be in the same feature space with training samples, and 2) adequate labeled samples should be provided to train the model fully. Deep learning algorithms, which achieve remarkable results in image classification and object detection for the past few years, generally require a large number of labeled samples to learn the accurate parameters.The main image classification methods select raining and test samples randomly from the same dataset, and adopt cross validation to testify the effectiveness of the model. However, obtaining scene labels is time consuming and expensive for remote sensing images. To deal with the insufficiency of labeled samples in remote sensing image scene recognition and the problem that labeled samples cannot be shared between different datasets due to different sensors and complex light conditions, deep learning architecture and adversarial learning are investigated. A feature transfer method based on adversarial variational autoencoder is proposed. Method: Feature transfer architecture can be divided into three parts. The first part is the pretrain model.Given the limited samples with scene labels, the unsupervised learning model, variational autoencoder(VAE), is adopted. The VAE is unsupervised trained on the source dataset, and the encoder part in the VAE is finetuned together with classifier network using labeled samples in the source dataset. The second part is adversarial learning module. In most of the research, adversarial learning is adopted to generate new samples, while the idea is used to transfer the features from source domain to target domain in this paper.Parameters of the finetuned encoder network for the source dataset are then used to initialize the target encoder. Using the idea of adversarial training in generative adversarial networks (GAN), a discrimination network is introduced into the training of the target encoder. The goal of the target encoder is to extract features in the target domain to have as much affinity to those of the source domain as possible, such that the discrimination network cannot distinguish the features are from either the source domain or target domain. The goal of the discrimination network is to optimize the parameters for better distinction. It is called adversarial learning because of the contradiction between the purpose of encoder and discrimination network. The features extracted by the target encoder increasingly resemble those by the source encoder by training and updating the parameters of the target encoder and the discrimination network alternately. In this manner, by the time the discrimination network can no longer differentiate between source features and target features, we can assume that the target encoder can extract similar features to the source samples, and remote sensing feature transfer between the source domain and target domain is accomplished. The third part is target finetuning and test module. A small number of labeled samples in target domain is employed to finetune the target encoder and source classifier, and the other samples are used for evaluation. Result: Two remote sensing scene recognition datasets, UCMerced-21 and NWPU-RESISC45, are adopted to prove the effectiveness of the proposed feature transfer method. SUN397, a natural scene recognition dataset is employed as an attempt for the cross-view feature transfer. Eight common scene types between the three datasets, namely, baseball field, beach, farmland, forest, harbor, industrial area, overpass, and river/lake, are selected for the feature transfer task.Correlation alignment (CORAL) and balanced distribution adaptation (BDA) are used as comparisons. In the experiments of adversarial learning between two remote sensing scene recognition datasets, the proposed method boosts the recognition accuracy by about 10% compared with the network trained only by the samples in the source domain. Results improve more substantially when few samples in the target domain are involved. Compared with CORAL and BDA, the proposed method improves scene recognition accuracy by more than 3% when using a few samples in the target domain and between 10%40% without samples in the target domain. When using the information of a natural scene image, the improvement is not as much as that of a remote sensing image, but the scene recognition accuracy using the proposed feature transfer method is still increased by approximately 6% after unsupervised feature transfer and 36% after a small number of samples in the target domain are involved in finetuning. Conclusion: In this paper, an adversarial transfer learning network is proposed. The experimental results show that the proposed adversarial learning method can make the most of sample information of other dataset when the labeled samples are insufficient in the target domain. The proposed method can achieve the feature transfer between different datasets and scene recognition effectively, and remarkably improve the scene recognition accuracy. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","Domain shift; Haze scene; Image recognition; Multiple adversarial; Multiple features","Article","Final","","Scopus","2-s2.0-85119277813"
"Li C.; Song H.; Zhang K.; Zhang X.; Liu Q.","Li, Changjie (57223047069); Song, Huihui (36572623600); Zhang, Kaihua (55475000500); Zhang, Xiaolu (57208245743); Liu, Qingshan (36063739200)","57223047069; 36572623600; 55475000500; 57208245743; 36063739200","Spatiotemporal fusion of satellite images via conditional generative adversarial learning; [条件生成对抗遥感图像时空融合]","2021","Journal of Image and Graphics","26","3","","714","726","12","10.11834/jig.200219","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104657247&doi=10.11834%2fjig.200219&partnerID=40&md5=52628aba68827e8dc0309c5198129282","Objective: Spatiotemporal fusion of satellite images is an important problem in the research of remote sensing fusion. With the intensification of global environmental changes, satellite remote sensing data plays an indispensable role in monitoring crop growth and landform changes. In the field of dynamic monitoring, high temporal resolution becomes an important attribute of required remote sensing data because continuous observation is basic requirement for dynamic monitoring. Moreover, the fragmentation of the global terrestrial landscape makes these applications require remote sensing data with higher spatial resolutions. However, remote sensing data with high spatial and high temporal resolutions are difficult to be captured by current satellite platforms due to constraints of technology and cost. For example, Landsat images mainly have a high spatial resolution but a low temporal resolution. By contrast, MODIS(moderate-resolution imaging spectroradiometer) images have a high temporal resolution but a low spatial resolution. Spatiotemporal fusion provides an effective method to fuse the two types of remote sensing data featured by complementary spatial and temporal properties (Landsat and MODIS images are typical representatives) to generate fused data with high spatial and high temporal resolutions, which can also bring great convenience to our research on the actual terrain and landform changes. Method: A spatiotemporal fusion method based on the conditional generative adversarial network (CGAN), which can effectively handle massive remote sensing data in practical applications, is proposed to solve this problem. As for CGAN, GAN(generative advensarial network) is extended to CGAN that introduces the internal ground truth image as the condition variable to guide discriminator network learning, making the training of the network more directional and easier. In this study, the asymmetric Laplacian pyramid network is used as the generator of the CGAN, and the VGG(visual geometry group) net is taken as the discriminator of the CGAN. The asymmetric Laplacian pyramid network mainly consists of two branches: a high-frequency branch (mainly extracts the image details or residual images) and a low-frequency extraction branch (extracts shallow features). The two branches progressively reconstruct the images in a coarse-to-fine manner. The discriminator of the CGAN is the VGG19 (visual geometry group 19-layer net) network, where the ReLU activation function is replaced by the Leaky ReLU function, and the number of channels of the convolutional kernels is increased by a factor of 2 from 64 to 1 024. Then, a fully connected layer and a sigmoid activation function are used to obtain the probability of the sample class. In this study, a CGAN model is designed for the nonlinear mapping and a CGAN superresolution model for downsampled Landsat to reconstruct original Landsat images. Compared with existing shallow learning methods, especially for the sparse-representation-based ones, the proposed CGAN based model has the following merits: 1) explicitly correlating MODIS and downsampled Landsat images by learning a nonlinear mapping relationship, 2) automatically learning and extracting effective image features and image details, and 3) unifying feature extraction, nonlinear mapping, and image reconstruction into one optimization framework. In the training stage, a nonlinear mapping is first trained between the MODIS and downsampled Landsat data using the CGAN model. Then, multiscale superresolution CGAN is trained between the downsampled Landsat and original Landsat data. The prediction procedure contains two layers, and each layer consists of a CGAN-based prediction and a fusion model. The fusion model takes the high pass model which will be explained in the next paper. One of the two layers achieves nonlinear mapping from the MODIS to downsampled Landsat data, and the other layer is the superresolution reconstructed network of the set that is used to perform image superresolution of two and five times of upsampling scales, respectively. Result: Four indicators are commonly used to evaluate the performance of spatiotemporal fusion of remote sensing images. The first one is root mean square error, which measures the radiometric between the fusion result and ground truth. The spectral angle mapper is leveraged as the second index to measure the spectral distortion of the result. The structural similarity is taken as the third metric, measuring the similarity of the overall spatial structures between the fusion result and ground truth. Finally, the erreur relative global adimensionnelle de synthese is selected as the last index to evaluate the overall fusion result. Extensive evaluations are executed on two groups of commonly used Landsat-MODIS benchmark datasets. For the fusion results, a quantitative evaluation of the visual effects of all predicted dates and one key date shows that the method can achieve more accurate fusion results compared with sparse representation-based methods and deep convolutional networks. Conclusion: A CGAN model that introduces an external condition to reconstruct images better is proposed. A non-linear mapping CGAN is trained to deal with the highly nonlinear correspondence relations between between downsampled Landset and MODIS data. Moreover, a multiscale superresolution CGAN is trained to bridge the huge spatial resolution gap (10 times) between original and downsampled Landsat data. Experimental verification is performed on existing methods, such as sparse representation-based methods and deep convolutional neural network methods. Experiment results show that our model outperforms several state-of-the-art spatiotemporal fusion approaches. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","Conditional generative adversarial network(CGAN); Deep learning; Laplacian pyramid network; Remote sensing image processing; Spatiotemporal fusion","Article","Final","","Scopus","2-s2.0-85104657247"
"Zhang H.; Xu H.; Tian X.; Jiang J.; Ma J.","Zhang, Hao (57215014270); Xu, Han (57201056465); Tian, Xin (55458978600); Jiang, Junjun (54902306100); Ma, Jiayi (26638975600)","57215014270; 57201056465; 55458978600; 54902306100; 26638975600","Image fusion meets deep learning: A survey and perspective","2021","Information Fusion","76","","","323","336","13","10.1016/j.inffus.2021.06.008","122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109457013&doi=10.1016%2fj.inffus.2021.06.008&partnerID=40&md5=a6b3eaa2262c1c3dd239ff43b969ae55","Image fusion, which refers to extracting and then combining the most meaningful information from different source images, aims to generate a single image that is more informative and beneficial for subsequent applications. The development of deep learning has promoted tremendous progress in image fusion, and the powerful feature extraction and reconstruction capabilities of neural networks make the fused results promising. Recently, several latest deep learning technologies have made image fusion explode, e.g., generative adversarial networks, autoencoder, etc. However, a comprehensive review and analysis of latest deep-learning methods in different fusion scenarios is lacking. To this end and in this survey, we first introduce the concept of image fusion, and classify the methods from the perspectives of the deep architectures adopted and fusion scenarios. Then, we review the state-of-the-art on the use of deep learning in various types of image fusion scenarios, including the digital photography image fusion, the multi-modal image fusion and the sharpening fusion. Subsequently, the evaluation for some representative methods in specific fusion tasks are performed qualitatively and quantitatively. Moreover, we briefly introduce several typical applications of image fusion, including photography visualization, RGBT object tracking, medical diagnosis, and remote sensing monitoring. Finally, we provide the conclusion, highlight the challenges in image fusion, and look forward to potential future research directions. © 2021 Elsevier B.V.","Deep learning; Diagnosis; Learning systems; Medical imaging; Object tracking; Photography; Remote sensing; Surveys; Adversarial networks; Deep architectures; Digital photography; Future research directions; Learning technology; Reconstruction capability; Remote sensing monitoring; Typical application; Image fusion","Deep learning; Digital photography; Image fusion; Multi-modal; Sharpening","Short survey","Final","","Scopus","2-s2.0-85109457013"
"Ji S.; Wang D.; Luo M.","Ji, Shunping (9633134900); Wang, DIngpan (57223100321); Luo, Muying (57203092560)","9633134900; 57223100321; 57203092560","Generative Adversarial Network-Based Full-Space Domain Adaptation for Land Cover Classification from Multiple-Source Remote Sensing Images","2021","IEEE Transactions on Geoscience and Remote Sensing","59","5","9198144","3816","3828","12","10.1109/TGRS.2020.3020804","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104734818&doi=10.1109%2fTGRS.2020.3020804&partnerID=40&md5=c4a30197f819290b2a09b5f793f4300e","The accuracy of remote sensing image segmentation and classification is known to dramatically decrease when the source and target images are from different sources; while deep learning-based models have boosted performance, they are only effective when trained with a large number of labeled source images that are similar to the target images. In this article, we propose a generative adversarial network (GAN) based domain adaptation for land cover classification using new target remote sensing images that are enormously different from the labeled source images. In GANs, the source and target images are fully aligned in the image space, feature space, and output space domains in two stages via adversarial learning. The source images are translated to the style of the target images, which are then used to train a fully convolutional network (FCN) for semantic segmentation to classify the land cover types of the target images. The domain adaptation and segmentation are integrated to form an end-to-end framework. The experiments that we conducted on a multisource data set covering more than 3500 km2 with 51 560 256times 256 high-resolution satellite images in Wuhan city and a cross-city data set with 11 383,,256times 256 aerial images in Potsdam and Vaihingen demonstrated that our method exceeded the recent GAN-based domain adaptation methods by at least 6.1% and 4.9% in the mean intersection over union (mIoU) and overall accuracy (OA) indexes, respectively. We also proved that our GAN is a generic framework that can be implemented for other domain transfer methods to boost their performance.  © 1980-2012 IEEE.","Antennas; Convolutional neural networks; Deep learning; Image classification; Remote sensing; Semantics; Space optics; Adversarial learning; Adversarial networks; Convolutional networks; High resolution satellite images; Land cover classification; Learning Based Models; Remote sensing images; Semantic segmentation; artificial neural network; image classification; land cover; remote sensing; satellite imagery; Image segmentation","Domain adaptation; generative adversarial network (GAN); image classification; remote sensing","Article","Final","","Scopus","2-s2.0-85104734818"
"Zhao R.; Shi Z.","Zhao, Rui (57225978410); Shi, Zhenwei (23398841900)","57225978410; 23398841900","Text-to-Remote-Sensing-Image Generation with Structured Generative Adversarial Networks","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3068391","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103784947&doi=10.1109%2fLGRS.2021.3068391&partnerID=40&md5=2ff4006eccfb5fa51c2747408160a909","Synthesizing high-resolution remote sensing images based on the given text descriptions has great potential in expanding the image data set to release the power of deep learning in the remote sensing image processing field. However, there has been no efficient research carried out on this formidable task yet. Given a remote sensing image, the structural rationality of ground objects is critical to judge it whether real or fake, e.g., real bridges are always straight, while a sinuous one can be easily judged as fake. Inspired by this, we propose a multistage structured generative adversarial network (StrucGAN) to synthesize remote sensing images in a structured way given the text descriptions. StrucGAN utilizes structural information extracted by an unsupervised segmentation module to enable the discriminators to distinguish the image in a structured way. The generators of StrucGAN are, thus, forced to synthesize structural reasonable image contents, which could enhance the image authenticity. The multistage framework enables the StrucGAN to generate remote sensing images with increasing resolution stage by stage. The quantitative and qualitative experiments' results show that the proposed StrucGAN achieves better performance compared with the baseline, and it could synthesize high resolution, realistic, structural reasonable remote sensing images that are semantically consistent with the given text descriptions.  © 2004-2012 IEEE.","Deep learning; Image enhancement; Image segmentation; Adversarial networks; High resolution; High resolution remote sensing images; Qualitative experiments; Remote sensing image processing; Remote sensing images; Structural information; Unsupervised segmentation; image; qualitative analysis; quantitative analysis; remote sensing; Remote sensing","Generative adversarial networks (GANs); remote sensing image synthesize; structural rationality; text description","Article","Final","","Scopus","2-s2.0-85103784947"
"Yang Z.-L.; Zhou R.-Y.; Wang F.; Xu F.","Yang, Zhi-Long (57204710319); Zhou, Ruo-Yi (57482370800); Wang, Feng (56459216100); Xu, Feng (9240823200)","57204710319; 57482370800; 56459216100; 9240823200","A POINT CLOUDS FRAMEWORK FOR 3-D RECONSTRUCTION OF SAR IMAGES BASED ON 3-D PARAMETRIC ELECTROMAGNETIC PART MODEL","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4818","4821","3","10.1109/IGARSS47720.2021.9553789","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126046692&doi=10.1109%2fIGARSS47720.2021.9553789&partnerID=40&md5=f460ddb45de9a442ce3fcd453e4c04af","3-D reconstruction is a hot topic in remote sensing as well as computer vision. The particularity and complexity of the microwave scattering mechanism bring great challenges to the 3D reconstruction of SAR images, and the applicability of existing methods need to be improved. This study proposes an efficient and explainable point clouds framework for three-dimensional reconstruction of SAR images based on three-dimensional parametric electromagnetic part models. This 3-D SAR reconstruction framework consists of two parts: a feature extraction generative adversarial network and a 3-D reconstruction generative network. The feature extraction generative adversarial network has 5 convolutional layers to extract the features of single SAR image and save them in the form of graph, then input this graph to the 3-D reconstruction generative network and we can get the main shape of the target from a SAR image. This framework effectively reduces the numbers of observation for 3-D reconstruction and make the 3-D reconstruction from single SAR image possible. © 2021 IEEE","","3-D reconstruction; Deep learning; Parametric electromagnetic part model; Point clouds generation; SAR","Conference paper","Final","","Scopus","2-s2.0-85126046692"
"Zhao S.; Yang S.; Gu J.; Liu Z.; Feng Z.","Zhao, Shihui (57211266107); Yang, Shuyuan (8159166000); Gu, Jing (57015938500); Liu, Zhi (57203466982); Feng, Zhixi (56047550500)","57211266107; 8159166000; 57015938500; 57203466982; 56047550500","Symmetrical lattice generative adversarial network for remote sensing images compression","2021","ISPRS Journal of Photogrammetry and Remote Sensing","176","","","169","181","12","10.1016/j.isprsjprs.2021.03.009","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105008640&doi=10.1016%2fj.isprsjprs.2021.03.009&partnerID=40&md5=0ce34e858236acac493cbea97dc35fef","Image compression usually includes two important operations: compression and decompression. The compression process includes the operation of discarding information, while the decompression process is to retrieve the lost information. In order to make the decompressed image more similar to the original image, the classic compression methods generally adopt the process of approximately reversible compression and decompression. Inspired by the symmetric structure in classic compression methods, we propose a new symmetrical lattice generating adversarial network (SLGAN) for the remote sensing images (RSIs) compression in this paper. Several pairs of symmetrical encoder-decoder lattices are constructed to build a generator to first generate deep representative codes of images and then decode them. For each pair of encoded lattice and decoded lattice, one discriminator is constructed to perform adversarial learning with the generator. When multiple discriminators are used for all the lattices, a cooperative learning algorithm is proposed to train jointly pairs of symmetric lattices in the generator. Moreover, to enhance edges, contours, and textures in the decomposed RSIs, an enhanced Laplacian of gaussian (ELoG) loss is designed as a regularizer to train the SLGAN. Experimental results on the panchromatic images from GF2 satellite show that SLGAN outperforms other existing state-of-the-art methods. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Decoding; Laplace transforms; Learning algorithms; Remote sensing; Textures; Adversarial networks; Compression methods; Compression process; Cooperative learning; Generative adversarial network; Images compression; Laplacian of Gaussian; Remote sensing image compression; Remote sensing images; Symmetrical lattice; algorithm; experimental study; image analysis; lattice dynamics; panchromatic image; remote sensing; satellite data; Image compression","Cooperative learning; Generative adversarial network; Laplacian of gaussian; Remote sensing image compression; Symmetrical lattice","Article","Final","","Scopus","2-s2.0-85105008640"
"Zheng J.; Liu X.-Y.; Wang X.","Zheng, Jiahao (57345876700); Liu, Xiao-Yang (44361326100); Wang, Xiaodong (56461470100)","57345876700; 44361326100; 56461470100","Single Image Cloud Removal Using U-Net and Generative Adversarial Networks","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9224941","6371","6385","14","10.1109/TGRS.2020.3027819","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111110267&doi=10.1109%2fTGRS.2020.3027819&partnerID=40&md5=bc15eb3738728242c8b7ced56c3c97d3","Cloud removal is a ubiquitous and important task in remote sensing image processing, which aims at restoring the ground regions shadowed by clouds. It is challenging to remove the clouds for a single satellite image due to the difficulty of distinguishing clouds from white objects on the ground and filling the irregular missing regions with visual consistency. In this article, we propose a novel two-stage cloud removal method. The first stage is cloud segmentation, i.e., extracting the clouds and removing the thin clouds directly using U-Net. The second stage is image restoration, i.e., removing the thick cloud and recovering the corresponding irregular missing regions using generative adversarial network (GAN). We evaluate the proposed scheme on both synthetic images and real satellite images (over $20\,000\, \times \,20\,000$ pixels). On synthetic images for cloud coverage less than 40%, the proposed scheme achieves improvements of 0.049-0.078 in Structural SIMilarity (SSIM) and 3.8-6.2 dB in peak signal-to-noise ratio (PSNR), while the $\ell _{1}$ -norm error reduces by 49%-78%, compared with a state-of-the-art deep learning method Pix2Pix. On real satellite images, we demonstrate the consistent visual results of the proposed scheme.  © 1980-2012 IEEE.","Deep learning; Image reconstruction; Learning systems; Remote sensing; Satellites; Signal to noise ratio; Adversarial networks; Cloud segmentation; Learning methods; Peak signal to noise ratio; Remote sensing image processing; State of the art; Structural similarity; Visual consistency; artificial neural network; cloud cover; image analysis; Image enhancement","Encoder-decoder; generative adversarial network (GAN); single image cloud removal; U-Net","Article","Final","","Scopus","2-s2.0-85111110267"
"Nesvold E.; Mukerji T.","Nesvold, Erik (57211241618); Mukerji, Tapan (7003413039)","57211241618; 7003413039","Simulation of Fluvial Patterns With GANs Trained on a Data Set of Satellite Imagery","2021","Water Resources Research","57","5","e2019WR025787","","","","10.1029/2019WR025787","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106867887&doi=10.1029%2f2019WR025787&partnerID=40&md5=c6d418a5709f0e59deb13a74dad8fa21","Models that can generate realistic Earth surface patterns are important both for geomorphological applications and as prior models for underdetermined inverse problems. Generative machine learning methods such as GANs and the increasing availability of large remote sensing data sets represents an exciting combination for this purpose. Several studies show promising results for GANs trained on artificial data sets in geostatistics, but it is necessary to further quantify how well such models reproduce and generalize real data. The conditioning ability of GANs is often evaluated based on output which originates from a trained generator. In reality, geophysical data necessarily arises from elsewhere. Here, we use more realistic training data than in previous studies and evaluate performance using an extensive set of metrics and real images outside the training data set. The data set consists of multispectral satellite imagery of 38 large river deltas, a type of Earth surface pattern which is limited in number. The channel network is used to create training images with four sedimentary facies, which are subsequently used to train a Wasserstein GAN of deltaic 2D patterns. GANs successfully reproduce all training data characteristics and produce manifold the number of combinations with respect to the training data. However, there does not seem to be an infinite number of discrete combinations of facies, and the posterior landscapes are not well-shaped for efficient exploration in the presence of so-called hard data. Thus, GANs should have many exciting applications in geosciences, but it will depend on the type of measurement data. © 2021. American Geophysical Union. All Rights Reserved.","Inverse problems; Learning systems; Remote sensing; Textile printing; Geophysical data; Infinite numbers; Machine learning methods; Measurement data; Multispectral satellite imagery; Remote sensing data; Sedimentary facies; Training data sets; data set; geostatistics; multispectral image; numerical model; performance assessment; remote sensing; satellite imagery; simulation; Satellite imagery","Bayesian inversion; generative adversarial networks; geomodeling; hydrogeology; river deltas","Article","Final","","Scopus","2-s2.0-85106867887"
"Wang M.; Meng X.; Shao F.; Fu R.","Wang, Mengyao (57210968797); Meng, Xiangchao (56158755000); Shao, Feng (7006717672); Fu, Randi (14821950200)","57210968797; 56158755000; 7006717672; 14821950200","SAR-Assisted Optical Remote Sensing Image Cloud Removal Method Based on Deep Learning; [基于深度学习的SAR辅助下光学遥感图像去云方法]","2021","Guangxue Xuebao/Acta Optica Sinica","41","12","1228002","","","","10.3788/AOS202141.1228002","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113278742&doi=10.3788%2fAOS202141.1228002&partnerID=40&md5=9b89574ba859279c7a4f3fd7c4491b52","The existing deep learning based SAR-assisted cloud removal methods do not take full into account the texture and spectral information of the optical images, which results in blurring and spectral loss. In this paper, we constructed a data set for SAR-assisted cloud removal based on the Sentinel-1 and Sentinel-2 satellite images in Yuhang District of Hangzhou. In addition, we established a conditional generative adversarial network (cGAN) based model by fully considering the details, texture, and color information of optical remote sensing images, achieving information recovery and reconstruction in the case of optical images covered by thin clouds, fog, and thick clouds. The results show that the proposed method outperforms other methods in SAR-assisted cloud removal. © 2021, Chinese Lasers Press. All right reserved.","Geometrical optics; Image texture; Learning systems; Radar imaging; Remote sensing; Textures; Adversarial networks; Cloud removal; Color information; Information recovery; Optical image; Optical remote sensing; Satellite images; Spectral information; Deep learning","Cloud removal; Conditional generative adversarial network (cGAN); Image fusion; Optical image; Remote sensing; Synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85113278742"
"Liu B.; Zhao L.; Li J.; Zhao H.; Liu W.; Li Y.; Wang Y.; Chen H.; Cao W.","Liu, Baodi (16319146900); Zhao, Lifei (57274635800); Li, Jiaoyue (57299552400); Zhao, Hengle (57376940600); Liu, Weifeng (36739405100); Li, Ye (35770968700); Wang, Yanjiang (57223714000); Chen, Honglong (24174425300); Cao, Weijia (55558020600)","16319146900; 57274635800; 57299552400; 57376940600; 36739405100; 35770968700; 57223714000; 24174425300; 55558020600","Saliency-guided remote sensing image super-resolution","2021","Remote Sensing","13","24","5144","","","","10.3390/rs13245144","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121442780&doi=10.3390%2frs13245144&partnerID=40&md5=fca1807ae0681758c0b7de7375f163af","Deep learning has recently attracted extensive attention and developed significantly in remote sensing image super-resolution. Although remote sensing images are composed of various scenes, most existing methods consider each part equally. These methods ignore the salient objects (e.g., buildings, airplanes, and vehicles) that have more complex structures and require more attention in recovery processing. This paper proposes a saliency-guided remote sensing image super-resolution (SG-GAN) method to alleviate the above issue while maintaining the merits of GAN-based methods for the generation of perceptual-pleasant details. More specifically, we exploit the salient maps of images to guide the recovery in two aspects: On the one hand, the saliency detection network in SG-GAN learns more high-resolution saliency maps to provide additional structure priors. On the other hand, the well-designed saliency loss imposes a second-order restriction on the super-resolution process, which helps SG-GAN concentrate more on the salient objects of remote sensing images. Experimental results show that SG-GAN achieves competitive PSNR and SSIM compared with the advanced super-resolution methods. Visual results demonstrate our superiority in restoring structures while generating remote sensing super-resolution images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Computer system recovery; Deep learning; Object detection; Optical resolving power; Remote sensing; Complexes structure; Detection networks; Image super resolutions; Learn+; Remote sensing images; Saliency detection; Salient maps; Salient object detection; Salient objects; Superresolution; Generative adversarial networks","Generative adversarial network; Image super-resolution; Remote sensing image; Salient object detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121442780"
"Liu Q.; Meng X.; Shao F.; Li S.","Liu, Qiang (57208497463); Meng, Xiangchao (56158755000); Shao, Feng (57551562000); Li, Shutao (7409240361)","57208497463; 56158755000; 57551562000; 7409240361","PSTAF-GAN: Progressive Spatio-Temporal Attention Fusion Method Based on Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5408513","","","","10.1109/TGRS.2022.3161563","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127042760&doi=10.1109%2fTGRS.2022.3161563&partnerID=40&md5=986ff628c27874e02078a0dee456c263","Spatio-temporal fusion aims to integrate multisource remote sensing images with complementary high spatial and temporal resolutions, so as to obtain time-series high spatial resolution fused images. Currently, deep learning (DL)-based spatio-temporal fusion methods have received broad attention. However, on one hand, most of the existing DL-based methods train the model in a band-by-band manner, ignoring the correlations among bands. On the other hand, the general coarse spatio-temporal changes in low spatial resolution images (e.g., MODIS) calculated at the pixel domain cannot completely cover the fine spatio-temporal changes in high spatial resolution images (e.g., Landsat), due to complex surface features and the general large spatial resolution ratio between fine and coarse images. Besides, the existing DL-based spatio-temporal fusion methods are insufficient in exploring multiscale information by only stacking convolutional kernels with different sizes. To alleviate the above challenges, we propose a progressive spatio-temporal attention fusion model in a multiband training manner based on generative adversarial network (PSTAF-GAN). Specifically, we design a flexible multiscale feature extraction architecture to extract multiscale feature hierarchies. Then, spatio-temporal changes are calculated on the feature domain in different feature hierarchies. Besides, a spatio-temporal attention fusion architecture is proposed to fuse the spatio-temporal changes and ground details in a coarse-to-fine manner, which can explore multiscale information more sufficient and gradually recover the target image. The results of quantitative and qualitative experiments on two publicly available benchmark datasets show that the proposed PSTAF-GAN can achieve the best performance compared with the state-of-the-art methods.  © 1980-2012 IEEE.","Benchmarking; Deep learning; Image fusion; Image resolution; Remote sensing; Fusion methods; Generative adversarial network; High spatial resolution; Mul-ti-level feature; Remote-sensing; Spatio-temporal; Spatio-temporal attention fusion; Spatio-temporal changes; Spatio-temporal fusions; Weight sharing; remote sensing; spatiotemporal analysis; Generative adversarial networks","Generative adversarial network (GAN); multilevel feature; remote sensing; spatio-temporal attention fusion; weight sharing","Article","Final","","Scopus","2-s2.0-85127042760"
"Dong R.; Zhang L.; Fu H.","Dong, Runmin (57205415789); Zhang, Lixian (57207392945); Fu, Haohuan (8713118400)","57205415789; 57207392945; 8713118400","RRSGAN: Reference-Based Super-Resolution for Remote Sensing Image","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2020.3046045","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099726810&doi=10.1109%2fTGRS.2020.3046045&partnerID=40&md5=9011834a954982a4325984a5efa39e97","Remote sensing image super-resolution (SR) plays an important role by supplementing the lack of original high-resolution (HR) images in the study scenarios of large spatial areas or long time series. However, due to the lack of imagery information in low-resolution (LR) images, single-image super-resolution (SISR) is an inherently ill-posed problem. Especially, it is difficult to reconstruct the fine textures of HR images at large upscaling factors (e.g., four times). In this work, based on Google Earth HR images, we explore the potential of the reference-based super-resolution (RefSR) method on remote sensing images, utilizing rich texture information from HR reference (Ref) images to reconstruct the details in LR images. This method can use existing HR images to help reconstruct the LR images of long time series or a specific time. We build a reference-based remote sensing SR data set (RRSSRD). Furthermore, by adopting the generative adversarial network (GAN), we propose a novel end-to-end reference-based remote sensing GAN (RRSGAN) for SR. RRSGAN can extract the Ref features and align them to the LR features. Eventually, the texture information in the Ref features can be transferred to the reconstructed HR images. In contrast to the existing RefSR methods, we propose a gradient-assisted feature alignment method that adopts the deformable convolutions to align the Ref and LR features and a relevance attention module (RAM) to improve the robustness of the model in different scenarios (e.g., land cover changes and cloud coverage). The experimental results demonstrate that RRSGAN is robust and outperforms the state-of-the-art SISR and RefSR methods in both quantitative evaluation and visual results, which indicates the great potential of the RefSR method for remote sensing tasks.  © 1980-2012 IEEE.","Image reconstruction; Image texture; Optical resolving power; Textures; Time series; Adversarial networks; High resolution image; Ill posed problem; Land-cover change; Low resolution images; Quantitative evaluation; Remote sensing images; Texture information; image resolution; remote sensing; satellite imagery; Remote sensing","Deep learning; Remote sensing imagery; Super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85099726810"
"Yuan B.; Zhao D.; Shao S.; Yuan Z.; Wang C.","Yuan, Bo (57222962760); Zhao, Danpei (24082070800); Shao, Shuai (57560050600); Yuan, Zehuan (55322757000); Wang, Changhu (17347274000)","57222962760; 24082070800; 57560050600; 55322757000; 17347274000","Birds of a Feather Flock Together: Category-Divergence Guidance for Domain Adaptive Segmentation","2022","IEEE Transactions on Image Processing","31","","","2878","2892","14","10.1109/TIP.2022.3162471","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127533411&doi=10.1109%2fTIP.2022.3162471&partnerID=40&md5=13a354cd72bf03e378bb6a170483d7bc","Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. Present UDA models focus on alleviating the domain shift by minimizing the feature discrepancy between the source domain and the target domain but usually ignore the class confusion problem. In this work, we propose an Inter-class Separation and Intra-class Aggregation (ISIA) mechanism. It encourages the cross-domain representative consistency between the same categories and differentiation among diverse categories. In this way, the features belonging to the same categories are aligned together and the confusable categories are separated. By measuring the align complexity of each category, we design an Adaptive-weighted Instance Matching (AIM) strategy to further optimize the instance-level adaptation. Based on our proposed methods, we also raise a hierarchical unsupervised domain adaptation framework for cross-domain semantic segmentation task. Through performing the image-level, feature-level, category-level and instance-level alignment, our method achieves a stronger generalization performance of the model from the source domain to the target domain. In two typical cross-domain semantic segmentation tasks, i.e., GTA $5\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $ Cityscapes, our method achieves the state-of-the-art segmentation accuracy. We also build two cross-domain semantic segmentation datasets based on the publicly available data, i.e., remote sensing building segmentation and road segmentation, for domain adaptive segmentation. Our code, models and datasets are available at https://github.com/HibiscusYB/BAFFT. © 1992-2012 IEEE.","Data Collection; Image Processing, Computer-Assisted; Semantics; Job analysis; Remote sensing; Semantic Segmentation; Semantics; Separation; Adaptation models; Category divergence; Class separation; Domain adaptation; Features extraction; Images segmentations; Inter class; Inter-class separation; Intra class; Intra-class aggregation; Semantic segmentation; Task analysis; Unsupervised domain adaptation; image processing; information processing; semantics; Generative adversarial networks","category divergence; inter-class separation; intra-class aggregation; semantic segmentation; Unsupervised domain adaptation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127533411"
"Rui X.; Cao Y.; Yuan X.; Kang Y.; Song W.","Rui, Xue (57222811749); Cao, Yang (57022583200); Yuan, Xin (57316095800); Kang, Yu (7402785017); Song, Weiguo (55726284700)","57222811749; 57022583200; 57316095800; 7402785017; 55726284700","Disastergan: Generative adversarial networks for remote sensing disaster image generation","2021","Remote Sensing","13","21","4284","","","","10.3390/rs13214284","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118221641&doi=10.3390%2frs13214284&partnerID=40&md5=48a47e83a0e37f8b7b8ce50c9c53d88b","Rapid progress on disaster detection and assessment has been achieved with the development of deep-learning techniques and the wide applications of remote sensing images. However, it is still a great challenge to train an accurate and robust disaster detection network due to the class imbalance of existing data sets and the lack of training data. This paper aims at synthesizing disaster remote sensing images with multiple disaster types and different building damage with generative adversarial networks (GANs), making up for the shortcomings of the existing data sets. However, existing models are inefficient in multi-disaster image translation due to the diversity of disaster and inevitably change building-irrelevant regions caused by directly operating on the whole image. Thus, we propose two models: disaster translation GAN can generate disaster images for multiple disaster types using only a single model, which uses an attribute to represent disaster types and a reconstruction process to further ensure the effect of the generator; damaged building generation GAN is a mask-guided image generation model, which can only alter the attribute-specific region while keeping the attribute-irrelevant region unchanged. Qualitative and quantitative experiments demonstrate the validity of the proposed methods. Further experimental results on the damaged building assessment model show the effectiveness of the proposed models and the superiority compared with other data augmentation methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Buildings; Deep learning; Disasters; Image processing; Remote sensing; Class imbalance; Data augmentation; Data set; Detection networks; Image generations; Learning techniques; Remote sensing disaster image; Remote sensing images; Remote-sensing; Training data; Generative adversarial networks","Data augmentation; GAN; Image generation; Remote sensing disaster image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118221641"
"Peng D.; Bruzzone L.; Zhang Y.; Guan H.; DIng H.; Huang X.","Peng, Daifeng (56622117600); Bruzzone, Lorenzo (7006892410); Zhang, Yongjun (55577971100); Guan, Haiyan (48260990900); DIng, Haiyong (25638449000); Huang, Xu (36816004500)","56622117600; 7006892410; 55577971100; 48260990900; 25638449000; 36816004500","SemiCDNet: A Semisupervised Convolutional Neural Network for Change Detection in High Resolution Remote-Sensing Images","2021","IEEE Transactions on Geoscience and Remote Sensing","59","7","9161009","5891","5906","15","10.1109/TGRS.2020.3011913","69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099581902&doi=10.1109%2fTGRS.2020.3011913&partnerID=40&md5=2225b39556721c237d2f25144c4a1bf5","Change detection (CD) is one of the main applications of remote sensing. With the increasing popularity of deep learning, most recent developments of CD methods have introduced the use of deep learning techniques to increase the accuracy and automation level over traditional methods. However, when using supervised CD methods, a large amount of labeled data is needed to train deep convolutional networks with millions of parameters. These labeled data are difficult to acquire for CD tasks. To address this limitation, a novel semisupervised convolutional network for CD (SemiCDNet) is proposed based on a generative adversarial network (GAN). First, both the labeled data and unlabeled data are input into the segmentation network to produce initial predictions and entropy maps. Then, to exploit the potential of unlabeled data, two discriminators are adopted to enforce the feature distribution consistency of segmentation maps and entropy maps between the labeled and unlabeled data. During the competitive training, the generator is continuously regularized by utilizing the unlabeled information, thus improving its generalization capability. The effectiveness and reliability of our proposed method are verified on two high-resolution remote sensing data sets. Extensive experimental results demonstrate the superiority of the proposed method against other state-of-the-art approaches.  © 1980-2012 IEEE.","Convolution; Deep learning; Entropy; Labeled data; Learning systems; Remote sensing; Adversarial networks; Convolutional networks; Feature distribution; Generalization capability; High resolution remote sensing; High resolution remote sensing images; Labeled and unlabeled data; State-of-the-art approach; artificial neural network; detection method; remote sensing; satellite imagery; spatial resolution; supervised classification; Convolutional neural networks","Change detection (CD); deep learning (DL); feature distribution; generative adversarial network (GAN); remote sensing (RS); semisupervised convolutional network","Article","Final","","Scopus","2-s2.0-85099581902"
"Cao C.; Cui Z.; Wang L.; Wang J.; Cao Z.; Yang J.","Cao, Changjie (57208467261); Cui, Zongyong (35118740300); Wang, Liying (57216439795); Wang, Jielei (57222712127); Cao, Zongjie (55271466500); Yang, Jianyu (9239230100)","57208467261; 35118740300; 57216439795; 57222712127; 55271466500; 9239230100","A Demand-Driven SAR Target Sample Generation Method for Imbalanced Data Learning","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3134674","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121351116&doi=10.1109%2fTGRS.2021.3134674&partnerID=40&md5=cb9d8381ef37028e8ff8db8e887016f6","Since there are differences in the natural frequency of various synthetic aperture radar (SAR) target samples in reality, the problem of imbalanced data on the automatic target recognition (ATR) model has gradually appeared in recent years. The problem makes the classification boundary learned by the ATR model often fuzzy or even wrong. In this article, an SAR target sample generation method was proposed, called demand-driven generative adversarial nets (DDGANs), which provided an effective way to implement imbalanced data learning. When the imbalanced data exacerbated the deterioration of the minority category target samples distribution, the proposed method generated samples to alleviate this negative impact. The proposed method innovatively used two convolutional neural networks to form the discriminator of DDGAN. Among them, a convolutional neural network was used to determine whether the generated sample is real or fake. Moreover, another convolutional neural network can simultaneously dig out the generation demands of different categories of target samples when recognizing the generated samples. The generation demands enabled DDGAN to allocate different generation capabilities to different target samples on demand, thereby alleviating the negative impact of data imbalance. At the same time, DDGAN can autonomously learn the generation demands from imbalanced training sets. Several experimental results based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset showed the advantages of DDGAN. Compared with existing imbalanced learning algorithms, the proposed method had obvious superiority in recognition performance and data generation efficiency.  © 2022 IEEE.","Automatic target recognition; Convolution; Learning algorithms; Neural networks; Radar target recognition; Synthetic aperture radar; Automatic target recognition; Classification boundary; Convolutional neural network; Demand-driven; Generation method; Imbalanced data; Radar target; Recognition models; Sample generations; Synthetic aperture radar; machine learning; methodology; remote sensing; synthetic aperture radar; Generative adversarial networks","Automatic target recognition (ATR); demand-driven; generative adversarial nets (GANs); imbalanced data; synthetic aperture radar (SAR)","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85121351116"
"Chen H.; Chen R.; Li N.","Chen, Hui (57222491699); Chen, Rong (56927156900); Li, Nannan (56086432000)","57222491699; 56927156900; 56086432000","Attentive generative adversarial network for removing thin cloud from a single remote sensing image","2021","IET Image Processing","15","4","","856","867","11","10.1049/ipr2.12067","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102887750&doi=10.1049%2fipr2.12067&partnerID=40&md5=4fe7da3013e5c6bdc7a40394a73a737a","Land-surface observation is easily affected by the light transmission and scattering of semi-transparent clouds, high or low, resulting in blurring and reduced contrast of ground objects. To improve the visual appearance of remote sensing images, the authors present a deep learning method for thin cloud removal using a new attentive generative adversarial network without prior knowledge or assumptions, which copes with thin clouds that are unevenly distributed on different images and learns the attention map with weighted information about spatial features. Such a spatial attention model can endow each pixel with the global spatial context information. Consequently, the generative network focuses on the thin cloud regions to generate better local image restoration, and the discriminative network can evaluate the local consistency of the repaired regions. The experimental results show that this method is superior to state-of-the-art methods in recovering detailed texture information. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Deep learning; Image enhancement; Image reconstruction; Learning systems; Light transmission; Textures; Adversarial networks; Discriminative networks; Local consistency; Remote sensing images; Semi-transparent; Spatial attention; State-of-the-art methods; Visual appearance; Remote sensing","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102887750"
"Ye Y.; Shen B.; Shen Y.","Ye, Yang (57208443644); Shen, Bingyan (57240520800); Shen, Yuqi (57209421796)","57208443644; 57240520800; 57209421796","Research on anti-shadow tree detection method based on generative adversarial network; [基于生成对抗网络的抗阴影树木检测方法]","2021","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","37","10","","118","126","8","10.11975/j.issn.1002-6819.2021.10.014","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113992369&doi=10.11975%2fj.issn.1002-6819.2021.10.014&partnerID=40&md5=fa2cb90fafa2003401a8c076658c1243","High-scoring remote sensing imaging has widely been applied in most management of agriculture and forestry, especially to monitor and evaluate crops and forest resources on a large scale. Nevertheless, there is a great challenge to the accuracy of single tree identification and detection during the image acquisition, due mainly to the fact that the shadow area is inevitably formed by the light. The shadow areas in the remote sensing images can be assumed as a kind of noise in the image sampling. As such, the degradation of high-resolution parameters can cause image distortion after post-processing. In this study, an anti-shadow tree detection method was proposed to detect the single tree with shadow interference using a generative adversarial network (GA-Faster RCNN). This framework consisted of a Faster RCNN network and a tree generator. The Faster RCNN network was mainly used for the tasks of feature extraction and detection. The tree generator was utilized to process the shadows in tree detection. The adversarial generation strategy was adopted by the tree generator to learn generating the minimum feature information characterizing trees. The generator was first trained separately and then put into the Faster RCNN network to finally lock its parameters. Two parts were then trained end-to-end to further improve the tree recognition ability of the network. The GA-Faster RCNN was also compared with 3 state-of-the-art methods, including region-growing, progressive cascaded convolutional neural network, and Faster RCNN on three test areas with shadows. Test area 1 presented a lot of shadows of trees, where the canopy density of trees was very high. Test area 2 showed fewer tree shadows and lower canopy closure, compared with test area 1. The shades of trees and the canopy density of trees in test area 3 were between those in test area 1 and 2. Results demonstrated that the GA-Faster RCNN achieved the highest harmonic average of precision and recall (F1) on the test area 1, 2, and 3, which were 78.4%, 91.6%, and 81.7%, respectively. The average F1 of three test areas was 84.7% for the GA-Faster RCNN, 6.2 percentage point higher than that of Faster RCNN. The user accuracy (UA) and producer accuracy (PA) of GA-Faster RCNN were also the highest among four methods, where UA was 79.8%, 95.0%, 85.3%, and PA was 77.0%, 88.5%, and 78.4% on test area 1, 2, 3, respectively. Moreover, a significance analysis, McNemar, was performed to eliminate the interference of experimental errors and other factors. It was found that there was a statistically significant difference between the three comparison methods and GA-Faster RCNN. The shadow misrecognition rate SR (proportion of the count of shadows misrecognized as trees to the count of total recognized trees) of GA-Faster RCNN was compared with that of Faster RCNN on test area 1, in order to clarify the effect of the mask on tree identification. Although the SR of GA-Faster RCNN was 13.8%, higher than that of Faster RCNN (8.6%), the UA and the number of missed tree identification were both better than those of Faster RCNN. Therefore, the GA-Faster RCNN behaved significant advantages over the other identification. In addition, the GA-Faster RCNN can still maintain the detection stability, when using different feature extraction networks, including ResNet101, ResNet50, and DenseNet. Consequently, the adversarial generative training strategy is highly suitable for learning the minimum feature information characterizing trees, while effectively reducing the interference of shadows, indicating the promising practical value for higher accuracy of tree detection. © 2021, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Agricultural robots; Convolutional neural networks; Extraction; Feature extraction; Image processing; Image sampling; Remote sensing; Testing; Adversarial networks; Precision and recall; Recognition abilities; Remote sensing images; Remote sensing imaging; Significance analysis; State-of-the-art methods; Statistically significant difference; Forestry","Algorithm; Deep learning; Detection; Image; Remote sensing; Shadow interference; Tree","Article","Final","","Scopus","2-s2.0-85113992369"
"Khan S.; Tufail M.; Khan M.T.; Khan Z.A.; Iqbal J.; Alam M.","Khan, Shahbaz (55473857800); Tufail, Muhammad (57198172314); Khan, Muhammad Tahir (55845698500); Khan, Zubair Ahmad (57205755665); Iqbal, Javaid (57212408542); Alam, Mansoor (57208291019)","55473857800; 57198172314; 55845698500; 57205755665; 57212408542; 57208291019","A novel semi-supervised framework for UAV based crop/weed classification","2021","PLoS ONE","16","5 May 2021","e0251008","","","","10.1371/journal.pone.0251008","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105660294&doi=10.1371%2fjournal.pone.0251008&partnerID=40&md5=6c19c6b4a4d3d63db162f9126ad3f34d","Excessive use of agrochemicals for weed controlling infestation has serious agronomic and environmental repercussions associated. An appropriate amount of pesticide/ chemicals is essential for achieving the desired smart farming and precision agriculture (PA). In this regard, targeted weed control will be a critical component significantly helping in achieving the goal. A prerequisite for such control is a robust classification system that could accurately identify weed crops in a field. In this regard, Unmanned Aerial Vehicles (UAVs) can acquire high-resolution images providing detailed information for the distribution of weeds and offers a cost-efficient solution. Most of the established classification systems deploying UAV imagery are supervised, relying on image labels. However, this is a time-consuming and tedious task. In this study, the development of an optimized semi-supervised learning approach is proposed, offering a semi-supervised generative adversarial network for crops and weeds classification at early growth stage. The proposed algorithm consists of a generator that provides extra training data for the discriminator, which distinguishes weeds and crops using a small number of image labels. The proposed system was evaluated extensively on the Red Green Blue (RGB) images obtained by a quadcopter in two different croplands (pea and strawberry). The method achieved an average accuracy of 90% when 80% of training data was unlabeled. The proposed system was compared with several standards supervised learning classifiers and the results demonstrated that this technique could be applied for challenging tasks of crops and weeds classification, mainly when the labeled samples are small at less training time. © 2021 Khan et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Agriculture; Aircraft; Algorithms; Crops, Agricultural; Deep Learning; Plant Weeds; Remote Sensing Technology; Satellite Imagery; Weed Control; algorithm; article; classifier; controlled study; crop; cropland; growth curve; human; human experiment; imagery; learning; nonhuman; strawberry; unmanned aerial vehicle; weed; agriculture; aircraft; algorithm; classification; crop; procedures; remote sensing; satellite imagery; weed control","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105660294"
"Fang B.; Chen G.; Pan L.; Kou R.; Wang L.","Fang, Bo (57209326962); Chen, Gang (57115575600); Pan, Li (54393873900); Kou, Rong (57202911013); Wang, Lizhe (23029267900)","57209326962; 57115575600; 54393873900; 57202911013; 23029267900","GAN-Based Siamese Framework for Landslide Inventory Mapping Using Bi-Temporal Optical Remote Sensing Images","2021","IEEE Geoscience and Remote Sensing Letters","18","3","9042318","391","395","4","10.1109/LGRS.2020.2979693","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100518613&doi=10.1109%2fLGRS.2020.2979693&partnerID=40&md5=bcff76145e7fdb388c503c299c04bee9","Regarding landslide inventory mapping (LIM) as a task similar to change detection, current methods for LIM using bi-temporal optical remote sensing images are generally derived from change detection methods. In practice, not all changed regions belong to landslides, e.g., new roads, canals, and vegetation. Therefore, an ideal strategy is supposed to present two steps: discriminating changed and unchanged regions, and detecting landslides apart from other changed regions. Owing to the complexity and uncertainty of landslides, it is difficult to simultaneously separate landslides with unchanged and other changed regions by a single model. Addressing this problem, in this letter, we apply a generative adversarial network (GAN) in a Siamese neural network, and then propose a GAN-based Siamese framework (GSF) for LIM. The GSF comprises two cascaded modules, namely, domain adaptation and landslide detection. The former module aims to make a cross-domain mapping between prelandslide and postlandslide images with adversarial learning, then translate paired images into the same domain to suppress the domain discrepancies of bi-temporal remote sensing images. Meanwhile, the latter module aims to perform pixel-level landslide detection with a Siamese model. By training this cascaded framework, our method learns to produce landslide inventory maps without any preprocessing or postprocessing. Extensive experiments and comparison with other state-of-the-art methods verify the efficiency and superiority of our method.  © 2004-2012 IEEE.","Landslides; Mapping; Adversarial learning; Adversarial networks; Domain adaptation; Landslide detection; Landslide inventories; Optical remote sensing; Remote sensing images; State-of-the-art methods; geological mapping; landslide; optical method; remote sensing; temporal analysis; Remote sensing","Change detection; domain adaptation; generative adversarial network (GAN); landslide inventory mapping (LIM); Siamese network","Article","Final","","Scopus","2-s2.0-85100518613"
"You S.","You, Shuangyu (57547499500)","57547499500","PCB Defect Detection based on Generative Adversarial Network","2022","2022 2nd International Conference on Consumer Electronics and Computer Engineering, ICCECE 2022","","","","557","560","3","10.1109/ICCECE54139.2022.9712737","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126957873&doi=10.1109%2fICCECE54139.2022.9712737&partnerID=40&md5=34c8b8364f44fc29ad2025d6ab643cb3","This paper proposes a PCB defect detection scheme based on the generative confrontation network, which can be applied to the automatic detection system of PCB vision inspection (vision inspection). We use the edge-enhanced super-resolution GAN (EESRGAN) applied in the field of remote sensing to enhance the PCB images and complete the super-resolution detection of the reconstructed picture. And use the PCB pictures of different preprocessing models in an end-to-end manner to compare the recognition of PCB defects after training. Experiments on the PCB data set show that the PCB pictures after sliding cutting are input into the result of EESRGAN training, which can relatively accurately identify the 6 types of defects contained in the data set. Our results show the effectiveness of our data processing methods.  © 2022 IEEE.","Data handling; Defects; Generative adversarial networks; Image enhancement; Organic pollutants; Polychlorinated biphenyls; Remote sensing; Automatic detection systems; Data set; Detection scheme; Edge-enhanced super-resolution GAN; End to end; PCB defects detections; Remote-sensing; Resolution detection; Superresolution; Vision inspection; Optical resolving power","EESRGAN; Generative Adversarial Network; PCB defect detection","Conference paper","Final","","Scopus","2-s2.0-85126957873"
"Sun Y.; Feng S.; Ye Y.; Li X.; Kang J.; Huang Z.; Luo C.","Sun, Yuxi (57337936500); Feng, Shanshan (56393590300); Ye, Yunming (25958578800); Li, Xutao (36816200800); Kang, Jian (57192706813); Huang, Zhichao (57194046545); Luo, Chuyao (56746989200)","57337936500; 56393590300; 25958578800; 36816200800; 57192706813; 57194046545; 56746989200","Multisensor Fusion and Explicit Semantic Preserving-Based Deep Hashing for Cross-Modal Remote Sensing Image Retrieval","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3136641","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122067539&doi=10.1109%2fTGRS.2021.3136641&partnerID=40&md5=aea4d0fbe1fa79d49f5826216aee74b3","Cross-modal hashing is an important tool for retrieving useful information from very-high-resolution (VHR) optical images and synthetic aperture radar (SAR) images. Dealing with the intermodal discrepancies, including both spatial-spectral and visual semantic aspects, between VHR and SAR images is extremely vital to generate high-quality common hash codes in the Hamming space. However, existing cross-modal hashing methods ignore the spatial-spectral discrepancy when representing VHR and SAR images. Moreover, existing methods employ derived supervised signals, such as pairwise training images, to implicitly guide hashing learning, which fails to effectively deal with the visual semantic discrepancy, i.e., cannot adequately preserve the intraclass similarity and interclass discrimination between VHR and SAR images. To address these drawbacks, this article proposes a multisensor fusion and explicit semantic preserving-based deep Hashing method, termed as MsEspH, which can effectively deal with the discrepancies. Specifically, we design a novel cross-modal hashing network to eliminate the spatial-spectral discrepancies by fusing extra multispectral images (MSIs), which are generated in real time by a generative adversarial network. Then, we propose an explicit semantic preserving-based objective function by analyzing the connection between classification and hash learning. The objective function can preserve the intraclass similarity and interclass discrimination with class labels directly. Moreover, we theoretically verify that hash learning and classification can be unified into a learning framework under certain conditions. To evaluate our method, we construct and release a large-scale VHR-SAR image dataset. Extensive experiments on the dataset demonstrate that our method outperforms various state-of-The-Art cross-modal hashing methods.  © 1980-2012 IEEE.","Data visualization; Geometrical optics; Hash functions; Image retrieval; Radar imaging; Remote sensing; Semantics; Sensor data fusion; Space optics; Space-based radar; Trajectories; Code; Content-based; Content-based remote sensing image retrieval; Cross-modal; Cross-modal dataset; Cross-source image retrieval; Deep cross-modal hashing; Optical and synthetic aperture radar data; Optical imaging; Optical-; Radar data; Radar polarimetry; Remote sensing image retrieval; Source images; data set; radar imagery; remote sensing; satellite data; semantic standardization; synthetic aperture radar; Synthetic aperture radar","Content-based remote sensing (RS) image retrieval; cross-modal dataset; cross-source image retrieval; deep cross-modal hashing; optical and synthetic aperture radar (SAR) data","Article","Final","","Scopus","2-s2.0-85122067539"
"Ai J.; Fan G.; Mao Y.; Jin J.; Xing M.; Yan H.","Ai, Jiaqiu (35319586300); Fan, Gaowei (57263273200); Mao, Yuxiang (57221601419); Jin, Jing (57212715723); Xing, Mengdao (7005922869); Yan, He (55550832600)","35319586300; 57263273200; 57221601419; 57212715723; 7005922869; 55550832600","An Improved SRGAN Based Ambiguity Suppression Algorithm for SAR Ship Target Contrast Enhancement","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3111553","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115129298&doi=10.1109%2fLGRS.2021.3111553&partnerID=40&md5=3a36826aa9ab1932d1845423a1b4cc1a","Due to the specific characteristics of synthetic aperture radar (SAR), there will be ambiguity interference in SAR images, resulting in low contrast of the ship target to the clutter. This letter proposes an improved super-resolution generative adversarial network (ISRGAN) based ambiguity suppression algorithm for SAR ship target contrast enhancement. The proposed ISRGAN is the first attempt of using GAN for SAR ambiguity suppression. As a post-processing procedure, it does not need prior information of SAR systems, so it can be applied to various observation scenes and different acquisition modes. The generator of ISRGAN embeds the residual dense network (RDN) to optimally fuse the global and local features of the image, and it effectively improves the completeness of the feature information used for SAR ship target contrast enhancement. The superiority of ISRGAN on ambiguity suppression is validated on the Chinese Gaofen-3 imagery.  © 2004-2012 IEEE.","Image enhancement; Radar imaging; Ships; Acquisition modes; Adversarial networks; Contrast Enhancement; Feature information; Post-processing procedure; Prior information; Super resolution; Suppression algorithm; accuracy assessment; algorithm; remote sensing; synthetic aperture radar; Synthetic aperture radar","Azimuth ambiguity suppression; improved super-resolution generative adversarial network (ISRGAN); synthetic aperture radar (SAR); target contrast enhancement","Article","Final","","Scopus","2-s2.0-85115129298"
"Jin X.; Tang P.; Zhang Z.","Jin, Xing (57212488415); Tang, Ping (35436105100); Zhang, Zheng (56255818100)","57212488415; 35436105100; 56255818100","Sequence image datasets construction via deep convolution networks","2021","Remote Sensing","13","9","1853","","","","10.3390/rs13091853","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106514678&doi=10.3390%2frs13091853&partnerID=40&md5=1ad41f1ec292afb1b34a541b2785c979","Remote-sensing time-series datasets are significant for global change research and a better understanding of the Earth. However, remote-sensing acquisitions often provide sparse time series due to sensor resolution limitations and environmental factors such as cloud noise for optical data. Image transformation is the method that is often used to deal with this issue. This paper considers the deep convolution networks to learn the complex mapping between sequence images, called adaptive filter generation network (AdaFG), convolution long short-term memory network (CLSTM), and cycle-consistent generative adversarial network (CyGAN) for construction of sequence image datasets. AdaFG network uses a separable 1D convolution kernel instead of 2D kernels to capture the spatial characteristics of input sequence images and then is trained end-to-end using sequence images. CLSTM network can map between different images using the state information of multiple time-series images. CyGAN network can map an image from a source domain to a target domain without additional information. Our experiments, which were performed with unmanned aerial vehicle (UAV) and Landsat-8 datasets, show that the deep convolution networks are effective to produce high-quality time-series image datasets, and the data-driven deep convolution networks can better simulate complex and diverse nonlinear data information. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Adaptive filtering; Adaptive filters; Antennas; Convolution; Remote sensing; Time series; Unmanned aerial vehicles (UAV); Adversarial networks; Convolution kernel; Environmental factors; Image transformations; Multiple time series; Short term memory; Spatial characteristics; State information; Complex networks","Adaptive filter generation network; Convolution long short-term memory network; Cycle-consistent generative adversarial network; Landsat-8 dataset; Sequence image datasets; UAV dataset","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106514678"
"Darbaghshahi F.N.; Mohammadi M.R.; Soryani M.","Darbaghshahi, Faramarz Naderi (57221918330); Mohammadi, Mohammad Reza (57196843044); Soryani, Mohsen (23391249100)","57221918330; 57196843044; 23391249100","Cloud Removal in Remote Sensing Images Using Generative Adversarial Networks and SAR-to-Optical Image Translation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3131035","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120548155&doi=10.1109%2fTGRS.2021.3131035&partnerID=40&md5=5080e8e67563e29fbd732e38863cecec","Satellite images are often contaminated by clouds. Cloud removal has received special attention due to the wide range of satellite image applications. As the clouds thicken, the process of removing them becomes more challenging. In such cases, using auxiliary images, such as near-infrared or synthetic aperture radar (SAR), for reconstructing is common. In this study, we attempt to solve the problem using two generative adversarial networks (GANs): the first translates SAR images to optical images and the second removes clouds using the translated images of prior GAN. Also, we propose dilated residual inception blocks (DRIBs) instead of vanilla U-net in the generator networks and use structural similarity index measure (SSIM) in addition to the L1 loss function. Reducing the number of downsamplings and expanding receptive fields by dilated convolutions increased the quality of output images. We used the SEN1-2 dataset to train and test both GANs, and we made cloudy images by adding synthetic clouds to optical images. In addition, we used the SEN12MS-CR dataset to test network performance to remove real clouds. The restored images are evaluated using PSNR, SSIM, SAM, MAE, RMSE, and $Q$. We compared the proposed method with state-of-the-art deep learning models and achieved more accurate results in both SAR-to-optical image translation and cloud removal parts.  © 1980-2012 IEEE.","Deep learning; Generative adversarial networks; Geometrical optics; Image reconstruction; Infrared devices; Radar imaging; Remote sensing; Space-based radar; Statistical tests; Cloud removal; Deep learning; Generative adversarial network; Generator; Optical image; Optical imagery; Optical imaging; Optical-; Radar polarimetry; Synthetic aperture radar-to-optical translation; image analysis; image classification; network analysis; satellite imagery; synthetic aperture radar; Synthetic aperture radar","Cloud removal; deep learning; generative adversarial network (GAN); optical imagery; SAR-to-optical translation; synthetic aperture radar (SAR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85120548155"
"Chen Z.; Tong L.; Qian B.; Yu J.; Xiao C.","Chen, Zhitao (57231816700); Tong, Lei (56022591200); Qian, Bin (56425195100); Yu, Jing (56156076900); Xiao, Chuangbai (8837867600)","57231816700; 56022591200; 56425195100; 56156076900; 8837867600","Self-attention-based conditional variational auto-encoder generative adversarial networks for hyperspectral classification","2021","Remote Sensing","13","16","3316","","","","10.3390/rs13163316","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113566847&doi=10.3390%2frs13163316&partnerID=40&md5=610ae21d448448a8288998a1d0a408e5","Hyperspectral classification is an important technique for remote sensing image analysis. For the current classification methods, limited training data affect the classification results. Recently, Conditional Variational Autoencoder Generative Adversarial Network (CVAEGAN) has been used to generate virtual samples to augment the training data, which could improve the classification performance. To further improve the classification performance, based on the CVAEGAN, we pro-pose a Self-Attention-Based Conditional Variational Autoencoder Generative Adversarial Network (SACVAEGAN). Compared with CVAEGAN, we first use random latent vectors to obtain more enhanced virtual samples, which can improve the generalization performance. Then, we introduce the self-attention mechanism into our model to force the training process to pay more attention to global information, which can achieve better classification accuracy. Moreover, we explore model stability by incorporating the WGAN-GP loss function into our model to reduce the mode collapse probability. Experiments on three data sets and a comparison of the state-of-art methods show that SACVAEGAN has great advantages in accuracy compared with state-of-the-art HSI classification methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Learning systems; Network coding; Remote sensing; Classification accuracy; Classification methods; Classification performance; Classification results; Collapse probabilities; Generalization performance; Hyper-spectral classification; Remote sensing images; Classification (of information)","Generative Adversarial Network (GAN); Hyperspectral classification; Self-attention","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113566847"
"Wang W.-Y.; Li H.-C.; Deng Y.-J.; Shao L.-Y.; Lu X.-Q.; Du Q.","Wang, Wei-Ye (57208223487); Li, Heng-Chao (24174798500); Deng, Yang-Jun (57194391169); Shao, Li-Yang (9244361300); Lu, Xiao-Qiang (35180125200); Du, Qian (7202060063)","57208223487; 24174798500; 57194391169; 9244361300; 35180125200; 7202060063","Generative Adversarial Capsule Network with ConvLSTM for Hyperspectral Image Classification","2021","IEEE Geoscience and Remote Sensing Letters","18","3","9032346","523","527","4","10.1109/LGRS.2020.2976482","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092389255&doi=10.1109%2fLGRS.2020.2976482&partnerID=40&md5=131e30bd13dd51359c23d3df4d105c72","Recently, deep learning has been widely applied in hyperspectral image (HSI) classification since it can extract high-level spatial-spectral features. However, deep learning methods are restricted due to the lack of sufficient annotated samples. To address this problem, this letter proposes a novel generative adversarial network (GAN) for HSI classification that can generate artificial samples for data augmentation to improve the HSI classification performance with few training samples. In the proposed network, a new discriminator is designed by exploiting capsule network (CapsNet) and convolutional long short-term memory (ConvLSTM), which extracts the low-level features and combines them together with local space sequence information to form the high-level contextual features. In addition, a structured sparse L_{2,1} constraint is imposed on sample generation to control the modes of data being generated and achieve more stable training. The experimental results on two real HSI data sets show that the proposed method can obtain better classification performance than the several state-of-the-art deep classification methods.  © 2004-2012 IEEE.","Classification (of information); Deep learning; Learning systems; Spectroscopy; Adversarial networks; Artificial samples; Classification performance; Contextual feature; Deep classifications; Low-level features; Sample generations; Sequence informations; artificial neural network; image classification; machine learning; network analysis; remote sensing; spectral analysis; Image classification","Capsule network (CapsNet); convolutional neural network (CNN); data augmentation; deep learning; generative adversarial network (GAN); hyperspectral image (HSI) classification","Article","Final","","Scopus","2-s2.0-85092389255"
"Patil A.; Venkatesh","Patil, Abhijit (57638071500); Venkatesh (35333039200)","57638071500; 35333039200","DCGAN: Deep Convolutional GAN with Attention Module for Remote View Classification","2021","2021 International Conference on Forensics, Analytics, Big Data, Security, FABS 2021","","","","","","","10.1109/FABS52071.2021.9702655","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127163430&doi=10.1109%2fFABS52071.2021.9702655&partnerID=40&md5=d7e581a4d993a5522ca41237d5f124c4","In recent times, the development of Deep Learning Techniques for Image Classification has increased. The deep learning module uses an unsupervised learning technique. The supervised learning requires an adequate and outsized dataset with labels to train a machine. This paper proposes a unique Unsupervised Deep Feature Learning Method called Deep Convolutional GAN (DCGAN) with Attention Module for Remote Scene Classification. The Attention module is integrated with DCGAN to optimize the power of feature extraction. To extract the contextual information, a feature fusion architecture is proposed and it is integrated with Discriminator. The proposed module optimizes the discriminator and generator losses. The extracted features are given as input to SVM for the classification. The proposed module DCGAN with Attention module is implemented with publicly available remote sensing scene UC-Merced dataset which have 21 different scene classes and the RSSCN7 dataset which have 7 different scene classes. The experimental results obtained by the proposed model DCGAN with Attention module are better than the state-of-art machines or methods results. The proposed model achieves an accuracy of 91.67% and 84.29 % for the RSSCN7 and UC-Merced datasets respectively. © 2021 IEEE.","Convolution; Convolutional neural networks; Deep learning; Learning algorithms; Remote sensing; Support vector machines; Attention module; Cross entropy; Deep feature learning; Features fusions; Images classification; Learning modules; Learning techniques; Loss functions; Remote views; SVM; Generative adversarial networks","Attention Module; Cross-Entropy; feature fusion; Generative Adversarial networks; Loss Function; SVM","Conference paper","Final","","Scopus","2-s2.0-85127163430"
"Jinzhen M.; Shuo Z.; Yu Z.; Yami F.; Yan Z.; Shuqing C.; Zongming L.","Jinzhen, Mu (57363206900); Shuo, Zhang (57363349800); Yu, Zhang (57223864009); Yami, Fang (57363057600); Yan, Zhou (57362767200); Shuqing, Cao (57363207000); Zongming, Liu (57362914400)","57363206900; 57363349800; 57223864009; 57363057600; 57362767200; 57363207000; 57362914400","Image Super-Resolution Using Quality Aware Generative Adversarial Networks","2022","Lecture Notes in Electrical Engineering","644 LNEE","","","1883","1893","10","10.1007/978-981-15-8155-7_158","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120644905&doi=10.1007%2f978-981-15-8155-7_158&partnerID=40&md5=8f29ad45aa44ec3901ff3a8bb7f73573","It has been demonstrated that GAN-based algorithms can generate realistic images in single image super-resolution. However, these methods usually generate undesired artifacts in the accompanied images. We proposes a new GAN-based super resolution method to further improve the performance of super-resolved results. In this fashion, we introduce dense compression unit as our basic unit. Then, we use an additional noise into the generator to enhance the quality of generator network. To enhance the supervision of texture recovery, we use a novel quality aware function that is inspired by the SSIM index as excellent regularizer for GAN objective functions. Finally, we demonstrate our method in extensive experiments that the generated images has more realistic textures and it has a great potential in remote sensing tiny-object detection. © 2022, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Generative adversarial networks; Object detection; Remote sensing; Textures; Basic units; Content loss; Image super resolutions; Performance; Quality aware; Realistic images; Single images; SSIM indices; Superresolution; Superresolution methods; Optical resolving power","Content loss; Generative adversarial networks; Quality aware; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85120644905"
"Pan Y.; Pi D.; Chen J.; Meng H.","Pan, Yue (57204029531); Pi, Dechang (14038259000); Chen, Junfu (57214761375); Meng, Han (57204031360)","57204029531; 14038259000; 57214761375; 57204031360","FDPPGAN: remote sensing image fusion based on deep perceptual patchGAN","2021","Neural Computing and Applications","33","15","","9589","9605","16","10.1007/s00521-021-05724-1","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099603089&doi=10.1007%2fs00521-021-05724-1&partnerID=40&md5=5a05b6e4aedf435e2681fcbe252cb412","Remote sensing satellites can simultaneously capture high spatial resolution panchromatic (PAN) images and low spatial resolution multispectral (MS) images. Pan-sharpening in the fusion of remote sensing images aims to generate high-resolution MS images by integrating the spatial information of PAN images and the spectral characteristics of MS images. In this study, a novel deep perceptual patch generative adversarial network (FDPPGAN) was proposed to solve the pan-sharpening problem. First, a perception generator was constructed, it included, a matching module, which can process as input images of different resolutions, a fusion module, a reconstruction module based on the residual structure, and a module for the extracting perceptual features. Second, patch discriminator was utilized to convert the dichotomy of the sample into that multiple partial images of the same size to ensure that the generated results can retain more detailed features. Finally, the loss function of FDPPGAN comprised perceptual feature loss, content loss, generator loss, and discriminator loss. Experiments on the QuickBird and WorldView datasets demonstrated that the proposed algorithm is superior to state-of-the-art algorithms in subjective and objective indexes. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.","Image resolution; Remote sensing; Different resolutions; High spatial resolution; Multispectral images; Panchromatic (Pan) image; Remote sensing images; Remote sensing satellites; Spectral characteristics; State-of-the-art algorithms; Image fusion","Image fusion; MS image; PAN image; patchGAN; Perceptual features","Article","Final","","Scopus","2-s2.0-85099603089"
"Zhang X.; Pun M.-O.; Liu M.","Zhang, Xiaokang (56703557200); Pun, Man-On (6603645302); Liu, Ming (57216968027)","56703557200; 6603645302; 57216968027","Semi‐supervised multi‐temporal deep representation fusion network for landslide mapping from aerial orthophotos","2021","Remote Sensing","13","4","548","1","22","21","10.3390/rs13040548","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100661538&doi=10.3390%2frs13040548&partnerID=40&md5=8255545a48d1424b03ae2b6033f93a03","Using remote sensing techniques to monitor landslides and their resultant land cover changes is fundamentally important for risk assessment and hazard prevention. Despite enormous efforts in developing intelligent landslide mapping (LM) approaches, LM remains challenging ow-ing to high spectral heterogeneity of very‐high‐resolution (VHR) images and the daunting labeling efforts. To this end, a deep learning model based on semi‐supervised multi‐temporal deep representation fusion network, namely SMDRF‐Net, is proposed for reliable and efficient LM. In comparison with previous methods, the SMDRF‐Net possesses three distinct properties. 1) Unsupervised deep representation learning at the pixel‐ and object‐level is performed by transfer learning using the Wasserstein generative adversarial network with gradient penalty to learn discriminative deep features and retain precise outlines of landslide objects in the high‐level feature space. 2) At-tention‐based adaptive fusion of multi‐temporal and multi‐level deep representations is developed to exploit the spatio‐temporal dependencies of deep representations and enhance the feature representation capability of the network. 3) The network is optimized using limited samples with pseudo‐labels that are automatically generated based on a comprehensive uncertainty index. Ex-perimental results from the analysis of VHR aerial orthophotos demonstrate the reliability and ro-bustness of the proposed approach for LM in comparison with state‐of‐the‐art methods. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Antennas; Landslides; Photomapping; Reliability analysis; Remote sensing; Risk assessment; Transfer learning; Adversarial networks; Aerial orthophotos; Automatically generated; Feature representation; Hazard prevention; Landslide mapping; Remote sensing techniques; Spectral heterogeneity; Deep learning","Attention; Deep representation learning; Landslide mapping; Multi‐temporal fusion; Semi‐supervised; WGAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100661538"
"Ma Y.; Wei J.; Tang W.; Tang R.","Ma, Yaobin (57312889300); Wei, Jingbo (55711392500); Tang, Wenchao (57189494044); Tang, Rongxin (11440620800)","57312889300; 55711392500; 57189494044; 11440620800","Explicit and stepwise models for spatiotemporal fusion of remote sensing images with deep neural networks","2021","International Journal of Applied Earth Observation and Geoinformation","105","","102611","","","","10.1016/j.jag.2021.102611","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121638570&doi=10.1016%2fj.jag.2021.102611&partnerID=40&md5=9d8c585485030bec2424747a55fa2d83","The spatial, sensor, and temporal differences can be observed in the process of spatiotemporal fusion because source images are from different sensors or moments. The existing spatiotemporal fusion methods have modelled the temporal difference, but they did not solve the spatial difference or the sensor difference to build complete models. In this paper, a step-by-step modelling framework is proposed, and three models are designed based on deep neural networks to model the spatial difference, sensor difference, and temporal difference in a separate and explicit way. The spatial difference is modelled with cascaded dual regression networks. The sensor difference is simulated with a four-layer convolutional neural network. The temporal difference is predicted with a generative adversarial network. The proposed method is compared with six algorithms for the reconstruction of Landsat-7 and Landsat-5 which validates the effectiveness of the spatial fusion strategy. The digital evaluation on radiometric, structural, and spectral loss illustrates that the proposed method can give the optimal performance steadily. The necessity of complete modelling is also tested by connecting the spatial and sensor models of the proposed method with one-pair fusion methods, and the steadily improved performance shows that all the difference models contribute to performance improvement. © 2021 The Author(s)","artificial neural network; computer simulation; Landsat; MODIS; satellite data; satellite imagery; spatiotemporal analysis","Convolutional neural network; Generative adversarial network; Landsat-7; MODIS; Spatiotemporal Fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121638570"
"Paoletti M.E.; Haut J.M.; Ghamisi P.; Yokoya N.; Plaza J.; Plaza A.","Paoletti, M.E. (57027389000); Haut, J.M. (57215636081); Ghamisi, P. (53663404300); Yokoya, N. (36440631200); Plaza, J. (57195716301); Plaza, A. (7006613644)","57027389000; 57215636081; 53663404300; 36440631200; 57195716301; 7006613644","U-IMG2DSM: Unpaired Simulation of Digital Surface Models with Generative Adversarial Networks","2021","IEEE Geoscience and Remote Sensing Letters","18","7","9108295","1288","1292","4","10.1109/LGRS.2020.2997295","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112416737&doi=10.1109%2fLGRS.2020.2997295&partnerID=40&md5=a16f2dc8622fa2a4e9f89c2fc5eccdcb","High-resolution digital surface models (DSMs) provide valuable height information about the Earth's surface, which can be successfully combined with other types of remotely sensed data in a wide range of applications. However, the acquisition of DSMs with high spatial resolution is extremely time-consuming and expensive with their estimation from a single optical image being an ill-possed problem. To overcome these limitations, this letter presents a new unpaired approach to obtain DSMs from optical images using deep learning techniques. Specifically, our new deep neural model is based on variational autoencoders (VAEs) and generative adversarial networks (GANs) to perform image-to-image translation, obtaining DSMs from optical images. Our newly proposed method has been tested in terms of photographic interpretation, reconstruction error, and classification accuracy using three well-known remotely sensed data sets with very high spatial resolution (obtained over Potsdam, Vaihingen, and Stockholm). Our experimental results demonstrate that the proposed approach obtains satisfactory reconstruction rates that allow enhancing the classification results for these images. The source code of our method is available from: https://github.com/mhaut/UIMG2DSM.  © 2004-2012 IEEE.","Aerial photography; Classification (of information); Deep learning; Geometrical optics; Image reconstruction; Image resolution; Learning systems; Remote sensing; Adversarial networks; Classification accuracy; Classification results; Digital surface models; High spatial resolution; Reconstruction error; Remotely sensed data; Very high spatial resolutions; algorithm; data processing; digital elevation model; image analysis; pixel; Image enhancement","Digital surface models (DSMs); Generative adversarial networks (GANs); Image-to-image problems; Optical imaging; Variational autoencoder (VAEs)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85112416737"
"Zhang L.; Lu W.; Huang Y.; Sun X.; Zhang H.","Zhang, Lize (57233449400); Lu, Wen (55484285200); Huang, Yuanfei (57203161002); Sun, Xiaopeng (57201075215); Zhang, Hongyi (35748167800)","57233449400; 55484285200; 57203161002; 57201075215; 35748167800","Unpaired remote sensing image super-resolution with multi-stage aggregation networks","2021","Remote Sensing","13","16","3167","","","","10.3390/rs13163167","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113645201&doi=10.3390%2frs13163167&partnerID=40&md5=9576728e45d6e5c535405f46b866fb2e","Mainstream image super-resolution (SR) methods are generally based on paired training samples. As the high-resolution (HR) remote sensing images are difficult to collect with a limited imaging device, most of the existing remote sensing super-resolution methods try to down-sample the collected original images to generate an auxiliary low-resolution (LR) image and form a paired pseudo HR-LR dataset for training. However, the distribution of the generated LR images is generally inconsistent with the real images due to the limitation of remote sensing imaging devices. In this paper, we propose a perceptually unpaired super-resolution method by constructing a multi-stage aggregation network (MSAN). The optimization of the network depends on consistency losses. In particular, the first phase is to preserve the contents of the super-resolved results, by constraining the content consistency between the down-scaled SR results and the low-quality low-resolution inputs. The second stage minimizes perceptual feature loss between the current result and LR input to constrain perceptual-content consistency. The final phase employs the generative adversarial network (GAN) to adding photo-realistic textures by constraining perceptual-distribution consistency. Numerous experiments on synthetic remote sensing datasets and real remote sensing images show that our method obtains more plausible results than other SR methods quantitatively and qualitatively. The PSNR of our network is 0.06dB higher than the SOTA method—HAN on the UC Merced test set with complex degradation. © 2021 by the authors.","Imaging techniques; Optical resolving power; Textures; Adversarial networks; Aggregation network; Content consistency; Image super resolutions; Low resolution images; Remote sensing images; Remote sensing imaging; Superresolution methods; Remote sensing","Consistency losses; Multi-stage aggregation network; Remote sensing; Unpaired super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85113645201"
"Zhao H.; Zhang M.; Chen F.","Zhao, Hang (57202772821); Zhang, Meimei (56746586200); Chen, Fang (57441279900)","57202772821; 56746586200; 57441279900","GAN-GL: Generative adversarial networks for glacial lake mapping","2021","Remote Sensing","13","22","4728","","","","10.3390/rs13224728","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119915825&doi=10.3390%2frs13224728&partnerID=40&md5=5959e572873e636d9b8c427e50b7d8c4","Remote sensing is a powerful tool that provides flexibility and scalability for monitoring and investigating glacial lakes in High Mountain Asia (HMA). However, existing methods for mapping glacial lakes are designed based on a combination of several spectral features and ancillary data (such as the digital elevation model, DEM) to highlight the lake extent and suppress background information. These methods, however, suffer from either the inevitable requirement of post-processing work or the high costs of additional data acquisition. Signifying a key advancement in the deep learning models, a generative adversarial network (GAN) can capture multi-level features and learn the mapping rules in source and target domains using a minimax game between a generator and discriminator. This provides a new and feasible way to conduct large-scale glacial lake mapping. In this work, a complete glacial lake dataset was first created, containing approximately 4600 patches of Landsat-8 OLI images edited in three ways—random cropping, density cropping, and uniform cropping. Then, a GAN model for glacial lake mapping (GAN-GL) was constructed. The GAN-GL consists of two parts—a generator that incorporates a water attention module and an image segmentation module to produce the glacial lake masks, and a discriminator which employs the ResNet-152 backbone to ascertain whether a given pixel belonged to a glacial lake. The model was evaluated using the created glacial lake dataset, delivering a good performance, with an F1 score of 92.17% and IoU of 86.34%. Moreover, compared to the mapping results derived from the global–local iterative segmentation algorithm and random forest for the entire Eastern Himalayas, our proposed model was superior regarding the segmentation of glacial lakes under complex and diverse environmental conditions, in terms of accuracy (precision = 93.19%) and segmentation effi-ciency. Our model was also very good at detecting small glacial lakes without assistance from ancillary data or human intervention. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Decision trees; Deep learning; Generative adversarial networks; Image segmentation; Iterative methods; Lakes; Photomapping; Remote sensing; Surveying; Ancillary data; Attention mechanisms; Feature data; Glacial lake mapping; Glacial lakes; High mountains; LANDSAT; Landsat-8 OLI; Remote-sensing; Spectral feature; Data acquisition","Attention mechanism; Generative adversarial networks; Glacial lake mapping; Landsat-8 OLI","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119915825"
"Bu Z.; Zhang Y.; Zheng A.","Bu, ZeCong (57259666100); Zhang, Yue (57216359245); Zheng, AnMai (57259642400)","57259666100; 57216359245; 57259642400","Optimizing the classification ability of CNN for SAR fully polarized radar data based on DCGAN","2021","Journal of Physics: Conference Series","2005","1","012002","","","","10.1088/1742-6596/2005/1/012002","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114964456&doi=10.1088%2f1742-6596%2f2005%2f1%2f012002&partnerID=40&md5=8db5c6875595762ab9abd1f858730534","In recent years, the deep learning network is widely used. In the field of remote sensing images, due to the high cost of image acquisition, there are still too few training samples, which greatly limits the application of deep learning in SAR data classification. This paper proposes a method that is generating simulated SAR image by generative adversarial network, and uses the image as the training data of convolutional neural network. Aiming at the impact of the simulated images generated by DCGAN's generator on the classification of convolutional neural networks. The results show that DCGAN can fully extract the main features of the image, and the convolution model based on DCGAN can make CNN have better classification ability and get rid of the dependence on the sample size. CNN can also make full use of simulation data. Whether it is test data set or random dataset, its F1 score can obviously surpass the classification ability without DCGAN's simulated data. In experiments with different sample numbers, the highest F1 score is 93.6479 in the dataset with DCGAN's simulated data. In another experiment, its F1 Score reached 87.32, higher than the dataset without DCGAN's simulated data. © 2021 Institute of Physics Publishing. All rights reserved.","Convolution; Convolutional neural networks; Deep learning; Radar imaging; Remote sensing; Statistical tests; Synthetic aperture radar; Adversarial networks; Classification ability; Convolution model; Learning network; Remote sensing images; SAR data classification; Simulated images; Simulation data; Classification (of information)","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85114964456"
"Huynh N.H.; Böer G.; Schramm H.","Huynh, Nhut Hai (57394573200); Böer, Gordon (55352178400); Schramm, Hauke (7102218265)","57394573200; 55352178400; 7102218265","Self-attention and generative adversarial networks for algae monitoring","2022","European Journal of Remote Sensing","55","1","","10","22","12","10.1080/22797254.2021.2010605","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122137794&doi=10.1080%2f22797254.2021.2010605&partnerID=40&md5=7d1acf76cfef22a933fc1851f5479a15","Water is important for the natural environment and human health. Monitoring algae concentrations yield information on the water quality. Compared with in situ measurements of water quality parameters, which are often complex and expensive, remote sensing techniques, using hyperspectral data analysis, are fast and cost-effective. The objectives of this study are (1) to estimate the algae concentrations from hyperspectral data using deep learning techniques, (2) to investigate the applicability of attention mechanisms in the analysis of hyperspectral data, and (3) to augment the training data using generative adversarial networks (GANs). The results show that the accuracy of deep learning techniques is 7.6% higher than that of simpler artificial neural networks. Compared to noise injection and principal component analysis-based data augmentation, the use of a GAN-based data augmentation method significantly improves the accuracy of algae concentration estimates (>5%). In addition, models with added attention mechanisms yield an on average 3.13% higher accuracy than those without attention techniques. This result demonstrates the improvement of spectral features of artificial hyperspectral data based on the self-attention approach, revealing the potential of attention techniques in hyperspectral remote sensing. © 2021 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Cost effectiveness; Deep learning; Generative adversarial networks; Learning algorithms; Neural networks; Principal component analysis; Quality control; Remote sensing; Water quality; Attention mechanisms; Data augmentation; Deep learning; Hyperspectral Data; Hyperspectral data augmentation; Learning techniques; Natural environments; Pca; Remote-sensing; Self-attention; abundance estimation; accuracy assessment; actor network theory; artificial neural network; concentration (composition); data set; estimation method; network analysis; principal component analysis; remote sensing; water quality; Algae","Deep learning; generative adversarial networks; hyperspectral data augmentation; pca; remote sensing; self-attention","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122137794"
"Shao M.; Wang C.; Wu T.; Meng D.; Luo J.","Shao, Mingwen (8306286100); Wang, Chao (57218503977); Wu, Tianjun (57193855591); Meng, Deyu (23393058400); Luo, Jiancheng (7404183561)","8306286100; 57218503977; 57193855591; 23393058400; 7404183561","Context-Based Multiscale Unified Network for Missing Data Reconstruction in Remote Sensing Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3021116","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122378375&doi=10.1109%2fLGRS.2020.3021116&partnerID=40&md5=75df7c7cd90e8983df38a2b35cd50a23","Missing data reconstruction is a classical yet challenging problem in remote sensing images. Most current methods based on traditional convolutional neural network require supplementary data and can only handle one specific task. To address these limitations, we propose a novel generative adversarial network-based missing data reconstruction method in this letter, which is capable of various reconstruction tasks given only single source data as input. Two auxiliary patch-based discriminators are deployed to impose additional constraints on the local and global regions, respectively. In order to better fit the nature of remote sensing images, we introduce special convolutions and attention mechanism in a two-stage generator, thereby benefiting the tradeoff between accuracy and efficiency. Combining with perceptual and multiscale adversarial losses, the proposed model can produce coherent structure with better details. Qualitative and quantitative experiments demonstrate the uncompromising performance of the proposed model against multisource methods in generating visually plausible reconstruction results. Moreover, further exploration shows a promising way for the proposed model to utilize spatio-spectral-temporal information. The codes and models are available at https://github.com/Oliiveralien/Inpainting-on-RSI. © 2004-2012 IEEE.","Convolution; Generative adversarial networks; Neural networks; Remote sensing; 'current; Context-Aware; Context-based; Convolutional neural network; Data reconstruction; Generative adversarial network; Images reconstruction; Missing data; Multiscale; Remote sensing images; artificial neural network; image analysis; reconstruction; remote sensing; satellite imagery; Image reconstruction","Context aware; generative adversarial network (GAN); image reconstruction; multiscale; remote sensing images","Article","Final","","Scopus","2-s2.0-85122378375"
"Chen B.; Li J.; Jin Y.","Chen, Bin (57210117458); Li, Jing (57207737643); Jin, Yufang (7404457584)","57210117458; 57207737643; 7404457584","Deep learning for feature-level data fusion: Higher resolution reconstruction of historical landsat archive","2021","Remote Sensing","13","2","167","1","23","22","10.3390/rs13020167","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099179435&doi=10.3390%2frs13020167&partnerID=40&md5=16cb0d20fb4cfacbe0f3984caadbbf53","Long-term record of fine spatial resolution remote sensing datasets is critical for monitoring and understanding global environmental change, especially with regard to fine scale processes. However, existing freely available global land surface observations are limited by medium to coarse resolutions (e.g., 30 m Landsat) or short time spans (e.g., five years for 10 m Sentinel-2). Here we developed a feature-level data fusion framework using a generative adversarial network (GAN), a deep learning technique, to leverage the overlapping Landsat and Sentinel-2 observations during 2016–2019, and reconstruct 10 m Sentinel-2 like imagery from 30 m historical Landsat archives. Our tests with both simulated data and actual Landsat/Sentinel-2 imagery showed that the GANbased fusion method could accurately reconstruct synthetic Landsat data at an effective resolution very close to that of the real Sentinel-2 observations. We applied the GAN-based model to two dynamic systems: (1) land over dynamics including phenology change, cropping rotation, and water inundation; and (2) human landscape changes such as airport construction, coastal expansion, and urbanization, via historical reconstruction of 10 m Landsat observations from 1985 to 2018. The resulting comparison further validated the robustness and efficiency of our proposed framework. Our pilot study demonstrated the promise of transforming 30 m historical Landsat data into a 10 m Sentinel-2-like archive with advanced data fusion. This will enhance Landsat and Sentinel-2 data science, facilitate higher resolution land cover and land use monitoring, and global change research. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data fusion; Data Science; Forestry; Image reconstruction; Land use; Metadata; Remote sensing; Adversarial networks; Airport construction; Effective resolutions; Global environmental change; Global land surface; Historical reconstruction; Learning techniques; Spatial resolution; Deep learning","Data fusion; Data reconstruction; GAN; Machine learning; Super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099179435"
"Wang H.; Li Y.; Dong X.","Wang, Hongzhou (57215426692); Li, Yue (55878683800); Dong, Xintong (57202398352)","57215426692; 55878683800; 57202398352","Generative Adversarial Network for Desert Seismic Data Denoising","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9246717","7062","7075","13","10.1109/TGRS.2020.3030692","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104816230&doi=10.1109%2fTGRS.2020.3030692&partnerID=40&md5=565204a162561300d75aaa59cb984476","Seismic exploration is a kind of exploration method for oil and gas resources. However, the disturbance of numerous random noise will decrease the quality and signal-to-noise ratio (SNR) of real seismic records, which brings difficulties to the following works of processing and interpretation. The seismic records of desert region pose a particular problem because of the strong energy noise and the spectrum overlapping between effective signals and random noise. Recent research works demonstrate that a convolutional neural network (CNN) can increase the SNR of seismic records. The optimization of denoising methods based on CNN is principally driven by the loss functions that largely focus on minimizing the mean-squared reconstruction error between denoising records and theoretical pure records. The denoising results estimated by the CNN model are often lacking the perfection of the signal structure. Therefore, when processing seismic records with low SNR, the denoising results often have a lack of effective signal in some traces, which leads to the poor continuity of events. In order to solve this problem, we adopt the strategy of generative adversarial network (GAN) to construct a GAN for denoising. It is divided into two parts: the generator (the denoising network based on CNN) is used to remove noise, while the discriminator is used to guide the generator to restore the structure information of effective signals. The generator and discriminator enhance the performance of each other through adversarial training, and the generator after adversarial training can greatly recover events and suppress random noise in synthetic and real desert seismic data.  © 1980-2012 IEEE.","Convolutional neural networks; Energy resources; Landforms; Petroleum deposits; Petroleum prospecting; Seismic prospecting; Seismic response; Seismic waves; Adversarial networks; Denoising methods; Exploration methods; Reconstruction error; Seismic exploration; Signal structures; Spectrum overlapping; Structure information; network analysis; numerical model; remote sensing; seismic data; Signal to noise ratio","Adversarial training; convolutional neural network (CNN); desert seismic data; generative adversarial network (GAN); low signal-to-noise ratios (SNRs)","Article","Final","","Scopus","2-s2.0-85104816230"
"Tao C.; Fu S.; Qi J.; Li H.","Tao, Chao (35235290700); Fu, Siyang (57486738700); Qi, Ji (57211483444); Li, Haifeng (57189334346)","35235290700; 57486738700; 57211483444; 57189334346","Thick Cloud Removal in Optical Remote Sensing Images Using a Texture Complexity Guided Self-Paced Learning Method","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5619612","","","","10.1109/TGRS.2022.3157917","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126282609&doi=10.1109%2fTGRS.2022.3157917&partnerID=40&md5=4803c292af362abba2ee4382e62af564","Thick clouds seriously impact the quality of optical remote sensing images (RSIs) and limit their application. For removing the cloud, some learning-based methods have been proposed and attracted considerable attention. However, these methods need to train paired multitemporal images with/without cloud, which are difficult and costly to collect. To solve this problem, we propose a novel texture complexity-guided self-paced learning (SPL) framework to remove the thick cloud from single RSIs. The framework does not need paired images and it exploits a texture complexity-guided mechanism to rank the self-generated cloud-corrupted training samples by texture complexity from low to high and then trains the generative adversarial cloud removal network using the SPL technique. In this way, the cloud removal network learns to restore the cloud-corrupted areas from easy to hard and thus to realize the image reconstruction for different difficulty levels. In addition, we introduce a structural similarity (SSIM) loss function to optimize the training network and improve the coherence of the image structure. Simulated and real experiments are performed on the single images acquired by Gao Fen-1 (GF-1) and Sentinel-2 satellites to validate the effectiveness of the proposed method. The results show that the proposed method has a better performance in cloud removal than other state-of-the-art methods, especially for the images of the areas with complex textures. The source codes are available at https://github.com/GeoX-Lab/TPL.  © 1980-2012 IEEE.","Complex networks; Distributed computer systems; Generative adversarial networks; Image enhancement; Image texture; Remote sensing; Cloud removal; Cloud-computing; Complexity theory; Images reconstruction; Optical imaging; Remote sensing images; Remote-sensing; Self-paced learning; Texture complexity; Thick cloud removal; cloud; complexity; image analysis; remote sensing; texture; Image reconstruction","Generative adversarial network (GAN); remote sensing; self-paced learning (SPL); thick cloud removal","Article","Final","","Scopus","2-s2.0-85126282609"
"Shi L.; Wang Z.; Pan B.; Shi Z.","Shi, Lukui (9737579900); Wang, Ziyuan (57325004800); Pan, Bin (56421342100); Shi, Zhenwei (23398841900)","9737579900; 57325004800; 56421342100; 23398841900","An End-to-End Network for Remote Sensing Imagery Semantic Segmentation via Joint Pixel- And Representation-Level Domain Adaptation","2021","IEEE Geoscience and Remote Sensing Letters","18","11","","1896","1900","4","10.1109/LGRS.2020.3010591","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118529864&doi=10.1109%2fLGRS.2020.3010591&partnerID=40&md5=8fb167fd1aaf54ad3195aa8be240bea5","It requires pixel-by-pixel annotations to obtain sufficient training data in supervised remote sensing image segmentation, which is a quite time-consuming process. In recent years, a series of domain-adaptation methods was developed for image semantic segmentation. In general, these methods are trained on the source domain and then validated on the target domain to avoid labeling new data repeatedly. However, most domain-adaptation algorithms only tried to align the source domain and the target domain in the pixel level or the representation level, while ignored their cooperation. In this letter, we propose an unsupervised domain-adaptation method by Joint Pixel and Representation level Network (JPRNet) alignment. The major novelty of the JPRNet is that it achieves joint domain adaptation in an end-to-end manner, so as to avoid the multisource problem in the remote sensing images. JPRNet is composed of two branches, each of which is a generative-adversarial network (GAN). In one branch, pixel-level domain adaptation is implemented by the style transfer with the Cycle GAN, which could transfer the source domain to a target domain. In the other branch, the representation-level domain adaptation is realized by adversarial learning between the transferred source-domain images and the target-domain images. The experimental results on the public data sets have indicated the effectiveness of the JPRNet.  © 2004-2012 IEEE.","Image segmentation; Pixels; Remote sensing; Semantic Web; Semantics; Adaptation methods; Domain adaptation; End-to-end network; Generative-adversarial network; Pixel level; Remote sensing imagery; Remote sensing images; Remote-sensing; Semantic segmentation; Target domain; image analysis; pixel; remote sensing; satellite imagery; segmentation; Generative adversarial networks; Semantic Segmentation","Domain adaptation; generative-adversarial network (GAN); remote sensing; semantic segmentation","Article","Final","","Scopus","2-s2.0-85118529864"
"Park J.-E.; Kim G.; Hong S.","Park, Jeong-Eun (57209229613); Kim, Goo (57213188741); Hong, Sungwook (55817600100)","57209229613; 57213188741; 55817600100","Green Band Generation for Advanced Baseline Imager Sensor Using Pix2Pix with Advanced Baseline Imager and Advanced Himawari Imager Observations","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9247535","6415","6423","8","10.1109/TGRS.2020.3032732","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096866025&doi=10.1109%2fTGRS.2020.3032732&partnerID=40&md5=c23d79d1e5e188dc85c7deb5d1561a9e","Green bands in satellite remote sensing play an important role in monitoring water and vegetation information. Due to the lack of observed green band, the Geostationary Operational Environmental Satellite (GOES-16) Advanced Baseline Imager (ABI) sensor uses a synthetic one. This study presents an ABI green band generation method using the Pix2Pix based on conditional generative adversarial networks (CGANs) and convolutional neural network techniques with data observed in the visible range of the GOES-16/ABI sensor. Our model was constructed from the radiance data sets in the red, blue, and green bands of the Advanced Himawari Imager (AHI) onboard Himawari-8/9 satellites from August 27, 2018 to May 1, 2019, and applied to generate a GOES-16 ABI green band using the ABI blue band radiance data. A comparison between the AHI and the Pix2Pix-generated AHI green bands displayed high accuracy, evaluated through bias = 0.120, root mean square error (RMSE) = 0.983 in digital number (DN) units, and correlation coefficient (CC) = 0.999. Furthermore, comparison between the Pix2Pix-generated and synthetic ABI green bands resulted in a good agreement (bias = 1.029 and RMSE = 2.892 in DN units, CC = 0.993). The statistical comparison between the green band, and red or blue band resulted in the exceptional performance of the Pix2Pix-generated ABI green band compared to the synthetic ABI green band. Consequently, our Pix2Pix-based model can be effectively used to generate nonexistent green band of ABI sensor and be applied in a variety of scientific applications requiring green band. © 1980-2012 IEEE.","Satellites; Convolutional neural networks; Mean square error; Remote sensing; Advanced Baseline Imager; Adversarial networks; Correlation coefficient; Geostationary operational environmental satellites; Root mean square errors; Satellite remote sensing; Scientific applications; Statistical comparisons; artificial neural network; data set; GOES; imagery; observational method; radiance; Geostationary satellites","Artificial intelligence (AI); green band; Himawari; Pix2Pix; radiance; satellite remote sensing","Article","Final","","Scopus","2-s2.0-85096866025"
"Cira C.-I.; Kada M.; Manso-Callejo M.-Á.; Alcarria R.; Sanchez B.B.","Cira, Calimanut-Ionut (57212240781); Kada, Martin (14052416300); Manso-Callejo, Miguel-Ángel (36650985500); Alcarria, Ramón (54984227800); Sanchez, Borja Bordel (57028472800)","57212240781; 14052416300; 36650985500; 54984227800; 57028472800","Improving Road Surface Area Extraction via Semantic Segmentation with Conditional Generative Learning for Deep Inpainting Operations","2022","ISPRS International Journal of Geo-Information","11","1","43","","","","10.3390/ijgi11010043","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123264317&doi=10.3390%2fijgi11010043&partnerID=40&md5=b29da5f00a23a64ab9d4ba3e8d368036","The road surface area extraction task is generally carried out via semantic segmentation over remotely-sensed imagery. However, this supervised learning task is often costly as it requires remote sensing images labelled at the pixel level, and the results are not always satisfactory (presence of discontinuities, overlooked connection points, or isolated road segments). On the other hand, unsupervised learning does not require labelled data and can be employed for post-processing the geometries of geospatial objects extracted via semantic segmentation. In this work, we implement a conditional Generative Adversarial Network to reconstruct road geometries via deep inpainting procedures on a new dataset containing unlabelled road samples from challenging areas present in official cartographic support from Spain. The goal is to improve the initial road representations obtained with semantic segmentation models via generative learning. The performance of the model was evaluated on unseen data by conducting a metrical comparison where a maximum Intersection over Union (IoU) score improvement of 1.3% was observed when compared to the initial semantic segmentation result. Next, we evaluated the appropriateness of applying unsupervised generative learning using a qualitative perceptual validation to identify the strengths and weaknesses of the proposed method in very complex scenarios and gain a better intuition of the model’s behaviour when performing large-scale post-processing with generative learning and deep inpainting procedures and observed important improvements in the generated data. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","Conditional learning; Generative adversarial network; Generative learning; Image inpainting; Image post-processing; Road extraction; Unsupervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123264317"
"Zhao Y.X.; Li Y.; Wu N.","Zhao, Y.X. (57204925238); Li, Y. (55878683800); Wu, N. (56443296900)","57204925238; 55878683800; 56443296900","Data augmentation and its application in distributed acoustic sensing data denoising","2022","Geophysical Journal International","228","1","","119","133","14","10.1093/gji/ggab345","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116553949&doi=10.1093%2fgji%2fggab345&partnerID=40&md5=6b0054ffbb6d5f992869fbf5e2391188","As a data-driven approach, the performance of deep learning models depends largely on the quantity and quality of the training data sets, which greatly limits the application of deep learning to tasks with small data sets. Unfortunately, sometimes we need to use limited small data sets to complete our tasks, such as distributed acoustic sensing (DAS) data denoising. However, using a small data set to train the network may cause overfitting, resulting in poor network generalization. To solve this problem, we propose an approach based on the combination of a generative adversarial network and a deep convolutional neural network. First, we used a small noise data set to train a generative adversarial network to generate synthetic noise samples, and then used these synthetic noise samples to augment the noise data set. Next, we used the augmented noise data set and the signal data set obtained through forward modelling to construct a synthetic training set. Finally, a denoising network based on a convolutional neural network was trained on the constructed synthetic training set. Experimental results show that the augmented data set can effectively improve the denoising performance and generalization ability of the network, and the denoising network trained on the augmented data set can more effectively reduce various kinds of noise in the DAS data. © 2021 The Author(s) 2021. Published by Oxford University Press on behalf of The Royal Astronomical Society.","Asia; Acoustic noise; Convolution; Convolutional neural networks; Deep neural networks; Fuzzy logic; Fuzzy neural networks; Image processing; Acoustic sensing; Asia; Data set; Downhole methods; Fuzzy-Logic; Images processing; Neural network, fuzzy logic; Neural-networks; Seismic noise; Sensing data; data assimilation; image processing; remote sensing; satellite data; satellite imagery; seismic noise; Generative adversarial networks","Asia; Downhole methods; Image processing; Neural networks, fuzzy logic; Seismic noise","Article","Final","","Scopus","2-s2.0-85116553949"
"Meng D.; Wu B.; Wang Z.; Zhu Z.","Meng, Delin (57220200612); Wu, Bangyu (55270666700); Wang, Zhiguo (56152102700); Zhu, Zhaolin (57716940300)","57220200612; 55270666700; 56152102700; 57716940300","Seismic Impedance Inversion Using Conditional Generative Adversarial Network","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3090108","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122598091&doi=10.1109%2fLGRS.2021.3090108&partnerID=40&md5=81928d25c34bb02a5bb3f03a6186bbf0","Deep-learning methods, such as convolutional neural networks (CNNs), have been successfully applied to seismic impedance inversion in recent years. Compared with traditional geophysical inversion, deep-learning inversion can give inversion results with higher resolution. In this letter, we further improve the performance of deep-learning inversion and propose a seismic impedance inversion method based on conditional generative adversarial network (cGAN). In the proposed method, a generator learns to predict seismic impedance from seismic data, and a discriminator learns to distinguish between fake and real impedance. We mix the cGAN objective with mean square error (MSE) loss to bring in more information for model training. Besides, a CNN-based seismic forward model is trained to introduce the constraint of unlabeled data in the training of cGAN. Tests on Marmousi2 model and overthrust model show that the proposed method can obtain more accurate impedance and have better robustness against random noise than CNN method.  © 2004-2012 IEEE.","Convolution; Deep learning; Mean square error; Neural networks; Seismology; Conditional generative adversarial network; Convolutional neural network; Deep learning; Geophysical inversion; Impedance inversion; Learn+; Learning methods; Seismic impedance; Seismic impedance inversion; artificial neural network; inversion layer; remote sensing; seismic data; Generative adversarial networks","Conditional generative adversarial network (cGAN); convolutional neural network (CNN); deep learning; seismic impedance inversion","Article","Final","","Scopus","2-s2.0-85122598091"
"Chen W.; Ouyang S.; Yang J.; Li X.; Zhou G.; Wang L.","Chen, Weitao (55822747800); Ouyang, Shubing (57413242300); Yang, Jiawei (57432868700); Li, Xianju (41261664800); Zhou, Gaodian (57219604979); Wang, Lizhe (23029267900)","55822747800; 57413242300; 57432868700; 41261664800; 57219604979; 23029267900","JAGAN: A Framework for Complex Land Cover Classification Using Gaofen-5 AHSI Images","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","1591","1603","12","10.1109/JSTARS.2022.3144339","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123773453&doi=10.1109%2fJSTARS.2022.3144339&partnerID=40&md5=b97d9f7d154419f0337b28bfa0a72734","Owing to their powerful feature extraction capabilities, deep learning-based methods have achieved significant progress in hyperspectral remote sensing classification. However, several issues still exist in these methods, including a lack of hyperspectral datasets for specific complicated scenarios and the need to improve the classification accuracy of land cover with limited samples. Thus, to highlight and distinguish effective features, we propose a hyperspectral classification framework based on a joint channel-space attention mechanism and generative adversarial network (JAGAN). To relearn feature-based weights, a higher priority was assigned to important features, which was developed by integrating a two-joint channel-space attention model to obtain the most valuable feature via the attention weight map. Additionally, two classifiers were designed in JAGAN: sigmoid was used to determine whether the input data were real or fake samples produced by the generator, while Softmax was adopted as a land cover classifier to yield the prediction type labels of the input samples. To test the classification performance of the JAGAN model, we used a self-constructed complex land cover dataset based on GaoFen-5 AHSI images, which consists of mixed landscapes of mining and agricultural areas from the urban-rural fringe. Compared with other methods, the proposed model achieved the highest overall classification accuracy of 86.09%, the highest kappa amount of 79.41%, the highest F1 score of 85.86%, and the highest average accuracy of 82.30%, indicating the JAGAN can effectively improve the classification accuracy for limited samples in complex regional environments using GF-5 AHSI images. © 2008-2012 IEEE.","Classification (of information); Complex networks; Deep learning; Extraction; Feature extraction; Hyperspectral imaging; Image classification; Image enhancement; Remote sensing; Space optics; Spectroscopy; Statistical tests; Attention mechanisms; Classification accuracy; Extraction capability; Features extraction; GF-5; Hyperspectral remote sensing; Images classification; Land cover; Land cover classification; Land surface; image classification; land classification; land cover; multispectral image; remote sensing; satellite imagery; Generative adversarial networks","Attention mechanism; Gaofen-5 (GF-5); generative adversarial network; hyperspectral remote sensing; land cover classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123773453"
"Yu T.; Zhang J.; Zhou J.","Yu, Tianzhu (57219568877); Zhang, Jiexin (57216517341); Zhou, Jianjiang (8901871000)","57219568877; 57216517341; 8901871000","Conditional GAN with Effective Attention for SAR-to-Optical Image Translation","2021","Proceedings - 2021 3rd International Conference on Advances in Computer Technology, Information Science and Communication, CTISC 2021","","","","7","11","4","10.1109/CTISC52352.2021.00009","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115680042&doi=10.1109%2fCTISC52352.2021.00009&partnerID=40&md5=a8bc16c44f99d30a33f0ac8322b4ee74","Synthetic aperture radar (SAR) is an effective observation technology, which is widely used in industry and agriculture. However, SAR images have speckle noise because of its imaging mechanism, so it is difficult to obtain useful information from them directly. Generative adversarial networks (GANs) have great performance in image translation with the development of deep learning, SAR images can be translated into optical images. However, due to the complex scene, low resolution and speckle noise, the generated images obtained by the existing methods are not satisfactory. In this paper, we propose a method based on conditional GAN (CGAN) for image translation from SAR images to optical images. We use the attention mechanism, which means that the network attaches importance to useful features and ignores unimportant ones. We apply discrete cosine transform (DCT) as loss function to extract the low frequency features in the image. Our experiments show that the quality of the images generated by our method is better than that of some famous methods.  © 2021 IEEE.","Agricultural robots; Deep learning; Discrete cosine transforms; Geometrical optics; Speckle; Synthetic aperture radar; Adversarial networks; Attention mechanisms; Complex scenes; Discrete Cosine Transform(DCT); Image translation; Imaging mechanism; Loss functions; Low frequency features; Radar imaging","Attention mechanism; component; Generative adversarial networks; Image translation; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85115680042"
"Fu S.; Xu F.; Jin Y.-Q.","Fu, Shilei (57214850859); Xu, Feng (9240823200); Jin, Ya-Qiu (57211730231)","57214850859; 9240823200; 57211730231","Reciprocal translation between SAR and optical remote sensing images with cascaded-residual adversarial networks","2021","Science China Information Sciences","64","2","122301","","","","10.1007/s11432-020-3077-5","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105234080&doi=10.1007%2fs11432-020-3077-5&partnerID=40&md5=140ad4c8dd068a60c0698ffd1fb6f6e7","Despite the advantages of all-weather and all-day high-resolution imaging, synthetic aperture radar (SAR) images are much less viewed and used by general people because human vision is not adapted to microwave scattering phenomenon. However, expert interpreters can be trained by comparing side-by-side SAR and optical images to learn the mapping rules from SAR to optical. This paper attempts to develop machine intelligence that is trainable with large-volume co-registered SAR and optical images to translate SAR images to optical version for assisted SAR image interpretation. Reciprocal SAR-optical image translation is a challenging task because it is a raw data translation between two physically very different sensing modalities. Inspired by recent progresses in image translation studies in computer vision, this paper tackles the problem of SAR-optical reciprocal translation with an adversarial network scheme where cascaded residual connections and hybrid L1-GAN loss are employed. It is trained and tested on both spaceborne Gaofen-3 (GF-3) and airborne Uninhabited Airborne Vehicle Synthetic Aperture Radar (UAVSAR) images. Results are presented for datasets of different resolutions and polarizations and compared with other state-of-the-art methods. The Frechet inception distance (FID) is used to quantitatively evaluate the translation performance. The possibility of unsupervised learning with unpaired/unregistered SAR and optical images is also explored. Results show that the proposed translation network works well under many scenarios and it could potentially be used for assisted SAR interpretation. © 2021, Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature.","Artificial intelligence; Geometrical optics; Remote sensing; Synthetic aperture radar; Adversarial networks; Different resolutions; High-resolution imaging; Machine intelligence; Microwave scattering; Optical remote sensing; State-of-the-art methods; Synthetic aperture radar (SAR) images; Radar imaging","cascaded residual connection; Frechet inception distance; generative adversarial network (GAN); image translation; synthetic aperture radar","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85105234080"
"Ding B.; Xia X.; Liang X.","Ding, Bin (56240489600); Xia, Xue (57226063612); Liang, Xuefeng (57226513029)","56240489600; 57226063612; 57226513029","Sea Clutter Data Augmentation Method Based on Deep Generative Adversarial Network; [基于深度生成对抗网络的海杂波数据增强方法]","2021","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","43","7","","1985","1991","6","10.11999/JEIT200447","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110389695&doi=10.11999%2fJEIT200447&partnerID=40&md5=cc3c8004a096ecfa26d8ca439296e541","Due to the scarcity of sea clutter data, the high cost and long period of obtaining sea clutter data greatly limit the research of sea clutter characteristics and the application of ocean remote sensing. The method of sea clutter data generation based on the Generative Adversarial Networks (GAN) is studied. By extending the traditional GAN framework, a one-dimensional sea clutter data generation and identification model is formed. Based on the radar measured sea clutter data set, the generation and identification model training in the adversarial network is carried out. The amplitude distribution characteristics and time and spatial correlation of the sea clutter data generated by the model are analyzed. Based on the measured data, it is verified that the method can generate more sea clutter data with more variety, and similar distribution to the real sea clutter data. © 2021, Science Press. All right reserved.","Clutter (information theory); Remote sensing; Adversarial networks; Amplitude distributions; Data augmentation; Data generation; Identification model; Ocean remote sensing; Real sea clutters; Spatial correlations; Radar clutter","Amplitude distribution characteristics; Generative Adversarial Networks(GAN); Sea clutter; Temporal correlation","Article","Final","","Scopus","2-s2.0-85110389695"
"Chen C.; Ma H.; Yao G.; Lv N.; Yang H.; Li C.; Wan S.","Chen, Chen (56640091900); Ma, Hongxiang (57222242244); Yao, Guorun (57224074185); Lv, Ning (57220505216); Yang, Hua (57881716800); Li, Cong (25930714700); Wan, Shaohua (35190004700)","56640091900; 57222242244; 57224074185; 57220505216; 57881716800; 25930714700; 35190004700","Remote sensing image augmentation based on text description for waterside change detection","2021","Remote Sensing","13","10","1894","","","","10.3390/rs13101894","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106888825&doi=10.3390%2frs13101894&partnerID=40&md5=4afbce72d2e6adbf88a04f5c9f311f82","Since remote sensing images are difficult to obtain and need to go through a complicated administrative procedure for use in China, it cannot meet the requirement of huge training samples for Waterside Change Detection based on deep learning. Recently, data augmentation has become an effective method to address the issue of an absence of training samples. Therefore, an improved Generative Adversarial Network (GAN), i.e., BTD-sGAN (Text-based Deeply-supervised GAN), is proposed to generate training samples for remote sensing images of Anhui Province, China. The principal structure of our model is based on Deeply-supervised GAN(D-sGAN), and D-sGAN is improved from the point of the diversity of the generated samples. First, the network takes Perlin Noise, image segmentation graph, and encoded text vector as input, in which the size of image segmentation graph is adjusted to 128 × 128 to facilitate fusion with the text vector. Then, to improve the diversity of the generated images, the text vector is used to modify the semantic loss of the downsampled text. Finally, to balance the time and quality of image generation, only a two-layer Unet++ structure is used to generate the image. Herein, “Inception Score”, “Human Rank”, and “Inference Time” are used to evaluate the performance of BTD-sGAN, StackGAN++, and GAN-INT-CLS. At the same time, to verify the diversity of the remote sensing images generated by BTD-sGAN, this paper compares the results when the generated images are sent to the remote sensing interpretation network and when the generated images are not added; the results show that the generated image can improve the precision of soil-moving detection by 5%, which proves the effectiveness of the proposed model. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image segmentation; Remote sensing; Sampling; Semantics; Administrative procedures; Adversarial networks; Change detection; Data augmentation; Image generations; Principal structures; Remote sensing images; Remote sensing interpretation; Image enhancement","Data augmentation; Deeply monitoring; GAN; Remote sensing image; Text description","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106888825"
"Senanayake D.; Halgamuge S.; Ackland D.C.","Senanayake, Damith (57205684145); Halgamuge, Saman (7004669238); Ackland, David C. (25122084400)","57205684145; 7004669238; 25122084400","Real-time conversion of inertial measurement unit data to ankle joint angles using deep neural networks","2021","Journal of Biomechanics","125","","110552","","","","10.1016/j.jbiomech.2021.110552","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109111036&doi=10.1016%2fj.jbiomech.2021.110552&partnerID=40&md5=838ef934e96d62c649a8351cc8f159d3","Joint angle quantification from inertial measurement units (IMUs) is commonly performed using kinematic modelling, which depends on anatomical sensor placement and/or functional joint calibration; however, accurate three-dimensional joint motion measurement remains challenging to achieve. The aims of this study were firstly to employ deep neural networks to convert IMU data to ankle joint angles that are indistinguishable from those derived from motion capture-based inverse kinematics (IK) - the reference standard; and secondly, to validate the robustness of this approach across contrasting walking speeds in healthy individuals. Kinematics data were simultaneously calculated using IMUs and IK from 9 subjects during walking on a treadmill at 0.5 m/s, 1.0 m/s and 1.5 m/s. A generative adversarial network was trained using gait data at two of the walking speeds to predict ankle kinematics from IMU data alone for the third walking speed. There were significant differences between IK and IMU joint angle predictions for ankle eversion and internal rotation during walking at 0.5 m/s and 1.0 m/s (p < 0.001); however, no significant differences in joint angles were observed between the generative adversarial network prediction and IK at any speed or plane of joint motion (p < 0.05). The RMS difference in ankle joint kinematics between the generative adversarial network and IK for walking at 1.0 m/s was 3.8°, 2.1° and 3.5° for dorsiflexion, inversion and axial rotation, respectively. The modeling approach presented for real-time IMU to ankle joint angle conversion, which can be readily expanded to other joints, may provide enhanced IMU capability in applications such as telemedicine, remote monitoring and rehabilitation. © 2021 Elsevier Ltd","Ankle Joint; Biomechanical Phenomena; Gait; Humans; Neural Networks, Computer; Walking; Data handling; Deep neural networks; Forecasting; Inverse kinematics; Inverse problems; Adversarial networks; Ankle joint angles; Biomechanical model; Deep learning; Generative adversarial network; Human movements; Inertial measurements units; Joint angle; Machine-learning; Walking speed; adult; Article; biomechanics; clinical article; controlled study; deep learning; deep neural network; female; gait; human; human experiment; joint function; kinematics; machine learning; male; normal human; remote sensing; telemedicine; walking speed; ankle; biomechanics; walking; Motion analysis","Biomechanical model; deep learning; generative adversarial network; human movement; IMU; kinematics; machine learning; motion analysis; Wearables","Article","Final","","Scopus","2-s2.0-85109111036"
"Illarionova S.; Shadrin D.; Trekin A.; Ignatiev V.; Oseledets I.","Illarionova, Svetlana (57221414430); Shadrin, Dmitrii (57203155501); Trekin, Alexey (56993686600); Ignatiev, Vladimir (55747479300); Oseledets, Ivan (8529104000)","57221414430; 57203155501; 56993686600; 55747479300; 8529104000","Generation of the nir spectral band for satellite images with convolutional neural networks","2021","Sensors","21","16","5646","","","","10.3390/s21165646","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113180464&doi=10.3390%2fs21165646&partnerID=40&md5=2cd80ecdf9af3801b67c39dcf1ce655a","The near-infrared (NIR) spectral range (from 780 to 2500 nm) of the multispectral remote sensing imagery provides vital information for landcover classification, especially concerning vegetation assessment. Despite the usefulness of NIR, it does not always accomplish common RGB. Modern achievements in image processing via deep neural networks make it possible to generate artificial spectral information, for example, to solve the image colorization problem. In this research, we aim to investigate whether this approach can produce not only visually similar images but also an artificial spectral band that can improve the performance of computer vision algorithms for solving remote sensing tasks. We study the use of a generative adversarial network (GAN) approach in the task of the NIR band generation using only RGB channels of high-resolution satellite imagery. We evaluate the impact of a generated channel on the model performance to solve the forest segmentation task. Our results show an increase in model accuracy when using generated NIR compared to the baseline model, which uses only RGB (0.947 and 0.914 F1-scores, respectively). The presented study shows the advantages of generating the extra band such as the opportunity to reduce the required amount of labeled data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Algorithms; Image Processing, Computer-Assisted; Neural Networks, Computer; Satellite Imagery; Classification (of information); Convolutional neural networks; Deep neural networks; Infrared devices; Remote sensing; Satellite imagery; Adversarial networks; Computer vision algorithms; High resolution satellite imagery; Image colorizations; Land-cover classification; Model performance; Multispectral remote sensing imagery; Spectral information; algorithm; image processing; satellite imagery; Image enhancement","Convolutional neural network; Feature engineering; GAN; Near-infrared channel; Satellite imagery","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113180464"
"He W.; Wang C.; Sun Z.","He, Wenlei (57921370500); Wang, Chaoli (57907091700); Sun, Zhanquan (57215661166)","57921370500; 57907091700; 57215661166","Super-resolution Reconstruction of Satellite Imagery Based on Generative Adversarial Network","2021","Information and Control","50","2","","195","203","8","10.13976/j.cnki.xk.2021.0181","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137653769&doi=10.13976%2fj.cnki.xk.2021.0181&partnerID=40&md5=797224bbccebfd2cbeac7b49a69bbc27","Although most indices associated with the super-resolution reconstruction of a single remote sensing image based on deep earning have been significantly mproved, the effect observed by the human eyes is not obvious. Previous methods for creating low-resolution mages cause some information losses. To avoid this problem, we use different scales to obtain high-and low-resolution remote sensing image pairs as training data sets. Through this method, we can effectively avoid the loss of original image information caused by downsampling. We use a generative adversarial network (GAN) image super-resolution model based on deep residual blocks so that the model can better learn a priori information. Thus, the quality of the mage generated by the algorithm and the efficiency improve. We also add the spatial position information between mage features to the contextual loss function, thereby reducing mage artifacts caused by feature matching errors. Then, we add a relative discriminator to evaluate the relative authenticity of the obtained image and optimize the super-resolution effect. Experimental results on MWPU-RESISG45 dataset verify that the proposed method greatly enhances PSNR (peak signal to noise ratio), SSIM (structural similarity), and AG(average gradient) indicators. The human eye observation reveals that the network outputs a good super-resolution effect map. © 2021 The Authors. All rights reserved.","","contextual loss function; convolutional neural network (CNN); generative adversarial network (GAN); remote sensing magery; super-resolution","Article","Final","","Scopus","2-s2.0-85137653769"
"JinYu W.; Yang L.; HaiTao Y.; FengJie Z.; YuGe G.; GaoYuan L.","JinYu, Wang (57934662700); Yang, Li (57226812251); HaiTao, Yang (56705241600); FengJie, Zheng (57226807276); YuGe, Gao (57280166300); GaoYuan, Li (57280674400)","57934662700; 57226812251; 56705241600; 57226807276; 57280166300; 57280674400","GaN evaluation method based on remote sensing image information","2021","2021 6th International Conference on Image, Vision and Computing, ICIVC 2021","","","","295","300","5","10.1109/ICIVC52351.2021.9526935","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116120552&doi=10.1109%2fICIVC52351.2021.9526935&partnerID=40&md5=850f98d5fe1fc1e461d8c001c3e306bb","To address the problem of poor applicability of the existing generative adversarial network (GAN) evaluation system, this paper introduces the remote sensing image information quantity calculation system into the generative adversarial network evaluation indicator system, proposes an end-to-end GAN evaluation method based on the single-pixel image information quantity indicator, and uses EDS, ENDS, and InfoDS to evaluate the stability of the generator training process, mode collapse, diversity generation, and training failure problem is effectively monitored, which independent of loss function, learning rate. © 2021 IEEE.","Remote sensing; Calculation systems; End to end; Evaluation indicator system; Evaluation methods; Image information; Information indicator; Information quantity; Network evaluation; Remote sensing image information; Remote sensing images; Generative adversarial networks","GAN; Information indicators; Remote sensing image information","Conference paper","Final","","Scopus","2-s2.0-85116120552"
"Li J.; Zi S.; Song R.; Li Y.; Hu Y.; Du Q.","Li, Jiaojiao (55934244200); Zi, Shunyao (57468855000); Song, Rui (36460216100); Li, Yunsong (55986546100); Hu, Yinlin (57190163532); Du, Qian (7202060063)","55934244200; 57468855000; 36460216100; 55986546100; 57190163532; 7202060063","A Stepwise Domain Adaptive Segmentation Network With Covariate Shift Alleviation for Remote Sensing Imagery","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5618515","","","","10.1109/TGRS.2022.3152587","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125316806&doi=10.1109%2fTGRS.2022.3152587&partnerID=40&md5=70a289989203528b423322de5dbae687","Semantic segmentation for remote sensing images (RSI) is critical for the Earth monitoring system. However, the covariate shift between RSI datasets under different capture conditions cannot be alleviated by directly using the unsupervised domain adaptation (UDA) method, which negatively affects the segmentation accuracy in RSI. We propose a stepwise domain adaptive segmentation network with covariate shift alleviation (Cov-DA) for RSI parsing to solve this issue. Specifically, to alleviate domain shift generated by different sensors, both the source and target domains are projected into a colorspace with normalized distribution through an elaborate colorspace mapping unified module (CMUM). The color distributions of these two domains tend to be more uniform. Furthermore, in the target domain, the multistatistics joint evaluation module (MJEM) is proposed to capture different statistical characteristics of subscenarios for selecting plain scenarios regarded as high-confidence segmentation results to assist the further improvement of segmentation performance. In addition, a pyramid perceptual attention module (PPAM) containing omnidirectional features without computational burdens is added to our network for effectively enhancing the multiscale feature capture ability. In the cross-city DA experiments based on the International Society for Photogrammetry and Remote Sensing (ISPRS) and aerial benchmarks, the superiority of our algorithm is significantly demonstrated. Furthermore, we release a large-scale Martian terrain dataset noted as 'Mars-Seg' containing 5 K images with pixel-level accurate annotations regarding issues, such as the lack of semantic segmentation datasets for unknown scenes. © 1980-2012 IEEE.","Antennas; Generative adversarial networks; Remote sensing; Semantic Segmentation; Semantic Web; Complexity theory; Covariate shift alleviation; Covariate shifts; Domain adaptation; Features extraction; Images segmentations; Remote sensing images; Semantic segmentation; Stepwise; Unsupervised domain adaptation; covariance analysis; radar imagery; remote sensing; segmentation; Semantics","Covariate shift alleviation; semantic segmentation; stepwise; unsupervised domain adaptation (UDA)","Article","Final","","Scopus","2-s2.0-85125316806"
"Huang Y.; Wu M.; Guo J.; Zhang C.; Xu M.","Huang, Yixiang (57226786806); Wu, Ming (57209361563); Guo, Jun (56168336500); Zhang, Chuang (55925736800); Xu, Mengqiu (57219573819)","57226786806; 57209361563; 56168336500; 55925736800; 57219573819","A Correlation Context-Driven Method for Sea Fog Detection in Meteorological Satellite Imagery","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3095731","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112670094&doi=10.1109%2fLGRS.2021.3095731&partnerID=40&md5=8c5bda69ca7f415c46eeddce527dba4b","Sea fog detection is a challenging and essential issue in satellite remote sensing. Although conventional threshold methods and deep learning methods can achieve pixel-level classification, it is difficult to distinguish ambiguous boundaries and thin structures from the background. Considering the correlations between neighbor pixels and the affinities between superpixels, a correlation context-driven method for sea fog detection is proposed in this letter, which mainly consists of a two-stage superpixel-based fully convolutional network (SFCNet), named SFCNet. A fully connected Conditional Random Field (CRF) is utilized to model the dependencies between pixels. To alleviate the problem of high cloud occlusion, an attentive Generative Adversarial Network (GAN) is implemented for image enhancement by exploiting contextual information. Experimental results demonstrate that our proposed method achieves 91.65% mIoU and obtains more refined segmentation results, performing well in detecting fogs in small, broken bits and weak contrast thin structures, as well as detects more obscured parts.  © 2004-2012 IEEE.","Convolutional neural networks; Deep learning; Fog; Image enhancement; Image segmentation; Random processes; Remote sensing; Satellite imagery; Superpixels; Adversarial networks; Conditional random field; Contextual information; Convolutional networks; Learning methods; Satellite remote sensing; Segmentation results; Threshold methods; correlation; detection method; fog; meteorological hazard; meteorology; satellite imagery; Learning systems","Deep learning; satellite imagery; sea fog detection; superpixel","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85112670094"
"Zhang Y.; Yang W.; Zhang W.; Yu J.; Zhang J.; Yang Y.; Lu Y.; Tang W.","Zhang, Yanchao (56909202300); Yang, Wen (57216210671); Zhang, Wenbo (57222958569); Yu, Jiya (57222962938); Zhang, Jianxin (57382716700); Yang, Yongjie (54682560200); Lu, Yongliang (55506501400); Tang, Wei (55957794100)","56909202300; 57216210671; 57222958569; 57222962938; 57382716700; 54682560200; 55506501400; 55957794100","Two-step ResUp&Down generative adversarial network to reconstruct multispectral image from aerial RGB image","2022","Computers and Electronics in Agriculture","192","","106617","","","","10.1016/j.compag.2021.106617","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121638313&doi=10.1016%2fj.compag.2021.106617&partnerID=40&md5=e34de040f948b9ff8b8a7a0cf8580b5a","Convolutional neural network has brought breakthroughs on multispectral image reconstruction research. Previous work has largely focused on reconstructing MSI using the R-G-B channels from the MSI as inputs of the model. However, it's image manipulation rather than practical use. In real application, to reconstruct multispectral image using images from RGB camera is a research that has hardly been studied. In this research, high resolution aerial RGB images are collected by drone with RGB camera and multispectral images are collected by drone with RedEdge-M multispectral Camera. Then a new two-step Generative Adversarial Network (GAN)-based reconstruction method was proposed as follows: At first, MSI and RGB images are carefully registered to make sure that pixels are one–one correspondent. Then two data sources are cropped to form dataset. After that, a novel R-MSI GAN using is proposed. It uses a ResUp&Down block to replace the ResNet block of the Generator network and it outperforms ResNet-based GAN. The experimental results show that: (1) the combination of Mean Square Error and Discriminator (MSE-D) can alleviate the problem of the high-frequency loss of generated images. (2) The root means square error (RMSE), mean relative absolute error (MRAE) and Structural Similarity (SSIM) can only reflect overall error but can't reflect details in reconstructed images, while different bands' statistical histogram can present the total high-frequency loss of generated bands. (3) 3 indexes, which are intersection over union (IoU) based normalized difference vegetation index (NDVI)-IoU, normalized difference red edge (NDRE)-IoU and enhance vegetation index (EVI)-IoU, were defined to verify the effect of the generated MSI and they show good consistence with vegetation index map. 4 In comparisons among ResNet-based GAN, single step ResUp&Down GAN and two-step ResUp&Down GAN(T-GAN) with 3 loss functions (L1, MSE, Discriminator), the two-step ResUp&Down GAN(T-GAN) with MSE-D loss function performs best in reconstructing RGB bands. The T-GAN with L1loss-D (mean absolute error loss) performs best in reconstructing NIR and rededge bands. In summary, the proposed methods can effectively reconstruct MSI using images from RGB camera at drone based remote sensing. © 2021 Elsevier B.V.","Antennas; Cameras; Convolution; Drones; Errors; Image enhancement; Image reconstruction; Mean square error; Vegetation; High frequency loss; Images reconstruction; Images registration; Means square errors; Multispectral image reconstruction; Multispectral images; Resnet; RGB cameras; RGB images; Vegetation index; aerial survey; image analysis; multispectral image; NDVI; pixel; reconstruction; Generative adversarial networks","Generative adversarial network; Images registration; Multispectral image reconstruction; ResNet; UAV","Article","Final","","Scopus","2-s2.0-85121638313"
"","","","International Conference on Computer Vision and Pattern Analysis, ICCPA 2021","2022","Proceedings of SPIE - The International Society for Optical Engineering","12158","","","","","290","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127346886&partnerID=40&md5=4a727167eb0e1994950a2472139f298a","The proceedings contain 44 papers. The topics discussed include: cross-scale feature extraction module for efficient rgbd images semantic segmentation; comprehensive indices and methods for estimating the quality of the depth generalization in chart cartography; research on land use classification with high resolution remote sensing images based on optimal segmentation; research on application of image ranging technology in geotechnical test; the study of L2 mispronunciation detection based on mandarin landmarks; LGA-GAN: landmarks guided attentive generative adversarial network for facial expression manipulation; multi-stage network for single image demoireing; quality assessment of satellite remote sensing of sea-surface temperature; and research on music evolution characteristics analysis based on factor analysis and Euclidean distance.","","","Conference review","Final","","Scopus","2-s2.0-85127346886"
"Xie J.; Fang L.; Zhang B.; Chanussot J.; Li S.","Xie, Jie (57210957012); Fang, Leyuan (57218451012); Zhang, Bob (57217861698); Chanussot, Jocelyn (6602159365); Li, Shutao (7409240361)","57210957012; 57218451012; 57217861698; 6602159365; 7409240361","Super Resolution Guided Deep Network for Land Cover Classification from Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3120891","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117777912&doi=10.1109%2fTGRS.2021.3120891&partnerID=40&md5=3c92c3e9453a108dbed15f797fcad970","The low resolution of remote sensing images often limits the land cover classification (LCC) performance. Super resolution (SR) can improve the image resolution, while greatly increasing the computational burden for the LCC due to the larger size of the input image. In this article, the SR-guided deep network (SRGDN) framework is proposed, which can generate meaningful structures from higher resolution images to improve the LCC performance without consuming more computational costs. In general, the SRGDN consists of two branches (i.e., SR branch and LCC branch) and a guidance module. The SR branch aims to increase the resolution of remote sensing images. Since high- and low-resolution image pairs cannot be directly provided by imaging sensors to train the SR branch, we introduce a self-supervised generative adversarial network (GAN) to estimate the downsampling kernel that can produce these image pairs. The LCC branch adopts the high-resolution network (HRNet) to retain as much resolution information with a few downsampling operations as possible. The guidance module teaches the LCC branch to learn the high-resolution information from the SR branch without the utilization of the higher-resolution images as the inputs. Furthermore, the guidance module introduces spatial pyramid pooling (SPP) to match the feature maps of different sizes in the two branches. In the testing stage, the guidance module and SR branch can be removed, and therefore do not create additional computational costs. Experimental results on three real datasets demonstrate the superiority of the proposed method over several well-known LCC approaches.  © 1980-2012 IEEE.","Image classification; Image resolution; Remote sensing; Signal sampling; Classification performance; Computational costs; Down sampling; Guidance; High resolution; High-resolution images; Image pairs; Land cover classification; Remote sensing images; Superresolution; artificial neural network; image analysis; land cover; remote sensing; satellite imagery; Image enhancement","Guidance; land cover classification (LCC); remote sensing image; super resolution (SR)","Article","Final","","Scopus","2-s2.0-85117777912"
"Zhang R.; Jiang X.; An J.; Cui T.","Zhang, Ruiyan (57220926023); Jiang, Xiujie (7404628322); An, Junshe (55889738000); Cui, Tianshu (57650994100)","57220926023; 7404628322; 55889738000; 57650994100","Data-Free Low-Bit Quantization for Remote Sensing Object Detection","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3122875","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118557807&doi=10.1109%2fLGRS.2021.3122875&partnerID=40&md5=d38fd51f5aafcd5647ddb98c36d376d8","Convolutional neural networks (CNNs) have been extensively used in remote sensing (RS) detection models. Although the memory size of detection models is massive, quantization can offer access to using these models in satellite embedded devices. However, for privacy reasons, institutions in charge of quantification operations may not obtain the original images. Based on this circumstance, current data-based quantization methods are no longer applicable, and data-free methods have poor performance for low-bit quantization. To address this problem, we propose a data-free quantization method for the CNN-based RS detection model. First, we use a generative adversarial network (GAN) to generate fake scene images. These images represent the global contextual information of each category. Second, we quantify the full-precision pretrained detection network. Finally, we train the quantized model to mimic the performance of the full-precision model by the proposed alternate training strategy with the generated fake scene images. We apply our method on CenterNet with a ResNet-18 backbone and evaluate the quantized model on the NWPU VHR-10 and DOTA datasets. The results show that our 5 bit quantized detection network obtains 94.1% mAP on NWPU VHR-10 and compresses the memory size to 0.158 times that of the full-precision network. Experiments verify that our data-free scene generation quantization algorithm maintains high performance with a large model compression ratio. © 2004-2012 IEEE.","Image compression; Job analysis; Neural networks; Object detection; Quantization (signal); Remote sensing; Data-free quantization; Fake scene image; Heating system; Location awareness; Predictive models; Quantisation; Quantization (signal); Remote-sensing; Scene image; Task analysis; algorithm; data set; detection method; experimental study; image classification; precision; remote sensing; training; Object recognition","Data-free quantization; fake scene images; object detection; remote sensing (RS)","Article","Final","","Scopus","2-s2.0-85118557807"
"Hu J.; Zhang Y.; Zhao D.; Yang G.; Chen F.; Zhou C.; Chen W.","Hu, Jun (57209298402); Zhang, Yanfeng (57189637304); Zhao, Dandan (56197460400); Yang, Guijun (55501651300); Chen, Feiyun (57217036682); Zhou, Chengquan (57200859124); Chen, Wenxuan (7409641778)","57209298402; 57189637304; 56197460400; 55501651300; 57217036682; 57200859124; 7409641778","A Robust Deep Learning Approach for the Quantitative Characterization and Clustering of Peach Tree Crowns Based on UAV Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4408613","","","","10.1109/TGRS.2022.3142288","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122848569&doi=10.1109%2fTGRS.2022.3142288&partnerID=40&md5=726b5edfdf373ce24ee1d007381bb927","The accurate large-scale measurement of peach crowns is vital in horticultural science and the optimization of orchard management. Nowadays, numerous crown parameters (e.g., crown area, height, and volume) can be obtained via the analysis of point clouds or photographs. Current laser-based sensors provide the required reliable and accurate information; however, they are costly and time-consuming. Therefore, a simpler approach for crown measurement is required. For this purpose, this study presents a pipeline for the monitoring and clustering of 259 peach tree crowns based on unmanned aerial vehicle (UAV) images of a peach orchard in Southeast China. Considering the limitation that the original aerial image dataset contains little information, a data augmentation process is adopted, and an efficient deep learning architecture based on conditional generative adversarial networks (cGANs) was designed to extract the crown area. Then, the shape of the crown area was clustered using an edge detection process and a $k$ -means algorithm. Finally, an ellipsoid volume method (EVM) was applied to estimate the crown volume. Five indicators - namely, $Q_{\mathrm {seg}}$ , $S_{\mathrm {r}}$ , Precision, Recall, and F-measure - were employed to evaluate the crown extraction effects, and the average results for testing samples were 0.832, 0.847, 0.851, 0.828, and 0.846, respectively. Compared with other approaches - namely, fully convolutional network (FCN), U-Net, SegNet21, the excess green index (ExG), and the color index of vegetation extraction (CIVE) - the proposed cGAN model performs better, achieving an accuracy improvement of 5%-25%. For the estimation of crown volume, using measurements from a light detection and ranging (LIDAR) scanner as a reference, the correlation coefficient and relative-root-mean-square error (R-RMSE) were found to be 0.836% and 14.93%, respectively. Overall, the results demonstrate that the proposed method is feasible for measuring peach tree crowns. The wide application of such technology would facilitate applied research in plant phenotyping and precision horticulture. © 1980-2012 IEEE.","Antennas; Autonomous vehicles; Deep learning; Extraction; K-means clustering; Mean square error; Optical radar; Remote sensing; Vegetation; Volume measurement; Crown area; Crown measurement; Crown volume; Deep learning; Peach trees; Remote-sensing; Shape clustering; Tree crowns; UAV image; Volume estimations; canopy architecture; cluster analysis; learning; quantitative analysis; Unmanned aerial vehicles (UAV)","Crown measurement; deep learning; shape clustering; unmanned aerial vehicle (UAV) images; volume estimation","Article","Final","","Scopus","2-s2.0-85122848569"
"Jamali A.; Mahdianpari M.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939)","56909712300; 57190371939","A cloud-based framework for large-scale monitoring of ocean plastics using multi-spectral satellite imagery and generative adversarial network","2021","Water (Switzerland)","13","18","2553","","","","10.3390/w13182553","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115435905&doi=10.3390%2fw13182553&partnerID=40&md5=7aa145df5785a2f6a098be414387695c","Marine debris is considered a threat to the inhabitants, as well as the marine environments. Accumulation of marine debris, besides climate change factors, including warming water, sea-level rise, and changes in oceans’ chemistry, are causing the potential collapse of the marine environment’s health. Due to the increase of marine debris, including plastics in coastlines, ocean and sea surfaces, and even in deep ocean layers, there is a need for developing new advanced technology for the detection of large-sized marine pollution (with sizes larger than 1 m) using state-of-the-art remote sensing and machine learning tools. Therefore, we developed a cloud-based framework for large-scale marine pollution detection with the integration of Sentinel-2 satellite imagery and advanced machine learning tools on the Sentinel Hub cloud application programming interface (API). Moreover, we evaluated the performance of two shallow machine learning algorithms of random forest (RF) and support vector machine (SVM), as well as the deep learning method of the generative adversarial network-random forest (GAN-RF) for the detection of ocean plastics in the pilot site of Mytilene Island, Greece. Based on the obtained results, the shallow algorithms of RF and SVM achieved an overall accuracy of 88% and 84%, respectively, with available training data of plastic debris. The GAN-RF classifier improved the detection of ocean plastics of the RF method by 8%, achieving an overall accuracy of 96% by generating several synthetic ocean plastic samples. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aegean Islands; Greece; Lesbos; Northern Aegean; Application programming interfaces (API); Classification (of information); Climate change; Debris; Decision trees; Deep learning; Elastomers; Learning algorithms; Marine pollution; Oil spills; Radiometers; Remote sensing; Satellite imagery; Sea level; Support vector machines; Cloud-based; Large-scale monitoring; Marine debris; Marine environment; Multi-spectral; Ocean plastic; Overall accuracies; Random forests; Sentinel hub; Support vectors machine; accuracy assessment; advanced technology; climate change; debris flow; deep water; marine environment; marine pollution; monitoring; monitoring system; plastic waste; satellite imagery; sea surface; Sentinel; spectral analysis; support vector machine; warming; Generative adversarial networks","Generative adversarial network; Marine debris; Marine pollution; Ocean plastics; Random forest; Sentinel Hub; Support vector machine","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115435905"
"Wang Y.; Wang H.; Xu T.","Wang, Yaoling (57213199984); Wang, Hongqi (55689018000); Xu, Tao (56783448700)","57213199984; 55689018000; 56783448700","Aircraft recognition of remote sensing image based on sample generated by CGAN; [CGAN样本生成的遥感图像飞机识别]","2021","Journal of Image and Graphics","26","3","","663","673","10","10.11834/jig.200001","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104650027&doi=10.11834%2fjig.200001&partnerID=40&md5=335ed0693ebc6880030f5ac4b8b0f619","Objective: Aircraft type recognition is a fundamental problem in remote sensing image interpretation, which aims to identify the type of aircraft in an image. Aircraft type recognition algorithms have been widely studied and improved ceaselessly. The traditional recognition algorithms are efficient, but their accuracy is limited by the small capacity and poor robustness. The deep-learning-based methods have been widely implemented because of good robustness and generalization, especially in the object recognition task. In remote sensing scenes, the objects are sparsely distributed; hence, the available samples are few. In addition, labeling is time consuming, resulting in a modest number of labeled samples. Generally, the deep-learning-based models rely on a large amount of labeled data due to thousands of weights needed to learn. Consequently, these models suffer from scarce data that are insufficient to meet the demand of large-scale datasets, especially in the remote sensing scene. Generative adversarial network (GAN) can produce realistic synthetic data and enlarge the scale of the real dataset. However, these algorithms usually take random noises as input; therefore, they are unable to control the position, angle, size, and category of objects in synthetic images. Conditional GAN (CGAN) have been proposed by previous researchers to generate synthetic images with designated content in a controlled scheme. CGANs take the pixel-wise labeled images as the input data and output the generated images that meet constraints from its corresponding input images. However, these generative adversarial models have been widely studied for natural sceneries, which are not suitable for remote sensing imageries due to the complex scenes and low resolutions. Hence, the GANs perform poorly when adopted to generate remote sensing images. An aircraft recognition framework of remote sensing images based on sample generation is proposed, which consists of an improved CGAN and a recognition model, to alleviate the lack of real samples and deal with the problems mentioned above. Method: In this framework, the masks of real aircraft images are labeled pixel by pixel. The masks of images serve as the conditions of the CGAN that are trained by the pairs of real aircraft images and corresponding masks. In this manner, the location, scale, and type of aircraft in the synthetic images can be controlled. Perceptual loss is introduced to promote the ability of the CGANs to model the scenes of remote sensing. The L2 distance between the features of real images and synthetic images extracted by the VGG-16 (visual geometry group 16-layer net) network measures the perceptual loss between the real images and synthetic images. Masked structural similarity (SSIM) loss is proposed, which forces the CGAN to focus on the masked region and improve the quality of the aircraft region in the synthetic images. SSIM is a measurement of image quantity according to the structure and texture. Masked SSIM loss is the sum of the product between masks and SSIM pixel by pixel. Afterward, the loss function of the CGAN consists of perceptual loss, masked SSIM loss, and origin CGAN loss. The recognition model in this framework is ResNet-50, which outputs the type and recognition score of an aircraft. In this paper, the recognition model trained on synthetic images is compared with the model trained on real images. The remote sensing images from QuickBird are cropped to build the real dataset, in which 800 images for each type are used for training and 1 000 images are used for testing. After data augmentation, the training dataset consists of 40 000 images, and the synthetic dataset consists of synthetic images generated by the generation module with flipped, rotated, and scaled masks. The generators are selected from different training stages to generate 2 000 synthetic images per type and determine the best end time in the training procedure. The chosen generator is used to produce different numbers of images for 10 aircraft types and find an acceptable number of synthetic images. These synthetic images serve as the training set for the recognition model, whose performances are compared. All our experiments are carried out on a single NVIDIA K80 GPU device with the framework of Pytorch, and the Adam optimizer is implemented to train the CGAN and ResNet-50 for 100 epochs. Result: The quantities of the synthetic images from the generator with and without our proposed loss functions on the training dataset are compared. The quantitative evaluation metrics contain peak signal to noise ratio (PSNR) and SSIM. Results show that PSNR and SSIM increase by 0.88 and 0.346 using our method, respectively. In addition, recognition accuracy increases with the training epoch of the generator and the number of synthetic images. Finally, the accuracy of the recognition model trained on the synthetic dataset is 0.33% less than that of the real dataset. Conclusion: An aircraft recognition framework of remote sensing images based on sample generation is proposed. The experiment results show that our method effectively improves the ability of CGAN to model the remote sensing scenes and alleviates the absence of data. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","Convolutional neural network; Deep learning; Generative adversarial network (GAN); Object recognition; Optical remote sensing images","Article","Final","","Scopus","2-s2.0-85104650027"
"Cai Y.; Yang Y.; Zheng Q.; Shen Z.; Shang Y.; Yin J.; Shi Z.","Cai, Yuxiang (57392976200); Yang, Yingchun (7409384281); Zheng, Qiyi (57273531900); Shen, Zhengwei (57272299200); Shang, Yongheng (55331397000); Yin, Jianwei (8249720800); Shi, Zhongtian (57393123300)","57392976200; 7409384281; 57273531900; 57272299200; 55331397000; 8249720800; 57393123300","BiFDANet: Unsupervised Bidirectional Domain Adaptation for Semantic Segmentation of Remote Sensing Images","2022","Remote Sensing","14","1","190","","","","10.3390/rs14010190","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122055851&doi=10.3390%2frs14010190&partnerID=40&md5=9b303bbbffad9ee971182ff60f563488","When segmenting massive amounts of remote sensing images collected from different satellites or geographic locations (cities), the pre-trained deep learning models cannot always output satisfactory predictions. To deal with this issue, domain adaptation has been widely utilized to enhance the generalization abilities of the segmentation models. Most of the existing domain adaptation methods, which based on image-to-image translation, firstly transfer the source images to the pseudo-target images, adapt the classifier from the source domain to the target domain. However, these unidirectional methods suffer from the following two limitations: (1) they do not consider the inverse procedure and they cannot fully take advantage of the information from the other domain, which is also beneficial, as confirmed by our experiments; (2) these methods may fail in the cases where transferring the source images to the pseudo-target images is difficult. In this paper, in order to solve these problems, we propose a novel framework BiFDANet for unsupervised bidirectional domain adaptation in the semantic segmentation of remote sensing images. It optimizes the segmentation models in two opposite directions. In the source-to-target direction, BiFDANet learns to transfer the source images to the pseudo-target images and adapts the classifier to the target domain. In the opposite direction, BiFDANet transfers the target images to the pseudo-source images and optimizes the source classifier. At test stage, we make the best of the source classifier and the target classifier, which complement each other with a simple linear combination method, further improving the performance of our BiFDANet. Furthermore, we propose a new bidirectional semantic consistency loss for our BiFDANet to maintain the semantic consistency during the bidirectional image-to-image translation process. The experiments on two datasets including satellite images and aerial images demonstrate the superiority of our method against existing unidirectional methods. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Antennas; Convolutional neural networks; Deep learning; Inverse problems; Remote sensing; Semantic Segmentation; Semantic Web; Semantics; Bidirectional domain adaptation; Convolutional neural network; Domain adaptation; Generative adversarial network; Image translation; Image-to-image translation; Remote sensing images; Semantic segmentation; Unsupervised domain adaptation; Generative adversarial networks","Bidirectional domain adaptation; Convolutional neural networks (CNNs); Generative adversarial networks (GANs); Image-to-image translation; Remote sensing images; Semantic segmentation; Unsupervised domain adaptation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122055851"
"Chen X.; Chen S.; Xu T.; Yin B.; Peng J.; Mei X.; Li H.","Chen, Xu (57190178100); Chen, Songqiang (57219765591); Xu, Tian (57219758916); Yin, Bangguo (57219763061); Peng, Jian (57215095598); Mei, Xiaoming (39061832100); Li, Haifeng (57189334346)","57190178100; 57219765591; 57219758916; 57219763061; 57215095598; 39061832100; 57189334346","SMAPGAN: Generative Adversarial Network-Based Semisupervised Styled Map Tile Generation Method","2021","IEEE Transactions on Geoscience and Remote Sensing","59","5","9200723","4388","4406","18","10.1109/TGRS.2020.3021819","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104817222&doi=10.1109%2fTGRS.2020.3021819&partnerID=40&md5=71f07d9b83fe34560c53578e0d71971b","Traditional online map tiles, which are widely used on the Internet, such as by Google Maps and Baidu Maps, are rendered from vector data. The timely updating of online map tiles from vector data, for which generation is time-consuming, is a difficult mission. Generating map tiles over time from remote sensing images is relatively simple and can be performed quickly without vector data. However, this approach used to be challenging or even impossible. Inspired by image-to-image translation (img2img) techniques based on generative adversarial networks (GANs), we proposed a semisupervised generation of styled map tiles based on the GANs (SMAPGAN) model to generate styled map tiles directly from remote sensing images. In this model, we designed a semisupervised learning strategy to pretrain SMAPGAN on rich unpaired samples and fine-tune it on limited paired samples in reality. We also designed the image gradient L1 loss and the image gradient structure loss to generate a styled map tile with global topological relationships and detailed edge curves for objects, which are important in cartography. Moreover, we proposed the edge structural similarity index (ESSI) as a metric to evaluate the quality of the topological consistency between the generated map tiles and ground truth. The experimental results show that SMAPGAN outperforms state-of-the-art (SOTA) works according to the mean squared error, the structural similarity index, and the ESSI. Also, SMAPGAN gained higher approval than SOTA in a human perceptual test on the visual realism of cartography. Our work shows that SMAPGAN is a new tool with excellent potential for producing styled map tiles. Our implementation of SMAPGAN is available at https://github.com/imcsq/SMAPGAN. © 1980-2012 IEEE.","HTTP; Learning systems; Maps; Mean square error; Topology; Adversarial networks; Generation method; Image translation; Mean squared error; Remote sensing images; Structural similarity indices; Topological consistency; Topological relationships; artificial neural network; cartography; detection method; image analysis; Internet; remote sensing; satellite imagery; similarity index; Remote sensing","Generative adversarial networks (GANs); map tiles generation; quality assessment; semisupervised","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104817222"
"Shamsolmoali P.; Zareapoor M.; Zhou H.; Wang R.; Yang J.","Shamsolmoali, Pourya (56350053200); Zareapoor, Masoumeh (56349635100); Zhou, Huiyu (23062556900); Wang, Ruili (55825442900); Yang, Jie (15039078800)","56350053200; 56349635100; 23062556900; 55825442900; 15039078800","Road Segmentation for Remote Sensing Images Using Adversarial Spatial Pyramid Networks","2021","IEEE Transactions on Geoscience and Remote Sensing","59","6","9173823","4673","4688","15","10.1109/TGRS.2020.3016086","47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106754652&doi=10.1109%2fTGRS.2020.3016086&partnerID=40&md5=51fcb47e12c1c17608d0ba3a345e06a4","Road extraction in remote sensing images is of great importance for a wide range of applications. Because of the complex background, and high density, most of the existing methods fail to accurately extract a road network that appears correct and complete. Moreover, they suffer from either insufficient training data or high costs of manual annotation. To address these problems, we introduce a new model to apply structured domain adaption for synthetic image generation and road segmentation. We incorporate a feature pyramid (FP) network into generative adversarial networks to minimize the difference between the source and target domains. A generator is learned to produce quality synthetic images, and the discriminator attempts to distinguish them. We also propose a FP network that improves the performance of the proposed model by extracting effective features from all the layers of the network for describing different scales' objects. Indeed, a novel scale-wise architecture is introduced to learn from the multilevel feature maps and improve the semantics of the features. For optimization, the model is trained by a joint reconstruction loss function, which minimizes the difference between the fake images and the real ones. A wide range of experiments on three data sets prove the superior performance of the proposed approach in terms of accuracy and efficiency. In particular, our model achieves state-of-the-art 78.86 IOU on the Massachusetts data set with 14.89M parameters and 86.78B FLOPs, with $4\times $ fewer FLOPs but higher accuracy (+3.47% IOU) than the top performer among state-of-the-art approaches used in the evaluation.  © 1980-2012 IEEE.","Feature extraction; Network layers; Remote sensing; Roads and streets; Semantics; Adversarial networks; Complex background; Manual annotation; Remote sensing images; Road segmentation; State-of-the-art approach; Synthetic image generation; Synthetic images; network analysis; satellite imagery; segmentation; Image segmentation","Adversarial network; domain adaptation; feature pyramid (FP); remote sensing (RS) images; road segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85106754652"
"Wang Y.; Xie Y.; Wu Y.; Liang K.; Qiao J.","Wang, Yajie (55734159700); Xie, Yanyan (57553191200); Wu, Yanyan (55977823200); Liang, Kai (57552266200); Qiao, Jilin (57552266300)","55734159700; 57553191200; 55977823200; 57552266200; 57552266300","An Unsupervised Multi-scale Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13142 LNCS","","","356","368","12","10.1007/978-3-030-98355-0_30","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127094685&doi=10.1007%2f978-3-030-98355-0_30&partnerID=40&md5=83048658bff66110025738d16f575ad4","Pan-sharpening of remote sensing images is an effective method to get high spatial resolution multi-spectral (HRMS) images by fusing low spatial resolution multi-spectral (LRMS) images and high spatial resolution panchromatic (PAN) images. Recently, many remote sensing images pan-sharpening methods based on convolutional neural networks (CNN) have been proposed and achieved excellent performance. However, two drawbacks still exist. On the one hand, since there are no ideal HRMS images as targets for learning, most existing methods require an extra effort to produce the simulated data for training. On the other hand, these methods ignore the local features of the original images. To address these issues, we propose an unsupervised multi-scale generative adversarial network method, which can train directly on the full-resolution images without down-sampling. Firstly, a multi-scale dense generator network is proposed to extract features from the original images to generate HRMS images. Secondly, two discriminators are used to protect the spectral information of LRMS images and spatial information of PAN images, respectively. Finally, to improve the quality of the fused image and implement training under the unsupervised setting, a new loss function is proposed. Experimental results based on QuickBird and GaoFen-2 data sets demonstrate that the proposed method can obtain much better fusion results for the full-resolution images. © 2022, Springer Nature Switzerland AG.","Convolutional neural networks; Image enhancement; Image resolution; Remote sensing; Full resolution images; High spatial resolution; Multi-scales; Multispectral images; Original images; Pan-sharpening; Remote sensing images; Remote-sensing; Spatial resolution; Unsupervised; Generative adversarial networks","Full-resolution images; Generative adversarial network; Pan-sharpening; Remote sensing; Unsupervised","Conference paper","Final","","Scopus","2-s2.0-85127094685"
"Tao L.; Ren H.; Ye Y.; Jiang J.","Tao, Liurong (57369916800); Ren, Haoran (23101165700); Ye, Yueming (23394324400); Jiang, Jinsheng (57217988653)","57369916800; 23101165700; 23394324400; 57217988653","Seismic Surface-Related Multiples Suppression Based on SAGAN","2022","IEEE Geoscience and Remote Sensing Letters","19","","3006605","","","","10.1109/LGRS.2022.3168143","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128615285&doi=10.1109%2fLGRS.2022.3168143&partnerID=40&md5=a79575d2cc8b967eda08bccc30699258","Suppressing multiples from seismic records is necessary to improve imaging quality. Deep neural networks (DNNs) can automatically mine features from data. Once a network is successfully trained, it can process data with extremely high efficiency. In this letter, a generative adversarial network (GAN) framework is proposed to remove surface-related multiples in both synthetic and field datasets, where the generator is U-Net with Markov discriminator. Adding self-attention (SA) blocks to GAN improves processing precision. Improved signal noise ratio (SNR), and accurate reverse time migration (RTM) images implemented by network's outputs of synthetic datasets, jointly support that this network is effective on surface-related multiple suppression. Based on the results from field application, deep learning method in this letter is comparable to conventional adaptive surface-related multiple elimination (SRME) method but time-saving. By constructing an end-to-end workflow for seismic surface-related multiples suppression, small batches dataset can be used to train the network, and large batches of datasets can be processed accurately and efficiently.  © 2004-2012 IEEE.","Data handling; Deep neural networks; Generative adversarial networks; Image enhancement; Large dataset; Seismology; Surface treatment; Surface waves; Higher efficiency; Imaging quality; Multiple suppression; Network frameworks; Process data; Processing precision; Seismic records; Self-attention; Surface impedances; Surface-related multiple; accuracy assessment; artificial neural network; remote sensing; satellite data; signal-to-noise ratio; Signal to noise ratio","Data processing; deep neural network (DNN); generative adversarial network (GAN); self-attention (SA); surface-related multiple","Article","Final","","Scopus","2-s2.0-85128615285"
"Wang Y.; Sun G.; Guo S.","Wang, Yuwu (57217011318); Sun, Guobing (57197195443); Guo, Shengwei (57297857800)","57217011318; 57197195443; 57297857800","Target detection method for low-resolution remote sensing image based on ESRGAN and ReDet","2021","Photonics","8","10","431","","","","10.3390/photonics8100431","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117216653&doi=10.3390%2fphotonics8100431&partnerID=40&md5=59e6d2953b206376c2513f1de03862d1","With the widespread use of remote sensing images, low-resolution target detection in remote sensing images has become a hot research topic in the field of computer vision. In this paper, we propose a Target Detection on Super-Resolution Reconstruction (TDoSR) method to solve the problem of low target recognition rates in low-resolution remote sensing images under foggy con-ditions. The TDoSR method uses the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) to perform defogging and super-resolution reconstruction of foggy low-resolution remote sensing images. In the target detection part, the Rotation Equivariant Detector (ReDet) algo-rithm, which has a higher recognition rate at this stage, is used to identify and classify various types of targets. While a large number of experiments have been carried out on the remote sensing image dataset DOTA-v1.5, the results of this paper suggest that the proposed method achieves good results in the target detection of low-resolution foggy remote sensing images. The principal result of this paper demonstrates that the recognition rate of the TDoSR method increases by roughly 20% when compared with low-resolution foggy remote sensing images. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.","","ESRGAN; ReDet; Remote sensing images; Super-resolution reconstruction; Target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117216653"
"Zhao Z.; Zhang J.; Xu S.; Sun K.; Huang L.; Liu J.; Zhang C.","Zhao, Zixiang (57218542866); Zhang, Jiangshe (9737712100); Xu, Shuang (56367405500); Sun, Kai (57193093191); Huang, Lu (57222016088); Liu, Junmin (42761838200); Zhang, Chunxia (55703936800)","57218542866; 9737712100; 56367405500; 57193093191; 57222016088; 42761838200; 55703936800","FGF-GAN: A LIGHTWEIGHT GENERATIVE ADVERSARIAL NETWORK FOR PANSHARPENING VIA FAST GUIDED FILTER","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428272","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126434284&doi=10.1109%2fICME51207.2021.9428272&partnerID=40&md5=8d20df2cce1d516406276ae77807356d","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters. © 2021 IEEE Computer Society. All rights reserved.","Deep learning; Image enhancement; Image fusion; Remote sensing; Spectroscopy; Channel dimension; Fast guided filter; Guided filters; High resolution; Input image; Lower resolution; Multispectral images; Pan-sharpening; Remote-sensing; Single channels; Generative adversarial networks","Fast guided filter; Generative adversarial network; Image fusion; Pansharpening","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126434284"
"Ning Y.; You Y.; Cao J.; Liu F.; Yan Q.; Zhang Y.","Ning, Yuanyong (57222241669); You, Yanan (54409614600); Cao, Jingyi (57210948716); Liu, Fang (57091791100); Yan, Qing (55307568700); Zhang, Yue (57839925000)","57222241669; 54409614600; 57210948716; 57091791100; 55307568700; 57839925000","FUSION DETECTION OF CLOSED WATER IN MEDIUM-LOW RESOLUTION REMOTE SENSING IMAGERY","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4027","4030","3","10.1109/IGARSS47720.2021.9553554","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126054343&doi=10.1109%2fIGARSS47720.2021.9553554&partnerID=40&md5=b537674d1f996c16fb1e1fa780654ec3","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance. © 2021 IEEE.","","Closed water detection; GAN; Image fusion","Conference paper","Final","","Scopus","2-s2.0-85126054343"
"de Lima-Hernandez R.; Vergauwen M.","de Lima-Hernandez, Roberto (57188956317); Vergauwen, Maarten (6602413782)","57188956317; 6602413782","A Generative and Entropy-Based Registration Approach for the Reassembly of Ancient Inscriptions","2022","Remote Sensing","14","1","6","","","","10.3390/rs14010006","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122159115&doi=10.3390%2frs14010006&partnerID=40&md5=025e0b00a6e7442816e90210a9cc50e7","An increased interest in computer-aided heritage reconstruction has emerged in recent years due to the maturity of sophisticated computer vision techniques. Concretely, feature-based matching methods have been conducted to reassemble heritage assets, yielding plausible results for data that contains enough salient points for matching. However, they fail to register ancient artifacts that have been badly deteriorated over the years. In particular, for monochromatic incomplete data, such as 3D sunk relief eroded decorations, damaged drawings, and ancient inscriptions. The main issue lies in the lack of regions of interest and poor quality of the data, which prevent feature-based algorithms from estimating distinctive descriptors. This paper addresses the reassembly of damaged decorations by deploying a Generative Adversarial Network (GAN) to predict the continuing decoration traces of broken heritage fragments. By extending the texture information of broken counterpart fragments, it is demonstrated that registration methods are now able to find mutual characteristics that allow for accurate optimal rigid transformation estimation for fragments alignment. This work steps away from feature-based approaches, hence employing Mutual Information (MI) as a similarity metric to estimate an alignment transformation. Moreover, high-resolution geometry and imagery are combined to cope with the fragility and severe damage of heritage fragments. Therefore, the testing data is composed of a set of ancient Egyptian decorated broken fragments recorded through 3D remote sensing techniques. More specifically, structured light technology for mesh models creation, as well as orthophotos, upon which digital drawings are created. Even though this study is restricted to Egyptian artifacts, the workflow can be applied to reconstruct different types of decoration patterns in the cultural heritage domain. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Remote sensing; Textures; Computer vision techniques; Computer-aided; Digital heritage; Digital heritage reassembling; Egyptians; Entropy-based; Feature based matching; Matching methods; Mutual information registrations; Reassembly; Generative adversarial networks","Digital heritage reassembling; Generative adversarial networks; Mutual information registration","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85122159115"
"Hong D.; Yao J.; Meng D.; Xu Z.; Chanussot J.","Hong, Danfeng (56108179600); Yao, Jing (57218422895); Meng, Deyu (23393058400); Xu, Zongben (7405426248); Chanussot, Jocelyn (6602159365)","56108179600; 57218422895; 23393058400; 7405426248; 6602159365","Multimodal GANs: Toward Crossmodal Hyperspectral-Multispectral Image Segmentation","2021","IEEE Transactions on Geoscience and Remote Sensing","59","6","9198910","5103","5113","10","10.1109/TGRS.2020.3020823","46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099115847&doi=10.1109%2fTGRS.2020.3020823&partnerID=40&md5=277c4ca3cf7dad4da074afe8c04a7dd6","This article addresses the problem of semantic segmentation with limited cross-modality data in large-scale urban scenes. Most prior works have attempted to address this issue by using multimodal deep neural networks (DNNs). However, their ability to effectively blending different properties across multimodalities and robustly learning representations from complex scenes remains limited, particularly in the absence of sufficient and well-annotated training images. This leads to a challenge related to cross-modality learning with multimodal DNNs. To this end, we introduce two novel plug-and-play units in the network: self-generative adversarial networks (GANs) module and mutual-GANs module, to learn perturbation-insensitive feature representations and to eliminate the gap between multimodalities, respectively, yielding more effective and robust information transfer. Furthermore, a patchwise progressive training strategy is devised to enable effective network learning with limited samples. We evaluate the proposed network on two multimodal (hyperspectral and multispectral) overhead image data sets and achieve a significant improvement in comparison with several state-of-the-art methods.  © 1980-2012 IEEE.","Deep learning; Deep neural networks; Image enhancement; Semantics; Adversarial networks; Information transfers; Insensitive features; Multispectral images; Network learning; Semantic segmentation; State-of-the-art methods; Training strategy; artificial neural network; image classification; mortality; remote sensing; sampling; segmentation; spectral analysis; urban area; Image segmentation","Classification; convolutional neural network (CNN); cross modality; deep learning (DL); generative adversarial networks (GANs); hyperspectral (HS); multispectral (MS); remote sensing; semantic segmentation; single image training","Article","Final","","Scopus","2-s2.0-85099115847"
"Chen J.; Wang L.; Feng R.; Liu P.; Han W.; Chen X.","Chen, Jia (57216636841); Wang, Lizhe (23029267900); Feng, Ruyi (55853730300); Liu, Peng (57075315400); Han, Wei (57191570975); Chen, Xiaodao (35247612500)","57216636841; 23029267900; 55853730300; 57075315400; 57191570975; 35247612500","CycleGAN-STF: Spatiotemporal Fusion via CycleGAN-Based Image Generation","2021","IEEE Transactions on Geoscience and Remote Sensing","59","7","9206067","5851","5865","14","10.1109/TGRS.2020.3023432","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106728266&doi=10.1109%2fTGRS.2020.3023432&partnerID=40&md5=d1466492958b214b4630f25756363f0c","Due to the trade-off of temporal resolution and spatial resolution, spatiotemporal image-fusion uses existing high-spatial-low-temporal (HSLT) and high-temporal-low-spatial (HTLS) images as prior knowledge to reconstruct high-temporal-high-spatial (HTHS) images. However, some existing spatiotemporal image-fusion algorithms ignore the issue that the spatial information of HTLS images is insufficient to support the acquisition of spatial information, which leads to the unsatisfactory accuracy of the fusion result. To introduce more spatial information, the algorithm in this article uses Cycle-generative adversarial networks (GANs) to simulate the change process of two HSLT images at $k-1$ and $k+1$ , and to generate some simulated images between $k-1$ and $k+1$. Then, the generated images are selected under the help of HTLS images, and the selected ones are then enhanced with wavelet transform. Finally, the image with spatial information is introduced into the Flexible Spatiotemporal DAta Fusion (FSDAF) framework to improve the performance of spatiotemporal image-fusion. Extensive experiments on two real data sets demonstrate that our proposed method outperforms current state-of-the-art spatiotemporal image-fusion methods.  © 1980-2012 IEEE.","Economic and social effects; Image compression; Image enhancement; Wavelet transforms; Adversarial networks; Image generations; Spatial informations; Spatial resolution; Spatio-temporal data; Spatio-temporal fusions; Spatiotemporal images; Temporal resolution; algorithm; data processing; image analysis; remote sensing; satellite data; spatiotemporal analysis; Image fusion","Generative adversarial network (GAN); Image-generation; Remote sensing; Spatiotemporal image-fusion; Wavelet transform","Article","Final","","Scopus","2-s2.0-85106728266"
"Yang X.; Wang Z.; Zhao J.; Yang D.","Yang, Xi (56124410500); Wang, Zihan (57565457300); Zhao, Jingyi (57226574507); Yang, Dong (55514964600)","56124410500; 57565457300; 57226574507; 55514964600","FG-GAN: A Fine-Grained Generative Adversarial Network for Unsupervised SAR-to-Optical Image Translation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5621211","","","","10.1109/TGRS.2022.3165371","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127771302&doi=10.1109%2fTGRS.2022.3165371&partnerID=40&md5=db45241a6842a3c1211692e6bd7ca032","Synthetic aperture radar (SAR) and optical sensing are two important means of Earth observation. SAR can be used for all-day and all-weather Earth observation, but it has the disadvantages of speckle noise and geometric distortion, which are not conducive to human eye recognition. Optical image conforms to the characteristics of human visual observation, but it is easily affected by climate and time. Therefore, to integrate the advantages of the two, researchers have carried out extensive work on SAR-to-optical (S2O) image translation. Most of the existing methods for S2O image translation are supervised and need paired training samples, limiting its large-scale application in remote sensing field. Thus, we give priority to an unsupervised S2O image translation method. Meanwhile, we find that the images generated by unsupervised methods suffer from significant detail deficiencies. To solve this problem, we propose a fine-grained generative adversarial network (FG-GAN) introducing three strategies to enhance the detailed information in generated optical images. First, we design an unbalanced generator (UBG) with complex encoder networks and relatively simple decoder networks. The complex encoder extracts abundant feature information, while the decoder obtains key details by filtering these features. Second, to match the learning ability of the generator, we present a multiscale discriminator (MSD) to enhance the discriminant ability of the network. Third, we propose a comprehensive normalization group (CNG) to promote the physical representation consistency of SAR and optical images. Extensive experiments have been conducted, and the results show that our method is superior to the state-of-the-art (SOTA) methods on both subjective and objective evaluation indicators. Moreover, our FG-GAN has a significant improvement on classification accuracy, indicating its potential in facilitating the performance of practical remote sensing tasks.  © 1980-2012 IEEE.","Adaptive optics; Complex networks; Decoding; Discriminators; Generative adversarial networks; Geometrical optics; Image enhancement; Information filtering; Pixels; Radar imaging; Remote sensing; Signal encoding; Comprehensive normalization group; Generator; Image translation; Multi-scale discriminator; Multi-scales; Normalisation; Optical distortion; Optical image; Optical imaging; Synthetic aperture radar-to-optical image translation; Unbalanced generator; accuracy assessment; artificial neural network; EOS; image analysis; satellite imagery; synthetic aperture radar; unsupervised classification; Synthetic aperture radar","Comprehensive normalization group (CNG); multiscale discriminator (MSD); synthetic aperture radar (SAR)-to-optical (S2O) image translation; unbalanced generator (UBG)","Article","Final","","Scopus","2-s2.0-85127771302"
"Li Y.; Luo X.; Wu N.; Dong X.","Li, Yue (55878683800); Luo, Xinming (57223156151); Wu, Ning (56443296900); Dong, Xintong (57202398352)","55878683800; 57223156151; 56443296900; 57202398352","The Application of Semisupervised Attentional Generative Adversarial Networks in Desert Seismic Data Denoising","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3073419","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105053287&doi=10.1109%2fLGRS.2021.3073419&partnerID=40&md5=fe36256ddeb3ce41174c4d2c02aa5c10","For imaging and interpretation, high-quality seismic data are necessary. However, noise, which is strong in field desert seismic data, inevitably diminishes the quality of the data and reduces the signal-to-noise ratio. Moreover, the effective signals and noise in field desert seismic data are mostly distributed in the low-frequency band, which leads to severe spectral aliasing. Recently, some deep learning methods have improved the quality of desert seismic data in certain aspects. However, due to limitations of their networks and the serious spectral aliasing of desert seismic data, the denoising results usually show some false seismic reflections. To solve the above problems, we introduce Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (U-GAT-IT) to the denoising of desert seismic data in a semisupervised manner. U-GAT-IT is an unsupervised attentional generative adversarial network (GAN) combined with an attention module guided by the class activation map (CAM). The attention module guided by the CAM can guide the model to better distinguish between noise and effective signals. The experiment shows that the U-GAT-IT can effectively suppress desert seismic noise. Also, the denoising result has fewer false seismic reflections.  © 2004-2012 IEEE.","Deep learning; Geophysical prospecting; Landforms; Learning systems; Network layers; Seismic response; Signal to noise ratio; Adversarial networks; Image translation; Learning methods; Low frequency band; Seismic noise; Seismic reflections; Semi-supervised; Spectral aliasing; artificial neural network; remote sensing; seismic data; unsupervised classification; Seismic waves","Class activation map (CAM); deep learning (DL); desert seismic data; generative adversarial networks (GANs)","Article","Final","","Scopus","2-s2.0-85105053287"
"Zheng Z.; Zhong Y.; Su Y.; Ma A.","Zheng, Zhendong (57200327454); Zhong, Yanfei (12039673900); Su, Yu (57216916135); Ma, Ailong (55972916000)","57200327454; 12039673900; 57216916135; 55972916000","Domain Adaptation via a Task-Specific Classifier Framework for Remote Sensing Cross-Scene Classification","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5620513","","","","10.1109/TGRS.2022.3151689","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124839588&doi=10.1109%2fTGRS.2022.3151689&partnerID=40&md5=3f76c8a2607622a75e922678f354062f","The scene classification of high spatial resolution (HSR) imagery involves labeling an HSR image with a specific high-level semantic class according to the composition of the semantic objects and their spatial relationships. As such, scene classification has attracted increased attention in recent years, and many different algorithms have now been proposed for the cross-scene classification task. However, the recently proposed scene classification methods based on deep convolutional neural networks (CNNs) still suffer from domain shift problems, because of the training data and validation data not following the assumption of independent and identical distributions. The employment of generative adversarial networks has been found to be an effective way to bridge the domain shift/gap. However, the existing cross-scene classification methods do not use the classification information in the target domain, and the domain classifier is task-independent for different scene classification tasks. In this article, to solve this problem, domain adaptation via a task-specific classifier (DATSNET) framework is proposed for HSR image scene classification. Task-specific classifiers and minimizing and maximizing, ' i.e., minimaxing,' of the classifier discrepancy are integrated in the DATSNET framework. The task-specific classifiers are proposed to align the distributions of the source domain features and target domain features by utilizing task-specific decision boundaries in the target domain. In order to align the two task-specific classifiers' feature distributions, minimaxing the defined discrepancy between the different classifiers in an adversarial manner is proposed to obtain better task-specific classifier boundaries in the target domain and a better-aligned feature distribution in both domains. The experimental results obtained with different remote sensing cross-scene classification tasks demonstrate that the proposed method achieves a significantly improved performance compared with the other state-of-the-art remote sensing cross-scene classification algorithms.  © 1980-2012 IEEE.","Classification (of information); Convolution; Deep neural networks; Generative adversarial networks; Image analysis; Image classification; Job analysis; Remote sensing; Semantic Web; Adaptation models; Convolutional neural network; Discrepancy; Domain adaptation; Features extraction; Image-analysis; Remote-sensing; Scene classification; Task analysis; Task-specific; artificial neural network; image classification; remote sensing; satellite imagery; semantic standardization; spatial resolution; Semantics","Convolutional neural network (CNN); discrepancy; domain adaptation; scene classification; task specific","Article","Final","","Scopus","2-s2.0-85124839588"
"Mateo-García G.; Laparra V.; Requena-Mesa C.; Gómez-Chova L.","Mateo-García, Gonzalo (57192947904); Laparra, Valero (34869963500); Requena-Mesa, Christian (57208244647); Gómez-Chova, Luis (6603354695)","57192947904; 34869963500; 57208244647; 6603354695","Generative adversarial networks in the geosciences","2021","Deep Learning for the Earth Sciences: A Comprehensive Approach to Remote Sensing, Climate Science and Geosciences","","","","24","36","12","10.1002/9781119646181.ch3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148111402&doi=10.1002%2f9781119646181.ch3&partnerID=40&md5=fdbb0a195b63cf5615570494c79ed6c0","One of the most exciting trends in machine learning nowadays is the use of deep networks to learn density properties. In this direction, the field of generative models has become a hot topic in deep learning, and different approaches have been proposed. In this chapter, we are going to overview some of the generative models based on deep learning and focus on one of the most used ones: the Generative Adversarial Networks (GANs). We review the main different families of GANs, and how these families have been applied to remote sensing problems. Finally, in the last section, we illustrate the use of GANs in remote sensing problems with two different applications. © 2021 John Wiley & Sons Ltd. All rights reserved.","","Deep learning; Earth observation images; Generative adversarial networks; Geosciences applications; Remote sensing","Book chapter","Final","","Scopus","2-s2.0-85148111402"
"Zhang Z.; Zhou Q.; Xu Y.; Ma L.; Iwasaki A.","Zhang, Zhaoxiang (57191228269); Zhou, Qing (57292691400); Xu, Yuelei (34878145700); Ma, Linhua (57671169100); Iwasaki, Akira (8073280000)","57191228269; 57292691400; 34878145700; 57671169100; 8073280000","REMOTE SENSING IMAGE JITTER RESTORATION BASED ON DEEP GENERATIVE ADVERSARIAL NETWORK","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","2683","2686","3","10.1109/IGARSS47720.2021.9554491","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126342892&doi=10.1109%2fIGARSS47720.2021.9554491&partnerID=40&md5=bd2f12b83ea28c4cdbb3f8c3f4f56527","High stability of the observation satellite platform is increasingly demanded in recent years. However, attitude jitter of observation satellites is a problem that degenerates the development of imaging quality and resolution. In order to reduce the geo-positioning errors and improve the geometric accuracy of remote sensing images, satellite jitter have been studied in recent years. In this work, a generative adversarial network (GAN) architecture is proposed to automatically learn and correct the deformed scene features from a single remote sensing image. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. In order to explore the usefulness and effectiveness of GAN for jitter detection, the proposed GAN are trained on part of PatternNet dataset and tested on three popular remote sensing datasets. Several experiments show that the proposed models provide competitive results compared to other methods. the proposed GAN reveals the huge potential of GAN-based methods for the analysis of attitude jitter from remote sensing images. © 2021 IEEE","Convolutional neural networks; Deep learning; Image enhancement; Image reconstruction; Jitter; Remote sensing; Satellites; Convolutional neural network; Deep learning; Imaging quality; Imaging resolutions; Jitter estimations; Jitter restoration; Observation satellites; Remote sensing images; Remote-sensing; Satellite platforms; Generative adversarial networks","deep learning; GAN; jitter estimation; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85126342892"
"Koumoutsou D.; Siolas G.; Charou E.; Stamou G.","Koumoutsou, Dimitra (57219025787); Siolas, Georgios (6506228575); Charou, Eleni (6507509159); Stamou, Georgios (7004137698)","57219025787; 6506228575; 6507509159; 7004137698","Generative Adversarial Networks for Data Augmentation in Hyperspectral Image Classification","2022","Intelligent Systems Reference Library","217","","","115","144","29","10.1007/978-3-030-91390-8_6","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124543775&doi=10.1007%2f978-3-030-91390-8_6&partnerID=40&md5=2bea13d979f0cc1beead10a36b4c87ed","Hyperspectral Imaging (HSI), or imaging spectroscopy, is an imaging method with numerous applications. Unlike conventional images, hyperspectral data contain information collected in narrow wavelength intervals at hundreds of bands across the electromagnetic spectrum. The resulting image has a high spatial and spectral resolution allowing for multiple features to be extracted from machine learning applications. However, HSI acquisition is expensive, resulting in low data availability. Especially in the field of remote sensing, hyperspectral datasets only have a limited number of labeled samples, and significant class imbalances are common. Therefore, synthetic samples are useful in complementing existing datasets used in classification tasks. To this end, a modification of a Gradient Penalty Wasserstein Generative Adversarial Network (WGAN-GP) is proposed for conditional generation of realistic hyperspectral data cubes that refrains from commonly used computationally intense model architectures. Dimensionality reduction is introduced as a preprocessing step that further reduces complexity while retaining only important information. The efficacy of the model is proven by verifying the similarity of the synthetic to the real samples, evaluated by comparing their spectral information and their performance on classification outcomes using spatio-spectral models. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","","","Book chapter","Final","","Scopus","2-s2.0-85124543775"
"Bashir S.M.A.; Wang Y.","Bashir, Syed Muhammad Arsalan (56385198200); Wang, Yi (12763268500)","56385198200; 12763268500","Small object detection in remote sensing images with residual feature aggregation-based super-resolution and object detector network","2021","Remote Sensing","13","9","1854","","","","10.3390/rs13091854","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106517229&doi=10.3390%2frs13091854&partnerID=40&md5=98b765460da6dde1bfa9f0ccc129d0ab","This paper deals with detecting small objects in remote sensing images from satellites or any aerial vehicle by utilizing the concept of image super-resolution for image resolution enhancement using a deep-learning-based detection method. This paper provides a rationale for image super-resolution for small objects by improving the current super-resolution (SR) framework by incorporating a cyclic generative adversarial network (GAN) and residual feature aggregation (RFA) to improve detection performance. The novelty of the method is threefold: first, a framework is proposed, independent of the final object detector used in research, i.e., YOLOv3 could be replaced with Faster R-CNN or any object detector to perform object detection; second, a residual feature aggregation network was used in the generator, which significantly improved the detection performance as the RFA network detected complex features; and third, the whole network was transformed into a cyclic GAN. The image super-resolution cyclic GAN with RFA and YOLO as the detection network is termed as SRCGAN-RFA-YOLO, which is compared with the detection accuracies of other methods. Rigorous experiments on both satellite images and aerial images (ISPRS Potsdam, VAID, and Draper Satellite Image Chronology datasets) were performed, and the results showed that the detection performance increased by using super-resolution methods for spatial resolution enhancement; for an IoU of 0.10, AP of 0.7867 was achieved for a scale factor of 16. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aircraft detection; Antennas; Deep learning; Feature extraction; Image enhancement; Image resolution; Object recognition; Optical resolving power; Remote sensing; Small satellites; Adversarial networks; Detection performance; Image resolution enhancements; Image super resolutions; Remote sensing images; Small object detection; Spatial-resolution enhancement; Superresolution methods; Object detection","Deep learning; Generative adversarial networks; Image classification; Object detection in satellite images; Remote sensing; Residual feature aggregation (RFA); Vehicle detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106517229"
"Wu M.; Jin X.; Jiang Q.; Lee S.-J.; Liang W.; Lin G.; Yao S.","Wu, Min (57195406241); Jin, Xin (56991832300); Jiang, Qian (57194699462); Lee, Shin-jye (34877262700); Liang, Wentao (57218670069); Lin, Guo (57218671417); Yao, Shaowen (24473851600)","57195406241; 56991832300; 57194699462; 34877262700; 57218670069; 57218671417; 24473851600","Remote sensing image colorization using symmetrical multi-scale DCGAN in YUV color space","2021","Visual Computer","37","7","","1707","1729","22","10.1007/s00371-020-01933-2","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089990039&doi=10.1007%2fs00371-020-01933-2&partnerID=40&md5=8fb5bead7ec60d9f33907b7c672d3d9f","Image colorization technique is used to colorize the gray-level image or single-channel image, which is a very significant and challenging task in image processing, especially the colorization of remote sensing images. This paper proposes a new method for coloring remote sensing images based on deep convolution generation adversarial network. The adopted generator model is a symmetrical structure using the principle of auto-encoder, and a multi-scale convolutional module is specially designed to introduce into the generator model. Thus, the proposed generator can enable the whole model to retain more image features in the process of up-sampling and down-sampling. Meanwhile, the discriminator uses residual neural network 18 that can compete with the generator, so that the generator and discriminator can effectively optimize each other. In the proposed method, the color space transformation technique is first utilized to convert remote sensing images from RGB to YUV. Then, the Y channel (a gray-level image) is used as the input of the neural network model to predict UV channels. Finally, the predicted UV channels are concatenated with the original Y channel as a whole YUV that is then transformed into RGB space to get the final color image. Experiments are conducted to test the performance of different image colorization methods, and the results show that the proposed method has good performance in both visual quality and objective indexes on the colorization of remote sensing image. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Color; Convolution; Neural networks; Remote sensing; Signal sampling; Space optics; Adversarial networks; Color space transformation; Generator modeling; Image colorizations; Neural network model; Remote sensing images; Symmetrical structure; Visual qualities; Color image processing","Deep convolutional generative adversarial networks; Image colorization; Multi-scale convolutional; Remote sensing image","Article","Final","","Scopus","2-s2.0-85089990039"
"Zhao D.; Yuan B.; Gao Y.; Qi X.; Shi Z.","Zhao, Danpei (24082070800); Yuan, Bo (57222962760); Gao, Yue (57221270916); Qi, Xinhu (57322823500); Shi, Zhenwei (23398841900)","24082070800; 57222962760; 57221270916; 57322823500; 23398841900","UGCNet: An Unsupervised Semantic Segmentation Network Embedded with Geometry Consistency for Remote-Sensing Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3129776","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123509328&doi=10.1109%2fLGRS.2021.3129776&partnerID=40&md5=3068d41709dc091ef1a6b27dfa1f00ad","In remote-sensing image (RSI) semantic segmentation, the dependence on large-scale and pixel-level annotated data has been a critical factor restricting its development. In this letter, we propose an unsupervised semantic segmentation network embedded with geometry consistency (UGCNet) for RSIs, which imports the adversarial-generative learning strategy into a semantic segmentation network. The proposed UGCNet can be trained on a source-domain dataset and achieve accurate segmentation results on a different target-domain dataset. Furthermore, for refining the remote-sensing target geometric representation such as densely distributed buildings, we propose a geometry-consistency (GC) constraint that can be embedded in both image-domain adaptation process and semantic segmentation network. Therefore, our model could achieve cross-domain semantic segmentation with target geometric property preservation. The experimental results on Massachusetts and Inria buildings datasets prove that the proposed unsupervised UGCNet could achieve a very comparable segmentation accuracy with the fully supervised model, which validates the effectiveness of the proposed method.  © 2004-2012 IEEE.","Generative adversarial networks; Remote sensing; Semantic Segmentation; Semantics; Adversarial learning; Generative-adversarial learning; Geometry consistency; Image semantics; Large-scales; Remote sensing images; Remote-sensing image; Scale levels; Semantic segmentation; Unsupervised; geometry; network analysis; satellite imagery; segmentation; unsupervised classification; Geometry","Generative-adversarial learning; geometry consistency (GC); remote-sensing images (RSIs); semantic segmentation; unsupervised","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123509328"
"Farooq M.; Dailey M.N.; Mahmood A.; Moonrinta J.; Ekpanyapong M.","Farooq, Muhammad (57212081048); Dailey, Matthew N. (56187964300); Mahmood, Arif (55636036300); Moonrinta, Jednipat (36460177500); Ekpanyapong, Mongkol (6506112110)","57212081048; 56187964300; 55636036300; 36460177500; 6506112110","Human face super-resolution on poor quality surveillance video footage","2021","Neural Computing and Applications","33","20","","13505","13523","18","10.1007/s00521-021-05973-0","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104103307&doi=10.1007%2fs00521-021-05973-0&partnerID=40&md5=10c866b5f78fe21082b5f3990b8d1892","Most super-resolution (SR) methods proposed to date do not use real ground-truth high-resolution (HR) and low-resolution (LR) image pairs; instead, the vast majority of methods use synthetic LR images generated from the HR images. This approach yields excellent performance on synthetic datasets, but on real-world poor quality surveillance video footage, they suffer from performance degradation. A promising alternative is to apply recent advances in style transfer for unpaired datasets, but state-of-the-art work along these lines has used LR images and HR images from completely different datasets, introducing more variation between the HR and LR domains than necessary. In this paper, we propose methods that overcome both of these shortcomings, applying unpaired style transfer learning methods to face SR but using HR and LR datasets that share important properties. The key is to acquire roughly paired training data from a high-quality main stream and a lower-quality sub-stream of the same IP camera. Based on this principle, we have constructed four datasets comprising more than 400 people, with 1–15 weakly aligned real HR–LR pairs for each subject. We adopt a cycle generative adversarial networks (Cycle GANs) approach that produces impressive super-resolved images for low-quality test images never seen during training. Experiments prove the efficacy of the method. The approach to face SR advocated for in this paper makes possible many real-world applications requiring the extraction of high-quality face images from low-resolution video streams such as those produced by security cameras. Developers of diverse applications such as face recognition, 3D face reconstruction, face alignment, face parsing, human–computer interaction, remote sensing, and access control will benefit from the methods introduced in this work. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Access control; Cameras; Data streams; Formal languages; Human computer interaction; Learning systems; Optical resolving power; Remote sensing; Security systems; Transfer learning; 3D face reconstruction; Adversarial networks; Computer interaction; Diverse applications; Low resolution images; Low resolution video; Performance degradation; Transfer learning methods; Face recognition","Cycle GANs; Face hallucination; Super-resolution","Article","Final","","Scopus","2-s2.0-85104103307"
"Li J.; Sun B.; Li S.; Kang X.","Li, Jiahao (57376164600); Sun, Bin (57188835664); Li, Shutao (7409240361); Kang, Xudong (47061561600)","57376164600; 57188835664; 7409240361; 47061561600","Semisupervised Semantic Segmentation of Remote Sensing Images with Consistency Self-Training","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3134277","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121399302&doi=10.1109%2fTGRS.2021.3134277&partnerID=40&md5=d39f1aa2684af5b9f83de89143e37fce","Semisupervised semantic segmentation is an effective way to reduce the expensive manual annotation cost and take advantage of the unlabeled data for remote sensing (RS) image interpretation. Recent related research has mainly adopted two strategies: Self-training and consistency regularization. Self-training tries to acquire accurate pseudo-labels to explicitly expand the train set. However, the existing methods cannot accurately identify false pseudo-labels, suffering from their negative impact on model optimization. The consistency regularization constrains the model by producing consistent predictions robust to the perturbations introduced in the sample or feature domain but requires a sufficient number of training data. Therefore, we propose a strategy for the semisupervised semantic segmentation of the RS images. The proposed model in the generative adversarial network (GAN) framework is optimized by consistency self-training, learning the distributions of both labeled and unlabeled data. The discriminator is optimized by accurate pixel-level training labels instead of the image-level ones, thereby assessing the confidence for the prediction of each pixel, which is then used to reweight the loss of the unlabeled data in self-training. The generator is optimized with the consistency constraint with respect to all random perturbations on the unlabeled data, which increases the sample diversity and prompts the model to learn the underlying distribution of the unlabeled data. Experimental results on the the large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) datasets and the International Society for Photogrammetry and Remote Sensing (ISPRS) datasets show that our framework outperforms several state-of-the-art semisupervised semantic segmentation methods.  © 2022 IEEE.","Generative adversarial networks; Image enhancement; Perturbation techniques; Pixels; Remote sensing; Semantic Segmentation; Semantic Web; Supervised learning; Consistency self-training; Images segmentations; Perturbation method; Predictive models; Regularisation; Remote sensing images; Self-training; Semantic segmentation; Semi-supervised; Unlabeled data; remote sensing; satellite imagery; segmentation; semantic standardization; supervised learning; Semantics","Consistency self-training; generative adversarial network (GAN); remote sensing (RS) image; semantic segmentation; semisupervised learning","Article","Final","","Scopus","2-s2.0-85121399302"
"Huang A.; Shen R.; Di W.; Han H.","Huang, Anqi (57207766452); Shen, Runping (8585826800); Di, Wenli (57383310900); Han, Huimin (57383527500)","57207766452; 8585826800; 57383310900; 57383527500","A methodology to reconstruct LAI time series data based on generative adversarial network and improved Savitzky-Golay filter","2021","International Journal of Applied Earth Observation and Geoinformation","105","","102633","","","","10.1016/j.jag.2021.102633","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121621791&doi=10.1016%2fj.jag.2021.102633&partnerID=40&md5=271b430908a2cb23fa84696dd1fa873b","High-quality leaf area index (LAI) data is essential for regional and global ecology, climate and environment research. However, there are still many quality problems in the continuity of current LAI time series products. Here we developed a new comprehensive three-step reconstruction method (GANSG) for satellite-retrieved LAI time series data based on generative adversarial network, improving the Savitzky-Golay (S-G) filter and median absolute deviation filter. We applied GANSG to the reconstruction of MODIS LAI data in China from 2001 to 2019. The reconstruction results show that the new method based on the unsupervised deep learning framework has an advantage in interpolating low-quality LAI with high precision. The new method can better retain high-quality pixel information to smoothen the interpolated LAI time series by improving the traditional S-G filter. Compared with the five other methods, including the adaptive S-G filter, double logistic, asymmetric Gaussian, modified temporal spatial filter and spatial temporal S-G filter, qualitative analysis showed the new method has a more resilient ability to handle the continuous loss of high-quality pixels and identify the phenological features of biomes. Quantitative analysis based on station observation showed that the new method performs best among the other three methods, with the optimal correlation coefficient of 0.84 relative to station observation and the lowest root mean square error of 0.71 m2/m2. © 2021","China; correlation; database; filter; leaf area index; machine learning; MODIS; pixel; qualitative analysis; remote sensing; time series analysis","Data reconstruction; Generative adversarial network; Leaf area index; Remote sensing; Time series","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121621791"
"Fu K.; Sun X.; Qiu X.; Diao W.; Yan Z.; Huang L.; Yu H.","Fu, Kun (7202283802); Sun, Xian (34875643000); Qiu, Xiaolan (18435166800); Diao, Wenhui (56816620400); Yan, Zhiyuan (57202787748); Huang, Lijia (55492438700); Yu, Hongfeng (57207947647)","7202283802; 34875643000; 18435166800; 56816620400; 57202787748; 55492438700; 57207947647","Multi-satellite integrated processing and analysis method under remote sensing big data; [遥感大数据条件下多星一体化处理与分析]","2021","National Remote Sensing Bulletin","25","3","","691","707","16","10.11834/jrs.20211058","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105418852&doi=10.11834%2fjrs.20211058&partnerID=40&md5=d1f34df152f560f0b8226d6ffb9b23ad","With the rapid development of aerospace technology and the continuous increase of China's high resolution earth observation data acquisition in the past few decades, the era of remote sensing big data has coming now. Developing multi-satellite integrated data processing and application technology has become an important trend. In this paper, we systematically review the technological development process from two aspects, including multi-satellite imaging processing and multi-element information extraction. Then, we analyze the advantages and characteristics of the existing cutting-edge methods, and points out the main challenge of establishing the integrated processing model for imaging processing field and efficient learning model for information extraction field. On this basis, as well as according to the practical application requirements, a novel method of multi-satellite integrated processing and analysis under remote sensing big data is proposed in this paper. We emphatically define the basic concepts, scientific problems, research ideas and solutions of this method. On one hand, in terms of multi-satellite imaging processing, aiming at the problem of difficult estimation of high-dimensional coupled imaging parameters at high resolution, the various errors caused by the whole cycles of electromagnetic waves are calculated quantitatively, including payload error, platform error, data transmission error, atmospheric influence and so on. Then a multi-satellite integrated imaging processing physical model based on the Generative Adversarial Networks (GAN) is constructed to approximate estimate imaging parameters. In this way, we can achieve high-precision geometric corrections and radiation corrections, as well as generate high-quality remote sensing image products. On the other hand, in terms of multi-element information extraction, to solve the problem that the accuracy of the original tasks is difficult to maintain due to the addition of new tasks and requires full sample retraining, we design a multi-task feature sharing model based on few-shot incremental learning. It has a novel memory retention unit and multi-modal joint optimization of convex non-negative matrix factors. Through this model, we can generate parallel and high-precision annotations for multiple objects. Compared with the existing methods, the information between different satellites and payloads, different missions and objects are complementary to each other, leading to the simultaneous improvement of multi-sensor imaging quality and object extraction accuracy. The specific technical approach and preliminary experimental verification are given in detail in this paper. Experiments on multi-satellite imaging processing show that our method can effectively estimate multiple imaging errors. The phase estimation accuracy is within 1 degree when the signal-to-noise ratio is -5dB, which indicates the good performance even under low signal-to-noise ratio condition. At the same time, experiments on multi-element information extraction show that compared to single-modal method, our method, which uses multi-modal data in combination and embedded the novel memory retention unit, improve the extraction accuracy in multi-tasks of object detection and semantic segmentation. In the future, we will pay more attention to the disconnection problem between multi-satellite imaging processing and multi-element information extraction in the field of remote sensing. By establishing a benign mutual feedback mechanism between these two procedures to maximize the benefits of the remote sensing big data. © 2021, Science Press. All right reserved.","Big data; Cutting; Data acquisition; Data handling; Electromagnetic waves; Errors; Image segmentation; Information retrieval; Learning systems; Modal analysis; Object detection; Satellites; Semantics; Signal to noise ratio; Aerospace technologies; Application requirements; Application technologies; Earth observation data; Experimental verification; Low signal-to-noise ratio; Remote sensing images; Technological development; Remote sensing","Generative Adversarial Networks(GAN); Multi-element information analysis; Multi-satellite imaging processing; Multi-satellite integrated; Multi-task; Remote sensing big data","Article","Final","","Scopus","2-s2.0-85105418852"
"Wang J.; Shao Z.; Huang X.; Lu T.; Zhang R.; Ma J.","Wang, Jiaming (57206676342); Shao, Zhenfeng (57203905559); Huang, Xiao (57201292422); Lu, Tao (56406646300); Zhang, Ruiqian (57190385256); Ma, Jiayi (26638975600)","57206676342; 57203905559; 57201292422; 56406646300; 57190385256; 26638975600","Enhanced image prior for unsupervised remoting sensing super-resolution","2021","Neural Networks","143","","","400","412","12","10.1016/j.neunet.2021.06.005","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109071626&doi=10.1016%2fj.neunet.2021.06.005&partnerID=40&md5=70dcc8ff5c43455549965e9d7e5d66ca","Numerous approaches based on training low-high resolution image pairs have been proposed to address the super-resolution (SR) task. Despite their success, low-high resolution image pairs are usually difficult to obtain in certain scenarios, and these methods are limited in the actual scene (unknown or non-ideal image acquisition process). In this paper, we proposed a novel unsupervised learning framework, termed Enhanced Image Prior (EIP), which achieves SR tasks without low/high resolution image pairs. We first feed random noise maps into a designed generative adversarial network (GAN) for satellite image SR reconstruction. Then, we convert the reference image to latent space as the enhanced image prior. Finally, we update the input noise in the latent space with a recurrent updating strategy, and further transfer the texture and structured information from the reference image. Results on extensive experiments on the Draper dataset show that EIP achieves significant improvements over state-of-the-art unsupervised SR methods both quantitatively and qualitatively. Our experiments on satellite (SuperView-1) images reveal the potential of the proposed approach in improving the resolution of remote sensing imagery compared with the supervised algorithms. Source code is available at https://github.com/jiaming-wang/EIP. © 2021 Elsevier Ltd","Algorithms; Image Processing, Computer-Assisted; Image enhancement; Optical resolving power; Remote sensing; Textures; Unsupervised learning; High-resolution images; Ideal images; Image pairs; Image priors; Latent space; Low-high; Nonideal; Prior enhancement; Reference image; Super resolution; algorithm; article; human; human experiment; learning; noise; quantitative analysis; satellite imagery; image processing; Satellite imagery","Latent space; Prior enhancement; Satellite imagery; Super resolution; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85109071626"
"Bejiga M.B.; Hoxha G.; Melgani F.","Bejiga, Mesay Belete (57192697078); Hoxha, Genc (57213198314); Melgani, Farid (35613488300)","57192697078; 57213198314; 35613488300","Improving Text Encoding for Retro-Remote Sensing","2021","IEEE Geoscience and Remote Sensing Letters","18","4","9066830","622","626","4","10.1109/LGRS.2020.2983851","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103405336&doi=10.1109%2fLGRS.2020.2983851&partnerID=40&md5=191c3564bbabf57658d20c378cee4ded","A recent work on retro-remote sensing (converting ancient text descriptions into images) was proposed using a multilabel encoding scheme in which an input text description is represented by a binary vector indicating the presence or absence of specific objects. However, this kind of encoding disregards information such as object attributes and spatial relationship between multiple objects in a description, resulting in images that do not semantically (fully) conform to the input description. In this letter, we propose an improved text-encoding mechanism that takes into account different levels of information available from an input text. The encoded text is then used as conditional information to guide the image synthesis process using generative adversarial networks (GANs). Besides, we present a modified GAN architecture intending to improve the semantic content of the generated images. Both the qualitative and quantitative results obtained indicate that the proposed method is particularly promising. © 2004-2012 IEEE.","Encoding (symbols); Image coding; Remote sensing; Semantics; Signal encoding; Adversarial networks; Encoding schemes; Image synthesis; Multiple objects; Object attributes; Quantitative result; Semantic content; Spatial relationships; image analysis; remote sensing; satellite imagery; spatial analysis; Image enhancement","Generative adversarial networks (GANs); multimodal learning; retro-remote sensing; text-To-image synthesis","Article","Final","","Scopus","2-s2.0-85103405336"
"Tian L.; Cao Y.; He B.; Zhang Y.; He C.; Li D.","Tian, Ling (57199646622); Cao, Yu (57222756910); He, Bokun (57205363676); Zhang, Yifan (56292472600); He, Chu (12345438500); Li, Deshi (13612901300)","57199646622; 57222756910; 57205363676; 56292472600; 12345438500; 13612901300","Image enhancement driven by object characteristics and dense feature reuse network for ship target detection in remote sensing imagery","2021","Remote Sensing","13","7","1327","","","","10.3390/rs13071327","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104036743&doi=10.3390%2frs13071327&partnerID=40&md5=83aba09669387c3028220cf881798606","As the application scenarios of remote sensing imagery (RSI) become richer, the task of ship detection from an overhead perspective is of great significance. Compared with traditional methods, the use of deep learning ideas has more prospects. However, the Convolutional Neural Network (CNN) has poor resistance to sample differences in detection tasks, and the huge differences in the image environment, background, and quality of RSIs affect the performance for target detection tasks; on the other hand, upsampling or pooling operations result in the loss of detailed information in the features, and the CNN with outstanding results are often accompanied by a high computation and a large amount of memory storage. Considering the characteristics of ship targets in RSIs, this study proposes a detection framework combining an image enhancement module with a dense feature reuse module: (1) drawing on the ideas of the generative adversarial network (GAN), we designed an image enhancement module driven by object characteristics, which improves the quality of the ship target in the images while augmenting the training set; (2) the intensive feature extraction module was designed to integrate low-level location information and high-level semantic information of different resolutions while minimizing the computation, which can improve the efficiency of feature reuse in the network; (3) we introduced the receptive field expansion module to obtain a wider range of deep semantic information and enhance the ability to extract features of targets were at different sizes. Experiments were carried out on two types of ship datasets, optical RSI and Synthetic Aperture Radar (SAR) images. The proposed framework was implemented on classic detection networks such as You Only Look Once (YOLO) and Mask-RCNN. The experimental results verify the effectiveness of the proposed method. © 2021 by the authors.","Convolutional neural networks; Deep learning; Feature extraction; Object detection; Remote sensing; Semantics; Ships; Synthetic aperture radar; Adversarial networks; Application scenario; Different resolutions; High level semantics; Object characteristics; Remote sensing imagery; Semantic information; Synthetic aperture radar (SAR) images; Image enhancement","Deep learning; Feature reuse; Image enhancement; Remote sensing imagery; Ship target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85104036743"
"Tan Z.; Gao M.; Yuan J.; Jiang L.; Duan H.","Tan, Zhenyu (56421169400); Gao, Meiling (57191226894); Yuan, Jun (57432147700); Jiang, Liangcun (55946431600); Duan, Hongtao (22233326100)","56421169400; 57191226894; 57432147700; 55946431600; 22233326100","A Robust Model for MODIS and Landsat Image Fusion Considering Input Noise","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5407217","","","","10.1109/TGRS.2022.3145086","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123713287&doi=10.1109%2fTGRS.2022.3145086&partnerID=40&md5=2065674a5485a003856a97624774f004","Significant progress has been made in spatiotemporal fusion for remote sensing images; however, most models require inputs to be free of clouds and without missing data, considerably confining their applications in practice. Due to recent advances in deep learning technologies, powerful modeling capabilities could be leveraged to bring potential solutions to this problem. This article proposes a novel architecture named the robust spatiotemporal fusion network (RSFN) based on the generative adversarial network and attention mechanism with dual temporal references to automatically handle input noise. The RSFN only needs one coarse-resolution image on the prediction date and two referential fine-resolution images before and after the prediction date as model inputs. Most notably, there is no special restriction attached on the data quality of referential images. The comparison with other models demonstrates the effectiveness of the RSFN model quantitatively and visually in four study areas using MODIS and Landsat images. Two main conclusions can draw from the experiments. First, the input data noise hardly affects the prediction results of the RSFN, and the RSFN can gain a comparable or even higher accuracy; conversely, the other methods only show limited resistance to input noise. Second, the RSFN with cloud-contaminated references outperforms the other models with cloud-free references after data filtering in the same study area during the same period. The satellite data quality usually varies significantly; the model robustness and fault tolerance are considered critical for actual applications. The RSFN is a simple end-to-end deep model with high accuracy and fault tolerance designed for spatiotemporal fusion with imperfect data inputs, showing promising prospects in practical applications. © 1980-2012 IEEE.","Biological systems; Deep learning; Fault tolerance; Forecasting; Generative adversarial networks; Image fusion; Radiometers; Biological system modeling; Deep learning; Noisy data; Remote-sensing; Robust spatiotemporal fusion network; Robustness; Spatial resolution; Spatio-temporal fusions; Spatiotemporal; Spatiotemporal phenomenon; data acquisition; data quality; Landsat; MODIS; remote sensing; satellite data; satellite imagery; Remote sensing","Data fusion; deep learning; noisy data; robust spatiotemporal fusion network (RSFN); robustness; spatiotemporal","Article","Final","","Scopus","2-s2.0-85123713287"
"Soto Vega P.J.; Costa G.A.O.P.D.; Feitosa R.Q.; Ortega Adarme M.X.; Almeida C.A.D.; Heipke C.; Rottensteiner F.","Soto Vega, Pedro Juan (57260423900); Costa, Gilson Alexandre Ostwald Pedro da (25642386000); Feitosa, Raul Queiroz (6602453684); Ortega Adarme, Mabel Ximena (57195569092); Almeida, Claudio Aparecido de (57209845177); Heipke, Christian (7004264889); Rottensteiner, Franz (6506388437)","57260423900; 25642386000; 6602453684; 57195569092; 57209845177; 7004264889; 6506388437","An unsupervised domain adaptation approach for change detection and its application to deforestation mapping in tropical biomes","2021","ISPRS Journal of Photogrammetry and Remote Sensing","181","","","113","128","15","10.1016/j.isprsjprs.2021.08.026","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115024669&doi=10.1016%2fj.isprsjprs.2021.08.026&partnerID=40&md5=f47009708b5533f0cd08aee85dc7d374","Changes in environmental conditions, geographical variability and different sensor properties typically make it almost impossible to employ previously trained classifiers for new data without a significant drop in classification accuracy. Domain adaptation (DA) techniques been proven useful to alleviate that problem. In particular, appearance adaptation techniques may be used to adapt images from a specific dataset in such a way that the generated images have a style that is similar to the images from another dataset. Such techniques are, however, prone to creating artifacts that hinder proper classification of the adapted images. In this work we propose an unsupervised DA approach for change detection tasks, which is based on a particular appearance adaptation method: the Cycle-Consistent Generative Adversarial Network (CycleGAN). Specifically, we extend that method by introducing additional constraints in the training phase of the model components, which make it preserve the semantic structure and class transitions in the adapted images. We evaluate the proposed approach on a deforestation detection application, considering different sites in the Amazon rain-forest and in the Brazilian Cerrado (savanna) using Landsat-8 images. In the experiments, each site corresponds to a domain, and the accuracy of a classifier trained with images and references from one (source) domain is measured in the classification of another (target) domain. The results show that the proposed approach is successful in producing artifact-free adapted images, which can be satisfactory classified by the pre-trained source classifiers. On average, the accuracies achieved in the classification of the adapted images outperformed the baselines (when no adaptation was made) by 7.1% in terms of mean average precision, and 9.1% in terms of F1-Score. To the best of our knowledge, the proposed method is the first unsupervised domain adaptation approach devised for change detection. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Amazonia; Classification (of information); Deep learning; Deforestation; Image classification; Remote sensing; Semantics; Tropics; Adaptation techniques; Change detection; Cycle-consistent generative adversarial network; Deep learning; Deforestation detection; Domain adaptation; Environmental conditions; Geographical variability; ITS applications; Remote-sensing; adaptation; climate change; data set; deforestation; detection method; geographical variation; Landsat; satellite imagery; sensor; Generative adversarial networks","Change detection; CycleGAN; Deep learning; Deforestation detection; Domain adaptation; Remote sensing","Article","Final","","Scopus","2-s2.0-85115024669"
"Gupta N.; Srivastava H.S.; Sivasankar T.; Patel P.","Gupta, Neeharika (57772595200); Srivastava, Hari Shanker (7102601428); Sivasankar, Thota (57202915146); Patel, Parul (57208689231)","57772595200; 7102601428; 57202915146; 57208689231","A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","2021","2021 IEEE India Geoscience and Remote Sensing Symposium, InGARSS 2021 - Proceedings","","","","488","491","3","10.1109/InGARSS51564.2021.9792062","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133166743&doi=10.1109%2fInGARSS51564.2021.9792062&partnerID=40&md5=f88fc43ad6c46be0b50dbe7a29230cc1","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.  © 2021 IEEE.","Deep learning; Generative adversarial networks; Geometrical optics; Optical remote sensing; Principal component analysis; Radar imaging; Satellite imagery; Space optics; Space-based radar; Synthetic aperture radar; Deep learning; Gradient feature; Learning frameworks; Optical image; Optical satellite imagery; Optical-; Remote sensing images; SAR Images; Single images; Source images; Image fusion","deep learning; GAN; image fusion; optical; SAR","Conference paper","Final","","Scopus","2-s2.0-85133166743"
"Chattopadhyay S.; Kak A.C.","Chattopadhyay, Somrita (56511015000); Kak, Avinash C. (7005399628)","56511015000; 7005399628","Uncertainty, Edge, and Reverse-Attention Guided Generative Adversarial Network for Automatic Building Detection in Remotely Sensed Images","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","3146","3167","21","10.1109/JSTARS.2022.3166929","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128607408&doi=10.1109%2fJSTARS.2022.3166929&partnerID=40&md5=7287a9663b3f6d7b60b6e04c21ccd920","Despite recent advances in deep-learning-based semantic segmentation, automatic building detection from remotely sensed imagery is still a challenging problem owing to large variability in the appearance of buildings across the globe. The errors occur mostly around the boundaries of the building footprints, in shadow areas, and when detecting buildings whose exterior surfaces have reflectivity properties that are very similar to those of the surrounding regions. To overcome these problems, we propose a generative adversarial network-based segmentation framework with uncertainty attention unit and refinement module embedded in the generator. The refinement module, composed of edge and reverse-attention units, is designed to refine the predicted building map. The edge attention enhances the boundary features to estimate building boundaries with greater precision, and the reverse attention allows the network to explore the features missing in the previously estimated regions. The uncertainty attention unit assists the network in resolving uncertainties in classification. As a measure of the power of our approach, as of December 4, 2021, it ranks at the second place on DeepGlobe's public leaderboard despite the fact that main focus of our approach - refinement of the building edges - does not align exactly with the metrics used for leaderboard rankings. Our overall F1-score on DeepGlobe's challenging dataset is 0.745. We also report improvements on the previous-best results for the challenging INRIA validation dataset for which our network achieves an overall IoU of 81.28% and an overall accuracy of 97.03%. Along the same lines, for the official INRIA test dataset, our network scores 77.86% and 96.41% in overall IoU and accuracy. We have also improved upon the previous best results on two other datasets: the WHU building dataset and the Massachusetts buildings dataset. © 2008-2012 IEEE.","Buildings; Deep learning; Remote sensing; Semantic Segmentation; Semantic Web; Semantics; Statistical tests; Uncertainty analysis; Attention; Automatic building detection; Deep learning; Exterior surfaces; Images segmentations; Property; Remotely sensed imagery; Remotely sensed images; Semantic segmentation; Uncertainty; detection method; image analysis; image resolution; remote sensing; satellite data; satellite imagery; segmentation; uncertainty analysis; Generative adversarial networks","Attention; deep learning; generative adversarial networks (GANs); semantic segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85128607408"
"Tan D.; Liu Y.; Li G.; Yao L.; Sun S.; He Y.","Tan, Daning (57217675586); Liu, Yu (57386764900); Li, Gang (55547117794); Yao, Libo (56048087400); Sun, Shun (57189712854); He, You (57212448603)","57217675586; 57386764900; 55547117794; 56048087400; 57189712854; 57212448603","Serial gans: A feature-preserving heterogeneous remote sensing image transformation model","2021","Remote Sensing","13","19","3968","","","","10.3390/rs13193968","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116408951&doi=10.3390%2frs13193968&partnerID=40&md5=51b9ef3c812b437a431aadbef921e714","In recent years, the interpretation of SAR images has been significantly improved with the development of deep learning technology, and using conditional generative adversarial nets (CGANs) for SAR-to-optical transformation, also known as image translation, has become popular. Most of the existing image translation methods based on conditional generative adversarial nets are modified based on CycleGAN and pix2pix, focusing on style transformation in practice. In addition, SAR images and optical images are characterized by heterogeneous features and large spectral differences, leading to problems such as incomplete image details and spectral distortion in the heterogeneous transformation of SAR images in urban or semiurban areas and with complex terrain. Aiming to solve the problems of SAR-to-optical transformation, Serial GANs, a feature-preserving heterogeneous remote sensing image transformation model, is proposed in this paper for the first time. This model uses the Serial Despeckling GAN and Colorization GAN to complete the SAR-to-optical transformation. Despeckling GAN transforms the SAR images into optical gray images, retaining the texture details and semantic information. Colorization GAN transforms the optical gray images obtained in the first step into optical color images and keeps the structural features unchanged. The model proposed in this paper provides a new idea for heterogeneous image transformation. Through decoupling network design, structural detail information and spectral information are relatively independent in the process of heterogeneous transformation, thereby enhancing the detail information of the generated optical images and reducing its spectral distortion. Using SEN-2 satellite images as the reference, this paper compares the degree of similarity between the images generated by different models and the reference, and the results revealed that the proposed model has obvious advantages in feature reconstruction and the economical volume of the parameters. It also showed that Serial GANs have great potential in decoupling image transformation. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Geometrical optics; Image enhancement; Radar imaging; Remote sensing; Semantics; Synthetic aperture radar; Textures; Conditional generative adversarial net; Feature-preserving; Heterogeneous transformation; Image transformations; Optical image; Optical transformations; Optical-; Remote sensing images; SAR Images; Transformation modeling; Generative adversarial networks","Conditional generative adversarial nets (CGANs); Heterogeneous transformation; Optical image; SAR image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116408951"
"Zhao S.; Yang S.; Liu Z.; Feng Z.; Zhang K.","Zhao, Shihui (57211266107); Yang, Shuyuan (8159166000); Liu, Zhi (57203466982); Feng, Zhixi (56047550500); Zhang, Kai (56451954400)","57211266107; 8159166000; 57203466982; 56047550500; 56451954400","Sparse flow adversarial model for robust image compression","2021","Knowledge-Based Systems","229","","107284","","","","10.1016/j.knosys.2021.107284","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111285384&doi=10.1016%2fj.knosys.2021.107284&partnerID=40&md5=9554dc9331b8bd59ab0e6bf48d6d4f64","Existing learned-based image compression methods have shown impressive performance. However, they rely mostly on the consistent distribution between training and test images, which reduces the robustness of the training model. In this paper, we propose a novel compression method called sparse flow adversarial model (SFAM). SFAM employs a deep generative framework to learn a reversible and stable mapping between image distributions, thus it can work in varied scenes for robust compression. The mapping explores the sparsity of the image by combining linear and nonlinear transformations, rather than extracting the features of a particular dataset as is the case with other learning-based methods. Moreover, a sparse adversarial map is introduced into SFAM, to constrain the SFAM to generate sparser features for efficient compression. Extensive experiments are performed on different datasets, in which the effectiveness and robustness of the proposed method are verified. Meanwhile, SFAM is trained only once and it can work well on three different datasets, which also prove the robustness of the proposed SFAM. © 2021 Elsevier B.V.","Image compression; Linear transformations; Mapping; Compression methods; Generative adversarial network; Image compression methods; Images compression; Performance; Remote sensing image compression; Sparse flow adversarial model; Test images; Training image; Training model; Remote sensing","Generative adversarial network; Remote sensing image compression; Sparse flow adversarial model","Article","Final","","Scopus","2-s2.0-85111285384"
"Zhang Z.; Zhang C.; Wu M.; Han Y.; Yin H.; Kong A.; Chen F.","Zhang, Ziyun (57215773184); Zhang, Chengming (16835435800); Wu, Menxin (7405594817); Han, Yingjuan (57225052187); Yin, Hao (57215779384); Kong, Ailing (57215534548); Chen, Fangfang (57225034051)","57215773184; 16835435800; 7405594817; 57225052187; 57215779384; 57215534548; 57225034051","Super-resolution method using generative adversarial network for Gaofen wide-field-view images","2021","Journal of Applied Remote Sensing","15","2","028506","","","","10.1117/1.JRS.15.028506","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109017050&doi=10.1117%2f1.JRS.15.028506&partnerID=40&md5=1e874a476e14df20afcb3c7a1509cd13","Accurate information on the spatial distribution of crops is of great significance for scientific research and production practices. Such accurate information can be extracted from high-spatial-resolution optical remote sensing images. However, acquiring these images with a wide coverage is difficult. We established a model named multispectral super-resolution generative adversarial network (MS_SRGAN) for generating high-resolution 4-m images using Gaofen 1 wide-field-view (WFV) 16-m images. The MS_SRGAN model contains a generator and a discriminator. The generator network is composed of feature extraction units and feature fusion units with a symmetric structure, and the attention mechanism is introduced to constrain the spectral value of the feature map during feature extraction. The generator loss introduces feature loss to describe the feature difference of the image. This is realized using pre-trained discriminator parameters and a partial discriminator network. In addition to realizing feature loss, the discriminator network, which is a simple convolutional neural network, also realizes adversarial loss. Adversarial loss can provide some fake high frequency details to the generator to get a more sharpened image. In the Gaofen 1 WFV image test, the performance of MS_SRGAN was compared with that of Bicubic, EDSR, SRGAN, and ESRGAN. The results show that the spectral angle mapper (3.387) and structural similarity index measure (0.998) of MS_SRGAN are higher than those of the other models. In addition, the image obtained by MS_SRGAN is more realistic; its texture details and color distribution are closer to the reference image to a greater extent.  © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.","Convolutional neural networks; Extraction; Feature extraction; Image acquisition; Optical resolving power; Remote sensing; Textures; Attention mechanisms; High spatial resolution; Optical remote sensing; Scientific researches; Spectral angle mappers; Structural similarity index measures; Superresolution methods; Symmetric structures; Discriminators","convolutional neural network; Gaofen 1 WFV image; Gaofen 2 image; generative adversarial network; multispectral image; super-resolution","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85109017050"
"Jamali A.; Mahdianpari M.; Mohammadimanesh F.; Brisco B.; Salehi B.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939); Mohammadimanesh, Fariba (56541784200); Brisco, Brian (7003505161); Salehi, Bahram (36610817400)","56909712300; 57190371939; 56541784200; 7003505161; 36610817400","A synergic use of sentinel-1 and sentinel-2 imagery for complex wetland classification using generative adversarial network (Gan) scheme","2021","Water (Switzerland)","13","24","3601","","","","10.3390/w13243601","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121375982&doi=10.3390%2fw13243601&partnerID=40&md5=b9a064aa3ff25becdd73fa48f05d4f5c","Due to anthropogenic activities and climate change, many natural ecosystems, especially wetlands, are lost or changing at a rapid pace. For the last decade, there has been increasing attention towards developing new tools and methods for the mapping and classification of wetlands using remote sensing. At the same time, advances in artificial intelligence and machine learning, particularly deep learning models, have provided opportunities to advance wetland classification methods. However, the developed deep and very deep algorithms require a higher number of training samples, which is costly, logistically demanding, and time-consuming. As such, in this study, we propose a Deep Convolutional Neural Network (DCNN) that uses a modified architecture of the well-known DCNN of the AlexNet and a Generative Adversarial Network (GAN) for the generation and classification of Sentinel-1 and Sentinel-2 data. Applying to an area of approximately 370 sq. km in the Avalon Peninsula, Newfoundland, the proposed model with an average accuracy of 92.30% resulted in F-1 scores of 0.82, 0.85, 0.87, 0.89, and 0.95 for the recognition of swamp, fen, marsh, bog, and shallow water, respectively. Moreover, the proposed DCNN model improved the F-1 score of bog, marsh, fen, and swamp wetland classes by 4%, 8%, 11%, and 26%, respectively, compared to the original CNN network of AlexNet. These results reveal that the proposed model is highly capable of the generation and classification of Sentinel-1 and Sentinel-2 wetland samples and can be used for large-extent classification problems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Avalon Peninsula; Canada; Newfoundland; Newfoundland and Labrador; Climate change; Convolution; Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image classification; Remote sensing; Anthropogenic activity; Anthropogenic climate; CNN; Natural ecosystem; Network scheme; Remote-sensing; Sentinel-1; Time advance; Tools and methods; Wetland classification; accuracy assessment; artificial intelligence; satellite imagery; Sentinel; wetland; Wetlands","CNN; Deep Convolutional Neural Network; Generative Adversarial Network; Machine learning; Wetland classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121375982"
"Dai H.; Liu X.; Qiao Y.; Zheng K.; Xiao X.; Cai Z.","Dai, Haoran (57573734600); Liu, Xiaobo (36647829600); Qiao, Yulin (57215855806); Zheng, Kexin (57573734500); Xiao, Xiao (57573926200); Cai, Zhihua (56424630500)","57573734600; 36647829600; 57215855806; 57573734500; 57573926200; 56424630500","UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","2021","Proceeding - 2021 China Automation Congress, CAC 2021","","","","1803","1808","5","10.1109/CAC53003.2021.9727490","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128081131&doi=10.1109%2fCAC53003.2021.9727490&partnerID=40&md5=d58e630e4449514acdb2991dd153cc28","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFN-GAN for remote sensing image fusion. In our proposed UFN-GAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets. © 2021 IEEE","Decoding; Deep learning; Edge detection; Image enhancement; Image fusion; Remote sensing; Signal encoding; Spectral resolution; Unsupervised learning; Deep learning; Feature map; Fusion image; High spectral resolution; Multispectral images; Original images; Remote sensing images; Spatial informations; Spatial resolution; Spectral information; Generative adversarial networks","Deep learning; Generative adversarial network; Image fusion; Remote sensing images; Unsupervised learning; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85128081131"
"Tan Z.; Gao M.; Li X.; Jiang L.","Tan, Zhenyu (56421169400); Gao, Meiling (57191226894); Li, Xinghua (55626987300); Jiang, Liangcun (55946431600)","56421169400; 57191226894; 55626987300; 55946431600","A Flexible Reference-Insensitive Spatiotemporal Fusion Model for Remote Sensing Images Using Conditional Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3050551","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100463952&doi=10.1109%2fTGRS.2021.3050551&partnerID=40&md5=917ee417718b3afaec23296f8a1b2296","Due to the tradeoff between spatial and temporal resolutions of remote sensing images, spatiotemporal fusion models were proposed to synthesize the high spatiotemporal image series. Currently, spatiotemporal fusion models usually employ one coarse-resolution image acquired on a prediction date and at least another pair of coarse-fine resolution images close to the prediction time as references to derive the fine-resolution image on the prediction date. After years of development, the model accuracy has gained a certain improvement, but nearly, all the models require at least three image inputs and rigid time constraints must be applied to the references to guarantee the fusion accuracy. However, it is not always that easy to collect adequate data pairs for fine-resolution image series simulation in practice because of the bad weather condition or the time inconsistency between the coarse-fine resolution data sources, which causes some difficulties in the actual application. This article introduces the conditional generative adversarial network (CGAN) and switchable normalization technique into the spatiotemporal fusion problem and proposes a flexible deep network named the GAN-based SpatioTemporal Fusion Model (GAN-STFM) to reduce the number of model inputs and broke the time restriction on reference image selection. The GAN-STFM just needs a coarse-resolution image on the prediction date and another fine-resolution reference image at an arbitrary time in the same area for model inputs. As far as we know, this is the first spatiotemporal fusion model that requires only two images as model inputs and puts no restriction on the acquisition time of references. Even so, the GAN-STFM performs on par or better than other classical fusion models in the experiments. With this improvement, the data preparation for spatiotemporal fusion tends to be much easier than before, showing a promising perspective for practical applications.  © 1980-2012 IEEE.","Forecasting; Image enhancement; Remote sensing; Adversarial networks; Fine-resolution images; Remote sensing images; Resolution images; Spatial and temporal resolutions; Spatio-temporal fusions; Spatiotemporal images; Time inconsistency; image; network analysis; remote sensing; spatiotemporal analysis; Image fusion","Conditional generative adversarial network (CGAN); convolutional neural network (CNN); data fusion; deep learning; generative adversarial network (GAN)-based SpatioTemporal Fusion Model (GAN-STFM); remote sensing; spatiotemporal","Article","Final","","Scopus","2-s2.0-85100463952"
"Li J.; Sun W.; Jiang M.; Yuan Q.","Li, Jie (57214207213); Sun, Weixuan (57393194200); Jiang, Menghui (57210173702); Yuan, Qiangqiang (36635300800)","57214207213; 57393194200; 57210173702; 36635300800","Self-Supervised Pansharpening Based on a Cycle-Consistent Generative Adversarial Network","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3137428","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122057300&doi=10.1109%2fLGRS.2021.3137428&partnerID=40&md5=a3c7f4ec804e78d07888b42a2d08d5f0","In the field of remote sensing image pansharpening, deep learning-based methods have shown impressive performances recently. However, most deep learning-based pansharpening methods are based on supervised learning, which requires a large number of training images. In addition, obtaining large amounts of images with a high spatial and spectral resolution for training may be difficult in practice. In this letter, a novel self-supervised learning method based on a cycle-consistent generative adversarial network (CycleGAN) is proposed for remote sensing image pansharpening, without requiring large volumes of data for training. The framework contains two generators and two discriminators, and applies a residual neural network to the first generator. The panchromatic (PAN) image and multispectral (MS) image are input into the first generator to obtain the fused image, and then the fused image is input into the second generator to obtain a PAN image, which should be consistent with the input PAN image. The experimental results show that the proposed method performs better than the state-of-the-art unsupervised pansharpening method, and also achieves a competitive performance when compared with a supervised method. © 2004-2012 IEEE.","Deep learning; Remote sensing; Supervised learning; Cycle-consistent generative adversarial network; Deep learning; Generator; Neural-networks; Pan-sharpening; Remote sensing images; Residual neural network; Self-supervised learning; Spatial resolution; artificial neural network; image analysis; remote sensing; spectral resolution; supervised classification; Generative adversarial networks","Cycle-consistent generative adversarial network (CycleGAN); pansharpening; residual neural network; self-supervised learning","Article","Final","","Scopus","2-s2.0-85122057300"
"Ma Y.; Wei J.; Huang X.","Ma, Yaobin (57312889300); Wei, Jingbo (55711392500); Huang, Xiangtao (57313113500)","57312889300; 55711392500; 57313113500","Balancing Colors of Nonoverlapping Mosaicking Images with Generative Adversarial Networks","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3126261","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123518111&doi=10.1109%2fLGRS.2021.3126261&partnerID=40&md5=99156f3f79d33732a76e9733418bef26","Remote sensing images of different moments or sensors can be stitched together to produce a new image under uniform geographic coordinate systems, where the overlapping areas were needed for color harmony. In this letter, a reference-based mosaicking method is proposed for images either with or without overlapping areas. The new method introduces a low-resolution image for spectral reference that spans all the mosaicking scope. A generative adversarial network is harnessed for color harmony, which transfers all the mosaicking images to the time of the reference image for further stitch with the graph cut and pyramid gradient methods. The proposed method is compared with three color harmony methods or tools by mosaicking the red, green, and blue bands of Landsat-8 images with MODIS as the reference. The digital evaluations demonstrate that the new method outweighs other methods regarding radiometric and spectral fidelity.  © 2004-2012 IEEE.","Color; Generative adversarial networks; Gradient methods; Graphic methods; Remote sensing; Co-ordinate system; Color harmony; Enblend; Geographic coordinates; LANDSAT; Mosaicking; Nonoverlapping; Overlapping area; Remote sensing images; Spatio-temporal fusions; detection method; image analysis; network analysis; satellite imagery; Deep neural networks","Color harmony; deep neural networks; enblend; generative adversarial networks; LandSat; mosaicking; spatiotemporal fusion","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85123518111"
"Ma T.; Ma J.; Yu K.; Zhang J.; Fu W.","Ma, Tao (57193406136); Ma, Jie (54974641800); Yu, Kun (56810059100); Zhang, Jun (55959902000); Fu, Wenxing (55966565600)","57193406136; 54974641800; 56810059100; 55959902000; 55966565600","Multispectral Remote Sensing Image Matching via Image Transfer by Regularized Conditional Generative Adversarial Networks and Local Feature","2021","IEEE Geoscience and Remote Sensing Letters","18","2","9019828","351","355","4","10.1109/LGRS.2020.2972361","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099886422&doi=10.1109%2fLGRS.2020.2972361&partnerID=40&md5=572033b7d454606dd7729a2ba7ff8e9c","Multispectral image matching is at the base for many remote sensing and computer vision applications. Due to the different imaging principles and spectra, there are significant nonlinear variations in intensity, texture, and style in multispectral images. This makes it difficult for many classic methods designed for the images of the same spectrum to achieve satisfactory matching performance. To cope with this problem, this letter proposes a new method based on image transfer and local feature for multispectral image matching. First, we propose a new regularized conditional generative adversarial network (GAN) for image transfer to preprocess the multispectral images. This step eliminates the differences in grayscale, texture, and style between the multispectral images. Then, we use a classic local feature to match the generated and original images. We evaluate our method on two commonly used data sets and compare with several state-of-the-art methods. The experiments show that our method performs well by significantly improving the matching accuracy and robustness, and slightly increasing the runtime.  © 2004-2012 IEEE.","Image matching; Remote sensing; Textures; Adversarial networks; Computer vision applications; Imaging principle; Matching performance; Multispectral images; Multispectral remote sensing image; Non-linear variation; State-of-the-art methods; artificial neural network; image analysis; remote sensing; satellite imagery; spectral analysis; Image texture","Feature matching; image transfer; multispectral remote sensing images; regularized conditional generative adversarial network (GAN)","Article","Final","","Scopus","2-s2.0-85099886422"
"Liu W.; Luo B.; Liu J.","Liu, Weixing (57219734367); Luo, Bin (57209592752); Liu, Jun (36659410800)","57219734367; 57209592752; 36659410800","Synthetic Data Augmentation Using Multiscale Attention CycleGAN for Aircraft Detection in Remote Sensing Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3052017","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100515757&doi=10.1109%2fLGRS.2021.3052017&partnerID=40&md5=124e6bbdfec1d7ae3b682413dcdb0237","Deep learning approaches require enough training samples to perform well, but it is a challenge to collect enough real training data and label them manually. In this letter, we propose a practical framework for automatically generating content-rich synthetic images with ground-truth annotations. By rendering 3-D CAD models, we generate two synthetic aircraft image data sets with wide distribution (Syn N and Syn U). For improving the quality of synthetic images, we propose a multiscale attention module which enhances the Cycle-Consistent Adversarial Network (CycleGAN) in spatial and channel dimensions. Then, we compare the synthetic images before and after translation qualitatively and quantitatively. Experiments on Northwestern Polytechnical University (NWPU) very high resolution (VHR)-10, University of Chinese Academy of Sciences, orientation robust object detection in aerial images (UCAS-AOD), and benchmark for object DetectIon in Optical Remote sensing images (DIOR) data sets demonstrate that synthetic data augmentation can improve the performance of aircraft detection in remote sensing images, especially when real data are insufficient. Synthetic data are available at: https://weix-liu.github.io/.  © 2004-2012 IEEE.","Aircraft detection; Antennas; Benchmarking; Computer aided design; Deep learning; Object detection; Object recognition; Remote sensing; Three dimensional computer graphics; Training aircraft; Adversarial networks; Channel dimension; Chinese Academy of Sciences; Learning approach; Optical remote sensing; Remote sensing images; Robust object detection; Very high resolution; algorithm; data set; image analysis; satellite imagery; Image enhancement","Generative adversarial networks (GANs); image translation; object detection; self-attention; synthetic images","Article","Final","","Scopus","2-s2.0-85100515757"
"Li Y.","Li, Yinglong (57226199599)","57226199599","Research and Application of Deep Learning in Image Recognition","2022","2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications, ICPECA 2022","","","","994","999","5","10.1109/ICPECA53709.2022.9718847","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127431409&doi=10.1109%2fICPECA53709.2022.9718847&partnerID=40&md5=2a3ee66499ace9e02bcdadb36db1ae17","Deep learning is a technical tool with broad application prospects and has an important role in the field of image recognition. In view of the theoretical value and practical significance of image recognition technology in promoting the development of computer vision and artificial intelligence, this paper will review and study the application of deep learning in image recognition. This paper first outlines the development of icon recognition technology, and then introduces three main learning models in deep learning: convolutional neural networks, recurrent neural networks, and generative adversarial networks, and provides a comparative analysis of these three learning models. Finally, the research results of deep learning image recognition application fields, such as face recognition, medical image recognition, and remote sensing image classification, are analyzed and discussed. This paper also analyze the development trend of deep learning in the field of image recognition, and conclude that the future development direction is the effective recognition of video images and the theoretical strengthening of models.  © 2022 IEEE.","Convolutional neural networks; Face recognition; Generative adversarial networks; Image recognition; Medical imaging; Recurrent neural networks; Remote sensing; Application prospect; Broad application; Comparative analyzes; Convolutional neural network; Deep learning; Image recognition technology; Learning models; Research and application; Technical tools; Theoretical values; Image analysis","Application; Deep learning; Image recognition","Conference paper","Final","","Scopus","2-s2.0-85127431409"
"Wu H.; Zhang L.; Ma J.","Wu, Hanlin (57221263814); Zhang, Libao (35325855000); Ma, Jie (57205916758)","57221263814; 35325855000; 57205916758","Remote Sensing Image Super-Resolution via Saliency-Guided Feedback GANs","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2020.3042515","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098788589&doi=10.1109%2fTGRS.2020.3042515&partnerID=40&md5=315a94441bc03b62dc3b253c3b5166c6","In remote sensing images (RSIs), the visual characteristics of different regions are versatile, which poses a considerable challenge to single image super-resolution (SISR). Most existing SISR methods for RSIs ignore the diverse reconstruction needs of different regions and thus face a serious contradiction between high perception quality and less spatial distortion. The mean square error (MSE) optimization-based methods produce results of unsatisfactory visual quality, while generative adversarial networks (GANs) can produce photo-realistic but severely distorted results caused by pseudotextures. In addition, increasingly deeper networks, although providing powerful feature representations, also face problems of overfitting and occupying too much storage space. In this article, we propose a new saliency-guided feedback GAN (SG-FBGAN) to address these problems. The proposed SG-FBGAN applies different reconstruction principles for areas with varying levels of saliency and uses feedback (FB) connections to improve the expressivity of the network while reducing parameters. First, we propose a saliency-guided FB generator with our carefully designed paired-feedback block (PFBB). The PFBB uses two branches, a salient and a nonsalient branch, to handle the FB information and generate powerful high-level representations for salient and nonsalient areas, respectively. Then, we measure the visual perception quality of salient areas, nonsalient areas, and the global image with a saliency-guided multidiscriminator, which can dramatically eliminate pseudotextures. Finally, we introduce a curriculum learning strategy to enable the proposed SG-FBGAN to handle complex degradation models. Comprehensive evaluations and ablation studies validate the effectiveness of our proposal.  © 1980-2012 IEEE.","Learning systems; Mean square error; Optical resolving power; Space optics; Adversarial networks; Comprehensive evaluation; Degradation model; Feature representation; Learning strategy; Optimization based methods; Remote sensing images; Spatial distortion; algorithm; image resolution; remote sensing; satellite imagery; Remote sensing","Deep learning (DL); generative adversarial network (GAN); remote sensing; saliency detection; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85098788589"
"Guo J.; He C.; Zhang M.; Li Y.; Gao X.; Song B.","Guo, Jie (55709458400); He, Chengyu (57254506000); Zhang, Mingjin (56402068600); Li, Yunsong (55986546100); Gao, Xinbo (7403873424); Song, Bangyu (57254276300)","55709458400; 57254506000; 56402068600; 55986546100; 7403873424; 57254276300","Edge-preserving convolutional generative adversarial networks for sar-to-optical image translation","2021","Remote Sensing","13","18","3575","","","","10.3390/rs13183575","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114690630&doi=10.3390%2frs13183575&partnerID=40&md5=b8253e325748eac7ee2ad01bfca33088","With the ability for all-day, all-weather acquisition, synthetic aperture radar (SAR) remote sensing is an important technique in modern Earth observation. However, the interpretation of SAR images is a highly challenging task, even for well-trained experts, due to the imaging principle of SAR images and the high-frequency speckle noise. Some image-to-image translation methods are used to convert SAR images into optical images that are closer to what we perceive through our eyes. There exist two weaknesses in these methods: (1) these methods are not designed for an SAR-to-optical translation task, thereby losing sight of the complexity of SAR images and the speckle noise. (2) The same convolution filters in a standard convolution layer are utilized for the whole feature maps, which ignore the details of SAR images in each window and generate images with unsatisfactory quality. In this paper, we propose an edge-preserving convolutional generative adversarial network (EPCGAN) to enhance the structure and aesthetics of the output image by leveraging the edge information of the SAR image and implementing content-adaptive convolution. The proposed edge-preserving convolution (EPC) decomposes the content of the convolution input into texture components and content components and then generates a content-adaptive kernel to modify standard convolutional filter weights for the content components. Based on the EPC, the EPCGAN is presented for SAR-to-optical image translation. It uses a gradient branch to assist in the recovery of structural image information. Experiments on the SEN1-2 dataset demonstrated that the proposed method can outperform other SAR-to-optical methods by recovering more structures and yielding a superior evaluation index. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolution; Convolutional neural networks; Geometrical optics; Image enhancement; Remote sensing; Speckle; Synthetic aperture radar; Textures; Adversarial networks; Convolution filters; Earth observations; High frequency HF; Image information; Image translation; Imaging principle; Texture components; Radar imaging","Deep learning; Edge-preserving convolution; Generative adversarial networks; SAR-to-optical image translation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114690630"
"Wang Z.; Zhang Z.; Dong L.; Xu G.","Wang, Zilin (57225154100); Zhang, Zhaoxiang (57191228269); Dong, Limin (57195246117); Xu, Guodong (15062204400)","57225154100; 57191228269; 57195246117; 15062204400","Jitter detection and image restoration based on generative adversarial networks in satellite images","2021","Sensors","21","14","4693","","","","10.3390/s21144693","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109288387&doi=10.3390%2fs21144693&partnerID=40&md5=1477770b5e6b3ecbc80df353dc13668a","High-resolution satellite images (HRSIs) obtained from onboard satellite linear array cameras suffer from geometric disturbance in the presence of attitude jitter. Therefore, detection and compensation of satellite attitude jitter are crucial to reduce the geopositioning error and to improve the geometric accuracy of HRSIs. In this work, a generative adversarial network (GAN) architecture is proposed to automatically learn and correct the deformed scene features from a single remote sensing image. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs, and another CNN is used to generate so-called fake inputs. To explore the usefulness and effectiveness of a GAN for jitter detection, the proposed GANs are trained on part of the PatternNet dataset and tested on three popular remote sensing datasets, along with a deformed Yaogan-26 satellite image. Several experiments show that the proposed model provides competitive results. The proposed GAN reveals the enormous potential of GAN-based methods for the analysis of attitude jitter from remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Error compensation; Jitter; Remote sensing; Satellites; Adversarial networks; Geo-positioning; Geometric accuracy; High resolution satellite images; Jitter detection; Remote sensing images; Satellite attitude; Satellite images; article; convolutional neural network; image reconstruction; satellite imagery; Image reconstruction","Convolutional neural network; Generative adversarial network; Image restoration; Jitter detection; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85109288387"
"Tao Y.; Muller J.-P.","Tao, Yu (56539197700); Muller, Jan-Peter (7404871794)","56539197700; 7404871794","Super-resolution restoration of spaceborne ultra-high-resolution images using the ucl optigan system","2021","Remote Sensing","13","12","2269","","","","10.3390/rs13122269","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108893859&doi=10.3390%2frs13122269&partnerID=40&md5=5a4502223f127ddc550ac2a938414e15","We introduce a robust and light-weight multi-image super-resolution restoration (SRR) method and processing system, called OpTiGAN, using a combination of a multi-image maximum a posteriori approach and a deep learning approach. We show the advantages of using a combined twostage SRR processing scheme for significantly reducing inference artefacts and improving effective resolution in comparison to other SRR techniques. We demonstrate the optimality of OpTiGAN for SRR of ultra-high-resolution satellite images and video frames from 31 cm/pixel WorldView-3, 75 cm/pixel Deimos-2 and 70 cm/pixel SkySat. Detailed qualitative and quantitative assessments are provided for the SRR results on a CEOS-WGCV-IVOS geo-calibration and validation site at Baotou, China, which features artificial permanent optical targets. Our measurements have shown a 3.69 times enhancement of effective resolution from 31 cm/pixel WorldView-3 imagery to 9 cm/pixel SRR. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image enhancement; Optical resolving power; Restoration; Calibration and validations; Effective resolutions; Learning approach; Maximum a posteriori; Processing systems; Qualitative and quantitative assessments; Super-resolution restoration; Ultrahigh resolution; Image reconstruction","Deimos-2; Earth observation; EarthDaily Analytics®; Generative adversarial network; HD video; Maxar® WorldView-3; OpTiGAN; Planet® SkySat; Remote sensing; Satellite; Super-resolution restoration; Ultra-high resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85108893859"
"Liu Q.; Zhou H.; Xu Q.; Liu X.; Wang Y.","Liu, Qingjie (55534263100); Zhou, Huanyu (57221308389); Xu, Qizhi (50562407300); Liu, Xiangyu (57192693924); Wang, Yunhong (34870959400)","55534263100; 57221308389; 50562407300; 57192693924; 34870959400","PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","2021","IEEE Transactions on Geoscience and Remote Sensing","59","12","","10227","10242","15","10.1109/TGRS.2020.3042974","57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098790048&doi=10.1109%2fTGRS.2020.3042974&partnerID=40&md5=7d88acdaecc009fb44a0ce0274f6b96e","This article addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network-based method named pansharpening GAN (PSGAN). To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with generative adversarial networks (GANs). The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images, and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this article, we evaluate several architectures and designs, namely, two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods but also generalize well to full-scale images. © 1980-2012 IEEE.","Deep neural networks; Adversarial learning; Adversarial networks; Attention mechanisms; Discriminative networks; Multispectral images; Optimal solutions; Remote sensing images; State-of-the-art methods; artificial neural network; QuickBird; remote sensing; satellite imagery; WorldView; Remote sensing","Convolutional neural network (CNN); deep learning; generative adversarial network (GAN); pan-sharpening; residual learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098790048"
"Zhang J.; Tian H.; Wang P.; Tansey K.; Zhang S.; Li H.","Zhang, Jingqi (57212685660); Tian, Huiren (57212676299); Wang, Pengxin (12806808600); Tansey, Kevin (6603916527); Zhang, Shuyu (55713514900); Li, Hongmei (57212191640)","57212685660; 57212676299; 12806808600; 6603916527; 55713514900; 57212191640","Improving wheat yield estimates using data augmentation models and remotely sensed biophysical indices within deep neural networks in the Guanzhong Plain, PR China","2022","Computers and Electronics in Agriculture","192","","106616","","","","10.1016/j.compag.2021.106616","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120995591&doi=10.1016%2fj.compag.2021.106616&partnerID=40&md5=fdd28ebb03869e6b71ffcd9a34ad4755","Crop yield estimation and prediction constitutes a key issue in agricultural management, particularly under the context of demographic pressure and climate change. Currently, the main challenge in estimating crop yields based on remotely sensed data and data-driven methods is how to cope with small datasets and the limited amount of annotated samples. In order to cope with small datasets and the limited amount of annotated samples and improve the accuracy of winter wheat yield estimation in the Guanzhong Plain, PR China, this study proposed a method of combining generative adversarial networks (GANs) and convolutional neural network (CNN) for comprehensive growth monitoring of winter wheat, in which the remotely sensed leaf area index (LAI), vegetation temperature condition index (VTCI) and meteorological data at four growth stages of winter wheat during 2012–2017 were generated as the inputs of multi-layer convolutional neural networks (CNNs), and GAN was employed to artificially increase the number of training samples. Then, a linear regression model between the simulated comprehensive growth monitoring (I) and the measured yields was established to estimate yields of winter wheat in the Guanzhong Plain pixel by pixel. The final results showed when GAN was used to double the size of the training samples, and the simulation values obtained by CNN based on augmented samples using GAN provided a better training (R2 = 0.95, RMSE = 0.05), validation (R2 = 0.54, RMSE = 0.16) and testing (R2 = 0.50, RMSE = 0.14) performance than that just using the original samples. The achieved best pixel-scale yield estimation accuracy of winter wheat (R2 = 0.50, RMSE = 591.46 kg/ha) in the Guanzhong Plain. These results showed that small samples can be enlarged by GAN, thus, more important features for reflecting the growth conditions and yields of winter wheat from the remotely sensed indices and meteorological indices can be extracted, and indicated that CNN accompanied with GAN could contribute a lot to the comprehensive growth monitoring and yield estimation of winter wheat and data augmentation methods are extremely useful for the application of small samples in deep learning. © 2021 The Authors","China; Guanzhong Plain; Shaanxi; Climate change; Convolution; Convolutional neural networks; Crops; Deep neural networks; Generative adversarial networks; Multilayer neural networks; Regression analysis; Remote sensing; Sampling; Vegetation; Convolutional neural network; Crop yield; Data augmentation; Growth monitoring; Leaf Area Index; Small data set; Training sample; Vegetation temperature condition index; Winter wheat; Yield estimation; artificial neural network; bioaugmentation; climate change; crop yield; leaf area index; regression analysis; remote sensing; wheat; Pixels","Convolutional neural network; Generative adversarial network; Leaf area index; Vegetation temperature condition index; Yield estimation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85120995591"
"Mu J.; Li S.; Liu Z.; Zhou Y.","Mu, Jinzhen (57219133143); Li, Shuang (56288781000); Liu, Zongming (57869062600); Zhou, Yan (57188815143)","57219133143; 56288781000; 57869062600; 57188815143","Integration of gradient guidance and edge enhancement into super-resolution for small object detection in aerial images","2021","IET Image Processing","15","13","","3037","3052","15","10.1049/ipr2.12288","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108818533&doi=10.1049%2fipr2.12288&partnerID=40&md5=a24cfb4a97fbcc4a87bdf786f63f3dd2","Detecting small objects are difficult because of their poor-quality appearance and small size, and such issues are especially pronounced for aerial images of great importance. To address the small object detection (SOD) problem, a united architecture that tries to upsample small objects into super-resolved versions, achieving characteristics similar to those large objects and thus resulting in more discriminative detection is used. For this purpose, a new end-to-end multi-task generative adversarial network (GAN) is proposed. In the architecture, the generator is a super-resolution (SR) network, and the discriminator is a multi-task network. In the generator, a gradient guide and an edge-enhancement strategy are introduced to alleviate structural distortions. In the discriminator, a faster region-based convolutional neural network (FRCNN) is incorporated for the task of object detection. Specifically, the discriminator outputs a distribution scalar to measure the realness. Then, each super-resolved image passes through the discriminator with a realness distribution, classification scores, and bounding box regression offsets. Furthermore, the losses of the detection task are backpropagated into the generator during training rather than being optimized independently. Extensive experiments on the challenging cars overhead with context dataset (COWC), detectIon in optical remote sensing images (DIOR), vision meets drones (VisDrone), and dataset for object detection in aerial images (DOTA) demonstrate the effectiveness of the proposed method in reconstructing structures while generating natural super-resolved images and show the superiority of the proposed method in detecting small objects over state-of-the-art detectors. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Air navigation; Aircraft detection; Antennas; Backpropagation; Convolutional neural networks; Discriminators; Image enhancement; Image segmentation; Network architecture; Object recognition; Optical resolving power; Remote sensing; Adversarial networks; Detection tasks; Edge enhancements; Optical remote sensing; Small object detection; State of the art; Structural distortions; Super resolution; Object detection","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85108818533"
"Wang G.; Zhang Y.; Xie W.; Qu Y.","Wang, Guangyi (57211228918); Zhang, Youmin (56846348100); Xie, Wenfang (25932438400); Qu, Yaohong (25651787800)","57211228918; 56846348100; 25932438400; 25651787800","Leveraging Google Earth Engine and Semi-Supervised Generative Adversarial Networks to Assess Initial Burn Severity in Forest; [Tirer parti de Google Earth Engine et des réseaux antagonistes génératifs semi-supervisés pour évaluer la sévérité initiale d’un feu de forêt]","2022","Canadian Journal of Remote Sensing","48","3","","411","424","13","10.1080/07038992.2022.2054405","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128180177&doi=10.1080%2f07038992.2022.2054405&partnerID=40&md5=acab01d3c696af077d430935d59425fa","Mapping and monitoring initial burn severity is a critical aspect of forest management and landscape restoration. Recently, supervised learning has achieved great success in evaluating the post-fire forest condition, but plenty of labeled samples are needed to carry out training for supervised learning. However, due to the limitation of accessible labeled data, it is often arduous to put supervised learning into practice in the remote sensing field. In this paper, a novel semi-unsupervised image classification framework by using generative adversarial networks under the Google Earth Engine (GEE) platform is proposed to undertake the burn severity assessment with limited samples. The generative model can produce additional adversarial samples that look like the real samples; therefore, the discriminative model gains better classification capabilities in burn severity assessment with the extra training data provided by the generator. The detailed evaluation and comparative experiments are done in the context of post-fire assessment on eleven forest fires in northern California in the United States. The experimental results demonstrate that our approach significantly outperforms the two other most well-known methods. ©, Copyright © CASI.","Classification (of information); Conservation; Deforestation; Engines; Remote sensing; Supervised learning; Burn Severity; Forest conditions; Google earths; Labeled data; Landscape restoration; Post-fire; Remote-sensing; Semi-supervised; Sensing fields; Unsupervised image classification; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85128180177"
"Hu A.; Chen S.; Wu L.; Xie Z.; Qiu Q.; Xu Y.","Hu, Anna (57205419850); Chen, Siqiong (57214459289); Wu, Liang (23101491100); Xie, Zhong (36164790400); Qiu, Qinjun (57203591589); Xu, Yongyang (57095192900)","57205419850; 57214459289; 23101491100; 36164790400; 57203591589; 57095192900","WSGAN: An improved generative adversarial network for remote sensing image road network extraction by weakly supervised processing","2021","Remote Sensing","13","13","2506","","","","10.3390/rs13132506","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109263776&doi=10.3390%2frs13132506&partnerID=40&md5=b9cc80699a1e78fa18564d8c178d3559","Road networks play an important role in navigation and city planning. However, current methods mainly adopt the supervised strategy that needs paired remote sensing images and segmentation images. These data requirements are difficult to achieve. The pair segmentation images are not easy to prepare. Thus, to alleviate the burden of acquiring large quantities of training images, this study designed an improved generative adversarial network to extract road networks through a weakly supervised process named WSGAN. The proposed method is divided into two steps: generating the mapping image and post-processing the binary image. During the generation of the mapping image, unlike other road extraction methods, this method overcomes the limitations of manually annotated segmentation images and uses mapping images that can be easily obtained from public data sets. The residual network block and Wasserstein generative adversarial network with gradient penalty loss were used in the mapping network to improve the retention of high-frequency information. In the binary image post-processing, this study used the dilation and erosion method to remove salt-and-pepper noise and obtain more accurate results. By comparing the generated road network results, the Intersection over Union scores reached 0.84, the detection accuracy of this method reached 97.83%, the precision reached 92.00%, and the recall rate reached 91.67%. The experiments used a public dataset from Google Earth screenshots. Benefiting from the powerful prediction ability of GAN, the experiments show that the proposed method performs well at extracting road networks from remote sensing images, even if the roads are covered by the shadows of buildings or trees. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Binary images; Extraction; Feature extraction; Image segmentation; Mapping; Motor transportation; Remote sensing; Roads and streets; Adversarial networks; Dilation and erosions; High-frequency informations; Remote sensing images; Road extraction method; Road network extraction; Salt-and-pepper noise; Segmentation images; Image enhancement","Generative adversarial networks; Remote sensing image; Road extraction; Weakly supervised","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85109263776"
"de Lima D.C.; Saqui D.; Mpinda S.A.T.; Saito J.H.","de Lima, Daniel Caio (57209397114); Saqui, Diego (56028559700); Mpinda, Steve Ataky Tsham (56786495100); Saito, José Hiroki (7102105877)","57209397114; 56028559700; 56786495100; 7102105877","Pix2Pix Network to Estimate Agricultural Near Infrared Images from RGB Data; [  Un réseau Pix2Pix pour générer des images dans le proche infrarouge en zones agricoles à partir de données RVB]","2022","Canadian Journal of Remote Sensing","48","2","","299","315","16","10.1080/07038992.2021.2016056","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123681785&doi=10.1080%2f07038992.2021.2016056&partnerID=40&md5=efc295efee187f1a7c7976414a8dfaeb","Remote sensing has been applied to agriculture, making it possible to acquire a large amount of data far away from crops, providing information for decision making by producers that can impact production costs and crops quality. One way of getting the production information is through vegetation indices, arithmetic operations that use spectral bands, especially the Near Infrared (NIR). However, sensors that capture this spectral information are very expensive for small producers to afford it. In a previous article, a pixel-to-pixel image synthesis model to estimate NIR images from RGB data using hyperspectral endmembers (pure hyperspectral signatures) was described. In this work, an image-to-image synthesis model, known as Pix2Pix, is used for estimating NIR images from low-cost RGB camera images. Pix2Pix is a kind of Generative Adversarial Networks (GANs), composed by two neural networks, a generator (G) and a discriminator (D), that compete. G learns to create images from a random noise inputs and D learns to verify if these images are real or fake. The results showed that the presented method generated NIR images quite similar to real ones, reaching a value of 0.912 on M3SIM similarity metric, outperforming results obtained with the previous endmembers method (0.775 on M3SIM). ©, Copyright © CASI.","Costs; Crops; Decision making; Infrared devices; Pixels; Remote sensing; Crop quality; Decisions makings; Endmembers; Images synthesis; Large amounts of data; Learn+; Near- infrared images; Production cost; Remote-sensing; Synthesis models; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85123681785"
"Wang J.; Sun Y.; Li Y.","Wang, Junjun (57331324300); Sun, Yue (55737851200); Li, Ying (55961756500)","57331324300; 55737851200; 55961756500","Cloud removal method for the remote sensing image based on the GAN; [一种生成对抗网络的遥感图像去云方法]","2021","Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University","48","5","","23","29","6","10.19665/j.issn1001-2400.2021.05.004","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118928769&doi=10.19665%2fj.issn1001-2400.2021.05.004&partnerID=40&md5=508bfec13b66ce4c40936793827ea1e9","Since remote sensing images will inevitably be affected by the climate in the acquisition process, obtained images may contain cloud information, which affects the subsequent use of images to a large extent. Image cloud removal methods based on deep learning can remove clouds well, but they have problems such as long training time, insufficient cloud removal effect and color distortion. To solve these problems, a cloud removal method based on the end-to-end generative adversarial network (GAN) is proposed to recover clear images from remote sensing images containing clouds. First, the U-Net is used as the main structure of the generator, and a continuous memory residual module is added between the encoder module and the decoder module to mine the depth characteristics of the input information. Then, a convolutional neural network is adopted as the discriminator to distinguish authenticity. Finally, the loss function, by combining the adversarial function with the absolute loss function, is designed to measure the advantages and disadvantages of the model by calculating the gap between the output of the network model and the real data. Experimental results show that the proposed method is superior to existing cloud removal methods in both quantitative indexes (peak signal to noise ratio and structural similarity) and running time. Under the same number of parameters, the proposed method has the lowest calculation amount (GFLOPs) and a lower algorithm complexity. Besides, remote sensing images obtained by the proposed method can lead to richer detailed information, almost no color distortion, and a better subjective visual effect. © 2021, The Editorial Board of Journal of Xidian University. All right reserved.","Computational complexity; Convolutional neural networks; Deep learning; Image processing; Remote sensing; Signal to noise ratio; Acquisition process; Cloud removal; Color distortions; Continuous memory residual; Image cloud removal; Image-based; Loss functions; Remote sensing images; Removal method; Training time; Generative adversarial networks","Continuous memory residual; Generative adversarial network; Image cloud removal; Remote sensing image","Article","Final","","Scopus","2-s2.0-85118928769"
"Deng L.; Zhang Y.; Wang X.","Deng, Liwei (51663261700); Zhang, Yuanzhi (57839766400); Wang, Xiaofei (57219132572)","51663261700; 57839766400; 57219132572","High-definition processing of remote sensing images based on CUT-CycleGAN","2021","Chinese Control Conference, CCC","2021-July","","","8158","8162","4","10.23919/CCC52363.2021.9549656","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117263619&doi=10.23919%2fCCC52363.2021.9549656&partnerID=40&md5=e289a72501a1a8d376e7030485de4cf6","High-definition remote sensing images are more and more widely used in research and life. However, due to hardware conditions and transmission rate limitations, it is too expensive to directly obtain high-definition original images. So, it has become a research hotspot on how to use algorithms to receive high-definition remote sensing images from low-resolution images. In view of the existing super-resolution methods for remote sensing images, the dependence on a large number of matching low-resolution and high-resolution(LR-HR) data sets and the slow network training time. In this paper, contrast learning is used for unpaired image-to-image conversion model (CUT-CycleGAN), which uses cyclic consistency to achieve the purpose of training using unpaired images, and adds a contrast learning framework to effectively shorten CycleGAN's training time and to improve efficiency. The experiment selects SRGAN, CycleGAN, EDSR, and FSRCNN four existing super-resolution methods to compare with the method in this paper. The results show that the training time of CUT-CycleGAN is reduced by nearly 55.7%, and after training with unpaired images, the quality of the generated high-definition images is good enough. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.","Image enhancement; Learning systems; Optical resolving power; Remote sensing; Comparative learning; Condition; High definition; Image-based; Remote sensing images; Remote-sensing; Superresolution; Superresolution methods; Training time; Transmission rates; Generative adversarial networks","Comparative learning; Generative adversarial network; Remote sensing; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85117263619"
"Zhang F.; Bai J.; Zhang J.; Xiao Z.; Pei C.","Zhang, Fan (57199242751); Bai, Jing (57193646200); Zhang, Jingsen (57216539302); Xiao, Zhu (36912987800); Pei, Changxing (7003719786)","57199242751; 57193646200; 57216539302; 36912987800; 7003719786","An Optimized Training Method for GAN-Based Hyperspectral Image Classification","2021","IEEE Geoscience and Remote Sensing Letters","18","10","","1791","1795","4","10.1109/LGRS.2020.3009017","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116293738&doi=10.1109%2fLGRS.2020.3009017&partnerID=40&md5=81ecabf50404a2b778d4a4a6373ff0a5","This letter explores how to apply a generative adversarial network (GAN) to the classification of hyperspectral images (HSIs) to obtain a smooth training process and better classification results. To this end, the ideas of the progressive growing GAN (PG-GAN) and Wasserstein generative adversarial network gradient penalty (WGAN-GP) are combined to propose a new method for HSI classification. PG-GAN is optimized from the training process of generating adversarial networks. It gradually increases the depth of the network and the size of the input image, making the training smoother. WGAN-GP is optimized in terms of the loss function. The gradient penalty method is used to solve the problems of vanishing gradient and exploding gradient, making the training more stable. Based on the combination of the two methods, a classifier is added to the model so that it can complete the HSI classification task. The proposed method is evaluated over two publicly available hyperspectral data sets, the Indian Pines and University of Pavia data sets. The results show that the proposed method can achieve good training results with only a small amount of labeled training data.  © 2004-2012 IEEE.","Constrained optimization; Gallium nitride; III-V semiconductors; Image classification; Spectroscopy; Adversarial networks; Classification results; Data set; Generative adversarial network; Hyperspectral image classification; Input image; Loss functions; Network-based; Training methods; Training process; image classification; multispectral image; optimization; remote sensing; Generative adversarial networks","Generative adversarial network (GAN); hyperspectral image (HSI) classification; semisupervised learning","Article","Final","","Scopus","2-s2.0-85116293738"
"Wu L.; Lu M.; Fang L.","Wu, Linshan (57560260700); Lu, Ming (57406628000); Fang, Leyuan (57218451012)","57560260700; 57406628000; 57218451012","Deep Covariance Alignment for Domain Adaptive Remote Sensing Image Segmentation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5620811","","","","10.1109/TGRS.2022.3163278","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127466616&doi=10.1109%2fTGRS.2022.3163278&partnerID=40&md5=b0f88332227e276e29cbf54e441d2e2b","Unsupervised domain adaptive (UDA) image segmentation has recently gained increasing attention, aiming to improve the generalization capability for transferring knowledge from the source domain to the target domain. However, in high spatial resolution remote sensing image (RSI), the same category from different domains (e.g., urban and rural) can appear to be totally different with extremely inconsistent distributions, which heavily limits the UDA accuracy. To address this problem, in this article, we propose a novel deep covariance alignment (DCA) model for UDA RSI segmentation. The DCA can explicitly align category features to learn shared domain-invariant discriminative feature representations, which enhance the ability of model generalization. Specifically, a category feature pooling (CFP) module is first used to extract category features by combining coarse outputs and deep features. Then, we leverage a novel covariance regularization (CR) to enforce the intracategory features to be closer and the intercategory features to be further separate. Compared with the existing category alignment methods, our CR aims to regularize the correlation between different dimensions of the features, and thus performs more robustly when dealing with divergent category features of imbalanced and inconsistent distributions. Finally, we propose a stagewise procedure to train the DCA to alleviate error accumulation. Experiments on both rural-to-urban and urban-to-rural scenarios of the LoveDA dataset demonstrate the superiority of our proposed DCA over other state-of-the-art UDA segmentation methods. Code is available at https://github.com/Luffy03/DCA. © 1980-2012 IEEE.","Alignment; Image enhancement; Remote sensing; Semantic Segmentation; Semantic Web; Semantics; Adaptation models; Deep covariance alignment; Domain adaptation; Features extraction; Images segmentations; Remote sensing image; Remote sensing images; Semantic segmentation; Task analysis; Unsupervised domain adaptation; image analysis; numerical model; remote sensing; segmentation; Generative adversarial networks","Deep covariance alignment (DCA); remote sensing image (RSI); semantic segmentation; unsupervised domain adaptive (UDA)","Article","Final","","Scopus","2-s2.0-85127466616"
"Wang C.; Tang G.; Gentine P.","Wang, Cunguang (57188854258); Tang, Guoqiang (56048613800); Gentine, Pierre (19639722300)","57188854258; 56048613800; 19639722300","PrecipGAN: Merging Microwave and Infrared Data for Satellite Precipitation Estimation Using Generative Adversarial Network","2021","Geophysical Research Letters","48","5","e2020GL092032","","","","10.1029/2020GL092032","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102534327&doi=10.1029%2f2020GL092032&partnerID=40&md5=5e82ee0ffd7e6dafc95a1b167f060e27","Global satellite precipitation estimation at high spatiotemporal resolutions is crucial for hydrological and meteorological applications but is still a challenging task. One major challenge is that the microwave data are discontinuous in space and time. We present a novel approach to merge incomplete passive microwave (PMW) precipitation estimates using the conditional information provided by complete infrared (IR) precipitation estimates based on the generative adversarial network (GAN), and name the algorithm PrecipGAN. PrecipGAN decomposes the precipitation system into content and evolution subspaces to propagate PMW estimates to regions outside the orbit coverage of PMW sensors. PrecipGAN can skillfully simulate the spatiotemporal changes of precipitation events, and produce precipitation estimates with overall better statistical performance than the baseline product Integrated MultisatellitE Retrievals for GPM (IMERG) Uncalibrated over the Continental US. PrecipGAN provides an alternative of accurate and computationally efficient algorithm that can be implemented globally to produce satellite-based precipitation estimates. © 2021. American Geophysical Union. All Rights Reserved.","Satellites; Adversarial networks; Computationally efficient; Precipitation events; Precipitation systems; Satellite precipitation; Spatio-temporal changes; Spatio-temporal resolution; Statistical performance; accuracy assessment; estimation method; numerical model; performance assessment; satellite data; simulation; spatiotemporal analysis; Orbits","deep learning; precipitation; remote sensing","Article","Final","","Scopus","2-s2.0-85102534327"
"Baier G.; Deschemps A.; Schmitt M.; Yokoya N.","Baier, Gerald (57188720676); Deschemps, Antonin (57221248595); Schmitt, Michael (7401931279); Yokoya, Naoto (36440631200)","57188720676; 57221248595; 7401931279; 36440631200","Synthesizing Optical and SAR Imagery from Land Cover Maps and Auxiliary Raster Data","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3068532","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104617195&doi=10.1109%2fTGRS.2021.3068532&partnerID=40&md5=4f1d774df6d33691b5d314efc9f28290","We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fréchet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth, and we published the newly created high-resolution data set at https://ieee-dataport.org/open-access/geonrw.  © 1980-2012 IEEE.","Data fusion; HTTP; Radar imaging; Rasterization; Remote sensing; Synthetic aperture radar; Water levels; Adversarial networks; Auxiliary information; Digital elevation model; High resolution data; Information contents; Remote sensing images; Segmentation models; Spatially adaptive; land cover; mapping method; optical method; radar imagery; raster; synthetic aperture radar; Image processing","Deep learning; generative adversarial network (GAN); image synthesis; synthetic aperture radar (SAR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104617195"
"Fang B.; Chen G.; Ouyang G.; Chen J.; Kou R.; Wang L.","Fang, Bo (57209326962); Chen, Gang (57115575600); Ouyang, Guichong (57222576509); Chen, Jifa (57219938705); Kou, Rong (57202911013); Wang, Lizhe (23029267900)","57209326962; 57115575600; 57222576509; 57219938705; 57202911013; 23029267900","Content-Invariant Dual Learning for Change Detection in Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3064501","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103242325&doi=10.1109%2fTGRS.2021.3064501&partnerID=40&md5=3109a1a69a38210056a92f5266c28c56","With the introduction of artificial intelligence (AI), current deep learning-driven change detection methods generally regard changes as a type of specific land cover information and try to detect them simply by using existing semantic labeling models. In practice, the changes that occur on different land cover categories may appear completely different for remote sensing images, making it difficult to detect multiple categories of changes with an end-to-end model. In addition, training such networks requires a large amount of prelabeled references, which are labor-intensive and time-consuming. Motivated by this observation, we integrate dual learning algorithm and disentangled representation theory to develop a novel approach, named content-invariant dual learning (CiDL), for either supervised or unsupervised change detection in remote sensing images. In our framework, two opposite Y-shaped networks, each of which consists of two encoders and one decoder, are introduced to translate bitemporal images from their original domains to each others, where their intrinsic content features are retained, while their style features are consistent. By training our hybrid framework, even without references, this method learns a category-wise cross-domain translation to suppress the discrepancies in paired unchanged regions, and meanwhile highlight those in paired changed regions. The experimental results on two typical change detection data sets and the comparison with other state-of-the-art deep learning-driven methods verify the effectiveness and competitiveness of our proposed CiDL.  © 1980-2012 IEEE.","Deep learning; Learning systems; Remote sensing; Semantics; Change detection; End-to-end models; Hybrid framework; Land cover informations; Remote sensing images; Representation theory; Semantic labeling; Unsupervised change detection; artificial intelligence; hybrid zone; land cover; learning; remote sensing; Learning algorithms","Change detection; Disentangled representation; Dual learning; Generative adversarial networks (GAN); Remote sensing","Article","Final","","Scopus","2-s2.0-85103242325"
"Jian P.; Chen K.; Cheng W.","Jian, Ping (56747520900); Chen, Keming (55683543900); Cheng, Wei (57193866252)","56747520900; 55683543900; 57193866252","GAN-Based One-Class Classification for Remote-Sensing Image Change Detection","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3066435","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103761896&doi=10.1109%2fLGRS.2021.3066435&partnerID=40&md5=63477df1a18dd572fbbb32e56425bf61","Currently, most of the supervised change detection approaches require a training data set that contains samples from both the changed and the unchanged data. However, under certain condition, such as natural disaster and military attack, the changed data samples are very few or even not available but the unchanged data are abundant. In this letter, we develop a generative adversarial networks (GANs)-based one-class classification (OCC) technique for time series remote-sensing image change detection. The proposed method is only trained with the unchanged data instead of both the changed and unchanged data. To achieve this purpose, first, spatial-spectral features are extracted from the time series remote-sensing images. Second, a GAN model is trained to detect the changes only with the extracted features of unchanged data. Remarkably, to offset the outlier errors caused by the incomplete supervision information provided by unchanged data alone, changed data, instead of unchanged data, are generated to improve power of discriminator. Finally, testing data are classified by the trained discriminator of GAN to produce a binary change map. Experimental results obtained on two optical time series remote-sensing data sets confirmed the effectiveness of our proposed method.  © 2004-2012 IEEE.","Disasters; Image classification; Time series; Adversarial networks; Natural disasters; One-class Classification; Remote sensing data; Remote sensing images; Spectral feature; Supervised change detection; Training data sets; detection method; image classification; remote sensing; satellite imagery; Remote sensing","Change detection; generative adversarial networks (GANs); one-class classification (OCC); time series","Article","Final","","Scopus","2-s2.0-85103761896"
"Li J.; Liu W.; Zhang K.; Liu B.","Li, Jiaoyue (57299552400); Liu, Weifeng (57835192100); Zhang, Kai (55769748056); Liu, Baodi (16319146900)","57299552400; 57835192100; 55769748056; 16319146900","Edge Loss for Remote Sensing Image Super-Resolution","2021","Frontiers in Artificial Intelligence and Applications","345","","","262","267","5","10.3233/FAIA210411","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123641314&doi=10.3233%2fFAIA210411&partnerID=40&md5=19ad01fe50cbbe874f489bddd25ce1e1","Remote sensing image super-resolution (SR) plays an essential role in many remote sensing applications. Recently, remote sensing image super-resolution methods based on deep learning have shown remarkable performance. However, directly utilizing the deep learning methods becomes helpless to recover the remote sensing images with a large number of complex objectives or scene. So we propose an edge-based dense connection generative adversarial network (SREDGAN), which minimizes the edge differences between the generated image and its corresponding ground truth. Experimental results on NWPU-VHR-10 and UCAS-AOD datasets demonstrate that our method improves 1.92 and 0.045 in PSNR and SSIM compared with SRGAN, respectively. © 2022 The authors and IOS Press.","Deep learning; Optical resolving power; Remote sensing; Edge difference; Edge loss; Edge-based; Ground truth; Image super resolutions; Learning methods; Performance; Remote sensing applications; Remote sensing images; Superresolution methods; Generative adversarial networks","Edge loss; Generative adversarial network; Image super-resolution; Remote sensing image","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123641314"
"Ma H.; Sun Y.; Wu N.; Li Y.","Ma, Haitao (55723465100); Sun, Yu (57428077000); Wu, Ning (56443296900); Li, Yue (55878683800)","55723465100; 57428077000; 56443296900; 55878683800","Relative Attributes-Based Generative Adversarial Network for Desert Seismic Noise Suppression","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3135034","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123530571&doi=10.1109%2fLGRS.2021.3135034&partnerID=40&md5=b7608575c7ea3ddf26f98f7b4554d44d","Since seismic data will be interfered with by a host of complicated noise during the acquisition process, the quality of the acquired seismic data is usually poor. The overlap of signals and noise makes it difficult to extract effective signals from desert seismic records. Therefore, the suppression of seismic noise and the retention of seismic signals are key issues in seismic signal processing. In order to improve the quality of the data obtained, we propose an unsupervised relative attributes-based generative adversarial network (RAGAN), which includes a generator, a discriminator, and an attribute match-aware discriminator. By encoding the data of different attributes in seismic records, the denoising task can be regarded as the conversion process of the data corresponding to the attributes. The relative attributes obtained by the difference between the target attribute and the original attribute are used to control the attributes of the data generated by the generator, so as to achieve the purpose of noise suppression. Experimental results of both synthetic and field seismic records show that the proposed method performs better than part of conventional methods.  © 2004-2012 IEEE.","Seismic prospecting; Seismic response; Seismic waves; Signal processing; Spurious signal noise; Attribute training set; Attribute-based; De-noising; Noise suppression; Relative attribute-based denoising; Seismic exploration; Seismic noise; Seismic noise suppression; Seismic records; Training sets; artificial neural network; desert; remote sensing; seismic noise; Generative adversarial networks","Attribute training set; relative attributes-based denoising; seismic exploration; seismic noise suppression","Article","Final","","Scopus","2-s2.0-85123530571"
"Christovam L.E.; Shimabukuro M.H.; Galo M.L.B.T.; Honkavaara E.","Christovam, Luiz E. (57204813121); Shimabukuro, Milton H. (7004876502); Galo, Maria de Lourdes B. T. (57195519405); Honkavaara, Eija (55927897800)","57204813121; 7004876502; 57195519405; 55927897800","Pix2pix conditional generative adversarial network with mlp loss function for cloud removal in a cropland time series","2022","Remote Sensing","14","1","144","","","","10.3390/rs14010144","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122012580&doi=10.3390%2frs14010144&partnerID=40&md5=3a3932d7698677b7ddfab61d5f85fe61","Clouds are one of the major limitations to crop monitoring using optical satellite images. Despite all efforts to provide decision-makers with high-quality agricultural statistics, there is still a lack of techniques to optimally process satellite image time series in the presence of clouds. In this regard, in this article it was proposed to add a Multi-Layer Perceptron loss function to the pix2pix conditional Generative Adversarial Network (cGAN) objective function. The aim was to enforce the generative model to learn how to deliver synthetic pixels whose values were proxies for the spectral response improving further crop type mapping. Furthermore, it was evaluated the generalization capacity of the generative models in producing pixels with plausible values for images not used in the training. To assess the performance of the proposed approach it was compared real images with synthetic images generated with the proposed approach as well as with the original pix2pix cGAN. The comparative analysis was performed through visual analysis, pixel values analysis, semantic segmentation and similarity metrics. In general, the proposed approach provided slightly better synthetic pixels than the original pix2pix cGAN, removing more noise than the original pix2pix algorithm as well as providing better crop type semantic segmentation; the semantic segmentation of the synthetic image generated with the proposed approach achieved an F1-score of 44.2%, while the real image achieved 44.7%. Regarding the generalization, the models trained utilizing different regions of the same image provided better pixels than models trained using other images in the time series. Besides this, the experiments also showed that the models trained using a pair of images selected every three months along the time series also provided acceptable results on images that do not have cloud-free areas. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Crops; Decision making; Generative adversarial networks; Geometrical optics; Photomapping; Pixels; Radar imaging; Semantic Segmentation; Semantics; Synthetic aperture radar; Time series; CGAN; Cloud removal; Crop type mappings; Custom loss function; Image translation; Image-to-image; Loss functions; Optical image; Remote-sensing; SAR to optical image translation; Synthetic images; Remote sensing","CGAN; Cloud removal; Crop type mapping; Custom loss function; Image-to-image; Remote sensing; SAR to optical image translation; Synthetic images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122012580"
"Zhang L.; Liu Y.","Zhang, Libao (35325855000); Liu, Yanan (57211089183)","35325855000; 57211089183","Remote Sensing Image Generation Based on Attention Mechanism and VAE-MSGAN for ROI Extraction","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3068271","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103797962&doi=10.1109%2fLGRS.2021.3068271&partnerID=40&md5=e7397055484da4e9b39b8a638206c169","A variety of deep learning approaches have been applied to region of interest (ROI) extraction, which is a fundamental task in the field of remote sensing image (RSI) processing. However, the unbalanced distribution of positive and negative samples in most RSIs greatly restricts the performance of these deep learning-based methods. In this study, a data augmentation method based on variational autoencoder-multiscale generative adversarial network (VAE-MSGAN) with spatial and channelwise attention (SCA) is proposed to balance the sample distribution and improve the subsequent ROI extraction results. First, we combine the original multispectral information with handcrafted texture features to make full use of the low-level visual features of RSIs. We then design a VAE-MSGAN to generate realistic RSIs with high quality and diversity. Specifically, in the generator construct, SCA blocks are introduced to adaptively recalibrate the varying importance of different channels and spatial regions. We also build a multiscale discriminator architecture to improve the visual quality of the generated samples. Finally, we compare the ROI extraction results before and after the augmentation. Our experimental results demonstrate that the proposed method can not only improve the performance of ROI extraction but also be superior to other classical generative methods.  © 2004-2012 IEEE.","Deep learning; Extraction; Image segmentation; Learning systems; Textures; Adversarial networks; Attention mechanisms; Generative methods; Learning-based methods; Region of interest; Remote sensing images; Sample distributions; Unbalanced distribution; extraction method; image analysis; remote sensing; satellite imagery; Remote sensing","Generative adversarial networks (GANs); region of interest (ROI) extraction; remote sensing; variational autoencoder (VAE)","Article","Final","","Scopus","2-s2.0-85103797962"
"Vassilo K.; Taha T.; Mehmood A.","Vassilo, Kyle (57218712112); Taha, Tarek (23013518500); Mehmood, Asif (36133731600)","57218712112; 23013518500; 36133731600","Infrared Image Super Resolution with Deep Neural Networks","2021","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2021-March","","9484045","","","","10.1109/WHISPERS52202.2021.9484045","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112856133&doi=10.1109%2fWHISPERS52202.2021.9484045&partnerID=40&md5=f61b4cd0953a391f10c4c0ff47aecdd8","Recent studies have shown that Deep Learning (DL) algorithms can significantly improve Super Resolution (SR) performance. Single image SR is useful in producing High Resolution (HR) images from their Low Resolution (LR) counterparts. The motivation for SR is the potential to assist algorithms such as object detection, localization, and classification. Insufficient work has been conducted using Generative Adversarial Networks (GANs) for SR on infrared (IR) images despite its promising ability to increase object detection accuracy by extracting more precise features from a given image. This work adopts the idea of a relativistic GAN that utilizes Residual in Residual Dense blocks (RRDBs) for feature ex- traction, a novel residual image addition, and a Pixel Transposed Convolutional Layer (PixelTCL) for up-sampling. Recent work has validated the use of GANs for Visible Light (VL) images, making them a strong candidate. The inclusion of these components produce more realistic and natural features while also receiving superior metric values.  © 2021 IEEE.","Deep learning; Deep neural networks; Infrared imaging; Neural networks; Object detection; Object recognition; Optical resolving power; Remote sensing; Signal receivers; Spectroscopy; Adversarial networks; Detection accuracy; High resolution image; Image super resolutions; Low resolution; Natural features; Residual images; Super resolution; Hyperspectral imaging","Deep Learning; Generative Adversarial Network; Infrared Imaging; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85112856133"
"Li T.; Zhang J.","Li, Tong (55728906900); Zhang, Junping (55961672900)","55728906900; 55961672900","Remote sensing image scene recognition based on adversarial learning; [对抗学习遥感图像场景识别]","2021","Journal of Image and Graphics","26","11","","2732","2740","8","10.11834/jig.200419","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119282900&doi=10.11834%2fjig.200419&partnerID=40&md5=bf11418cb852175498e9496cd50e7852","Objective: While dealing with high-resolution remote sensing image scene recognition, classical supervised machine learning algorithms are considered effective on two conditions, namely, 1) test samples should be in the same feature space with training samples, and 2) adequate labeled samples should be provided to train the model fully. Deep learning algorithms, which achieve remarkable results in image classification and object detection for the past few years, generally require a large number of labeled samples to learn the accurate parameters. The main image classification methods select training and test samples randomly from the same dataset, and adopt cross validation to testify the effectiveness of the model. However, obtaining scene labels is time consuming and expensive for remote sensing images. To deal with the insufficiency of labeled samples in remote sensing image scene recognition and the problem that labeled samples cannot be shared between different datasets due to different sensors and complex light conditions, deep learning architecture and adversarial learning are investigated. A feature transfer method based on adversarial variational autoencoder (VAE) is proposed. Method: Feature transfer architecture can be divided into three parts. The first part is the pretrain module. Given the limited samples with scene labels, the unsupervised learning model, VAE, is adopted. The VAE is unsupervised trained on the source dataset, and the encoder part in the VAE is finetuned together with classifier network using labeled samples in the source dataset. The second part is adversarial learning module. In most of the research, adversarial learning is adopted to generate new samples, while the idea is used to transfer the features from source domain to target domain in this paper. Parameters of the finetuned encoder network for the source dataset are then used to initialize the target encoder. Using the idea of adversarial training in generative adversarial networks (GAN), a discrimination network is introduced into the training of the target encoder. The goal of the target encoder is to extract features in the target domain to have as much affinity to those of the source domain as possible, such that the discrimination network cannot distinguish the features are from either the source domain or target domain. The goal of the discrimination network is to optimize the parameters for better distinction. It is called adversarial learning because of the contradiction between the purpose of encoder and discrimination network. The features extracted by the target encoder increasingly resemble those by the source encoder by training and updating the parameters of the target encoder and the discrimination network alternately. In this manner, by the time the discrimination network can no longer differentiate between source features and target features, we can assume that the target encoder can extract similar features to the source samples, and remote sensing feature transfer between the source domain and target domain is accomplished. The third part is target finetuning and test module. A small number of labeled samples in target domain is employed to finetune the target encoder and source classifier, and the other samples are used for evaluation. Result: Two remote sensing scene recognition datasets, UCMerced-21 and NWPU-RESISC45, are adopted to prove the effectiveness of the proposed feature transfer method. SUN397, a natural scene recognition dataset is employed as an attempt for the cross-view feature transfer. Eight common scene types between the three datasets, namely, baseball field, beach, farmland, forest, harbor, industrial area, overpass, and river/lake, are selected for the feature transfer task. Correlation alignment (CORAL) and balanced distribution adaptation (BDA) are used as comparisons. In the experiments of adversarial learning between two remote sensing scene recognition datasets, the proposed method boosts the recognition accuracy by about 10% compared with the network trained only by the samples in the source domain. Results improve more substantially when few samples in the target domain are involved. Compared with CORAL and BDA, the proposed method improves scene recognition accuracy by more than 3% when using a few samples in the target domain and between 10%40% without samples in the target domain. When using the information of a natural scene image, the improvement is not as much as that of a remote sensing image, but the scene recognition accuracy using the proposed feature transfer method is still increased by approximately 6% after unsupervised feature transfer and 36% after a small number of samples in the target domain are involved in finetuning. Conclusion: In this paper, an adversarial VAE-based transfer learning network is proposed. The experimental results show that the proposed adversarial learning method can make the most of sample information of other dataset when the labeled samples are insufficient in the target domain. The proposed method can achieve the feature transfer between different datasets and scene recognition effectively, and remarkably improve the scene recognition accuracy. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","Adversarial learning; Remote sensing image; Scene recognition; Transfer learning; Variational autoencoder(VAE)","Article","Final","","Scopus","2-s2.0-85119282900"
"Gong Y.; Liao P.; Zhang X.; Zhang L.; Chen G.; Zhu K.; Tan X.; Lv Z.","Gong, Yuanfu (57200512932); Liao, Puyun (57208162705); Zhang, Xiaodong (57192504939); Zhang, Lifei (57207389916); Chen, Guanzhou (56181390800); Zhu, Kun (57200511212); Tan, Xiaoliang (57202231708); Lv, Zhiyong (23111268400)","57200512932; 57208162705; 57192504939; 57207389916; 56181390800; 57200511212; 57202231708; 23111268400","Enlighten-gan for super resolution reconstruction in mid-resolution remote sensing images","2021","Remote Sensing","13","6","1104","","","","10.3390/rs13061104","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103080085&doi=10.3390%2frs13061104&partnerID=40&md5=5d1cb69627959e3776eec7312ce71b58","Previously, generative adversarial networks (GAN) have been widely applied on super resolution reconstruction (SRR) methods, which turn low-resolution (LR) images into high-resolution (HR) ones. However, as these methods recover high frequency information with what they observed from the other images, they tend to produce artifacts when processing unfamiliar images. Optical satellite remote sensing images are of a far more complicated scene than natural images. Therefore, applying the previous networks on remote sensing images, especially mid-resolution ones, leads to unstable convergence and thus unpleasing artifacts. In this paper, we propose Enlighten-GAN for SRR tasks on large-size optical mid-resolution remote sensing images. Specifically, we design the enlighten blocks to induce network converging to a reliable point, and bring the Self-Supervised Hierarchical Perceptual Loss to attain performance improvement overpassing the other loss functions. Furthermore, limited by memory, large-scale images need to be cropped into patches to get through the network separately. To merge the reconstructed patches into a whole, we employ the internal inconsistency loss and cropping-and-clipping strategy, to avoid the seam line. Experiment results certify that Enlighten-GAN outperforms the state-of-the-art methods in terms of gradient similarity metric (GSM) on mid-resolution Sentinel-2 remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Global system for mobile communications; Optical resolving power; Remote sensing; Adversarial networks; High-frequency informations; Low resolution images; Optical satellites; Remote sensing images; Similarity metrics; State-of-the-art methods; Super resolution reconstruction; Image reconstruction","Generative adversarial network; Mid-resolution remote sensing images; Super resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103080085"
"Ge Z.; Zhao Y.; Wang J.; Wang D.; Si Q.","Ge, Zhijin (57222405160); Zhao, Yanling (55726102600); Wang, Jin (57882088400); Wang, Duo (57207431247); Si, Qi (57213141618)","57222405160; 55726102600; 57882088400; 57207431247; 57213141618","Deep Feature-Review Transmit Network of Contour-Enhanced Road Extraction from Remote Sensing Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3061764","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102620609&doi=10.1109%2fLGRS.2021.3061764&partnerID=40&md5=32c8f97377ce55a2de006e9019589b77","The acquisition of road information from remote sensing images is of significant value with regard to intelligent transportation research. This study focuses on enhancing the contour-learning ability to mitigate the phenomenon of fragmented road segments and missing connections in road extraction. A novel Deep Feature-Review (FR) Transmit Network (TransNet) is proposed to review and facilitate the flow of contour features into an encoder network. Meanwhile, multiscale features are linked via a bridge between the encoder and the decoder. Compared with the state-of-the-art models such as fully convolutional network (FCN), SegNet, DeepLabv3, D-LinkNet, spatial consistency-FCN, and generative adversarial network (GAN), the proposed network achieves better overall performance for the Massachusetts Roads data set, with accuracy, precision, recall, and mean intersection-over-union (IoU) scores of 97.48%, 83.72%, 78.13%, and 0.6286%, respectively. For the DeepGlobe Road Extraction data set, the proposed network outperforms FCN, SegNet, DeepLabv3, D-LinkNet, and Deep TransNet, achieving accuracy, precision, recall, and mean IoU scores of 98.70%, 87.30%, 81.15%, and 0.7244%, respectively. Overall, these experiments indicate that the proposed network can effectively address the phenomenon of fragmented road segments and poor connectivity in remote sensing images, indicating its potential for utilization in practical intelligent transportation scenarios.  © 2004-2012 IEEE.","Convolutional neural networks; Extraction; Feature extraction; Remote sensing; Roads and streets; Signal encoding; Adversarial networks; Convolutional networks; Intelligent transportation; Learning abilities; Multi-scale features; Remote sensing images; Spatial consistency; State of the art; algorithm; artificial nest; artificial neural network; network analysis; remote sensing; satellite imagery; Image enhancement","Deep Feature-Review (FR) Transmit Network (TransNet); remote sensing (RS) images; road contour-learning; road extraction","Article","Final","","Scopus","2-s2.0-85102620609"
"Gastineau A.; Aujol J.-F.; Berthoumieu Y.; Germain C.","Gastineau, Anais (57221265780); Aujol, Jean-Francois (8283963500); Berthoumieu, Yannick (6603229955); Germain, Christian (7006254669)","57221265780; 8283963500; 6603229955; 7006254669","Generative Adversarial Network for Pansharpening with Spectral and Spatial Discriminators","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3060958","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102314699&doi=10.1109%2fTGRS.2021.3060958&partnerID=40&md5=7abb806dcd671c1c891783eb6a15f088","The pansharpening problem amounts to fusing a high-resolution panchromatic image with a low-resolution multispectral image so as to obtain a high-resolution multispectral image. Therefore, the preservation of the spatial resolution of the panchromatic image and the spectral resolution of the multispectral image is of key importance for the pansharpening problem. To cope with it, we propose a new method based on a bidiscriminator in a generative adversarial network (GAN) framework. The first discriminator is optimized to preserve textures of images by taking as input the luminance and the near-infrared band of images, and the second discriminator preserves the color by comparing the chroma components Cb and Cr. Thus, this method allows to train two discriminators, each one with a different and complementary task. Moreover, to enhance these aspects, the proposed method based on bidiscriminator, and called MDSSC-GAN SAM, considers a spatial and a spectral constraint in the loss function of the generator. We show the advantages of this new method on experiments carried out on Pléiades and World View 3 satellite images. © 1980-2012 IEEE.","Discriminators; Textures; Adversarial networks; Low resolution multispectral images; Multispectral images; Near infrared band; Panchromatic images; Satellite images; Spatial resolution; Spectral constraints; image analysis; multispectral image; numerical method; Pleiades; satellite imagery; spatial resolution; spectral analysis; WorldView; Infrared devices","Bidiscriminator; deep learning; generative adversarial network (GAN); pansharpening; remote sensing","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102314699"
"Zhang H.; Song Y.; Han C.; Zhang L.","Zhang, Hongyan (54954032600); Song, Yiyao (57221595735); Han, Chang (57037564100); Zhang, Liangpei (8359720900)","54954032600; 57221595735; 57037564100; 8359720900","Remote Sensing Image Spatiotemporal Fusion Using a Generative Adversarial Network","2021","IEEE Transactions on Geoscience and Remote Sensing","59","5","9159647","4273","4286","13","10.1109/TGRS.2020.3010530","46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099555332&doi=10.1109%2fTGRS.2020.3010530&partnerID=40&md5=d16c320c94837f71c0d3161be454faee","Due to technological limitations and budget constraints, spatiotemporal fusion is considered a promising way to deal with the tradeoff between the temporal and spatial resolutions of remote sensing images. Furthermore, the generative adversarial network (GAN) has shown its capability in a variety of applications. This article presents a remote sensing image spatiotemporal fusion method using a GAN (STFGAN), which adopts a two-stage framework with an end-to-end image fusion GAN (IFGAN) for each stage. The IFGAN contains a generator and a discriminator in competition with each other under the guidance of the optimization function. Considering the huge spatial resolution gap between the high-spatial, low-temporal (HSLT) resolution Landsat imagery and the corresponding low-spatial, high-temporal (LSHT) resolution MODIS imagery, a feature-level fusion strategy is adopted. Specifically, for the generator, we first super-resolve the MODIS images while also extracting the high-frequency features of the Landsat images. Finally, we integrate the features from the MODIS and Landsat images. STFGAN is able to learn an end-to-end mapping between the Landsat-MODIS image pairs and predicts the Landsat-like image for a prediction date by considering all the bands. STFGAN significantly improves the accuracy of phenological change and land-cover-type change prediction with the help of residual blocks and two prior Landsat-MODIS image pairs. To examine the performance of the proposed STFGAN method, experiments were conducted on three representative Landsat-MODIS data sets. The results clearly illustrate the effectiveness of the proposed method.  © 1980-2012 IEEE.","Budget control; Image fusion; Radiometers; Remote sensing; Adversarial networks; Feature level fusion; Optimization function; Phenological changes; Remote sensing images; Spatio-temporal fusions; Technological limitations; Temporal and spatial; artificial neural network; remote sensing; satellite imagery; spatiotemporal analysis; Image enhancement","Generative adversarial network (GAN); multisource satellite data; remote sensing; spatiotemporal fusion","Article","Final","","Scopus","2-s2.0-85099555332"
"Ren C.; Wang X.; Gao J.; Zhou X.; Chen H.","Ren, Caijun (57221144345); Wang, Xiangyu (57205349881); Gao, Jian (57221117891); Zhou, Xiren (57200965161); Chen, Huanhuan (35261486600)","57221144345; 57205349881; 57221117891; 57200965161; 35261486600","Unsupervised Change Detection in Satellite Images with Generative Adversarial Network","2021","IEEE Transactions on Geoscience and Remote Sensing","59","12","","10047","10061","14","10.1109/TGRS.2020.3043766","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099102005&doi=10.1109%2fTGRS.2020.3043766&partnerID=40&md5=ed1f697b949add68e1cd02fcd2e3dbbe","Detecting changed regions in paired satellite images plays a key role in many remote sensing applications. The evolution of recent techniques could provide satellite images with a very high spatial resolution (VHR) but made it challenging to apply image coregistration, and many change detection methods are dependent on its accuracy. Two images of the same scene taken at different times or from different angles would introduce unregistered objects and the existence of both unregistered areas and actual changed areas would lower the performance of many change detection algorithms in unsupervised conditions. To alleviate the effect of unregistered objects in the paired images, we propose a novel change detection framework utilizing a special neural network architecture-Generative Adversarial Network (GAN) to generate many better coregistered images. In this article, we show that the GAN model can be trained upon a pair of images by using the proposed expanding strategy to create a training set and optimizing designed objective functions. The optimized GAN model would produce better coregistered images where changes can be easily spotted and then the change map can be presented through a comparison strategy using these generated images explicitly. Compared to other deep learning-based methods, our method is less sensitive to the problem of unregistered images and makes most of the deep learning structure. Experimental results on synthetic images and real data with many different scenes could demonstrate the effectiveness of the proposed approach. © 1980-2012 IEEE.","Network architecture; Remote sensing; Satellites; Adversarial networks; Change detection algorithms; Image co-registration; Learning-based methods; Objective functions; Remote sensing applications; Unsupervised change detection; Very-high spatial resolutions; artificial neural network; remote sensing; satellite; spatial resolution; Deep learning","Change detection; deep learning; generative adversarial networks (GANs); satellite images; unsupervised","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099102005"
"Singh A.; Bruzzone L.","Singh, Abhishek (57221148913); Bruzzone, Lorenzo (7006892410)","57221148913; 7006892410","SIGAN: Spectral Index Generative Adversarial Network for Data Augmentation in Multispectral Remote Sensing Images","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3093238","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122358038&doi=10.1109%2fLGRS.2021.3093238&partnerID=40&md5=666a4b602b3b4c84986808d2c4c08de9","Generative models are typically employed to approximate the distribution of deep features. Recently, these state-of-the-art methods have been applied to estimate image transformations by an unsupervised learning approach. In this letter, a novel spectral index generative adversarial network (SIGAN) is proposed for the generation of multispectral (MS) remote sensing images. This network is defined to effectively perform data augmentation starting from a limited number of training samples in the MS remote sensing domain for training deep learning models. The SIGAN model is able to capture class-specific properties in data augmentation, by incorporating the task-specific normalized spectral indices to model class-by-class properties of MS images. Experimental results obtained on a Sentinel 2 dataset show that the proposed model provides better performance than other generative adversarial networks (GANs) in MS data generation.  © 2004-2012 IEEE.","Deep learning; Remote sensing; Data augmentation; Generative model; Image transformations; Labeled sample; Multispectral images; Multispectral remote sensing image; Remote-sensing; Spectral indices; State-of-the-art methods; Training sample; estimation method; machine learning; remote sensing; sampling; satellite data; satellite imagery; Sentinel; unsupervised classification; Generative adversarial networks","Data augmentation; labeled sample; multispectral (MS) images; remote sensing","Article","Final","","Scopus","2-s2.0-85122358038"
"Chen J.; Zhao W.; Chen X.","Chen, Jiage (56964709100); Zhao, Wenzhi (56669054000); Chen, Xi (57202621654)","56964709100; 56669054000; 57202621654","Cropland Change Detection with Harmonic Function and Generative Adversarial Network","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3023137","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121768583&doi=10.1109%2fLGRS.2020.3023137&partnerID=40&md5=2e9adac4d2d330c8afce0eacad360e42","Time-series image change detection is one of the most challenging tasks to remote sensing society. Due to complex phenological patterns of cropland, it is difficult to design an efficient strategy for cropland change detection. In this work, an integrated framework is proposed to perform change detection with a limited number of training samples. There are two improvements in this proposed cropland change detection method: 1) the harmonic function is utilized to fill the missing data within a time-series image stack by considering phenological patterns of cropland and 2) the CropGAN was developed to generate realistic samples for training data set enrichment. Compared to the traditional change detection methods, the proposed strategy able to detect different kinds of cropland changes even with few number of samples. Experiments on a Landsat time-series image stack demonstrated that the proposed CropGAN can significantly improve change detection accuracies, given a limited number of labeled samples.  © 2004-2012 IEEE.","Harmonic functions; Image enhancement; Remote sensing; Time series; Change detection; Detection methods; Efficient strategy; Generative adversarial network; Image change detection; Image stacks; Integrated frameworks; Remote-sensing; Time-series imagery; Times series; agricultural labor; agricultural land; detection method; network analysis; Generative adversarial networks","Change detection; Generative adversarial network (GAN); Time-series imagery","Article","Final","","Scopus","2-s2.0-85121768583"
"Panchal P.; Raman V.C.; Baraskar T.; Sinha S.; Purohit S.; Modi J.","Panchal, Poojan (57216312490); Raman, Vignesh Charan (57216314498); Baraskar, Trupti (36871711400); Sinha, Shambhavi (57336512200); Purohit, Swaraj (57337347400); Modi, Jaynam (57337347500)","57216312490; 57216314498; 36871711400; 57336512200; 57337347400; 57337347500","Reconstruction of Missing Data in Satellite Imagery Using SN-GANs","2022","Lecture Notes in Networks and Systems","286","","","629","638","9","10.1007/978-981-16-4016-2_60","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119013878&doi=10.1007%2f978-981-16-4016-2_60&partnerID=40&md5=d26cfbaf737f3013a7a3d9b000135b61","In the field of remote sensing satellite imagery, malfunctions in the available raw data are prominent. Especially in Short-Wave Infrared (SWIR) detectors used in satellite imaging cameras, which suffer from dropouts in pixel and line direction in raw data. With the recent development in generative adversarial networks and its vast application in inpainting the missing data, the possibility to predict and fill in the missing data accurately with contextual attention has become prevalent. This paper presents SN-GANs (SN-generative adversarial networks) which is two-staged architecture, and it is based on the concept of feed forward neural networks with contextual attention layers. While reconstructing the corrupted part of the images, the model takes surrounding pixels into consideration. Moreover, this architecture is adept enough to fill in the multiple lines and pixel dropouts efficiently even in super-resolution satellite images. The available traditional methods fail to address the loss of data that incurs, while inpainting a 16-bit raw image because they are effective enough for 8-bit RGB images. SN-GANs have effectively resolved this issue with a lossless image inpainting method for 16-bit satellite images as it retains the features of non-corrupted data. The performance of the model is evaluated using similarity metrics like structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR) and mean-squared error (MSE). © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Column-dropouts; Deep learning; GANS; Image reconstruction; Satellite imagery; SN-GANs; SWIR","Conference paper","Final","","Scopus","2-s2.0-85119013878"
"Burnel J.-C.; Fatras K.; Flamary R.; Courty N.","Burnel, Jean-Christophe (57219689782); Fatras, Kilian (57216858015); Flamary, Remi (36004549700); Courty, Nicolas (23088029900)","57219689782; 57216858015; 36004549700; 23088029900","Generating Natural Adversarial Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3110601","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115697331&doi=10.1109%2fTGRS.2021.3110601&partnerID=40&md5=d4bf113e6b9d863b7a97d86936f254ee","Over the last years, remote sensing image (RSI) analysis has started resorting to using deep neural networks to solve most of the commonly faced problems, such as detection, land cover classification, or segmentation. As far as critical decision-making can be based upon the results of RSI analysis, it is important to clearly identify and understand potential security threats occurring in those machine learning algorithms. Notably, it has recently been found that neural networks are particularly sensitive to carefully designed attacks, generally crafted given the full knowledge of the considered deep network. In this article, we consider the more realistic but challenging case where one wants to generate such attacks in the case of a black-box neural network. In this case, only the prediction score of the network is accessible, on a specific dataset. Examples that lure away the network's prediction, while being perceptually similar to real images, are called natural or unrestricted adversarial examples. We present an original method to generate such examples based on a variant of the Wasserstein generative adversarial network. We demonstrate its effectiveness on natural adversarial hyperspectral image generation and image modification for fooling a state-of-the-art detector. Among others, we also conduct a perceptual evaluation with human annotators to better assess the effectiveness of the proposed method. Our code is available for the community: https://github.com/PythonOT/ARWGAN. © 2021 IEEE.","Decision making; Deep neural networks; Electric inverters; Image segmentation; Learning algorithms; Perturbation techniques; Remote sensing; Spectroscopy; Adversarial example; Deep learning; Generative model; Generator; Inverter; Neural-networks; Perturbation method; Remote sensing.; Remote-sensing; Security; algorithm; artificial neural network; detection method; image analysis; instrumentation; machine learning; remote sensing; satellite imagery; Generative adversarial networks","Generative adversarial networks; Generators; Inverters; Neural networks; Perturbation methods; Security; Training","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85115697331"
"Chen L.; Wang H.; Meng X.","Chen, Lu (57200280918); Wang, Hongjun (57269384100); Meng, Xianghao (57226017062)","57200280918; 57269384100; 57226017062","Remote sensing image dataset expansion based on generative adversarial networks with modified shuffle attention","2021","Sensors","21","14","4867","","","","10.3390/s21144867","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110194893&doi=10.3390%2fs21144867&partnerID=40&md5=ee438a7c6da6a71fa00059ef5701124f","With the development of science and technology, neural networks, as an effective tool in image processing, play an important role in gradual remote-sensing image-processing. However, the training of neural networks requires a large sample database. Therefore, expanding datasets with limited samples has gradually become a research hotspot. The emergence of the generative adversarial network (GAN) provides new ideas for data expansion. Traditional GANs either require a large number of input data, or lack detail in the pictures generated. In this paper, we modify a shuffle attention network and introduce it into GAN to generate higher quality pictures with limited inputs. In addition, we improved the existing resize method and proposed an equal stretch resize method to solve the problem of image distortion caused by different input sizes. In the experiment, we also embed the newly proposed coordinate attention (CA) module into the backbone network as a control test. Qualitative indexes and six quantitative evaluation indexes were used to evaluate the experimental results, which show that, compared with other GANs used for picture generation, the modified Shuffle Attention GAN proposed in this paper can generate more refined and highquality diversified aircraft pictures with more detailed features of the object under limited datasets. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Databases, Factual; Image Processing, Computer-Assisted; Neural Networks, Computer; Remote Sensing Technology; Image enhancement; Remote sensing; Training aircraft; Adversarial networks; Back-bone network; Development of science and technologies; Equal stretches; Image distortions; Quantitative evaluation; Remote sensing image processing; Remote sensing images; factual database; image processing; remote sensing; Neural networks","Limited input dataset; Modified shuffle attention GAN; Picture generation; Resize method","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110194893"
"Yang Z.; Wang Y.","Yang, Zhiwei (57225126998); Wang, Yunyan (55734131800)","57225126998; 55734131800","Image Enhancement and Improvement Algorithm Based on Esrgan Singal Frame Remote Sensing Image","2021","Journal of Physics: Conference Series","1952","2","022012","","","","10.1088/1742-6596/1952/2/022012","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109215707&doi=10.1088%2f1742-6596%2f1952%2f2%2f022012&partnerID=40&md5=0923ff999a706afabade40947471dc47","Traditional spline interpolation algorithm for reconstruction of visual effects are not good, based on Super Resolution against Network (Super - Resolution Generative Adversarial Network, SRGAN) edge character to deal with such problems as imperfect, using a generated based on the enhanced Super Resolution against Network to improve the Resolution of ordinary optical remote sensing images, the first Network generated by high Resolution data sets to train (G network), then lower Resolution test data model test, The test results and real results are put into the discrimination network (D network) to get the adversarial loss, and then the generated network is modified according to the adversarial loss. The superiority of the network is improved by introducing dense residuals to SRGAN, modifying the judgment object of the discriminator to be relatively real, and using the eigenvalue before activation to improve the perceived loss. The desert, farmland, forest and mountain data were tested on AID data set, and the algorithm in this paper could obtain the recomposition of the real image more closely. Compared with SRResNet and SRGAN algorithms, PSNR improved by about 4.0db and SSIM improved by about 0.14. This method improves the feature comprehensiveness by increasing the network fineness degree, and USES the modified perception loss to get the brightness closer to the real image, which is beneficial to improve the quality of single frame remote sensing image.  © Published under licence by IOP Publishing Ltd.","Discriminators; Edge detection; Eigenvalues and eigenfunctions; Interpolation; Optical data processing; Optical resolving power; Remote sensing; Adversarial networks; High resolution data; Lower resolution; Optical remote sensing; Remote sensing images; Spline interpolation; Super resolution; Visual effects; Image enhancement","Confrontation; image; network; Reconstruction error; Remote; sensing; Super resolution","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85109215707"
"Karaca A.C.; Kara O.; Güllü M.K.","Karaca, Ali Can (55292760600); Kara, Ozan (57221815490); Güllü, Mehmet Kemal (55666247200)","55292760600; 57221815490; 55666247200","MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks","2021","Journal of Visual Communication and Image Representation","81","","103385","","","","10.1016/j.jvcir.2021.103385","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119176940&doi=10.1016%2fj.jvcir.2021.103385&partnerID=40&md5=0dbc36c8a377487bfb873208d19d64b7","Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications. © 2021 Elsevier Inc.","Big data; Deep learning; Generative adversarial networks; Geographical regions; Image compression; Mean square error; Signal to noise ratio; Change detection; Compression methods; Multi-spectral; Multi-temporal; Multi-temporal image; Multispectral images; Multispectral-image compression; Reflected energy; Remote-sensing; Spectral correlation; Remote sensing","Big data; Generative adversarial networks; Multispectral image compression; Multitemporal images; Remote sensing","Article","Final","","Scopus","2-s2.0-85119176940"
"Ahmad T.; Chen X.; Saqlain A.S.; Ma Y.","Ahmad, Tanvir (57214283388); Chen, Xiaona (57214780944); Saqlain, Ali Syed (57224473435); Ma, Yinglong (8348553300)","57214283388; 57214780944; 57224473435; 8348553300","FPN-GAN: Multi-class Small Object Detection in Remote Sensing Images","2021","2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics, ICCCBDA 2021","","","9442506","478","482","4","10.1109/ICCCBDA51879.2021.9442506","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107674950&doi=10.1109%2fICCCBDA51879.2021.9442506&partnerID=40&md5=0b38878451c4ba35ccf8732f0ae9ad20","Despite the recent dramatic advances in object detection, detecting a small object in general and in remote sensing images is still a challenging problem. One main reason for this is the appearance of small objects in images. Specifically low resolution and noisy representation makes it hard to detect small objects. We tickle down this problem by proposing a novel object detector based on Generative adversarial network (GAN), which we called FPN-GAN in short. The proposed method is composed of GAN, Resnet-50 as a backbone, and Feature Pyramid Network for detection. We combine both of these methods to achieve a single end to end GAN model for multi class-small object detection and image enhancement simultaneously. Extensive experiments on a challenging benchmark DIOR remote sensing dataset demonstrate the superiority of the proposed method for small objects as well as large and the medium size objects. © 2021 IEEE.","Advanced Analytics; Cloud computing; Image enhancement; Large dataset; Object recognition; Remote sensing; Adversarial networks; Feature pyramid; Low resolution; Medium size; Object detectors; Remote sensing images; Small object detection; Small objects; Object detection","generative adversarial networks (GANs); object detection; remote sensing","Conference paper","Final","","Scopus","2-s2.0-85107674950"
"Chen X.; Huang Y.","Chen, Xiang (57213456857); Huang, Yufeng (57201386016)","57213456857; 57201386016","Memory-Oriented Unpaired Learning for Single Remote Sensing Image Dehazing","2022","IEEE Geoscience and Remote Sensing Letters","19","","3511705","","","","10.1109/LGRS.2022.3167476","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128645888&doi=10.1109%2fLGRS.2022.3167476&partnerID=40&md5=a9b8fd597190599bcc1bd16be52cec7c","Remote sensing image dehazing (RSID) is an extremely challenging problem due to the irregular and nonuniform distribution of haze. The existing RSID methods achieve excellent performance using deep learning; however, relying on paired synthetic data is limited to their generality in various haze distribution. In this letter, we present a memory-oriented generative adversarial network (MO-GAN), which tries to capture the desired hazy features in an unpaired learning manner toward single RSID. For better extracting the haze-relevant features, a novel multistage attentive-recurrent memory module is designed to guide an autoencoder neural network, which can record the various appearances of haze distribution at different stages. To well differentiate fake images from real ones, a dual region discriminator is constructed to handle spatially varying haze densities in global and local regions. Extensive experiments demonstrate that our designed MO-GAN outperforms the recent comparing approaches on the various frequently used datasets, especially in real world nonuniform haze conditions. The source code is released in https://github.com/cxtalk/MO-GAN.  © 2004-2012 IEEE.","Demulsification; Image reconstruction; Recurrent neural networks; Remote sensing; Attention mechanisms; Features extraction; Generative adversarial network; Haze removal; Images reconstruction; Memory modules; Memory network; Remote sensing images; Task analysis; Unpaired learning; artificial neural network; haze; image analysis; satellite imagery; Generative adversarial networks","Attention mechanism; generative adversarial networks (GANs); haze removal; memory network; remote sensing (RS) image; unpaired learning","Article","Final","","Scopus","2-s2.0-85128645888"
"Ma C.; Gao H.","Ma, Conghui (56562155900); Gao, Hongchao (57471481400)","56562155900; 57471481400","A GAN based method for SAR and optical images fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12166","","121664F","","","","10.1117/12.2617316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125474916&doi=10.1117%2f12.2617316&partnerID=40&md5=04c5d0df6dd2dfd1a20109005ebc35dd","With the development of the remote sensing technology, the availability of satellite images has been dramatically increased with high quantity and quality. Diverse information can be obtained from these multiple imaging sources. For example, synthetic aperture radar (SAR) imagery measures physical properties of the observed scene in all-weather and full-time situation and follows a range-based imaging geometry, while optical imagery measures chemical characteristics of the scene and follows a perspective imaging geometry and needs both daylight and a cloudless sky. These multisource remote sensing images, once fused together, provide a more comprehensive interpretation of remote sensing scenes. Recent advances in Generative adversarial networks (GANs) have shown great promise in translating imagery between modalities, as well in the generation of high resolution and realistic imagery. In this paper, a GAN architecture is used to solve the task of fusing SAR and optical remote sensing imagery. The network learns the mapping between input and output image, and learns a loss function to train this mapping. Specifically, the generated network is divided into two parts, encoding and decoding. The fused image including SAR intensity and texture information is generated by the generator. Other details of the optical image are added to the fusion image gradually by the discriminator. The structural similarity loss function of GAN is to make the training of GAN model more accurate on the whole structure. Experiments on Sentinel-1and Sentinel-2 imagery confirm the effectiveness and efficiency of the proposed method.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Generative adversarial networks; Geometrical optics; Image fusion; Mapping; Radar imaging; Remote sensing; Textures; Imaging geometry; Learn+; Loss functions; Multiple imaging; Network-based; Optical image; Optical imagery; Remote sensing technology; Satellite images; Synthetic aperture radar images; Synthetic aperture radar","Generative adversarial network; Image fusion; Optical imagery; SAR","Conference paper","Final","","Scopus","2-s2.0-85125474916"
"Bhambani K.; Takalikar M.","Bhambani, Krisha (57222110246); Takalikar, Mukta (57202432369)","57222110246; 57202432369","DeCloud GAN: An Advanced Generative Adversarial Network for Removing Cloud Cover in Optical Remote Sensing Imagery","2021","ACM International Conference Proceeding Series","","","","25","30","5","10.1145/3507623.3507628","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128882302&doi=10.1145%2f3507623.3507628&partnerID=40&md5=dddf1b8d6d5d39c5c809594de4088e70","Optical Remote Sensing imagery has several applications in monitoring the states of natural and man-made features around the globe. However, due to clouds and other climatic conditions, information extracted from the imagery retrieved is very limited. Deep learning has often been used in several image processing and remote sensing tasks. In this work, we propose the usage of generative adversarial networks to remove clouds and other climatic interference from high-resolution remote sensing imagery. We have trained and tested upon the Remote sensing Image Cloud rEmoving dataset (RICE). The novel network(DeCloud GAN) we propose, makes use of residual UNets and pixel shuffle layers in the generator, which yield high quality cloudless satellite images. We have tested 4 methods for comparison, and have found that DeCloudGAN achieves the best performance on two main metrics, peak signal to noise ratio (PSNR) and structural similarity index (SSIM), to measure similarity in visual perception of the produced and target images.  © 2021 ACM.","Deep learning; Image processing; Optical data processing; Remote sensing; Signal to noise ratio; Climatic conditions; Cloud cover; Deep learning; Image generations; Images processing; Optical imagery; Optical remote-sensing imagery; Remote-sensing; Removing cloud; Sensing tasks; Generative adversarial networks","Deep Learning; Generative Adversarial Networks; Image generation; Optical Imagery; Remote Sensing","Conference paper","Final","","Scopus","2-s2.0-85128882302"
"Oliveira D.A.B.; Semin D.G.; Zaytsev S.","Oliveira, Dario A. B. (27567900100); Semin, Daniil G. (57194723100); Zaytsev, Semen (57223088188)","27567900100; 57194723100; 57223088188","Self-Supervised Ground-Roll Noise Attenuation Using Self-Labeling and Paired Data Synthesis","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9235523","7147","7159","12","10.1109/TGRS.2020.3029914","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111117831&doi=10.1109%2fTGRS.2020.3029914&partnerID=40&md5=fac52096d97ac064fb6f2dc4c89fff84","Seismic exploration is a complex process that depends on different sources of information. An essential one is seismic imaging, and much of its interpretation performance relies on high-quality processing, which is currently still very dependent on prone-to-error human mediation. Automation of such processing steps is necessary to reduce the amount of time to treat seismic data - usually months - and improve the outcome overall quality by reducing the inherent subjectivity in the process. One of the most critical steps in seismic processing is noise suppression, and ground roll is one of the most challenging and everyday noises observed in seismic prestack data. In this article, we propose a self-supervised two-step approach to attenuate ground-roll noise in seismic prestack images. First, we detect ground-roll-affected area using convolutional neural networks, and then, we filter ground-roll noise in the detected area using conditional generative adversarial networks (cGANs). For each of these steps, we propose to build paired noisy/noise-free training sets with no supervision or reference data, hence creating a self-supervised pipeline for filtering ground-roll noise. Our two-stage approach enables noise suppression in the affected area while preserving the signal in unaffected areas. In addition, we propose to refactor conventional qualitative metrics in the industry into quantitative scores disregarding any reference data to evaluate ground-roll suppression for different geologies and report reliable results compared with expert filtering.  © 1980-2012 IEEE.","Convolutional neural networks; Seismic prospecting; Spurious signal noise; Adversarial networks; Complex Processes; Noise attenuation; Noise suppression; Seismic exploration; Seismic processing; Sources of informations; Two stage approach; noise pollution; numerical model; remote sensing; supervised classification; wave attenuation; Seismology","Deep learning; geophysical image processing; noise attenuation; self-supervised learning","Article","Final","","Scopus","2-s2.0-85111117831"
"Vandal T.J.; McDuff D.; Wang W.; Duffy K.; Michaelis A.; Nemani R.R.","Vandal, Thomas J. (56916071800); McDuff, Daniel (36106217600); Wang, Weile (16641272900); Duffy, Kate (57207765032); Michaelis, Andrew (8308107100); Nemani, Ramakrishna R. (35509463200)","56916071800; 36106217600; 16641272900; 57207765032; 8308107100; 35509463200","Spectral Synthesis for Geostationary Satellite-to-Satellite Translation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3088686","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112470844&doi=10.1109%2fTGRS.2021.3088686&partnerID=40&md5=2b056cfc093c94297218f4087a4afdeb","Earth-observing satellites carrying multispectral sensors are widely used to monitor the physical and biological states of the atmosphere, land, and oceans. These satellites have different vantage points above the Earth and different spectral imaging bands resulting in inconsistent imagery from one to another. This presents challenges in building downstream applications. What if we could generate synthetic bands for existing satellites from the union of all domains? We tackle the problem of generating synthetic spectral imagery for multispectral sensors as an unsupervised image-to-image translation problem modeled with a variational autoencoder (VAE) and generative adversarial network (GAN) architecture. Our approach introduces a novel shared spectral reconstruction loss to constrain the high-dimensional feature space of multispectral images. Simulated experiments performed by dropping one or more spectral bands show that cross-domain reconstruction outperforms measurements obtained from a second vantage point. Our proposed approach enables the synchronization of multispectral data and provides a basis for more homogeneous remote sensing datasets.  © 1980-2012 IEEE.","Earth atmosphere; Remote sensing; Spectroscopy; Adversarial networks; Downstream applications; Earth observing satellite; High-dimensional feature space; Multispectral images; Multispectral sensors; Simulated experiments; Spectral reconstruction; geostationary satellite; satellite data; spectral analysis; Geostationary satellites","Geophysical image processing; neural networks (NNs); remote sensing; unsupervised learning","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85112470844"
"Chen J.; Zhu J.; Guo Y.; Sun G.; Zhang Y.; Deng M.","Chen, Jie (56075077300); Zhu, Jingru (57216241574); Guo, Ya (57221663466); Sun, Geng (57215896562); Zhang, Yi (57840775500); Deng, Min (27267549700)","56075077300; 57216241574; 57221663466; 57215896562; 57840775500; 27267549700","Unsupervised Domain Adaptation for Semantic Segmentation of High-Resolution Remote Sensing Imagery Driven by Category-Certainty Attention","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3140108","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122581209&doi=10.1109%2fTGRS.2021.3140108&partnerID=40&md5=81bb8cfe9fe50f1300923000110b458f","Semantic segmentation is an important task of analysis and understanding of high-resolution remote sensing images (HRSIs). The deep convolutional neural network (DCNN)-based model shows their excellent performance in remote sensing image semantic segmentation. Most of the existing HRSI semantic segmentation methods are only designed for a very limited data domain, that is, the training and test images are from the same dataset. The accuracy drops sharply once a model trained on a certain dataset is used for cross-domain prediction due to the difference in feature distribution of the dataset. To this end, this article proposes an unsupervised domain adaptation framework based on adversarial learning for HRSI semantic segmentation. This framework uses high-level feature alignment to narrow the difference between the source and target domains at the semantic level. It uses the category-certainty attention module to reduce the attention of the classifier on category-level aligned features and increase the attention on category-level unaligned features. Experimental results show that the proposed method performs favorably against the state-of-the-art methods in cross-domain segmentation. © 1980-2012 IEEE.","Deep neural networks; Generative adversarial networks; Image analysis; Remote sensing; Semantic Segmentation; Semantic Web; Statistical tests; Adaptation models; Category-certainty attention; Domain adaptation; Features extraction; High-resolution remote sensing images; Image semantics; Images segmentations; Remote-sensing; Semantic segmentation; Task analysis; algorithm; detection method; image resolution; remote sensing; satellite imagery; segmentation; Semantics","Category-certainty attention; Domain adaptation; Generative adversarial networks; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85122581209"
"Liu N.; Celik T.; Li H.-C.","Liu, Nanqing (57221303417); Celik, Turgay (35101499300); Li, Heng-Chao (24174798500)","57221303417; 35101499300; 24174798500","MSNet: A Multiple Supervision Network for Remote Sensing Scene Classification","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3043020","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098796056&doi=10.1109%2fLGRS.2020.3043020&partnerID=40&md5=a0582eeca7e750a7856b80f29503fb09","Remote sensing scene classification is a complex task due to large intraclass variations in object appearances with a small number of samples per class and high interclass similarities due to shared objects in different classes, which usually cause model overfitting and high interclass confusion. To address these challenges, a multiple supervision approach, called multiple supervision network (MSNet), consisting of the ResNet-50 backbone, a feature discriminative branch (FDB), and a feature confusion branch (FCB) is proposed in this letter. The FDB selects discriminative features per class and suppresses peaks in feature maps to examine more informative regions with lower feature magnitudes. Meanwhile, the FCB reduces overfitting by introducing confusion to the input of a fully connected layer which also enhances the robust features. The FDB and FCB are only used in training of the backbone and not used in inference. Thus, the proposed method does not introduce additional computing time on the backbone while it significantly boosts its performance in scene classification. The experimental results show that MSNet outperforms the methods considered in this letter.  © 2004-2012 IEEE.","Electrical engineering; Geotechnical engineering; Computing time; Different class; Discriminative features; Intra-class variation; Number of samples; Object appearance; Scene classification; Shared objects; accuracy assessment; image analysis; remote sensing; satellite data; supervised classification; Remote sensing","Convolutional neural network (CNN); deep learning; generative adversarial network (GAN); remote sensing scene classification (RSSC)","Article","Final","","Scopus","2-s2.0-85098796056"
"Wang J.; Liu B.; Zhou Y.; Zhao J.; Xia S.; Yang Y.; Zhang M.; Ming L.M.","Wang, Jiaqi (57881540500); Liu, Bing (56499529900); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Xia, Shixiong (16644270400); Yang, Yuancan (57220579481); Zhang, Man (57212870780); Ming, Liu Ming (57220572351)","57881540500; 56499529900; 35480110700; 57138970300; 16644270400; 57220579481; 57212870780; 57220572351","Semisupervised Multiscale Generative Adversarial Network for Semantic Segmentation of Remote Sensing Image","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3036823","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097447657&doi=10.1109%2fLGRS.2020.3036823&partnerID=40&md5=f45fce77b9a7fb49661ec932c7aa0072","Semantic segmentation of remote sensing images based on deep neural networks has gained wide attention recently. Although many methods have achieved amazing performance, they need large amounts of labeled images to distinguish the differences in angle, color, size, and other aspects for small targets in remote sensing data sets. However, with a few labeled images, it is difficult to extract the key features of small targets. We propose a semisupervised multiscale generative adversarial network (GAN), which not only utilizes the multipath input and atrous spatial pyramid pooling (ASPP) module but leverages unlabeled images and semisupervised learning strategy to improve the performance of small target segmentation in semantic segmentation when labeled data amount is small. Experimental results show that our model outperforms state-of-the-art methods with insufficient labeled data.  © 2004-2012 IEEE.","Deep neural networks; Image enhancement; Image segmentation; Labeled data; Semantic Web; Semantics; Adversarial networks; Labeled images; Remote sensing data; Remote sensing images; Semantic segmentation; Semi-supervised; Spatial pyramids; State-of-the-art methods; image classification; satellite imagery; segmentation; semisubmersible platform; Remote sensing","Generative adversarial network (GAN); multiscale; remote sensing; semantic segmentation; semisupervised","Article","Final","","Scopus","2-s2.0-85097447657"
"Xu Q.; Yuan X.; Ouyang C.","Xu, Qingsong (57214084444); Yuan, Xin (57188549342); Ouyang, Chaojun (23390621600)","57214084444; 57188549342; 23390621600","Class-Aware Domain Adaptation for Semantic Segmentation of Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2020.3031926","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097165347&doi=10.1109%2fTGRS.2020.3031926&partnerID=40&md5=6e1e6687c59073992c126f44a8b9d85b","Unsupervised domain adaptation (UDA) for the semantic segmentation of remote sensing images is challenging since the same class of objects may have different spectra while the different class of objects may have the same spectrum. To address this issue, we propose a class-aware generative adversarial network (CaGAN) for UDA semantic segmentation of multisource remote sensing images, which explicitly models the discrepancies of intraclass and the interclass between the source domain images with labels and the target domain images without labels. Specifically, first, to enhance the global domain alignment (GDA), we propose a transferable attention alignment (TAA) procedure to add more fine-grained features into the adversarial learning framework. Then, we propose a novel class-aware domain alignment (CDA) approach in semantic segmentation. CDA mainly includes two parts: the first one is adaptive category selection, which is to alleviate the class imbalance and select the reliable per-category centers in the source and target domains; the second one is adaptive category alignment, which is to model the intraclass compactness and interclass separability from source-only, target-only, and joint source and target images. Finally, the CDA plays as a penalty of GDA to train GaGAN in an alternating and iterative manner. Experiments on domain adaptation of space to space, spectrum to spectrum, both space-to-space and spectrum-to-spectrum data sets demonstrate that CaGAN outperforms the current state-of-the-art methods, which may serve as a starting point and baseline for the comprehensive applications of semantic segmentation in cross-space and cross-spectrum remote sensing images.  © 1980-2012 IEEE.","Alignment; Image segmentation; Iterative methods; Semantics; Space optics; Adversarial learning; Adversarial networks; Class imbalance; Domain adaptation; Multisource remote sensing images; Remote sensing images; Semantic segmentation; State-of-the-art methods; algorithm; image analysis; satellite imagery; segmentation; Remote sensing","Class-aware domain alignment (CDA); class-aware generative adversarial network (CaGAN); cross-scene and cross-spectrum remote sensing images; global domain alignment (GDA); unsupervised domain adaptation (UDA) semantic segmentation","Article","Final","","Scopus","2-s2.0-85097165347"
"Han Z.; Wang C.; Fu Q.; Zhao B.","Han, Zishuo (55927549000); Wang, Chunping (35070727200); Fu, Qiang (57198712117); Zhao, Bin (57215339347)","55927549000; 35070727200; 57198712117; 57215339347","Remote Sensing Image Mode Translation by Spatial Disentangled Representation Based GAN; [基于空间分离表征GAN的遥感图像模式互转]","2021","Guangxue Xuebao/Acta Optica Sinica","41","7","0728003","","","","10.3788/AOS202141.0728003","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106369782&doi=10.3788%2fAOS202141.0728003&partnerID=40&md5=0b3dcff324c6c563d3274856d2585555","Resting on the translation framework of spatially separated images, we proposed a cycle-consistent generative adversarial network (GAN) based on spatial disentangled representation to address the large mode difference and difficult translation between synthetic aperture radar images and optical remote sensing images. The proposed model separates images into style and content features by a deeper network layer and jump connection. Furthermore, the content features are translated by content mapping learning and combined with target style features for image translation. In addition, PatchGAN, as the discriminator, enhances the image detail generation, and target error loss and generation & reconstruction loss are introduced to limit the translation task to one-to-one mapping, thus reducing the information added and constraining the GAN. The experimental results in SEN1-2, SARptical, and WHU-SEN-City datasets show that compared with other image translation algorithms, the proposed method can translate two types of remote sensing images and generate images of high resolution, complete detail features, and strong authenticity. © 2021, Chinese Lasers Press. All right reserved.","Mapping; Network layers; Radar imaging; Remote sensing; Synthetic aperture radar; Adversarial networks; High resolution; Image translation; Jump connections; One-to-one mappings; Optical remote sensing; Remote sensing images; Style and contents; Image enhancement","Cycle-consistent adversarial networks; Image translation; Optical remote sensing image; Remote sensing; Synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85106369782"
"Wei Z.; Li C.-L.; Shen Y.-A.; Liu Y.-F.; Zhou P.-C.","Wei, Zhe (55900977300); Li, Cong-Li (56404814600); Shen, Yan-An (57194940455); Liu, Yong-Feng (57219556153); Zhou, Pu-Cheng (7401848718)","55900977300; 56404814600; 57194940455; 57219556153; 7401848718","Thick Cloud Region Content Generation of UAV Image Based on Two-Stage Model; [基于两阶段模型的无人机图像厚云区域内容生成]","2021","Jisuanji Xuebao/Chinese Journal of Computers","44","11","","2233","2247","14","10.11897/SP.J.1016.2021.02233","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118560876&doi=10.11897%2fSP.J.1016.2021.02233&partnerID=40&md5=473e68e02950d6da1fb590a3ba2c4860","Cloud covering often causes the loss of information on the underlying surface of the image during UAV flight. However, existing cloud region estimating methods based on multi-spectral and multi-temporal mainly orient to satellite remote sensing images, and cannot be applied directly to UAV images. How to use the available information to reasonably infer the content covered by the thick cloud, so as to improve the image availability, remains an urgent problem to be solved. With image inpainting theory, which regards the covered regions as the missing or damaged parts of the image and devotes to reconstruct their consistency, a two-stage thick cloud region content generating method based on DCGAN is proposed for the characteristics of single spectrum, short flight time and random flight path of UAV imaging. The two-stage model consists of a first stage DCGAN, an image retrieval module, an affine transformation net and a second stage DCGAN from front to back sequentially. The first stage DCGAN takes the masked image in, and generates preliminary completion result. In order to make the most of the homogeneous samples in dataset, an image retrieval module and an affine transformation is added. BoW retrieval algorithm is used to search for top N homogeneous samples of the image completed in the first stage, and the affine transform network is designed to align them with attention mechanisms for the second stage. The second stage DCGAN, which has the same structure as the first, takes the preliminary completion result and the output of the affine transform network, and generates the refined result in the end. The 4 parts constitute a complete forward form. This model makes image generation easier to utilize the information of the known image with identical distribution, and solves the difficulty of feature extraction with multiple distributions in UAV images, and addresses the limitation that existing inpainting methods rely heavily on single image. This paper also improves the structure of classical DCGAN, and designs a new joint loss function, combining local and global adversarial loss with the perceptual loss and the total variation loss, which not only prevent blurry result, but also generate pixels that approximate the true semantic distribution with less noise. In the training phase, image retrieval module is trained firstly to get the whole bags of visual word and clusters. Then affine transform network is trained by affined samples with manual random setting. 2-stage DCGANs are trained end-to-end using Adam optimization alternately with samples generated by the first 2 module. In the testing phase, these modules are cascaded and worked at fixed parameters. Simulation experiments with masks and real cloud-containing image are carried out respectively. On the central 1/4 mask of the simulation experiments, PSNR and SSIM are improved by 0.3214~3.6793 and 0.0005~0.0543, and average pixel L1 loss, NIQE and BLIINDS are decreased by 0.0171~4.1120, 0.0565~4.7440 and 0.8841~4.2586, compared with other classical methods, respectively. In the real cloud-containing image experiments, NIQE and BLIINDS indexes are decreased by 0.1062~1.8992 and 1.0903~5.6495. Visual effects under the same conditions are shown and analyzed. The subjective and objective experimental results show that compared with the classical method, the proposed method has certain advantages in semantic rationality, information accuracy and visual naturalness, and provides a better solution for single spectral image pixel value prediction against thick cloud covering. © 2021, Science Press. All right reserved.","Affine transforms; Antennas; Deep learning; Feature extraction; Image enhancement; Image retrieval; Remote sensing; Semantics; Unmanned aerial vehicles (UAV); Affine transformations; Classical methods; Deep convolutional generative adversarial net; Deep learning; Homogeneous samples; Image generations; Image-based; Two stage model; Unmanned aerial vehicle image; Vehicle images; Generative adversarial networks","Deep convolutional generative adversarial net; Deep learning; Image generation; Two-stage model; Unmanned aerial vehicle image","Article","Final","","Scopus","2-s2.0-85118560876"
"Gao J.; Yuan Q.; Li J.; Zhang H.; Su X.","Gao, Jianhao (57211516266); Yuan, Qiangqiang (36635300800); Li, Jie (57214207213); Zhang, Hai (57192694132); Su, Xin (57200950410)","57211516266; 36635300800; 57214207213; 57192694132; 57200950410","Cloud removal with fusion of high resolution optical and SAR images using generative adversarial networks","2020","Remote Sensing","12","1","191","","","","10.3390/RS12010191","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083956677&doi=10.3390%2fRS12010191&partnerID=40&md5=90bf07a924911d7fca557a2c635a2b74","The existence of clouds is one of the main factors that contributes to missing information in optical remote sensing images, restricting their further applications for Earth observation, so how to reconstruct the missing information caused by clouds is of great concern. Inspired by the image-to-image translation work based on convolutional neural network model and the heterogeneous information fusion thought, we propose a novel cloud removal method in this paper. The approach can be roughly divided into two steps: in the first step, a specially designed convolutional neural network (CNN) translates the synthetic aperture radar (SAR) images into simulated optical images in an object-to-object manner; in the second step, the simulated optical image, together with the SAR image and the optical image corrupted by clouds, is fused to reconstruct the corrupted area by a generative adversarial network (GAN) with a particular loss function. Between the first step and the second step, the contrast and luminance of the simulated optical image are randomly altered to make the model more robust. Two simulation experiments and one real-data experiment are conducted to confirm the effectiveness of the proposed method on Sentinel 1/2, GF 2/3 and airborne SAR/optical data. The results demonstrate that the proposed method outperforms state-of-the-art algorithms that also employ SAR images as auxiliary data. © 2020 by the authors.","Convolution; Convolutional neural networks; Geometrical optics; Image fusion; Image reconstruction; Remote sensing; Synthetic aperture radar; Adversarial networks; Earth observations; Heterogeneous information; Image translation; Missing information; Optical remote sensing; State-of-the-art algorithms; Synthetic aperture radar (SAR) images; Radar imaging","Cloud removal; Deep learning; GAN; Information fusion; SAR","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083956677"
"Hwang J.; Shin Y.","Hwang, Jieon (57221372578); Shin, Yoan (7402816673)","57221372578; 7402816673","Image Data Augmentation for SAR Automatic Target Recognition Using TripleGAN","2021","International Conference on ICT Convergence","2021-October","","","312","314","2","10.1109/ICTC52510.2021.9621194","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122916182&doi=10.1109%2fICTC52510.2021.9621194&partnerID=40&md5=0d8ef8cca2ecea834a1744ad7b173388","Synthetic aperture radar (SAR) images can be observed in all weather conditions. In addition, SAR automatic target recognition (ATR) is an important technology in the field of remote sensing image analysis. However, it is expensive to acquire SAR images, which limits the construction of additional datasets. In this paper, we propose a method to enhance ATR performance of the corresponding SAR images based on triple-generative adversarial networks (GANs). We added a classifier to learn together so that generator converges into the real data distribution. Experiments on the MSTAR dataset confirmed that the proposed model is applicable through the classification model.  © 2021 IEEE.","Automatic target recognition; Classification (of information); Image enhancement; Radar imaging; Radar target recognition; Remote sensing; Synthetic aperture radar; Automatic target recognition; Data augmentation; Generative adversarial network; Image data; Image-analysis; Performance; Remote sensing images; Synthetic aperture radar; Synthetic aperture radar automatic target recognition; Synthetic aperture radar images; Generative adversarial networks","automatic target recognition (ATR); data augmentation; generative adversarial network (GAN); Synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85122916182"
"Pineda F.; Ayma V.; Aduviri R.; Beltran C.","Pineda, Ferdinand (57216822078); Ayma, Victor (56566776600); Aduviri, Robert (57207467513); Beltran, Cesar (55602499700)","57216822078; 56566776600; 57207467513; 55602499700","Super Resolution Approach Using Generative Adversarial Network Models for Improving Satellite Image Resolution","2020","Communications in Computer and Information Science","1070 CCIS","","","291","298","7","10.1007/978-3-030-46140-9_27","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084819739&doi=10.1007%2f978-3-030-46140-9_27&partnerID=40&md5=9fe165b60f94c8fbbadd58a0c7da0341","Recently, the number of satellite imaging sensors deployed in space has experienced a considerable increase, but most of these sensors provide low spatial resolution images, and only a small proportion contribute with images at higher resolutions. This work proposes an alternative to improve the spatial resolution of Landsat-8 images to the reference of Sentinel-2 images, by applying a Super Resolution (SR) approach based on the use of Generative Adversarial Network (GAN) models for image processing, as an alternative to traditional methods to achieve higher resolution images, hence, remote sensing applications could take advantage of this new information and improve its outcomes. We used two datasets to train and validate our approach, the first composed by images from the DIV2K open access dataset and the second by images from Sentinel-2 satellite. The experimental results are based on the comparison of the similarity between the Landsat-8 images obtained by the super resolution processing by our approach (for both datasets), against its corresponding reference from Sentinel-2 satellite image, computing the Peak Signal-to-Noise Ratio (PSNR) and the Structural Similarity (SSIM) as metrics for this purpose. In addition, we present a visual report in order to compare the performance of each trained model, analysis that shows interesting improvements of the resolution of Landsat-8 satellite images. © Springer Nature Switzerland AG 2020.","Big data; Image resolution; Information management; Optical resolving power; Remote sensing; Signal to noise ratio; Small satellites; Adversarial networks; Higher resolution images; Peak signal to noise ratio; Remote sensing applications; Satellite imaging; Spatial resolution; Spatial resolution images; Structural similarity; Image enhancement","Landsat-8; Sentinel-2; SR-GAN; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85084819739"
"Zhang H.; Mou F.; Duan S.; Huang S.; Wang S.; Xu D.; Zheng Z.","Zhang, Hui (57224914657); Mou, Fan (57207776578); Duan, Shangqi (57222243491); Huang, Shuangde (57210374145); Wang, Shengwei (57210368463); Xu, Debin (57222241212); Zheng, Zezhong (18439055300)","57224914657; 57207776578; 57222243491; 57210374145; 57210368463; 57222241212; 18439055300","A Method to Create Training Dataset for Dehazing with Cyclegan","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324099","6997","7000","3","10.1109/IGARSS39084.2020.9324099","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101968772&doi=10.1109%2fIGARSS39084.2020.9324099&partnerID=40&md5=979e174ebb12b2b5a7d4315a707806e1","Haze usually blurs the characteristics of images shotted in adverse weather conditions. It brings many challenges for computer vision such as object detection. Due to the lack of effective training dataset of remote sensing image, the dehazing usually utilize the physical model rather than the deep learning approach based on a large number of training dataset. An effective method to create training dataset may provide some new ideas for remote sensing image dehazing. In this paper, a novel approach based on the visual features rather than the physical counterparts is developed to create a training dataset for remote sensing images dehazing. Firstly, 400 haze images and 400 clear images with size of 240×240 pixels were gathered from Landsat 8. Secondly, the dataset was utilized to train the cycle-consistent generative adversarial network (CycleGAN) to derive an image transform model which can convert a clear image into a haze one. Thirdly, 4000 clear images with size of 240×240 pixels were collected from Landsat 8 and the images were transformed into the haze images with our model. Finally, the ratio of training dataset and testing dataset is set as 4:1. The haze images created by us were selected as the input and the original clear images were chosen as the output to train the convolutional neural networks to derive the dehazing models. The created dataset and the dehazing models derived were tested, and the experimental results showed that our approach is better to keep the brightnessinformation of the original image, but it is not good at keeping the chroma information. © 2020 IEEE.","Convolutional neural networks; Deep learning; Demulsification; Geology; Large dataset; Object detection; Pixels; Statistical tests; Adversarial networks; Adverse weather; Image transforms; Learning approach; Original images; Remote sensing images; Training dataset; Visual feature; Remote sensing","CycleGAN; Dehazing","Conference paper","Final","","Scopus","2-s2.0-85101968772"
"Kuznetsov A.; Gashnikov M.","Kuznetsov, Andrey (57220713929); Gashnikov, Mikhail (12644970700)","57220713929; 12644970700","Remote Sensing Image Inpainting with Generative Adversarial Networks","2020","8th International Symposium on Digital Forensics and Security, ISDFS 2020","","","9116347","","","","10.1109/ISDFS49300.2020.9116347","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087640839&doi=10.1109%2fISDFS49300.2020.9116347&partnerID=40&md5=61f1d5d06b2ebdbe192ae5acc69aee18","We investigate generative adversarial neural networks (GAN) for remote sensing image inpainting. We are considering a generative neural network with a contour predictor. We use this neural network to inpainting of the natural remote sensing images obtained by 'Canopus', 'Meteor', 'AIST', 'Resurs' aircrafts, as well as Google Earth images. As a basis for comparison, we use an exemplar-based algorithm. We experimentally prove the effectiveness of the generative neural network with the contour predictor for remote sensing image inpainting, in particular for generation forgery Earth remote sensing data. © 2020 IEEE.","Electronic crime countermeasures; Image processing; Remote sensing; Adversarial networks; Earth remote sensing; Exemplar-based; Google earths; Inpainting; Remote sensing images; Digital forensics","exemplar -based algorithm; forgery generation; generative adversarial neural networks; image inpainting; remote sensing","Conference paper","Final","","Scopus","2-s2.0-85087640839"
"Kou R.; Fang B.; Chen G.; Wang L.","Kou, Rong (57202911013); Fang, Bo (57209326962); Chen, Gang (57115575600); Wang, Lizhe (23029267900)","57202911013; 57209326962; 57115575600; 23029267900","Progressive domain adaptation for change detection using season-varying remote sensing images","2020","Remote Sensing","12","22","3815","1","25","24","10.3390/rs12223815","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096415376&doi=10.3390%2frs12223815&partnerID=40&md5=4b776e558c56283e0c9b81541d59b373","The development of artificial intelligence technology has prompted an immense amount of researches on improving the performance of change detection approaches. Existing deep learning-driven methods generally regard changes as a specific type of land cover, and try to identify them relying on the powerful expression capabilities of neural networks. However, in practice, different types of land cover changes are generally influenced by environmental factors at different degrees. Furthermore, seasonal variation-induced spectral differences seriously interfere with those of real changes in different land cover types. All these problems pose great challenges for season-varying change detection because the real and seasonal variation-induced changes are technically difficult to separate by a single end-to-end model. In this paper, by embedding a convolutional long short-term memory (ConvLSTM) network into a conditional generative adversarial network (cGAN), we develop a novel method, named progressive domain adaptation (PDA), for change detection using season-varying remote sensing images. In our idea, two cascaded modules, progressive translation and group discrimination, are introduced to progressively translate pre-event images from their own domain to the post-event one, where their seasonal features are consistent and their intrinsic land cover distribution features are retained. By training this hybrid multi-model framework with certain reference change maps, the seasonal variation-induced changes between paired images are effectively suppressed, and meanwhile the natural and human activity-caused changes are greatly emphasized. Extensive experiments on two types of season-varying change detection datasets and a comparison with other state-of-the-art methods verify the effectiveness and competitiveness of our proposed PDA. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Adversarial networks; Artificial intelligence technologies; Distribution features; Environmental factors; Remote sensing images; Seasonal variation; Spectral differences; State-of-the-art methods; Remote sensing","Change detection; Convolutional long short-term memory; Domain adaptation; Generative adversarial network; Image-to-image translation; Remote sensing images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85096415376"
"Liu L.; Lei S.; Shi Z.; Zhang N.; Zhu X.","Liu, Liqin (57215536317); Lei, Sen (57195618353); Shi, Zhenwei (23398841900); Zhang, Ning (57191477108); Zhu, Xinzhong (57203681230)","57215536317; 57195618353; 23398841900; 57191477108; 57203681230","Hyperspectral Remote Sensing Imagery Generation from RGB Images Based on Joint Discrimination","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9495279","7624","7636","12","10.1109/JSTARS.2021.3099242","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112595813&doi=10.1109%2fJSTARS.2021.3099242&partnerID=40&md5=0c672639e218927a7b8ddd80dc7b8773","Spatial resolution and spectral resolution both play an important role in the recognition of objects in hyperspectral remote sensing. However, the imaging characteristics of hyperspectral images (HSIs) result in a mutually restrictive relationship between the spatial and spectral resolutions. Generative adversarial networks (GANs) have achieved significant success in image generation. The introduce of the discriminators plays a key role in improving the reality. In this article, we propose an RGB to multiband hyperspectral imagery (150 bands) generation method based on GAN (R2HGAN). The method solves the high ill-posed problem and introduces high spectral resolution into RGB images by learning from multiple scenes of HSI. In R2HGAN, we extend the adversarial learning from spatial to spectral dimensions and joint discrimination is designed to generate HSIs closer to the real ones, where two discriminators (the conditional D and the spectral D) are put forward to supervise the spectral similarity and the conditional reality of the HSI jointly. In detail, the conditional discriminator comprehensively judges the quality of each area in the reconstructed HSI. At the same time, to ensure that the generated spectra are close to the real ones, a spectral discriminator based on multilayer perceptron is designed. Through the experiments on GF-5 imagery, the method has significantly improved the quality of the generated images over other state-of-the-art methods.  © 2008-2012 IEEE.","Hyperspectral imaging; Multilayer neural networks; Remote sensing; Spectral resolution; Spectroscopy; High spectral resolution; Hyper-spectral imageries; Hyperspectral remote sensing; Imaging characteristics; Multi layer perceptron; Recognition of objects; Reconstruction method; State-of-the-art methods; multispectral image; remote sensing; satellite imagery; spatial resolution; spectral resolution; Image enhancement","Generation adversarial network (GAN); hyperspectral image (HSI); remote sensing; spectral superresolution (SSR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112595813"
"Zhang Q.; Liu X.; Liu M.; Zou X.; Zhu L.; Ruan X.","Zhang, Qian (57205360783); Liu, Xiangnan (57225920938); Liu, Meiling (55743456400); Zou, Xinyu (57188839697); Zhu, Lihong (57209174403); Ruan, Xiaohao (57221477599)","57205360783; 57225920938; 55743456400; 57188839697; 57209174403; 57221477599","Comparative analysis of edge information and polarization on sar-to-optical translation based on conditional generative adversarial networks","2021","Remote Sensing","13","1","128","1","20","19","10.3390/rs13010128","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099251729&doi=10.3390%2frs13010128&partnerID=40&md5=8b950ae152b95796c6a2fd7272efeb38","To accurately describe dynamic vegetation changes, high temporal and spectral resolution data are urgently required. Optical images contain rich spectral information but are limited by poor weather conditions and cloud contamination. Conversely, synthetic-aperture radar (SAR) is effective under all weather conditions but contains insufficient spectral information to recognize certain vegetation changes. Conditional adversarial networks (cGANs) can be adopted to transform SAR images (Sentinel-1) into optical images (Landsat8), which exploits the advantages of both optical and SAR images. As the features of SAR and optical remote sensing data play a decisive role in the translation process, this study explores the quantitative impact of edge information and polarization (VV, VH, VV&VH) on the peak signal-to-noise ratio, structural similarity index measure, correlation coefficient (r), and root mean squared error. The addition of edge information improves the structural similarity between generated and real images. Moreover, using the VH and VV&VH polarization modes as the input provides the cGANs with more effective information and results in better image quality. The optimal polarization mode with the addition of edge information is VV&VH, whereas that without edge information is VV. Near-infrared and short-wave infrared bands in the generated image exhibit higher accuracy (r > 0.8) than visible light bands. The conclusions of this study could serve as an important reference for selecting cGANs input features, and as a potential reference for the applications of cGANs to the SAR-to-optical translation of other multi-source remote sensing data. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.","Binary alloys; Geometrical optics; Image enhancement; Image quality; Infrared devices; Infrared radiation; Mean square error; Meteorology; Optical correlation; Polarization; Remote sensing; Signal to noise ratio; Synthetic aperture radar; Vegetation; Correlation coefficient; Optical remote sensing data; Peak signal to noise ratio; Root mean squared errors; Short wave infrared bands; Structural similarity; Structural similarity index measures; Temporal and spectral resolutions; Radar imaging","Conditional generative adversarial networks (cGANs); Deep learning; SAR-to-optical image translation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099251729"
"Abdollahi A.; Pradhan B.; Sharma G.; Maulud K.N.A.; Alamri A.","Abdollahi, Abolfazl (57193647122); Pradhan, Biswajeet (12753037900); Sharma, Gaurav (7202756906); Maulud, Khairul Nizam Abdul (57215915257); Alamri, Abdullah (57215408871)","57193647122; 12753037900; 7202756906; 57215915257; 57215408871","Improving Road Semantic Segmentation Using Generative Adversarial Network","2021","IEEE Access","9","","9416669","64381","64392","11","10.1109/ACCESS.2021.3075951","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107236395&doi=10.1109%2fACCESS.2021.3075951&partnerID=40&md5=b029fe8118f2c949b994b6ed1652d170","Road network extraction from remotely sensed imagery has become a powerful tool for updating geospatial databases, owing to the success of convolutional neural network (CNN) based deep learning semantic segmentation techniques combined with the high-resolution imagery that modern remote sensing provides. However, most CNN approaches cannot obtain high precision segmentation maps with rich details when processing high-resolution remote sensing imagery. In this study, we propose a generative adversarial network (GAN)-based deep learning approach for road segmentation from high-resolution aerial imagery. In the generative part of the presented GAN approach, we use a modified UNet model (MUNet) to obtain a high-resolution segmentation map of the road network. In combination with simple pre-processing comprising edge-preserving filtering, the proposed approach offers a significant improvement in road network segmentation compared with prior approaches. In experiments conducted on the Massachusetts road image dataset, the proposed approach achieves 91.54% precision and 92.92% recall, which correspond to a Mathews correlation coefficient (MCC) of 91.13%, a Mean intersection over union (MIOU) of 87.43% and a F1-score of 92.20%. Comparisons demonstrate that the proposed GAN framework outperforms prior CNN-based approaches and is particularly effective in preserving edge information.  © 2013 IEEE.","Aerial photography; Antennas; Data reduction; Deep learning; Remote sensing; Roads and streets; Semantic Web; Semantics; Correlation coefficient; Edge-preserving filtering; High resolution aerial imagery; High resolution imagery; High resolution remote sensing imagery; Remotely sensed imagery; Road network extraction; Semantic segmentation; Convolutional neural networks","deep learning; GAN; remote sensing; road segmentation; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85107236395"
"Wang K.; Zhang S.; Chen J.; Ren F.; Xiao L.","Wang, Ke (57198953604); Zhang, Siyuan (57770839500); Chen, Junlan (57208898993); Ren, Fan (57218512083); Xiao, Lei (57193928226)","57198953604; 57770839500; 57208898993; 57218512083; 57193928226","A feature-supervised generative adversarial network for environmental monitoring during hazy days","2020","Science of the Total Environment","748","","141445","","","","10.1016/j.scitotenv.2020.141445","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089363967&doi=10.1016%2fj.scitotenv.2020.141445&partnerID=40&md5=4e09eae21b46f2f8ccea788f21c364dd","The adverse haze weather condition has brought considerable difficulties in vision-based environmental applications. While, until now, most of the existing environmental monitoring studies are under ordinary conditions, and the studies of complex haze weather conditions have been ignored. Thence, this paper proposes a feature-supervised learning network based on generative adversarial networks (GAN) for environmental monitoring during hazy days. Its main idea is to train the model under the supervision of feature maps from the ground truth. Four key technical contributions are made in the paper. First, pairs of hazy and clean images are used as inputs to supervise the encoding process and obtain high-quality feature maps. Second, the basic GAN formulation is modified by introducing perception loss, style loss, and feature regularization loss to generate better results. Third, multi-scale images are applied as the input to enhance the performance of discriminator. Finally, a hazy remote sensing dataset is created for testing our dehazing method and environmental detection. Extensive experimental results show that the proposed method has achieved better performance than current state-of-the-art methods on both synthetic datasets and real-world remote sensing images. © 2020 Elsevier B.V.","Demulsification; Image coding; Meteorology; Remote sensing; Statistical tests; Adversarial networks; Environmental applications; Environmental detection; Environmental Monitoring; Remote sensing images; State-of-the-art methods; Synthetic datasets; Technical contribution; artificial neural network; atmospheric modeling; data set; environmental monitoring; haze; remote sensing; supervised learning; article; environmental monitoring; learning; perception; remote sensing; Image enhancement","Adversarial generative networks (GAN); Environmental monitoring; Feature-supervised encoder; Multi-scale discriminator; Remote sensing","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85089363967"
"Sun X.; Xu J.","Sun, Xiao (57208776868); Xu, Jindong (35176864300)","57208776868; 35176864300","Remote Sensing Images Dehazing Algorithm based on Cascade Generative Adversarial Networks","2020","Proceedings - 2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2020","","","9263540","316","321","5","10.1109/CISP-BMEI51763.2020.9263540","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099542180&doi=10.1109%2fCISP-BMEI51763.2020.9263540&partnerID=40&md5=2953923d5ae31c858ac54510dd488daf","The existing remote sensing image dehazing methods based on deep learning networks usually use pairs of clear images and corresponding haze images to train the model. However, pairs of clear images and their haze counterparts are extremely lacking, and synthetically haze images could not accurately simulate the real haze generation process in real-world scenarios. To address this problem, a cascade method combining two GANs (generative adversarial networks) is proposed. It contains a learning-to-haze GAN (UGAN) and learning-to-dehaze GAN (PAGAN). UGAN learns how to haze remote sensing images with unpaired clear and haze images sets, and then guides the PAGAN to learn how to correctly dehaze such images. To reduce the discrepancy between real haze and synthetic haze images, we added self-attention mechanism to PAGAN. The details can be generated using cues from all feature locations. Moreover, the discriminator could check that highly detailed features in distant portions of the images that are consistent with each other. Compared with other dehazing methods, this algorithm does not require numerous pairs of images to train the network repeatedly. And the results show that the cascaded generative adversarial networks has visual and quantitative effectiveness for the removal of haze, thin clouds. © 2020 IEEE.","Biomedical engineering; Deep learning; Demulsification; Learning systems; Remote sensing; Adversarial networks; Attention mechanisms; Feature location; Generation process; Images sets; Learning network; Real-world scenario; Remote sensing images; Image processing","cloud removal; dehazing; generative adversarial networks; remote sensing images; visual attention","Conference paper","Final","","Scopus","2-s2.0-85099542180"
"Yari M.; Ibikunle O.; Varshney D.; Chowdhury T.; Sarkar A.; Paden J.; Li J.; Rahnemoonfar M.","Yari, Masoud (16235210400); Ibikunle, Oluwanisola (57219778451); Varshney, Debvrat (57197926939); Chowdhury, Tashnim (57221149963); Sarkar, Argho (57221708601); Paden, John (8338654000); Li, Jilu (55322660300); Rahnemoonfar, Maryam (54411066400)","16235210400; 57219778451; 57197926939; 57221149963; 57221708601; 8338654000; 55322660300; 54411066400","Airborne Snow Radar Data Simulation with Deep Learning and Physics-Driven Methods","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","12035","12047","12","10.1109/JSTARS.2021.3126547","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120072277&doi=10.1109%2fJSTARS.2021.3126547&partnerID=40&md5=69c8bbc8d5baec95519eeac87cfbde38","Monitoring properties of ice sheets in polar regions is one of the main challenges in glaciology. There is a large amount of heterogeneous radar data from the polar regions that have been gathered through expensive missions. However, retrieving meaningful information from this large volume of data is still a great challenge. With the advancement of machine learning techniques in recent years, many scientists are eager to take advantage of these algorithms and techniques to explore and mine Arctic and Antarctic data. These advancements, however, have happened mainly in the area of supervised learning where the models are data hungry and require large amounts of annotated data. Generating simulated data can be an effective and inexpensive approach to provide large labeled datasets for training machine learning models. In this work, we explore two approaches to simulate arctic snow radar echogram images, namely a radar scattering physics based approach combined with some statistical measures and a purely data-driven approach based on a conditional generative adversarial network. Using several image comparison metrics, we analyze the utility of both methods for the purpose of simulating echograms. Our results show that the physics simulator generates images with good structural similarities, while the purely data-driven approach achieves better textural similarities for simulated image. Finally, we also show that by augmenting our real dataset by the simulated echograms, we can improve our deep learning model for tracking internal layers of snow. © 2008-2012 IEEE.","Deep learning; Generative adversarial networks; Remote sensing; Snow; Statistical Physics; Tracking radar; Data simulation; Data-driven approach; GAN; Large amounts; Polar Regions; Radar data; Radar remote sensing; Remote-sensing; Simulation; Snow radar; airborne sensing; computer simulation; radar; remote sensing; satellite data; satellite imagery; snow cover; Radar imaging","Generative adversarial networks (GANs); remote sensing; simulation; snow radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120072277"
"Ding Q.; Liu H.; Luo H.; Chen X.","Ding, Qichen (57226882409); Liu, Hongkun (57226888031); Luo, Haokun (57226879276); Chen, Xueyun (56076920300)","57226882409; 57226888031; 57226879276; 56076920300","Road Detection Network Based on Anti-Disturbance and Variable-Scale Spatial Context Detector","2021","IEEE Access","9","","9514847","114640","114648","8","10.1109/ACCESS.2021.3105190","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113217322&doi=10.1109%2fACCESS.2021.3105190&partnerID=40&md5=727a34cce658ba1a0c760f7daf5b7bf5","Road detection plays a critical role in the application of smart transportation. The performance of the mainstream methods such as PSPNet (Pyramid Scene Parsing Network), DeepLab V3, FCRN (Fully Convolutional Residual Networks) still suffers from uncertain disturbances of surface abrasion buildings, pedestrians, and variation of illumination like tree-shadow. The extracted features are vulnerable to extra-disturbance, and non-local spatial-context information has not been fully utilized. In this paper, a detector based on anti-disturbance and variable-scale spatial context features (AVD) is proposed: the training of the multi-layer features of the detector is always taken under the imposing of fake-feature-disturbance from an independent generator, which is trained to exacerbate the detector errors and the mistakes of feature discriminator. The detector is prepared to be immune from the fake-feature-disturbance, and the discriminator is trained to distinguish the differences between the non-interference features and disturbing features. We also designed a novel variable-scale spatial context module to enhance the richness performance of the extracted features. And a soft connection link is bridged between the low and high feature layers. The detection experiments on the Munich road dataset and urban road dataset show that AVD is better than all the mainstream above methods. Our method increases the accuracy by 3% on the Munich remote sensing dataset and 0.4% on the urban road dataset. Our code and datasets are available at https://github.com/Ding-Q/AVD for download. © 2013 IEEE.","Convolutional neural networks; Remote sensing; Roads and streets; Detection experiments; Non interference; Nonlocal; Road detection; Spatial context; Surface abrasion; Uncertain disturbances; Urban road; Feature extraction","anti-disturbance; convolutional neural network; generative adversarial network; Object detection; remote sensing road detection; road detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85113217322"
"Tian L.; Zhong X.; Chen M.","Tian, Liang (55501930600); Zhong, Xiaorou (57193802465); Chen, Ming (56482364800)","55501930600; 57193802465; 56482364800","Semantic Segmentation of Remote Sensing Image Based on GAN and FCN Network Model","2021","Scientific Programming","2021","","9491376","","","","10.1155/2021/9491376","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122979775&doi=10.1155%2f2021%2f9491376&partnerID=40&md5=6bf45a742a62d3a1fdcaa8cad9d670b7","Accurate remote sensing image segmentation can guide human activities well, but current image semantic segmentation methods cannot meet the high-precision semantic recognition requirements of complex images. In order to further improve the accuracy of remote sensing image semantic segmentation, this paper proposes a new image semantic segmentation method based on Generative Adversarial Network (GAN) and Fully Convolutional Neural Network (FCN). This method constructs a deep semantic segmentation network based on FCN, which can enhance the receptive field of the model. GAN is integrated into FCN semantic segmentation network to synthesize the global image feature information and then accurately segment the complex remote sensing image. Through experiments on a variety of datasets, it can be seen that the proposed method can meet the high-efficiency requirements of complex image semantic segmentation and has good semantic segmentation capabilities. Copyright © 2021 Liang Tian et al.","Complex networks; Convolutional neural networks; Generative adversarial networks; Image enhancement; Remote sensing; Semantic Web; Semantics; Complex image; Convolutional neural network; Human activities; Image semantics; Image-based; Images segmentations; Network models; Remote sensing images; Segmentation methods; Semantic segmentation; Semantic Segmentation","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122979775"
"Xiong W.; Lv Y.; Zhang X.; Cui Y.","Xiong, Wei (57204014507); Lv, Yafei (57205744608); Zhang, Xiaohan (57192412934); Cui, Yaqi (37070416300)","57204014507; 57205744608; 57192412934; 37070416300","Learning to translate for cross-source remote sensing image retrieval","2020","IEEE Transactions on Geoscience and Remote Sensing","58","7","8985543","4860","4874","14","10.1109/TGRS.2020.2968096","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087439962&doi=10.1109%2fTGRS.2020.2968096&partnerID=40&md5=3574c3baa7fd50f9c3255ff411dd8548","Content-based remote sensing image retrieval (CBRSIR) is one of the important techniques for the mining and analysis of big remote sensing data. Recently, unified-source CBRSIR (US-CBRSIR) has been extensively studied and explored, but there has been little attention on cross-source CBRSIR (CS-CBRSIR). Although there is motivation for exploration of CS-CBRSIR for the continually increasing multisource remote sensing data, CS-CBRSIR suffers data drift due to multisource data. In this article, we explore to explicitly address the problem by mapping the source domain to target domain and propose an image translation-based framework for CS-CBRSIR. On the one hand, a novel cycle-identity-generative adversarial network (CI-GAN) is proposed based on the cycle-GAN. In addition to the generator and discriminator, a pretrained classifier, identity module, is designed to further boost the discriminative ability of translated images and facilitate the implementation of feature extraction and similarity measure. On the other hand, to alleviate the impact of style difference between the generated and real images, translated image augmentations and label smoothing regularization (LSR) are adopted to enhance training and contribute toward generation of a robust feature extractor. Extensive experiments on a public data set and a detailed ablation study confirm the effectiveness of our approach.  © 1980-2012 IEEE.","Image enhancement; Image retrieval; Adversarial networks; Discriminative ability; Feature extractor; Image translation; Multisource remote sensing data; Remote sensing data; Remote sensing image retrieval; Similarity measure; artificial neural network; error analysis; image analysis; machine learning; remote sensing; satellite data; satellite imagery; Remote sensing","Cross-source content-based remote sensing image retrieval (CS-CBRSIR); domain adaptation; generative adversarial networks (GANs); image translation","Article","Final","","Scopus","2-s2.0-85087439962"
"Sun S.; Mu L.; Feng R.; Wang L.; He J.","Sun, Shuting (57221313989); Mu, Lin (15071932700); Feng, Ruyi (55853730300); Wang, Lizhe (23029267900); He, Jijun (56159708700)","57221313989; 15071932700; 55853730300; 23029267900; 56159708700","GAN-Based LUCC Prediction via the Combination of Prior City Planning Information and Land-Use Probability","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","10189","10198","9","10.1109/JSTARS.2021.3106481","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113297577&doi=10.1109%2fJSTARS.2021.3106481&partnerID=40&md5=8edf9628c52f4cfbe29f00fc41cd588f","Currently, the world is in a period of urbanization that will accelerate the processes of land-use cover and ecological change. Thus, establishing a land-use and land-cover change (LUCC) prediction and simulation model is of great significance for understanding the process of urban change and assessing its ecological impact. In previous studies, LUCC prediction models have been mainly based on cellular automata structures that calculate a future state pixel by pixel through transition rules. Because these transition rules are usually based on the global state and each pixel is calculated according to these fixed rules, the results of these methods have room for improvement in terms of generating details and heterogeneity. In this article, a generative adversarial network (GAN)-based LUCC prediction model using multiscale local spatial information is proposed. The model is based on a pix2pix GAN and an attention structure that predicts future land use through multiscale local spatial information. To validate our model, Shenzhen, a region that is experiencing rapid urbanization, was chosen as the source of the experimental data. The results indicate that the proposed method achieved the highest accuracy in both short-time interval and long-time interval scenarios. In addition, the results of the proposed method were also closest to the ground truth from the perspective of the landscape pattern.  © 2008-2012 IEEE.","China; Guangdong; Shenzhen; Cellular automata; Ecology; Forecasting; Land use; Pixels; Adversarial networks; Ecological impacts; Land use and land cover change; Planning information; Prediction and simulations; Rapid urbanizations; Short time intervals; Spatial informations; computer simulation; land cover; land use planning; numerical model; pixel; prediction; remote sensing; smart city; spatiotemporal analysis; urban planning; Predictive analytics","Deep learning; Generative adversarial network (GAN); LUCC simulation; Remote sensing; Smart city","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85113297577"
"Ren C.X.; Ziemann A.; Theiler J.; Durieux A.M.S.","Ren, Christopher X. (57004276700); Ziemann, Amanda (36134071500); Theiler, James (7004449154); Durieux, Alice M.S. (57212026338)","57004276700; 36134071500; 7004449154; 57212026338","Cycle-Consistent Adversarial Networks for Realistic Pervasive Change Generation in Remote Sensing Imagery","2020","Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation","2020-March","","9094603","42","45","3","10.1109/SSIAI49293.2020.9094603","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085490462&doi=10.1109%2fSSIAI49293.2020.9094603&partnerID=40&md5=17ae73c5ca32f7d9f596595252ae0d13","This paper introduces a new method of generating realistic pervasive changes in the context of evaluating the effectiveness of change detection algorithms in controlled settings. The method-a cycle-consistent adversarial network (CycleGAN)-requires low quantities of training data to generate realistic changes. Here we show an application of CycleGAN in creating realistic snow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how these images can be used as a test bed for anomalous change detection algorithms. © 2020 IEEE.","Remote sensing; Signal detection; Adversarial networks; Change detection algorithms; Multi-spectral; Remote sensing imagery; Training data; Image analysis","change detection; deep learning; generative adversarial networks; image analysis; multispectral; remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085490462"
"Hu A.; Xie Z.; Xu Y.; Xie M.; Wu L.; Qiu Q.","Hu, Anna (57205419850); Xie, Zhong (36164790400); Xu, Yongyang (57095192900); Xie, Mingyu (57218381473); Wu, Liang (23101491100); Qiu, Qinjun (57203591589)","57205419850; 36164790400; 57095192900; 57218381473; 23101491100; 57203591589","Unsupervised haze removal for high-resolution optical remote-sensing images based on improved generative adversarial networks","2020","Remote Sensing","12","24","4162","1","20","19","10.3390/rs12244162","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098256660&doi=10.3390%2frs12244162&partnerID=40&md5=6d261b2e0f60182df38fb181c4a5da15","One major limitation of remote-sensing images is bad weather conditions, such as haze. Haze significantly reduces the accuracy of satellite image interpretation. To solve this problem, this paper proposes a novel unsupervised method to remove haze from high-resolution optical remote-sensing images. The proposed method, based on cycle generative adversarial networks, is called the edge-sharpening cycle-consistent adversarial network (ES-CCGAN). Most importantly, unlike existing methods, this approach does not require prior information; the training data are unsupervised, which mitigates the pressure of preparing the training data set. To enhance the ability to extract ground-object information, the generative network replaces a residual neural network (ResNet) with a dense convolutional network (DenseNet). The edge-sharpening loss function of the deep-learning model is designed to recover clear ground-object edges and obtain more detailed information from hazy images. In the high-frequency information extraction model, this study re-trained the Visual Geometry Group (VGG) network using remote-sensing images. Experimental results reveal that the proposed method can recover different kinds of scenes from hazy images successfully and obtain excellent color consistency. Moreover, the ability of the proposed method to obtain clear edges and rich texture feature information makes it superior to the existing methods. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Deep learning; Optical resolving power; Remote sensing; Textures; Adversarial networks; Convolutional networks; High-frequency informations; Optical remote sensing; Prior information; Remote sensing images; Training data sets; Unsupervised method; Image enhancement","CycleGAN (cycle generative adversarial networks); Deep learning; Dehazing; Information recovery; Remote-sensing image","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85098256660"
"Cao Y.; Wang Y.; Peng J.; Zhang L.; Xu L.; Yan K.; Li L.","Cao, Yun (57202049913); Wang, Yuebin (55986579100); Peng, Junhuan (12785442100); Zhang, Liqiang (36064917000); Xu, Linlin (55921131900); Yan, Kai (57201413383); Li, Lihua (56200143500)","57202049913; 55986579100; 12785442100; 36064917000; 55921131900; 57201413383; 56200143500","DML-GANR: Deep Metric Learning with Generative Adversarial Network Regularization for High Spatial Resolution Remote Sensing Image Retrieval","2020","IEEE Transactions on Geoscience and Remote Sensing","58","12","9090971","8888","8904","16","10.1109/TGRS.2020.2991545","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093968398&doi=10.1109%2fTGRS.2020.2991545&partnerID=40&md5=990c8849dbea7c60c6e8932018ace147","With a small number of labeled samples for training, it can save considerable manpower and material resources, especially when the amount of high spatial resolution remote sensing images (HSR-RSIs) increases considerably. However, many deep models face the problem of overfitting when using a small number of labeled samples. This might degrade HSR-RSI retrieval accuracy. Aiming at obtaining more accurate HSR-RSI retrieval performance with small training samples, we develop a deep metric learning approach with generative adversarial network regularization (DML-GANR) for HSR-RSI retrieval. The DML-GANR starts from a high-level feature extraction (HFE) to extract high-level features, which includes convolutional layers and fully connected (FC) layers. Each of the FC layers is constructed by deep metric learning (DML) to maximize the interclass variations and minimize the intraclass variations. The generative adversarial network (GAN) is adopted to mitigate the overfitting problem and validate the qualities of extracted high-level features. DML-GANR is optimized through a customized approach, and the optimal parameters are obtained. The experimental results on the three data sets demonstrate the superior performance of DML-GANR over state-of-the-art techniques in HSR-RSI retrieval. © 1980-2012 IEEE.","Image resolution; Remote sensing; Customized approaches; High spatial resolution; High-level feature extractions; Intra-class variation; Remote sensing image retrieval; Remote sensing images; Retrieval performance; State-of-the-art techniques; accuracy assessment; extraction method; model validation; optimization; performance assessment; remote sensing; satellite imagery; spatial analysis; spatial resolution; Deep learning","Convolutional neural network (CNN); deep learning; deep metric learning (DML); generative adversarial network (GAN); image retrieval","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093968398"
"Fan W.; Zhou F.; Tian T.","Fan, Weiwei (57192689555); Zhou, Feng (56421055500); Tian, Tian (57197771288)","57192689555; 56421055500; 57197771288","A Deceptive Jamming Template Synthesis Method for SAR Using Generative Adversarial Nets","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323221","6926","6929","3","10.1109/IGARSS39084.2020.9323221","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102001838&doi=10.1109%2fIGARSS39084.2020.9323221&partnerID=40&md5=20e8bcf48681fb6a6093e62d52957fe9","In this paper, a deceptive jamming template generative adversarial network (DJTGAN) is proposed, which can intelligently generate high-fidelity deceptive jamming template matched with the practical SAR scenario. The DJTGAN consists of a deceptive jamming template generative network and a discriminative network. The generative network combines low-frequency content and high-frequency details of the target, and the discriminative network adopts PatchGAN architecture to capture local texture statistics to improve the fidelity of the deceptive jamming template. The MSTAR dataset is utilized to verify the effectiveness of the proposed DJTGAN. Moreover, the strip SAR deceptive jamming experiment based on the deceptive jamming templates generated by DJTGAN is done to further validate the effectiveness of the DJTGAN. © 2020 IEEE.","Geology; Remote sensing; Textures; Adversarial networks; Deceptive jamming; Discriminative networks; High frequency HF; High-fidelity; Local Texture; Low-frequency; Template synthesis methods; Jamming","Deceptive jamming; Generative adversarial networks (GANs); SAR image generation; synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85102001838"
"Wang X.; Zhang X.; Guo H.; Yu S.; Zhang S.","Wang, Xili (36761950100); Zhang, Xiaojuan (57774594700); Guo, Huimin (57213188560); Yu, Shuai (57219555303); Zhang, Shiru (56275637800)","36761950100; 57774594700; 57213188560; 57219555303; 56275637800","A Remote Sensing Image Segmentation Model Based on CGAN Combining Multi-scale Contextual Information","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12305 LNCS","","","365","373","8","10.1007/978-3-030-60633-6_30","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093838435&doi=10.1007%2f978-3-030-60633-6_30&partnerID=40&md5=7faffa65700a3b158908e615b064f5e4","In order to extract targets in different scale under complex scene, this paper proposes a remote sensing image segmentation model based on Conditional Generative Adversarial Network (CGAN) combing multi-scale contextual information. This end-to-end model consists of a generative network and a discriminant network. A SegNet model fusing multi-scale contextual information is proposed as the generative network. In order to extract multi-scale contextual information, the multi-scale features of the end pooling feature map in the encoder are extracted using different proportion of dilated convolution. The multi-scale features are further fused with the global feature. The discriminant network is a convolution neural network for two category classification, determines whether the input is a generated result or the ground truth. After alternate adversarial training, the experimental results on a remote sensing road dataset show that the road segmentation results of the proposed model are superior to those of the comparable models in terms of target integrity and details preserving. © 2020, Springer Nature Switzerland AG.","Computer vision; Convolution; Remote sensing; Roads and streets; Adversarial networks; Category Classification; Contextual information; Convolution neural network; Different proportions; Multi-scale features; Remote sensing images; Road segmentation; Image segmentation","Conditional Generative Adversarial Network; Feature fusion; Multi-scale context; Road segmentation","Conference paper","Final","","Scopus","2-s2.0-85093838435"
"","","","Polarization: Measurement, Analysis, and Remote Sensing XIV","2020","Proceedings of SPIE - The International Society for Optical Engineering","11412","","","","","121","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086939112&partnerID=40&md5=cb195c9840980776d05d7acaf19d561e","The proceedings contain 10 papers. The topics discussed include: optical crosstalk in division of focal plane imagers; linear stokes measurement of thermal targets using compact LWIR spectropolarimeter; diagonal Mueller matrix measurements based on a single pulse LiDAR polarimeter; measuring the polarization response of a VNIR hyperspectral imager; a guided generative adversarial network demosaicing strategy for integrated microgrid polarimeter imagery; road scene object detection using pre-trained RGB neural networks on linear stokes images; effects of surface roughness and Albedo on depolarization in Mueller matrices; and updated material identification from remote sensing of polarized self-emission.","","","Conference review","Final","","Scopus","2-s2.0-85086939112"
"Peters T.; Brenner C.","Peters, Torben (57203129679); Brenner, Claus (7102383497)","57203129679; 7102383497","Conditional Adversarial Networks for Multimodal Photo-Realistic Point Cloud Rendering","2020","PFG - Journal of Photogrammetry, Remote Sensing and Geoinformation Science","88","3-4","","257","269","12","10.1007/s41064-020-00114-z","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087646238&doi=10.1007%2fs41064-020-00114-z&partnerID=40&md5=306e50712d7992352b85e37ae35293d3","We investigate whether conditional generative adversarial networks (C-GANs) are suitable for point cloud rendering. For this purpose, we created a dataset containing approximately 150,000 renderings of point cloud–image pairs. The dataset was recorded using our mobile mapping system, with capture dates that spread across 1 year. Our model learns how to predict realistically looking images from just point cloud data. We show that we can use this approach to colourize point clouds without the usage of any camera images. Additionally, we show that by parameterizing the recording date, we are even able to predict realistically looking views for different seasons, from identical input point clouds. © 2020, The Author(s).","data set; image analysis; image resolution; machine learning; network analysis; parameter estimation; remote sensing","Deep learning; GAN; Point cloud","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85087646238"
"Langheinrich M.; Bittner K.; Reinartz P.","Langheinrich, Maximilian (57194444577); Bittner, Ksenia (57194603356); Reinartz, Peter (56216874200)","57194444577; 57194603356; 56216874200","GAN-Generated Elevation Models in Computational Fluid Dynamics: A Feasibility Study for Complex Urban Terrain","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324564","5302","5305","3","10.1109/IGARSS39084.2020.9324564","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101995812&doi=10.1109%2fIGARSS39084.2020.9324564&partnerID=40&md5=4cfed77a4e38a4d80026f36e025267f0","Recently developed methods to simulate very high-resolution (VHR) wind fields over complex urban terrain rely on high-quality three-dimensional vector representations of building information. Unfortunately data of that kind is sparsely available on a worldwide scale. In this work, we investigate the applicability of computational fluid dynamics (CFD) on 2.5D digital surface models (DSMs) automatically generated by generative adversarial network (GAN) from globally available satellite data which includes photogrammetric DSMs and pan-chromatic (PAN) images. The obtained results demonstrate that the GAN-based DSMs are reasonable alternatives to rarely available level of detail 2 (LoD2) vector data, promoting large coverage, continuous wind field derivation over complex terrain. © 2020 IEEE.","Complex networks; Geology; Landforms; Remote sensing; Adversarial networks; Automatically generated; Complex terrains; Digital surface models; Elevation models; Feasibility studies; Three-dimensional vectors; Very high resolution; Computational fluid dynamics","complex urban terrain; computational fluid dynamic; detached-eddy simulation; digital surface model; generative adversarial network; Open-FOAM; Reynolds-averaged Navier-Stokes","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101995812"
"Gao P.; Tian T.; Li L.; Ma J.; Tian J.","Gao, Peng (57216120423); Tian, Tian (57206471430); Li, Linfeng (57216234297); Ma, Jiayi (26638975600); Tian, Jinwen (7401635999)","57216120423; 57206471430; 57216234297; 26638975600; 7401635999","DE-CycleGAN: An object enhancement network for weak vehicle detection in satellite images","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9363513","3403","3414","11","10.1109/JSTARS.2021.3062057","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101815490&doi=10.1109%2fJSTARS.2021.3062057&partnerID=40&md5=9185e8244958469b0782e33bfea3e692","Vehicle detection is a very important application of remote sensing. However, suffering from the low acutance and insufficient color information, the detection of weak vehicles in satellite imagery still remains a challenge. Image enhancement can improve the visual effects of remote sensing images. Nevertheless, most existing image enhancement methods aim to improve the quality of the entire image without target guidance, which have ambiguous contributions to the detection performance. Methods based on generative adversarial networks (GANs) have realized image enhancement with target guidance by the addition of target-guided branches, but paired training data is not available in some scenarios. In this article, a novel model of detection-guided CycleGAN (DE-CycleGAN) is proposed to enhance the weak targets for the purpose of accurate vehicle detection, where a backbone GAN with a target-guided branch is learned in the absence of paired images. Specifically, enhancements of two levels are mutually executed. At the image level, the color information of the entire satellite image is enriched by refined CycleGAN, and its sharpness is enhanced by the gradient enhancement model. At the object level, the target-guided branch for detection is added to enhance features of the target. The experimental results validate that the detection performance has been significantly improved on the images enhanced by the proposed DE-CycleGAN model, which shows a positive effect on weak target detection.  © 2008-2012 IEEE.","Object detection; Remote sensing; Satellite imagery; Adversarial networks; Color information; Detection performance; Remote sensing images; Satellite images; Vehicle detection; Visual effects; Weak target detection; detection method; image analysis; remote sensing; satellite imagery; transport vehicle; Image enhancement","CycleGAN; generative adversarial networks (GANs); object enhancement; target-guided branch; weak vehicle detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101815490"
"","","","15th Conference on Image and Graphics Technology and Applications, IGTA 2020","2021","Communications in Computer and Information Science","1314 CCIS","","","","","324","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107336660&partnerID=40&md5=732561f30fcff4a8e043e84759d04152","The proceedings contain 24 papers. The special focus in this conference is on Image and Graphics Technology and Applications. The topics include: Infrared Small Target Recognition with Improved Particle Filtering Based on Feature Fusion; single Image Super-Resolution Based on Generative Adversarial Networks; Simplifying Sketches with Conditional GAN; Improved Method of Target Tracking Based on SiamRPN; An Improved Target Tracking Method Based on DIMP; target Recognition Framework and Learning Mode Based on Parallel Images; view Consistent 3D Face Reconstruction Using Siamese Encoder-Decoders; an Angle-Based Smoothing Method for Triangular and Tetrahedral Meshes; AUIF: An Adaptive User Interface Framework for Multiple Devices; A Striping Removal Method Based on Spectral Correlation in MODIS Data; deep Attention Network for Remote Sensing Scene Classification; thin Cloud Removal Using Cirrus Spectral Property for Remote Sensing Images; accurate Estimation of Motion Blur Kernel Based on Genetic Algorithms; graph Embedding Discriminant Analysis and Semi-Supervised Extension for Face Recognition; 3D Human Body Reconstruction from a Single Image; abnormal Crowd Behavior Detection Based on Movement Trajectory; image Recognition Method of Defective Button Battery Base on Improved MobileNetV1; preface; control and on-Board Calibration Method for in-Situ Detection Using the Visible and Near-Infrared Imaging Spectrometer on the Yutu-2 Rover; full Convolutional Color Constancy with Attention; fast and Accurate Face Alignment Algorithm Based on Deep Knowledge Distillation; multi-modal 3-D Medical Image Fusion Based on Tensor Robust Principal Component Analysis.","","","Conference review","Final","","Scopus","2-s2.0-85107336660"
"Dietrich-Sussner R.; Davari A.; Seehaus T.; Braun M.; Christlein V.; Maier A.; Riess C.","Dietrich-Sussner, Rosanna (57222070671); Davari, Amirabbas (56717425400); Seehaus, Thorsten (56724058100); Braun, Matthias (55482393700); Christlein, Vincent (37017226100); Maier, Andreas (23392966100); Riess, Christian (23393831200)","57222070671; 56717425400; 56724058100; 55482393700; 37017226100; 23392966100; 23393831200","Synthetic Glacier SAR Image Generation from Arbitrary Masks Using Pix2Pix Algorithm","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4548","4551","3","10.1109/IGARSS47720.2021.9553853","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125663286&doi=10.1109%2fIGARSS47720.2021.9553853&partnerID=40&md5=2be655f3d0bc962af839fc7055f5890d","Supervised machine learning requires a large amount of labeled data to achieve proper test results. However, generating accurately labeled segmentation maps on remote sensing imagery, including images from synthetic aperture radar (SAR), is tedious and highly subjective. In this work, we propose to alleviate the issue of limited training data by generating synthetic SAR images with the pix2pix algorithm [1]. This algorithm uses conditional Generative Adversarial Networks (cGANs) to generate an artificial image while preserving the structure of the input. In our case, the input is a segmentation mask, from which a corresponding synthetic SAR image is generated. We present different models, perform a comparative study and demonstrate that this approach synthesizes convincing glaciers in SAR images with promising qualitative and quantitative results. © 2021 IEEE","","Conditional GAN; Image-to-image translation; Limited training data; Synthetic data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125663286"
"Ciolino M.; Noever D.; Kalin J.","Ciolino, Matthew (57218126114); Noever, David (55955470100); Kalin, Josh (57218127569)","57218126114; 55955470100; 57218127569","Training set effect on super resolution for automated target recognition","2020","Proceedings of SPIE - The International Society for Optical Engineering","11394","","113940P","","","","10.1117/12.2557845","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087930621&doi=10.1117%2f12.2557845&partnerID=40&md5=82672ac17807da7720af1c8c9b1a5836","Single Image Super Resolution (SISR) is the process of mapping a low-resolution image to a high-resolution image. This inherently has applications in remote sensing as a way to increase the spatial resolution in satellite imagery. This suggests a possible improvement to automated target recognition in image classification and object detection. We explore the effect that different training sets have on SISR with the network, Super Resolution Generative Adversarial Network (SRGAN). We train 5 SRGANs on different land-use classes (e.g. agriculture, cities, ports) and test them on the same unseen dataset. We attempt to find the qualitative and quantitative differences in SISR, binary classification, and object detection performance. We find that curated training sets that contain objects in the test ontology perform better on both computer vision tasks while having a complex distribution of images allows object detection models to perform better. However, Super Resolution (SR) might not be beneficial to certain problems and will see a diminishing amount of returns for datasets that are closer to being solved. © 2020 SPIE.","Agricultural robots; Automatic target recognition; Image enhancement; Land use; Object recognition; Optical resolving power; Remote sensing; Satellite imagery; Statistical tests; Adversarial networks; Automated target recognition; Binary classification; Detection performance; High resolution image; Low resolution images; Spatial resolution; Super resolution; Object detection","Deep learning; Image classification; Object detection; Satellite imagery; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85087930621"
"Nielsen A.H.; Iosifidis A.; Karstoft H.","Nielsen, Andreas Holm (57219758648); Iosifidis, Alexandros (36720841400); Karstoft, Henrik (36771565700)","57219758648; 36720841400; 36771565700","CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9366908","3485","3494","9","10.1109/JSTARS.2021.3062936","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102308665&doi=10.1109%2fJSTARS.2021.3062936&partnerID=40&md5=fd7e4546ddc3337e39cfa878dbfe2924","Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect cloud forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this article, we present a novel satellite-based dataset called 'CloudCast.' It consists of 70 080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 × 1530 pixels (3 × 3 km per pixel) with 15-min intervals between frames for the period January 1, 2017 to December 31, 2018. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors' best knowledge. © 2008-2012 IEEE.","Europe; Earth (planet); Optical flows; Pixels; Turing machines; Adversarial networks; Extrapolation methods; High-resolution datasets; Historical observation; Overall accuracies; Short term memory; Spatial resolution; Temporal granularity; cloud cover; data set; forecasting method; pixel; remote sensing; satellite; spatial data; spatial resolution; spatiotemporal analysis; Weather forecasting","Atmospheric forecasting; remote sensing datasets; spatiotemporal deep learning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102308665"
"Ma J.; Zhang L.; Zhang J.","Ma, Jie (57205916758); Zhang, Libao (35325855000); Zhang, Jue (56513505100)","57205916758; 35325855000; 56513505100","SD-GAN: Saliency-Discriminated GAN for Remote Sensing Image Superresolution","2020","IEEE Geoscience and Remote Sensing Letters","17","11","8933080","1973","1977","4","10.1109/LGRS.2019.2956969","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095797768&doi=10.1109%2fLGRS.2019.2956969&partnerID=40&md5=815e545e039bd3a0e0768e85794499f2","Recently, convolutional neural networks have shown superior performance in single-image superresolution. Although existing mean-square-error-based methods achieve high peak signal-to-noise ratio (PSNR), they tend to generate oversmooth results. Generative adversarial network (GAN)-based methods can provide high-resolution (HR) images with higher perceptual quality, but produce pseudotextures in images, which generally leads to lower PSNR. Besides, different regions in remote sensing images (RSIs) reflect discrepant surface topography and visual characteristics. This means a uniform reconstruction strategy may not be suitable for all targets in RSIs. To solve these problems, we propose a novel saliency-discriminated GAN for RSI superresolution. First, hierarchical weakly supervised saliency analysis is introduced to compute a saliency map, which is subsequently employed to distinguish the diverse demands of regions in the following generator and discriminator part. Different from previous GANs, the proposed residual dense saliency generator takes saliency maps as a supplementary condition in the generator. Simultaneously, combining the characteristic of RSIs, we design a new paired discriminator to enhance the perceptual quality, which measures the distance between generated images and HR images in salient areas and nonsalient areas, respectively. Comprehensive evaluations validate the superiority of the proposed model.  © 2004-2012 IEEE.","Convolutional neural networks; Image segmentation; Mean square error; Optical resolving power; Remote sensing; Signal to noise ratio; Topography; Adversarial networks; Comprehensive evaluation; High resolution image; Peak signal to noise ratio; Perceptual quality; Remote sensing images; Saliency analysis; Super resolution; image analysis; image resolution; machine learning; remote sensing; satellite altimetry; signal-to-noise ratio; topographic mapping; Image enhancement","Deep learning; generative adversarial network (GAN); remote sensing; saliency analysis; superresolution","Article","Final","","Scopus","2-s2.0-85095797768"
"Ma J.; Yu W.; Chen C.; Liang P.; Guo X.; Jiang J.","Ma, Jiayi (26638975600); Yu, Wei (56479633000); Chen, Chen (57192217138); Liang, Pengwei (57201500677); Guo, Xiaojie (36607970100); Jiang, Junjun (54902306100)","26638975600; 56479633000; 57192217138; 57201500677; 36607970100; 54902306100","Pan-GAN: An unsupervised pan-sharpening method for remote sensing image fusion","2020","Information Fusion","62","","","110","120","10","10.1016/j.inffus.2020.04.006","183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084634225&doi=10.1016%2fj.inffus.2020.04.006&partnerID=40&md5=dedd4ce965c307d547a20af2658b9785","Pan-sharpening in remote sensing image fusion refers to obtaining multi-spectral images of high-resolution by fusing panchromatic images and multi-spectral images of low-resolution. Recently, convolution neural network (CNN)-based pan-sharpening methods have achieved the state-of-the-art performance. Even though, two problems still remain. On the one hand, the existing CNN-based strategies require supervision, where the low-resolution multi-spectral image is obtained by simply blurring and down-sampling the high-resolution one. On the other hand, they typically ignore rich spatial information of panchromatic images. To address these issues, we propose a novel unsupervised framework for pan-sharpening based on a generative adversarial network, termed as Pan-GAN, which does not rely on the so-called ground-truth during network training. In our method, the generator separately establishes the adversarial games with the spectral discriminator and the spatial discriminator, so as to preserve the rich spectral information of multi-spectral images and the spatial information of panchromatic images. Extensive experiments are conducted to demonstrate the effectiveness of the proposed Pan-GAN compared with other state-of-the-art pan-sharpening approaches. Our Pan-GAN has shown promising performance in terms of qualitative visual effects and quantitative evaluation metrics. © 2020 Elsevier B.V.","Remote sensing; Spectroscopy; Adversarial networks; Convolution neural network; Multispectral images; Quantitative evaluation; Remote sensing images; Spatial informations; Spectral information; State-of-the-art performance; Image fusion","Deep learning; Generative adversarial network; Image fusion; Pan-sharpening; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85084634225"
"Yu X.; Hong S.; Yu J.; Lu Y.; Peng Y.","Yu, Ximing (57211663667); Hong, Shuo (57215834027); Yu, Jinxiang (57193861428); Lu, Yibo (57281996000); Peng, Yu (8713314400)","57211663667; 57215834027; 57193861428; 57281996000; 8713314400","Research on a ship target data augmentation method of visible remote sensing image; [可见光遥感图像船舶目标数据增强方法研究]","2020","Yi Qi Yi Biao Xue Bao/Chinese Journal of Scientific Instrument","41","11","","261","269","8","10.19650/j.cnki.cjsi.J2006978","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099877145&doi=10.19650%2fj.cnki.cjsi.J2006978&partnerID=40&md5=8d65f2f8a974b1b625ad342a1024d3fd","The lack of ship target samples in visible remote sensing image leads to the imbalance between positive and negative sample classes in training process. The lightweight neural network sparse MobileNetV2 ship target detection model obtained in training is easy to fall into overfitting. In this study, based on the generative adversarial networks and the image operation in the sample space, a ship target data augmentation method is proposed to realize the supplement of ship target, which combines the feature space and sample space (SADA). According to the characteristics of target size and gray distribution in remote sensing image, a deep convolution ship target generative adversarial network is established. The methods of sample data transformation and feature space fitting are fused in dada set layer to augment the training set. The experiments were conducted adopting the 0.5m resolution image data set screened in Google Earth. Experiment results show that the sparse MobileNetV2 network trained by the augmentation data set can increase the recall rate of ship target detection by 68.5%, which proves the feasibility of this method in improving the accuracy of lightweight deep learning model in small sample size visible remote sensing image ship target detection application. © 2020, Science Press. All right reserved.","Deep learning; Image enhancement; Metadata; Ships; Space optics; Adversarial networks; Data augmentation; Gray distribution; Negative samples; Remote sensing images; Resolution images; Small Sample Size; Training process; Remote sensing","Data augmentation; Generative adversarial network; Ship target; Visible remote sensing","Article","Final","","Scopus","2-s2.0-85099877145"
"Song Y.; Zhang H.; Zhang L.","Song, Yiyao (57221595735); Zhang, Hongyan (54954032600); Zhang, Liangpei (8359720900)","57221595735; 54954032600; 8359720900","Remote Sensing Image Spatio-Temporal Fusion via a Generative Adversarial Network Through One Prior Image Pair","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324101","7009","7012","3","10.1109/IGARSS39084.2020.9324101","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101970280&doi=10.1109%2fIGARSS39084.2020.9324101&partnerID=40&md5=b466b9de0709752c257e83ee82bc3b54","Spatio-temporal fusion is a promising way to deal with the tradeoff between the temporal resolution and spatial resolution of the remote sensing images. This paper presents a novel remote sensing image spatio-temporal fusion model to expand the application of spatio-temporal fusion with insufficient data, based on a generative adversarial network to handle one prior image pair cases (OPGAN). Considering the huge spatial resolution gap between the high-spatial, low-temporal (HSLT) resolution Landsat imagery and the corresponding low-spatial, high-temporal (LSHT) resolution MODIS imagery, the proposed OPGAN simultaneously trains a generator and a discriminator in a min-max game to reconstruct the high-spatial-high-temporal (HSHT) resolution Landsat images, significantly improving the accuracy of change prediction with the help of the temporal changes and sensor differences. Experimental results on three representative Landsat-MODIS datasets illustrate the effectiveness of the proposed OPGAN method. © 2020 IEEE.","Geology; Image enhancement; Image fusion; Image resolution; Radiometers; Adversarial networks; Change prediction; Landsat imagery; Remote sensing images; Spatial resolution; Spatio-temporal fusions; Temporal change; Temporal resolution; Remote sensing","generative adversarial network; one prior image pair; Remote sensing; spatio-temporal fusion","Conference paper","Final","","Scopus","2-s2.0-85101970280"
"Yuchen J.; Bin Z.","Yuchen, Jiang (57204682498); Bin, Zhu (55774177500)","57204682498; 55774177500","Data augmentation for remote sensing image based on generative adversarial networks under condition of few samples","2021","Laser and Optoelectronics Progress","58","8","0810022","","","","10.3788/LOP202158.0810022","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106308557&doi=10.3788%2fLOP202158.0810022&partnerID=40&md5=083bffb3b0cbc69692220f194fb1be92","To solve the problem that the detection accuracy of remote sensing image targets is affected by convolution neural network overfitting under the condition of small samples, a data augmentation method based on generative adversarial networks is proposed. The discrimination model is used to provide local and global decisions for the generation model to improve the quality of the image generated by the generative model. The new samples are obtained by fusing the generated target and the training set image, and the new samples do not need to be labeled manually. Experimental results show that: the accuracy of detection and recognition is improved after adding the generated data to the original data; this method can be superimposed with the data augmentation method based on image affine transformation to further improve the effect of data augmentation. © 2021 Universitat zu Koln. All rights reserved.","","Data augmentation; Discrimination model; Generative adversarial networks; Generative model; Image processing","Article","Final","","Scopus","2-s2.0-85106308557"
"Sun S.; Mu L.; Wang L.; Liu P.; Liu X.; Zhang Y.","Sun, Shuting (57221313989); Mu, Lin (15071932700); Wang, Lizhe (23029267900); Liu, Peng (57075315400); Liu, Xiaolei (57221810219); Zhang, Yuwei (57267434700)","57221313989; 15071932700; 23029267900; 57075315400; 57221810219; 57267434700","Semantic segmentation for buildings of large intra-class variation in remote sensing images with o-gan","2021","Remote Sensing","13","3","475","1","21","20","10.3390/rs13030475","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100278627&doi=10.3390%2frs13030475&partnerID=40&md5=3a8db449b92f82ea7911842fef16e880","Remote sensing building extraction is of great importance to many applications, such as urban planning and economic status assessment. Deep learning with deep network structures and back-propagation optimization can automatically learn features of targets in high-resolution remote sensing images. However, it is also obvious that the generalizability of deep networks is almost entirely dependent on the quality and quantity of the labels. Therefore, building extraction perfor-mances will be greatly affected if there is a large intra-class variation among samples of one class target. To solve the problem, a subdivision method for reducing intra-class differences is proposed to enhance semantic segmentation. We proposed that backgrounds and targets be separately generated by two orthogonal generative adversarial networks (O-GAN). The two O-GANs are connected by adding the new loss function to their discriminators. To better extract building features, drawing on the idea of fine-grained image classification, feature vectors for a target are obtained through an intermediate convolution layer of O-GAN with selective convolutional descriptor aggregation (SCDA). Subsequently, feature vectors are clustered into new, different subdivisions to train semantic segmentation networks. In the prediction stages, the subdivisions will be merged into one class. Experiments were conducted with remote sensing images of the Tibet area, where there are both tall buildings and herdsmen’s tents. The results indicate that, compared with direct semantic segmenta-tion, the proposed subdivision method can make an improvement on accuracy of about 4%. Besides, statistics and visualizing building features validated the rationality of features and subdivisions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Backpropagation; Convolution; Deep learning; Extraction; Image segmentation; Semantics; Tall buildings; Adversarial networks; Building extraction; High resolution remote sensing images; Intra-class variation; Network structures; Remote sensing images; Semantic segmentation; Subdivision methods; Remote sensing","Building extraction; GF-2; Orthogonal generative adversarial networks; Subdivision","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100278627"
"Ji H.; Gao Z.; Liu X.; Zhang Y.; Mei T.","Ji, Hong (57205763449); Gao, Zhi (55256514200); Liu, Xiaodong (56420642200); Zhang, Yongjun (55577971100); Mei, Tiancan (8914886000)","57205763449; 55256514200; 56420642200; 55577971100; 8914886000","Small object detection leveraging on simultaneous super-resolution","2020","Proceedings - International Conference on Pattern Recognition","","","9413058","9054","9061","7","10.1109/ICPR48806.2021.9413058","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110485870&doi=10.1109%2fICPR48806.2021.9413058&partnerID=40&md5=f0b28d82151c3dfaf7af0c2683cd264e","Despite the impressive advancement achieved in object detection, the detection performance of small object is still far from satisfactory due to the lack of sufficient detailed appearance to distinguish it from similar objects. Inspired by the positive effects of super-resolution for object detection, we propose a framework that can be incorporated with detector networks to improve the performance of small object detection, in which the low-resolution image is super-resolved via generative adversarial network (GAN) in an unsupervised manner. In our method, the super-resolution network and the detection network are trained jointly. In particular, the detection loss is back-propagated into the super-resolution network during training to facilitate detection. Compared with available simultaneous super-resolution and detection methods which heavily rely on low-/high-resolution image pairs, our work breaks through such restriction via applying the CycleGAN strategy, achieving increased generality and applicability, while remaining an elegant structure. Extensive experiments on datasets from both computer vision and remote sensing communities demonstrate that our method obtains competitive performance on a wide range of complex scenarios. © 2020 IEEE","Image enhancement; Object recognition; Optical resolving power; Remote sensing; Adversarial networks; Competitive performance; Detection methods; Detection networks; Detection performance; Detector networks; Low resolution images; Small object detection; Object detection","","Conference paper","Final","","Scopus","2-s2.0-85110485870"
"Ren C.X.; Ziemann A.; Theiler J.; Moore J.","Ren, Christopher X. (57004276700); Ziemann, Amanda (36134071500); Theiler, James (7004449154); Moore, Juston (25723538500)","57004276700; 36134071500; 7004449154; 25723538500","Deepfaking it: Experiments in generative, adversarial multispectral remote sensing","2021","Proceedings of SPIE - The International Society for Optical Engineering","11727","","117270M","","","","10.1117/12.2587753","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108698773&doi=10.1117%2f12.2587753&partnerID=40&md5=1be0ef3d95265a89a9420c6ed640e76e","In this work we utilize generative adversarial networks (GANs) to synthesize realistic transformations for remote sensing imagery in the multispectral domain. Despite the apparent perceptual realism of the transformed images at a first glance, we show that a deep learning classifier can very easily be trained to differentiate between real and GAN-generated images, likely due to subtle but pervasive artifacts introduced by the GAN during the synthesis process. We also show that a very low-amplitude adversarial attack can easily fool the aforementioned deep learning classifier, although these types of attacks can be partially mitigated via adversarial training. Finally, we explore the features utilized by the classifier to differentiate real images from GAN-generated ones, and how adversarial training causes the classifier to focus on different, lower-frequency features. © 2021 SPIE.","Clustering algorithms; Deep learning; Hyperspectral imaging; Spectroscopy; Adversarial networks; Learning classifiers; Low-amplitude; Lower frequencies; Multi-spectral; Multispectral remote sensing; Remote sensing imagery; Synthesis process; Remote sensing","Deep learning; Generative Adversarial Networks; Machine learning; Multispectral imagery; Remote Sensing","Conference paper","Final","","Scopus","2-s2.0-85108698773"
"Huang B.; Li Z.; Yang C.; Sun F.; Song Y.","Huang, Binghui (57216946822); Li, Zhi (57208551292); Yang, Chao (57195032153); Sun, Fuchun (57204699218); Song, Yixu (15124457200)","57216946822; 57208551292; 57195032153; 57204699218; 15124457200","Single satellite optical imagery dehazing using SAR image prior based on conditional generative adversarial networks","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093471","1795","1802","7","10.1109/WACV45572.2020.9093471","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085521974&doi=10.1109%2fWACV45572.2020.9093471&partnerID=40&md5=4ebab6919c32b079b180d91c1193b85f","Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models. © 2020 IEEE.","Computer vision; Demulsification; Image fusion; Remote sensing; Satellite imagery; Sensor data fusion; Space-based radar; Synthetic aperture radar; Adversarial networks; Atmospheric conditions; Auxiliary information; Cloudy conditions; Multi-sensor data; Multisensor data fusion; Remote sensing images; Satellite optical imagery; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85085521974"
"Bjork S.; Anfinsen S.N.; Naesset E.; Gobakken T.; Zahabu E.","Bjork, Sara (57222246916); Anfinsen, Stian Normann (6504079727); Naesset, Erik (7003991987); Gobakken, Terje (6602549190); Zahabu, Eliakimu (6506945264)","57222246916; 6504079727; 7003991987; 6602549190; 6506945264","Generation of Lidar-Predicted Forest Biomass Maps from Radar Backscatter with Conditional Generative Adversarial Networks","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324296","4327","4330","3","10.1109/IGARSS39084.2020.9324296","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101988592&doi=10.1109%2fIGARSS39084.2020.9324296&partnerID=40&md5=ad5c7360071f079f688d0bc1369032b6","This paper studies the generation of LiDAR-predicted aboveground biomass (AGB) maps from synthetic aperture radar (SAR) intensity images by use of conditional generative adversarial networks (cGANs). The purpose is to improve on traditional regression models based on SAR intensity, which are trained with a limited amount of AGB in situ measurements. Although they are costly to collect, data from airborne laser scanning (ALS) sensors are highly correlated with AGB and can replace in situ measurements as the regression target. Thus, the amount of training data increases dramatically, and we can learn an expressive two-stage regression model for SAR backscatter intensity. We propose to model the regression function between SAR intensity and ALS-predicted AGB with a Pix2Pix convolutional neural network for image translation that uses a ResNet-5-based cGAN architecture with the Wasserstein GAN gradient penalty (WGAN-GP) objective function. The synthesized ALS-predicted AGB maps are evaluated qualitatively and quantitatively against real ALS-predicted AGB maps. Our results show that the proposed architecture manages to capture characteristics of the real data, which suggests further use of the ResNet-5 for a SAR intensity regression model of AGB. © 2020 IEEE.","Backscattering; Convolutional neural networks; Geology; Network architecture; Optical radar; Regression analysis; Remote sensing; Synthetic aperture radar; Aboveground biomass; Adversarial networks; Airborne Laser scanning; Highly-correlated; In-situ measurement; Objective functions; Proposed architectures; Regression function; Radar imaging","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101988592"
"Pai M.M.M.; Mehrotra V.; Verma U.; Pai R.M.","Pai, M.M. Manohara (57219054419); Mehrotra, Vaibhav (57210804511); Verma, Ujjwal (57218527486); Pai, Radhika M. (57194587874)","57219054419; 57210804511; 57218527486; 57194587874","Improved semantic segmentation of water bodies and land in sar images using generative adversarial networks","2020","International Journal of Semantic Computing","14","1","","55","69","14","10.1142/S1793351X20400036","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087530539&doi=10.1142%2fS1793351X20400036&partnerID=40&md5=7fcf43ef013a6e9e18dfdc872d2c47ac","The availability of computationally efficient and powerful Deep Learning frameworks and high-resolution satellite imagery has created new approach for developing complex applications in the field of remote sensing. The easy access to abundant image data repository made available by different satellites of space agencies such as Copernicus, Landsat, etc. has opened various avenues of research in monitoring the world's oceans, land, rivers, etc. The challenging research problem in this direction is the accurate identification and subsequent segmentation of surface water in images in the microwave spectrum. In the recent years, deep learning methods for semantic segmentation are the preferred choice given its high accuracy and ease of use. One major bottleneck in semantic segmentation pipelines is the manual annotation of data. This paper proposes Generative Adversarial Networks (GANs) on the training data (images and their corresponding labels) to create an enhanced dataset on which the networks can be trained, therefore, reducing human effort of manual labeling. Further, the research also proposes the use of deep-learning approaches such as U-Net and FCN-8 to perform an efficient segmentation of auto annotated, enhanced data of water body and land. The experimental results show that the U-Net model without GAN achieves superior performance on SAR images with pixel accuracy of 0.98 and F1 score of 0.9923. However, when augmented with GANs, the results saw a rise in these metrics with PA of 0.99 and F1 score of 0.9954. © 2020 World Scientific Publishing Company.","Deep learning; Image segmentation; Learning systems; Microwave spectroscopy; Radar imaging; Remote sensing; Satellite imagery; Semantic Web; Semantics; Surface waters; Synthetic aperture radar; Adversarial networks; Complex applications; Computationally efficient; High resolution satellite imagery; Learning frameworks; Manual annotation; Microwave spectrum; Semantic segmentation; Image enhancement","Deep learning; GANs; SAR images; Semantic segmentation; U-net","Article","Final","","Scopus","2-s2.0-85087530539"
"Lian R.; Wang W.; Mustafa N.; Huang L.","Lian, Renbao (57197713850); Wang, Weixing (7501756835); Mustafa, Nadir (56602498400); Huang, Liqin (55492418700)","57197713850; 7501756835; 56602498400; 55492418700","Road extraction methods in high-resolution remote sensing images: A comprehensive review","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9195124","5489","5507","18","10.1109/JSTARS.2020.3023549","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092338245&doi=10.1109%2fJSTARS.2020.3023549&partnerID=40&md5=578d9a31e9099db8e9312cac6a4b8a94","Road extraction from high-resolution remote sensing images is a challenging but hot research topic in the past decades. A large number of methods are invented to deal with this problem. This article provides a comprehensive review of these existing approaches. We classified the methods into heuristic and data-driven. The heuristic methods are the mainstream in the early years, and the data-driven methods based on deep learning have been quickly developed recently. With regard to the heuristic methods, the road feature model is first introduced, then, the classic extraction methods are reviewed in two subcategories: semiautomatic and automatic. The principles, inspirations, advantages, and disadvantages of these methods are described. In terms of the data-driven methods, the road extraction methods based on deep neural network, particularly those based on patched convolutional neural network, fully convolutional network, and generative adversarial network are reviewed. We perform subjective comparisons between the methods inner each type. Furthermore, the quantity performances achieved on the same dataset are compared between the heuristic and data-driven methods to show the strengthening of the data-driven methods. Finally, the conclusion and prospects are summarized. © 2008-2012 IEEE.","Convolution; Convolutional neural networks; Data mining; Deep learning; Deep neural networks; Extraction; Feature extraction; Image processing; Remote sensing; Roads and streets; Adversarial networks; Convolutional networks; Data-driven methods; Extraction method; High resolution remote sensing images; Hot research topics; Number of methods; Road extraction method; data set; decadal variation; heuristics; image analysis; image resolution; machine learning; remote sensing; Heuristic methods","Data-driven; heuristic; high resolution; remote sensing image; road extraction","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092338245"
"Haoyang T.; Jiaxin X.; Yang L.; Dongfang Y.","Haoyang, Tang (57217945713); Jiaxin, Xiao (57439193800); Yang, Liu (57203122217); Dongfang, Yang (57220117110)","57217945713; 57439193800; 57203122217; 57220117110","An Edge Feature Extraction Method for Remote Sensing Image Edge Based on Generative Adversarial Strategy","2021","10th International Conference on Control, Automation and Information Sciences, ICCAIS 2021 - Proceedings","","","","524","531","7","10.1109/ICCAIS52680.2021.9624520","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124034369&doi=10.1109%2fICCAIS52680.2021.9624520&partnerID=40&md5=6887d9a28fffdcd2b493f96a1f32bca1","Affected by changes in illumination, weather, and surface activities, multi-temporal remote sensing image data changes slowly all the time. How to extract edge feature is a key common problem that needs to be resolved in the field of remote sensing image intelligent processing. This paper proposes an edge feature extraction method for remote sensing image edges based on a generative confrontation strategy. This method first designs a CycleGAN network based on the Smooth L1 loss function to meet the requirements of robust feature extraction for edges with semantic information in remote sensing images. The network transfers the style of remote sensing images, and finally extracts edge feature for the images after the style transfer, which is used as the description of the robust features on the surface. Experimental results show that the edge extraction model designed in this paper improves the accuracy of robust feature extraction of remote sensing images by 8.1%. In addition, the method is less affected by environmental factors such as illumination, and can realize the intelligent extraction of robust features of the remote sensing image. The relevant code of this article has been published on the research group's home page. © 2021 IEEE.","Extraction; Feature extraction; Image enhancement; Remote sensing; Semantics; Edge features; Edge-based; Feature extraction methods; Image edge; Multi-temporal remote sensing; Remote sensing image processing; Remote sensing images; Robust feature extractions; Style migration; Surface activities; Generative adversarial networks","Generative adversarial network; Remote sensing image processing; Robust feature extraction; Style migration","Conference paper","Final","","Scopus","2-s2.0-85124034369"
"Yan P.; He F.; Yang Y.; Hu F.","Yan, Peiyao (57217474389); He, Feng (57002038100); Yang, Yajie (57216130953); Hu, Fei (36069638000)","57217474389; 57002038100; 57216130953; 36069638000","Semi-Supervised Representation Learning for Remote Sensing Image Classification Based on Generative Adversarial Networks","2020","IEEE Access","8","","9039665","54135","54144","9","10.1109/ACCESS.2020.2981358","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082614672&doi=10.1109%2fACCESS.2020.2981358&partnerID=40&md5=2eddf7bd901b88fff7402ddc10e0c138","In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training. © 2013 IEEE.","Deep learning; Image classification; Image enhancement; Labeled data; Learning algorithms; Remote sensing; Semi-supervised learning; Support vector machines; Unsupervised learning; Adversarial networks; Classification performance; Data augmentation; Discriminative features; Remote sensing image classification; Scene classification; Supervised learning methods; Unsupervised learning method; Learning systems","data augmentation; generative adversarial networks; Remote sensing scene classification; semi-supervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082614672"
"Xi Y.; Jia W.; Zheng J.; Fan X.; Xie Y.; Ren J.; He X.","Xi, Yue (57193703373); Jia, Wenjing (7202412193); Zheng, Jiangbin (7403975791); Fan, Xiaochen (56576864600); Xie, Yefan (57204288463); Ren, Jinchang (23398632100); He, Xiangjian (7404409118)","57193703373; 7202412193; 7403975791; 56576864600; 57204288463; 23398632100; 7404409118","DRL-GAN: Dual-Stream Representation Learning GAN for Low-Resolution Image Classification in UAV Applications","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9286580","1705","1716","11","10.1109/JSTARS.2020.3043109","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097924268&doi=10.1109%2fJSTARS.2020.3043109&partnerID=40&md5=8010606b4f383beb894e6405920ab6f9","Identifying tiny objects from extremely low-resolution (LR) unmanned-Aerial-vehicle-based remote sensing images is generally considered as a very challenging task, because of very limited information in the object areas. In recent years, there have been very limited attempts to approach this problem. These attempts intend to deal with LR image classification by enhancing either the poor image quality or image representations. In this article, we argue that the performance improvement in LR image classification is affected by the inconsistency of the information loss and learning priority on low-frequency (LF) components and high-frequency (HF) components. To address this LF-HF inconsistency problem, we propose a dual-stream representation learning generative adversarial network (DRL-GAN). The core idea is to produce enhanced image representations optimal for LR recognition by simultaneously recovering the missing information in LF and HF components, respectively, under the guidance of high-resolution (HR) images. We evaluate the performance of DRL-GAN on the challenging task of LR image classification. A comparison of the experimental results on the LR benchmark, namely HRSC and CIFAR-10, and our newly collected 'WIDER-SHIP' dataset demonstrates the effectiveness of our DRL-GAN, which significantly improves the classification performance, with up to 10% gain on average. © 2008-2012 IEEE.","Benchmarking; Classification (of information); Image enhancement; Knowledge representation; Remote sensing; Unmanned aerial vehicles (UAV); Adversarial networks; Classification performance; High frequency HF; Image representations; Limited information; Low resolution images; Missing information; Remote sensing images; artificial neural network; image classification; remote sensing; unmanned vehicle; Image classification","Convolutional neural networks (CNNs); generative adversarial networks; low-resolution (LR) image classification; representation learning; unmanned aerial vehicle (UAV)-based remote sensing","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85097924268"
"Dramsch J.S.","Dramsch, Jesper Sören (50960916900)","50960916900","70 years of machine learning in geoscience in review","2020","Advances in Geophysics","61","","","1","55","54","10.1016/bs.agph.2020.08.002","79","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090327530&doi=10.1016%2fbs.agph.2020.08.002&partnerID=40&md5=6cca0910992d0b392432df83fc5c35c7","This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the codevelopments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging toward a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development toward skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g., decision trees, random forests, support-vector machines, and Gaussian processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks, and generative adversarial networks. Regarding geoscience, the review has a bias toward geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however, excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science. © 2020 Jesper Sören Dramsch","algorithm; artificial neural network; computer simulation; Gaussian method; kriging; machine learning; numerical model; support vector machine; trend analysis","Deep learning; Earth science; Geology; Geophysics; Geoscience; Kriging; Machine learning; Neural networks; Review","Book chapter","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090327530"
"Shin Y.H.; Lee D.-C.","Shin, Young Ha (57219247571); Lee, Dong-Cheon (36138668700)","57219247571; 36138668700","True Orthoimage Generation Using Airborne LiDAR Data with Generative Adversarial Network-Based Deep Learning Model","2021","Journal of Sensors","2021","","4304548","","","","10.1155/2021/4304548","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108956537&doi=10.1155%2f2021%2f4304548&partnerID=40&md5=1d0b1f277488d00add335f8751f7bc3f","Orthoimage, which is geometrically equivalent to a map, is one of the important geospatial products. Displacement and occlusion in optical images are caused by perspective projection, camera tilt, and object relief. A digital surface model (DSM) is essential data for generating true orthoimages to correct displacement and to recover occlusion areas. Light detection and ranging (LiDAR) data collected from an airborne laser scanner (ALS) system is a major source of DSM. The traditional methods require sophisticated procedures to produce a true orthoimage. Most methods utilize 3D coordinates of the DSM and multiview images with overlapping areas for orthorectifying displacement and detecting and recovering occlusion areas. LiDAR point cloud data provides not only 3D coordinates but also intensity information reflected from object surfaces in the georeferenced orthoprojected space. This paper proposes true orthoimage generation based on a generative adversarial network (GAN) deep learning (DL) with the Pix2Pix model using intensity and DSM of the LiDAR data. The major advantage of using LiDAR data is that the data is occlusion-free true orthoimage in terms of projection geometry except in the case of low image quality. Intensive experiments were performed using the benchmark datasets provided by the International Society for Photogrammetry and Remote Sensing (ISPRS). The results demonstrate that the proposed approach could have the capability of efficiently generating true orthoimages directly from LiDAR data. However, it is crucial to find appropriate preprocessing to improve the quality of the intensity of the LiDAR data to produce a higher quality of the true orthoimages. © 2021 Young Ha Shin and Dong-Cheon Lee.","Deep learning; Geometrical optics; Learning systems; Remote sensing; Adversarial networks; Airborne laser scanners; Digital surface models; Intensity information; International society; Lidar point cloud datum; Light detection and ranging; Perspective projections; Optical radar","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85108956537"
"Li J.; Wu Z.; Hu Z.; Zhang Y.; Molinier M.","Li, Jun (55902669000); Wu, Zhaocong (15023850900); Hu, Zhongwen (55630272400); Zhang, Yi (55642647900); Molinier, Matthieu (22234853700)","55902669000; 15023850900; 55630272400; 55642647900; 22234853700","Automatic Cloud Detection Method Based on Generative Adversarial Networks in Remote Sensing Images","2020","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","2","","885","892","7","10.5194/isprs-annals-V-2-2020-885-2020","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091063427&doi=10.5194%2fisprs-annals-V-2-2020-885-2020&partnerID=40&md5=ae1dbbc5465b42640356c34254f00e27","Clouds in optical remote sensing images seriously affect the visibility of background pixels and greatly reduce the availability of images. It is necessary to detect clouds before processing images. In this paper, a novel cloud detection method based on attentive generative adversarial network (Auto-GAN) is proposed for cloud detection. Our main idea is to inject visual attention into the domain transformation to detect clouds automatically. First, we use a discriminator (D) to distinguish between cloudy and cloud free images. Then, a segmentation network is used to detect the difference between cloudy and cloud-free images (i.e. clouds). Last, a generator (G) is used to fill in the different regions in cloud image in order to confuse the discriminator. Auto-GAN only requires images and their labels (1 for a cloud-free image, 0 for a cloudy image) in the training phase which is more time-saving to acquire than existing methods based on CNNs that require pixel-level labels. Auto-GAN is applied to cloud detection in Sentinel-2A Level 1C imagery. The results indicate that Auto-GAN method performs well in cloud detection over different land surfaces. © 2020 Copernicus GmbH. All rights reserved.","Behavioral research; Pixels; Remote sensing; Adversarial networks; Automatic cloud detection; Background pixels; Cloud detection method; Domain transformation; Optical remote sensing; Remote sensing images; Visual Attention; Image segmentation","Attention mechanism; Auto-GAN; Cloud detection; Deep learning; Generative adversarial networks (GANs)","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091063427"
"Sun Y.; Qi H.; Wang C.; Tao L.","Sun, Yujie (57213265987); Qi, Heping (57223271863); Wang, Chuanyou (57223258691); Tao, Lei (57223255553)","57213265987; 57223271863; 57223258691; 57223255553","Image registration method based on Generative Adversarial Networks","2020","Proceedings - 2020 8th International Conference on Advanced Cloud and Big Data, CBD 2020","","","9406840","183","188","5","10.1109/CBD51900.2020.00041","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105416162&doi=10.1109%2fCBD51900.2020.00041&partnerID=40&md5=5132779ee0a5fe94d3e4dda3f6ae2562","Image registration technology has been gradually applied in military and civil fields, such as unmanned aerial vehicle (UAV) target recognition, remote sensing image registration, 3D object reconstruction and so on. The registration accuracy of traditional registration algorithms, such as sift and surf, are not high and even mismatch, when the image content changes or affine transformation occurs. In order to solve this problem, an image registration method (IR-GAN method) based on Generative adversarial networks (GAN) was proposed, in view of the content change and affine transformation of images for training. At the same time, the sample images are expanded by translation, rotation, reflection, scaling, shearing, brightness transformation to improve the precision and accuracy of the registration algorithm. Simulation results show that our method is correct in principle and can improve the registration accuracy of both images with content changing and affine transform images. © 2020 IEEE.","Affine transforms; Antennas; Big data; Image registration; Military photography; Military vehicles; Remote sensing; Unmanned aerial vehicles (UAV); 3-D object reconstruction; Adversarial networks; Affine transformations; Registration accuracy; Registration algorithms; Registration methods; Remote sensing images; Target recognition; Image enhancement","Convolution network; Feature vectors; Generative adversarial networks (GAN); Image registration","Conference paper","Final","","Scopus","2-s2.0-85105416162"
"Zaytar M.A.; El Amrani C.","Zaytar, Mohamed Akram (57204795723); El Amrani, Chaker (55308112200)","57204795723; 55308112200","Satellite imagery noising with generative adversarial networks","2021","International Journal of Cognitive Informatics and Natural Intelligence","15","1","","16","25","9","10.4018/IJCINI.2021010102","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097218294&doi=10.4018%2fIJCINI.2021010102&partnerID=40&md5=aff681d0c405f3c51f096d52c89c1346","Using satellite imagery and remote sensing data for supervised and self-supervised learning problems can be quite challenging when parts of the underlying datasets are missing due to natural phenomena (clouds, fog, haze, mist, etc.). Solving this problem will improve remote sensing data augmentation and make use of it in a world where satellite imagery represents a great resource to exploit in any big data pipeline setup. In this paper, the authors present a generative adversarial network (GANs) model that can generate natural atmospheric noise that serves as a data augmentation preprocessing tool to produce input to supervised machine learning algorithms. Copyright © 2021, IGI Global","Image enhancement; Learning algorithms; Remote sensing; Supervised learning; Adversarial networks; Atmospheric noise; Data augmentation; Natural phenomena; Preprocessing tools; Remote sensing data; Supervised learning problems; Supervised machine learning; Satellite imagery","Artificial Neural Networks; Data Augmentation; EUMETSAT; Generative Adversarial Networks; MDEO; MetOp; Remote Sensing; Satellite Imagery","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85097218294"
"Liu M.; Shi Q.; Liu P.; Wan C.","Liu, Mengxi (57208160778); Shi, Qian (55286447700); Liu, Penghua (57191632651); Wan, Cheng (57222241570)","57208160778; 55286447700; 57191632651; 57222241570","Siamese Generative Adversarial Network for Change Detection under Different Scales","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323499","2543","2546","3","10.1109/IGARSS39084.2020.9323499","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101998421&doi=10.1109%2fIGARSS39084.2020.9323499&partnerID=40&md5=422672dc13ab48da34f0895a22057648","Change detection methods based on low-resolution (LR) images with higher temporal resolution often lead to fuzzy results, while high-resolution images (HRIs) can provide more detailed information to solve this problem. However, it's hard to obtain two tiles of HRIs with high-quality for rapid change detection in actual production due to low temporal resolution and high cost. Therefore, it is necessary to explore a change detection method combing low- and high-resolution images to acquire urban change areas more accurately and quickly. In this paper, an end-to-end siamese generative adversarial network (SiamGAN) integrating a super resolution network and the siamese structure was proposed for change detection under different scales. The super-resolution network is used to reconstruct low-resolution images into high-resolution images, while the siamese structure is adopted as the classification network to detect changes. In the experiments, SiamGAN achieved an F1 of 76.06% and an IoU of 61.52% in the test set, which is respectively 5.68% and 6.92% higher than the CNN-based methods using LR images after bicubic interpolation. The results show that our proposed method can effectively overcome difference in scale between low- and high-resolution images and perform change detection more precisely and rapidly. © 2020 IEEE.","Geology; Optical resolving power; Remote sensing; Adversarial networks; Bicubic interpolation; Change detection; Classification networks; High resolution image; Low resolution images; Super resolution; Temporal resolution; Chemical detection","Change detection; high resolution images; siamese network; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85101998421"
"Dou X.; Li C.; Shi Q.; Liu M.","Dou, Xinyu (57216671300); Li, Chenyu (57216672432); Shi, Qian (55286447700); Liu, Mengxi (57208160778)","57216671300; 57216672432; 55286447700; 57208160778","Super-resolution for hyperspectral remote sensing images based on the 3D Attention-SRGAN Network","2020","Remote Sensing","12","7","1204","","","","10.3390/rs12071204","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084251625&doi=10.3390%2frs12071204&partnerID=40&md5=371b3af26f6fab6f3ac50bce7b220ac9","Hyperspectral remote sensing images (HSIs) have a higher spectral resolution compared to multispectral remote sensing images, providing the possibility for more reasonable and effective analysis and processing of spectral data. However, rich spectral information usually comes at the expense of low spatial resolution owing to the physical limitations of sensors, which brings difficulties for identifying and analyzing targets in HSIs. In the super-resolution (SR) field, many methods have been focusing on the restoration of the spatial information while ignoring the spectral aspect. To better restore the spectral information in the HSI SR field, a novel super-resolution (SR) method was proposed in this study. Firstly, we innovatively used three-dimensional (3D) convolution based on SRGAN (Super-Resolution Generative Adversarial Network) structure to not only exploit the spatial features but also preserve spectral properties in the process of SR. Moreover, we used the attention mechanism to deal with the multiply features from the 3D convolution layers, and we enhanced the output of our model by improving the content of the generator's loss function. The experimental results indicate that the 3DASRGAN (3D Attention-based Super-Resolution Generative Adversarial Network) is both visually quantitatively better than the comparison methods, which proves that the 3DASRGAN model can reconstruct high-resolution HSIs with high efficiency. © 2020, by the authors.","Convolution; Data handling; Optical resolving power; Restoration; Three dimensional computer graphics; Adversarial networks; Attention mechanisms; Hyperspectral Remote Sensing Image; Multispectral remote sensing image; Physical limitations; Spatial informations; Spectral information; Three-dimensional (3D) convolution; Remote sensing","3D convolution; Generative adversarial networks; Hyperspectral image; Spectral angle; Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084251625"
"Niu X.; Yang D.; Yang K.; Pan H.; Dou Y.; Xia F.","Niu, Xin (38663051000); Yang, Di (57215195951); Yang, Ke (57207166059); Pan, Hengyue (57190795198); Dou, Yong (15131095400); Xia, Fei (56386768800)","38663051000; 57215195951; 57207166059; 57190795198; 15131095400; 56386768800","Image translation between high-resolution optical and synthetic aperture radar (SAR) data","2021","International Journal of Remote Sensing","42","12","","4762","4788","26","10.1080/01431161.2020.1836426","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103840459&doi=10.1080%2f01431161.2020.1836426&partnerID=40&md5=252400818f51f641464d36fade4cfa59","This paper presents a novel study: remote-sensing image translation between high-resolution optical and Synthetic Aperture Radar (SAR) data through machine learning approaches. To this end, conditional Generative Adversarial Networks (cGANs) have been proposed with the guide of high-level image features. Efficiency of the proposed methods have been verified with different SAR parameters on three regions from the world: Toronto, Vancouver in Canada and Shanghai in China. The generated SAR and optical images have been evaluated by pixel-based image classification with detailed land cover types including: low and high-density residential area, industry area, construction site, golf course, water, forest, pasture and crops. Results showed that the translated image could effectively keep many land cover types with compatible classification accuracy to the ground truth data. In comparison with state-of-the-art image translation approaches, the proposed methods could improve the translation results under the criteria of common similarity indicators. This is one of first study on multi-source remote-sensing data translation by machine learning. © 2020 Informa UK Limited, trading as Taylor & Francis Group.","British Columbia; Canada; China; Ontario [Canada]; Shanghai; Toronto; Vancouver [British Columbia]; Geometrical optics; Golf; Image enhancement; Machine learning; Optical resolving power; Remote sensing; Synthetic aperture radar; Adversarial networks; Classification accuracy; Construction sites; Ground truth data; High-density residential areas; Machine learning approaches; Remote sensing data; Remote sensing images; image classification; image resolution; land cover; machine learning; synthetic aperture radar; Radar imaging","","Article","Final","","Scopus","2-s2.0-85103840459"
"Xu Z.; Kang Y.; Cao Y.","Xu, Zhenyi (57198998581); Kang, Yu (7402785017); Cao, Yang (57022583200)","57198998581; 7402785017; 57022583200","Emission stations location selection based on conditional measurement GAN data","2020","Neurocomputing","388","","","170","180","10","10.1016/j.neucom.2020.01.013","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078172248&doi=10.1016%2fj.neucom.2020.01.013&partnerID=40&md5=0800958d76b218c8acda9d7a1af16140","Urban vehicle emission monitoring can help make suggestions for the pollution emission control, and can protect public health. However, it is hard to get an overview of the vehicle emission in the city scale due to the sparse emission remote sensing stations in city, and selecting the appropriate stations locations which can reflect the emission variation in the given region mostly is another challenge. The existing methods solve the problem by spatial interpolation based on geographical statistics methods, without considering that urban vehicle emission varies by locations non-linearly and depends on many complex external factors. To tackle the spatial sparsity of vehicle remote sensing data, we design a data augmentation strategy based on Generative Adversarial Networks (GAN), which leverages prior model COPERT with conditional measurement data to filling the missing entries at other locations. With this strategy, we can generate realistic emission data while accelerating the training process. In addition, to address the emission stations location selection problem, we design a novel location selection strategy based on Spearman's rank correlation coefficients, which leverages the realistic data generated to discover the grids with maximum link correlation for the pre-deployed station. Finally, we present experiments with the remote emission sensing data in Hefei, and the results demonstrate that our proposed model can mimics the real vehicle emission distribution in the given region effectively. © 2020 Elsevier B.V.","Emission control; Remote sensing; Vehicles; Adversarial networks; Conditional measurements; Pollution emissions; Remote sensing data; Spatial interpolation; Spearman's rank correlation coefficients; Station location; Vehicle emission; article; correlation coefficient; remote sensing; Location","Generative Adversarial Networks; Station location selection; Vehicle emission monitoring","Article","Final","","Scopus","2-s2.0-85078172248"
"Gastineau A.; Aujol J.-F.; Berthoumieu Y.; Germain C.","Gastineau, Anais (57221265780); Aujol, Jean-Francois (8283963500); Berthoumieu, Yannick (6603229955); Germain, Christian (7006254669)","57221265780; 8283963500; 6603229955; 7006254669","A Residual Dense Generative Adversarial Network for Pansharpening with Geometrical Constraints","2020","Proceedings - International Conference on Image Processing, ICIP","2020-October","","9191230","493","497","4","10.1109/ICIP40778.2020.9191230","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098624486&doi=10.1109%2fICIP40778.2020.9191230&partnerID=40&md5=96f4106fd00e5abb7ed29a51329c4f01","The pansharpening problem consists in fusing a high resolution panchromatic image with a low resolution multispectral image in order to obtain a high resolution multispectral image. In this paper, we adapt a Residual Dense architecture for the generator in a Generative Adversarial Network framework. Indeed, this type of architecture avoids the vanishing gradient problem faced when training a network by re-injecting previous information thanks to dense and residual connections. Moreover, an important point for the pansharpening problem is to preserve the geometry of the image. Hence, we propose to add a regularization term in the loss function of the generator: it preserves the geometry of the target image so that a better solution is obtained. In addition, we propose geometrical measures that illustrate the advantages of this new method. © 2020 IEEE.","Geometry; Network architecture; Adversarial networks; Geometrical constraints; High resolution; Low resolution multispectral images; Multispectral images; Panchromatic images; Regularization terms; Vanishing gradient; Image processing","Generative Adversarial Network; Pansharpening; regularization; remote sensing; residual dense network","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098624486"
"Uz S.S.; Ames T.J.; Memarsadeghi N.; McDonnell S.M.; Blough N.V.; Mehta A.V.; McKay J.R.","Uz, Stephanie Schollaert (57222243479); Ames, Troy J. (7007088611); Memarsadeghi, Nargess (6505931234); McDonnell, Shannon M. (57214838218); Blough, Neil V. (35582170600); Mehta, Amita V. (7402755025); McKay, John R. (57217096059)","57222243479; 7007088611; 6505931234; 57214838218; 35582170600; 7402755025; 57217096059","Supporting Aquaculture in the Chesapeake Bay Using Artificial Intelligence to Detect Poor Water Quality with Remote Sensing","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323465","3629","3632","3","10.1109/IGARSS39084.2020.9323465","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101965189&doi=10.1109%2fIGARSS39084.2020.9323465&partnerID=40&md5=a1781d41cb30aec0178cd6b5e06043f6","Reliable information on water quality is not currently available at the space and time scales that are required for aquaculture and other resource management needs. For example, shellfish growing areas may be impacted by harmful algal blooms or runoff from land that increases turbidity, lowers salinity, or introduces contaminants. Shellfish resource managers in the Chesapeake Bay are especially concerned with sources of bacteria from land such as failing onsite waste systems, failing wastewater infrastructure, and concentrated animal feeding operations. There is an urgent need for remote sensing of water quality indicators beyond chlorophyll-a and suspended sediments to augment field sampling programs. Artificial Intelligence trained with simultaneous in situ and satellite observations is explored in preparation for future hyperspectral satellite missions, which offer potential to detect additional water quality indicators not previously possible. This first step identifies and develops a method to harmonize disparate, unlinked aquatic datasets to derive information about where water quality is likely degraded. © 2020 IEEE.","Aquaculture; Artificial intelligence; Geology; Information management; Shellfish; Space optics; Suspended sediments; Water quality; Concentrated animal feeding operations; Harmful algal blooms; Hyperspectral satellite; Resource management; Resource managers; Satellite observations; Wastewater infrastructure; Water quality indicators; Remote sensing","aquatic remote sensing; Artificial Intelligence; data fusion; hyperspectral satellite missions; machine learning; neural networks; semi-supervised Generative Adversarial Networks; water quality monitoring","Conference paper","Final","","Scopus","2-s2.0-85101965189"
"Sener E.; Çolak E.; Erten E.; Taskin G.","Sener, Ecre (57482537300); Çolak, Emre (57201377169); Erten, Esra (23011359600); Taskin, Gülsen (35105306400)","57482537300; 57201377169; 23011359600; 35105306400","THE ADDED VALUE OF CYCLE-GAN FOR AGRICULTURE STUDIES","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","7039","7042","3","10.1109/IGARSS47720.2021.9553876","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126041204&doi=10.1109%2fIGARSS47720.2021.9553876&partnerID=40&md5=d9d7e2727ac594969804309acca1762b","It is significant to monitor the phenological stages of agricultural crops with accurate and up-to-date information. In monitoring the phenological phases of some crops, optical remote sensing data offers significant spectral information and outstanding feature identification. However, a continuous time series of optical remote sensing data is difficult to obtain due to the weather dependency of optical acquisitions. In this paper, the feasibility of transfer learning between the features of Sentinel-1 and Sentinel-2 is evaluated to reduce these difficulties. A feature translation based on deep learning (DL) method, namely Cycle-Consistent Generative Adversarial Networks (cycle-GAN), was applied between Sentinel-1 and Sentinel-2 data. In order to evaluate the effect of the cycle-GAN method on crop type mapping and identification, Random Forest classification was applied to four different cases (Real SAR, Fake Optical + Real SAR, Real Optical, and Real Optical + Real SAR). © 2021 IEEE","","Agriculture; Consistent adversarial networks; Cycle-GAN; Random forest classification; Sentinel-1; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85126041204"
"Su H.-W.; Tan R.-H.; Chen C.-C.; Hu Z.; Shankaranarayanan A.","Su, Hui-Wei (57371043900); Tan, Ri-Hui (57297159700); Chen, Chih-Cheng (23033912700); Hu, Zhongzheng (57371044000); Shankaranarayanan, Avinash (21743927500)","57371043900; 57297159700; 23033912700; 57371044000; 21743927500","Hyperspectral image classification based on visible–infrared sensors and residual generative adversarial networks","2021","Sensors and Materials","33","11","","4045","4056","11","10.18494/SAM.2021.3527","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121037451&doi=10.18494%2fSAM.2021.3527&partnerID=40&md5=cfcfeeca93dbf6739114df18f7e46d08","Hyperspectral remote sensing images have high spectral resolution and provide rich information on the types of features, but their high data dimensions and large data volume pose challenges in data processing. In addition, it is difficult to obtain ground truths of hyperspectral images (HSIs). Owing to the small number of training samples, the super-normative classification of HSIs is particularly challenging and time-consuming. As deep learning techniques continue to evolve, an increasing number of models have emerged for HSI classification. In this paper, we propose a classification algorithm for HSIs called the residual generative adversarial network (ResGAN), which automatically extracts spectral and spatial features for HSI classification. When unlabeled HSI data are used to train ResGAN, the generator generates fake HSI samples with a similar distribution to real data, and the discriminator contains high values suitable for training a small number of samples with real labels. The main innovations of this method are twofold. First, the generative adversarial network (GAN) is based on a dense residual network, which fully learns the higher-level features of HSIs. Second, the loss function is modified using the Wasserstein distance with a gradient penalty, and the discriminant model of the network is changed to enhance the training stability. Using image data obtained from airborne visible–infrared sensors of an imaging spectrometer, the performance of ResGAN was compared with that of two HSI classification methods. The proposed network obtains excellent classification results after only marking a small number of samples. From both subjective and objective viewpoints, ResGAN is an excellent alternative to the standard GAN for HSI classification. © MYU K.K.","Classification (of information); Data handling; Deep learning; Hyperspectral imaging; Image classification; Infrared detectors; Remote sensing; Spectral resolution; Spectroscopy; Data dimensions; Dimension Data; Features extraction; Ground truth; High spectral resolution; Hyperspectral image classification; Hyperspectral Remote Sensing Image; Large data volumes; Number of samples; Training sample; Generative adversarial networks","Feature extraction; Generative adversarial network; Hyperspectral image classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121037451"
"Xu M.; Li Y.; Zhong J.; Zhang Y.; Liu X.","Xu, Miao (57224799413); Li, Yuanxiang (57208656802); Zhong, Juanjuan (55234626400); Zhang, Yuxuan (57189471764); Liu, Xingang (57195568543)","57224799413; 57208656802; 55234626400; 57189471764; 57195568543","Edge Prediction Net for Reconstructing Road Labels Contaminated by Clouds","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323273","6969","6972","3","10.1109/IGARSS39084.2020.9323273","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101959187&doi=10.1109%2fIGARSS39084.2020.9323273&partnerID=40&md5=d03767dd1d97df7f110c863efebb2a0a","Extracting road information from remote sensing images has been a popular issue for decades. However, most studies focus on cloudless datasets, without considering cloud occlusion. The thick clouds especially make it impossible to extract the road information from the blocked parts. To address this problem, we propose a new two-stage method. Since generative adversarial networks (GAN) have powerful image generation capabilities, the two-stage method comprises an edge prediction net relied on GAN and a color filling part. The edge prediction net sketches the contours of the region contaminated by thick clouds in road labels, and the second part fills colors in the missing part by using the edges predicted at the first stage. We evaluate our model over the DeepGlobe Road Extraction dataset. The results show that our model performs excellently on visual effects and evaluation indicators. © 2020 IEEE.","Forecasting; Geology; Roads and streets; Tunneling (excavation); Adversarial networks; Evaluation indicators; Image generations; Remote sensing images; Road extraction; Two-stage methods; Visual effects; Remote sensing","Cloud Removal; Edge Prediction; GAN; Road Extraction","Conference paper","Final","","Scopus","2-s2.0-85101959187"
"Wei Y.; Luo X.; Hu L.; Peng Y.; Feng J.","Wei, Yufan (57216437051); Luo, Xiaobo (36562124600); Hu, Lixin (57216441516); Peng, Yidong (57192995836); Feng, Jiangfan (55468575400)","57216437051; 36562124600; 57216441516; 57192995836; 55468575400","An improved unsupervised representation learning generative adversarial network for remote sensing image scene classification","2020","Remote Sensing Letters","11","6","","598","607","9","10.1080/2150704X.2020.1746854","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083482447&doi=10.1080%2f2150704X.2020.1746854&partnerID=40&md5=008203327ab0498004d4e7a1b66c0928","Unsupervised representation learning plays an important role in remote sensing image applications. Generative adversarial network (GAN) is the most popular unsupervised learning method in recent years. However, due to poor data augmentation, many GAN-based methods are often difﬁcult to carry out. In this paper, we propose an improved unsupervised representation learning model called multi-layer feature fusion Wasserstein GAN (MF-WGANs) which considers extracting the feature information for remote sensing scene classification from unlabelled samples. First, we introduced a multi-feature fusion layer behind the discriminator to extract the high-level and mid-level feature information. Second, we combined the loss of multi-feature fusion layer and WGAN-GP to generate more stable and high-quality remote sensing images with a resolution of 256 × 256. Finally, the multi-layer perceptron classifier (MLP-classifier) is used to classify the features extracted from the multi-feature fusion layer and evaluated with the UC Merced Land-Use, AID and NWPU-RESISC45 data sets. Experiments show that MF-WGANs has richer data augmentation and better classification performance than other unsupervised representation learning classification models (e.g., MARTA GANs). © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Image classification; Image enhancement; Land use; Learning systems; Remote sensing; Unsupervised learning; Adversarial networks; Classification models; Classification performance; Multi-feature fusion; Multi-layer perceptron classifiers; Remote sensing images; Scene classification; Unsupervised learning method; data quality; image analysis; image classification; image resolution; network analysis; remote sensing; unsupervised classification; Classification (of information)","","Article","Final","","Scopus","2-s2.0-85083482447"
"Panagiotou E.; Charou E.","Panagiotou, Emmanouil (57217281424); Charou, Eleni (6507509159)","57217281424; 6507509159","Procedural 3D terrain generation using generative adversarial networks","2020","CEUR Workshop Proceedings","2844","","","9","14","5","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104639159&partnerID=40&md5=f8796d0111c984f3f8f99cfbb802fa59","Procedural 3D Terrain generation has become a necessity in open world games, as it can provide unlimited content, through a functionally infinite number of different areas, for players to explore. In our approach, we use Generative Adversarial Networks (GAN) to yield realistic 3D environments based on the distribution of remotely sensed images of landscapes, captured by satellites or drones. Our task consists of synthesizing a random but plausible RGB satellite image and generating a corresponding Height Map in the form of a 3D point cloud that will serve as an appropriate mesh of the landscape. For the first step, we utilize a GAN trained with satellite images that manages to learn the distribution of the dataset, creating novel satellite images. For the second part, we need a one-to-one mapping from RGB images to Digital Elevation Models (DEM). We deploy a Conditional Generative Adversarial network (CGAN), which is the state-of-the-art approach to image-to-image translation, to generate a plausible height map for every randomly generated image of the first model. Combining the generated DEM and RGB image, we are able to construct 3D scenery consisting of a plausible height distribution and colorization, in relation to the remotely sensed landscapes provided during training. Copyright © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Remote sensing; Satellites; Surveying; Adversarial networks; Digital elevation model; Height distribution; Image translation; Infinite numbers; One-to-one mappings; Remotely sensed images; State-of-the-art approach; Artificial intelligence","3D point cloud; Deep learning; Digital elevation models; Gaming; General adversarial networks; Procedural generation; Satellite imagery","Conference paper","Final","","Scopus","2-s2.0-85104639159"
"Shin Y.H.; Hyung S.W.; Lee D.-C.","Shin, Young Ha (57219247571); Hyung, Sung Woong (57219915811); Lee, Dong-Cheon (36138668700)","57219247571; 57219915811; 36138668700","True orthoimage generation from LiDAR intensity using deep learning","2020","Journal of the Korean Society of Surveying, Geodesy, Photogrammetry and Cartography","38","4","","363","373","10","10.7848/ksgpc.2020.38.4.363","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096051488&doi=10.7848%2fksgpc.2020.38.4.363&partnerID=40&md5=5b5612c1271a745db5110a059774def7","During last decades numerous studies generating orthoimage have been carried out. Traditional methods require exterior orientation parameters of aerial images and precise 3D object modeling data and DTM (Digital Terrain Model) to detect and recover occlusion areas. Furthermore, it is challenging task to automate the complicated process. In this paper, we proposed a new concept of true orthoimage generation using DL (Deep Learning). DL is rapidly used in wide range of fields. In particular, GAN (Generative Adversarial Network) is one of the DL models for various tasks in imaging processing and computer vision. The generator tries to produce results similar to the real images, while discriminator judges fake and real images until the results are satisfied. Such mutually adversarial mechanism improves quality of the results. Experiments were performed using GAN-based Pix2Pix model by utilizing IR (Infrared) orthoimages, intensity from LiDAR data provided by the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF) through the ISPRS (International Society for Photogrammetry and Remote Sensing). Two approaches were implemented: (1) One-step training with intensity data and high resolution orthoimages, (2) Recursive training with intensity data and color-coded low resolution intensity images for progressive enhancement of the results. Two methods provided similar quality based on FID (Fréchet Inception Distance) measures. However, if quality of the input data is close to the target image, better results could be obtained by increasing epoch. This paper is an early experimental study for feasibility of DL-based true orthoimage generation and further improvement would be necessary. © 2020 Korean Society of Surveying. All rights reserved.","detection method; image analysis; image classification; image processing; lidar; machine learning; remote sensing","Deep Learning; GAN; LiDAR Intensity; True Orthoimage","Article","Final","","Scopus","2-s2.0-85096051488"
"Shashank A.; Sajithvariyar V.V.; Sowmya V.; Soman K.P.; Sivanpillai R.; Brown G.K.","Shashank, A. (57220023850); Sajithvariyar, V.V. (57189307235); Sowmya, V. (36096164300); Soman, K.P. (57205365723); Sivanpillai, R. (8970929800); Brown, G.K. (7406467671)","57220023850; 57189307235; 36096164300; 57205365723; 8970929800; 7406467671","Identifying epiphytes in drones photos with a conditional generative adversarial network (C-GAN)","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","44","M-2","","99","104","5","","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097566749&partnerID=40&md5=6edfcde607c994f9b47f5ecaa40c2cb3","Unmanned Aerial Vehicle (UAV) missions often collect large volumes of imagery data. However, not all images will have useful information, or be of sufficient quality. Manually sorting these images and selecting useful data are both time consuming and prone to interpreter bias. Deep neural network algorithms are capable of processing large image datasets and can be trained to identify specific targets. Generative Adversarial Networks (GANs) consist of two competing networks, Generator and Discriminator that can analyze, capture, and copy the variations within a given dataset. In this study, we selected a variant of GAN called Conditional-GAN that incorporates an additional label parameter, for identifying epiphytes in photos acquired by a UAV in forests within Costa Rica. We trained the network with 70%, 80%, and 90% of 119 photos containing the target epiphyte, Werauhia kupperiana (Bromeliaceae) and validated the algorithm's performance using a validation data that were not used for training. The accuracy of the output was measured using structural similarity index measure (SSIM) index and histogram correlation (HC) coefficient. Results obtained in this study indicated that the output images generated by C-GAN were similar (average SSIM = 0.89 - 0.91 and average HC 0.97 - 0.99) to the analyst annotated images. However, C-GAN had difficulty to identify when the target plant was away from the camera, was not well lit, or covered by other plants. Results obtained in this study demonstrate the potential of C-GAN to reduce the time spent by botanists to identity epiphytes in images acquired by UAVs. © 2020 International Society for Photogrammetry and Remote Sensing. All rights reserved.","Antennas; Deep neural networks; Drones; Large dataset; Remote sensing; Adversarial networks; Algorithm's performance; Bromeliaceae; Large images; Large volumes; Neural network algorithm; Structural similarity index measures (SSIM); Validation data; Image acquisition","CNN; Image translation; PIX2PIX; Segmentation; UAV; UNET","Conference paper","Final","","Scopus","2-s2.0-85097566749"
"Li J.; Wu Z.; Hu Z.; Zhang J.; Li M.; Mo L.; Molinier M.","Li, Jun (55902669000); Wu, Zhaocong (15023850900); Hu, Zhongwen (55630272400); Zhang, Jiaqi (57216751326); Li, Mingliang (57850159600); Mo, Lu (57217675347); Molinier, Matthieu (22234853700)","55902669000; 15023850900; 55630272400; 57216751326; 57850159600; 57217675347; 22234853700","Thin cloud removal in optical remote sensing images based on generative adversarial networks and physical model of cloud distortion","2020","ISPRS Journal of Photogrammetry and Remote Sensing","166","","","373","389","16","10.1016/j.isprsjprs.2020.06.021","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087526039&doi=10.1016%2fj.isprsjprs.2020.06.021&partnerID=40&md5=94e3dce17405ccf1cf7703204fe5079b","Cloud contamination is an inevitable problem in optical remote sensing images. Unlike thick clouds, thin clouds do not completely block out background which makes it possible to restore background information. In this paper, we propose a semi-supervised method based on generative adversarial networks (GANs) and a physical model of cloud distortion (CR-GAN-PM) for thin cloud removal with unpaired images from different regions. A physical model of cloud distortion which takes the absorption of cloud into consideration was also defined in this paper. It is worth noting that many state-of-the-art methods based on deep learning require paired cloud and cloud-free images from the same region, which is often unavailable or time-consuming to collect. CR-GAN-PM has two main steps: first, the cloud-free background and cloud distortion layers were decomposed from an input cloudy image based on GANs and the principles of image decomposition; then, the input cloudy image was reconstructed by putting those layers into the redefined physical model of cloud distortion. The decomposition process ensured that the decomposed background layer was cloud-free and the reconstruction process ensured that generated background layer was correlated with the input cloudy image. Experiments were conducted on Sentinel-2A imagery to validate the proposed CR-GAN-PM. Averaged over all testing images, the SSIMs values (structural similarity index measurement) of CR-GAN-PM were 0.72, 0.77, 0.81 and 0.83 for visible and NIR bands respectively. Those results were similar to the end-to-end deep learning-based methods and better than traditional methods. The number of input bands and values of hyper-parameters affected little on the performance of CR-GAN-PM. Experimental results show that CR-GAN-PM is effective and robust for thin cloud removal in different bands. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)","Deep learning; Learning systems; Remote sensing; Semi-supervised learning; Background information; Decomposition process; Learning-based methods; Optical remote sensing; Reconstruction process; Semi-supervised method; State-of-the-art methods; Structural similarity indices; cloud cover; decomposition analysis; image analysis; machine learning; optical property; remote sensing; satellite imagery; similarity index; Image reconstruction","Cloud removal; Generative Adversarial Networks (GANs); Image decomposition; Physical model of cloud distortion; Thin clouds","Article","Final","","Scopus","2-s2.0-85087526039"
"Xiao X.; Ganguli S.; Pandey V.","Xiao, Xuerong (57203623814); Ganguli, Swetava (57220208283); Pandey, Vipul (57220208165)","57203623814; 57220208283; 57220208165","VAE-Info-cGAN: Generating synthetic images by combining pixel-level and feature-level geospatial conditional inputs","2020","Proceedings of the 13th ACM SIGSPATIAL International Workshop on Computational Transportation Science, IWCTS 2020","","","","","","","10.1145/3423457.3429361","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097272707&doi=10.1145%2f3423457.3429361&partnerID=40&md5=697551368855e0793eec48fb7a9afca0","Training robust supervised deep learning models for many geospatial applications of computer vision is difficult due to dearth of class-balanced and diverse training data. Conversely, obtaining enough training data for many applications is financially prohibitive or may be infeasible, especially when the application involves modeling rare or extreme events. Synthetically generating data (and labels) using a generative model that can sample from a target distribution and exploit the multi-scale nature of images can be an inexpensive solution to address scarcity of labeled data. Towards this goal, we present a deep conditional generative model, called VAE-Info-cGAN, that combines a Variational Autoencoder (VAE) with a conditional Information Maximizing Generative Adversarial Network (InfoGAN), for synthesizing semantically rich images simultaneously conditioned on a pixel-level condition (PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC can only vary in the channel dimension from the synthesized image and is meant to be a task-specific input. The FLC is modeled as an attribute vector, a, in the latent space of the generated image which controls the contributions of various characteristic attributes germane to the target distribution. During generation, a is sampled from U[0, 1], while it is learned directly from the ground truth during training. An interpretation of a to systematically generate synthetic images by varying a chosen binary macroscopic feature is explored by training a linear binary classifier in the latent space. Experiments on a GPS trajectories dataset show that the proposed model can accurately generate various forms of spatio-temporal aggregates across different geographic locations while conditioned only on a raster representation of the road network. The primary intended application of the VAE-Info-cGAN is synthetic data (and label) generation for targeted data augmentation for computer vision-based modeling of problems relevant to geospatial analysis and remote sensing. © 2020 ACM.","Computer vision; Deep learning; Pixels; Remote sensing; Vector spaces; Adversarial networks; Channel dimension; Geo-spatial analysis; Geographic location; Geospatial applications; Linear binary classifiers; Spatio-temporal aggregates; Synthesized images; Learning systems","deep conditional generative models; GAN; synthetic data; VAE","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097272707"
"Lv N.; Ma H.; Chen C.; Pei Q.; Zhou Y.; Xiao F.; Li J.","Lv, Ning (57220505216); Ma, Hongxiang (57222242244); Chen, Chen (56640091900); Pei, Qingqi (15763462800); Zhou, Yang (35209382100); Xiao, Fenglin (57222239228); Li, Ji (57218474499)","57220505216; 57222242244; 56640091900; 15763462800; 35209382100; 57222239228; 57218474499","Remote Sensing Data Augmentation through Adversarial Training","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","9318","9333","15","10.1109/JSTARS.2021.3110842","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114721913&doi=10.1109%2fJSTARS.2021.3110842&partnerID=40&md5=0667824a78e6bf3eb85efaf13e4bfadb","The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9%, meeting actual generation requirements.  © 2008-2012 IEEE.","Anhui; China; Jiangxi; Convolutional neural networks; Deep neural networks; Image enhancement; Image segmentation; Semantics; Signal sampling; Adversarial networks; Generation requirements; High quality images; Remote sensing data; Remote sensing images; Remote sensing interpretation; Semantic information; Semantic interpretation; artificial neural network; computer simulation; data quality; image resolution; numerical model; performance assessment; remote sensing; satellite data; segmentation; Remote sensing","Data augmentation; deep supervision; downsampling; GAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114721913"
"Mateo-Garcia G.; Laparra V.; Lopez-Puigdollers D.; Gomez-Chova L.","Mateo-Garcia, Gonzalo (57192947904); Laparra, Valero (34869963500); Lopez-Puigdollers, Dan (57203813986); Gomez-Chova, Luis (6603354695)","57192947904; 34869963500; 57203813986; 6603354695","Cross-Sensor Adversarial Domain Adaptation of Landsat-8 and Proba-V Images for Cloud Detection","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9234601","747","761","14","10.1109/JSTARS.2020.3031741","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099347164&doi=10.1109%2fJSTARS.2020.3031741&partnerID=40&md5=938f6a72163db79fe9750116d29a9e48","The number of Earth observation satellites carrying optical sensors with similar characteristics is constantly growing. Despite their similarities and the potential synergies among them, derived satellite products are often developed for each sensor independently. Differences in retrieved radiances lead to significant drops in accuracy, which hampers knowledge and information sharing across sensors. This is particularly harmful for machine learning algorithms, since gathering new ground-truth data to train models for each sensor is costly and requires experienced manpower. In this work, we propose a domain adaptation transformation to reduce the statistical differences between images of two satellite sensors in order to boost the performance of transfer learning models. The proposed methodology is based on the cycle consistent generative adversarial domain adaptation framework that trains the transformation model in an unpaired manner. In particular, Landsat-8 and Proba-V satellites, which present different but compatible spatio-spectral characteristics, are used to illustrate the method. The obtained transformation significantly reduces differences between the image datasets while preserving the spatial and spectral information of adapted images, which is, hence, useful for any general purpose cross-sensor application. In addition, the training of the proposed adversarial domain adaptation model can be modified to improve the performance in a specific remote sensing application, such as cloud detection, by including a dedicated term in the cost function. Results show that, when the proposed transformation is applied, cloud detection models trained in Landsat-8 data increase cloud detection accuracy in Proba-V. © 2008-2012 IEEE.","Satellites; Cost functions; Learning algorithms; Metadata; Remote sensing; Satellites; Transfer learning; Earth observation satellites; Potential synergies; Remote sensing applications; Sensor applications; Spectral characteristics; Spectral information; Statistical differences; Transformation model; algorithm; artificial neural network; data set; image processing; Landsat; Proba; satellite sensor; Learning systems","cloud detection; convolutional neural networks; domain adaptation; Generative adversarial networks; Landsat-8; Proba-V","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099347164"
"He Y.; Xie K.; Li T.; Sun X.; Li T.; Chen S.","He, Yiqing (57215548011); Xie, Kai (57212154137); Li, Tong (57211431170); Sun, Xingyu (57215548007); Li, Ting (57206667268); Chen, Shilong (57215543182)","57215548011; 57212154137; 57211431170; 57215548007; 57206667268; 57215543182","Generating satisfactory terrain by terrain maker generative adversarial nets","2020","Proceedings of SPIE - The International Society for Optical Engineering","11432","","114320Q","","","","10.1117/12.2536635","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081120707&doi=10.1117%2f12.2536635&partnerID=40&md5=12c491850e527c60cdb4b223415bc218","Generative Adversarial Networks (GANs) is one of the most promising generative model in recently years. In this paper, we proposed a model called terrain maker Generative Adversarial Networks (TMGAN). It differs from the original GANs in three points: first, based on given topographic map, TMGAN can generate corresponding satellite aerial map, and vice versa. Second, TMGAN can modeled the terrain adaptively. Third, TMGAN can predict the height map of surface environment. We collected two data sets of paired and unpaired topographic maps and satellite aerial maps to train our model and test the influence of hidden variables. In this paper, we demonstrate the three-dimensional modeling ability of TMGAN. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","3D modeling; Antennas; Deep learning; Deep neural networks; Geographic information systems; Information systems; Information use; Landforms; Maps; Neural networks; Pattern recognition systems; Remote sensing; Adversarial networks; Generative Adversarial Nets; Generative model; Height map; Hidden variable; Surface environments; Three-dimensional model; Topographic map; Image processing","3D modeling; Deep learning; Generative Adversarial Nets; Neural network","Conference paper","Final","","Scopus","2-s2.0-85081120707"
"Meng D.; Wu B.; Liu N.; Chen W.","Meng, Delin (57220200612); Wu, Bangyu (55270666700); Liu, Naihao (57192300490); Chen, Wenchao (55761629500)","57220200612; 55270666700; 57192300490; 55761629500","Semi-Supervised Deep Learning Seismic Impedance Inversion Using Generative Adversarial Networks","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323119","1393","1396","3","10.1109/IGARSS39084.2020.9323119","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102004073&doi=10.1109%2fIGARSS39084.2020.9323119&partnerID=40&md5=bf9ccda31b424dc888b16aae2ed7fcbb","Deep learning methods have been successfully applied to solve seismic inversion problems in recent years. Though deep learning inversion can obtain results with much higher resolution compared to geophysical inversion, its performance often suffers from the limitation of the well logs which are main source of labels in training data. To overcome this problem, we propose a semi-supervised deep learning workflow based on Generative Adversarial Network (GAN) for seismic impedance inversion. The workflow contains three networks: a generator, a discriminator, and a forward model. The training of the generator and discriminator are guided by well logs and constrained by unlabeled data via the forward model. Test on Marmousi2 model shows that, by making use of both labeled and unlabeled data, the proposed method predicts impedance with better consistency than conventional deep learning inversion. © 2020 IEEE.","Geology; Learning systems; Remote sensing; Seismology; Semi-supervised learning; Well logging; Adversarial networks; Geophysical inversion; Higher resolution; Labeled and unlabeled data; Learning methods; Marmousi-2 model; Seismic impedance; Seismic inversion; Deep learning","deep learning; Generative adversarial network; seismic impedance inversion; semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85102004073"
"Romero L.S.; Marcello J.; Vilaplana V.","Romero, Luis Salgueiro (57218455911); Marcello, Javier (6602158797); Vilaplana, Verónica (23394280500)","57218455911; 6602158797; 23394280500","Super-resolution of Sentinel-2 imagery using generative adversarial networks","2020","Remote Sensing","12","15","2424","","","","10.3390/RS12152424","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089853089&doi=10.3390%2fRS12152424&partnerID=40&md5=6bb6fc362a861fc8952daf3ca71d5b36","Sentinel-2 satellites provide multi-spectral optical remote sensing images with four bands at 10 m of spatial resolution. These images, due to the open data distribution policy, are becoming an important resource for several applications. However, for small scale studies, the spatial detail of these images might not be sufficient. On the other hand, WorldView commercial satellites offer multi-spectral images with a very high spatial resolution, typically less than 2 m, but their use can be impractical for large areas or multi-temporal analysis due to their high cost. To exploit the free availability of Sentinel imagery, it is worth considering deep learning techniques for single-image super-resolution tasks, allowing the spatial enhancement of low-resolution (LR) images by recovering high-frequency details to produce high-resolution (HR) super-resolved images. In this work, we implement and train a model based on the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) with pairs of WorldView-Sentinel images to generate a super-resolved multispectral Sentinel-2 output with a scaling factor of 5. Our model, named RS-ESRGAN, removes the upsampling layers of the network to make it feasible to train with co-registered remote sensing images. Results obtained outperform state-of-the-art models using standard metrics like PSNR, SSIM, ERGAS, SAM and CC. Moreover, qualitative visual analysis shows spatial improvements as well as the preservation of the spectral information, allowing the super-resolved Sentinel-2 imagery to be used in studies requiring very high spatial resolution. © 2020 by the authors.","Deep learning; Image analysis; Image resolution; Network layers; Open Data; Optical resolving power; Remote sensing; Spectroscopy; Commercial satellites; Data distribution policies; Low resolution images; Multi-temporal analysis; Optical remote sensing; Remote sensing images; Spectral information; Very high spatial resolutions; Image enhancement","Deep learning; Generative adversarial network; Sentinel-2; Super-resolution; WorldView","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089853089"
"","","","7th International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2021","2021","Communications in Computer and Information Science","1452 CCIS","","","","","1055","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115713811&partnerID=40&md5=f06be6e5abb6d7c68eba6453168663ef","The proceedings contain 81 papers. The special focus in this conference is on Pioneering Computer Scientists, Engineers and Educators. The topics include: MA Mask R-CNN: MPR and AFPN Based Mask R-CNN; improved Non-negative Matrix Factorization Algorithm for Sparse Graph Regularization; a Blockchain-Based Scheme of Data Sharing for Housing Provident Fund; intelligent Storage System of Machine Learning Model Based on Task Similarity; predicting Stock Price Movement with Multiple Data Sources and Machine Learning Models; channel Context and Dual-Domain Attention Based U-Net for Skin Lesion Attributes Segmentation; study on the Protection and Product Development of Intangible Cultural Heritage with Computer Virtual Reality Technology; ECG-Based Arrhythmia Detection Using Attention-Based Convolutional Neural Network; Quantum Color Image Scaling on QIRHSI Model; WSN Data Compression Model Based on K-SVD Dictionary and Compressed Sensing; human Body Pose Recognition System Based on Teaching Interaction; adaptive Densely Residual Network for Image Super-Resolution; Real-Time Image and Video Artistic Style Rendering System Based on GPU; semantic Segmentation of High Resolution Remote Sensing Images Based on Improved ResU-Net; Exploring Classification Capability of CNN Features; generative Adversarial Network Based Status Generation Simulation Approach; the Construction of Case Event Logic Graph for Judgment Documents; anti-obfuscation Binary Code Clone Detection Based on Software Gene; thread Private Variable Access Optimization Technique for Sunway High-Performance Multi-core Processors; parallel Region Reconstruction Technique for Sunway High-Performance Multi-core Processors; research on Route Optimization of Battlefield Collection Equipment Based on Improved Ant Algorithm; a Collaborative Cache Strategy Based on Utility Optimization; integrating Local Closure Coefficient into Weighted Networks for Link Prediction.","","","Conference review","Final","","Scopus","2-s2.0-85115713811"
"Sustika R.; Suksmono A.B.; Danudirdjo D.; Wikantika K.","Sustika, Rika (56523358200); Suksmono, Andriyan Bayu (6602490139); Danudirdjo, Donny (14019254300); Wikantika, Ketut (6603393238)","56523358200; 6602490139; 14019254300; 6603393238","Generative Adversarial Network with Residual Dense Generator for Remote Sensing Image Super Resolution","2020","Proceeding - 2020 International Conference on Radar, Antenna, Microwave, Electronics and Telecommunications, ICRAMET 2020","","","9298648","34","39","5","10.1109/ICRAMET51080.2020.9298648","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099336277&doi=10.1109%2fICRAMET51080.2020.9298648&partnerID=40&md5=d15bcbb688d091181df293460aec5d83","Improving image resolution, especially spatial resolution, has been one of the most important concerns on remote sensing research communities. An efficient solution for improving spatial resolution is by using algorithm, known as super-resolution (SR). The super-resolution technique that received special attention recently is super-resolution based on deep learning. In this paper, we propose deep learning approach based on generative adversarial network (GAN) for remote sensing images super resolution. We used residual dense network (RDN) as generator network. Generally, deep learning with residual dense network (RDN) gives high performance on classical (objective) evaluation metrics meanwhile generative adversarial network (GAN) based approach shows a high perceptual quality. Experiment results show that combination of residual dense network generator with generative adversarial network training is found to be effective. Our proposed method outperforms the baseline method in terms of objective and perceptual quality evaluation metrics. © 2020 IEEE.","Deep learning; Image enhancement; Image resolution; Optical resolving power; Quality control; Radar; Adversarial networks; Evaluation metrics; Learning approach; Perceptual quality; Remote sensing images; Research communities; Spatial resolution; Super resolution; Remote sensing","convolutional neural network; generative adversarial network; image; remote sensing; residual dense network; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85099336277"
"Jayanarayan A.; Sowmya V.; Soman K.P.","Jayanarayan, Abhijith (57215898504); Sowmya, V. (36096164300); Soman, K.P. (57205365723)","57215898504; 36096164300; 57205365723","Remote Sensing Image Super-Resolution Using Residual Dense Network","2020","Advances in Intelligent Systems and Computing","1118","","","721","729","8","10.1007/978-981-15-2475-2_66","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082294431&doi=10.1007%2f978-981-15-2475-2_66&partnerID=40&md5=b751e81e9d91e257762a3953bbf3cebe","Image super-resolution (SR) is a wide research topic, as it has found multiple applications in different fields. We implement image super-resolution for satellite images using a residual dense network (RDN). RDN is a CNN-based model, but unlike most CNN-based super-resolution models, it utilizes the hierarchic features from the input low resolution (LR) images and combines both the specific and general features present in the image, therefore resulting in a better performance. The novelty of our work lies in two aspects. First, we apply the residual dense network to remote sensing data to obtain higher structure similarity index metric (SSIM) and peak signal-to-noise ratio (PSNR) values than the existing models. Second, we use transfer learning due to the lack of training samples in remote sensing domain. Our RDN is first trained using an external dataset DIVerse2K (DIV2K). This model is then used to obtain high-resolution(HR) images of the remote sensing U.C Merced dataset, and the corresponding PSNR and SSIM values are computed for different scaling factors such as 2, 4 and 8. The experimental results obtained using the proposed work demonstrates the better performance of RDN for the super-resolution of remote sensing images, when compared to the existing methods like super-resolution generative adversarial network (SRGAN) and transferred generative adversarial network (TGAN). © 2020, Springer Nature Singapore Pte Ltd.","Deep learning; Optical resolving power; Signal processing; Signal to noise ratio; Soft computing; Transfer learning; High resolution image; Image; Image super resolutions; Multiple applications; Peak signal to noise ratio; Remote sensing images; Super resolution; Super-resolution models; Remote sensing","Deep learning; Image; Remote sensing; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85082294431"
"Zhao C.; Li C.; Feng S.; Su N.","Zhao, Chunhui (7403563984); Li, Chuang (57216409923); Feng, Shou (57200002403); Su, Nan (57203308751)","7403563984; 57216409923; 57200002403; 57203308751","HYPERSPECTRAL ANOMALY DETECTION USING BILATERAL-FILTERED GENERATIVE ADVERSARIAL NETWORKS","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4408","4411","3","10.1109/IGARSS47720.2021.9553233","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121967387&doi=10.1109%2fIGARSS47720.2021.9553233&partnerID=40&md5=650cf20d36550acd84585d7a3346d808","Without any prior information of anomalies or background, hyperspectral anomaly detection has received a wide attention. However, such unsupervised style brings difficulties in training and learning effective features of hyperspectral image to perform detection. This paper proposes a novel hyperspectral anomaly detection algorithm using bilateral-filtered generative adversarial networks (BFGAN). Bilateral filter can smooth images and remove anomalous points while preserving edges. With closeness weights and similarity weights, the bilateral-filtered hyperspectral image can be considered as background data, so that hyperspectral background labels are obtained. Only with one class of labels, the structure of generative adversarial networks has an ability to solve two-class problem. By using the filtered background data and their labels, generative adversarial networks are trained to improve discriminator's discriminative capability for background data in a competing style. Finally, the model discriminator can finally output big probabilities for background samples and small probabilities for anomalous samples. Experiments on two real hyperspectral images demonstrate that the proposed method outperforms other state-of-the-art competitors. © 2021 IEEE.","","Anomaly detection; Bilateral filter; Deep learning; Generative adversarial networks; Hyperspectral remote sensing","Conference paper","Final","","Scopus","2-s2.0-85121967387"
"Jiang F.; Gong M.; Zhan T.; Fan X.","Jiang, Fenlong (57210730944); Gong, Maoguo (8933846400); Zhan, Tao (57052462500); Fan, Xiaolong (57209827950)","57210730944; 8933846400; 57052462500; 57209827950","A Semisupervised GAN-Based Multiple Change Detection Framework in Multi-Spectral Images","2020","IEEE Geoscience and Remote Sensing Letters","17","7","8854295","1223","1227","4","10.1109/LGRS.2019.2941318","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090386350&doi=10.1109%2fLGRS.2019.2941318&partnerID=40&md5=9996c3aeb4484448b04822ecaf9650ad","Effectively highlighting multiple changes in the earth surface from multi-temporal remote sensing images is a meaningful but challenging task. In order to reduce costs and ensure the performance, it is advisable to employ a semisupervised strategy to achieve this goal. As a discriminative joint classification task, semisupervised change detection aims to extract useful and discriminative features from a large amount of unlabeled data in addition to limited labeled samples. The discriminator of a well-trained generative adversarial network (GAN) is just right for this. Therefore, in this letter, we proposed a semisupervised GAN-based multiple change detection framework for multi-spectral images. First, the GAN is trained by all data without any prior information. Then, we combine two identical trained discriminators to construct a dual-pipeline joint classifier. Finally, the classifier is fine-tuned by a very small amount of labeled data to detect multiple changes. The superior performance of the proposed model over both real multi-spectral data sets demonstrates its robustness and effectiveness.  © 2004-2012 IEEE.","Pipelines; Spectroscopy; Adversarial networks; Change detection; Classification tasks; Discriminative features; Multi-spectral data; Multi-temporal remote sensing; Multispectral images; Prior information; algorithm; detection method; image analysis; spectral analysis; supervised learning; Remote sensing","Generative adversarial network (GAN); multi-spectral images; multiple change detection; semisupervised","Article","Final","","Scopus","2-s2.0-85090386350"
"Chaabane F.; Réjichi S.; Tupin F.","Chaabane, Ferdaous (6602356964); Réjichi, Safa (36816454600); Tupin, Florence (6603939644)","6602356964; 36816454600; 6603939644","SELF-ATTENTION GENERATIVE ADVERSARIAL NETWORKS FOR TIMES SERIES VHR MULTISPECTRAL IMAGE GENERATION","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4644","4647","3","10.1109/IGARSS47720.2021.9553597","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126023484&doi=10.1109%2fIGARSS47720.2021.9553597&partnerID=40&md5=dafa91eb19c5c3957af0a9a2e301a9f0","Recently classical deep learning approaches are commonly used to perform spatial and temporal classification especially for Very High Resolution (VHR) images. They learn from existing low resolution or undersized datasets because of the availability and prices of VHR remote sensing images. Thus, they have witnessed a conspicuous success because it is quite challenging to classify high-dimensional multispectral time series data with few labeled samples. It is also difficult to simulate high quality samples having the same features as the real ones. It goes without saying that the introduction of GANs (Generative Adversarial Network) models as an unsupervised learning method, has allowed the extraction of accurate representations of the data via latent codes and back-propagation techniques. However, it is difficult to acquire high-quality samples with unwanted noises and uncontrolled divergences. To generate high-quality multispectral time series samples, a Self-Attention Generative Adversarial Network (SAGAN) is proposed in this work. SAGAN allows attention-driven, long-range dependency modeling for VHR Multispectral time series image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations which improves training dynamics. The proposed SAGAN performs better than traditional GANs, boosting the best inception score. The main contribution of this work is the use of one of the new generation of learning techniques, SAGAN, for Times Series VHR Multispectral Image Generation. SAGAN has been recently used only for single image generation. © 2021 IEEE","","Deep learning techniques; Etc; Multispectral VHR time series; Self-attention generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85126023484"
"Wang Z.; Jiang K.; Yi P.; Han Z.; He Z.","Wang, Zhongyuan (57203515592); Jiang, Kui (57203871718); Yi, Peng (57203880354); Han, Zhen (56415515700); He, Zheng (53881225400)","57203515592; 57203871718; 57203880354; 56415515700; 53881225400","Ultra-dense GAN for satellite imagery super-resolution","2020","Neurocomputing","398","","","328","337","9","10.1016/j.neucom.2019.03.106","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075425341&doi=10.1016%2fj.neucom.2019.03.106&partnerID=40&md5=5268eacbe62512668065597c63767082","Image super-resolution (SR) techniques improve various remote sensing applications by allowing for finer spatial details than those captured by the original acquisition sensors. Recent advances in deep learning bring a new opportunity for SR by learning the mapping from low to high resolution. The most used convolutional neural networks (CNN) based approaches are prone to excessive smoothing or blurring due to the optimization objective in mean squared error (MSE). Instead, generative adversarial network (GAN) based approaches can achieve more perceptually acceptable results. However, the preliminary design of GANs generator with simple direct- or skip-connection residual blocks compromises its SR potential. Emerging dense convolutional network (DenseNet) equipped with dense connections has shown a promising prospect in classification and super-resolution. An intuitive idea to introduce DenseNet into GAN is expected to boost SR performance. However, because convolutional kernels in the existing residual block are arranged into a one-dimensional flat structure, the formation of dense connections highly relies on skip connections (linking the current layer to all subsequent layers with a shortcut path). In order to increase connection density, the depth of the layer has to be accordingly expanded, which in turn results in training difficulties such as vanishing gradient and information propagation loss. To this end, this paper proposes an ultra-dense GAN (udGAN) for image SR, where we reform the internal layout of the residual block into a two-dimensional matrix topology. This topology can provide additional diagonal connections so that we can still accomplish enough pathways with fewer layers. In particular, the pathways are almost doubled compared to previous dense connections under the same number of layers. The achievable rich connections are flexibly adapted to the diversity of image content, thus leading to improved SR performance. Extensive experiments on public benchmark datasets and real-world satellite imagery show that our model outperforms state-of-the-art counterparts in both subjective and quantitative assessments, especially those related to perception. © 2019 Elsevier B.V.","Backpropagation; Convolution; Deep learning; Image enhancement; Information dissemination; Mean square error; Neural networks; Optical resolving power; Remote sensing; Topology; Convolutional networks; Convolutional neural network; Image super resolutions; Information propagation; Quantitative assessments; Remote sensing applications; Super resolution; Ultra-dense residual block; Article; convolutional neural network; correlation analysis; data base; deep recursive residual network; dense convolutional network; generative adversarial network; image analysis; image processing; image quality; image reconstruction; priority journal; process optimization; satellite imagery; super resolution convolutional neural network; Satellite imagery","GAN; Satellite imagery; Super-resolution; Ultra-dense residual block","Article","Final","","Scopus","2-s2.0-85075425341"
"Cao X.; Song C.; Zhang J.; Liu C.","Cao, Xia (57222516531); Song, Chenggang (57222516158); Zhang, Jian (57190179102); Liu, Chang (57157480800)","57222516531; 57222516158; 57190179102; 57157480800","Remote Sensing Image Segmentation based on Generative Adversarial Network with Wasserstein divergence","2020","ACM International Conference Proceeding Series","","","3446187","","","","10.1145/3446132.3446187","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102973676&doi=10.1145%2f3446132.3446187&partnerID=40&md5=83070cab2b95d610140a1cb68c85ad92","In the image segmentation fields, traditional methods can be classified into four main categories: threshold-based (e.g. Otsu [1]), .edge-based (e.g. Canny [2], Hough transform [3]), region-based (e.g. Super pixel [4]), and energy functional-based segmentation methods(e.g. Level set [5]). These segmentation methods have been successfully applied to natural and medical Images [6-8]. Although a general image segmentation problem has been extensively investigated, it is still challenging for remote sensing data. Due to the high spatial resolution of remotely sensed images, the segmentation task is complicated by the increased intra-class variance and decreased inter-class difference [9]. Therefore, it is necessary to construct a strong segmentation system, which performs highquality, pixelwise, and large-scale segmentation. In the last decade, convolutional neural networks (CNNs) [10] have shown superior performance over existing state-of-art techniques of image segmentation. As a kind of typical CNNs, fully convolutional networks (FCNs) [11] have been proposed firstly as an end-to-end learning framework. However, FCNs obtained only coarse segmentation results with a simple deconvolution procedure. Based on FCNs, Seg- Nets [12] presented a symmetric encoder-decoder framework for the appropriate decoders using the max-pooling indices received from the corresponding encoder performed non-linear upsampling of their input feature maps. The encoder-decoder structure has been proven effective to recover the losing details. In addition to traditional encoder-decoder layers, U-net [13] architecture uses skip connections between the mirror layers. The architecture of skip connection makes the high-resolution features from the contracting path combined with the upsampling output enable to increase the precision of localization. Among the FCNs-based method, U-net architecture has gained particular attention due to the improvement of segmentation accuracy. However, the common per-pixel loss functions for semantic segmentation such as L1 regression, cross-entropy, or dice coefficient used typically in CNNs are not sensitive enough to context relationships to some misalignment of localization [14]. There is a considerable solution to this problem by employing generative adversarial networks (GANs) [15] from a perspective of image generation. GANs have achieved impressive results for generating realistic and reasonable images through a minimax two-player game: a generator manages to generate samples as close to the real data as possible to fool the discriminator, and a discriminator needs to distinguish the fake samples came from the generator rather than the training data. GANs shows the great ability to describe object semantic concepts and the contextual relationship with the background [16]. This ability of GANs to semantic understanding is suitable for semantic segmentation tasks. Luc et al. [14] have firstly proposed a GANs-based network for natural image segmentation. And they have gained a little accuracy improvement on the Stanford Background and PASCAL VOC 2012 datasets. Pix2pix [17] has proposed a conditional GANs(cGANs) [18] and patchGANs [17] method for segmentation from a view of image translation. It is a pity that the results showed that generating a segmentation result with L1 regression gets better scores than cGANs. [19] has proved that original GANs have the defect of gradient disappearing, and proposed a new Wasserstein GANs(WGANs) [20] model to overcome this drawback. WGANs requires the discriminator to satisfy the Lipschitz constraint. Many WGANs variants had continued to search for a graceful way to construct 1-Lipschitz conditions, from weight clipping to gradient  © 2020 ACM.","Convolution; Convolutional neural networks; Decoding; Feature extraction; Game theory; Hough transforms; Medical imaging; Network architecture; Numerical methods; Pixels; Remote sensing; Semantics; Signal encoding; Signal sampling; Contextual relationships; Convolutional networks; High spatial resolution; Lipschitz constraints; Natural image segmentations; Remotely sensed images; Semantic segmentation; Semantic understanding; Image segmentation","Generative Adversarial Networks(GANs); Remote sensing; Semantic segmentation; Wasserstein divergence","Conference paper","Final","","Scopus","2-s2.0-85102973676"
"Wang G.; Ren P.","Wang, Guangxing (57213191777); Ren, Peng (25960361900)","57213191777; 25960361900","Delving into Classifying Hyperspectral Images via Graphical Adversarial Learning","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9086159","2019","2031","12","10.1109/JSTARS.2020.2992310","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085652739&doi=10.1109%2fJSTARS.2020.2992310&partnerID=40&md5=c5e6d2c3a42d4ef6fe16de8041bf62fd","Recent remote sensing literature has seen generative adversarial network (GAN)-based models developed for hyperspectral image classification, especially in a spatiospectral manner. The intuition is that training classifiers with additional generated hyperspectral data improves model generalization and hence increases classification accuracy. Existing GAN-based hyperspectral image classification methods tend to straightforwardly characterize spatiospectral characteristics of hyperspectral data subject to basic distributions (e.g., Gaussian or uniform distributions). However, hyperspectral imageries have high dimensions in both spectral and spatial representations, which are possibly derived from a latent space following more sophisticated distributions. In this scenario, we believe that comprehensively modeling the latent space would favor accurate hyperspectral classification. To this end, we develop a graphical adversarial learning (GAL) framework that explores the latent variable structure for generating diversified hyperspectral samples. The comprehensive modeling strategy enables GAL to be capable of accurately characterizing full-depth hyperspectral data such that it establishes an end-to-end framework that does not require reduction on spectral bands. We conduct extensive experiments on three public hyperspectral datasets in terms of processing full-depth hyperspectral images without dimension reduction. The experimental results validate that, first, our GAL excels at full-depth hyperspectral image classification, and second, our GAL is competitive with state-of-the-art methods which use global data (i.e., both training and testing data) for learning spectral reduction. © 2008-2012 IEEE.","Remote sensing; Spectroscopy; Adversarial networks; Classification accuracy; Hyper-spectral classification; Hyper-spectral imageries; Spatial representations; State-of-the-art methods; Training and testing; Uniform distribution; data set; image analysis; image classification; machine learning; remote sensing; scenario analysis; spectral analysis; Image classification","Generative adversarial networks (GANs); graphical learning; hyperspectral image classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85085652739"
"Wang B.; Zhang S.; Feng Y.; Mei S.; Jia S.; Du Q.","Wang, Baorui (57255138300); Zhang, Shun (56198001500); Feng, Yan (36696426500); Mei, Shaohui (25822578400); Jia, Sen (7202859948); Du, Qian (7202060063)","57255138300; 56198001500; 36696426500; 25822578400; 7202859948; 7202060063","Hyperspectral Imagery Spatial Super-Resolution Using Generative Adversarial Network","2021","IEEE Transactions on Computational Imaging","7","","","948","960","12","10.1109/TCI.2021.3110103","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114738560&doi=10.1109%2fTCI.2021.3110103&partnerID=40&md5=ae42111f52deef50449fba0ec8cd0b9c","Hyperspectral imagery contains both spatial structure information and abundant spectral features of imaged objects. However, due to sensor limitations, abundant spectral information always comes at the sacrifice of low spatial resolution, which brings about difficulties with object analysis and identification. The super-resolution (SR) of HSIs, restored by the traditional interpolation algorithms or the network models trained with the mean-square-error-based loss function, tends to produce over-smoothed images. In this paper, we propose a novel Hyperspectral imagery Spatial Super-Resolution algorithm based on a Generative Adversarial Network (HSSRGAN). The generator network in HSSRGAN consists of two interacting part, i.e., a spatial feature enhanced network (SFEN) and a spectral refined network (SRN), while the discriminator network is employed to predict the probability that the authentic HR image is comparatively more similar than the forged generated image. Concretely, SFEN with the special dense residual blocks is designed to fully extract and enhance more deep hierarchical spatial features of hyperspectral imagery, while SRN is constructed to capture spectral interrelationships and refine the spatial context information so as to increase spatial resolution and alleviate spectral distortion. Moreover, SFEN and SRN are trained by the least-absolute-deviation based loss function to investigate spatial context and the spectral-angle-mapper based loss function to refine spectral information. We validate two versions of our proposed algorithm, 3D-HSSRGAN and 2D-HSSRGAN, on Pavia Centre dataset and Cuprite dataset. Experimental results show that the presented approach is superior to several existing state-of-the-art works.  © 2015 IEEE.","Image resolution; Mean square error; Optical resolving power; Oxide minerals; Remote sensing; Spectroscopy; Adversarial networks; Hyper-spectral imageries; Interpolation algorithms; Least absolute deviations; Spatial structure information; Spectral angle mappers; Spectral information; Super resolution algorithms; Image enhancement","Generative Adversarial Network(GAN); Hyperspectral Image; Spatial Resolution; Spatial Super-resolution","Article","Final","","Scopus","2-s2.0-85114738560"
"Efremova N.; Erten E.","Efremova, Natalia (42261427800); Erten, E. (23011359600)","42261427800; 23011359600","BIOPHYSICAL PARAMETER ESTIMATION USING EARTH OBSERVATION DATA IN A MULTI-SENSOR DATA FUSION APPROACH: CYCLEGAN","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","5965","5968","3","10.1109/IGARSS47720.2021.9553561","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125673847&doi=10.1109%2fIGARSS47720.2021.9553561&partnerID=40&md5=1ef050ca029c0e825bcd1a4f163f262c","Water management and up-to-date soil moisture (SM) information are crucial to ensure agricultural activities in dry-land farming regions. In this context, remote sensing imagery coupled with machine learning techniques can provide large scale SM information if there is enough data for training, which is really limited in reality. In this paper, we explored the potential of cycle-consistent Generative Adversarial Network (GAN) for data augmentation for training machine learning algorithms, which try to model spatial and temporal dependencies between the SM prediction (output) and the remote sensing imagery (input features). Specifically, the freely available SAR (Sentinel-1) and optical (Sentinel-2) time series data were evaluated together to predict SM using GANs. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there is not enough data to train a regression convolutional neural networks (CNN) to predict SM content. © 2021 IEEE.","","Autoencoders; CNN; CycleGAN; PCA; Ridge regression; Sentinel-1; Sentinel-2; Soil moisture; Support vector regression","Conference paper","Final","","Scopus","2-s2.0-85125673847"
"Pashaei M.; Starek M.J.; Kamangir H.; Berryhill J.","Pashaei, Mohammad (57213189326); Starek, Michael J. (23494042900); Kamangir, Hamid (57196243463); Berryhill, Jacob (56542823200)","57213189326; 23494042900; 57196243463; 56542823200","Deep learning-based single image super-resolution: An investigation for dense scene reconstruction with UAS photogrammetry","2020","Remote Sensing","12","11","1757","","","","10.3390/rs12111757","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086470516&doi=10.3390%2frs12111757&partnerID=40&md5=2371fcfa424e180cf1f098715b7cd45d","The deep convolutional neural network (DCNN) has recently been applied to the highly challenging and ill-posed problem of single image super-resolution (SISR), which aims to predict high-resolution (HR) images from their corresponding low-resolution (LR) images. In many remote sensing (RS) applications, spatial resolution of the aerial or satellite imagery has a great impact on the accuracy and reliability of information extracted from the images. In this study, the potential of a DCNN-based SISR model, called enhanced super-resolution generative adversarial network (ESRGAN), to predict the spatial information degraded or lost in a hyper-spatial resolution unmanned aircraft system (UAS) RGB image set is investigated. ESRGAN model is trained over a limited number of original HR (50 out of 450 total images) and virtually-generated LR UAS images by downsampling the original HR images using a bicubic kernel with a factor x 4. Quantitative and qualitative assessments of super-resolved images using standard image quality measures (IQMs) confirm that the DCNN-based SISR approach can be successfully applied on LR UAS imagery for spatial resolution enhancement. The performance of DCNN-based SISR approach for the UAS image set closely approximates performances reported on standard SISR image sets with mean peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index values of around 28 dB and 0.85 dB, respectively. Furthermore, by exploiting the rigorous Structure-from-Motion (SfM) photogrammetry procedure, an accurate task-based IQM for evaluating the quality of the super-resolved images is carried out. Results verify that the interior and exterior imaging geometry, which are extremely important for extracting highly accurate spatial information from UAS imagery in photogrammetric applications, can be accurately retrieved from a super-resolved image set. The number of corresponding keypoints and dense points generated from the SfM photogrammetry process are about 6 and 17 times more than those extracted from the corresponding LR image set, respectively. © 2020 by the authors.","Antennas; Convolutional neural networks; Deep learning; Deep neural networks; Image quality; Image reconstruction; Image resolution; Optical resolving power; Photogrammetry; Remote sensing; Satellite imagery; Signal to noise ratio; Unmanned aerial vehicles (UAV); High resolution image; Peak signal to noise ratio; Quantitative and qualitative assessments; Reliability of information; Spatial-resolution enhancement; Structural similarity indices (SSIM); Structure from motion; Unmanned aircraft system; Image enhancement","Convolutional neural network (CNN); Deep learning; Generative adversarial network (GAN); Photogrammetry; Remote sensing; Structure-from-motion; Super-resolution (SR); Unmanned aircraft system (UAS)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086470516"
"Alipour-Fard T.; Arefi H.","Alipour-Fard, Tayeb (55576495200); Arefi, Hossein (14031194500)","55576495200; 14031194500","Structure aware generative adversarial networks for hyperspectral image classification","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9187967","5398","5412","14","10.1109/JSTARS.2020.3022781","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092402196&doi=10.1109%2fJSTARS.2020.3022781&partnerID=40&md5=96eaf0ecf6ba5969497560feed1d2cbd","Generative adversarial networks (GANs) have shown striking performances in computer vision applications to augment virtual training samples (VTS). However, the VTS generating by GANs in the context of hyperspectral image classification suffer from structural inconsistency due to the insufficient number of training samples in order to learn high-order features from the discriminator. This work addresses the scarcity of training samples by designing a GAN, in which the performance of discriminator is improved to produce more structurally coherent VTS. In the proposed method, by splitting the discriminator into two parts, GAN undertakes two tasks: The main task is to learn to distinguish between real and fake samples, and the auxiliary task is to learn to distinguish structurally corrupted and real samples. With this setup, GAN will produce real-like VTS with a higher variation than conventional GAN. Furthermore, in order to reduce the computational cost, subspace-based dimension reduction was performed to obtain the dominant features around the training samples to generate meaningful patterns from the original ones to be used in the learning phase. Based on the experimental results on real, and well-known hyperspectral benchmark images, the proposed method improves the performance compared with GANs-related, and conventional data augmentation strategies.1  © 2008-2012 IEEE.","Benchmarking; Image enhancement; Sampling; Spectroscopy; Adversarial networks; Computational costs; Computer vision applications; Data augmentation; Dimension reduction; Structure-aware; Training sample; Virtual training; artificial neural network; data processing; image classification; machine learning; multispectral image; remote sensing; Image classification","convolutional neural network (CNN); Deep learning (DL); generative adversarial networks (GANs); hyperspectral images (HSIs); remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092402196"
"Pan X.; Zhao J.; Xu J.","Pan, Xin (35422588500); Zhao, Jian (57188561431); Xu, Jun (57210253499)","35422588500; 57188561431; 57210253499","Conditional Generative Adversarial Network-Based Training Sample Set Improvement Model for the Semantic Segmentation of High-Resolution Remote Sensing Images","2020","IEEE Transactions on Geoscience and Remote Sensing","","","","","","","10.1109/TGRS.2020.3033816","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096843055&doi=10.1109%2fTGRS.2020.3033816&partnerID=40&md5=7857eba9490c6db58a39c77e73670e7e","To achieve high segmentation quality, deep semantic segmentation neural networks (DSSNNs) need to be trained on diverse direction, location, and neighboring category combinations for each pixel in the input-output image patches. Achieving this goal requires a large sample set. However, in many practical application scenarios, a very large training sample set is too expensive to achieve, or insufficient remote sensing image data are available. These limitations directly affect the quality of the results of DSSNNs. To address the above-mentioned problem, this article proposes a conditional generative adversarial network (CGAN)-based training sample set improvement model (CGAN-TSIM) for the semantic segmentation of high-resolution remote sensing images. In CGAN-TSIM, the generator model of the CGAN can generate a sample image when a ground-truth image is an input as a ``condition.'' A condition generation mechanism is designed to create ground-truth images, and these ground-truth conditions are used to drive the CGAN to generate samples containing more diverse object combinations, directions, and locations. These generated images can be added to the original training sample set to improve their spatial information diversity. Rather than simply relying on passively finding samples that contain diverse spatial information, CGAN-TSIM extracts high-level spatial information from the original training images and actively generates new sample images. Experiments show that the samples generated by CGAN-TSIM can improve the quality of the sample set. Compared with other traditional methods, CGAN-TSIM enables better classification accuracy. IEEE","Deep neural networks; Digital storage; Image segmentation; Remote sensing; Sampling; Semantic Web; Semantics; Adversarial networks; Classification accuracy; Generation mechanism; High resolution remote sensing images; Remote sensing images; Segmentation quality; Semantic segmentation; Spatial informations; Image enhancement","Condition generation; Feature extraction; generative adversarial network (GAN); Generative adversarial networks; Generators; high-resolution remote sensing classification; Image segmentation; Remote sensing; sample generation; semantic segmentation.; Semantics; Training","Article","Article in press","","Scopus","2-s2.0-85096843055"
"Lei D.; Zhang C.; Li Z.; Wu Y.","Lei, Dajiang (36627356400); Zhang, Ce (57216935063); Li, Zhixing (56181457200); Wu, Yu (56468297100)","36627356400; 57216935063; 56181457200; 56468297100","Remote Sensing Image Fusion Based on Generative Adversarial Network with Multi-stream Fusion Architecture; [基于多流融合生成对抗网络的遥感图像融合方法]","2020","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","42","8","","1942","1949","7","10.11999/JEIT17_190273","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089700151&doi=10.11999%2fJEIT17_190273&partnerID=40&md5=45ddf43c40faef4e31b10888e507a93d","The generative adversarial network receives extensive attention in the study of computer vision such as image fusion and image super-resolution, due to its strong ability of generating high quality images. At present, the remote sensing image fusion method based on generative adversarial network only learns the mapping between the images, and lacks the unique Pan-sharpening domain knowledge. This paper proposes a remote sensing image fusion method based on optimized generative adversarial network with the integration of the spatial structure information of panchromatic image. The proposed algorithm extracts the spatial structure information of the panchromatic image by the gradient operator. The extracted feature would be added to both the discriminator and the generator which uses a multi-stream fusion architecture. The corresponding optimization objective and fusion rules are then designed to improve the quality of the fused image. Experiments on images acquired by WorldView-3 satellites demonstrate that the proposed method can generate high quality fused images, which is better than the most of advanced remote sensing image fusion methods in both subjective visual and objective evaluation indicators.","Image enhancement; Network architecture; Remote sensing; Adversarial networks; Fusion architecture; High quality images; Image super resolutions; Objective evaluation; Panchromatic images; Remote sensing images; Spatial structure information; Image fusion","Computer vision; Generative adversarial network; Multi-stream fusion architecture; Remote sensing image fusion","Article","Final","","Scopus","2-s2.0-85089700151"
"Cao M.; Ji H.; Gao Z.; Mei T.","Cao, Min (57226775285); Ji, Hong (57205763449); Gao, Zhi (55256514200); Mei, Tincan (8914886000)","57226775285; 57205763449; 55256514200; 8914886000","Vehicle Detection in Remote Sensing Images Using Deep Neural Networks and Multi-Task Learning","2020","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","2","","797","804","7","10.5194/isprs-annals-V-2-2020-797-2020","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091110780&doi=10.5194%2fisprs-annals-V-2-2020-797-2020&partnerID=40&md5=daec2d2a13004100f9a2033c9127ef4c","Vehicle detection in remote sensing image has been attracting remarkable attention over past years for its applications in traffic, security, military, and surveillance fields. Due to the stunning success of deep learning techniques in object detection community, we consider to utilize CNNs for vehicle detection task in remote sensing image. Specifically, we take advantage of deep residual network, multi-scale feature fusion, hard example mining and homography augmentation to realize vehicle detection, which almost integrates all the advanced techniques in deep learning community. Furthermore, we simultaneously address super-resolution (SR) and detection problems of low-resolution (LR) image in an end-to-end manner. In consideration of the absence of paired low-/highresolution data which are generally time-consuming and cumbersome to collect, we leverage generative adversarial network (GAN) for unsupervised SR. Detection loss is back-propagated to SR generator to boost detection performance. We conduct experiments on representative benchmark datasets and demonstrate that our model yields significant improvements over state-of-the-art methods in deep learning and remote sensing areas. © 2020 Copernicus GmbH. All rights reserved.","Deep neural networks; Learning systems; Military applications; Military photography; Military vehicles; Multi-task learning; Neural networks; Object detection; Remote sensing; Adversarial networks; Benchmark datasets; Detection performance; Learning techniques; Low resolution images; Multi-scale features; Remote sensing images; State-of-the-art methods; Deep learning","GAN; hard example mining; homography augmentation; multi-scale feature fusion; Remote sensing images; super-resolution; Vehicle detection","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091110780"
"Ren C.X.; Ziemann A.; Theiler J.; Durieux A.M.S.","Ren, Christopher X. (57004276700); Ziemann, Amanda (36134071500); Theiler, James (7004449154); Durieux, Alice M. S. (57212026338)","57004276700; 36134071500; 7004449154; 57212026338","Deep snow: Synthesizing remote sensing imagery with generative adversarial nets","2020","Proceedings of SPIE - The International Society for Optical Engineering","11392","","113920T","","","","10.1117/12.2560716","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088166415&doi=10.1117%2f12.2560716&partnerID=40&md5=b8f6a2d732ef0fb3bb98c3374d028e10","In this work we demonstrate that generative adversarial networks (GANs) can be used to generate realistic pervasive changes in RGB remote sensing imagery, even in an unpaired training setting. We investigate some transformation quality metrics based on deep embedding of the generated and real images which enable visualization and understanding of the training dynamics of the GAN, and provide a useful measure in terms of quantifying how distinguishable the generated images are from real images. We also identify some artifacts introduced by the GAN in the generated images, which are likely to contribute to the differences seen between the real and generated samples in the deep embedding feature space even in cases where the real and generated samples appear perceptually similar. © 2020 SPIE.","Embeddings; Space optics; Spectroscopy; Adversarial networks; Feature space; Quality metrics; Real images; Remote sensing imagery; Remote sensing","Deep learning; Generative adversarial networks; Machine learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85088166415"
"Ma J.; Wu H.; Zhang J.; Zhang L.","Ma, Jie (57205916758); Wu, Hanlin (57221263814); Zhang, Jue (56513505100); Zhang, Libao (35325855000)","57205916758; 57221263814; 56513505100; 35325855000","SD-FB-GAN: Saliency-Driven Feedback Gan for Remote Sensing Image Super-Resolution Reconstruction","2020","Proceedings - International Conference on Image Processing, ICIP","2020-October","","9190835","528","532","4","10.1109/ICIP40778.2020.9190835","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098652564&doi=10.1109%2fICIP40778.2020.9190835&partnerID=40&md5=d13a6e4f699e22b71ce072e504a572b0","The visual characteristics of different regions in remote sensing images are significantly versatile, which poses a huge challenge to single image super-resolution. Although generative adversarial network (GAN) has shown great potential in generating photo-realistic results, it provides unsatisfactory performance in objective metrics owning to pseudo textures brought by adversarial learning. In this paper, we propose a new saliency-driven feedback GAN to cope with these problems. We design a saliency-driven feedback generator based on paired-feedback blocks (PFBBs) and recurrent structure to provide strong reconstruction ability. In the PFBB, the saliency map serves as an indicator to reflect the texture complexity, so different reconstruction principles can be applied to restore areas with varying levels of saliency. Besides, we propose to measure the visual quality of salient areas, non-salient areas, and the whole image with multi-discriminators, which can dramatically eliminate pseudo textures. Comprehensive evaluations and ablation studies validate the superiority of our proposal. © 2020 IEEE.","Optical resolving power; Recurrent neural networks; Remote sensing; Textures; Adversarial learning; Adversarial networks; Comprehensive evaluation; Objective metrics; Photo-realistic; Remote sensing images; Texture complexity; Visual qualities; Image reconstruction","deep learning; GAN; Image reconstruction; saliency analysis; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85098652564"
"Yi W.; Liu M.; Dong L.; Zhao Y.; Liu X.; Hui M.","Yi, Weichao (57211414456); Liu, Ming (8622501800); Dong, Liquan (10739346400); Zhao, Yuejin (34874166500); Liu, Xiaohua (56122206700); Hui, Mei (7005746183)","57211414456; 8622501800; 10739346400; 34874166500; 56122206700; 7005746183","Restoration of haze-free images using generative adversarial network","2020","Proceedings of SPIE - The International Society for Optical Engineering","11432","","114321J","","","","10.1117/12.2541893","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081125628&doi=10.1117%2f12.2541893&partnerID=40&md5=077832344b7c0eb9d21511ad8e17f003","Haze is the result of the interaction between specific climate and human activities. When observing objects in hazy conditions, optical system will produce degradation problems such as color attenuation, image detail loss and contrast reduction. Image haze removal is a challenging and ill-conditioned problem because of the ambiguities of unknown radiance and medium transmission. In order to get clean images, traditional machine vision methods usually use various constraints/prior conditions to obtain a reasonable haze removal solutions, the key to achieve haze removal is to estimate the medium transmission of the input hazy image in earlier studies. In this paper, however, we concentrated on recovering a clear image from a hazy input directly by using Generative Adversarial Network (GAN) without estimating the transmission matrix and atmospheric scattering model parameters, we present an end-To-end model that consists of an encoder and a decoder, the encoder is extracting the features of the hazy images, and represents these features in high dimensional space, while the decoder is employed to recover the corresponding images from high-level coding features. And based perceptual losses optimization could get high quality of textural information of haze recovery and reproduce more natural haze-removal images. Experimental results on hazy image datasets input shows better subjective visual quality than traditional methods. Furthermore, we test the haze removal images on a specialized object detection network-YOLO, the detection result shows that our method can improve the object detection performance on haze removal images, indicated that we can get clean haze-free images from hazy input through our GAN model. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Decoding; Geographic information systems; Image coding; Image reconstruction; Information systems; Information use; Object detection; Object recognition; Optical data processing; Optical systems; Pattern recognition systems; Recovery; Remote sensing; Restoration; Signal encoding; Space optics; Adversarial networks; Atmospheric scattering models; Detection performance; Haze removal; High dimensional spaces; Ill conditioned problems; Textural information; Transmission matrix; Image enhancement","GAN; Haze-removal; Image restoration; object detection","Conference paper","Final","","Scopus","2-s2.0-85081125628"
"Soto P.J.; P. Costa G.A.O.; Feitosa R.Q.; Happ P.N.; Ortega M.X.; Noa J.; Almeida C.A.; Heipke C.","Soto, P.J. (57216790448); P. Costa, G.A.O. (57219054054); Feitosa, R.Q. (6602453684); Happ, P.N. (55768214000); Ortega, M.X. (57201864105); Noa, J. (57219049845); Almeida, C.A. (57209845177); Heipke, C. (7004264889)","57216790448; 57219054054; 6602453684; 55768214000; 57201864105; 57219049845; 57209845177; 7004264889","Domain adaptation with cyclegan for change detection in the amazon forest","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3","","1635","1643","8","10.5194/isprs-archives-XLIII-B3-2020-1635-2020","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091150880&doi=10.5194%2fisprs-archives-XLIII-B3-2020-1635-2020&partnerID=40&md5=d3bf65c374ec1c3da2c3ff3d3be0fb68","Deep learning classification models require large amounts of labeled training data to perform properly, but the production of reference data for most Earth observation applications is a labor intensive, costly process. In that sense, transfer learning is an option to mitigate the demand for labeled data. In many remote sensing applications, however, the accuracy of a deep learning-based classification model trained with a specific dataset drops significantly when it is tested on a different dataset, even after fine-tuning. In general, this behavior can be credited to the domain shift phenomenon. In remote sensing applications, domain shift can be associated with changes in the environmental conditions during the acquisition of new data, variations of objects' appearances, geographical variability and different sensor properties, among other aspects. In recent years, deep learning-based domain adaptation techniques have been used to alleviate the domain shift problem. Recent improvements in domain adaptation technology rely on techniques based on Generative Adversarial Networks (GANs), such as the Cycle-Consistent Generative Adversarial Network (CycleGAN), which adapts images across different domains by learning nonlinear mapping functions between the domains. In this work, we exploit the CycleGAN approach for domain adaptation in a particular change detection application, namely, deforestation detection in the Amazon forest. Experimental results indicate that the proposed approach is capable of alleviating the effects associated with domain shift in the context of the target application. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Classification (of information); Deforestation; Image enhancement; Learning systems; Remote sensing; Transfer learning; Adversarial networks; Classification models; Environmental conditions; Geographical variability; Labeled training data; Nonlinear mapping functions; Remote sensing applications; Target application; Deep learning","Change Detection; Cycle-Consistent Generative Adversarial Networks; Domain Adaptation; Remote Sensing","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85091150880"
"Zhou L.; Xia Y.; Liu Z.","Zhou, Luo (57443314600); Xia, Yan (57696663900); Liu, Zhen (57443504500)","57443314600; 57696663900; 57443504500","Super-Resolution Reconstruction of Remote Sensing Images Based on GAN","2021","Proceedings of 2021 IEEE International Conference on Data Science and Computer Application, ICDSCA 2021","","","","270","274","4","10.1109/ICDSCA53499.2021.9649727","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124205370&doi=10.1109%2fICDSCA53499.2021.9649727&partnerID=40&md5=eeb613b46ef8e793b5294be68161c987","In recent years, the methods of super-resolution image reconstruction that based on deep learning have become a hot topic in research of computer vision. The methods of super-resolution image reconstruction that based on the Generative Adversarial Network (GAN) are not controlled in network generation, the models are easy to collapse, the generalization ability is undesirable, and the time complexity degree is too high. To fill these gaps, we propose a super-resolution image reconstruction method based on the GAN of encoding and decoding, which improves the quality of image reconstruction. First of all, our approach uses a design network with regularized structure to avoid model collapse. Then we build a generation network structure that based on encoding and decoding to suppress the uncontrollable defects of GAN network generated images. Finally, in the last layer of the generator, NN convolutional feature layer is included to replace the Softmax layer, which speeds up the training of the model. The experimental results show that the super-resolution remote sensing image reconstructed by the proposed method has higher reconstruction quality and better generalization ability in the DOTA training data sets. At the same time, the image reconstruction process can take much less time. © 2021 IEEE.","Decoding; Deep learning; Encoding (symbols); Image coding; Image enhancement; Image reconstruction; Optical resolving power; Remote sensing; Signal encoding; Coding and decoding; Encoding and decoding; Generalization ability; Hot topics; Image-based; Images reconstruction; In networks; Remote sensing images; Super-resolution image reconstruction; Super-resolution reconstruction; Generative adversarial networks","Coding and Decoding; GAN; Remote Sensing Images; Super Resolution Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85124205370"
"Zhang Z.; Lin Y.","Zhang, Zhongping (57207845451); Lin, Youzuo (53463916800)","57207845451; 53463916800","Data-Driven Seismic Waveform Inversion: A Study on the Robustness and Generalization","2020","IEEE Transactions on Geoscience and Remote Sensing","58","10","9044635","6900","6913","13","10.1109/TGRS.2020.2977635","49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092387994&doi=10.1109%2fTGRS.2020.2977635&partnerID=40&md5=e7e6bf6ed3883af63ee0284b7529183d","Full-waveform inversion is an important and widely used method to reconstruct subsurface velocity images. Waveform inversion is a typical nonlinear and ill-posed inverse problem. Existing physics-driven computational methods for solving waveform inversion suffer from the cycle-skipping and local-minima issues, and do not mention that solving waveform inversion is computationally expensive. In recent years, data-driven methods become a promising way to solve the waveform-inversion problem. However, most deep-learning frameworks suffer from the generalization and overfitting issue. In this article, we developed a real-time data-driven technique and we call it VelocityGAN, to reconstruct accurately the subsurface velocities. Our VelocityGAN is built on a generative adversarial network (GAN) and trained end to end to learn a mapping function from the raw seismic waveform data to the velocity image. Different from other encoder-decoder-based data-driven seismic waveform-inversion approaches, our VelocityGAN learns regularization from data and further imposes the regularization to the generator so that inversion accuracy is improved. We further develop a transfer-learning strategy based on VelocityGAN to alleviate the generalization issue. A series of experiments is conducted on the synthetic seismic reflection data to evaluate the effectiveness, efficiency, and generalization of VelocityGAN. We not only compare it with the existing physics-driven approaches and data-driven frameworks but also conduct several transfer-learning experiments. The experimental results show that VelocityGAN achieves the state-of-the-art performance among the baselines and can improve the generalization results to some extent.  © 1980-2012 IEEE.","Deep learning; Inverse problems; Seismic waves; Seismographs; Seismology; Transfer learning; Adversarial networks; Full-waveform inversion; ILL-posed inverse problem; Learning frameworks; Seismic reflection data; Seismic waveform inversion; State-of-the-art performance; Subsurface velocity; inversion layer; remote sensing; satellite data; seismic wave; waveform analysis; Waveform analysis","Condition adversarial networks; data-driven method; full-waveform inversion (FWI); transfer learning","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85092387994"
"Li J.; Chen Z.; Zhao X.; Shao L.","Li, Jingtao (57217013418); Chen, Zhanlong (26322275400); Zhao, Xiaozhen (57217013673); Shao, Lijia (57217014471)","57217013418; 26322275400; 57217013673; 57217014471","MAPGAN: An intelligent generation model for network tile maps","2020","Sensors (Switzerland)","20","11","3119","","","","10.3390/s20113119","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729281&doi=10.3390%2fs20113119&partnerID=40&md5=11c206c75755ee4a1045444f289c8a7b","In recent years, the generative adversarial network (GAN)-based image translation model has achieved great success in image synthesis, image inpainting, image super-resolution, and other tasks. However, the images generated by these models often have problems such as insufficient details and low quality. Especially for the task of map generation, the generated electronic map cannot achieve effects comparable to industrial production in terms of accuracy and aesthetics. This paper proposes a model called Map Generative Adversarial Networks (MapGAN) for generating multitype electronic maps accurately and quickly based on both remote sensing images and render matrices. MapGAN improves the generator architecture of Pix2pixHD and adds a classifier to enhance the model, enabling it to learn the characteristics and style differences of different types of maps. Using the datasets of Google Maps, Baidu maps, and Map World maps, we compare MapGAN with some recent image translation models in the fields of one-to-one map generation and one-to-many domain map generation. The results show that the quality of the electronic maps generated by MapGAN is optimal in terms of both intuitive vision and classic evaluation indicators. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Maps; Remote sensing; Adversarial networks; Evaluation indicators; Image Inpainting; Image super resolutions; Image synthesis; Image translation; Industrial production; Remote sensing images; article; classifier; human; human experiment; remote sensing; vision; Image processing","Deep generation model; Image translation; Map generation; Network tile map","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085729281"
"Yuan X.; Tian J.; Reinartz P.","Yuan, X. (57210963035); Tian, J. (36987117000); Reinartz, P. (56216874200)","57210963035; 36987117000; 56216874200","Generating artificial near infrared spectral band from rgb image using conditional generative adversarial network","2020","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","279","285","6","10.5194/isprs-Annals-V-3-2020-279-2020","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090342564&doi=10.5194%2fisprs-Annals-V-3-2020-279-2020&partnerID=40&md5=8d8086af7fca14d2cb6f01e9f0da0207","Near infrared bands (NIR) provide rich information for many remote sensing applications. In addition to deriving useful indices to delineate water and vegetation, near infrared channels could also be used to facilitate image pre-processing. However, synthesizing bands from RGB spectrum is not an easy task. The inter-correlations between bands are not clearly identified in physical models. Generative adversarial networks (GAN) have been used in many tasks such as generating photorealistic images, monocular depth estimation and Digital Surface Model (DSM) refinement etc. Conditional GAN is different in that it observes some data as a condition. In this paper, we explore a cGAN network structure to generate a NIR spectral band that is conditioned on the input RGB image. We test different discriminators and loss functions, and evaluate results using various metrics. The best simulated NIR channel has a mean absolute error of around 5 percent in Sentinel-2 dataset. In addition, the simulated NIR image can correctly distinguish between various classes of landcover. © Authors 2020. All rights reserved.","Remote sensing; Adversarial networks; Digital surface models; Image preprocessing; Mean absolute error; Near infrared spectral; Near-infrared channels; Photorealistic images; Remote sensing applications; Infrared devices","Conditional GAN; Gerative adversarial networks; Near-infrared; RGB; Robust loss function","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090342564"
"Zhu Q.; Fan X.; Zhong Y.; Guan Q.; Zhang L.; Li D.","Zhu, Qiqi (56420945500); Fan, Xin (57222248289); Zhong, Yanfei (12039673900); Guan, Qingfeng (55838509944); Zhang, Liangpei (8359720900); Li, Deren (57212271839)","56420945500; 57222248289; 12039673900; 55838509944; 8359720900; 57212271839","Super Resolution Generative Adversarial Network Based Image Augmentation for Scene Classification of Remote Sensing Images","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324043","573","576","3","10.1109/IGARSS39084.2020.9324043","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102001018&doi=10.1109%2fIGARSS39084.2020.9324043&partnerID=40&md5=2c6d3f95bad091ad3ea96988131867ed","High spatial resolution remote sensing image (RSI) scene classification, aimed at automatically labelling images with the given semantic categories, has been a hot issue. As it's difficult for RSI to quickly obtain a large number of training samples from a specific area. Traditional scene classification researches were mainly using deep learning models to transfer natural images to RSI. Considering the differences between natural images and RSI, we trained several Super Resolution GAN models by using different resolution RSI data from Google earth image. This paper proposed a novel SRGAN-CNN framework. Through transferring the data with scene classification dataset to obtain high resolution fake RSI. The experimental results demonstrate that the proposed framework can enhance transfer effect and help improve the accuracy of scene classification using low resolution RSI. © 2020 IEEE.","Classification (of information); Deep learning; Geology; Optical resolving power; Remote sensing; Semantics; Adversarial networks; Different resolutions; High spatial resolution; Remote sensing images; Scene classification; Semantic category; Super resolution; Training sample; Image classification","deep learning; Image super resolution; remote sensing images; scene classification","Conference paper","Final","","Scopus","2-s2.0-85102001018"
"Han L.; Fang L.; Zhang W.; Ge Y.","Han, Lei (36630198900); Fang, Liyuan (57221980898); Zhang, Wei (56760576100); Ge, Yurong (35752812300)","36630198900; 57221980898; 56760576100; 35752812300","Radar image prediction using generative adversarial networks","2021","Proceedings of SPIE - The International Society for Optical Engineering","11720","","1172020","","","","10.1117/12.2589379","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100862020&doi=10.1117%2f12.2589379&partnerID=40&md5=63d2fdef9d8d992ad4e65083ed20f43d","Doppler radar is the main remote sensing equipment to monitor severe convective weather which has significant threats to social and economic activities. It is important to accurately predict the time and location of severe weather events. In this study, we use a deep learning technique to predict severe weather events based on radar images. Firstly, we transform the prediction problem into a binary classification problem and use Generative Adversarial Networks (GANs) to construct a classifier. Then Doppler radar images are used to train the model. The critical success index, probability of detection, and false alarm ratio are used to evaluate the prediction results. The experimental results show that the GANs model provides satisfactory results.  © 2021 SPIE.","Deep learning; Doppler radar; Economics; Forecasting; Image processing; Remote sensing; Weather information services; Adversarial networks; Binary classification problems; Convective weather; Economic activities; Learning techniques; Probability of detection; Remote sensing equipment; Severe weather events; Radar imaging","Deep Learning; Generative Adversarial Networks; Radar Image","Conference paper","Final","","Scopus","2-s2.0-85100862020"
"Liu W.; Su F.","Liu, Wei (57192704571); Su, Fulin (7102864861)","57192704571; 7102864861","Unsupervised Adversarial Domain Adaptation Network for Semantic Segmentation","2020","IEEE Geoscience and Remote Sensing Letters","17","11","8932673","1978","1982","4","10.1109/LGRS.2019.2956490","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095752114&doi=10.1109%2fLGRS.2019.2956490&partnerID=40&md5=9a1fb9b308ee4219b75e8a0f15058b20","With the rapid development of deep learning technology, semantic segmentation methods have been widely used in remote sensing data. A pretrained semantic segmentation model usually cannot perform well when the testing images (target domain) have an obvious difference from the training data set (source domain), while a large enough labeled data set is almost impossible to be acquired for each scenario. Unsupervised domain adaptation (DA) techniques aim to transfer knowledge learned from the source domain to a totally unlabeled target domain. By reducing the domain shift, DA methods have shown the ability to improve the classification accuracy for the target domain. Hence, in this letter, we propose an unsupervised adversarial DA network that converts deep features into 2-D feature curves and reduces the discrepancy between curves from the source domain and curves from the target domain based on a conditional generative adversarial networks (cGANs) model. Our proposed DA network is able to improve the semantic labeling accuracy when we apply a pretrained semantic segmentation model to the target domain. To test the effectiveness of the proposed method, experiments are conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D Semantic Labeling data set. Results show that our proposed network is able to stably improve overall accuracy not only when the source and target domains are from the same city but with different building styles but also when the source and target domains are from different cities and acquired by different sensors. By comparing with a few state-of-the-art DA methods, we demonstrate that our proposed method achieves the best cross-domain semantic segmentation performance.  © 2004-2012 IEEE.","Deep learning; Remote sensing; Semantic Web; Semantics; Statistical tests; Well testing; Adversarial networks; Classification accuracy; International society; Learning technology; Overall accuracies; Remote sensing data; Semantic segmentation; Training data sets; adaptive management; algorithm; image analysis; machine learning; numerical method; tagging; two-dimensional modeling; unsupervised classification; Image segmentation","Domain adaptation (DA); generative adversarial networks (GANs); remote sensing image; semantic segmentation; transfer learning","Article","Final","","Scopus","2-s2.0-85095752114"
"Li H.; Gao S.; Liu G.; Guo D.; Grecos C.; Ren P.","Li, Hui (57207879663); Gao, Song (57206840029); Liu, Guiyan (57218844569); Guo, Donglin (57204435638); Grecos, Christos (6602085249); Ren, Peng (25960361900)","57207879663; 57206840029; 57218844569; 57204435638; 6602085249; 25960361900","Visual Prediction of Typhoon Clouds with Hierarchical Generative Adversarial Networks","2020","IEEE Geoscience and Remote Sensing Letters","17","9","8897004","1478","1482","4","10.1109/LGRS.2019.2950687","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090425891&doi=10.1109%2fLGRS.2019.2950687&partnerID=40&md5=8140e9364bd2177b0bf93959f9b0838f","We develop a hierarchical generative adversarial network (HGAN) for generating future typhoon cloud remote sensing images, which enables a visual means to typhoon cloud prediction. The HGAN consists of a global generator and a local discriminator. The global generator aims at producing the future typhoon cloud images as realistic as possible and accordingly reveals the structure and future location of the typhoon clouds. It is constructed in terms of a hierarchical architecture with multiple subnetworks, which capture the overall typhoon variations and favor generating clear future typhoon cloud images. The local discriminator tries its best to distinguish generated typhoon cloud images from ground-truth ones, based on the local patches. The local procedure encourages the discriminator to focus on characterizing the moving typhoon clouds rather than the still background. The global generator and the local discriminator are trained in an adversarial fashion with respect to historical typhoon cloud image sequences. The trained HGAN is capable of producing reliable visual predictions that are not only enabled by the global generator and but also examined by the local discriminator. Experiments validate the effectiveness of the HGAN for typhoon cloud prediction. © 2004-2012 IEEE.","Forecasting; Remote sensing; Adversarial networks; Cloud image; Cloud predictions; Cloud remote sensing; Ground truth; Hierarchical architectures; Subnetworks; cloud; experimental study; future prospect; historical record; model validation; prediction; remote sensing; satellite imagery; typhoon; visual analysis; Hurricanes","Future image generation; hierarchical generative adversarial networks (HGANs); typhoon cloud prediction","Article","Final","","Scopus","2-s2.0-85090425891"
"Liu J.; Xiang P.; Zhang X.","Liu, Jing (56843976400); Xiang, Pengxia (57225186150); Zhang, Xiaoyan (57207319973)","56843976400; 57225186150; 57207319973","An improved generative adversarial network for remote sensing image denoising","2021","Proceedings of SPIE - The International Society for Optical Engineering","11878","","1187811","","","","10.1117/12.2599751","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109421297&doi=10.1117%2f12.2599751&partnerID=40&md5=dc9a485a980a886e8792d49334df6231","Existing methods for remote sensing image denoising typically suffer from a common drawback of fuzzy edge information. In this paper, we proposed a Generative Adversarial Network(GAN) based on the residual learning and perceptual loss for image denoising. The proposed GAN is designed with the two parts: The generator network takes the high-frequency layer of noisy image as the input and outputs a clean image after training. In order to eliminate noise better while retaining more edges and details, three residual blocks are embedded in the generator and a perceptual loss function is added to learn the perceptual differences between the denoised images and the ground truth images. The discriminator network based on 70×70 PatchGAN can discern between the denoised image and the clean image through a confidence value. The experiments show that our proposed network achieves superior performances and preserve majority the edge contours and fine details from low-quality observations. © 2021 SPIE.","Image enhancement; Remote sensing; Adversarial networks; Confidence values; Edge information; High frequency HF; Input and outputs; Loss functions; Perceptual difference; Remote sensing image denoising; Image denoising","Generative Adversarial Network(GAN); Image denoising; Perceptual loss function","Conference paper","Final","","Scopus","2-s2.0-85109421297"
"Tasar O.; Happy S.L.; Tarabalka Y.; Alliez P.","Tasar, Onur (57192690263); Happy, S.L. (55638461500); Tarabalka, Yuliya (24512498300); Alliez, Pierre (57203999691)","57192690263; 55638461500; 24512498300; 57203999691","ColorMapGAN: Unsupervised Domain Adaptation for Semantic Segmentation Using Color Mapping Generative Adversarial Networks","2020","IEEE Transactions on Geoscience and Remote Sensing","58","10","9047180","7178","7193","15","10.1109/TGRS.2020.2980417","62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089388022&doi=10.1109%2fTGRS.2020.2980417&partnerID=40&md5=9659bbb3179e2445ac39cd0857448179","Due to the various reasons, such as atmospheric effects and differences in acquisition, it is often the case that there exists a large difference between the spectral bands of satellite images collected from different geographic locations. The large shift between the spectral distributions of training and test data causes the current state-of-the-art supervised learning approaches to output unsatisfactory maps. We present a novel semantic segmentation framework that is robust to such a shift. The key component of the proposed framework is color mapping generative adversarial networks (ColorMapGANs) that can generate fake training images that are semantically exactly the same as training images, but whose spectral distribution is similar to the distribution of the test images. We then use the fake images and the ground truth for the training images to fine-tune the already trained classifier. Contrary to the existing generative adversarial networks (GANs), the generator in ColorMapGAN does not have any convolutional or pooling layers. It learns to transform the colors of the training data to the colors of the test data by performing only one elementwise matrix multiplication and one matrix-addition operation. Due to the architecturally simple but powerful design of ColorMapGAN, the proposed framework outperforms the existing approaches with a large margin in terms of both accuracy and computational complexity.  © 1980-2012 IEEE.","Color; Matrix algebra; Semantic Web; Semantics; Adversarial networks; Atmospheric effects; Domain adaptation; Geographic location; MAtrix multiplication; Semantic segmentation; Spectral distribution; Supervised learning approaches; adaptive management; artificial neural network; color; mapping method; remote sensing; segmentation; unsupervised classification; Photomapping","Convolutional neural networks (CNNs); dense labeling; domain adaptation; generative adversarial networks (GANs); semantic segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85089388022"
"Li Y.; Huang D.","Li, Yin (57216686461); Huang, Da (55180278600)","57216686461; 55180278600","Generating hyperspectral data based on 3D CNN and improved wasserstein generative adversarial network using homemade high-resolution datasets","2020","ACM International Conference Proceeding Series","","","","49","55","6","10.1145/3411201.3411210","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091448984&doi=10.1145%2f3411201.3411210&partnerID=40&md5=12225974e5149db819b4400701d205a5","Hyperspectral images contain rich information on the fingerprints of materials and are being popularly used in the exploration of oil and gas, environmental monitoring, and remote sensing. Since hyperspectral images cover a wide range of wavelengths with high resolution, they can provide rich features for enhancing the subsequent target detection and classification procedure. The recently proposed deep learning algorithms have been frequently utilized to extract features from hyperspectral images. However, these algorithms are all data-hungry, and need large volume of data to learn an even usable model. To overcome this deficiency, this paper proposes an improved 3D Wasserstein generative adversarial network (3D WGAN-GP) model to generate hyperspectral images with the help of a small amount of training homemade unlabeled hyperspectral images. In particular, WGAN-GP generates hyperspectral images through three-dimensional convolutional neural networks (3D-CNN) and discriminates it against the real ones through a discriminator network. We train the proposed 3D WGAN-GP model by jointly learning both networks with the Adam algorithm. To the best of our knowledge, this is the first work that augments hyperspectral images by using the deep learning algorithm. We evaluate the quality of the generated hyperspectral images by comparing their spectrums to the corresponding real ones. The experimental results confirm the effectiveness of the proposed model.  © 2020 ACM.","Convolutional neural networks; Deep learning; Image enhancement; Petroleum prospecting; Radar target recognition; Remote sensing; Spectroscopy; Three dimensional computer graphics; Wireless sensor networks; Adversarial networks; Environmental Monitoring; High resolution; High-resolution datasets; Hyperspectral Data; Large volumes; Rich features; Target detection and classifications; Learning algorithms","3D CNN; Hyperspectral Data; improved Wasserstein GAN","Conference paper","Final","","Scopus","2-s2.0-85091448984"
"Bittner K.; Liebel L.; Körner M.; Reinartz P.","Bittner, K. (57194603356); Liebel, L. (56938680000); Körner, M. (57190168095); Reinartz, P. (56216874200)","57194603356; 56938680000; 57190168095; 56216874200","LONG-SHORT SKIP CONNECTIONS in DEEP NEURAL NETWORKS for DSM REFINEMENT","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B2","","383","390","7","10.5194/isprs-archives-XLIII-B2-2020-383-2020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091111762&doi=10.5194%2fisprs-archives-XLIII-B2-2020-383-2020&partnerID=40&md5=dd8314a8bdab309bacaae6b0f86109c0","Detailed digital surface models (DSMs) from space-borne sensors are the key to successful solutions for many remote sensing problems, like environmental disaster simulations, change detection in rural and urban areas, 3D urban modeling for city planning and management, etc. Traditional methodologies, e.g., stereo matching, used to generate photogrammetric DSMs from stereo imagery, usually deliver low-quality results due to the matching errors in homogeneous areas or the lack of information when observing the scene under different viewing angles. This makes the tasks related to building reconstruction very challenging since in most cases it is difficult to recognize the type of roofs, especially if overlaid with trees. This work represents a continuation of research regarding the automatic optimization of building geometries in photogrammetric DSMs with half-meter resolution and introduces an improved generative adversarial network (GAN) architecture which allows to reconstruct complete and detailed building structures without neglecting even low-rise urban constructions. The generative part of the network is constructed in a way that it simultaneously processes height and intensity information, and combines short and long skip connections within one architecture. To improve different aspects of the surface, several loss terms are used, the contributions of which are automatically balanced during training. The obtained results demonstrate that the proposed methodology can achieve two goals without any manual intervention: improve the roof surfaces by making them more planar and also recognize and optimize even small residential buildings which are hard to detect. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","3D modeling; Deep neural networks; Network architecture; Neural networks; Photogrammetry; Remote sensing; Roofs; Urban planning; Adversarial networks; Automatic optimization; Building reconstruction; Digital surface models; Environmental disasters; Intensity information; Manual intervention; Residential building; Stereo image processing","3D scene refinement; balancing hyper-parameters; building geometry; Conditional generative adversarial networks (cGANs); long-short skip connections","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85091111762"
"Cheng G.; Xie X.; Han J.; Guo L.; Xia G.-S.","Cheng, Gong (36801169800); Xie, Xingxing (57218439885); Han, Junwei (24450644400); Guo, Lei (56428255600); Xia, Gui-Song (12781686200)","36801169800; 57218439885; 24450644400; 56428255600; 12781686200","Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9127795","3735","3756","21","10.1109/JSTARS.2020.3005403","300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089103661&doi=10.1109%2fJSTARS.2020.3005403&partnerID=40&md5=95354924c8b41c9cdc38b15704668a18","Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this article provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey: first, autoencoder-based remote sensing image scene classification methods; second, convolutional neural network-based remote sensing image scene classification methods; and third, generative adversarial network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly used benchmark datasets. Finally, we discuss the promising opportunities for further research.  © 2008-2012 IEEE.","Benchmarking; Classification (of information); Convolutional neural networks; Deep learning; Deep neural networks; Image classification; Learning systems; Semantics; Surveys; Adversarial networks; Benchmark datasets; Broad application; Feature learning; Learning methods; Remote sensing images; Scene classification; Semantic category; artificial neural network; benchmarking; data set; image classification; remote sensing; satellite imagery; Remote sensing","Deep learning; remote sensing image; scene classification","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089103661"
"Huang X.; Xu D.; Li Z.; Wang C.","Huang, Xiao (57201292422); Xu, Dong (57194030432); Li, Zhenlong (55809947500); Wang, Cuizhen (57195968532)","57201292422; 57194030432; 55809947500; 57195968532","Translating Multispectral Imagery to Nighttime Imagery via Conditional Generative Adversarial Networks","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323669","6758","6761","3","10.1109/IGARSS39084.2020.9323669","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101976290&doi=10.1109%2fIGARSS39084.2020.9323669&partnerID=40&md5=bbb2015f039c5435d5d9d9bc6cac9b01","Nighttime satellite imagery has been applied in a wide range of fields. However, our limited understanding of how observed light intensity is formed and whether it can be simulated greatly hinders its further application. This study explores the potential of conditional Generative Adversarial Networks (cGAN) in translating multispectral imagery to nighttime imagery. A popular cGAN framework, pix2pix, was adopted and modified to facilitate this translation using gridded training image pairs derived from Landsat 8 and Visible Infrared Imaging Radiometer Suite (VIIRS). The results of this study prove the possibility of multispectral-to-nighttime translation and further indicate that, with the additional social media data, the generated nighttime imagery can be very similar to the ground-truth imagery. This study fills the gap in understanding the composition of satellite observed nighttime light and provides new paradigms to solve the emerging problems in nighttime remote sensing fields, including nighttime series construction, light desaturation, and multi-sensor calibration. © 2020 IEEE.","Geology; Satellite imagery; Thermography (imaging); Adversarial networks; Light intensity; Multi-spectral; Multi-spectral imagery; Night-time lights; Social media datum; Training image; Visible infrared imaging radiometer suites; Remote sensing","generative adversarial network; image translation; nighttime imagery","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101976290"
"Maggiolo L.; Solarna D.; Moser G.; Serpico S.B.","Maggiolo, Luca (57207878137); Solarna, David (57192703421); Moser, Gabriele (7101795745); Serpico, Sebastiano B. (7005306316)","57207878137; 57192703421; 7101795745; 7005306316","Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323235","2089","2092","3","10.1109/IGARSS39084.2020.9323235","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101981992&doi=10.1109%2fIGARSS39084.2020.9323235&partnerID=40&md5=3caf6717d140cae9e0879f4fddf8cb3a","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type \ell^{2} similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an \ell^{2} similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration. © 2020 IEEE.","Clustering algorithms; Computational efficiency; Deep learning; Geology; Optical correlation; Remote sensing; Synthetic aperture radar; Accurate registration; Adversarial networks; Automatic registration; Cross correlations; Maximization algorithm; Multisensor remote sensing; Similarity metrics; Textural properties; Radar imaging","COBYLA; conditional generative adversarial network; Multisensor image registration; \ell^{2} similarity","Conference paper","Final","","Scopus","2-s2.0-85101981992"
"Luo Z.; Jiang X.; Liu X.","Luo, Zhongming (57222244917); Jiang, Xue (55724182500); Liu, Xingzhao (35242655900)","57222244917; 55724182500; 35242655900","SYNTHETIC MINORITY CLASS DATA by GENERATIVE ADVERSARIAL NETWORK for IMBALANCED SAR TARGET RECOGNITION","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323439","2459","2462","3","10.1109/IGARSS39084.2020.9323439","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101961369&doi=10.1109%2fIGARSS39084.2020.9323439&partnerID=40&md5=3ecd31fa4b824875fcb08e907c6e494e","The deep convolutional neural networks (CNNs) have achieved the state of art performance in synthetic aperture radar (SAR) automatic target recognition (ATR). However, these networks often provide sub-optimal recognition results in the case of imbalanced SAR data distribution. In this paper, a synthetic minority class data method for improving imbalanced SAR target recognition using the generative adversarial network (GAN) is proposed. The minority class SAR data is first over-sampled by optimized data augmentation policies from automatic search method, which enlarge the training set for GAN. The progressive growing of GANs (PGGAN) is then trained on these data and generates high quality and diverse minority class SAR data to alleviate imbalanced data distribution. Experimental results on the designed imbalanced distributed Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset indicate that our method can effectively improve the recognition accuracy of minority class by approximately 11.68%. © 2020 IEEE.","Automatic target recognition; Convolutional neural networks; Deep neural networks; Geology; Remote sensing; Synthetic aperture radar; Adversarial networks; Automatic searches; Data augmentation; Imbalanced data; Recognition accuracy; State-of-art performance; Stationary targets; Target recognition; Radar target recognition","generative adversarial network; imbalanced target recognition; SAR","Conference paper","Final","","Scopus","2-s2.0-85101961369"
"Zexing D.; Jinyong Y.; Jian Y.","Zexing, Du (57212091275); Jinyong, Yin (57212170523); Jian, Yang (57212173476)","57212091275; 57212170523; 57212173476","Remote Sensing Aircraft Image Detection Based on Semi-Supervised Learning","2020","Laser and Optoelectronics Progress","57","6","061009","","","","10.3788/LOP57.061009","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082700682&doi=10.3788%2fLOP57.061009&partnerID=40&md5=a6c5cdf2deeb787d845a67d6e63de91c","Aiming at the existing remote sensing aircraft image detection methods based on deep learning, which require a large number of tagged data sets and a long training time, we propose a semi-supervised learning method based on generative adversarial networks (GANs). Two granularity deep-learning generative adversarial networks are used to get the edge feature and deep semantic feature information. By combining these two discriminator networks of the GANs, we design the object detection model. The experiment shows that the proposed method has a faster training speed and less labeled dataset is needed during the training process. © 2020 Universitat zu Koln. All rights reserved.","","Generativeadversarialnetwork; Imageprocessing; Remotesensingimage objectdetection; Semigsupervisedlearning","Article","Final","","Scopus","2-s2.0-85082700682"
"Huang J.; Liu S.; Tang Y.; Zhang X.","Huang, Jian (57207290265); Liu, Shanhui (57273381500); Tang, Yutian (56784643500); Zhang, Xiushan (8856829500)","57207290265; 57273381500; 56784643500; 8856829500","Object-Level Remote Sensing Image Augmentation Using U-Net-Based Generative Adversarial Networks","2021","Wireless Communications and Mobile Computing","2021","","1230279","","","","10.1155/2021/1230279","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115753130&doi=10.1155%2f2021%2f1230279&partnerID=40&md5=d4491b55bc54c2c09c62afd204b01360","With the continuous development of deep learning in computer vision, semantic segmentation technology is constantly employed for processing remote sensing images. For instance, it is a key technology to automatically mark important objects such as ships or port land from port area remote sensing images. However, the existing supervised semantic segmentation model based on deep learning requires a large number of training samples. Otherwise, it will not be able to correctly learn the characteristics of the target objects, which results in the poor performance or even failure of semantic segmentation task. Since the target objects such as ships may move from time to time, it is nontrivial to collect enough samples to achieve satisfactory segmentation performance. And this severely hinders the performance improvement of most of existing augmentation methods. To tackle this problem, in this paper, we propose an object-level remote sensing image augmentation approach based on leveraging the U-Net-based generative adversarial networks. Specifically, our proposed approach consists two components including the semantic tag image generator and the U-Net GAN-based translator. To evaluate the effectiveness of the proposed approach, comprehensive experiments are conducted on a public dataset HRSC2016. State-of-the-art generative models, DCGAN, WGAN, and CycleGAN, are selected as baselines. According to the experimental results, our proposed approach significantly outperforms the baselines in terms of not only drawing the outlines of target objects but also capturing their meaningful details. © 2021 Jian Huang et al.","Deep learning; Generative adversarial networks; Image segmentation; Remote sensing; Semantics; Ships; Vehicle performance; Continuous development; Important object; Key technologies; Model-based OPC; Port areas; Remote sensing images; Segmentation models; Semantic segmentation; Target object; Training sample; Semantic Segmentation","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115753130"
"Abady L.; Barni M.; Garzelli A.; Tondi B.","Abady, L. (55917282400); Barni, M. (7005442155); Garzelli, A. (7004594292); Tondi, B. (55389019900)","55917282400; 7005442155; 7004594292; 55389019900","GAN generation of synthetic multispectral satellite images","2020","Proceedings of SPIE - The International Society for Optical Engineering","11533","","115330L","","","","10.1117/12.2575765","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093963449&doi=10.1117%2f12.2575765&partnerID=40&md5=d39a85943c4777bd1ec0c3d653eee216","Generative Adversarial Networks (GAN) have been used for both image generation and image style translation. In this paper, we aim to apply GANs to multispectral satellite image. For the image generation, we take advantage of the progressive GAN training methodology, that is purposely modified to generate multi-band 16 bits satellite images that are similar to a Sentinel-2 level-1C product. The generated images that we obtained imitate closely the spectral signatures of the kind of terrain in the images, as it can be seen by comparing typical spectral view between synthetic and natural images. Furthermore, we consider the recent use of GAN architectures for transferring the style of the images and apply them to perform land-cover transfer of satellite images. Specifically, we used the unpaired style transfer method to modify images that are dominant in vegetation land cover into images that are dominated by bare land cover and vice versa. The land-cover transfer via GANs gives very promising results and the visual quality for the transferred images is also satisfactory, showing that the land-cover transfer is an easier task compared to the GAN generation from scratch. Especially, results are good when the target domain is bare land, in which the visual quality for the transferred images is also very good. © SPIE. Downloading of the abstract is permitted for personal use only.","Remote sensing; Satellites; Adversarial networks; Image generations; Multispectral satellite image; Natural images; Satellite images; Spectral signature; Transfer method; Visual qualities; Image processing","GAN generation; GANs for image remote sensing; NICE-GAN; ProGAN; Sentinel-2; Style-transfer GANs; Synthetic multispectral image generation","Conference paper","Final","","Scopus","2-s2.0-85093963449"
"Zhang L.; Chen D.; Ma J.; Zhang J.","Zhang, Libao (35325855000); Chen, Donghui (57203720403); Ma, Jie (57205916758); Zhang, Jue (56513505100)","35325855000; 57203720403; 57205916758; 56513505100","Remote-Sensing Image Superresolution Based on Visual Saliency Analysis and Unequal Reconstruction Networks","2020","IEEE Transactions on Geoscience and Remote Sensing","58","6","8954940","4099","4115","16","10.1109/TGRS.2019.2960781","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085613184&doi=10.1109%2fTGRS.2019.2960781&partnerID=40&md5=416597218b2442243c162f90f89c836a","Remote-sensing images (RSIs) generally have strong spatial characteristics for surface features. Various ground objects, such as residential areas, roads, forests, and rivers, differ substantially. According to this visual attention characteristic, regions with complicated texture features require more realistic details to reflect a better description of the topography, while regions such as farmlands should be smooth and have less noise. However, most existing single-image superresolution (SISR) methods fail to fully utilize these properties and therefore apply a uniform reconstruction strategy to the whole image. In this article, we propose a novel saliency-driven unequal single-image reconstruction network in which the demands of various regions in the superresolution (SR) process are distinguished by saliency maps. First, we design a new gradient-based saliency analysis method to produce more accurate saliency maps with imagewise annotations. The method utilizes the superiority of a multireception field to extract both high-level features and low-level features. Second, we propose a novel saliency-driven gate conditional generative adversarial network, where the saliency map is regarded as a medium during the training procedure of the whole network. The saliency map is regarded as a pixelwise condition in a generator to enhance the training capability of the network. Additionally, we design a new loss function that combines normalized content loss, saliency-driven perceptual loss, and gate-control adversarial loss to further refine details of texture-complex areas for RSIs. We evaluate the performance of our algorithm and compare it with many other state-of-the-art SR methods using a remote-sensing data set. The experimental results show that our approach achieves the optimal outcome in salient areas. Our method attains the best effect on global quality and visual performance. © 1980-2012 IEEE.","Behavioral research; Image analysis; Image segmentation; Optical resolving power; Remote sensing; Textures; Topography; Adversarial networks; High-level features; Reconstruction networks; Remote sensing data; Remote sensing images; Single-image reconstruction; Spatial characteristics; Training procedures; accuracy assessment; algorithm; data set; image resolution; numerical method; pixel; reconstruction; satellite data; satellite imagery; visual analysis; Image reconstruction","Deep learning; generative adversarial network; remote sensing; saliency; single-image superresolution (SISR); unequal reconstruction","Article","Final","","Scopus","2-s2.0-85085613184"
"Zhao C.; Dong X.; Yan Y.; Su N.; Huang B.","Zhao, Chunhui (7403563984); Dong, Xiaoyu (57212387140); Yan, Yiming (55243504600); Su, Nan (57203308751); Huang, Bowen (57222247635)","7403563984; 57212387140; 55243504600; 57203308751; 57222247635","A Distribution Controllable Simulation Method of Remote Sensing Sea-Ice Images","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323550","3063","3065","2","10.1109/IGARSS39084.2020.9323550","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101991430&doi=10.1109%2fIGARSS39084.2020.9323550&partnerID=40&md5=98e7c83166c301e62d44006cdd50f9a3","In the case of sailing out in the sea-ice areas, it is instructive for route planning to research the distribution characters of ice in the target sea. Existing deep learning methods have shown their strength on sea-ice images processing like image classification. Due to the complex environment around sea-ice area, capturing large quantities of images is not easy. Besides, it's often hard to guarantee the abundance of sea-ice distribution of each different scene class, which causes unsatisfactory classification results. Therefore, it is of considerable practical value to research on sea-ice images simulation. In this paper, a distribution controllable simulation method is proposed based on generative adversarial networks for remote sensing sea-ice images. This research can help settle the problem of small sea-ice samples, as well as can provide a practical method for optical image simulation and similar type problems. © 2020 IEEE.","Deep learning; Geology; Geometrical optics; Ice problems; Image classification; Learning systems; Sea ice; Adversarial networks; Classification results; Complex environments; Distribution character; Images processing; Images simulations; Practical method; Sea ice distribution; Remote sensing","generative adversarial networks; image simulation; remote sensing sea-ice","Conference paper","Final","","Scopus","2-s2.0-85101991430"
"Shao Z.; Lu Z.; Ran M.; Fang L.; Zhou J.; Zhang Y.","Shao, Zhimin (57208469061); Lu, Zexin (57208471749); Ran, Maosong (57208473625); Fang, Leyuan (57218451012); Zhou, Jiliu (21234416400); Zhang, Yi (57203829244)","57208469061; 57208471749; 57208473625; 57218451012; 21234416400; 57203829244","Residual Encoder-Decoder Conditional Generative Adversarial Network for Pansharpening","2020","IEEE Geoscience and Remote Sensing Letters","17","9","8894479","1573","1577","4","10.1109/LGRS.2019.2949745","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090410602&doi=10.1109%2fLGRS.2019.2949745&partnerID=40&md5=c37970f4dfadedb91c53727e545b1df3","Due to the limitation of the satellite sensor, it is difficult to acquire a high-resolution (HR) multispectral (HRMS) image directly. The aim of pansharpening (PNN) is to fuse the spatial in panchromatic (PAN) with the spectral information in multispectral (MS). Recently, deep learning has drawn much attention, and in the field of remote sensing, several pioneering attempts have been made related to PNN. However, the big size of remote sensing data will produce more training samples, which require a deeper neural network. Most current networks are relatively shallow and raise the possibility of detail loss. In this letter, we propose a residual encoder-decoder conditional generative adversarial network (RED-cGAN) for PNN to produce more details with sharpened images. The proposed method combines the idea of an autoencoder with generative adversarial network (GAN), which can effectively preserve the spatial and spectral information of the PAN and MS images simultaneously. First, the residual encoder-decoder module is adopted to extract the multiscale features from the last step to yield pansharpened images and relieve the training difficulty caused by deepening the network layers. Second, to further enhance the performance of the generator to preserve more spatial information, a conditional discriminator network with the input of PAN and MS images is proposed to encourage that the estimated MS images share the same distribution as that of the referenced HRMS images. The experiments conducted on the Worldview2 (WV2) and Worldview3 (WV3) images demonstrate that our proposed method provides better results than several state-of-the-art PNN methods. © 2004-2012 IEEE.","Decoding; Deep learning; Network coding; Network layers; Remote sensing; Adversarial networks; Multi-scale features; Pansharpened images; Remote sensing data; Satellite sensors; Spatial informations; Spectral information; State of the art; artificial neural network; data acquisition; machine learning; performance assessment; remote sensing; satellite imagery; satellite sensor; training; WorldView; Image enhancement","Deep learning; generative adversarial network (GAN); multispectral (MS) image; panchromatic (PAN); pansharpening (PNN)","Article","Final","","Scopus","2-s2.0-85090410602"
"Yang C.; Wang Z.","Yang, Chuan (57199911167); Wang, Zhenghong (57222470058)","57199911167; 57222470058","An ensemble Wasserstein generative adversarial network method for road extraction from high resolution remote sensing images in rural areas","2020","IEEE Access","8","","","174317","174324","7","10.1109/ACCESS.2020.3026084","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099824981&doi=10.1109%2fACCESS.2020.3026084&partnerID=40&md5=8f922bcfece0a4c5063059480ab4f5ec","Road extraction from high resolution remote sensing (HR-RS) images is an important yet challenging computer vision task. In this study, we propose an ensemble Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) method called E-WGAN-GP for road extraction from HR-RS images in rural areas. The WGAN-GP model modifies the standard GANs with Wasserstein distance and gradient penalty. We add a spatial penalty term in the loss function of the WGAN-GP model to solve the class imbalance problem typically in road extraction. Parameter experiments are undertaken to determine the best spatial penalty and the weight term in the new loss function based on GaoFen-2 dataset. In addition, we execute an ensemble strategy in which we first train two WGAN-GP models using the U-Net and BiSeNet as generator respectively, and then intersect their inferred outputs to yield better road vectors. We train our new model with GaoFen-2 HR-RS images in rural areas from China and also the DeepGlobe Road Extraction dataset. Compared with the U-Net, BiSeNet, D-LinkNet and WGAN-GP methods without ensemble, our new method makes a good trade-off between precision and recall with F1-score = 0.85 and IoU = 0.73. © 2021","Economic and social effects; Extraction; Feature extraction; Remote sensing; Roads and streets; Rural areas; Adversarial networks; Class imbalance problems; Ensemble strategies; High resolution remote sensing; High resolution remote sensing images; Precision and recall; Spatial penalty term; Wasserstein distance; Image processing","Deep Learning; Ensemble Learning; High Resolution Remote Sensing; Road Extraction; Wasserstein GAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099824981"
"Huang Y.; Jiang Z.; Wang Q.; Jiang Q.; Pang G.","Huang, Yongsong (57214838902); Jiang, Zetao (24512367900); Wang, Qingzhong (57209217850); Jiang, Qi (57216755792); Pang, Guoming (57271443900)","57214838902; 24512367900; 57209217850; 57216755792; 57271443900","Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13032 LNAI","","","461","472","11","10.1007/978-3-030-89363-7_35","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119336194&doi=10.1007%2f978-3-030-89363-7_35&partnerID=40&md5=76d0ba3806088d01010ba8bc8d086b7a","Image super-resolution is important in many fields, such as surveillance and remote sensing. However, infrared (IR) images normally have low resolution since the optical equipment is relatively expensive. Recently, deep learning methods have dominated image super-resolution and achieved remarkable performance on visible images; however, IR images have received less attention. IR images have fewer patterns, and hence, it is difficult for deep neural networks (DNNs) to learn diverse features from IR images. In this paper, we present a framework that employs heterogeneous convolution and adversarial training, namely, heterogeneous kernel-based super-resolution Wasserstein GAN (HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a lightweight GAN architecture that applies a plug-and-play heterogeneous kernel-based residual block. Moreover, a novel loss function that employs image gradients is adopted, which can be applied to an arbitrary model. The proposed HetSRWGAN achieves consistently better performance in both qualitative and quantitative evaluations. According to the experimental results, the whole training process is more stable. © 2021, Springer Nature Switzerland AG.","Deep neural networks; Generative adversarial networks; Infrared imaging; Optical data processing; Optical resolving power; Remote sensing; Heterogeneous kernel-based convolution; Image super resolutions; Images processing; Learning methods; Lower resolution; Optical equipment; Performance; Remote-sensing; Superresolution; Visible image; Convolution","Generative adversarial networks; Heterogeneous kernel-based convolution; Image processing; Infrared image; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119336194"
"Gao L.; Sun H.-M.; Cui Z.; Du Y.-B.; Sun H.-B.; Jia R.-S.","Gao, Li (57221482568); Sun, Hong-Mei (55729286100); Cui, Zhe (57218325391); Du, Yan-Bin (57221413033); Sun, Hai-Bin (57207025120); Jia, Rui-Sheng (25927894300)","57221482568; 55729286100; 57218325391; 57221413033; 57207025120; 25927894300","Super-resolution reconstruction of single remote sensing images based on residual channel attention","2021","Journal of Applied Remote Sensing","15","1","016513","","","","10.1117/1.JRS.15.016513","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103623109&doi=10.1117%2f1.JRS.15.016513&partnerID=40&md5=4933992be2b6797c799b6325aa5ce724","The existing methods of remote sensing image super-resolution reconstruction based on deep learning have some problems, such as insufficient feature extraction abilities, blurred image edges, and difficult model training. To solve these problems, a super-resolution reconstruction method combining residual channel attention (CA) is proposed. Based on the framework of generative adversarial networks, the residual structure is designed to enhance the ability of deep feature extraction ability for remote sensing images. The CA module is added to extract the deep feature information of remote sensing images, and the shallow features and deep features are fused using the skip connection. The perceptual loss function is combined with the loss function represented by the Wasserstein distance to improve the stability of model training. The experimental results show that this method is superior to the comparison algorithms in the objective evaluation criteria of the peak-signal-To-noise ratio and structural similarity of the reconstructed remote sensing images. After optimizing the model training process, the reconstructed remote sensing images are visually clearer and have sharper edges. © 2021 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Extraction; Feature extraction; Image enhancement; Image reconstruction; Optical resolving power; Signal to noise ratio; Adversarial networks; Feature information; Objective evaluation criteria; Peak signal to noise ratio; Remote sensing images; Structural similarity; Super resolution reconstruction; Wasserstein distance; Remote sensing","generative adversarial network; remote sensing image; residual channel attention; super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85103623109"
"Han X.; He T.; Ong Y.-S.; Zhong Y.","Han, Xiaobing (55267754400); He, Tiantian (56403933800); Ong, Yew-Soon (7006735298); Zhong, Yanfei (12039673900)","55267754400; 56403933800; 7006735298; 12039673900","Precise object detection using adversarially augmented local/global feature fusion","2020","Engineering Applications of Artificial Intelligence","94","","103710","","","","10.1016/j.engappai.2020.103710","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088228053&doi=10.1016%2fj.engappai.2020.103710&partnerID=40&md5=7c98cb2fc7e7c624b2319b95047dca33","Object detection, which aims at recognizing or locating the objects of interest in remote sensing imagery with high spatial resolutions (HSR), plays a significant role in many real-world scenarios, e.g., environment monitoring, urban planning, civil infrastructure construction, disaster rescuing, and geographic image retrieval. As a long-lasting challenging problem in both machine learning and geoinformatics communities, many approaches have been proposed to tackle it. However, previous methods always overlook the abundant information embedded in the HSR remote sensing images. The effectiveness of these methods, e.g., accuracy of detection, is therefore limited to some extent. To overcome the mentioned challenge, in this paper, we propose a novel two-phase deep framework, dubbed GLGOD-Net, to effectively detect meaningful objects in HSR images. GLGOD-Net firstly attempts to learn the enhanced deep representations from super-resolution image data. Fully utilizing the augmented image representations, GLGOD-Net then learns the fused representations into which both local and global latent features are implanted. Such fused representations learned by GLGOD-Net can be used to precisely detect different objects in remote sensing images. The proposed framework has been extensively tested on a real-world HSR image dataset for object detection and has been compared with several strong baselines. The remarkable experimental results validate the effectiveness of GLGOD-Net. The success of GLGOD-Net not only advances the cutting-edge of image data analytics, but also promotes the corresponding applicability of deep learning in remote sensing imagery. © 2020","Data Analytics; Deep learning; Image enhancement; Image retrieval; Object recognition; Remote sensing; Civil infrastructures; Environment monitoring; Geographic image retrieval; High spatial resolution; Real-world scenario; Remote sensing imagery; Remote sensing images; Super resolution; Object detection","Data augmentation; Geospatial object detection; High spatial resolution (HSR) remote sensing imagery; Local/global feature fusion; Super resolution generative adversarial network","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85088228053"
"Tasar O.; Happy S.L.; Tarabalka Y.; Alliez P.","Tasar, Onur (57192690263); Happy, S.L. (55638461500); Tarabalka, Yuliya (24512498300); Alliez, Pierre (57203999691)","57192690263; 55638461500; 24512498300; 57203999691","SEMI2I: Semantically Consistent Image-to-Image Translation for Domain Adaptation of Remote Sensing Data","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323711","1837","1840","3","10.1109/IGARSS39084.2020.9323711","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101965661&doi=10.1109%2fIGARSS39084.2020.9323711&partnerID=40&md5=78f0a703c886a6022cb8389d84ca5d25","Although convolutional neural networks have been proven to be an effective tool to generate high quality maps from remote sensing images, their performance significantly deteriorates when there exists a large domain shift between training and test data. To address this issue, we propose a new data augmentation approach that transfers the style of test data to training data using generative adversarial networks. Our semantic segmentation framework consists in first training a U-net from the real training data and then fine-tuning it on the test stylized fake training data generated by the proposed approach. Our experimental results prove that our framework outperforms the existing domain adaptation methods. © 2020 IEEE.","Convolutional neural networks; Geology; Semantics; Adversarial networks; Data augmentation; Domain adaptation; Existing domains; Image translation; Remote sensing data; Remote sensing images; Semantic segmentation; Remote sensing","data augmentation; dense labeling; Domain adaptation; GANs; generative adversarial networks; image-to-image translation; semantic segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101965661"
"Dong G.; Huang W.; Smith W.A.P.; Ren P.","Dong, Guoshuai (57208247636); Huang, Weimin (55586097900); Smith, William A.P. (8855793100); Ren, Peng (25960361900)","57208247636; 55586097900; 8855793100; 25960361900","A shadow constrained conditional generative adversarial net for SRTM data restoration","2020","Remote Sensing of Environment","237","","111602","","","","10.1016/j.rse.2019.111602","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076547406&doi=10.1016%2fj.rse.2019.111602&partnerID=40&md5=b69dd3953fc0c2b3d71ea6e4f9a7cdd9","The original data produced by the Shuttle Radar Topography Mission (SRTM) tend to have an abundance of voids in mountainous areas where the elevation measurements are missing. In this paper, deep learning models are investigated for restoring SRTM data. To this end, we explore generative adversarial nets, which represent one state-of-the-art family of deep learning models. A conditional generative adversarial network (CGAN) is introduced as the baseline method for filling voids in incomplete SRTM data. The problem regarding shadow violation that possibly arises from the CGAN restored data is investigated. To address this deficiency, shadow geometric constraints based on shadow maps of satellite images are devised. In addition, a shadow constrained conditional generative adversarial network (SCGAN), which incorporates the shadow geometric constraints into the CGAN, is developed. Training the SCGAN model requires both the remote sensing observations (i.e., the original incomplete SRTM data and satellite images) and the ground truth data (i.e., the complete SRTM data, which are manually refined from the incomplete SRTM data with the reference of in-situ measurements). The integration of the multi-source training data enables the SCGAN model to be characterized by comprehensive information including both mountain shape variation and mountain shadow geometry. Experimental results validate the superiority of the SCGAN over the comparison methods, i.e., the interpolation, the convolutional neural network (CNN) and the baseline CGAN, in SRTM data restoration. © 2019 Elsevier Inc.","Deep learning; Geometry; Maps; Neural networks; Remote sensing; Restoration; Topography; Adversarial networks; Comprehensive information; Convolutional neural network; Data restoration; Geometric constraint; In-situ measurement; Multisource data; Shuttle radar topography mission; artificial neural network; data assimilation; elevation; geometry; mountain region; numerical model; remote sensing; satellite data; satellite imagery; Shuttle Radar Topography Mission; Tracking radar","Multi-source data; Shadow constrained conditional generative network; Shadow geometric constraints; SRTM data restoration","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076547406"
"Li X.; Zhang L.; Wang Q.; Ai H.","Li, Xue (49663402500); Zhang, Li (55943300700); Wang, Qingdong (57220167155); Ai, Haibin (36974932200)","49663402500; 55943300700; 57220167155; 36974932200","Multi-temporal remote sensing imagery semantic segmentation color consistency adversarial network; [多时相遥感影像语义分割色彩一致性对抗网络]","2020","Cehui Xuebao/Acta Geodaetica et Cartographica Sinica","49","11","","1473","1484","11","10.11947/j.AGCS.2020.20190439","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097042530&doi=10.11947%2fj.AGCS.2020.20190439&partnerID=40&md5=885d5a731b8d3eec0c337c81d1def3fd","Using deep convolutional neural network (CNN) to intelligently extract buildings from remote sensing images is of great significance for digital city construction, disaster detection and land management. The color difference between multi-temporal remote sensing images will lead to the decrease of generalization ability of building semantic segmentation model. In view of this, this paper proposes the attention-guided color consistency adversarial network (ACGAN). The algorithm takes the reference color style images and the images to be corrected in the same area and different phases as the training set and adopts the consistency adversarial network with the U-shaped attention mechanism to train the color consistency model. In the prediction stage, this model converts the hue of the images to that of the reference color style image, which is based on the reasoning ability of the deep learning model, instead of the corresponding reference color style image. This model transforms the hue of the images to be corrected into that of the reference color style images. This stage is based on the reasoning ability of the deep learning model, and the corresponding reference color style image is no longer needed. In order to verify the effectiveness of the algorithm, firstly, we compare the algorithm of this paper with the traditional image processing algorithm and other consistency adversarial network. The results show that the images after ACGAN color consistency processing are more similar to that of the reference color style images. Secondly, we carried out the building semantic segmentation experiment on the images processed by the above different color consistency algorithms, which proved that the method in this paper is more conducive to the impro-vement of the generalization ability of multi-temporal remote sensing image semantic segmentation model. © 2020, Surveying and Mapping Press. All right reserved.","Color; Colorimetry; Convolutional neural networks; Deep learning; Deep neural networks; Image segmentation; Learning systems; Remote sensing; Semantic Web; Semantics; Adversarial networks; Attention mechanisms; Generalization ability; Image processing algorithm; Multi-temporal remote sensing; Reasoning ability; Remote sensing images; Semantic segmentation; algorithm; artificial neural network; color; experimental study; image processing; remote sensing; satellite imagery; segmentation; training; Color image processing","Attention mechanism; Color consistency; Generative adversarial networks; Multi-temporal remote sensing imagery; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85097042530"
"Wang C.; Ruifei Z.; Bai Y.; Zhang P.; Fan H.","Wang, Chao (57222581465); Ruifei, Zhu (56646483700); Bai, Yang (57695846100); Zhang, Peng (57750475600); Fan, Haiyang (57297816400)","57222581465; 56646483700; 57695846100; 57750475600; 57297816400","Single-frame super-resolution for high resolution optical remote-sensing data products","2021","International Journal of Remote Sensing","42","21","","8099","8123","24","10.1080/01431161.2021.1971790","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117227562&doi=10.1080%2f01431161.2021.1971790&partnerID=40&md5=57a39bea9cd92f2d85fce3864c3bf395","The resolution of remote-sensing images is directly related to the application value of data products. Due to differences in imaging characteristics between digital cameras and remote-sensing cameras, the existing network models cannot get the best super resolution (SR) results of satellite images. In response to the requirements of remote-sensing image production, we propose a single image super-resolution (SISR) reconstruction method for specific type of remote-sensing satellite. First, we measure and model the imaging degradation phenomenon of remote-sensing satellite, including the image blur and noise model. Then, high-quality aerial images are down-sampled and degraded to construct paired training image datasets. We chose the most popular Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) as the basic structure and optimized the number of Residual-in-Residual Dense Block (RRDB) modules to further improve the processing efficiency. Finally, we perform a series of quantitative measurements of the SR image results, including image interpretation capability, reconstruction accuracy, ground resolution distance, and data processing efficiency, using higher resolution remote-sensing images as the benchmark. The experimental results demonstrate that our proposed method has higher interpretation capability and reconstruction accuracy for the SR processing of specific type remote-sensing satellite. Our proposed method is evaluated within a real satellite image product, that demonstrated it has the capability of pipeline super-resolution processing of remote sensing data products. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Antennas; Data handling; Image reconstruction; Optical resolving power; Pipeline processing systems; Remote sensing; Satellites; Data products; Imaging characteristics; Optical remote sensing data; Reconstruction accuracy; Remote sensing cameras; Remote sensing images; Remote sensing satellites; Satellite images; Single frame super resolutions; Superresolution; digital image; image resolution; quantitative analysis; remote sensing; satellite data; satellite imagery; Efficiency","","Article","Final","","Scopus","2-s2.0-85117227562"
"Hou B.; Liu Q.; Wang H.; Wang Y.","Hou, Bin (57188724169); Liu, Qingjie (55534263100); Wang, Heng (57196430737); Wang, Yunhong (34870959400)","57188724169; 55534263100; 57196430737; 34870959400","From W-Net to CDGAN: Bitemporal Change Detection via Deep Learning Techniques","2020","IEEE Transactions on Geoscience and Remote Sensing","58","3","8891676","1790","1802","12","10.1109/TGRS.2019.2948659","66","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080912869&doi=10.1109%2fTGRS.2019.2948659&partnerID=40&md5=f23aab3ee693bfa13a3996c94236710a","Traditional change detection methods usually follow the image differencing, change feature extraction, and classification framework, and their performance is limited by such simple image domain differencing and also the hand-crafted features. Recently, the success of deep convolutional neural networks (CNNs) has widely spread across the whole field of computer vision for their powerful representation abilities. Therefore, in this article, we address the remote sensing image change detection problem with deep learning techniques. We first propose an end-to-end dual-branch architecture, termed the W-Net, with each branch taking as input one of the two bitemporal images as in the traditional change detection models. In this way, CNN features with more powerful representative abilities can be obtained to boost the final detection performance. In addition, W-Net performs differencing in the feature domain rather than in the traditional image domain, which greatly alleviates loss of useful information for determining the changes. Furthermore, by reformulating change detection as an image translation problem, we apply the recently popular generative adversarial network (GAN) in which our W-Net serves as the generator, leading to a new GAN architecture for change detection which we call CDGAN. To train our networks and also facilitate future research, we construct a large scale data set by collecting images from Google Earth and provide carefully manually annotated ground truths. Experiments show that our proposed methods can provide fine-grained change detection results superior to the existing state-of-the-art baselines. © 1980-2012 IEEE.","Convolution; Convolutional neural networks; Deep neural networks; Feature extraction; Learning algorithms; Learning systems; Network architecture; Remote sensing; Adversarial networks; Change detection; Classification framework; Detection performance; Fine-grained changes; Large scale data sets; Learning techniques; Remote sensing images; baseline conditions; computer vision; data set; learning; remote sensing; satellite imagery; software; World Wide Web; Deep learning","Change detection; change detection generative adversarial network (CDGAN); convolutional neural network (CNN); remote sensing; W-Net","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85080912869"
"Zhang J.; Liu J.; Shi L.; Pan B.; Xu X.","Zhang, Jun (57196384015); Liu, Jiao (57258908500); Shi, Lukui (9737579900); Pan, Bin (56421342100); Xu, Xia (57192694193)","57196384015; 57258908500; 9737579900; 56421342100; 57192694193","An Open Set Domain Adaptation Network Based on Adversarial Learning for Remote Sensing Image Scene Classification","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323944","1365","1368","3","10.1109/IGARSS39084.2020.9323944","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101979950&doi=10.1109%2fIGARSS39084.2020.9323944&partnerID=40&md5=abf84fbfa9f51c0a3f592fcc05986164","Remote sensing image scene classification refers to assigning specific semantic labels for remote sensing images. Due to the lack of labeled remote sensing images, domain adaptation is applied to remote sensing image scene classification. However, recent proposed methods mainly focus on the closed set scenario. In this paper, we explore the open set scenario and introduce an open set domain adaptation network (OSDANet) for remote sensing image scene classification. Inspired by the idea of Generative Adversarial Network (GAN), we design a feature generator as well as a classifier which are learnt in an adversarial way. The purpose of the classifier is to find a boundary between the source and the target samples, while the feature generator attempts to force target samples away from the boundary. Especially, for the target samples, the feature generator will determine whether to align them with source samples or reject them as unknown target samples. The experimental results have indicated the effectiveness of the proposed method. © 2020 IEEE.","Geology; Image classification; Semantics; Adversarial learning; Adversarial networks; Closed set; Domain adaptation; Network-based; Remote sensing images; Specific semantics; Remote sensing","generative adversarial network; open set domain adaptation; Remote sensing scene classification","Conference paper","Final","","Scopus","2-s2.0-85101979950"
"Yu W.; Bai J.; Jiao L.","Yu, Wentao (57218160020); Bai, Jing (57193646200); Jiao, Licheng (7102491544)","57218160020; 57193646200; 7102491544","Background Subtraction Based on GAN and Domain Adaptation for VHR Optical Remote Sensing Videos","2020","IEEE Access","8","","9123388","119144","119157","13","10.1109/ACCESS.2020.3004495","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088154736&doi=10.1109%2fACCESS.2020.3004495&partnerID=40&md5=b1a009ad72156e9f0075da37ebfcb8ba","The application of deep learning techniques in background subtraction for VHR optical remote sensing videos holds the potential to facilitate multiple intelligent remote sensing processing tasks. However, existing methods on background subtraction for VHR optical remote sensing videos are still facing technical challenges. First, conventional CNN and other networks are limited by performance constraints. Second, existing background subtraction methods are mostly trained by natural videos due to the lack of VHR optical remote sensing video datasets. Third, VHR optical remote sensing videos have large scene sizes. In our article, we design a novel deep learning network via fully utilizing GAN and domain adaptation, which has the ability to measure and minimize the discrepancy between feature distributions of natural videos and VHR optical remote sensing videos so that the background subtraction performance for VHR optical remote sensing videos is improved significantly. Numerous experiments are conducted on the CDnet 2014 dataset and VHR optical remote sensing video dataset. Tremendous experiments demonstrate that our proposed method achieves an average FM of 0.8533, which reveals excellent performance on background subtraction. © 2013 IEEE.","Deep learning; Learning systems; Background subtraction; Background subtraction method; Domain adaptation; Feature distribution; Learning techniques; Optical remote sensing; Performance constraints; Technical challenges; Remote sensing","Background subtraction; domain adaptation; generative adversarial networks (GAN); very high resolution (VHR) optical remote sensing videos","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85088154736"
"Li X.; Luo M.; Ji S.; Zhang L.; Lu M.","Li, Xue (49663402500); Luo, Muying (57203092560); Ji, Shunping (9633134900); Zhang, Li (55943300700); Lu, Meng (57188670335)","49663402500; 57203092560; 9633134900; 55943300700; 57188670335","Evaluating generative adversarial networks based image-level domain transfer for multi-source remote sensing image segmentation and object detection","2020","International Journal of Remote Sensing","41","19","","7327","7351","24","10.1080/01431161.2020.1757782","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087591853&doi=10.1080%2f01431161.2020.1757782&partnerID=40&md5=2461a2f3dc82357e7d12e3beb9058925","Appearances and qualities of remote sensing images are affected by different atmospheric conditions, quality of sensors, and radiometric calibrations. This heavily challenges the generalization ability of a deep learning or other machine learning model: the performance of a model pretrained on a source remote sensing data set can significantly decrease when applied to a different target data set. The popular generative adversarial networks (GANs) can realize style or appearance transfer between a source and target data sets, which may boost the performance of a deep learning model through generating new target images similar to source samples. In this study, we comprehensively evaluate the performance of GAN-based image-level transfer methods on convolutional neural network (CNN) based image processing models that are trained on one dataset and tested on another one. Firstly, we designed the framework for the evaluation process. The framework consists of two main parts, the GAN-based image-level domain adaptation, which transfers a target image to a new image with similar probability distribution of source image space, and the CNN-based image processing tasks, which are used to test the effects of GAN-based domain adaptation. Second, the domain adaptation is implemented with two mainstream GAN methods for style transfer, the CycleGAN and the AgGAN. The image processing contains two major tasks, segmentation and object detection. The former and the latter are designed based on the widely applied U-Net and Faster R-CNN, respectively. Finally, three experiments, associated with three datasets, are designed to cover different application cases, a change detection case where temporal data is collected from the same scene, a two-city case where images are collected from different regions and a two-sensor case where images are obtained from aerial and satellite platforms respectively. Results revealed that, the GAN-based image transfer can significantly boost the performance of the segmentation model in the change detection case, however, it did not surpass conventional methods; in the other two cases, the GAN-based methods obtained worse results. In object detection, almost all the methods failed to boost the performance of the Faster R-CNN and the GAN-based methods performed the worst. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.","Antennas; Convolutional neural networks; Deep learning; Learning systems; Object detection; Object recognition; Probability distributions; Remote sensing; Space optics; Adversarial networks; Atmospheric conditions; Conventional methods; Generalization ability; Machine learning models; Radiometric calibrations; Remote sensing images; Satellite platforms; artificial neural network; data set; image processing; remote sensing; segmentation; Image segmentation","","Article","Final","","Scopus","2-s2.0-85087591853"
"Gandikota R.; Radha Krishna K.; Sharma A.; ManjuSarma M.; Bothale V.M.","Gandikota, Rohit (57212350460); Radha Krishna, K. (57222241774); Sharma, Anupama (55605769875); ManjuSarma, M. (57222244883); Bothale, Vinod M (6505542077)","57212350460; 57222241774; 55605769875; 57222244883; 6505542077","RTC-GAN: REAL-TIME CLASSIFICATION of SATELLITE IMAGERY USING DEEP GENERATIVE ADVERSARIAL NETWORKS with INFUSED SPECTRAL INFORMATION","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323363","6993","6996","3","10.1109/IGARSS39084.2020.9323363","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101979819&doi=10.1109%2fIGARSS39084.2020.9323363&partnerID=40&md5=6e47f541ca713a2956765bbf3703c248","This paper implements a deep learning-based Convolutional Neural Network (CNN) with adversarial training and infused pixel information to classify multi-spectral data into 4 LULC classes and cloud. The network is capable of classifying the image on a real-time basis at acquisition time in pixel level by considering the various spectral band values at the pixel and a spatial region around the pixel to collect the spatial features. This way, both spatial information, and spectral information are considered to classify the image. This novel GAN architecture named RTC-GAN is generalized over all the satellites that have their sensors in and around standard NIR, R and G spectral bands while being able to classify the images in realtime. This network is realized and tested on data obtained from satellites Landsat 8 Sentinel2 and Indian Remote Sensing (IRS) satellites like Cartosat-2S, Resourcesat-2/2A. The dataset is not open-sourced and hence very minimal information is provided regarding the IRS data. © 2020 IEEE.","Convolutional neural networks; Deep learning; Geology; Image classification; Pixels; Remote sensing; Satellite imagery; Acquisition time; Adversarial networks; Minimal information; Multi-spectral data; Pixel information; Spatial features; Spatial informations; Spectral information; Classification (of information)","Cartosat; Generative Adversarial Networks; Landsat8; LULC; Real-Time Analysis; Satellite Imagery","Conference paper","Final","","Scopus","2-s2.0-85101979819"
"Jin X.; Huang S.; Jiang Q.; Lee S.-J.; Wu L.; Yao S.","Jin, Xin (56991832300); Huang, Shanshan (57214939600); Jiang, Qian (57194699462); Lee, Shin-Jye (34877262700); Wu, Liwen (57200984308); Yao, Shaowen (24473851600)","56991832300; 57214939600; 57194699462; 34877262700; 57200984308; 24473851600","Semisupervised Remote Sensing Image Fusion Using Multiscale Conditional Generative Adversarial Network with Siamese Structure","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9461404","7066","7084","18","10.1109/JSTARS.2021.3090958","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111651246&doi=10.1109%2fJSTARS.2021.3090958&partnerID=40&md5=3031ab016fdaacc95522cebf92003b10","Remote sensing image fusion (RSIF) can generate an integrated image with high spatial and spectral resolution. The fused remote sensing image is conducive to applications including disaster monitoring, ecological environment investigation, and dynamic monitoring. However, most existing deep learning based RSIF methods require ground truths (or reference images) to train a model, and the acquisition of ground truths is a difficult problem. To address this, we propose a semisupervised RSIF method based on the multiscale conditional generative adversarial networks by combining the multiskip connection and pseudo-Siamese structure. This new method can simultaneously extract the features of panchromatic and multispectral images to fuse them without a ground truth; the adopted multiskip connection contributes to presenting image details. In addition, we propose a composite loss function, which combines the least squares loss, L1 loss, and peak signal-to-noise ratio loss to train the model; the composite loss function can help to retain the spatial details and spectral information of the source images. Moreover, we verify the proposed method by extensive experiments, and the results show that the new method can achieve outstanding performance without relying on the ground truth. © 2008-2012 IEEE.","Deep learning; Remote sensing; Signal to noise ratio; Adversarial networks; Disaster monitoring; Dynamic monitoring; Ecological environments; Multispectral images; Peak signal to noise ratio; Remote sensing images; Spectral information; computer simulation; data acquisition; experimental study; numerical model; performance assessment; remote sensing; spatial resolution; spectral resolution; supervised classification; Image fusion","Conditional generative adversarial network (cGAN); deep learning (DL); image fusion; loss function; remote sensing image fusion (RSIF)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111651246"
"Gashnikov M.V.; Kuznetsov A.V.","Gashnikov, M.V. (12644970700); Kuznetsov, A.V. (57220713929)","12644970700; 57220713929","Use of Generative Adversarial Networks to Altering Remote Sensing Data","2020","Optical Memory and Neural Networks (Information Optics)","29","3","","220","227","7","10.3103/S1060992X20030108","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094839918&doi=10.3103%2fS1060992X20030108&partnerID=40&md5=8917f8aeff868a5add3c84538ca33d06","Abstract—: The paper investigates the use of generative adversarial networks (GAN) for intentional modification of Earth remote sensing data. A generative neural network that includes a special subnet for object boundary inpainting is considered. The network comprises two GAN: the first completes the object boundary, and the second repaints blank areas. Actual remote sensing data are used to test the generative network under consideration. The exemplar-based Patch-Match algorithm is taken as a reference for comparison purposes. The experimental results allow the conclusion that the approach is an effective tool for the intentional modification of large terrestrial area images in falsification of Earth remote sensing data. © 2020, Allerton Press, Inc.","Computer science; Electrical engineering; Adversarial networks; Earth remote sensing; Effective tool; Exemplar-based; Inpainting; Object boundaries; Remote sensing data; Remote sensing","falsification; image modification; neural networks; object boundaries; remote sensing data","Article","Final","","Scopus","2-s2.0-85094839918"
"Liang H.; Bao W.; Shen X.; Zhang X.","Liang, Hongbo (57221468243); Bao, Wenxing (35770357700); Shen, Xiangfei (57207728133); Zhang, Xiaowu (55491192000)","57221468243; 35770357700; 57207728133; 55491192000","Spectral-Spatial Attention Feature Extraction for Hyperspectral Image Classification Based on Generative Adversarial Network","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","10017","10032","15","10.1109/JSTARS.2021.3115971","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117340963&doi=10.1109%2fJSTARS.2021.3115971&partnerID=40&md5=7191a3565c3ec87a6c47285995225f3a","Recent research shows that generative adversarial network (GAN) based deep learning derived frameworks can improve the accuracy of hyperspectral image (HSI) classification on limited labeled samples. However, several studies point out that existing GAN-based methods are heavily affected by the complexity and inefficient description issues of HSIs. The discriminator in GAN always attempts to interpret high-dimensional nonlinear spectral knowledge of HSIs, thus resulting in the Hughes phenomenon. Another critical issue is sample generation. The generator is only used as a regularizer for the discriminator, which seriously restricts the performance for classification. In this article, we propose SSAT-GAN, a semisupervised spectral-spatial attention feature extraction approach based on the GAN that feeds raw data into a deep learning framework, in an end-to-end fashion. First, the unlabeled data is added into the discriminator to alleviate the problems of training samples and supplies a reconstructed real HSI data distribution through adversarial training. Second, to enhance the description of HSIs, we build spectral-spatial attention modules (SSAT) and extend them to the discriminator and the generator to extract discriminative characteristics from abundant spatial contexts and spectral signatures. The SSAT modules learn a three-dimensional filter bank with spectral-spatial attention weights to obtain meaningful feature maps to improve the discrimination of the feature representation. In terms of the mode collapse of GANs, the mean minimization loss is employed for unsupervised learning. Experimental results from three real datasets indicate that SSAT-GAN has certain advantages over the state-of-the-art methods.  © 2008-2012 IEEE.","Classification (of information); Deep learning; Extraction; Feature extraction; Image classification; Image enhancement; Spectroscopy; Attention module; Features extraction; Generative adversarial network; Hyperspectral image classification; Network-based; Semi-supervised; Semisupervised deep learning; Spatial attention; Spatial informations; Spectral-spatial information; artificial neural network; image analysis; image classification; machine learning; remote sensing; spectral analysis; Generative adversarial networks","Attention module; generative adversarial network (GAN); hyperspectral image (HSI) classification; semisupervised deep learning; spectral-spatial information","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117340963"
"Bejiga M.B.; Hoxha G.; Melgani F.","Bejiga, Mesay Belete (57192697078); Hoxha, Genc (57213198314); Melgani, Farid (35613488300)","57192697078; 57213198314; 35613488300","Retro-Remote Sensing with Doc2Vec Encoding","2020","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium, M2GARSS 2020 - Proceedings","","","9105139","89","92","3","10.1109/M2GARSS47143.2020.9105139","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086743107&doi=10.1109%2fM2GARSS47143.2020.9105139&partnerID=40&md5=922348b5f0dd3c64fbbfe33476e00728","In this work, we attempt to address the issue of developing a sophisticated text encoder for retro-remote sensing application. The encoder converts ancient landscape descriptions into a fixed-size vector that, adequately, represents the available information. This vector is then used as a conditioning data to a Generative adversarial network (GAN) that synthesizes the equivalent image. We propose using a pre-trained Doc2Vec encoder for text encoding and train a Wasserstein GAN (a variant of GAN) to convert landscape descriptions written by travelers and geographers into the equivalent image. Qualitative and quantitative analysis of the generated images signify usefulness of the proposed method. © 2020 IEEE.","Encoding (symbols); Geology; Signal encoding; Adversarial networks; Fixed size; Qualitative and quantitative analysis; Remote sensing applications; Text encoding; Remote sensing","Deep learning; Generative adversarial networks; Retro-remote sensing; Text embedding; Text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85086743107"
"Wang X.; Tan K.; Du Q.; Chen Y.; Du P.","Wang, Xue (57191338882); Tan, Kun (36111941200); Du, Qian (7202060063); Chen, Yu (56667358500); Du, Peijun (7103064199)","57191338882; 36111941200; 7202060063; 56667358500; 7103064199","CVA2E: A Conditional Variational Autoencoder with an Adversarial Training Process for Hyperspectral Imagery Classification","2020","IEEE Transactions on Geoscience and Remote Sensing","58","8","8989966","5676","5692","16","10.1109/TGRS.2020.2968304","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089213574&doi=10.1109%2fTGRS.2020.2968304&partnerID=40&md5=dfdb2432597c91c925073f533db14a43","Deep generative models such as the generative adversarial network (GAN) and the variational autoencoder (VAE) have obtained increasing attention in a wide variety of applications. Nevertheless, the existing methods cannot fully consider the inherent features of the spectral information, which leads to the applications being of low practical performance. In this article, in order to better handle this problem, a novel generative model named the conditional variational autoencoder with an adversarial training process (CVA2E) is proposed for hyperspectral imagery classification by combining variational inference and an adversarial training process in the spectral sample generation. Moreover, two penalty terms are added to promote the diversity and optimize the spectral shape features of the generated samples. The performance on three different real hyperspectral data sets confirms the superiority of the proposed method. © 1980-2012 IEEE.","Learning systems; Remote sensing; Spectroscopy; Adversarial networks; Generative model; Hyperspectral Data; Hyperspectral imagery classifications; Sample generations; Spectral information; Training process; Variational inference; image classification; multispectral image; performance assessment; satellite altimetry; satellite imagery; spectral analysis; Image classification","Generative adversarial network (GAN); hyperspectral image (HSI) classification; variational autoencoder (VAE)","Article","Final","","Scopus","2-s2.0-85089213574"
"Zhou C.; Zhang J.; Liu J.; Zhang C.; Fei R.; Xu S.","Zhou, Changsheng (56177673400); Zhang, Jiangshe (9737712100); Liu, Junmin (42761838200); Zhang, Chunxia (55703936800); Fei, Rongrong (57207741581); Xu, Shuang (56367405500)","56177673400; 9737712100; 42761838200; 55703936800; 57207741581; 56367405500","PercepPan: Towards unsupervised pan-sharpening based on perceptual loss","2020","Remote Sensing","12","14","2318","","","","10.3390/rs12142318","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088648006&doi=10.3390%2frs12142318&partnerID=40&md5=9f333c06834e7b96f2b341be0265e059","In the literature of pan-sharpening based on neural networks, high resolution multispectral images as ground-truth labels generally are unavailable. To tackle the issue, a common method is to degrade original images into a lower resolution space for supervised training under the Wald's protocol. In this paper, we propose an unsupervised pan-sharpening framework, referred to as ""perceptual pan-sharpening"". This novel method is based on auto-encoder and perceptual loss, and it does not need the degradation step for training. For performance boosting, we also suggest a novel training paradigm, called ""first supervised pre-training and then unsupervised fine-tuning"", to train the unsupervised framework. Experiments on the QuickBird dataset show that the framework with different generator architectures could get comparable results with the traditional supervised counterpart, and the novel training paradigm performs better than random initialization. When generalizing to the IKONOS dataset, the unsupervised framework could still get competitive results over the supervised ones. © 2020 by the authors.","Earth sciences; Auto encoders; High resolution; Lower resolution; Multispectral images; Original images; Pan-sharpening; Pre-training; Supervised trainings; Remote sensing","Auto-encoder; Generative adversarial networks; Pan-sharpening; Perceptual loss; Unsupervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85088648006"
"Sui B.; Jiang T.; Zhang Z.; Pan X.","Sui, Baikai (57215827051); Jiang, Tao (55737611000); Zhang, Zhen (57211499690); Pan, Xinliang (57215835993)","57215827051; 55737611000; 57211499690; 57215835993","ECGAN: An Improved Conditional Generative Adversarial Network with Edge Detection to Augment Limited Training Data for the Classification of Remote Sensing Images with High Spatial Resolution","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9240038","1311","1325","14","10.1109/JSTARS.2020.3033529","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099265832&doi=10.1109%2fJSTARS.2020.3033529&partnerID=40&md5=0f51d1f7e124dc1896da1782bc55f0da","The classification of remote sensing images with high spatial resolution requires considerable training samples, but the process of sample making is slow and laborious. How to guarantee the accuracy of supervised classification under the condition of limited samples is an urgent problem to be solved in the field of supervised classification. For addressing this problem, we propose an improved conditional generative adversarial network with edge feature (ECGAN) to augment limited training data for the classification of remote sensing images with high spatial resolution in this article. On the basis of conditional generative adversarial network, feature factors of interclass boundaries and intraclass edges are added to networks, and an objective function with multiscale and multilevel features is constructed. The ISPRS potsdam and Vaihingen remote sensing datasets are regarded as examples. Results indicate that the high-resolution remote sensing images generated by using the network proposed in this article have abundant texture, accurate edges, and are highly similar to real images. The generated images are used to augment training samples, and an experiment for classifying high-resolution remote sensing images is conducted. The classification results of the proposed augmentation method perform better than that of the traditional sample augmentation method. We prove that ECGAN as a means of sample augmentation can effectively solves the problem that the classification effect is unideal when the supervised classification sample is insufficient.  © 2008-2012 IEEE.","Edge detection; Image classification; Image enhancement; Image resolution; Remote sensing; Sampling; Supervised learning; Textures; Adversarial networks; Augmentation methods; Classification of remote sensing image; Classification results; High resolution remote sensing images; High spatial resolution; Limited training data; Supervised classification; data set; detection method; image classification; remote sensing; spatial resolution; training; Classification (of information)","Conditional generative adversarial network (CGAN); edge feature extraction; high spatial resolution remote sensing image; image generation; sample augment","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099265832"
"Sedona R.; Paris C.; Cavallaro G.; Bruzzone L.; Riedel M.","Sedona, Rocco (57213598861); Paris, Claudia (56042202900); Cavallaro, Gabriele (55636444100); Bruzzone, Lorenzo (7006892410); Riedel, Morris (15770244400)","57213598861; 56042202900; 55636444100; 7006892410; 15770244400","A High-Performance Multispectral Adaptation GAN for Harmonizing Dense Time Series of Landsat-8 and Sentinel-2 Images","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","","10134","10146","12","10.1109/JSTARS.2021.3115604","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117353591&doi=10.1109%2fJSTARS.2021.3115604&partnerID=40&md5=05a1938bd1d54b7fb5395d6ae8fdab4a","The combination of data acquired by Landsat-8 and Sentinel-2 earth observation missions produces dense time series (TSs) of multispectral images that are essential for monitoring the dynamics of land-cover and land-use classes across the earth's surface with high temporal resolution. However, the optical sensors of the two missions have different spectral and spatial properties, thus they require a harmonization processing step before they can be exploited in remote sensing applications. In this work, we propose a workflow-based on a deep learning approach to harmonize these two products developed and deployed on an high-performance computing environment. In particular, we use a multispectral generative adversarial network with a U-Net generator and a PatchGan discriminator to integrate existing Landsat-8 TSs with data sensed by the Sentinel-2 mission. We show a qualitative and quantitative comparison with an existing physical method [National Aeronautics and Space Administration (NASA) Harmonized Landsat and Sentinel (HLS)] and analyze original and generated data in different experimental setups with the support of spectral distortion metrics. To demonstrate the effectiveness of the proposed approach, a crop type mapping task is addressed using the harmonized dense TS of images, which achieved an overall accuracy of 87.83% compared to 81.66% of the state-of-the-art method.  © 2008-2012 IEEE.","Deep learning; Land use; NASA; Remote sensing; Time series; Deep learning; Dense time series; Dense-time; Generative adversarial network; Harmonisation; High performance computing; LANDSAT; Landsat-8; Performance computing; Remote sensing; Remote-sensing; Sentinel-2; Times series; Virtual constellation; land cover; land use change; Landsat; machine learning; multispectral image; remote sensing; satellite mission; Sentinel; time series analysis; Generative adversarial networks","Deep learning (DL); dense time series (TSs); generative adversarial network (GAN); harmonization; high performance computing (HPC); Landsat-8; remote sensing (RS); sentinel-2; virtual constellation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117353591"
"Liu Y.-X.; Zhang B.; Wang B.","Liu, Yu-Xi (57219443635); Zhang, Bo (57196371859); Wang, Bin (56584968800)","57219443635; 57196371859; 56584968800","Semi-supervised semantic segmentation based on Generative Adversarial Networks for remote sensing images; [基于生成式对抗网络的遥感图像半监督语义分割]","2020","Hongwai Yu Haomibo Xuebao/Journal of Infrared and Millimeter Waves","39","4","","473","482","9","10.11972/j.issn.1001-9014.2020.04.012","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094931006&doi=10.11972%2fj.issn.1001-9014.2020.04.012&partnerID=40&md5=d0ff74f33999b2bb806f39f9caab7f6c","Semantic segmentation of very high resolution (VHR) remote sensing images is one of the hot topics in the field of remote sensing image processing. Traditional supervised segmentation methods demand a huge mass of labeled data while the labeling process is very consuming. To solve this problem, a semi-supervised semantic segmentation method for VHR remote sensing images based on Generative Adversarial Networks (GANs) is proposed, and only a few labeled samples are needed to obtain pretty good segmentation results. A fully convolutional auxiliary adversarial network is added to the segmentation network, conducing to keeping the consistency of labels in the segmentation results of VHR remote sensing images. Furthermore, a novel adversarial loss with attention mechanism is proposed in the paper in order to solve the problem of easy sample over-whelming during the updating process of the segmentation network constrained by the discriminator when the segmentation results can confuse the discriminator. The experimental results on ISPRS Vaihingen 2D Semantic Labeling Challenge Dataset show that the proposed method can greatly improve the segmentation accuracy of remote sensing images compared with other state-of-the-art methods. © 2020, Science Press. All right reserved.","Convolutional neural networks; Image enhancement; Remote sensing; Semantic Web; Semantics; Semi-supervised learning; Adversarial networks; Remote sensing image processing; Remote sensing images; Segmentation accuracy; Semantic segmentation; State-of-the-art methods; Supervised segmentation; Very high resolution; Image segmentation","Deep learning; Generative adversarial networks; Loss function; Semantic segmentation; Very high resolution remote sensing images","Article","Final","","Scopus","2-s2.0-85094931006"
"Jiang T.; Li Y.; Xie W.; Du Q.","Jiang, Tao (56518850800); Li, Yunsong (55986546100); Xie, Weiying (56768656200); Du, Qian (7202060063)","56518850800; 55986546100; 56768656200; 7202060063","Discriminative reconstruction constrained generative adversarial network for hyperspectral anomaly detection","2020","IEEE Transactions on Geoscience and Remote Sensing","58","7","8972475","4666","4679","13","10.1109/TGRS.2020.2965961","78","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080881930&doi=10.1109%2fTGRS.2020.2965961&partnerID=40&md5=155af58f00f5c8e96f98633c2980ebb7","The rich and distinguishable spectral information in hyperspectral images (HSIs) makes it possible to capture anomalous samples [i.e., anomaly detection (AD)] that deviate from background samples. However, hyperspectral anomaly detection (HAD) faces various challenges due to high dimensionality, redundant information, and unlabeled and limited samples. To address these problems, this article proposes an unsupervised discriminative reconstruction constrained generative adversarial network for HAD (HADGAN). Our solution is mainly based on the assumption that the number of normal samples is much larger than the number of abnormal ones. The key contribution of this article is to learn a discriminative background reconstruction with anomaly targets being suppressed, which produces the initial detection image (i.e., the residual image between the original image and reconstructed image) with anomaly targets being highlighted and background samples being suppressed. To accomplish this goal, first, by using an autoencoder (AE) network and an adversarial latent discriminator, the latent feature layer learns normal background distribution and AE learns a background reconstruction as much as possible. Second, consistency enhanced representation and shrink constraints are added to the latent feature layer to ensure that anomaly samples are projected to similar positions as normal samples in the latent feature layer. Third, using an adversarial image feature corrector in the input space can guarantee the reliability of the generated samples. Finally, an energy-based spatial and distance-based spectral joint anomaly detector is applied in the residual map to generate the final detection map. Experiments conducted on several data sets over different scenes demonstrate its state-of-the-art performance.  © 1980-2012 IEEE.","Spectroscopy; Adversarial networks; Anomaly detector; Background reconstruction; High dimensionality; Hyperspectral anomaly detection; Reconstructed image; Spectral information; State-of-the-art performance; anomaly; artificial neural network; detection method; reconstruction; remote sensing; spectral analysis; Anomaly detection","Anomaly detection (AD); autoencoder (AE); generative adversarial network (GAN); hyperspectral image (HSI); latent constraint; spatial-spectral detector","Article","Final","","Scopus","2-s2.0-85080881930"
"Li L.; Zhou Z.; Cui S.","Li, Linhao (57204549892); Zhou, Zhiqiang (55728196000); Cui, Saijia (57299898400)","57204549892; 55728196000; 57299898400","Boosting Small Ship Detection in Optical Remote Sensing Images via Image Super-Resolution","2021","Proceedings of the 33rd Chinese Control and Decision Conference, CCDC 2021","","","","1508","1512","4","10.1109/CCDC52312.2021.9601674","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125184752&doi=10.1109%2fCCDC52312.2021.9601674&partnerID=40&md5=649763eca9e4925aad8eef3363ab157c","Small ships in optical remote sensing images are hard to detect due to the lack of sufficient detail information. In this paper, we adopt the image super-resolution technology to solve this problem. Specifically, an effective super-resolution network is designed to generate clear super-resolution ship images from small blurry ones produced by the ship detector. Inspired by the idea of generative adversarial network (GAN), the super-resolution network is trained together with a discriminator network in an adversarial way, aiming at generating more realistic super-resolution images. Moreover, to eliminate false detections, the discriminator network is also used to distinguish ship and non-ship images via an additional classification branch. Experimental results demonstrate the effectiveness of the proposed method.  © 2021 IEEE.","Discriminators; Geology; Optical resolving power; Remote sensing; Ships; False detections; Image super resolutions; Optical remote sensing; Remote sensing images; Resolution images; Ship detection; Superresolution; Generative adversarial networks","Generative adversarial network; Image super-resolution; Ship detection","Conference paper","Final","","Scopus","2-s2.0-85125184752"
"Zhang D.; Zhou Y.; Zhao J.; Zhou Z.; Yao R.","Zhang, Di (57219908378); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Zhou, Ziyuan (57219904145); Yao, Rui (23567304500)","57219908378; 35480110700; 57138970300; 57219904145; 23567304500","Structural similarity preserving GAN for infrared and visible image fusion","2021","International Journal of Wavelets, Multiresolution and Information Processing","19","1","2050063","","","","10.1142/S0219691320500630","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095988911&doi=10.1142%2fS0219691320500630&partnerID=40&md5=06fa53c486ac16725df7fb1bfa02932b","Compared with a single image, in a complex environment, image fusion can utilize the complementary information provided by multiple sensors to significantly improve the image clarity and the information, more accurate, reliable, comprehensive access to target and scene information. It is widely used in military and civil fields, such as remote sensing, medicine, security and other fields. In this paper, we propose an end-to-end fusion framework based on structural similarity preserving GAN (SSP-GAN) to learn a mapping of the fusion tasks for visible and infrared images. Specifically, on the one hand, for making the fusion image natural and conforming to visual habits, structure similarity is introduced to guide the generator network produce abundant texture structure information. On the other hand, to fully take advantage of shallow detail information and deep semantic information for achieving feature reuse, we redesign the network architecture of multi-modal image fusion meticulously. Finally, a wide range of experiments on real infrared and visible TNO dataset and RoadScene dataset prove the superior performance of the proposed approach in terms of accuracy and visual. In particular, compared with the best results of other seven algorithms, our model has improved entropy, edge information transfer factor, multi-scale structural similarity and other evaluation metrics, respectively, by 3.05%, 2.4% and 0.7% on TNO dataset. And our model has also improved by 0.7%, 2.82% and 1.1% on RoadScene dataset.  © 2021 World Scientific Publishing Company.","Image enhancement; Infrared imaging; Network architecture; Remote sensing; Semantics; Textures; Complex environments; Evaluation metrics; Improved entropies; Infrared and visible image; Semantic information; Structural similarity; Structure similarity; Texture structure; Image fusion","generative adversarial network; Image fusion; infrared images; structural similarity; visible images","Article","Final","","Scopus","2-s2.0-85095988911"
"Sun H.; Wang P.; Chang Y.; Qi L.; Wang H.; Xiao D.; Zhong C.; Wu X.; Li W.; Sun B.","Sun, Hai (57215064466); Wang, Ping (57309154600); Chang, Yifan (57211169238); Qi, Li (57215066382); Wang, Hailei (55862398400); Xiao, Dan (57206626278); Zhong, Cheng (57213115199); Wu, Xuelian (56129590300); Li, Wenbo (57207126023); Sun, Bingyu (7401983972)","57215064466; 57309154600; 57211169238; 57215066382; 55862398400; 57206626278; 57213115199; 56129590300; 57207126023; 7401983972","HRPGAN: A GAN-based Model to Generate High-resolution Remote Sensing Images","2020","IOP Conference Series: Earth and Environmental Science","428","1","012060","","","","10.1088/1755-1315/428/1/012060","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079746270&doi=10.1088%2f1755-1315%2f428%2f1%2f012060&partnerID=40&md5=a5c7cb03f40c59a996673cff239ce539","Generative adversarial networks (GAN) has been mainly used in the generation of natural images such as MNIST, CIFAR10 as well as Imagenet datasets and achieves satisfying generation results. However, GAN always fails in generating high quality high-resolution remote sensing images because remote sensing images are large in size and have various ground objects. To address this issue, a novel framework called High-Resolution PatchGAN (HRPGAN) is introduced in this paper. The structure of HRPGAN follows PatchGAN, but the batch normalization layers are removed and the ReLU activation is replaced by the SELU activation. In addition, a new loss function consisting of the adversarial loss, perceptual reconstruction loss and regularization loss is used in HRPGAN. Experiment results show that the proposed HRPGAN model generates the more diverse and lifelike images in HR remote sensing generation than Bicubic method and TGAN model. © Published under licence by IOP Publishing Ltd.","Chemical activation; Adversarial networks; Ground objects; High quality; High resolution; High resolution remote sensing images; Loss functions; Natural images; Remote sensing images; Remote sensing","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85079746270"
"Holland W.J.; Du Q.","Holland, Wesley J. (57219563770); Du, Qian (7202060063)","57219563770; 7202060063","Adversarially regularized autoencoder for hyperspectral image unmixing","2020","Proceedings of SPIE - The International Society for Optical Engineering","11533","","115330U","","","","10.1117/12.2575212","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093921238&doi=10.1117%2f12.2575212&partnerID=40&md5=72d171f3783070db24f7ac0250095fc2","Deep autoencoders have recently been applied to blind hyperspectral unmixing task to estimate endmembers and their corresponding abundances simultaneously. The objective of an original autoencoder is to reconstruct an input data matrix unsupervisedly with an encoder network and a decoder network. For the purpose of spectral unmixing, the activations of the final layer of the encoder and the weights of the decoder form abundances and endmember signatures, respectively; constraints, e.g., abundance non-negativity and abundance sum-to-one, can be imposed. In this paper, we present a novel regularization technique for autoencoder-based hyperspectral unmixing. The basic idea is the inclusion of a generative adversarial network (GAN) joint training objective to condition the decoder to generalize to unseen abundance mixtures. In addition to regularizing the endmember weights of the decoder, this approach has the benefit of explicitly modeling the prior distribution of hyperspectral pixels for a given scene as the abundance output of the generator. The benefit of the proposed strategy is evaluated on synthetic and real data sets, demonstrating that it can produce endmember estimates closer to the ground truth. © SPIE. Downloading of the abstract is permitted for personal use only.","Decoding; Learning systems; Remote sensing; Signal encoding; Spectroscopy; Adversarial networks; HyperSpectral; Hyperspectral unmixing; Non-negativity; Prior distribution; Regularization technique; Spectral unmixing; Synthetic and real data; Image processing","Adversarial; Autoencoder; GAN; Hyperspectral unmixing; Regularization","Conference paper","Final","","Scopus","2-s2.0-85093921238"
"Oliveira W.D.G.D.; Penatti O.A.B.; Berton L.","Oliveira, Willian Dihanster G. De (57221603249); Penatti, Otavio A. B. (25655324300); Berton, Lilian (35112729100)","57221603249; 25655324300; 35112729100","A comparison of graph-based semi-supervised learning for data augmentation","2020","Proceedings - 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images, SIBGRAPI 2020","","","9265996","264","271","7","10.1109/SIBGRAPI51738.2020.00043","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099576474&doi=10.1109%2fSIBGRAPI51738.2020.00043&partnerID=40&md5=72769d88fb178650533e5b348aca23a2","In supervised learning, the algorithm accuracy usually improves with the size of the labeled dataset used for training the classifier. However, in many real-life scenarios, obtaining enough labeled data is costly or even not possible. In many circumstances, Data Augmentation (DA) techniques are usually employed, generating more labeled data for training machine learning algorithms. The common DA techniques are applied to already labeled data, generating simple variations of this data. For example, for image classification, image samples are rotated, cropped, flipped or other operators to generate variations of input image samples, and keeping their original labels. Other options are using Neural Networks algorithms that create new synthetic data or to employ Semi-supervised Learning (SSL) that label existing unlabeled data. In this paper, we perform a comparison among graph-based semi-supervised learning (GSSL) algorithms to augment the labeled dataset. The main advantage of using GSSL is that we can increase the training set by adding non-annotated images to the training set, therefore, we can benefit from the huge amount of unlabeled data available. Experiments are performed on five datasets for recognition of handwritten digits and letters (MNIST and EMINIST), animals (Dogs vs Cats), clothes (MNIST-Fashion) and remote sensing images (Brazilian Coffee Scenes), in which we compare different possibilities for DA, including the GSSL, Generative Adversarial Networks (GANs) and traditional Image Transformations (IT) applied on input labeled data. We also evaluated the impact of such techniques on different convolutional neural networks (CNN). Results indicate that, although all DA techniques performed well, GSSL was more robust to different image properties, presenting less accuracy variation across datasets.  © 2020 IEEE.","Character recognition; Convolutional neural networks; Graph algorithms; Graphic methods; Image classification; Labeled data; Remote sensing; Semi-supervised learning; Adversarial networks; Algorithm accuracies; Data augmentation; Handwritten digit; Image transformations; Neural networks algorithms; Remote sensing images; Semi-supervised learning (SSL); Learning algorithms","data augmentation; GANs; Image classification; image transformation; machine learning; semi supervised learning","Conference paper","Final","","Scopus","2-s2.0-85099576474"
"Choi Y.; Kim M.; Kim Y.; Han S.","Choi, Yeonju (57215967828); Kim, Minsik (57223883024); Kim, Yongwoo (57202143770); Han, Sanghyuck (37101680300)","57215967828; 57223883024; 57202143770; 37101680300","A Study of CNN-based Super-Resolution Method for Remote Sensing Image","2020","Korean Journal of Remote Sensing","36","3","","449","460","11","10.7780/kjrs.2020.36.3.5","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106324741&doi=10.7780%2fkjrs.2020.36.3.5&partnerID=40&md5=d10c8da1971505cfb3328e5823c91add","Super-resolution is a technique used to reconstruct an image with low-resolution into that of high-resolution. Recently, deep-learning based super resolution has become the mainstream, and applications of these methods are widely used in the remote sensing field. In this paper, we propose a super-resolution method based on the deep back-projection network model to improve the satellite image resolution by the factor of four. In the process, we customized the loss function with the edge loss to result in a more detailed feature of the boundary of each object and to improve the stability of the model training using generative adversarial network based on Wasserstein distance loss. Also, we have applied the detail preserving image down-scaling method to enhance the naturalness of the training output. Finally, by including the modified-residual learning with a panchromatic feature in the final step of the training process. Our proposed method is able to reconstruct fine features and high frequency information. Comparing the results of our method with that of the others, we propose that the super-resolution method improves the sharpness and the clarity of WorldView-3 and KOMPSAT-2 images. © 2020 Annals of Laparoscopic and Endoscopic Surgery. All rights reserved.","","DPID; Edge loss; Remote sensing image; SISR; Super resolution","Article","Final","","Scopus","2-s2.0-85106324741"
"Jiang T.; Xie W.; Li Y.; Du Q.","Jiang, Tao (56518850800); Xie, Weiying (56768656200); Li, Yunsong (55986546100); Du, Qian (7202060063)","56518850800; 56768656200; 55986546100; 7202060063","Discriminative Semi-Supervised Generative Adversarial Network for Hyperspectral Anomaly Detection","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323688","2420","2423","3","10.1109/IGARSS39084.2020.9323688","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101954993&doi=10.1109%2fIGARSS39084.2020.9323688&partnerID=40&md5=ed5dd3648df73945bd74980e14f1006a","Hyperspectral anomaly detection has been facing great challenges in the field of deep learning due to high dimensions and limited samples. To address these challenges, a novel discriminative semi-supervised generative adversarial network (GAN) method with dual RX (Reed-Xiaoli), called semiDRX, is proposed in this paper. The main contribution of the proposed method is to learn a reconstruction of background homogenization and anomaly saliency through a semi-supervised GAN. To achieve this goal, firstly, the coarse RX detection is performed to obtain a background sample set with potential anomalous pixels being removed. Secondly, the obtained coarse background set learns more comprehensive background characteristics through the network. The original hyperspectral image (HSI) is fed into the learned network to obtain reconstructions with homogeneous backgrounds and salient anomalies. The refined detection results are generated by a second RX detector. Experiments on three HSIs over different scenes demonstrate its advancement and effectiveness. © 2020 IEEE.","Deep learning; Geology; Homogenization method; Remote sensing; Spectroscopy; Adversarial networks; High dimensions; Hyperspectral anomaly detection; Rx detectors; Sample sets; Semi-supervised; Anomaly detection","background homogenization; dual RX; generative adversarial network (GAN); Hyperspectral anomaly detection (HAD); semi-supervised","Conference paper","Final","","Scopus","2-s2.0-85101954993"
"Sui B.; Jiang T.; Zhang Z.; Pan X.; Liu C.","Sui, Baikai (57215827051); Jiang, Tao (55737611000); Zhang, Zhen (57211499690); Pan, Xinliang (57215835993); Liu, Chenxi (57215210429)","57215827051; 55737611000; 57211499690; 57215835993; 57215210429","A modeling method for automatic extraction of offshore aquaculture zones based on semantic segmentation","2020","ISPRS International Journal of Geo-Information","9","3","145","","","","10.3390/ijgi9030145","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081994235&doi=10.3390%2fijgi9030145&partnerID=40&md5=5b6131d67f7ca3491ac3f74242cf5ceb","Monitoring of offshore aquaculture zones is important to marine ecological environment protection and maritime safety and security. Remote sensing technology has the advantages of large-area simultaneous observation and strong timeliness, which provide normalized monitoring of marine aquaculture zones. Aiming at the problems of weak generalization ability and low recognition rate in weak signal environments of traditional target recognition algorithm, this paper proposes a method for automatic extraction of offshore fish cage and floating raft aquaculture zones based on semantic segmentation. This method uses Generative Adversarial Networks to expand the data to compensate for the lack of training samples, and uses ratio of green band to red band (G/R) instead of red band to enhance the characteristics of aquaculture spectral information, combined with atrous convolution and atrous space pyramid pooling to enhance the context semantic information, to extract and identify two types of offshore fish cage zones and floating raft aquaculture zones. The experiment is carried out in the eastern coastal waters of Shandong Province, China, and the overall identification accuracy of the two types of aquaculture zones can reach 94.8%. The results show that the method proposed in this paper can realize high-precision extraction both of offshore fish cage and floating raft aquaculture zones. © 2020 by the authors.","","Generative adversarial networks; High-resolution remote sensing image; Offshore aquaculture; Semantic segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081994235"
"Liu W.; Wu H.; Hu K.; Luo Q.; Cheng X.","Liu, Wenxuan (57202813858); Wu, Huayi (55745907000); Hu, Kai (56732360400); Luo, Qing (57355289500); Cheng, Xiaoqiang (35742876100)","57202813858; 55745907000; 56732360400; 57355289500; 35742876100","A Scientometric Visualization Analysis of Image Captioning Research from 2010 to 2020","2021","IEEE Access","9","","","156799","156817","18","10.1109/ACCESS.2021.3129782","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120077663&doi=10.1109%2fACCESS.2021.3129782&partnerID=40&md5=04183b940e2cea076084e1d2c81fc883","Image captioning has gradually gained attention in the field of artificial intelligence and become an interesting and challenging task for image understanding. It needs to identify important objects in images, extract attributes, tell relationships, and help the machine generate human-like descriptions. Recent works in deep neural networks have greatly improved the performance of image caption models. However, machines are still unable to imitate the way humans think, talk and communicate, so image captioning remains an ongoing task. It is thus very important to keep up with the latest research and results in the field of image captioning whereas publications on this topic are numerous. Our work aims to help researchers to have a macro-level understanding of image captioning from four aspects: spatial-temporal distribution characteristics, collaborative networks, trends in subject research, and historical evolutionary path. We employ scientometric visualization methods to achieve this goal. The results show that China has published the largest amount of publications in image captioning, but the United States has the greatest impact on research in this area. Besides, thirteen academic groups are identified in the field of image description, with institutions such as Microsoft, Google, Australian National University, and Georgia Institute of Technology being the most prominent research institutions. Meanwhile, we find that evaluation methods, datasets, novel image captioning models based on generative adversarial networks, reinforcement learning, and Transformer, as well as remote sensing image captioning, are the new research trends. Lastly, we conclude that image captioning research has gone through three major development stages from 2010 to 2020, and on this basis, we propose a more comprehensive taxonomy of image captioning.  © 2013 IEEE.","Data visualization; Deep neural networks; Flow visualization; Generative adversarial networks; Image enhancement; Image recognition; Market Research; Reinforcement learning; Remote sensing; Bibliometric; Conference; Image captioning; Image description generation; Image descriptions; Index; Market researches; Remote-sensing; Scientometric analysis; Scientometrics; Image analysis","Image captioning; image description generation; scientometric analysis; visualization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120077663"
"Zhang L.; Liu Y.","Zhang, Libao (35325855000); Liu, Yanan (57211089183)","35325855000; 57211089183","Image generation based on texture guided vae-agan for regions of Interest detection in remote sensing images","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","2310","2314","4","10.1109/ICASSP39728.2021.9413823","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115146769&doi=10.1109%2fICASSP39728.2021.9413823&partnerID=40&md5=4dd924cd5026334ae2f7def4f6e82a59","Deep learning has shown great strength in regions of interest (ROIs) detection for remote sensing images (RSIs). However, for most of RSIs, the unbalanced distribution of positive and negative samples greatly limits the performance of the deep learning-based methods. To cope with this issue, we propose a novel method based on texture guided variational autoencoder-attention wise generative adversarial network (VAE-AGAN) to augment the training data for ROI detection. First, to generate realistic texture details of RSIs, we propose a texture guidance block to embed texture prior information into encoder and decoder networks. Second, we introduce the channel and spatial-wise attention layers in the discriminator construct to adaptively recalibrate the varying importance of different channels and spatial regions of input RSIs. Finally, we apply the RSI dataset balanced by our proposal to the weakly supervised ROI detection method. Experimental results demonstrate that the proposal can not only improve the performance of ROI detection, but also outperform other competing augmentation methods. ©2021 IEEE.","Deep learning; Learning systems; Signal processing; Textures; Adversarial networks; Augmentation methods; Image generations; Learning-based methods; Prior information; Regions of interest; Remote sensing images; Unbalanced distribution; Remote sensing","Attention; Generative adversarial networks; Image generation; Texture guidance","Conference paper","Final","","Scopus","2-s2.0-85115146769"
"Krishnendu C. S; Sowmya V.; Soman K.P.","Krishnendu C. S (57219121065); Sowmya, V. (36096164300); Soman, K.P. (57205365723)","57219121065; 36096164300; 57205365723","Impact of Dimension Reduced Spectral Features on Open Set Domain Adaptation for Hyperspectral Image Classification","2021","Advances in Intelligent Systems and Computing","1176","","","737","746","9","10.1007/978-981-15-5788-0_69","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091330090&doi=10.1007%2f978-981-15-5788-0_69&partnerID=40&md5=d178787ee881f275974f8ffd2a76a6e9","Hyperspectral image classification has so many applications in the area of remote sensing. In recent years, deep learning has been accepted as a powerful tool for feature extraction and ensuring better classification accuracies. In this paper, model for HSI classification is created by implementing open set domain adaptation and generative adversarial networks (GAN). Open set domain adaptation is a type of domain adaptation where target has more classes which are not present in the source distribution. Huge dimension of hyperspectral image needs to be reduced for an efficient classification. In this work, we analysed the effect of dimensionality reduction for open set domain adaptation for hyperspectral image classification by using dynamic mode decomposition (DMD) technique. Experimental results show that 20% of the total available bands of Salinas and 30% of the bands of PaviaU dataset are the highest achievable reduction in feature dimension that results in almost same classification accuracy. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Computation theory; Deep learning; Dimensionality reduction; Image classification; Intelligent computing; Remote sensing; Spectroscopy; Adversarial networks; Classification accuracy; Domain adaptation; Dynamic mode decompositions; Feature dimensions; Source distribution; Spectral feature; Classification (of information)","Dimension reduction; GAN; Image classification; Open set domain adaptation","Conference paper","Final","","Scopus","2-s2.0-85091330090"
"Lei S.; Shi Z.; Zou Z.","Lei, Sen (57195618353); Shi, Zhenwei (23398841900); Zou, Zhengxia (56073977200)","57195618353; 23398841900; 56073977200","Coupled Adversarial Training for Remote Sensing Image Super-Resolution","2020","IEEE Transactions on Geoscience and Remote Sensing","58","5","8946581","3633","3643","10","10.1109/TGRS.2019.2959020","54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083899027&doi=10.1109%2fTGRS.2019.2959020&partnerID=40&md5=7215beb42d0cc9dcb4e96f78e3b6c307","Generative adversarial network (GAN) has made great progress in recent natural image super-resolution tasks. The key to its success is the integration of a discriminator which is trained to classify whether the input is a real high-resolution (HR) image or a generated one. Arguably, learning a strong discriminative prior is essential for generating high-quality images. However, in remote sensing images, we discover, through extensive statistical analysis, that there are more low-frequency components than natural images, which may lead to a 'discrimination-ambiguity' problem, i.e., the discriminator will become 'confused' to tell whether its input is real or not when dealing with those low-frequency regions, and therefore, the quality of generated HR images may be deeply affected. To address this problem, we propose a novel GAN-based super-resolution algorithm named coupled-discriminated GANs (CDGANs) for remote sensing images. Different from the previous GAN-based super-resolution models in which their discriminator takes in a single image at one time, in our model, the discriminator is specifically designed to take in a pair of images: a generated image and its HR ground truth, to make better discrimination of the inputs. We further introduce a dual pathway network architecture, a random gate, and a coupled adversarial loss to learn better correspondence between the discriminative results and the paired inputs. Experimental results on two public data sets demonstrate that our model can obtain more accurate super-resolution results in terms of both visual appearance and local details compared with other state of the arts. Our code will be made publicly available. © 1980-2012 IEEE.","Network architecture; Optical resolving power; Adversarial networks; High quality images; High resolution image; Low frequency regions; Low-frequency components; Remote sensing images; Super resolution algorithms; Super-resolution models; algorithm; data set; image resolution; remote sensing; satellite imagery; visual analysis; Remote sensing","Coupled adversarial training; deep convolutional neural networks; generative adversarial networks (GANs); remote sensing images; super-resolution","Article","Final","","Scopus","2-s2.0-85083899027"
"Pan X.; Zhao J.; Xu J.","Pan, Xin (35422588500); Zhao, Jian (57188561431); Xu, Jun (57210253499)","35422588500; 57188561431; 57210253499","A Scene Images Diversity Improvement Generative Adversarial Network for Remote Sensing Image Scene Classification","2020","IEEE Geoscience and Remote Sensing Letters","17","10","8915710","1692","1696","4","10.1109/LGRS.2019.2953192","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093955119&doi=10.1109%2fLGRS.2019.2953192&partnerID=40&md5=a4a1b45c59e87cba392969b2f679f75c","To achieve good remote sensing image scene classification, deep learning models usually require a large number of samples in the training stage. Unfortunately, collecting a large number of training scene images usually involves large acquisition and processing costs. In contrast, after training a generative adversarial network (GAN), scene samples can subsequently be generated automatically by the generator at a low cost. Then, the generated images can be added to the training set. A model with better classification ability will be obtained when these samples include more diverse scene structures and essential features than the original real images. In this letter, we propose the scene images diversity improvement GAN (diversity-GAN). Diversity-GAN has two important advantages. 1) The training process is designed in a progressive manner: the GAN's generator and discriminator progress from coarse-to fine-resolution scene images. This characteristic can guarantee the diversity of generated samples. In particular, it guarantees the diversity of the structure of the generated scene images. 2) The training progress is controllable: by introducing control parameters, diversity-GAN can directly determine the scene image resolution on which the training process should focus. This characteristic allows diversity-GAN to achieve scene image structure diversity at the coarse-resolution training stage with a few iterations. In the experiments, the UC-Merced and AID data sets are introduced. The results show that the samples generated by diversity-GAN can effectively improve the diversity of the sample set, and these generated samples can grant convolutional neural networks (CNNs) better classification ability in the training stage.  © 2004-2012 IEEE.","Convolutional neural networks; Deep learning; Image classification; Image resolution; Remote sensing; Adversarial networks; Classification ability; Control parameters; Diversity improvement; Essential features; Number of samples; Processing costs; Remote sensing images; artificial neural network; automation; image analysis; image classification; image resolution; learning; parameterization; remote sensing; Image enhancement","Deep learning; diversity improvement; generative adversarial network (GAN); sample generation; scene classification","Article","Final","","Scopus","2-s2.0-85093955119"
"Wu Y.; Bai Z.; Miao Q.; Ma W.; Yang Y.; Gong M.","Wu, Yue (56215531900); Bai, Zhuangfei (57215535197); Miao, Qiguang (9133503300); Ma, Wenping (57205878746); Yang, Yuelei (57204044678); Gong, Maoguo (8933846400)","56215531900; 57215535197; 9133503300; 57205878746; 57204044678; 8933846400","A classified adversarial network for multi-spectral remote sensing image change detection","2020","Remote Sensing","12","13","2098","","","","10.3390/rs12132098","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087546161&doi=10.3390%2frs12132098&partnerID=40&md5=d6e7fa657f548ae8c96252ad77cb39e0","Adversarial training has demonstrated advanced capabilities for generating image models. In this paper, we propose a deep neural network, named a classified adversarial network (CAN), for multi-spectral image change detection. This network is based on generative adversarial networks (GANs). The generator captures the distribution of the bitemporal multi-spectral image data and transforms it into change detection results, and these change detection results (as the fake data) are input into the discriminator to train the discriminator. The results obtained by pre-classification are also input into the discriminator as the real data. The adversarial training can facilitate the generator learning the transformation from a bitemporal image to a change map. When the generator is trained well, the generator has the ability to generate the final result. The bitemporal multi-spectral images are input into the generator, and then the final change detection results are obtained from the generator. The proposed method is completely unsupervised, and we only need to input the preprocessed data that were obtained from the pre-classification and training sample selection. Through adversarial training, the generator can better learn the relationship between the bitemporal multi-spectral image data and the corresponding labels. Finally, the well-trained generator can be applied to process the raw bitemporal multi-spectral images to obtain the final change map (CM). The effectiveness and robustness of the proposed method were verified by the experimental results on the real high-resolution multi-spectral image data sets. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep neural networks; Image classification; Spectroscopy; Adversarial networks; Change detection; High resolution; Multi-spectral image data; Multispectral images; Pre-processed data; Remote sensing images; Training sample selection; Remote sensing","Change detection; Generative adversarial networks (GANs); Multi-spectral remote sensing image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85087546161"
"Abdollahi A.; Pradhan B.; Gite S.; Alamri A.","Abdollahi, Abolfazl (57193647122); Pradhan, Biswajeet (12753037900); Gite, Shilpa (56656365900); Alamri, Abdullah (57215408871)","57193647122; 12753037900; 56656365900; 57215408871","Building Footprint Extraction from High Resolution Aerial Images Using Generative Adversarial Network (GAN) Architecture","2020","IEEE Access","8","","9260150","209517","209527","10","10.1109/ACCESS.2020.3038225","29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097646944&doi=10.1109%2fACCESS.2020.3038225&partnerID=40&md5=e8b2c44be61715abae37e69ff858f574","Building extraction with high accuracy using semantic segmentation from high-resolution remotely sensed imagery has a wide range of applications like urban planning, updating of geospatial database, and disaster management. However, automatic building extraction with non-noisy segmentation map and obtaining accurate boundary information is a big challenge for most of the popular deep learning methods due to the existence of some barriers like cars, vegetation cover and shadow of trees in the high-resolution remote sensing imagery. Thus, we introduce an end-to-end convolutional neural network called Generative Adversarial Network (GAN) in this study to tackle these issues. In the generative model, we utilized SegNet model with Bi-directional Convolutional LSTM (BConvLSTM) to generate the segmentation map from Massachusetts building dataset containing high-resolution aerial imagery. BConvLSTM combines encoded features (containing of more local information) and decoded features (containing of more semantic information) to improve the performance of the model even with the presence of complex backgrounds and barriers. The adversarial training method enforces long-range spatial label vicinity to tackle with the issue of covering building objects with the existing occlusions such as trees, cars and shadows and achieve high-quality building segmentation outcomes under the complex areas. The quantitative results obtained by the proposed technique with an average F1-score of 96.81% show that the suggested approach could achieve better results through detecting and adjusting the difference between the segmentation model output and the reference map compared to other state-of-the-art approaches such as autoencoder method with 91.36%, SegNet+BConvLSTM with 95.96%, FCN-CRFs with 95.36%% SegNet with 94.77%, and GAN-SCA model with 96.36% accuracy.  © 2013 IEEE.","Aerial photography; Antennas; Buildings; Complex networks; Convolution; Convolutional neural networks; Deep learning; Disaster prevention; Disasters; Extraction; Forestry; Learning systems; Remote sensing; Semantics; Automatic building extraction; Boundary information; High resolution aerial imagery; High resolution remote sensing imagery; High-resolution aerial images; High-resolution remotely sensed imageries; Semantic segmentation; State-of-the-art approach; Long short-term memory","Building extraction; GAN; remote sensing; SegNet","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85097646944"
"Courtrai L.; Pham M.-T.; Lefèvre S.","Courtrai, Luc (6507861482); Pham, Minh-Tan (56070990300); Lefèvre, Sébastien (57203070803)","6507861482; 56070990300; 57203070803","Small object detection in remote sensing images based on super-resolution with auxiliary generative adversarial networks","2020","Remote Sensing","12","19","3152","1","19","18","10.3390/rs12193152","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092761573&doi=10.3390%2frs12193152&partnerID=40&md5=514218afa5c2d6b706c213d8d17edd87","This article tackles the problem of detecting small objects in satellite or aerial remote sensing images by relying on super-resolution to increase image spatial resolution, thus the size and details of objects to be detected. We show how to improve the super-resolution framework starting from the learning of a generative adversarial network (GAN) based on residual blocks and then its integration into a cycle model. Furthermore, by adding to the framework an auxiliary network tailored for object detection, we considerably improve the learning and the quality of our final super-resolution architecture, and more importantly increase the object detection performance. Besides the improvement dedicated to the network architecture, we also focus on the training of super-resolution on target objects, leading to an object-focused approach. Furthermore, the proposed strategies do not depend on the choice of a baseline super-resolution framework, hence could be adopted for current and future state-of-the-art models. Our experimental study on small vehicle detection in remote sensing data conducted on both aerial and satellite images (i.e., ISPRS Potsdam and xView datasets) confirms the effectiveness of the improved super-resolution methods to assist with the small object detection tasks. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Antennas; Image enhancement; Network architecture; Object recognition; Optical resolving power; Remote sensing; Small satellites; Adversarial networks; Aerial remote sensing; Detection performance; Image spatial resolution; Remote sensing data; Remote sensing images; Small object detection; Superresolution methods; Object detection","Auxiliary network; Cycle GAN; Deep learning; Generative adversarial network (GAN); Remote sensing; Small object detection; Super-resolution; Wasserstein GAN","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092761573"
"Yu Y.; Li X.; Liu F.","Yu, Yunlong (57188735471); Li, Xianzhi (57276731100); Liu, Fuxian (55553732044)","57188735471; 57276731100; 55553732044","E-DBPN: Enhanced Deep Back-Projection Networks for Remote Sensing Scene Image Superresolution","2020","IEEE Transactions on Geoscience and Remote Sensing","58","8","8974409","5503","5515","12","10.1109/TGRS.2020.2966669","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089233187&doi=10.1109%2fTGRS.2020.2966669&partnerID=40&md5=b65f1f01787b572522a99de259e21bcc","Generative adversarial networks (GANs) have been widely used to single image superresolution (SR) (SISR), and these GAN-based methods have achieved significant performance on natural images due to their abilities to generate realistic textures. However, previous GAN-based methods are of poor performance when applied to remote sensing scene image SR. In order to further enhance the visual quality, in this article, we present a GAN-based SISR method by proposing a novel generator, which is capable of generating perceptually pleasing remote sensing scene images. First, we design the enhanced deep back-projection network (E-DBPN) generators based on the architecture of the original DBPN and mainly make two modifications. The first one is to add the proposed enhanced residual channel attention module (ERCAM) into the original DBPN, which can keep good properties of the original input features but also has the ability to emphasize more important features and suppressing less useful features. The other is to replace the concatenation operation with the proposed sequential feature fusion module (SFFM) for dealing with the feature maps generated by different up-projection units discriminatorily. As for the training process, the E-DBPN generator is first trained using the mean squared error (MSE) loss. Next, in order to improve the perceptual quality of the recovered images, we employ the content loss and the adversarial loss to train our initialized generator network. Experiments show that our method achieves state-of-the-art performance compared to other SISR methods.  © 1980-2012 IEEE.","Mean square error; Optical resolving power; Remote sensing; Textures; Adversarial networks; Important features; Mean squared error; Perceptual quality; Poor performance; State-of-the-art performance; Training process; Visual qualities; image analysis; image resolution; network analysis; remote sensing; Image enhancement","Attention mechanism; generative adversarial networks (GANs); perceptual quality; remote sensing scene image; single image superresolution (SISR)","Article","Final","","Scopus","2-s2.0-85089233187"
"Zhu D.; Xia S.; Zhao J.; Zhou Y.; Jian M.; Niu Q.; Yao R.; Chen Y.","Zhu, Dongjun (57204810481); Xia, Shixiong (55650066900); Zhao, Jiaqi (57138970300); Zhou, Yong (35480110700); Jian, Meng (55832971100); Niu, Qiang (22635551200); Yao, Rui (23567304500); Chen, Ying (57195284871)","57204810481; 55650066900; 57138970300; 35480110700; 55832971100; 22635551200; 23567304500; 57195284871","Diverse sample generation with multi-branch conditional generative adversarial network for remote sensing objects detection","2020","Neurocomputing","381","","","40","51","11","10.1016/j.neucom.2019.10.065","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075908981&doi=10.1016%2fj.neucom.2019.10.065&partnerID=40&md5=ecdebc680b00049be9db786fa26ff6a8","The remote sensing data is difficult to collect and lack of diversity, which extremely limits the performance of object detection on remote sensing images. In this paper, a multi-branch conditional generative adversarial network (MCGAN) is proposed to augment data for object detection in optical remote sensing images, which is the first GANs-based data augmentation framework proposed for this topic. We use MCGAN to generate the diverse objects based on the existing remote sensing datasets. The multi-branch dilated convolution and the classification branch are adopted into MCGAN to help the generator to generate the diverse and high-quality images. Meanwhile, an adaptive samples selection strategy based on the Faster R-CNN is proposed to select the samples for data augmentation from the objects generated by MCGAN, which can ensure the quality of new augmented training sets and improve the diversity of samples. Experiments based on NWPU VHR-10 and DOTA show that the objects generated by MCGAN have the higher quality compared with the objects generated by WGAN and LSGAN. And the mean average precision detected by the state-of-the-art object detection models used in the experiments has the satisfactory improvement after the MCGAN based data augmentation, which indicates that data augmentation by MCGAN can effectively improve the accuracy of remote sensing images object detection. © 2019","Deep learning; Image enhancement; Object recognition; Remote sensing; Adversarial networks; Data augmentation; High quality images; Objects detection; Optical remote sensing; Remote sensing data; Remote sensing images; Sample generations; article; deep learning; remote sensing; Object detection","Data augmentation; Deep learning; Generative adversarial network; Object detection; Remote sensing","Article","Final","","Scopus","2-s2.0-85075908981"
"Teng W.; Wang N.; Shi H.; Liu Y.; Wang J.","Teng, Wenxiu (57223020753); Wang, Ni (57094386800); Shi, Huihui (57205125773); Liu, Yuchan (57216638243); Wang, Jing (57221359723)","57223020753; 57094386800; 57205125773; 57216638243; 57221359723","Classifier-Constrained Deep Adversarial Domain Adaptation for Cross-Domain Semisupervised Classification in Remote Sensing Images","2020","IEEE Geoscience and Remote Sensing Letters","17","5","8794530","789","793","4","10.1109/LGRS.2019.2931305","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083915981&doi=10.1109%2fLGRS.2019.2931305&partnerID=40&md5=4ce21c5ea3543126764ebd33a8239ae4","This letter presents a classifier-constrained deep adversarial domain adaptation (CDADA) method for cross-domain semisupervised classification in remote sensing (RS) images. A deep convolutional neural network (DCNN) is used to build feature representations to describe the semantic content of scenes before the adaptation process. Then, adversarial domain adaptation is used to align the feature distribution of the source and the target. Specifically, two different land-cover classifiers are used as a discriminator to consider land-cover decision boundaries between classes and increase their distance to separate them from the original land-cover class boundaries. The generator then creates robust transferable features far from the original land-cover class boundaries under the classifier constraint. The experimental results of six scenarios built from three benchmark RS scene data sets (AID, Merced, and RSI-CB data sets) are reported and discussed. © 2019 IEEE.","Convolutional neural networks; Deep neural networks; Image classification; Semantics; Adaptation process; Decision boundary; Domain adaptation; Feature distribution; Feature representation; Remote sensing images; Semantic content; Semi-supervised classification; benchmarking; data set; land cover; remote sensing; satellite imagery; supervised classification; Remote sensing","Cross-domain classification; deep convolutional neural networks (DCNNs); domain adaptation (DA); generative adversarial networks (GANs); remote sensing (RS)","Article","Final","","Scopus","2-s2.0-85083915981"
"Zhang K.; Sumbul G.; Demir B.","Zhang, Kexin (57221087691); Sumbul, Gencer (57196192158); Demir, Begum (15131434800)","57221087691; 57196192158; 15131434800","An Approach to Super-Resolution of Sentinel-2 Images Based on Generative Adversarial Networks","2020","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium, M2GARSS 2020 - Proceedings","","","9105165","69","72","3","10.1109/M2GARSS47143.2020.9105165","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086704517&doi=10.1109%2fM2GARSS47143.2020.9105165&partnerID=40&md5=9f39cc5fcc004c7aac1a60ab0390b2aa","This paper presents a generative adversarial network based super-resolution (SR) approach (which is called as S2GAN) to enhance the spatial resolution of Sentinel-2 spectral bands. The proposed approach consists of two main steps. The first step aims to increase the spatial resolution of the bands with 20m and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To this end, we introduce a generator network that performs SR on the lower resolution bands with the guidance of the bands associated to 10m spatial resolution by utilizing the convolutional layers with residual connections and a long skip-connection between inputs and outputs. The second step aims to distinguish SR bands from their ground truth bands. This is achieved by the proposed discriminator network, which alternately characterizes the high level features of the two sets of bands and applying binary classification on the extracted features. Then, we formulate the adversarial learning of the generator and discriminator networks as a min-max game. In this learning procedure, the generator aims to produce realistic SR bands as much as possible so that the discriminator incorrectly classifies SR bands. Experimental results obtained on different Sentinel-2 images show the effectiveness of the proposed approach compared to both conventional and deep learning based SR approaches. © 2020 IEEE.","Deep learning; Geology; Image resolution; Optical resolving power; Remote sensing; Adversarial learning; Adversarial networks; Binary classification; High-level features; Learning procedures; Lower resolution; Spatial resolution; Super resolution; Discriminators","generative adversarial network; remote sensing; Sentinel-2 images; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086704517"
"Abdollahi A.; Pradhan B.; Shukla N.; Chakraborty S.; Alamri A.","Abdollahi, Abolfazl (57193647122); Pradhan, Biswajeet (12753037900); Shukla, Nagesh (24179276600); Chakraborty, Subrata (56377149900); Alamri, Abdullah (57215408871)","57193647122; 12753037900; 24179276600; 56377149900; 57215408871","Deep learning approaches applied to remote sensing datasets for road extraction: A state-of-the-art review","2020","Remote Sensing","12","9","1444","","","","10.3390/RS12091444","100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085972386&doi=10.3390%2fRS12091444&partnerID=40&md5=6bb4af042a253924d205053fd2cfe898","One of the most challenging research subjects in remote sensing is feature extraction, such as road features, from remote sensing images. Such an extraction influences multiple scenes, including map updating, traffic management, emergency tasks, road monitoring, and others. Therefore, a systematic review of deep learning techniques applied to common remote sensing benchmarks for road extraction is conducted in this study. The research is conducted based on four main types of deep learning methods, namely, the GANs model, deconvolutional networks, FCNs, and patch-based CNNs models. We also compare these various deep learning models applied to remote sensing datasets to show which method performs well in extracting road parts from high-resolution remote sensing images. Moreover, we describe future research directions and research gaps. Results indicate that the largest reported performance record is related to the deconvolutional nets applied to remote sensing images, and the F1 score metric of the generative adversarial network model, DenseNet method, and FCN-32 applied to UAV and Google Earth images are high: 96.08%, 95.72%, and 94.59%, respectively. © 2020 by the authors.","Extraction; Image processing; Learning systems; Remote sensing; Roads and streets; Adversarial networks; Future research directions; High resolution remote sensing images; Learning techniques; Remote sensing images; Research subjects; State-of-the art reviews; Traffic management; Deep learning","Common benchmarks; Deep learning; Machine learning; Remote sensing; Road extraction","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085972386"
"Knyaz V.; Kniaz V.","Knyaz, V. (6507787519); Kniaz, V. (56540022100)","6507787519; 56540022100","Object recognition for UAV navigation in complex environment","2020","Proceedings of SPIE - The International Society for Optical Engineering","11533","","115330P","","","","10.1117/12.2574078","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093984685&doi=10.1117%2f12.2574078&partnerID=40&md5=a7894be2bb39a0c5b0ddb329a8e175c2","Impressive progress in technical characteristics of modern unmanned aerial vehicles (UAV) provides new opportunities for their exploiting in different applications and missions which were impossible earlier. The growing applicability of UAVs is based on high performance of modern computers and latest advances in sensor data processing techniques. Recent decades modern convolutional neural network (CNN) models have demonstrated the state of the art performance in many computer vision problems seemed to be solved properly only by a human. The study is aimed at developing a deep learning techniques for UAV autonomous navigation in complex environment in obstacle avoidance mode. Such kind of navigation is required for cargo delivery or rescue mission in urban, industrial or forestry environment when global geo-positioning system can be unavailable. For navigating in complex environment UAV have to recognize objects of observed scene and to estimate distance for possible obstacle. The proposed technique to solve these tasks exploits deep learning approach for image segmentation and depth map estimation using an image of the observed scene. The convolutional neural network model is developed capable to predict depth map of the observed scene along with scene segmentation according the predefined object classes. The proposed neural network architecture is based on generative adversarial model with generative part translating an input color image into an output voxel model. The aim of the discriminative part is to estimate how close the output to real data and to penalize false output. Both generative and discriminative parts are trained simultaneously on the specially prepared dataset. Evaluation on the testing part of the prepared dataset has demonstrated the ability of the developed neural network model to perform segmentation of unobserved complex scenes containing several objects and estimating depth map for this scene. The proposed neural network architecture provides high generalization ability for new scenes. © SPIE. Downloading of the abstract is permitted for personal use only.","Ability testing; Antennas; Complex networks; Convolution; Convolutional neural networks; Data handling; Deep learning; Image segmentation; Navigation; Navigation systems; Object recognition; Remote sensing; Statistical tests; Unmanned aerial vehicles (UAV); Autonomous navigation; Complex environments; Computer vision problems; Depth map estimation; Generalization ability; Neural network model; Sensor data processing; State-of-the-art performance; Network architecture","Dataset; Deep learning; Depth map; Generative adversarial network; Object recognition; Segmentation; UAV","Conference paper","Final","","Scopus","2-s2.0-85093984685"
"Doi K.; Sakurada K.; Onishi M.; Iwasaki A.","Doi, Kento (57208264052); Sakurada, Ken (8669789300); Onishi, Masaki (7102474552); Iwasaki, Akira (8073280000)","57208264052; 8669789300; 7102474552; 8073280000","GAN-Based SAR-to-Optical Image Translation with Region Information","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323085","2069","2072","3","10.1109/IGARSS39084.2020.9323085","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101999476&doi=10.1109%2fIGARSS39084.2020.9323085&partnerID=40&md5=83aefd118492ab1f30f8602a4e3a2c20","In this paper, we propose a SAR-to-optical image translation method based on conditional generative adversarial networks (cGANs). Though cGANs have achieved great success in image translation, some problems remain in SAR-to-optical image translation. One of the problems is the colorization error owing to the lack of color information in SAR data. Since the colors of optical images are varied, while SAR images have no color information, the generator network is confused and fail to generate correctly colorized optical images. To prevent it, we introduce a region information to the image translation network. Specifically, the feature vector from the pre-trained classification network is fed to the generator and discriminator network. Experimental results with SEN1-2 dataset show the advantage of our proposed method over the baseline method that does not use any additional information. © 2020 IEEE.","Color; Geology; Geometrical optics; Remote sensing; Synthetic aperture radar; Adversarial networks; Baseline methods; Classification networks; Color information; Feature vectors; Image translation; Optical image; Region information; Radar imaging","Generative adversarial network (GAN); Image translation; Optical remote sensing; SAR","Conference paper","Final","","Scopus","2-s2.0-85101999476"
"Ji H.; Gao Z.; Mei T.; Ramesh B.","Ji, Hong (57205763449); Gao, Zhi (55256514200); Mei, Tiancan (8914886000); Ramesh, Bharath (56442158400)","57205763449; 55256514200; 8914886000; 56442158400","Vehicle Detection in Remote Sensing Images Leveraging on Simultaneous Super-Resolution","2020","IEEE Geoscience and Remote Sensing Letters","17","4","8792159","676","680","4","10.1109/LGRS.2019.2930308","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081677087&doi=10.1109%2fLGRS.2019.2930308&partnerID=40&md5=5a95ef4a661454d292e0a2fe01bbb40d","Owing to the relatively small size of vehicles in remote sensing images, lacking sufficient detailed appearance to distinguish vehicles from similar objects, the detection performance is still far from satisfactory compared with the detection results on everyday images. Inspired by the positive effects of super-resolution convolutional neural network (SRCNN) for object detection and the stunning success of deep CNN techniques, we apply generative adversarial network frameworks to realize simultaneous SRCNN and vehicle detection in an end-to-end manner, and the detection loss is backpropagated into the SRCNN during training to facilitate detection. In particular, our work is unsupervised and bypasses the requirement of low-/high-resolution image pairs during the training stage, achieving increased generality and applicability. Extensive experiments on representative data sets demonstrate that our method outperforms the state-of-the-art detectors. (The source code will be made available after the review process. © 2004-2012 IEEE.","Convolution; Object detection; Optical resolving power; Remote sensing; Vehicles; Feature fusion; Region-based; Remote sensing images; Super resolution; Vehicle detection; artificial neural network; detection method; image resolution; remote sensing; satellite imagery; Convolutional neural networks","Faster region-based convolutional neural network (R-CNN); feature fusion; remote sensing images; super-resolution convolutional neural network (SRCNN); vehicle detection","Article","Final","","Scopus","2-s2.0-85081677087"
"Liu B.; Li H.; Zhou Y.; Peng Y.; Elazab A.; Wang C.","Liu, Bo (57221214127); Li, Heng (57220036007); Zhou, Yutao (57220033102); Peng, Yuqing (57220025030); Elazab, Ahmed (56523141500); Wang, Changmiao (57226651647)","57221214127; 57220036007; 57220033102; 57220025030; 56523141500; 57226651647","A super resolution method for remote sensing images based on cascaded conditional wasserstein GANs","2020","2020 3rd IEEE International Conference on Information Communication and Signal Processing, ICICSP 2020","","","9232066","284","289","5","10.1109/ICICSP50920.2020.9232066","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096514768&doi=10.1109%2fICICSP50920.2020.9232066&partnerID=40&md5=8a44523392137bdab79644c5f0436c29","High-resolution (HR) remote sensing imagery is quite beneficial for subsequent interpretation. Obtaining HR images can be achieved by upgrading the imaging device. Yet, the cost to perform this task is very huge. Thus, it is necessary to obtain HR images from low-resolution (LR) ones. In the literature, the super-resolution image reconstruction methods based on deep learning have unparalleled advantages in comparison to traditional reconstruction methods. This work is inspired by these current mainstream methods and proposes a novel cascaded conditional Wasserstein generative adversarial network (CCWGAN) architecture with the residual dense block to generate high quality remote sensing images. We validate the proposed method on the NWPU VHR-10 dataset. Experimental results show our CCWGAN method has superior performance compared with the state-of-the-art GAN methods. © 2020 IEEE.","Deep learning; Image reconstruction; Optical resolving power; Adversarial networks; High resolution; Reconstruction method; Remote sensing imagery; Remote sensing images; State of the art; Super-resolution image reconstruction; Superresolution methods; Remote sensing","Cascaded conditional generative adversarial networks; Remote sensing images; Residual dense block; Wasserstein generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85096514768"
"","","","7th International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2021","2021","Communications in Computer and Information Science","1451","","","","","1055","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115417831&partnerID=40&md5=8e3ecb39f537ed5de5801347839e6843","The proceedings contain 81 papers. The special focus in this conference is on Pioneering Computer Scientists, Engineers and Educators. The topics include: MA Mask R-CNN: MPR and AFPN Based Mask R-CNN; improved Non-negative Matrix Factorization Algorithm for Sparse Graph Regularization; a Blockchain-Based Scheme of Data Sharing for Housing Provident Fund; intelligent Storage System of Machine Learning Model Based on Task Similarity; predicting Stock Price Movement with Multiple Data Sources and Machine Learning Models; channel Context and Dual-Domain Attention Based U-Net for Skin Lesion Attributes Segmentation; study on the Protection and Product Development of Intangible Cultural Heritage with Computer Virtual Reality Technology; ECG-Based Arrhythmia Detection Using Attention-Based Convolutional Neural Network; Quantum Color Image Scaling on QIRHSI Model; WSN Data Compression Model Based on K-SVD Dictionary and Compressed Sensing; human Body Pose Recognition System Based on Teaching Interaction; adaptive Densely Residual Network for Image Super-Resolution; Real-Time Image and Video Artistic Style Rendering System Based on GPU; semantic Segmentation of High Resolution Remote Sensing Images Based on Improved ResU-Net; Exploring Classification Capability of CNN Features; generative Adversarial Network Based Status Generation Simulation Approach; the Construction of Case Event Logic Graph for Judgment Documents; anti-obfuscation Binary Code Clone Detection Based on Software Gene; thread Private Variable Access Optimization Technique for Sunway High-Performance Multi-core Processors; parallel Region Reconstruction Technique for Sunway High-Performance Multi-core Processors; research on Route Optimization of Battlefield Collection Equipment Based on Improved Ant Algorithm; a Collaborative Cache Strategy Based on Utility Optimization; integrating Local Closure Coefficient into Weighted Networks for Link Prediction.","","","Conference review","Final","","Scopus","2-s2.0-85115417831"
"Hou Y.; Zhang J.","Hou, Yangshuan (36494442100); Zhang, Jishuai (57218298226)","36494442100; 57218298226","Unsupervised remote sensing image super-resolution method based on adaptive domain distance measurement network","2020","Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020","","","9131243","256","259","3","10.1109/AEMCSE50948.2020.00062","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088658770&doi=10.1109%2fAEMCSE50948.2020.00062&partnerID=40&md5=1f3e8c001c401b0f7b410d6433581906","Compared with supervised learning, unsupervised learning is more practical; however, the associated training process is more difficult and complex. To solve the problems of unstable training and insufficient diversity of generative adversarial networks (GAN), which are widely used to realize unsupervised learning, we propose a novel unsupervised remote sensing image super-resolution method based on a reverse generating network module and the adaptive domain distance measurement network. The discriminant network of GAN is considered as a tool to measure a certain image attribute instead of the original GAN binary classification network. Furthermore, the adaptive domain distance measurement network is used to back feed the information of a high-resolution image to guide the optimization of the generating network. The results of experiments performed on various datasets demonstrate the effectiveness of the proposed method. © 2020 IEEE.","Distance measurement; Optical resolving power; Software engineering; Unsupervised learning; Adversarial networks; Binary classification; High resolution image; Image attributes; Remote sensing images; Training process; Remote sensing","Domain; GAN; Remote senseng; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85088658770"
"Wang S.; Mu X.; He H.; Yang D.; Ma C.","Wang, Shuyang (57196156058); Mu, Xiaodong (57074918400); He, Hao (57204310141); Yang, Dongfang (55888542900); Ma, Chenhui (57194441804)","57196156058; 57074918400; 57204310141; 55888542900; 57194441804","Feature-representation-transfer based road extraction method for cross-domain aerial images; [航拍图像跨数据域特征迁移道路提取方法]","2020","Cehui Xuebao/Acta Geodaetica et Cartographica Sinica","49","5","","611","621","10","10.11947/j.AGCS.2020.20190274","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086070558&doi=10.11947%2fj.AGCS.2020.20190274&partnerID=40&md5=bec4b3f77e790a49c27707446314158a","Aiming at the problem of the insufficient generalization ability of traditional road extraction methods when applying to a new dataset, this paper proposes a cross-domain road extraction method that realized by feature-representation-transfer and encoder-decoder network. Firstly, a basic road extraction model based on encoder-decoder network is designed to segment the road from a single data source. Then, based on the structure of road extraction network and the principle of cycle-consistent, a cycle generative adversarial network for feature transfer of cross-domain imagery is used, which maps the feature of target city images to the domain of source data. Finally, the pre-trained road extraction model is used to segment the target domain images after the feature transfer, so that the cross-domain road extraction can be realized. The experimental results show that the proposed method improves the generalization ability of the road extraction network and can extract the road target from cross-domain images accurately and effectively. Compared with the results without feature transfer, the proposed method greatly improves the road extraction metric, and increases the F1-score by more than 50%. The proposed method does not require any annotation of the target domain images, nor does it need to fine-tune the road extraction network, while it only need to train the feature transfer model from the target domain to the source domain. Therefore, it has good application value. © 2020, Surveying and Mapping Press. All right reserved.","Antennas; Data mining; Decoding; Extraction; Image enhancement; Image segmentation; Network coding; Roads and streets; Adversarial networks; Aerial images; Encoder-decoder; Feature representation; Feature transfers; Generalization ability; Road extraction; Road extraction method; algorithm; data set; genetic algorithm; image analysis; numerical method; numerical model; satellite imagery; segmentation; Feature extraction","Deep learning; Encoder-decoder network; Generative adversarial network; Remote sensing; Road extraction; Transfer learning","Article","Final","","Scopus","2-s2.0-85086070558"
"Chen N.; Li C.","Chen, Naigeng (57191526700); Li, Chenming (36806296000)","57191526700; 36806296000","Hyperspectral image classification approach based on wasserstein generative adversarial networks","2020","Proceedings - International Conference on Machine Learning and Cybernetics","2020-December","","9469586","53","63","10","10.1109/ICMLC51923.2020.9469586","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113771634&doi=10.1109%2fICMLC51923.2020.9469586&partnerID=40&md5=40e2512112be052ac80e61237756f3f9","Hyperspectral image classification is an important research direction in the application of remote sensing technology. In the process of labeling different types of objects based on spectral information and geometric spatial characteristics, noise interference often exists in continuous multi-band spectral information, which brings great troubles to spectral feature extraction. Besides, far from enough spectral samples will restrict the classification performance of the algorithm to some extent. In order to solve the problem of small amount of original spectral sample data and noisy signal, Wasserstein generative adversarial networks (WGAN) is used to generate samples similar to the original spectrum, and spectral features are extracted from the samples. In the case of small samples, the original materials are provided for the classification of hyperspectral images and a semi-supervised classification model WGAN-CNN for hyperspectral images based on Wasserstein generation antagonistic network is proposed in this paper. This model combines with CNN classifier and completes the classification of terrain objects according to the label for the synthesized samples. The proposed method is compared with several classical hyperspectral image classification methods in classification accuracy. WGAN-CNN can achieve higher classification accuracy in the case of small sample size, which proves the effectiveness of the proposed method. © 2020 IEEE.","Learning systems; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification accuracy; Classification performance; Remote sensing technology; Semi-supervised classification; Spatial characteristics; Spectral feature extraction; Spectral information; Image classification","CNN; Hyperspectral images classification; Semisupervised lea<sup>1</sup>rning; WGAN","Conference paper","Final","","Scopus","2-s2.0-85113771634"
"Dong H.; Ma W.; Wu Y.; Zhang J.; Jiao L.","Dong, Huihui (57215045543); Ma, Wenping (57205878746); Wu, Yue (56215531900); Zhang, Jun (57375869100); Jiao, Licheng (7102491544)","57215045543; 57205878746; 56215531900; 57375869100; 7102491544","Self-supervised representation learning for remote sensing image change detection based on temporal prediction","2020","Remote Sensing","12","11","1868","","","","10.3390/rs12111868","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086467056&doi=10.3390%2frs12111868&partnerID=40&md5=a36e5714cb3a7285145087db8a4a80ca","Traditional change detection (CD) methods operate in the simple image domain or hand-crafted features, which has less robustness to the inconsistencies (e.g., brightness and noise distribution, etc.) between bitemporal satellite images. Recently, deep learning techniques have reported compelling performance on robust feature learning. However, generating accurate semantic supervision that reveals real change information in satellite images still remains challenging, especially for manual annotation. To solve this problem, we propose a novel self-supervised representation learning method based on temporal prediction for remote sensing image CD. The main idea of our algorithm is to transform two satellite images into more consistent feature representations through a self-supervised mechanism without semantic supervision and any additional computations. Based on the transformed feature representations, a better difference image (DI) can be obtained, which reduces the propagated error of DI on the final detection result. In the self-supervised mechanism, the network is asked to identify different sample patches between two temporal images, namely, temporal prediction. By designing the network for the temporal prediction task to imitate the discriminator of generative adversarial networks, the distribution-aware feature representations are automatically captured and the result with powerful robustness can be acquired. Experimental results on real remote sensing data sets show the effectiveness and superiority of our method, improving the detection precision up to 0.94-35.49%. © 2020 by the authors.","Deep learning; Feature extraction; Forecasting; Learning systems; Satellites; Semantics; Adversarial networks; Detection precision; Feature representation; Learning techniques; Noise distribution; Remote sensing data; Remote sensing images; Temporal prediction; Remote sensing","Deep belief networks; Generative adversarial networks; Remote sensing images; Self-supervised representation learning; Unsupervised change detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086467056"
"Poterek Q.; Herrault P.-A.; Skupinski G.; Sheeren D.","Poterek, Quentin (57217247820); Herrault, Pierre-Alexis (55830661800); Skupinski, Grzegorz (29567547300); Sheeren, David (35146599100)","57217247820; 55830661800; 29567547300; 35146599100","Deep learning for automatic colorization of legacy grayscale aerial photographs","2020","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13","","9103274","2899","2915","16","10.1109/JSTARS.2020.2992082","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086893002&doi=10.1109%2fJSTARS.2020.2992082&partnerID=40&md5=ca33f2b7f62a00a9674bb482511f3db0","Legacy grayscale aerial photographs represent one of the main available sources for studying the past state of the environment and its relationship to the present. However, these photographs lack spectral information thereby hindering their use in current remote sensing approaches that rely on spectral data for characterizing surfaces. This article proposes a conditional generative adversarial network, a deep learning model, to enrich legacy photographs by predicting color channels for an input grayscale image. The technique was used to colorize two orthophotographs (taken in 1956 and 1978) covering the entire Eurométropole de Strasbourg. To assess the model's performances, two strategies were proposed: first, colorized photographs were evaluated with metrics such as peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM); second, random forest classifications were performed to extract land cover classes from grayscale and colorized photographs, respectively. The results revealed strong performances, with PSNR = 25.56 ± 2.20 and SSIM = 0.93 ± 0.06 indicating that the model successfully learned the mapping between grayscale and color photographs over a large territory. Moreover, land cover classifications performed on colorized data showed significant improvements over grayscale photographs, respectively, +6% and +17% for 1956 and 1978. Finally, the plausibility of outputs images was evaluated visually. We conclude that deep learning models are powerful tools for improving radiometric properties of old aerial grayscale photographs and land cover mapping. We also argue that the proposed approach could serve as a basis for further developments aiming to promote the use of aerial photographs archives for landscapes reconstruction.  © 2008-2012 IEEE.","Aerial photography; Antennas; Decision trees; Image enhancement; Learning systems; Mapping; Photographic equipment; Remote sensing; Signal to noise ratio; Adversarial networks; Further development; Land cover classification; Peak signal to noise ratio; Random forest classification; Remote sensing approaches; Spectral information; Structural similarity indices (SSIM); aerial photograph; color; image analysis; land cover; orthophoto; remote sensing; signal-to-noise ratio; spectral analysis; Deep learning","Colorization; deep learning; generative adversarial network (GAN); grayscale imagery; historical aerial photograph; remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086893002"
"Luo X.; Li X.; Wu Y.; Hou W.; Wang M.; Jin Y.; Xu W.","Luo, Xin (57196018157); Li, Xiaoxi (57220954647); Wu, Yuxuan (57220962777); Hou, Weimin (56210062900); Wang, Meng (57218085285); Jin, Yuwei (57207758252); Xu, Wenbo (56237288700)","57196018157; 57220954647; 57220962777; 56210062900; 57218085285; 57207758252; 56237288700","Research on Change Detection Method of High-Resolution Remote Sensing Images Based on Subpixel Convolution","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9291461","1447","1457","10","10.1109/JSTARS.2020.3044060","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097926512&doi=10.1109%2fJSTARS.2020.3044060&partnerID=40&md5=975b7c85b1e6746d364253aa115a882a","Remote sensing image change detection method plays a great role in land cover research, disaster assessment, medical diagnosis, video surveillance, and other fields, so it has attracted wide attention. Based on a small sample dataset from SZTAKI AirChange Benchmark Set, in order to solve the problem that the deep learning network needs a large number of samples, this work first uses nongenerative sample augmentation method and generative sample augmentation method based on deep convolutional generative adversarial networks, and then, constructs a remote sensing image change detection model based on an improved DeepLabv3+ network. This model can realize end-to-end training and prediction of remote sensing image change detection with subpixel convolution. Finally, Landsat 8, Google Earth, and Onera satellite change detection datasets are used to verify the generalization performance of this network. The experimental results show that the improved network accuracy is 95.1% and the generalization performance is acceptable.  © 2008-2012 IEEE.","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Image enhancement; Learning systems; Medical imaging; Pixels; Security systems; Adversarial networks; Augmentation methods; Generalization performance; High resolution remote sensing images; Remote sensing images; Satellite change detection; Small sample datum; Video surveillance; artificial neural network; benchmarking; detection method; image analysis; remote sensing; research work; satellite data; satellite imagery; Remote sensing","Change detection; deep convolutional generative adversarial networks (DCGAN); deep learning; DeepLabv3+; subpixel convolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097926512"
"Zaytar M.A.; Amrani C.E.","Zaytar, Mohamed Akram (57204795723); Amrani, Chaker El (55308112200)","57204795723; 55308112200","Satellite image inpainting with deep generative adversarial neural networks","2021","IAES International Journal of Artificial Intelligence","10","1","","121","130","9","10.11591/ijai.v10.i1.pp121-130","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103077361&doi=10.11591%2fijai.v10.i1.pp121-130&partnerID=40&md5=a7e62a057f39ab07c05ff870b680ba76","This work addresses the problem of recovering lost or damaged satellite image pixels (gaps) caused by sensor processing errors or by natural phenomena like cloud presence. Such errors decrease our ability to monitor regions of interest and significantly increase the average revisit time for all satellites. This paper presents a novel neural system based on conditional deep generative adversarial networks (cGAN) optimized to fill satellite imagery gaps using surrounding pixel values and static high-resolution visual priors. Experimental results show that the proposed system outperforms traditional and neural network baselines. It achieves a normalized least absolute deviations error of L1 = 0.33 (21% and 60% decrease in error compared with the two baselines) and a mean squared error loss of ℒ2 = 0.15 (29% and 73% decrease in error) over the test set. The model can be deployed within a remote sensing data pipeline to reconstruct missing pixel measurements for near-real-time monitoring and inference purposes, thus empowering policymakers and users to make environmentally informed decisions. © 2021, Institute of Advanced Engineering and Science. All rights reserved.","","Air pollution; Generative adversarial nets; Image inpainting; Neural networks; Satellite imagery","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85103077361"
"Deng K.; Zhang K.; Yao P.; Cheng S.; He P.","Deng, Kai (57217296176); Zhang, Kun (57214938091); Yao, Ping (57206347584); Cheng, Siyuan (57261961400); He, Peng (57192956908)","57217296176; 57214938091; 57206347584; 57261961400; 57192956908","Skip attention GaN for remote sensing image synthesis","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","2305","2309","4","10.1109/ICASSP39728.2021.9414701","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115066093&doi=10.1109%2fICASSP39728.2021.9414701&partnerID=40&md5=41f0d63d2d07c053c46253fa30d878df","High-quality remote sensing images are difficult to obtain due to limited conditions and high cost for data acquisition. With the development of machine vision and deep learning, some image generation methods (e.g., GANs) are introduced into this field, but it's still hard to generate images with good texture details and structural dependencies. We establish Skip Attention Mechanism to deal with this problem, which learns dependencies between local points on low-resolution feature maps, and then upsample the attention map and combine it with high-resolution feature maps. With this method, long-range dependencies learned from low-resolution are used for generating remote sensing images with more structural details. We name this method as Skip Attention GAN, which is the first method applying cross-scale attention mechanism for unsupervised remote sensing image generation. Experiments show that our method outperforms previous methods under several metrics. Visual and ablation results of attention layers show that Skip Attention has learned long-distance structural dependencies between similar targets. © 2021 IEEE","Data acquisition; Deep learning; Gallium nitride; III-V semiconductors; Signal processing; Textures; Attention mechanisms; High quality; High resolution; Image generations; Long-range dependencies; Low resolution; Remote sensing images; Structural details; Remote sensing","Attention mechanism; Generative adversarial network; Remote sensing image synthesis","Conference paper","Final","","Scopus","2-s2.0-85115066093"
"","","","2021 6th International Conference on Signal and Image Processing, ICSIP 2021","2021","2021 6th International Conference on Signal and Image Processing, ICSIP 2021","","","","","","1285","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125194577&partnerID=40&md5=61b3bc34d85e49d537ff4dd09c9f9a3e","The proceedings contain 239 papers. The topics discussed include: single-channel speech enhancement using multi-task learning and attention mechanism; KL divergence based objective state selection method for EW conflict; semi-supervised anomaly detection and location based on generative adversarial network; high resolution imaging of GEO SAR under the influence of terrain elevation; research on hand soft rehabilitation system based on brain-computer interface and virtual reality; use of depth completion and SLAM algorithm to build dense maps of large scenes; efficient FFT based multi-source DOA estimation for ULA; Pivot-V: an optimized algorithm for password generation in PCFGs model; a co-simulation method of polarization imaging for temporary birefringent materials; and a two-stage line matching method for multi-temporal remote sensing images.","","","Conference review","Final","","Scopus","2-s2.0-85125194577"
"Ji Leong W.; Joseph Horgan H.","Ji Leong, Wei (57219904637); Joseph Horgan, Huw (16205109500)","57219904637; 16205109500","DeepBedMap: A deep neural network for resolving the bed topography of Antarctica","2020","Cryosphere","14","11","","3687","3705","18","10.5194/tc-14-3687-2020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096017317&doi=10.5194%2ftc-14-3687-2020&partnerID=40&md5=1a4344613cbc69353ef3ed92854b30ba","To resolve the bed elevation of Antarctica, we present DeepBedMap - a novel machine learning method that can produce Antarctic bed topography with adequate surface roughness from multiple remote sensing data inputs. The super-resolution deep convolutional neural network model is trained on scattered regions in Antarctica where high-resolution (250 m) ground-truth bed elevation grids are available. This model is then used to generate high-resolution bed topography in less surveyed areas. DeepBedMap improves on previous interpolation methods by not restricting itself to a low-spatial-resolution (1000 m) BEDMAP2 raster image as its prior image. It takes in additional high-spatialresolution datasets, such as ice surface elevation, velocity and snow accumulation, to better inform the bed topography even in the absence of ice thickness data from direct icepenetrating-radar surveys. The DeepBedMap model is based on an adapted architecture of the Enhanced Super-Resolution Generative Adversarial Network, chosen to minimize perpixel elevation errors while producing realistic topography. The final product is a four-times-upsampled (250 m) bed elevation model of Antarctica that can be used by glaciologists interested in the subglacial terrain and by ice sheet modellers wanting to run catchment- or continent-scale ice sheet model simulations. We show that DeepBedMap offers a rougher topographic profile compared to the standard bicubically interpolated BEDMAP2 and BedMachine Antarctica and envision it being used where a high-resolution bed elevation model is required.  © Author(s) 2020.","Antarctica; artificial neural network; cryosphere; topography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096017317"
"SargentG.C. GarrettC.; Ratliff B.M.; Asari V.","SargentG.C., GarrettC. (57193162199); Ratliff, Bradley M. (6603879539); Asari, Vijayan (6701420692)","57193162199; 6603879539; 6701420692","A guided generative adversarial network demosaicing strategy for integrated microgrid polarimeter imagery","2020","Proceedings of SPIE - The International Society for Optical Engineering","11412","","2557448","","","","10.1117/12.2557448","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086902855&doi=10.1117%2f12.2557448&partnerID=40&md5=d5f7fc64037ef0a1d6519d6b51a8884a","Division of focal plane (DoFP), or integrated microgrid imaging polarimeters, typically consist of a 2×2 mosaic of linear polarization filters overlaid upon a focal plane array sensor and obtain temporally synchronized polar- ized intensity measurements across a scene, similar in concept to a Bayer color filter array camera. However, the resulting estimated polarimetric images suffer a loss in resolution and can be plagued by aliasing due to the modulated microgrid measurement strategy. Demosaicing strategies have been proposed that attempt to minimize these effects, but result in some level of residual artifacts. In this work, we present a conditional and guided generative adversarial network (GAN) strategy for demosaicing integrated microgrid polarimeter imagery. The GAN is trained using high resolution polarized intensity measurements that contain minimal spatial aliasing artifacts obtained from a division-of-time polarimeter. We apply the algorithm to test data collected from real visible microgrid imagery and compare the results with other state-of-the-art microgrid demosaicing strategies. © 2020 SPIE. All rights reserved.","Focusing; Polarimeters; Polarization; Remote sensing; Adversarial networks; Bayer color filter array; Imaging Polarimeter; Intensity measurements; Linear polarization; Measurement strategies; Polarimetric image; Spatial aliasing; Microgrids","Deep learning; Demosaicing; Division of focal plane; Generative adversarial network; Imaging polarimeter; Microgrid; Polarimetry","Conference paper","Final","","Scopus","2-s2.0-85086902855"
"Holgado Alvarez J.L.; Ravanbakhsh M.; Demir B.","Holgado Alvarez, Jose Luis (57222243160); Ravanbakhsh, Mahdyar (57192545758); Demir, Begum (15131434800)","57222243160; 57192545758; 15131434800","S2-CGAN: Self-Supervised Adversarial Representation Learning for Binary Change Detection in Multispectral Images","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324345","2515","2518","3","10.1109/IGARSS39084.2020.9324345","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101992272&doi=10.1109%2fIGARSS39084.2020.9324345&partnerID=40&md5=171559755fde98baaedcb935473c67c0","Deep Neural Networks have recently demonstrated promising performance in binary change detection (CD) problems in remote sensing (RS), requiring a large amount of labeled multitemporal training samples. Since collecting such data is time-consuming and costly, most of the existing methods rely on pre-trained networks on publicly available computer vision (CV) datasets. However, because of the differences in image characteristics in CV and RS, this approach limits the performance of the existing CD methods. To address this problem, we propose a self-supervised conditional Generative Adversarial Network (S2-cGAN). The proposed S2-cGAN is trained to generate only the distribution of unchanged samples. To this end, the proposed method consists of two main steps: 1) Generating a reconstructed version of the input image as an unchanged image 2) Learning the distribution of unchanged samples through an adversarial game. Unlike the existing GAN based methods (which only use the discriminator during the adversarial training to supervise the generator), the S2-cGAN directly exploits the discriminator likelihood to solve the binary CD task. Experimental results show the effectiveness of the proposed S2-cGAN when compared to the state of the art CD methods. © 2020 IEEE.","Deep learning; Deep neural networks; Geology; Adversarial networks; Change detection; Image characteristics; Large amounts; Multi-temporal; Multispectral images; State of the art; Training sample; Remote sensing","binary change detection; Generative adversarial networks; multitemporal images; remote sensing; self-supervised learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101992272"
"Guo X.; Chen Z.; Wang C.","Guo, Xuejun (57116791700); Chen, Zehua (56000549900); Wang, Chengyi (57129401700)","57116791700; 56000549900; 57129401700","Fully convolutional densenet with adversarial training for semantic segmentation of high-resolution remote sensing images","2021","Journal of Applied Remote Sensing","15","1","016520","","","","10.1117/1.JRS.15.016520","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103690531&doi=10.1117%2f1.JRS.15.016520&partnerID=40&md5=d713d2035eb731de4ab751d44f8c0b77","Semantic segmentation is an important and foundational task in the application of high-resolution remote sensing images (HRRSIs). However, HRRSIs feature large differences within categories and minor variances across categories, posing a significant challenge to the high-Accuracy semantic segmentation of HRRSIs. To address this issue and obtain powerful feature expressiveness, a deep conditional generative adversarial network (DCGAN), integrating fully convolutional DenseNet (FC-DenseNet) and Pix2pix, is proposed. The DCGAN is composed of a generator-discriminator pair, which is built on a modified downsampling unit of FC-DenseNet. The proposed method possesses strong feature expression ability because of its skip connections, the very deep network structure and multiscale supervision introduced by FC-DenseNet, and the supervision from the discriminator. Experiments on a Deep Globe Land Cover dataset demonstrate the feasibility and effectiveness of this approach for the semantic segmentation of HRRSIs. The results also reveal that our method can mitigate the influence of class imbalance. Our approach for precise semantic segmentation can effectively facilitate the application of HRRSIs. © 2021 Society of Photo-Optical Instrumentation Engineers (SPIE).","Convolution; Remote sensing; Semantics; Adversarial networks; Class imbalance; Downsampling; Feature expression; High resolution remote sensing images; High-accuracy; Network structures; Semantic segmentation; Image segmentation","Densenets; Fully convolutional neural network; Generative adversarial network; High resolution remote sensing; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85103690531"
"Zuo Z.; Li Y.","Zuo, Zongcheng (57343777600); Li, Yuanxiang (57208656802)","57343777600; 57208656802","A SAR-TO-OPTICAL IMAGE TRANSLATION METHOD BASED ON PIX2PIX","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","3026","3029","3","10.1109/IGARSS47720.2021.9555111","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123861300&doi=10.1109%2fIGARSS47720.2021.9555111&partnerID=40&md5=e92f09e998ab3b4d47d380d630b65b71","Optical remote sensing images are susceptible to adverse weather effects, such as cloud occlusion, which lead to low availability of optical image data. However, synthetic aperture radar (SAR) can well overcome these shortcomings of optical imaging because of SAR working in an all-weather environment. Due to the imaging mechanism of SAR image is essentially different from optical image, the interpretation of SAR image is a huge challenge. Inspired by the powerful image-to-image translation capability of Generative Adversarial Networks (GANs), this paper proposes an improved Pix2Pix network to achieve the translation task from SAR image to optical image. In order to better maintain the structural information after image translation, this paper introduces a constraint based on phase consistency as the consideration of the loss function. Experimental results show that the proposed method has obvious advantages for multimodal remote sensing data translation tasks in comparison with the current state-of-the-art methods.  © 2021 IEEE.","Generative adversarial networks; Geometrical optics; Image enhancement; Radar imaging; Remote sensing; Image translation; Optical image; Optical remote sensing; Optical-; Phase consistencies; Pix2pix; Remote sensing images; Synthetic aperture radar images; Synthetic aperture radar-to-optical translation; Translation method; Synthetic aperture radar","GANs; phase consistency; Pix2Pix; SAR; SAR-to-optical translation","Conference paper","Final","","Scopus","2-s2.0-85123861300"
"Rabbi J.; Ray N.; Schubert M.; Chowdhury S.; Chao D.","Rabbi, Jakaria (56606267600); Ray, Nilanjan (7102751487); Schubert, Matthias (55605776884); Chowdhury, Subir (35619919300); Chao, Dennis (55967460400)","56606267600; 7102751487; 55605776884; 35619919300; 55967460400","Small-object detection in remote sensing images with end-to-end edge-enhanced GAN and object detector network","2020","Remote Sensing","12","9","1432","","","","10.3390/RS12091432","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085963738&doi=10.3390%2fRS12091432&partnerID=40&md5=930f9b94ac65cd2662d76c77627aaa66","The detection performance of small objects in remote sensing images has not been satisfactory compared to large objects, especially in low-resolution and noisy images. A generative adversarial network (GAN)-based model called enhanced super-resolution GAN (ESRGAN) showed remarkable image enhancement performance, but reconstructed images usually miss high-frequency edge information. Therefore, object detection performance showed degradation for small objects on recovered noisy and low-resolution remote sensing images. Inspired by the success of edge enhanced GAN (EEGAN) and ESRGAN, we applied a new edge-enhanced super-resolution GAN (EESRGAN) to improve the quality of remote sensing images and used different detector networks in an end-to-end manner where detector loss was backpropagated into the EESRGAN to improve the detection performance. We proposed an architecture with three components: ESRGAN, EEN, and Detection network. We used residual-in-residual dense blocks (RRDB) for both the ESRGAN and EEN, and for the detector network, we used a faster region-based convolutional network (FRCNN) (two-stage detector) and a single-shot multibox detector (SSD) (one stage detector). Extensive experiments on a public (car overhead with context) dataset and another self-assembled (oil and gas storage tank) satellite dataset showed superior performance of our method compared to the standalone state-of-the-art object detectors. © 2020 by the author.","Image enhancement; Object recognition; Optical resolving power; Remote sensing; Signal receivers; Adversarial networks; Convolutional networks; Detection networks; Detection performance; High frequency HF; Reconstructed image; Remote sensing images; Small object detection; Object detection","Edge enhancement; Faster region-based convolutional neural network (FRCNN); Object detection; Remote sensing imagery; Satellites; Single-shot multibox detector (SSD); Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085963738"
"Yagmur M.; Doi K.; Iwasaki A.","Yagmur, Mustafa (57215414482); Doi, Kento (57208264052); Iwasaki, Akira (8073280000)","57215414482; 57208264052; 8073280000","SAR2Map: SAR to map image transfer with conditional generative adversarial networks","2020","40th Asian Conference on Remote Sensing, ACRS 2019: Progress of Remote Sensing Technology for Smart Future","","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105829347&partnerID=40&md5=d0c340ae3375ccc7c02786a9fe61840e","SAR sensors are capable of capturing Earth images without obstructed by atmospheric effects such as clouds, fogs and weather conditions. Unlike optical imaging, SAR images are obtained by utilizing microwave propagation bands that are easily penetrate the atmosphere and reach the ground surface without any intervention. However, although many preprocessing steps are applied on raw SAR images, it is quite difficult for human interpreter to understand those images visually. In this work, we propose a domain transfer approach in which to convert SAR images into corresponding maps in order to make them more interpretable for end-point users. There have been similar studies using GANs to perform domain transfer on SAR images, however, yet no study conducted on to transfer SAR images into map pairs to our knowledge. © 2020 40th Asian Conference on Remote Sensing, ACRS 2019: ""Progress of Remote Sensing Technology for Smart Future"". All rights reserved.","Remote sensing; Synthetic aperture radar; Adversarial networks; Atmospheric effects; Domain transfers; GANs; Image transfer; Microwave propagation; Pre-processing step; SAR sensors; Radar imaging","Aerial maps; Aerial SAR sensor image; GANs; Image-to-image transfer","Conference paper","Final","","Scopus","2-s2.0-85105829347"
"Balaji Prabhu B.V.; Salian N.P.; Nikhil B.M.; Narasipura O.S.J.","Balaji Prabhu, B.V. (57202716108); Salian, Nikith P. (57223916625); Nikhil, B.M. (57209534063); Narasipura, Omkar Subbaram Jois (55626396900)","57202716108; 57223916625; 57209534063; 55626396900","Super-Resolution of Level-17 Images Using Generative Adversarial Networks","2021","Lecture Notes on Data Engineering and Communications Technologies","61","","","379","392","13","10.1007/978-981-33-4582-9_29","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106443712&doi=10.1007%2f978-981-33-4582-9_29&partnerID=40&md5=75752fd3e89aa1dc4645635fe587090f","The generative adversarial networks (GAN) deep learning models are being widely used in the field of image processing and its applications such as image generation, feature extraction, image recovery and image super-resolution to name a few. Image super-resolution has a board range of applications like satellite and aerial image analysis, medical image processing, compressed image/video enhancement, etc. This work implements an image super-resolution using generative adversarial network for super-resolution of level-17 low-resolution geospatial images obtained from Indian Remote Sensing (IRS) imagery. The results show that the generated super-resolution image can recuperate photo-realistic textures from low-resolution input pictures. The performance of the model is evaluated with qualitative measure indices such as structural similarity (SSIM) and peak signal-to-noise ratio (PSNR). The performance metric demonstrates that the model can generate images as close to that of the high-resolution image and it also has finer details. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Antennas; Deep learning; Image compression; Medical imaging; Optical resolving power; Remote sensing; Satellite imagery; Signal to noise ratio; Textures; Adversarial networks; Compressed images; High resolution image; Image super resolutions; Peak signal to noise ratio; Performance metrices; Satellite and aerial images; Structural similarity; Image enhancement","Deep learning; GAN; Indian remote sensing; Satellite image; Super-resolution","Book chapter","Final","","Scopus","2-s2.0-85106443712"
"Ning K.; Zhang Z.; Han K.; Han S.; Zhang X.","Ning, Keqing (36926065800); Zhang, Zhihao (57192639278); Han, Kai (57141420300); Han, Siyu (57226693080); Zhang, Xiqing (55662831100)","36926065800; 57192639278; 57141420300; 57226693080; 55662831100","Multi-Frame Super-Resolution Algorithm Based on a WGAN","2021","IEEE Access","9","","9452141","85839","85851","12","10.1109/ACCESS.2021.3088128","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112369897&doi=10.1109%2fACCESS.2021.3088128&partnerID=40&md5=20ec45b1c01d9c2afdda262c5f9ef38b","Image super-resolution reconstruction has been widely used in remote sensing, medicine and other fields. In recent years, due to the rise of deep learning research and the successful application of convolutional neural networks in the image field, the super-resolution reconstruction technology based on deep learning has also achieved great development. However, there are still some problems that need to be solved. For example, the current mainstream image super-resolution algorithms based on single or multiple frames pursue high performance indicators such as PSNR and SSIM, while the reconstructed image is relatively smooth and lacks many high-frequency details. It is not conducive to application in a real environment. To address such problem, this paper proposes a super-resolution reconstruction model of sequential images based on Generative Adversarial Networks (GAN). The proposed approach combines the registration module to fuse adjacent frames, effectively use the detailed information in multiple consecutive frames, and enhances the spatio-temporality of low-resolution images in sequential images. While the GAN was used to improve the effect of image high-frequency texture detail reconstruction, WGAN was introduced to optimize model training. The reconstruction results not only improved the PSNR and SSIM indexes but also reconstructed more high-frequency detail textures. Finally, in order to further improve the perception effect, an additional registration loss item RLT is introduced in the GAN network perception loss. Through extensive experiments, it shows that the model proposed in this paper effectively obtains the information between the sequence images. When the PSNR and SSIM indicators are improve, it can reconstruct better high-frequency texture details than the current advanced multi-frame algorithms. © 2013 IEEE.","Convolutional neural networks; Deep learning; Image reconstruction; Optical resolving power; Remote sensing; Textures; Adversarial networks; Image super resolutions; Image super-resolution reconstruction; Low resolution images; Performance indicators; Spatio temporalities; Super resolution algorithms; Super resolution reconstruction; Image enhancement","convolutional neural network; sequential images; Super-resolution reconstruction; Wasserstein generative adversarial network (WGAN)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112369897"
"Panagiotou E.; Chochlakis G.; Grammatikopoulos L.; Charou E.","Panagiotou, Emmanouil (57217281424); Chochlakis, Georgios (57217281247); Grammatikopoulos, Lazaros (16068804700); Charou, Eleni (6507509159)","57217281424; 57217281247; 16068804700; 6507509159","Generating elevation surface from a single RGB remotely sensed image using deep learning","2020","Remote Sensing","12","12","2002","","","","10.3390/rs12122002","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087000238&doi=10.3390%2frs12122002&partnerID=40&md5=9ed85b9d4bff82ed6c458fa5d78c05c5","Generating Digital Elevation Models (DEM) from satellite imagery or other data sources constitutes an essential tool for a plethora of applications and disciplines, ranging from 3D flight planning and simulation, autonomous driving and satellite navigation, such as GPS, to modeling water flow, precision farming and forestry. The task of extracting this 3D geometry from a given surface hitherto requires a combination of appropriately collected corresponding samples and/or specialized equipment, as inferring the elevation from single image data is out of reach for contemporary approaches. On the other hand, Artificial Intelligence (AI) and Machine Learning (ML) algorithms have experienced unprecedented growth in recent years as they can extrapolate rules in a data-driven manner and retrieve convoluted, nonlinear one-to-one mappings, such as an approximate mapping from satellite imagery to DEMs. Therefore, we propose an end-to-end Deep Learning (DL) approach to construct this mapping and to generate an absolute or relative point cloud estimation of a DEM given a single RGB satellite (Sentinel-2 imagery in this work) or drone image. The model has been readily extended to incorporate available information from the non-visible electromagnetic spectrum. Unlike existing methods, we only exploit one image for the production of the elevation data, rendering our approach less restrictive and constrained, but suboptimal compared to them at the same time. Moreover, recent advances in software and hardware allow us to make the inference and the generation extremely fast, even on moderate hardware. We deploy Conditional Generative Adversarial networks (CGAN), which are the state-of-the-art approach to image-to-image translation. We expect our work to serve as a springboard for further development in this field and to foster the integration of such methods in the process of generating, updating and analyzing DEMs. © 2020 by the authors.","Flight simulators; Flow of water; Mapping; Satellite imagery; Surveying; Three dimensional computer graphics; Adversarial networks; Digital elevation model; Electromagnetic spectra; Remotely sensed images; Satellite navigation; Software and hardwares; Specialized equipment; State-of-the-art approach; Deep learning","3D point cloud; Deep learning; Digital elevation models; Drones; Generative adversarial networks; Height maps; Remote sensing; Satellite imagery","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85087000238"
"Zheng C.; Fu Y.; Zhao Z.; Wang C.; Nie J.","Zheng, Chengyu (57221979496); Fu, Yu (57225870155); Zhao, Zian (57221981344); Wang, Chenglong (57221980427); Nie, Jie (56379566800)","57221979496; 57225870155; 57221981344; 57221980427; 56379566800","Imbalance satellite image colorization with semantic salience priors","2021","Proceedings of SPIE - The International Society for Optical Engineering","11720","","117201U","","","","10.1117/12.2589397","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100847274&doi=10.1117%2f12.2589397&partnerID=40&md5=737adcb3be78bafe82355f1cbd66f470","Considering the low spatial resolution of remote sensing images, it is unreliable to achieve accurate pixel-level color restoration. Inspirited by human perception, which is sensitive to salience features, in this paper, we propose an approach for colorization of remote sensing images with semantic salience priors. Firstly, based on the DCGAN architecture, we introduce the semantic salience prior, which is designed and learned from existing data set with semantic labels to supervise the training of the network. Then, to eliminate the distortion in foreground color caused by the overwhelming amount of marine or bare land backgrounds, we leverage the idea of focal loss to prevent the vast number of backgrounds from overwhelming the generator. Finally, we evaluate the proposed method on the NWPU-RESISC45 public data set. Both the evaluations and comparisons validate the proposed colorization approach is superior to the state-of-The-Art methods on remote sensing images.  © 2021 SPIE.","Remote sensing; Semantics; Color restoration; Human perception; Pixel level; Remote sensing images; Satellite images; Semantic labels; Spatial resolution; State-of-the-art methods; Image processing","Deep convolutional generative adversarial network; Focal loss; Salience; Satellite image colorization; Semantic priors","Conference paper","Final","","Scopus","2-s2.0-85100847274"
"Guo D.; Xia Y.; Luo X.","Guo, Dongen (36720716400); Xia, Ying (36626744600); Luo, Xiaobo (36562124600)","36720716400; 36626744600; 36562124600","Self-Supervised GANs with Similarity Loss for Remote Sensing Image Scene Classification","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9345971","2508","2521","13","10.1109/JSTARS.2021.3056883","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100811681&doi=10.1109%2fJSTARS.2021.3056883&partnerID=40&md5=3a81e9654d10ee1274b9c9b3628bcc3e","With the development of supervised deep neural networks, classification performance on existing remote sensing scene datasets has been markedly improved. However, supervised learning methods rely heavily on large-scale tagged examples to obtain a better prediction performance. The lack of large-scale tagged remote sensing scene images has become the primary bottleneck in scene classification. To deal with this issue, a novel scene classification method using self-supervised gated self-attention generative adversarial networks (GANs) with similarity loss is proposed. Specifically, the gated self-attention module is first introduced into GANs to focus on key scene areas and filter useless information for strengthening feature representations. Then, the pyramidal convolution block is introduced into the residual block of the discriminator to capture different levels of details in the image using different types of filters with varying sizes and depths for enhancing the feature representations of the discriminator. Additionally, a novel similarity loss item is integrated into the discriminator to leverage self-supervised learning. Besides, spectral normalization is introduced into both the generative network and discriminative network to stabilize training and enhance feature representations. The architecture of multilevel feature fusion is integrated into the discriminative network to achieve more discriminant representation. Experimental results on the AID and NWPU-RESISC45 datasets show that the proposed method achieves the best performance compared to the existing unsupervised classification methods.  © 2008-2012 IEEE.","Classification (of information); Deep learning; Deep neural networks; Image classification; Image enhancement; Learning systems; Supervised learning; Classification performance; Discriminative networks; Feature representation; Prediction performance; Remote sensing images; Spectral normalization; Supervised learning methods; Unsupervised classification; artificial neural network; data set; experimental study; image classification; remote sensing; satellite imagery; spectral analysis; Remote sensing","Gated self-attention (GAS) module; generative adversarial networks (GANs); pyramidal convolution (PyConv); remote sensing image scene classification; self-supervised learning; similarity loss; spectral normalization (SN)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100811681"
"Jafrasteh B.; Manighetti I.; Zerubia J.","Jafrasteh, B. (56026163300); Manighetti, I. (6603447302); Zerubia, J. (56211628500)","56026163300; 6603447302; 56211628500","Generative adversarial networks as a novel approach for tectonic fault and fracture extraction in high-resolution satellite and airborne optical images","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B3","","1219","1227","8","10.5194/isprs-archives-XLIII-B3-2020-1219-2020","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091131632&doi=10.5194%2fisprs-archives-XLIII-B3-2020-1219-2020&partnerID=40&md5=0cde74c70daf1f12ef83ba24af12124b","We develop a novel method based on Deep Convolutional Networks (DCN) to automate the identification and mapping of fracture and fault traces in optical images. The method employs two DCNs in a two players game: a first network, called Generator, learns to segment images to make them resembling the ground truth; a second network, called Discriminator, measures the differences between the ground truth image and each segmented image and sends its score feedback to the Generator; based on these scores, the Generator improves its segmentation progressively. As we condition both networks to the ground truth images, the method is called Conditional Generative Adversarial Network (CGAN). We propose a new loss function for both the Generator and the Discriminator networks, to improve their accuracy. Using two criteria and a manually annotated optical image, we compare the generalization performance of the proposed method to that of a classical DCN architecture, U-net. The comparison demonstrates the suitability of the proposed CGAN architecture. Further work is however needed to improve its efficiency. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Convolutional neural networks; Discriminators; Fracture; Geometrical optics; Image segmentation; Network architecture; Adversarial networks; Convolutional networks; Generalization performance; High resolution satellites; Its efficiencies; Loss functions; Segmented images; Tectonic faults; Image enhancement","Curvilinear feature extraction; Deep learning; Fault mapping; Generative adversarial networks; High resolution; Image processing; Remote sensing; Tectonic fault and fractures","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85091131632"
"Liang H.; Bao W.; Lei B.; Zhang J.; Qu K.","Liang, Hongbo (57221468243); Bao, Wenxing (35770357700); Lei, Bingbing (56192494200); Zhang, Jian (57225122136); Qu, Kewen (57192695275)","57221468243; 35770357700; 56192494200; 57225122136; 57192695275","Adaptive Neighborhood Strategy Based Generative Adversarial Network for Hyperspectral Image Classification","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324088","862","865","3","10.1109/IGARSS39084.2020.9324088","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102004110&doi=10.1109%2fIGARSS39084.2020.9324088&partnerID=40&md5=0d601f447acf6bba321a565eb11a1829","Hyperspectral image (HSI) is usually composed of hundreds of continuous bands, leading a challenge task for pixel-level classification owing to high-dimensional spectral features and insufficient labeled samples. In this paper, an adaptive neighborhood strategy based generative adversarial network with (AN-GAN) for semi-supervised HSI classification is proposed. The proposed AN-GAN approach firstly uses superpixel algorithm, e.g., simple linear iterative clustering (SLIC), to generate multiple spatially homogeneous regions. Furthermore, each superpixel is merged with its spectrally similar neighbor superpixels. Then, for the reconstructed superpixels, the limited labeled samples are used to train discriminator, and a large number of unlabeled samples are utilized to generate noise using sparse autoencoder and also used to train discriminator for purpose of improving discriminator performance. Experiments were conducted on both Pavia University and Indian Pines datasets, which show that AN-GAN could provide better classification performance comparing with state-of-the-art classification models. © 2020 IEEE.","Geology; Image classification; Iterative methods; Remote sensing; Spectroscopy; Superpixels; Adaptive neighborhood; Adversarial networks; Classification models; Classification performance; Iterative clustering; Similar neighbors; Spatially homogeneous; Unlabeled samples; Classification (of information)","generative adversarial network; Hyperspectral classification; neighborhood adaptive; semi-supervised learning; spectral-spatial features","Conference paper","Final","","Scopus","2-s2.0-85102004110"
"Rahnemoonfar M.; Yari M.; Paden J.","Rahnemoonfar, Maryam (54411066400); Yari, Masoud (16235210400); Paden, John (8338654000)","54411066400; 16235210400; 8338654000","Radar Sensor Simulation with Generative Adversarial Network","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323676","7001","7004","3","10.1109/IGARSS39084.2020.9323676","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099551523&doi=10.1109%2fIGARSS39084.2020.9323676&partnerID=40&md5=137a387ade331b310b8b6ca427a6a9bc","Significant resources have been spent in collecting and storing large and heterogeneous radar datasets during expensive Arctic and Antarctic fieldwork. The vast majority of data available is unlabeled, and the labeling process is both time-consuming and expensive. One possible alternative to the labeling process is the use of synthetically generated data with artificial intelligence. In this research, we evaluated the performance of synthetically generated snow radar images based on modified cycle-consistent adversarial networks. We conducted several experiments to test the quality of the generated radar imagery. Our experiments show a very good similarity between real and synthetic snow radar images. © 2020 IEEE.","Artificial intelligence; Geology; Large dataset; Remote sensing; Snow; Tracking radar; Adversarial networks; Radar datasets; Radar imagery; Radar sensors; Radar imaging","convolutional neural network; generative adversarial network; ice tracking; radar imagery","Conference paper","Final","","Scopus","2-s2.0-85099551523"
"Xue X.; Zhang X.; Li H.; Wang W.","Xue, Xiangyu (57219313682); Zhang, Xiangnan (57219320238); Li, Haibing (57219320302); Wang, Wenyong (7501758663)","57219313682; 57219320238; 57219320302; 7501758663","Research on GAN-based Image Super-Resolution Method","2020","Proceedings of 2020 IEEE International Conference on Artificial Intelligence and Computer Applications, ICAICA 2020","","","9182617","602","605","3","10.1109/ICAICA50127.2020.9182617","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092136503&doi=10.1109%2fICAICA50127.2020.9182617&partnerID=40&md5=6a7639aefe374e4024d997c3db7c0cd3","Super-Resolution (SR) refers to the reconstruction of high-resolution image from low-resolution image, which has important application value in object detection, medical imaging, satellite remote sensing and other fields. In recent years, with the rapid development of deep learning, the image super-resolution reconstruction method based on deep learning has made remarkable progress. In this paper, R-SRGAN (Residual Super-Resolution Generative Adversarial Networks) is used to build the model and realize image super-resolution. By adding residual blocks between adjacent convolutional layers of the GAN generator, more detailed information is retained. At the same time, the Wassertein distance is used as a loss function to enhance the training effect and achieve image super-resolution. © 2020 IEEE.","Deep learning; Image enhancement; Medical imaging; Object detection; Optical resolving power; Remote sensing; Adversarial networks; High resolution image; Image super resolutions; Image super-resolution reconstruction; Low resolution images; Satellite remote sensing; Super resolution; Training effects; Image reconstruction","Generative Adversarial Networks; Image Processing; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85092136503"
"Lu P.; Xu D.; Ren F.; Xu S.; Qiu T.; Peng R.","Lu, Pengjie (57218645854); Xu, Dalu (57218649424); Ren, Fu (55121193300); Xu, Shenghua (56181373400); Qiu, Tianqi (57203870046); Peng, Rui (57192161095)","57218645854; 57218649424; 55121193300; 56181373400; 57203870046; 57192161095","Auto-detection and Hiding of Sensitive Targets in Emergency Mapping Based on Remote Sensing Data; [应急遥感制图中敏感目标自动检测与隐藏方法]","2020","Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University","45","8","","1263","1272","9","10.13203/j.whugis20200131","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089886053&doi=10.13203%2fj.whugis20200131&partnerID=40&md5=a7cd04edabae299aa48bfeb97a9dbcc4","Objectives: Emergency remote sensing mapping can provide support for decision-making in disaster assessment or disaster relief, and therefore plays an important role in disaster response.Traditional emergency remote sensing mapping methods use the decryption algorithms based on manual retrieval and image editing tools when processing sensitive targets. Although the traditional methods can achieve target recognition, they are inefficient and cannot meet the immediate requirements of disaster relief, which are unable to be released or applied in time. The main objective is to propose a method for auto-detecting and hiding of sensitive targets in emergency remote sensing mapping to accelerate the rapid production and release emergency remote sensing products.Methods: Because of the huge size of remote sensing images, it is not realistic to directly hide sensitive targets. We propose a two-stage processing method automatic target detection and hiding of sensitive targets method, which consists of two neural network models: target detection model and generative adversarial network model. Firstly, Mask R-CNN, a well-known and effective target detection method, was used to detect sensitive targets from massive remote sensing data and generate target coordinates and Masks. Then, Deepfill model, one of GAN(generative adversarial networks) models, can ignore other normal areas and directly hide sensitive objects in the local area based on the coordinates and Masks information. The aircraft objects in the remote sensing image was used as an application example to verify the feasibility of our method, furthermore, we added the reconstruction of loss function, candidate frame optimization of region recommendation network, Mask optimization algorithm, and attention mechanism reconstruction based on the characteristics of the aircraft objects. Mask R-CNN model and Deepfill model have different training principles, so we trained and tuned them separately, and finally combine the trained models. We randomly extracted images from RSOD(remote sensing object detection) and DOTA(a large-scale dataset for object detection in aerial images) to form a new dataset. A total of 1 607 images were obtained for Mask R-CNN model training, and 9 502 images were used for Deepfill model training. All these samples are divided into training set and verification set according to the ratio of 0.8: 0.2. The performance of the Mask R-CNN model was evaluated by precision, recall rate, missing detect rate and F1-score; the performance evaluation indicators of the Deepfill model are PSNR(peak signal to noise ratio) and SSIM(structural similarity).Results: 46 images were extracted separately from the original dataset to test the performance of the trained models. In the target detect stage, the accuracy of the benchmark model was 98.13%, the recall rate was 44.21%, the missed detection rate was 55.79%, and the F1-score was 60.96%. Many targets were not detected. For comparison, the accuracy of our method reaches 94.65%, the recall rate reaches 81.89%, which is 85.23% higher than the benchmark model; the missed detection rate reaches 18.11%, which is 67.54% lower than the benchmark model; the F1-score reaches 87.81%, which is 44.05% higher than the benchmark model. In the inpainting stage, the average PSNR in this method reaches 32.26, and the average SSIM value is 0.98.Conclusions: In the proposed method, the recall rate and F1-score of aircraft targets in remote sensing images have been significantly improved, the inpainting processing effect is reasonable and natural, and the overall time of the emergency remote sensing mapping process is saved, indicating that the two-stage model works well. In the future, it can further expand the detection and processing of other sensitive targets, accelerate the production and release efficiency of disaster emergency response map products, and thus improve the ability of disaster prevention and relief. © 2020, Editorial Board of Geomatics and Information Science of Wuhan University. All right reserved.","Aircraft accidents; Antennas; Automotive industry; Convolutional neural networks; Decision making; Disaster prevention; Emergency services; Image enhancement; Large dataset; Mapping; Object detection; Object recognition; Processing; Signal to noise ratio; Statistical tests; Training aircraft; Application examples; Automatic target detection; Evaluation indicators; Massive remote sensing datum; PSNR (peak signal to noise ratio); Remote sensing images; Structural similarity; Two-stage processing; artificial neural network; decision making; disaster relief; mapping method; remote sensing; Remote sensing","Deepfill model; Emergency mapping based on remote sensing data; Mask R-CNN model; Sensitive target detection; Sensitive target hiding","Article","Final","","Scopus","2-s2.0-85089886053"
"Tang M.; Qu Y.; Qi H.","Tang, Maofeng (57200037401); Qu, Ying (57192066795); Qi, Hairong (7202348750)","57200037401; 57192066795; 7202348750","Hyperspectral Nonlinear Unmixing via Generative Adversarial Network","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324087","2404","2407","3","10.1109/IGARSS39084.2020.9324087","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101966527&doi=10.1109%2fIGARSS39084.2020.9324087&partnerID=40&md5=317eb6d0030a99855029b58f2291ea51","Hyperspectral nonlinear unmixing (HNU) is an extremely challenging problem as it is very difficult, if possible at all, to derive an explicit model to describe the underlying nonlinear mixing process. This paper gives the first attempt to tackle this problem by taking advantage of recent advances in deep learning, in specific, the development in generative adversarial network (GAN). The biggest contribution of GAN is that upon training, the network can generate samples with the same probabilistic distribution as that of the training samples, without explicitly knowing what the distribution actually is. Hence, we ask a similar question: can we unmix a hyperspectal image without explicitly knowing the nonlinear mixing model? In order to test this hypothesis, this paper proposes a data-driven supervised HNU method as compared to the traditional model-based approaches and uses a specific GAN framework, CycleGAN to solve the challenging nonlinear unmixing problem. We exploit the linkage between the cycle consistency loss used in CycleGAN and the spectral reconstruction loss used in traditional methods. We make the essential discovery that the usage of the cycle consistency loss enables the learning of the mixing and unmixing processes to be dependent on the training data only, without the need of an explicit mixing model. We refer to the proposed approach as CycleGAN unmixing net, or CGU net. Experimental results indicate that the proposed CGU net exhibits stable and competitive performance on different datasets as compared to traditional HNU methods that are model-based. © 2020 IEEE.","Deep learning; Geology; Mixing; Probability distributions; Adversarial networks; Competitive performance; Explicit modeling; Non-linear unmixing; Nonlinear mixing models; Probabilistic distribution; Spectral reconstruction; Traditional models; Remote sensing","Data Driven; Generative Network; Hyperspectral Image; Nonlinear Unmixing","Conference paper","Final","","Scopus","2-s2.0-85101966527"
"Han W.; Wang L.; Feng R.; Gao L.; Chen X.; Deng Z.; Chen J.; Liu P.","Han, Wei (57191570975); Wang, Lizhe (23029267900); Feng, Ruyi (55853730300); Gao, Lang (57196486321); Chen, Xiaodao (35247612500); Deng, Ze (55388245300); Chen, Jia (57216636841); Liu, Peng (57075315400)","57191570975; 23029267900; 55853730300; 57196486321; 35247612500; 55388245300; 57216636841; 57075315400","Sample generation based on a supervised Wasserstein Generative Adversarial Network for high-resolution remote-sensing scene classification","2020","Information Sciences","539","","","177","194","17","10.1016/j.ins.2020.06.018","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086823569&doi=10.1016%2fj.ins.2020.06.018&partnerID=40&md5=6c8aefc771f8eb6751fd29a56129be6d","As high-resolution remote-sensing (HRRS) images have become increasingly widely available, scene classification focusing on the smart classification of land cover and land use has also attracted more attention. However, mainstream methods encounter a severe problem in that many annotation samples are required to obtain an ideal model for scene classification. In the remote sensing community, there is no dataset with a comparative scale to ImageNet (which contains over 14 million images) to meet the sample requirements of the convolutional neural network (CNN)-based methods. In addition, labeling new images is both labor intensive and time consuming. To address these problems, we present a new generative adversarial network (GAN)-based remote-sensing image generation method (GAN-RSIGM) that can be applied to create high-resolution annotated samples for scene classification. In GAN-RSIGM, the Wasserstein distance is used to measure the difference between the generator distribution and the real data distribution. This addresses the problem of the gradient disappearing during sample generation, and distinctly promotes a generator distribution close to the real data distribution. An auxiliary classifier is added to the discriminator, guiding the generator to produce consistent and distinct images. With regard to the network structure, the discriminator and the generator are implemented by stacking residual blocks, which further stabilize the training process of the GAN-RSIGM. Extensive experiments were conducted to evaluate the proposed method with two public HRRS datasets. The experimental results demonstrated that the proposed method could achieve satisfactory performance for high-quality annotation sample generation, scene classification, and data augmentation. © 2020 Elsevier Inc.","Classification (of information); Convolutional neural networks; Land use; Adversarial networks; High resolution remote sensing; Network structures; Remote sensing images; Sample generations; Scene classification; Smart classification; Wasserstein distance; Remote sensing","Generative adversarial network; High-resolution remote sensing; Sample generation; Scene classification","Article","Final","","Scopus","2-s2.0-85086823569"
"Li Y.; Ku B.; Kim G.; Ahn J.-K.; Ko H.","Li, Yuanming (57202254201); Ku, Bonhwa (7007114814); Kim, Gwantae (57218709502); Ahn, Jae-Kwang (57214806947); Ko, Hanseok (35069749800)","57202254201; 7007114814; 57218709502; 57214806947; 35069749800","Seismic Signal Synthesis by Generative Adversarial Network with Gated Convolutional Neural Network Structure","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323670","3857","3860","3","10.1109/IGARSS39084.2020.9323670","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102017542&doi=10.1109%2fIGARSS39084.2020.9323670&partnerID=40&md5=79c4770c3f9474299c741ff7cf59ec28","Detecting earthquake events from seismic time series signal is a challenging task. Recently, detection methods based on machine learning have been developed to improve the accuracy and efficiency. However, accuracy of those methods rely on sufficient amount of high-quality training data. In many situations, the high-quality data is difficulty to obtain. We address and resolve this issue by using a Generative Adversarial Network (GAN) model for seismic signal synthesis. GAN already shows its powerful capability in generating high quality synthetic samples in multiple domains. In this paper, we propose a GAN model with gated CNN which can excellently capture sequential structure of seismic time series. We demonstrate its effectiveness via earthquake classification performance. The results show the synthetic data generated by our model indeed can improve the classification performance over the one trained with only real samples. © 2020 IEEE.","Earthquakes; Geology; Remote sensing; Seismic waves; Time series; Adversarial networks; Classification performance; Detection methods; Earthquake events; High quality data; Multiple domains; Sequential structure; Time series signals; Convolutional neural networks","data augmentation; deep learning; earthquake detection; generative adversarial network","Conference paper","Final","","Scopus","2-s2.0-85102017542"
"Yue H.; Cheng J.; Liu Z.; Chen W.","Yue, Haosong (37014128300); Cheng, Jiaxiang (57207733866); Liu, Zhong (57190886072); Chen, Weihai (35776127700)","37014128300; 57207733866; 57190886072; 35776127700","Remote-sensing image super-resolution using classifier-based generative adversarial networks","2020","Journal of Applied Remote Sensing","14","4","046514","","","","10.1117/1.JRS.14.046514","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098650782&doi=10.1117%2f1.JRS.14.046514&partnerID=40&md5=6fb617d08f5f403af88d154f6511eb61","The rapid development of the aerospace industry has significantly increased the demand for remote-sensing images with high resolution and quality. Generating images with expected resolution from the samples obtained by common acquisition devices is a challenging task as the trade-off between cost and efficiency must be considered. We propose a super-resolution (SR) algorithm especially for remote-sensing images that is based on generative adversarial networks optimized by a classifier, which is called classifier-based super-resolution generative adversarial network (CSRGAN). We hypothesize that the confidence scores of classification can be a critical factor for representing the features in target remote-sensing images. To sufficiently take this factor into account during training, we add the class-score as an error into the loss function in addition to mean square error and high-dimensional features extracted from deep neural networks. Then, the classifier is utilized for both better SR performance and more precise classification. The classifier-testing branch of our system can also be flexibly combined with other network architectures to optimize SR performance on remote-sensing images. We validate the model on the NWPU-RESISC45 dataset considering both SR and classification performance. The final analysis is also provided and shows that the proposed CSRGAN outperforms existing algorithms.  © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Aerospace industry; Classification (of information); Deep neural networks; Economic and social effects; Image classification; Mean square error; Network architecture; Optical resolving power; Acquisition device; Adversarial networks; Classification performance; Confidence score; Critical factors; High dimensional feature; Remote sensing images; Super resolution; Remote sensing","classifier; generative adversarial networks; remote-sensing image; super-resolution","Article","Final","","Scopus","2-s2.0-85098650782"
"Cira C.-I.; Manso-Callejo M.-Á.; Alcarria R.; Pareja T.F.; Sánchez B.B.; Serradilla F.","Cira, Calimanut-Ionut (57212240781); Manso-Callejo, Miguel-Ángel (36650985500); Alcarria, Ramón (54984227800); Pareja, Teresa Fernández (56263257500); Sánchez, Borja Bordel (57028472800); Serradilla, Francisco (6504126599)","57212240781; 36650985500; 54984227800; 56263257500; 57028472800; 6504126599","Generative learning for postprocessing semantic segmentation predictions: A lightweight conditional generative adversarial network based on pix2pix to improve the extraction of road surface areas","2021","Land","10","1","79","1","15","14","10.3390/land10010079","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099918577&doi=10.3390%2fland10010079&partnerID=40&md5=fc3705888cced6bb50457f7883c1cf42","Remote sensing experts have been actively using deep neural networks to solve extraction tasks in high-resolution aerial imagery by means of supervised semantic segmentation operations. However, the extraction operation is imperfect, due to the complex nature of geospatial objects, limitations of sensing resolution, or occlusions present in the scenes. In this work, we tackle the challenge of postprocessing semantic segmentation predictions of road surface areas obtained with a state-of-the-art segmentation model and present a technique based on generative learning and image-to-image translations concepts to improve these initial segmentation predictions. The proposed model is a conditional Generative Adversarial Network based on Pix2pix, heavily modified for computational efficiency (92.4% decrease in the number of parameters in the generator network and 61.3% decrease in the discriminator network). The model is trained to learn the distribution of the road network present in official cartography, using a novel dataset containing 6784 tiles of 256 × 256 pixels in size, covering representative areas of Spain. Afterwards, we conduct a metrical comparison using the Intersection over Union (IoU) score (measuring the ratio between the overlap and union areas) on a novel testing set containing 1696 tiles (unseen during training) and observe a maximum increase of 11.6% in the IoU score (from 0.6726 to 0.7515). In the end, we conduct a qualitative comparison to visually assess the effectiveness of the technique and observe great improvements with respect to the initial semantic segmentation predictions. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Conditional Generative Adversarial Network; Generative learning; Postprocessing semantic segmentation predictions; Road extraction; Road surface areas","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099918577"
"Tang W.; Deng C.; Han Y.; Huang Y.; Zhao B.","Tang, Wei (57219250261); Deng, Chenwei (25958671000); Han, Yuqi (57195983797); Huang, Yun (57218183818); Zhao, Baojun (7403059245)","57219250261; 25958671000; 57195983797; 57218183818; 7403059245","SRARNet: A Unified Framework for Joint Superresolution and Aircraft Recognition","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9254000","327","336","9","10.1109/JSTARS.2020.3037225","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098793454&doi=10.1109%2fJSTARS.2020.3037225&partnerID=40&md5=c78d7f87941ced8270ff13be6a7ce407","Aircraft recognition in high-resolution remote sensing images has rapidly progressed with the advance of convolutional neural networks (CNNs). However, the previous CNN-based methods may not work well for recognizing aircraft in low-resolution remote sensing images because the blurred aircraft in these images offer insufficient details to distinguish them from similar types of targets. An intuitive solution is to introduce superresolution preprocessing. However, conventional superresolution methods mainly focus on reconstructing natural images with detailed texture rather than constructing a high-resolution object with strong discriminative information for the recognition task. To address these problems, we propose a unified framework for joint superresolution and aircraft recognition (Joint-SRARNet) that tries to improve the recognition performance by generating discriminative, high-resolution aircraft from low-resolution remote sensing images. Technically, this network integrates superresolution and recognition tasks into the generative adversarial network (GAN) framework through a joint loss function. The generator is constructed as a joint superresolution and refining subnetwork that can upsample small blurred images into high-resolution ones and restore high-frequency information. In the discriminator, we introduce a new classification loss function that forces the discriminator to distinguish between real and fake images while recognizing the type of aircraft. In addition, the classification loss function is back-propagated to the generator to obtain high-resolution images with discriminative information for easier recognition. Extensive experiments on the challenging multitype aircraft of remote sensing images (MTARSI) dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a small blurred image and significant improvement in the recognition performance. To our knowledge, this is the first work on joint superresolution and aircraft recognition tasks.  © 2008-2012 IEEE.","Aircraft; Classification (of information); Image reconstruction; Optical resolving power; Remote sensing; Textures; Vehicle performance; Adversarial networks; High resolution image; High resolution remote sensing images; High-frequency informations; Remote sensing images; Super resolution; Superresolution methods; Unified framework; aerial photography; aircraft; artificial neural network; back propagation; image analysis; image resolution; remote sensing; Image enhancement","Aircraft recognition; multitask GAN; superresolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85098793454"
"Song J.; Li J.; Chen H.; Wu J.","Song, Jieqiong (57221595332); Li, Jun (56023443800); Chen, Hao (57056710000); Wu, Jiangjiang (36976260700)","57221595332; 56023443800; 57056710000; 36976260700","MapGen-GAN: A Fast Translator for Remote Sensing Image to Map Via Unsupervised Adversarial Learning","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9316788","2341","2357","16","10.1109/JSTARS.2021.3049905","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099603124&doi=10.1109%2fJSTARS.2021.3049905&partnerID=40&md5=f57da84b8918bdf7812c95da095bcc90","Map is an essential medium for people to understand our changing planet. Recently, research on generating and updating maps through remote sensing images has been an important and challenging task in geographic information. Traditional methods for map generation are time-consuming and labor-intensive. Besides, most supervised learning methods for map generation lack labeled training samples. It is challenging to generate maps quickly and efficiently for emergency rescue operations such as earthquakes, fire disasters, or tsunami. In this article, we propose an unsupervised domain mapping model based on adversarial learning called MapGen-GAN. MapGen-GAN is a generative adversarial network (GAN) that can do end-to-end translation from remote sensing images to general map quickly, and trained with no human annotation data. In order to improve the fidelity and the geometry precision of generated maps, we employ circularity-consistency and geometrical-consistency constraints as a part of the loss function of the proposed model. And then, an improved residual block Unet is designed and adopted as the generator of MapGen-GAN to capture the geographic structure information of buildings, roads, and topography outlines under different resolutions in the map generation. By applying the proposed model to two distinct datasets, experiments demonstrate that our model can generate maps efficiently and quickly and outperform the state-of-the-art approaches. © 2008-2012 IEEE.","Learning systems; Topography; Adversarial learning; Consistency constraints; Different resolutions; Geographic information; Geographic structures; Remote sensing images; State-of-the-art approach; Supervised learning methods; experimental study; image analysis; image processing; mapping method; remote sensing; unsupervised classification; Remote sensing","Adversarial learning; map generation; remote sensing images; unsupervised domain mapping","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099603124"
"Du W.-L.; Zhou Y.; Zhao J.; Tian X.; Yang Z.; Bian F.","Du, Wen-Liang (55265123100); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Tian, Xiaolin (7202380154); Yang, Zhi (57284929100); Bian, Fuqiang (57202867285)","55265123100; 35480110700; 57138970300; 7202380154; 57284929100; 57202867285","Exploring the Potential of Unsupervised Image Synthesis for SAR-Optical Image Matching","2021","IEEE Access","9","","9427486","71022","71033","11","10.1109/ACCESS.2021.3079327","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105853208&doi=10.1109%2fACCESS.2021.3079327&partnerID=40&md5=3ebbd56086511abe8160d0a847102a35","We consider SAR-optical image matching problems, where correspondences are acquired from a pair of SAR and optical images. Recent methods for such a problem typically simplify the SAR-optical image matching to the SAR-SAR or optical-optical image matchings using supervised-image-synthesis methods. However, training supervised-image-synthesis needs plenty of aligned SAR-optical image pairs while gathering sufficient amounts of aligned multi-modal image pairs is challenging in remote sensing. In this work, we investigate the applicability of unsupervised-image-synthesis for SAR-optical image matching such that the unaligned SAR-optical images could be used. To this end, we apply feature matching loss to a well known unsupervised-image-synthesis method, i.e., CycleGAN, to enforce the feature matching consistency. Moreover, we develop a shared-matching-strategy to improve the results of SAR-optical image matching further. Qualitative comparisons against CycleGAN, StarGAN, and DualGAN demonstrate the superiority of our approach. Quantitative results show that, compared with CycleGAN, StarGAN, and DualGAN, our method obtains at least 2.6 times more qualified SAR-optical matchings.  © 2013 IEEE.","Geometrical optics; Image enhancement; Image matching; Remote sensing; Feature matching; Image synthesis; Matching problems; Matchings; Multi-modal image; Optical image; Quantitative result; Radar imaging","generative adversarial networks (GANs); Image matching; synthetic aperture radar (SAR); unsupervised-image-synthesis","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105853208"
"Wu G.; Ma X.; Huang K.; Guo H.","Wu, Guoqiang (57216902267); Ma, Xiangsen (57216901323); Huang, Kun (57216895575); Guo, Haolun (57216898306)","57216902267; 57216901323; 57216895575; 57216898306","Remote Sensing Image Enhancement Technology of UAV Based on Improved GAN","2020","Lecture Notes in Electrical Engineering","628 LNEE","","","703","709","6","10.1007/978-981-15-4163-6_84","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085177054&doi=10.1007%2f978-981-15-4163-6_84&partnerID=40&md5=9393be87c72cac93b5e673e0363e7541","In view of the problem that the remote sensing image of UAV is greatly affected by the weather, this paper proposes a UAV remote sensing image enhancement technology based on improved GAN. Firstly, the noise feature is extracted by the noise feature extraction network based on convolutional neural network (CNN). Secondly, using the noise features extracted by CNNs, the generation of GAN for image enhancement is constructed. For the problem of large storage overhead, low computational efficiency and pixel block size limiting the size of the perceptual region, this paper introduces a full CNN model (FCN) as a generation network. Finally, a data set for training is constructed using image enhancement techniques based on damaged sample learning. The experimental results show that the enhanced algorithm in this paper shows excellent performance in remote sensing images of UAV. © 2020, Springer Nature Singapore Pte Ltd.","Computational efficiency; Convolutional neural networks; Digital storage; Remote sensing; Unmanned aerial vehicles (UAV); Block sizes; CNN models; Network-based; Noise features; Remote sensing images; Sample learning; Storage overhead; UAV remote sensing; Image enhancement","Enhancement technology; GAN (Generative Adversarial Networks); UAV remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85085177054"
"Wu Z.; Li J.; Wang Y.; Hu Z.; Molinier M.","Wu, Zhaocong (15023850900); Li, Jun (55902669000); Wang, Yisong (57219606030); Hu, Zhongwen (55630272400); Molinier, Matthieu (22234853700)","15023850900; 55902669000; 57219606030; 55630272400; 22234853700","Self-Attentive Generative Adversarial Network for Cloud Detection in High Resolution Remote Sensing Images","2020","IEEE Geoscience and Remote Sensing Letters","17","10","8924781","1792","1796","4","10.1109/LGRS.2019.2955071","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087502582&doi=10.1109%2fLGRS.2019.2955071&partnerID=40&md5=69a66984ca57ac263a3552029b8ce596","Cloud detection is an important step in the processing of remote sensing images. Most methods based on convolutional neural networks (CNNs) for cloud detection require pixel-level labels, which are time-consuming and expensive to annotate. To overcome this challenge, this letter proposes a novel semisupervised algorithm for cloud detection by training a self-attentive generative adversarial network (SAGAN) to extract the feature difference between cloud images and cloud-free images. Our main idea is to introduce visual attention into the process of generating 'real' cloud-free images. The training of SAGAN is based on three guiding principles: expansion of attention maps of cloud regions which will be replaced with translated cloud-free images, reduction of attention maps to coincide with cloud boundaries, and optimization of a self-attentive network to handle the extreme cases. The inputs for SAGAN training are the images and image-level labels, which are easier, cheaper, and more time-saving than the existing methods based on CNN. To test the performance of SAGAN, experiments are conducted on the Sentinel-2A Level 1C image data. The results show that the proposed method achieves very promising results with only the image-level labels of training samples.  © 2004-2012 IEEE.","Behavioral research; Convolutional neural networks; Remote sensing; Adversarial networks; Cloud detection; Feature differences; Guiding principles; High resolution remote sensing images; Remote sensing images; Semi-supervised algorithm; Visual Attention; computer system; detection method; image resolution; optimization; pixel; remote sensing; satellite imagery; Sentinel; Image processing","Cloud detection; deep learning (DL); generative adversarial network (GAN); remote sensing; self-attention","Article","Final","","Scopus","2-s2.0-85087502582"
"Ahn S.; Kim S.; Do J.; Park J.; Kang J.","Ahn, Sangho (57196286842); Kim, Sehyeong (57221378831); Do, Jinwoo (57221383293); Park, Jaehyeong (57221372426); Kang, Juyoung (14056159800)","57196286842; 57221378831; 57221383293; 57221372426; 14056159800","Cloud Removal on Satellite Image using Transfer Learning based Generative Adversarial Network","2020","International Conference on ICT Convergence","2020-October","","9289278","203","205","2","10.1109/ICTC49870.2020.9289278","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098998418&doi=10.1109%2fICTC49870.2020.9289278&partnerID=40&md5=d82972ea3a732050f65189497d15f8ca","Satellite Image Processing (SIP) is important in both academic and practical aspects because it has a wide range of applications. However, since the collected Optical Remote Sensing images often contain cloudy images, it is difficult to extract complete information from satellite image data. Therefore, cloud removal from satellite imagery is important to rethink the efficiency of satellite image processing. Therefore, in this study, we propose a methodology for pre-learning the Generator based on U-net and applying Generative Adverserial Network to the satellite image data of (Cloudy, non-Cloudy) pairs collected based on Google Earth engine. This solves the quantitative problem of data that it is not easy to obtain a usable data set due to weather problems, and it will show that the pre-learning results learned from abundant data in the SIP field are effective.  © 2020 IEEE.","Internet protocols; Meteorological problems; Optical data processing; Remote sensing; Satellite imagery; Transfer learning; Adversarial networks; Cloud removal; Complete information; Google earths; Optical remote sensing; Satellite image datas; Satellite image processing; Satellite images; Image processing","cloud removal; generative adversarial network; land cover classification; satellite image processing; transfer learning","Conference paper","Final","","Scopus","2-s2.0-85098998418"
"Courtrai L.; Pham M.-T.; Friguet C.; Lefevre S.","Courtrai, Luc (6507861482); Pham, Minh-Tan (56070990300); Friguet, Chloe (35071291100); Lefevre, Sebastien (57203070803)","6507861482; 56070990300; 35071291100; 57203070803","Small Object Detection from Remote Sensing Images with the Help of Object-Focused Super-Resolution Using Wasserstein GANs","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323236","260","263","3","10.1109/IGARSS39084.2020.9323236","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101963148&doi=10.1109%2fIGARSS39084.2020.9323236&partnerID=40&md5=8a904d6ebf4966e659c174dfc6e1c802","In this paper, we investigate and improve the use of a super-resolution approach to benefit the detection of small objects from aerial and satellite remote sensing images. The main idea is to focus the super-resolution on target objects within the training phase. Such a technique requires a reduced number of network layers depending on the desired scale factor and the reduced size of the target objects. The learning of our super-resolution network is performed using deep residual blocks integrated in a Wasserstein Generative adversarial network. Then, detection task is performed by exploiting two state-of-the-art detectors including Faster-RCNN and YOLOv3. Experiments were conducted on small vehicle detection from both aerial and satellite images from the VEDAI and xView data sets. Results showed that object-focused super-resolution improves the detection performance and facilitates the transfer learning from one data set to another. © 2020 IEEE.","Antennas; Geology; Image enhancement; Network layers; Optical resolving power; Remote sensing; Small satellites; Transfer learning; Adversarial networks; Detection performance; Detection tasks; Remote sensing images; Satellite images; Satellite remote sensing; Small object detection; Super resolution; Object detection","deep learning; remote sensing imagery; Small object detection; super-resolution; Wasserstein GANs","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101963148"
"Lv N.; Ma H.; Chen C.; Pei Q.; Zhou Y.; Xiao F.; Li J.","Lv, Ning (57220505216); Ma, Hongxiang (57222242244); Chen, Chen (56640091900); Pei, Qingqi (15763462800); Zhou, Yang (35209382100); Xiao, Fenglin (57222239228); Li, Ji (57218474499)","57220505216; 57222242244; 56640091900; 15763462800; 35209382100; 57222239228; 57218474499","Remote Sensing Data Augmentation Through Adversarial Training","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324263","2511","2514","3","10.1109/IGARSS39084.2020.9324263","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102016343&doi=10.1109%2fIGARSS39084.2020.9324263&partnerID=40&md5=5004209da409dca7f1526b0c637c8c82","In this paper, a Generative Adversarial Network(GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangsu province in China, i.e., D-sGAN(Deeply-supervised GAN). At First, to modulate the layer activations, a down-sampling scheme is designed based on the segmentation map. Then, the architecture of the generator is UNet++ with the proposed down-sampling module. Next, the generator of this net is deeply supervised by the discriminator using deep Convolutional Neural Network(CNN). This paper further proved that the proposed down-sampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with a better Fully Convolutional Networks Score(FCN-Score) compared to the GoGAN, SimGAN and CycleGAN models. © 2020 IEEE.","Convolution; Convolutional neural networks; Deep neural networks; Geology; Image enhancement; Image segmentation; Semantics; Signal sampling; Adversarial networks; Convolutional networks; Data augmentation; Numerical results; Remote sensing data; Remote sensing images; Segmentation map; Semantic information; Remote sensing","data augmentation; deep supervision; down-sampling; GAN","Conference paper","Final","","Scopus","2-s2.0-85102016343"
"Jiang Z.; Ma Y.; Yang J.","Jiang, Zongchen (57216491304); Ma, Yi (35220028600); Yang, Junfang (57192705519)","57216491304; 35220028600; 57192705519","Inversion of the thickness of crude oil film based on an og-cnn model","2020","Journal of Marine Science and Engineering","8","9","653","1","21","20","10.3390/jmse8090653","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092795595&doi=10.3390%2fjmse8090653&partnerID=40&md5=231403a20e2146f7fc65b570339c2931","In recent years, marine oil spill accidents have occurred frequently, seriously endangering marine ecological security. It is highly important to protect the marine ecological environment by carrying out research on the estimation of sea oil spills based on remote sensing technology. In this paper, we combine deep learning with remote sensing technology and propose an oil thickness inversion generative adversarial and convolutional neural network (OG-CNN) model for oil spill emergency monitoring. The model consists of a self-expanding module for the oil film spectral feature data and an oil film thickness inversion module. The feature data self-expanding module can automatically select spectral feature intervals with good spectral separability based on the measured spectral data and then expand the number of samples using a generative adversarial network (GAN) to enhance the generalization of the model. The oil film thickness inversion module is based on a one-dimensional convolutional neural network (1D-CNN). It extracts the characteristics of the spectral feature data of oil film with different thicknesses, and then accurately inverts the oil film’s absolute thickness. In this study, emulsification was not a factor considered, the results show that the absolute oil thickness inversion accuracy of the OG-CNN model proposed in this paper can reach 98.12%, the coefficient of determination can reach 0.987, and the mean deviation remains within ±0.06% under controlled experimental conditions. In the model stability test, the model maintains relatively stable inversion results under the interference of random Gaussian noise. The accuracy of the oil film thickness inversion result remains above 96%, the coefficient of determination can reach 0.973, and the mean deviation is controlled within ±0.6%, which indicates excellent robustness. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","","1D-CNN; Absolute thickness inversion; Crude oil film; Deep learning; GAN; Remote sensing","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092795595"
"Zheng K.; Wei M.; Sun G.; Anas B.; Li Y.","Zheng, Kun (57072537200); Wei, Mengfei (57683694800); Sun, Guangmin (8431278000); Anas, Bilal (57211069206); Li, Yu (57214959189)","57072537200; 57683694800; 8431278000; 57211069206; 57214959189","Using vehicle synthesis generative adversarial networks to improve vehicle detection in remote sensing images","2019","ISPRS International Journal of Geo-Information","8","9","390","","","","10.3390/ijgi8090390","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072573692&doi=10.3390%2fijgi8090390&partnerID=40&md5=f57165d2787a747dea2e2c6df41cae6d","Vehicle detection based on very high-resolution (VHR) remote sensing images is beneficial in many fields such as military surveillance, traffic control, and social/economic studies. However, intricate details about the vehicle and the surrounding background provided by VHR images require sophisticated analysis based on massive data samples, though the number of reliable labeled training data is limited. In practice, data augmentation is often leveraged to solve this conflict. The traditional data augmentation strategy uses a combination of rotation, scaling, and flipping transformations, etc., and has limited capabilities in capturing the essence of feature distribution and proving data diversity. In this study, we propose a learning method named Vehicle Synthesis Generative Adversarial Networks (VS-GANs) to generate annotated vehicles from remote sensing images. The proposed framework has one generator and two discriminators, which try to synthesize realistic vehicles and learn the background context simultaneously. The method can quickly generate high-quality annotated vehicle data samples and greatly helps in the training of vehicle detectors. Experimental results show that the proposed framework can synthesize vehicles and their background images with variations and different levels of details. Compared with traditional data augmentation methods, the proposed method significantly improves the generalization capability of vehicle detectors. Finally, the contribution of VS-GANs to vehicle detection in VHR remote sensing images was proved in experiments conducted on UCAS-AOD and NWPU VHR-10 datasets using up-to-date target detection frameworks. © 2019 by the authors.","","Data augmentation; Deep learning; Generative adversarial network; Remote sensing; Vehicle detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85072573692"
"Bao X.; Pan Z.; Liu L.; Lei B.","Bao, Xianjie (57193348223); Pan, Zongxu (54788169800); Liu, Lei (56194753300); Lei, Bin (14063767500)","57193348223; 54788169800; 56194753300; 14063767500","SAR Image Simulation by Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899286","9995","9998","3","10.1109/IGARSS.2019.8899286","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077719203&doi=10.1109%2fIGARSS.2019.8899286&partnerID=40&md5=11336d2102cfd308dbe0511b02bfdbc4","SAR image simulation plays an important role in the process of SAR target interpretation and recognition, especially when the number of SAR images is limited. Due to the restriction of acquisition process, the numbers of SAR target images are always insufficient. The traditional SAR image simulation, which is based on calculation of electromagnetic theory, is easily to be affected by parameter distortion due to the lack of joint optimization. Consequently, it makes a big effect on the quality of the simulated images. This paper presents a novel approach, end-to-end models, to simulate the desired images from the SAR image database. A series of generative adversarial networks include DCGAN, weight clipping WGAN and WGAN with gradient penalty are optimized and applied to generate typical SAR target images. Three kinds of network structures are used, include structure of DCGAN, newly proposed structure of four residual blocks networks and Resnet. Experimental results show that the proposed novel method is not only efficient for SAR image simulation, but also can generate excellent SAR images. Furthermore, we analysis the results and the characteristics of different networks, which pave a good way for SAR image simulation based on artificial intelligence method. © 2019 IEEE.","Artificial intelligence; Electric fields; Geology; Image processing; Information dissemination; Radar target recognition; Remote sensing; Synthetic aperture radar; Acquisition process; Adversarial networks; Artificial intelligence methods; Electromagnetic theories; End-to-end models; Image simulations; Joint optimization; Network structures; Radar imaging","generative adversarial networks; image processing; image simulation; synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85077719203"
"Shi Y.; Li Q.; Zhu X.X.","Shi, Yilei (55495784300); Li, Qingyu (57208661005); Zhu, Xiao Xiang (55696622200)","55495784300; 57208661005; 55696622200","BFGAN-building footprint extraction from satellite images","2019","2019 Joint Urban Remote Sensing Event, JURSE 2019","","","8809048","","","","10.1109/JURSE.2019.8809048","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072051590&doi=10.1109%2fJURSE.2019.8809048&partnerID=40&md5=557adae56e282766e41ca4dec6647fb1","Building footprint information is an essential ingredient for 3-D reconstruction of urban models. The automatic generation of building footprints from satellite images presents a considerable challenge due to the complexity of building shapes. In this work, we have proposed improved generative adversarial networks (GANs) for the automatic generation of building footprints from satellite images. We used a conditional GAN with a cost function derived from the Wasserstein distance and added a gradient penalty term. The achieved results indicated that the proposed method can significantly improve the quality of building footprint generation compared to conditional generative adversarial networks, the U-Net, and other networks. In addition, our method nearly removes all hyperparameter tuning. © 2019 IEEE.","Buildings; Cost functions; Remote sensing; Satellites; Three dimensional computer graphics; 3D reconstruction; Adversarial networks; Automatic Generation; Building footprint; Hyper-parameter; Penalty term; Satellite images; Wasserstein distance; Image enhancement","building footprint; conditional generative adversarial networks (CGANs); generative adversarial networks (GANs); Wasserstein generative adversarial networks (WGANs)","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85072051590"
"Zhang Y.; Xiong Z.; Zang Y.; Wang C.; Li J.; Li X.","Zhang, Yang (57207764722); Xiong, Zhangyue (57208757140); Zang, Yu (34977976000); Wang, Cheng (36990982800); Li, Jonathan (57235557700); Li, Xiang (57192491402)","57207764722; 57208757140; 34977976000; 36990982800; 57235557700; 57192491402","Topology-aware road network extraction via Multi-supervised Generative Adversarial Networks","2019","Remote Sensing","11","9","1017","","","","10.3390/rs11091017","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065707958&doi=10.3390%2frs11091017&partnerID=40&md5=248a6d1e001cfc1358cb0b4ca4f0ed40","Road network extraction from remote sensing images has played an important role in various areas. However, due to complex imaging conditions and terrain factors, such as occlusion and shades, it is very challenging to extract road networks with complete topology structures. In this paper, we propose a learning-based road network extraction framework via a Multi-supervised Generative Adversarial Network (MsGAN), which is jointly trained by the spectral and topology features of the road network. Such a design makes the network capable of learning how to ""guess"" the aberrant road cases, which is caused by occlusion and shadow, based on the relationship between the road region and centerline; thus, it is able to provide a road network with integrated topology. Additionally, we also present a sample quality measurement to efficiently generate a large number of training samples with a little human interaction. Through the experiments on images from various satellites and the comprehensive comparisons to state-of-the-art approaches on the public datasets, it is demonstrated that the proposed method is able to provide high-quality results, especially for the completeness of the road network. © 2019 by the authors.","Extraction; Image processing; Motor transportation; Remote sensing; Topology; Adversarial networks; Centerline extraction; Comprehensive comparisons; Integrated topologies; Remote sensing images; Road network extraction; State-of-the-art approach; Topology reconstruction; Roads and streets","Multi-supervised generative adversarial network; Road centerline extraction; Road topology reconstruction","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85065707958"
"Gu J.; Zhang Y.; Zhang W.; Yu H.; Wang S.; Wang Y.; Wang L.","Gu, Jun (57217635042); Zhang, Yue (57267634500); Zhang, Wenkai (57194594257); Yu, Hongfeng (57207947647); Wang, Siyue (57204970785); Wang, Yaoling (57213199984); Wang, Lei (57268131600)","57217635042; 57267634500; 57194594257; 57207947647; 57204970785; 57213199984; 57268131600","Aerial image and map synthesis using generative adversarial networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900222","9803","9806","3","10.1109/IGARSS.2019.8900222","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102803022&doi=10.1109%2fIGARSS.2019.8900222&partnerID=40&md5=60d2f62911e891fd54a943ed7148325b","Accurate automatic conversion between aerial images and maps is a valuable and challenging task in computer vision and computer graphics. Deep convolutional neural networks (CNN) have achieved promising results on this task but the results accuracy is not ideal. In this paper, we propose a solution to improve the precision and quality of the transforming results. The core learning method is based on generative adversarial networks (GANs). A novel generator and a multi-scale discriminator are introduced in our network. The generator operates at the progressive method to gurantee the spatial consistency between the inputs and outputs, and our multi-scale discriminator focuses on increasing the capacity of the network and guides the generator to generate better results. In particular, our architecture can also be used as a general neural network for style translation. Analytic experiments on the aerial-to-map dataset show that our network outperforms the existing method, advancing both accuracy and visual appearance. ©2019 IEEE","Antennas; Computer graphics; Convolutional neural networks; Deep neural networks; Image processing; Learning systems; Remote sensing; Adversarial networks; Aerial images; Automatic conversion; Learning methods; Spatial consistency; Visual appearance; Discriminators","Aerial photos; Generative adversarial network; Image-to-image translation","Conference paper","Final","","Scopus","2-s2.0-85102803022"
"Liu C.; Ma J.; Tang X.; Zhang X.; Jiao L.","Liu, Chao (57203997823); Ma, Jingjing (35389074700); Tang, Xu (57020306700); Zhang, Xiangrong (55802358000); Jiao, Licheng (7102491544)","57203997823; 35389074700; 57020306700; 55802358000; 7102491544","Adversarial hash-code learning for remote sensing image retrieval","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900431","4324","4327","3","10.1109/IGARSS.2019.8900431","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083450414&doi=10.1109%2fIGARSS.2019.8900431&partnerID=40&md5=0ad10221caaaf3a1c886d219f7187565","Hashing, a useful solution for approximate nearest neighbor (ANN) search, is popular for large-scale image retrieval. In this paper, we presents a deep supervised hashing model for remote sensing image retrieval (RSIR) in the framework of generative adversarial networks (GAN), named GAN-assist Hashing (GAAH). First, to learn the compact and useful hash codes from the images, we define a novel loss function for the generator. The loss function mainly consists of classification, similarity, and bits entropy terms. The classification term makes the hash code is discriminative, the similarity term constrains the binary code is similarity preserving, and the bits entropy term assures the learned code is low-error in the quantization. Second, we construct the unique “true” matrix with the uniform distribution as the input of discriminator to limit the leaned hash codes are bit balanced. The final hash code is learned by a minimax optimization. The positive experimental results on a ground-truth remote sensing image archive validate the usefulness of our GAAH model. Compare with the popular deep hashing methods, our GAAH achieves improved performance. ©2019 IEEE","Entropy; Hash functions; Image retrieval; Nearest neighbor search; Adversarial networks; Approximate nearest neighbors (ANN); Hashing method; Minimax optimization; Remote sensing image retrieval; Remote sensing images; Similarity preserving; Uniform distribution; Remote sensing","Generative adversarial networks; Hashing; Remote sensing image retrieval","Conference paper","Final","","Scopus","2-s2.0-85083450414"
"Wang G.; Dong G.; Li H.; Han L.; Tao X.; Ren P.","Wang, Guangxing (57213191777); Dong, Guoshuai (57208247636); Li, Hui (57207879663); Han, Lirong (57203552486); Tao, Xuanwen (57205509060); Ren, Peng (25960361900)","57213191777; 57208247636; 57207879663; 57203552486; 57205509060; 25960361900","Remote Sensing Image Synthesis via Graphical Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898915","10027","10030","3","10.1109/IGARSS.2019.8898915","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077676346&doi=10.1109%2fIGARSS.2019.8898915&partnerID=40&md5=3895a5ecd138121cbdc7d04b7c58f859","We explore the use of graphical generative adversarial networks (Graphical-GAN) for synthesizing remote sensing images. The model is probabilistic graphical based generative adversarial networks (GAN). It pairs a generative network G with a recognition network R. Both of them are adversarially trained with a discriminative network D. Particularly, R is employed to infer the underlying causal relationships among both observed and latent variables from real remote sensing images. The advantages of the Graphical-GAN for synthesizing multiple categories of remote sensing images are two fold. Firstly, it considers the underlying causal relationships and captures the true data distribution of remote sensing images. Secondly, the adversarial learning generates synthetic sensing images that are similar to real ones with slight differences. Our remote sensing image synthesis scheme paves a promising way for remote sensing dataset augmentation, which is an effective means of improving the accuracy of learning models. Experimental results with high Inception Scores (IS) validate the effectiveness of the Graphical-GAN for remote sensing image synthesis. © 2019 IEEE.","Geology; Image enhancement; Adversarial learning; Adversarial networks; Causal relationships; Data distribution; Discriminative networks; Latent variable; Learning models; Remote sensing images; Remote sensing","Graphical Generative Adversarial Networks; Remote Sensing Image Synthesis","Conference paper","Final","","Scopus","2-s2.0-85077676346"
"","","","10th International Conference on Image and Graphics, ICIG 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11903 LNCS","","","","","2248","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076856024&partnerID=40&md5=2c95f5dd23bc9c8f8079fa65a7a6ed77","The proceedings contain 183 papers. The special focus in this conference is on Image and Graphics. The topics include: Robust Dynamic 3D Shape Measurement with Hybrid Fourier-Transform Phase-Shifting Profilometry; an Improved Clustering Method for Multi-view Images; common Subspace Based Low-Rank and Joint Sparse Representation for Multi-view Face Recognition; target Positioning Based on Binocular Vision; an Efficient Quality Enhancement Solution for Stereo Images; camera Pose Free Depth Sensing Based on Focus Stacking; objective Quality Assessment for Light Field Based on Refocus Characteristic; Fast Stereo 3D Imaging Based on Random Speckle Projection and Its FPGA Implementation; A Novel Robust Blind Digital Image Watermarking Scheme Against JPEG2000 Compression; semantic Map Based Image Compression via Conditional Generative Adversarial Network; JPEG Reversible Data Hiding with Matrix Embedding; Detection and Localization of Video Object Removal by Spatio-Temporal LBP Coherence Analysis; an Image Splicing and Copy-Move Detection Method Based on Convolutional Neural Networks with Global Average Pooling; Digital Media Copyright and Content Protection Using IPFS and Blockchain; TQR-Net: Tighter Quadrangle-Based Convolutional Neural Network for Dense Building Instance Localization in Remote Sensing Imagery; a Semantic Segmentation Approach Based on DeepLab Network in High-Resolution Remote Sensing Images; Modified LDE for Dimensionality Reduction of Hyperspectral Image; mapping of Native Plant Species and Noxious Weeds in Typical Area of the Three-River Headwaters Region by Using Worldview-2 Imagery; a New Smoothing-Based Farmland Extraction Approach with Vectorization from Raster Remote Sensing Images; S3OD: Single Stage Small Object Detector from Scratch for Remote Sensing Images; AVE-WLS Method for Lossless Image Coding.","","","Conference review","Final","","Scopus","2-s2.0-85076856024"
"Yan Y.; Huang X.; Rangarajan A.; Ranka S.","Yan, Yupeng (56404870300); Huang, Xiaohui (57202590475); Rangarajan, Anand (7007138209); Ranka, Sanjay (7006264465)","56404870300; 57202590475; 7007138209; 7006264465","Densely labeling large-scale satellite images with generative adversarial networks","2018","Proceedings - IEEE 16th International Conference on Dependable, Autonomic and Secure Computing, IEEE 16th International Conference on Pervasive Intelligence and Computing, IEEE 4th International Conference on Big Data Intelligence and Computing and IEEE 3rd Cyber Science and Technology Congress, DASC-PICom-DataCom-CyberSciTec 2018","","","8511999","919","926","7","10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-7","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056814064&doi=10.1109%2fDASC%2fPiCom%2fDataCom%2fCyberSciTec.2018.000-7&partnerID=40&md5=3b64b9c208fab19f7e8c5fdbcb9891a5","Building an efficient and accurate pixel-level labeling framework for large-scale and high-resolution satellite imagery is an important machine learning application in the remote sensing area. Due to the very limited amount of the ground-Truth data, we employ a well-performing superpixel tessellation approach to segment the image into homogeneous regions and then use these irregular-shaped regions as the foundation for the dense labeling work. A deep model based on generative adversarial networks is trained to learn the discriminating features from the image data without requiring any additional labeled information. In the subsequent classification step, we adopt the discriminator of this unsupervised model as a feature extractor and train a fast and robust support vector machine to assign the pixel-level labels. In the experiments, we evaluate our framework in terms of the pixel-level classification accuracy on satellite imagery with different geographical types. The results show that our dense-labeling framework is very competitive compared to the state-of-The-Art methods that heavily rely on prior knowledge or other large-scale annotated datasets. © 2018 IEEE.","Computer aided instruction; Image segmentation; Learning systems; Pixels; Remote sensing; Satellite imagery; Superpixels; Adversarial networks; Classification accuracy; High resolution satellite imagery; Large-scale satellites; Machine learning applications; State-of-the-art methods; Superpixel segmentations; Unsupervised feature learning; Big data","Generative adversarial networks; Remote sensing; superpixel segmentation; Unsupervised feature learning","Conference paper","Final","","Scopus","2-s2.0-85056814064"
"Hu B.; Yao P.; Fu L.; Li X.; Dong K.; Zheng T.","Hu, Bo (57215310440); Yao, Ping (57206347584); Fu, Li (57215326325); Li, Xingyu (57225160254); Dong, Ke (57215311120); Zheng, Tianyao (13408199200)","57215310440; 57206347584; 57215326325; 57225160254; 57215311120; 13408199200","Transfer learning in remote sensing images with generative adversarial networks","2019","Proceedings - 18th IEEE/ACIS International Conference on Computer and Information Science, ICIS 2019","","","8940298","124","129","5","10.1109/ICIS46139.2019.8940298","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078565660&doi=10.1109%2fICIS46139.2019.8940298&partnerID=40&md5=2911f36b743bee58a91c0e131f0858e8","In recent years Transfer Learning has become an important research issue and it is employed to solve the lack of data problem in remote sensing field. As one kind of the most popular generative models, Generative Adversarial Networks (GANs) can generate hierarchy representations and realistic textures of objects. Now GANs have become the most successful and usual way to synthesize images. In this paper we train different GANs and generate images on the UC Merced Land Use Dataset and the aircraft carrier dataset compiled by ourselves. Our images are better than the ones generated by MARTA GAN in remote sensing field. We also creatively adopt the Image Similarity Deep Ranking model (ISDR) to evaluate our GANs and reuse high-quality generated images in an objective detection task. We train the object detection model SSD on our dataset. With the augmentation of generated images, the SSD model achieves a better detection accuracy, which means GANs are beneficial for addressing the lack of training data problem in remote sensing field. © 2019 IEEE.","Aircraft carriers; Land use; Object detection; Textures; Training aircraft; Adversarial networks; Detection accuracy; Detection tasks; Generative model; Imagesimilarity; Remote sensing images; Research issues; Transfer learning; Remote sensing","Generative Adversarial Networks; Imagesimilarity; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85078565660"
"He Z.; He D.; Mei X.; Hu S.","He, Zhi (36604533800); He, Dan (57216097405); Mei, Xiangqin (57211498948); Hu, Saihan (57211500223)","36604533800; 57216097405; 57211498948; 57211500223","Wetland classification based on a new efficient generative adversarial network and Jilin-1 satellite image","2019","Remote Sensing","11","20","2455","","","","10.3390/rs11202455","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074199313&doi=10.3390%2frs11202455&partnerID=40&md5=d3315428da4167b779f2d029c6fc58d8","Recent studies have shown that deep learning methods provide useful tools for wetland classification. However, it is difficult to perform species-level classification with limited labeled samples. In this paper, we propose a semi-supervised method for wetland species classification by using a new efficient generative adversarial network (GAN) and Jilin-1 satellite image. The main contributions of this paper are twofold. First, the proposed method, namely ShuffleGAN, requires only a small number of labeled samples. ShuffleGAN is composed of two neural networks (i.e., generator and discriminator), which perform an adversarial game in the training phase and ShuffleNet units are added in both generator and discriminator to obtain speed-accuracy tradeoff. Second, ShuffleGAN can perform species-level wetland classification. In addition to distinguishing the wetland areas from non-wetlands, different tree species located in the wetland are also identified, thus providing a more detailed distribution of the wetland land-covers. Experiments are conducted on the Haizhu Lake wetland data acquired by the Jilin-1 satellite. Compared with existing GAN, the improvement in overall accuracy (OA) of the proposed ShuffleGAN is more than 2%. This work can not only deepen the application of deep learning in wetland classification but also promote the study of fine classification of wetland land-covers. © 2019 by the authors.","Classification (of information); Deep learning; Image classification; Remote sensing; Satellites; Supervised learning; Adversarial networks; Learning methods; Overall accuracies; Satellite images; Semi-supervised method; Species classification; Speed accuracy; Wetland classification; Wetlands","Classification; Deep learning; Generative adversarial networks; Remote sensing; Wetland","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85074199313"
"Wang M.; He L.; Chang X.; Cheng Y.","Wang, Mi (57205635094); He, Luxiao (57201260628); Chang, Xueli (56421263200); Cheng, Yufeng (56861501800)","57205635094; 57201260628; 56421263200; 56861501800","Corrections to: Superresolution of single gaofen-4 visible-light and near-infrared (VNIR) image based on texture image extraction (IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing DOI: 10.1109/JSTARS.2019.2926490)","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","8","8823074","3149","","","10.1109/JSTARS.2019.2937427","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072749190&doi=10.1109%2fJSTARS.2019.2937427&partnerID=40&md5=f37eaf1a7aff60b63f26d90894a244bc","In the above article [1], production errors are corrected as follows. Funding information should be added as follows: This work was supported in part by the China National Funds for Distinguished Young Scientists under Grant 61825103 and in part by the National Natural Science Foundation of China under Grant 91838303. On page 3, Ref. [36] should be cited in the sentence: ""For panchromatic and multispectral images of the same scene, the two have strong correlation [34]-[36]."" On page 5, the sentence ""For referenced frame construction, it is more important to choose which LR texture image."" should be ""For referenced frame construction, it is important to choose LR texture image."" On page 5, the sentence ""Local or global motion can be ignored since the data a single GF-4 VNIR image."" should be ""Local or global motion can be ignored since the input data is a single image."" On page 7, a sentence should be added as follows. In Tables III-V, the evaluation parameters of SR results in urban, lake, and mountain area, respectively, have been presented. The bold values in Tables III-V are the best parameters of the five methods."" An acknowledgement should be added as follows: ""The authors would like to thank CRESDA for providing the experimental data. Corrected references are as follows. [15] M. Irani and S. Peleg, ""Improving resolution by image registration,"" CVGIP, Graphical Models Image Process., vol. 53, no. 3, pp. 231-239, 1991.[19] R. R. Schultz and R. L. Stevenson, ""Extraction of HR frames from video sequences,"" IEEE Trans. Image Process., vol. 5, no. 6, pp. 996-1011, Jun. 1996. [28] C. Ledig et al., ""Photo-realistic single image super-resolution using a generative adversarial network,"" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., Jul. 2017, pp. 105-114. [30] C. Dong, C. C. Loy, K. He, and X. Tang, ""Learning a deep convolutional network for image super-resolution,"" in Proc. Eur. Conf. Comput. Vision, 2014, pp. 184-199. [33] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, ""Residual dense network for image super-resolution,"" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2018, pp. 2472-2481. [34] X. U. Qi-Zhi and G. Feng, ""High fidelity panchromatic and multispectral image fusion based on ratio transform,"" Comput. Sci., vol. 41, no. 10, pp. 19-22, 2014. [37] R. C. Gonzalez and R. E. Woods, Digital Image Processing. Beijing, China: Publishing House Electron. Ind., 2007. © 2008-2012 IEEE.","","","Erratum","Final","","Scopus","2-s2.0-85072749190"
"","","","19th Pacific-Rim Conference on Multimedia, PCM 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11164 LNCS","","","","","2541","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057246470&partnerID=40&md5=64e6c86a5145909e8190f3f5dcb1b542","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.","","","Conference review","Final","","Scopus","2-s2.0-85057246470"
"Xia Y.; Zhang H.; Zhang L.; Fan Z.","Xia, Yu (35319088500); Zhang, Hongyan (54954032600); Zhang, Liangpei (8359720900); Fan, Zhiyu (57204877960)","35319088500; 54954032600; 8359720900; 57204877960","Cloud Removal of Optical Remote Sensing Imagery with Multitemporal Sar-Optical Data Using X-Mtgan","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899105","3396","3399","3","10.1109/IGARSS.2019.8899105","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077693096&doi=10.1109%2fIGARSS.2019.8899105&partnerID=40&md5=2eeb9bcbacb654e079e0c4d5923d94ac","Optical remote sensing images are inevitably corrupted by clouds during the acquisition process. To reconstruct the missing information contaminated by clouds, this paper introduces a new cloud removal method based on X-fork generative adversarial network with multitemporal data, which can be named X-MTGAN. By utilizing the auxiliary differential image between two imaging times, X-MTGAN can be well trained with multitemporal SAR-optical data. Then, the target optical image is synthesized with an end-to-end generator of the X-MTGAN, which has advantages in capturing change information between two temporal images. Finally, the cloud-free image can be subsequently acquired by replacing cloud-contaminated regions with the simulated image. By utilizing Setinel-1 and Sentinel-2 data, experiments are conducted to validate the feasibility of the proposed approach. Compared with the state-of-the-art methods, the results illustrate that X-MTGAN is visually and quantitatively effective in the removal of clouds, which has favorable applicability and competitive performance. © 2019 IEEE.","Geology; Geometrical optics; Image processing; Adversarial networks; Cloud removal; Competitive performance; Optical image; Optical remote sensing; Optical remote-sensing imagery; SAR data; State-of-the-art methods; Remote sensing","cloud removal; GAN; optical image simulation; SAR data","Conference paper","Final","","Scopus","2-s2.0-85077693096"
"Wu C.; Ju B.; Wu Y.; Xiong N.","Wu, Chunxue (23399010900); Ju, Bobo (57211888104); Wu, Yan (57222587696); Xiong, Naixue (35231569200)","23399010900; 57211888104; 57222587696; 35231569200","SlimRGBD: a Geographic Information Photography Noise Reduction System for aerial Remote Sensing","2020","IEEE Access","8","","8959217","15144","15158","14","10.1109/aCCESS.2020.2966497","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079752602&doi=10.1109%2faCCESS.2020.2966497&partnerID=40&md5=177f2c907f16d0858c3df2ca6a6db6b7","In the past ten years, civil drone technology has developed rapidly, and UaV (Unmanned aerial Vehicle) has been widely used in various industries. Especially in the field of aerial remote sensing, the emergence of UaV technology has enabled the geographical information of remote areas that are not concerned to be quickly presented. However, UaV aerial photography is greatly affected by the weather. Pictures that use aerial drones for aerial photography in rainy weather will appear noise. In this paper, how to eliminate the noise of aerial image is to be talked, the multi-channel pruning technology is used to pruning the RnResNet network. Based on this, a new anti-convergence-convolution neural network noise reduction system for the operation of UaV airborne embedded equipment is proposed. The system is used to eliminate noise in the aerial image. This type of noise reducer has got rid of the current situation that the neural network noise reducer consumes too much power and is inefficient, and has certain advantages. © 2013 IEEE.","Aerial photography; Drones; Noise abatement; Photographic equipment; Remote sensing; Unmanned aerial vehicles (UAV); Adversarial networks; channel pruning; Image noise reduction; ResNet; SlimRGBD; Antennas","channel pruning; generative adversarial networks; image noise reduction; ResNet; SlimRGBD; sparse training; UaV","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85079752602"
"Ma D.; Tang P.; Zhao L.","Ma, Dongao (57209605909); Tang, Ping (35436105100); Zhao, Lijun (57213186907)","57209605909; 35436105100; 57213186907","SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline In Vitro","2019","IEEE Geoscience and Remote Sensing Letters","16","7","8611213","1046","1050","4","10.1109/LGRS.2018.2890413","61","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068202699&doi=10.1109%2fLGRS.2018.2890413&partnerID=40&md5=74c339d4987181eb9923db41fba40bc6","Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although research studies have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning-based generative adversarial networks (GANs) brings us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this letter proposes a SiftingGAN approach to generate more numerous, more diverse, and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known aerial image data set demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations. © 2004-2012 IEEE.","Antennas; Classification (of information); Deep learning; Image enhancement; Mathematical transformations; Metadata; Remote sensing; Adversarial networks; Aerial image data; Comparison methods; Data augmentation; Image transformations; Remote sensing images; Sample generations; Scene classification; data quality; data set; image analysis; image classification; remote sensing; research work; Image classification","Data augmentation; deep learning; generative adversarial networks (GANs); scene classification","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068202699"
"Bittner K.; d'Angelo P.; Körner M.; Reinartz P.","Bittner, Ksenia (57194603356); d'Angelo, Pablo (9838984100); Körner, Marco (57190168095); Reinartz, Peter (56216874200)","57194603356; 9838984100; 57190168095; 56216874200","DSM-to-LoD2: Spaceborne stereo digital surface model refinement","2018","Remote Sensing","10","12","1926","","","","10.3390/rs10121926","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058873455&doi=10.3390%2frs10121926&partnerID=40&md5=cf5f2747c86413742836895228bd3a38","A digital surface model (DSM) provides the geometry and structure of an urban environment with buildings being the most prominent objects in it. Built-up areas change with time due to the rapid expansion of cities. New buildings are being built, existing ones are expanded, and old buildings are torn down. As a result, 3D surface models can increase the understanding and explanation of complex urban scenarios. They are very useful in numerous fields of remote sensing applications, in tasks related to 3D reconstruction and city modeling, planning, visualization, disaster management, navigation, and decision-making, among others. DSMs are typically derived from various acquisition techniques, like photogrammetry, laser scanning, or synthetic aperture radar (SAR). The generation of DSMs from very high resolution optical stereo satellite imagery leads to high resolution DSMs which often suffer from mismatches, missing values, or blunders, resulting in coarse building shape representation. To overcome these problems, we propose a method for 3D surface model generation with refined building shapes to level of detail (LoD) 2 from stereo half-meter resolution satellite DSMs using deep learning techniques. Mainly, we train a conditional generative adversarial network (cGAN) with an objective function based on least square residuals to generate an accurate LoD2-like DSM with enhanced 3D object shapes directly from the noisy stereo DSM input. In addition, to achieve close to LoD2 shapes of buildings, we introduce a new approach to generate an artificial DSM with accurate and realistic building geometries from city geography markup language (CityGML) data, on which we later perform a training of the proposed cGAN architecture. The experimental results demonstrate the strong potential to create large-scale remote sensing elevation models where the buildings exhibit better-quality shapes and roof forms than just using the matching process. Moreover, the developed model is successfully applied to a different city that is unseen during the training to show its generalization capacity. © 2018 by the authors.","Buildings; Decision making; Deep learning; Disaster prevention; Disasters; Markup languages; Remote sensing; Satellite imagery; Space-based radar; Stereo image processing; Synthetic aperture radar; 3d buildings; 3D scenes; Adversarial networks; Digital surface models; Urban regions; Three dimensional computer graphics","3D building shape; 3D scene refinement; Conditional generative adversarial networks; Digital surface model; Urban region","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85058873455"
"Chen Y.; Wu F.; Zhao J.","Chen, Yi (57204499384); Wu, Fengge (8262906200); Zhao, Junsuo (36811032700)","57204499384; 8262906200; 36811032700","Motion Deblurring via Using Generative Adversarial Networks for Space-Based Imaging","2018","Proceedings - 2018 IEEE/ACIS 16th International Conference on Software Engineering Research, Management and Application, SERA 2018","","","8477191","37","41","4","10.1109/SERA.2018.8477191","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055820300&doi=10.1109%2fSERA.2018.8477191&partnerID=40&md5=68c379f0641bd3fdebd0eff918600c9c","In some missions of NanoSats, we find images captured are disturbed by motion blur which caused under the situation that NanoSats work in low-earth orbit at high speeds. In this paper, we address the problem of deblurring images degraded due to space-based imaging system shaking or movements of observing targets. We propose a motion deblurring strategy via using Generative Adversarial Networks(GAN) to realize an end-to-end image processing without kernel estimation in orbit. We combine Wasserstein GAN(WGAN) and loss function based on adversarial loss and perceptual loss to optimize the result of deblurred image. The experimental results on the two different datasets prove the feasibility and effectiveness of the proposed strategy which outperforms the state-of-the-art blind deblurring algorithms using for remote sensing images both quantitatively and qualitatively. © 2018 IEEE.","Application programs; Engineering research; Nanosatellites; Orbits; Remote sensing; Adversarial networks; Deblurring images; Kernel estimation; Motion deblurring; NanoSats; Remote sensing images; Space-based; Space-based imaging systems; Image enhancement","Generative Adversarial Networks; Motion De-blurring; NanoSats; Space-Based Imaging","Conference paper","Final","","Scopus","2-s2.0-85055820300"
"Tao Y.; Muller J.-P.","Tao, Y. (56539197700); Muller, J.-P. (7404871794)","56539197700; 7404871794","Repeat multiview panchromatic super-resolution restoration using the UCL MAGiGAN system","2018","Proceedings of SPIE - The International Society for Optical Engineering","10789","","1078903","","","","10.1117/12.2500196","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059024601&doi=10.1117%2f12.2500196&partnerID=40&md5=eb5c8dab554c5cc6473c3417b6e63921","High spatial resolution imaging data is always considered desirable in the field of remote sensing, particularly Earth observation. However, given the physical constraints of the imaging instruments themselves, one needs to be able to trade-off spatial resolution against launch mass as well as telecommunications bandwidth for transmitting data back to the Earth. In this paper, we present a newly developed super-resolution restoration system, called MAGiGAN, based on our original GPT-SRR system combined with deep learning image networks to be able to restore up to 4x higher resolution enhancement using multi-angle repeat images as input. © 2018 SPIE.","Deep learning; Economic and social effects; Image enhancement; Image resolution; Observatories; Optical resolving power; Restoration; Adversarial networks; Earth observations; MAGiGAN; Multi angle; Super-resolution restoration; Remote sensing","Deep learning; Earth observation; Generative adversarial network; MAGiGAN; Multi-angle; Super-resolution restoration","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85059024601"
"","","","14th Asian Conference on Computer Vision, ACCV 2018","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11363 LNCS","","","","","4380","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067240912&partnerID=40&md5=c9ff463e7ef36999fb318e7a10f821fc","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery; panorama from Representative Frames of Unconstrained Videos Using DiffeoMeshes; robust and Efficient Ellipse Fitting Using Tangent Chord Distance; knowledge Distillation with Feature Maps for Image Classification; bidirectional Conditional Generative Adversarial Networks; cross-Resolution Person Re-identification with Deep Antithetical Learning; a Temporally-Aware Interpolation Network for Video Frame Inpainting; linear Solution to the Minimal Absolute Pose Rolling Shutter Problem; scale Estimation of Monocular SfM for a Multi-modal Stereo Camera; geometry Meets Semantics for Semi-supervised Monocular Depth Estimation; zero-Shot Facial Expression Recognition with Multi-label Label Propagation; deep Manifold Alignment for Mid-Grain Sketch Based Image Retrieval; Visual Graphs from Motion (VGfM): Scene Understanding with Object Geometry Reasoning; deep Semantic Matching with Foreground Detection and Cycle-Consistency; hidden Two-Stream Convolutional Networks for Action Recognition; a Multi-purpose Convolutional Neural Network for Simultaneous Super-Resolution and High Dynamic Range Image Reconstruction; ITM-CNN: Learning the Inverse Tone Mapping from Low Dynamic Range Video to High Dynamic Range Displays Using Convolutional Neural Networks; Structure Aware SLAM Using Quadrics and Planes; maintaining Natural Image Statistics with the Contextual Loss; U-DADA: Unsupervised Deep Action Domain Adaptation; artistic Object Recognition by Unsupervised Style Adaptation; believe It or Not, We Know What You Are Looking At!; iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects; multi-Attribute Probabilistic Linear Discriminant Analysis for 3D Facial Shapes; combination of Two Fully Convolutional Neural Networks for Robust Binarization; deep Depth from Focus.","","","Conference review","Final","","Scopus","2-s2.0-85067240912"
"Xiong D.; He C.; Liu X.; Liao M.","Xiong, Dehui (57201065746); He, Chu (12345438500); Liu, Xinlong (57193138291); Liao, Mingsheng (7202371636)","57201065746; 12345438500; 57193138291; 7202371636","An end-to-end Bayesian segmentation network based on a generative adversarial network for remote sensing images","2020","Remote Sensing","12","2","216","","","","10.3390/rs12020216","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081088395&doi=10.3390%2frs12020216&partnerID=40&md5=9d74078b1b608bb6cc1541a73ffcf9d2","Due to the development of deep convolutional neural networks (CNNs), great progress has been made in semantic segmentation recently. In this paper, we present an end-to-end Bayesian segmentation network based on generative adversarial networks (GANs) for remote sensing images. First, fully convolutional networks (FCNs) and GANs are utilized to realize the derivation of the prior probability and the likelihood to the posterior probability in Bayesian theory. Second, the cross-entropy loss in the FCN serves as an a priori to guide the training of the GAN, so as to avoid the problem of mode collapse during the training process. Third, the generator of the GAN is used as a teachable spatial filter to construct the spatial relationship between each label. Some experiments were performed on two remote sensing datasets, and the results demonstrate that the training of the proposed method is more stable than other GAN based models. The average accuracy and mean intersection (MIoU) of the two datasets were 0.0465 and 0.0821, and 0.0772 and 0.1708 higher than FCN, respectively. © 2020 by the authors.","Bayesian networks; Convolution; Convolutional neural networks; Deep neural networks; Remote sensing; Semantics; Synthetic aperture radar; Adversarial networks; Bayesian; Bayesian segmentation; Convolutional networks; Image semantics; Remote sensing images; Semantic segmentation; Spatial relationships; Image segmentation","Bayesian; Fully convolutional networks (FCN); Generative adversarial networks (GAN); Image semantic segmentation; Synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081088395"
"Huang Z.-X.; Jing C.-W.","Huang, Zhi-Xing (57215537930); Jing, Chang-Wei (57542839000)","57215537930; 57542839000","Super-Resolution Reconstruction Method of Remote Sensing Image Based on Multi-Feature Fusion","2020","IEEE Access","8","","8963714","18764","18771","7","10.1109/ACCESS.2020.2967804","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081112726&doi=10.1109%2fACCESS.2020.2967804&partnerID=40&md5=dde8f91a5cb3bdf1f9a0c3c688eb09c8","The acquisition of remote sensing images is affected by imaging equipment and environmental conditions. Usually on lower performance devices, the resolution of the acquired images is also low. Among many methods, the super-resolution reconstruction method based on generative adversarial networks has obvious advantages over previous network models in reconstructing image texture details. However, it is found in experiments that not all of these reconstructed textures exist in the image itself. Aiming at the problem of whether the texture details of the reconstructed image are accurate and clear, we propose a super-resolution reconstruction method combining wavelet transform and generative adversarial network. Using wavelet multi-resolution analysis, training wavelet decomposition coefficients in the generative adversarial network can effectively improve the local detail information of the reconstructed image. Experimental results show that our method can effectively reconstruct more natural image textures and make the images more visually clear. In the remote sensing image test set, the four indicators of the algorithm, peak signal to noise ratio (PSNR), structural similarity (SSIM), Feature Similarity (FSIM) and Universal Image Quality (UIQ) are slightly better than the algorithms mentioned in the article. © 2020 IEEE.","Image acquisition; Image enhancement; Image quality; Image reconstruction; Optical resolving power; Remote sensing; Signal to noise ratio; Textures; Wavelet decomposition; Decomposition coefficient; Feature similarity(FSIM); Peak signal to noise ratio; Remote sensing images; Self correlation; Super resolution; Super resolution reconstruction; Wavelet multi-resolution analysis; Image texture","image texture; Remote sensing image; self-correlation; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081112726"
"Li J.; Cui R.; Li Y.; Li B.; Du Q.; Ge C.","Li, Jiaojiao (55934244200); Cui, Ruxing (57211523782); Li, Yunsong (55986546100); Li, Bo (57188584536); Du, Qian (7202060063); Ge, Chiru (57189461615)","55934244200; 57211523782; 55986546100; 57188584536; 7202060063; 57189461615","Multitemporal Hyperspectral Image Super-Resolution through 3D Generative Adversarial Network","2019","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2019","","","8866956","","","","10.1109/Multi-Temp.2019.8866956","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074259539&doi=10.1109%2fMulti-Temp.2019.8866956&partnerID=40&md5=b367650ef774fd46193554b7d9860789","The super-resolution of multitemporal hyperspectral imagery is considered, wherein a 3D generative adversarial network (GAN) is promoted and employed. Firstly, we put the SR process in a generative adversarial network (GAN) framework, so that the resulted high resolution HSI can keep more texture details. Secondly, the input of our method is of full bands due to 3D kernel exploited. Furthermore, a series of spatial-spectral constraints or loss functions are imposed to guide the training of our generative network so as to further alleviate spectral distortion and texture blur. The experiments on the houston datasets demonstrate that the proposed GAN-based SR method with the best generalization ability can yield very high quality results. © 2019 IEEE.","Image analysis; Optical resolving power; Spectroscopy; Textures; Adversarial networks; Generalization ability; High resolution; Hyper-spectral imageries; Image super resolutions; Spectral constraints; Spectral distortions; Super resolution; Remote sensing","GAN; generalization ability; hyperspectral super-resolution; Multitemporal Hyperspectral imagery","Conference paper","Final","","Scopus","2-s2.0-85074259539"
"Toriya H.; Dewan A.; Kitahara I.","Toriya, Hisatoshi (56125898000); Dewan, Ashraf (15925234800); Kitahara, Itaru (6603549979)","56125898000; 15925234800; 6603549979","SAR2OPT: Image Alignment between Multi-Modal Images Using Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898605","923","926","3","10.1109/IGARSS.2019.8898605","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077679078&doi=10.1109%2fIGARSS.2019.8898605&partnerID=40&md5=8ef86e926d9d4342d74fca5c81938f22","This work proposes an image-alignment method for multi-modal images (e.g., synthetic aperture radar (SAR) and optical satellite images) using an image-feature-based keypoint-matching algorithm. In applying the matching algorithm to multi-modal images, common features need to be obtained at the corresponding positions. However, the appearances of features among images are different. We solve this issue by translating the appearance of one modal image to the other using generative adversarial networks (GANs). In this work, we attempt to generate optical images from SAR images as a way to extract common features. Through an experiment, we confirm that the proposed method can estimate accurate correspondences between SAR and optical images. © 2019 IEEE.","Alignment; Geology; Geometrical optics; Image matching; Remote sensing; Synthetic aperture radar; Adversarial networks; Image alignment; Image features; Key point matching; Matching algorithm; Multi-modal; Multi-modal image; Optical satellite images; Radar imaging","Generative Adversarial Networks (GAN); Image Alignment; Image Feature; Image Matching; Multi-Modal; SAR","Conference paper","Final","","Scopus","2-s2.0-85077679078"
"Li W.; Yin J.; Han B.; Zhu H.","Li, Wenyue (57212481505); Yin, Jihao (22235943500); Han, Bingnan (57188723661); Zhu, Hongmei (57089354000)","57212481505; 22235943500; 57188723661; 57089354000","Generative Adversarial Network with Folded Spectrum for Hyperspectral Image Classification","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899034","883","886","3","10.1109/IGARSS.2019.8899034","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077697712&doi=10.1109%2fIGARSS.2019.8899034&partnerID=40&md5=481a2e34c964a30ff6c7122939f40508","Hyperspectral image (HSIs) with abundant spectral information but limited labeled dataset endows the rationality and necessity of semi-supervised spectral-based classification methods. Where, the utilizing approach of spectral information is significant to classification accuracy. In this paper, we propose a novel semi-supervised method based on generative adversarial network (GAN) with folded spectrum (FS-GAN). Specifically, the original spectral vector is folded to 2D square spectrum as input of GAN, which can generate spectral texture and provide larger receptive field over both adjacent and non-adjacent spectral bands for deep feature extraction. The generated fake folded spectrum, the labeled and unlabeled real folded spectrum are then fed to the discriminator for semi-supervised learning. A feature matching strategy is applied to prevent model collapse. Extensive experimental comparisons demonstrate the effectiveness of the proposed method. © 2019 IEEE.","Classification (of information); Geology; Image classification; Remote sensing; Spectroscopy; Textures; Adversarial networks; Classification accuracy; Classification methods; Experimental comparison; Feature matching; Hyper-spectral classification; Semi-supervised method; Spectral information; Semi-supervised learning","generative adversarial network; Hyperspectral classification; semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85077697712"
"Niu X.; Yang D.; Yang K.; Pan H.; Dou Y.","Niu, Xin (38663051000); Yang, Di (57215195951); Yang, Ke (57207166059); Pan, Hengyue (57190795198); Dou, Yong (15131095400)","38663051000; 57215195951; 57207166059; 57190795198; 15131095400","Image translation between high-resolution remote sensing optical and SAR data using conditional GAN","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11166 LNCS","","","245","255","10","10.1007/978-3-030-00764-5_23","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054552945&doi=10.1007%2f978-3-030-00764-5_23&partnerID=40&md5=d7649203f8a3f0edf092fe532d50b3f1","This paper presents a study on a new problem: applying machine learning approaches to translate remote sensing images between high-resolution optical and Synthetic Aperture Radar (SAR) data. To this end, conditional Generative Adversarial Networks (GAN) have been explored. Efficiency of the conditional GAN have been verified with different SAR parameters on three regions from the world: Toronto, Vancouver in Canada and Shanghai in China. The generated SAR images have been evaluated by pixel-based image classification with detailed land cover types including: low and high density residential area, industry area, construction site, golf course, water, forest, pasture and crops. In comparison with an unsupervised GAN translation approach, the proposed conditional GAN could effectively keep many land cover types with compatible classification accuracy to the ground truth SAR data. This is one of first study on multi-source remote sensing data translation by machine learning. © Springer Nature Switzerland AG 2018.","Artificial intelligence; Deep learning; Radar imaging; Synthetic aperture radar; Adversarial networks; Classification accuracy; Construction sites; High resolution remote sensing; High-density residential areas; Machine learning approaches; Remote sensing data; Remote sensing images; Remote sensing","Deep learning; Generative Adversarial Network; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85054552945"
"Emerson T.H.; Edelberg J.A.; Doster T.; Merrill N.; Olson C.C.","Emerson, Tegan H. (56519554900); Edelberg, Jason A. (45760911000); Doster, Timothy (34869489300); Merrill, Nicholas (51562007600); Olson, Colin C. (9839685500)","56519554900; 45760911000; 34869489300; 51562007600; 9839685500","Generative and Encoded Anomaly Detectors","2019","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2019-September","","8920850","","","","10.1109/WHISPERS.2019.8920850","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077525167&doi=10.1109%2fWHISPERS.2019.8920850&partnerID=40&md5=3f4bcb4cd8d3ac0e3e3c01b812e67ee6","We present two fully unsupervised deep learning approaches for hyperspectral anomaly detection. In one approach we formulate the anomaly detection problem as an adversarial game where a generator network learns the distribution of the hyperspectral background pixels comprising a single hyperspectral image and the output of the corresponding discriminator network yields a detection statistic. The other approach formulates the detection statistic as the error between an input hyperspectral pixel and the reconstruction of that pixel by an autoencoder network trained on the image. Both methods leverage a sub-sampling scheme that allows for unsupervised training and testing on the same data set. Our approaches are validated on a four-class synthetic hyperspectral data set and compared to a statistical approach (RX) and a geometric approach (skelton kernel principal component analysis). The proposed Generative Anomaly Detector algorithm achieves top performance on the data set while the autoencoder detection scheme also demonstrates performance gains relative to the comparison algorithms. Benefits and drawbacks of the approaches are discussed and highlight the many potential directions for future work. © 2019 IEEE.","Anomaly detection; Deep learning; Pixels; Principal component analysis; Remote sensing; Spectroscopy; Statistical tests; Adversarial networks; Auto encoders; Geometric approaches; Hyperspectral anomaly detection; Hyperspectral backgrounds; Kernel principal component analyses (KPCA); Statistical approach; Unsupervised training; Hyperspectral imaging","Anomaly Detection; Autoencoder; Generative Adversarial Networks; Hyperspectral Imaging","Conference paper","Final","","Scopus","2-s2.0-85077525167"
"Tran K.; Panahi A.; Adiga A.; Sakla W.; Krim H.","Tran, Kenneth (57052318100); Panahi, Ashkan (36470876800); Adiga, Aniruddha (36447297900); Sakla, Wesam (14069004700); Krim, Hamid (26643057600)","57052318100; 36470876800; 36447297900; 14069004700; 26643057600","Nonlinear Multi-scale Super-resolution Using Deep Learning","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8682354","3182","3186","4","10.1109/ICASSP.2019.8682354","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069000640&doi=10.1109%2fICASSP.2019.8682354&partnerID=40&md5=7553e8638c9a937e4390ebb9c7d1e70f","We propose a deep learning architecture capable of performing up to 8× single image super-resolution. Our architecture incorporates an adversarial component from the super-resolution generative adversarial networks (SRGANs) and a multi-scale learning component from the multiple scale super-resolution network (MSSRNet), which only together can recover smaller structures inherent in satellite images. To further enhance our performance, we integrate progressive growing and training to our network. This, aided by feed forwarding connections in the network to move along and enrich information from previous inputs, produces super-resolved images at scaling factors of 2, 4, and 8. To ensure and enhance the stability of GANs, we employ Wasserstein GANs (WGANs) during training. Experimentally, we find that our architecture can recover small objects in satellite images during super-resolution whereas previous methods cannot. © 2019 IEEE.","Audio signal processing; Network architecture; Optical resolving power; Remote sensing; Small satellites; Speech communication; Adversarial networks; GANs; Learning architectures; Multiple scale; Remote sensing data; Satellite images; Scaling factors; Super resolution; Deep learning","dilated convolutions; GANs; remote sensing data; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85069000640"
"Gong M.; Niu X.; Zhan T.; Zhang M.","Gong, Maoguo (8933846400); Niu, Xudong (57196705445); Zhan, Tao (57052462500); Zhang, Mingyang (56549116200)","8933846400; 57196705445; 57052462500; 56549116200","A coupling translation network for change detection in heterogeneous images","2019","International Journal of Remote Sensing","40","9","","3647","3672","25","10.1080/01431161.2018.1547934","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057604296&doi=10.1080%2f01431161.2018.1547934&partnerID=40&md5=f2d36fd9383f1893920e0b31cdf2859e","Based on the images acquired through different sensors, change detection is much more challenging than those based on homogeneous images. The main reason behind it is that the heterogeneous image-pair cannot be directly compared in original observation space due to their distinct statistical properties. In order to detect the changes, we establish a coupling variational autoencoder (VAE) to transform the heterogeneous images into a shared-latent space, where they have more consistent representations and hence the prior changed regions can be highlighted by direct comparison. And based on the shared space, we build coupled generative adversarial networks (GANs) associated with the coupling VAE to translate the heterogeneous images into homogeneous, from which more accurate change detection results can be obtained in their common observation spaces. The proposed framework is totally unsupervised, and the experimental results on real heterogeneous data sets demonstrate its superiority over some other existing algorithms. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.","Remote sensing; Adversarial networks; Auto encoders; Change detection; Heterogeneous data; Image pairs; Observation space; Shared spaces; Statistical properties; accuracy assessment; algorithm; comparative study; detection method; experimental study; heterogeneity; image analysis; Sensors","","Article","Final","","Scopus","2-s2.0-85057604296"
"Cong L.; Gao L.; Zhang H.; Sun P.","Cong, Longjian (57201653461); Gao, Lei (57043841400); Zhang, Hui (56080957700); Sun, Peng (55375660500)","57201653461; 57043841400; 56080957700; 55375660500","Convolutional neural network using generated data for SAR ATR with limited samples","2017","Proceedings of SPIE - The International Society for Optical Engineering","10609","","106091P","","","","10.1117/12.2292997","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045564736&doi=10.1117%2f12.2292997&partnerID=40&md5=80d91e0352962f205a3df8ebf323f32a","Being able to adapt all weather at all times, it has been a hot research topic that using Synthetic Aperture Radar(SAR) for remote sensing. Despite all the well-known advantages of SAR, it is hard to extract features because of its unique imaging methodology, and this challenge attracts the research interest of traditional Automatic Target Recognition(ATR) methods. With the development of deep learning technologies, convolutional neural networks(CNNs) give us another way out to detect and recognize targets, when a huge number of samples are available, but this premise is often not hold, when it comes to monitoring a specific type of ships. In this paper, we propose a method to enhance the performance of Faster R-CNN with limited samples to detect and recognize ships in SAR images. © 2018 SPIE.","Automatic target recognition; Convolution; Deep learning; Image enhancement; Neural networks; Radar imaging; Remote sensing; Ships; Synthetic aperture radar; Adversarial networks; Convolutional neural network; Hot research topics; Learning technology; Number of samples; Research interests; SAR ATR; SAR Images; Radar target recognition","ATR; Convolutional Neural Networks; Generative Adversarial Networks; SAR","Conference paper","Final","","Scopus","2-s2.0-85045564736"
"Alnujaim I.; Oh D.; Kim Y.","Alnujaim, Ibrahim (57201449342); Oh, Daegun (28267930300); Kim, Youngwook (55763987400)","57201449342; 28267930300; 55763987400","Generative Adversarial Networks to Augment Micro-Doppler Signatures for the Classification of Human Activity","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898073","9459","9461","2","10.1109/IGARSS.2019.8898073","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077679228&doi=10.1109%2fIGARSS.2019.8898073&partnerID=40&md5=6fa00f12fb74f379cb05fb88dc3a4e21","Collecting a large amount of data for radar requires a significant amount of time, labor, and money. In deep convolutional neural networks, a small dataset causes the problem of overfitting. We herein introduce the employment of data augmentation using generative adversarial networks (GANs) to solve the data deficiency problem. In this study, we tested the feasibility of using generative adversarial networks to generate micro-Doppler signatures for seven human activities. Moreover, we use produced fake images to train deep convolutional neural networks. We found that the use of augmented data improves classification accuracy. In addition, the quality of GAN output was evaluated in terms of classification accuracy. © 2019 IEEE.","Convolution; Deep learning; Deep neural networks; Geology; Remote sensing; Adversarial networks; Classification accuracy; Data augmentation; Human activities; Large amounts; Micro-Doppler; Overfitting; Convolutional neural networks","adversarial generative network; deep learning; Human activity classification; microDoppler signatures","Conference paper","Final","","Scopus","2-s2.0-85077679228"
"Wang L.; Xu X.; Yu Y.; Yang R.; Gui R.; Xu Z.; Pu F.","Wang, Lei (57211488504); Xu, Xin (56294598500); Yu, Yue (57214104632); Yang, Rui (57208294306); Gui, Rong (57211231417); Xu, Zhaozhuo (57171068000); Pu, Fangling (13408173100)","57211488504; 56294598500; 57214104632; 57208294306; 57211231417; 57171068000; 13408173100","SAR-to-optical image translation using supervised cycle-consistent adversarial networks","2019","IEEE Access","7","","8825802","129136","129149","13","10.1109/ACCESS.2019.2939649","70","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078034428&doi=10.1109%2fACCESS.2019.2939649&partnerID=40&md5=a6086960f8b2c0f3cb16077d491dcb0b","Optical remote sensing (RS) data suffer from the limitation of bad weather and cloud contamination, whereas synthetic aperture radar (SAR) can work under all weather conditions and overcome this disadvantage of optical RS data. However, due to the imaging mechanism of SAR and the speckle noise, untrained people are difficult to recognize the land cover types visually from SAR images. Inspired by the excellent image-to-image translation performance of Generative Adversarial Networks (GANs), a supervised Cycle-Consistent Adversarial Network (S-CycleGAN) was proposed to generate large optical images from the SAR images. When the optical RS data are unavailable or partly unavailable, the generated optical images can be alternative data that aid in land cover visual recognition for untrained people. The main steps of SAR-to-optical image translation were as follows. First, the large SAR image was split to small patches. Then S-CycleGAN was used to translate the SAR patches to optical image patches. Finally, the optical image patches were stitched to generate the large optical image. A paired SAR-optical image dataset which covered 32 Chinese cities was published to evaluate the proposed method. The dataset was generated from Sentinel-1 (SEN-1) SAR images and Sentinel-2 (SEN-2) multi-spectral images. S-CycleGAN was applied to two experiments, which were SAR-to-optical image translation and cloud removal, and the results showed that S-CycleGAN could keep both the land cover and structure information well, and its performance was superior to some famous image-to-image translation models. © 2013 IEEE.","Flow visualization; Geometrical optics; Remote sensing; Spectroscopy; Synthetic aperture radar; Adversarial networks; Cloud contamination; Cloud removal; Multispectral images; Optical image; Optical remote sensing; Sentinel; Structure information; Radar imaging","cloud removal; GAN; SAR-to-optical image translation; Sentinel; visualization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078034428"
"","","","19th Pacific-Rim Conference on Multimedia, PCM 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11165 LNCS","","","","","2541","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057222002&partnerID=40&md5=b43bc9db0a6c6dd8ce8e69bb3979fe1a","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.","","","Conference review","Final","","Scopus","2-s2.0-85057222002"
"Jing J.R.; Li Q.; Ding X.Y.; Sun N.L.; Tang R.; Cai Y.L.","Jing, J.R. (57211059464); Li, Q. (55703453900); Ding, X.Y. (57215656096); Sun, N.L. (57215666741); Tang, R. (57215655512); Cai, Y.L. (57242833900)","57211059464; 55703453900; 57215656096; 57215666741; 57215655512; 57242833900","AENN: A GENERATIVE ADVERSARIAL NEURAL NETWORK for WEATHER RADAR ECHO EXTRAPOLATION","2019","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","3/W9","","89","94","5","10.5194/isprs-archives-XLII-3-W9-89-2019","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081576080&doi=10.5194%2fisprs-archives-XLII-3-W9-89-2019&partnerID=40&md5=7a7d74925921a90f0fb6f0ed61e37bab","Weather radar echo is one of the fundamental data for meteorological workers to weather systems identification and classification. Through the technique of weather radar echo extrapolation, the future short-term weather conditions can be predicted and severe convection storms can be warned. However, traditional extrapolation methods cannot offer accurate enough extrapolation results since their modeling capacity is limited, the recent deep learning based methods make some progress but still remains a problem of blurry prediction when making deeper extrapolation, which may due to they choose the mean square error as their loss function and that will lead to losing echo details. To address this problem and make a more realistic and accurate extrapolation, we propose a deep learning model called Adversarial Extrapolation Neural Network (AENN), which is a Generative Adversarial Network (GAN) structure and consist of a conditional generator and two discriminators, echo-frame discriminator and echo-sequence discriminator. The generator and discriminators are trained alternately in an adversarial way to make the final extrapolation results be realistic and accurate. To evaluate the model, we conduct experiments on extrapolating 0.5h, 1h, and 1.5h imminent future echoes, the results show that our proposed AENN can achieve the expected effect and outperforms other models significantly, which has a powerful potential application value for short-term weather forecasting. © 2020 Authors.","Deep learning; Deep neural networks; Extrapolation; Learning systems; Mean square error; Meteorological radar; Recurrent neural networks; Remote sensing; Adversarial networks; Expected effects; Extrapolation methods; Learning-based methods; Radar echoes; Severe convections; Short term; Weather systems; Weather forecasting","Adversarial Training; Deep Learning; Generative Adversarial Network; Radar Echo Extrapolation; Recurrent Neural Network; Short-term Weather Forecasting; Weather Radar","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081576080"
"","","","Computational Optical Sensing and Imaging, COSI 2018","2018","Optics InfoBase Conference Papers","Part F99-COSI 2018","","","","","105","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051247040&partnerID=40&md5=592bc145e4770885fa672c5161d89849","This proceedings contains 53 papers. COSI encompasses the latest advances in computational imaging research. Representative topics include compressive sensing, tomographic imaging, light-field sensing, digital holography, SAR, phase retrieval, computational spectroscopy, blind deconvolution and phase diversity, pointspread function engineering and digital/optical super resolution. The conference topics include: Non-Line-of-Sight Imaging using Superheterodyne Interferometry; Resolving Non Line-of-Sight (NLoS) motion using Speckle; Indirect Imaging Using Correlography; Micro Resolution Time-of-Flight Imaging; Passive Non-line-of-sight Source Classification from Coherence Measurements; Diffuse Time-of-flight Imaging with a Single-Photon Camera; Imaging with Phasor Fields for Non-Line-of Sight Applications; Indirect Imaging Using Virtualized Pattern Projection; Aparna Viswanath, Muralidhar M. Balaji, Prasanna Rangarajan, Duncan MacFarlane, and Marc P Christensen; Multi-layered Born scattering model for 3D phase imaging with multiple scattering objects; Depth-resolved Lensless Imaging; 3D Fluorescence Microscopy with DiffuserCam; Double-Cubic Point Spread Function for 3D Extended-Depth Localization Microscopy; Depth Sensitivity Improvement of Region-of-Interest Diffuse Optical Tomography from Superficial Signal Regression; Manob Jyoti Saikia, Rakesh Manjappa, Kunal Mankodiya, and Rajan Kanhirodan; On Block-Reference Coherent Diffraction Imaging; Deep Learning Enhances Mobile Microscopy; A Novel Optical Structure to Implement One-dimensional Fourier Transform with Spherical Lenses; Seeing through Multimode Fibers with Deep Learning; Plenoptic imaging from intensity correlations; A new method for designing highly efficient metasurface devices: Local Phase Method; Novel Optimizations for Phase Retrieval; Phase Retrieval Based on Wave Modulation; Enhanced Phase Retrieval using Quantum Illumination; Temporal Super-resolution Full Waveform LiDAR; Super-Resolution Imaging Based on Spectral Dimensional Information; Remote Sensing of Photoplethysmogram using Multi Spot Illumination; Enlarged Field of View Scattering Imaging Using Speckle Autocorrelation; Mitigating metalens aberrations via computational imaging; Shane Colburn and Arka Majumdar; Binarization threshold optimization of ghost imaging; Ghost Imaging With Gram-Schmidt Orthogonalization; Comparison between ghost imaging and traditional active optical imaging; Demonstration of computational temporal ghost imaging: detecting fast signals beyond bandwidth of detectors; Imaging the Joint Probability Distribution of Spatially Entangled Photon Pairs with a Camera; Optimization of light field fluctuation patterns in ghost imaging by mutual coherence minimization based on dictionary learning; Characterizing the optical memory effect using quantum illumination; Compressive Ultrafast Single Pixel Camera; Encrypted Single Pixel Imaging with Basis Illumination Patterns; Correlation Matrix Estimation from Compressed Measurements in a Pattern Recognition System; Exploiting Inter Voxel Correlation in Compressed Computational Imaging; Naren Viswanathan, Suresh Venkatesh, and David Schurig; Double-threshold Denoising for Single-pixel Camera; Multi-object Recognition in Turbid Water Using Compressive Sensing; Covariance Matrix Estimation from Multiple Subsets in Compressive Spectral Imaging; Compressive Spectral Polarization Imaging Using a Single Pixel Detector; Subsampling Schemes for the 2D Nuclear Magnetic Resonance Spectroscopy; Compressive coded LED and coded aperture spectral video system; Spatial Super-resolution reconstruction via SSCSI Compressive Spectral Imagers; Snapshot Compressive Spectral+Depth Imaging with Color-Coded Apertures; Spectral zooming in SSCSI Compressive Spectral Imagers; Compressive Photon-Sieve Spectral Imaging; Field-varying aberration recovery in EUV microscopy using mask roughness; Computational Cannula Microscopy: Utilizing a Simple Glass Needle for Imaging; Integral Refractive Index Imaging of Flowing Cell Nuclei; Compressive hyperspectral imaging for snapshot multi-channel fluorescence microscopy; Cell imaging by phase extraction neural network (PhENN); Quantitative Phase Maps of Live Cells Classified By Transfer Learning and Generative Adversarial Network (GAN); A Neuro-Inspired Model for Image Motion Processing; Optical Sensing and Control Based on Machine Learning; Deep Learned Phase Mask for Single Image Depth Estimation and 3D scanning; Neural Network classification for intensity imaging through multimode optical fibres; Phase Unwrapping Using Residual Neural Networks; Bending-Independent Imaging through Glass-Air Disordered Fiber Based on Deep Learning; Speckle suppression using the convolutional neural network with an exponential linear unit. The key terms of this proceedings include Compressive Sensing, Computational microscopy, Depth-resolved and turbid imaging, Imaging through aberrations, Structured illumination  and super resolution, Indirect and non-line-of-sight imaging, Machine Learning in Computational Sensing and Imaging, Phase Retrieval, Postdeadline Papers - COSI, Quantum Computational Imaging.","","","Conference review","Final","","Scopus","2-s2.0-85051247040"
"He Z.; Liu H.; Wang Y.; Hu J.","He, Zhi (36604533800); Liu, Han (57196404904); Wang, Yiwen (57196405114); Hu, Jie (57077271700)","36604533800; 57196404904; 57196405114; 57077271700","Generative adversarial networks-based semi-supervised learning for hyperspectral image classification","2017","Remote Sensing","9","10","1042","","","","10.3390/rs9101042","115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032860153&doi=10.3390%2frs9101042&partnerID=40&md5=e77ea2ac30af37958bdbbb61d3e6894e","Classification of hyperspectral image (HSI) is an important research topic in the remote sensing community. Significant efforts (e.g., deep learning) have been concentrated on this task. However, it is still an open issue to classify the high-dimensional HSI with a limited number of training samples. In this paper, we propose a semi-supervised HSI classification method inspired by the generative adversarial networks (GANs). Unlike the supervised methods, the proposed HSI classification method is semi-supervised, which can make full use of the limited labeled samples as well as the sufficient unlabeled samples. Core ideas of the proposed method are twofold. First, the three-dimensional bilateral filter (3DBF) is adopted to extract the spectral-spatial features by naturally treating the HSI as a volumetric dataset. The spatial information is integrated into the extracted features by 3DBF, which is propitious to the subsequent classification step. Second, GANs are trained on the spectral-spatial features for semi-supervised learning. A GAN contains two neural networks (i.e., generator and discriminator) trained in opposition to one another. The semi-supervised learning is achieved by adding samples from the generator to the features and increasing the dimension of the classifier output. Experimental results obtained on three benchmark HSI datasets have confirmed the effectiveness of the proposed method, especially with a limited number of labeled samples. © 2017 by the authors.","Image classification; Nonlinear filtering; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Bilateral filters; Classification methods; Semi- supervised learning; Semi-supervised classification; Spatial informations; Supervised methods; Volumetric dataset; Classification (of information)","Generative adversarial networks (GANs); Hyperspectral image (HSI); Semi-supervised classification; Three-dimensional bilateral filter (3DBF)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85032860153"
"Aung H.T.; Pha S.H.; Takeuchi W.","Aung, Hein Thura (57210917670); Pha, Sao Hone (57194043445); Takeuchi, Wataru (55348898500)","57210917670; 57194043445; 55348898500","Automatic building footprints extraction of Yangon city from geoeye monocular optical satellite image by using deep learning","2018","Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","4","","","1987","1996","9","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071861850&partnerID=40&md5=5a0229462966c42f21b3e70f925583c6","In this paper, building footprints in Yangon city are automatically extracted only from high resolution optical satellite image by using deep learning algorithm. Automatic extraction of building footprints only from optical satellite images is a very challenging task because of spatial and spectral complexities of urban objects, especially trees, buildings occluded by trees, shadow and water bodies. Moreover, very high spatial resolution of satellite image is necessary for building level extraction. In this work, pix2pix is used for automatic building footprint extraction in Yangon City. Pix2pix is based on conditional generative adversarial network which is an unsupervised deep learning algorithm that only needs fewer labelled samples. First, four different models of pix2pix are designed and trained based on different number of epochs, and different types and number of training images. Second, the trained pix2pix models are tested on the images of the study area. After that, the output images of pix2pix are digitized, compared with manually digitized reference image, and evaluated by using performance metrics such as completeness, correctness, and quality, and area assessment. According to the results, pix2pix can successfully delineate building boundaries out of other urban objects up to 71.43% of completeness, 62.5% of extraction quality, and 56.8% of overlap for extracted area and reference area on some parts of the study area. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018","Buildings; Extraction; Forestry; Image processing; Learning algorithms; Optical resolving power; Remote sensing; Satellites; Adversarial networks; Automatic buildings; Automatic extraction; Building footprint; High-resolution optical satellite images; Optical satellite images; Pix2pix; Very high spatial resolutions; Deep learning","Building footprint extraction; Deep learning; High resolution optical satellite image; Pix2pix","Conference paper","Final","","Scopus","2-s2.0-85071861850"
"Lebedev M.A.; Vizilter Yu.V.; Vygolov O.V.; Knyaz V.A.; Rubis A.Yu.","Lebedev, M.A. (56539470900); Vizilter, Yu.V. (6506127474); Vygolov, O.V. (10439430400); Knyaz, V.A. (6507787519); Rubis, A.Yu. (40462401700)","56539470900; 6506127474; 10439430400; 6507787519; 40462401700","Change detection in remote sensing images using conditional adversarial networks","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2","","565","571","6","10.5194/isprs-archives-XLII-2-565-2018","154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048364175&doi=10.5194%2fisprs-archives-XLII-2-565-2018&partnerID=40&md5=1b3e947fe79dc32c753cad8168f87200","We present a method for change detection in images using Conditional Adversarial Network approach. The original network architecture based on pix2pix is proposed and evaluated for difference map creation. The paper address three types of experiments: change detection in synthetic images without objects relative shift, change detection in synthetic images with small relative shift of objects, and change detection in real season-varying remote sensing images. © Authors 2018.","Database systems; Deep neural networks; Network architecture; Neural networks; Object detection; Adversarial networks; Change detection; Deep convolutional neural networks; Relative shift; Remote sensing images; Synthetic images; Remote sensing","Change detection; Database; Deep convolutional neural networks; Generative adversarial networks","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85048364175"
"Bi F.; Lei M.; Wang Y.; Huang D.","Bi, Fukun (35241807800); Lei, Mingyang (57209618230); Wang, Yanping (55958045700); Huang, Dan (57209615062)","35241807800; 57209618230; 55958045700; 57209615062","Remote Sensing Target Tracking in UAV Aerial Video Based on Saliency Enhanced MDnet","2019","IEEE Access","7","","8732335","76731","76740","9","10.1109/ACCESS.2019.2921315","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068258206&doi=10.1109%2fACCESS.2019.2921315&partnerID=40&md5=478bf0a3fb8e9e142797bf2c73258c64","Remote sensing target tracking in the aerial video from unmanned aerial vehicles (UAV) plays a key role in public security. As the UAV aerial video has rapid changes in scale and perspective, few pixels in the target region, and multiple similar disruptors, and the main tracking methods in this research field generally have relatively low tracking performance and timeliness, we propose a remote sensing target tracking method for the UAV aerial video based on a saliency enhanced multi-domain convolutional neural network (SEMD). First, in the pre-training stage, we combine the least squares generative adversarial networks (LSGANs) with a multi-orientation Gaussian Pyramid to augment typical easily confused negative samples for enhancing the capacity to distinguish between targets and the background. Then, a saliency module was integrated into our tracking network architecture to boost the saliency of the feature map, which can improve the representation power of a rapid dynamic change target. Finally, in the stage for generating tracking samples, we implemented a local weight allocation model to screen for hard negative samples. This approach can not only improve the stability in tracking but also boost efficiency. The comprehensive evaluations of public and homemade hard datasets demonstrate that the proposed method can achieve high accuracy and efficiency results compared with state-of-the-art methods. © 2013 IEEE.","Antennas; Clutter (information theory); Efficiency; Network architecture; Neural networks; Remote sensing; Unmanned aerial vehicles (UAV); Comprehensive evaluation; Convolutional neural network; Multi domains; Representation power; saliency enhanced; State-of-the-art methods; Tracking performance; Visual Tracking; Aircraft detection","multi-domain learning; saliency enhanced; sample augmentation; Visual tracking","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85068258206"
"Bittner K.; Korner M.; Reinartz P.","Bittner, Ksenia (57194603356); Korner, Marco (57190168095); Reinartz, Peter (56216874200)","57194603356; 57190168095; 56216874200","DSM Building Shape Refinement from Combined Remote Sensing Images Based on WNET-CGANS","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8897865","783","786","3","10.1109/IGARSS.2019.8897865","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077695207&doi=10.1109%2fIGARSS.2019.8897865&partnerID=40&md5=ad105ac9547bc7d4164cc81f2bf8bea0","We describe the workflow of a digital surface models (DSMs) refinement algorithm using a hybrid conditional generative adversarial network (cGAN) where the generative part consists of two parallel networks merged at the last stage forming a WNET architecture. The inputs to the so-called WNET-CGAN are stereo DSMs and panchromatic (PAN) half-meter resolution satellite images. Fusing these helps to propagate fine detailed information from a spectral image and complete the missing 3D knowledge from a stereo DSM about building shapes. Besides, it refines the building outlines and edges making them more rectangular and sharp. © 2019 IEEE.","Data fusion; Geology; Spectroscopy; Stereo image processing; 3d buildings; 3D scenes; Adversarial networks; Digital surface models; Satellite images; Remote sensing","3D building shape; 3D scene refinement; Conditional generative adversarial networks; data fusion; digital surface model; satellite images","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077695207"
"Peng L.; Qiu X.; Ding C.; Tie W.","Peng, Lingxiao (57225930921); Qiu, Xiaolan (18435166800); Ding, Chibiao (7202622015); Tie, Wenjie (57238255400)","57225930921; 18435166800; 7202622015; 57238255400","Generating 3D point clouds from a single SAR image using 3D reconstruction network","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900449","3685","3688","3","10.1109/IGARSS.2019.8900449","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100084841&doi=10.1109%2fIGARSS.2019.8900449&partnerID=40&md5=73b09b5b1c0c89629df089058a706779","Obtaining the three-dimensional data of the target is very useful for the interpretation and application of the SAR target. This paper proposes a deep learning framework to recover the three-dimensional structure of the target from a single SAR image, which is expressed in the form of 3D point cloud. Due to the small data set of SAR images, the network is combined by two parts. First, the two-dimensional image in the optical perspective is predicted from the SAR target image, and then the 3D points of the target is reconstructed based on the pre-trained 3D reconstruction network model from the optical images. The experiment is based on the MSTAR datasets. The results confirm the effectiveness of the three-dimensional reconstruction method. ©2019 IEEE","Computerized tomography; Deep learning; Geometrical optics; Image reconstruction; Remote sensing; Synthetic aperture radar; Three dimensional computer graphics; 3D point cloud; 3D reconstruction; Learning frameworks; Small data set; Three-dimensional data; Three-dimensional reconstruction; Three-dimensional structure; Two dimensional images; Radar imaging","3D reconstruction; Deep learning; Generative adversarial networks; Synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85100084841"
"Merkle N.; Fischer P.; Auer S.; Muller R.","Merkle, N. (57194604557); Fischer, P. (56539240200); Auer, S. (57216043589); Muller, R. (7404246697)","57194604557; 56539240200; 57216043589; 7404246697","On the possibility of conditional adversarial networks for multi-sensor image matching","2017","International Geoscience and Remote Sensing Symposium (IGARSS)","2017-July","","8127535","2633","2636","3","10.1109/IGARSS.2017.8127535","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041859053&doi=10.1109%2fIGARSS.2017.8127535&partnerID=40&md5=75e032c043b90fe0c774269f9cd25bb5","A major research area in remote sensing is the problem of multi-sensor data fusion. Especially the combination of images acquired by different sensor types, e.g. active and passive, is a difficult task. Over the last years deep learning methods have proven their high potential for remote sensing applications. In this paper we will show how a deep learning method can be valuable for the problem of optical and SAR image matching. We investigate the possible of conditional generative adversarial networks (cGANs) for the generation of artificial templates. Contrary to common template generation approaches for image matching, the generation of templates using cGANs does not require the extraction of features. Our results show the possibility of realistic SAR-like template generation from optical images through cGANs and the potential of these templates for enhancing the matching of optical and SAR images by means of reliability and accuracy. © 2017 IEEE.","","artificial template generation; conditional GANs; deep learning; image matching; multi-sensor","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85041859053"
"Fukun B.; Mingyang L.; Jiayi S.","Fukun, Bi (35241807800); Mingyang, Lei (57210970624); Jiayi, Sun (57492344300)","35241807800; 57210970624; 57492344300","A robust and effective tracking method in remote sensing video sequences","2019","ACM International Conference Proceeding Series","","","","13","17","4","10.1145/3341016.3341022","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072062740&doi=10.1145%2f3341016.3341022&partnerID=40&md5=0e5ec64b065f08e89d014b95b9c53062","With the popularization of high resolution imaging technologyand the progress of artificial intelligence, remote sensing targettracking in the aerial video plays a very important role in publicsecurity, such as antiterrorism efforts and military reconnaissance.As aerial video has rapid changes in orientations, low resolution,and multiple similar disruptors, and the main tracking methodsgenerally have relatively low tracking performance in thisresearch field, we develop a robust tracking method for remotesensing videos based on a saliency enhanced multi-domainconvolutional neural network (SEMD). The process can bedivided into two main stages: (1) in the offline pretraining stage,we combine the Least Squares Generative Adversarial Networks(LSGANs) with a rotation strategy to augment typical easilyconfused negative samples, which can improve the capacity todistinguish between target and the background. (2) in the onlinetracking process, a saliency module is embedded betweenconvolutional layers and we optimize the arrangement of itsfunctional sub-modules to boost the saliency of the feature map,which improve the network representation power for rapiddynamic changes in the target. Comprehensive evaluations ofhomemade datasets demonstrate that the proposed method canachieve high efficiency and accuracy results compared to stateof-the-art methods. © 2019 Association for Computing Machinery.","Antennas; Artificial intelligence; Arts computing; Computer vision; Comprehensive evaluation; High-resolution imaging; Multi domains; Network representation; Robust tracking methods; Saliency enhanced; State-of-the-art methods; Visual Tracking; Remote sensing","Multi-domain learning; Saliency enhanced; Sample augmentation; Visual tracking","Conference paper","Final","","Scopus","2-s2.0-85072062740"
"Gu F.; Zhang H.; Wang C.; Wu F.","Gu, Feng (57205776909); Zhang, Hong (56179236500); Wang, Chao (55141316100); Wu, Fan (57104120200)","57205776909; 56179236500; 55141316100; 57104120200","SAR Image Super-Resolution Based on Noise-Free Generative Adversarial Network","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899202","2575","2578","3","10.1109/IGARSS.2019.8899202","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077718212&doi=10.1109%2fIGARSS.2019.8899202&partnerID=40&md5=307d0f20e5820821cb10bd710579743c","Deep learning has been successfully applied to the ordinary image super-resolution (SR). However, since the synthetic aperture radar (SAR) images are often disturbed by multiplicative noise known as speckle and more blurry than ordinary images, there are few deep learning methods for the SAR image SR. In this paper, a deep generative adversarial network (DGAN) is proposed to reconstruct the pseudo high-resolution (HR) SAR images. First, a generator network is constructed to remove the noise of low-resolution SAR image and generate HR SAR image. Second, a discriminator network is used to differentiate between the pseudo super-resolution images and the realistic HR images. The adversarial objective function is introduced to make the pseudo HR SAR images closer to real SAR images. The experimental results show that our method can maintain the SAR image content with high-level noise suppression. The performance evaluation based on peak signal-to-noise-ratio and structural similarity index shows the superiority of the proposed method to the conventional CNN baselines. © 2019 IEEE.","Deep learning; Geology; Learning systems; Optical resolving power; Remote sensing; Signal to noise ratio; Synthetic aperture radar; Adversarial networks; Image super resolutions; Multiplicative noise; Objective functions; Peak signal to noise ratio; Structural similarity indices; Super resolution; Synthetic aperture radar (SAR) images; Radar imaging","generative adversarial network; super-resolution; Synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85077718212"
"Wu M.; Jin X.; Jiang Q.; Lee S.-J.; Guo L.; Di Y.; Huang S.; Huang J.","Wu, Min (57195406241); Jin, Xin (56991832300); Jiang, Qian (57194699462); Lee, Shin-Jye (34877262700); Guo, Lin (57216424064); Di, Yide (57214795069); Huang, Shanshan (57214939600); Huang, Jinfang (57214807009)","57195406241; 56991832300; 57194699462; 34877262700; 57216424064; 57214795069; 57214939600; 57214807009","Remote Sensing Image Colorization Based on Multiscale SEnet GAN","2019","Proceedings - 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2019","","","8965902","","","","10.1109/CISP-BMEI48845.2019.8965902","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079164978&doi=10.1109%2fCISP-BMEI48845.2019.8965902&partnerID=40&md5=627e115a61c8277cdffe663082810f91","Image colorization technique is to colorize the grayscale images or single-channel images. In the research of image colorization, the coloring of remote sensing images is a challenging problem. This paper proposes a new method of remote sensing image colorization method based on Deep Convolution Generative Adversarial Network (DCGAN). We combine multi-scale convolution with Squeeze-and-Excitation Networks (SEnet) to propose a new model that is applied to the generator of DCGAN. Therefore, the generator not only retains the largest image features in the process of the generating images, but also can adjust the channel weights in the training process. We have compared the proposed method with other image colorization methods, and the results show that the proposed method has a good performance on both human vision and image evaluation indicators on the colorization of remote sensing images. © 2019 IEEE.","Biomedical engineering; Convolution; Feature extraction; Remote sensing; Adversarial networks; Gray-scale images; Image colorizations; Image evaluation; Image features; Remote sensing images; Single channels; Training process; Image processing","Feature extraction; Generative adversarial network; Image Colorization; Remote sensing; Squeeze-and-excitation networks","Conference paper","Final","","Scopus","2-s2.0-85079164978"
"Quan D.; Wang S.; Liang X.; Wang R.; Fang S.; Hou B.; Jiao L.","Quan, Dou (57192699561); Wang, Shuang (55940463600); Liang, Xuefeng (12243088300); Wang, Ruojing (57208228687); Fang, Shuai (57207883931); Hou, Biao (7102142690); Jiao, Licheng (7102491544)","57192699561; 55940463600; 12243088300; 57208228687; 57207883931; 7102142690; 7102491544","Deep generative matching network for optical and SAR image registration","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518653","6215","6218","3","10.1109/IGARSS.2018.8518653","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064167705&doi=10.1109%2fIGARSS.2018.8518653&partnerID=40&md5=850010c431fe88bb00ca2c02e2bf744d","Multimodal remote sensing images contain complementary information, thus, could potentially benefit many remote sensing applications. To this end, the image registration is a common requirement for utilizing the multimodal images. However, due to the rather different imaging mechanisms, multimodal image registration becomes much more challenging than ordinary registration, particular for optical and synthetic aperture radar (SAR) images. In this work, we design a deep matching network to exploit the latent and coherent features between multimodal patch pairs for inferring their matching labels. But, the network requires immense data for training, which is not usually met. To address this issue, we propose a generative matching network (GMN) to generate the coupled optical and SAR images, hence, improve the quantity and diversity of the training data. The experimental results show that our proposal significantly improves the registration performance of optical and SAR image registration, and achieves subpixel or close to subpixel error. © 2018 IEEE.","Geology; Image enhancement; Image registration; Pixels; Remote sensing; Synthetic aperture radar; Adversarial networks; Matching networks; Multi-modal image; Multimodal image registration; Optical; Registration performance; Remote sensing applications; Synthetic aperture radar (SAR) images; Radar imaging","Deep matching network; Generative adversarial network; Image registration; Multimodal images; Optical; SAR","Conference paper","Final","","Scopus","2-s2.0-85064167705"
"Zhang N.; Wang Y.; Zhang X.; Xu D.; Wang X.","Zhang, Ning (57188816117); Wang, Yongcheng (56437944700); Zhang, Xin (57774426900); Xu, Dongdong (56299205100); Wang, Xiaodong (57208088951)","57188816117; 56437944700; 57774426900; 56299205100; 57208088951","An Unsupervised Remote Sensing Single-Image Super-Resolution Method Based on Generative Adversarial Network","2020","IEEE Access","8","","8986554","29027","29039","12","10.1109/ACCESS.2020.2972300","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079744090&doi=10.1109%2fACCESS.2020.2972300&partnerID=40&md5=e9a0ab049e0b5d66745dacf6396fc44f","Image super-resolution (SR) technique can improve the spatial resolution of images without upgrading the imaging system. As a result, SR promotes the development of high resolution (HR) remote sensing image applications. Many remote sensing image SR algorithms based on deep learning have been proposed recently, which can effectively improve the spatial resolution under the constraints of HR images. However, images acquired by remote sensing imaging devices typically have lower resolution. Hence, an insufficient number of HR remote sensing images are available for training deep neural networks. In view of this problem, we propose an unsupervised SR method that does not require HR remote sensing images. The proposed method introduces a generative adversarial network (GAN) that obtains SR images through the generator; then, the SR images are downsampled to train the discriminator with low resolution (LR) images. Our method outperformed several methods in terms of the quality of the obtained SR images as measured by 6 evaluation metrics, which proves the satisfactory performance of the proposed unsupervised method for improving the spatial resolution of remote sensing images. © 2013 IEEE.","Deep learning; Deep neural networks; Image resolution; Optical resolving power; Remote sensing; Unsupervised learning; Adversarial networks; Evaluation metrics; Image super resolutions; Low resolution images; Remote sensing images; Remote sensing imaging; Spatial resolution; Unsupervised method; Image enhancement","generative adversarial network; Image super-resolution; remote sensing; unsupervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85079744090"
"Belete Bejiga M.; Melgani F.","Belete Bejiga, Mesay (57192697078); Melgani, Farid (35613488300)","57192697078; 35613488300","GaN-based domain adaptation for object classification","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518649","1264","1267","3","10.1109/IGARSS.2018.8518649","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064183505&doi=10.1109%2fIGARSS.2018.8518649&partnerID=40&md5=75cb981dde6b10d5f044e416bdb451f3","Recent trends in image classification focus on training deep neural networks that require having a large amount of training images related to the considered task. However, obtaining enough labeled image samples is often time-consuming and expensive. An alternative solution proposed is to transfer the knowledge learned while solving one problem to another but related problem, also called transfer learning. Domain adaptation is a type of transfer learning that deals with learning a model that performs well on two datasets that have different (but somehow correlated) data distributions. In this work, we present a new domain adaptation method based on generative adversarial networks (GANs) in the context of aerial image classification. Experimental results obtained on two datasets for a single object scenario show that the proposed method is particularly promising. © 2018 IEEE","Antennas; Deep learning; Deep neural networks; Gallium nitride; Geology; III-V semiconductors; Remote sensing; Adversarial networks; Alternative solutions; Data distribution; Domain adaptation; Labeled images; Object classification; Training image; Transfer learning; Image classification","Deep learning; Domain adaptation; GAN; Transfer learning","Conference paper","Final","","Scopus","2-s2.0-85064183505"
"Xu S.; Mu X.; Chai D.; Zhang X.","Xu, Suhui (57157826100); Mu, Xiaodong (57074918400); Chai, Dong (55177722200); Zhang, Xiongmei (24537804700)","57157826100; 57074918400; 55177722200; 24537804700","Remote sensing image scene classification based on generative adversarial networks","2018","Remote Sensing Letters","9","7","","617","626","9","10.1080/2150704X.2018.1453173","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047012932&doi=10.1080%2f2150704X.2018.1453173&partnerID=40&md5=3af348c80cd3fa59cc73d0dc3c767d1c","Scene classification of remote sensing images plays an important role in many remote sensing image applications. Training a good classifier needs a large number of training samples. The labeled samples are often scarce and difficult to obtain, and annotating a large number of samples is time-consuming. We propose a novel remote sensing image scene classification framework based on generative adversarial networks (GAN) in this paper. GAN can improve the generalization ability of machine learning network model. However, generating large-size images, especially high-resolution remote sensing images is difficult. To address this issue, the scaled exponential linear units (SELU) are applied into the GAN to generate high quality remote sensing images. Experiments carried out on two datasets show that our approach can obtain the state-of-the-art results compared with the classification results of the classic deep convolutional neural networks, especially when the number of training samples is small. © 2018 Informa UK Limited, trading as Taylor & Francis Group.","Classification (of information); Deep neural networks; Image classification; Neural networks; Sampling; Adversarial networks; Classification results; Deep convolutional neural networks; Generalization ability; High resolution remote sensing images; Number of samples; Remote sensing images; Scene classification; artificial neural network; experimental study; image analysis; image classification; machine learning; network analysis; numerical model; remote sensing; sampling; Remote sensing","","Article","Final","","Scopus","2-s2.0-85047012932"
"Zhang Y.; Li X.; Zhou J.","Zhang, Yutian (57210126022); Li, Xiaohua (55718203400); Zhou, Jiliu (21234416400)","57210126022; 55718203400; 21234416400","SFTGAN: A generative adversarial network for pan-sharpening equipped with spatial feature transform layers","2019","Journal of Applied Remote Sensing","13","2","026507","","","","10.1117/1.JRS.13.026507","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069457255&doi=10.1117%2f1.JRS.13.026507&partnerID=40&md5=81b55330e192044dfa7b6f388c1ef808","Pan-sharpening is an indispensable technology for remote sensing that aims to combine low-resolution multispectral images and high-resolution panchromatic images to create a multispectral image with high resolution. However, pan-sharpening approaches often encounter spectral distortion and detail distortion issues. In order to overcome the drawbacks of pan-sharpening methodologies, we propose an end-to-end pan-sharpening model consisting of an effective generative adversarial network architecture equipped with spatial feature transform layers that generate spatial detail features under spectral feature constraints. Through a large number of quantitative and visual assessments, we demonstrate that the proposed method achieves superior performance to other state-of-the-art methods. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Image fusion; Network architecture; Adversarial networks; Low resolution multispectral images; Multispectral images; Pan-sharpening; Panchromatic images; Spatial features; Spectral distortions; State-of-the-art methods; Remote sensing","deep learning; multispectral image; pan-sharpening; remote image fusion; spatial feature transform","Article","Final","","Scopus","2-s2.0-85069457255"
"Zhang Y.; Li X.; Zhang Q.","Zhang, Yang (57207764722); Li, Xiang (57192491402); Zhang, Qianyu (57207758614)","57207764722; 57192491402; 57207758614","Road topology refinement via a multi-conditional generative adversarial network","2019","Sensors (Switzerland)","19","5","1162","","","","10.3390/s19051162","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062873248&doi=10.3390%2fs19051162&partnerID=40&md5=7b01d4ceb5b31a247790effde1d44ced","With the rapid development of intelligent transportation, there comes huge demands for high-precision road network maps. However, due to the complex road spectral performance, it is very challenging to extract road networks with complete topologies. Based on the topological networks produced by previous road extraction methods, in this paper, we propose a Multi-conditional Generative Adversarial Network (McGAN) to obtain complete road networks by refining the imperfect road topology. The proposed McGAN, which is composed of two discriminators and a generator, takes both original remote sensing image and the initial road network produced by existing road extraction methods as input. The first discriminator employs the original spectral information to instruct the reconstruction, and the other discriminator aims to refine the road network topology. Such a structure makes the generator capable of receiving both spectral and topological information of the road region, thus producing more complete road networks compared with the initial road network. Three different datasets were used to compare McGan with several recent approaches, which showed that the proposed method significantly improved the precision and recall of the road networks, and also worked well for those road regions where previous methods could hardly obtain complete structures. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Extraction; Feature extraction; Motor transportation; Remote sensing; Roads and streets; Topology; Adversarial networks; Intelligent transportation; Remote sensing images; Road extraction method; Road network extraction; Road topology refinement; Topological information; Topological networks; Discriminators","Multi-conditional generative adversarial network; Road network extraction; Road topology refinement","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85062873248"
"Zhan Y.; Wu K.; Liu W.; Qin J.; Yang Z.; Medjadba Y.; Wang G.; Yu X.","Zhan, Ying (57197867950); Wu, Kang (57200601886); Liu, Wei (57835236600); Qin, Jin (57208224824); Yang, Zhaoying (57200607038); Medjadba, Yasmine (57208224017); Wang, Guian (25930227300); Yu, Xianchuan (12785792300)","57197867950; 57200601886; 57835236600; 57208224824; 57200607038; 57208224017; 25930227300; 12785792300","Semi-supervised classification of hyperspectral data based on generative adversarial networks and neighborhood majority voting","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518846","5756","5759","3","10.1109/IGARSS.2018.8518846","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064156520&doi=10.1109%2fIGARSS.2018.8518846&partnerID=40&md5=6c1fc214faf8ae409ab95b27173bfa78","How to classify hyperspectral images using few training samples is an important and challenging problem because the collection of the samples is difficult and expensive. Because semi-supervised approaches can utilize information contained in the unlabeled samples and labeled samples, it is a suitable choice. A novel semi-supervised spectral-spatial classification method for hyperspectral data based on generative adversarial network (GAN) is proposed in this paper. First, we use a custom one-dimensional GAN to train the hyperspectral data to obtain spectral features. After using a new small convolutional neural network (CNN) to classify the spectral features, we use a new classification method based on a majority voting strategy further to improve the classification result. The performance of our method is evaluated on ROSIS image data, and the results show that the proposed method can acquire satisfactory results when compared with traditional methods using a few of labeled samples. © 2018 IEEE.","Deep learning; Geology; Image classification; Machine learning; Neural networks; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification methods; Classification results; Convolutional neural network; HyperSpectral; Semi-supervised classification; Semi-supervised learning (SSL); Spectral-spatial classification; Classification (of information)","Deep learning; Generative adversarial networks (GAN); Hyperspectral images classification; Semi-supervised learning (SSL); Spectral-spatial classification","Conference paper","Final","","Scopus","2-s2.0-85064156520"
"Tsagkatakis G.; Aidini A.; Fotiadou K.; Giannopoulos M.; Pentari A.; Tsakalides P.","Tsagkatakis, Grigorios (34870845200); Aidini, Anastasia (57203761304); Fotiadou, Konstantina (25824915900); Giannopoulos, Michalis (57205378265); Pentari, Anastasia (56442653900); Tsakalides, Panagiotis (6701848334)","34870845200; 57203761304; 25824915900; 57205378265; 56442653900; 6701848334","Survey of deep-learning approaches for remote sensing observation enhancement","2019","Sensors (Switzerland)","19","18","3929","","","","10.3390/s19183929","82","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072555757&doi=10.3390%2fs19183929&partnerID=40&md5=857b0d03e174ccd53dc03eb73ca52949","Deep Learning, and Deep Neural Networks in particular, have established themselves as the new norm in signal and data processing, achieving state-of-the-art performance in image, audio, and natural language understanding. In remote sensing, a large body of research has been devoted to the application of deep learning for typical supervised learning tasks such as classification. Less yet equally important effort has also been allocated to addressing the challenges associated with the enhancement of low-quality observations from remote sensing platforms. Addressing such channels is of paramount importance, both in itself, since high-altitude imaging, environmental conditions, and imaging systems trade-offs lead to low-quality observation, as well as to facilitate subsequent analysis, such as classification and detection. In this paper, we provide a comprehensive review of deep-learning methods for the enhancement of remote sensing observations, focusing on critical tasks including single and multi-band super-resolution, denoising, restoration, pan-sharpening, and fusion, among others. In addition to the detailed analysis and comparison of recently presented approaches, different research avenues which could be explored in the future are also discussed. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Chemical detection; Data handling; Deep learning; Deep neural networks; Economic and social effects; Fusion reactions; Neural networks; Optical resolving power; Quality control; Adversarial networks; Convolutional neural network; De-noising; Earth observations; Pan-sharpening; Satellite imaging; Super resolution; Remote sensing","Convolutional neural networks; Deep learning; Denoising; Earth observations; Fusion; Generative adversarial networks; Pan-sharpening; Satellite imaging; Super-resolution","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85072555757"
"Chen G.; Liu L.; Hu W.; Pan Z.","Chen, Guowei (57208237242); Liu, Lei (56194753300); Hu, Wenlong (47962223000); Pan, Zongxu (54788169800)","57208237242; 56194753300; 47962223000; 54788169800","Semi-supervised object detection in remote sensing images using generative adversarial networks","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519132","2503","2506","3","10.1109/IGARSS.2018.8519132","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064277703&doi=10.1109%2fIGARSS.2018.8519132&partnerID=40&md5=7bd12bd7e2dd47a8aa2cee0aa07aa4b3","Object detection is a challenging task in computer vision. Now many detection networks can get a good detection result when applying large training dataset. However, annotating sufficient amount of data for training is often time-consuming. To address this problem, a semi-supervised learning based method is proposed in this paper. Semi-supervised learning trains detection networks with few annotated data and massive amount of unannotated data. In the proposed method, Generative Adversarial Network is applied to extract data distribution from unannotated data. The extracted information is then applied to improve the performance of detection network. Experiment shows that the method in this paper greatly improves the detection performance compared w1ith supervised learning using only few annotated data. The results prove that it is possible to achieve acceptable detection result when only few target object is annotated in the training dataset. © 2018 IEEE.","Geology; Large dataset; Machine learning; Neural networks; Object recognition; Remote sensing; Supervised learning; Adversarial networks; Convolutional neural network; Data distribution; Detection networks; Detection performance; Remote sensing images; Semi- supervised learning; Training dataset; Object detection","Convolutional neural networks (CNN); Generative adversarial networks (GAN); Objet detection; Semi-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85064277703"
"Sharafi S.; Majidi B.; Movaghar A.","Sharafi, Shahab (57209638606); Majidi, Babak (15845948800); Movaghar, Ali (6603397985)","57209638606; 15845948800; 6603397985","Low Altitude Aerial Scene Synthesis Using Generative Adversarial Networks for Autonomous Natural Resource Management","2019","2019 IEEE 5th Conference on Knowledge Based Engineering and Innovation, KBEI 2019","","","8734904","322","326","4","10.1109/KBEI.2019.8734904","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068340213&doi=10.1109%2fKBEI.2019.8734904&partnerID=40&md5=afa986e04a7d6dd53b4d065ce1f74d49","Deep neural networks are currently the best solution for aerial scene interpretation for Unmanned Aerial Vehicle (UAV) based remote sensing. A problem faced by the deep neural networks is that the deep models require significantly large training datasets which should cover almost all of the scenarios. Gathering these datasets is usually very time consuming and expensive. In this paper, data augmentation and generative adversarial network are used for autonomous synthesis of low altitude aerial scenes for creating a training dataset for deep low altitude aerial video interpretation. The proposed system is evaluated using a real world scenario of road following under foliage in a jungle and the experimental results show that the proposed framework is capable of producing high accuracy training datasets for UAV vision system in natural resource management scenarios. © 2019 IEEE.","Deep learning; Deep neural networks; Engineering research; Knowledge based systems; Large dataset; Natural resources management; Remote sensing; Resource allocation; Unmanned aerial vehicles (UAV); Adversarial networks; Data augmentation; Natural environments; Natural resource management; Real-world scenario; Scene interpretation; Training data sets; Training dataset; Antennas","deep learning; generative adversarial network; natural environment management; remote sensing","Conference paper","Final","","Scopus","2-s2.0-85068340213"
"Bittner K.; Körner M.; Fraundorfer F.; Reinartz P.","Bittner, Ksenia (57194603356); Körner, Marco (57190168095); Fraundorfer, Friedrich (8977349800); Reinartz, Peter (56216874200)","57194603356; 57190168095; 8977349800; 56216874200","Multi-task cGAN for simultaneous spaceborne DSM refinement and roof-type classification","2019","Remote Sensing","11","11","1262","","","","10.3390/rs11111262","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067397435&doi=10.3390%2frs11111262&partnerID=40&md5=154191cfa925592748eb801a5f60e663","Various deep learning applications benefit from multi-task learning with multiple regression and classification objectives by taking advantage of the similarities between individual tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models compared to separately trained models. In this paper, we make an observation of such influences for important remote sensing applications like elevation model generation and semantic segmentation tasks from the stereo half-meter resolution satellite digital surface models (DSMs). Mainly, we aim to generate good-quality DSMs with complete, as well as accurate level of detail (LoD)2-like building forms and to assign an object class label to each pixel in the DSMs. For the label assignment task, we select the roof type classification problem to distinguish between flat, non-flat, and background pixels. To realize those tasks, we train a conditional generative adversarial network (cGAN) with an objective function based on least squares residuals and an auxiliary term based on normal vectors for further roof surface refinement. Besides, we investigate recently published deep learning architectures for both tasks and develop the final end-to-end network, which combines different models, as using them first separately, they provide the best results for their individual tasks. © 2019 by the authors.","Pixels; Remote sensing; Roofs; Satellite imagery; Semantics; Stereo image processing; 3D scenes; Adversarial networks; Digital surface models; Multitask learning; Semantic segmentation; Type classifications; Urban regions; Deep learning","3D scene refinement; Conditional generative adversarial networks; Digital surface model; Multi-task learning; Roof type classification; Satellite imagery; Semantic segmentation; Urban region","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85067397435"
"Chen Y.; Ouyang X.; Agam G.","Chen, Ying (57207798471); Ouyang, Xu (57202459862); Agam, Gady (7005725869)","57207798471; 57202459862; 7005725869","ChangeNet: Learning to detect changes in satellite images","2019","Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, GeoAI 2019","","","","24","31","7","10.1145/3356471.3365232","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075583040&doi=10.1145%2f3356471.3365232&partnerID=40&md5=82bdb576530ea87d4ce3c1dd62f75d2a","Change detection in temporal sequences of satellite images is an important component of many remote sensing applications such as land cover monitoring, urban expansion evaluation, forest degradation assessment, and mine site monitoring. The objective of this paper is to localize and identify relevant pixelwise changes in time-varying images taken at the same location. Detecting relevant change in images is difficult due to ""unimportant"" or ""nuisance"" forms of change such as illumination variation, shadows, occlusion, and possible seasonal changes. Traditional methods for change detection require sophisticated image preprocessing and possibly manual interaction. In this work, we present an end-to-end approach for dense change detection in satellite images by employing conditional Generative Adversarial Networks. We use the conditional GAN network to improve classification results by closing the gap between expected and predicted label distributions. Experimental results show that the proposed method achieves better performance compared with existing methods. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Remote sensing; Urban growth; Adversarial networks; Change detection; ChangeNet; Classification results; Illumination variation; Image preprocessing; Remote sensing applications; Satellite images; Satellites","Change detection; ChangeNet; DNN; GAN; Satellite images; U-Net","Conference paper","Final","","Scopus","2-s2.0-85075583040"
"Bischke B.; Helber P.; Koenig F.; Borth D.; Dengel A.","Bischke, Benjamin (57163820000); Helber, Patrick (57197818391); Koenig, Florian (57062234800); Borth, Damian (25652857800); Dengel, Andreas (6603764314)","57163820000; 57197818391; 57062234800; 25652857800; 6603764314","Overcoming missing and incomplete modalities with generative adversarial networks for building footprint segmentation","2018","Proceedings - International Workshop on Content-Based Multimedia Indexing","2018-September","","8516271","","","","10.1109/CBMI.2018.8516271","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057046144&doi=10.1109%2fCBMI.2018.8516271&partnerID=40&md5=1a0150bd64784474a869a2c57c86580f","The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality. © 2018 IEEE.","Data fusion; Indexing (of information); Remote sensing; Semantics; Adversarial networks; Building footprint; Data collection process; Missing Modalities; Multi-modal fusion; Predictive accuracy; Semantic segmentation; Spatial resolution; Image segmentation","Generative Adversarial Networks; Missing Modalities; Semantic Segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85057046144"
"Teng W.; Wang N.; Chen T.; Wang B.; Chen M.; Shi H.","Teng, Wenxiu (57223020753); Wang, Ni (57094386800); Chen, Taisheng (57748841100); Wang, Benlin (57209587716); Chen, Menglin (56134553900); Shi, Huihui (57205125773)","57223020753; 57094386800; 57748841100; 57209587716; 56134553900; 57205125773","Deep adversarial domain adaptation method for cross-domain classification in high-resolution remote sensing images","2019","Laser and Optoelectronics Progress","56","11","112801","","","","10.3788/LOP56.112801","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068152367&doi=10.3788%2fLOP56.112801&partnerID=40&md5=fac8792b507adb03463b66b6bb9dd345","In this study, a deep adversarial domain adaptation method is proposed for cross-domain classification in high-resolution remote sensing images. A deep convolutional neural network VGG16 is used to learn the deep features of scene images. The adversarial learning method is used to minimize the difference of feature distribution between source and target domains. RSI-CB256 ( Remote Sensing Image Classification Benchmark), NWPU- RES1SC45( Northwestern Polytcchnical University Remote Sensing Image Scene Classification) and AID ( Aerial Image data set) are used as source domain datasets, and UC-Mcrccd(University of California, Mcrccd)and WHU- RS 19( Wuhan University Remote Scnsing)arc used as target domain datasets. The experimental results denote that the proposed method can improve the generalization ability of the model for target domain dataset without labels. © 2019 Universitat zu Koln. All rights reserved.","","Convolutional neural network; Generative adversarial networks; Remote sensing; Scene classification; Unsupervised domain adaptation","Article","Final","","Scopus","2-s2.0-85068152367"
"Kim S.; Park S.; Yu K.","Kim, Seongyong (57196230822); Park, Seula (56867783600); Yu, Kiyun (24177307200)","57196230822; 56867783600; 24177307200","Proposal for a method of extracting road layers from remote sensing images using conditional GANs","2018","ACM International Conference Proceeding Series","","","","84","87","3","10.1145/3193025.3193051","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048385485&doi=10.1145%2f3193025.3193051&partnerID=40&md5=7bf11ceaa3238041f7d7bb13ed3514aa","With the recent advances in unmanned aerial vehicle (UAV) technology, remote sensing images have become relatively easy to obtain and their accuracy has increased enough to be able to handle land information. Therefore, there is a growing demand to utilize remote sensing images for extracting semantic objects Conventional methods are mainly focused on pixel-based classification and recently people commonly use convolutional neural networks, which post processing is required to linearize roads that are cut off and accurately shape the contours of buildings. We propose the use of a generative model to carry out this post processing in the networks. Using conditional Generative Adversarial Network (GANs), we translate remote sensing images into map-based images from which roads are easily extracted, while retaining the underlying structure. Next, we extract road layers from the generated images. Through this approach, it is possible to achieve the same effect as if complicating post processing were done in the networks during the object extraction process. © 2018 Association for Computing Machinery.","Antennas; Deep learning; Digital signal processing; Neural networks; Remote sensing; Roads and streets; Semantics; Unmanned aerial vehicles (UAV); Adversarial networks; Conventional methods; Convolutional neural network; Extracting roads; Object extraction; Pixel based classifications; Remote sensing images; Satellite images; Image processing","Deep learning; Extracting roads; Generative adversarial networks; Remote sensing; Satellite images","Conference paper","Final","","Scopus","2-s2.0-85048385485"
"Bi X.; Zhou Z.","Bi, Xiaojun (35267810500); Zhou, Zeyu (57210928009)","35267810500; 57210928009","Hyperspectral Image Classification Algorithm Based on Two-Channel Generative Adversarial Network; [基于双通道GAN的高光谱图像分类算法]","2019","Guangxue Xuebao/Acta Optica Sinica","39","10","1028002","","","","10.3788/AOS201939.1028002","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075142028&doi=10.3788%2fAOS201939.1028002&partnerID=40&md5=c098b401c5573efc54b9a765b26618f2","The existing hyperspectral image generative adversarial network(GAN) classification algorithm cannot fully extract spectral and spatial-spectral features, which leads to the degradation of hyperspectral image classification accuracy. To resolve this issue, this study proposes a hyperspectral image classification algorithm based on a two-channel GAN. Improved one- and two-dimensional GAN classification frameworks are used to extract complete spectral and spatial-spectral features, respectively. Those features are nonlinearly fused to form a more comprehensive spatial-spectral features for classification. The experiments on two commonly used hyperspectral image datasets show that the proposed algorithm achieves the best classification accuracy; further, the results verify the effectiveness and advantages of the proposed algorithm. © 2019, Chinese Lasers Press. All right reserved.","Hyperspectral imaging; Image classification; Remote sensing; Spectroscopy; Adversarial networks; Classification accuracy; Classification algorithm; Classification framework; Spectral feature; Two channel; Classification (of information)","Classification; Generative adversarial network; Hyperspectral image; Remote sensing; Spatial-spectral features","Article","Final","","Scopus","2-s2.0-85075142028"
"Fick R.; Gader P.; Zare A.; Meerdink S.","Fick, Ronald (57195309298); Gader, Paul (7006867196); Zare, Alina (14069307000); Meerdink, Susan (57190965732)","57195309298; 7006867196; 14069307000; 57190965732","Temporal Mapping of Hyperspectral Data","2019","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2019-September","","8921373","","","","10.1109/WHISPERS.2019.8921373","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077582716&doi=10.1109%2fWHISPERS.2019.8921373&partnerID=40&md5=d665498e6c95a17a41a4d0115f999fee","The increasing popularity of hyperspectral sensors is dramatically increasing the temporal availability of data. To date, algorithms struggle to compare hyperspectral data collected across dates due to different environmental conditions during collection. In this work, we develop a temporal mapping in order to map data collected from one year to a different year. We investigated both conditional generative adversarial networks (cGANs) as well as affine transformations to perform this mapping. Both methods showed an improvement over using data from past collections without mapping, with cGANs outperforming the affine transformation. © 2019 IEEE.","Mapping; Metadata; Remote sensing; Spectroscopy; Adversarial networks; Affine transformations; Environmental conditions; Hyperspectral Data; Hyperspectral sensors; Map data; Temporal availability; Temporal mappings; Hyperspectral imaging","","Conference paper","Final","","Scopus","2-s2.0-85077582716"
"Requena-Mesa C.; Reichstein M.; Mahecha M.; Kraft B.; Denzler J.","Requena-Mesa, C. (57208244647); Reichstein, M. (57206534330); Mahecha, M. (16444433900); Kraft, B. (57207884535); Denzler, J. (6701534437)","57208244647; 57206534330; 16444433900; 57207884535; 6701534437","Predicting landscapes as seen from space from environmental conditions","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519427","1768","1771","3","10.1109/IGARSS.2018.8519427","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061616822&doi=10.1109%2fIGARSS.2018.8519427&partnerID=40&md5=451e8f72ed938b70ac10d3cf76deb8ee","Satellite images are information rich snapshots of ecosystems and landscapes. In consequence, the features in the images strongly depend on the environmental conditions. Such dependency between climate and landscapes has been regarded since the beginning of earth sciences; however, it has never been taken as literally as in the present study. We adapted a deep learning generative model as a first demonstration of the potential behind deep learning for spatial pattern generation in geoscience. The purpose is to build a conditional Generative Adversarial Network (cGAN) useful to establish the relationship between two loosely linked set of variables that show multitude of complex spatial features such as climate conditions to aerial image. We trained a custom cGAN to generate Sentinel-2 multispectral imagery given a set of climatic and terrain predictors. Results show that the generated imagery shares many characteristics with the real one. In some cases, the quality of the generated imagery is high enough to deceive humans. We envision that such use of deep learning for geoscience could become an important tool to test the effects of climate on landscapes and ecosystems. © 2018 IEEE.","Antennas; Earth (planet); Ecosystems; Geology; Remote sensing; Satellite imagery; Space optics; Adversarial networks; Climate; Climate condition; Environmental conditions; Generative model; Landscape ecology; Multi-spectral imagery; Sentinel 2; Deep learning","Climate; Deep learning; GAN; Landscape ecology; Satellite imagery; Sentinel 2","Conference paper","Final","","Scopus","2-s2.0-85061616822"
"Grohnfeldt C.; Schmitt M.; Zhu X.","Grohnfeldt, Claas (55946211600); Schmitt, Michael (7401931279); Zhu, Xiaoxiang (55696622200)","55946211600; 7401931279; 55696622200","A conditional generative adversarial network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 images","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519215","1726","1729","3","10.1109/IGARSS.2018.8519215","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056354296&doi=10.1109%2fIGARSS.2018.8519215&partnerID=40&md5=86a96bdfdcfcff984120e3d9c682d5d1","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and hazefree MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-1 SAR data confirm that our extended SAR-OptcGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input. © 2018 IEEE.","Data fusion; Deep learning; Geology; Network architecture; Remote sensing; Synthetic aperture radar; Adversarial networks; Cloudremoval; Equivalent model; Multi-spectral; Optical data; Optical remote sensing; Sentinel-1; Single sensor; Radar imaging","Cloudremoval; Data fusion; Deep learning; Generative adversarial network (GAN); Optical remote sensing; SAR","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85056354296"
"Xia H.; Liu C.","Xia, Haiying (24538087700); Liu, Chenxu (57208471306)","24538087700; 57208471306","Remote sensing image deblurring algorithm based on WGAN","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11434 LNCS","","","113","125","12","10.1007/978-3-030-17642-6_10","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064861662&doi=10.1007%2f978-3-030-17642-6_10&partnerID=40&md5=915c818670c878b02bc68d4f278e1ba2","Remote sensing images are blurred due to large and wide imaging, long shooting distance, fast scanning speed, interference from external light, etc. At the same time, because of that remote sensing images have the characteristics of diverse and dense shooting objects, deblurring remote sensing images is a major problem in remote sensing research. Therefore, we propose a remote sensing image deblurring algorithm, which based on WGAN. The algorithm is different from the traditional method in estimating the blur kernel of image. What’s more our method does not require an explicit estimation of the blur kernel, and it implements an end-to-end image deblurring process. We use a WGAN-based deblurring model. First, the training images are processed in pairs. Then, in order to increase the generalization ability, a image of 256 * 256 that is a sub-region cropped at the random position in the original image is chosen as the input image. Finally, to achieve a better deblurring effect, a content loss function and a perceptual loss function are added to the loss function to achieve the specific implementation. The remote sensing image deblurring model trained by the proposed method has achieved better results on the remote sensing image dataset. The experimental results show that the proposed algorithm have better performance than the traditional method in filtering out the blur of remote sensing images, which could optimize the overall visual effect subjectively and improve the peak signal-to-noise ratio of the image objectively. © Springer Nature Switzerland AG 2019.","Deep learning; Distributed computer systems; Image segmentation; Quality of service; Remote sensing; Signal to noise ratio; Adversarial networks; Deblurring; Generalization ability; Image deblurring; Motion blur; Peak signal to noise ratio; Remote sensing images; Shooting distance; Image enhancement","Deblurring; Deep learning; Generative Adversarial Networks; Motion blur; Remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85064861662"
"","","","18th International Conference on Geometry and Graphics, ICGG 2018","2019","Advances in Intelligent Systems and Computing","809","","","","","2294","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050589165&partnerID=40&md5=929a51e34c8b752ec9767afa4e0ade86","The proceedings contain 220 papers. The special focus in this conference is on Geometry and Graphics. The topics include: Interactive visualization for analysis of air traffic model; geometrical universality of Truss-Z system; interaction between 3D printing and geometry studies; the story of NanoJesus: Combining 3D scanning and femtosecond laser nanolithography for the fabrication of the smallest nativity scene in the world; geometrically constrained surface (re)construction; challenges in modelling complex geometry in historical buildings for numerical simulations; transient finite elements analysis of thin-walled structure in selective laser melting process; three-dimensional modeling and finite element analysis of cycloid hyperbolic Arch Dam; mixing of fluids with space-filling curves; electronic geometric modeling is the basis of modern geometric-graphic education in the technical university; graphical model of the biquadratic transformation; the optimization algorithm for gait planning and foot trajectory on the quadruped robot; analytic approach to finding lines in images and estimating their uncertainty; geometric fitting, registration and identification for large sets of spatial measurement data; 3-D reconstruction of bubbles inside valve based on multi-mirrors imaging; Facial expression recognition base on weighted KNN and RF; directing scheme support method based on analysis of angry characters; generating videos based on convolutional recurrent generative adversarial networks; study on adaptive threshold image processing method for detection of vaccine activity; a creo-based modeling and toolpath generating system for the nc whirling process of propeller blades; aircraft target recognition in remote sensing images based on saliency maps and invariant moments; curves with special aesthetics generated by an original mechanism.","","","Conference review","Final","","Scopus","2-s2.0-85050589165"
"Ma W.; Pan Z.; Yuan F.; Lei B.","Ma, Wen (57207877267); Pan, Zongxu (54788169800); Yuan, Feng (57214435272); Lei, Bin (14063767500)","57207877267; 54788169800; 57214435272; 14063767500","Super-resolution of remote sensing images via a dense residual generative adversarial network","2019","Remote Sensing","11","21","2578","","","","10.3390/rs11212578","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074641655&doi=10.3390%2frs11212578&partnerID=40&md5=bca7c02fd281744f4a4761f26f63a6d7","Single image super-resolution (SISR) has been widely studied in recent years as a crucial technique for remote sensing applications. In this paper, a dense residual generative adversarial network (DRGAN)-based SISR method is proposed to promote the resolution of remote sensing images. Different from previous super-resolution (SR) approaches based on generative adversarial networks (GANs), the novelty of our method mainly lies in the following factors. First, we made a breakthrough in terms of network architecture to improve performance. We designed a dense residual network as the generative network in GAN, which can make full use of the hierarchical features from low-resolution (LR) images. We also introduced a contiguous memory mechanism into the network to take advantage of the dense residual block. Second, we modified the loss function and altered the model of the discriminative network according to theWasserstein GAN with a gradient penalty (WGAN-GP) for stable training. Extensive experiments were performed using the NWPU-RESISC45 dataset, and the results demonstrated that the proposed method outperforms state-of-the-art methods in terms of both objective evaluation and subjective perspective. © 2019 by the authors.","Network architecture; Optical resolving power; Adversarial networks; Dense residual network (DRN); Remote sensing images; Single images; Wasserstein GAN with gradient penalty (WGAN-GP); Remote sensing","Dense residual network (DRN); Generative adversarial network (GAN); Remote sensing images; Single image super-resolution (SISR); Wasserstein GAN with gradient penalty (WGAN-GP)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85074641655"
"Benjdira B.; Bazi Y.; Koubaa A.; Ouni K.","Benjdira, Bilel (57193950453); Bazi, Yakoub (8213665300); Koubaa, Anis (15923354900); Ouni, Kais (6505828746)","57193950453; 8213665300; 15923354900; 6505828746","Unsupervised domain adaptation using generative adversarial networks for semantic segmentation of aerial images","2019","Remote Sensing","11","11","1369","","","","10.3390/rs11111369","106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067379111&doi=10.3390%2frs11111369&partnerID=40&md5=26b202297cb9a3a5b12cafc9b7a2e581","Segmenting aerial images is of great potential in surveillance and scene understanding of urban areas. It provides a mean for automatic reporting of the different events that happen in inhabited areas. This remarkably promotes public safety and traffic management applications. After the wide adoption of convolutional neural networks methods, the accuracy of semantic segmentation algorithms could easily surpass 80% if a robust dataset is provided. Despite this success, the deployment of a pretrained segmentation model to survey a new city that is not included in the training set significantly decreases accuracy. This is due to the domain shift between the source dataset on which the model is trained and the new target domain of the new city images. In this paper, we address this issue and consider the challenge of domain adaptation in semantic segmentation of aerial images. We designed an algorithm that reduces the domain shift impact using generative adversarial networks (GANs). In the experiments, we tested the proposed methodology on the International Society for Photogrammetry and Remote Sensing (ISPRS) semantic segmentation dataset and found that our method improves overall accuracy from 35% to 52% when passing from the Potsdam domain (considered as source domain) to the Vaihingen domain (considered as target domain). In addition, the method allows efficiently recovering the inverted classes due to sensor variation. In particular, it improves the average segmentation accuracy of the inverted classes due to sensor variation from 14% to 61%. © 2019 by the authors.","Aerial photography; Antennas; Convolution; Neural networks; Remote sensing; Semantic Web; Semantics; Adversarial networks; Aerial imagery; Convolutional neural network; Domain adaptation; Semantic segmentation; Image segmentation","Aerial imagery; Convolutional neural networks; Domain adaptation; Gener ative adversarial networks; Semantic segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85067379111"
"Schwegmann C.P.; Kleynhans W.; Salmon B.P.; Mdakane L.W.; Meyer R.G.V.","Schwegmann, C.P. (56041093900); Kleynhans, W. (16028782500); Salmon, B.P. (23010220500); Mdakane, L.W. (57188721487); Meyer, R.G.V. (57192701679)","56041093900; 16028782500; 23010220500; 57188721487; 57192701679","Synthetic aperture radar ship discrimination, generation and latent variable extraction using information maximizing generative adversarial networks","2017","International Geoscience and Remote Sensing Symposium (IGARSS)","2017-July","","8127440","2263","2266","3","10.1109/IGARSS.2017.8127440","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025662781&doi=10.1109%2fIGARSS.2017.8127440&partnerID=40&md5=60782cb9bdaf5d8b79cc504a0a6ff5e4","A major task in any discrimination scenario requires the collection and validation of as many examples as possible. Depending on the type of data this can be a time consuming process, especially when dealing with large remote sensing data such as Synthetic Aperture Radar imagery. To aid in the creation of improved machine learning-based ship detection and discrimination methods this paper applies a type of neural network known as an Information Maximizing Generative Adversarial Network. Generative Adversarial Networks pit a generating and discriminating network against each other. A generator tries to create samples that are indistinguishable from real data whereas the discriminator tries to identify whether a sample is real or generated. Information Maximizing Generative Adversarial Network extend this idea by extracting untangled latent variables as part of the discrimination process which help to classify the data in terms of categories/classes and properties such as ship rotation. Despite the limited size and class distribution of the dataset, the paper showed that the trained network was able to generate convincing samples from the three given classes as well as create a discriminator that performs similarly to state-of-the-art ship discrimination methods despite using no labels for training. © 2017 IEEE.","","Machine learning; Marine technology; Synthetic aperture radar","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85025662781"
"Zheng C.; Jiang X.; Zhang Y.; Liu X.; Yuan B.; Li Z.","Zheng, Ce (57672921100); Jiang, Xue (55724182500); Zhang, Ye (57214253643); Liu, Xingzhao (57770107000); Yuan, Bin (36109487100); Li, Zhixin (57241049300)","57672921100; 55724182500; 57214253643; 57770107000; 36109487100; 57241049300","Self-normalizing generative adversarial network for super-resolution reconstruction of SAR images","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900084","1911","1914","3","10.1109/IGARSS.2019.8900084","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104026398&doi=10.1109%2fIGARSS.2019.8900084&partnerID=40&md5=d8052b974df78aea9e33f718d307e7e5","High-resolution images with abundant detailed information are necessary elements for various applications of synthetic aperture radar (SAR). In this paper, a novel super-resolution image reconstruction method based on self-normalizing generative adversarial network (SNGAN) is proposed. Compared with other published GAN-based super-resolution algorithms, the proposed method reflects its superiority in two aspects. First, the scaled exponential linear units (SeLU) is introduced as the activation function of generator to give the GAN system self-normalization ability and make it more suitable for SAR images. Second, the batch normalization layers after convolution are canceled to reduce the computational requirement and model oscillation. Experiment results on the images of TerraSAR and MSTAR dataset demonstrate that the proposed method acquires satisfactory performance on the resolution enhancement and target recognition of SAR images. ©2019 IEEE","Image enhancement; Image reconstruction; Optical resolving power; Radar target recognition; Remote sensing; Synthetic aperture radar; Activation functions; Adversarial networks; Computational requirements; High resolution image; Resolution enhancement; Super resolution algorithms; Super resolution reconstruction; Super-resolution image reconstruction; Radar imaging","Generative adversarial network (GAN); Super-resolution image reconstruction; Target recognition","Conference paper","Final","","Scopus","2-s2.0-85104026398"
"Bosch M.; Munoz Abujder R.R.R.; Gifford C.","Bosch, Marc (24398475400); Munoz Abujder, Rodrigo Rene Rai (57211071520); Gifford, Christopher (24467858000)","24398475400; 57211071520; 24467858000","Towards image and video super-resolution for improved analytics from overhead imagery","2019","Proceedings of SPIE - The International Society for Optical Engineering","10992","","1099203","","","","10.1117/12.2518179","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072569349&doi=10.1117%2f12.2518179&partnerID=40&md5=0532d54f73676e98e6b8379d5ea61274","In this work, we address the problem of losing details in the overhead remote sensing image acquisition and generation process due to sensor resolution and distance to target by leveraging state-of-the-art deep neural network architectures. The goal is to recover such details by super-resolving the images acquired by overhead imaging sensors in order for human analysts to interpret data more accurately, and consequentially, for automated visual exploitation algorithms to be applied more effectively. We have developed a super-resolution framework operating on overhead full motion video (FMV) and still imagery (e.g. satellite images). Our framework consists of a neural network capable of learning the mapping between low and high resolution images in order to produce plausible details about the scene. Our framework combines Generative Adversarial Networks (GANs) and Recurrent Neural Networks (RNNs) to process low resolution signals both spatially and, in the case of FMV, temporally. We have applied the output of our system to several visual perception tasks, including object detection, object tracking, and semantic segmentation. We have also applied our methods to data from different geographical areas, sensors, and even modalities to demonstrate broad and generalized applicability. Copyright © 2019 SPIE.","Computer vision; Deep neural networks; Image acquisition; Network architecture; Object detection; Object recognition; Optical resolving power; Recurrent neural networks; Remote sensing; Satellite imagery; Semantics; Adversarial networks; Full motion video; High resolution image; Recurrent neural network (RNNs); Remote sensing images; Semantic segmentation; Super resolution; Video super-resolution; Image enhancement","computer vision; full motion video; generative adversarial networks (GANs); object detection; satellite imagery; semantic segmentation; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85072569349"
"Nesvold E.; Mukerji T.","Nesvold, E. (57211241618); Mukerji, T. (7003413039)","57211241618; 7003413039","Geomodeling using generative adversarial networks and a database of satellite imagery of modern river deltas","2019","4th EAGE Conference on Petroleum Geostatistics","","","ThP19","","","","10.3997/2214-4609.201902196","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073110004&doi=10.3997%2f2214-4609.201902196&partnerID=40&md5=2f2165af752e1986ed463743cf848918","Several studies on deep generative models for use in geomodeling show encouraging results with binary training data. An important question is what type of training data to use, since realistic 3D geology with natural variability is difficult to create. The advent of multiple types of remote sensing data of subaerial and subaqueous sedimentary patterns provides new possibilities in this context. Here, we train a Wasserstein GAN using 20,000 multispectral satellite images of subsections of 40 modern river deltas. The generated output has three facies and a background facies, and all quantitative evaluation methods of the unconditional output show a close overlap between the model and training data distributions. Standard MCMC sampling conditional on soft and hard data works well as long as the likelihood model is balanced against the prior model. Transfer learning, i.e. fine-training a small subset of the network parameters on smaller dataset of interest, such as highly non-stationary images of river deltas with similar characteristics, also shows promising results. © EAGE 2019.","Gasoline; Remote sensing; Rivers; Adversarial networks; Generative model; Multispectral satellite image; Natural variability; Network parameters; Quantitative evaluation methods; Remote sensing data; Transfer learning; Satellite imagery","","Conference paper","Final","","Scopus","2-s2.0-85073110004"
"Tao Y.; Muller J.-P.","Tao, Y. (56539197700); Muller, J.-P. (7404871794)","56539197700; 7404871794","Super-resolution restoration of spaceborne HD videos using the UCL MAGiGAN system","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","1115508","","","","10.1117/12.2532889","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078198191&doi=10.1117%2f12.2532889&partnerID=40&md5=828817e6907aad33d8482ac8726058cf","We developed a novel SRR system, called Multi-Angle Gotcha image restoration with Generative Adversarial Network (MAGiGAN), to produce resolution enhancement of 3-5 times from multi-pass EO images. The MAGiGAN SRR system uses a combination of photogrammetric and machine vision approaches including image segmentation and shadow labelling, feature matching and densification, estimation of an image degradation model, and deep learning approaches, to retrieve image information from distorted features and training networks. We have tested the MAGiGAN SRR using the NVIDIA® Jetson TX-2 GPU card for onboard processing within a smart-satellite capturing high definition satellite videos, which will enable many innovative remote-sensing applications to be implemented in the future. In this paper, we show SRR processing results from a Planet® SkySat HD 70cm spaceborne video using a GPU version of the MAGiGAN system. Image quality and effective resolution enhancement are measured and discussed. © 2019 SPIE.","Deep learning; Digital television; Earth (planet); Graphics processing unit; Image enhancement; Image segmentation; Optical resolving power; Remote sensing; Restoration; Video signal processing; Adversarial networks; Earth observations; HD videos; MAGiGAN; Multi angle; Super-resolution restoration; Image reconstruction","Earth Observation; Generative Adversarial Network; MAGiGAN; Multi-angle; Planet® SkySat® HD Video; Super-Resolution Restoration","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078198191"
"Xu Y.; Du B.; Zhang L.","Xu, Yonghao (57199421344); Du, Bo (57217375214); Zhang, Liangpei (8359720900)","57199421344; 57217375214; 8359720900","Can we generate good samples for hyperspectral classification? - A generative adversarial network based method","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519295","5752","5755","3","10.1109/IGARSS.2018.8519295","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064149453&doi=10.1109%2fIGARSS.2018.8519295&partnerID=40&md5=c0bb8e59db9fd7d6ddaad1df9060bb2d","The insufficiency of training samples is really a great challenge for hyperspectral image (HSI) classification. Samples generation is a commonly used technique in deep learning based remote sensing field which can extend the training set. However, previous methods ignore the real distribution of the training samples in the feature space and thus can hardly ensure that the generated samples possess the same patterns with the real ones. In this paper, we propose a generative adversarial network based method (SpecGAN) to handle this problem. Different from traditional GAN framework where the generated samples have no categories, for the first time we take the label information into consideration for hyperspectral images. Feeding a random noise z and a class label vector y into the generator, we can get a spectral sample of the corresponding category. The experiments on the Pavia University data set demonstrate the potential of the proposed SpecGAN in spectral samples generation. © 2018 IEEE.","Deep learning; Geology; Hyperspectral imaging; Image classification; Sampling; Space optics; Spectroscopy; Adversarial networks; Feature space; Hyper-spectral classification; Label information; Real distribution; Sample generations; Training sample; Training sets; Remote sensing","Deep learning; Generative adversarial network; Hyperspectral image classification; Sample generation","Conference paper","Final","","Scopus","2-s2.0-85064149453"
"Zhang J.; Chen L.; Liang X.; Zhuo L.; Tian Q.","Zhang, Jing (55720152700); Chen, Lu (57192588502); Liang, Xi (57194468225); Zhuo, Li (55568078600); Tian, Qi (57209993060)","55720152700; 57192588502; 57194468225; 55568078600; 57209993060","Hyperspectral image secure retrieval based on encrypted deep spectralspatial features","2019","Journal of Applied Remote Sensing","13","1","018501","","","","10.1117/1.JRS.13.018501","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062606177&doi=10.1117%2f1.JRS.13.018501&partnerID=40&md5=6b539ac15317156f00d2d33e3827615f","With the rapid development of remote-sensing earth observation technology, hyperspectral imagery has shown exponential growth. The quick and accurate retrieval of hyperspectral images has become a practical challenge in applications. Moreover, open network sharing has rendered network information security increasingly important. It is necessary to prevent breach of confidentiality events during retrieval, particularly for hyperspectral images containing crucial information. Therefore, a method for hyperspectral image secure retrieval based on encrypted deep spectralspatial features is proposed. In principle, our method includes the following steps: (1)Considering the powerful feature learning capability of deep networks, deep spectralspatial features of hyperspectral image are extracted with a deep convolutional generative adversarial network. (2)For high-dimensional deep features, t-distributed Stochastic neighbor embedding based nonlinear manifold hashing is utilized to reduce the dimensionality of deep spectralspatial features. (3)To ensure data security during retrieval, deep spectralspatial features are encrypted with feature randomization encryption. (4)Multi-index hashing is utilized to measure similarities among the deep spatialspectral features of hyperspectral images. (5)Relevance feedback based on feature reweighting is introduced to further improve retrieval accuracy. Four experiments are conducted to prove the effectiveness of the proposed method based on retrieval and security performance. Our experimental results on two hyperspectral datasets show that our method can effectively protect the security of image content with sufficient image retrieval accuracy. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Cryptography; Hyperspectral imaging; Image retrieval; Random processes; Remote sensing; Spectroscopy; Stochastic systems; Earth Observation Technology; Encrypted deep spectralspatial features; Hyper-spectral imageries; Multi-index; Network information securities; Secure retrieval; Security performance; Stochastic neighbor embedding; Network security","Encrypted deep spectralspatial features; Feature randomization encryption; Hyperspectral image; Multi-index hashing; Secure retrieval","Article","Final","","Scopus","2-s2.0-85062606177"
"Dong J.; Yin R.; Sun X.; Li Q.; Yang Y.; Qin X.","Dong, Junyu (22634069200); Yin, Ruiying (57204103920); Sun, Xin (56366080900); Li, Qiong (57037171600); Yang, Yuting (57192115362); Qin, Xukun (57204632561)","22634069200; 57204103920; 56366080900; 57037171600; 57192115362; 57204632561","Inpainting of Remote Sensing SST Images with Deep Convolutional Generative Adversarial Network","2019","IEEE Geoscience and Remote Sensing Letters","16","2","8480867","173","177","4","10.1109/LGRS.2018.2870880","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054490519&doi=10.1109%2fLGRS.2018.2870880&partnerID=40&md5=0ff4fcf58721fe495f4103bcae883555","Cloud occlusion is a common problem in the satellite remote sensing (RS) field and poses great challenges for image processing and object detection. Most existing methods for cloud occlusion recovery extract the surrounding information from the single corrupted image rather than the historical RS image records. Moreover, the existing algorithms can only handle small and regular-shaped obnubilation regions. This letter introduces a deep convolutional generative adversarial network to recover the RS sea surface temperature images with cloud occlusion from the big historical image records. We propose a new loss function for the inpainting network, which adds a supervision term to solve our specific problem. Given a trained generative model, we search for the closest encoding of the corrupted image in the low-dimensional space using our inpainting loss function. This encoding is then passed through the generative model to infer the missing content. We conduct experiments on the RS image data set from the national oceanic and atmospheric administration. Compared with traditional and machine learning methods, both qualitative and quantitative results show that our method has advantages over existing methods. © 2004-2012 IEEE.","Atmospheric temperature; Clouds; Convolution; Encoding (symbols); Gallium nitride; Gas generators; III-V semiconductors; Learning systems; Object detection; Remote sensing; Signal encoding; Space optics; Submarine geophysics; Surface properties; Surface waters; Adversarial networks; Image color analysis; Inpainting; Land surface temperature; Occlusion image; Ocean temperature; Sea surface temperature (SST); algorithm; artificial neural network; historical record; image processing; machine learning; remote sensing; satellite imagery; sea surface temperature; Oceanography","Cloud occlusion images; deep convolutional generative adversarial network (DCGAN); inpainting; sea surface temperature (SST) images","Article","Final","","Scopus","2-s2.0-85054490519"
"Varia N.; Dokania A.; Senthilnath J.","Varia, Neelanshi (57204874358); Dokania, Akanksha (57190796780); Senthilnath, J. (35183910200)","57204874358; 57190796780; 35183910200","DeepExt: A Convolution Neural Network for Road Extraction using RGB images captured by UAV","2019","Proceedings of the 2018 IEEE Symposium Series on Computational Intelligence, SSCI 2018","","","8628717","1890","1895","5","10.1109/SSCI.2018.8628717","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062796548&doi=10.1109%2fSSCI.2018.8628717&partnerID=40&md5=0d8faadee5d7457af7f29412354668be","In this paper, we propose automatic road extraction using Unmanned Aerial Vehicle (UAV) based Remote Sensing data. Road extraction using UAV data is very useful in traffic management, city planning, GPS based applications, etc. Deep learning techniques namely, Fully Convolutional Network (FCN) and conditional Generative Adversarial Networks (GAN) are used to extract roads from a UAV dataset available in the literature. FCN performs semantic segmentation on the image whereas the GAN generates output images from the model it learns. The results demonstrate the efficiency of the deep learning methods for the task of road extraction. © 2018 IEEE.","Antennas; Artificial intelligence; Color image processing; Convolution; Data mining; Deep learning; Extraction; Image segmentation; Information management; Remote sensing; Roads and streets; Semantics; Unmanned aerial vehicles (UAV); Adversarial networks; Automatic road extraction; Convolution neural network; Convolutional networks; convolutional neural nenvork; Remote sensing data; Road extraction; Semantic segmentation; Feature extraction","convolutional neural nenvork; generative adversarial networks; road extraction; semantic segmentation","Conference paper","Final","","Scopus","2-s2.0-85062796548"
"Bittner K.; Korner M.","Bittner, Ksenia (57194603356); Korner, Marco (57190168095)","57194603356; 57190168095","Automatic large-scale 3D building shape refinement using conditional generative adversarial networks","2018","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June","","8575415","1968","1970","2","10.1109/CVPRW.2018.00249","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058891213&doi=10.1109%2fCVPRW.2018.00249&partnerID=40&md5=2dd1891edf9e51a0e375c27f805608f8","Three-dimensional realistic representations of buildings in urban environments have been increasingly applied as data sources in a growing number of remote sensing fields such as urban planning and city management, navigation, environmental simulation (i.e. flood, earthquake, air pollution), 3D change detection after events like natural disasters or conflicts, etc. With recent technological developments, it becomes possible to acquire high-quality 3D input data. There are two main ways to obtain elevation information: from active remote sensing systems, such as light detection and ranging (LIDAR), and from passive remote sensing systems, such as optical images, which allow the acquisition of stereo images for automatic digital surface models (DSMs) generation. Although airborne laser scanning provides very accurate DSMs, it is a costly method. On the other hand, the DSMs from stereo satellite imagery show a large coverage and lower costs. However, they are not as accurate as LIDAR DSMs. With respect to automatic 3D information extraction, the availability of accurate and detailed DSMs is a crucial issue for automatic 3D building model reconstruction. We present a novel methodology for generating a better-quality stereo DSM with refined buildings shapes using a deep learning framework. To this end, a conditional generative adversarial network (cGAN) is trained to generate accurate LIDAR DSM-like height images from noisy stereo DSMs. © 2018 IEEE.","Air navigation; Computer vision; Deep learning; Disasters; Geometrical optics; Optical radar; Remote sensing; Satellite imagery; Three dimensional computer graphics; 3d information extractions; Active remote sensing systems; Airborne Laser scanning; Digital surface models; Environmental simulation; Light detection and ranging; Passive remote sensing; Technological development; Stereo image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85058891213"
"Singh P.; Komodakis N.","Singh, Praveer (57220983904); Komodakis, Nikos (57210774496)","57220983904; 57210774496","Cloud-GAN: Cloud removal for sentinel-2 imagery using a cyclic consistent generative adversarial networks","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519033","1772","1775","3","10.1109/IGARSS.2018.8519033","98","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064168458&doi=10.1109%2fIGARSS.2018.8519033&partnerID=40&md5=bd33a91f221d44b923417ea9badca61e","Cloud cover is a serious impediment in land surface analysis from Remote Sensing images either causing complete obstruction (thick clouds) with loss of information or blurry effects when being semi-transparent (thin clouds). While thick clouds require complete pixel replacement, thin cloud removal is fairly challenging as the atmospheric and land-cover information is inter-twined. In this paper, we address this problem and propose a Cloud-GAN to learn the mapping between cloudy images and cloud-free images. The adversarial loss in the proposed method constrains the distribution of generated images to be close enough to the underlying distribution of the non-cloudy images. An additional cycle consistency loss is used to further restrain the generator to predict cloud-free images only of the same scene as reflected in the cloudy images. Our method not only rejects the necessity of any paired (cloud/cloud-free) training dataset but also avoids the need of any additional (expensive) spectral source of information such as Synthetic Aperture Radar imagery which is cloud penetrable. Lastly, we demonstrate the efficacy of our technique by training on an openly available and fairly new Sentinel-2 Imagery dataset consisting of real clouds. We also show significant improvement in PSNR values after removing clouds on synthetic images thus validating the competency of our methodology. © 2018 IEEE.","Deep learning; Geology; Radar imaging; Remote sensing; Surface analysis; Synthetic aperture radar; Tracking radar; Adversarial networks; Cloud removal; Land cover informations; Land surface analysis; Remote sensing images; Sentinel-2 imagery; Synthetic Aperture Radar Imagery; Underlying distribution; Image enhancement","Cloud removal; Deep learning; Generative adversarial networks; Sentinel-2 imagery","Conference paper","Final","","Scopus","2-s2.0-85064168458"
"Ball J.E.; Anderson D.T.; Wei P.","Ball, John E. (22233664000); Anderson, Derek T. (55483345800); Wei, Pan (57188849095)","22233664000; 55483345800; 57188849095","State-of-the-art and gaps for deep learning on limited training data in remote sensing","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518681","4119","4122","3","10.1109/IGARSS.2018.8518681","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064162234&doi=10.1109%2fIGARSS.2018.8518681&partnerID=40&md5=4b3b6d1112fa73ec97a9405faa45dfa3","Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving. © 2018 IEEE","Geology; Remote sensing; Adversarial networks; Learning network; Limited training data; Remote sensing applications; State of the art; State-of-the-art approach; Transfer learning; Unlabeled data; Deep learning","Deep learning; Generative adversarial networks; Limited training data; Remote sensing; Transfer learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85064162234"
"Zhang Y.; Xiang Y.; Bai L.","Zhang, Yungang (36761729500); Xiang, Yu (57204287654); Bai, Lei (57191228241)","36761729500; 57204287654; 57191228241","Generative Adversarial Network for Deblurring of Remote Sensing Image","2018","International Conference on Geoinformatics","2018-June","","8557110","","","","10.1109/GEOINFORMATICS.2018.8557110","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059935363&doi=10.1109%2fGEOINFORMATICS.2018.8557110&partnerID=40&md5=66e47077965f5d3d28347766b0eed273","Deblurring is a classical problem for remote sensing images, which is known to be difficult as an ill-posed problem. A feasible solution for the problem is incorporating various priors into restoration procedure as constrained conditions. However, the learning of priors usually assumes that the blurs in an image are produced by fixed types of reasons, and thus a possible decrease in model's description ability. In this paper, an end-to-end learned method based on generative adversarial networks (GANs) is proposed to tackle the deblurring problem for remote sensing images. The proposed deblurring model does not need any prior assumptions for the blurs. The proposed method was evaluated on a satellite map image data set and state-of-the-art performance was obtained. © 2018 IEEE.","Image enhancement; Remote sensing; Classical problems; Constrained conditions; Deblurring; Feasible solution; Generative adversatial network; Ill posed problem; Image deblurring; Loss functions; Remote sensing images; Restoration procedure; Generative adversarial networks","Generative Adversatial Network (GAN); Image deblurring; Loss function; Remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85059935363"
"Zhang J.; Shamsolmoali P.; Zhang P.; Feng D.; Yang J.","Zhang, Junhao (56368684700); Shamsolmoali, Pourya (56350053200); Zhang, Pengpeng (56104492700); Feng, Deying (35219755900); Yang, Jie (15039078800)","56368684700; 56350053200; 56104492700; 35219755900; 15039078800","Multispectral image fusion using super-resolution conditional generative adversarial networks","2019","Journal of Applied Remote Sensing","13","2","022002","","","","10.1117/1.JRS.13.022002","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055792733&doi=10.1117%2f1.JRS.13.022002&partnerID=40&md5=88db0212e3ac0f09a92f60ac8917e082","In multispectral image fusion scenarios, deep learning has been widely applied. However, the fusion performance and image quality are still restricted by inflexible architecture and supervised learning mode. We proposed multispectral image fusion using super-resolution conditional generative adversarial networks (MS-cGANs) based on conditional cGANs, which produces the fused image through the flexible encode-and-decode procedure. In the proposed network, a least square model is extended to solve the gradients vanishing problem in cGANs. Then, to improve the fusion quality, the multiscale features are used to preserve the details. Furthermore, the image resolution is promoted by adding the perceptual loss in object function and injecting the super-resolution structure into a deconvolution procedure. In experimental results, MS-cGANs demonstrates a significant performance in fusing multispectral images and top-ranking image quality compared with the state-of-the-art methods. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Fusion reactions; Image quality; Image resolution; Least squares approximations; Optical resolving power; Remote sensing; Adversarial networks; Fusion performance; Least square model; Multi-scale features; Multi-spectral image fusions; Multispectral images; State-of-the-art methods; Super resolution; Image fusion","fusion; multispectral image; multispectral-conditional generative adversarial network; remote sensing","Article","Final","","Scopus","2-s2.0-85055792733"
"Wang A.; Wang Y.; Song X.; Iwahori Y.","Wang, Aili (55483869300); Wang, Ying (57207001725); Song, Xiaoying (57210435647); Iwahori, Yuji (7003339167)","55483869300; 57207001725; 57210435647; 7003339167","Remote Sensing Image Super-Resolution Reconstruction based on Generative Adversarial Network","2019","International Journal of Performability Engineering","15","7","","1783","1791","8","10.23940/ijpe.19.07.p4.17831791","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070709753&doi=10.23940%2fijpe.19.07.p4.17831791&partnerID=40&md5=0c36fb174d47d236fbcf8f8086646e1b","The super-resolution reconstruction algorithm based on generative adversarial network (GAN) can generate realistic texture in the super-resolution process of a single remote sensing image. In order to further improve the visual quality of the reconstructed image, this paper will improve the generation network, discrimination network, and perceptual loss of the generated confrontation network. Firstly, the batch normalization layer is removed and dense connections are used in the residual blocks, which effectively improves the performance of the generated network. Then, we use the relative discriminant network to learn more detailed texture. Finally, we obtain the perception loss before the activation function to maintain the consistency of brightness. In addition, transfer learning is used to solve the problem of insufficient remote sensing data. The experimental results show that the proposed algorithm has superiority in the super-resolution reconstruction of remote sensing images and can obtain better subjective visual effects. © 2019 Totem Publisher, Inc. All rights reserved.","Image enhancement; Optical resolving power; Remote sensing; Textures; Activation functions; Adversarial networks; Reconstructed image; Remote sensing data; Remote sensing images; Residual dense block; Super resolution reconstruction; Transfer learning; Image reconstruction","Generative adversarial network; Remote sensing image super-resolution reconstruction; Residual dense block; Transfer learning","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85070709753"
"Liu X.; Wang Y.; Liu Q.","Liu, Xiangyu (57192693924); Wang, Yunhong (34870959400); Liu, Qingjie (55534263100)","57192693924; 34870959400; 55534263100","Psgan: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","2018","Proceedings - International Conference on Image Processing, ICIP","","","8451049","873","877","4","10.1109/ICIP.2018.8451049","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062900060&doi=10.1109%2fICIP.2018.8451049&partnerID=40&md5=1849950b74eeded4a0073d53039a6838","Remote sensing image fusion (also known as pan-sharpening) aims to generate a high resolution multi -spectral image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral (MS) image. In this paper, we propose PSGAN, a generative adversarial network (GAN) for remote sensing image pansharpening. To the best of our knowledge, this is the first attempt at producing high quality pan-sharpened images with GANs. The PSGAN consists of two parts. Firstly, a two-stream fusion architecture is designed to generate the desired high resolution multi -spectral images, then a fully convolutional network serving as a discriminator is applied to distinct 'real' or 'pan-sharpened' MS images. Experiments on images acquired by Quickbird and GaoFen-1 satellites demonstrate that the proposed PSGAN can fuse PAN and MS images effectively and significantly improve the results over the state of the art traditional and CNN based pan-sharpening methods. © 2018 IEEE.","Deep learning; Image fusion; Image resolution; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Fusion architecture; High spatial resolution; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Image enhancement","Deep learning; GAN; Image fusion; Pan-sharpening; Remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062900060"
"Aigner S.; Körner M.","Aigner, S. (57211639840); Körner, M. (57190168095)","57211639840; 57190168095","Futuregan: Anticipating the future frames of video sequences using spatio-temporal 3D convolutions in progressively growing GANs","2019","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2/W16","","3","11","8","10.5194/isprs-archives-XLII-2-W16-3-2019","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074715200&doi=10.5194%2fisprs-archives-XLII-2-W16-3-2019&partnerID=40&md5=975bf3bf4421bd473b6b080bce333339","We introduce a new encoder-decoder GAN model, FutureGAN, that predicts future frames of a video sequence conditioned on a sequence of past frames. During training, the networks solely receive the raw pixel values as an input, without relying on additional constraints or dataset specific conditions. To capture both the spatial and temporal components of a video sequence, spatio-temporal 3d convolutions are used in all encoder and decoder modules. Further, we utilize concepts of the existing progressively growing GAN (PGGAN) that achieves high-quality results on generating high-resolution single images. The FutureGAN model extends this concept to the complex task of video prediction. We conducted experiments on three different datasets, MovingMNIST, KTH Action, and Cityscapes. Our results show that the model learned representations to transform the information of an input sequence into a plausible future sequence effectively for all three datasets. The main advantage of the FutureGAN framework is that it is applicable to various different datasets without additional changes, whilst achieving stable results that are competitive to the state-of-the-art in video prediction. The code to reproduce the results of this paper is publicly available at https://github.com/TUM-LMF/FutureGAN. © Authors 2019. CC BY 4.0 License.","Convolution; Decoding; Deep learning; Forecasting; Remote sensing; Signal encoding; Adversarial networks; Encoder-decoder; Generative model; High resolution; Spatio temporal; State of the art; Video prediction; Video sequences; Video recording","Deep Learning; Generative Adversarial Networks; Generative Modeling; Video Prediction","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85074715200"
"Mateo-Garcia G.; Laparra V.; Gomez-Chova L.","Mateo-Garcia, Gonzalo (57192947904); Laparra, Valero (34869963500); Gomez-Chova, Luis (6603354695)","57192947904; 34869963500; 6603354695","Domain Adaptation of Landsat-8 and Proba-V Data Using Generative Adversarial Networks for Cloud Detection","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899193","712","715","3","10.1109/IGARSS.2019.8899193","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076112397&doi=10.1109%2fIGARSS.2019.8899193&partnerID=40&md5=66c070099c71f0ae9a4abd1dba92fa21","Training machine learning algorithms for new satellites requires collecting new data. This is a critical drawback for most remote sensing applications and specially for cloud detection. A sensible strategy to mitigate this problem is to exploit available data from a similar sensor, which involves transforming this data to resemble the new sensor data. However, even taking into account the technical characteristics of both sensors to transform the images, statistical differences between data distributions still remain. This results in a poor performance of the methods trained on one sensor and applied to the new one. In this this work, we propose to use the generative adversarial networks (GANs) framework to adapt the data from the new satellite. In particular, we use Landsat-8 images, with the corresponding ground truth, to perform cloud detection in Proba-V. Results show that the GANs adaptation significantly improves the detection accuracy. © 2019 IEEE.","Convolutional neural networks; Geology; Learning algorithms; Machine learning; Metadata; Adversarial networks; Cloud detection; Data distribution; Detection accuracy; Domain adaptation; LANDSAT; Remote sensing applications; Statistical differences; Remote sensing","Cloud Detection; Convolutional Neural Networks; Domain Adaptation; Generative Adversarial Networks; Landsat-8; Proba-V","Conference paper","Final","","Scopus","2-s2.0-85076112397"
"Gavriil K.; Muntingh G.; Barrowclough O.J.D.","Gavriil, Konstantinos (57204611871); Muntingh, Georg (43762272300); Barrowclough, Oliver J. D. (37010659500)","57204611871; 43762272300; 37010659500","Void Filling of Digital Elevation Models with Deep Generative Models","2019","IEEE Geoscience and Remote Sensing Letters","16","10","8669876","1645","1649","4","10.1109/LGRS.2019.2902222","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076575363&doi=10.1109%2fLGRS.2019.2902222&partnerID=40&md5=e5222a7aed6097a3c7c16d12cdfee1f2","In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to a superhuman performance in various tasks, such as classification, localization, and segmentation, whereas unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images. In this letter, we consider a state-of-the-art machine learning model for image inpainting, namely, a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can be successfully transferred to the setting of digital elevation models for the purpose of generating semantically plausible data for filling voids. Training, testing, and experimentation are done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority. © 2019 IEEE.","Norway; Deep learning; Digital instruments; Geomorphology; Image segmentation; Learning algorithms; Remote sensing; Supervised learning; Surveying; Unsupervised learning; Adversarial networks; Attention mechanisms; Computational resources; Digital elevation model; Generative model; Machine learning models; Predictive models; Synthetic images; algorithm; computer simulation; digital elevation model; image analysis; image resolution; machine learning; performance assessment; segmentation; supervised learning; unsupervised classification; Machine learning","Digital elevation models (DEMs); predictive models; remote sensing; unsupervised learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076575363"
"Hughes L.H.; Merkle N.; Burgmann T.; Auer S.; Schmitt M.","Hughes, Lloyd Haydn (57201113391); Merkle, Nina (57194604557); Burgmann, Tatjana (57211533315); Auer, Stefan (57216043589); Schmitt, Michael (7401931279)","57201113391; 57194604557; 57211533315; 57216043589; 7401931279","Deep Learning for SAR-Optical Image Matching","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898635","4877","4880","3","10.1109/IGARSS.2019.8898635","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077692111&doi=10.1109%2fIGARSS.2019.8898635&partnerID=40&md5=c382fe0d785719e2bb15b46747bf7656","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors. © 2019 IEEE.","Convolutional neural networks; Data fusion; Deep learning; Geology; Geometrical optics; Image fusion; Image matching; Network architecture; Radar target recognition; Remote sensing; Synthetic aperture radar; Adversarial networks; Automatic matching; Multi sensor images; Optical image; Remote sensing imagery; Remote sensing missions; SAR Images; Target recognition; Radar imaging","Data Fusion; Deep Learning; Image Matching; Optical Images; SAR Images","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077692111"
"Qian X.; Li J.; Cheng G.; Yao X.; Zhao S.; Chen Y.; Jiang L.","Qian, Xiaoliang (36465575400); Li, Jia (57206965588); Cheng, Gong (36801169800); Yao, Xiwen (55813585000); Zhao, Suna (57020366900); Chen, Yibin (57204183485); Jiang, Liying (57192317855)","36465575400; 57206965588; 36801169800; 55813585000; 57020366900; 57204183485; 57192317855","Evaluation of the effect of feature extraction strategy on the performance of high-resolution remote sensing image scene classification; [特征提取策略对高分辨率遥感图像场景分类性能影响的评估]","2018","Yaogan Xuebao/Journal of Remote Sensing","22","5","","758","776","18","10.11834/jrs.20188015","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054837402&doi=10.11834%2fjrs.20188015&partnerID=40&md5=2c311a1ca35c45fd86d52158784b9a88","Remote sensing image scene classification aims to tag remote sensing images with semantic categories according to the content of the image and is important in disaster monitoring, environmental detection, and urban planning. Scene classification results can provide valuable information about object recognition and image retrieval and can effectively improve the performance of image interpretation. The general process of remote sensing image scene classification mainly consists of feature extraction and scene classification based on image features. Given that the design of classifiers is relatively mature, this work focuses on feature extraction strategy. The influence of various strategies on the performance of scene classification is short of unified evaluation, which limits its development. The effect of various feature extraction strategies on the performance of high-resolution remote sensing image scene classification is evaluated in this study. In the second section of this paper, existing feature extraction strategies are divided into two categories: (1) hand-designed and (2) data-driven feature extraction. Hand-designed features, such as Color Histograms (CH) and Scale Invariant Feature Transform (SIFT), provide the primary description of images and are presented in the early period. Further abstract description of the images is introduced by coding of hand-designed features, such as Bag of Visual Words (BoVW) and has higher classification accuracy than hand-designed features. However, these feature extraction strategies generally suffer from poor generalization capability due to specific requirements for designing. Furthermore, hand-designed features require significant domain knowledge. By contrast, data-driven features can directly learn powerful features from a large number of sample images and are generally divided into shallow and deep learning features. Shallow learning feature extraction mainly involves Principal Component Analysis (PCA), Independent Component Analysis (ICA), and sparse coding algorithms. Typical deep learning feature extraction strategies include stacked autoencoder (SAE), Deep Belief Network (DBN), and Convolutional Neural Network (CNN). Compared with deep learning models, shallow learning models can be regarded as a neural network with a single hidden layer and thus cannot capture high-level semantic features. The superiority of deep learning features is obvious when dealing with complex scene classification. Furthermore, CNN-based features exhibit improved performance compared with SAE- and DBN-based features because the one-dimensional structure of SAE and DBN destroys the spatial information of images. In the third section of this paper, 29 feature descriptors are quantitatively compared in UC Merced, AID, and NWPU RESISC-45 datasets and eight combinations of feature descriptors are quantitatively compared in the NWPU RESISC-45 dataset. The effect of different feature extraction strategies on the performance of scene classification and the complexity of each dataset are evaluated through quantitative comparison. The experimental results are as follows. (1) The classification accuracy and stability of hand-designed features is poor, however the efficiency of most features is satisfactory and can attain better performance by combining with other types of features. (2) Among all feature extraction strategies, the coding of hand-designed features possesses moderate levels of classification accuracy, efficiency, and stability. (3) The classification accuracy and stability of data-driven features are best, but most of them have low efficiency. (4) AlexNet, a deep learning model with few layers, exhibits the best comprehensive performance and is suitable for occasions that require high classification accuracy, efficiency, and stability. (5) Some scene classes belonging to land use type are easy to be confused because of similar landmark buildings or sites. Moreover, some scene classes belonging to land cover type are easy to be confused because of their similar geomorphologic features. (6) The recently proposed NWPU RESISC-45 dataset is more complex than the other datasets and is more challenging for scene classification algorithms. Finally, the summary and conclusion of this paper are presented, and the discussion of future development is provided. On the one hand, combining prior knowledge introduced by hand-designed features with the CNN model may be one of the future development directions. On the other hand, introducing Generative Adversarial Networks (GAN) into CNN training may be a research hotspot in the future. In addition, remote sensing parameters, such as NDVI and NDWI, and multi-spectral information can be integrated with current feature extraction strategies for practical applications. © 2018, Science Press. All right reserved.","Abstracting; Codes (symbols); Complex networks; Data mining; Deep learning; Efficiency; Extraction; Feature extraction; Image classification; Image coding; Image enhancement; Image retrieval; Independent component analysis; Land use; Neural networks; Object recognition; Principal component analysis; Remote sensing; Semantics; Stability; Convolutional Neural Networks (CNN); Data driven; Hand-designed features; High resolution; High resolution remote sensing images; Independent component analyses (ICA); Scale invariant feature transforms; Scene classification; Classification (of information)","Data driven features; Deep learning; Feature extraction strategy; Hand-designed features; High-resolution; Scene classification","Article","Final","","Scopus","2-s2.0-85054837402"
"Sharma A.; Jindal N.; Thakur A.","Sharma, Akanksha (57188975329); Jindal, Neeru (8571819900); Thakur, Abhishek (57055028500)","57188975329; 8571819900; 57055028500","Comparison on Generative Adversarial Networks - A Study","2018","ICSCCC 2018 - 1st International Conference on Secure Cyber Computing and Communications","","","8703267","391","396","5","10.1109/ICSCCC.2018.8703267","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065640687&doi=10.1109%2fICSCCC.2018.8703267&partnerID=40&md5=58155d051aa1abe8791a3d2595bfc668","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks. © 2018 IEEE.","Deep learning; Learning systems; Remote sensing; Adversarial networks; Diverse fields; Image synthesis; Learning models; Video generation; Image processing","Generative Adversarial Networks; Machine learning","Conference paper","Final","","Scopus","2-s2.0-85065640687"
"Reyes M.F.; Auer S.; Merkle N.; Henry C.; Schmitt M.","Reyes, Mario Fuentes (57210949743); Auer, Stefan (57216043589); Merkle, Nina (57194604557); Henry, Corentin (57203712534); Schmitt, Michael (7401931279)","57210949743; 57216043589; 57194604557; 57203712534; 7401931279","SAR-to-optical image translation based on conditional generative adversarial networks-optimization, opportunities and limits","2019","Remote Sensing","11","17","2067","","","","10.3390/rs11172067","72","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071977712&doi=10.3390%2frs11172067&partnerID=40&md5=8171f22fbc5bc8c3c9cab6b5ac7ecf26","Due to its all time capability, synthetic aperture radar (SAR) remote sensing plays an important role in Earth observation. The ability to interpret the data is limited, even for experts, as the human eye is not familiar to the impact of distance-dependent imaging, signal intensities detected in the radar spectrum as well as image characteristics related to speckle or steps of post-processing. This paper is concerned with machine learning for SAR-to-optical image-to-image translation in order to support the interpretation and analysis of original data. A conditional adversarial network is adopted and optimized in order to generate alternative SAR image representations based on the combination of SAR images (starting point) and optical images (reference) for training. Following this strategy, the focus is set on the value of empirical knowledge for initialization, the impact of results on follow-up applications, and the discussion of opportunities/drawbacks related to this application of deep learning. Case study results are shown for high resolution (SAR: TerraSAR-X, optical: ALOS PRISM) and low resolution (Sentinel-1 and -2) data. The properties of the alternative image representation are evaluated based on feedback from experts in SAR remote sensing and the impact on road extraction as an example for follow-up applications. The results provide the basis to explain fundamental limitations affecting the SAR-to-optical image translation idea but also indicate benefits from alternative SAR image representations. © 2019 by the authors.","Deep learning; Feature extraction; Geometrical optics; Image processing; Remote sensing; Synthetic aperture radar; Adversarial networks; Earth observations; Empirical knowledge; Fundamental limitations; Image characteristics; Image representations; Interpretation; Signal intensities; Radar imaging","Deep learning; Generative adversarial networks; Interpretation; Synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85071977712"
"Nan J.; Bo L.","Nan, Jing (56516351300); Bo, Lei (55956811800)","56516351300; 55956811800","Image super-resolution reconstructing based on generative adversarial network","2019","Proceedings of SPIE - The International Society for Optical Engineering","11342","","113420F","","","","10.1117/12.2547435","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077823628&doi=10.1117%2f12.2547435&partnerID=40&md5=5fa365531378a43b815a86ca39443f66","Image super-resolution refers to the technique of reconstructing a high-resolution image by processing one or more complementary low-resolution images. It is widely used in medical imaging, video surveillance, remote sensing imaging and other fields. The learning-based super-resolution algorithm obtains a mapping relationship between the highresolution image and the low-resolution image by learning, and then guides the generation of the high-resolution image according to the obtained mapping relationship. The generative adversarial network (GAN) is composed of a generative network model and a discriminator network model, and the two play each other until the Nash equilibrium is reached, and the texture information and the high-frequency details of the downsampled image can be restored based on the super-resolution method of generative adversarial network. However, super-resolution algorithms based on generative adversarial network can only be used for one kind of magnification time, and the versatility is insufficient. Despite convolutional neural networks has achieved breakthroughs in accuracy and speed of traditional single-frame superresolution reconstruction, and can achieve a higher peak signal-to-noise ratio (PSNR). Most of them use Mean Square Error (MSE) as the minimum optimization objective function, so although a higher peak signal-to-noise ratio can be achieved, when the image downsampling factors is higher, the reconstructed image will be too smooth, lack highfrequency details and perceptually unsatisfy in the sense that they fail to match the fidelity expected at the higher resolution. When dealing with complex data of real scenes, the model's representation ability is not high; and the generative adversarial network training is very unstable, seriously affecting the model training process. This paper is based on generative adversarial network, improving the network structure and optimizing the training method to improve the quality of generating images. The following improvements have been made to the generator model: the multi-level structure is used to enlarge the image step by step, so that the model can simultaneously generate multiple images with a larger scale, and also ensure that the image obtained at a larger magnification has higher quality; ResNet model is improved by recursive learning and residual learning, and the batch normalization structure in the model is removed. On the basis of ensuring the image quality, the efficiency of the model is effectively improved. The recursive and residual learning methods can effectively improve the feature expression ability of the model, and thus significantly improve the quality of the generated image. The Expand-Squeeze method is proposed to generate images. The basic idea is to expand the dimension of the last layer of the convolution layer of the model. In this way, more context information is obtained, and then the image is generated by using the 1x1 convolution kernel. The Expand-Squeeze method can effectively reduce the checkerboard effect and improve the quality of the generated image to some extent. This paper improves the discriminator network loss function. Measure the similarity between generated image and real image by introducing Wasserstein distance. The loss function proposed consists of two parts: the loss function of resistance and the loss function of content. The experimental results verify that the improved generation of the generative adversarial network can effectively improve the quality of the generated image and effectively improve the stability of the model training. © 2019 SPIE.","Convolution; Discriminators; Image quality; Image reconstruction; Image segmentation; Learning systems; Mapping; Mean square error; Medical imaging; Neural networks; Optical resolving power; Photonics; Remote sensing; Security systems; Signal to noise ratio; Textures; Adversarial networks; Convolutional neural network; Image super resolutions; Learning-based super-resolution; Optimization objective function; Reconstructing; ResNet; Super-resolution reconstruction; Image enhancement","Generative adversarial network; Image super-resolution; Reconstructing; ResNet","Conference paper","Final","","Scopus","2-s2.0-85077823628"
"","","","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","2018","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","1","","","","178","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060419385&partnerID=40&md5=9c12bd0dc33bbf219d0df980041bcf20","The proceedings contain 22 papers. The topics discussed include: SAR to optical image synthesis for cloud removal with generative adversarial networks; development of a portable high performance mobile mapping system using the robot operating system; data processing and recording using a versatile multi-sensor vehicle; semantic segmentation of aerial imagery via multi-scale shuffling convolutional neural networks with deep supervision; Copernicus sentinel-2 data for the determination of groundwater withdrawal in the maghreb region; calibration study of a trimble ACX4 system for direct georeferencing mapping applications; infrared measurements and estimation of temperature in the restrictive scope of an industrial cement plant; extraction of solar cells from UAV-based thermal image sequences; disparity refinement of building edges using robustly matched straight lines for stereo matching; and pilot study on the retrieval of DBH and diameter distribution of deciduous forest stands using cast shadows in UAV-based orthomosaics.","","","Conference review","Final","","Scopus","2-s2.0-85060419385"
"Ghamisi P.; Yokoya N.","Ghamisi, Pedram (53663404300); Yokoya, Naoto (36440631200)","53663404300; 36440631200","IMG2DSM: Height Simulation from Single Imagery Using Conditional Generative Adversarial Net","2018","IEEE Geoscience and Remote Sensing Letters","15","5","","794","798","4","10.1109/LGRS.2018.2806945","78","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042846217&doi=10.1109%2fLGRS.2018.2806945&partnerID=40&md5=566c8a6d765dac73c83d8451f80d6cca","This letter proposes a groundbreaking approach in the remote-sensing community to simulating the digital surface model (DSM) from a single optical image. This novel technique uses conditional generative adversarial networks whose architecture is based on an encoder-decoder network with skip connections (generator) and penalizing structures at the scale of image patches (discriminator). The network is trained on scenes where both the DSM and optical data are available to establish an image-to-DSM translation rule. The trained network is then utilized to simulate elevation information on target scenes where no corresponding elevation information exists. The capability of the approach is evaluated both visually (in terms of photographic interpretation) and quantitatively (in terms of reconstruction errors and classification accuracies) on subdecimeter spatial resolution data sets captured over Vaihingen, Potsdam, and Stockholm. The results confirm the promising performance of the proposed framework. © 2004-2012 IEEE.","Baden-Wurttemberg; Brandenburg [Germany]; Germany; Potsdam; Stockholm [Sweden]; Sweden; Vaihingen an der Enz; Aerial photography; Classification (of information); Data visualization; Decoding; Deep learning; Gallium nitride; Gas generators; Geometrical optics; III-V semiconductors; Network coding; Neural networks; Optical sensors; Personnel training; Remote sensing; Adversarial networks; Convolutional Neural Networks (CNN); Digital surface model (DSM); Encoder-decoder; Optical image; Optical imaging; accuracy assessment; algorithm; artificial neural network; data set; detection method; image analysis; network analysis; numerical model; reconstruction; satellite data; satellite imagery; spatial resolution; Adaptive optics","Conditional generative adversarial networks (cGANs); convolutional neural network (CNN); deep learning; digital surface model (DSM); encoder-decoder networks; optical images","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85042846217"
"Senthilnath J.; Varia N.; Dokania A.; Anand G.; Benediktsson J.A.","Senthilnath, J. (35183910200); Varia, Neelanshi (57204874358); Dokania, Akanksha (57190796780); Anand, Gaotham (57190792438); Benediktsson, Jón Atli (7004494483)","35183910200; 57204874358; 57190796780; 57190792438; 7004494483","Deep TEC: Deep transfer learning with ensemble classifier for road extraction from UAV imagery","2020","Remote Sensing","12","2","245","","","","10.3390/rs12020245","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081087800&doi=10.3390%2frs12020245&partnerID=40&md5=5f10adb843e0b2f5604fd3467165550c","Unmanned aerial vehicle (UAV) remote sensing has a wide area of applications and in this paper, we attempt to address one such problem-road extraction from UAV-captured RGB images. The key challenge here is to solve the road extraction problem using the UAV multiple remote sensing scene datasets that are acquired with different sensors over different locations. We aim to extract the knowledge from a dataset that is available in the literature and apply this extracted knowledge on our dataset. The paper focuses on a novel method which consists of deep TEC (deep transfer learning with ensemble classifier) for road extraction using UAV imagery. The proposed deep TEC performs road extraction on UAV imagery in two stages, namely, deep transfer learning and ensemble classifier. In the first stage, with the help of deep learning methods, namely, the conditional generative adversarial network, the cycle generative adversarial network and the fully convolutional network, the model is pre-trained on the benchmark UAV road extraction dataset that is available in the literature. With this extracted knowledge (based on the pre-trained model) the road regions are then extracted on our UAV acquired images. Finally, for the road classified images, ensemble classification is carried out. In particular, the deep TEC method has an average quality of 71%, which is 10% higher than the next best standard deep learning methods. Deep TEC also shows a higher level of performance measures such as completeness, correctness and F1 score measures. Therefore, the obtained results show that the deep TEC is efficient in extracting road networks in an urban region. © 2020 by the authors.","Antennas; Data mining; Extraction; Feature extraction; Image classification; Learning systems; Remote sensing; Road vehicles; Roads and streets; Transfer learning; Unmanned aerial vehicles (UAV); Adversarial networks; Convolutional networks; Ensemble classification; Ensemble classifiers; Learning methods; Performance measure; Road extraction; Urban regions; Deep learning","Deep learning; Ensemble classifier; Remote sensing; Road extraction; Transfer learning; UAV","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85081087800"
"Zhan Y.; Medjadba Y.; Qin J.; Huang T.; Wu K.; Hu D.; Zhao Z.; Wang Y.; Cao Y.; Jiao R.; Wang G.; Yu X.","Zhan, Ying (57197867950); Medjadba, Yasmine (57208224017); Qin, Jin (57208224824); Huang, Tao (57213192948); Wu, Kang (57200601886); Hu, Dan (36161111200); Zhao, Zhengang (57213197560); Wang, Yuntao (55535376100); Cao, Ying (57211108967); Jiao, Runcheng (57213199516); Wang, Guian (25930227300); Yu, Xianchuan (12785792300)","57197867950; 57208224017; 57208224824; 57213192948; 57200601886; 36161111200; 57213197560; 55535376100; 57211108967; 57213199516; 25930227300; 12785792300","Hyperspectral Image Classification Based on Generative Adversarial Networks with Feature Fusing and Dynamic Neighborhood Voting Mechanism","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899291","811","814","3","10.1109/IGARSS.2019.8899291","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077688406&doi=10.1109%2fIGARSS.2019.8899291&partnerID=40&md5=37b3383c9ab2ec5f8dbd6dcdd6a47471","Classifying Hyperspectral images with few training samples is a challenging problem. The generative adversarial networks (GAN) are promising techniques to address the problems. GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. In this paper, by introducing multilayer features fusion in GAN and a dynamic neighborhood voting mechanism, a novel algorithm for HSIs classification based on 1-D GAN was proposed. Extracting and fusing multiple layers features in discriminator, and using a little labeled samples, we fine-tuned a new sample 1-D CNN spectral classifier for HSIs. In order to improve the accuracy of the classification, we proposed a dynamic neighborhood voting mechanism to classify the HSIs with spatial features. The obtained results show that the proposed models provide competitive results compared to the state-of-the-art methods. © 2019 IEEE.","Deep learning; Dynamics; Geology; Remote sensing; Semi-supervised learning; Spectroscopy; Adversarial networks; Dynamic neighborhood; Features fusions; HyperSpectral; Semi-supervised learning (SSL); Spectral classifier; Spectral-spatial classification; State-of-the-art methods; Image classification","deep learning; generative adversarial networks (GAN); Hyperspectral images classification; semi-supervised learning (SSL); spectral-spatial classification","Conference paper","Final","","Scopus","2-s2.0-85077688406"
"Howe J.; Pula K.; Reite A.A.","Howe, Jonathan (57212019844); Pula, Kyle (26041007800); Reite, Aaron A. (57212024255)","57212019844; 26041007800; 57212024255","Conditional generative adversarial networks for data augmentation and adaptation in remotely sensed imagery","2019","Proceedings of SPIE - The International Society for Optical Engineering","11139","","111390G","","","","10.1117/12.2529586","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075761560&doi=10.1117%2f12.2529586&partnerID=40&md5=2ba1b48d95dcb4e7c01da1a6b14c9827","The difficulty in obtaining labeled data relevant to a given task is among the most common and well-known practical obstacles to applying deep learning techniques to new or even slightly modified domains. The data volumes required by the current generation of supervised learning algorithms typically far exceed what a human needs to learn and complete a given task. We investigate ways to expand a given labeled corpus of remote sensed imagery into a larger corpus using Generative Adversarial Networks (GANs). We then measure how these additional synthetic data affect supervised machine learning performance on an object detection task. Our data driven strategy is to train GANs to (1) generate synthetic segmentation masks and (2) generate plausible synthetic remote sensing imagery corresponding to these segmentation masks. Run sequentially, these GANs allow the generation of synthetic remote sensing imagery complete with segmentation labels. We apply this strategy to the data set from ISPRS' 2D Semantic Labeling Contest-Potsdam, with a follow on vehicle detection task. We find that in scenarios with limited training data, augmenting the available data with such synthetically generated data can improve detector performance. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Learning algorithms; Machine learning; Object detection; Object recognition; Semantics; Supervised learning; Adversarial networks; Detector performance; Limited training data; Remote sensed imagery; Remote sensing imagery; Remotely sensed imagery; Supervised machine learning; Synthetic data; Remote sensing","Deep Learning; Generative Adversarial Net-works; Object Detection; Remote sensing; Synthetic Data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075761560"
"Gong M.; Yang Y.; Zhan T.; Niu X.; Li S.","Gong, Maoguo (8933846400); Yang, Yuelei (57204044678); Zhan, Tao (57052462500); Niu, Xudong (57196705445); Li, Shuwei (57210320838)","8933846400; 57204044678; 57052462500; 57196705445; 57210320838","A Generative Discriminatory Classified Network for Change Detection in Multispectral Imagery","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","1","8600384","321","333","12","10.1109/JSTARS.2018.2887108","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059449220&doi=10.1109%2fJSTARS.2018.2887108&partnerID=40&md5=fff6fc5380de5e6c6f863588252413a5","Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods. © 2008-2012 IEEE.","Deep learning; Image classification; Remote sensing; Adversarial networks; Change detection; Competitive performance; Large amounts; Multi-spectral imagery; Multispectral images; Training sample; Unlabeled data; data set; detection method; image analysis; image classification; machine learning; spatiotemporal analysis; spectral analysis; Discriminators","Change detection; deep learning; generative adversarial networks (GANs); multispectral imagery","Article","Final","","Scopus","2-s2.0-85059449220"
"Jiang W.; Luo X.","Jiang, Wenjie (57210322716); Luo, Xiaoshu (7402870658)","57210322716; 7402870658","Research on super-resolution reconstruction algorithm of remote sensing image based on generative adversarial networks","2019","Proceedings of 2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering, AUTEEE 2019","","","9033352","438","441","3","10.1109/AUTEEE48671.2019.9033352","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083255066&doi=10.1109%2fAUTEEE48671.2019.9033352&partnerID=40&md5=41de8d3169fbce26074b67d30d347923","Due to natural conditions and hardware manufacturing processes, the resolution of remote sensing images is generally low. Obtaining high-definition remote sensing images by simply improving hardware and manufacturing processes is not only costly and technically challenging but also cannot be deployed on a large scale. Aiming at the limitations of the traditional methods, this paper studies the image super-resolution reconstruction method for improving the generated anti-network. Firstly, the generator network is optimized, and an RRDB (Residual-in-Residual Dense without BN (Batch Normalization) is used. Block) module; secondly, the related idea of relativistic GAN (relativistic generative adversarial network) is introduced, the relative value of the discriminator is not the absolute value; finally, the sensation loss is improved, and the feature is used before the function is activated. The test results show that the proposed algorithm is better than SRGAN (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network), SRCNN (Super-Resolution Convolutional Neural Network) and FSRCNN (Fast Super-Resolution Convolutional Neural Network). The clarity of the reconstructed image is improved, and the reconstructed image quality is significantly improved. © 2019 IEEE.","Convolution; Convolutional neural networks; Image enhancement; Manufacture; Optical resolving power; Remote sensing; Adversarial networks; Image super-resolution reconstruction; Manufacturing process; Natural conditions; Reconstructed image; Remote sensing images; Super resolution; Super resolution reconstruction; Image reconstruction","Aerial image; Generative Adversarial Networks; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85083255066"
"Sun Q.; Li X.; Li L.; Liu X.; Liu F.; Jiao L.","Sun, Qigong (57204953843); Li, Xiufang (57213196586); Li, Lingling (56327010200); Liu, Xu (57209845337); Liu, Fang (56182993400); Jiao, Licheng (7102491544)","57204953843; 57213196586; 56327010200; 57209845337; 56182993400; 7102491544","Semi-Supervised Complex-Valued GAN for Polarimetric SAR Image Classification","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898217","3245","3248","3","10.1109/IGARSS.2019.8898217","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077688005&doi=10.1109%2fIGARSS.2019.8898217&partnerID=40&md5=6f1e9c853f9fc6ca8ac1da9438b37d28","Polarimetric synthetic aperture radar (PolSAR) images are widely used in disaster detection and military reconnaissance and so on. However, their interpretation faces some challenges, e.g., deficiency of labeled data, inadequate utilization of data information and so on. In this paper, a complex-valued generative adversarial network (GAN) is proposed for the first time to address these issues. The complex number form of model complies with the physical mechanism of PolSAR data and in favor of utilizing and retaining amplitude and phase information of PolSAR data. GAN architecture and semi-supervised learning are combined to handle deficiency of la-beled data. GAN expands training data and semi-supervised learning is used to train network with generated, labeled and unlabeled data. Experimental results on two benchmark data sets show that our model outperforms existing state-of-the-art models, especially for conditions with fewer labeled data. © 2019 IEEE.","Complex networks; Geology; Labeled data; Military photography; Polarimeters; Radar imaging; Remote sensing; Semi-supervised learning; Synthetic aperture radar; Adversarial networks; Complex-valued; Data informations; Labeled and unlabeled data; Military reconnaissance; Phase information; Physical mechanism; Polarimetric synthetic aperture radars; Image classification","complex-valued operations; generative adversarial network; PolSAR image classification; semi-supervised learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077688005"
"Huang Q.; Li W.; Hu T.; Tao R.","Huang, Qian (57210124335); Li, Wei (56215159000); Hu, Ting (57212948752); Tao, Ran (7102857080)","57210124335; 56215159000; 57212948752; 7102857080","Hyperspectral Image Super-resolution Using Generative Adversarial Network and Residual Learning","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8683893","3012","3016","4","10.1109/ICASSP.2019.8683893","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069455619&doi=10.1109%2fICASSP.2019.8683893&partnerID=40&md5=85cc31b6adcef332903c828fbe7093aa","Due to the limitation of image acquisition, hyperspectral remote sensing imagery is hard to reflect in both high spatial and spectral resolutions. Super-resolution (SR) is a technique which can improve the spatial resolution. Inspired by recent achievements in deep convolutional neural network (CNN) and generative adversarial network (GAN), a GAN based framework is proposed for hyperspectral image super-resolution. In the proposed method, residual learning is used to obtain a high metrics and spectral fidelity, and a shorter connection is set between the input layer and output layer. The gradient features from low-resolution (LR) image to high-resolution (HR) are utilized as auxiliary information to assist deep CNN to carry out counter training with discriminator. Experimental results demonstrate that the proposed SR algorithm achieves superior performance in spectral fidelity and spatial resolution compared with baseline methods. © 2019 IEEE.","Deep neural networks; Image resolution; Neural networks; Optical resolving power; Remote sensing; Spectroscopy; Speech communication; Adversarial networks; Auxiliary information; Convolutional neural network; Hyper-spectral imageries; Hyperspectral remote sensing; Image super resolutions; Low resolution images; Super resolution; Audio signal processing","Generative Adversarial Network; Hyperspectral Imagery; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85069455619"
"Kerdegari H.; Razaak M.; Argyriou V.; Remagnino P.","Kerdegari, Hamideh (55438018000); Razaak, Manzoor (55639468300); Argyriou, Vasileios (13806485100); Remagnino, Paolo (6602806859)","55438018000; 55639468300; 13806485100; 6602806859","Urban scene segmentation using semi-supervised GAN","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","111551H","","","","10.1117/12.2533055","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078136549&doi=10.1117%2f12.2533055&partnerID=40&md5=300b8ecab1c88e4fe9f5c879afc2c092","Semantic segmentation of remote sensing data such as multispectral imagery has been boosted recently using deep convolutional neural networks (CNN). However, segmentation of multispectral images using supervised machine learning algorithms such as CNN requires a significant number of pixel-level annotated data, often unavailable, making the task extremely challenging. To address this, this paper puts forward a semi-supervised framework, based on generative adversarial networks (GAN). The proposed solution consists of a generator network to provide photo-realistic images as extra training data to a multi-class classifier acting as a discriminator and trained on a small annotated dataset. Performance of the proposed semi-supervised GAN is evaluated on two benchmarks multispectral semantic segmentation datasets collected from urban scenes of Vaihingen and Potsdam. Results indicate that the proposed framework achieves competitive performance compared to state-of-the-art semantic segmentation methods and show the potential of GAN-based methods for the challenging task of multispectral image segmentation. © 2019 SPIE.","Benchmarking; Classification (of information); Deep neural networks; Learning algorithms; Machine learning; Neural networks; Remote sensing; Semantics; Supervised learning; Adversarial networks; Competitive performance; Convolutional neural network; Multi-class classifier; Multi-spectral imagery; Semantic segmentation; Semi-supervised; Supervised machine learning; Image segmentation","Convolu-tional neural network; Generative adversarial networks (GAN); Multispectral imagery; Semantic segmentation; Semi-supervised GAN","Conference paper","Final","","Scopus","2-s2.0-85078136549"
"Merkle N.; Auer S.; Muller R.; Reinartz P.","Merkle, Nina (57194604557); Auer, Stefan (57216043589); Muller, Rupert (7404246697); Reinartz, Peter (56216874200)","57194604557; 57216043589; 7404246697; 56216874200","Exploring the potential of conditional adversarial networks for optical and SAR Image Matching","2018","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11","6","","1811","1820","9","10.1109/JSTARS.2018.2803212","101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044761197&doi=10.1109%2fJSTARS.2018.2803212&partnerID=40&md5=49f77a044147131a385ffa132f9f4010","Tasks such as the monitoring of natural disasters or the detection of change highly benefit from complementary information about an area or a specific object of interest. The required information is provided by fusing high accurate coregistered and georeferenced datasets. Aligned high-resolution optical and synthetic aperture radar (SAR) data additionally enable an absolute geolocation accuracy improvement of the optical images by extracting accurate and reliable ground control points (GCPs) from the SAR images. In this paper, we investigate the applicability of a deep learning based matching concept for the generation of precise and accurate GCPs from SAR satellite images by matching optical and SAR images. To this end, conditional generative adversarial networks (cGANs) are trained to generate SAR-like image patches from optical images. For training and testing, optical and SAR image patches are extracted from TerraSAR-X and PRISM image pairs covering greater urban areas spread over Europe. The artificially generated patches are then used to improve the conditions for three known matching approaches based on normalized cross-correlation (NCC), scale-invariant feature transform (SIFT), and binary robust invariant scalable key (BRISK), which are normally not usable for the matching of optical and SAR images. The results validate that a NCC-, SIFT-, and BRISK-based matching greatly benefit, in terms of matching accuracy and precision, from the use of the artificial templates. The comparison with two state-of-the-art optical and SAR matching approaches shows the potential of the proposed method but also revealed some challenges and the necessity for further developments. © 2008-2012 IEEE.","Europe; Adaptive optics; Deep learning; Disasters; Geology; Geometrical optics; Image enhancement; Image matching; Optical correlation; Optical image storage; Optical sensors; Rock mechanics; Space-based radar; Synthetic aperture radar; Adversarial networks; Artificial image; Biomedical optical imaging; Multi sensor images; Optical distortion; Optical imaging; Optical satellite images; accuracy assessment; correlation; data set; image resolution; optical method; remote sensing; satellite imagery; synthetic aperture radar; TerraSAR-X; Radar imaging","Artificial image generation; conditional generative adversarial networks (cGANs); multisensor image matching; optical satellite images; synthetic aperture radar (SAR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85044761197"
"Bona D.S.; Murni A.; Mursanto P.","Bona, Daniel Sande (57207304146); Murni, Aniati (36815724000); Mursanto, Petrus (24176993000)","57207304146; 36815724000; 24176993000","Semantic segmentation and segmentation refinement using machine learning case study: Water turbidity segmentation","2019","Proceedings of the 2019 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology, ICARES 2019","","","8914551","","","","10.1109/ICARES.2019.8914551","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077499784&doi=10.1109%2fICARES.2019.8914551&partnerID=40&md5=66e7e807d22f7d68063a33afd1bff417","Classical methods for image segmentation such as pixel thresholding, clustering, region growing, maximum likelihood have been used regularly and relied on for a long time. However, these classical methods have limitations, particularly on images where there are many overlapping pixel values between features, which is common in remote sensing images. The advent of machine learning, in particular, deep learning in computer vision and image analysis, has gained interest in the remote sensing field. Current deep learning architecture has been able to achieve high accuracy for image recognition, object detection, and segmentation. This study performed image segmentation on the coastal area with high water turbidity using Landsat-8 images. Currently, the standard tool to derive water turbidity data from Landsat-8 images is the level-2 plugin of SEADAS software. However, due to its rigorous processing method, the processing time using SEADAS Level-2 Plugin is quite long; for example, processing one Landsat-8 image took around 8 hours. As a consequence, the amount of time needed to process multiple images is increasing. Deep learning has advantages once the model trained, the inference or prediction process is quite fast. Therefore it has the potential to be used as a complementary tool to predict and segment high turbidity areas, because in deep learning. In this study, we implemented U-Net architecture with ResNet connection and used Generative-Adversarial Network (GAN) to refined segmentation results. © 2019 IEEE.","Deep learning; Image recognition; Learning systems; Machine learning; Maximum likelihood; Network architecture; Object detection; Pixels; Processing; Remote sensing; Semantics; Turbidity; Adversarial networks; Complementary tools; Learning architectures; Overlapping pixels; Prediction process; Remote sensing images; Segmentation results; Semantic segmentation; Image segmentation","CNN; Deep learning; GAN; Image segmentation; Machine learning; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85077499784"
"Wang X.; Xu G.; Wang Y.; Lin D.; Li P.; Lin X.","Wang, Xiaoke (57205550798); Xu, Guangluan (56420820800); Wang, Yang (57208313225); Lin, Daoyu (57196095251); Li, Peiguang (57213189982); Lin, Xiujing (57213192918)","57205550798; 56420820800; 57208313225; 57196095251; 57213189982; 57213192918","Thin and Thick Cloud Removal on Remote Sensing Image by Conditional Generative Adversarial Network","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8897958","1426","1429","3","10.1109/IGARSS.2019.8897958","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077673844&doi=10.1109%2fIGARSS.2019.8897958&partnerID=40&md5=35dd245789a1748d665d9e14fec0742a","Cloud removal is an essential step to enhance the quality of cloud-covered remote sensing image. In recent years, conditional Generative Adversarial Network (cGAN) yields promising improvement in plentiful image-to-image translation tasks. In this paper, we propose a novel objective function to upgrade the structural similarity index based on cGAN. We discover that ImageGAN is effective to focus on global information for thick cloud-covered images and Patch-GAN has fewer parameters while maintaining outstanding performance for thin cloud-covered remote sensing images in the experiments. Experimental results demonstrate that our method achieves remarkable performance in both PSNR, SSIM and visual effect on cloud-covered remote sensing images especially thin cloud-covered images. © 2019 IEEE.","Geology; Image enhancement; Adversarial networks; Cloud removal; Global informations; Image translation; Objective functions; Remote sensing images; Structural similarity indices; Visual effects; Remote sensing","cloud removal; Conditional generative adversarial network; remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85077673844"
"Zhu L.; Chen Y.; Ghamisi P.; Benediktsson J.A.","Zhu, Lin (57202316432); Chen, Yushi (15059992600); Ghamisi, Pedram (53663404300); Benediktsson, Jón Atli (7004494483)","57202316432; 15059992600; 53663404300; 7004494483","Generative Adversarial Networks for Hyperspectral Image Classification","2018","IEEE Transactions on Geoscience and Remote Sensing","56","9","8307247","5046","5063","17","10.1109/TGRS.2018.2805286","406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043383663&doi=10.1109%2fTGRS.2018.2805286&partnerID=40&md5=e832d8017551b5a4c9265496d935eae2","A generative adversarial network (GAN) usually contains a generative network and a discriminative network in competition with each other. The GAN has shown its capability in a variety of applications. In this paper, the usefulness and effectiveness of GAN for classification of hyperspectral images (HSIs) are explored for the first time. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. The aforementioned CNNs are trained together: the generative CNN tries to generate fake inputs that are as real as possible, and the discriminative CNN tries to classify the real and fake inputs. This kind of adversarial training improves the generalization capability of the discriminative CNN, which is really important when the training samples are limited. Specifically, we propose two schemes: 1) a well-designed 1D-GAN as a spectral classifier and 2) a robust 3D-GAN as a spectral-spatial classifier. Furthermore, the generated adversarial samples are used with real training samples to fine-tune the discriminative CNN, which improves the final classification performance. The proposed classifiers are carried out on three widely used hyperspectral data sets: Salinas, Indiana Pines, and Kennedy Space Center. The obtained results reveal that the proposed models provide competitive results compared to the state-of-the-art methods. In addition, the proposed GANs open new opportunities in the remote sensing community for the challenging task of HSI classification and also reveal the huge potential of GAN-based methods for the analysis of such complex and inherently nonlinear data. © 2018 IEEE.","California; Florida [United States]; Indiana; Kennedy Space Center; Salinas [California]; United States; Classification (of information); Convolution; Deep learning; Feature extraction; Gallium nitride; Gas generators; Hyperspectral imaging; III-V semiconductors; Independent component analysis; Neural networks; Personnel training; Remote sensing; Sampling; Space platforms; Spectroscopy; Adversarial networks; Classification performance; Convolutional Neural Networks (CNN); Discriminative networks; Generalization capability; Kennedy space centers; Spectral classifier; State-of-the-art methods; artificial neural network; data set; image classification; methodology; network analysis; remote sensing; spatial analysis; spectral analysis; Image classification","Convolutional neural network (CNN); deep learning; generative adversarial network (GAN); hyperspectral image (HSI) classification","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85043383663"
"Chen Y.; Huang L.; Zhu L.; Yokoya N.; Jia X.","Chen, Yushi (15059992600); Huang, Lingbo (57211925405); Zhu, Lin (57202316432); Yokoya, Naoto (36440631200); Jia, Xiuping (7201933692)","15059992600; 57211925405; 57202316432; 36440631200; 7201933692","Fine-grained classification of hyperspectral imagery based on deep learning","2019","Remote Sensing","11","22","2690","","","","10.3390/rs11222690","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075348422&doi=10.3390%2frs11222690&partnerID=40&md5=551e9aa50fb640cff13f474bd314e5fc","Hyperspectral remote sensing obtains abundant spectral and spatial information of the observed object simultaneously. It is an opportunity to classify hyperspectral imagery (HSI) with a fine-grained manner. In this study, the fine-grained classification of HSI, which contains a large number of classes, is investigated. On one hand, traditional classification methods cannot handle fine-grained classification of HSI well; on the other hand, deep learning methods have shown their powerfulness in fine-grained classification. So, in this paper, deep learning is explored for HSI supervised and semi-supervised fine-grained classification. For supervised HSI fine-grained classification, densely connected convolutional neural network (DenseNet) is explored for accurate classification. Moreover, DenseNet is combined with pre-processing technique (i.e., principal component analysis or auto-encoder) or post-processing technique (i.e., conditional random field) to further improve classification performance. For semi-supervised HSI fine-grained classification, a generative adversarial network (GAN), which includes a discriminative CNN and a generative CNN, is carefully designed. The GAN fully uses the labeled and unlabeled samples to improve classification accuracy. The proposed methods were tested on the Indian Pines data set, which contains 33,3951 samples with 52 classes. The experimental results show that the deep learning-based methods provide great improvements compared with other traditional methods, which demonstrate that deep models have huge potential for HSI fine-grained classification. © 2019 by the authors.","Convolution; Deep learning; Deep neural networks; Neural networks; Principal component analysis; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification performance; Convolutional neural network; Hyper-spectral imageries; Hyperspectral imagery classifications; Hyperspectral remote sensing; Post-processing techniques; Semi-supervised classification; Image classification","Convolutional neural network (CNN); Deep learning; Generative adversarial network (GAN); Hyperspectral imagery classification; Semi-supervised classification","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85075348422"
"Armanious K.; Abdulatif S.; Aziz F.; Schneider U.; Yang B.","Armanious, Karim (57208782510); Abdulatif, Sherif (57194649144); Aziz, Fady (57194650513); Schneider, Urs (55968038100); Yang, Bin (55584795030)","57208782510; 57194649144; 57194650513; 55968038100; 55584795030","An adversarial super-resolution remedy for radar design trade-offs","2019","European Signal Processing Conference","2019-September","","","","","","10.23919/EUSIPCO.2019.8902510","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075601097&doi=10.23919%2fEUSIPCO.2019.8902510&partnerID=40&md5=fb1669ec6d341145d913d5713255968b","Radar is of vital importance in many fields, such as autonomous driving, safety and surveillance applications. However, it suffers from stringent constraints on its design parametrization leading to multiple trade-offs. For example, the bandwidth in FMCW radars is inversely proportional with both the maximum unambiguous range and range resolution. In this work, we introduce a new method for circumventing radar design trade-offs. We propose the use of recent advances in computer vision, more specifically generative adversarial networks (GANs), to enhance low-resolution radar acquisitions into higher resolution counterparts while maintaining the advantages of the low-resolution parametrization. The capability of the proposed method was evaluated on the velocity resolution and range-azimuth trade-offs in micro-Doppler signatures and FMCW uniform linear array (ULA) radars, respectively. © 2019 IEEE","MIMO radar; MIMO systems; Neural networks; Optical resolving power; Radar; Radar signal processing; Remote sensing; Adversarial networks; Convolutional neural network; Micro-Doppler; Range Azimuth; Super resolution; Economic and social effects","CNN; Convolutional neural network; GAN; Generative adversarial networks; Micro-Doppler; MIMO; Radar; Range-azimuth; Remote sensing; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075601097"
"Zhang Q.; Wang H.; Du T.; Yang S.; Wang Y.; Xing Z.; Bai W.; Yi Y.","Zhang, Qi (57212800057); Wang, Huafeng (36783787700); Du, Tao (56038798600); Yang, Sichen (57212138469); Wang, Yuehai (57194466632); Xing, Zhiqiang (36832316200); Bai, Wenle (16548808600); Yi, Yang (57212149988)","57212800057; 36783787700; 56038798600; 57212138469; 57194466632; 36832316200; 16548808600; 57212149988","Super-resolution reconstruction algorithms based on fusion of deep learning mechanism and wavelet","2019","ACM International Conference Proceeding Series","","","","102","107","5","10.1145/3357254.3358600","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076009305&doi=10.1145%2f3357254.3358600&partnerID=40&md5=d9950831e29011996c436064dead3789","In this paper, we consider the problem of super-resolution reconstruction. This is a hot topic because super-resolution reconstruction has a wide range of applications in the medical field, remote sensing monitoring, and criminal investigation. Compared with traditional algorithms, the current super-resolution reconstruction algorithm based on deep learning greatly improves the clarity of reconstructed pictures. Existing work like Super-Resolution Using a Generative Adversarial Network (SRGAN) can effectively restore the texture details of the image. However, experimentally verified that the texture details of the image recovered by the SRGAN are not robust. In order to get super-resolution reconstructed images with richer high-frequency details, we improve the network structure and propose a super-resolution reconstruction algorithm combining wavelet transform and Generative Adversarial Network. The proposed algorithm can efficiently reconstruct high-resolution images with rich global information and local texture details. We have trained our model by Py Torch framework and VOC2012 dataset, and tested it by Set5, Set14, BSD100 and Urban100 test datasets. © 2019 Association for Computing Machinery.","Image enhancement; Image reconstruction; Image texture; Learning algorithms; Optical resolving power; Pattern recognition; Remote sensing; Statistical tests; Textures; Wavelet transforms; Adversarial networks; Criminal investigation; Global informations; High resolution image; Learning mechanism; Reconstructed image; Remote sensing monitoring; Super resolution reconstruction; Deep learning","Deep learning; Generative Adversarial Network; Super-resolution reconstruction; Wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85076009305"
"Qin J.; Zhan Y.; Wu K.; Liu W.; Yang Z.; Yao W.; Medjadba Y.; Zhang Y.; Yu X.","Qin, Jin (57208224824); Zhan, Ying (57197867950); Wu, Kang (57200601886); Liu, Wei (57835236600); Yang, Zhaoying (57200607038); Yao, Wang (57208224977); Medjadba, Yasmine (57208224017); Zhang, Yuanfei (57208227722); Yu, Xianchuan (12785792300)","57208224824; 57197867950; 57200601886; 57835236600; 57200607038; 57208224977; 57208224017; 57208227722; 12785792300","Semi-supervised classification of hyperspectral data for geologic body based on generative adversarial networks at tianshan area","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518946","4776","4779","3","10.1109/IGARSS.2018.8518946","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064161136&doi=10.1109%2fIGARSS.2018.8518946&partnerID=40&md5=e745a827c1e2f9c909ee474a63c7923b","Hyperspectral remote sensing data contains near continuous spectral information of the object, which is very suitable for mineral classification and geologic body mapping. However, the collecting of a lot of labeled hyperspectral data is expensive, time-consuming and labor-intensive. We choose a semi-supervised method to classify hyperspectral data based on a generative adversarial nertwork (GAN), just use a small amount of labeled data, named HSGAN. The GAN is made up of a generator and a discriminator, and the generator generates data similar to the real data so that the discriminator cannot tell if it is real data or generated data. We designed a one-dimensional GAN to extract spectral features from hyperspectral data. Using this method, we test the Tianshan hyperspectral data and use the actual geological map as the ground-truth produced by us. We find that HSGAN still achieves better results than the traditional CNN and SVM. © 2018 IEEE.","Deep learning; Geology; Machine learning; Photomapping; Remote sensing; Supervised learning; Adversarial networks; Geological mapping; Hymap data; Hyperspectral remote sensing data; Semi-supervised classification; Semi-supervised learning (SSL); Semi-supervised method; Spectral information; Classification (of information)","Deep learning; Generative adversarial networks (GAN); Geological mapping; Hymap data; Semi-supervised learning (SSL)","Conference paper","Final","","Scopus","2-s2.0-85064161136"
"Zheng R.; Wu G.; Yan C.; Zhang R.; Luo Z.; Yan B.","Zheng, Ruobing (57204454300); Wu, Guoqiang (57196020466); Yan, Chao (57201776358); Zhang, Renyu (57207402278); Luo, Ze (8552617500); Yan, Baoping (8675419400)","57204454300; 57196020466; 57201776358; 57207402278; 8552617500; 8675419400","Exploration in mapping kernel-based home range models from remote sensing imagery with conditional adversarial networks","2018","Remote Sensing","10","11","1722","","","","10.3390/rs10111722","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057082754&doi=10.3390%2frs10111722&partnerID=40&md5=a7ffe935913ebd44fcf5adee37acf200","Kernel-based home range models are widely-used to estimate animal habitats and develop conservation strategies. They provide a probabilistic measure of animal space use instead of assuming the uniform utilization within an outside boundary. However, this type of models estimates the home ranges from animal relocations, and the inadequate locational data often prevents scientists from applying them in long-term and large-scale research. In this paper, we propose an end-to-end deep learning framework to simulate kernel home range models. We use the conditional adversarial network as a supervised model to learn the home range mapping from time-series remote sensing imagery. Our approach enables scientists to eliminate the persistent dependence on locational data in home range analysis. In experiments, we illustrate our approach by mapping the home ranges of Bar-headed Geese in Qinghai Lake area. The proposed framework outperforms all baselines in both qualitative and quantitative evaluations, achieving visually recognizable results and high mapping accuracy. The experiment also shows that learning the mapping between images is a more effective way to map such complex targets than traditional pixel-based schemes. © 2018 by the authors.","Animals; Deep learning; Ecosystems; Mapping; Space optics; Adversarial networks; Conservation strategies; Habitat mapping; Home range; Large-scale research; Probabilistic measures; Quantitative evaluation; Remote sensing imagery; Remote sensing","Bar-headed Geese; Deep learning; Generative adversarial networks; Habitat mapping; Home range; Remote sensing","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85057082754"
"Lin D.; Fu K.; Wang Y.; Xu G.; Sun X.","Lin, Daoyu (57196095251); Fu, Kun (7202283802); Wang, Yang (57200055675); Xu, Guangluan (56420820800); Sun, Xian (34875643000)","57196095251; 7202283802; 57200055675; 56420820800; 34875643000","MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification","2017","IEEE Geoscience and Remote Sensing Letters","14","11","8059820","2092","2096","4","10.1109/LGRS.2017.2752750","137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031806729&doi=10.1109%2fLGRS.2017.2752750&partnerID=40&md5=801f4f67b235492d76f4bc0c15a32605","With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks. However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model G and a discriminative model D. We treat D as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. G can produce numerous images that are similar to the training data; therefore, D can learn better representations of remotely sensed images using the training data provided by G. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-theart methods. © 2004-2012 IEEE.","Feature extraction; Gallium nitride; Gas generators; Image classification; Image enhancement; Personnel training; Remote sensing; Supervised learning; Adversarial networks; Computational model; Scene classification; Training data; unsupervised representation learning; Classification (of information)","Generative adversarial networks (GANs); scene classification; unsupervised representation learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85031806729"
"Zhang M.; Hu X.; Zhao L.; Pang S.; Gong J.; Luo M.","Zhang, Mi (56130265200); Hu, Xiangyun (7404709263); Zhao, Like (57193203084); Pang, Shiyan (55576918700); Gong, Jinqi (57217562658); Luo, Min (57220998437)","56130265200; 7404709263; 57193203084; 55576918700; 57217562658; 57220998437","Translation-Aware semantic segmentation via conditional least-square generative adversarial networks","2017","Journal of Applied Remote Sensing","11","4","42622","","","","10.1117/1.JRS.11.042622","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040311479&doi=10.1117%2f1.JRS.11.042622&partnerID=40&md5=dc2dfe2f51297a51d76b480e2f57de18","Semantic segmentation has recently made rapid progress in the field of remote sensing and computer vision. However, many leading approaches cannot simultaneously translate label maps to possible source images with a limited number of training images. The core issue is insufficient adversarial information to interpret the inverse process and proper objective loss function to overcome the vanishing gradient problem. We propose the use of conditional least squares generative adversarial networks (CLS-GAN) to delineate visual objects and solve these problems. We trained the CLS-GAN network for semantic segmentation to discriminate dense prediction information either from training images or generative networks. We show that the optimal objective function of CLS-GAN is a special class of f-divergence and yields a generator that lies on the decision boundary of discriminator that reduces possible vanished gradient.We also demonstrate the effectiveness of the proposed architecture at translating images from label maps in the learning process. Experiments on a limited number of high resolution images, including close-range and remote sensing datasets, indicate that the proposed method leads to the improved semantic segmentation accuracy and can simultaneously generate high quality images from label maps. © 2017 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Image segmentation; Inverse problems; Remote sensing; Semantic Web; Semantics; Adversarial networks; Divergence; High quality images; High resolution image; Objective functions; Prediction informations; Proposed architectures; Semantic segmentation; Image enhancement","Deep Learning; Divergence.; Generative Adversarial Network; Semantic Segmentation","Article","Final","","Scopus","2-s2.0-85040311479"
"Lore K.G.; Reddy K.K.; Giering M.; Bernal E.A.","Lore, Kin Gwn (57190128995); Reddy, Kishore K. (55443153000); Giering, Michael (56464245300); Bernal, Edgar A. (12800366000)","57190128995; 55443153000; 56464245300; 12800366000","Generative adversarial networks for spectral super-resolution and bidirectional rgb-to-multispectral mapping","2019","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2019-June","","9025450","926","933","7","10.1109/CVPRW.2019.00122","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071519076&doi=10.1109%2fCVPRW.2019.00122&partnerID=40&md5=a0acb08f88953b4bf73f88bc6b5ad48a","Acquisition of multi-and hyperspectral imagery imposes significant requirements on the hardware capabilities of the sensors involved. In order to keep costs manageable, and due to limitations in the sensing technology, tradeoffs between the spectral and the spatial resolution of hyperspectral images are usually made. Such tradeoffs are usually not necessary when considering acquisition of traditional RGB imagery. We investigate the use of statistical learning, and in particular, of conditional generative adversarial networks (cGANs) to estimate mappings from three-channel RGB to 31-band multispectral imagery. We demonstrate the application of the proposed approach to (i) RGB-to-multispectral image mapping, (ii) spectral super-resolution of image data, and (iii) recovery of RGB imagery from multispectral data. © 2019 IEEE.","Computer vision; Optical resolving power; Remote sensing; Spectroscopy; Adversarial networks; Hyper-spectral imageries; Multi-spectral data; Multi-spectral imagery; Multispectral images; Sensing technology; Spatial resolution; Statistical learning; Photomapping","","Conference paper","Final","","Scopus","2-s2.0-85071519076"
"Alipourfard T.; Arefi H.","Alipourfard, T. (57216345975); Arefi, H. (14031194500)","57216345975; 14031194500","Virtual training sample generation by generative adversarial networks for hyperspectral images classification","2019","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","4/W18","","63","69","6","10.5194/isprs-archives-XLII-4-W18-63-2019","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083198692&doi=10.5194%2fisprs-archives-XLII-4-W18-63-2019&partnerID=40&md5=ace3f94edf92f9c576b53b1548f816a0","Convolutional Neural Networks (CNNs) as a well-known deep learning technique has shown a remarkable performance in visual recognition applications. However, using such networks in the area of hyperspectral image classification is a challenging and time-consuming process due to the high dimensionality and the insufficient training samples. In addition, Generative Adversarial Networks (GANs) has attracted a lot of attentions in order to generate virtual training samples. In this paper, we present a new classification framework based on integration of multi-channel CNNs and new architecture for generator and discriminator of GANs to overcome Small Sample Size (SSS) problem in hyperspectral image classification. Further, in order to reduce the computational cost, the methods related to the reduction of subspace dimension were proposed to obtain the dominant feature around the training sample to generate meaningful training samples from the original one. The proposed framework overcomes SSS and overfitting problem in classifying hyperspectral images. Based on the experimental results on real and well-known hyperspectral benchmark images, our proposed strategy improves the performance compared to standard CNNs and conventional data augmentation strategy. The overall classification accuracy in Pavia University and Indian Pines datasets was 99.8% and 94.9%, respectively. © 2019 T. Alipourfard.","Benchmarking; Classification (of information); Convolutional neural networks; Deep learning; E-learning; Image enhancement; Remote sensing; Sampling; Silicon compounds; Spectroscopy; Adversarial networks; Classification accuracy; Classification framework; High dimensionality; Images classification; Learning techniques; Over fitting problem; Small sample size problems; Image classification","Convolutional Neural Network; Deep Learning; Generative Adversarial Networks; Hyperspectral Image","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083198692"
"Forster A.; Behley J.; Behmann J.; Roscher R.","Forster, Alina (57204607800); Behley, Jens (35207956200); Behmann, Jan (55471258600); Roscher, Ribana (37113871300)","57204607800; 35207956200; 55471258600; 37113871300","Hyperspectral Plant Disease Forecasting Using Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898749","1793","1796","3","10.1109/IGARSS.2019.8898749","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077681986&doi=10.1109%2fIGARSS.2019.8898749&partnerID=40&md5=4b22e5861aa8f215f5c04a15a31e712b","With a limited amount of arable land, increasing demand for food induced by growth in population can only be meet with more effective crop production and more resistant plants. since crop plants are exposed to many different stress factors, it is relevant to investigate those factors as well as their behavior and reactions. One of the most severe stress factors are diseases, resulting in a high loss of cultivated plants. Our main objective is the forecasting of the spread of disease symptons on barley plants using a Cycle-Consistent Generative Adversarial Network. Our contributions are: (1) we provide a daily forecast for one week to advance research for better planning of plant protection measures, and (2) in contrast to most approaches which use only RGB images, we learn a model with hyperspectral images, providing an information-rich result. In our experiments, we analyze healthy barley leaves and leaves which were inoculated by powdery mildew. Images of the leaves were acquired daily with a hyperspectral microscope, from day 3 to day 14 after inoculation. We provide two methods for evaluating the predicted time series with respect to the reference time series. © 2019 IEEE.","Crops; Cultivation; Deep learning; Forecasting; Geology; Population statistics; Remote sensing; Spectroscopy; Time series; Adversarial networks; barley; Crop production; Phenotyping; Plant disease; Plant protection; Reference time; Spread of disease; Plants (botany)","barley; deep learning; generative adversarial networks; hyperspectral phenotyping; plant disease","Conference paper","Final","","Scopus","2-s2.0-85077681986"
"Palsson F.; Sveinsson J.R.; Ulfarsson M.O.","Palsson, Frosti (55052918200); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","55052918200; 7003642214; 6507677875","Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","2018","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2018-September","","8747268","","","","10.1109/WHISPERS.2018.8747268","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072027457&doi=10.1109%2fWHISPERS.2018.8747268&partnerID=40&md5=5e989f4449d87f7a978a0f1c7090f7ec","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel2 datasets. © 2018 IEEE.","Convolution; Image processing; Image resolution; Infrared devices; Infrared radiation; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Electromagnetic spectra; Multispectral images; Multispectral sensors; Sentinel-2; Short wave infrared; Spatial resolution; Image fusion","convolutional network; generative adversarial network; Image fusion; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85072027457"
"Luo Z.; Ding S.","Luo, Zijuan (24279796000); Ding, Shuai (57211227278)","24279796000; 57211227278","Object detection in remote sensing images based on GaN","2019","ACM International Conference Proceeding Series","","","","499","503","4","10.1145/3349341.3349458","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073071197&doi=10.1145%2f3349341.3349458&partnerID=40&md5=2b3227d478d57bb6b154fd096096c072","Remote sensing image recognition has been widely used in civil and military fields. In view of plenty of interference factors in remote-sensing aircraft such as shade, noise, the changing of perspective, etc. An improved target recognition algorithm in remote sensing image based on generative adversarial network is proposed. Before target recognition, Image enhancement based on generative adversarial network is carried out, and noise is removed through attention cycle sub-network to realize the reconstruction of high-resolution images from low-resolution images that retain high-frequency details of the target and improve the accuracy of target detection. Simulation results show that the feasibility of aircraft target recognition algorithm in removing sensing image and the scale and posture changes of target can be overcome. © 2019 Association for Computing Machinery.","Aircraft; Deep learning; Gallium nitride; III-V semiconductors; Image recognition; Military photography; Object detection; Object recognition; Optical character recognition; Remote sensing; Adversarial networks; High resolution image; Interference factor; Low resolution images; Recognition rules; Remote sensing aircrafts; Remote sensing images; Target recognition algorithms; Image enhancement","Aircraft recognition rule; Deep learning; Enhancement image; Generative adversarial Networks","Conference paper","Final","","Scopus","2-s2.0-85073071197"
"Sun Q.; Bourennane S.","Sun, Qiaoqiao (57197870870); Bourennane, Salah (6603752589)","57197870870; 6603752589","Unsupervised feature extraction based on improved Wasserstein generative adversarial network for hyperspectral classification","2019","Proceedings of SPIE - The International Society for Optical Engineering","11059","","","","","","10.1117/12.2527466","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072633405&doi=10.1117%2f12.2527466&partnerID=40&md5=b3c6e9c0bb97da686af10d3a04b7c048","Accurate classification is one of the most important prerequisites for hyperspectral applications and feature extraction is the key step of classification. Recently, deep learning models have been successfully used to extract the spectral-spatial features in hyperspectral images (HSIs). However, most deep learning-based classification methods are supervised and require sufficient samples to guarantee their performance. And the labeled samples in HSI are limited. To solve this problem, unsupervised feature extraction based on improved Wasserstein generative adversarial network (WGAN) is proposed in this paper. Further, in order to fully explore the spectral-spatial features in HSIs, a three-dimensional (3D) model is designed. Considering that its difficult for GANs to generate high dimension data, the dimension of HSI is reduced firstly by principle component analysis (PCA). Then, an improved WGAN is trained unsupervised with HSIs after dimensionality reduction to achieve the stable status. Finally, the discriminator of the trained improved WGAN is used as a feature extractor and followed by a classifier, which can be used to classify the HSIs. In order to evaluate the performance of the proposed method, PCA-based methods and GAN-based methods are compared in the experiment of a real-world HSI. Experimental results have shown that our proposed method gained more promising results which has great potential prospects in HSI classification. © 2019 SPIE.","3D modeling; Classification (of information); Deep learning; Deep neural networks; Extraction; Feature extraction; Neural networks; Remote sensing; Spectroscopy; Adversarial networks; Classification methods; Convolutional neural network; Dimensionality reduction; High-dimension data; Hyper-spectral classification; Principle component analysis; Three dimensional (3-D) modeling; Principal component analysis","Classification; Convolutional neural network (CNN); Deep learning; Feature extraction; Generative adversarial network (GAN); Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85072633405"
"Wang R.; Xiao X.; Guo B.; Qin Q.; Chen R.","Wang, Ruihua (57202650819); Xiao, Xiongwu (55967395700); Guo, Bingxuan (26432664400); Qin, Qianqing (7202497984); Chen, Ruizhi (25029686800)","57202650819; 55967395700; 26432664400; 7202497984; 25029686800","An effective image denoising method for UAV images via improved generative adversarial networks","2018","Sensors (Switzerland)","18","7","1985","","","","10.3390/s18071985","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048977614&doi=10.3390%2fs18071985&partnerID=40&md5=e64062180ea0e1fa4fca6647ac43b7e4","Unmanned aerial vehicles (UAVs) are an inexpensive platform for collecting remote sensing images, but UAV images suffer from a content loss problem caused by noise. In order to solve the noise problem of UAV images, we propose a new methods to denoise UAV images. This paper introduces a novel deep neural network method based on generative adversarial learning to trace the mapping relationship between noisy and clean images. In our approach, perceptual reconstruction loss is used to establish a loss equation that continuously optimizes a min-max game theoretic model to obtain better UAV image denoising results. The generated denoised images by the proposed method enjoy clearer ground objects edges and more detailed textures of ground objects. In addition to the traditional comparison method, denoised UAV images and corresponding original clean UAV images were employed to perform image matching based on local features. At the same time, the classification experiment on the denoised images was also conducted to compare the denoising results of UAV images with others. The proposed method had achieved better results in these comparison experiments. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.","Antennas; Deep neural networks; Game theory; Image denoising; Image reconstruction; Noise pollution; Remote sensing; Unmanned aerial vehicles (UAV); Adversarial learning; Adversarial networks; Game-theoretic model; Image dedoising; Image denoising methods; Mapping relationships; Neural network method; Remote sensing images; article; deep neural network; intermethod comparison; learning; theoretical study; Image enhancement","Generative adversarial networks; Image dedoising; Perceptual reconstruction loss; UAV images","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048977614"
"Soto P.J.; Bermudez J.D.; Happ P.N.; Feitosa R.Q.","Soto, P.J. (57216790448); Bermudez, J.D. (57200270007); Happ, P.N. (55768214000); Feitosa, R.Q. (6602453684)","57216790448; 57200270007; 55768214000; 6602453684","A COMPARATIVE ANALYSIS of UNSUPERVISED and SEMI-SUPERVISED REPRESENTATION LEARNING for REMOTE SENSING IMAGE CATEGORIZATION","2019","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","2/W7","","167","173","6","10.5194/isprs-annals-IV-2-W7-167-2019","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084680670&doi=10.5194%2fisprs-annals-IV-2-W7-167-2019&partnerID=40&md5=2f3aaf589a6bc7f1f43f3889776ca17b","This work aims at investigating unsupervised and semi-supervised representation learning methods based on generative adversarial networks for remote sensing scene classification. The work introduces a novel approach, which consists in a semi-supervised extension of a prior unsupervised method, known as MARTA-GAN. The proposed approach was compared experimentally with two baselines upon two public datasets, UC-MERCED and NWPU-RESISC45. The experiments assessed the performance of each approach under different amounts of labeled data. The impact of fine-tuning was also investigated. The proposed method delivered in our analysis the best overall accuracy under scarce labeled samples, both in terms of absolute value and in terms of variability across multiple runs. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Image analysis; Learning systems; Semi-supervised learning; Absolute values; Adversarial networks; Comparative analysis; Learning methods; Overall accuracies; Remote sensing images; Scene classification; Unsupervised method; Remote sensing","Deep Learning; Generative Adversarial Networks; Representation Learning; Semi-supervised Learning","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084680670"
"Saha S.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 9943212600; 7006892410","Unsupervised multiple-change detection in vhr multisensor images via deep-learning based adaptation","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900173","5033","5036","3","10.1109/IGARSS.2019.8900173","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091066032&doi=10.1109%2fIGARSS.2019.8900173&partnerID=40&md5=b0391853cc5285db6a03366210e2ea21","Change Detection (CD) using multitemporal satellite images is an important application of remote sensing. In this work, we propose a Convolutional-Neural-Network (CNN) based unsupervised multiple-change detection approach that simultaneously accounts for the high spatial correlation among pixels in Very High spatial Resolution (VHR) images and the differences in multisensor images. We accomplish this by learning in an unsupervised way a transcoding between multisensor multitemporal data by exploiting a cycle-consistent Generative Adversarial Network (CycleGAN) that consists of two generator CNN networks. After unsupervised training, one generator of the CycleGAN is used to mitigate multisensor differences, while the other is used as a feature extractor that enables the computation of multitemporal deep features. These features are then compared pixelwise to generate a change detection map. Changed pixels are then further analyzed based on multitemporal deep features for identifying different kind of changes (multiple-change detection). Results obtained on multisensor multitemporal dataset consisting of Quickbird and Pleiades images confirm the effectiveness of the proposed approach. ©2019 IEEE","Convolutional neural networks; Feature extraction; Pixels; Remote sensing; Adversarial networks; Feature extractor; Multi sensor images; Multi-temporal data; Multi-temporal satellite images; Spatial correlations; Unsupervised training; Very-high spatial resolutions; Deep learning","Change detection; Deep Change Vector Analysis; Deep learning; Generative Adversarial Network; Multisensor images; Multitemporal images; Very High Resolution","Conference paper","Final","","Scopus","2-s2.0-85091066032"
"","","","19th Pacific-Rim Conference on Multimedia, PCM 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11166 LNCS","","","","","2541","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054544658&partnerID=40&md5=78422fb46096c60049b42c102cf40f2e","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.","","","Conference review","Final","","Scopus","2-s2.0-85054544658"
"Li L.; Li P.; Yang M.; Gao S.","Li, Lei (56200687500); Li, Pengfei (57222710836); Yang, Meng (55703267800); Gao, Shibo (55478484300)","56200687500; 57222710836; 55703267800; 55478484300","Multi-branch Semantic GAN for Infrared Image Generation from Optical Image","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11935 LNCS","","","484","494","10","10.1007/978-3-030-36189-1_40","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077126552&doi=10.1007%2f978-3-030-36189-1_40&partnerID=40&md5=4447a927531fbaecf37dc82a12261dd3","Infrared remote sensing images capture the information of ground objects by their thermal radiation differences. However, the facility required for infrared imaging is not only priced high but also demands strict testing conditions. Thus it becomes an important topic to seek a way to convert easily-obtained optical remote sensing images into infrared remote sensing images. The conventional approaches cannot generate satisfactory infrared images due to the challenge of this task and many unknown parameters to be determined. In this paper, we proposed a novel multi-branch semantic GAN (MBS-GAN) for infrared image generation from the optical image. In the proposed model, we draw on the idea from Ensemble Learning and propose to use more than one generator to synthesize the infrared images with different semantic information. Specially, we integrate scene classification into image transformation to train models with scene information, which assists learned generation models to capture more semantic characteristics. The generated images are evaluated by PSNR, SSIM and cosine similarity. The experimental results prove that this proposed method is able to generate images retaining the infrared radiation characteristics of ground objects and performs well in converting optical images to infrared images. © 2019, Springer Nature Switzerland AG.","Big data; Classification (of information); Geometrical optics; Infrared radiation; Semantics; Thermography (imaging); Adversarial networks; Conventional approach; Image generations; Image transformations; Infrared radiation characteristic; Infrared remote sensing; Optical remote sensing; Scene classification; Remote sensing","Generative adversarial networks; Infrared image generation; Residual neural network","Conference paper","Final","","Scopus","2-s2.0-85077126552"
"Shi Q.; Liu X.; Li X.","Shi, Qian (55286447700); Liu, Xiaoping (15757680000); Li, Xia (34872691500)","55286447700; 15757680000; 34872691500","Road Detection from Remote Sensing Images by Generative Adversarial Networks","2017","IEEE Access","6","","","25486","25494","8","10.1109/ACCESS.2017.2773142","64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034248459&doi=10.1109%2fACCESS.2017.2773142&partnerID=40&md5=6fd6005c06376eb28cfd183cb868a516","Road detection with high-precision from very high resolution remote sensing imagery is very important in a huge variety of applications. However, most existing approaches do not automatically extract the road with a smooth appearance and accurate boundaries. To address this problem, we proposed a novel end-to-end generative adversarial network. In particular, we construct a convolutional network based on adversarial training that could discriminate between segmentation maps coming either from the ground truth or generated by the segmentation model. The proposed method could improve the segmentation result by finding and correcting the difference between ground truth and result output by the segmentation model. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods greatly on the performance of segmentation map. © 2013 IEEE.","Roads and streets; Adversarial networks; Convolutional networks; End to end; Remote sensing imagery; Remote sensing images; Road detection; State-of-the-art methods; Very high resolution; Remote sensing","end-to-end learning; Generative adversarial networks; road detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85034248459"
"Lu X.; Zhang J.; Zhou J.","Lu, Xiwen (57215129705); Zhang, Jiexin (57216517341); Zhou, Jianjiang (8901871000)","57215129705; 57216517341; 8901871000","Remote Sensing Image Translation Using Spatial-Frequency Consistency GAN","2019","Proceedings - 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2019","","","8965788","","","","10.1109/CISP-BMEI48845.2019.8965788","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079126856&doi=10.1109%2fCISP-BMEI48845.2019.8965788&partnerID=40&md5=9af2d14fc5e18a46cd691ccc0749f441","Synthesis aperture radar(SAR) is a powerful technique for remote sensing. The major advantage of SAR is all-weather and all-day imaging. However, SAR images are too hard for ordinary people. Motivated by the development of deep learning, it is available for the image translation from SAR images to optical images. Unfortunately, existed image translation methods ignore unique imaging of remote sensing images. Under unique imaging, remote sensing images are noisy and low resolution. Based on the observation, we propose our method, SFGAN, for remote sensing image translation using a generative adversarial network, which is trained on SEN1-2 Dataset. Our experiments compare our method with some state-of-the-art methods. Results show that the proposed method preserves more linear features under various area and has a higher PSNR. © 2019 IEEE.","Biomedical engineering; Deep learning; Geometrical optics; Image processing; Remote sensing; Synthetic aperture radar; Adversarial networks; Image translation; Linear feature; Low resolution; Ordinary people; Remote sensing images; Spatial frequency; State-of-the-art methods; Radar imaging","Generative adversarial network; Image translation; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85079126856"
"Liu J.; Chen K.; Xu G.; Li H.; Yan M.; DIao W.; Sun X.","Liu, Junfu (57213195524); Chen, Keming (55683543900); Xu, Guangluan (56420820800); Li, Hao (57196362306); Yan, Menglong (57200852160); DIao, Wenhui (56816620400); Sun, Xian (34875643000)","57213195524; 55683543900; 56420820800; 57196362306; 57200852160; 56816620400; 34875643000","Semi-Supervised Change Detection Based on Graphs with Generative Adversarial Networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898913","74","77","3","10.1109/IGARSS.2019.8898913","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077722752&doi=10.1109%2fIGARSS.2019.8898913&partnerID=40&md5=48946b2c31785f5179da3c7ed8e39908","In this paper, we present a semi-supervised remote sensing change detection method based on graph model with Generative Adversarial Networks (GANs). Firstly, the multi-temporal remote sensing change detection problem is converted as a problem of semi-supervised learning on graph where a majority of unlabeled nodes and a few labeled nodes are contained. Then, GANs are adopted to generate samples in a competitive manner and help improve the classification accuracy. Finally, a binary change map is produced by classifying the unlabeled nodes to a certain class with the help of both the labeled nodes and the unlabeled nodes on graph. Experimental results carried on several very high resolution remote sensing image data sets demonstrate the effectiveness of our method. © 2019 IEEE.","Geology; Graph theory; Semi-supervised learning; Adversarial networks; Change detection; Classification accuracy; Graph model; Multi-temporal remote sensing; Semi-supervised; Very high resolution; Remote sensing","Change Detection; Generative Adversarial Network; Graph Model; Semi-supervised Learning","Conference paper","Final","","Scopus","2-s2.0-85077722752"
"Jiang K.; Wang Z.; Yi P.; Wang G.; Lu T.; Jiang J.","Jiang, Kui (57203871718); Wang, Zhongyuan (57203515592); Yi, Peng (57203880354); Wang, Guangcheng (57209891180); Lu, Tao (56406646300); Jiang, Junjun (54902306100)","57203871718; 57203515592; 57203880354; 57209891180; 56406646300; 54902306100","Edge-Enhanced GAN for Remote Sensing Image Superresolution","2019","IEEE Transactions on Geoscience and Remote Sensing","57","8","8677274","5799","5812","13","10.1109/TGRS.2019.2902431","209","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069773369&doi=10.1109%2fTGRS.2019.2902431&partnerID=40&md5=4ce590788aca7fbb4534236883443046","The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches. © 1980-2012 IEEE.","Deep learning; Image reconstruction; Optical resolving power; Remote sensing; Satellites; Adversarial learning; dense connection; Edge enhancements; Remote sensing imagery; Super resolution; algorithm; experimental design; image processing; reconstruction; remote sensing; satellite data; satellite imagery; Image enhancement","Adversarial learning; dense connection; edge enhancement; remote sensing imagery; superresolution","Article","Final","","Scopus","2-s2.0-85069773369"
"Bashmal L.; Bazi Y.; AlHichri H.; Alajlan N.","Bashmal, Laila (57194728824); Bazi, Yakoub (8213665300); AlHichri, Haikel (6506519446); Alajlan, Naif (23134518500)","57194728824; 8213665300; 6506519446; 23134518500","Generative adversarial networks for cross-scene classification in remote sensing images","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8517487","4752","4755","3","10.1109/IGARSS.2018.8517487","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063124645&doi=10.1109%2fIGARSS.2018.8517487&partnerID=40&md5=0c39a5d0459376653cec2a605ead298f","In this paper, we present a novel method for cross-scene classification in remote sensing images based on generative adversarial networks (GANs). To this end, we train in an adversarial manner an encoder-decoder network coupled with a discriminator network on labeled and unlabeled data coming from two different domains. The encoder-decoder network aims to reduce the discrepancy between the distributions of the two domains, while the discriminator tries to discriminate between them. At the end of the optimization process, we train an extra network on the obtained encoded labeled data and then classify the encoded unlabeled data. Experimental results on two datasets acquired over the cities of Potsdam and Vaihingen with spatial resolutions of 5cm and 9cm, respectively, confirm the promising capability of the proposed method. © 2018 IEEE","","Cross-scene classification; Domain adaptation; Generative adversarial networks (GANs)","Conference paper","Final","","Scopus","2-s2.0-85063124645"
"Li G.; Sun Z.; Zhang Y.","Li, Gaopeng (8547935800); Sun, Zhao (57216846433); Zhang, Yun (56097923300)","8547935800; 57216846433; 56097923300","ISAR Target Recognition Using Pix2pix Network Derived from cGAN","2019","2019 International Radar Conference, RADAR 2019","","","9078939","","","","10.1109/RADAR41533.2019.171345","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084949774&doi=10.1109%2fRADAR41533.2019.171345&partnerID=40&md5=474363712da597c227abd48f7414f8b2","Inverse Synthetic Aperture Radar (ISAR) image processing has received much interest in recent years, due to its effectiveness in remote sensing and military use. Although ISAR can achieve all-time all-weather target detection, the quality of images is unstable due to many factors such as sea clutter, which will interfere with target recognition. Since, for sea objects, strong correlation exists between ISAR data and optical camera data, target information extraction accuracy and reliability can be improved by jointly processing the two types of data. In this paper, the pix2pix network derived from the conditional generative adversarial network (cGAN) is used to realize the translation of the ISAR images to the corresponding optical images. In order to remove the influence of lighting conditions on the color of the optical images, we use the grayscale images instead. We combine the generated and the ISAR images to train the CNN network for recognition. Experimental results demonstrate that the proposed method can effectively improve the recognition rate of target recognition based on the ISAR images. © 2019 IEEE.","Geometrical optics; Image enhancement; Inverse problems; Inverse synthetic aperture radar; Military photography; Optical correlation; Remote sensing; Adversarial networks; Extraction accuracy; Gray-scale images; Inverse synthetic aperture radars (ISAR); Lighting conditions; Optical camera; Strong correlation; Target recognition; Radar imaging","cGAN; ISAR target recognition; pix2pix network","Conference paper","Final","","Scopus","2-s2.0-85084949774"
"Lebedev M.A.; Komarov D.V.; Vygolov O.V.; Vizilter Y.V.","Lebedev, M.A. (56539470900); Komarov, D.V. (56539656000); Vygolov, O.V. (10439430400); Vizilter, Yu. V. (6506127474)","56539470900; 56539656000; 10439430400; 6506127474","Multisensor image fusion based on generative adversarial networks","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","111551T","","","","10.1117/12.2533098","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078141442&doi=10.1117%2f12.2533098&partnerID=40&md5=f38c3486368ebb972b244f944ea32e65","The paper addresses the further advance in our complex research in the field of multisensory image fusion based on generative adversarial models [1-2] and their application to such practical tasks as visual representation of fused images, acquired in different spectral ranges (e.g. TV and IR), and changes detection on images, acquired in different conditions (e.g. season-varying images). A developed architecture of a neural network based on pix2pix model is presented, which can solve the both tasks mentioned above. A technique for generating training and test datasets including data augmentation process is described. The results are demonstrated on real-world images. © 2019 SPIE.","Deep learning; Deep neural networks; Image acquisition; Image processing; Neural networks; Remote sensing; Adversarial networks; Changes detections; Data augmentation; Multisensor image fusion; Multisensory image fusions; Multispectral images; Real-world image; Visual representations; Image fusion","Deep learning; Generative adversarial networks; Image fusion; Image processing; Multispectral images; Neural networks","Conference paper","Final","","Scopus","2-s2.0-85078141442"
"Gao H.; Yao D.; Wang M.; Li C.; Liu H.; Hua Z.; Wang J.","Gao, Hongmin (34770690700); Yao, Dan (57204450382); Wang, Mingxia (57210431666); Li, Chenming (36806296000); Liu, Haiyun (37016646300); Hua, Zaijun (57118936200); Wang, Jiawei (57216323357)","34770690700; 57204450382; 57210431666; 36806296000; 37016646300; 57118936200; 57216323357","A hyperspectral image classification method based on multi-discriminator generative adversarial networks","2019","Sensors (Switzerland)","19","15","3269","","","","10.3390/s19153269","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070723724&doi=10.3390%2fs19153269&partnerID=40&md5=83e8385b86999a1eeb0fd6d8295e133d","Hyperspectral remote sensing images (HSIs) have great research and application value. At present, deep learning has become an important method for studying image processing. The Generative Adversarial Network (GAN) model is a typical network of deep learning developed in recent years and the GAN model can also be used to classify HSIs. However, there are still some problems in the classification of HSIs. On the one hand, due to the existence of different objects with the same spectrum phenomenon, if only according to the original GAN model to generate samples from spectral samples, it will produce the wrong detailed characteristic information. On the other hand, the gradient disappears in the original GAN model and the scoring ability of a single discriminator limits the quality of the generated samples. In order to solve the above problems, we introduce the scoring mechanism of multi-discriminator collaboration and complete semi-supervised classification on three hyperspectral data sets. Compared with the original GAN model with a single discriminator, the adjusted criterion is more rigorous and accurate and the generated samples can show more accurate characteristics. Aiming at the pattern collapse and diversity deficiency of the original GAN generated by single discriminator, this paper proposes a multi-discriminator generative adversarial networks (MDGANs) and studies the influence of the number of discriminators on the classification results. The experimental results show that the introduction of multi-discriminator improves the judgment ability of the model, ensures the effect of generating samples, solves the problem of noise in generating spectral samples and can improve the classification effect of HSIs. At the same time, the number of discriminators has different effects on different data sets. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Deep learning; Hyperspectral imaging; Image classification; Remote sensing; Spectroscopy; Supervised learning; Adversarial networks; Classification results; Different effects; Generating samples; Hyperspectral Data; Hyperspectral Remote Sensing Image; Research and application; Semi-supervised classification; Discriminators","Generative adversarial networks; Hyperspectral image classification; Multi-discriminator generative adversarial network; Semi-supervised classification","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85070723724"
"Liu M.; Hu Y.; Wang S.; Guo Y.; Hou B.; Jiao L.; Hou X.","Liu, Mengchen (57208264060); Hu, Yue (57208257621); Wang, Shuang (55940463600); Guo, Yanhe (57198927090); Hou, Biao (7102142690); Jiao, Licheng (7102491544); Hou, Xiaojin (56040912200)","57208264060; 57208257621; 55940463600; 57198927090; 7102142690; 7102491544; 56040912200","Fully convolutional semi-supervised GAN for PolSAR classification","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519541","621","624","3","10.1109/IGARSS.2018.8519541","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064275217&doi=10.1109%2fIGARSS.2018.8519541&partnerID=40&md5=26bce9a0921a50bd8cf3b4f25d34d516","We propose a novel semi-supervised fully convolutional network for Polarimetric synthetic aperture radar (PolSAR) terrain classification. First, by designing a fully convolutional structure, we can perform pixel-based classification tasks. Then, by applying semi-supervised generative adversarial networks (GANs), we utilize both labeled and unlabeled samples and aim to obtain higher classification accuracy. Through a mini-max two-player game, GAN has better performance than other ""single-player"" classifiers. Finally, we combine the fully convolutional structure with the semisupervised GAN. Our fully convolutional semi-supervised GAN (FC-SGAN) has excellent spatial feature learning ability and can perform end-to-end pixel-based classification tasks. Experimental results show that compared with existing works, the proposed method has better performances. Even when the training set gets smaller, our method keeps high accuracy. © 2018 IEEE.","Game theory; Geology; Machine learning; Pixels; Remote sensing; Supervised learning; Synthetic aperture radar; Adversarial networks; Classification accuracy; Convolutional networks; Pixel based classifications; Polarimetric synthetic aperture radars; Semi- supervised learning; Terrain classification; Unlabeled samples; Convolution","Fully convolutional network; Generative adversarial network; Semi-supervised learning; Terrain classification","Conference paper","Final","","Scopus","2-s2.0-85064275217"
"Bejiga M.B.; Melgani F.; Vascotto A.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300); Vascotto, Antonio (57208124510)","57192697078; 35613488300; 57208124510","Retro-Remote Sensing: Generating Images from Ancient Texts","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","3","8660422","950","960","10","10.1109/JSTARS.2019.2895693","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063894914&doi=10.1109%2fJSTARS.2019.2895693&partnerID=40&md5=300d238b0b3891057325948999c8e038","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area. © 2008-2012 IEEE.","Deep learning; Deep neural networks; Image processing; Neural networks; Adversarial networks; Convolutional neural network; Geographical area; Image synthesis; Multi-modal learning; Multimodal sources; Remote sensing images; Statistical properties; artificial neural network; civilization; geographical variation; image analysis; learning; pixel; remote sensing; Remote sensing","Convolutional neural networks (CNN); deep learning; generative adversarial networks (GAN); multimodal learning; remote sensing; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85063894914"
"Mueller M.S.; Sattler T.; Pollefeys M.; Jutzi B.","Mueller, M.S. (55422177900); Sattler, T. (36100746800); Pollefeys, M. (7004040532); Jutzi, B. (7801347466)","55422177900; 36100746800; 7004040532; 7801347466","IMAGE-TO-IMAGE TRANSLATION for ENHANCED FEATURE MATCHING, IMAGE RETRIEVAL and VISUAL LOCALIZATION","2019","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","4","2/W7","","111","119","8","10.5194/isprs-annals-IV-2-W7-111-2019","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084672809&doi=10.5194%2fisprs-annals-IV-2-W7-111-2019&partnerID=40&md5=f80cb9abbd1eebbc825d347b56232dcb","The performance of machine learning and deep learning algorithms for image analysis depends significantly on the quantity and quality of the training data. The generation of annotated training data is often costly, time-consuming and laborious. Data augmentation is a powerful option to overcome these drawbacks. Therefore, we augment training data by rendering images with arbitrary poses from 3D models to increase the quantity of training images. These training images usually show artifacts and are of limited use for advanced image analysis. Therefore, we propose to use image-to-image translation to transform images from a rendered domain to a captured domain. We show that translated images in the captured domain are of higher quality than the rendered images. Moreover, we demonstrate that image-to-image translation based on rendered 3D models enhances the performance of common computer vision tasks, namely feature matching, image retrieval and visual localization. The experimental results clearly show the enhancement on translated images over rendered images for all investigated tasks. In addition to this, we present the advantages utilizing translated images over exclusively captured images for visual localization. © 2019 ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences. All rights reserved.","Deep learning; Image analysis; Image retrieval; Learning algorithms; Quality control; Remote sensing; Rendering (computer graphics); Three dimensional computer graphics; Annotated training data; Data augmentation; Feature matching; Image translation; Rendered images; Training data; Training image; Visual localization; Image enhancement","3D Models; Convolutional Neural Networks; Data Augmentation; Feature Matching; Generative Adversarial Networks; Image Retrieval; Image-to-Image Translation; Visual Localization","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084672809"
"Bejiga M.B.; Melgani F.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300)","57192697078; 35613488300","Towards Generating Remote Sensing Images of the Far Past","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899834","9502","9505","3","10.1109/IGARSS.2019.8899834","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077699173&doi=10.1109%2fIGARSS.2019.8899834&partnerID=40&md5=ca6577d1479893c8d5b6ea3d7f491233","Text-to-image synthesis is a research topic that has not yet been addressed by the remote sensing community. It consists in learning a mapping from text description to image pixels. In this paper, we propose to address this topic for the very first time. More specifically, our objective is to convert ancient text descriptions of geographic areas written by past explorers into an equivalent remote sensing image. To this effect, we rely on generative adversarial networks (GANs) to learn the mapping. GANs aim to represent the distribution of a dataset using weights of a deep neural network, which are trained as an adversarial competition between two networks. We collected ancient texts dating back to 7 BC to train our network and obtained interesting results, which form the basis to highlight future research directions to advance this new topic. © 2019 IEEE.","Deep neural networks; Geology; Image processing; Mapping; Adversarial networks; Future research directions; GANs; Geographic areas; Image pixels; Image synthesis; Remote sensing images; Research topics; Remote sensing","GANs; remote sensing; text-to-image synthesis","Conference paper","Final","","Scopus","2-s2.0-85077699173"
"Han W.; Feng R.; Wang L.; Chen J.","Han, Wei (57191570975); Feng, Ruyi (55853730300); Wang, Lizhe (23029267900); Chen, Jia (57216636841)","57191570975; 55853730300; 23029267900; 57216636841","Supervised generative adversarial network based sample generation for scene classification","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-January","","8900525","3041","3044","3","10.1109/IGARSS.2019.8900525","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084269928&doi=10.1109%2fIGARSS.2019.8900525&partnerID=40&md5=b33ed194fde0b079ff1c5fb8cd8f9c04","High-resolution remote sensing (HRRS) image scene classification has been a critical task and greatly important for many applications, wherein convolutional neural network (CNN)-based methods have achieved considerable improvements. However, the CNN-based methods have countered a severe problem that massive annotation samples are required to obtain ideal model for scene classification. There is no dataset with a comparative scale to ImageNet to meet the sample requirement and labelling samples is labor-intensive and time-consuming. To solve the problem of insufficient annotation samples, a new generative adversarial network (GAN)-based sample generation method for scene classification is implemented. The proposed method is able to generate HRRS images with specific label and improve scene classification performance for the CNN-based methods.  © 2019 IEEE.","Image enhancement; Remote sensing; Adversarial networks; Critical tasks; High resolution remote sensing; Ideal model; Image scene classification; Labor intensive; Sample generations; Scene classification; Convolutional neural networks","Deep learning; High-resolution remote sensing; Scene classification","Conference paper","Final","","Scopus","2-s2.0-85084269928"
"Zhan Y.; Hu D.; Wang Y.; Yu X.","Zhan, Ying (57197867950); Hu, Dan (36161111200); Wang, Yuntao (55535376100); Yu, Xianchuan (12785792300)","57197867950; 36161111200; 55535376100; 12785792300","Semisupervised Hyperspectral Image Classification Based on Generative Adversarial Networks","2018","IEEE Geoscience and Remote Sensing Letters","15","2","8241773","212","216","4","10.1109/LGRS.2017.2780890","157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041227580&doi=10.1109%2fLGRS.2017.2780890&partnerID=40&md5=f1b8b078938310cc83d7dcb6c3e1984b","Because the collection of ground-truth labels is difficult, expensive, and time-consuming, classifying hyperspectral images (HSIs) with few training samples is a challenging problem. In this letter, we propose a novel semisupervised algorithm for the classification of hyperspectral data by training a customized generative adversarial network (GAN) for hyperspectral data. The GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. We design a semisupervised framework for HSI data based on a 1-D GAN (HSGAN). This framework enables the automatic extraction of spectral features for HSI classification. When HSGAN is trained using unlabeled hyperspectral data, the generator can generate hyperspectral samples that are similar to the real data, while the discriminator contains the features, which can be used to classify hyperspectral data with only a small number of labeled samples. The performance of the HSGAN is evaluated on the Airborne Visible Infrared Imaging Spectrometer image data, and the results show that the proposed framework achieves very promising results with a small number of labeled samples. © 2017 IEEE.","Deep learning; Image classification; Independent component analysis; Remote sensing; Spectroscopy; Thermography (imaging); Adversarial networks; Airborne visible infrared imaging spectrometer; Automatic extraction; Hyperspectral Data; Semi-supervised; Semi-supervised algorithm; Semi-supervised learning (SSL); Spectral feature; algorithm; AVIRIS; image classification; machine learning; remote sensing; spectral analysis; supervised classification; Classification (of information)","Deep learning; generative adversarial network (GAN); hyperspectral image (HSI) classification; remote sensing; semisupervised learning (SSL)","Article","Final","","Scopus","2-s2.0-85041227580"
"Duan Y.; Tao X.; Xu M.; Han C.; Lu J.","Duan, Yiping (57195315827); Tao, Xiaoming (57208894708); Xu, Mai (55703599800); Han, Chaoyi (57202250817); Lu, Jianhua (7601561492)","57195315827; 57208894708; 55703599800; 57202250817; 7601561492","GAN-NL: Unsupervised representation learning for remote sensing image classification","2019","2018 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2018 - Proceedings","","","8646414","375","379","4","10.1109/GlobalSIP.2018.8646414","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063079226&doi=10.1109%2fGlobalSIP.2018.8646414&partnerID=40&md5=dc241ed07157416d3fbe8a0c0c5baa1b","Recently, deep learning methods have greatly enhanced the classification performance because of their strong representation ability in the local receptive field. However, the non-local spatial information always exist in the images. Moreover, the limited amount of the labeled data imposes great challenges on the supervised representation learning model, especially the remote sensing images. With the consideration, we propose a generative adversarial network with non-local spatial information (GAN-NL) for remote sensing image classification. Specifically, a non-local layer is incorporated into a generative adversarial network for unsupervised representation learning. Then, a classification network is designed to infer the labels of the images. The classification results on the challenging NWPU-RESISC45 remote sensing image dataset show that our proposed method performs favorably against the state-of-the-art methods in terms of the classification accuracy without any pre-training. © 2018 IEEE.","Classification (of information); Deep learning; Remote sensing; Adversarial networks; Classification accuracy; Classification networks; Classification performance; Remote sensing image classification; Spatial informations; State-of-the-art methods; Unsupervised representation learning; Image classification","Generative adversarial networks (GANs); Non-local spatial information; Remote sensing image classification; Unsupervised representation learning","Conference paper","Final","","Scopus","2-s2.0-85063079226"
"Zhang M.; Cui Z.; Wang X.; Cao Z.","Zhang, Mingrui (57208225502); Cui, Zongyong (35118740300); Wang, Xianyuan (57194651633); Cao, Zongjie (55271466500)","57208225502; 35118740300; 57194651633; 55271466500","Data augmentation method of SAR image dataset","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518825","5292","5295","3","10.1109/IGARSS.2018.8518825","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064177307&doi=10.1109%2fIGARSS.2018.8518825&partnerID=40&md5=887e0a0b83f3bc6db18bf89867cd1fea","Large-scale high-quality, standardized, measurable and accurate data is the key to promote the progress of the algorithm in the radar remote sensing. Data scaling is a widespread technology that increases the size of a labeled training set dataset through specific data transformations. Synthetic Aperture Radar (SAR) image simulators based on computer-aided mapping models play an important role in SAR applications such as automatic target recognition and image interpretation, but the accuracy of this simulator is due to geometric errors and simplification of electromagnetic calculations. In order to achieve a SAR image datasets with the known target and azimuth angles, we can generate the desired image directly from a known image database. We can realize the augmentation of SAR image data set through linear synthesis and Generative Adversarial Networks, which can generate SAR images for the specified azimuth. © 2018 IEEE","Automatic target recognition; Computer aided analysis; Geology; Image processing; Linear network synthesis; Radar target recognition; Remote sensing; Synthetic aperture radar; Adversarial networks; Computer aided mapping; Data transformation; Electromagnetic calculations; Image interpretation; Radar remote sensing; SAR Images; Synthetic aperture radar (SAR) images; Radar imaging","Generative adversarial networks; Linear synthesis; SAR image","Conference paper","Final","","Scopus","2-s2.0-85064177307"
"Enomoto K.; Sakurada K.; Wang W.; Kawaguchi N.; Matsuoka M.; Nakamura R.","Enomoto, Kenji (57195940743); Sakurada, Ken (8669789300); Wang, Weiming (57208222597); Kawaguchi, Nobuo (8873031600); Matsuoka, Masashi (7401543073); Nakamura, Ryosuke (55439424500)","57195940743; 8669789300; 57208222597; 8873031600; 7401543073; 55439424500","Image translation between SAR and optical imagery with generative adversarial nets","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518719","1752","1755","3","10.1109/IGARSS.2018.8518719","30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064152008&doi=10.1109%2fIGARSS.2018.8518719&partnerID=40&md5=fa80fae182f802ab5ddf804463de552f","In this paper, we propose a method for the translation from Synthetic Aperture Radar (SAR) to optical images using conditional Generative Adversarial Networks (cGANs). Satellite images have been widely utilized for various purposes, such as natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. However, the obscurity caused by clouds leads to unstable monitoring of the ground situation while using the optical camera. Images captured by a longer wavelength are introduced to reduce the effects of clouds. In particular, SAR images are known to be nearly unaffected by clouds and are often used for stably observing the ground situation. On the other hand, SAR images have lower spatial resolution and visibility than optical images. Therefore, we propose a deep neural network that generates optical images from SAR images. Finally, we confirm the feasibility of the proposed network on a dataset consisting of optical images and the corresponding SAR images. © 2018 IEEE.","Deep learning; Deep neural networks; Geology; Geometrical optics; Image enhancement; Pollution control; Remote sensing; River pollution; Satellite imagery; Space-based radar; Synthetic aperture radar; Adversarial networks; Emergency response; GANs; Image translation; Natural environments; Optical imagery; Satellite images; Spatial resolution; Radar imaging","Deep learning; GANs; SAR; Satellite imagery","Conference paper","Final","","Scopus","2-s2.0-85064152008"
"Pan X.; Yang F.; Gao L.; Chen Z.; Zhang B.; Fan H.; Ren J.","Pan, Xuran (57705361100); Yang, Fan (56479614100); Gao, Lianru (14031580000); Chen, Zhengchao (7409481437); Zhang, Bing (8835983800); Fan, Hairui (57208630337); Ren, Jinchang (23398632100)","57705361100; 56479614100; 14031580000; 7409481437; 8835983800; 57208630337; 23398632100","Building extraction from high-resolution aerial imagery using a generative adversarial network with spatial and channel attention mechanisms","2019","Remote Sensing","11","8","917","","","","10.3390/rs11080917","85","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065020057&doi=10.3390%2frs11080917&partnerID=40&md5=1f461d4c01e0a9ce57bfd07dcc23a71a","Segmentation of high-resolution remote sensing images is an important challenge with wide practical applications. The increasing spatial resolution provides fine details for image segmentation but also incurs segmentation ambiguities. In this paper, we propose a generative adversarial network with spatial and channel attention mechanisms (GAN-SCA) for the robust segmentation of buildings in remote sensing images. The segmentation network (generator) of the proposed framework is composed of the well-known semantic segmentation architecture (U-Net) and the spatial and channel attention mechanisms (SCA). The adoption of SCA enables the segmentation network to selectively enhance more useful features in specific positions and channels and enables improved results closer to the ground truth. The discriminator is an adversarial network with channel attention mechanisms that can properly discriminate the outputs of the generator and the ground truth maps. The segmentation network and adversarial network are trained in an alternating fashion on the Inria aerial image labeling dataset and Massachusetts buildings dataset. Experimental results show that the proposed GAN-SCA achieves a higher score (the overall accuracy and intersection over the union of Inria aerial image labeling dataset are 96.61% and 77.75%, respectively, and the F1-measure of the Massachusetts buildings dataset is 96.36%) and outperforms several state-of-the-art approaches. © 2019 by the authors.","Aerial photography; Antennas; Buildings; Deep learning; Object recognition; Remote sensing; Semantics; Adversarial networks; Aerial images; High-resolution aerial images; Massachusetts; Semantic segmentation; Image segmentation","Deep learning; Generative adversarial network; High-resolution aerial images; Inria aerial image labeling dataset; Massachusetts buildings dataset; Semantic segmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85065020057"
"Leinonen J.; Guillaume A.; Yuan T.","Leinonen, Jussi (22980018800); Guillaume, Alexandre (21742642500); Yuan, Tianle (24081888700)","22980018800; 21742642500; 24081888700","Reconstruction of Cloud Vertical Structure With a Generative Adversarial Network","2019","Geophysical Research Letters","46","12","","7035","7044","9","10.1029/2019GL082532","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068180615&doi=10.1029%2f2019GL082532&partnerID=40&md5=0129a56da6c98b3418f6a862312fd8a9","We demonstrate the feasibility of solving atmospheric remote sensing problems with machine learning using conditional generative adversarial networks (CGANs), implemented using convolutional neural networks. We apply the CGAN to generating two-dimensional cloud vertical structures that would be observed by the CloudSat satellite-based radar, using only the collocated Moderate-Resolution Imaging Spectrometer measurements as input. The CGAN is usually able to generate reasonable guesses of the cloud structure and can infer complex structures such as multilayer clouds from the Moderate-Resolution Imaging Spectrometer data. This network, which is formulated probabilistically, also estimates the uncertainty of its own predictions. We examine the statistics of the generated data and analyze the response of the network to each input parameter. The success of the CGAN in solving this problem suggests that generative adversarial networks are applicable to a wide range of problems in atmospheric science, a field characterized by complex spatial structures and observational uncertainties. ©2019. American Geophysical Union. All Rights Reserved.","Clouds; Neural networks; Radar; Radar measurement; Remote sensing; Spectrometers; Uncertainty analysis; Adversarial networks; Atmospheric remote sensing; Cloud vertical structure; CloudSat; Convolutional neural network; Moderate res-olution imaging spectrometers; MODIS; Satellite-based radar; artificial neural network; cloud; CloudSat; feasibility study; machine learning; MODIS; radar; radar altimetry; reconstruction; remote sensing; two-dimensional modeling; vertical profile; Complex networks","clouds; CloudSat; GAN; generative adversarial network; MODIS; radar","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068180615"
"He C.; Fang P.; Zhang Z.; Xiong D.; Liao M.","He, Chu (12345438500); Fang, Peizhang (57204776176); Zhang, Zhi (57201398249); Xiong, Dehui (57201065746); Liao, Mingsheng (7202371636)","12345438500; 57204776176; 57201398249; 57201065746; 7202371636","An end-to-end conditional random fields and skip-connected generative adversarial segmentation network for remote sensing images","2019","Remote Sensing","11","13","1604","","","","10.3390/rs11131604","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068508238&doi=10.3390%2frs11131604&partnerID=40&md5=4a2a08fbd7a721fe262938d69c2cbac6","Semantic segmentation is an important process of scene recognition with deep learning frameworks achieving state of the art results, thus gaining much attention from the remote sensing community. In this paper, an end-to-end conditional random fields generative adversarial segmentation network is proposed. Three key factors of this algorithm are as follows. First, the network combines generative adversarial network and Bayesian framework to realize the estimation from the prior probability to the posterior probability. Second, the skip connected encoder-decoder network is combined with CRF layer to implement end-to-end network training. Finally, the adversarial loss and the cross-entropy loss guide the training of the segmentation network through back propagation. The experimental results show that our proposed method outperformed FCN in terms of mIoU for 0.0342 and 0.11 on two data sets, respectively. © 2019 by the authors.","Backpropagation; Deep learning; Random processes; Remote sensing; Semantics; Adversarial networks; Bayesian frameworks; Conditional random field; Learning frameworks; Loss functions; Posterior probability; Remote sensing images; Semantic segmentation; Image segmentation","Conditional random fields; Generative adversarial network; Loss function; Semantic segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85068508238"
"Bermudez J.D.; Happ P.N.; Feitosa R.Q.; Oliveira D.A.B.","Bermudez, Jose D. (57200270007); Happ, Patrick N. (55768214000); Feitosa, Raul Q. (6602453684); Oliveira, Dario A.B. (27567900100)","57200270007; 55768214000; 6602453684; 27567900100","Synthesis of Multispectral Optical Images from SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks","2019","IEEE Geoscience and Remote Sensing Letters","16","8","8637007","1220","1224","4","10.1109/LGRS.2019.2894734","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069501813&doi=10.1109%2fLGRS.2019.2894734&partnerID=40&md5=273712603dde1a4c2477a5546fab9439","The synthesis of realistic data using deep learning techniques has greatly improved the performance of classifiers in handling incomplete data. Remote sensing applications that have profited from those techniques include translating images of different sensors, improving the image resolution and completing missing temporal or spatial data such as in cloudy optical images. In this context, this letter proposes a new deep-learning-based framework to synthesize missing or corrupted multispectral optical images using multimodal/multitemporal data. Specifically, we use conditional generative adversarial networks (cGANs) to generate the missing optical image by exploiting the correspondent synthetic aperture radar (SAR) data with a SAR-optical data from the same area at a different acquisition date. The proposed framework was evaluated in two land-cover applications over tropical regions, where cloud coverage is a major problem: crop recognition and wildfire detection. In both applications, our proposal was superior to alternative approaches tested in our experiments. In particular, our approach outperformed recent cGAN-based proposals for cloud removal, on average, by 7.7% and 8.6% in terms of overall accuracy and F1-score, respectively. © 2019 IEEE.","Crops; Data handling; Deep learning; Fires; Geometrical optics; Image resolution; Radar imaging; Remote sensing; Synthetic aperture radar; Adversarial networks; Crop recognition; Land cover applications; Learning techniques; Multi-temporal data; Performance of classifier; Remote sensing applications; Wildfire detection; artificial neural network; data acquisition; experimental study; image analysis; image resolution; optical method; pattern recognition; remote sensing; satellite data; synthetic aperture radar; wildfire; Image enhancement","Conditional generative adversarial networks (cGANs); crop recognition; deep learning; remote sensing; wildfire detection","Article","Final","","Scopus","2-s2.0-85069501813"
"Hayatbini N.; Kong B.; Hsu K.-L.; Nguyen P.; Sorooshian S.; Stephens G.; Fowlkes C.; Nemani R.; Ganguly S.","Hayatbini, Negin (57195412150); Kong, Bailey (57205353363); Hsu, Kuo-Lin (7401526171); Nguyen, Phu (55331455800); Sorooshian, Soroosh (7005052907); Stephens, Graeme (57208765667); Fowlkes, Charless (6701549596); Nemani, Ramakrishna (35509463200); Ganguly, Sangram (56012593900)","57195412150; 57205353363; 7401526171; 55331455800; 7005052907; 57208765667; 6701549596; 35509463200; 56012593900","Conditional generative adversarial networks (cGANs) for near real-time precipitation estimation from multispectral GOES-16 satellite imageries-PERSIANN-cGAN","2019","Remote Sensing","11","19","2193","","","","10.3390/rs11192193","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073408806&doi=10.3390%2frs11192193&partnerID=40&md5=ee563f287877925796524d0b1da5cda2","In this paper, we present a state-of-the-art precipitation estimation framework which leverages advances in satellite remote sensing as well as Deep Learning (DL). The framework takes advantage of the improvements in spatial, spectral and temporal resolutions of the Advanced Baseline Imager (ABI) onboard the GOES-16 platform along with elevation information to improve the precipitation estimates. The procedure begins by first deriving a Rain/No Rain (R/NR) binary mask through classification of the pixels and then applying regression to estimate the amount of rainfall for rainy pixels. A Fully Convolutional Network is used as a regressor to predict precipitation estimates. The network is trained using the non-saturating conditional Generative Adversarial Network (cGAN) and Mean Squared Error (MSE) loss terms to generate results that better learn the complex distribution of precipitation in the observed data. Common verification metrics such as Probability Of Detection (POD), False Alarm Ratio (FAR), Critical Success Index (CSI), Bias, Correlation and MSE are used to evaluate the accuracy of both R/NR classification and real-valued precipitation estimates. Statistics and visualizations of the evaluation measures show improvements in the precipitation retrieval accuracy in the proposed framework compared to the baseline models trained using conventional MSE loss terms. This framework is proposed as an augmentation for PERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information using Artificial Neural Network- Cloud Classification System) algorithm for estimating global precipitation. © 2019 by the authors.","Classification (of information); Convolution; Deep learning; Image enhancement; Learning algorithms; Learning systems; Mean square error; Neural networks; Pixels; Precipitation (chemical); Rain; Remote sensing; Adversarial networks; Cloud classification systems; Convolutional neural network; Multispectral satellite imagery; Precipitation estimation; Precipitation estimation from remotely sensed information; Precipitation retrievals; Probability of detection; Satellite imagery","Convolutional neural networks (CNNs); Generative adversarial networks (GANs); Machine learning; Multispectral satellite imagery; Precipitation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85073408806"
"Ma W.; Pan Z.; Guo J.; Lei B.","Ma, Wen (57207877267); Pan, Zongxu (54788169800); Guo, Jiayi (57194143247); Lei, Bin (14063767500)","57207877267; 54788169800; 57194143247; 14063767500","Super-resolution of remote sensing images based on transferred generative adversarial network","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8517442","1148","1151","3","10.1109/IGARSS.2018.8517442","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063138028&doi=10.1109%2fIGARSS.2018.8517442&partnerID=40&md5=39673883be5d097f6a3bd4bf2fe8f3fc","Single image super-resolution (SR) has been widely studied in recent years as a crucial technique for remote sensing applications. This paper proposes a SR method for remote sensing images based on a transferred generative adversarial network (TGAN). Different from the previous GAN-based SR approaches, the novelty of our method mainly reflects from two aspects. First, the batch normalization layers are removed to reduce the memory consumption and the computational burden, as well as raising the accuracy. Second, our model is trained in a transfer-learning fashion to cope with the insufficiency of training data, which is the crux of applying deep learning methods to remote sensing applications. The model is firstly trained on an external dataset DIV2K and further fine-tuned with the remote sensing dataset. Our experimental results demonstrate that the proposed method is superior to SRCNN and SRGAN in terms of both the objective evaluation and the subjective perspective. © 2018 IEEE","","Generative adversarial network; Remote sensing images; Super-resolution; Transfer learning","Conference paper","Final","","Scopus","2-s2.0-85063138028"
"Wang A.; Li Y.; Jiang K.; Zhao L.; Iwahori Y.","Wang, Aili (55483869300); Li, Yao (57204667010); Jiang, Kaiyuan (57211691470); Zhao, Lanfei (55818662300); Iwahori, Yuji (7003339167)","55483869300; 57204667010; 57211691470; 55818662300; 7003339167","Lidar Data Classification Algorithm Based on Generative Adversarial Network","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899182","2487","2490","3","10.1109/IGARSS.2019.8899182","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077712933&doi=10.1109%2fIGARSS.2019.8899182&partnerID=40&md5=688f5d618474df7ade9fde2609d8ea7d","In this paper, the Generative Adversarial Network (GAN) is applied to LiDAR data classification. Generative Adversarial Network usually includes a generating network and a discriminant network. In GAN, a convolutional neural network (CNN) is designed to distinguish inputs. Another CNN is used to generate so-called false inputs. Combining with the actual training samples, the discriminant CNN is fine-tuned to improve the final classification performance. The proposed classifier is implemented on real data sets. The results show that the accuracy of the proposed network is higher than that of the classification method based on CNN, which shows that the features extracted by the network have better discrimination and stronger competitiveness. © 2019 IEEE.","Convolutional neural networks; Geology; Optical radar; Remote sensing; Adversarial networks; Classification methods; Classification performance; LIDAR data; Real data sets; Training sample; Classification (of information)","CNN; fine-tune; GAN; LiDAR","Conference paper","Final","","Scopus","2-s2.0-85077712933"
"Ma J.; Zhou Z.; Wang B.; An Z.","Ma, Jinlei (57193420327); Zhou, Zhiqiang (55728196000); Wang, Bo (36012839000); An, Zhe (57211239563)","57193420327; 55728196000; 36012839000; 57211239563","Hard Ship Detection via Generative Adversarial Networks","2019","Proceedings of the 31st Chinese Control and Decision Conference, CCDC 2019","","","8833176","3961","3965","4","10.1109/CCDC.2019.8833176","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073112074&doi=10.1109%2fCCDC.2019.8833176&partnerID=40&md5=492cfd589b8f456e50e2cc7d45b8ba22","In optical remote sensing images, many ships have very similar shapes and textures with backgrounds. In this case, it is very hard to accurately detect these ships. In this paper, we introduce generative adversarial networks (GANs) to perform hard ship detection. GANs consist of one generative network and one discriminator network. We take state-of-the-art object (ship) detection network Faster R-CNN as the generative network, which outputs the detection results as fake samples. The ground-truth ships in the input image are set as the real samples. The discriminator network is responsible for distinguishing between fake samples and real samples. The two networks are simultaneously trained. Through continuous adversarial training, the fake samples generated by the generative network can be very similar to the real samples, and the discriminator network would not correctly distinguish between fake samples and real samples. As a result, the ship detection network (generative network) correctly recognizes hard-detection ships, producing satisfactory detection results. What's more, the discriminator network is only used in training process, and thus the proposed method not only improves detection accuracy, but also does not increase computational cost. © 2019 IEEE.","","Convolutional neural networks; Generative adversarial networks; Ship detection","Conference paper","Final","","Scopus","2-s2.0-85073112074"
"Huang H.; Zhang F.; Zhou Y.; Yin Q.; Hu W.","Huang, Henghua (57219442515); Zhang, Fan (56320587700); Zhou, Yongsheng (22959334700); Yin, Qiang (36959885000); Hu, Wei (56316293300)","57219442515; 56320587700; 22959334700; 36959885000; 56316293300","High resolution sar image synthesis with hierarchical generative adversarial networks","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900494","2782","2785","3","10.1109/IGARSS.2019.8900494","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082673599&doi=10.1109%2fIGARSS.2019.8900494&partnerID=40&md5=a10eaaed3665d0fd8aacfcae65f8bade","Generative adversarial network (GAN) is an artificial neural network based on unsupervised learning method. Due to its powerful model representation capabilities, GAN has been introduced to synthesize synthetic aperture radar (SAR) image data, for the real sample is difficult to acquire. Largescale, high-resolution SAR images play an important role in promoting SAR applications, such as automatic target recognition and image interpretation. However, on account of the difficult training problem of GAN network, especially for SAR images with speckle noise, it is difficult to obtain high-resolution SAR images by simply transfer the net from optical image. Recent studies in other image fields have shown that hierarchical structure is an effective and useful way to decompose a generation task into several smaller subtasks. How to obtain more high-resolution SAR images from limited original samples through GAN is the target of our research. Therefore, in this paper, we introduce a hierarchical GAN network model to generate SAR images, through the multi-stage network, gradually improve the quality of the generated image, and finally obtain highresolution images. The type and aspect of generated images are determined by the input of condition vectors in the last two stages. In addition, we introduce the triple loss, in which the background loss is used to imitating background clutter noise of SAR image, the condition loss is to make the generated images' type and aspect become controllable, and the global loss for getting higher image generation quality. The generated images show high similarity with the real samples.  © 2019 IEEE.","Automatic target recognition; Geometrical optics; Image enhancement; Learning systems; Radar target recognition; Remote sensing; Synthetic aperture radar; Unsupervised learning; Adversarial networks; Hierarchical structures; High resolution image; High-resolution SAR; Image interpretation; Model representation; Synthetic aperture radar (SAR) images; Unsupervised learning method; Radar imaging","Automatic target recognition (ATR); Generative adversarial network(GAN); SAR simulator; Synthetic aperture radar (SAR); Triple loss","Conference paper","Final","","Scopus","2-s2.0-85082673599"
"Kniaz V.V.","Kniaz, Vladimir V. (56540022100)","56540022100","Deep learning for dense labeling of hydrographic regions in very high resolution imagery","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","111550W","","","","10.1117/12.2533161","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078138217&doi=10.1117%2f12.2533161&partnerID=40&md5=87257f6e7757df3b50cc4b40f797dbfd","Automatic dense labeling of multispectral satellite images facilitates faster map update process. Water objects are essential elements of a geographic map. While modern dense labeling methods perform robust segmentation of such objects like roads, buildings, and vegetation, dense labeling of hydrographic regions remains a challenging problem. Water objects change their surface albedo, color, and reflection in different weather and different seasons. Moreover, rivers and lakes can change their boundaries after floods or droughts. Robust documentation of such seasonal changes is an essential task in the field of analysis of satellite imagery. Due to the high variance in water object appearance, their segmentation is usually performed manually by a human operator. Recent advances in machine learning have made possible robust segmentation of static objects such as buildings and roads. To the best of our knowledge, there is little research in the modern literature regarding dense labeling of water regions. This paper is focused on the development of a deep-learning-based method for dense labeling of hydrographic in aerial and satellite imagery. We use the GeoGAN framework and MobileNetV2 as the starting point for our research. The GeoGAN framework uses an aerial image as an input to generate pixel-level annotations of five object classes: building, low vegetation, high vegetation, road, and car. The GeoGAN framework leverages two deep learning approaches to ensure robust labeling: a generator with skip connections and Generative Adversarial Networks. A generator with skip connections performs image→label translation using feed-forward connections between convolutional and deconvolutional layers of the same depth. A GAN framework consists of two competing networks: a generator and a discriminator. The adversarial loss improves the quality of the resulting dense labeling. We made the following contributions to the GeoGAN framework: (1) new MobileNetV2-based generator, (2) adversarial loss function. We term the resulting framework as HydroGAN. We evaluate our HydroGAN model using a new HydroViews dataset focused on dense labeling of areas that are subject to severe flooding during the spring season. The evaluation results are encouraging and demonstrate that our HydroGAN model competes with the state-of-the-art models for dense labeling of aerial and satellite imagery. The evaluation demonstrates that our model can generalize from the training data to previously unseen samples. The developed HydroGAN model is capable of performing dense labeling of water objects in different seasons. We made our model publicly available. © 2019 SPIE.","Antennas; Convolution; Floods; Image segmentation; Neural networks; Remote sensing; Satellite imagery; Semantics; Vegetation; Adversarial networks; Convolutional neural network; Evaluation results; Learning-based methods; Multispectral satellite image; Robust segmentation; Semantic segmentation; Very high resolution; Deep learning","Convolutional Neural Networks; Generative Adversarial Networks; Multispectral satellite images; Remote sensing; Semantic segmentation","Conference paper","Final","","Scopus","2-s2.0-85078138217"
"Hahn A.; Tummala M.; Scrofani J.","Hahn, Andrew (57215828729); Tummala, Murali (7004626258); Scrofani, James (55536175800)","57215828729; 7004626258; 55536175800","Extended Semi-Supervised Learning GAN for Hyperspectral Imagery Classification","2019","2019, 13th International Conference on Signal Processing and Communication Systems, ICSPCS 2019 - Proceedings","","","9008719","","","","10.1109/ICSPCS47537.2019.9008719","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081997157&doi=10.1109%2fICSPCS47537.2019.9008719&partnerID=40&md5=d5326ed9afd8e7f21944cc3659d01ff4","Hyperspectral imagery (HSI) cubes are high-dimensional datasets that lend themselves well to deep learning approaches for classification. Deep learning approaches, specifically generative adversarial networks (GANs), have been shown to be very effective in classification and generation of accurate synthetic data in computer vision problems. This work proposes an extension of an existing GAN training scheme, called extended semi-supervised learning (ESSL), metrics for evaluating GAN training performance, and demonstrates the effectiveness of the proposed training scheme to improve classification of HSI. Using ESSL with GAN, we have been able to achieve approximately 0.8% increase in classification accuracy over convolutional neural networks as well as generate extremely accurate synthetic imagery. © 2019 IEEE.","Classification (of information); Convolutional neural networks; Deep learning; Image classification; Remote sensing; Spectroscopy; Adversarial networks; Classification accuracy; Computer vision problems; High dimensional datasets; Hyperspectral imagery; Hyperspectral imagery classifications; Learning approach; Synthetic imagery; Semi-supervised learning","","Conference paper","Final","","Scopus","2-s2.0-85081997157"
"Saha S.; Solano-Correa Y.T.; Bovolo F.; Bruzzone L.","Saha, Sudipan (57205200597); Solano-Correa, Yady Tatiana (57156565000); Bovolo, Francesca (9943212600); Bruzzone, Lorenzo (7006892410)","57205200597; 57156565000; 9943212600; 7006892410","Unsupervised deep learning based change detection in Sentinel-2 images","2019","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2019","","","8866899","","","","10.1109/Multi-Temp.2019.8866899","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074261635&doi=10.1109%2fMulti-Temp.2019.8866899&partnerID=40&md5=25ba3a12a30dfec24e683bb4a0a26b96","Change Detection (CD) is an important application of remote sensing. Recent technological evolution resulted in the availability of optical multispectral sensors that provide High spatial Resolution (HR) images with many spectral bands. Such characteristics allow for new applications of CD, however present new challenges on the proper exploitation of the information. HR multitemporal data processing is challenging due to spatial correlation of pixels and spatial context information needs to be exploited to benefit from multitemporal HR images. Moreover most of the state-of-The-Art CD methods exploit single or couple of spectral channels from the optical sensors to derive CD map. To overcome these challenges, this paper presents a novel unsupervised deep-learning based method that can effectively model contextual information and handle all the bands in multispectral images. In particular, we focus on the Sentinel-2 images provided by the European Space Agency (ESA) that provides both higher spatial and temporal resolution optical images with 13 spectral bands with respect to previous generation sensors. Experimental results on the urban Onera satellite CD (OSCD) dataset and on agricultural multitemporal images from Barrax, Spain confirms the effectiveness of the proposed method. © 2019 IEEE.","Data handling; Geometrical optics; Image analysis; Remote sensing; Space optics; Adversarial networks; Change detection; High resolution; High spatial resolution; Learning-based methods; Sentinel-2; Spatial and temporal resolutions; Technological evolution; Deep learning","Change detection; Deep learning; Generative Adversarial Network; High Resolution; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85074261635"
"Bittner K.; d'Angelo P.; Körner M.; Reinartz P.","Bittner, K. (57194603356); d'Angelo, P. (9838984100); Körner, M. (57190168095); Reinartz, P. (56216874200)","57194603356; 9838984100; 57190168095; 56216874200","Automatic large-scalae 3D building shape refinement using conditional generative adversarial networks","2018","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","42","2","","103","108","5","10.5194/isprs-archives-XLII-2-103-2018","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048370080&doi=10.5194%2fisprs-archives-XLII-2-103-2018&partnerID=40&md5=efdfb0c917f776dc2b4fd05baac8333c","Three-dimensional building reconstruction from remote sensing imagery is one of the most difficult and important 3D modeling problems for complex urban environments. The main data sources provided the digital representation of the Earths surface and related natural, cultural, and man-made objects of the urban areas in remote sensing are the digital surface models (DSMs). The DSMs can be obtained either by light detection and ranging (LIDAR), SAR interferometry or from stereo images. Our approach relies on automatic global 3D building shape refinement from stereo DSMs using deep learning techniques. This refinement is necessary as the DSMs, which are extracted from image matching point clouds, suffer from occlusions, outliers, and noise. Though most previous works have shown promising results for building modeling, this topic remains an open research area. We present a new methodology which not only generates images with continuous values representing the elevation models but, at the same time, enhances the 3D object shapes, buildings in our case. Mainly, we train a conditional generative adversarial network (cGAN) to generate accurate LIDAR-like DSM height images from the noisy stereo DSM input. The obtained results demonstrate the strong potential of creating large areas remote sensing depth images where the buildings exhibit better-quality shapes and roof forms. © Authors 2018.","Buildings; Deep learning; Image enhancement; Optical radar; Remote sensing; Three dimensional computer graphics; 3d buildings; 3D scenes; Adversarial networks; Building reconstruction; Complex urban environments; Digital representations; Digital surface models; Light detection and ranging; Stereo image processing","3D building shape; 3D scene refinement; Conditional generative adversarial networks (cGANs); Digital surface model","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85048370080"
"Liu L.; Lei B.","Liu, Lei (56194753300); Lei, Bin (14063767500)","56194753300; 14063767500","Can SAR images and optical images transfer with each other?","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8518921","7019","7022","3","10.1109/IGARSS.2018.8518921","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064147595&doi=10.1109%2fIGARSS.2018.8518921&partnerID=40&md5=445382ff8906c7640cf51634c3f753c5","Synthetic aperture radar (SAR) and optical imaging are different remote sensing methods. Given a SAR image, is it possible to predict what the observed scene looks like in an optical image? Transfer between SAR data and optical data seems to be impossible. However, this article shows examples that by applying deep learning techniques on high resolution airborne SAR images and GoogleEarth optical images, the SAR images and optical images can transfer with each other. The transferring help us to better understand the relationship between SAR and optical image, and can be potentially used to transfer detection or classification algorithms for optical image straightforwardly to be applied on SAR image. © 2018 IEEE.","Deep learning; Geology; Geometrical optics; Remote sensing; Synthetic aperture radar; Adversarial networks; CycleGAN; Image transfer; SAR Images; Transfer learning; Radar imaging","CycleGAN; Deep learning; Generative adversarial networks; Image transfer between optical; SAR images; Transfer learning","Conference paper","Final","","Scopus","2-s2.0-85064147595"
"Xu F.; Zhang R.; Yang W.; Xia G.-S.","Xu, Fang (57219687686); Zhang, Ruixiang (57226816818); Yang, Wen (57155382600); Xia, Gui-Song (12781686200)","57219687686; 57226816818; 57155382600; 12781686200","Mental retrieval of large-scale satellite images via learned sketch-image deep features","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-January","","8900605","3356","3359","3","10.1109/IGARSS.2019.8900605","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095492285&doi=10.1109%2fIGARSS.2019.8900605&partnerID=40&md5=2f4b8cbcea53f3e86b474b5fa6a2b496","Searching targets of interest in large-scale satellite images is an imperative task, which becomes a challenging issue when the targets reside only in the mind of the user as a set of subjective visual patterns. In this paper, we take the advantage of hand-drawn sketches' strong intuition of describing mental target to address the problem of no available exemplar query. We introduce a multi-level-of-detail model to learn a cross-domain representation for bridging the gap between sketches and satellite images. To train the model, we propose a novel method of generating satellite images with corresponding level of details based on generative adversarial network. Experiments on both large-scale satellite images and commonly used RS datasets demonstrate the effectiveness and superiority of our method.  © 2019 IEEE.","Drawing (graphics); Large dataset; Remote sensing; Adversarial networks; Cross-domain; Hand-drawn sketches; Large-scale satellites; Level of detail; Satellite images; Targets of interest; Visual pattern; Satellites","Mental search; Multi-level-of-detail model; Satellite images; Sketch","Conference paper","Final","","Scopus","2-s2.0-85095492285"
"Kniaz V.V.","Kniaz, Vladimir V. (56540022100)","56540022100","Conditional GANs for semantic segmentation of multispectral satellite images","2018","Proceedings of SPIE - The International Society for Optical Engineering","10789","","107890R","","","","10.1117/12.2325601","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059043296&doi=10.1117%2f12.2325601&partnerID=40&md5=f0cd61e201bca2a5efca5a36ead2a75f","Algorithms for automatic semantic segmentation of the satellite images provide an effective approach for the generation of vector maps. Convolutional neural networks (CNN) have achieved the state-of-the-art quality of the output segmentation on the satellite images-to-semantic labels task. However, the generalization ability of such methods is not sufficient to process the satellite images that were captured in the different area or during the different season. Recently, the Generative Adversarial Networks (GAN) were introduced that can overcome the overfitting using the adversarial loss. This paper is focused on the development of the new GAN model for effective semantic segmentation of multispectral satellite images. The pix2pix1 model is used as the starting point of the research. It is trained in the semi-supervised setting on the aligned pairs of images. The perceptual validation has demonstrated the high quality of the output labels. The evaluation on the independent test dataset has proved the robustness of GANs on the task of semantic segmentation of multispectral satellite images. © 2018 SPIE.","Convolution; Neural networks; Remote sensing; Satellites; Semantics; Statistical tests; Adversarial networks; Convolutional neural network; Convolutional Neural Networks (CNN); Effective approaches; Generalization ability; Multispectral satellite image; Semantic segmentation; State of the art; Image segmentation","Convolutional neural networks; Generative adversarial networks; Multispectral satellite images; Remote sensing; Semantic segmentation","Conference paper","Final","","Scopus","2-s2.0-85059043296"
"Ui Hoque M.R.; Burks R.; Kwan C.; Li J.","Ui Hoque, Md Reshad (57215223328); Burks, Roland (57215221542); Kwan, Chiman (7201421216); Li, Jiang (56226550100)","57215223328; 57215221542; 7201421216; 56226550100","Deep Learning for Remote Sensing Image Super-Resolution","2019","2019 IEEE 10th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2019","","","8993047","0286","0292","6","10.1109/UEMCON47517.2019.8993047","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080145619&doi=10.1109%2fUEMCON47517.2019.8993047&partnerID=40&md5=736354c0ac5bc10838c078f6109ef3a9","The aim of image super-Resolution (SR) is to enhance image resolution while still retain the integrity of the original image. There are many ongoing types of research on image super-resolution for natural images, but any a few on remote sensing images. In this paper, we proposed deep learning-based image super-resolution techniques, including convolutional neural network (CNN) and generative adversarial network (GAN) to enhance the resolution of remote sensing images by a factor 4. In CNN, it learns an end to end mapping from low-resolution image to high-resolution image whereas, in GAN, the model learns the mapping guided by the GAN loss and gives the sharper appearance in high-resolution images. Our experimental results show that visually GAN models perform well but are inferior to other models in terms of image quality metrics, whereas quantitatively CNN models outperform other super-resolution models. © 2019 IEEE.","Convolutional neural networks; Image enhancement; Image resolution; Learning systems; Mapping; Mobile telecommunication systems; Optical resolving power; Remote sensing; Ubiquitous computing; Adversarial networks; High resolution image; Image quality metrics; Image super resolutions; Low resolution images; Remote sensing images; Super resolution; Super-resolution models; Deep learning","CNN; Deep Learning; GAN; Machine Learning; Remote sensing image; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85080145619"
"Adriano B.; Yokoya N.; Xia J.; Baier G.; Koshimura S.","Adriano, Bruno (55613844500); Yokoya, Naoto (36440631200); Xia, Junshi (35099662000); Baier, Gerald (57188720676); Koshimura, Shunichi (6701566049)","55613844500; 36440631200; 35099662000; 57188720676; 6701566049","Cross-Domain-Classification of Tsunami Damage Via Data Simulation and Residual-Network-Derived Features from Multi-Source Images","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899155","4947","4950","3","10.1109/IGARSS.2019.8899155","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077693062&doi=10.1109%2fIGARSS.2019.8899155&partnerID=40&md5=478436287b8ed494d247b123a879fd8d","This paper presents a novel application of remote sensing data and machine learning technologies for damage classification in a real-world cross-domain application. The proposed methodology trains models to learn the building damage characteristics recorded in the 2011 Tohoku Tsunami from multi-sensor and multi-temporal remote sensing images. Then, the trained models are tested in the recent 2018 Sulawesi Tsunami. Additionally, a simulation of high-resolution SAR image was carried to deal with missing data modality. Our initial results show that the ResNet-derived features from optical images acquired after the disaster together with moderate- and high-resolution synthetic aperture radar (SAR) post-event intensity data showed significant accuracy in classifying two levels of tsunami-induced damage, with an average f-score of approximately 0.72. Taking into account that no training data from the 2018 Sulawesi Tsunami was used, our methodology shows excellent potential for future implementation of a rapid response system based on a database of building damage constructed from previous majors disasters. © 2019 IEEE.","Chemical sensors; Disasters; Geology; Geometrical optics; Image classification; Radar imaging; Remote sensing; Synthetic aperture radar; Tsunamis; Adversarial networks; Cross-domain; Damage classification; High resolution synthetic aperture radar; High-resolution SAR; Induced damage; Machine learning technology; Multi-temporal remote sensing; Classification (of information)","Conditional generative adversarial network; cross-domain classification; residual networks; tsunami-induced damage.","Conference paper","Final","","Scopus","2-s2.0-85077693062"
"Yu Y.; Li X.; Liu F.","Yu, Yunlong (57188735471); Li, Xianzhi (57192496554); Liu, Fuxian (55553732044)","57188735471; 57192496554; 55553732044","Attention GANs: Unsupervised Deep Feature Learning for Aerial Scene Classification","2020","IEEE Transactions on Geoscience and Remote Sensing","58","1","8842616","519","531","12","10.1109/TGRS.2019.2937830","61","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077965736&doi=10.1109%2fTGRS.2019.2937830&partnerID=40&md5=9ca054135d0ad556c8b7f4aaf3a85bf2","With the development of deep learning, supervised feature learning methods have achieved prominent performance in the field of aerial scene classification. However, supervised feature learning methods require a large amount of labeled training data. To address this limitation, in this article, a novel unsupervised deep feature learning method, namely, Attention generative adversarial networks (Attention GANs), is proposed for aerial scene classification. First, Attention GANs integrates the attention mechanism into GANs to enhance the representation power of the discriminator. Then, to obtain contextual information, a context-aggregation-based feature fusion architecture is designed in the discriminator. Furthermore, the generator and discriminator losses are improved on basis of the Relativistic GAN. At the same time, a content loss is formed by using the feature representations from the context-aggregation-based feature fusion architecture. In the experiments, our Attention GANs is evaluated via comprehensive experiments with four publicly available remote sensing scene data sets, i.e., the UC-Merced data set with 21 scene classes, the RSSCN7 data set with 7 scene classes, the AID data set with 30 scene classes, and the NWPU-RESISC45 data set with 45 scene classes. Experimental results demonstrate that our Attention GANs can obtain the best performance compared with the state-of-the-art methods. © 2019 IEEE.","Antennas; Classification (of information); Machine learning; Network architecture; Remote sensing; Adversarial networks; Attention mechanisms; Contextual information; Deep feature learning; Feature representation; Labeled training data; Scene classification; State-of-the-art methods; algorithm; artificial neural network; data set; image classification; remote sensing; satellite data; Deep learning","Aerial scene classification; attention mechanism; context aggregation; generative adversarial networks (GANs); unsupervised deep feature learning","Article","Final","","Scopus","2-s2.0-85077965736"
"Suárez P.L.; Sappa A.D.; Vintimilla B.X.","Suárez, Patricia L. (57194765069); Sappa, Angel D. (6603014642); Vintimilla, Boris X. (6507077933)","57194765069; 6603014642; 6507077933","Learning to colorize infrared images","2017","Advances in Intelligent Systems and Computing","619","","","164","172","8","10.1007/978-3-319-61578-3_16","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026888454&doi=10.1007%2f978-3-319-61578-3_16&partnerID=40&md5=0c142c94c4590f99cdacbb8d15126435","This paper focuses on near infrared (NIR) image colorization by using a Generative Adversarial Network (GAN) architecture model. The proposed architecture consists of two stages. Firstly, it learns to colorize the given input, resulting in a RGB image. Then, in the second stage, a discriminative model is used to estimate the probability that the generated image came from the training dataset, rather than the image automatically generated. The proposed model starts the learning process from scratch, because our set of images is very different from the dataset used in existing pre-trained models, so transfer learning strategies cannot be used. Infrared image colorization is an important problem when human perception need to be considered, e.g., in remote sensing applications. Experimental results with a large set of real images are provided showing the validity of the proposed approach. © Springer International Publishing AG 2018.","Infrared devices; Infrared imaging; Learning systems; Network architecture; Remote sensing; Adversarial networks; Architecture modeling; Automatically generated; Discriminative models; Image colorizations; Multispectral imaging; Proposed architectures; Remote sensing applications; Multi agent systems","CNN in multispectral imaging; Image colorization","Conference paper","Final","","Scopus","2-s2.0-85026888454"
"Zhang Y.; Sun H.; Zuo J.; Wang H.; Xu G.; Sun X.","Zhang, Yuhang (57194837012); Sun, Hao (36018820000); Zuo, Jiawei (57194830641); Wang, Hongqi (55689018000); Xu, Guangluan (56420820800); Sun, Xian (34875643000)","57194837012; 36018820000; 57194830641; 55689018000; 56420820800; 34875643000","Aircraft type recognition in remote sensing images based on feature learning with conditional generative adversarial networks","2018","Remote Sensing","10","7","1123","","","","10.3390/rs10071123","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050462362&doi=10.3390%2frs10071123&partnerID=40&md5=cb7314439db7c09e4a2c52e34aba21af","Aircraft type recognition plays an important role in remote sensing image interpretation. Traditional methods suffer from bad generalization performance, while deep learning methods require large amounts of data with type labels, which are quite expensive and time-consuming to obtain. To overcome the aforementioned problems, in this paper, we propose an aircraft type recognition framework based on conditional generative adversarial networks (GANs). First, we design a new method to precisely detect aircrafts' keypoints, which are used to generate aircraft masks and locate the positions of the aircrafts. Second, a conditional GAN with a region of interest (ROI)-weighted loss function is trained on unlabeled aircraft images and their corresponding masks. Third, an ROI feature extraction method is carefully designed to extract multi-scale features from the GAN in the regions of aircrafts. After that, a linear support vector machine (SVM) classifier is adopted to classify each sample using their features. Benefiting from the GAN, we can learn features which are strong enough to represent aircrafts based on a large unlabeled dataset. Additionally, the ROI-weighted loss function and the ROI feature extraction method make the features more related to the aircrafts rather than the background, which improves the quality of features and increases the recognition accuracy significantly. Thorough experiments were conducted on a challenging dataset, and the results prove the effectiveness of the proposed aircraft type recognition framework. © 2018 by the authors.","Deep learning; Extraction; Feature extraction; Image segmentation; Neural networks; Remote sensing; Support vector machines; Adversarial networks; Aircraft type recognition; Convolutional neural network; Feature extraction methods; Generalization performance; Linear Support Vector Machines; Remote sensing image interpretations; Weighted loss function; Fighter aircraft","Aircraft type recognition; Convolutional neural networks; Generative adversarial networks","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85050462362"
"Xu C.; Zhao B.","Xu, Chunxue (57200796997); Zhao, Bo (57192668697)","57200796997; 57192668697","Satellite image spoofing: Creating remote sensing dataset with generative adversarial networks","2018","Leibniz International Proceedings in Informatics, LIPIcs","114","","","","","","10.4230/LIPIcs.GIScience.2018.67","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051359490&doi=10.4230%2fLIPIcs.GIScience.2018.67&partnerID=40&md5=73d08a83cff8caabec976d31d4f8f08c","The rise of Artificial Intelligence (AI) has brought up both opportunities and challenges for today's evolving GIScience. Its ability in image classification, object detection and feature extraction has been frequently praised. However, it may also apply for falsifying geospatial data. To demonstrate the thrilling power of AI, this research explored the potentials of deep learning algorithms in capturing geographic features and creating fake satellite images according to the learned 'sense'. Specifically, Generative Adversarial Networks (GANs) is used to capture geographic features of a certain place from a group of web maps and satellite images, and transfer the features to another place. Corvallis is selected as the study area, and fake datasets with 'learned' style from three big cities (i.e. New York City, Seattle and Beijing) are generated through CycleGAN. The empirical results show that GANs can 'remember' a certain 'sense of place' and further apply that 'sense' to another place. With this paper, we would like to raise both public and GIScientists' awareness in the potential occurrence of fake satellite images, and its impacts on various geospatial applications, such as environmental monitoring, urban planning, and land use development. © Chun X. Xu and Bo Zhao.","Deep learning; Geographic information systems; Land use; Learning algorithms; Object detection; Remote sensing; Satellites; Adversarial networks; Environmental Monitoring; GANs; Geo-spatial data; Geographic feature; Geospatial applications; Satellite images; Sense of place; Feature extraction","Deep learning and AI; Fake satellite image; GANs; Geographic feature","Conference paper","Final","","Scopus","2-s2.0-85051359490"
