"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Yu H.; Wang S.; Fan Y.; Wang G.; Li J.; Liu C.; Li Z.; Sun J.","Yu, Hui (57829119300); Wang, Shuo (57205322753); Fan, Yinuo (57780830500); Wang, Guangpu (57735724000); Li, Jinqiu (57942526000); Liu, Chong (57941663000); Li, Zhigang (57942103400); Sun, Jinglai (57207254334)","57829119300; 57205322753; 57780830500; 57735724000; 57942526000; 57941663000; 57942103400; 57207254334","Large-factor Micro-CT super-resolution of bone microstructure","2022","Frontiers in Physics","10","","997582","","","","10.3389/fphy.2022.997582","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140628004&doi=10.3389%2ffphy.2022.997582&partnerID=40&md5=e01b21534fa23676a971504bde8087fc","Background: Bone microstructure is important for evaluating bone strength and requires the support of high-resolution (HR) imaging equipment. Computed tomography (CT) is widely used for medical imaging, but the spatial resolution is not sufficient for bone microstructure. Micro-CT scan data is the gold standard for human bone microstructure or animal experiment. However, Micro-CT has more ionizing radiation and longer scanning time while providing high-quality imaging. It makes sense to reconstruct HR images with less radiation. Image super-resolution (SR) is adapted to the above-mentioned research. The specific objective of this study is to reconstruct HR images of bone microstructure based on low-resolution (LR) images under large-factor condition. Methods: We propose a generative adversarial network (GAN) based on Res2Net and residual channel attention network which is named R2-RCANGAN. We use real high-resolution and low-resolution training data to make the model learn the image corruption of Micro-CT, and we train six super-resolution models such as super-resolution convolutional neural network to evaluate our method performance. Results: In terms of peak signal-to-noise ratio (PSNR), our proposed generator network R2-RCAN sets a new state of the art. Such PSNR-oriented methods have high reconstruction accuracy, but the perceptual index to evaluate perceptual quality is very poor. Thus, we combine the generator network R2-RCAN with the U-Net discriminator and loss function with adjusted weights, and the proposed R2-RCANGAN shows the pleasing results in reconstruction accuracy and perceptual quality as compared to the other methods. Conclusion: The proposed R2-RCANGAN is the first to apply large-factor SR to improve Micro-CT images of bone microstructure. The next steps of the study are to investigate the role of SR in image enhancement during fracture rehabilitation period, which would be of great value in reducing ionizing radiation and promoting recovery. Copyright © 2022 Yu, Wang, Fan, Wang, Li, Liu, Li and Sun.","","bone microstructure; general adversarial network; large-factor; Micro-CT (computed tomography); super-resolution (SR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140628004"
"Liu Y.; Zhang Q.; Zhang N.; Lv J.; Gong M.; Cao J.","Liu, Ye (56070051000); Zhang, Qidi (57766347800); Zhang, Nan (57213989292); Lv, Jintao (57765052000); Gong, Meichen (57219125338); Cao, Jie (57201825338)","56070051000; 57766347800; 57213989292; 57765052000; 57219125338; 57201825338","Enhancement of thin-section image using super-resolution method with application to the mineral segmentation and classification in tight sandstone reservoir","2022","Journal of Petroleum Science and Engineering","216","","110774","","","","10.1016/j.petrol.2022.110774","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132938019&doi=10.1016%2fj.petrol.2022.110774&partnerID=40&md5=80aef230a0816680053a76bd21153edd","The accurate characterization of rock and fluid properties in porous media of oil reservoir using thin sections depends on reliable segmentation and classification of the involved phases. However, in tight sandstone reservoirs, rock image segmentation and classification become a challenging task due to the limitation of resolution on representing the pores, throats, and other minerals at the microscale size. The resolution of an image has thus become a critical factor. Therefore, this study aims to use the super-resolution technique, which can enhance the image resolution by deep learning methods to overcome the limitation of original image resolution, and significantly improve the traditional segmentation and classification results. A Generative Adversarial Network (GAN)-based Super-resolution(SR) model was used as a pre-processing step to enhance the resolution of a given image. Then the effectiveness of super-resolution technique in post-processing procedures like segmentation and classification is evaluated using a tight sandstone data set from Ordos basin. The performance of segmentation with super-resolution enhancement is compared among Level set, Simple Linear Iterative Clustering (SLIC), and Watershed. As numerical test results reflect, segmentation in super-resolution image can achieve the segmentation of tiny minerals, pores, throats, and other blurry edges which can't be correctly segmented in the original image. Furthermore, in classification, we use a Convolutional Neural Network (CNN) model and a logistic regression model to demonstrate the advantage of SR enhancement. After the super-resolution process, the classification accuracy of CNN and logistic models have both improved for over 10%. The comparisons and analyses are presented to show that super-resolution could significantly improve these post-processing procedures. In addition, we discuss the potentials and limitations of applying super-resolution in segmentation and classification. © 2022 Elsevier B.V.","China; Ordos Basin; Classification (of information); Convolutional neural networks; Deep learning; Generative adversarial networks; Image classification; Image enhancement; Image segmentation; Iterative methods; Learning systems; Minerals; Petroleum reservoir engineering; Petroleum reservoirs; Porous materials; Sandstone; Convolutional neural network; Neural network model; Original images; Pore throat; Post-processing procedure; Resolution enhancement; Resolution techniques; Superresolution; Thin-sections; Tight sandstone reservoirs; classification; hydrocarbon reservoir; image resolution; imaging method; machine learning; mineralogy; sandstone; segmentation; Image resolution","","Article","Final","","Scopus","2-s2.0-85132938019"
"Batchuluun G.; Nam S.H.; Park C.; Park K.R.","Batchuluun, Ganbayar (57188649020); Nam, Se Hyun (57209210189); Park, Chanhum (57206697052); Park, Kang Ryoung (57735631000)","57188649020; 57209210189; 57206697052; 57735631000","Super-Resolution Reconstruction-Based Plant Image Classification Using Thermal and Visible-Light Images","2023","Mathematics","11","1","76","","","","10.3390/math11010076","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145934329&doi=10.3390%2fmath11010076&partnerID=40&md5=9ed8541815c5e9d3dbb420e56002e121","Few studies have been conducted on thermal plant images. This is because of the difficulty in extracting and analyzing various color-related patterns and features from the plant image obtained using a thermal camera, which does not provide color information. In addition, the thermal camera is sensitive to the surrounding temperature and humidity. However, the thermal camera enables the extraction of invisible patterns in the plant by providing external and internal heat information. Therefore, this study proposed a novel plant classification method based on both the thermal and visible-light plant images to exploit the strengths of both types of cameras. To the best of our knowledge, this study is the first to perform super-resolution reconstruction using visible-light and thermal plant images. Furthermore, a method to improve the classification performance through generative adversarial network (GAN)-based super-resolution reconstruction was proposed. Through the experiments using a self-collected dataset of thermal and visible-light images, our method shows higher accuracies than the state-of-the-art methods. © 2022 by the authors.","","classification; deep learning; plant image; super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145934329"
"","","","Society of Petroleum Engineers - SPE Reservoir Characterisation and Simulation Conference and Exhibition 2023, RCSC 2023","2023","Society of Petroleum Engineers - SPE Reservoir Characterisation and Simulation Conference and Exhibition 2023, RCSC 2023","","","","","","1500","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147493533&partnerID=40&md5=3729811f31da9f09dba3f60ee84aed2b","The proceedings contain 102 papers. The topics discussed include: novel approach for automated pore network characterization and machine learning assisted capillary pressure modeling for an improved water saturation modelling in carbonate reservoirs; permeability prediction of carbonate cores with gaussian process regression model; application of machine learning for estimating petrophysical properties of carbonate rocks using NMR core measurements; real time artificial intelligence assisted operations geology: lithology and fluids early identification and automatic well correlation; machine learning assisted flash calculation for sour gas and crude oil; super-resolution reconstruction of reservoir saturation map with physical constraints using generative adversarial network; deriving new type curves through machine learning in the Wolfcamp formation; a simulation model with unstructured grid provides better predictions of water movement in heavily fractured reservoir – a case study at onshore Abu Dhabi; and effective modeling of stimulation and production decline from tight naturally fractured carbonate reservoirs.","","","Conference review","Final","","Scopus","2-s2.0-85147493533"
"Sun Z.; Ng C.K.C.","Sun, Zhonghua (12544503300); Ng, Curtise K. C. (26030030100)","12544503300; 26030030100","Artificial Intelligence (Enhanced Super-Resolution Generative Adversarial Network) for Calcium Deblooming in Coronary Computed Tomography Angiography: A Feasibility Study","2022","Diagnostics","12","4","991","","","","10.3390/diagnostics12040991","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128992160&doi=10.3390%2fdiagnostics12040991&partnerID=40&md5=d818d3e480ba307908bef039dc834e2c","Background: The presence of heavy calcification in the coronary artery always presents a challenge for coronary computed tomography angiography (CCTA) in assessing the degree of coronary stenosis due to blooming artifacts associated with calcified plaques. Our study purpose was to use an advanced artificial intelligence (enhanced super-resolution generative adversarial network [ESRGAN]) model to suppress the blooming artifact in CCTA and determine its effect on improving the diagnostic performance of CCTA in calcified plaques. Methods: A total of 184 calcified plaques from 50 patients who underwent both CCTA and invasive coronary angiography (ICA) were analysed with measurements of coronary lumen on the original CCTA, and three sets of ESRGAN-processed images including ESRGAN-high-resolution (ESRGAN-HR), ESRGAN-average and ESRGAN-median with ICA as the reference method for determining sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV). Results: ESRGAN-processed images improved the specificity and PPV at all three coronary arteries (LAD-left anterior descending, LCx-left circumflex and RCA-right coronary artery) compared to original CCTA with ESRGAN-median resulting in the highest values being 41.0% (95% confidence interval [CI]: 30%, 52.7%) and 26.9% (95% CI: 22.9%, 31.4%) at LAD; 41.7% (95% CI: 22.1%, 63.4%) and 36.4% (95% CI: 28.9%, 44.5%) at LCx; 55% (95% CI: 38.5%, 70.7%) and 47.1% (95% CI: 38.7%, 55.6%) at RCA; while corresponding values for original CCTA were 21.8% (95% CI: 13.2%, 32.6%) and 22.8% (95% CI: 20.8%, 24.9%); 12.5% (95% CI: 2.6%, 32.4%) and 27.6% (95% CI: 24.7%, 30.7%); 17.5% (95% CI: 7.3%, 32.8%) and 32.7% (95% CI: 29.6%, 35.9%) at LAD, LCx and RCA, respectively. There was no significant effect on sensitivity and NPV between the original CCTA and ESRGAN-processed images at all three coronary arteries. The area under the receiver operating characteristic curve was the highest with ESRGAN-median images at the RCA level with values being 0.76 (95% CI: 0.64, 0.89), 0.81 (95% CI: 0.69, 0.93), 0.82 (95% CI: 0.71, 0.94) and 0.86 (95% CI: 0.76, 0.96) corresponding to original CCTA and ESRGAN-HR, average and median images, respectively. Conclusions: This feasibility study shows the potential value of ESRGAN-processed images in improving the diagnostic value of CCTA for patients with calcified plaques. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","calcium; adult; Article; artificial intelligence; computed tomographic angiography; controlled study; coronary artery; coronary artery obstruction; diagnostic test accuracy study; diagnostic value; feasibility study; female; human; image artifact; major clinical study; male; middle aged; predictive value; sensitivity and specificity","assessment; calcification; cardiac computed tomography; coronary artery disease; deep learning; model","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85128992160"
"Wang Y.; Bashir S.M.A.; Khan M.; Ullah Q.; Wang R.; Song Y.; Guo Z.; Niu Y.","Wang, Yi (12763268500); Bashir, Syed Muhammad Arsalan (56385198200); Khan, Mahrukh (57197808732); Ullah, Qudrat (57212812840); Wang, Rui (57226734727); Song, Yilin (57339519000); Guo, Zhe (36026786800); Niu, Yilong (18234173100)","12763268500; 56385198200; 57197808732; 57212812840; 57226734727; 57339519000; 36026786800; 18234173100","Remote sensing image super-resolution and object detection: Benchmark and state of the art","2022","Expert Systems with Applications","197","","116793","","","","10.1016/j.eswa.2022.116793","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125859223&doi=10.1016%2fj.eswa.2022.116793&partnerID=40&md5=151a05325bb6827420a29e481c57df4f","For the past two decades, there have been significant efforts to develop methods for object detection in Remote Sensing (RS) images. In most cases, the datasets for small object detection in remote sensing images are inadequate. Many researchers used scene classification datasets for object detection, which has its limitations; for example, the large-sized objects outnumber the small objects in object categories. Thus, they lack diversity; this further affects the detection performance of small object detectors in RS images. This paper reviews current datasets and object detection methods (deep learning-based) for remote sensing images. We also propose a large-scale, publicly available benchmark Remote Sensing Super-resolution Object Detection (RSSOD) dataset. The RSSOD dataset consists of 1,759 hand-annotated images with 22,091 instances of very high-resolution (VHR) images with a spatial resolution of ∼ 0.05 m. There are five classes with varying frequencies of labels per class; the images are annotated in You Only Look Once (YOLO) and Common Objects in Context (COCO) format. The image patches are extracted from satellite images, including real image distortions such as tangential scale distortion and skew distortion. The proposed RSSOD dataset will help researchers benchmark the state-of-the-art object detection methods across various classes, especially for small objects using image super-resolution. We also propose a novel Multi-class Cyclic super-resolution Generative adversarial network with Residual feature aggregation (MCGR) and auxiliary YOLOv5 detector to benchmark image super-resolution-based object detection and compare with the existing state-of-the-art methods based on image super-resolution (SR). The proposed MCGR achieved state-of-the-art performance for image SR with an improvement of 1.2 dB in peak signal-to-noise ratio (PSNR) compared to the current state-of-the-art non-local sparse network (NLSN). MCGR achieved best object detection mean average precisions (mAPs) of 0.758, 0.881, 0.841, and 0.983, respectively, for five-class, four-class, two-class, and single classes, respectively surpassing the performance of the state-of-the-art object detectors YOLOv5, EfficientDet, Faster RCNN, SSD, and RetinaNet. © 2022 Elsevier Ltd","Benchmarking; Classification (of information); Deep learning; Generative adversarial networks; Image enhancement; Image resolution; Image segmentation; Large dataset; Object recognition; Remote sensing; Signal to noise ratio; Deep learning object detection; Image super resolutions; MCGR; Multiclass GAN; Object detection in remote sensing; Remote sensing benchmark; Remote sensing images; Remote-sensing; Small object detection; Object detection","Deep learning object detection; MCGR; multiclass GAN; Object detection in remote sensing; Remote sensing benchmark; Small object detection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125859223"
"Dahari A.; Kench S.; Squires I.; Cooper S.J.","Dahari, Amir (57221324548); Kench, Steve (57219948420); Squires, Isaac (57322239000); Cooper, Samuel J. (56595734000)","57221324548; 57219948420; 57322239000; 56595734000","Fusion of Complementary 2D and 3D Mesostructural Datasets Using Generative Adversarial Networks","2023","Advanced Energy Materials","13","2","2202407","","","","10.1002/aenm.202202407","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142252864&doi=10.1002%2faenm.202202407&partnerID=40&md5=a84713a46dc826b2263a8b2123f10111","Modelling the impact of a material's mesostructure on device level performance typically requires access to 3D image data containing all the relevant information to define the geometry of the simulation domain. This image data must include sufficient contrast between phases, be of high enough resolution to capture the key details, but also have a large enough 3D field-of-view to be representative of the material in general. It is rarely possible to obtain data with all of these properties from a single imaging technique. In this paper, we present a method for combining information from pairs of distinct but complementary imaging techniques in order to accurately reconstruct the desired multi-phase, high-resolution, representative, 3D images. Specifically, the deep convolutional generative adversarial networks to implement super-resolution, style-transfer and dimensionality expansion. It is believed that this data-driven approach is superior to previously reported statistical material reconstruction methods, both in terms of its fidelity and ease of use. Furthermore, much of the data required to train this algorithm already exists in the literature, waiting to be combined. As such, our open-source code could precipitate a step change in the materials sciences by generating the desired high quality image volumes necessary to simulate behaviour at the mesoscale. © 2022 The Authors. Advanced Energy Materials published by Wiley-VCH GmbH.","Open source software; Open systems; Optical resolving power; 3D image data; Field of views; High resolution; Machine-learning; Mesostructures; Performance; Property; Simulation domain; Superresolution; Generative adversarial networks","electrodes; imaging; machine learning; microstructures; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85142252864"
"Shim S.; Kim J.; Lee S.-W.; Cho G.-C.","Shim, Seungbo (57208105856); Kim, Jin (57222477366); Lee, Seong-Won (57223720059); Cho, Gye-Chun (7201438162)","57208105856; 57222477366; 57223720059; 7201438162","Road damage detection using super-resolution and semi-supervised learning with generative adversarial network","2022","Automation in Construction","135","","104139","","","","10.1016/j.autcon.2022.104139","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123032890&doi=10.1016%2fj.autcon.2022.104139&partnerID=40&md5=2203115326c559bd8b17d90c24e8ff54","Road maintenance technology is required to maintain favorable driving conditions and prevent accidents. In particular, a sensor technology is required for detecting road damage. In this study, we developed a new sensor technology that can detect road damage using a deep learning-based image processing algorithm. The proposed technology includes a super-resolution and semi-supervised learning method based on a generative adversarial network. The former improves the quality of the road image to make the damaged area clearly visible. The latter enhances the detection performance using 5327 road images and 1327 label images. These two methods were applied to four lightweight segmentation neural networks. For 400 road images, the average recognition performance was 81.540% and 79.228% in terms of the mean intersection over union and F1-score, respectively. Consequently, the proposed training method improves the road damage detection algorithm and can be used for efficient road management in the future. © 2022 The Authors","Damage detection; Deep learning; Generative adversarial networks; Highway planning; Image enhancement; Learning algorithms; Motor transportation; Roads and streets; Supervised learning; Driving conditions; GAN; Images processing; Maintenance technologies; Road damage; Road damage detection; Road images; Road maintenance; Sensor technologies; Superresolution; Optical resolving power","GAN; Image processing; Road damage detection; Semi-supervised learning; Super resolution","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123032890"
"Huang Z.; Zhu Z.; Ma W.; Fang H.; Zhang Y.","Huang, Zhenghua (57104017400); Zhu, Zifan (57224625953); Ma, Wanhao (57660885300); Fang, Hao (51664871900); Zhang, Yaozong (56997595500)","57104017400; 57224625953; 57660885300; 51664871900; 56997595500","B2GAN: Bidirectional-branch generative adversarial network for text image super-resolution with structure preservation","2022","Optik","261","","169093","","","","10.1016/j.ijleo.2022.169093","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129324406&doi=10.1016%2fj.ijleo.2022.169093&partnerID=40&md5=d4dac9ddb18bd3387970ae69101c1539","Text image super-resolution (SR) is one of the most hot topic research in computer vision. The traditional convolutional neural networks usually over-smooth fine structures of SR images while the popular generative adversarial network (GAN) based SR methods may produce undesirable SR results with distorted edges. To cope with these problems, this paper proposes a bidirectional-branch generative adversarial network (B2GAN) for text image SR. The proposed B2GAN consists of two parts: one is a generator, the other is a discriminator. In the generator, a forward branch is used for mapping the low-resolution (LR) image to its corresponding SR image to generate photo-realistic images, while a reverse branch is responsible for mapping the generated SR image to a low-resolution image which is viewed as its original LR image. A loss function is proposed to train a pleasing generator and guarantee a stable convergence to the original LR image. In the discriminator, The VGG16 and the adversarial loss are employed. In the experiments, we establish two text image datasets including Chinese and multi-language characters which are used to test the effectiveness of the proposed B2GAN method. Quantitative associated with qualitative experimental results validate that the proposed B2GAN method can achieve competitive performance and even outperform the state-of-the-arts. © 2022 Elsevier GmbH","Convolution; Convolutional neural networks; Mapping; Optical resolving power; Bidirectional-branch generative adversarial network (B2generative adversarial network); Convolutional neural network; Image super resolutions; Low resolution images; Network methods; Resolution images; Structure preservation; Superresolution; Text image super-resolution; Text images; Generative adversarial networks","Bidirectional-branch generative adversarial network (B<sup>2</sup>GAN); Convolutional neural networks; Structure preservation; Text image super-resolution","Article","Final","","Scopus","2-s2.0-85129324406"
"Xiao G.; Zhang L.","Xiao, Guangyi (57328144900); Zhang, Long (57222227721)","57328144900; 57222227721","Super-resolved synthetic aperture radar image reconstruction based on multiresolution fusion discrimination","2022","Journal of Electronic Imaging","31","4","043036","","","","10.1117/1.JEI.31.4.043036","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142247391&doi=10.1117%2f1.JEI.31.4.043036&partnerID=40&md5=dfa7f2de212cf7f1070bdf169a0185bd","Generative adversarial networks (GANs) are utilized for synthetic aperture radar (SAR) image super-resolution reconstruction, affording realistic texture details. However, existing GANs only discriminate the final generated high-resolution (HR) image after two consecutive upsampling processes, which ignore some high-frequency information of the reconstructed images. To resolve this issue, a multiresolution fusion discrimination (MRFD) algorithm is proposed to discriminate the reconstructed feature maps after each upsampling. First, a multiresolution discrimination process discriminates the authenticity of each upsampled feature map separately, which reduces the image distortion imposed during two consecutive upsampling processes. Besides, multiresolution feature fusion further preserves the consistent high-frequency texture structures. Finally, a multiscale dense network extracts image features in different scales, with multiscale dense block's dense connections improving parameter utilization. The experimental results on a SAR dataset demonstrate that the proposed MRFD algorithm performs better in reconstructing the texture details of HR images.  © 2022 SPIE and IS&T.","Image enhancement; Image fusion; Image reconstruction; Image texture; Optical resolving power; Radar imaging; Signal sampling; Synthetic aperture radar; Textures; Discrimination algorithms; Feature map; High-resolution images; Image super-resolution reconstruction; Images reconstruction; Multiresolution fusion; Multiresolution fusion discrimination; Superresolution; Synthetic aperture radar images; Upsampling; Generative adversarial networks","generative adversarial network; multiresolution fusion discrimination; super-resolution; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85142247391"
"Sharma S.; Dhall A.; Kumar V.","Sharma, Shailza (57307338300); Dhall, Abhinav (35229206900); Kumar, Vinay (57224776041)","57307338300; 35229206900; 57224776041","Frequency aware face hallucination generative adversarial network with semantic structural constraint","2022","Computer Vision and Image Understanding","223","","103553","","","","10.1016/j.cviu.2022.103553","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138758524&doi=10.1016%2fj.cviu.2022.103553&partnerID=40&md5=a2ffce99b1b1f73495f0f9479c434d21","In this paper, we address the issue of face hallucination. Most current face hallucination methods rely on two-dimensional facial priors to generate high resolution face images from low resolution face images. These methods are only capable of assimilating global information into the generated image. Still there exist some inherent problems in these methods, such as, local features, subtle structural details and depth information are missing in final output image. This work proposes a generative adversarial network (GAN) based novel progressive face hallucination (FH) network to address these issues present among current methods. The generator of the proposed model comprises of FH network and two sub-networks, assisting FH network to generate high resolution images. The first sub-network leverages on explicitly adding high frequency components into the model. To explicitly encode the high frequency components, an auto encoder is proposed to generate high resolution coefficients of discrete cosine transform (DCT). To add three dimensional parametric information into the network, second sub-network is proposed. This network uses a shape model of 3D morphable models (3DMM) to add structural constraint to the FH network. Extensive experimentation evaluation show the usefulness of proposed architecture in the form of state-of-the-art quantitative results. © 2022 Elsevier Inc.","3D modeling; Discrete cosine transforms; Semantics; 'current; 3D Morphable model; Face hallucination; High frequency components; High resolution; Higher-frequency components; Structural constraints; Subnetworks; Superresolution; Two-dimensional; Generative adversarial networks","3D morphable models; Discrete cosine transform; Face hallucination; Generative adversarial networks; Super resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138758524"
"Kim J.; Choe Y.","Kim, Jaehun (58098572200); Choe, Yoonsik (7102998299)","58098572200; 7102998299","Document Image Restore via SPADE-Based Super-Resolution Network","2023","Electronics (Switzerland)","12","3","748","","","","10.3390/electronics12030748","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147824794&doi=10.3390%2felectronics12030748&partnerID=40&md5=abe41461bb4a520932767fb693f06744","With the development of deep learning technology, various structures and research methods for the super-resolution restoration of natural images and document images have been introduced. In particular, a number of recent studies have been conducted and developed in image restoration using generative adversarial networks. Super-resolution restoration is an ill-posed problem because of some complex restraints, such as many high-resolution images being restored for the same low-resolution image, as well as difficulty in restoring noises such as edges, light smudging, and blurring. In this study, we applied super-resolution restoration to text images using the spatially adaptive denormalization (SPADE) structure, different from previous methods. This paper used SPADE for document image restoration to solve previous problems such as edge unclearness, hardness to catch features of texts, and the image color transition. As a result of this study, it can be confirmed that the edge of the character and the ambiguous stroke are restored more clearly when contrasting with the other previously suggested methods. Additionally, the proposed method’s PSNR and SSIM scores are 8% and 15% higher compared to the previous methods. © 2023 by the authors.","","convolutional neural network; generative adversarial network; spatially adaptive denormalization (SPADE); super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85147824794"
"Liu H.; Ren W.; Wang R.; Cao X.","Liu, Huacheng (57698363700); Ren, Wenqi (56528850100); Wang, Rui (55717793300); Cao, Xiaochun (8920951000)","57698363700; 56528850100; 55717793300; 8920951000","A super-resolution Transformer fusion network for single blurred image; [用于单幅模糊图像超分辨的Transformer融合网络]","2022","Journal of Image and Graphics","27","5","","1616","1631","15","10.11834/jig.210847","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130218022&doi=10.11834%2fjig.210847&partnerID=40&md5=e5612baa2b25df4aa4d7d0b16f05b35c","Objective: Single image super-resolution is an essential task for vision applications to enhance the spatial resolution based image quality in the context of computer vision. Deep learning based methods are beneficial to single image super-resolution nowadays. Low-resolution images are regarded as clear images without blur effects. However, low-resolution images in real scenes are constrained of blur artifacts factors like camera shake and object motion. The degradation derived blur artifacts could be amplified in the super-resolution reconstruction process. Hence, our research focus on the single image super-resolution task to resolve motion blurred issue. Method: Our Transformer fusion network (TFN) can be handle super-resolution reconstruction of low-resolution blurred images for super-resolution reconstruction of blurred images. Our TFN method implements a dual-branch strategy to remove some blurring regions based on super-resolution reconstruction of blurry images. First, we facilitate a deblurring module (DM) to extract deblurring features like clear edge structures. Specifically, we use the encoder-decoder architecture to design our DM module. For the encoder part of DM module, we use three convolutional layers to decrease the spatial resolution of feature maps and increase the channels of feature maps. For the decoder part of DM module, we use two de-convolutional layers to increase the spatial resolution of feature maps and decrease the channels of feature maps. In terms of the supervision of L1 deblurring loss function, the DM module is used to generate the clear feature maps in related to the down-sampling and up-sampling process of the DM module. But, our DM module tends to some detailed information loss of input images due to detailed information removal with the blur artifacts. Then, we designate additional texture feature extraction module (TFEM) to extract detailed texture features. The TFEM module is composed of six residual blocks, which can resolve some gradient explosion issues and speed up convergence. Apparently, the TFEM does not have down-sampling and up-sampling process like DM module, so TFEM can extract more detailed texture features than DM although this features has some blur artifacts. In order to take advantage of both clear deblurring features extracted by DM module and the detailed features extracted by TFEM module, we make use of a Transformer fusion module (TFM) to fuse them. We can use the clear deblurring features and detailed features in TFM module. We customize the multi-head attention layer to design the TFM module. Because the input of the transformer encoder part is one dimensional vector, we use flatten and unflatten operations in the TFM module. In addition, we can use the TFM module to fuse deblurring features extracted by the DM module and detailed texture features extracted by the TFEM module more effectively in the global sematic level based on long-range and global dependencies multi-head attention capturing ability. Finally, we use reconstruction module (RM) to carry out super-resolution reconstruction based on the fusion features obtained to generate a better super-resolved image. Result: The extensive experiments demonstrate that our method generates sharper super-resolved images based on low-resolution blurred input images. We compare the proposed TFN to several algorithms, including the tailored single image super-resolution methods, the joint image deblurring and image super-resolution approaches, the combinations of image super-resolution algorithms and non-uniform deblurring algorithms. Specially, the single image super-resolution methods are based on the residual channel attention network(RCAN) and holistic attention network(HAN) algorithms, the image deblurring methods are melted scale-recurrent network(SRN) and deblur generative adversarial network(DBGAN) in, and the joint image deblurring and image super-resolution approaches are integrated the gated fusion network(GFN). To further evaluate the proposed TFN, we conduct experiments on two test data sets, including GOPRO test dataset and Kohler dataset. For GOPRO test dataset, the peak signal-to-noise ratio(PSNR) value of our TFN based super-resolved results by is 0.12 dB, 0.18 dB, and 0.07 dB higher than the very recent work of GFN for the 2×, 4× and 8× scales, respectively. For Kohler dataset, the PSNR value of our TFN based super-resolved results is 0.17 dB, 0.28 dB, and 0.16 dB in the 2×, 4× and 8× scales, respectively. In addition, the PSNR value of model with DM model result is 1.04 dB higher than model with TFEM in ablation study. the PSNR value of model with DM and TFME module is 1.84 dB and 0.80 dB higher than model with TFEM, and model with DM respectively. The PSNR value of TFN model with TFEM, DM, and TFM, which is 2.28 dB, 1.24 dB, and 0.44 dB higher than model with TFEM, model with DM, and model with TFEM/DM, respectively. To sum up, the GOPRO dataset based ablation experiments illustrates that the TFM promote global semantic hierarchical feature fusion in the context of deblurring features and detailed texture features, which greatly improves the effect of the network on the super-resolution reconstruction of low-resolution blurred images. The GOPRO test dataset and Kohler dataset based experimental results illustrates our network has a certain improvement of visual results qualitatively quantitatively. Conclusion: We harnesses a Transformer fusion network for blurred image super-resolution. This network can super-resolve blurred image and remove blur artifacts, to fuse DB-module-extracted deblurring features by and TFEM-module-extracted texture features via a transformer fusion module. In the transformer fusion module, we uses the multi-head self-attention layer to calculate the response of local information of the feature map to global information, which can effectively fuse deblurring features and detailed texture features at the global semantic level and improves the effect of super-resolution reconstruction of blurred images. Extensive ablation experiments and comparative experiment demonstrate that our TFN demonstrations have its priority on the visual result quantity and quantitative ability. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Blurred images; Fusion network; Single image super-resolution; Super-resolution; Transformer","Article","Final","","Scopus","2-s2.0-85130218022"
"Güemes A.; Sanmiguel Vila C.; Discetti S.","Güemes, Alejandro (57209686139); Sanmiguel Vila, Carlos (57190061692); Discetti, Stefano (37123621500)","57209686139; 57190061692; 37123621500","Super-resolution generative adversarial networks of randomly-seeded fields","2022","Nature Machine Intelligence","4","12","","1165","1173","8","10.1038/s42256-022-00572-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143812619&doi=10.1038%2fs42256-022-00572-7&partnerID=40&md5=8c8f4890d0c3a17877290d64fdc7aadd","Reconstruction of field quantities from sparse measurements is a problem arising in a broad spectrum of applications. This task is particularly challenging when the mapping between sparse measurements and field quantities is performed in an unsupervised manner. Further complexity is added for moving sensors and/or random on–off status. Under such conditions, the most straightforward solution is to interpolate the scattered data onto a regular grid. However, the spatial resolution achieved with this approach is ultimately limited by the mean spacing between the sparse measurements. In this work, we propose a super-resolution generative adversarial network framework to estimate field quantities from random sparse sensors. The algorithm exploits random sampling to provide incomplete views of the high-resolution underlying distributions. It is hereby referred to as the randomly seeded super-resolution generative adversarial network (RaSeedGAN). The proposed technique is tested on synthetic databases of fluid flow simulations, ocean surface temperature distribution measurements and particle-image velocimetry data of a zero-pressure-gradient turbulent boundary layer. The results show excellent performance even in cases with high sparsity or noise level. This generative adversarial network algorithm provides full-field high-resolution estimation from randomly seeded fields with no need of full-field high-resolution representations for training. © 2022, The Author(s), under exclusive licence to Springer Nature Limited.","Boundary layer flow; Generative adversarial networks; Optical resolving power; Turbulent flow; Velocity measurement; Broad spectrum; Condition; High resolution; Moving sensors; Network frameworks; Random sampling; Regular grids; Scattered data; Spatial resolution; Superresolution; Boundary layers","","Article","Final","","Scopus","2-s2.0-85143812619"
"Kapilaratne R.G.C.J.; Kakuta S.; Kaneta S.","Kapilaratne, R.G.C.J. (57194537565); Kakuta, S. (56536327700); Kaneta, S. (57219049570)","57194537565; 56536327700; 57219049570","Enhanced super resolution for remote sensing imageries","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","53","60","7","10.5194/isprs-Annals-V-3-2022-53-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132043853&doi=10.5194%2fisprs-Annals-V-3-2022-53-2022&partnerID=40&md5=e739cd48d213e5e5de51b63d8bcee7db","Single image super resolution (SISR) technology has been attracted much attention from remote sensing community due to its proven potentials in remote sensing applications. Existing SISR techniques varying from conventional interpolation methods to different network architectures. Generative adversarial networks (GANs) are one of the latest network architectures proven a greater potential as a SISR method whereas least attention has been given by the remote sensing community. Several studies have already been carried out on this context. However, yet there is no generalized GAN based approach to super resolve remote sensing imageries. Therefore, this study investigated the potentials of enhanced super resolution generative adversarial (ESRGAN) model to super resolve very high to medium resolution images from high to coarse resolution images for remote sensing applications. Two models were trained and Worldview-3 (WV3) images used as for very high resolution images. Whereas, down sampled WV3 and Sentinel-2(S2) were used as low resolution counterparts. Model performances were qualitatively and quantitatively analysed using standard metrics such as PSNR, SSIM, UIQI, CC, SAM, SID. Evaluation results emphasised super resolved images were preserved the original quality of the satellite images to a greater extent while improving its ground resolution.  © Authors 2022.","Deep learning; Generative adversarial networks; Image enhancement; Network architecture; Optical resolving power; Deep learning; ESRGAN; Image super resolutions; Remote sensing applications; Remote sensing imagery; Remote-sensing; Sentinel-2.; Single images; Superresolution; Worldview-3; Remote sensing","Deep Learning; ESRGAN; Remote Sensing; Sentinel-2.; Super Resolution; WorldView-3","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132043853"
"Shao D.; Qin L.; Xiang Y.; Ma L.; Xu H.","Shao, Dangguo (42662301600); Qin, Li (58085352300); Xiang, Yan (55836565000); Ma, Lei (56349331800); Xu, Hui (57678087800)","42662301600; 58085352300; 55836565000; 56349331800; 57678087800","Medical image blind super-resolution based on improved degradation process","2023","IET Image Processing","","","","","","","10.1049/ipr2.12742","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147167743&doi=10.1049%2fipr2.12742&partnerID=40&md5=44c8ce9a103d50f6d31165a99ae855ff","Clinical diagnosis has high requirements for the resolution of medical images, but most existing medical images super- resolution (SR) methods are performed under a known or specific degradation kernel. However, the difference between the actual degradations and their assumed degradation kernels results in a severe performance drop for the advanced SR methods in real applications. This paper proposes a medical image blind super-resolution model (Med-BSR) based on an improved degradation process to handle this issue. The model makes each of the degradation factors in medical image blind SR, such as blur, noise, and downsampling, more complex and practical. Specifically, the authors use the random select/combine strategy to randomly arrange and combine the type and order of each degradation factor, which significantly expands the degradation space. The authors also improved the loss function of the primary enhanced super-resolution generative adversarial networks (ESRGAN) network. The extensive experimental results demonstrate that the authors’ designed model can accurately restore the natural degradation process, which can reconstruct high-quality SR medical images. It also has a good generalization ability to realistic images simultaneously. © 2023 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Diagnosis; Generative adversarial networks; Image enhancement; Medical image processing; Optical resolving power; Blind Super-resolution; Clinical diagnosis; Degradation factor; Degradation process; Image super resolutions; Images reconstruction; Medical images processing; Superresolution; Superresolution methods; Visual perception; Image reconstruction","image reconstruction; medical image processing; visual perception","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85147167743"
"Wu Y.; Wang B.; Yuan R.; Watada J.","Wu, Yifei (57986164900); Wang, Bo (57986059900); Yuan, Ran (57222535351); Watada, Junzo (57189052014)","57986164900; 57986059900; 57222535351; 57189052014","A Gramian angular field-based data-driven approach for multiregion and multisource renewable scenario generation","2023","Information Sciences","619","","","578","602","24","10.1016/j.ins.2022.11.027","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142866054&doi=10.1016%2fj.ins.2022.11.027&partnerID=40&md5=c2b1c3bd86d6a2f10a4053b2beb2cf5f","Scenario generation is a pivotal method for providing system operators with a reasonable quantity of power scenarios that are capable of reflecting uncertainties and spatiotemporal processes to make exact and effective decisions for power systems. Aiming at improving the forecasting performance of renewable generation and capturing uncertainty as well as dependency over renewable site groups in different regions, this paper proposes a data-driven approach for parallel scenario generation. To capture the complex spatiotemporal dynamics of renewable energy sources (RESs), the proposed approach utilizes Gramian angular field (GAF) to process time sequences and constructs style-based super-resolution models that correspond with the idea of multi-model ensembles. Thereafter, a two-stage stochastic optimization strategy is adopted to accomplish scenario forecasting using point forecasts and historical error information as input. Based on two real-world datasets from the National Renewable Energy Laboratory (NREL) and the Belgian transmission operator ELIA, the effectiveness of the proposed approach is verified by methods including statistical analysis, spatiotemporal correlations, power system scheduling, and out-of-sample evaluations. Compared with three advanced benchmarks, the proposed approach has superior forecasting performance and spatiotemporal dynamic capture capability. At a 24-h lead time, the proposed model achieves continuous ranked probability scores (CRPSs) of 4–14% over the other models with consistent performance during the economic dispatching of actual power system operations. © 2022 Elsevier Inc.","Benchmarking; Forecasting; Optical resolving power; Optimization; Renewable energy resources; Angular field; Data-driven approach; Enhanced super-resolution generative adversarial network; Gramian angular field; Gramians; Renewable scenario generation; Scenarios generation; Stochastic optimizations; Style-based generative adversarial network; Superresolution; Generative adversarial networks","Enhanced super-resolution generative adversarial networks; Gramian angular field; Renewable scenario generation; Stochastic optimization; Style-based generative adversarial networks","Article","Final","","Scopus","2-s2.0-85142866054"
"Yi P.; Wang Z.; Jiang K.; Jiang J.; Lu T.; Ma J.","Yi, Peng (57203880354); Wang, Zhongyuan (57203515592); Jiang, Kui (57203871718); Jiang, Junjun (54902306100); Lu, Tao (56406646300); Ma, Jiayi (26638975600)","57203880354; 57203515592; 57203871718; 54902306100; 56406646300; 26638975600","A Progressive Fusion Generative Adversarial Network for Realistic and Consistent Video Super-Resolution","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","5","","2264","2280","16","10.1109/TPAMI.2020.3042298","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097928166&doi=10.1109%2fTPAMI.2020.3042298&partnerID=40&md5=e23a1e7bd8254066dbf4605ab033d079","How to effectively fuse temporal information from consecutive frames remains to be a non-trivial problem in video super-resolution (SR), since most existing fusion strategies (direct fusion, slow fusion, or 3D convolution) either fail to make full use of temporal information or cost too much calculation. To this end, we propose a novel progressive fusion network for video SR, in which frames are processed in a way of progressive separation and fusion for the thorough utilization of spatio-temporal information. We particularly incorporate multi-scale structure and hybrid convolutions into the network to capture a wide range of dependencies. We further propose a non-local operation to extract long-range spatio-temporal correlations directly, taking place of traditional motion estimation and motion compensation (MEMC). This design relieves the complicated MEMC algorithms, but enjoys better performance than various MEMC schemes. Finally, we improve generative adversarial training for video SR to avoid temporal artifacts such as flickering and ghosting. In particular, we propose a frame variation loss with a single-sequence training method to generate more realistic and temporally consistent videos. Extensive experiments on public datasets show the superiority of our method over state-of-the-art methods in terms of performance and complexity. Our code is available at https://github.com/psychopa4/MSHPFNL. © 1979-2012 IEEE.","Convolution; Motion compensation; Optical resolving power; Adversarial networks; Motion estimation and motion compensation; Multi-scale structures; Spatiotemporal correlation; Spatiotemporal information; State-of-the-art methods; Temporal information; Video super-resolution; algorithm; article; artifact; compensation; motion; videorecording; Motion estimation","Convolutional neural network; generative adversarial network; progressive fusion; spatio-temporal correlation; video super-resolution","Article","Final","","Scopus","2-s2.0-85097928166"
"Song Z.; Qiu D.; Zhao X.; Lin D.; Hui Y.","Song, Zhaoyang (57212392978); Qiu, Defu (57211118363); Zhao, Xiaoqiang (55705060000); Lin, Dongmei (57863382900); Hui, Yongyong (57194348820)","57212392978; 57211118363; 55705060000; 57863382900; 57194348820","Channel attention generative adversarial network for super-resolution of glioma magnetic resonance image","2023","Computer Methods and Programs in Biomedicine","229","","107255","","","","10.1016/j.cmpb.2022.107255","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145607228&doi=10.1016%2fj.cmpb.2022.107255&partnerID=40&md5=40d74c126cc2db25f5ab0e559893083d","Background and objective: Glioma is the most common primary craniocerebral tumor caused by the cancelation of glial cells in the brain and spinal cord, with a high incidence and cure rate. Magnetic resonance imaging (MRI) is a common technique for detecting and analyzing brain tumors. Due to improper hardware and operation, the obtained brain MRI images are low-resolution, making it difficult to detect and grade gliomas accurately. However, super-resolution reconstruction technology can improve the clarity of MRI images and help experts accurately detect and grade glioma. Methods: We propose a glioma magnetic resonance image super-resolution reconstruction method based on channel attention generative adversarial network (CGAN). First, we replace the base block of SRGAN with a residual dense block based on the channel attention mechanism. Second, we adopt a relative average discriminator to replace the discriminator in standard GAN. Finally, we add the mean squared error loss to the training, consisting of the mean squared error loss, the L1 norm loss, and the generator's adversarial loss to form the generator loss function. Results: On the Set5, Set14, Urban100, and glioma datasets, compared with the state-of-the-art algorithms, our proposed CGAN method has improved peak signal-to-noise ratio and structural similarity, and the reconstructed glioma images are more precise than other algorithms. Conclusion: The experimental results show that our CGAN method has apparent improvements in objective evaluation indicators and subjective visual effects, indicating its effectiveness and superiority. © 2022","Algorithms; Brain; Glioma; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Generative adversarial networks; Image enhancement; Image reconstruction; Mean square error; Optical resolving power; Signal to noise ratio; Tumors; Attention mechanisms; Glial cells; Glioma magnetic resonance imaging; High incidence; Incidence rate; Mean squared error; Network methods; Spinal-cord; Squared error loss; Superresolution; Article; attention network; channel attention generative adversarial network; controlled study; glioma; human; image quality; intermethod comparison; nuclear magnetic resonance imaging; quantitative analysis; reconstruction algorithm; signal noise ratio; algorithm; brain; diagnostic imaging; glioma; image processing; nuclear magnetic resonance imaging; procedures; Magnetic resonance imaging","Attention mechanism; Generative adversarial network; Glioma magnetic resonance imaging; Super-resolution","Article","Final","","Scopus","2-s2.0-85145607228"
"Li M.; McComb C.","Li, Matthew (57211535115); McComb, Christopher (56400780700)","57211535115; 56400780700","Using Physics-Informed Generative Adversarial Networks to Perform Super-Resolution for Multiphase Fluid Simulations","2022","Journal of Computing and Information Science in Engineering","22","4","044501","","","","10.1115/1.4053671","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143073891&doi=10.1115%2f1.4053671&partnerID=40&md5=23428c9a4d046d03d557fd655f076bf2","Computational fluid dynamics (CFD) simulations are useful in the field of engineering design as they provide deep insights on product or system performance without the need to construct and test physical prototypes. However, they can be very computationally intensive to run. Machine learning methods have been shown to reconstruct high-resolution single-phase turbulent fluid flow simulations from low-resolution inputs. This offers a potential avenue towards alleviating computational cost in iterative engineering design applications. However, little work thus far has explored the application of machine learning image super-resolution methods to multiphase fluid flow (which is important for emerging fields such as marine hydrokinetic energy conversion). In this work, we apply a modified version of the super-resolution generative adversarial network (SRGAN) model to a multiphase turbulent fluid flow problem, specifically to reconstruct fluid phase fraction at a higher resolution. Two models were created in this work, one which incorporates a physics-informed term in the loss function and one which does not, and the results are discussed and compared. We found that both models significantly outperform non-machine learning upsampling methods and can preserve a substantial amount of detail, showing the versatility of the SRGAN model for upsampling multiphase fluid simulations. However, the difference in accuracy between the two models is minimal indicating that, in the context studied here, the additional complexity of a physics-informed approach may not be justified. Copyright © 2022 by ASME","Computational fluid dynamics; Generative adversarial networks; Iterative methods; Multiphase flow; Optical resolving power; Product design; Signal sampling; Engineering applications; Engineering design; Fluid simulations; High resolution; Machine learning for engineering application; Machine-learning; Multiphase fluids; Physics-based Simulation; Superresolution; Turbulent fluid flow; Cost engineering","machine learning for engineering applications; physics-based simulations","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85143073891"
"Yuan C.; Deng K.; Li C.; Zhang X.; Li Y.","Yuan, Cao (9041176000); Deng, Kaidi (57877648200); Li, Chen (57209287816); Zhang, Xueting (57877278700); Li, Yaqin (35109390400)","9041176000; 57877648200; 57209287816; 57877278700; 35109390400","Improving Image Super-Resolution Based on Multiscale Generative Adversarial Networks","2022","Entropy","24","8","1030","","","","10.3390/e24081030","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137328406&doi=10.3390%2fe24081030&partnerID=40&md5=8f7727f9241c5b52d7de943195b4b238","Convolutional neural networks have greatly improved the performance of image super-resolution. However, perceptual networks have problems such as blurred line structures and a lack of high-frequency information when reconstructing image textures. To mitigate these issues, a generative adversarial network based on multiscale asynchronous learning is proposed in this paper, whereby a pyramid structure is employed in the network model to integrate high-frequency information at different scales. Our scheme employs a U-net as a discriminator to focus on the consistency of adjacent pixels in the input image and uses the LPIPS loss for perceptual extreme super-resolution with stronger supervision. Experiments on benchmark datasets and independent datasets Set5, Set14, BSD100, and SunHays80 show that our approach is effective in restoring detailed texture information from low-resolution images. © 2022 by the authors.","","deep generative model; deep learning; feature transform; generative adversarial network; multiscale feature extraction; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137328406"
"Fan Y.; Zhu C.","Fan, Yawen (57941310600); Zhu, Changming (56468407800)","57941310600; 56468407800","Super-resolution reconstruction of underwater sonar image based on multi-scale feature fusion","2022","ACM International Conference Proceeding Series","","","","898","901","3","10.1145/3548608.3559331","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140583270&doi=10.1145%2f3548608.3559331&partnerID=40&md5=c7a3b7ceb8f5db2978d3a7eca44a1a09","Because the underwater environment is too complex, underwater sonar images have many problems, such as low resolution, blurred details, speckle noise and so on. Therefore, it is urgent to improve the quality of underwater sonar images. To solve this problem, we propose the generative adversarial network based on octave convolution and channel attention mechanism (OCTGAN), which is a multi-scale feature fusion super-resolution reconstruction method suitable for underwater sonar images. OCTGAN consists of two parts, namely a generator and a discriminator. The generator is a network after the fusion of Octave convolution and channel attention mechanism. Octave convolution divides the feature map into a low-frequency part and a high-frequency part. The information of the feature map is updated and exchanged before fusion, and then recalibration is performed through the channel attention mechanism module. OCTGAN can learn more high-frequency details in the image and reconstruct super-resolution images with clearer texture details. Experimental results show that compared with the existing super-resolution methods, this method has better image reconstruction effects.  © 2022 ACM.","Generative adversarial networks; Image enhancement; Image fusion; Image reconstruction; Optical resolving power; Sonar; Textures; Underwater acoustics; Attention mechanisms; Channel attention mechanism; Feature map; Features fusions; Multi-scale features; Octave convolution; Sonar image; Super-resolution reconstruction; Superresolution; Underwater sonars; Convolution","channel attention mechanism; generative adversarial network; octave convolution; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85140583270"
"Bode M.","Bode, Mathis (56135121000)","56135121000","Applying Physics-Informed Enhanced Super-Resolution Generative Adversarial Networks to Large-Eddy Simulations of ECN Spray C","2022","SAE Technical Papers","","2022","","","","","10.4271/2022-01-0503","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128048566&doi=10.4271%2f2022-01-0503&partnerID=40&md5=923f8a8d51b2b5f1aa3084de4cc1021e","Large-eddy simulation (LES) is an important tool to understand and analyze sprays, such as those found in engines. Subfilter models are crucial for the accuracy of spray-LES, thereby signifying the importance of their development for predictive spray-LES. Recently, new subfilter models based on physics-informed generative adversarial networks (GANs) were developed, known as physics-informed enhanced super-resolution GANs (PIESRGANs). These models were successfully applied to the Spray A case defined by the Engine Combustion Network (ECN). This work presents technical details of this novel method, which are relevant for the modeling of spray combustion, and applies PIESRGANs to the ECN Spray C case. The results are validated against experimental data, and computational challenges and advantages are particularly emphasized compared to classical simulation approaches. © 2022 SAE International. All Rights Reserved.","Combustion; Engines; Large eddy simulation; Optical resolving power; Computational advantages; Computational challenges; Data challenges; Large-eddy simulations; Model-based OPC; Novel methods; Spray combustion; Subfilter models; Superresolution; Technical details; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85128048566"
"Ong Y.Z.; Yang H.","Ong, Yong Zheng (57219508792); Yang, Haizhao (55174878900)","57219508792; 55174878900","Generative imaging and image processing via generative encoder","2022","Inverse Problems and Imaging","16","3","","525","545","20","10.3934/ipi.2021060","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131574370&doi=10.3934%2fipi.2021060&partnerID=40&md5=80e7386018b9f9fcb608423fac66bd3a","This paper introduces a novel generative encoder (GE) framework for generative imaging and image processing tasks like image reconstruction, compression, denoising, inpainting, deblurring, and super-resolution. GE unifies the generative capacity of GANs and the stability of AEs in an optimization framework instead of stacking GANs and AEs into a single network or combining their loss functions as in existing literature. GE provides a novel approach to visualizing relationships between latent spaces and the data space. The GE framework is made up of a pre-training phase and a solving phase. In the former, a GAN with generator G capturing the data distribution of a given image set, and an AE network with encoder E that compresses images following the estimated distribution by G are trained separately, resulting in two latent representations of the data, denoted as the generative and encoding latent space respectively. In the solving phase, given noisy image x = P(x∗), where x∗ is the target unknown image, P is an operator adding an addictive, or multiplicative, or convolutional noise, or equivalently given such an image x in the compressed domain, i.e., given m = E(x), the two latent spaces are unified via solving the optimization problem z∗=argminz∥E(G(z)) − m∥2 2+ λ∥z∥2 2 and the image x∗ is recovered in a generative way via ˆx:= G(z∗) ≈ x∗, where λ > 0 is a hyperparameter. The unification of the two spaces allows improved performance against corresponding GAN and AE networks while visualizing interesting properties in each latent space. © 2022, American Institute of Mathematical Sciences. All rights reserved.","","Autoencoder; Deblurring; Denoising; Generative adversarial network; Image reconstruction; Inpainting; Inverse problem; Latent space; Super resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131574370"
"Shi J.; Ye Y.; Liu H.; Zhu D.; Su L.; Chen Y.; Huang Y.; Huang J.","Shi, Jianshe (57226526510); Ye, Yuguang (57226531235); Liu, Huifang (57419130900); Zhu, Daxin (37062535200); Su, Lianta (55695542800); Chen, Yijie (57419131000); Huang, Yifeng (57226537007); Huang, Jianlong (57210983352)","57226526510; 57226531235; 57419130900; 37062535200; 55695542800; 57419131000; 57226537007; 57210983352","Super-resolution reconstruction of pneumocystis carinii pneumonia images based on generative confrontation network","2022","Computer Methods and Programs in Biomedicine","215","","106578","","","","10.1016/j.cmpb.2021.106578","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123178367&doi=10.1016%2fj.cmpb.2021.106578&partnerID=40&md5=640fdc19621f72b677bfbd2deb4308f9","Objective: Pneumocystis carinii pneumonia, also known as pneumocystis carinii pneumonia (PCP), is an interstitial plasma cell pneumonia caused by pneumocystis spp. It is a conditional lung infectious disease. Because the early and correct diagnosis of PCP has a great influence on the prognosis of patients, the image processing of PCP's high-resolution CT (HRCT) is extremely important. Traditional image super-resolution reconstruction algorithms have difficulties in network training and artifacts in generated images. The super-resolution reconstruction algorithm of generative counter-networks can optimize these two problems well. Methods: In this paper, the texture enhanced super-resolution generative adversarial network (TESRGAN) is based on a generative confrontation network, which mainly includes a generative network and a discriminant network. In order to improve the quality of image reconstruction, TESRGAN improved the structure of the Super-Resolution Generative Adversarial Network (SRGAN) generation network, removed all BN layers in SRGAN, and replaced the ReLU function with the LeakyReLU function as the nonlinear activation function of the network to avoid the disappearance of the gradient. Experimental results: The TESRGAN algorithm in this paper is compared with the image reconstruction results of Bicubic, SRGAN, Enhanced Deep Super-Resolution network (EDSR), and ESRGAN. Compared with algorithms such as SRGAN and EDSR, our algorithm has clearer texture details and more accurate brightness information without extending the running time. Our reconstruction algorithm can improve the accuracy of image low-frequency information. Conclusion: The texture details of the reconstruction result are clearer and the brightness information is more accurate, which is more in line with the requirements of visual sensory evaluation. © 2021","Algorithms; Artifacts; Humans; Image Processing, Computer-Assisted; Pneumonia, Pneumocystis; Tomography, X-Ray Computed; Computerized tomography; Convolutional neural networks; Diagnosis; Generative adversarial networks; Image enhancement; Image quality; Image reconstruction; Luminance; Optical resolving power; Convolutional networks; Dense convolutional network; Image-based; Images reconstruction; Interstitials; Pneumocysti carinii pneumonia; Reconstruction algorithms; Super-resolution reconstruction; Superresolution; Texture loss; Article; convolutional neural network; discriminant analysis; enhanced deep super resolution network; generative adversarial network; image analysis; image quality; image reconstruction; nonlinear system; Pneumocystis pneumonia; reconstruction algorithm; sensory evaluation; super resolution generative adversarial network; texture enhanced super resolution generative adversarial network; algorithm; artifact; diagnostic imaging; human; image processing; x-ray computed tomography; Textures","Dense convolutional network; Generative adversarial network; Pneumocystis carinii pneumonia; Super-resolution reconstruction; Texture loss","Article","Final","","Scopus","2-s2.0-85123178367"
"Shahbakhsh M.B.; Hassanpour H.","Shahbakhsh, M.B. (57559069200); Hassanpour, H. (56919429700)","57559069200; 56919429700","Empowering Face Recognition Methods using a GAN-based Single Image Super-Resolution Network","2022","International Journal of Engineering, Transactions A: Basics","35","10","","","","","10.5829/ije.2022.35.10a.05","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132355001&doi=10.5829%2fije.2022.35.10a.05&partnerID=40&md5=fc587dd700d41eff7fdca1d0845673d9","Face recognition is one of the most common authentication techniques widely used due to its easy access. In many face recognition applications, captured images are often of low resolution. Face recognition methods perform poorly on low resolution images because they are trained on high resolution face images. Although existing face hallucination methods may generate visually pleasing images, they cannot improve the performance of face recognition methods at low resolution as the structure of the face image and high-frequency details are not sufficiently preserved. Recent advances in deep learning have been used in this paper to propose a new face super-resolution approach to empower face recognition methods. In this paper, a Generative Adversarial Network is used to empower face recognition in low-resolution images. This network considers image edges and reconstructs high-frequency details to preserve the face structure. The proposed technique to generate super-resolved features is usable in any face recognition method. We have used some state-of-the-art face recognition methods to evaluate the proposed method. The results showed a significant impact of the proposed method on the accuracy of face recognition of low resolution images. © 2022 Materials and Energy Research Center. All rights reserved.","Deep learning; Face recognition; Image enhancement; Optical resolving power; Face hallucination; Face images; Face recognition methods; High frequency HF; Identity preserving; Low resolution images; Lower resolution; Residual self-attention; Single images; Superresolution; Generative adversarial networks","Face Hallucination; Face Recognition; Generative Adversarial Network; Identity Preserving; Residual Self-attention; Super-Resolution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85132355001"
"Lepcha D.C.; Goyal B.; Dogra A.; Goyal V.","Lepcha, Dawa Chyophel (57224953540); Goyal, Bhawna (57190228335); Dogra, Ayush (56073519500); Goyal, Vishal (57196964308)","57224953540; 57190228335; 56073519500; 57196964308","Image super-resolution: A comprehensive review, recent trends, challenges and applications","2023","Information Fusion","91","","","230","260","30","10.1016/j.inffus.2022.10.007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140931028&doi=10.1016%2fj.inffus.2022.10.007&partnerID=40&md5=da526fe06e266ffc71ae311ded50d4ab","Super resolution (SR) is an eminent system in the field of computer vison and image processing to improve the visual perception of the poor-quality images. The key objective of image super resolution is to address the limitations of imaging systems mainly due to hardware problems and requirements for clinical processing of medical imaging using post-processing operations. Numerous super resolution strategies have been put-forward in the computer vision community to improve and achieve high-resolution images over the years. In the past few years, there has been a significant advancement in image super-resolution algorithms. This paper aims to provide the detailed survey on recent advancements in image super-resolution in terms of traditional, deep learning and the latest transformer-based algorithms. The in-depth taxonomy of broadly classified super-resolution techniques within these categories has been broadly discussed. An extensive survey has been carried out on deep learning techniques in terms of parameters, architecture, network complexity, depth, learning rate, framework, optimization, and loss function. Furthermore, we also address some of the significant parameters such as problem definition, evaluation metrics, publicly benchmarks datasets, loss functions and applications. In addition, we have performed an experimental analysis and comparison of various benchmark algorithms on publicly available datasets both qualitively and quantitively. Lastly, we conclude our survey by emphasizing some of the prospective future directions and open issues that the community need to address in the future. © 2022 Elsevier B.V.","Benchmarking; Convolutional neural networks; Deep learning; Generative adversarial networks; Image enhancement; Image resolution; Learning systems; Medical imaging; Convolutional neural network; Deep learning; Generative adversarial network; High resolution; Image quality assessment; Learning strategy; Low resolution; Lower resolution; Super- resolution; Superresolution; Surveys","Convolutional neural network (CNN); Deep learning; Generative adversarial network (GAN); High resolution (HR); Image quality assessment (IQA); Learning strategies; Low resolution (LR); Super- resolution (SR); Survey","Short survey","Final","","Scopus","2-s2.0-85140931028"
"Zhao B.; Saxena N.; Hofmann R.; Pradhan C.; Hows A.","Zhao, Bochao (57220599751); Saxena, Nishank (55450282600); Hofmann, Ronny (12140095100); Pradhan, Chaitanya (57204464931); Hows, Amie (56012389400)","57220599751; 55450282600; 12140095100; 57204464931; 56012389400","Enhancing resolution of micro-CT images of reservoir rocks using super resolution","2023","Computers and Geosciences","170","","105265","","","","10.1016/j.cageo.2022.105265","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141469648&doi=10.1016%2fj.cageo.2022.105265&partnerID=40&md5=6ca9203b77c3580a7089775c2689093a","Current hardware configuration of micro-CT detectors puts a lower limit on voxel size that can be acquired while maintaining a sufficiently large field of view. This limits the degree to which rock pores can be resolved in a micro-CT image and thus restricting the application envelope of Digital Rock technology. Super resolution techniques can refine voxel size while maintaining a sufficiently large field of view using pairs of low- and high-resolution images for training. However, for interpretation of quality of Digital Rock results, image quality is not determined by voxel size alone but by the degree to which a feature such as pore throat is resolved, which depends on both the physical size of the feature and voxel size. Furthermore, artificially down-sampling finer voxel size images to obtain images of coarser voxel size for training deep learning networks is not sufficient to capture the mapping between images acquired at different resolutions. This is especially true for reservoir rocks because the noise and artifacts introduced during imaging and reconstruction are more complex than that captured by simple down-sampling operation. We overcome these two limitations, by (1) using the ratio of pore throat size and voxel size (N) to group training dataset instead of voxel size and (2) using pairs of registered micro-CT images acquired using a state-of-the-art detector instead of synthetically down-sampled images. We show that combination of these two techniques produced images with better sharpness and contrast and enabled us to refine voxel size significantly beyond what is possible using the current imaging technology while maintaining the field of view. © 2022","Computerized tomography; Deep learning; Image acquisition; Image enhancement; Optical resolving power; Rocks; Signal sampling; 'current; CT Image; Down sampling; Hardware configurations; Large field of views; Micro CT; Modern imaging; Reservoir rock; Superresolution; Voxel size; artificial neural network; hardware; image resolution; learning; noise; reservoir rock; sampling; Generative adversarial networks","Generative adversarial networks; Modern imaging; Super resolution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85141469648"
"Wang H.; Wu Y.; Ni Q.; Liu W.","Wang, Hancong (57807054000); Wu, Yin (56093628700); Ni, Qiang (57614625100); Liu, Wenbo (55723547200)","57807054000; 56093628700; 57614625100; 55723547200","A Novel Wireless Leaf Area Index Sensor Based on a Combined U-Net Deep Learning Model","2022","IEEE Sensors Journal","22","16","","16573","16585","12","10.1109/JSEN.2022.3188697","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134328841&doi=10.1109%2fJSEN.2022.3188697&partnerID=40&md5=8ad0e55d3bf3341ff686cb5f1b34da75","Leaf area index (LAI) is an important parameter for forestry vegetation canopy structure investigation and ecological environment model study. Traditional ground direct measuring method is too time and labor consuming, while the remote sensing technique lacks of adequate validation and comparative analysis. Here, a novel wireless LAI sensor based on a lightweight deep learning model (LAINET) has been designed with a Raspberry Pi microcomputer and a LoRa transceiver. The mainly metering pattern of sensor system is the digital hemispherical photo-graphy (DHP) methodology based on Beer-Lambert law: firstly, the crown canopy's image is captured and segmented by LAINET, then the vegetation gap fraction can be extracted to calculate the LAI value. Our proposed LAINET consists of a lightweight convolutional neural network (CNN) and a generative adversarial network (GAN). The average accuracy of semantic segmentation (i.e. CNN part) could reach 0.978, and the combination of GAN for image super-resolution reconstruction can improve the accuracy of gap fraction measurement more by 5.5%. In addition, LAINET effectively solves the problem of low segmentation accuracy brought by environmental effects, the separation accuracy in direct sunlight or clear weather has been improved significantly. So the ultimate LAI value can be calculated precisely and stably. Experiment results show that the proposed sensor obtains a fine measuring error of less than 4% when comparing with the commercial plant canopy analyzer HM-G20. Combined with Uninterruptible Power Supply module of 5200 mAh, the sensor can work effectively for about 8 months, principally meeting the deployment and measurement criteria of forestry LAI. Therefore, the wireless sensor presented in this paper has a great application prospect.  © 2001-2012 IEEE.","Deep learning; Forestry; Generative adversarial networks; Image enhancement; Neural networks; Radio transceivers; Remote sensing; Semantic Segmentation; Semantics; Timber; Vegetation mapping; Canopy fisheye image; Deep learning; Fisheye images; Images segmentations; Leaf Area Index; Learning models; Raspberry pi; Vegetation mapping; Wireless communications; Wireless sensor; Wireless sensor networks","canopy fisheye image; deep learning; Leaf area index; Raspberry Pi; wireless sensor","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134328841"
"Zhang L.; Zhang W.; Lu G.; Yang P.; Rao Z.","Zhang, Lizong (56404489100); Zhang, Wei (57775746900); Lu, Guoming (55197339600); Yang, Pengcheng (57221541696); Rao, Zhihong (57209219103)","56404489100; 57775746900; 55197339600; 57221541696; 57209219103","Feature-level interpolation-based GAN for image super-resolution","2022","Personal and Ubiquitous Computing","26","4","","995","1010","15","10.1007/s00779-020-01488-y","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099418587&doi=10.1007%2fs00779-020-01488-y&partnerID=40&md5=1688c732647070b7d045ab4b2613a99e","Image super-resolution is widely applied in face recognition, video perception, medical imaging, and many other fields. Although significant progress has been made, existing methods remain limited in reconstructing fine-grained texture details, making the pixels of the resulting images coarse. To address this problem, we propose a novel interpolation-based generative adversarial network (GAN) for high-resolution image reconstruction. First, an interpolation algorithm is introduced into the generator to carry out self-interpolation and channel interpolation using advanced features extracted from the low-resolution images. Second, the idea of residuals is introduced into both the generator and discriminator to expand the receptive field of the model and fully exploit the global features of the image jointly improving the visual perception of the resulting super-resolution image. Extensive experiments are conducted to evaluate the performances of the proposed models from two aspects: convergence speed and the resolution improvement effect. The experimental results demonstrate that the proposed model reaches a faster convergence speed with and comparable resolution improvement effect with respect to other state-of-the-art methods. © 2021, Springer-Verlag London Ltd., part of Springer Nature.","Face recognition; Image reconstruction; Interpolation; Medical imaging; Optical resolving power; Textures; Adversarial networks; Faster convergence; High resolution image reconstruction; Image super resolutions; Interpolation algorithms; Low resolution images; Resolution improvement; State-of-the-art methods; Image enhancement","Generative adversarial networks; Interpolation; Residuals; Super-resolution","Article","Final","","Scopus","2-s2.0-85099418587"
"Yulianto; Nurhasanah; Yulistiani R.; Kusuma G.P.","Yulianto (57488531000); Nurhasanah (57488203500); Yulistiani, Risma (57487873800); Kusuma, Gede Putra (24474615100)","57488531000; 57488203500; 57487873800; 24474615100","Face Image Super-Resolution Using Combination of Max-Feature-Map and CMU-Net to Enhance Low-Resolution Face Recognition","2022","International Journal of Engineering Trends and Technology","70","3","","1","12","11","10.14445/22315381/IJETT-V70I3P201","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126365331&doi=10.14445%2f22315381%2fIJETT-V70I3P201&partnerID=40&md5=3c3cd3d29ba79637bc0bbfaa96dc53bb","The far distance of the camera while taking a picture makes the visual image look blurred. In deep learning, the blurred image eliminates the classification accuracy. The decreasing classification accuracy is caused by the loss of detailed information on High-Resolution (HR) images. The Generative Adversarial Networks (GAN) for Super-Resolution (SR) can overcome the problem and deal with the blurred images. In the usual term, GAN is used for reconstructing the quality of images visually. While not all GANs may be utilized to increase classification accuracy, SR image findings are highly realistic. This study offers a CMU-Net MFM SR technique based on this challenge, which combines a modified U-Net plus a Max-Feature-Map (MFM) module and a mix of BCE loss, Cosine matrix loss, and magnitude loss to restore the identity result of the SR image. With a classification accuracy of 78.45%, the experimental results of this method employing the LFW (Labelled Face in the Wild) dataset may be utilized to boost the image resolution from 8 x 8 pixels to 64 x 64 pixels. © 2022 Seventh Sense Research Group. All Rights Reserved.","","Convolution Neural Networks; Generative Adversarial Networks; Low-Resolution Face Recognition; Super-Resolution; U-Net Model","Article","Final","","Scopus","2-s2.0-85126365331"
"Prajapati K.; Chudasama V.; Patel H.; Sarvaiya A.; Upla K.","Prajapati, Kalpesh (57217177596); Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Sarvaiya, Anjali (57224979212); Upla, Kishor (53985429600)","57217177596; 57202047982; 57209145428; 57224979212; 53985429600","Comparative Analysis of Generative Adversarial Network-Based Single-Image Super-Resolution Approaches","2023","Lecture Notes in Electrical Engineering","952","","","321","335","14","10.1007/978-981-19-6737-5_26","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144828417&doi=10.1007%2f978-981-19-6737-5_26&partnerID=40&md5=fbb07782102d0e5d64706bfc37e03117","In the past decade, a convolutional neural network that can work on image became most popular since there were many powerful GPUs and datasets available to work. Convolutional neural network advanced the performance in various image processing tasks. In single-image-based super-resolution, convolutional neural network predicated methods have obtained remarkable performance in terms of error quantification (i.e., PSNR and SSIM) than earlier traditional machine learning predicated methods. Even though achieving better error measurements, the super-resolution results look blurry in appearance because of the classical L1 or L2 loss functions that have been used in the training process. Recently, generative adversarial network has been used to obtain a super-resolve image with better perceptual quality as compared to a convolutional neural network. In this manuscript, the technique and performance of those generative adversarial networks for single-image super-resolution approaches are compared for upscaling factor ×4. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Convolution; Convolutional neural networks; Image processing; Optical resolving power; Program processors; Comparative analyzes; Convolutional neural network; Image super resolutions; Image-based; Images processing; Network-based; Performance; Single images; Single-image super-resolution; Superresolution; Generative adversarial networks","Convolutional neural network; Generative adversarial network; Single-image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85144828417"
"Niu Y.; Jackson S.J.; Alqahtani N.; Mostaghimi P.; Armstrong R.T.","Niu, Yufu (7202225505); Jackson, Samuel J. (56683816900); Alqahtani, Naif (57210133722); Mostaghimi, Peyman (35759133600); Armstrong, Ryan T. (37025484700)","7202225505; 56683816900; 57210133722; 35759133600; 37025484700","Paired and Unpaired Deep Learning Methods for Physically Accurate Super-Resolution Carbonate Rock Images","2022","Transport in Porous Media","144","3","","825","847","22","10.1007/s11242-022-01842-z","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136666687&doi=10.1007%2fs11242-022-01842-z&partnerID=40&md5=74a4789bf1c129f0c1adab8f28438814","X-ray micro-computed tomography (micro-CT) has been widely leveraged to characterise the pore-scale geometry of subsurface porous rocks. Recent developments in super-resolution (SR) methods using deep learning allow for the digital enhancement of low-resolution (LR) images over large spatial scales, creating SR images comparable to high-resolution (HR) ground truth images. This circumvents the common trade-off between resolution and field-of-view. An outstanding issue is the use of paired LR and HR data, which is often required in the training step of such methods but is difficult to obtain. In this work, we rigorously compare two state-of-the-art SR deep learning techniques, using both paired and unpaired data, with like-for-like ground truth data. The first approach requires paired images to train a convolutional neural network (CNN), while the second approach uses unpaired images to train a generative adversarial network (GAN). The two approaches are compared using a micro-CT carbonate rock sample with complicated micro-porous textures. We implemented various image-based and numerical verifications and experimental validation to quantitatively evaluate the physical accuracy and sensitivities of the two methods. Our quantitative results show that the unpaired GAN approach can reconstruct super-resolution images as precise as the paired CNN method, with comparable training times and dataset requirements. This unlocks new applications for micro-CT image enhancement using unpaired deep learning methods; image registration is no longer needed during the data processing stage. Decoupled images from data storage platforms can be exploited to train networks for SR digital rock applications. This opens up a new pathway for various applications related to multi-scale flow simulations in heterogeneous porous media. © 2022, The Author(s).","Computerized tomography; Convolutional neural networks; Data handling; Deep learning; Digital storage; E-learning; Economic and social effects; Generative adversarial networks; Learning systems; Numerical methods; Optical resolving power; Porous materials; Carbonate rock; Convolutional neural network; Deep learning; Digital rock; Learning methods; Micro CT; Pore-scale geometry; Resolution images; Superresolution; X ray micro-computed tomography; artificial neural network; carbonate rock; multiphase flow; porous medium; X-ray tomography; Image enhancement","Deep learning; Digital rock; Micro-CT; Multiphase flow; Super-resolution","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85136666687"
"Tang Y.; Liu C.; Zhang X.","Tang, Yinggan (55968227100); Liu, Chenglu (57911864300); Zhang, Xuguang (57911864400)","55968227100; 57911864300; 57911864400","Single image super-resolution using Wasserstein generative adversarial network with gradient penalty","2022","Pattern Recognition Letters","163","","","32","39","7","10.1016/j.patrec.2022.09.012","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139045217&doi=10.1016%2fj.patrec.2022.09.012&partnerID=40&md5=9149959c5ba67ad0c7a29ec8e3a73b05","Due to its strong sample generating ability, Generative Adversarial Network (GAN) has been used to solve single image super-resolution (SISR) problem and obtains high perceptual quality super-resolution (SR) images. However, GAN suffers from the disadvantage of training instability, even fails to converge. In this paper, a new SISR method is proposed based on Wasserstein GAN, which is a training more stable GAN with Wasserstein metric. To further increase the SR performance and make the training process more easier and stable, two modifications are made on the original WGAN. First, a gradient penalty (GP) is adopted to replace weight clipping. Second, a new residual block with “pre-activation” of the weight layer is constructed in the generators of WGAN. Extensive experiments show that the proposed method yields superior SR performance than original GAN based SR methods and many other methods in accuracy and perceptual quality of ×4 magnification factor on four diverse testing datasets. © 2022 Elsevier B.V.","Optical resolving power; Gradient penalty; Image; Image super resolutions; Perceptual quality; Performance; Residual network; Resolution images; Single images; Superresolution; Superresolution methods; Generative adversarial networks","Generative adversarial network; Gradient penalty; Image; Residual network; Super-resolution","Article","Final","","Scopus","2-s2.0-85139045217"
"Li Y.; Wang Y.; Li B.; Wu S.","Li, Yunhe (55647591200); Wang, Yi (57204548320); Li, Bo (57777715900); Wu, Shaohua (57189245768)","55647591200; 57204548320; 57777715900; 57189245768","Super-Resolution of Remote Sensing Images for ×4 Resolution without Reference Images","2022","Electronics (Switzerland)","11","21","3474","","","","10.3390/electronics11213474","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141875470&doi=10.3390%2felectronics11213474&partnerID=40&md5=2b0b794fd72a9dfbed4cf586766ec1e8","Sentinel-2 satellites can provide free optical remote-sensing images with a spatial resolution of up to 10 M, but the spatial details provided are not enough for many applications, so it is worth considering improving the spatial resolution of Sentinel-2 satellites images through super-resolution (SR). Currently, the most effective SR models are mainly based on deep learning, especially the generative adversarial network (GAN). Models based on GAN need to be trained on LR–HR image pairs. In this paper, a two-step super-resolution generative adversarial network (TS-SRGAN) model is proposed. The first step is having the GAN train the degraded models. Without supervised HR images, only the 10 m resolution images provided by Sentinel-2 satellites are used to generate the degraded images, which are in the same domain as the real LR images, and then to construct the near-natural LR–HR image pairs. The second step is to design a super-resolution generative adversarial network with strengthened perceptual features, to enhance the perceptual effects of the generated images. Through experiments, the proposed method obtained an average NIQE as low as 2.54, and outperformed state-of-the-art models according to other two NR-IQA metrics, such as BRISQUE and PIQE. At the same time, the comparison of the intuitive visual effects of the generated images also proved the effectiveness of TS-SRGAN. © 2022 by the authors.","","generative adversarial network; remote-sensing image; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141875470"
"Cui J.; Gong K.; Han P.; Liu H.; Li Q.","Cui, Jianan (57195733866); Gong, Kuang (57146819700); Han, Paul (56514598400); Liu, Huafeng (7409750199); Li, Quanzheng (7405862484)","57195733866; 57146819700; 56514598400; 7409750199; 7405862484","Unsupervised arterial spin labeling image superresolution via multiscale generative adversarial network","2022","Medical Physics","49","4","","2373","2385","12","10.1002/mp.15468","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125628379&doi=10.1002%2fmp.15468&partnerID=40&md5=079bf0e4fbf130e1876fad6c37111cad","Purpose: Arterial spin labeling (ASL) magnetic resonance imaging (MRI) is an advanced noninvasive imaging technology that can measure cerebral blood flow (CBF) quantitatively without a contrast agent injection or radiation exposure. However, because of the weak labeling, conventional ASL images usually suffer from low signal-to-noise ratio (SNR), poor spatial resolution, and long acquisition time. Therefore, a method that can simultaneously improve the spatial resolution and SNR is needed. Methods: In this work, we proposed an unsupervised superresolution (SR) method to improve ASL image resolution based on a pyramid of generative adversarial networks (GAN). Through layer-by-layer training, the generators can learn features from the coarsest to the finest. The last layer's generator that contains fine details and textures was used to generate the final SR ASL images. In our proposed framework, the corresponding T1-weighted MR image was supplied as a second-channel input of the generators to provide high-resolution prior information. In addition, a low-pass-filter loss term was included to suppress the noise of the original ASL images. To evaluate the performance of the proposed framework, a simulation study and two real-patient experiments based on the in vivo datasets obtained from three healthy subjects on a 3T MR scanner were conducted, regarding the low-resolution (LR) to normal-resolution (NR) and the NR-to-SR tasks. The proposed method was compared to the nearest neighbor interpolation, trilinear interpolation, third-order B-splines interpolation methods, and deep image prior (DIP) with the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) as the quantification metrics. The averaged ASL images acquired with 44 min acquisition time were used as the ground truth for real-patient LR-to-NR study. The ablation studies of low-pass-filter loss term and T1-weighted MR image were performed based on simulation data. Results: For the simulation study, results show that the proposed method achieved significantly higher PSNR ((Formula presented.) -value (Formula presented.) 0.05) and SSIM ((Formula presented.) -value (Formula presented.) 0.05) than the nearest neighbor interpolation, trilinear interpolation, third-order B-splines interpolation, and DIP methods. For the real-patient LR-to-NR experiment, results show that the proposed method can generate high-quality SR ASL images with clearer structure boundaries and low noise levels and has the highest mean PSNR and SSIM. For real-patient NR-to-SR tasks, the structure of the results using the proposed method is sharper and clearer, which are the most similar to the structure of the reference 44 min acquisition image than other methods. The proposed method also shows the ability to remove artifacts in the NR image while superresolution. The ablation study verified that the low-pass-filter loss term and T1-weighted MR image are necessary for the proposed method. Conclusions: The proposed unsupervised multiscale GAN framework can simultaneously improve spatial resolution and reduce image noise. Experiment results from simulation data and three healthy subjects show that the proposed method achieves better performance than the nearest neighbor interpolation, the trilinear interpolation, the third-order B-splines interpolation, and DIP methods. © 2022 American Association of Physicists in Medicine.","Artifacts; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Signal-To-Noise Ratio; Spin Labels; Blood vessels; Image enhancement; Image resolution; Image segmentation; Interpolation; Low pass filters; Magnetic resonance imaging; Signal to noise ratio; Textures; Unsupervised learning; spin label; Arterial spin labeling; Filter loss; Labeling image; Low-pass filters; Lower resolution; MR-images; Nearest neighbor interpolation; Spatial resolution; Superresolution; T1-weighted; adult; arterial spin labeling; Article; feature learning (machine learning); female; generative adversarial network; human; human experiment; image artifact; in vivo study; machine learning; male; noise; normal human; signal noise ratio; simulation; T1 weighted imaging; unsupervised image superresolution; unsupervised machine learning; artifact; image processing; nuclear magnetic resonance imaging; procedures; Generative adversarial networks","arterial spin labeling; generative adversarial network; super-resolution; unsupervised learning","Article","Final","","Scopus","2-s2.0-85125628379"
"Wu X.; Chen Z.; Peng C.; Ye X.","Wu, Xinyue (57993126500); Chen, Zhineng (36170296000); Peng, Changgen (12807482700); Ye, Xiongjun (57992635600)","57993126500; 36170296000; 12807482700; 57992635600","MMSRNet: Pathological image super-resolution by multi-task and multi-scale learning","2023","Biomedical Signal Processing and Control","81","","104428","","","","10.1016/j.bspc.2022.104428","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143323580&doi=10.1016%2fj.bspc.2022.104428&partnerID=40&md5=bb828d34456f805a17e52975ad2b15f4","Pathological diagnosis is the gold standard for disease assessment in clinical practice. It is conducted by inspecting the specimen at the microscopical level. Therefore, a very high-resolution pathological image that precisely describes the submicron-scale appearance is essential in the era of digital pathology, which is not easily obtained. Recently, pathological image super-resolution (SR) has shown promising prospects in bridging this gap. However, existing studies have not fully explored the peculiarity of pathological data, which contains several gradually enlarged images describing the specimen at different magnifications. In this paper, we propose a novel MMSRNet that formulates the pathological image SR in a multi-task learning way. It adds an image magnification classification branch on top of the CNN-based SR network, e.g., RCAN. Therefore, the learning objective is transformed into performing the SR while classifying the magnification as accurately as possible. The incorporated classification label guides the network to learn a more powerful feature representation. Meanwhile, the multi-task learning paradigm also encourages the joint learning of multi-scale mapping functions corresponding to multiple magnifications. It thus enables the learned model to adaptively accommodate the magnification variants, overcoming the problem that performing SR from different magnifications is treated as independent tasks in existing studies. Extensive experiments are conducted to validate the effectiveness of MMSRNet. It not only gains better performance in performing SR across magnifications and scaling factors, but also exhibits attractive plug-and-play nature when RCAN is substituted by other SR networks. The generated images are also supposed to be helpful in clinical diagnosis. © 2022 Elsevier Ltd","Diagnosis; Learning systems; Optical resolving power; Clinical practices; Gold standards; Image super resolutions; Multi tasks; Multi-scale learning; Multi-scales; Multitask learning; Pathological images; Superresolution; Very high resolution; accuracy; Article; classification algorithm; clinical effectiveness; controlled study; deep learning; digital pathology; imaging algorithm; Generative adversarial networks","Generative adversarial networks; Multi-scale learning; Multi-task learning; Pathological image; Super-resolution","Article","Final","","Scopus","2-s2.0-85143323580"
"Sun J.; Li Z.-Y.; Li P.-C.; Li H.; Pang X.-W.; Wang H.","Sun, Jun (57222873461); Li, Zhang-Yu (57208319246); Li, Peng-Cheng (57216279808); Li, Hao (57211219513); Pang, Xiong-Wen (16175839500); Wang, Hui (55941458000)","57222873461; 57208319246; 57216279808; 57211219513; 16175839500; 55941458000","Improving the diagnostic performance of computed tomography angiography for intracranial large arterial stenosis by a novel super-resolution algorithm based on multi-scale residual denoising generative adversarial network","2023","Clinical Imaging","96","","","1","8","7","10.1016/j.clinimag.2023.01.009","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147421445&doi=10.1016%2fj.clinimag.2023.01.009&partnerID=40&md5=c13f12ef4ee77374c25a2f016290bf1c","Background: Computed tomography angiography (CTA) is very popular because it is characterized by rapidity and accessibility. However, CTA is inferior to digital subtraction angiography (DSA) in the diagnosis of intracranial artery stenosis or occlusion. DSA is an invasive examination, so we optimized the quality of cephalic CTA images. Methods: We used 5000 CTA images to train multi-scale residual denoising generative adversarial network (MRDGAN). And then 71 CTA images with intracranial large arterial stenosis were treated by Super-Resolution based on Generative Adversarial Network (SRGAN), Enhanced Super-Resolution based on Generative Adversarial Network (ESRGAN) and post-trained MRDGAN, respectively. Peak signal-to-noise ratio (PSNR) and structural similarity index measurement (SSIM) of the SRGAN, ESRGAN, MRDGAN and original CTA images were measured respectively. The qualities of MRDGAN and original images were visually assessed using a 4-point scale. The diagnostic coherence of digital subtraction angiography (DSA) with MRDGAN and original images was analyzed. Results: The PSNR was significantly higher in the MRDGAN CTA images (35.96 ± 1.51) than in the original (31.51 ± 1.43), SRGAN (25.75 ± 1.18) and ESRGAN (30.36 ± 1.05) CTA images (all P < 0.001). The SSIM was significantly higher in the MRDGAN CTA images (0.95 ± 0.02) than in the SRGAN (0.88 ± 0.03) and ESRGAN (0.90 ± 0.02) CTA images (all P < 0.01). The visual assessment was significantly higher in the MRDGAN CTA images (3.52 ± 0.58) than in the original CTA images (2.39 ± 0.69) (P < 0.05). The diagnostic coherence between MRDGAN and DSA (κ = 0.89) was superior to that between original images and DSA (κ = 0.62). Conclusion: Our MRDGAN can effectively optimize original CTA images and improve its clinical diagnostic value for intracranial large artery stenosis. © 2023","Angiography; Computerized tomography; Deep learning; Image enhancement; Image resolution; Signal to noise ratio; Angiography images; Artery stenosis; Computed tomography angiography; De-noising; Deep learning; Digital subtraction angiography; Intracranial large artery stenose; Multi-scales; Super resolution algorithms; Superresolution; aged; Article; artificial neural network; computed tomographic angiography; diagnostic test accuracy study; diagnostic value; digital subtraction angiography; enhanced super resolution based on generative adversarial network; female; human; image analysis; image quality; imaging algorithm; intermethod comparison; intracranial arterial stenosis; major clinical study; male; mathematical analysis; multilayer perceptron; noise reduction; retrospective study; signal noise ratio; super resolution based on generative adversarial network; Generative adversarial networks","Artificial intelligence; Computed tomography angiography; Deep learning; Intracranial large artery stenosis; Super-resolution algorithm","Article","Final","","Scopus","2-s2.0-85147421445"
"Aljarrah I.A.; Alshare E.M.","Aljarrah, Inad A. (12799816300); Alshare, Eman M. (57641058000)","12799816300; 57641058000","Improved Residual Dense Network for Large Scale Super-Resolution via Generative Adversarial Network","2022","International Journal of Communication Networks and Information Security","14","1","","118","125","7","10.54039/ijcnis.v14i1.5221","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128840584&doi=10.54039%2fijcnis.v14i1.5221&partnerID=40&md5=96e40e00eaf60ee414c18fd5ee27c8cd","Recent single image super resolution (SISR) studies were conducted extensively on small upscaling factors such as x2 and x4 on remote sensing images, while less work was conducted on large factors such as the factor x8 and x16. Owing to the high performance of the generative adversarial networks (GANs), in this paper, two GAN’s frameworks are implemented to study the SISR on the residual remote sensing image with large magnification under x8 scale factor, which is still lacking acceptable results. This work proposes a modified version of the residual dense network (RDN) and then it been implemented within GAN framework which named RDGAN. The second GAN framework has been built based on the densely sampled super resolution network (DSSR) and we named DSGAN. The used loss function for the training employs the adversarial, mean squared error (MSE) and the perceptual loss derived from the VGG19 model. We optimize the training by using Adam for number of epochs then switching to the SGD optimizer. We validate the frameworks on the proposed dataset of this work and other three remote sensing datasets: the UC Merced, WHURS19 and RSSCN7. To validate the frameworks, we use the following image quality assessment metrics: the PSNR and the SSIM on the RGB and the Y channel and the MSE. The RDGAN evaluation values on the proposed dataset were 26.02, 0.704, and 257.70 for PSNR, SSIM and the MSE, respectively, and the DSGAN evaluation on the same dataset yielded 26.13, 0.708 and 251.89 for the PSNR, the SSIM, and the MSE. © 2022 International Journal of Communication Networks and Information Security","","Generative adversarial network; Remote sensing; Residual dense generative adversarial network.; Residual dense network; Single image super-resolution","Article","Final","","Scopus","2-s2.0-85128840584"
"Tian C.; Yang J.; Li P.; Zhang S.; Mi S.","Tian, Chunhao (57559475000); Yang, Jian (57839670600); Li, Peng (57559688500); Zhang, Shaochong (7409374962); Mi, Shengli (36151045600)","57559475000; 57839670600; 57559688500; 7409374962; 36151045600","Retinal fundus image superresolution generated by optical coherence tomography based on a realistic mixed attention GAN","2022","Medical Physics","49","5","","3185","3198","13","10.1002/mp.15580","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127502794&doi=10.1002%2fmp.15580&partnerID=40&md5=09a38cc4cf05cd245a73423351f198da","Purpose: Optical coherence tomography (OCT) is widely used to diagnose retinal diseases. However, due to the limited resolution of OCT imaging systems, the quality of fundus images displayed is not satisfactory, which hinders the diagnosis of patients by ophthalmologists. This is an inevitable problem of OCT imaging systems, but few people have given attention to it. We attempt to solve this problem through deep learning methods. Methods: In this paper, we propose a single-image superresolution (SISR) model that is based on a generative adversarial network (GAN) for restoring low-resolution (LR) OCT fundus images to high-resolution (HR) counterparts. To obtain more realistic images, we craft the training data set by obtaining the real blur kernels of the LR images instead of using the bicubic interpolation kernel. The baseline of our generator is similar to that of an enhanced superresolution generative adversarial network (ESRGAN), but we creatively propose a mixed attention block (MAB). In contrast to other superresolution (SR) tasks, to adapt to the characteristics of OCT imaging systems, our network can reconstruct LR images with different upscaling factors in the height and width directions. Results: The results of qualitative and quantitative experiments prove that our model is capable of reconstructing retinal fundus images clearly and accurately. Conclusions: We propose a new GAN model for enhancing the quality of displayed OCT retinal fundus images and achieve state-of-the-art results. © 2022 American Association of Physicists in Medicine.","Deep learning; Diagnosis; Image enhancement; Image reconstruction; Image resolution; Imaging systems; Ophthalmology; Optical tomography; Fundus image; Image super resolutions; Images reconstruction; Learning methods; Limited resolution; Low resolution images; Retinal disease; Retinal fundus images; Superresolution; Tomography imaging; Article; deep learning; eye fundus; human; image analysis; image quality; image reconstruction; kernel method; optical coherence tomography; qualitative analysis; quantitative analysis; screening; Generative adversarial networks","generative adversarial network; image reconstruction; optical coherence tomography","Article","Final","","Scopus","2-s2.0-85127502794"
"Pourbagian M.; Ashrafizadeh A.","Pourbagian, Mahdi (39661171000); Ashrafizadeh, Ali (6506870520)","39661171000; 6506870520","Super-resolution of low-fidelity flow solutions via generative adversarial networks","2022","Simulation","98","8","","645","663","18","10.1177/00375497211061260","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120994157&doi=10.1177%2f00375497211061260&partnerID=40&md5=4507ed710fa0a87a1ee0766fc22b556f","While computational fluid dynamics (CFD) can solve a wide variety of fluid flow problems, accurate CFD simulations require significant computational resources and time. We propose a general method for super-resolution of low-fidelity flow simulations using deep learning. The approach is based on a conditional generative adversarial network (GAN) with inexpensive, low-fidelity solutions as inputs and high-fidelity simulations as outputs. The details, including the flexible structure, unique loss functions, and handling strategies, are thoroughly discussed, and the methodology is demonstrated using numerical simulations of incompressible flows. The distinction between low- and high-fidelity solutions is made in terms of discretization and physical modeling errors. Numerical experiments demonstrate that the approach is capable of accurately forecasting high-fidelity simulations. © The Author(s) 2021.","Deep learning; Flexible structures; Flow simulation; Generative adversarial networks; Incompressible flow; Computational fluid dynamics simulations; Deep learning; Dynamic solutions; Fluid flow problems; Fluid flow simulation; High-fidelity; High-fidelity simulations; Low and high-fidelity computational fluid dynamic solution; Low fidelities; Superresolution; Computational fluid dynamics","deep learning; Fluid flow simulation; generative adversarial network; low and high-fidelity CFD solution","Article","Final","","Scopus","2-s2.0-85120994157"
"Sui Y.; Afacan O.; Jaimes C.; Gholipour A.; Warfield S.K.","Sui, Yao (56460849200); Afacan, Onur (15839135000); Jaimes, Camilo (37079070300); Gholipour, Ali (57192251309); Warfield, Simon K. (7005171959)","56460849200; 15839135000; 37079070300; 57192251309; 7005171959","Scan-Specific Generative Neural Network for MRI Super-Resolution Reconstruction","2022","IEEE Transactions on Medical Imaging","41","6","","1383","1399","16","10.1109/TMI.2022.3142610","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122893866&doi=10.1109%2fTMI.2022.3142610&partnerID=40&md5=ed028000209f22b192fa30b9a16f20cd","The interpretation and analysis of Magnetic resonance imaging (MRI) benefit from high spatial resolution. Unfortunately, direct acquisition of high spatial resolution MRI is time-consuming and costly, which increases the potential for motion artifact, and suffers from reduced signal-To-noise ratio (SNR). Super-resolution reconstruction (SRR) is one of the most widely used methods in MRI since it allows for the trade-off between high spatial resolution, high SNR, and reduced scan times. Deep learning has emerged for improved SRR as compared to conventional methods. However, current deep learning-based SRR methods require large-scale training datasets of high-resolution images, which are practically difficult to obtain at a suitable SNR. We sought to develop a methodology that allows for dataset-free deep learning-based SRR, through which to construct images with higher spatial resolution and of higher SNR than can be practically obtained by direct Fourier encoding. We developed a dataset-free learning method that leverages a generative neural network trained for each specific scan or set of scans, which in turn, allows for SRR tailored to the individual patient. With the SRR from three short duration scans, we achieved high quality brain MRI at an isotropic spatial resolution of 0.125 cubic mm with six minutes of imaging time for T2 contrast and an average increase of 7.2 dB (34.2%) in SNR to these short duration scans. Motion compensation was achieved by aligning the three short duration scans together. We assessed our technique on simulated MRI data and clinical data acquired from 15 subjects. Extensive experimental results demonstrate that our approach achieved superior results to state-of-The-Art methods, while in parallel, performed at reduced cost as scans delivered with direct high-resolution acquisition. © 1982-2012 IEEE.","Brain; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Radionuclide Imaging; Signal-To-Noise Ratio; Deep learning; Economic and social effects; Generative adversarial networks; Image reconstruction; Image resolution; Magnetic resonance imaging; Magnetism; Medical imaging; Motion compensation; Resonance; Deep learning; Generative neural network; Images reconstruction; Neural-networks; Patient specific; Patient-specific learning; Signal resolution; Spatial resolution; Specific learning; Superresolution; adult; article; brain; clinical article; compensation; controlled study; deep learning; female; human; image reconstruction; male; motion; nuclear magnetic resonance imaging; signal noise ratio; simulation; diagnostic imaging; procedures; scintiscanning; signal noise ratio; Signal to noise ratio","deep learning; generative neural network; image reconstruction; Magnetic resonance imaging; patient-specific learning; super-resolution","Article","Final","","Scopus","2-s2.0-85122893866"
"Singh J.; Tant K.; Curtis A.; Mulholland A.","Singh, Jonathan (57205283108); Tant, Katherine (56644670700); Curtis, Andrew (35592475300); Mulholland, Anthony (7004313839)","57205283108; 56644670700; 35592475300; 7004313839","Real-time super-resolution mapping of locally anisotropic grain orientations for ultrasonic non-destructive evaluation of crystalline material","2022","Neural Computing and Applications","34","6","","4993","5010","17","10.1007/s00521-021-06670-8","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119301117&doi=10.1007%2fs00521-021-06670-8&partnerID=40&md5=e1fe1d86e7e4e20629ea0a57a6e9dbb7","Estimating the spatially varying microstructures of heterogeneous and locally anisotropic media non-destructively is necessary for the accurate detection of flaws and reliable monitoring of manufacturing processes. Conventional algorithms used for solving this inverse problem come with significant computational cost, particularly in the case of high-dimensional, nonlinear tomographic problems, and are thus not suitable for near-real-time applications. In this paper, for the first time, we propose a framework which uses deep neural networks (DNNs) with full aperture, pitch-catch and pulse-echo transducer configurations, to reconstruct material maps of crystallographic orientation. We also present the first application of generative adversarial networks (GANs) to achieve super-resolution of ultrasonic tomographic images, providing a factor-four increase in image resolution and up to a 50% increase in structural similarity. The importance of including appropriate prior knowledge in the GAN training data set to increase inversion accuracy is demonstrated: known information about the material’s structure should be represented in the training data. We show that after a computationally expensive training process, the DNNs and GANs can be used in less than 1 second (0.9 s on a standard desktop computer) to provide a high-resolution map of the material’s grain orientations, addressing the challenge of significant computational cost faced by conventional tomography algorithms. © 2021, The Author(s).","Anisotropic media; Deep neural networks; Generative adversarial networks; Image resolution; Inverse problems; Nondestructive examination; Tomography; Ultrasonic imaging; Ultrasonic testing; Anisotropic grains; Anisotropic medium; Computational costs; Conventional algorithms; Grain orientation; Manufacturing process; Non destructive evaluation; Real- time; Superresolution mapping; Ultrasound tomography; Anisotropy","Anisotropy; Deep neural networks; Generative adversarial networks; Ultrasound tomography","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85119301117"
"Doloi N.; Ghosh S.; Phirani J.","Doloi, Nandita (58093480300); Ghosh, Somnath (58092746400); Phirani, Jyoti (26321932100)","58093480300; 58092746400; 26321932100","Super-Resolution Reconstruction of Reservoir Saturation Map with Physical Constraints Using Generative Adversarial Network","2023","Society of Petroleum Engineers - SPE Reservoir Characterisation and Simulation Conference and Exhibition 2023, RCSC 2023","","","","","","","10.2118/212611-MS","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147500854&doi=10.2118%2f212611-MS&partnerID=40&md5=c58e707098f8580f8fecaa1d6540d42a","Complete physics-based numerical simulations currently provide the most accurate approach for predicting fluid flow behavior in geological reservoirs. However, the amount of computational resources required to perform these simulations increase exponentially with the increase in resolution to the point that they are infeasible. Therefore, a common practice is to upscale the reservoir model to reduce the resolution such that numerous simulations, as required, can be performed within a reasonable time. The problem we are trying to solve here is that the simulation results from these upscaled models, although they provide a zoomed-out and global view of the reservoir dynamics, however, they lack a detailed zoomed-in view of a local region in the reservoir, which is required to take actionable decisions. This work proposes using super-resolution techniques, recently developed using machine learning methods, to obtain fine-scale flow behavior given flow behavior from a low-resolution simulation of an upscaled-reservoir model. We demonstrate our model on a two-phase, deal-oil, and heterogenous oil reservoir, and we reconstruct the oil saturation map of the reservoir. We also demonstrate how the network can be trained using dynamic coarse geological properties at various resolutions. The findings imply that even when coarse geological features and with limited resolution, the super-resolution reconstructions are able to recreate missing information that is close to the ground facts. Copyright 2023, Society of Petroleum Engineers.","Gasoline; Generative adversarial networks; Geology; Learning systems; Optical resolving power; Petroleum reservoir engineering; Petroleum reservoirs; Computational resources; Flow behaviours; Fluid-flow; Geological reservoirs; Global view; Physical constraints; Physics-based; Reservoir models; Saturation maps; Super-resolution reconstruction; Flow of fluids","","Conference paper","Final","","Scopus","2-s2.0-85147500854"
"Li B.M.; Castorina L.V.; Valdés Hernández M.D.C.; Clancy U.; Wiseman S.J.; Sakka E.; Storkey A.J.; Jaime Garcia D.; Cheng Y.; Doubal F.; Thrippleton M.T.; Stringer M.; Wardlaw J.M.","Li, Bryan M. (57221149840); Castorina, Leonardo V. (57222285993); Valdés Hernández, Maria del C. (57189602736); Clancy, Una (57216640608); Wiseman, Stewart J. (55987275700); Sakka, Eleni (56376296200); Storkey, Amos J. (6602511464); Jaime Garcia, Daniela (57216641159); Cheng, Yajun (57889295800); Doubal, Fergus (23984561000); Thrippleton, Michael T. (57889905000); Stringer, Michael (56694059900); Wardlaw, Joanna M. (35417533500)","57221149840; 57222285993; 57189602736; 57216640608; 55987275700; 56376296200; 6602511464; 57216641159; 57889295800; 23984561000; 57889905000; 56694059900; 35417533500","Deep attention super-resolution of brain magnetic resonance images acquired under clinical protocols","2022","Frontiers in Computational Neuroscience","16","","887633","","","","10.3389/fncom.2022.887633","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138002534&doi=10.3389%2ffncom.2022.887633&partnerID=40&md5=17dc4918475ead6b1ba763348b8271dd","Vast quantities of Magnetic Resonance Images (MRI) are routinely acquired in clinical practice but, to speed up acquisition, these scans are typically of a quality that is sufficient for clinical diagnosis but sub-optimal for large-scale precision medicine, computational diagnostics, and large-scale neuroimaging collaborative research. Here, we present a critic-guided framework to upsample low-resolution (often 2D) MRI full scans to help overcome these limitations. We incorporate feature-importance and self-attention methods into our model to improve the interpretability of this study. We evaluate our framework on paired low- and high-resolution brain MRI structural full scans (i.e., T1-, T2-weighted, and FLAIR sequences are simultaneously input) obtained in clinical and research settings from scanners manufactured by Siemens, Phillips, and GE. We show that the upsampled MRIs are qualitatively faithful to the ground-truth high-quality scans (PSNR = 35.39; MAE = 3.78E−3; NMSE = 4.32E−10; SSIM = 0.9852; mean normal-appearing gray/white matter ratio intensity differences ranging from 0.0363 to 0.0784 for FLAIR, from 0.0010 to 0.0138 for T1-weighted and from 0.0156 to 0.074 for T2-weighted sequences). The automatic raw segmentation of tissues and lesions using the super-resolved images has fewer false positives and higher accuracy than those obtained from interpolated images in protocols represented with more than three sets in the training sample, making our approach a strong candidate for practical application in clinical and collaborative research. Copyright © 2022 Li, Castorina, Valdés Hernández, Clancy, Wiseman, Sakka, Storkey, Jaime Garcia, Cheng, Doubal, Thrippleton, Stringer and Wardlaw.","Brain mapping; Clinical research; Deep learning; Diagnosis; Generative adversarial networks; Image reconstruction; Image segmentation; Magnetism; Optical resolving power; Resonance; Brain imaging; Brain magnetic resonance images; Collaborative research; Deep learning; Explainable artificial intelligence; Images reconstruction; Large-scales; Lower resolution; Superresolution; U-net; article; artificial intelligence; attention; clinical protocol; deep learning; false positive result; human tissue; image reconstruction; neuroimaging; nuclear magnetic resonance imaging; T2 weighted imaging; white matter; Magnetic resonance imaging","brain imaging; deep learning; explainable artificial intelligence; generative adversarial networks; image reconstruction; Magnetic Resonance Imaging; super-resolution; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85138002534"
"Sun Z.; Ng C.K.C.","Sun, Zhonghua (12544503300); Ng, Curtise K. C. (26030030100)","12544503300; 26030030100","Finetuned Super-Resolution Generative Adversarial Network (Artificial Intelligence) Model for Calcium Deblooming in Coronary Computed Tomography Angiography","2022","Journal of Personalized Medicine","12","9","1354","","","","10.3390/jpm12091354","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138639595&doi=10.3390%2fjpm12091354&partnerID=40&md5=e357be2c47e32f6956705280f9f222f5","The purpose of this study was to finetune a deep learning model, real-enhanced super-resolution generative adversarial network (Real-ESRGAN), and investigate its diagnostic value in calcified coronary plaques with the aim of suppressing blooming artifacts for the further improvement of coronary lumen assessment. We finetuned the Real-ESRGAN model and applied it to 50 patients with 184 calcified plaques detected at three main coronary arteries (left anterior descending [LAD], left circumflex [LCx] and right coronary artery [RCA]). Measurements of coronary stenosis were collected from original coronary computed tomography angiography (CCTA) and Real-ESRGAN-processed images, including Real-ESRGAN-high-resolution, Real-ESRGAN-average and Real-ESRGAN-median (Real-ESRGAN-HR, Real-ESRGAN-A and Real-ESRGAN-M) with invasive coronary angiography as the reference. Our results showed specificity and positive predictive value (PPV) of the Real-ESRGAN-processed images were improved at all of the three coronary arteries, leading to significant reduction in the false positive rates when compared to those of the original CCTA images. The specificity and PPV of the Real-ESRGAN-M images were the highest at the RCA level, with values being 80% (95% CI: 64.4%, 90.9%) and 61.9% (95% CI: 45.6%, 75.9%), although the sensitivity was reduced to 81.3% (95% CI: 54.5%, 95.9%) due to false negative results. The corresponding specificity and PPV of the Real-ESRGAN-M images were 51.9 (95% CI: 40.3%, 63.5%) and 31.5% (95% CI: 25.8%, 37.8%) at LAD, 62.5% (95% CI: 40.6%, 81.2%) and 43.8% (95% CI: 30.3%, 58.1%) at LCx, respectively. The area under the receiver operating characteristic curve was also the highest at the RCA with value of 0.76 (95% CI: 0.64, 0.89), 0.84 (95% CI: 0.73, 0.94), 0.85 (95% CI: 0.75, 0.95) and 0.73 (95% CI: 0.58, 0.89), corresponding to original CCTA, Real-ESRGAN-HR, Real-ESRGAN-A and Real-ESRGAN-M images, respectively. This study proves that the finetuned Real-ESRGAN model significantly improves the diagnostic performance of CCTA in assessing calcified plaques. © 2022 by the authors.","2 propanol; calcium; metal organic framework; nanoparticle; ruthenium; Article; artifact; artificial intelligence; calcification; catalysis; computed tomographic angiography; controlled study; coronary angiography; deep learning; diagnostic test accuracy study; diagnostic value; false negative result; human; hydrogenation; nonhuman; positivity rate; predictive value; receiver operating characteristic; right coronary artery; sensitivity and specificity; trimerization","calcification; coronary computed tomography angiography; deep learning; generative adversarial network; plaque","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138639595"
"Luo S.; Zhou J.; Yang Z.; Wei H.; Fu Y.","Luo, Suyang (57457151200); Zhou, Jiliu (21234416400); Yang, Zhipeng (57187186400); Wei, Hong (35293930100); Fu, Ying (55712884400)","57457151200; 21234416400; 57187186400; 35293930100; 55712884400","Diffusion MRI super-resolution reconstruction via sub-pixel convolution generative adversarial network","2022","Magnetic Resonance Imaging","88","","","101","107","6","10.1016/j.mri.2022.02.001","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124801102&doi=10.1016%2fj.mri.2022.02.001&partnerID=40&md5=84b4c3109cddc10056dd7abcfddb24a1","To solve the problem of long sampling time for diffusion magnetic resonance imaging (dMRI), in this study we propose a dMRI super-resolution reconstruction network. This method not only uses a three-dimensional (3D) convolution kernel to reconstruct the dMRI data in the space and angle domains, but also introduces an adversarial learning and attention mechanism to solve the problem of the traditional loss function not fully quantifying the gap between high-dimensional data and not paying more attention to important feature maps. Experimental results from the comparison of peak signal-to-noise ratio, structural similarity, and orientation distribution function visualization show that these methods bring better results. They also prove the feasibility of using an attention mechanism in dMRI reconstruction and the use of adversarial learning in a 3D convolution kernel. © 2022 Elsevier Inc.","Algorithms; Diffusion Magnetic Resonance Imaging; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Signal-To-Noise Ratio; article; attention; diffusion weighted imaging; feasibility study; human; learning; loss of function mutation; quantitative analysis; signal noise ratio; algorithm; diffusion weighted imaging; image processing; nuclear magnetic resonance imaging; procedures","Attention mechanism; Diffusion magnetic resonance imaging; Generative adversarial network; Super-resolution reconstruction; Three-dimensional convolution kernel","Article","Final","","Scopus","2-s2.0-85124801102"
"Zhu F.; Wang C.; Zhu B.; Sun C.; Qi C.","Zhu, Fuzhen (12780819500); Wang, Chen (57980554000); Zhu, Bing (57199801240); Sun, Ce (58059130100); Qi, Chengxiao (57282587700)","12780819500; 57980554000; 57199801240; 58059130100; 57282587700","An improved generative adversarial networks for remote sensing image super-resolution reconstruction via multi-scale residual block","2023","Egyptian Journal of Remote Sensing and Space Science","26","1","","151","160","9","10.1016/j.ejrs.2022.12.008","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146055217&doi=10.1016%2fj.ejrs.2022.12.008&partnerID=40&md5=0718459def821c5b8689d5680f7a98b8","Existing image super-resolution algorithms still suffer from the problems of not extracting rich image features and losing realistic high-frequency details. In order to solve these problems, this paper proposes an improved generative adversarial network algorithm for super-resolution reconstruction of remote sensing images by multi-scale residual blocks. The original generative adversarial network (GAN) structure is improved and multi-scale residual blocks are introduced in the generator to fuse features at different scales. After extracting the parallel information of multi-scale features, information is exchanged between multi-resolution information streams to obtain contextual information through spatial and channel attention mechanisms, and multi-scale features are fused according to the attention mechanism. In the discriminator, the concept of relative average GAN (RaGAN) is introduced, and the loss function of the network is redesigned so that the discriminator can predict relative probabilities instead of absolute probabilities thus enabling clear learning of edge and texture details. Experimental results show that the proposed method in this paper significantly outperforms state-of-the-art (SOTA) methods in terms of both subjective and objective metrics.In three test datasets, compared with SOTA methods, the Peak Signal to Noise Ratio(PSNR) is improved by a maximum of 1.18 dB, 0.84 dB and 1.29 dB respectively, and the Structural Similarity Index (SSIM) is improved by 0.0264, 0.0077 and 0.0109 respectively in scale of 2, 3 and 4 times images super-resolution.The model proposed in this paper effectively improved the super-resolution re-construction results of remote sensing images. © 2022 National Authority of Remote Sensing & Space Science","Image enhancement; Image reconstruction; Optical resolving power; Remote sensing; Signal to noise ratio; Textures; Attention mechanisms; Image super resolutions; Image super-resolution reconstruction; Multi-scale features; Multi-scale residual block; Multi-scales; Relative average GAN; Remote sensing images; State-of-the-art methods; Super-resolution reconstruction; algorithm; artificial neural network; image processing; image resolution; reconstruction; remote sensing; signal-to-noise ratio; Generative adversarial networks","GAN; Multi-scale residual block; RaGAN; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85146055217"
"Monday H.N.; Li J.; Nneji G.U.; Nahar S.; Hossin M.A.; Jackson J.; Oluwasanmi A.","Monday, Happy Nkanta (57201671873); Li, Jianping (56003087700); Nneji, Grace Ugochi (57206902001); Nahar, Saifun (57215928272); Hossin, Md Altab (57202841195); Jackson, Jehoiada (57195632099); Oluwasanmi, Ariyo (57210636841)","57201671873; 56003087700; 57206902001; 57215928272; 57202841195; 57195632099; 57210636841","A wavelet convolutional capsule network with modified super resolution generative adversarial network for fault diagnosis and classification","2022","Complex and Intelligent Systems","8","6","","4831","4847","16","10.1007/s40747-022-00733-6","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134225236&doi=10.1007%2fs40747-022-00733-6&partnerID=40&md5=f187bedb3f1da57f9982ff03e477592d","The study of fault diagnosis and classification has gained tremendous attention in various aspects of modern industry. However, the performance of traditional fault diagnosis technique solely depends on handcrafted features based on expert knowledge which is difficult to pre-design and has failed in several applications. Deep learning (DL) has achieved remarkable performance in hierarchical feature extraction and learning distinctive feature of dataset from related distribution. However, the challenge associated with DL models is that max-pooling operation usually leads to loss of spatial details during high-level feature extraction. Another concern is the low quality characteristics of 2D time-frequency image which is mostly caused by the presence of noise and poor resolution. This paper proposes a modified wavelet convolutional capsule network with modified enhanced super resolution generative adversarial network plus for fault diagnosis and classification. It uses continuous wavelet transform to convert raw data signals to 2D time-frequency images and applies super resolution generative adversarial technique to enhance the quality of the time-frequency images and finally, the convolutional capsule network learns the extracted high-level features without loss of spatial details for the diagnosis and classification of faults. We validated our proposed model on the famous motor bearing dataset from the Case Western Reserve University. The experimental results show that our proposed fault diagnostic model obtains higher diagnosis accuracy of 99.84% outweighing most traditional deep learning models including state-of-the-art methods. © 2022, The Author(s).","","Capsule network; CNN; Fault diagnosis; GAN; Super resolution; Wavelet","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134225236"
"Han J.; Wang C.","Han, Jun (57209633331); Wang, Chaoli (57203797020)","57209633331; 57203797020","TSR-VFD: Generating temporal super-resolution for unsteady vector field data","2022","Computers and Graphics (Pergamon)","103","","","168","179","11","10.1016/j.cag.2022.02.001","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125113331&doi=10.1016%2fj.cag.2022.02.001&partnerID=40&md5=f45479176df0ce9b5cff90c184ccfcd9","We present TSR-VFD, a novel deep learning solution that recovers temporal super-resolution (TSR) of three-dimensional vector field data (VFD) for unsteady flow. In scientific visualization, TSR-VFD is the first work that leverages deep neural nets to interpolate intermediate vector fields from temporally sparsely sampled unsteady vector fields. The core of TSR-VFD lies in using two networks: InterpolationNet and MaskNet, that process the vector components of different scales from sampled vector fields as input and jointly output synthesized intermediate vector fields. To demonstrate our approach's effectiveness, we report qualitative and quantitative results with several data sets and compare TSR-VFD against vector field interpolation using linear interpolation (LERP), generative adversarial network (GAN), and recurrent neural network (RNN). In addition, we compare TSR-VFD with a lossy compression (LC) scheme. Finally, we conduct a comprehensive study to evaluate critical parameter settings and network designs. © 2022","Deep neural networks; Interpolation; Optical resolving power; Recurrent neural networks; Vectors; Data reconstruction; Deep learning; Deep neural nets; Synthesised; Temporal super resolution; Three-dimensional vectors; Unsteady vector fields; Vector components; Vector field datum; Vector fields; Generative adversarial networks","Data reconstruction; Deep learning; Temporal super-resolution; Unsteady vector field","Article","Final","","Scopus","2-s2.0-85125113331"
"More D.; Acharya S.; Aryan S.","More, Deeptej (57737497600); Acharya, Sagar (57737356200); Aryan, Suryansh (57738222700)","57737497600; 57737356200; 57738222700","SRGAN-TQT, an Improved Motion Tracking Technique for UAVs with Super-Resolution Generative Adversarial Network (SRGAN) and Temporal Quad-Tree (TQT)","2022","SAE Technical Papers","","","","","","","10.4271/2022-26-0021","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131819677&doi=10.4271%2f2022-26-0021&partnerID=40&md5=b8078071206725ea91dc24ecea4ac6cc","Unmanned Aerial Vehicles (UAVs) are gaining significant popularity due to wide-scale applications in civilian and military use. Unmanned Aerial Vehicles are most commonly used for surveillance. Object tracking is one of the most important things that an autonomous UAV has to perform. However, the accuracy of the object tracking model degrades when the object fades away to some distance or if the input images have low resolution. High-resolution cameras are expensive and increase the overall cost of the UAV. The concept of SRGAN-TQT (Super-Resolution Generative Adversarial Network - Temporal Quad-Tree), an improved object tracking pipeline for UAVs in the presence of low-resolution cameras or distant objects, provides a cost-effective solution with enhanced accuracy to perform object tracking. Implementation of Super-Resolution - Generative Adversarial Networks (SRGANs) and Temporal Quad-Tree (TQT) along with state-of-the-art object detection algorithms serve as the backend of the pipeline. This approach uses Deep Neural Network-based GANs to upsample images precisely. Temporal Quad-Tree (TQT) is a motion tracking technique that is an extension of the popular Quad-Tree segmentation algorithm. The Temporal Quad-Tree algorithm is present to reduce the computational complexity and give a highly reliable tracking algorithm. This consequently omits the requirement of a high-resolution camera for UAVs while increasing the object tracking capability.  © 2022 SAE International. All Rights Reserved.","Aircraft detection; Antennas; Cameras; Cost effectiveness; Deep neural networks; Generative adversarial networks; Image segmentation; Military applications; Motion analysis; Object detection; Optical resolving power; Pipelines; Trees (mathematics); Autonomous; High resolution camera; Lower resolution; Object Tracking; Quad trees; Super-resolution generative adversarial network; Superresolution; Temporal quad-tree; Unmanned aerial vehicles (UAV)","autonomous; motion tracking; object tracking; Quad-Tree; SRGAN; super-resolution; TQT; UAV","Conference paper","Final","","Scopus","2-s2.0-85131819677"
"Han J.; Wang C.","Han, Jun (57209633331); Wang, Chaoli (57203797020)","57209633331; 57203797020","SSR-TVD: Spatial Super-Resolution for Time-Varying Data Analysis and Visualization","2022","IEEE Transactions on Visualization and Computer Graphics","28","6","","2445","2456","11","10.1109/TVCG.2020.3032123","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127274997&doi=10.1109%2fTVCG.2020.3032123&partnerID=40&md5=9db9b8a33fc698815cfb1f84f205deb4","We present SSR-TVD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of time-varying data (TVD) using adversarial learning. In scientific visualization, SSR-TVD is the first work that applies the generative adversarial network (GAN) to generate high-resolution volumes for three-dimensional time-varying data sets. The design of SSR-TVD includes a generator and two discriminators (spatial and temporal discriminators). The generator takes a low-resolution volume as input and outputs a synthesized high-resolution volume. To capture spatial and temporal coherence in the volume sequence, the two discriminators take the synthesized high-resolution volume(s) as input and produce a score indicating the realness of the volume(s). Our method can work in the in situ visualization setting by downscaling volumetric data from selected time steps as the simulation runs and upscaling downsampled volumes to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-TVD, we show quantitative and qualitative results with several time-varying data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation and a solution solely based on CNN. © 2022 IEEE.","Computer vision; Data visualization; Deep learning; Optical resolving power; Three dimensional computer graphics; Time varying networks; Visualization; Volumetric analysis; Deep learning; High resolution; Learning frameworks; Resolution time; Superresolution; Synthesised; Time-varying data; Time-varying data sets; Time-varying data visualization; Upscaling; article; data analysis; deep learning; quantitative analysis; simulation; Generative adversarial networks","deep learning; generative adversarial network; super-resolution; Time-varying data visualization","Article","Final","","Scopus","2-s2.0-85127274997"
"Lee Y.W.; Kim J.S.; Park K.R.","Lee, Young Won (57195539045); Kim, Jung Soo (57944580900); Park, Kang Ryoung (57735631000)","57195539045; 57944580900; 57735631000","Ocular Biometrics with Low-Resolution Images Based on Ocular Super-Resolution CycleGAN","2022","Mathematics","10","20","3818","","","","10.3390/math10203818","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140794364&doi=10.3390%2fmath10203818&partnerID=40&md5=ae306473ddc0e006279ae37e174a2476","Iris recognition, which is known to have outstanding performance among conventional biometrics techniques, requires a high-resolution camera and a sufficient amount of lighting to capture images containing various iris patterns. To address these issues, research is actively conducted on ocular recognition to include a periocular region in addition to the iris region, which also requires a high-resolution camera to capture images, indicating limited applications due to costs and size limitation. Accordingly, this study proposes an ocular super-resolution cycle-consistent generative adversarial network (OSRCycleGAN) for ocular super-resolution reconstruction, and additionally proposes a method to improve recognition performance in case that ocular images are acquired at a low-resolution. The results of the experiment conducted using open databases, namely, CASIA-iris-Distance and Lamp v4, and IIT Delhi iris database, showed that the equal error rate of recognition of the proposed method was 3.02%, 4.06% and 2.13% for each database, respectively, which outperformed state-of-the-art methods. © 2022 by the authors.","","biometrics; ocular recognition; OSRCycleGAN; super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140794364"
"Narikawa R.; Fukatsu Y.; Wang Z.-L.; Ogawa T.; Adachi Y.; Tanaka Y.; Ishikawa S.","Narikawa, Ryuichi (57458457800); Fukatsu, Yoshihito (57458457900); Wang, Zhi-Lei (56990494900); Ogawa, Toshio (57026354000); Adachi, Yoshitaka (55251588100); Tanaka, Yuji (55736289100); Ishikawa, Shin (56228872500)","57458457800; 57458457900; 56990494900; 57026354000; 55251588100; 55736289100; 56228872500","Generative Adversarial Networks-Based Synthetic Microstructures for Data-Driven Materials Design","2022","Advanced Theory and Simulations","5","5","2100470","","","","10.1002/adts.202100470","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124875537&doi=10.1002%2fadts.202100470&partnerID=40&md5=3958cc1328777811c1c9da023c313d02","To understand the material paradigm, data-driven material design necessitates both microstructural input and output in the form of visual images. Therefore, generative adversarial networks (GAN)-based deep convolutional GAN, cycle-consistent GAN, and super-resolution GAN techniques are used to generate, translate, and improve the quality of microstructural images in this study. The reconstructed virtual microstructural images are realistic and indistinguishable from the real ones. Furthermore, using GAN techniques to reconstruct microstructural image suggests promising ways to design desired microstructures using parameterized descriptors and image augmentation, which are expected to advance data-driven materials research. © 2022 Wiley-VCH GmbH.","Convolution; Generative adversarial networks; Image enhancement; Microstructure; Optical resolving power; Cycle generative adversarial network; Data driven; Data-driven material design; Deep convolutional generative adversarial network; Images reconstruction; Materials design; Microstructural images; Microstructural-image reconstruction; Super-resolution generative adversarial network; Superresolution; Image reconstruction","cycle generative adversarial networks; data-driven material design; deep convolutional generative adversarial networks; microstructural-image reconstruction; super-resolution generative adversarial networks","Article","Final","","Scopus","2-s2.0-85124875537"
"Liu Z.; Zhu H.; Chen Z.","Liu, Ziyu (58087753100); Zhu, Han (57221180146); Chen, Zhenzhong (57985265500)","58087753100; 57221180146; 57985265500","Adversarial Spectral Super-Resolution for Multispectral Imagery Using Spatial Spectral Feature Attention Module","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","","","1","14","13","10.1109/JSTARS.2023.3238853","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147300696&doi=10.1109%2fJSTARS.2023.3238853&partnerID=40&md5=9c1fcd62e9714f69543d9e53282a0dbf","Acquiring high-quality hyperspectral imagery with high spatial and spectral resolution plays an important role in remote sensing. Due to the limited capacity of sensors, providing high spatial and spectral resolution is still a challenging issue. Spectral super-resolution (SSR) increases the spectral dimensionality of multispectral images to achieve resolution enhancement. In this paper, we propose a spectral resolution enhancement method based on the generative adversarial network (GAN) framework without introducing additional spectral responses prior. In order to adaptively rescale informative features for capturing interdependencies in the spectral and spatial dimensions, a spatial spectral feature attention module (SSFAM) is introduced. The proposed method jointly exploits spatio-spectral distribution in the hyperspectral manifold to increase spectral resolution while maintaining spatial content consistency. Experiments are conducted on both synthetic Landsat 8 and Sentinel-2 radiance data and real co-registered ALI and Hyperion (MS and HS) images, which indicates the superiority of the proposed method compared to other state-of-the-art methods. Author","Generative adversarial networks; Hyperspectral imaging; Image enhancement; Remote sensing; Spectral resolution; Adversarial learning; Attention mechanisms; Correlation; Hyper-spectral imageries; Images reconstruction; Spatial resolution; Spectral feature; Spectral super-resolution; Superresolution; Image reconstruction","adversarial learning; attention Mechanism; Cameras; Correlation; Hyperspectral imagery; Hyperspectral imaging; Image reconstruction; Sensors; Spatial resolution; spectral super-resolution; Superresolution","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85147300696"
"Juhong A.; Li B.; Yao C.-Y.; Yang C.-W.; Agnew D.W.; Lei Y.L.; Huang X.; Piyawattanametha W.; Qiu Z.","Juhong, Aniwat (57193696911); Li, Bo (57202239053); Yao, Cheng-You (57211396547); Yang, Chia-Wei (57221419155); Agnew, Dalen W. (8607425400); Lei, Yu Leo (58028478600); Huang, Xuefei (58028312100); Piyawattanametha, Wibool (11640956000); Qiu, Zhen (57202420028)","57193696911; 57202239053; 57211396547; 57221419155; 8607425400; 58028478600; 58028312100; 11640956000; 57202420028","Super-resolution and segmentation deep learning for breast cancer histopathology image analysis","2023","Biomedical Optics Express","14","1","","18","36","18","10.1364/BOE.463839","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144606185&doi=10.1364%2fBOE.463839&partnerID=40&md5=a01ac50fbe2527f15b7aca8e028a9c59","Traditionally, a high-performance microscope with a large numerical aperture is required to acquire high-resolution images. However, the images’ size is typically tremendous. Therefore, they are not conveniently managed and transferred across a computer network or stored in a limited computer storage system. As a result, image compression is commonly used to reduce image size resulting in poor image resolution. Here, we demonstrate custom convolution neural networks (CNNs) for both super-resolution image enhancement from low-resolution images and characterization of both cells and nuclei from hematoxylin and eosin (H&E) stained breast cancer histopathological images by using a combination of generator and discriminator networks so-called super-resolution generative adversarial network-based on aggregated residual transformation (SRGAN-ResNeXt) to facilitate cancer diagnosis in low resource settings. The results provide high enhancement in image quality where the peak signal-to-noise ratio and structural similarity of our network results are over 30 dB and 0.93, respectively. The derived performance is superior to the results obtained from both the bicubic interpolation and the well-known SRGAN deep-learning methods. In addition, another custom CNN is used to perform image segmentation from the generated high-resolution breast cancer images derived with our model with an average Intersection over Union of 0.869 and an average dice similarity coefficient of 0.893 for the H&E image segmentation results. Finally, we propose the jointly trained SRGAN-ResNeXt and Inception U-net Models, which applied the weights from the individually trained SRGAN-ResNeXt and inception U-net models as the pre-trained weights for transfer learning. The jointly trained model’s results are progressively improved and promising. We anticipate these custom CNNs can help resolve the inaccessibility of advanced microscopes or whole slide imaging (WSI) systems to acquire high-resolution images from low-performance microscopes located in remote-constraint settings. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.","Deep learning; Diseases; Generative adversarial networks; Image acquisition; Image compression; Image enhancement; Image resolution; Learning systems; Medical imaging; Microscopes; Signal to noise ratio; eosin; hematoxylin; Breast Cancer; Convolution neural network; Haematoxylin; High-resolution images; Image-analysis; Images segmentations; Net model; Numerical aperture; Performance; Superresolution; accuracy; animal experiment; animal model; animal tissue; Article; breast cancer; controlled study; convolutional neural network; deep learning; female; frequency discrimination; histopathology; image analysis; image enhancement; image quality; image reconstruction; image segmentation; invasive breast cancer; learning; learning algorithm; mouse; noise; nonhuman; signal noise ratio; support vector machine; training; transitional cell carcinoma of the bladder; uterine cervix carcinoma; Image segmentation","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144606185"
"Zhao H.; Peng Y.; Liu B.; Wang Z.; Wang K.","Zhao, Hongshan (55513434800); Peng, Yihao (57240529800); Liu, Bingcong (57224548846); Wang, Zhen (57240216900); Wang, Kui (57239893300)","55513434800; 57240529800; 57224548846; 57240216900; 57239893300","Super-resolution Reconstruction of Electric Equipment's Thermal Imaging Based on Generative Adversarial Network With Edge-attention; [基于边缘注意力生成对抗网络的电力设备热成像超分辨率重建]","2022","Zhongguo Dianji Gongcheng Xuebao/Proceedings of the Chinese Society of Electrical Engineering","42","10","","3564","3572","8","10.13334/j.0258-8013.pcsee.210946","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130835309&doi=10.13334%2fj.0258-8013.pcsee.210946&partnerID=40&md5=b2d28953b92f889eed0462a9ade4b1d1","Infrared photoelectric technology helps humans to surpass visual barriers and intuitively see the temperature distribution on the surface of objects, which is increasingly demanded in the state monitoring of power systems. However, the development of high-resolution thermal imagers and technical barriers limit the visual application of online monitoring of power systems. Super-resolution reconstruction meets the needs of the power Internet of Things while reducing the costs. Focusing on the super-resolution reconstruction of thermal imaging of power equipment, this paper constructed the edge-attention generative adversarial network (EA-GAN), which is based on the Wasserstein generative adversarial network with gradient penalty (WGAN-GP). The residual network module was improved with an edge attention mechanism, and the learning ability of the network was improved with a 1×1 convolutional layer added to the upsampling structure, which enhanced the characterization ability of the network. Experimental results show that the performance of the network has improved in the EA-GAN. The peak signal-to-noise ratio and structural similarity indicators have significantly improved and a better subjective visual has achieved in the overall image and the recovery of image edge attributes. It has high universality in the field of image processing and has high engineering practical value. © 2022 Chin. Soc. for Elec. Eng.","Image enhancement; Image reconstruction; Infrared imaging; Monitoring; Online systems; Optical resolving power; Personnel training; Signal to noise ratio; Attention mechanisms; Edge; High resolution; Infrared photoelectric; Power; State monitoring; Super-resolution reconstruction; Thermal imagers; Thermal images; Thermal-imaging; Generative adversarial networks","Attention mechanism; Edge; Super-resolution reconstruction; Thermal image","Article","Final","","Scopus","2-s2.0-85130835309"
"Zhao Z.; Ren C.; Teng Q.; He X.","Zhao, Zhibo (57419279700); Ren, Chao (57207076112); Teng, Qizhi (7005503530); He, Xiaohai (9237988800)","57419279700; 57207076112; 7005503530; 9237988800","A practical super-resolution method for multi-degradation remote sensing images with deep convolutional neural networks","2022","Journal of Real-Time Image Processing","19","6","","1139","1154","15","10.1007/s11554-022-01245-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138184640&doi=10.1007%2fs11554-022-01245-9&partnerID=40&md5=1fd64565de0db2ce864c451a6ebcfb74","Recent studies have proved that convolutional neural networks (CNNs) have great potential for image super-resolution (SR) tasks. However, most existing methods rely on paired high-resolution (HR) and low-resolution (LR) images to train the CNN, where the LR images are routinely synthesized by applying predefined degradation operations (e.g., bicubic). Because the degradation process of LR images is usually unknown and more complex than those predefined, these methods suffer a significant performance decrease when applied to real-world SR problems. In addition, a deeper and wider network structure enables superior performance while increasing the network parameters and inference time, making it difficult to process real-time data. Inspired by the above motivations, we present an efficient two-step SR method for multi-degradation remote sensing images. Specifically, we first present a novel kernel estimation framework based on generative adversarial networks that can accurately extract the latent blur kernel from the input LR image without any image priors. We then train an efficient SR deep neural network with paired HR and corresponding LR images degraded with the generated kernels. To better balance network parameters and network performance, the densely connected attention mechanism and multi-scale feature extract blocks are introduced in the SR network by increasing the flow of feature information within the network. Extensive experiments indicate that the proposed method outperforms current methods with desired network parameters and complexity, making it feasible to enable real-time image processing. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Complex networks; Convolution; Convolutional neural networks; Deep neural networks; Image processing; Optical resolving power; Remote sensing; Convolutional neural network; High resolution; Kernel models; Low resolution images; Multi-degradation; Network parameters; Performance; Remote sensing images; Superresolution; Superresolution methods; Generative adversarial networks","Convolutional neural network; Kernel modeling; Multi-degradation; Remote sensing image; Super-resolution","Article","Final","","Scopus","2-s2.0-85138184640"
"Jing B.; Ding H.; Yang Z.; Li B.; Liu Q.","Jing, Beibei (57225923498); Ding, Hongwei (35995295900); Yang, Zhijun (57188805913); Li, Bo (57206245809); Liu, Qianlin (35220172100)","57225923498; 35995295900; 57188805913; 57206245809; 35220172100","Image generation step by step: animation generation-image translation","2022","Applied Intelligence","52","7","","8087","8100","13","10.1007/s10489-021-02835-z","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117288407&doi=10.1007%2fs10489-021-02835-z&partnerID=40&md5=5ce95e0ff297014c19ec8b3736ca1c12","Generative adversarial networks play an important role in image generation, but the successful generation of high-resolution images from complex data sets remains a challenging goal. In this paper, we propose the LGAN (Link Generative Adversarial Networks) model, which can effectively enhance the quality of the synthesized images. The LGAN model consists of two parts, G1 and G2. G1 is responsible for the unconditional generation part, which generates anime images with highly abstract features containing few coefficients but continuous image elements covering the overall image features. Moreover, G2 is responsible for the conditional generation part (image translation), consisting of mapping and Superresolution networks. The mapping network fills the output of G1 into the real-world image after semantic segmentation or edge detection processing; the Superresolution network super-resolves the actual picture after completing mapping to improve the image’s resolution. In the comparison test with WGAN, SAGAN, WGAN-GP and PG-GAN, this paper’s LGAN(SEG) leads 64.36 and 12.28, respectively, fully proving the model’s superiority. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Edge detection; Image enhancement; Image segmentation; Mapping; Optical resolving power; Semantics; Animation generation; Anime image conditional generation part; Conditional generation; Image generations; Image translation; Link generative adversarial network; Network models; Super-resolution network; Superresolution; Unconditional generation part; Generative adversarial networks","Anime images conditional generation part; Generative adversarial networks; LGAN (Link Generative Adversarial Networks); Super-resolution network; Unconditional generation part","Article","Final","","Scopus","2-s2.0-85117288407"
"Huang L.; Huang Y.","Huang, Li (58097912900); Huang, Yaping (33967661200)","58097912900; 33967661200","DRGAN: A dual resolution guided low-resolution image inpainting","2023","Knowledge-Based Systems","264","","110346","","","","10.1016/j.knosys.2023.110346","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147793224&doi=10.1016%2fj.knosys.2023.110346&partnerID=40&md5=395d91615c10289376d854df9d56dda3","Although image inpainting is a challenging task in computer vision, most existing image inpainting methods have achieved remarkable progress. However, occlusion and low resolution often appear on one image simultaneously in the real world, and only a few methods can address low-resolution image inpainting. To tackle this problem, we propose a novel method called the Dual Resolution Generative Adversarial Network (DRGAN), which formulates low-resolution (LR) image inpainting as a constrained image generation problem. The proposed DRGAN first trains a generative network to obtain high-quality images from noise vectors. Then a dual-resolution loss is designed to effectively integrate both high- and low-resolution information to optimize the input vectors initialized by random noise to generate high-quality images. Compared with the model that learns the mapping function from low- to high-quality images, the proposed model maps input noise vectors to images and can thus adapt more effectively to various types of degradation and large occlusions. Extensive experiments on synthetic LR data and real-world images demonstrate the effectiveness of the proposed DRGAN. © 2023 Elsevier B.V.","Image processing; Pattern recognition; Dual resolutions; GAN; High quality images; Image Inpainting; Image super resolutions; Inpainting method; Low resolution images; Lower resolution; Noise vectors; Real-world; Generative adversarial networks","GANs; Image inpainting; Image super-resolution","Article","Final","","Scopus","2-s2.0-85147793224"
"Yuan B.; Cao P.","Yuan, Bo (58042071000); Cao, Peng (57203197650)","58042071000; 57203197650","Research on Image Information Restoration Algorithm of Printing Micro Dots Based on GAN","2022","ACM International Conference Proceeding Series","","","","615","622","7","10.1145/3569966.3571169","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145602868&doi=10.1145%2f3569966.3571169&partnerID=40&md5=479fe576c0b5dfcabb256c2501dffc21","During printing and shooting, the degradation of printing micro dots significantly affects the decoding and reading of hidden anti-counterfeiting information. However, existing image restoration methods cannot effectively restore image information. Moreover, there are relatively few datasets related to halftone dot images, and most datasets differ from the real data. Therefore, we propose an end-to-end restoration model based on the single-image super-resolution information. Specifically, we constructed a PMD dataset for real printing of anti-counterfeiting scenes. Based on this dataset, we used the high-resolution image information as the target. The positional inclination of the degraded images is corrected using the blank and interline characteristics of the printing micro dots images. The restoration is completed with the help of feature extraction and upsample of ESRGAN. In addition, we propose evaluation measures suitable for error detection, correction, and decoding requirements for microscopic image information. The experimental results show that, within the noise tolerance range, the image information restored by our method has a maximum average bit error rate is 0.97% and a Euclidean distance is 0.00804 pixels, whereas traditional filtering measures cannot effectively restore image information. The experimental results verified the effectiveness and robustness of the proposed method. © 2022 ACM.","Bit error rate; Decoding; Deep learning; Generative adversarial networks; Image reconstruction; Information filtering; Anti-counterfeiting; Bitmap images; Deep learning; Halftone dots; Image information; Image information restoration; Printing anti-counterfeiting; Restoration algorithm; Restoration methods; Restore image; Restoration","bitmap image; deep learning; generative adversarial networks; image information restoration; printing anti-counterfeiting","Conference paper","Final","","Scopus","2-s2.0-85145602868"
"Han S.; Remedios S.W.; Schär M.; Carass A.; Prince J.L.","Han, Shuo (57202847922); Remedios, Samuel W. (57201854184); Schär, Michael (14065126500); Carass, Aaron (15061054500); Prince, Jerry L. (56600943200)","57202847922; 57201854184; 14065126500; 15061054500; 56600943200","ESPRESO: An algorithm to estimate the slice profile of a single magnetic resonance image","2023","Magnetic Resonance Imaging","98","","","155","163","8","10.1016/j.mri.2023.01.012","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147274110&doi=10.1016%2fj.mri.2023.01.012&partnerID=40&md5=5ca536f76b28410ad268b38daf2c7803","To reduce scan time, magnetic resonance (MR) images are often acquired using 2D multi-slice protocols with thick slices that may also have gaps between them. The resulting image volumes have lower resolution in the through-plane direction than in the in-plane direction, and the through-plane resolution is in part characterized by the protocol's slice profile which acts as a through-plane point spread function (PSF). Although super-resolution (SR) has been shown to improve the visualization and down-stream processing of 2D multi-slice MR acquisitions, previous algorithms are usually unaware of the true slice profile, which may lead to sub-optimal SR performance. In this work, we present an algorithm to estimate the slice profile of a 2D multi-slice acquisition given only its own image volume without any external training data. We assume that an anatomical image is isotropic in the sense that, after accounting for a correctly estimated slice profile, the image patches along different orientations have the same probability distribution. Our proposed algorithm uses a modified generative adversarial network (GAN) where the generator network estimates the slice profile to reduce the resolution of the in-plane direction, and the discriminator network determines whether a direction is generated or real low resolution. The proposed algorithm, ESPRESO, which stands for “estimating the slice profile for resolution enhancement of a single image only”, was tested with a state-of-the-art internally supervised SR algorithm. Specifically, ESPRESO is used to create training data for this SR algorithm, and results show improvements when ESPRESO is used over commonly-used PSFs. © 2023","Algorithms; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Phantoms, Imaging; Radionuclide Imaging; algorithm; image processing; imaging phantom; nuclear magnetic resonance imaging; procedures; scintiscanning","GAN; MRI; Slice profile; Super-resolution","Article","Final","","Scopus","2-s2.0-85147274110"
"Sui Y.-H.; Guo X.-Y.; Yu J.-J.; Solovev A.A.; Ta D.-A.; Xu K.-L.","Sui, Yi-Hui (57413494500); Guo, Xing-Yi (57413775300); Yu, Jun-Jin (57735741100); Solovev, Alexander A. (57202972309); Ta, De-An (57610009100); Xu, Kai-Liang (34875930800)","57413494500; 57413775300; 57735741100; 57202972309; 57610009100; 34875930800","Accelerating super-resolution ultrasound localization microscopy using generative adversarial net; [生成对抗网络加速超分辨率超声定位显微成像方法研究]","2022","Wuli Xuebao/Acta Physica Sinica","71","22","224301","","","","10.7498/aps.71.20220954","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143376698&doi=10.7498%2faps.71.20220954&partnerID=40&md5=3cf3d2c5eea8a84ec50efb456f6a1a8a","Ultrafast ultrasound localization microscopy (uULM) has broken through the fundamental acoustic diffraction limit by accumulating thousands of sub-wavelength microbubble localisation points and improved the spatial resolution by more than one order of magnitude, which is conducive to clinical diagnosis. By localizing individually injected microbubbles and tracking their movement with a subwavelength resolution, the vasculature microscopy can be achieved with micrometer scale. However, the reconstruction of a uULM image often requires tens or even hundreds of seconds of continuous long-range image acquisition, which limits its clinical application. In order to solve this problem, a generative adversarial network (GAN) based deep learning method is proposed to reconstruct the super-resolution ultrasound localization microscopy. In vivo uULM ultrasound datasets are used to train the network to reconstruct dense vascular networks via localized microbubbles. This approach is validated by using another in-vivo dataset obtained in a rat brain. Results show that GAN based ultrafast ultrasound localization microscopy (GAN-uULM) can resolve micro vessels smaller than 10 µm. Besides, GAN-uULM is able to distinguish small vessels that cannot be continuously reconstructed by using a standard uULM reconstruction method. Saturation parameter based on counting the number of explored pixels is used to evaluate the reconstruction quality. The proposed reconstruction approach reduces the data requirement by half and thus significantly accelerates the uULM imaging. It is illustrasted that for a dataset of 292 s ultrafast acquisition, the saturation of standard uULM image is 33%, while that of GAN-uULM can reach 46%. Fourier ring correlation (FRC) method is utilized to measure the spatial resolution in uULM. Resolutions of the images obtained by standard uULM and GAN-ULM are 7.8 µm and 8.9 µm, respectively. In conclusion, the developed deep learning model is able to connect trajectories with less computational complexity and avoids manual tuning and trajectory screening, providing an effective solution for accelerating ultrasound localization microscopy. © 2022 Chinese Physical Society.","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Diffraction; Image resolution; Learning systems; Ultrasonics; Convolutional neural network; Microbubbles; Microscopy images; Network-based; Spatial resolution; Superresolution; Ultra-fast; Ultrasonic localization; Ultrasonic localization microscopy; Ultrasound localization; Generative adversarial networks","convolutional neural network; generative adversarial network; super-resolution; ultrasonic localization microscopy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143376698"
"Pandey G.; Ghanekar U.","Pandey, Garima (57201527252); Ghanekar, Umesh (23994982000)","57201527252; 23994982000","A Conspectus of Deep Learning Techniques for Single-Image Super-Resolution","2022","Pattern Recognition and Image Analysis","32","1","","11","32","21","10.1134/S1054661822010059","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126819848&doi=10.1134%2fS1054661822010059&partnerID=40&md5=8f137f173f241432687bc0f1209d14b2","Abstract: Single image super-resolution (SISR) is one of the contemporary research areas in the field of image restoration that involves solving an ill-posed inverse equation. A rich profusion of techniques has been proposed in the past four decades. However, the expansion of deep learning (DL) in recent years has improved image reconstruction drastically. In this paper, a compendium of DL applications in the SISR field has been provided, mainly focusing on inspection of the latest advancements and categorization. For completeness of the study, different aspects of DL based SISR are included in brief with a synoptic study on the available image datasets. In conclusion, issues existing in DL-based SISR and viable solutions for them are proffered. © 2022, Pleiades Publishing, Ltd.","Convolutional neural networks; Deep learning; Image enhancement; Image reconstruction; Optical resolving power; Convolutional neural network; Deep learning; Ill posed; Image database; Image super resolutions; Images reconstruction; Learning techniques; Research areas; Single image super-resolution; Single images; Generative adversarial networks","convolutional neural network; deep learning; generative adversarial network; image database; single image super-resolution","Article","Final","","Scopus","2-s2.0-85126819848"
"Jiang P.; Liu J.; Wu L.; Xu L.; Hu J.; Zhang J.; Zhang Y.; Yang X.","Jiang, Pengfei (57215331516); Liu, Jianlong (54898410700); Wu, Long (55714357200); Xu, Lu (56081135000); Hu, Jiemin (25959600800); Zhang, Jianlong (12646581200); Zhang, Yong (7601319872); Yang, Xu (56941044900)","57215331516; 54898410700; 55714357200; 56081135000; 25959600800; 12646581200; 7601319872; 56941044900","Fourier single pixel imaging reconstruction method based on the U-net and attention mechanism at a low sampling rate","2022","Optics Express","30","11","","18638","18654","16","10.1364/OE.457551","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130484402&doi=10.1364%2fOE.457551&partnerID=40&md5=61e8d864eb1161c600278b111f223425","There exists the contradiction between imaging efficiency and imaging quality for Fourier single-pixel imaging (FSI). Although the deep learning approaches have solved this problem to some extent, the reconstruction quality at low sampling rate is still not enough to meet the practical requirements. To solve this problem, inspired by the idea of super-resolution, this paper proposes the paralleled fusing of the U-net and attention mechanism to improve the quality of FSI reconstruction at a low sampling rate. This paper builds a generative adversarial network structure to achieve recovery of high-resolution target images from low-resolution FSI reconstruction results under low sampling rate conditions. Compared with conventional FSI and other deep learning methods based on FSI, the proposed method can get better quality and higher resolution results at low sampling rates in simulation and experiments. This approach is particularly important to high-speed Fourier single pixel imaging applications. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement","Fourier transforms; Generative adversarial networks; Pixels; Attention mechanisms; Fourier; Imaging efficiency; Imaging quality; Imaging reconstruction; Learning approach; Reconstruction method; Reconstruction quality; Sampling rates; Single pixel; article; attention; controlled study; deep learning; simulation; velocity; Deep learning","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130484402"
"Xie N.; Ding Y.; Li M.; Liu Y.; Lyu R.; Yan T.","Xie, Ningyu (57212031317); Ding, Yuyang (57212036761); Li, Mingyue (57222316286); Liu, Yuan (35262605500); Lyu, Ruimin (55926774800); Yan, Tao (57206514225)","57212031317; 57212036761; 57222316286; 35262605500; 55926774800; 57206514225","Light field image re-focusing based on conditional generative adversarial networks leverage; [利用条件生成对抗网络的光场图像重聚焦]","2022","Journal of Image and Graphics","27","4","","1056","1065","9","10.11834/jig.200471","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129767231&doi=10.11834%2fjig.200471&partnerID=40&md5=2c91c24b83b768aab94c9e1c3a94cc4a","Objective: Light field images like rich spatial and angular information are widely used in computer vision applications. Light field information application can significantly improve the visual effect based on the focal plane and depth of field of an image. The current methods can be divided into two categories as mentioned below: One of the categories increases the angular resolution of a light field image via light field reconstruction. Since aliasing phenomenon is derived of disparity amongst light-field-images-based of the sub-aperture views. These methods require high computational costs and may introduce color errors or other artifacts. In addition, these methods can just improve the quality of refocusing straightforward under original focus plane and depth of field. Another category illustrates various filters derived of the circle of confusion (COC) map to defocus/render the center sub-aperture view to produce bokeh rendering effect. A rough defocusing visual effect can obtained. This above category has low computational cost and can sort both the focus plane and depth of field out. Deep convolutional neural network (DCNN) has its priority in bokeh rendering. To this end, we facilitate a novel conditional generative adversarial network based (C-GAN-based) for bokeh rendering. Method: Our analysis takes a light field image as input. It contains three aspects as following: First, it calculates the COC map with different focal planes and depths of field derived of the disparity map for the input light field image estimation. The obtained COC map and the central sub-view of the light field image are fed into the generator of the conditional GAN. Next, the generator processes two input data each based on two four-layer encoders in order to integrate two-encoders-based features extraction, which add the four consecutive residual modules. At the end, the acquired refocused image is melted into the discriminator to identify that the obtained refocused image corresponding to the COC map. To enhance the high-frequency details of the refocused/rendered image, we adopt a pre-trained Visual Geometry Group 16-layer (VGG-16) network to calculate the style loss and the perceptual loss. L1 loss is used as the loss of the generator, and the discriminator adopts the cross-entropy loss. The Blender is used to adjust the position of focus planes and depths of field and render corresponding light field images. A digital single lens reflex(DSLR) camera plug-in tool of the Blender is used to render the corresponding refocused images as the ground truth. Our network is implemented based on the Keras framework. The input and output sizes of the network are both 512 × 512 × 3. The network is trained on a Titan XP GPU card. The number of epochs for training our targeted neural network is set to 3 500. The initial learning rate is set to 0.000 2. The training process took about 28 hours. Result: Our synthetic dataset and the real-world dataset are compared with similar algorithms, including current refocusing algorithms, three different light field reconstruction algorithms, and defocusing algorithm using anisotropic filtering with COC map. Our quantitative analysis uses the peak signal to noise ratio (PSNR) and structural similarity (SSIM) for evaluation. Our proposed network-structure-based qualitative evaluation can obtain refocused images with different focus planes and depths of field in terms of the input COC map analysis. In the process of quantitative analysis, our average PSNR obtained is 1.82 dB. The average SSIM was improved by 0.02. Compared with the methods that use COC map and anisotropic filtering, our average PSNR was improved 7.92 dB and the average SSIM is improved 0.08. The methods had achieved poor PSNR values in the context of reconstruction/super-resolution due to the chromatic aberration of the generated sub-views. Conclusion: Our algorithm can generate the disparity-map-based corresponding COC map obtained from the input light field image, refocusing plane and depth of field. To produce the corresponding refocused image, our conditional generative adversarial network demonstration can perform bokeh rendering on the central sub-view image based on differentiate COC map. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Bokeh rendering; Circle of confusion (COC); Conditional generative adversarial networks; Image refocusing; Light field","Article","Final","","Scopus","2-s2.0-85129767231"
"Yu L.; Yousif M.Z.; Zhang M.; Hoyas S.; Vinuesa R.; Lim H.-C.","Yu, Linqi (57271595700); Yousif, Mustafa Z. (57221872306); Zhang, Meng (57550023700); Hoyas, Sergio (6508374001); Vinuesa, Ricardo (55553485100); Lim, Hee-Chang (57728736600)","57271595700; 57221872306; 57550023700; 6508374001; 55553485100; 57728736600","Three-dimensional ESRGAN for super-resolution reconstruction of turbulent flows with tricubic interpolation-based transfer learning","2022","Physics of Fluids","34","12","125126","","","","10.1063/5.0129203","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144610431&doi=10.1063%2f5.0129203&partnerID=40&md5=d66e0ca70ef7a816604317ca9e0e5cc0","Turbulence is a complicated phenomenon because of its chaotic behavior with multiple spatiotemporal scales. Turbulence also has irregularity and diffusivity, making predicting and reconstructing turbulence more challenging. This study proposes a deep-learning approach to reconstruct three-dimensional (3D) high-resolution turbulent flows from spatially limited data using a 3D enhanced super-resolution generative adversarial networks (3D-ESRGAN). In addition, a novel transfer-learning method based on tricubic interpolation is employed. Turbulent channel flow data at friction Reynolds numbers R e τ = 180 and R e τ = 500 were generated by direct numerical simulation (DNS) and used to estimate the performance of the deep-learning model as well as that of tricubic interpolation-based transfer learning. The results, including instantaneous velocity fields and turbulence statistics, show that the reconstructed high-resolution data agree well with the reference DNS data. The findings also indicate that the proposed 3D-ESRGAN can reconstruct 3D high-resolution turbulent flows even with limited training data.  © 2022 Author(s).","Deep learning; Generative adversarial networks; Image reconstruction; Internet protocols; Interpolation; Learning systems; Optical resolving power; Reynolds number; Turbulent flow; Chaotic behaviour; High resolution; Learning approach; Limited data; Spatio-temporal scale; Super-resolution reconstruction; Superresolution; Transfer learning; Transfer learning methods; Tricubic interpolation; Turbulence","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85144610431"
"Ren K.; Gao Y.; Wan M.; Gu G.; Chen Q.","Ren, Kan (55877072700); Gao, Yuan (57872425900); Wan, Minjie (57188754227); Gu, Guohua (7203056000); Chen, Qian (57192659447)","55877072700; 57872425900; 57188754227; 7203056000; 57192659447","Infrared small target detection via region super resolution generative adversarial network","2022","Applied Intelligence","52","10","","11725","11737","12","10.1007/s10489-021-02955-6","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123828280&doi=10.1007%2fs10489-021-02955-6&partnerID=40&md5=d74d44c471c4ced19ad2171e38924d6f","Infrared small target detection has always been a difficult problem in the field of object detection. The main reason affecting the accuracy is that the small infrared target has fewer pixels and weaker features. The current optimization methods for the small target are mainly based on multi-scale feature fusion or super-resolution enhancement. When super-resolution networks are applied to infrared target detection, there are still two non-negligible problems: first, the super-resolution structure will consume too much arithmetic power, resulting in a low detection rate, and second, the low-resolution images characterizing small targets are usually obtained by downsampling with high-resolution images during training, which is different from the distribution of tiny target in actual detection applications, resulting in poor detection accuracy. We propose a new detection network to solve the above problem: Region Super Resolution Generative Adversarial Network(RSRGAN). It contains a simple structured network, Region Context Network(RCN) as the backbone that consumes less computational cost to extract the possible regions. The generator of the Generative Adversarial Network(GAN) includes two modules: distribution transformation and super-resolution enhancement. First, the blurred infrared small target is converted into a clear target with similar distribution as the training set. Then the resolution is increased, which can achieve a better enhancement effect. The discriminator distinguishes whether the input comes from the generator or the actual image to assist in generating a better super-resolution image. Meanwhile, we produced an infrared Unmanned Aerial Vehicle(UAV) small target dataset, target pixels below 20*20, containing birds, leaves, and other similar disturbances, which is more challenging for the detection algorithm. Our method proves better detection of small IR targets and shows superior performance over state-of-the-art methods through experiments. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Aircraft detection; Antennas; Object detection; Optical resolving power; Pixels; Unmanned aerial vehicles (UAV); Current optimization; Infrared small targets; Infrared target; Region context network; Region super resolution; Resolution enhancement; Small target detection; Superresolution; Targets detection; Generative adversarial networks","Generative adversarial network; RCN; Region super resolution; Target detection","Article","Final","","Scopus","2-s2.0-85123828280"
"Scherrer R.; Quiniou T.; Jauffrais T.; Lemonnier H.; Bonnet S.; Selmaoui-Folcher N.","Scherrer, Romane (57989419500); Quiniou, Thomas (8324123600); Jauffrais, Thierry (43461363600); Lemonnier, Hugues (56263662000); Bonnet, Sophie (57989419600); Selmaoui-Folcher, Nazha (57260506500)","57989419500; 8324123600; 43461363600; 56263662000; 57989419600; 57260506500","Holographic reconstruction enhancement via unpaired image-to-image translation","2022","Applied Optics","61","33","","9807","9816","9","10.1364/AO.471131","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143127566&doi=10.1364%2fAO.471131&partnerID=40&md5=23ff9013aef6abd38beb7f1efc3bad4b","Digital holographic microscopy is an imaging process that encodes the 3D information of a sample into a single 2D hologram. The holographic reconstruction that decodes the hologram is conventionally based on the diffraction formula and involves various iterative steps in order to recover the lost phase information of the hologram. In the past few years, the deep-learning-based model has shown great potential to perform holographic reconstruction directly on a single hologram. However, preparing a large and high-quality dataset to train the models remains a challenge, especially when the holographic reconstruction images that serve as ground truth are difficult to obtain and can have a deteriorated quality due to various interferences of the imaging device. A cycle generative adversarial network is first trained with unpaired brightfield microscope images to restore the visual quality of the holographic reconstructions. The enhanced holographic reconstructions then serve as ground truth for the supervised learning of a U-Net that performs the holographic reconstruction on a single hologram. The proposed method was evaluated on plankton images and could also be applied to achieve super-resolution or colorization of the holographic reconstructions. © 2022 Optica Publishing Group.","Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; Iterative methods; Large dataset; Microscopic examination; 3D information; Digital holographic microscopy; Ground truth; High quality; Holographic reconstruction; Image translation; Imaging process; Learning Based Models; Phase information; Reconstruction image; Holograms","","Article","Final","","Scopus","2-s2.0-85143127566"
"Aria M.; Hashemzadeh M.; Farajzadeh N.","Aria, Mehrad (57223269041); Hashemzadeh, Mahdi (55579430200); Farajzadeh, Nacer (14059779000)","57223269041; 55579430200; 14059779000","QDL-CMFD: A Quality-independent and deep Learning-based Copy-Move image forgery detection method","2022","Neurocomputing","511","","","213","236","23","10.1016/j.neucom.2022.09.017","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138083561&doi=10.1016%2fj.neucom.2022.09.017&partnerID=40&md5=9d7a83c6551817977bcc281587b8d908","One of the prevalent methods of image forgery is copy-move, where one or more regions of an image are duplicated and moved elsewhere in the image. It is usually difficult to detect this type of forgery due to the similarity of the copied and forged areas. Also, forgers perform pre-processing and/or post-processing operations on the manipulated regions to make it even more difficult to detect. In this study, an image quality-independent method based on deep learning approach, termed QDL-CMFD, is presented for detecting this type of forgery. QDL-CMFD utilizes generative adversarial networks for image quality enhancement, and convolutional neural networks (CNN) for forgery detection. A tailored dual-branch CNN architecture is introduced consisting of two subnetworks, namely a manipulation detection subnetwork and a similarity detection subnetwork. Accordingly, unlike most existing methods, QDL-CMFD is able to simultaneous detection of several forged areas, as well as determining the source and target of the forgery. Also, QDL-CMFD is robust against various pre-processing/post-processing attacks. It shows excellent performance for detecting low-quality forged images and small areas. Experiments conducted on the CASIA and CoMoFoD benchmark datasets confirm that QDL-CMFD performs significantly better than the competitors. All the implementation source codes of QDL-CMFD are available at https://github.com/MehradAria/QDL-CMFD. © 2022 Elsevier B.V.","Convolution; Convolutional neural networks; Deep learning; Image enhancement; Image quality; Convolutional neural network; Copy moves; Copy-move forgeries; Deep learning; Image forgery detections; Pre-processing; Resolution enhancement; Subnetworks; Super-resolution enhancement; Superresolution; article; convolutional neural network; deep learning; forgery; image quality; Generative adversarial networks","Convolutional neural Networks; Copy-move forgery; Deep learning; Generative adversarial networks; Image forgery detection; Super-resolution enhancement","Article","Final","","Scopus","2-s2.0-85138083561"
"Zhou Z.; Ma A.; Feng Q.; Wang R.; Cheng L.; Chen X.; Yang X.; Liao K.; Miao Y.; Qiu Y.","Zhou, Zhiyi (57193861822); Ma, Anbang (57891514900); Feng, Qiuting (57891290400); Wang, Ran (57799315100); Cheng, Lilin (57211346732); Chen, Xin (57220963158); Yang, Xi (57551970200); Liao, Keman (57204716062); Miao, Yifeng (35337252900); Qiu, Yongming (55552950800)","57193861822; 57891514900; 57891290400; 57799315100; 57211346732; 57220963158; 57551970200; 57204716062; 35337252900; 55552950800","Super-resolution of brain tumor MRI images based on deep learning","2022","Journal of Applied Clinical Medical Physics","23","11","e13758","","","","10.1002/acm2.13758","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138017512&doi=10.1002%2facm2.13758&partnerID=40&md5=866244500a0e70f4d92a1091fa877f3c","Introduction: To explore and evaluate the performance of MRI-based brain tumor super-resolution generative adversarial network (MRBT-SR-GAN) for improving the MRI image resolution in brain tumors. Methods: A total of 237 patients from December 2018 and April 2020 with T2-fluid attenuated inversion recovery (FLAIR) MR images (one image per patient) were included in the present research to form the super-resolution MR dataset. The MRBT-SR-GAN was modified from the enhanced super-resolution generative adversarial networks (ESRGAN) architecture, which could effectively recover high-resolution MRI images while retaining the quality of the images. The T2-FLAIR images from the brain tumor segmentation (BRATS) dataset were used to evaluate the performance of MRBT-SR-GAN contributed to the BRATS task. Results: The super-resolution T2-FLAIR images yielded a 0.062 dice ratio improvement from 0.724 to 0.786 compared with the original low-resolution T2-FLAIR images, indicating the robustness of MRBT-SR-GAN in providing more substantial supervision for intensity consistency and texture recovery of the MRI images. The MRBT-SR-GAN was also modified and generalized to perform slice interpolation and other tasks. Conclusions: MRBT-SR-GAN exhibited great potential in the early detection and accurate evaluation of the recurrence and prognosis of brain tumors, which could be employed in brain tumor surgery planning and navigation. In addition, this technique renders precise radiotherapy possible. The design paradigm of the MRBT-SR-GAN neural network may be applied for medical image super-resolution in other diseases with different modalities as well. © 2022 The Authors. Journal of Applied Clinical Medical Physics published by Wiley Periodicals, LLC on behalf of The American Association of Physicists in Medicine.","Brain Neoplasms; Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; brain tumor; diagnostic imaging; human; image processing; nuclear magnetic resonance imaging; procedures","brain tumor; generative adversarial network; magnetic resonance imaging; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138017512"
"Zhang C.; Zhang Z.; Deng Y.; Zhang Y.; Chong M.; Tan Y.; Liu P.","Zhang, Chongqi (57729323800); Zhang, Ziwen (58068354400); Deng, Yao (58068364200); Zhang, Yueyi (58068383700); Chong, Mingzhe (57423462600); Tan, Yunhua (56023661500); Liu, Pukun (58068349800)","57729323800; 58068354400; 58068364200; 58068383700; 57423462600; 56023661500; 58068349800","Blind Super-Resolution for SAR Images with Speckle Noise Based on Deep Learning Probabilistic Degradation Model and SAR Priors","2023","Remote Sensing","15","2","330","","","","10.3390/rs15020330","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146399620&doi=10.3390%2frs15020330&partnerID=40&md5=fb6a2a238b5c09754d3db8f38bd775f3","As an active microwave coherent imaging technology, synthetic aperture radar (SAR) images suffer from severe speckle noise and low-resolution problems due to the limitations of the imaging system, which cause difficulties in image interpretation and target detection. However, the existing SAR super-resolution (SR) methods usually reconstruct the images by a determined degradation model and hardly consider multiplicative speckle noise, meanwhile, most SR models are trained with synthetic datasets in which the low-resolution (LR) images are down-sampled from their high-resolution (HR) counterparts. These constraints cause a serious domain gap between the synthetic and real SAR images. To solve the above problems, this paper proposes an unsupervised blind SR method for SAR images by introducing SAR priors in a cycle-GAN framework. First, a learnable probabilistic degradation model combined with SAR noise priors was presented to satisfy various SAR images produced from different platforms. Then, a degradation model and a SR model in a unified cycle-GAN framework were trained simultaneously to learn the intrinsic relationship between HR–LR domains. The model was trained with real LR and HR SAR images instead of synthetic paired images to conquer the domain gap. Finally, experimental results on both synthetic and real SAR images demonstrated the high performance of the proposed method in terms of image quality and visual perception. Additionally, we found the proposed SR method demonstrates the tremendous potential for target detection tasks by reducing missed detection and false alarms significantly. © 2023 by the authors.","Deep learning; Image resolution; Radar imaging; Speckle; Synthetic aperture radar; Blind Super-resolution; Cycle-generative adversarial network; Degradation model; Generative adversarial network; Lower resolution; Probabilistic degradation model; Probabilistics; Synthetic aperture radar; Synthetic aperture radar images; Generative adversarial networks","blind super-resolution (SR); cycle-GAN; generative adversarial networks (GAN); probabilistic degradation model; synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146399620"
"Ma H.; Liu D.; Wu F.","Ma, Haichuan (57200651920); Liu, Dong (56597085700); Wu, Feng (7403465570)","57200651920; 56597085700; 7403465570","Rectified Wasserstein Generative Adversarial Networks for Perceptual Image Restoration","2023","IEEE Transactions on Pattern Analysis and Machine Intelligence","45","3","","3648","3663","15","10.1109/TPAMI.2022.3185316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133623143&doi=10.1109%2fTPAMI.2022.3185316&partnerID=40&md5=b60a5a559729e406b51a968965da097a","Wasserstein generative adversarial network (WGAN) has attracted great attention due to its solid mathematical background, i.e., to minimize the Wasserstein distance between the generated distribution and the distribution of interest. In WGAN, the Wasserstein distance is quantitatively evaluated by the discriminator, also known as the critic. The vanilla WGAN trained the critic with the simple Lipschitz condition, which was later shown less effective for modeling complex distributions, like the distribution of natural images. We try to improve the WGAN training by introducing pairwise constraint on the critic, oriented to image restoration tasks. In principle, pairwise constraint is to suggest the critic assign a higher rating to the original (real) image than to the restored (generated) image, as long as such a pair of images are available. We show that such pairwise constraint may be implemented by rectifying the gradients in WGAN training, which leads to the proposed rectified Wasserstein generative adversarial network (ReWaGAN). In addition, we build interesting connections between ReWaGAN and the perception-distortion tradeoff. We verify ReWaGAN on two representative image restoration tasks: single image super-resolution (4× and 8×) and compression artifact reduction, where our ReWaGAN not only beats the vanilla WGAN consistently, but also outperforms the state-of-the-art perceptual quality-oriented methods significantly. Our code and models are publicly available at https://github.com/mahaichuan/ReWaGAN.  © 1979-2012 IEEE.","Computer vision; Generative adversarial networks; Image enhancement; Job analysis; Optical resolving power; Personnel training; Restoration; Artefact reduction; Compression artifact reduction; Compression artifacts; Generative adversarial network; Generator; Image super resolutions; Rectified wasserstein generative adversarial network; Superresolution; Task analysis; Wasserstein generative adversarial network; Image reconstruction","Compression artifact reduction; generative adversarial network (GAN); image restoration; image super-resolution; rectified Wasserstein GAN (ReWaGAN); Wasserstein GAN (WGAN)","Article","Final","","Scopus","2-s2.0-85133623143"
"Shang C.; Jiang S.; Ling F.; Li X.; Zhou Y.; Du Y.","Shang, Cheng (57209801137); Jiang, Shan (57198704721); Ling, Feng (56278268300); Li, Xiaodong (55878368700); Zhou, Yadong (57207472820); Du, Yun (56420121700)","57209801137; 57198704721; 56278268300; 55878368700; 57207472820; 56420121700","Spectral-Spatial Generative Adversarial Network for Super-Resolution Land Cover Mapping With Multispectral Remotely Sensed Imagery","2023","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16","","","522","537","15","10.1109/JSTARS.2022.3228741","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144774026&doi=10.1109%2fJSTARS.2022.3228741&partnerID=40&md5=3284eca3eb21b450a28c16cc8502de49","Super-resolution mapping (SRM) can effectively predict the spatial distribution of land cover classes within mixed pixels at a higher spatial resolution than the original remotely sensed imagery. The uncertainty of land cover fraction errors within mixed pixels is one of the most important factors affecting SRM accuracy. Studies have shown that SRM methods using deep learning techniques have significantly improved land cover mapping accuracy but have not coped well with spectral-spatial errors. This study proposes an end-to-end SRM model using a spectral-spatial generative adversarial network (SGS) with the direct input of multispectral remotely sensed imagery, which deals with spectral-spatial error. The proposed SGS comprises the following three parts: first, cube-based convolution for spectral unmixing is adopted to generate land cover fraction images. Second, a residual-in-residual dense block fully and jointly considers spectral and spatial information and reduces spectral errors. Third, a relativistic average GAN is designed as a backbone to further improve the super-resolution performance and reduce spectral-spatial errors. SGS was tested in one synthetic and two realistic experiments with multi/hyperspectral remotely sensed imagery as the input, comparing the results with those of hard classification and several classic SRM methods. The results showed that SGS performed well at reducing land cover fraction errors, reconstructing spatial details, removing unpleasant and unrealistic land cover artifacts, and eliminating false recognition.  © 2008-2012 IEEE.","Deep learning; Errors; Generative adversarial networks; Image resolution; Photomapping; Pixels; Remote sensing; Deep learning; Distribution-functions; GraphicaL model; Land cover; Land cover fraction; Layout; Remote-sensing; Spatial errors; Spatial resolution; Spectral-spatial error; Superresolution; Superresolution mapping; image resolution; land cover; machine learning; mapping; multispectral image; remote sensing; spatial analysis; spectral analysis; Distribution functions","Deep learning (DL); generative adversarial network (GAN); land cover fractions; spectral-spatial errors; super-resolution mapping (SRM)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144774026"
"Zhang B.; Zhong P.; Yang F.; Zhou T.; Shen L.","Zhang, Bo (57810279400); Zhong, Ping (57204622399); Yang, Fu (57941432900); Zhou, Tianhua (55648222000); Shen, Lingfei (57942307200)","57810279400; 57204622399; 57941432900; 55648222000; 57942307200","Fast Underwater Optical Beacon Finding and High Accuracy Visual Ranging Method Based on Deep Learning","2022","Sensors","22","20","7940","","","","10.3390/s22207940","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140610378&doi=10.3390%2fs22207940&partnerID=40&md5=347dc8d87a8424827f7dc0f9aa162c16","Visual recognition and localization of underwater optical beacons is an important step in autonomous underwater vehicle (AUV) docking. The main issues that restrict the use of underwater monocular vision range are the attenuation of light in water, the mirror image between the water surface and the light source, and the small size of the optical beacon. In this study, a fast monocular camera localization method for small 4-light beacons is proposed. A YOLO V5 (You Only Look Once) model with coordinated attention (CA) mechanisms is constructed. Compared with the original model and the model with convolutional block attention mechanisms (CBAM), and our model improves the prediction accuracy to 96.1% and the recall to 95.1%. A sub-pixel light source centroid localization method combining super-resolution generative adversarial networks (SRGAN) image enhancement and Zernike moments is proposed. The detection range of small optical beacons is increased from 7 m to 10 m. In the laboratory self-made pool and anechoic pool experiments, the average relative distance error of our method is 1.04 percent, and the average detection speed is 0.088 s (11.36 FPS). This study offers a solution for the long-distance fast and accurate positioning of underwater small optical beacons due to their fast recognition, accurate ranging, and wide detection range characteristics. © 2022 by the authors.","Deep Learning; Image Enhancement; Research Design; Water; Autonomous vehicles; Deep learning; Feature extraction; Generative adversarial networks; Image enhancement; Light sources; water; Attention mechanisms; Autonomous underwater vehicles]; Deep learning; Detection range; High-accuracy; Localization method; Monocular vision; Optical beacon; Targets detection; Visual recognition; image enhancement; methodology; procedures; Autonomous underwater vehicles","autonomous underwater vehicles; deep learning; monocular vision; target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140610378"
"Wu J.; Wang J.; Zhao J.; Luo X.; Ma B.","Wu, Junfeng (57849926700); Wang, Jinwei (57192930875); Zhao, Junjie (57221607917); Luo, Xiangyang (8976166200); Ma, Bin (57204589541)","57849926700; 57192930875; 57221607917; 8976166200; 57204589541","ESGAN for generating high quality enhanced samples","2022","Multimedia Systems","28","5","","1809","1822","13","10.1007/s00530-022-00953-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130716744&doi=10.1007%2fs00530-022-00953-3&partnerID=40&md5=21592558f191886f110fc41937934be7","Recently, convolutional neural networks (CNN) have shown significant success in image classification tasks. However, studies show that neural networks are susceptible to tiny perturbations. When the disturbing image is input, the neural network will make a different judgment. At present, most studies use the negative side of perturbation to mislead the neural network, such as adversarial examples. In this paper, considering the positive side of perturbation, we propose Enhanced Samples Generative Adversarial Networks (ESGAN) to generate high-quality enhanced samples with positive perturbation, which is designed to further improve the performance of the target classifier. Enhanced samples’ generation is composed of two parts. The super-resolution (SR) network is used to generate high visual quality images, and the noise network is used to generate positive perturbations. Our ESGAN is independent of the target classifier, so it can improve performance without retraining the classifier, thus effectively reducing the computing resources and training time of the classifier. Experiments show that the enhanced samples generated by our proposed ESGAN can effectively improve the performance of the target classifier without affecting human eye recognition. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Convolutional neural networks; Image enhancement; Optical resolving power; Adversarial example; Classification tasks; Convolutional neural network; Enhanced sample; High quality; Image super resolutions; Images classification; Negative sides; Performance; Generative adversarial networks","Adversarial examples; Enhanced samples; Generative adversarial networks; Image super-resolution","Article","Final","","Scopus","2-s2.0-85130716744"
"Deng H.; Wei T.; Du Y.","Deng, Haibin (57211046793); Wei, Tangzhong (58046926800); Du, Yinfei (57882079400)","57211046793; 58046926800; 57882079400","Asphalt Pavement Texture Image Restoration Method Based on Generative Adversarial Network; [基于对抗生成网络的沥青路面纹理图像修复方法]","2022","Wuhan Ligong Daxue Xuebao (Jiaotong Kexue Yu Gongcheng Ban)/Journal of Wuhan University of Technology (Transportation Science and Engineering)","46","6","","1078","1084","6","10.3963/j.issn.2095-3844.2022.06.026","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145941905&doi=10.3963%2fj.issn.2095-3844.2022.06.026&partnerID=40&md5=8ecdbc10aa7efd770d804e29f080eed9","Aiming at three common problems: low resolution, Gaussian blur and motion blur, a pavement texture image inpainting method based on confrontation generation network was proposed. Perception loss and texture feature loss term based on GLCM were added to the generator loss function to better restore the details of pavement texture. Subjective judgment and comprehensive indicators based on peak signal-to-noise ratio and structural similarity were used to measure the restoration effect of the model. The results show that the model with GLCM texture features added to the loss function is better. It can be seen that the extraction of GLCM texture features is helpful for the generator to learn the detailed features of texture, and it is helpful for pavement image restoration. © 2022, Editorial Department of Journal of Wuhan University of Technology. All right reserved.","","Asphalt pavement; Blurred image restoration; Generative adversarial network; Road engineering; Super resolution; Texture image restoration","Article","Final","","Scopus","2-s2.0-85145941905"
"Srivastava A.; Chanda S.; Pal U.","Srivastava, Abhishek (57224567257); Chanda, Sukalpa (7005580236); Pal, Umapada (57200742116)","57224567257; 7005580236; 57200742116","AGA-GAN: Attribute Guided Attention Generative Adversarial Network with U-Net for face hallucination","2022","Image and Vision Computing","126","","104534","","","","10.1016/j.imavis.2022.104534","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137172909&doi=10.1016%2fj.imavis.2022.104534&partnerID=40&md5=7ac10d5c557ec9594daf62b4cd03bca3","The performance of facial super-resolution methods relies on their ability to recover facial structures and salient features effectively. Even though the convolutional neural network and generative adversarial network-based methods deliver impressive performances on face hallucination tasks, the ability to use attributes associated with the low-resolution images to improve performance is unsatisfactory. In this paper, we propose an Attribute Guided Attention Generative Adversarial Network which employs novel attribute guided attention (AGA) modules to identify and focus the generation process on various facial features in the image. Stacking multiple AGA modules enables the recovery of both high and low-level facial structures. We design the discriminator to learn discriminative features by exploiting the relationship between the high-resolution image and their corresponding facial attribute annotations. We then explore the use of U-Net based architecture to refine existing predictions and synthesize further facial details. Extensive experiments across several metrics show that our AGA-GAN and AGA-GAN + U-Net framework outperforms several other cutting-edge face hallucination state-of-the-art methods. We also demonstrate the viability of our method when every attribute descriptor is not known and thus, establishing its application in real-world scenarios. Our code is available at https://github.com/NoviceMAn-prog/AGA-GAN. © 2022 Elsevier B.V.","Convolutional neural networks; Image enhancement; Convolutional neural network; Face hallucination; Facial structure; Network-based; Performance; Salient features; Spatial attention; Structure features; Superresolution methods; U-net; Generative adversarial networks","Face hallucination; Generative adversarial network; Spatial attention; U-Net","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85137172909"
"Wang C.; Shen Q.; Wang X.; Jiang G.","Wang, Cailing (35367319800); Shen, Qi (57222314955); Wang, Xingbo (57384929500); Jiang, Guoping (7401706522)","35367319800; 57222314955; 57384929500; 7401706522","Momentum feature comparison network based on generative adversarial network for single image super-resolution","2022","Signal Processing: Image Communication","106","","116726","","","","10.1016/j.image.2022.116726","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130399483&doi=10.1016%2fj.image.2022.116726&partnerID=40&md5=9aabf43be0d812e7fdac01720b8a6344","Most super-resolution methods are trained on datasets where high-resolution images and corresponding low-resolution images are obtained by the fixed degradation method. However, these external-based methods would fail to recover the detailed information of the test images if it could not be found in datasets. In this paper, we propose the momentum feature comparison network to generate the super-resolution image with rich texture information. Without the participation of high-resolution images in the training process, our method belongs to the completely unsupervised super-resolution method. A siamese structure with momentum update in generator is proposed to expand the content information of the low-resolution image and maintains the continuous consistency of intermediate feature maps. Furthermore, the results of two branches are fused through the feature fusion module to retain the global distribution of features and enhance local high-frequency details. The experimental results show that our method achieves great results compared with the state-of-the-art methods. © 2022 Elsevier B.V.","Momentum; Optical resolving power; Textures; Unsupervised learning; Comparison networks; High-resolution images; Image super resolutions; Low resolution images; Momentum feature comparison; Network-based; Single images; Superresolution; Superresolution methods; Test images; Generative adversarial networks","Generative adversarial networks; Momentum feature comparison; Super resolution; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85130399483"
"Xiao Y.; Zhang J.; Chen W.; Wang Y.; You J.; Wang Q.","Xiao, Yuzhen (57779229400); Zhang, Jidong (57780262900); Chen, Wei (57779708700); Wang, Yichen (57766387200); You, Jianing (57778557300); Wang, Qing (57064449700)","57779229400; 57780262900; 57779708700; 57766387200; 57778557300; 57064449700","SR-DeblurUGAN: An End-to-End Super-Resolution and Deblurring Model with High Performance","2022","Drones","6","7","162","","","","10.3390/drones6070162","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133329290&doi=10.3390%2fdrones6070162&partnerID=40&md5=86bdacb3391600f976441cb6b137380e","In this paper, we consider the difference in the abstraction level of features extracted by different perceptual layers and use a weighted perceptual loss-based generative adversarial network to deblur the UAV images, which removes the blur and restores the texture details of the images well. The perceptual loss is used as an objective evaluation index for training process monitoring and model selection, which eliminates the need for extensive manual comparison of the deblurring effect and facilitates model selection. The UNet jump connection structure facilitates the transfer of features across layers in the network, reduces the learning difficulty of the generator, and improves the stability of adversarial training. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","generative adversarial network; image deblurring; super-resolution; UAV; UNet","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133329290"
"Yu Z.; Chen W.; Zhang T.","Yu, Zihang (57285146800); Chen, Wanzhong (7409641669); Zhang, Tao (57198705509)","57285146800; 7409641669; 57198705509","Motor imagery EEG classification algorithm based on improved lightweight feature fusion network","2022","Biomedical Signal Processing and Control","75","","103618","","","","10.1016/j.bspc.2022.103618","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126275538&doi=10.1016%2fj.bspc.2022.103618&partnerID=40&md5=5c97060cc06c330ef59127fccff58ddb","When deep learning techniques are introduced for Motor Imagery(MI) EEG signal classification, a multitude of state-of-the-art models, cannot be trained effectively because of the relatively small datasets. Proposing a model specialized for MI EEG signals classification plays a prominent role in promoting the combination of deep learning technology and MI EEG signal classification. In this paper, a novel Lightweight Feature Fusion Network(LFANN) based on an improved attention mechanism and tensor decomposition approach has been introduced. The proposed algorithm has been evaluated on a public benchmark dataset from BCI Competition IV, and the original dataset has been augmented with Enhance-Super-Resolution Generative Adversarial Network(ESRGAN). The experimental results demonstrate that the average accuracy of 91.58% and the average Kappa value of 0.881 can be achieved through the proposed algorithm. Furthermore, the compressed LAFFN, whose parameters have been compressed nearly ten times, creates no significant difference in performance compared to LAFFN. The investigation carried out through this experiment has provided novel insights into the classification research for MI EEG signals. © 2022 Elsevier Ltd","Biomedical signal processing; Classification (of information); Deep learning; Image classification; Image enhancement; Tensors; Attention mechanisms; Classification algorithm; Data augmentation; Deep learning; EEG classification; EEG signals classification; Features fusions; Motor imagery; Motor imagery EEG; Tensor decomposition; article; attention; classification algorithm; competition; decomposition; deep learning; electroencephalogram; human; human experiment; imagery; Generative adversarial networks","Attention mechanism; Data augmentation; Deep learning; motor imagery; Tensor decomposition","Article","Final","","Scopus","2-s2.0-85126275538"
"Sarishvili A.; Jirstrand M.; Adrian B.; Wirsen A.","Sarishvili, A. (14828496000); Jirstrand, M. (6602434691); Adrian, B. (57580957400); Wirsen, A. (57222475923)","14828496000; 6602434691; 57580957400; 57222475923","Sparse Inversion of Stacked Autoencoder Classification Machines","2023","Lecture Notes in Networks and Systems","465","","","617","631","14","10.1007/978-981-19-2397-5_55","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136973049&doi=10.1007%2f978-981-19-2397-5_55&partnerID=40&md5=0d0dd7634b9f4379c3afb6c0452e4d36","This paper describes a new approach to solve an inverse classification problem. In many applications of supervised learning, it is of interest to generate a number of synthetic high-dimensional model input from a high dimensional as well output. Recent works have shown the benefit of using artificially augmented data in deep machine learning applications. Especially in image processing augmentation algorithms, e.g., generative adversarial networks (GAN), conditional GANs, variational type of autoencoders, restricted Boltzmann machines, geometric transformations (translation, rotation, and scaling fractional linear Möbius transformation), mixing images, feature space augmentation, etc., are used as methods to solve the problem of limited data space. Furthermore, the classical inverse problem, e.g., reconstruction of images from incomplete data, is still of high interest. There are several studies where the use of deep learning methods in solving inverse problems has shown good results. These studies have been focused on image denoising by, e.g., stacked denoising autoencoders (SDA), removing noisy patterns from images, and image super-resolution by using special type of deep convolutional networks. The proposed approach generates/reconstructs handwritten digits by using compressed sensing (CS) in combination with SAE+SM (Stacked Autoencoder Softmax classification machine) and sparse coding (SC) in forward step. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Compressed sensing; Dictionary learning; Sparse coding","Conference paper","Final","","Scopus","2-s2.0-85136973049"
"Chen S.; Lan J.; Liu H.; Chen C.; Wang X.","Chen, Shuai (57195431910); Lan, Jinhui (7102691503); Liu, Haoting (34882031200); Chen, Chengkai (58031863300); Wang, Xiaohan (57915571700)","57195431910; 7102691503; 34882031200; 58031863300; 57915571700","Helmet Wearing Detection of Motorcycle Drivers Using Deep Learning Network with Residual Transformer-Spatial Attention","2022","Drones","6","12","415","","","","10.3390/drones6120415","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144877413&doi=10.3390%2fdrones6120415&partnerID=40&md5=472c7b966720d61569b417814b73ea13","Aiming at the existing problem of unmanned aerial vehicle (UAV) aerial photography for riders’ helmet wearing detection, a novel aerial remote sensing detection paradigm is proposed by combining super-resolution reconstruction, residual transformer-spatial attention, and you only look once version 5 (YOLOv5) image classifier. Due to its small target size, significant size change, and strong motion blur in UAV aerial images, the helmet detection model for riders has weak generalization ability and low accuracy. First, a ladder-type multi-attention network (LMNet) for target detection is designed to conquer these difficulties. The LMNet enables information interaction and fusion at each stage, fully extracts image features, and minimizes information loss. Second, the Residual Transformer 3D-spatial Attention Module (RT3DsAM) is proposed in this work, which digests information from global data that is important for feature representation and final classification detection. It also builds self-attention and enhances correlation between information. Third, the rider images detected by LMNet are cropped out and reconstructed by the enhanced super-resolution generative adversarial networks (ESRGAN) to restore more realistic texture information and sharp edges. Finally, the reconstructed images of riders are classified by the YOLOv5 classifier. The results of the experiment show that, when compared with the existing methods, our method improves the detection accuracy of riders’ helmets in aerial photography scenes, with the target detection mean average precision (mAP) evaluation indicator reaching 91.67%, and the image classification top1 accuracy (TOP1 ACC) gaining 94.23%. © 2022 by the authors.","","helmet wearing detection; LMNet; residual transformer-spatial attention; super-resolution reconstruction; UAV","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144877413"
"Xin L.; Li Z.; Wang S.","Xin, L. (57742773600); Li, Z. (57884534800); Wang, S. (57852799900)","57742773600; 57884534800; 57852799900","Super-resolution research on remote sensing images in the megacity based on improved srgan","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","603","609","6","10.5194/isprs-Annals-V-3-2022-603-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132009410&doi=10.5194%2fisprs-Annals-V-3-2022-603-2022&partnerID=40&md5=e08573da31b29a3d21c06ab392dd8075","Remote sensing images of Earth observation with high spatial resolution and high temporal resolution are critical for the application of remote sensing technology in Megacities.With the development of Smart City,more demands which are still difficult to be perfectly satisfied on the spatial resolution and temporal resolution of remote sensing images have been put forward.This paper studies the use of SRGAN which means Super-Resolution using a Generative Adversarial Network (a network structure that uses the loss function considering the perceptual loss and the adversarial loss to improve the spatial resolution of remote sensing images) for super-resolution reconstruction of single remote sensing image.It is able to enhance the spatial resolution of remote sensing images and improve the depth and breadth of remote sensing images.We adjust the reasonable parameters and network structure for our research by analysing the SRGAN in the network architecture, the perceptual loss and the adversarial loss.A super-resolution model is obtained by training with aerial photogrammetry images whose spatial resolution are 0.1 meter in Shanghai.We find the improved SRGAN has a good performance in in remote sensing image super-resolution by comparing the super-resoved images with real high-resolution images in visual perception, spatial position mapping accuracy and chromaticity spatial information. In addition, it is proved that the trained model is also effective to deal with Worldview-2 and SuperView-1 satellite images whose spatial resolution are 0.5 m. Our research shows that our method which can effectively realize the super-resolution of remote sensing images has great potential in the application of remote sensing technology such as urban mapping and changes monitoring.  © Authors 2022.","Antennas; Generative adversarial networks; Image enhancement; Mapping; Network architecture; Remote sensing; Satellite imagery; Deeplearning; High resolution satellite imagery; Megacities; Network structures; Remote sensing images; Remote sensing technology; Spatial resolution; SRGAN; Superresolution; Urban remote sensing; Image resolution","Deeplearning; High resolution satellite imagery; SRGAN; Super resolution; Urban Remote Sensing","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132009410"
"Sun C.; Gu D.; Zhang Y.; Lu X.","Sun, Chujin (57210263835); Gu, Donglian (57796098000); Zhang, Yi (57215008772); Lu, Xinzheng (15728275900)","57210263835; 57796098000; 57215008772; 15728275900","Vision-based displacement measurement enhanced by super-resolution using generative adversarial networks","2022","Structural Control and Health Monitoring","29","10","e3048","","","","10.1002/stc.3048","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133946991&doi=10.1002%2fstc.3048&partnerID=40&md5=bfb68ef5faf167547b6b5a5ecd647bcb","Monitoring the deformation or displacement response of buildings is critical for structural safety. Recently, the development of computer vision has led to extensive research on the application of vision-based measurements in the structural monitoring. This enables the use of urban surveillance video cameras, which are widely installed and can produce numerous images and videos of urban scenes to measure the structural displacement. However, the structural displacement measurement may be inaccurate owing to the limited hardware resolution of the surveillance video cameras or the long distance from the cameras to the monitored targets. To this end, this study proposes a method to improve the displacement measurement accuracy using a deep learning super-resolution model based on generative adversarial networks. The proposed method achieves texture detail enhancement of low-resolution images or videos by supplementing high-resolution photographs of the target, thus improving the accuracy of the vision-based displacement measurement. The proposed method shows good accuracy and stability in both the static and dynamic experimental validations compared with the original low-resolution images/video and interpolation-based super-resolution images/video. In conclusion, the proposed method can support the displacement measurement of buildings and infrastructures based on urban surveillance video cameras. © 2022 John Wiley & Sons Ltd.","Computer vision; Deep learning; Displacement measurement; Image enhancement; Monitoring; Optical resolving power; Security systems; Textures; Video cameras; Deformation response; Displacement response; Displacements measurements; Low resolution images; Structural displacement; Superresolution; Surveillance video; Surveillance video camera; Urban surveillance; Vision based; Generative adversarial networks","computer vision; displacement measurement; generative adversarial networks; super-resolution; surveillance video cameras","Article","Final","","Scopus","2-s2.0-85133946991"
"Nitzan Y.; Aberman K.; He Q.; Liba O.; Yarom M.; Gandelsman Y.; Mosseri I.; Pritch Y.; Cohen-Or D.","Nitzan, Yotam (57219740103); Aberman, Kfir (57190192015); He, Qiurui (57204732480); Liba, Orly (24577722500); Yarom, Michal (57219697771); Gandelsman, Yossi (57214469458); Mosseri, Inbar (55811234800); Pritch, Yael (6507185318); Cohen-Or, Daniel (7004252391)","57219740103; 57190192015; 57204732480; 24577722500; 57219697771; 57214469458; 55811234800; 6507185318; 7004252391","MyStyle: A Personalized Generative Prior","2022","ACM Transactions on Graphics","41","6","3555436","","","","10.1145/3550454.3555436","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146365734&doi=10.1145%2f3550454.3555436&partnerID=40&md5=26dda2aa1982e9147b30f1d53b899802","We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person's key facial characteristics. Given a small reference set of portrait images of a person (∼ 100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives.  © 2022 ACM.","Computer vision; Generative adversarial networks; Semantics; Fair use; High-fidelity; Ill posed; Inpainting; Input image; Low dimensional; Portrait image; Reference set; Superresolution; Unified approach; Image enhancement","generative adversarial networks","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85146365734"
"Li P.; Li Z.; Pang X.; Wang H.; Lin W.; Wu W.","Li, Pengcheng (57216279808); Li, Zhangyu (57208319246); Pang, Xiongwen (16175839500); Wang, Hui (55941458000); Lin, Weiwei (55723146900); Wu, Wentai (56582973300)","57216279808; 57208319246; 16175839500; 55941458000; 55723146900; 56582973300","Multi-scale residual denoising GAN model for producing super-resolution CTA images","2022","Journal of Ambient Intelligence and Humanized Computing","13","3","","1515","1524","9","10.1007/s12652-021-03009-y","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102442936&doi=10.1007%2fs12652-021-03009-y&partnerID=40&md5=86783920795f802a4c7733cbf19b36fc","Computed tomography angiography (CTA) is one of the salient radiological techniques in the virtualization and diagnosis of cerebral vascular diseases. However, there are various obstacles to the acquisition of highly legible CTA images, such as the lack of high-resolution CT scanners in community hospitals. And it is time-consuming for radiologists to perform CTA post-processing. These predicaments that medical institutions face make it necessary to automatically covert cerebrovascular images of low resolution to high-quality ones by means of artificial intelligence systems. In this paper, we propose a deep learning technique to improve the resolution of blurred CTA images. We develop MRDGAN, a novel generative adversarial network (GAN) model, to address the outstanding problems in CTA images such as high-frequency noise information (black pixels) and the scarcity of useful information (blood vessel pixels). We introduce spatial and channel attention into MRDGAN’s generator to facilitate feature extraction and incorporate a multi-scale residual block and a noise reduction block to retain micro vessels’ information and eliminate the noise in the generated images. Experiment results show that the CTA images generated by our model MRDGAN outperform the state-of-the-art models SRGAN and ESRGAN in terms of quality and quantity—MRDGAN obtains the highest score (35.89) in peak signal-to-noise ratio, showing a great potential as a low-cost solution of acquiring high-resolution CTA images. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Blood vessels; Deep learning; Diagnosis; Image enhancement; Medical imaging; Pixels; Signal to noise ratio; Adversarial networks; Artificial intelligence systems; Cerebral vascular disease; Computed tomography angiography; High-frequency noise; Medical institutions; Peak signal to noise ratio; Radiological technique; Computerized tomography","Computed tomography angiography; Generative adversarial network; Multi-scale residual block; Super resolution","Article","Final","","Scopus","2-s2.0-85102442936"
"Wang Y.; Wu W.; Yang Y.; Hu H.; Yu S.; Dong X.; Chen F.; Liu Q.","Wang, Yulin (57767485600); Wu, Wenyuan (57752875300); Yang, Yuxin (57701396000); Hu, Haifeng (57731488400); Yu, Shangqian (57701123200); Dong, Xiangjiang (57696491500); Chen, Feng (57190131900); Liu, Qian (57207736524)","57767485600; 57752875300; 57701396000; 57731488400; 57701123200; 57696491500; 57190131900; 57207736524","Deep learning-based 3D MRI contrast-enhanced synthesis from a 2D noncontrast T2Flair sequence","2022","Medical Physics","49","7","","4478","4493","15","10.1002/mp.15636","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130435853&doi=10.1002%2fmp.15636&partnerID=40&md5=19d26a0ac3a1b92a8f368054054d1a79","Purpose: Gadolinium-based contrast agents (GBCAs) have been successfully applied in magnetic resonance (MR) imaging to facilitate better lesion visualization. However, gadolinium deposition in the human brain raised widespread concerns recently. On the other hand, although high-resolution three-dimensional (3D) MR images are more desired for most existing medical image processing algorithms, their long scan duration and high acquiring costs make 2D MR images still much more common clinically. Therefore, developing alternative solutions for 3D contrast-enhanced MR image synthesis to replace GBCAs injection becomes an urgent requirement. Methods: This study proposed a deep learning framework that produces 3D isotropic full-contrast T2Flair images from 2D anisotropic noncontrast T2Flair image stacks. The super-resolution (SR) and contrast-enhanced (CE) synthesis tasks are completed in sequence by using an identical generative adversarial network (GAN) with the same techniques. To solve the problem that intramodality datasets from different scanners have specific combinations of orientations, contrasts, and resolutions, we conducted a region-based data augmentation technique on the fly during training to simulate various imaging protocols in the clinic. We further improved our network by introducing atrous spatial pyramid pooling, enhanced residual blocks, and deep supervision for better quantitative and qualitative results. Results: Our proposed method achieved superior CE-synthesized performance in quantitative metrics and perceptual evaluation. In detail, the PSNR, structural-similarity-index, and AUC are 32.25 dB, 0.932, and 0.991 in the whole brain and 24.93 dB, 0.851, and 0.929 in tumor regions. The radiologists’ evaluations confirmed that our proposed method has high confidence in the diagnosis. Analysis of the generalization ability showed that benefiting from the proposed data augmentation technique, our network can be applied to “unseen” datasets with slight drops in quantitative and qualitative results. Conclusion: Our work demonstrates the clinical potential of synthesizing diagnostic 3D isotropic CE brain MR images from a single 2D anisotropic noncontrast sequence. © 2022 American Association of Physicists in Medicine.","Contrast Media; Deep Learning; Gadolinium; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Deep learning; Diagnosis; Gadolinium; Image enhancement; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Optical resolving power; gadobutrol; gadolinium pentetate meglumine; contrast medium; gadolinium; Augmentation techniques; Contrast agent; Contrast synthesis; Contrast-enhanced; Data augmentation; Gadolinia; Gadolinia-based contrast agent; Isotropics; MRI contrasts; Superresolution; algorithm; Article; Bayesian learning; brain tumor; controlled study; convolutional neural network; deep learning; diagnostic accuracy; diagnostic test accuracy study; dynamic contrast-enhanced magnetic resonance imaging; fluid-attenuated inversion recovery imaging; human; image processing; intermethod comparison; major clinical study; neuroimaging; qualitative diagnosis; radiologist; residual neural network; signal noise ratio; simulation; T2 weighted imaging; three-dimensional imaging; workflow; nuclear magnetic resonance imaging; procedures; three-dimensional imaging; Generative adversarial networks","contrast synthesis; GBCAs; generative adversarial network; MR imaging; super-resolution","Article","Final","","Scopus","2-s2.0-85130435853"
"Zhang K.; Hu H.; Philbrick K.; Conte G.M.; Sobek J.D.; Rouzrokh P.; Erickson B.J.","Zhang, Kuan (57224929198); Hu, Haoji (57210646621); Philbrick, Kenneth (57225684280); Conte, Gian Marco (57188754673); Sobek, Joseph D. (57224938304); Rouzrokh, Pouria (55785357100); Erickson, Bradley J. (7201472755)","57224929198; 57210646621; 57225684280; 57188754673; 57224938304; 55785357100; 7201472755","SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks","2022","Tomography","8","2","","905","919","14","10.3390/tomography8020073","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127514201&doi=10.3390%2ftomography8020073&partnerID=40&md5=2a485577f4cd4ae58b386bce25022780","There is a growing demand for high-resolution (HR) medical images for both clinical and research applications. Image quality is inevitably traded off with acquisition time, which in turn impacts patient comfort, examination costs, dose, and motion-induced artifacts. For many image-based tasks, increasing the apparent spatial resolution in the perpendicular plane to produce multi-planar reformats or 3D images is commonly used. Single-image super-resolution (SR) is a promising technique to provide HR images based on deep learning to increase the resolution of a 2D image, but there are few reports on 3D SR. Further, perceptual loss is proposed in the literature to better capture the textural details and edges versus pixel-wise loss functions, by comparing the semantic distances in the high-dimensional feature space of a pre-trained 2D network (e.g., VGG). However, it is not clear how one should generalize it to 3D medical images, and the attendant implications are unclear. In this paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using Perceptual-tuned Generative Adversarial Network (GAN), in order to produce thinner slices (e.g., higher resolution in the ‘Z’ plane) with anti-aliasing and deblurring. The proposed method outperforms other conventional resolution-enhancement methods and previous SR work on medical images based on both qualitative and quantitative comparisons. Moreover, we examine the model in terms of its generalization for arbitrarily user-selected SR ratios and imaging modalities. Our model shows promise as a novel 3D SR interpolation technique, providing potential applications for both clinical and research applications. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Artifacts; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Motion; artifact; human; motion; nuclear magnetic resonance imaging; three-dimensional imaging","3D perceptual loss; deep learning; generative adversarial networks (GAN); magnetic resonance imaging (MRI); medical imaging interpolation; super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85127514201"
"Li W.; Abrashitova K.; Osnabrugge G.; Amitonova L.V.","Li, Wei (57407084700); Abrashitova, Ksenia (57193745903); Osnabrugge, Gerwin (57190004921); Amitonova, Lyubov V. (57208316205)","57407084700; 57193745903; 57190004921; 57208316205","Generative Adversarial Network for Superresolution Imaging through a Fiber","2022","Physical Review Applied","18","3","034075","","","","10.1103/PhysRevApplied.18.034075","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139278179&doi=10.1103%2fPhysRevApplied.18.034075&partnerID=40&md5=2e6eb182e6752354858101547fbdcc76","A multimode fiber represents the ultimate limit in miniaturization of imaging endoscopes. However, such a miniaturization usually comes as a cost of a low spatial resolution and a long acquisition time. Here we propose a fast superresolution-fiber-imaging technique employing compressive sensing through a multimode fiber with a data-driven machine-learning framework. We implement a generative adversarial network (GAN) to explore the sparsity inherent to the model and provide compressive reconstruction images that are not sparse in a representation basis. The proposed method outperforms other widespread compressive imaging algorithms in terms of both image quality and noise robustness. We experimentally demonstrate machine-learning ghost imaging below the diffraction limit at a sub-Nyquist speed through a thin multimode fiber probe. We believe that this work has great potential in applications in various fields ranging from biomedical imaging to remote sensing.  © 2022 authors. Published by the American Physical Society. Published by the American Physical Society under the terms of the ""https://creativecommons.org/licenses/by/4.0/""Creative Commons Attribution 4.0 International license. Further distribution of this work must maintain attribution to the author(s) and the published article's title, journal citation, and DOI.","Diffraction; Medical imaging; Miniature instruments; Multimode fibers; Optical resolving power; A.Fibres; Acquisition time; Compressive sensing; Data driven; Learning frameworks; Machine-learning; Miniaturisation; Spatial resolution; Super resolution imaging; Superresolution; Generative adversarial networks","","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139278179"
"Sun Q.-F.; Xu J.-Y.; Zhang H.-X.; Duan Y.-X.; Sun Y.-K.","Sun, Qi-Feng (37039999300); Xu, Jia-Yue (57408293800); Zhang, Han-Xiao (57409500000); Duan, You-Xiang (15750350300); Sun, You-Kai (51462364200)","37039999300; 57408293800; 57409500000; 15750350300; 51462364200","Random noise suppression and super-resolution reconstruction algorithm of seismic profile based on GAN","2022","Journal of Petroleum Exploration and Production Technology","12","8","","2107","2119","12","10.1007/s13202-021-01447-0","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122701163&doi=10.1007%2fs13202-021-01447-0&partnerID=40&md5=e325ea00f8abaf706db321035079f4a4","In this paper, we propose a random noise suppression and super-resolution reconstruction algorithm for seismic profiles based on Generative Adversarial Networks, in anticipation of reducing the influence of random noise and low resolution on seismic profiles. Firstly, the algorithm used the residual learning strategy to construct a de-noising subnet to accurate separate the interference noise on the basis of protecting the effective signal. Furthermore, it iterated the back-projection unit to complete the reconstruction of the high-resolution seismic sections image, while responsed sampling error to enhance the super-resolution performance of the algorithm. For seismic data characteristics, designed the discriminator to be a fully convolutional neural network, used a larger convolution kernels to extract data features and continuously strengthened the supervision of the generator performance optimization during the training process. The results on the synthetic data and the actual data indicated that the algorithm could improve the quality of seismic cross-section, make ideal signal-to-noise ratio and further improve the resolution of the reconstructed cross-sectional image. Besides, the observations of geological structures such as fractures were also clearer. © 2022, The Author(s).","Convolution; Convolutional neural networks; Image denoising; Image enhancement; Image reconstruction; Optical resolving power; Seismic waves; Seismology; Signal to noise ratio; Back-projection unit; Backprojections; Learning strategy; Lower resolution; Noise suppression; Projection units; Random noise; Reconstruction algorithms; Seismic profiles; Super-resolution reconstruction; Generative adversarial networks","Back-projection unit; Generative Adversarial Network; Noise reduction; Seismic profile; Super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122701163"
"Su J.; Xu B.; Yin H.","Su, Jingwen (57212414266); Xu, Boyan (57220593030); Yin, Hujun (55155712700)","57212414266; 57220593030; 55155712700","A survey of deep learning approaches to image restoration","2022","Neurocomputing","487","","","46","65","19","10.1016/j.neucom.2022.02.046","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125517510&doi=10.1016%2fj.neucom.2022.02.046&partnerID=40&md5=52377954306a26ee27f20ec6c04cdbf4","In this paper, we present an extensive review on deep learning methods for image restoration tasks. Deep learning techniques, led by convolutional neural networks, have received a great deal of attention in almost all areas of image processing, especially in image classification. However, image restoration is a fundamental and challenging topic and plays significant roles in image processing, understanding and representation. It typically addresses image deblurring, denoising, dehazing and super-resolution. There are substantial differences in the approaches and mechanisms in deep learning methods for image restoration. Discriminative learning based methods are able to deal with issues of learning a restoration mapping function effectively, while optimisation models based methods can further enhance the performance with certain learning constraints. In this paper, we offer a comparative study of deep learning techniques in image denoising, deblurring, dehazing, and super-resolution, and summarise the principles involved in these tasks from various supervised deep network architectures, residual or skip connection and receptive field to unsupervised autoencoder mechanisms. Image quality criteria are also reviewed and their roles in image restoration are assessed. Based on our analysis, we further present an efficient network for deblurring and a couple of multi-objective training functions for super-resolution restoration tasks. The proposed methods are compared extensively with the state-of-the-art methods with both quantitative and qualitative analyses. Finally, we point out potential challenges and directions for future research. © 2022 Elsevier B.V.","Convolution; Convolutional neural networks; Deep learning; Demulsification; Generative adversarial networks; Image denoising; Image reconstruction; Learning algorithms; Network architecture; Optical resolving power; Restoration; Convolutional neural network; Deblurring; Deep learning; Dehazing; Image deblurring; Image super resolutions; Images processing; Learning methods; Learning techniques; Superresolution; article; attention; autoencoder; comparative study; controlled study; convolutional neural network; deep learning; discrimination learning; image processing; image quality; image reconstruction; learning; qualitative analysis; quantitative analysis; receptive field; Image enhancement","Convolutional neural networks; Deep learning; Image deblurring; Image restoration; Image super-resolution","Article","Final","","Scopus","2-s2.0-85125517510"
"Shan L.; Liu C.; Liu Y.; Kong W.; Hei X.","Shan, Liqun (34972109800); Liu, Chengqian (57539845500); Liu, Yanchang (42661940800); Kong, Weifang (57853983600); Hei, Xiali (36930304000)","34972109800; 57539845500; 42661940800; 57853983600; 36930304000","Rock CT Image Super-Resolution Using Residual Dual-Channel Attention Generative Adversarial Network","2022","Energies","15","14","5115","","","","10.3390/en15145115","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136364113&doi=10.3390%2fen15145115&partnerID=40&md5=5e1feefa744e0ff65e68e8c09176bbc3","Because of its benefits in terms of high speed, non-destructiveness, and three-dimensionality, as well as ease of integration with computer simulation, computed tomography (CT) technology is widely applied in reservoir geology research. However, rock imaging is restricted by the device used as there is not a win–win for both the image receptive field and corresponding resolution. Convolutional neural network-based super-resolution reconstruction has become a hot topic in improving the performance of CT images. With the help of a convolution kernel, it can effectively extract characteristics and ignore disturbance information. The dismal truth is that convolutional neural networks still have numerous issues, particularly unclear texture details. To address these challenges, a generative adversarial network (RDCA-SRGAN) was designed to improve rock CT image resolution using the combination of residual learning and a dual-channel attention mechanism. Specifically, our generator employs residual attention to extract additional features; similarly, the discriminator builds on dual-channel attention and residual learning to distinguish generated contextual information and decrease computational consumption. Quantitative and qualitative analyses demonstrate that the proposed model is superior to earlier advanced frameworks and is capable to constructure visually indistinguishable high-frequency details. The quantitative analysis shows our model contributes the highest value of structural similarity, enriching the more detailed texture information. From the qualitative analysis, in enlarged details of the reconstructed images, the edges of the images generated by the RDCA-SRGAN can be shown to be clearer and sharper. Our model not only performs well in subtle coal cracks but also enriches more dissolved carbonate and carbon minerals. The RDCA-SRGAN has substantially enhanced the reconstructed image resolution and our model has great potential to be used in geomorphological study and exploration. © 2022 by the authors.","Computerized tomography; Convolution; Convolutional neural networks; Generative adversarial networks; Image enhancement; Image reconstruction; Rocks; Textures; Attention mechanisms; Channel attention mechanism; Computed tomography images; Convolutional neural network; Dual channel; Image super resolutions; Reconstructed image; Residual learning; Rock computed tomography image; Superresolution; Image resolution","channel attention mechanism; convolutional neural networks; generative adversarial network; residual learning; rock CT images; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136364113"
"Cheng L.; Kersemans M.","Cheng, Liangliang (57191042910); Kersemans, Mathias (54958029400)","57191042910; 54958029400","Dual-IRT-GAN: A defect-aware deep adversarial network to perform super-resolution tasks in infrared thermographic inspection","2022","Composites Part B: Engineering","247","","110309","","","","10.1016/j.compositesb.2022.110309","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139591777&doi=10.1016%2fj.compositesb.2022.110309&partnerID=40&md5=bbdbd57551e1fa27abe2f1e9ebff6a3e","InfraRed Thermography (IRT) is a valuable diagnostic tool for detecting defects in fiber-reinforced polymers in a non-destructive manner through the measurement of surface temperature distribution. Yet, thermal cameras typically have a low native spatial resolution resulting in a blurry and low-quality thermal image sequence. This study proposes a defect-aware Generative Adversarial Network (GAN) framework, termed Dual-IRT-GAN, in order to simultaneously perform Super-Resolution (SR) and defect detection tasks in infrared thermography. Furthermore, the visibility of defective regions in generated high-resolution images are enhanced by leveraging defect-aware attention maps from segmented defect images. Following a series of augmentation techniques and a second-order degradation process, the proposed Dual-IRT-GAN model is trained on an extensive numerically generated thermographic dataset of composite materials with various defect types, sizes and depts. The high inference performance of the virtually trained Dual-IRT-GAN is demonstrated on several experimental thermographic datasets which were obtained from composite coupon specimens with various defect types, sizes, and depths, as well as from aircraft stiffened composite panels having real (production) defects. © 2022 Elsevier Ltd","Bridge decks; Deep learning; Defects; Fiber reinforced plastics; Generative adversarial networks; Image enhancement; Nondestructive examination; Optical resolving power; Adversarial networks; Deep learning; Defect detection; Defect type; Detecting defects; Diagnostics tools; Fiber-reinforced polymers; Fibre reinforced polymers; Non destructive testing; Superresolution; Thermography (imaging)","Composite; Deep learning; Defect detection; Fiber reinforced polymer; Generative adversarial network; Infrared thermography; Non-destructive testing; Super-resolution","Article","Final","","Scopus","2-s2.0-85139591777"
"Xu B.; Li W.; Zhu Q.","Xu, Bin (57808461600); Li, Weiran (57216172230); Zhu, Qing (57189497573)","57808461600; 57216172230; 57189497573","A Facial Expression Synthesis Method Based on Generative Adversarial Network","2022","ACM International Conference Proceeding Series","","","","677","681","4","10.1145/3532213.3532316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134394204&doi=10.1145%2f3532213.3532316&partnerID=40&md5=f1cee39122d71f71545f2d47c113658a","Recently, machine learning, especially the emergence of generative adversarial networks (GANs), has further enhanced the robustness and realism of facial expression conversion models. However, most models have flaws such as fuzziness in the details. Based on this, this article mainly studies the facial expression synthesis method based on GANs. Firstly, we created a dataset containing 127,616 expression annotations suitable for the study of facial expressions. The dataset has been tested on mainstream models with good generation results. Secondly, we propose a GAN network structure named SRFEGAN with a super-resolution synthesis module. This module helps solve the artifact problem in the process of image conversion. Experimental results on our dataset show that the average recognition accuracy rate of the generated images is 63.76% and the Frechet Inception distance (FID) is 36.581. This shows that our network can accurately synthesize the facial expression image of the subject, and the image quality is better.  © 2022 ACM.","Optical resolving power; Conversion model; Expression editing; Face generation; Facial expression synthesis; Facial Expressions; Machine-learning; Network structures; Resolution synthesis; Superresolution; Synthesis method; Generative adversarial networks","Expression editing; Face generation; Generative adversarial network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85134394204"
"Kazemi N.; Musilek P.","Kazemi, Nazli (57210219082); Musilek, Petr (6602209739)","57210219082; 6602209739","Resolution enhancement of microwave sensors using super-resolution generative adversarial network","2023","Expert Systems with Applications","213","","119252","","","","10.1016/j.eswa.2022.119252","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142712880&doi=10.1016%2fj.eswa.2022.119252&partnerID=40&md5=b5dafa2bfa785d13a517fb8c32cd0da6","This article presents an approach to significantly improve the resolution of a highly-sensitive microwave planar sensor response with a super-resolution generative adversarial network (SRGAN). Three identical complementary split-ring resonators are coupled so that the sensitivity is doubled. This highly-sensitive resonator with a deep transmission zero at 4.7 GHz is deployed to measure minute variations of glucose in interstitial fluid. Measuring the sensor response with 1001 frequency-points allows differentiating 10 glucose samples within the range of 40–400 mg/dL. However, in practical readout systems with limited number of frequency-points (here 28), recognizing the deep zero in the S21 response lacks precision. Sensor responses (magnitude vs. frequency and phase vs. frequency) are converted into equivalent 2D images (heatmaps: phase vs. frequency with colored pixels as amplitude) to be compatible as SRGAN input. As a result of 8-fold resolution enhancement using SRGAN, the classification accuracy is substantially improved from 62.1% to 93.3%. The proposed passive sensor followed by an SRGAN unit is shown to be practical as a wearable glucose monitoring sensor due to its high-sensitivity and high resolution features in a low-profile design. © 2022 Elsevier Ltd","Generative adversarial networks; Glucose; Optical resolving power; Resonators; Wearable sensors; Complementary split-ring resonator; Coupled CSRR; Interstitial fluids; Planar sensors; Resolution; Resolution enhancement; Sensor response; Super-resolution generative adversarial network; Superresolution; Transmission zeros; Microwave sensors","Coupled CSRR; Glucose; Microwave sensor; Resolution; SRGAN","Article","Final","","Scopus","2-s2.0-85142712880"
"Alwakid G.; Gouda W.; Humayun M.; Sama N.U.","Alwakid, Ghadah (57199285762); Gouda, Walaa (56121350600); Humayun, Mamoona (56580449600); Sama, Najm Us (55969328900)","57199285762; 56121350600; 56580449600; 55969328900","Melanoma Detection Using Deep Learning-Based Classifications","2022","Healthcare (Switzerland)","10","12","2481","","","","10.3390/healthcare10122481","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144652068&doi=10.3390%2fhealthcare10122481&partnerID=40&md5=a91a120277b7e0555ff4898ca2329836","One of the most prevalent cancers worldwide is skin cancer, and it is becoming more common as the population ages. As a general rule, the earlier skin cancer can be diagnosed, the better. As a result of the success of deep learning (DL) algorithms in other industries, there has been a substantial increase in automated diagnosis systems in healthcare. This work proposes DL as a method for extracting a lesion zone with precision. First, the image is enhanced using Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) to improve the image’s quality. Then, segmentation is used to segment Regions of Interest (ROI) from the full image. We employed data augmentation to rectify the data disparity. The image is then analyzed with a convolutional neural network (CNN) and a modified version of Resnet-50 to classify skin lesions. This analysis utilized an unequal sample of seven kinds of skin cancer from the HAM10000 dataset. With an accuracy of 0.86, a precision of 0.84, a recall of 0.86, and an F-score of 0.86, the proposed CNN-based Model outperformed the earlier study’s results by a significant margin. The study culminates with an improved automated method for diagnosing skin cancer that benefits medical professionals and patients. © 2022 by the authors.","","convolutional neural network; deep learning; ESRGAN; HAM10000; machine learning; skin lesion","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144652068"
"Chan K.C.K.; Xu X.; Wang X.; Gu J.; Loy C.C.","Chan, Kelvin C.K. (57216374468); Xu, Xiangyu (57195934971); Wang, Xintao (57828962100); Gu, Jinwei (55661597500); Loy, Chen Change (25522308800)","57216374468; 57195934971; 57828962100; 55661597500; 25522308800","GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond","2023","IEEE Transactions on Pattern Analysis and Machine Intelligence","45","3","","3154","3168","14","10.1109/TPAMI.2022.3186715","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133613688&doi=10.1109%2fTPAMI.2022.3186715&partnerID=40&md5=af89f71f6a892413f1d17fb2149bcf99","We show that pre-trained Generative Adversarial Networks (GANs) such as StyleGAN and BigGAN can be used as a latent bank to improve the performance of image super-resolution. While most existing perceptual-oriented approaches attempt to generate realistic outputs through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass for restoration. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Employing priors from different generative models allows GLEAN to be applied to diverse categories (e.g., human faces, cats, buildings, and cars). We further present a lightweight version of GLEAN, named LightGLEAN, which retains only the critical components in GLEAN. Notably, LightGLEAN consists of only 21% of parameters and 35% of FLOPs while achieving comparable image quality. We extend our method to different tasks including image colorization and blind image restoration, and extensive experiments show that our proposed models perform favorably in comparison to existing methods. Codes and models are available at https://github.com/open-mmlab/mmediting.  © 1979-2012 IEEE.","Computer vision; Generative adversarial networks; Image enhancement; Job analysis; Optical resolving power; Restoration; Colorization; Face; Generative prior; Generator; Image super resolutions; Optimisations; Performance; Superresolution; Task analysis; Image reconstruction","colorization; generative adversarial networks; generative prior; restoration; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133613688"
"Hoque M.R.U.; Wu J.; Kwan C.; Koperski K.; Li J.","Hoque, Md Reshad Ul (57215344852); Wu, Jian (57193141747); Kwan, Chiman (7201421216); Koperski, Krzysztof (6603540174); Li, Jiang (56226550100)","57215344852; 57193141747; 7201421216; 6603540174; 56226550100","ArithFusion: An Arithmetic Deep Model for Temporal Remote Sensing Image Fusion","2022","Remote Sensing","14","23","6160","","","","10.3390/rs14236160","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143761796&doi=10.3390%2frs14236160&partnerID=40&md5=6d162c4fc285294f57b7c7e89e83f5f4","Different satellite images may consist of variable numbers of channels which have different resolutions, and each satellite has a unique revisit period. For example, the Landsat-8 satellite images have 30 m resolution in their multispectral channels, the Sentinel-2 satellite images have 10 m resolution in the pan-sharp channel, and the National Agriculture Imagery Program (NAIP) aerial images have 1 m resolution. In this study, we propose a simple yet effective arithmetic deep model for multimodal temporal remote sensing image fusion. The proposed model takes both low- and high-resolution remote sensing images at (Formula presented.) together with low-resolution images at a future time (Formula presented.) from the same location as inputs and fuses them to generate high-resolution images for the same location at (Formula presented.). We propose an arithmetic operation applied to the low-resolution images at the two time points in feature space to take care of temporal changes. We evaluated the proposed model on three modality pairs for multimodal temporal image fusion, including downsampled WorldView-2/original WorldView-2, Landsat-8/Sentinel-2, and Sentinel-2/NAIP. Experimental results show that our model outperforms traditional algorithms and recent deep learning-based models by large margins in most scenarios, achieving sharp fused images while appropriately addressing temporal changes. © 2022 by the authors.","Antennas; Deep learning; Generative adversarial networks; Image fusion; Satellite imagery; Space optics; Deep learning; Generative adversarial network; HRNet; LANDSAT; Neural-networks; Remote sensing images; Remote-sensing; Satellite images; Superresolution; U-net; Remote sensing","deep learning; generative adversarial network (GAN); HRNet; image fusion; neural networks; remote sensing; super-resolution; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143761796"
"La Grassa R.; Gallo I.; Re C.; Cremonese G.; Landro N.; Pernechele C.; Simioni E.; Gatti M.","La Grassa, Riccardo (57204648786); Gallo, Ignazio (7003336792); Re, Cristina (6603585550); Cremonese, Gabriele (56240953900); Landro, Nicola (57214364871); Pernechele, Claudio (6701418383); Simioni, Emanuele (55598125000); Gatti, Mattia (57903868600)","57204648786; 7003336792; 6603585550; 56240953900; 57214364871; 6701418383; 55598125000; 57903868600","An Adversarial Generative Network Designed for High-Resolution Monocular Depth Estimation from 2D HiRISE Images of Mars","2022","Remote Sensing","14","18","4619","","","","10.3390/rs14184619","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138727611&doi=10.3390%2frs14184619&partnerID=40&md5=1278e98bd7b4ca15a47aceec717c5f90","In computer vision, stereoscopy allows the three-dimensional reconstruction of a scene using two 2D images taken from two slightly different points of view, to extract spatial information on the depth of the scene in the form of a map of disparities. In stereophotogrammetry, the disparity map is essential in extracting the digital terrain model (DTM) and thus obtaining a 3D spatial mapping, which is necessary for a better analysis of planetary surfaces. However, the entire reconstruction process performed with the stereo-matching algorithm can be time consuming and can generate many artifacts. Coupled with the lack of adequate stereo coverage, it can pose a significant obstacle to 3D planetary mapping. Recently, many deep learning architectures have been proposed for monocular depth estimation, which aspires to predict the third dimension given a single 2D image, with considerable advantages thanks to the simplification of the reconstruction problem, leading to a significant increase in interest in deep models for the generation of super-resolution images and DTM estimation. In this paper, we combine these last two concepts into a single end-to-end model and introduce a new generative adversarial network solution that estimates the DTM at 4× resolution from a single monocular image, called SRDiNet (super-resolution depth image network). Furthermore, we introduce a sub-network able to apply a refinement using interpolated input images to better enhance the fine details of the final product, and we demonstrate the effectiveness of its benefits through three different versions of the proposal: SRDiNet with GAN approach, SRDiNet without adversarial network, and SRDiNet without the refinement learned network plus GAN approach. The results of Oxia Planum (the landing site of the European Space Agency’s Rosalind Franklin ExoMars rover 2023) are reported, applying the best model along all Oxia Planum tiles and releasing a 3D product enhanced by (Formula presented.). © 2022 by the authors.","Deep learning; E-learning; Generative adversarial networks; Image enhancement; Image reconstruction; Mapping; Optical resolving power; Remote sensing; Three dimensional computer graphics; 2D images; 3-D mapping; Deep learning; Depth Estimation; Depth image; Digital terrain model; Mars; Remote-sensing; Satellite images; Superresolution; Stereo image processing","3D mapping; deep learning; digital terrain model; Mars; remote sensing; satellite images; super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138727611"
"Zhou S.; Yu L.; Jin M.","Zhou, Shiwei (56477680200); Yu, Lifeng (7404164329); Jin, Mingwu (14522751700)","56477680200; 7404164329; 14522751700","Texture transformer super-resolution for low-dose computed tomography","2022","Biomedical Physics and Engineering Express","8","6","065024","","","","10.1088/2057-1976/ac9da7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141799786&doi=10.1088%2f2057-1976%2fac9da7&partnerID=40&md5=55b8e2c6c6ddb1e91b0d9acef9256963","Computed tomography (CT) is widely used to diagnose many diseases. Low-dose CT has been actively pursued to lower the ionizing radiation risk. A relatively smoother kernel is typically used in low-dose CT to suppress image noise, which may sacrifice spatial resolution. In this work, we propose a texture transformer network to simultaneously reduce image noise and improve spatial resolution in CT images. This network, referred to as Texture Transformer for Super Resolution (TTSR), is a reference-based deep-learning image super-resolution method built upon a generative adversarial network (GAN). The noisy low-resolution CT (LRCT) image and the routine-dose high-resolution (HRCT) image are severed as the query and key in a transformer, respectively. Image translation is optimized through deep neural network (DNN) texture extraction, correlation embedding, and attention-based texture transfer and synthesis to achieve joint feature learning between LRCT and HRCT images for super-resolution CT (SRCT) images. To evaluate SRCT performance, we use the data from both simulations of the XCAT phantom program and the real patient data. Peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and feature similarity (FSIM) index are used as quantitative metrics. For comparison of SRCT performance, the cubic spline interpolation, SRGAN (a GAN super-resolution with an additional content loss), and GAN-CIRCLE (a GAN super-resolution with cycle consistency) were used. Compared to the other two methods, TTSR can restore more details in SRCT images and achieve better PSNR, SSIM, and FSIM for both simulation and real-patient data. In addition, we show that TTSR can yield better image quality and demand much less computation time than high-resolution low-dose CT images denoised by block-matching and 3D filtering (BM3D) and GAN-CIRCLE. In summary, the proposed TTSR method based on texture transformer and attention mechanism provides an effective and efficient tool to improve spatial resolution and suppress noise of low-dose CT images. © 2022 IOP Publishing Ltd.","Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Phantoms, Imaging; Signal-To-Noise Ratio; Tomography, X-Ray Computed; Computerized tomography; Deep neural networks; Image enhancement; Image quality; Image resolution; Interpolation; Ionizing radiation; Medical imaging; Signal to noise ratio; Textures; Computed tomography images; Computed tomography super-resolution; Dose computed tomographies; Generative adversarial network; Generative adversarial network with cycle-consistency (generative adversarial network-CIRCLE); Low dose; Low-dose computed tomography; Superresolution; Texture transformer super-resolution; Article; attention; comparative study; controlled study; deep learning; deep neural network; embedding; feature extraction; generative adversarial network; high resolution computed tomography; human; image analysis; image quality; ionizing radiation; low resolution computed tomography; low-dose computed tomography; patient coding; radiological parameters; signal noise ratio; super resolution computed tomography; texture analysis; texture transformer for super resolution; image processing; imaging phantom; procedures; x-ray computed tomography; Generative adversarial networks","CT super-resolution; GAN with cycle-consistency (GAN-CIRCLE); generative adversarial network (GAN); low-dose CT; texture transformer super-resolution (TTSR)","Article","Final","","Scopus","2-s2.0-85141799786"
"Yoshimura T.; Nishioka K.; Hashimoto T.; Mori T.; Kogame S.; Seki K.; Sugimori H.; Yamashina H.; Nomura Y.; Kato F.; Kudo K.; Shimizu S.; Aoyama H.","Yoshimura, Takaaki (57993977600); Nishioka, Kentaro (57817094100); Hashimoto, Takayuki (7404793674); Mori, Takashi (58047046400); Kogame, Shoki (57222535237); Seki, Kazuya (57222534729); Sugimori, Hiroyuki (14054827600); Yamashina, Hiroko (58046941000); Nomura, Yusuke (57209331452); Kato, Fumi (58046507000); Kudo, Kohsuke (7402926338); Shimizu, Shinichi (7403432592); Aoyama, Hidefumi (58046287300)","57993977600; 57817094100; 7404793674; 58047046400; 57222535237; 57222534729; 14054827600; 58046941000; 57209331452; 58046507000; 7402926338; 7403432592; 58046287300","Prostatic urinary tract visualization with super-resolution deep learning models","2023","PLoS ONE","18","1 January","e0280076","","","","10.1371/journal.pone.0280076","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145946270&doi=10.1371%2fjournal.pone.0280076&partnerID=40&md5=a959c1ee068ade50b9ad6d77c4279e0f","In urethra-sparing radiation therapy, prostatic urinary tract visualization is important in decreasing the urinary side effect. A methodology has been developed to visualize the prostatic urinary tract using post-urination magnetic resonance imaging (PU-MRI) without a urethral catheter. This study investigated whether the combination of PU-MRI and super-resolution (SR) deep learning models improves the visibility of the prostatic urinary tract. We enrolled 30 patients who had previously undergone real-time-image-gated spot scanning proton therapy by insertion of fiducial markers. PU-MRI was performed using a non-contrast high-resolution two-dimensional T2-weighted turbo spin-echo imaging sequence. Four different SR deep learning models were used: the enhanced deep SR network (EDSR), widely activated SR network (WDSR), SR generative adversarial network (SRGAN), and residual dense network (RDN). The complex wavelet structural similarity index measure (CW-SSIM) was used to quantitatively assess the performance of the proposed SR images compared to PU-MRI. Two radiation oncologists used a 1-to-5 scale to subjectively evaluate the visibility of the prostatic urinary tract. Cohen’s weighted kappa (k) was used as a measure of agreement of inter-operator reliability. The mean CW-SSIM in EDSR, WDSR, SRGAN, and RDN was 99.86%, 99.89%, 99.30%, and 99.67%, respectively. The mean prostatic urinary tract visibility scores of the radiation oncologists were 3.70 and 3.53 for PU-MRI (k = 0.93), 3.67 and 2.70 for EDSR (k = 0.89), 3.70 and 2.73 for WDSR (k = 0.88), 3.67 and 2.73 for SRGAN (k = 0.88), and 4.37 and 3.73 for RDN (k = 0.93), respectively. The results suggest that SR images using RDN are similar to the original images, and the SR deep learning models subjectively improve the visibility of the prostatic urinary tract. © 2023 Yoshimura et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Deep Learning; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Male; Prostate; Reproducibility of Results; Urethra; adult; aged; Article; clinical article; complex wavelet structural similarity index measure; controlled study; deep learning; enhanced deep super resolution network; human; image analysis; image processing; image quality; interrater reliability; male; nuclear magnetic resonance imaging; observational study; post urination magnetic resonance imaging; prostate; prostate cancer; proton therapy; quantitative analysis; radiation oncologist; residual dense network; retrospective study; super resolution deep learning; super resolution generative adversarial network; T2 weighted imaging; turbo spin echo imaging sequence; two-dimensional imaging; urinary tract; widely activated super resolution network; diagnostic imaging; procedures; prostate; reproducibility; urethra","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145946270"
"Harris L.; McRae A.T.T.; Chantry M.; Dueben P.D.; Palmer T.N.","Harris, Lucy (57579658300); McRae, Andrew T. T. (56035467200); Chantry, Matthew (56126198500); Dueben, Peter D. (54879515900); Palmer, Tim N. (55924208000)","57579658300; 56035467200; 56126198500; 54879515900; 55924208000","A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts","2022","Journal of Advances in Modeling Earth Systems","14","10","e2022MS003120","","","","10.1029/2022MS003120","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141707766&doi=10.1029%2f2022MS003120&partnerID=40&md5=5da7dd5b909846f06521e3a1fe499442","Despite continuous improvements, precipitation forecasts are still not as accurate and reliable as those of other meteorological variables. A major contributing factor to this is that several key processes affecting precipitation distribution and intensity occur below the resolved scale of global weather models. Generative adversarial networks (GANs) have been demonstrated by the computer vision community to be successful at super-resolution problems, that is, learning to add fine-scale structure to coarse images. Leinonen et al. (2020, https://doi.org/10.1109/TGRS.2020.3032790) previously applied a GAN to produce ensembles of reconstructed high-resolution atmospheric fields, given coarsened input data. In this paper, we demonstrate this approach can be extended to the more challenging problem of increasing the accuracy and resolution of comparatively low-resolution input from a weather forecasting model, using high-resolution radar measurements as a “ground truth.” The neural network must learn to add resolution and structure whilst accounting for non-negligible forecast error. We show that GANs and VAE-GANs can match the statistical properties of state-of-the-art pointwise post-processing methods whilst creating high-resolution, spatially coherent precipitation maps. Our model compares favorably to the best existing downscaling methods in both pixel-wise and pooled CRPS scores, power spectrum information and rank histograms (used to assess calibration). We test our models and show that they perform in a range of scenarios, including heavy rainfall. © 2022 The Authors. Journal of Advances in Modeling Earth Systems published by Wiley Periodicals LLC on behalf of American Geophysical Union.","Deep learning; Learning systems; Radar measurement; Rain; Stochastic systems; Weather forecasting; Deep learning; Down-scaling; High resolution; Learning approach; Machine-learning; Neural-networks; Postprocessing; Precipitation forecast; Precipitation forecasting; Stochastics; accuracy assessment; artificial neural network; computer vision; data processing; downscaling; machine learning; precipitation (climatology); weather forecasting; Generative adversarial networks","deep learning; downscaling; machine learning; neural networks; postprocessing; precipitation forecasting","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141707766"
"Imperatore N.; Dumas L.","Imperatore, N. (57750400800); Dumas, L. (57191202595)","57750400800; 57191202595","CONTRIBUTION OF SUPER RESOLUTION TO 3D RECONSTRUCTION FROM PAIRS OF SATELLITE IMAGES","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","2","","61","68","7","10.5194/isprs-annals-V-2-2022-61-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132260019&doi=10.5194%2fisprs-annals-V-2-2022-61-2022&partnerID=40&md5=d0b3c5db4d8d0521a0f3e9116bac613e","The photogrammetric 3D stereo reconstruction from pairs of strereo images is rising interest in the past few years in space field downstream. Nowadays, it is conceivable that a large production of DSMs from satellite images can become the primary source of 3D information on a global scale. However, in urban areas, DSMs produced with current technology suffer from poor quality. Indeed, even using very high resolution (VHR) images, there is too little information to generate disparity maps that reproduce very well defined shaped objects such as buildings. To address this issue, one solution may be to artificially increase image resolution beyond the sensor limits. Super resolution (SR) algorithms are designed to recover high frequencies, introducing significant information in a scene characterized by strong and frequent discontinuities such as a city. State-of-the-art methods relying on Deep Learning have shown remarkable results in this sense. The aim of this work is therefore to assess the contribution of single image SR Deep Learning techniques to the stereo matching and DSMs generation in an urban context, highlighting potential advantages and limitations that can show up when introducing such a technology in a multi-view stereo pipeline. The proposed contributions are: a methodology for super resolution of VHR data that takes into account realistic simulation of a satellite product; a testbed for the evaluation of the impact of super resolution on 3D photogrammetric reconstruction; a local analysis of the consequences of deep learning SR of VHR images on stereo matching.  © 2022 N. Imperatore.","Deep neural networks; Generative adversarial networks; Image reconstruction; Learning systems; Photogrammetry; Satellites; Stereo image processing; Three dimensional computer graphics; 3D reconstruction; 3d stereos; Digital surface models; Down-stream; Primary sources; Satellite images; Stereo reconstruction; Stereo-matching; Superresolution; Very high resolution image; Image resolution","Deep Neural Network; Digital Surface Model; Generative Adversarial Network; Stereo-Matching; Super resolution","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132260019"
"Wang C.; Jiang J.; Zhong Z.; Liu X.","Wang, Chenyang (57215965241); Jiang, Junjun (54902306100); Zhong, Zhiwei (57208238541); Liu, Xianming (35230216700)","57215965241; 54902306100; 57208238541; 35230216700","Propagating Facial Prior Knowledge for Multitask Learning in Face Super-Resolution","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","11","","7317","7331","14","10.1109/TCSVT.2022.3181828","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132780730&doi=10.1109%2fTCSVT.2022.3181828&partnerID=40&md5=0ea5d0f3ed2463848cd5893a821bc919","Existing face hallucination methods always achieve improved performance through regularizing the model with facial prior. Most of them always estimate facial prior information first and then leverage it to help the prediction of the target high-resolution face image. However, the accuracy of prior estimation is difficult to guarantee, especially for the low-resolution face image. Once the estimated prior is inaccurate or wrong, the following face super-resolution performance is unavoidably influenced. A natural question that arises: how to incorporate facial prior effectively and efficiently without prior estimation? To achieve this goal, we propose to learn facial prior knowledge at training stage, but test only with low-resolution face image, which can overcome the difficulty of estimating accurate prior. In addition, instead of estimating facial prior, we directly explore the potential of high-quality facial prior in the training phase and progressively propagate the facial prior knowledge from the teacher network (trained with the low-resolution face/high-quality facial prior and high-resolution face image pairs) to the student network (trained with the low-resolution face and high-resolution face image pairs). Quantitative and qualitative comparisons on benchmark face datasets demonstrate that our method outperforms the state-of-the-art face super-resolution methods. The source codes of the proposed method will be available at https://github.com/wcy-cs/KDFSRNet.  © 1991-2012 IEEE.","Computer vision; Distillation; Generative adversarial networks; Learning systems; Optical resolving power; Personnel training; Face; Face hallucination; Face images; Face super-resolution; Facial prior; High resolution; Knowledge distillation; Performance; Prior-knowledge; Superresolution; Face recognition","Face hallucination; face super-resolution; facial prior; knowledge distillation","Article","Final","","Scopus","2-s2.0-85132780730"
"Zhong Z.; Chen Y.; Hou S.; Wang B.; Liu Y.; Geng J.; Fan S.; Wang D.; Zhang X.","Zhong, Zheng (57197732646); Chen, Yuan (56300201500); Hou, Sizu (7201471361); Wang, Bowen (55615255000); Liu, Yunpeng (56514207200); Geng, Jianghai (35753257200); Fan, Shuochao (56150935500); Wang, Dewen (57450452500); Zhang, Xu (57774449400)","57197732646; 56300201500; 7201471361; 55615255000; 56514207200; 35753257200; 56150935500; 57450452500; 57774449400","Super-resolution reconstruction method of infrared images of composite insulators with abnormal heating based on improved SRGAN","2022","IET Generation, Transmission and Distribution","16","10","","2063","2073","10","10.1049/gtd2.12414","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124535255&doi=10.1049%2fgtd2.12414&partnerID=40&md5=6e513d178356cc216862e5a287316d57","Abnormal heating of composite insulators on ultra-high voltage (UHV) transmission lines is widespread under high humidity and heat in southern China. The infrared imaging technology can be used of quickly and effectively detect insulators with abnormal heating and determine their defect types. However, it may cause misjudgement when the infrared images are not clear. A super-resolution reconstruction method of infrared images of composite insulators with abnormal heating based on improved super-resolution generative adversarial networks (SRGAN) is proposed in this paper, and the dense residual network in SRGAN is perfected by introducing the residual channel attention (ResCA). This method overcomes the problems that the resolution of images reconstructed by traditional methods is not significantly improved, and images are too smooth, and the edge details are easily lost. Compared with the traditional methods, the five reference-free image quality evaluation indexes of the image reconstructed by the improved SRGAN are increased by −0.73% to 63.84%. Further the effectiveness and superiority of the improved SRGAN are verified by comparing the changes in the distribution of grey values of infrared images before and after reconstruction. Finally, the improved SRGAN is used to perform super-resolution reconstruction of the low-resolution infrared image examples. © 2022 The Authors. IET Generation, Transmission & Distribution published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Generative adversarial networks; Heating; Image enhancement; Image reconstruction; Insulating materials; Thermography (imaging); Composite insulators; Defect type; High humidity; Infrared imaging technology; Reconstruction method; Reference-free; Southern China; Super-resolution reconstruction; Superresolution; Ultrahigh voltage transmission lines; Optical resolving power","","Article","Final","","Scopus","2-s2.0-85124535255"
"Wang Y.-R.J.; Wang P.; Adams L.C.; Sheybani N.D.; Qu L.; Sarrami A.H.; Theruvath A.J.; Gatidis S.; Ho T.; Zhou Q.; Pribnow A.; Thakor A.S.; Rubin D.; Daldrup-Link H.E.","Wang, Yan-Ran (Joyce) (57221810132); Wang, Pengcheng (57213430335); Adams, Lisa Christine (57192991712); Sheybani, Natasha Diba (56268584600); Qu, Liangqiong (57217038105); Sarrami, Amir Hossein (50562157500); Theruvath, Ashok Joseph (36015982400); Gatidis, Sergios (26658018800); Ho, Tina (58062240600); Zhou, Quan (58062150900); Pribnow, Allison (57194045167); Thakor, Avnesh S. (8948702600); Rubin, Daniel (57977832600); Daldrup-Link, Heike E. (6604068073)","57221810132; 57213430335; 57192991712; 56268584600; 57217038105; 50562157500; 36015982400; 26658018800; 58062240600; 58062150900; 57194045167; 8948702600; 57977832600; 6604068073","Low-count whole-body PET/MRI restoration: an evaluation of dose reduction spectrum and five state-of-the-art artificial intelligence models","2023","European Journal of Nuclear Medicine and Molecular Imaging","","","","","","","10.1007/s00259-022-06097-w","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146174106&doi=10.1007%2fs00259-022-06097-w&partnerID=40&md5=735f21ecad3165cf7b472422543cb483","Purpose: To provide a holistic and complete comparison of the five most advanced AI models in the augmentation of low-dose 18F-FDG PET data over the entire dose reduction spectrum. Methods: In this multicenter study, five AI models were investigated for restoring low-count whole-body PET/MRI, covering convolutional benchmarks — U-Net, enhanced deep super-resolution network (EDSR), generative adversarial network (GAN) — and the most cutting-edge image reconstruction transformer models in computer vision to date — Swin transformer image restoration network (SwinIR) and EDSR-ViT (vision transformer). The models were evaluated against six groups of count levels representing the simulated 75%, 50%, 25%, 12.5%, 6.25%, and 1% (extremely ultra-low-count) of the clinical standard 3 MBq/kg 18F-FDG dose. The comparisons were performed upon two independent cohorts — (1) a primary cohort from Stanford University and (2) a cross-continental external validation cohort from Tübingen University — in order to ensure the findings are generalizable. A total of 476 original count and simulated low-count whole-body PET/MRI scans were incorporated into this analysis. Results: For low-count PET restoration on the primary cohort, the mean structural similarity index (SSIM) scores for dose 6.25% were 0.898 (95% CI, 0.887–0.910) for EDSR, 0.893 (0.881–0.905) for EDSR-ViT, 0.873 (0.859–0.887) for GAN, 0.885 (0.873–0.898) for U-Net, and 0.910 (0.900–0.920) for SwinIR. In continuation, SwinIR and U-Net’s performances were also discreetly evaluated at each simulated radiotracer dose levels. Using the primary Stanford cohort, the mean diagnostic image quality (DIQ; 5-point Likert scale) scores of SwinIR restoration were 5 (SD, 0) for dose 75%, 4.50 (0.535) for dose 50%, 3.75 (0.463) for dose 25%, 3.25 (0.463) for dose 12.5%, 4 (0.926) for dose 6.25%, and 2.5 (0.534) for dose 1%. Conclusion: Compared to low-count PET images, with near-to or nondiagnostic images at higher dose reduction levels (up to 6.25%), both SwinIR and U-Net significantly improve the diagnostic quality of PET images. A radiotracer dose reduction to 1% of the current clinical standard radiotracer dose is out of scope for current AI techniques. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","","CNN; Deep learning; PET restoration; Transformer model; Whole-body PET imaging","Article","Article in press","","Scopus","2-s2.0-85146174106"
"Jing H.; Shi J.; Qiu M.; Qi Y.; Zhu W.","Jing, Haizhao (57759652300); Shi, Jianglin (57207270235); Qiu, Mengzhe (57915825600); Qi, Yong (57760216500); Zhu, Wenxiao (57936774800)","57759652300; 57207270235; 57915825600; 57760216500; 57936774800","Super-resolution reconstruction method for space target images based on dense residual block-based GAN; [基于密集残差块生成对抗网络的空间目标图像超分辨率重建]","2022","Guangxue Jingmi Gongcheng/Optics and Precision Engineering","30","17","","2155","2165","10","10.37188/OPE.20223017.2155","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140292188&doi=10.37188%2fOPE.20223017.2155&partnerID=40&md5=5b447d74027034a3effeb28e9b256c36","To obtain the optical images of space targets with higher resolution and clarity， it is necessary to perform super-resolution reconstruction on the degraded images corrected by ground-based adaptive optics （AO） imaging telescopes. The image super-resolution reconstruction method based on deep learning has a fast operation speed and provides rich high-frequency detail information of the image； it has been widely used in natural， medical， and remote sensing images， among other applications. Aiming at the characteristics of spatial target AO images with a single background， limited resolution， motion blur， turbulent blur， and overexposure， this study proposes using a deep learning-based generative adversarial network （GAN） method to realize the super-resolution of spatial target AO images. For resolution reconstruction， a training set of spatial target AO simulation images is first constructed for neural network training， and a GAN super-resolution reconstruction method based on dense residual blocks is then proposed. By changing the traditional residual network to dense residual blocks， improving the network depth， and introducing a relative average loss function into the discriminator network， the discriminator becomes more robust， and the training of the generative adversarial network becomes more stable. Experiments show that the proposed method improves the peak-to-noise ratio （PSNR） and structural similarity index measure （SSIM） by more than 11.6% and 10.3%， respectively， compared with traditional interpolation super-resolution methods. In addition， it improves the PSNR and SSIM by 6.5% and 4.9% on average， respectively， compared with the deep learning-based blind image super-resolution method. The proposed method effectively realizes the clear reconstruction of a spatial target AO image， reduces the artifacts of the reconstructed image， enriches image details， and achieves a better reconstruction effect. © 2022 Guangxue Jingmi Gongcheng/Optics and Precision Engineering. All rights reserved.","Adaptive optics; Deep learning; Geometrical optics; Image enhancement; Image reconstruction; Learning systems; Medical imaging; Neural networks; Optical remote sensing; Optical resolving power; Dense residual block; Generative adversarial network（GAN）; Reconstruction method; Space object image; Space objects; Space targets; Super-resolution reconstruction; Superresolution; Superresolution methods; Target images; Generative adversarial networks","dense residual blocks; Generative Adversarial Network（GAN）; space object images; super resolution","Article","Final","","Scopus","2-s2.0-85140292188"
"Liu C.; Li H.; Liang Z.; Zhang Y.; Yan Y.; Zhong R.Y.; Peng S.","Liu, Changhong (55680773200); Li, Hongyin (57928961800); Liang, Zhongwei (57885785300); Zhang, Yongjun (57928961900); Yan, Yier (57929523100); Zhong, Ray Y. (55353690000); Peng, Shaohu (25927484000)","55680773200; 57928961800; 57885785300; 57928961900; 57929523100; 55353690000; 25927484000","A Novel Deep-Learning-Based Enhanced Texture Transformer Network for Reference Image Super-Resolution","2022","Electronics (Switzerland)","11","19","3038","","","","10.3390/electronics11193038","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139915061&doi=10.3390%2felectronics11193038&partnerID=40&md5=1c0d24c0463cf8168939f00e1a972a79","The study explored a deep learning image super-resolution approach which is commonly used in face recognition, video perception and other fields. These generative adversarial networks usually have high-frequency texture details. The relevant textures of high-resolution images could be transferred as reference images to low-resolution images. The latest existing methods use transformer ideas to transfer related textures to low-resolution images, but there are still some problems with channel learning and detailed textures. Therefore, the study proposed an enhanced texture transformer network (ETTN) to improve the channel learning ability and details of the texture. It could learn the corresponding structural information of high-resolution texture images and convert it into low-resolution texture images. Through this, finding the feature map can change the exact feature of images and improve the learning ability between channels. We then used multi-scale feature integration (MSFI) to further enhance the effect of fusion and achieved different degrees of texture restoration. The experimental results show that the model has a good resolution enhancement effect on texture transformers. In different datasets, the peak signal to noise ratio (PSNR) and structural similarity (SSIM) were improved by 0.1–0.5 dB and 0.02, respectively. © 2022 by the authors.","","attention mechanism; deep learning; generative adversarial network; super-resolution; texture transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139915061"
"Chen Z.; Zhang H.; Zeng N.; Li H.","Chen, Zining (57697450700); Zhang, Hongyi (57203318760); Zeng, Nianyin (36703553900); Li, Han (57206473363)","57697450700; 57203318760; 36703553900; 57206473363","Attention mechanism embedded multi-scale restoration method for blurred image; [融合注意力机制的模糊图像多尺度复原]","2022","Journal of Image and Graphics","27","5","","1682","1696","14","10.11834/jig.210249","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130258547&doi=10.11834%2fjig.210249&partnerID=40&md5=e67decb1ac09b6dbd55498791b49b00a","Objective: Image de-blurring task aims at a qualified image derived from the low-quality blurred one. Traditional fuzzy kernels based de-blurring techniques are challenging to sort out ideal fuzzy kernel for each pixel. Current restoration methods are based on manpowered prior knowledge of the images. Simultaneously, generalization capability is constrained to be extended. To harness image de-blurring processing, convolutional neural network (CNN)model has its priority in computer vision in the context of deep learning techniques. Nevertheless, poor CNN structure adaptive issues like over-fitting are extremely constrained of parameters and topology. A challenged image de-blurring tasks is to capture detailed texture. Tiny feature constrained images restoration have their deficiencies like inadequate detail information and indistinct image edge. To facilitate image in-painting and super-resolution restoration, generative adversarial networks (GAN) has its priority to preserve texture details.an adversarial network based end-to-end framework for image de-blurring is demonstrated based on GAN-based image-to-image translation, which can speed up multifaceted image restoration process. Method: First, a variety of modified residual blocks are cascaded to build a multi-scale architecture, which facilitates extracting features from coarse to fine so that more texture details in a blurred image could be well restored. Next, extensive convolution module extended the receptive fields in parallel with no computational burden. Thirdly, channel attention mechanism is also applied to strengthen weights of useful features and suppress the invalid ones simultaneously via inter-channel modeling interdependencies. Finally, network-based perceptual loss is integrated with conventional mean squared error (MSE) to serve as the total loss function in order to maintain the fidelity of the image content. Consequently, our restored images quality can be guaranteed on the aspect of qualified semantic and fine texture details both. The application of minimum MSE loss between pixels also makes the generated de-blurred image have smoother edge information. Result: GoPro database is adopted for our model training and testing, including 3 214 pairs of samples among which 2 013 pairs are used as training set and the remaining 1 111 pairs serve as testing data. To enhance the generalization capability of the network, data augmentation techniques like flipping and random angle rotation are conducted. Each training sample is randomly cropped into 256×256 pixels resolution images, and pixel values of clear and blurred images are all normalized to the range of [-1, 1]. In order to comprehensively evaluate the our method, several indicators like peak signal to noise ratio (PSNR), structural similarity (SSIM) and restoration time are used for evaluation. Our experimental results have demonstrated that overall performance of proposed method is satisfactory, which can effectively eliminate the blurred region in images. Compared with some other existing works, our method increases PSNR by 3.8% in less running time, which indicates the feasibility and superiority of our proposal. Restored images obtained by proposed method have clarified edges, and our method is efficient to restore the blurry images with different sizes of blur kernels to a certain extent. It is also found that while applying the restored images to the YOLO-v4(you only look once) object detection task, the results have been significantly improved regarding the identification accuracy and the confidence coefficient both, which reflects that designed strategies in proposed method. Conclusion: Our image de-blurring method aims at blurred images and extracts features sufficiently from coarse to fine. Specifically, multi-scale improved residual blocks are cascaded for learned subtle texture details. Parallel enlarged convolution and channel attention mechanism are also intervened in our model to improve their adopted residual blocks capability. Moreover, trained loss function is modified via perceptual loss introducing to traditional mean square errors. Consequently, quality of restored images can be guaranteed to some extent like sharper edges and more abundant detail information. Our analyzed results have demonstrated their qualified and efficient priorities. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Attention mechanism; Deep learning; Generative adversarial network(GAN); Image restoration; Multi-scale","Article","Final","","Scopus","2-s2.0-85130258547"
"Guo M.; Zhang Z.; Liu H.; Huang Y.","Guo, Mingqiang (34872040100); Zhang, Zeyuan (57567072200); Liu, Heng (57022065500); Huang, Ying (57013696500)","34872040100; 57567072200; 57022065500; 57013696500","NDSRGAN: A Novel Dense Generative Adversarial Network for Real Aerial Imagery Super-Resolution Reconstruction","2022","Remote Sensing","14","7","1574","","","","10.3390/rs14071574","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127852163&doi=10.3390%2frs14071574&partnerID=40&md5=3101b5c6877bde898c09b50c7a40375b","In recent years, more and more researchers have used deep learning methods for super-resolution reconstruction and have made good progress. However, most of the existing super-resolution reconstruction models generate low-resolution images for training by downsampling high-resolution images through bicubic interpolation, and the models trained from these data have poor reconstruction results on real-world low-resolution images. In the field of unmanned aerial vehicle (UAV) aerial photography, the use of existing super-resolution reconstruction models in reconstructing real-world low-resolution aerial images captured by UAVs is prone to producing some artifacts, texture detail distortion and other problems, due to compression and fusion processing of the aerial images, thereby resulting in serious loss of texture detail in the obtained low-resolution aerial images. To address this problem, this paper proposes a novel dense generative adversarial network for real aerial imagery super-resolution reconstruction (NDSRGAN), and we produce image datasets with paired high-and low-resolution real aerial remote sensing images. In the generative network, we use a multilevel dense network to connect the dense connections in a residual dense block. In the discriminative network, we use a matrix mean discriminator that can discriminate the generated images locally, no longer discriminating the whole input image using a single value but instead in chunks of regions. We also use smoothL1 loss instead of the L1 loss used in most existing super-resolution models, to accelerate the model convergence and reach the global optimum faster. Compared with traditional models, our model can better utilise the feature information in the original image and discriminate the image in patches. A series of experiments is conducted with real aerial imagery datasets, and the results show that our model achieves good performance on quantitative metrics and visual perception. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Aerial photography; Antennas; Deep learning; Generative adversarial networks; Image reconstruction; Optical resolving power; Textures; Unmanned aerial vehicles (UAV); Aerial imagery; Aerial images; Deep learning; Down sampling; Learning methods; Low resolution images; Lower resolution; Real-world; Remote-sensing; Super-resolution reconstruction; Remote sensing","aerial imagery; deep learning; generative adversarial network; remote sensing; super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127852163"
"Izumi T.; Amagasaki M.; Ishida K.; Kiyama M.","Izumi, Tomoki (57681699500); Amagasaki, Motoki (19639698300); Ishida, Kei (57681422800); Kiyama, Masato (16309870000)","57681699500; 19639698300; 57681422800; 16309870000","Super-resolution of sea surface temperature with convolutional neural network-and generative adversarial network-based methods","2022","Journal of Water and Climate Change","13","4","","1673","1683","10","10.2166/wcc.2022.291","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129995801&doi=10.2166%2fwcc.2022.291&partnerID=40&md5=3a9ba0dc383b9903866d472672ba3665","In this paper, we perform the super-resolution of sea surface temperature data with the enhanced super-resolution generative adversarial network (ESRGAN), which is a deep neural network-based single-image super-resolution (SISR) method that uses a generative adversarial network (GAN). We generate high-quality super-resolution data with ESRGAN and with the super-resolution convolutional neural network (SRCNN) and residual-in-residual dense block network (RRDBNet) methods, which are based on convolutional neural networks (CNNs). The images generated with these methods are compared with high-resolution optimum interpolation sea surface temperature (OISST) data using root mean square error (RMSE), learned perceptual image patch similarity (LPIPS), and perceptual index (PI) evaluation methods. RRDBNet has a better RMSE than SRCNN and ESRGAN. However, CNN-based SISR methods do not provide a faithful representation of the ocean currents of OISST. ESRGAN has a better LPIPS and PI than CNN-based methods and can represent the complex distribution of ocean currents. © 2022.","Atmospheric temperature; Convolution; Convolutional neural networks; Deep neural networks; Image enhancement; Mean square error; Ocean currents; Optical resolving power; Submarine geophysics; Surface properties; Surface waters; Convolutional neural network; Enhanced super-resolution generative adversarial network; Image super resolutions; Network-based; Residual-in-residual dense block network; Single images; Single-image super-resolution; Superresolution; Superresolution methods; Temperature data; artificial neural network; comparative study; data quality; error analysis; image analysis; image resolution; oceanic current; sea surface temperature; Generative adversarial networks","convolutional neural network; ESRGAN; generative adversarial network; RRDBNet; single-image super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85129995801"
"Salmi A.; Benierbah S.; Ghazi M.","Salmi, Abderrahmane (57440844800); Benierbah, Said (6506916542); Ghazi, Mehdi (57441451600)","57440844800; 6506916542; 57441451600","Low complexity image enhancement GAN-based algorithm for improving low-resolution image crop disease recognition and diagnosis","2022","Multimedia Tools and Applications","81","6","","8519","8538","19","10.1007/s11042-022-12256-w","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124082731&doi=10.1007%2fs11042-022-12256-w&partnerID=40&md5=2370a831f4917a89822d45f90e47773a","Image analysis plays a crucial role in many real-world applications such as smart agriculture. For plant diseases diagnosis, one of the most recent challenges is the improvement of the plant diseases classification on Low-Resolution (LR) images. The farmer is supposed to obtain High-Resolution (HR) images of plant leaves from the field. Because of the small size of plant leaves and other limitations, the obtained HR images can miss some detailed information that results in blurred LR images of leaves with fewer details. In this paper, we introduce a novel Super-Resolution (SR) algorithm named Wider-activation for Attention-mechanism based on a Generative Adversarial Network (WAGAN) to improve the classification of the tomato diseases LR images. The WAGAN consists of three main parts; the generator network, which has the Wider Activation for Residual Channel Attention Block (WARCAB) as the principal block, the discriminator network and the adversarial loss. To evaluate the potential of the proposed method in plant diseases recognition, we first recovered SR plant diseases images from LR images using the WAGAN. Next, we implemented diseases classification using LR, SR, and HR images. The results proved the efficacy of the proposed method (97.63%) with ×3.6 lower complexity than the state-of-the-art method and very close to the reference HR (97.81%) accuracy. Due to the efficient design of WARCAB and the adversarial loss, the WAGAN focuses more on edges, textures, and other valuable information, which are the key information, needed for the classifier to recognize the disease. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Agriculture; Classification (of information); Complex networks; Computer aided diagnosis; Generative adversarial networks; Image analysis; Image classification; Image enhancement; Optical resolving power; Plants (botany); Textures; Crop disease; Disease classification; High-resolution images; Image-analysis; Low resolution images; Lower complexity; Plant disease; Plant leaves; Smart agricultures; Superresolution; Chemical activation","Diseases classification; Image analysis; Plant diseases; Smart agriculture; Super-resolution","Article","Final","","Scopus","2-s2.0-85124082731"
"Rai D.; Rajput S.S.","Rai, Deepak (57985227600); Rajput, Shyam Singh (7003703543)","57985227600; 7003703543","Robust Face Hallucination Algorithm Using Motion Blur Embedded Nearest Proximate Patch Representation","2023","IEEE Transactions on Instrumentation and Measurement","72","","5002510","","","","10.1109/TIM.2022.3223141","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142813118&doi=10.1109%2fTIM.2022.3223141&partnerID=40&md5=ef8de39bc6a82ac5727428d2dbbd2b0e","Face hallucination (FH) techniques have received a lot of attention in recent years for generating high-resolution (HR) face images from captured low-resolution (LR), noisy, and blurry images. However, existing FH techniques are incapable of dealing with motion blur, which is commonly introduced in captured images due to camera defocussing and other factors. Therefore, to make the FH process more resistant to motion blur, in this article, we present a novel learning-based FH algorithm called Motion Blur Embedded Nearest Proximate Patch Representation (MBENPPR). The MBENPPR algorithm begins by estimating the motion blur kernel from a motion-blurred LR test face. The estimated kernel is then embedded in training images to make them compatible with test images. It assists in reducing the effect of motion blur in the reconstruction process. Furthermore, the nearest proximate patches are selected from the training space to represent the test image patches as a weighted linear combination of selected patches. It facilitates the proposed algorithm in preserving sharp edges and texture information in the resulting faces. The results of simulations on standard datasets and locally captured real-life faces show that the MBENPPR algorithm outperforms the compared existing algorithms. © 1963-2012 IEEE.","Edge detection; Face recognition; Generative adversarial networks; Image reconstruction; Face; Face hallucination; High resolution; Images reconstruction; Kernel; Motion blur; Position patches; Proximate patch; Superresolution; Test images; Image resolution","Face hallucination (FH); motion-blur; position-patch; proximate patch; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85142813118"
"Viriyavisuthisakul S.; Kaothanthong N.; Sanguansat P.; Nguyen M.L.; Haruechaiyasak C.","Viriyavisuthisakul, Supatta (57095072800); Kaothanthong, Natsuda (36767966300); Sanguansat, Parinya (8576229900); Nguyen, Minh Le (55664630500); Haruechaiyasak, Choochart (15050636000)","57095072800; 36767966300; 8576229900; 55664630500; 15050636000","Parametric regularization loss in super-resolution reconstruction","2022","Machine Vision and Applications","33","5","71","","","","10.1007/s00138-022-01315-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135800275&doi=10.1007%2fs00138-022-01315-9&partnerID=40&md5=ec73bcea66353cbfe21ea5d59f16ea76","A noise-enhanced super-resolution generative adversarial network plus (nESRGAN+) was proposed to improve the enhanced super-resolution GAN (ESRGAN). The contributions of nESRGAN+ generate an impressive reconstructed image with more texture details and greater sharpness. However, the perceptual quality of the output lacks hallucinated details and undesirable artifacts and takes a long time to converge. To address these problems, we propose four types of parametric regularization algorithms as loss functions of the model to enable the iterative weight adjustment of the network gradient. Several experiments were conducted to confirm that the generator can achieve a better-quality reconstructed image, including restoring the unseen texture. Our method accomplished the average peak signal-to-noise ratio (PSNR) of the reconstructed image at 27.96 dB, the average Structural Similarity Index Measure (SSIM) at 0.8303, and the average Learned Perceptual Image Patch Similarity (LPIPS) at 0.1949. It took seven times less training time than the state of the art. In addition to the better visual quality of the reconstructed result, the proposed loss functions allow the generator to converge faster. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Generative adversarial networks; Image texture; Iterative methods; Optical resolving power; Signal to noise ratio; Textures; Images reconstruction; Iterative weight; Loss functions; Parametric; Perceptual quality; Reconstructed image; Regularisation; Regularization algorithms; Super-resolution reconstruction; Superresolution; Image reconstruction","Generative adversarial network; Image reconstruction; Loss function; Parametric; Regularization; Super-resolution","Article","Final","","Scopus","2-s2.0-85135800275"
"","","","4th International Conference on Emerging Technology Trends in Electronics, Communication and Networking, ET2ECN 2021","2023","Lecture Notes in Electrical Engineering","952","","","","","335","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144928882&partnerID=40&md5=3e99cf074e15c91e3154316af95645f6","The proceedings contain 26 papers. The special focus in this conference is on Emerging Technology Trends in Electronics, Communication and Networking. The topics include: GUI Development of IRNSS Receiver; A 1 Gbps VLC System Based on Daylight and Intensity Modulator; Performance of MISO Systems with Alamaouti Transmit Diversity and Antenna Selection in TDD and FDD; Performance Analysis of OFDM-Based Optical Wireless Communication System; Performance Comparison of Different Diversity and Combining Techniques Over Gamma–Gamma FSO Link; abstract Data Models and System Design for Big Data Geospatial Analytics; circularly Polarized Sector Patch Antenna with Fractal Defected Ground Structure; Two-Element MIMO Antenna with Polarization Diversity for 5G Application; machine Learning-Based Investigation of Employee Attrition Prediction and Analysis; Millimeter Wave Overmoded Circular Waveguide Tapers for ECRH Applications; CNN-Based Leaf Wilting Classification Using Modified ResNet152; Deep Learning-Based COVID-19 Detection Using Transfer Learning Through ResNet-50; OCR for Devanagari Script Using a Deep Hybrid CNN-RNN Network; a Dataset Preparation Framework for Education Data Mining; Generative Adversarial Network-Based Improved Progressive Approach for Image Super-Resolution: ImProSRGAN; community Detection Using Label Propagation Algorithm with Random Walk Approach; comparative Analysis of Generative Adversarial Network-Based Single-Image Super-Resolution Approaches; analysis of Logical Effort-Based Optimization in the Deep Submicron Technologies; a Single Electron Transistor-Based Floating Point Multiplier Realization at Room Temperature Operation; Comparison of Total Ionizing Dose Effect on Tolerance of SCL 180 nm Bulk and SOI CMOS Using TCAD Simulation; performance Analysis of Corrugated Horn Antenna for Liquid Level Measurement Application; Quad-Element with Penta-Band MIMO Antenna for 5G Millimeter-Wave Applications; Quad Band Planar Monopole Antenna with Polarization Diversity for FSS and SAR Application.","","","Conference review","Final","","Scopus","2-s2.0-85144928882"
"Arumugasamy M.K.; Bouazizi M.; Ohtsuki T.","Arumugasamy, Muthukumar Krishnan (57698123100); Bouazizi, Mondher (57188727792); Ohtsuki, Tomoaki (7202417835)","57698123100; 57188727792; 7202417835","An Infrared Array Sensor-Based Approach for Activity Detection, Combining Low-Cost Technology with Advanced Deep Learning Techniques","2022","Sensors","22","10","3898","","","","10.3390/s22103898","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130221811&doi=10.3390%2fs22103898&partnerID=40&md5=ef61cd369e4f9ea89c397b21cc94fbed","In this paper, we propose an activity detection system using a 24 × 32 resolution infrared array sensor placed on the ceiling. We first collect the data at different resolutions (i.e., 24 × 32, 12 × 16, and 6 × 8) and apply the advanced deep learning (DL) techniques of Super-Resolution (SR) and denoising to enhance the quality of the images. We then classify the images/sequences of images depending on the activities the subject is performing using a hybrid deep learning model combining a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM). We use data augmentation to improve the training of the neural networks by incorporating a wider variety of samples. The process of data augmentation is performed by a Conditional Generative Adversarial Network (CGAN). By enhancing the images using SR, removing the noise, and adding more training samples via data augmentation, our target is to improve the classification accuracy of the neural network. Through experiments, we show that employing these deep learning techniques to low-resolution noisy infrared images leads to a noticeable improvement in performance. The classification accuracy improved from 78.32% to 84.43% (for images with 6 × 8 resolution), and from 90.11% to 94.54% (for images with 12 × 16 resolution) when we used the CNN and CNN + LSTM networks, respectively. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep Learning; Image Processing, Computer-Assisted; Neural Networks, Computer; Technology; Classification (of information); Convolutional neural networks; Costs; Generative adversarial networks; Image classification; Image enhancement; Infrared imaging; Learning algorithms; Long short-term memory; Optical resolving power; Activity detection; Array sensors; Conditional generative adversarial network; De-noising; Deep learning; Infrared array sensor; Infrared arrays; Learning techniques; Lower resolution; Superresolution; image processing; procedures; technology; Computer vision","activity detection; CGAN; computer vision; deep learning; denoising; healthcare; infrared array sensor; low-resolution; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130221811"
"Yuan B.; Sun Z.; Pei L.; Li W.; Ding M.; Hao X.","Yuan, Bo (57222962760); Sun, Zhaoyun (15319533900); Pei, Lili (57210366953); Li, Wei (55172058400); Ding, Minghang (57207449745); Hao, Xueli (55355289900)","57222962760; 15319533900; 57210366953; 55172058400; 57207449745; 55355289900","Super-Resolution Reconstruction Method of Pavement Crack Images Based on an Improved Generative Adversarial Network","2022","Sensors","22","23","9092","","","","10.3390/s22239092","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143795116&doi=10.3390%2fs22239092&partnerID=40&md5=881ed0a27a222ee9ad3a054ae67b3820","A super-resolution reconstruction approach based on an improved generative adversarial network is presented to overcome the huge disparities in image quality due to variable equipment and illumination conditions in the image-collecting stage of intelligent pavement detection. The nonlinear network of the generator is first improved, and the Residual Dense Block (RDB) is created to serve as Batch Normalization (BN). The Attention Module is then formed by combining the RDB, Gated Recurrent Unit (GRU), and Conv Layer. Finally, a loss function based on the L1 norm is utilized to replace the original loss function. The experimental findings demonstrate that the self-built pavement crack dataset’s Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) of the reconstructed images reach 29.21 dB and 0.854, respectively. The results improved compared to the Set5, Set14, and BSD100 datasets. Additionally, by employing Faster-RCNN and a Fully Convolutional Network (FCN), the effects of image reconstruction on detection and segmentation are confirmed. The findings indicate that the segmentation results’ F1 is enhanced by 0.012 to 0.737 and the detection results’ confidence is increased by 0.031 to 0.9102 when compared to state-of-the-art methods. It has a significant engineering application value and can successfully increase pavement crack-detecting accuracy. © 2022 by the authors.","Crack detection; Deep learning; Generative adversarial networks; Image enhancement; Image segmentation; Optical resolving power; Pavements; Signal to noise ratio; Crack image; Deep learning; GAN; Images reconstruction; Loss functions; Pavement crack image; Pavement cracks; Reconstruction method; Super-resolution reconstruction; Superresolution; article; deep learning; image reconstruction; loss of function mutation; signal noise ratio; Image reconstruction","deep Learning; GAN; image reconstruction; pavement crack image; super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143795116"
"Frizza T.; Dansereau D.G.; Seresht N.M.; Bewley M.","Frizza, Tristan (57719531800); Dansereau, Donald G. (7004275679); Seresht, Nagita Mehr (57719531900); Bewley, Michael (55789301800)","57719531800; 7004275679; 57719531900; 55789301800","Semantically accurate super-resolution Generative Adversarial Networks","2022","Computer Vision and Image Understanding","221","","103464","","","","10.1016/j.cviu.2022.103464","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131076842&doi=10.1016%2fj.cviu.2022.103464&partnerID=40&md5=b27b11d5bb2721eb1a69c0a2a5eec91a","This work addresses the problems of semantic segmentation and image super-resolution by jointly considering the performance of both in training a Generative Adversarial Network (GAN). We propose a novel architecture and domain-specific feature loss, allowing super-resolution to operate as a pre-processing step to increase the performance of downstream computer vision tasks, specifically semantic segmentation. We demonstrate this approach using Nearmap's aerial imagery dataset which covers hundreds of urban areas at 5–7 cm per pixel resolution. We show the proposed approach improves perceived image quality as well as quantitative segmentation accuracy across all prediction classes, yielding an average accuracy improvement of 11.8% and 108% at 4× and 32× super-resolution, compared with state-of-the art single-network methods. This work demonstrates that jointly considering image-based and task-specific losses can improve the performance of both, and advances the state-of-the-art in semantic-aware super-resolution of aerial imagery. © 2022 Elsevier Inc.","Aerial photography; Antennas; Image enhancement; Optical resolving power; Semantic Segmentation; Semantic Web; Semantics; Aerial imagery; Image super resolutions; Multi-modal learning; Novel architecture; Novel domain; Performance; Semantic images; Semantic segmentation; State of the art; Superresolution; Generative adversarial networks","Generative adversarial networks; Multi-modal learning; Semantic segmentation; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131076842"
"Zhang W.; Liu Y.; Dong C.; Qiao Y.","Zhang, Wenlong (57200855630); Liu, Yihao (57206486742); Dong, Chao (56335662200); Qiao, Yu (57685352100)","57200855630; 57206486742; 56335662200; 57685352100","RankSRGAN: Super Resolution Generative Adversarial Networks With Learning to Rank","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","10","","7149","7166","17","10.1109/TPAMI.2021.3096327","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138445896&doi=10.1109%2fTPAMI.2021.3096327&partnerID=40&md5=3f6a1f14372c4e45d91ce8e773d93d9d","Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of different perceptual metrics. Specifically, we first train a Ranker which can learn the behaviour of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Furthermore, we extend our method to multiple Rankers to provide multi-dimension constraints for the generator. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics and quality. Project page: https://wenlongzhang0517.github.io/Projects/RankSRGAN.  © 1979-2012 IEEE.","Algorithms; Benchmarking; Humans; Image Processing, Computer-Assisted; Computer vision; Optical resolving power; Adversarial networks; Features extraction; Generator; Highly-correlated; Image super resolutions; Perceptual metrics; Perceptual quality; Single images; Superresolution; Visual qualities; algorithm; benchmarking; human; image processing; procedures; Generative adversarial networks","generative adversarial network; Image super resolution; learning to rank","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138445896"
"Zeng W.; Zhang W.; Chen P.; Hu G.; Liang D.","Zeng, Weihui (56149611600); Zhang, Wenfeng (57205393153); Chen, Peng (56881789100); Hu, Gensheng (8644853100); Liang, Dong (56742660800)","56149611600; 57205393153; 56881789100; 8644853100; 56742660800","Low-resolution Rice Pest Image Recognition Based on SCResNeSt; [基于SCResNeSt的低分辨率水稻害虫图像识别方法]","2022","Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery","53","9","","277","285","8","10.6041/j.issn.1000-1298.2022.09.028","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141073077&doi=10.6041%2fj.issn.1000-1298.2022.09.028&partnerID=40&md5=fe2147d5cf15ebeb864a402629e6f811","It was difficult to take high-quality images when pests were still and in close distance in the natural environment of rice field, which led to the problem that satisfactory identification accuracy could not be achieved when using the actual environmental identification model detection. A low-resolution rice pest image recognition method based on self-calibrated convolutions and ResNeSt block for ResNet50 (SCResNeSt) was proposed. Firstly, the enhanced super-resolution generative adversarial networks (ESRGAN) super partition network was used to enhance the data of low-resolution images to solve the problem of less effective information about rice pests. In SCResNeSt network, three consecutive 3 × 3 convolutional layers were used to replace the first 7 × 7 convolutional layer to reduce the computational cost. Using self-calibrated convolution instead of the 3 × 3 convolution in layer 2, through internal communication, the field of view of each convolutional layer was explicitly extended to obtain part of the background information of pest images, to enrich the output features. The split-attention network block (ResNeSt block) was used in the backbone network to further improve the accuracy of obtaining pest information in the image. Finally, the optimized model was deployed on the mobile terminal, and a lightweight mobile rice pest identification system was developed. The experimental results showed that compared with the existing methods, the ESRGAN model could recover the real information about crop pests, and the SCResNeSt model could effectively improve the performance of rice pest identification, the accuracy can reach 91. 20%, which showed that the depth model could accurately identify rice pest types. The research result can provide an important technical basis for the intelligent identification and control of rice pests, and it would improve the level of rice production informatization. © 2022 Chinese Society of Agricultural Machinery. All rights reserved.","Convolutional neural networks; Generative adversarial networks; Image enhancement; Image recognition; Closest distance; Convolutional neural network; High quality images; Image of rice pest; Lower resolution; Natural environments; Pests images; Rice pests; Self-calibrated convolution and resnest block for resnet50; Superresolution; Convolution","convolutional neural network; identification system; image of rice pests; low-resolution; SCResNeSt","Article","Final","","Scopus","2-s2.0-85141073077"
"Apostolopoulos I.D.; Papathanasiou N.D.; Apostolopoulos D.J.; Panayiotakis G.S.","Apostolopoulos, Ioannis D. (57195641603); Papathanasiou, Nikolaos D. (23995562200); Apostolopoulos, Dimitris J. (24068454400); Panayiotakis, George S. (7006755481)","57195641603; 23995562200; 24068454400; 7006755481","Applications of Generative Adversarial Networks (GANs) in Positron Emission Tomography (PET) imaging: A review","2022","European Journal of Nuclear Medicine and Molecular Imaging","49","11","","3717","3739","22","10.1007/s00259-022-05805-w","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128743709&doi=10.1007%2fs00259-022-05805-w&partnerID=40&md5=0f68fe139fab472500baa33b207c781f","Purpose: This paper reviews recent applications of Generative Adversarial Networks (GANs) in Positron Emission Tomography (PET) imaging. Recent advances in Deep Learning (DL) and GANs catalysed the research of their applications in medical imaging modalities. As a result, several unique GAN topologies have emerged and been assessed in an experimental environment over the last two years. Methods: The present work extensively describes GAN architectures and their applications in PET imaging. The identification of relevant publications was performed via approved publication indexing websites and repositories. Web of Science, Scopus, and Google Scholar were the major sources of information. Results: The research identified a hundred articles that address PET imaging applications such as attenuation correction, de-noising, scatter correction, removal of artefacts, image fusion, high-dose image estimation, super-resolution, segmentation, and cross-modality synthesis. These applications are presented and accompanied by the corresponding research works. Conclusion: GANs are rapidly employed in PET imaging tasks. However, specific limitations must be eliminated to reach their full potential and gain the medical community's trust in everyday clinical practice. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Artifacts; Humans; Image Processing, Computer-Assisted; Positron-Emission Tomography; artifact; catalysis; clinical practice; deep learning; diagnostic imaging; drug megadose; human; nuclear medicine; positron emission tomography; review; Scopus; search engine; synthesis; trust; Web of Science; image processing; procedures","Deep Learning; Generative Adversarial Networks; Nuclear Medicine; Positron Emission Tomography","Review","Final","","Scopus","2-s2.0-85128743709"
"Guo K.; Hu M.; Ren S.; Li F.; Zhang J.; Guo H.; Kui X.","Guo, Kehua (35317515700); Hu, Min (57223438798); Ren, Sheng (57207860153); Li, Fangfang (56318889800); Zhang, Jian (57220121365); Guo, Haifu (57211693200); Kui, Xiaoyan (26640432800)","35317515700; 57223438798; 57207860153; 56318889800; 57220121365; 57211693200; 26640432800","Deep Illumination-Enhanced Face Super-Resolution Network for Low-Light Images","2022","ACM Transactions on Multimedia Computing, Communications and Applications","18","3","87","","","","10.1145/3495258","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127423369&doi=10.1145%2f3495258&partnerID=40&md5=46f0c6da99668cd37a29697f9deacd86","Face images are typically a key component in the fields of security and criminal investigation. However, due to lighting and shooting angles, faces taken under low-light conditions are often difficult to recognize. Face super-resolution (FSR) technology can restore high-resolution faces based on low-resolution inputs. However, existing face super-resolution methods typically rely on prior knowledge of inaccurate faces estimated from low-resolution images. Faces restored by low-light inputs may suffer from problems such as low brightness and many missing details. In this article, we proposed an Illumination-Enhanced Face Super-Resolution (IEFSR) model that can progressively super-resolve low-light faces of 32 × 32 pixels by an upscaling factor of 8. While reconstructing the low-light low-resolution face into a clear and high-quality face, we introduce a coarse low-resolution (LR) restoration network to recover the LR face details hidden in the dark. In the generator, we use a series of style blocks with noise to make the generated faces appear to have a more realistic visual aesthetic. Additionally, we introduce spectrum normalization in the discriminator to improve training stability. Extensive experimental evaluations show that the proposed IEFSR yields visually and metrically more attractive results than existing state-of-the-art FSR methods.  © 2022 Association for Computing Machinery.","Image enhancement; Image reconstruction; Optical resolving power; Restoration; Face images; Face super-resolution; Low light; Low-light face image; Low-light images; Lower resolution; Normalisation; Spectra's; Spectrum normalization; Superresolution methods; Generative adversarial networks","Face super-resolution; Generative adversarial network; Low-light face images; Spectrum normalization","Article","Final","","Scopus","2-s2.0-85127423369"
"Liang M.; Wang X.-L.","Liang, Min (57226533656); Wang, Xi-Li (36761950100)","57226533656; 36761950100","Semantic Segmentation Model for Remote Sensing Images Combining Super Resolution and Domain Adaption; [结合超分辨率和域适应的遥感图像语义分割方法]","2022","Jisuanji Xuebao/Chinese Journal of Computers","45","12","","2619","2636","17","10.11897/SP.J.1016.2022.02619","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144541942&doi=10.11897%2fSP.J.1016.2022.02619&partnerID=40&md5=c45068813edecd34a2af74c74190f28d","The semantic segmentation method based on convolutional neural network rely on supervised learning with ground truth, but it cannot be well generalized to unlabeled datasets with different sources. Unsupervised domain adaptation can solve the problem of inconsistent feature distribution between unlabeled target domain data and labeled source domain data. This is due to that remote sensing images are often come from different sources, they are variable in their spatial resolution and are influenced by different imaging regions, imaging conditions and imaging times. Even images from the same region may have large differences in spectral features. The generalization of the semantic segmentation model relies on the reduction of these inter-domain differences mentioned above. Therefore, unsupervised domain adaptation methods for remote sensing image should not only reduce the differences in features between domains, but also address the problem of different spatial resolutions. This paper designs a new end-to-end semantic segmentation deep network combined with image super resolution-Semantic Segmentation Model Combining Super Resolution and Domain Adaption, which can reduce the spatial resolution difference and feature distribution difference between the low spatial resolution source domain and high spatial resolution target domain remote sensing images, and accomplish the super-resolution task for the source domain and the domain adaptation semantic segmentation task for the target domain. The SSM-SRDA model consists of three parts one is Semantic Segmentation Network with Super Resolution (SSNSR), the second is Pixel-level Domain Discriminator (PDD), and the third is Output-space Domain Discriminator (ODD). SSNSR consists of a semantic segmentation network and a super-resolution network, which share the same feature extraction network. The super-resolution network generates a high-resolution synthetic image with target image style from a low-resolution source domain image, which can eliminate spatial resolution differences and style differences to help the feature extraction module learn the same features between the source and target domains. The Feature Affinity-Loss module enhances the learning of the semantic segmentation deep network by the feature maps with more detailed structural information obtained by super-resolution. The pixel-level domain discriminator is used to reduce the pixel-level feature differences between the high-resolution target domain and the synthetic image of the source domain. High-resolution source domain synthetic images with target domain style are generated by generative adversarial learning with the super-resolution network and participate in the training of the model as additional training data. The output-space domain discriminator reduces the feature differences between source and target domain images in the output space of the semantic segmentation network. Through the adversarial learning of the semantic segmentation network and the two discriminators, SSM-SRDA aligns the feature distribution of the source domain and the target domain at the input and output stages of the segmentation network, and can be applied to the target domain datas of more different sources through domain adaptation. It is a practical and more popular model. Experiments show that SSM-SRDA is superior to the existing domain adaptive semantic segmentation methods on four sets of remote sensing image data sets, and the intersection ratio is improved by 0.7%, 1.7%, 2.2% and 3.3%, respectively. © 2022, Science Press. All right reserved.","Convolutional neural networks; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Image resolution; Learning systems; Pixels; Remote sensing; Semantics; Space optics; Adversarial learning; Domain adaptation; Feature distribution; Pixel level; Remote sensing images; Segmentation models; Semantic segmentation; Spatial resolution; Superresolution; Target domain; Semantic Segmentation","Adversarial learning; Domain adaptation; Remote sensing image; Semantic segmentation; Super resolution","Article","Final","","Scopus","2-s2.0-85144541942"
"Jia Y.; Liang Z.; Huo X.; Chen W.; Chai Y.; Yu R.","Jia, Yixiong (58083435400); Liang, Zhifeng (55851283600); Huo, Xuesong (56492331600); Chen, Wenjin (58083467000); Chai, Yun (57189306192); Yu, Ruoying (55295449600)","58083435400; 55851283600; 56492331600; 58083467000; 57189306192; 55295449600","The impact of load-PV profile resolution on distribution system risk assessment","2023","Energy Reports","9","","","2653","2664","11","10.1016/j.egyr.2023.01.079","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147091178&doi=10.1016%2fj.egyr.2023.01.079&partnerID=40&md5=33f8ad233c3ab287424c0a3b3099a4ca","Risk assessment is utilized to evaluate the risk level of the system and guide further operation and planning of utilities. However, to lower the cost of communication and storing, only a coarse resolution profile of load and PV can be collected and used to evaluate the risk level of the system, which may lead to inaccurate assessment. To this end, this paper investigates the impact of load-PV profile resolution on distribution system risk assessment. A super-resolution Generative Adversarial Network with pre- and post-processing processes is proposed to restore the high-frequency component of the load-PV profile. Then, with the generated high-resolution profile, a risk assessment model based on the exterior point method is proposed to evaluate the risk level of the distribution system. Case studies in a modified 33-bus system showed that the proposed super-resolution method could achieve the best performance to restore the peak values and high-frequency component of the load-PV profile compared to the other four methods. In addition, compared to super-resolution Generative Adversarial Networks without the post-processing process, the accuracy of risk assessment can be significantly improved using the proposed super-resolution method, which proves that the post-processing process is necessary. © 2023","Electric power distribution; Generative adversarial networks; Optical resolving power; Restoration; Distribution systems; High frequency components; Higher-frequency components; Load-PV super-resolution method; Post-processing; Risk levels; Risks assessments; Superresolution; Superresolution methods; System risk assessment; Risk assessment","Generative adversarial networks; Load-PV super-resolution method; Risk assessment","Article","Final","","Scopus","2-s2.0-85147091178"
"Zhang Q.; Jia R.-S.; Li Z.-H.; Li Y.-C.; Sun H.-M.","Zhang, Qi (57367179500); Jia, Rui-Sheng (25927894300); Li, Zeng-Hu (57563441600); Li, Yong-Chao (57411683300); Sun, Hong-Mei (55729286100)","57367179500; 25927894300; 57563441600; 57411683300; 55729286100","Superresolution reconstruction of optical remote sensing images based on a multiscale attention adversarial network","2022","Applied Intelligence","52","15","","17896","17911","15","10.1007/s10489-022-03548-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127706645&doi=10.1007%2fs10489-022-03548-7&partnerID=40&md5=82bde35960d7e8ee7f087ef8569b915c","Due to the influence of imaging equipment and environmental conditions on optical remote sensing image acquisition, image resolution is generally low. Superresolution reconstruction technology is an important way to improve image quality. However, the existing optical remote sensing image superresolution reconstruction methods have some problems, such as insufficient feature extraction, blurred texture details of reconstructed images, and excessive network accumulation. To solve the above problems, a superresolution reconstruction method for optical remote sensing images based on a multiscale attention adversarial network is proposed in this paper. The method takes a generative adversarial network (GAN) as the basic framework. The generator uses four multiscale attention residual blocks (MSARBs) to extract image multiscale feature information and carries out feature fusion through a binary feature fusion structure (BFFS) to generate more realistic images. The discriminator uses a depth convolution network to distinguish the differences between real images and superresolution images. In the aspect of loss function construction, the perceptual loss and adversarial loss are combined to improve the perceptual quality of the images. Experimental results show that this method is superior to the compared algorithm in regard to the objective evaluation metrics of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), and its reconstructed images have better visual effect. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image acquisition; Image enhancement; Image fusion; Image quality; Image reconstruction; Image resolution; Optical remote sensing; Signal to noise ratio; Textures; Adversarial networks; Features fusions; Image-based; Multiscale attention residual network; Optical remote sensing; Optical remote sensing image; Reconstructed image; Reconstruction method; Remote sensing images; Super-resolution reconstruction; Generative adversarial networks","Generative adversarial network; Multiscale attention residual network; Optical remote sensing images; Superresolution reconstruction","Article","Final","","Scopus","2-s2.0-85127706645"
"Ma C.; Rao Y.; Lu J.; Zhou J.","Ma, Cheng (8914125600); Rao, Yongming (57200621657); Lu, Jiwen (14065346100); Zhou, Jie (56939394300)","8914125600; 57200621657; 14065346100; 56939394300","Structure-Preserving Image Super-Resolution","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","11","","7898","7911","13","10.1109/TPAMI.2021.3114428","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115675230&doi=10.1109%2fTPAMI.2021.3114428&partnerID=40&md5=792322c1ab78b845746b68f9ce6860b0","Structures matter in single image super-resolution (SISR). Benefiting from generative adversarial networks (GANs), recent studies have promoted the development of SISR by recovering photo-realistic images. However, there are still undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super-resolution (SPSR) method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. First, we propose SPSR with gradient guidance (SPSR-G) by exploiting gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss to impose a second-order restriction on the super-resolved images, which helps generative networks concentrate more on geometric structures. Second, since the gradient maps are handcrafted and may only be able to capture limited aspects of structural information, we further extend SPSR-G by introducing a learnable neural structure extractor (NSE) to unearth richer local structures and provide stronger supervision for SR. We propose two self-supervised structure learning methods, contrastive prediction and solving jigsaw puzzles, to train the NSEs. Our methods are model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results on five benchmark datasets show that the proposed methods outperform state-of-the-art perceptual-driven SR methods under LPIPS, PSNR, and SSIM metrics. Visual results demonstrate the superiority of our methods in restoring structures while generating natural SR images. Code is available at https://github.com/Maclory/SPSR.  © 1979-2012 IEEE.","Computer vision; Edge detection; Feature extraction; Generative adversarial networks; Image reconstruction; Job analysis; Optical resolving power; Restoration; Features extraction; Gradient map; Image edge detection; Image super resolutions; Self-supervised learning; Structure-preserving; Superresolution; Superresolution methods; Task analysis; Image enhancement","generative adversarial network; image enhancement; self-supervised learning; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85115675230"
"Xu S.; Deng B.; Shi Y.; Meng Y.; Liu G.; Han J.","Xu, Shengjun (25928952700); Deng, Bowen (57204061384); Shi, Ya (57956588800); Meng, Yuebo (8868247200); Liu, Guanghui (56267813000); Han, Jiuqiang (22334362400)","25928952700; 57204061384; 57956588800; 8868247200; 56267813000; 22334362400","An Encoder-Decoder-Based Super Resolution Network for License Plate Images; [一种编解码结构的车牌图像超分辨率网络]","2022","Hsi-An Chiao Tung Ta Hsueh/Journal of Xi'an Jiaotong University","56","10","","101","110","9","10.7652/xjtuxb202210010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141374913&doi=10.7652%2fxjtuxb202210010&partnerID=40&md5=d88594c467e813b0885411ccb2fe1a7a","An encoder-decoder-based super resolution network for license plate images was proposed for the lack of key information on license plate images caused by blur, stain, damage, distortion, and tilt in complex actual scenes and for the recognition difficulty due to low contrast between the license plate background and characters of new-energy vehicles. Firstly, a license plate reconstruction generator network based on the encoder-decoder structure is constructed. The texture and characters of the license plate image are extracted by an encoder, and the license plate features are reconstructed by a decoder. Then, a discriminator network based on semantic supervision is designed, and the adversarial loss and CTC loss are introduced into the network loss to enhance the ability of the generator network to represent the semantic features of license plate images. Finally, the features of the vertex points of the license plate are extracted based on VGG16 network, and a coordinate transformation method is utilized to correct the license plate image and further improve the reconstruction quality and recognition accuracy. Super-resolution reconstruction and recognition tests were performed on the self-built XAUAT-Parking dataset and the public CCPD dataset with the proposed network. The test results showed that the proposed network has an average peak signal to noise ratio(PSNR) of 25.5 dB and a structural similarity(SSIM) of 0.989 on the CCPD dataset. The PSNR and SSIM on the XAUAT-Parking dataset can reach 26.6 dB and 0.997 respectively. According to the research results, the proposed network has a good super-resolution reconstruction effect of license plate images and a strong robustness to the missing of key license plate information. © 2022 Xi'an Jiaotong University. All rights reserved.","Decoding; Image enhancement; Image reconstruction; License plates (automobile); Nanocomposites; Network coding; Optical resolving power; Restoration; Semantics; Signal to noise ratio; Statistical tests; Textures; Decoder structures; Encoder-decoder; Encoder-decoder structure; Image-correction; License plate images; Network-based; Peak signal to noise ratio; Super-resolution reconstruction; Superresolution; Vgg16 network; Generative adversarial networks","encoder-decoder structure; generative adversarial network; image correction; license plate image; super resolution; vgg16 network","Article","Final","","Scopus","2-s2.0-85141374913"
"Wafa A.; Nasiopoulos P.","Wafa, Abrar (56785978300); Nasiopoulos, Panos (6701848250)","56785978300; 6701848250","Light Field GAN-based View Synthesis using full 4D information","2022","Proceedings - CVMP 2022: 19th ACM SIGGRAPH European Conference on Visual Media Production","","","3","","","","10.1145/3565516.3565519","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144819772&doi=10.1145%2f3565516.3565519&partnerID=40&md5=56fe641c83c2c976d1dad30581653cb5","Light Field (LF) technology offers a truly immersive experience having the potential to revolutionize entertainment, training, education, virtual and augmented reality, gaming, autonomous driving, and digital health. However, one of the main issues when working with LF is the amount of data needed to create a mesmerizing experience with realistic disparity, smooth motion parallax between views. In this paper, we introduce a learning based LF angular super-resolution approach for efficient view synthesis of novel virtual images. This is achieved by taking four corner views and then generating up to five in-between views. Our generative adversarial network approach uses LF spatial and angular information to ensure smooth disparity between the generated and original views. We consider plenoptic, synthetic LF content and camera array implementations which support different baseline settings. Experimental results show that our proposed method outperforms state-of-the-art light field view synthesis techniques, offering novel generated views with high visual quality.  © 2022 ACM.","Augmented reality; Geometrical optics; Optical resolving power; Angular super-resolution; Augmented reality gaming; Autonomous driving; Education virtual; Field technology; Immersive; Light fields; Superresolution; View synthesis; Virtual and augmented reality; Generative adversarial networks","angular super-resolution; generative adversarial network; light field; view synthesis","Conference paper","Final","","Scopus","2-s2.0-85144819772"
"Hu Y.; Li J.; Huang Y.; Gao X.","Hu, Yanting (55791826100); Li, Jie (7410068291); Huang, Yuanfei (57203161002); Gao, Xinbo (7403873424)","55791826100; 7410068291; 57203161002; 7403873424","Image Super-Resolution With Self-Similarity Prior Guided Network and Sample-Discriminating Learning","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","4","","1966","1985","19","10.1109/TCSVT.2021.3093483","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112136879&doi=10.1109%2fTCSVT.2021.3093483&partnerID=40&md5=a0932787da9ef47f954f6a1fda51822b","The nonlocal self-similarity in natural image provides an effective prior for single image super-resolution (SISR), which is beneficial to contextual information capture and performance improvement, as demonstrated by conventional SISR methods. However, it is little explored to utilize this property in deep neural networks. In this paper, we propose a self-similarity prior guided (SSPG) network to incorporate self-similarity-based nonlocal operation into deep neural network for SISR. Specifically, we design a cross-scale nearest-neighbor residual (CSNNR) block via introducing cross-scale $k$ -nearest neighbors (KNN) matching into a residual block, which can be flexibly integrated into deep networks to capture long-range correlations among multi-scale and multi-level features. Meanwhile, by stacking a CSNNR block and a sequence of wide-activated residual blocks with a local skip-connection, a multi-level residual self-similarity (MRSS) module is developed to effectively employ local and nonlocal information for detail recovery. Thus, through cascading multiple MRSS modules, the proposed SSPG network performs both self-similarity-based nonlocal operation and convolution-based local operation on multi-level features to reconstruct informative features for accurate SISR. In addition, for pursuing visually pleasing results, we apply our SSPG network to the perception-oriented SISR field by following the framework of generative adversarial networks. In particular, we explore a sample-discriminating learning mechanism based on the statistical descriptions of training samples, and include it in optimization procedure to automatically tune the contributions of different samples according to their characteristics and then focus the network on creating realistic results. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate the superiority of our proposed models over the state-of-the-art methods for both distortion-oriented and perception-oriented image super-resolution tasks.  © 1991-2012 IEEE.","Deep learning; Deep neural networks; Discriminators; Image enhancement; Nearest neighbor search; Optical resolving power; Contextual information; Image super resolutions; K nearest neighbor (KNN); Long range correlations; Optimization procedures; Qualitative evaluations; State-of-the-art methods; Statistical descriptions; Neural networks","k-nearest neighbors; Nonlocal self-similarity prior; sample-discriminating learning; single image super-resolution","Article","Final","","Scopus","2-s2.0-85112136879"
"Bode M.","Bode, M. (56135121000)","56135121000","AI Super-Resolution: Application to Turbulence and Combustion","2023","Lecture Notes in Energy","44","","","279","305","26","10.1007/978-3-031-16248-0_10","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146217734&doi=10.1007%2f978-3-031-16248-0_10&partnerID=40&md5=9f0c1ddd1d57d150cd2dc4912ae82a01","This article summarizes and discusses recent developments with respect to artificial intelligence (AI) super-resolution as a subfilter model for large-eddy simulations. The focus is on the application of physics-informed enhanced super-resolution generative adversarial networks (PIESRGANs) for subfilter closure in turbulence and combustion applications. A priori and a posteriori results are presented for various applications, ranging from decaying turbulence to finite-rate chemistry flows. The high accuracy of AI super-resolution-based subfilter models is emphasized, and advantages and shortcoming are described. © 2023, The Author(s).","Combustion; Generative adversarial networks; Large eddy simulation; Optical resolving power; Decaying turbulence; Finite-rate chemistry; High-accuracy; Large-eddy simulations; Posteriori; Subfilter models; Subfilters; Superresolution; Turbulence","","Book chapter","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85146217734"
"Lai Y.; Zhou Z.; Su B.; Chen J.; Liang W.; Ma C.; Wang T.; Zheng Z.","Lai, Yunting (57981982300); Zhou, Zhuang (57216129837); Su, Binghua (37082293700); Chen, Jiongjiang (57780479500); Liang, Wanxin (57981818500); Ma, Chenhao (58091078600); Wang, Tenghui (58086985600); Zheng, Zexin (58091534600)","57981982300; 57216129837; 37082293700; 57780479500; 57981818500; 58091078600; 58086985600; 58091534600","Super-resolution of underwater images based on generative adversarial network","2023","Proceedings of SPIE - The International Society for Optical Engineering","12561","","125610E","","","","10.1117/12.2652062","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147441827&doi=10.1117%2f12.2652062&partnerID=40&md5=48c521e3e85f38886f25c98cc6b58980","High-quality underwater images and videos play an important role for exploitation tasks in underwater environments, but the complexity of the underwater imaging environment makes the quality of the acquired underwater images generally low. In order to improve underwater image quality by enhancing the resolution of underwater images, we propose an underwater image super-resolution method based on the improvement of SRGAN. The images generated by conventional super-resolution methods lose details and high-frequency information, resulting in overly smooth images. We have improved SRGAN by replacing the original Residual Block with Residual Dense Block in the feature extraction network, which improves the network performance and speeds up the training and testing of the model. And the Shuffle Attention mechanism is incorporated after each residual block, which efficiently combines the channel attention mechanism and the spatial attention mechanism, significantly improving the network's ability to extract features and generating high-resolution images with richer detail information. At the same time, our method effectively solves the problem that the generator generates images that are too smooth and without grain details. Our method is compared with SRGAN and SRResnet methods in USR-248, UFO public underwater image dataset, and the experimental results demonstrate that our method generates super-resolution images with better image detail enhancement and higher PSNR and SSIM values. © 2023 SPIE.","Image enhancement; Optical resolving power; Underwater imaging; Attention mechanisms; High quality; High-frequency informations; Image super resolutions; Image-based; Superresolution; Superresolution methods; Supersolution; Underwater environments; Underwater image; Generative adversarial networks","Generative adversarial networks; Super-solution; Underwater image","Conference paper","Final","","Scopus","2-s2.0-85147441827"
"Ahmad W.; Ali H.; Shah Z.; Azmat S.","Ahmad, Waqar (57224819356); Ali, Hazrat (56501336300); Shah, Zubair (56428700200); Azmat, Shoaib (55263841400)","57224819356; 56501336300; 56428700200; 55263841400","A new generative adversarial network for medical images super resolution","2022","Scientific Reports","12","1","9533","","","","10.1038/s41598-022-13658-4","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131708599&doi=10.1038%2fs41598-022-13658-4&partnerID=40&md5=67779d38e3e111f1a10d0fc7e796f9cd","For medical image analysis, there is always an immense need for rich details in an image. Typically, the diagnosis will be served best if the fine details in the image are retained and the image is available in high resolution. In medical imaging, acquiring high-resolution images is challenging and costly as it requires sophisticated and expensive instruments, trained human resources, and often causes operation delays. Deep learning based super resolution techniques can help us to extract rich details from a low-resolution image acquired using the existing devices. In this paper, we propose a new Generative Adversarial Network (GAN) based architecture for medical images, which maps low-resolution medical images to high-resolution images. The proposed architecture is divided into three steps. In the first step, we use a multi-path architecture to extract shallow features on multiple scales instead of single scale. In the second step, we use a ResNet34 architecture to extract deep features and upscale the features map by a factor of two. In the third step, we extract features of the upscaled version of the image using a residual connection-based mini-CNN and again upscale the feature map by a factor of two. The progressive upscaling overcomes the limitation for previous methods in generating true colors. Finally, we use a reconstruction convolutional layer to map back the upscaled features to a high-resolution image. Our addition of an extra loss term helps in overcoming large errors, thus, generating more realistic and smooth images. We evaluate the proposed architecture on four different medical image modalities: (1) the DRIVE and STARE datasets of retinal fundoscopy images, (2) the BraTS dataset of brain MRI, (3) the ISIC skin cancer dataset of dermoscopy images, and (4) the CAMUS dataset of cardiac ultrasound images. The proposed architecture achieves superior accuracy compared to other state-of-the-art super-resolution architectures. © 2022, The Author(s).","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; human; image processing; nuclear magnetic resonance imaging; procedures","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131708599"
"Cao J.; Jia Y.; Yan M.; Tian X.","Cao, Jianfang (36175317900); Jia, Yiming (57226255807); Yan, Minmin (57226240994); Tian, Xiaodong (57226413345)","36175317900; 57226255807; 57226240994; 57226413345","Murals Super-resolution Reconstruction with the Stable Enhanced Generative Adversarial Network; [稳定增强生成对抗网络在壁画的超分辨率重建]","2022","Xitong Fangzhen Xuebao / Journal of System Simulation","34","5","","1076","1089","13","10.16182/j.issn1004731x.joss.20-0989","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129765008&doi=10.16182%2fj.issn1004731x.joss.20-0989&partnerID=40&md5=5459139b3e00eac924d99af3f9d35f57","Aiming at the problems of low resolution and unclear texture details of ancient murals, which led to insufficient viewing of murals and low research value, a stable enhanced super-resolution generative adversarial networks (SESRGAN) reconstruction algorithm is proposed. Based on the generative adversarial network, the generative network uses dense residual blocks to extract mural features, and uses the visual geometry group (VGG) network as the basic framework of the discriminating network to determine the authenticity of the input mural, and introduces perception loss, content loss and penalty loss to jointly optimize the model. Experimental results show that, compared with other related super-resolution algorithms, the peak signal-to-noise ratio (PSNR) is improved by 0.4~2.62 dB on average, the structural similarity is improved by 0.013~0.027, and the subjective perception evaluation is also improved. © 2022, The Editorial Board of Journal of System Simulation. All right reserved.","","Ancient mural; Dense residual block; Generation adversarial network; Penalty loss; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85129765008"
"Hemmati S.; Huff E.; Nayyeri H.; Ferté A.; Melchior P.; Mobasher B.; Rhodes J.; Shahidi A.; Teplitz H.","Hemmati, Shoubaneh (55505778800); Huff, Eric (14063437600); Nayyeri, Hooshang (54795728400); Ferté, Agnès (55819290300); Melchior, Peter (24074819700); Mobasher, Bahram (57203074321); Rhodes, Jason (7402364894); Shahidi, Abtin (57210599224); Teplitz, Harry (35314080800)","55505778800; 14063437600; 54795728400; 55819290300; 24074819700; 57203074321; 7402364894; 57210599224; 35314080800","Deblending Galaxies with Generative Adversarial Networks","2022","Astrophysical Journal","941","2","141","","","","10.3847/1538-4357/aca1b8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145314730&doi=10.3847%2f1538-4357%2faca1b8&partnerID=40&md5=12981e152153c7022c47623388212793","Deep generative models including generative adversarial networks (GANs) are powerful unsupervised tools in learning the distributions of data sets. Building a simple GAN architecture in PyTorch and training on the CANDELS data set, we generate galaxy images with the Hubble Space Telescope (HST) resolution starting from a noise vector. We proceed by modifying the GAN architecture to improve Subaru Hyper Suprime-Cam (HSC) ground-based images by increasing their resolution to the HST resolution. We use the super-resolution GAN on a large sample of blended galaxies, which we create using CANDELS cutouts. In our simulated blend sample, ∼20% would unrecognizably be blended even in the HST-resolution cutouts. In the HSC-like cutouts this fraction rises to ∼90%. With our modified GAN we can lower this value to ∼50%. We quantify the blending fraction in the high, low, and GAN resolutions over the whole manifold of angular separation, flux ratios, sizes, and redshift difference between the two blended objects. The two peaks found by the GAN deblender result in improvement by a factor of 10 in the photometry measurement of the blended objects. Modifying the architecture of the GAN, we also train a multiwavelength GAN with HST cutouts in seven optical + near-infrared bands. This multiwavelength GAN improves the fraction of detected blends by another ∼10% compared to the single-band GAN. This is most beneficial to the current and future precision cosmology experiments (e.g., LSST, SPHEREx, Euclid, Roman), specifically those relying on weak gravitational lensing, where blending is a major source of systematic error. © 2022. The Author(s). Published by the American Astronomical Society.","","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145314730"
"Lei J.; Xue H.; Yang S.; Shi W.; Zhang S.; Wu Y.","Lei, Jingsheng (7201403736); Xue, Hanbo (57789327300); Yang, Shengying (57339881500); Shi, Wenbin (57202393300); Zhang, Shuping (57222227736); Wu, Yi (57759739500)","7201403736; 57789327300; 57339881500; 57202393300; 57222227736; 57759739500","HFF-SRGAN: Super-resolution generative adversarial network based on high-frequency feature fusion","2022","Journal of Electronic Imaging","31","3","033011","","","","10.1117/1.JEI.31.3.033011","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133663427&doi=10.1117%2f1.JEI.31.3.033011&partnerID=40&md5=358c3194b38ed966ddf80ec69342b47b","At present, generative adversarial network (GAN) has made outstanding progress for image super-resolution (SR), solving the problem of edge smoothing in SR images. Nevertheless, there are also problems with a lack of texture details, noise, and artifacts. The high-frequency (HF) information of an image is significant for keeping the texture part of the reconstructed image, and the introduction of HF information can increase the learning ability of super-resolution generative adversarial network (SRGAN) for the image features, so we propose a super-resolution GAN based on high-frequency feature fusion (HFF-SRGAN), which can increase the texture detail of the reconstructed image and reduce the noise and artifacts with less computational cost. Experimental results show that our algorithm evaluates significantly better than SRGAN and has significant advantages in texture details and noise compared with other SR algorithms. © 2022 A.D.A.C.. All rights reserved.","Generative adversarial networks; Image reconstruction; Image texture; Optical resolving power; Features fusions; Frequency features; High frequency HF; High-frequency informations; Image super resolutions; Network-based; Reconstructed image; Super-resolution generative adversarial network; Superresolution; Textures","high-frequency information; super-resolution; super-resolution generative adversarial network; texture","Article","Final","","Scopus","2-s2.0-85133663427"
"Liu T.-J.; Chen Y.-Z.","Liu, Tsung-Jung (36625954800); Chen, Yu-Zhang (57549949400)","36625954800; 57549949400","Satellite Image Super-Resolution by 2D RRDB and Edge-Enhanced Generative Adversarial Network †","2022","Applied Sciences (Switzerland)","12","23","12311","","","","10.3390/app122312311","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143837514&doi=10.3390%2fapp122312311&partnerID=40&md5=d09087cd4c7c2222a65ec026d6c63fba","With the gradually increasing demand for high-resolution images, image super-resolution (SR) technology has become more and more important in our daily life. In general, high resolution is often accomplished by increasing the accuracy and density of the sensor. However, such an approach is too expensive on the design and equipment. Particularly, increasing the sensor density of satellites incurs great risks. Inspired by EEGAN, some parts of networks: Ultra-Dense Subnet (UDSN) and Edge-Enhanced Subnet (EESN) are modified. The UDSN is used to extract features and obtain high-resolution images which look clear but are deteriorated by artifacts in the intermediate stage, while the EESN is used to purify, enhance and extract the image contours and uses mask processing to eliminate the image corrupted by noise. Then, the restored intermediate image and the enhanced edge are combined to become a high-resolution image with clear contents and high authenticity. Finally, we use Kaggle open source, AID, WHU-RS19, and SpaceWill datasets to perform the test and compare the SR results among different models. It shows that our proposed approach outperforms other state-of-the-art SR models on satellite images. © 2022 by the authors.","","edge enhancement; generative adversarial network (GAN); residual in residual dense block (RRDB); satellite images; super-resolution (SR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143837514"
"Liu Z.; Li Z.; Wu X.; Liu Z.; Chen W.","Liu, Ziyang (57203758441); Li, Zhengguo (7409079258); Wu, Xingming (8976009300); Liu, Zhong (57190886072); Chen, Weihai (35776127700)","57203758441; 7409079258; 8976009300; 57190886072; 35776127700","DSRGAN: Detail Prior-Assisted Perceptual Single Image Super-Resolution via Generative Adversarial Networks","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","11","","7418","7431","13","10.1109/TCSVT.2022.3188433","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134261668&doi=10.1109%2fTCSVT.2022.3188433&partnerID=40&md5=2e723eb6468c2bec9415b9b9dbfc6e4c","The generative adversarial network (GAN) is successfully applied to study the perceptual single image super-resolution (SISR). However, since the GAN is data-driven, it has a fundamental limitation on restoring real high frequency information for an unknown instance (or image) during test. On the other hand, the conventional model-based methods have a superiority to achieve instance adaptation as they operate by considering the statistics of each instance (or image) only. Motivated by this, we propose a novel model-based algorithm, which can extract the detail layer of an image efficiently. The detail layer represents the high frequency information of image and it is constituted of image edges and fine textures. It is seamlessly incorporated into the GAN and serves as a prior knowledge to assist the GAN in generating more realistic details. The proposed method, named DSRGAN, takes advantages from both the model-based conventional algorithm and the data-driven deep learning network. Experimental results demonstrate that the DSRGAN outperforms the state-of-the-art SISR methods on perceptual metrics, meanwhile achieving comparable results in terms of fidelity metrics. Following the DSRGAN, it is feasible to incorporate other conventional image processing algorithms into a deep learning network to form a model-based deep SISR.  © 1991-2012 IEEE.","Deep learning; Edge detection; Generative adversarial networks; Learning algorithms; Optical resolving power; Textures; Data driven; Deep learning; Detail prior; High frequency HF; Image edge detection; Image super resolutions; Images reconstruction; Model-based OPC; Single image super-resolution; Single images; Image reconstruction","detail prior; generative adversarial networks; model-based and data-driven; Single image super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134261668"
"Ma C.; Zhu J.; Li Y.; Li J.; Jiang Y.; Li X.","Ma, Chunyan (57209848050); Zhu, Junwu (55704615100); Li, Yujie (36761032400); Li, Jianru (56201183700); Jiang, Yi (55613237374); Li, Xin (55718311000)","57209848050; 55704615100; 36761032400; 56201183700; 55613237374; 55718311000","Single image super resolution via wavelet transform fusion and SRFeat network","2022","Journal of Ambient Intelligence and Humanized Computing","13","11","","5023","5031","8","10.1007/s12652-020-02065-0","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084968696&doi=10.1007%2fs12652-020-02065-0&partnerID=40&md5=40605a9bc0c43b43d7751e3f44e05862","Image super resolution is a vital research topic in the field of computer vision. It aims to reconstruct high resolution images from low resolution images. Although the conventional image super resolution methods have achieved good performance and effect, there are still have some issues, e.g., the high-frequency details information is insufficient, and the reconstruction process will bring additional noise, and most basic interpolation techniques produce blurry results. To settle the problems mentioned above, we consider combining the deep learning method with the frequency domain fusion method. In this paper, a novel single image super resolution method based on SRFeat network and wavelet fusion is proposed. First, the training image is taken as the input of the backbone SRFeat network, then the generative adversarial network training is carried out. Then, the up-sampling is utilized to obtain the coarse super resolved image. Finally, the output image after the network training is combined with the up-sampling image of the low-resolution image by Wavelet fusion to obtain the final result. Without increasing the depth of the network and the redundant parameters, the proposed method can achieve better reconstruct result. The experimental results show that the proposed method can not only reduce the probability of image distortion, but recover the global information of the reconstructed image and remove the noise brought by the reconstruction process. The PSNR value of the proposed method is improved 0.3 dB, and the SSIM is improved 0.02. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Frequency domain analysis; Generative adversarial networks; Image compression; Image fusion; Image reconstruction; Learning systems; Optical resolving power; Signal sampling; De-noising; Image super resolutions; Low resolution images; Reconstruction process; Single images; Superresolution; Superresolution methods; Wavelet fusion; Wavelet transform fusion; Wavelets transform; Wavelet transforms","Denoising; Generative adversarial network; Super resolution; Wavelet transform fusion","Article","Final","","Scopus","2-s2.0-85084968696"
"Liu Y.; Wang Y.; Yang Q.","Liu, Yixian (57987965500); Wang, Yubin (57203770732); Yang, Qiang (57189235801)","57987965500; 57203770732; 57189235801","Spatio-Temporal Generative Adversarial Network based Power Distribution Network State Estimation with Multiple Time-scale Measurements","2023","IEEE Transactions on Industrial Informatics","","","","1","8","7","10.1109/TII.2023.3234624","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147286923&doi=10.1109%2fTII.2023.3234624&partnerID=40&md5=3a5efc5d9c3efa44fb58d4569c997c24","The increasing penetration of distributed renewable generation has introduced significant uncertainties and randomness to the power distribution network operation. Accurate and timely awareness of the network operation is of paramount importance to ensure system safety and reliability and is considered non-trivial and costly as substantial network reinforcement with advanced measurement devices is generally required. Also, the existing state estimation methods, e.g., weighted least square (WLS), may not converge in the presence of incomplete and inaccurate measurements. This paper proposes a spatio-temporal estimation generative adversarial network (ST-EGAN) consisting of feature extraction, information completion, data reconstruction and fake data discrimination to generate high-resolution pseudo-measurements to promote the accuracy and robustness of state estimation. The task of high-resolution power distribution network state estimation is carried out based on the mixed dataset of multiple time-scale measurements obtained from Supervisory Control and Data Acquisition (SCADA) and Phasor Measurement Units (PMU). The proposed solution is extensively assessed using the IEEE 33-bus test network compared with the existing solutions for a range of scenarios with different resolutions and noise intensities. The numerical results demonstrated that the proposed ST-EGAN can reduce the mean RMSE by 4.78&#x0025; compared to interpolation algorithms, and reduce the RMSE by 0.14&#x0025; and 0.21&#x0025; compared with deep convolutional generative adversarial networks (DCGAN) and super-resolution convolutional networks (SRCNN) in the presence of noises with different intensities. The proposed method can be generalized to cases with different topological structures and measurement assembly conditions. IEEE","Convolution; Data acquisition; Data mining; Electric power distribution; Interpolation; Job analysis; Least squares approximations; Phase measurement; State estimation; Uncertainty analysis; Data generation; High resolution; High-resolution perception; Power; Power distribution network; Power measurement; Power system; Spatio-temporal; Task analysis; Phasor measurement units","data generation; Distribution networks; Generative adversarial networks; high-resolution perception; interpolation; Phasor measurement units; Power measurement; Power systems; State estimation; State estimation; Task analysis","Article","Article in press","","Scopus","2-s2.0-85147286923"
"Song L.; Li Y.; Lu N.","Song, Lidong (57226333064); Li, Yiyan (36139508700); Lu, Ning (57203484083)","57226333064; 36139508700; 57203484083","ProfileSR-GAN: A GAN Based Super-Resolution Method for Generating High-Resolution Load Profiles","2022","IEEE Transactions on Smart Grid","13","4","","3278","3289","11","10.1109/TSG.2022.3158235","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126311116&doi=10.1109%2fTSG.2022.3158235&partnerID=40&md5=d103bb7aba0a5939e9041b889bac829d","This paper presents a novel two-stage load profile super-resolution (LPSR) framework, ProfileSR-GAN, to upsample the low-resolution load profiles (LRLPs) to high-resolution load profiles (HRLPs). The LPSR problem is formulated as a Maximum-a-Posteriori problem. In the first-stage, a GAN-based model is adopted to restore high-frequency components from the LRLPs. To reflect the load-weather dependency, aside from the LRLPs, the weather data is added as an input to the GAN-based model. In the second-stage, a polishing network guided by outline loss and switching loss is novelly introduced to remove the unrealistic power fluctuations in the generated HRLPs and improve the point-to-point matching accuracy. To evaluate the realisticness of the generated HRLPs, a new set of load shape evaluation metrics is developed. Simulation results show that: I) ProfileSR-GAN outperforms the state-of-the-art methods in all shape-based metrics and can achieve comparable performance with those methods in point-to-point matching accuracy, and ii) after applying ProfileSR-GAN to convert LRLPs to HRLPs, the performance of a downstream task, non-intrusive load monitoring, can be significantly improved. This demonstrates that ProfileSR-GAN is an effective new mechanism for restoring high-frequency components in downsampled time-series data sets and improves the performance of downstream tasks that require HR load profiles as inputs.  © 2022 IEEE.","Electric load management; Meteorology; Optical resolving power; Fluctuation; Generator; Load modeling; Load profile generation; Load profiles; Nonintrusive load monitoring; Superresolution; Synthetic data; Synthetic data.; Generative adversarial networks","Generative adversarial networks; load profile generation; machine learning; non-intrusive load monitoring; super-resolution; synthetic data","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126311116"
"Daihong J.; Sai Z.; Lei D.; Yueming D.","Daihong, Jiang (57220897886); Sai, Zhang (57493339000); Lei, Dai (57192204923); Yueming, Dai (57191480467)","57220897886; 57493339000; 57192204923; 57191480467","Multi-scale generative adversarial network for image super-resolution","2022","Soft Computing","26","8","","3631","3641","10","10.1007/s00500-022-06822-5","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125088229&doi=10.1007%2fs00500-022-06822-5&partnerID=40&md5=bbf0f95e225d1bad6d06e5a64ca3729a","In recent years, deep convolutional neural networks (CNNs) have been widely employed in image super-resolution. Thanks to the power of deep CNNs, the reconstruction performance is largely improved. However, the high-frequency information and details in the low-resolution image still can hardly be reconstructed. To deal with the above problems, we propose a multi-scale generative adversarial network in this paper. The multi-scale Pyramid module inside the generator could extract the features containing high-frequency information, and then the high-resolution image with the results of the bicubic interpolations is reconstructed. The discriminator in our model is used to identify the authenticity of the input image after refactoring. Our final loss function includes an adversarial loss and the mean square error (L2) reconstruction loss. In order to further improve the efficiency of training, the generator is pre-trained with the L2 loss, so as to improve the efficiency of the discriminator optimization. Compared with the algorithms based solely on normal plain convolutional networks, the proposed algorithm performs better in two indexes PSNR and SSIM of the super-resolution task. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Convolution; Convolutional neural networks; Deep neural networks; Efficiency; Image reconstruction; Mean square error; Optical resolving power; Bicubic interpolation; Compression activation module; High-frequency informations; High-resolution images; Image super resolutions; Low resolution images; Multi-Scale pyramids; Multi-scales; Performance; Power; Generative adversarial networks","Compression activation module; Generative adversarial network; Image super-resolution; Multi-scale","Article","Final","","Scopus","2-s2.0-85125088229"
"Zuo Y.; Fang Y.; Ma K.","Zuo, Yifan (36996817300); Fang, Yuming (8435698900); Ma, Kede (55624220200)","36996817300; 8435698900; 55624220200","The critical review of the growth of deep learning-based image fusion techniques; [深度学习时代图像融合技术进展]","2023","Journal of Image and Graphics","28","1","","102","117","15","10.11834/jig.220556","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147181781&doi=10.11834%2fjig.220556&partnerID=40&md5=020952af9a43f2d7392f384888b582b3","To capture more effective visual information of the natural scenes, multi-sensor imaging systems have been challenging in multiple configurations or modalities due to the hardware design constraints. It is required to fuse multiple source images into a single high-quality image in terms of rich and feasible perceptual information and few artifacts. To facilitate various image processing and computer vision tasks, image fusion technique can be used to generate a single and clarified image features. Traditional image fusion models are often constructed in accordance with label-manual features or unidentified feature-learned representations. The generalization ability of the models needs to be developed further. Deep learning technique is focused on progressive multi-layer features extraction via end-to-end model training. Most of demonstration-relevant can be learned for specific task automatically. Compared with the traditional methods, deep learning-based models can improve the fusion performance intensively in terms of image fusion. Current image fusion-related deep learning models are often beneficial based on convolutional neural networks (CNNs) and generative adversarial networks (GANs). In recent years, the newly network structures and training techniques have been incorporated for the growth of image fusion like vision transformers and meta-learning techniques. Most of image fusion-relevant literatures are analyzed from specific multi-fusion issues like exposure, focus, spectrum image, and modality issues. However, more deep learning-related model designs and training techniques is required to be incorporated between multi-fusion tasks. To draw a clear picture of deep learning-based image fusion techniques, we try to review the latest image fusion researches in terms of 1) dataset generation, 2) neural network construction, 3) loss function design, 4) model optimization, and 5) performance evaluation. For dataset generation, we emphasize two categories: a) supervised learning and b) unsupervised (or self-supervised) learning. For neural network construction, we distinguish the early or late stages of this construction process, and the issue of information fusion is implemented between multi-scale, coarse-to-fine and the adversarial networks-incorporated (i. e., discriminative networks) as well. For loss function design, the perceptual loss functions-specific method is essential for image fusion-related perceptual applications like multi-exposure and multi-focus image fusion. For model optimization, the generic first-order optimization techniques are covered (e. g., stochastic gradient descent (SGD), SGD + momentum, Adam, and AdamW) and the advanced alternation and bi-level optimization methods are both taken into consideration. For performance evaluation, a commonly-used quantitative metrics are reviewed for the manifested measurement of fusion performance. The relationship between the loss functions (also as a form of evaluation metrics) are used to drive the learning of CNN-based image fusion methods and the evaluation metrics. In addition, to illustrate the transfer feasibility of image fusion-consensus to a tailored image fusion application, a selection of image fusion methods is discussed (e. g., a high-quality texture image-fused depth map enhancement). Some popular computer vision tasks are involved in (such as image denoising, blind image deblurring, and image super-resolution), which can be resolved by image fusion innovatively. Finally, we review some potential challenging issues, including: 1) reliable and efficient ground-truth training data-constructed (i. e., the input image sequence and the predictable image-fused), 2) lightweight, interpretable, and generalizable CNN-based image fusion methods, 3) human or machine-related vision-perceptual calibrated loss functions, 4) convergence-accelerated image fusion models in related to adversarial training setting-specific and the bias-related of the test-time training, and 5) human-related ethical issues in relevant to fairness and unbiased performance evaluation. © 2023 Editorial and Publishing Board of JIG. All rights reserved.","","deep learning; deep neural networks; image fusion; image quality assessment; stochastic gradient descent (SGD)","Article","Final","","Scopus","2-s2.0-85147181781"
"Tavse S.; Varadarajan V.; Bachute M.; Gite S.; Kotecha K.","Tavse, Sampada (58028395100); Varadarajan, Vijayakumar (57314747500); Bachute, Mrinal (57189389452); Gite, Shilpa (56656365900); Kotecha, Ketan (6506676097)","58028395100; 57314747500; 57189389452; 56656365900; 6506676097","A Systematic Literature Review on Applications of GAN-Synthesized Images for Brain MRI","2022","Future Internet","14","12","351","","","","10.3390/fi14120351","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144628583&doi=10.3390%2ffi14120351&partnerID=40&md5=d385cc30621bb80718652aefb0d68b2b","With the advances in brain imaging, magnetic resonance imaging (MRI) is evolving as a popular radiological tool in clinical diagnosis. Deep learning (DL) methods can detect abnormalities in brain images without an extensive manual feature extraction process. Generative adversarial network (GAN)-synthesized images have many applications in this field besides augmentation, such as image translation, registration, super-resolution, denoising, motion correction, segmentation, reconstruction, and contrast enhancement. The existing literature was reviewed systematically to understand the role of GAN-synthesized dummy images in brain disease diagnosis. Web of Science and Scopus databases were extensively searched to find relevant studies from the last 6 years to write this systematic literature review (SLR). Predefined inclusion and exclusion criteria helped in filtering the search results. Data extraction is based on related research questions (RQ). This SLR identifies various loss functions used in the above applications and software to process brain MRIs. A comparative study of existing evaluation metrics for GAN-synthesized images helps choose the proper metric for an application. GAN-synthesized images will have a crucial role in the clinical sector in the coming years, and this paper gives a baseline for other researchers in the field. © 2022 by the authors.","Application programs; Brain mapping; Deep learning; Diagnosis; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Image segmentation; Learning systems; Brain imaging; Brain magnetic resonance imaging; Clinical diagnosis; Conditional generative adversarial network; Deep learning; Generative adversarial network loss function; Loss functions; Network loss; Synthesized images; Systematic literature review; Magnetic resonance imaging","brain MRI; conditional GAN; deep learning; GAN loss function; generative adversarial network","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144628583"
"Qin P.; Huang H.; Tang H.; Wang J.; Liu C.","Qin, Peng (57215550489); Huang, Huabing (57216494536); Tang, Hailong (57210148473); Wang, Jie (56050004800); Liu, Chong (57970016500)","57215550489; 57216494536; 57210148473; 56050004800; 57970016500","MUSTFN: A spatiotemporal fusion method for multi-scale and multi-sensor remote sensing images based on a convolutional neural network","2022","International Journal of Applied Earth Observation and Geoinformation","115","","103113","","","","10.1016/j.jag.2022.103113","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142150477&doi=10.1016%2fj.jag.2022.103113&partnerID=40&md5=397559d93ab6afae2c76f93f62f85af1","Spatiotemporal data fusion is a commonly-used and well-proven technique to enhance the application potential of multi-source remote sensing images. However, most existing methods have trouble in generating quality fusion results when areas covered by the images undergoes rapid land cover changes or images have substantial registration errors. While deep learning algorithms have demonstrated their capabilities for imagery fusion, it is challenging to apply deep-learning-based fusion methods in regions that experiences persistent cloud covers and have limited cloud-free imagery observations. To address these challenges, we developed a Multi-scene Spatiotemporal Fusion Network (MUSTFN) algorithm based on a Convolutional Neural Network (CNN). Our approach uses multi-level features to fuse images at different resolutions acquired by multiple sensors. Furthermore, MUSTFN uses the multi-scale features to overcome the effects of geometric registration errors between different images. Additionally, a multi-constrained loss function is proposed to improve the accuracy of imagery fusion over large areas and solve fusion and gap-filling problems simultaneously by utilizing cloud-contaminated images with the fine-tuning method. Compared with several commonly-used methods, our proposed MUSTFN performs better in fusing the 30-m Landsat-7 images and 500-m MODIS images over a small area that has undergone large changes (the average relative Mean Absolute Errors (rMAE) of the first four bands are 6.8% by MUSTFN as compared to 14.1% by the Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model (ESTARFM), 12.8% by the Flexible Spatiotemporal Data Fusion (FSDAF), 8.4% by the Extended Super-Resolution Convolutional Neural Network (ESRCNN), 8.1% by the Spatiotemporal Fusion Using a Generative Adversarial Network (STFGAN)). In particularly for images at different resolutions with different registration accuracies (e.g., 16-m Chinese GaoFen-1 and 500-m MODIS), MUSTFN achieved fusion results of good quality with an average rMAE of 9.3% in spectral reflectance at the first four bands. Finally, we demonstrated the applicability of MUSTFN (average rMAE of 9.18%) when fusing long-term Landsat-8 composite images and MODIS images over a large region (830 km × 600 km). Overall, our results suggest the effectiveness of MUSTFN to address the challenges in imagery fusion, including rapid land cover changes between image acquisition dates, geometric misregistration between images and limited availabilities of cloud-free images. The program of MUSTFN is freely available at: https://github.com/qpyeah/MUSTFN. © 2022 The Authors","algorithm; artificial neural network; image processing; remote sensing; satellite data; spatiotemporal analysis","CNN; Large-area image fusion; Multi-scale fusion scenarios; Multi-sensor satellite data; Spatiotemporal fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142150477"
"Li S.; Feng C.; Yu K.; Liu X.; Jiang X.; Zhao D.","Li, Shulin (57560441300); Feng, Chaolu (36451839400); Yu, Kun (57211443820); Liu, Xin (57560001600); Jiang, Xin (57560441400); Zhao, Dazhe (7403490069)","57560441300; 36451839400; 57211443820; 57560001600; 57560441400; 7403490069","Critical review of human cardiac magnetic resonance image super resolution reconstruction based on deep learning method; [基于深度学习的心脏磁共振影像超分辨率前沿进展]","2022","Journal of Image and Graphics","27","3","","704","721","17","10.11834/jig.210150","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127516546&doi=10.11834%2fjig.210150&partnerID=40&md5=ee6cd0b8e843d9c85160d4d6921290c2","Heart diseases diagnosis and treatment has become one of the major public health issues for human beings where non-invasive cardiac imaging is of great significance. Unfortunately, imaging scan time and cardiac image resolution become an intensive contradiction due to the characteristics of living heartbeat. Super-resolution (SR) image reconstruction like low-resolution-images based high-resolution cardiac images reconstruction are obtained based on the determined tolerable error within a short imaging time. Deep learning methods have revealed great vitality in the field of medical image processing nowadays. In terms of its good learning ability and data-driven factors, deep-learning-based SR reconstruction is qualified based on deep residual networks, generative adversarial networks compared with traditional methods. Our research analysis reviews the field via analyzing characteristics of representative methods, summing up cardiac image resources and the scale, summarizing commonly used evaluation indicators, giving performance evaluation and application conclusions of the methods and discussing methods in other fields that can be adapted for SR reconstructing cardiac magnetic resonance (CMR) images. Our analysis is derived from 13 opted literature reviews from Google, database systems and logic programming (DBLP), and CNKI taking deep learning, cardiac and SR reconstruction as search keywords. This review first categorizes all methods by their evaluation datasets, which are open resource or not. Details on the datasets and links to the open-source datasets are also facilitated. By defining standard evaluation indicators based on the reconstructed SR images and their ground truth, namely high-resolution (HR) images, the performance of SR reconstruction methods can be evaluated quantitatively. This demonstration summarizes the 8 evaluation indicators in the cardiac SR reconstruction methods, including structural similarity, cardiovascular diameter measured value, dice coefficient. The evaluation indicators are divided into three categories with respect to the following aspects, those are evaluation of the quality of SR reconstructed images, evaluation of cardiac function and evaluation of the effectiveness of cardiac segmentation. Meanwhile, all 13 literature reviews are only used to increase the spatial resolution of the CMR images. Our research classifies the methods as CMR 2D SR reconstruction, CMR 3D SR reconstruction, and CMR SR reconstruction in other dimensions, depending on the dimension of the cardiac images processing based on the deep learning methods. In general, most CMR 2D SR reconstruction methods can reconstruct high resolution cardiac images in a relative short time span to assure SR reconstruction quality. In contrast, the CMR 3D SR reconstruction methods are involved 3D convolution, which take the spatial structure of the heart into account. The integrated information is analyzed amongst adjacent slices of CMR. Some of these methods achieve the SR reconstruction result qualified than CMR 2D SR reconstruction methods. However, a number of CMR data is small size and most of them are not open. The larger perceptual domain of the methods increases the computation complexity and reduces the temporal performance to some extent. As for CMR SR reconstruction in higher dimensions, corresponding methods meet the requirement of high-resolution image generation and image denoising in clinical analysis. All the selected CMR SR reconstruction methods can also be organized in accordance with network models and high-resolution image degradation methods, such as U-Net, generative adversarial network and long short-term memory network from the aspect of network models. Fourier degradation and different interpolation degradation methods are from the aspect of degradation methods. SR reconstructed high-resolution images can accurately facilitate heart anatomy, blood flow evaluation and heart tissue segmentation level. This research also reviewed the feasibility of adapting other SR reconstruction methods for CMR SR reconstruction as they are currently proposed and applied to images of other in vivo tissues and structures. Our research also tries to find some of the SR reconstruction methods from the field of natural images computer vision to discuss the feasibility that adapting them to CMR SR reconstruction, such as channel attention mechanisms, video SR methods and SR of real scenes. In summary, SR reconstruction of CMR images has its distinctive features than SR reconstruction of natural images like more diverse and purposeful evaluation metrics constraint of local reconstruction quality and to the difficulties of getting training data. It is found that deep-learning-based cardiac SR reconstruction has concerned more in motion artifact suppression, model simplification, and time performance further. In addition, current methods basically rely on the powerful expression ability of the CNN(convolutional neural network), and little clinical prior knowledge is melted to the network to guide its learning. Performance comparison between existing models is relatively less, and there is no representative image repository to evaluate performances of different cardiac SR reconstruction methods. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Cardiac imaging; Convolutional neural networks; Datasets of cardiac images; Deep learning; High-resolution images; Magnetic resonance imaging; Super-resolution reconstruction","Review","Final","","Scopus","2-s2.0-85127516546"
"Hu Y.; Shang Q.","Hu, Yuting (57197806945); Shang, Qiufeng (8431847400)","57197806945; 8431847400","Performance Enhancement of BOTDA Based on the Image Super-Resolution Reconstruction","2022","IEEE Sensors Journal","22","4","","3397","3404","7","10.1109/JSEN.2021.3139321","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122561018&doi=10.1109%2fJSEN.2021.3139321&partnerID=40&md5=73e6fb4956a25824a73ec13b23f6a51e","The acquisition time of Brillouin optical time domain analysis (BOTDA) sensor is relatively long since it requires multiple scans of probe tones to map the Brillouin gain spectra (BGS). To enhance the performance of BOTDA with fewer probe tones and lower time-domain sampling rate, we propose a method based on generative adversarial network (GAN) to reconstruct the super-resolution BGS from its low-resolution counterpart for 4times upscaling factors. We preliminarily prepare a dataset composed of ideal BGSs with different linewidths, spatial resolutions, and Brillouin frequency shifts (BFS) for network training and create a 2048-level colormap for the conversion between BGS and its RGB image. By cropping the measured BGS into consecutive frames and making cross-correlation to transform each frame to a new one with nearly ideal Lorentzian shape as input for the independent reconstruction, the 4times enlarged super-resolved BGS conforming to the real BGS distribution could be established. The validity of our method was shown through a conventional BOTDA experiment. The results show that 75% measurements were reduced and the resolution in spatial and frequency domain were raised by 4 times, and the measurement accuracy also got greatly improved.  © 2001-2012 IEEE.","Deep learning; Frequency domain analysis; Generative adversarial networks; Image enhancement; Image reconstruction; Optical resolving power; Probes; Brillouin gain spectrum; Brillouin optical time domain analysis; Brillouin optical timedomain analyzer (BOTDA); Deep learning; Frequency measurements; Generator; Image super resolutions; Images reconstruction; Performance enhancements; Time-domain analysis; Time domain analysis","Brillouin gain spectrum; Brillouin optical time-domain analyzer; deep learning; generative adversarial networks; image super-resolution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85122561018"
"Min J.; Zhang Y.; Yu Y.; Lv K.; Wang Z.; Zhang L.","Min, Jie (57226783874); Zhang, Yongsheng (56414442400); Yu, Ying (55731485300); Lv, Kefeng (57730290400); Wang, Ziquan (57196351372); Zhang, Lei (57196133151)","57226783874; 56414442400; 55731485300; 57730290400; 57196351372; 57196133151","Enhanced Remote Sensing Image SRGAN Algorithm and Its Application in Improving the Accuracy of 3D Reconstruction; [增强型遥感影像SRGAN算法及其在三维重建精度提升中的应用]","2022","Journal of Geo-Information Science","24","8","","1631","1644","13","10.12082/dqxxkx.2022.210766","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137638286&doi=10.12082%2fdqxxkx.2022.210766&partnerID=40&md5=47a95d9b46e8eded52331a3e5d81ba44","Remote sensing images are important data sources for terrain mapping, 3D reconstruction, and other tasks. The spatial resolution of remote sensing images determines the representation ability of the measured object on the image and plays an important role in the positioning accuracy and reconstruction effect of 3D model in the later stage. In view of the characteristics of high resolution remote sensing images including large scale, complex target features, and rich details, an enhanced SRGAN algorithm for remote sensing image reconstruction is proposed to meet the needs of 3D model reconstruction. The proposed algorithm overcomes the problems of edge effect and fuzzy reconstruction using traditional methods for super-resolution reconstruction. In traditional methods, there is limitation that simple convolutional networks can only extract the shallow feature information of the image and cannot retain the rich details of the image with the increasing resolution. The proposed algorithm is based on the generative adversarial networks using deep learning, in which dense residual blocks are used to extract deep features, and multi-scale discrimination is introduced into the discriminant model. In the training, the generation model and the discrimination model learn features together and are optimized to finally obtain a super-resolution reconstruction model suitable for remote sensing image application. This model can improve the resolution and image quality of remote sensing images, and ensure the integrity and accuracy of feature texture, detail information, and high-frequency target. In our study, the proposed algorithm is compared with the Bicubic, SRGAN, and ESRGAN algorithms. Our results show that the PSNR of the proposed algorithm is improved by about three units, the Penetration Index (PI) is stable and closer to one, and the SSIM and clarity index Q are also improved. In 3D reconstruction, the number of image dense matching points is increased, and the error is reduced. The measured point values of the model are closer to the measured point values from the field. The visual perception of the model is also more real and delicate, which indicates that the precision and positioning accuracy of the 3D model can be significantly improved using the remote sensing images constructed by the proposed algorithm. The results demonstrate the proposed algorithm that considers the characteristics of remote sensing images performs better than other algorithms for the super-resolution reconstruction, and the geometric accuracy and visual accuracy of the real 3D models based on the constructed images are also significantly improved. © 2022, Science Press. All right reserved.","3D modeling; Convolution; Convolutional neural networks; Deep learning; Generative adversarial networks; Image enhancement; Image quality; Image resolution; Mapping; Remote sensing; Textures; Three dimensional computer graphics; 3D reconstruction; Deep learning; High resolution remote sensing; Images processing; Multi-scale relative discrimination; Multi-scales; Positioning accuracy; Remote sensing images; SRGAN; Super-resolution reconstruction; accuracy assessment; algorithm; image processing; image resolution; machine learning; remote sensing; satellite imagery; three-dimensional modeling; Image reconstruction","3D reconstruction; Deep learning; High-resolution remote sensing; Image processing; Multi-scale relative discrimination; Positioning accuracy; SRGAN; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85137638286"
"Son C.-H.; Jeong D.-H.","Son, Chang-Hwan (7005976985); Jeong, Da-Hee (57644135900)","7005976985; 57644135900","Heavy Rain Face Image Restoration: Integrating Physical Degradation Model and Facial Component-Guided Adversarial Learning","2022","Sensors","22","14","5359","","","","10.3390/s22145359","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135134624&doi=10.3390%2fs22145359&partnerID=40&md5=edb6c080d94b6d0e970f994b2e021e3f","With the recent increase in intelligent CCTVs for visual surveillance, a new image degradation that integrates resolution conversion and synthetic rain models is required. For example, in heavy rain, face images captured by CCTV from a distance have significant deterioration in both visibility and resolution. Unlike traditional image degradation models (IDM), such as rain removal and super resolution, this study addresses a new IDM referred to as a scale-aware heavy rain model and proposes a method for restoring high-resolution face images (HR-FIs) from low-resolution heavy rain face images (LRHR-FI). To this end, a two-stage network is presented. The first stage generates low-resolution face images (LR-FIs), from which heavy rain has been removed from the LRHR-FIs to improve visibility. To realize this, an interpretable IDM-based network is constructed to predict physical parameters, such as rain streaks, transmission maps, and atmospheric light. In addition, the image reconstruction loss is evaluated to enhance the estimates of the physical parameters. For the second stage, which aims to reconstruct the HR-FIs from the LR-FIs outputted in the first stage, facial component-guided adversarial learning (FCGAL) is applied to boost facial structure expressions. To focus on informative facial features and reinforce the authenticity of facial components, such as the eyes and nose, a face parsing-guided generator and facial local discriminators are designed for FCGAL. The experimental results verify that the proposed approach based on a physical-based network design and FCGAL can remove heavy rain and increase the resolution and visibility simultaneously. Moreover, the proposed heavy rain face image restoration outperforms state-of-the-art models of heavy rain removal, image-to-image translation, and super resolution. © 2022 by the authors.","Image Processing, Computer-Assisted; Rain; Deterioration; Generative adversarial networks; Image enhancement; Light transmission; Optical resolving power; Rain; Restoration; Visibility; rain; Adversarial learning; Face images; Facial components; Heavy rains; High resolution; Image degradation model; Intelligent CCTV; Rain modeling; Rain removals; Superresolution; image processing; procedures; Image reconstruction","generative adversarial network; image restoration; intelligent CCTV; rain removal","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85135134624"
"Wicaksono K.P.; Fujimoto K.; Fushimi Y.; Sakata A.; Okuchi S.; Hinoda T.; Nakajima S.; Yamao Y.; Yoshida K.; Miyake K.K.; Numamoto H.; Saga T.; Nakamoto Y.","Wicaksono, Krishna Pandu (57219407117); Fujimoto, Koji (7403304369); Fushimi, Yasutaka (57202409662); Sakata, Akihiko (56478556600); Okuchi, Sachi (57217500066); Hinoda, Takuya (55778214200); Nakajima, Satoshi (55619095000); Yamao, Yukihiro (55416809200); Yoshida, Kazumichi (8966354800); Miyake, Kanae Kawai (55111783100); Numamoto, Hitomi (57213568012); Saga, Tsuneo (7101682623); Nakamoto, Yuji (57317897300)","57219407117; 7403304369; 57202409662; 56478556600; 57217500066; 55778214200; 55619095000; 55416809200; 8966354800; 55111783100; 57213568012; 7101682623; 57317897300","Super-resolution application of generative adversarial network on brain time-of-flight MR angiography: image quality and diagnostic utility evaluation","2023","European Radiology","33","2","","936","946","10","10.1007/s00330-022-09103-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137031809&doi=10.1007%2fs00330-022-09103-9&partnerID=40&md5=21110d07f5f6187c13e67a2a977403e4","Objectives: To develop a generative adversarial network (GAN) model to improve image resolution of brain time-of-flight MR angiography (TOF-MRA) and to evaluate the image quality and diagnostic utility of the reconstructed images. Methods: We included 180 patients who underwent 1-min low-resolution (LR) and 4-min high-resolution (routine) brain TOF-MRA scans. We used 50 patients’ datasets for training, 12 for quantitative image quality evaluation, and the rest for diagnostic validation. We modified a pix2pix GAN to suit TOF-MRA datasets and fine-tuned GAN-related parameters, including loss functions. Maximum intensity projection images were generated and compared using multi-scale structural similarity (MS-SSIM) and information theoretic-based statistic similarity measure (ISSM) index. Two radiologists scored vessels’ visibilities using a 5-point Likert scale. Finally, we evaluated sensitivities and specificities of GAN-MRA in depicting aneurysms, stenoses, and occlusions. Results: The optimal model was achieved with a lambda of 1e5 and L1 + MS-SSIM loss. Image quality metrics for GAN-MRA were higher than those for LR-MRA (MS-SSIM, 0.87 vs. 0.73; ISSM, 0.60 vs. 0.35; p.adjusted < 0.001). Vessels’ visibility of GAN-MRA was superior to LR-MRA (rater A, 4.18 vs. 2.53; rater B, 4.61 vs. 2.65; p.adjusted < 0.001). In depicting vascular abnormalities, GAN-MRA showed comparable sensitivities and specificities, with greater sensitivity for aneurysm detection by one rater (93% vs. 84%, p < 0.05). Conclusions: An optimized GAN could significantly improve the image quality and vessel visibility of low-resolution brain TOF-MRA with equivalent sensitivity and specificity in detecting aneurysms, stenoses, and occlusions. Key Points: • GAN could significantly improve the image quality and vessel visualization of low-resolution brain MR angiography (MRA). • With optimally adjusted training parameters, the GAN model did not degrade diagnostic performance by generating substantial false positives or false negatives. • GAN could be a promising approach for obtaining higher resolution TOF-MRA from images scanned in a fraction of time. © 2022, The Author(s), under exclusive licence to European Society of Radiology.","Brain; Cerebral Angiography; Constriction, Pathologic; Humans; Magnetic Resonance Angiography; Magnetic Resonance Imaging; adult; aged; Article; artificial neural network; brain artery aneurysm; brain hemorrhage; cerebral artery disease; clinical article; clinical evaluation; comparative study; controlled study; convolutional neural network; coronary stenosis; diagnostic test accuracy study; diagnostic value; female; generative adversarial network; human; image enhancement; image quality; image reconstruction; ischemic stroke; Likert scale; magnetic resonance angiography; male; neuroimaging; quantitative analysis; radiologist; retrospective study; sensitivity and specificity; time of flight magnetic resonance angiography; visibility; brain; brain angiography; diagnostic imaging; nuclear magnetic resonance imaging; procedures; stenosis, occlusion and obstruction; vascularization","Brain; Image enhancement; Intracranial arterial diseases; Magnetic resonance angiography; Neural networks, computer","Article","Final","","Scopus","2-s2.0-85137031809"
"Park H.; Na M.; Kim B.; Park S.; Kim K.H.; Chang S.; Ye J.C.","Park, Hyoungjun (57218499750); Na, Myeongsu (56647740800); Kim, Bumju (56007494800); Park, Soohyun (57221875518); Kim, Ki Hean (34770690200); Chang, Sunghoe (9238330400); Ye, Jong Chul (57223117835)","57218499750; 56647740800; 56007494800; 57221875518; 34770690200; 9238330400; 57223117835","Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy","2022","Nature Communications","13","1","3297","","","","10.1038/s41467-022-30949-6","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131627261&doi=10.1038%2fs41467-022-30949-6&partnerID=40&md5=4b7422557526ae7e933f0a7cae299a74","Volumetric imaging by fluorescence microscopy is often limited by anisotropic spatial resolution, in which the axial resolution is inferior to the lateral resolution. To address this problem, we present a deep-learning-enabled unsupervised super-resolution technique that enhances anisotropic images in volumetric fluorescence microscopy. In contrast to the existing deep learning approaches that require matched high-resolution target images, our method greatly reduces the effort to be put into practice as the training of a network requires only a single 3D image stack, without a priori knowledge of the image formation process, registration of training data, or separate acquisition of target data. This is achieved based on the optimal transport-driven cycle-consistent generative adversarial network that learns from an unpaired matching between high-resolution 2D images in the lateral image plane and low-resolution 2D images in other planes. Using fluorescence confocal microscopy and light-sheet microscopy, we demonstrate that the trained network not only enhances axial resolution but also restores suppressed visual details between the imaging planes and removes imaging artifacts. © 2022, The Author(s).","Anisotropy; Deep Learning; Imaging, Three-Dimensional; Microscopy, Fluorescence; fluorescence; knowledge; learning; microscopy; spatial resolution; article; confocal microscopy; deep learning; fluorescence microscopy; image artifact; image display; light-sheet microscopy; three-dimensional imaging; anisotropy; fluorescence microscopy; procedures","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131627261"
"Jiang M.; Qian W.; Xu D.; Wu H.; Liu C.","Jiang, Mengjie (57222518013); Qian, Wenhua (55570494500); Xu, Dan (35622853100); Wu, Hao (57203455738); Liu, Chunyu (57215071709)","57222518013; 55570494500; 35622853100; 57203455738; 57215071709","Gradual model reconstruction of Dongba painting based on residual dense structure; [残差密集结构的东巴画渐进式重建]","2022","Journal of Image and Graphics","27","4","","1084","1096","12","10.11834/jig.200523","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129731497&doi=10.11834%2fjig.200523&partnerID=40&md5=7691def1821f0c01cd1c8e0ac81735ad","Objective: Dongba painting is an essential part of Na'xi culture in China. It is the fundamental medium to develop Dongba culture in art theory, natural aesthetics, religion and history. However, the current low resolution digital image of Dongba painting has affected the application, inheritance and development of Dongba culture. Super-resolution reconstruction technology is the process of recovering high-resolution image from low-resolution image. Because Dongba painting has a unique artistic style, compared with natural images, there are no dimension, distance and depth factors, and there is no light and shadow effect via natural light. While the existing super-resolution algorithm for natural images applied to Dongba painting straitforward, the reconstruction effect of lines, color blocks and materials of Dongba painting is not ideal. Therefore, the super-resolution reconstruction of Dongba painting is adopted and high-resolution Dongba painting obtained via the reconstruction of low-resolution Dongba painting. Method: First, the Dongba painting dataset for network training is constructed, which makes the learning of Dongba painting image characteristics more targeted. The data set contains 298 high-definition Dongba paintings, and each Dongba painting has one dimension with a resolution of 2 K at least. Among them, 278 paintings are used for training, 20 paintings are used for testing, and the training set is randomly cropped for data enhancement. Next, in accordance with the characteristics of Dongba painting image which is rich in high frequency information, a reconstruction network is built and named Dongba super-resolution network (DBSRN): the overall structure of the network uses multi-level sub network cascade to gradually reconstruct high-resolution Dongba painting. During the feedforward process of a large-scale reconstruction, multiple intermediate Super-resolution predictions were produced. The final reconstruction results are constrained by multiple intermediate prediction values, so as to reconstruct Dongba paintings of different scales from small to large gradually. The reconstruction results of each sub network and the corresponding scale tags are calculated at the pixel level, and the tags of different scales jointly guide the reconstruction. The loss of high-frequency details is reduced in the up sampling process of Dongba painting image. To extract and fuse features at different levels, each level of super-resolution sub-network has a shallow feature extraction module, a deep feature extraction module, and a global feature fusion module. In this way, the disappearance of features with the deepening of network can be avoided. The extracted feature maps are input to the up-sampling module for reconstruction. The residual dense structure, which combines residual connection and dense connection, can effectively enhance feature reuse and slow down gradient disappearance. The demonstrated algorithm takes the residual dense structure as the core in deep feature extraction module, extracts the deep feature of Dongba painting for fusion, reduce the feature loss caused by simple chain stacking in the convolution layer; At the end, added-discriminator, perception loss and adversarial loss for adversarial training are implemented to improve the visual quality of Dongba painting based on pixel level loss. This network is named DBGAN (Dongba generative adversarial network). Result: This illustration demonstrate the Dongba paintings reconstructed in this paper get better results in both subjective visual quality and objective indicators, compared to bicubic interpolation (Bicubic), super-resolution convolutional neural network (SRCNN), super-resolution residual network (Srresnet) and information multi-distillation network (IMDN).The peak signal to noise ratio (PSNR)/structural similarity index (SSIM) in DBSRN reach 33.46 dB and 0.911 2 when upsampling factor is 2, 28.54 dB and 0.776 2 when upsampling factor is 4, and 24.61 dB and 0.643 0 when upsampling factor is 8. The objective indicators of DBSRN are improved to varying degrees compared with Bicubic, SRCNN, and Srresnet. Compared with Srresnet, when the upsampling factor is 2, PSNR and SSIM are increased by 0.10 dB and 0.000 8, respectively, when the upsampling factor is 4, they are increased by 0.18 dB and 0.003 2, and when the upsampling factor is 8, they are increased by 0.23 dB and 0.004 4. DBGAN improves the clarity and fidelity of the reconstruction results further and reconstruct Dongba paintings with more edge and texture details. Conclusion: This super-resolution universal network model can improve the resolution and clarity of low-resolution Dongba paintings effectively. © 2022, Editorial Office of Journal of Image and Graphics. All right reserved.","","Adversarial training; Dongba painting; Progressive reconstruction; Residual dense structure; Super-resolution","Article","Final","","Scopus","2-s2.0-85129731497"
"Singh A.B.; Awasthi L.K.; Urvashi; Shorfuzzaman M.; Alsufyani A.; Uddin M.","Singh, Amitoj Bir (57577603800); Awasthi, Lalit Kumar (57802015500); Urvashi (57764943600); Shorfuzzaman, Mohammad (8298383000); Alsufyani, Abdulmajeed (55567418500); Uddin, Mueen (42062605100)","57577603800; 57802015500; 57764943600; 8298383000; 55567418500; 42062605100","Chained Dual-Generative Adversarial Network: A Generalized Defense Against Adversarial Attacks","2023","Computers, Materials and Continua","74","2","","2541","2555","14","10.32604/cmc.2023.032795","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141892073&doi=10.32604%2fcmc.2023.032795&partnerID=40&md5=6a2e5b75d821c3f55be7a76ef1997b4b","Neural networks play a significant role in the field of image classification. When an input image is modified by adversarial attacks, the changes are imperceptible to the human eye, but it still leads to misclassification of the images. Researchers have demonstrated these attacks to make production self-driving cars misclassify Stop Road signs as 45 Miles Per Hour (MPH) road signs and a turtle being misclassified as AK47. Three primary types of defense approaches exist which can safeguard against such attacks i.e., Gradient Masking, Robust Optimization, and Adversarial Example Detection. Very few approaches use Generative Adversarial Networks (GAN) for Defense against Adversarial Attacks. In this paper, we create a new approach to defend against adversarial attacks, dubbed Chained Dual-Generative Adversarial Network (CD-GAN) that tackles the defense against adversarial attacks by minimizing the perturbations of the adversarial image using iterative oversampling and undersampling using GANs. CD-GAN is created using two GANs, i.e., CDGAN’s Sub-Resolution GAN and CDGAN’s Super-Resolution GAN. The first is CDGAN’s Sub-Resolution GAN which takes the original resolution input image and oversamples it to generate a lower resolution neutralized image. The second is CDGAN’s Super-Resolution GAN which takes the output of the CDGAN’s Sub-Resolution and undersamples, it to generate the higher resolution image which removes any remaining perturbations. Chained Dual GAN is formed by chaining these two GANs together. Both of these GANs are trained independently. CDGAN’s Sub-Resolution GAN is trained using higher resolution adversarial images as inputs and lower resolution neutralized images as output image examples. Hence, this GAN downscales the image while removing adversarial attack noise. CDGAN’s Super-Resolution GAN is trained using lower resolution adversarial images as inputs and higher resolution neutralized images as output images. Because of this, it acts as an Upscaling GAN while removing the adversarial attak noise. Furthermore, CD-GAN has a modular design such that it can be prefixed to any existing classifier without any retraining or extra effort, and can defend any classifier model against adversarial attack. In this way, it is a Generalized Defense against adversarial attacks, capable of defending any classifier model against any attacks. This enables the user to directly integrate CD-GAN with an existing production deployed classifier smoothly. CD-GAN iteratively removes the adversarial noise using a multi-step approach in a modular approach. It performs comparably to the state of the arts with mean accuracy of 33.67 while using minimal compute resources in training. © 2023 Tech Science Press. All rights reserved.","Image classification; Network security; Optical resolving power; Optimization; Roads and streets; Adversarial attack; Adversarial defense; Classification models; Generative adversarial network-based adversarial defense; Image classification model; Images classification; Lower resolution; Network-based; Subresolution; Superresolution; Generative adversarial networks","Adversarial attacks; adversarial defense; GAN-based adversarial defense; image classification models","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141892073"
"Wang F.; Zhao J.","Wang, Fuyong (56719959500); Zhao, Jiuyu (57197823795)","56719959500; 57197823795","Digital rock image reconstruction based on deep learning and its reconstruction performance evaluation; [基于深度学习的数字岩心图像重构及其重构效果评价]","2022","Zhongnan Daxue Xuebao (Ziran Kexue Ban)/Journal of Central South University (Science and Technology)","53","11","","4412","4424","12","10.11817/j.issn.1672-7207.2022.11.021","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144626711&doi=10.11817%2fj.issn.1672-7207.2022.11.021&partnerID=40&md5=d792a157f989a9ba86c8159202d1c2af","Taking the tight sandstone of Yanchang Formation in Ordos Basin as the research object, a super-resolution generative adversarial neural network(SRGAN) was constructed to improve the resolution of digital core image. The reconstruction performance of SRGAN with different rescale factors was evaluated. The peak signal-to-noise ratio(PSNR), structural similarity(SSIM), porosity, fractal dimension and two-point probability function were used to quantitatively evaluate the reconstruction performance. The results show that the reconstructed high-resolution images are very close to the original image in visual effect, and its PSNR can be improved by more than 20 dB. With the increase of rescale factor, PSNR decreases about 10 dB, SSIM decreases nearly half, fractal dimension and porosity change little. The two-point probability function of the reconstructed image with 16 times magnification is quite different from that of the original image, and the images distort and have multiple artifacts, which cannot be used as the basis for subsequent work. Then deep convolution generative adversarial network(DCGAN) is used to reconstruct one-dimensional noise data to two-dimensional digital rock image. The reconstructed images have the similar characteristics of porosity, fractal dimension distribution and connectivity compared with the original CT images, but the two-point probability function of reconstructed rock images are generally low and the connectivity is poor. The generative adversarial neural network can be used to improve the resolution of digital rock image and realize digital rock reconstruction. It has a broad application prospect in the field of digital rock research. © 2022 Central South University of Technology. All rights reserved.","Computerized tomography; Generative adversarial networks; Image enhancement; Image reconstruction; Optical resolving power; Porosity; Probability distributions; Sandstone; Signal to noise ratio; CT Image; Digital rock; Generative adversarial neural network; Neural-networks; Peak signal to noise ratio; Probability functions; Super-resolution reconstruction; Superresolution; Tight sandstones; Two-point; Fractal dimension","CT image; digital rock; generative adversarial neural network; super-resolution reconstruction; tight sandstone","Article","Final","","Scopus","2-s2.0-85144626711"
"Nneji G.U.; Cai J.; Monday H.N.; Hossin M.A.; Nahar S.; Mgbejime G.T.; Deng J.","Nneji, Grace Ugochi (57206902001); Cai, Jingye (10738940600); Monday, Happy Nkanta (57201671873); Hossin, Md Altab (57202841195); Nahar, Saifun (57215928272); Mgbejime, Goodness Temofe (57311893400); Deng, Jianhua (56102351300)","57206902001; 10738940600; 57201671873; 57202841195; 57215928272; 57311893400; 56102351300","Fine-Tuned Siamese Network with Modified Enhanced Super-Resolution GAN Plus Based on Low-Quality Chest X-ray Images for COVID-19 Identification","2022","Diagnostics","12","3","717","","","","10.3390/diagnostics12030717","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127440941&doi=10.3390%2fdiagnostics12030717&partnerID=40&md5=a1e5856c6eeaf87c8460994ed15ca60d","Coronavirus disease has rapidly spread globally since early January of 2020. With millions of deaths, it is essential for an automated system to be utilized to aid in the clinical diagnosis and reduce time consumption for image analysis. This article presents a generative adversarial network (GAN)-based deep learning application for precisely regaining high-resolution (HR) CXR images from low-resolution (LR) CXR correspondents for COVID-19 identification. Respectively, using the building blocks of GAN, we introduce a modified enhanced super-resolution generative adversarial network plus (MESRGAN+) to implement a connected nonlinear mapping collected from noise-contaminated low-resolution input images to produce deblurred and denoised HR images. As opposed to the latest trends of network complexity and computational costs, we incorporate an enhanced VGG19 fine-tuned twin network with the wavelet pooling strategy in order to extract distinct features for COVID-19 identification. We demonstrate our proposed model on a publicly available dataset of 11,920 samples of chest X-ray images, with 2980 cases of COVID-19 CXR, healthy, viral and bacterial cases. Our proposed model performs efficiently both on the binary and four-class classification. The proposed method achieves accuracy of 98.8%, precision of 98.6%, sensitivity of 97.5%, specificity of 98.9%, an F1 score of 97.8% and ROC AUC of 98.8% for the multi-class task, while, for the binary class, the model achieves accuracy of 99.7%, precision of 98.9%, sensitivity of 98.7%, specificity of 99.3%, an F1 score of 98.2% and ROC AUC of 99.7%. Our method obtains state-of-the-art (SOTA) performance, according to the experimental results, which is helpful for COVID-19 screening. This new conceptual framework is proposed to play an influential role in addressing the issues facing COVID-19 examination and other diseases. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Article; binary classification; conceptual framework; coronavirus disease 2019; deep learning; diagnostic accuracy; diagnostic test accuracy study; feature extraction; generative adversarial network; human; image quality; noise; receiver operating characteristic; sensitivity and specificity; thorax radiography","adversarial learning; chest X-ray images; contrastive loss; COVID-19; deep learning; Siamese network; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127440941"
"Choi Y.; Park H.","Choi, Yoonsil (58039968800); Park, Hanhoon (8529467300)","58039968800; 8529467300","Improving ESRGAN with an additional image quality loss","2023","Multimedia Tools and Applications","82","2","","3123","3137","14","10.1007/s11042-022-13452-4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145429380&doi=10.1007%2fs11042-022-13452-4&partnerID=40&md5=8c07d21d28d3ff2184884cb433e5c762","ESRGAN is a generative adversarial network that produces visually pleasing super-resolution (SR) images with high perceptual quality from low-resolution images. However, it frequently fails to recover local details, resulting in blurry or unnatural visual artifacts. To address this problem, we propose using an additional perceptual loss (computed using the pretrained PieAPP network) for training the generator, adding skip connections to the discriminator to use a combination of features with different scales, and replacing the Leaky ReLU activation functions in the discriminator with the ReLU ones. Through ×4 SR experiments utilizing real and computer-generated image benchmark datasets, it is demonstrated that the proposed method can produce SR images with significantly higher perceptual quality than ESRGAN and other ESRGAN enhancements. Specifically, when compared to ESRGAN, the proposed method resulted in 5.95 higher DMOS values, 0.46 lower PI values, and 0.01 lower LPIPS values. The source code is accessible at https://github.com/cyun-404/PieESRGAN. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Benchmarking; Chemical activation; Discriminators; Image enhancement; Image quality; Optical resolving power; Additional perceptual loss; Generative adversarial network, ESRGAN; Image quality assessment; Image super resolutions; Multiscale discriminator; Perceptual image super-resolution; PieAPP-based image quality assessment; ReLU activation; Resolution images; Superresolution; Generative adversarial networks","Additional perceptual loss; Generative adversarial network, ESRGAN; Multiscale discriminator; Perceptual image super-resolution; PieAPP-based image quality assessment; ReLU activation","Article","Final","","Scopus","2-s2.0-85145429380"
"Yuan X.; Huang Y.; An L.; Qin J.; Lan G.; Qiu H.; Yu B.; Jia H.; Ren S.; Tan H.; Xu J.","Yuan, Xing (57223252954); Huang, Yanping (55716986100); An, Lin (35793682900); Qin, Jia (35995872900); Lan, Gongpu (36183789200); Qiu, Haixia (36009041400); Yu, Bo (57199692240); Jia, Haibo (8151042800); Ren, Shangjie (43561717000); Tan, Haishu (7403011453); Xu, Jingjiang (55444322300)","57223252954; 55716986100; 35793682900; 35995872900; 36183789200; 36009041400; 57199692240; 8151042800; 43561717000; 7403011453; 55444322300","Image enhancement of wide-field retinal optical coherence tomography angiography by super-resolution angiogram reconstruction generative adversarial network","2022","Biomedical Signal Processing and Control","78","","103957","","","","10.1016/j.bspc.2022.103957","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134573696&doi=10.1016%2fj.bspc.2022.103957&partnerID=40&md5=50c5c699ba5712edd72e27df1d80d811","Wide-field retinal optical coherence tomography angiography (OCTA) usually suffers from low image resolution in clinical practice because of insufficient lateral sampling. In this study, we develop a deep-learning-based method named super-resolution angiogram reconstruction generative adversarial network (SAR-GAN) to enhance the en face OCTA image quality. A sophisticated home-made spectral-domain OCTA system is employed to capture the data of retinal angiograms with different scanning protocols. High-resolution 3 × 3 mm2 OCTA images and low-resolution (LR) 6 × 6 mm2 OCTA images are utilised in training the network. We propose an improved loss function for SAR-GAN for the reconstruction of perceptually enhanced super-resolution images. The well-trained network is utilized to processing the LR OCTA images with a field of view (FOV) of 3 × 3 mm2, 6 × 6 mm2 and as large as 9 × 9 mm2. The qualitative and quantitative comparisons show that SAR-GAN provides perceptually better visualization and significantly enhances the image quality in terms of noise intensity, contrast-to-noise ratio and vessel connectivity. Moreover, it demonstrates superior image enhancement for retinal OCTA with small or large FOVs, compared with other traditional and deep-learning based methods. The SAR-GAN has great potential to improve the clinical assessment by wide-field OCTA. © 2022","Angiography; Deep learning; Generative adversarial networks; Image quality; Image reconstruction; Image resolution; Ophthalmology; Optical data processing; Optical tomography; Angiography images; Clinical practices; Images processing; Learning-based methods; Lower resolution; Medical and biological imaging; Optical coherence tomography angiography; Retinal optical coherence tomography; Superresolution; Wide-field; article; clinical assessment; contrast to noise ratio; controlled study; deep learning; human; image enhancement; image processing; image quality; loss of function mutation; noise; optical coherence tomography; optical coherence tomography angiography; spectral domain optical coherence tomography; Image enhancement","Image Enhancement; Image Processing; Medical and biological imaging; Optical coherence tomography; Optical coherence tomography angiography","Article","Final","","Scopus","2-s2.0-85134573696"
"Ota J.; Umehara K.; Kershaw J.; Kishimoto R.; Hirano Y.; Tachibana Y.; Ohba H.; Obata T.","Ota, Junko (57191333946); Umehara, Kensuke (57194469605); Kershaw, Jeff (7006044699); Kishimoto, Riwa (35322398800); Hirano, Yoshiyuki (57221670345); Tachibana, Yasuhiko (57591392800); Ohba, Hisateru (57750794600); Obata, Takayuki (57751305400)","57191333946; 57194469605; 7006044699; 35322398800; 57221670345; 57591392800; 57750794600; 57751305400","Super-resolution generative adversarial networks with static T2*WI-based subject-specific learning to improve spatial difference sensitivity in fMRI activation","2022","Scientific Reports","12","1","10319","","","","10.1038/s41598-022-14421-5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132267524&doi=10.1038%2fs41598-022-14421-5&partnerID=40&md5=612ef085d6a5be1bd9c1fd0c511732fe","The spatial resolution of fMRI is relatively poor and improvements are needed to indicate more specific locations for functional activities. Here, we propose a novel scheme, called Static T2*WI-based Subject-Specific Super Resolution fMRI (STSS-SRfMRI), to enhance the functional resolution, or ability to discriminate spatially adjacent but functionally different responses, of fMRI. The scheme is based on super-resolution generative adversarial networks (SRGAN) that utilize a T2*-weighted image (T2*WI) dataset as a training reference. The efficacy of the scheme was evaluated through comparison with the activation maps obtained from the raw unpreprocessed functional data (raw fMRI). MRI images were acquired from 30 healthy volunteers using a 3 Tesla scanner. The modified SRGAN reconstructs a high-resolution image series from the original low-resolution fMRI data. For quantitative comparison, several metrics were calculated for both the STSS-SRfMRI and the raw fMRI activation maps. The ability to distinguish between two different finger-tapping tasks was significantly higher [p = 0.00466] for the reconstructed STSS-SRfMRI images than for the raw fMRI images. The results indicate that the functional resolution of the STSS-SRfMRI scheme is superior, which suggests that the scheme is a potential solution to realizing higher functional resolution in fMRI images obtained using 3T MRI. © 2022, The Author(s).","Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; adult; article; clinical article; comparative effectiveness; controlled study; female; finger tapping test; functional magnetic resonance imaging; human; human experiment; learning; male; nuclear magnetic resonance imaging; image processing; nuclear magnetic resonance imaging; procedures","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132267524"
"Huang B.; Li J.; Yao B.; Yang Z.; Lam E.Y.; Zhang J.; Yan W.; Qu J.","Huang, Boyi (57444024300); Li, Jia (37067453800); Yao, Bowen (57216728675); Yang, Zhigang (55716627900); Lam, Edmund Y. (55694417300); Zhang, Jia (57216517749); Yan, Wei (55773252800); Qu, Junle (7201534904)","57444024300; 37067453800; 57216728675; 55716627900; 55694417300; 57216517749; 55773252800; 7201534904","Enhancing image resolution of confocal fluorescence microscopy with deep learning","2023","PhotoniX","4","1","2","","","","10.1186/s43074-022-00077-x","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145573023&doi=10.1186%2fs43074-022-00077-x&partnerID=40&md5=c3d0c9200bb27a30b8b4b65d79de24a7","Super-resolution optical imaging is crucial to the study of cellular processes. Current super-resolution fluorescence microscopy is restricted by the need of special fluorophores or sophisticated optical systems, or long acquisition and computational times. In this work, we present a deep-learning-based super-resolution technique of confocal microscopy. We devise a two-channel attention network (TCAN), which takes advantage of both spatial representations and frequency contents to learn a more precise mapping from low-resolution images to high-resolution ones. This scheme is robust against changes in the pixel size and the imaging setup, enabling the optimal model to generalize to different fluorescence microscopy modalities unseen in the training set. Our algorithm is validated on diverse biological structures and dual-color confocal images of actin-microtubules, improving the resolution from ~ 230 nm to ~ 110 nm. Last but not least, we demonstrate live-cell super-resolution imaging by revealing the detailed structures and dynamic instability of microtubules. © 2023, The Author(s).","Bioinformatics; Deep learning; Fluorescence microscopy; Fluorophores; Generative adversarial networks; Image enhancement; Proteins; 'current; Acquisition time; Cellular process; Confocal fluorescence microscopy; Deep learning; Image resolution enhancements; Microtubules; Optical imaging; Super-resolution fluorescence microscopy; Superresolution; Image resolution","Deep learning; Generative adversarial network; Image resolution enhancement; Super-resolution fluorescence microscopy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145573023"
"Ali H.; Biswas R.; Ali F.; Shah U.; Alamgir A.; Mousa O.; Shah Z.","Ali, Hazrat (56501336300); Biswas, Rafiul (57420389600); Ali, Farida (57737659400); Shah, Uzair (57217587104); Alamgir, Asma (57389797000); Mousa, Osama (57359622600); Shah, Zubair (56428700200)","56501336300; 57420389600; 57737659400; 57217587104; 57389797000; 57359622600; 56428700200","The role of generative adversarial networks in brain MRI: a scoping review","2022","Insights into Imaging","13","1","98","","","","10.1186/s13244-022-01237-0","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131790521&doi=10.1186%2fs13244-022-01237-0&partnerID=40&md5=ac992705f6360ec971c4947b71025eef","The performance of artificial intelligence (AI) for brain MRI can improve if enough data are made available. Generative adversarial networks (GANs) showed a lot of potential to generate synthetic MRI data that can capture the distribution of real MRI. Besides, GANs are also popular for segmentation, noise removal, and super-resolution of brain MRI images. This scoping review aims to explore how GANs methods are being used on brain MRI data, as reported in the literature. The review describes the different applications of GANs for brain MRI, presents the most commonly used GANs architectures, and summarizes the publicly available brain MRI datasets for advancing the research and development of GANs-based approaches. This review followed the guidelines of PRISMA-ScR to perform the study search and selection. The search was conducted on five popular scientific databases. The screening and selection of studies were performed by two independent reviewers, followed by validation by a third reviewer. Finally, the data were synthesized using a narrative approach. This review included 139 studies out of 789 search results. The most common use case of GANs was the synthesis of brain MRI images for data augmentation. GANs were also used to segment brain tumors and translate healthy images to diseased images or CT to MRI and vice versa. The included studies showed that GANs could enhance the performance of AI methods used on brain MRI imaging data. However, more efforts are needed to transform the GANs-based methods in clinical applications. © 2022, The Author(s).","algorithm; artificial intelligence; brain tumor; convolutional neural network; data extraction; data synthesis; functional magnetic resonance imaging; generative adversarial network; human; image segmentation; machine learning; neuroimaging; nuclear magnetic resonance imaging; recurrent neural network; Review; training; tumor growth; validation study","Artificial intelligence; Data augmentation; Generative adversarial networks; Magnetic resonance imaging; Medical imaging","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85131790521"
"Li R.; Liu W.; Gong W.; Zhu X.; Wang X.","Li, R. (13608752200); Liu, W. (57835130000); Gong, W. (57448532700); Zhu, X. (57753356900); Wang, X. (57828984600)","13608752200; 57835130000; 57448532700; 57753356900; 57828984600","Super resolution for single satellite image using a generative adversarial network","2022","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","3","","591","596","5","10.5194/isprs-Annals-V-3-2022-591-2022","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132028311&doi=10.5194%2fisprs-Annals-V-3-2022-591-2022&partnerID=40&md5=7b3f68979d3147f64a10c9a18f2c35f1","Inspired by the immense success of deep neural network in image processing and object recognition, learning-based image super resolution (SR) methods have been highly valued and have become the mainstream direction of super resolution research. Base on the recent proposed state-of-Art convolution neural network (CNN) super-resolution methods, this paper proposed a generative adversarial network for single satellite image Super Resolution reconstruction. It built on a trained deep residual network to generate preliminary SR images, combined with a discriminative network learns to differentiate preliminary SR images and High resolution samples. The experiments results show that our method can use existing model parameters to refine SR image performance.  © Authors 2022.","Deep neural networks; Generative adversarial networks; Object recognition; Satellite imagery; Convolution neural network; Image objects; Image super resolutions; Images processing; Objects recognition; Residual network; Resolution images; Satellite images; Superresolution; Superresolution methods; Optical resolving power","Generative Adversarial Network; Residual Network; Satellite Imagery; Super Resolution","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132028311"
"Hu R.; Cui X.","Hu, Ruizhe (57211456153); Cui, Xiaocan (57222651345)","57211456153; 57222651345","RETRACTED ARTICLE: Application of single frame image super-resolution algorithm based on generative adversarial network in tennis motion image resolution","2022","Journal of Ambient Intelligence and Humanized Computing","13","","","131","","","10.1007/s12652-021-03100-4","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103658763&doi=10.1007%2fs12652-021-03100-4&partnerID=40&md5=880b413f197a69c47bfbcf72f9e418ea","[No abstract available]","Application programs; Convolution; Convolutional neural networks; Generative adversarial networks; Image enhancement; Image reconstruction; Optical resolving power; Personnel training; Confrontation network; Convolutional neural network; Image degradation process; Image super resolutions; Images reconstruction; Low resolution images; Motion images; Single frame image; Super resolution algorithms; Super-resolution reconstruction; Tennis","","Article","Final","","Scopus","2-s2.0-85103658763"
"Qiu D.; Cheng Y.; Wang X.","Qiu, Defu (57211118363); Cheng, Yuhu (9733966500); Wang, Xuesong (56090094900)","57211118363; 9733966500; 56090094900","Improved generative adversarial network for retinal image super-resolution","2022","Computer Methods and Programs in Biomedicine","225","","106995","","","","10.1016/j.cmpb.2022.106995","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135864324&doi=10.1016%2fj.cmpb.2022.106995&partnerID=40&md5=547eeb45dde2272ce23532a4361e84c6","Background and objective: The retina is the only organ in the body that can use visible light for non-invasive observation. By analyzing retinal images, we can achieve early screening, diagnosis and prevention of many ophthalmological and systemic diseases, helping patients avoid the risk of blindness. Due to the powerful feature extraction capabilities, many deep learning super-resolution reconstruction networks have been applied to retinal image analysis and achieved excellent results. Methods: Given the lack of high-frequency information and poor visual perception in the current reconstruction results of super-resolution reconstruction networks under large-scale factors, we present an improved generative adversarial network (IGAN) algorithm for retinal image super-resolution reconstruction. Firstly, we construct a novel residual attention block, improving the reconstruction results lacking high-frequency information and texture details under large-scale factors. Secondly, we remove the Batch Normalization layer that affects the quality of image generation in the residual network. Finally, we use the more robust Charbonnier loss function instead of the mean square error loss function and the TV regular term to smooth the training results. Results: Experimental results show that our proposed method significantly improves objective evaluation indicators such as peak signal-to-noise ratio and structural similarity. The obtained image has rich texture details and a better visual experience than the state-of-the-art image super-resolution methods. Conclusion: Our proposed method can better learn the mapping relationship between low-resolution and high-resolution retinal images. This method can be effectively and stably applied to the analysis of retinal images, providing an effective basis for early clinical treatment. © 2022 Elsevier B.V.","Algorithms; Humans; Image Processing, Computer-Assisted; Signal-To-Noise Ratio; Convolutional neural networks; Deep learning; Diagnosis; Image analysis; Image enhancement; Image reconstruction; Mean square error; Ophthalmology; Optical resolving power; Signal to noise ratio; Textures; Convolutional neural network; High-frequency informations; Image super resolutions; Large-scales; Reconstruction networks; Residual learning; Retinal image; Scale Factor; Super-resolution reconstruction; Superresolution; Article; attention; brightness; computer assisted tomography; contrast; convolutional neural network; diabetic retinopathy; entropy; human; image analysis; image quality; image reconstruction; learning; mean squared error; Nash equilibrium; retina image; tooth root canal; vision; algorithm; image processing; procedures; signal noise ratio; Generative adversarial networks","Convolutional neural network; Generative adversarial network; Residual learning; Retinal image; Super-resolution","Article","Final","","Scopus","2-s2.0-85135864324"
"Dastmalchi H.; Aghaeinia H.","Dastmalchi, Hamidreza (36138646000); Aghaeinia, Hassan (9335619600)","36138646000; 9335619600","Super-resolution of very low-resolution face images with a wavelet integrated, identity preserving, adversarial network","2022","Signal Processing: Image Communication","107","","116755","","","","10.1016/j.image.2022.116755","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132745887&doi=10.1016%2fj.image.2022.116755&partnerID=40&md5=bd4ab4f8a9fb3d31e64e61fe752cb6e9","Super-resolution of face images, known as Face Hallucination (FH), has been excessively studied in recent years. Modern FH methods use deep Convolution Neural Networks (CNN) with a pixel-wise MSE loss function to infer high-resolution facial images. The MSE-oriented approaches generate over-smooth results, particularly when dealing with very low-resolution images. Recently, Generative Adversarial Networks (GANs) have successfully been exploited to synthesize perceptually more pleasant images. However, the GAN-based models do not guarantee identity preservation during face super-resolution. To address these challenges, we have proposed a novel Wavelet-integrated, Identity Preserving, Adversarial (WIPA) approach. Specifically, we present Wavelet Prediction blocks attached to a Baseline CNN network to predict wavelet missing details of facial images. The extracted wavelet coefficients are concatenated with original feature maps in different scales to recover fine details. Unlike other wavelet-based FH methods, this algorithm exploits the wavelet-enriched feature maps as complementary information to facilitate the hallucination task. We introduce a wavelet prediction loss to push the network to generate wavelet coefficients. In addition to the wavelet-domain cost function, a combination of perceptual, adversarial, and identity loss functions has been utilized to achieve low-distortion and perceptually high-quality images while maintaining identity. The extensive experiments prove the superiority of the proposed approach over the state-of-the-art methods by achieving PSNR of 25.16 dB for CelebA dataset and verification rate of 86.1% for LFW dataset; both conducted on 8X magnification factor. © 2022 Elsevier B.V.","Cost functions; Generative adversarial networks; Optical resolving power; Wavelet transforms; Adversarial networks; Convolution neural network; Face hallucination; Feature map; Identity preserving; Loss functions; Low-resolution face images; Perceptual quality; Superresolution; Wavelet prediction; Forecasting","Face Hallucination; Generative Adversarial Networks; Identity preserving; Perceptual quality; Super-resolution; Wavelet prediction","Article","Final","","Scopus","2-s2.0-85132745887"
"Sreelakshmy I.J.; Kovoor B.C.","Sreelakshmy, I.J. (57194448677); Kovoor, Binsu C. (57194778322)","57194448677; 57194778322","Generative Inpainting of High-resolution Images: Redefined with Real-ESRGAN","2022","International Journal on Artificial Intelligence Tools","31","5","2250035","","","","10.1142/S021821302250035X","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136238963&doi=10.1142%2fS021821302250035X&partnerID=40&md5=b1cfe8c83a650c223b9e2171875b64c5","Embracing generative models to the inpainting realm has resulted in its overwhelming acceptability as an image editing technique. However, these generative inpainting techniques tend to be cumbersome in view of the large memory footprint and resources consumed during the operation. This accounts for why images above 2k resolution are far from being considered as input for inpainting operations. In the present era of super-resolution, where human eyes are accustomed to viewing images beyond 6k and 8k, inpainting algorithms were not at par with the resolution benchmark. This paper proposes a high-resolution inpainting algorithm that addresses these concerns. To improve the computational overhead, the proposed architecture follows a hierarchically scaled 2-stage generative inpainting network. The proposed framework showcases a novel approach of injecting a blind Super Resolution Generative Adversarial Network network in an inpainting scenario. The resolution lost in the first phase is regained by the embedded pre-trained Real-ESRGAN model employed in the final phase. Qualitative and quantitative evaluation of the proposed method was performed using various datasets. Results obtained on comparing with the state-of-the-art inpainting methods were promising and ought to be a big leap towards super-resolution inpainting methods. © 2022 World Scientific Publishing Company.","Image processing; Network architecture; Optical resolving power; Coarse to fine; Coarse-to-fine GAN architecture; Gated convolution; Generative inpainting; High resolution; High-resolution images; Inpainting; Inpainting method; Real-ESRGAN; Superresolution; Generative adversarial networks","coarse-to-fine GAN architecture; gated convolution; Generative inpainting; high resolution; Real-ESRGAN","Article","Final","","Scopus","2-s2.0-85136238963"
"Chudasama V.; Patel H.; Prajapati K.; Sarvaiya A.; Upla K.","Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Prajapati, Kalpesh (57217177596); Sarvaiya, Anjali (57224979212); Upla, Kishor (53985429600)","57202047982; 57209145428; 57217177596; 57224979212; 53985429600","Generative Adversarial Network-Based Improved Progressive Approach for Image Super-Resolution: ImProSRGAN","2023","Lecture Notes in Electrical Engineering","952","","","291","305","14","10.1007/978-981-19-6737-5_24","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144821229&doi=10.1007%2f978-981-19-6737-5_24&partnerID=40&md5=a9f3e9a5ffce80852e4092ed80d8a8d8","Recently, convolutional neural networks (CNNs) have been explored to achieve exceptional performance on super-resolution (SR) in terms of distortion metrics. In these methods, pixel-based loss functions have been employed to optimize their networks leading to overly smooth blurry solutions. In contrast, a generative adversarial network (GAN) has the proficiency to bring out perceptually better SR solutions. In the case of larger upscaling factors, some degradations are still discovered in the SR observations that can be reduced by increasing the number of convolution layers. However, such approach tends to increase the number of trainable parameters and additionally provides a lot of burden on resources which leads it to be unavailable for many real-world tasks. Here, we propose an improved progressive approach for SISR using GAN (i.e. ImProSRGAN). The potency of the proposed model has been seen by conducting different experiments where we observe that the introduced ImProSRGAN model performs better than existing GAN-based SISR approaches even though picking up fewer training parameters. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Generative adversarial networks; Image enhancement; Optical resolving power; Adversarial learning; Convolution layer; Convolutional neural network; Distortion metrics; Image super resolutions; Loss functions; Network-based; Perceptual quality; Performance; Superresolution; Convolution","Adversarial learning; Convolution layers; Perceptual quality; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85144821229"
"Xu Y.; Luo W.; Hu A.; Xie Z.; Xie X.; Tao L.","Xu, Yongyang (57095192900); Luo, Wei (55584802653); Hu, Anna (57205419850); Xie, Zhong (36164790400); Xie, Xuejing (57212576401); Tao, Liufeng (57210018064)","57095192900; 55584802653; 57205419850; 36164790400; 57212576401; 57210018064","TE-SAGAN: An Improved Generative Adversarial Network for Remote Sensing Super-Resolution Images","2022","Remote Sensing","14","10","2425","","","","10.3390/rs14102425","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130994887&doi=10.3390%2frs14102425&partnerID=40&md5=28149b448945d41d9c05e4fe052bb09d","Resolution is a comprehensive reflection and evaluation index for the visual quality of remote sensing images. Super-resolution processing has been widely applied for extracting information from remote sensing images. Recently, deep learning methods have found increasing application in the super-resolution processing of remote sensing images. However, issues such as blurry object edges and existing artifacts persist. To overcome these issues, this study proposes an improved generative adversarial network with self-attention and texture enhancement (TE-SAGAN) for remote sensing super-resolution images. We first designed an improved generator based on the residual dense block with a self-attention mechanism and weight normalization. The generator gains the feature extraction capability and enhances the training model stability to improve edge contour and texture. Subsequently, a joint loss, which is a combination of L1-norm, perceptual, and texture losses, is designed to optimize the training process and remove artifacts. The L1-norm loss is designed to ensure the consistency of low-frequency pixels; perceptual loss is used to entrench medium-and high-frequency details; and texture loss provides the local features for the super-resolution process. The results of experiments using a publicly available dataset (UC Merced Land Use dataset) and our dataset show that the proposed TE-SAGAN yields clear edges and textures in the super-resolution reconstruction of remote sensing images. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image enhancement; Image texture; Land use; Optical resolving power; Remote sensing; Textures; Attention mechanisms; Joint loss; Normalisation; Remote sensing images; Remote-sensing; Resolution images; Self-attention mechanism; Super-resolution image; Superresolution; Weight normalization; Generative adversarial networks","generative adversarial network; joint loss; self-attention mechanism; super-resolution image; weight normalization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85130994887"
"Jeihouni P.; Dehzangi O.; Amireskandari A.; Rezai A.; Nasrabadi N.M.","Jeihouni, Paria (57215874065); Dehzangi, Omid (23396296800); Amireskandari, Annahita (55135990500); Rezai, Ali (35479361100); Nasrabadi, Nasser M. (7006312852)","57215874065; 23396296800; 55135990500; 35479361100; 7006312852","MultiSDGAN: Translation of OCT Images to Superresolved Segmentation Labels Using Multi-Discriminators in Multi-Stages","2022","IEEE Journal of Biomedical and Health Informatics","26","4","","1614","1627","13","10.1109/JBHI.2021.3110265","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115195145&doi=10.1109%2fJBHI.2021.3110265&partnerID=40&md5=09903e6aaf84e693d320c194b1f54ac3","Optical coherence tomography (OCT) has been identified as a non-invasive and inexpensive imaging modality to discover potential biomarkers for Alzheimer's diagnosis and progress determination. Current hypotheses presume the thickness of the retinal layers, which are analyzable within OCT scans, as an effective biomarker for the presence of Alzheimer's. As a logical first step, this work concentrates on the accurate segmentation of retinal layers to isolate the layers for further analysis. This paper proposes a generative adversarial network (GAN) that concurrently learns to increase the image resolution for higher clarity and then segment the retinal layers. We propose a multi-stage and multi-discriminatory generative adversarial network (MultiSDGAN) specifically for superresolution and segmentation of OCT scans of the retinal layer. The resulting generator is adversarially trained against multiple discriminator networks at multiple stages. We aim to avoid early saturation of generator model training leading to poor segmentation accuracies and enhance the process of OCT domain translation by satisfying all the discriminators in multiple scales. We also investigated incorporating the Dice loss and Structured Similarity Index Measure (SSIM) as additional loss functions to specifically target and improve our proposed GAN architecture's segmentation and superresolution performance, respectively. The ablation study results conducted on our data set suggest that the proposed MultiSDGAN with ten-fold cross-validation (10-CV) provides a reduced equal error rate with 44.24% and 34.09% relative improvements, respectively (p-values of the improvement level tests$< $.01). Furthermore, our experimental results also demonstrate that the addition of the new terms to the loss function improves the segmentation results significantly by relative improvements of 31.33% (p-value$<$.01).  © 2013 IEEE.","Alzheimer Disease; Humans; Image Processing, Computer-Assisted; Retina; Tomography, Optical Coherence; Biomarkers; Discriminators; Image resolution; Ophthalmology; Optical resolving power; Optical tomography; Statistical tests; biological marker; Adversarial networks; Cross validation; Generator modeling; Segmentation accuracy; Segmentation results; Similarity indices; Super resolution; Superresolution performance; accuracy; Alzheimer disease; Article; classifier; convolutional neural network; cross validation; feature extraction; human; image segmentation; learning; optical coherence tomography; parameters; training; diagnostic imaging; image processing; procedures; retina; Image segmentation","dice loss; generative adversarial networks; multi-discriminatory; multi-stage generator; Optical coherence tomography; SSIM; superresolution","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85115195145"
"Ren X.; Hui Q.; Zhao X.; Xiong J.; Yin J.","Ren, Xinyi (58092774900); Hui, Qiang (58092775000); Zhao, Xingke (58092880300); Xiong, Jianping (58092676300); Yin, Jun (58092880400)","58092774900; 58092775000; 58092880300; 58092676300; 58092880400","BESRGAN: Boundary equilibrium face super-resolution generative adversarial networks","2023","IET Image Processing","","","","","","","10.1049/ipr2.12755","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147510142&doi=10.1049%2fipr2.12755&partnerID=40&md5=c209015c16bf29c7a82a9a371bc7e488","Existing Generative Adversarial Networks (GAN)-based face hallucination algorithms are hard to control the face fidelity of the generated samples, and easily generate flawed faces with unfavourable artefacts and distortions. To address this problem, the authors propose a fidelity-controllable face super-resolution (FSR) network boundary equilibrium face super-resolution generative adversarial networks (BESRGAN), a fidelity ratio is introduced in their network to control how much the adversarial effect the discriminator is put on the generator; therefore, the authors’ network better trades off the objective and perceptual quality. Additionally, the authors design an equilibrium perceptual discriminator to match the perception loss distributions. Under the equilibrium constraint, the discriminator pays more attention to learning fine-grained feature statistics of ground truths, and further guides the generator to produce photo-realistic faces, especially in terms of facial textures. Moreover, the authors propose a novel channel-spatial attention module (CSAM) to eliminate local distortions, by further fusing richer information from the facial prior knowledge and global high-level facial descriptions. Extensive experiments illustrate that the authors’ approach preserves high pixel-wise accuracy while achieving superior visual performance against state-of-the-art methods. Specifically, the peak signal to noise ratio (PSNR) and structural similarity index (SSIM) of the authors’ proposed BESRGAN rise 0.64 dB and 0.02 for CelebA compared with one of the state-of-the-art face super-resolution (FSR) methods. © 2023 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Generative adversarial networks; Image quality; Image resolution; Quality control; Signal to noise ratio; Textures; Boundary equilibrium; Face hallucination; Face super-resolution; Images reconstruction; Loss distribution; Network boundaries; Network-based; Objective qualities; Perceptual quality; Trade off; Image reconstruction","image reconstruction; image resolution; image restoration","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85147510142"
"Zhang Y.; Tsang I.W.; Luo Y.; Hu C.; Lu X.; Yu X.","Zhang, Yang (57201701049); Tsang, Ivor W. (7004570429); Luo, Yawei (55766236300); Hu, Changhui (55903374400); Lu, Xiaobo (14627586600); Yu, Xin (57191429254)","57201701049; 7004570429; 55766236300; 55903374400; 14627586600; 57191429254","Recursive Copy and Paste GAN: Face Hallucination From Shaded Thumbnails","2022","IEEE Transactions on Pattern Analysis and Machine Intelligence","44","8","","4321","4338","17","10.1109/TPAMI.2021.3061312","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101736853&doi=10.1109%2fTPAMI.2021.3061312&partnerID=40&md5=49244f94da3857e2c8d949ae7c4dab10","Existing face hallucination methods based on convolutional neural networks (CNNs) have achieved impressive performance on low-resolution (LR) faces in a normal illumination condition. However, their performance degrades dramatically when LR faces are captured in non-uniform illumination conditions. This paper proposes a Recursive Copy and Paste Generative Adversarial Network (Re-CPGAN) to recover authentic high-resolution (HR) face images while compensating for non-uniform illumination. To this end, we develop two key components in our Re-CPGAN: internal and recursive external Copy and Paste networks (CPnets). Our internal CPnet exploits facial self-similarity information residing in the input image to enhance facial details; while our recursive external CPnet leverages an external guided face for illumination compensation. Specifically, our recursive external CPnet stacks multiple external Copy and Paste (EX-CP) units in a compact model to learn normal illumination and enhance facial details recursively. By doing so, our method offsets illumination and upsamples facial details progressively in a coarse-to-fine fashion, thus alleviating the ambiguity of correspondences between LR inputs and external guided inputs. Furthermore, a new illumination compensation loss is developed to capture illumination from the external guided face image effectively. Extensive experiments demonstrate that our method achieves authentic HR face images in a uniform illumination condition with a 16× magnification factor and outperforms state-of-the-art methods qualitatively and quantitatively.  © 1979-2012 IEEE.","Algorithms; Face; Hallucinations; Humans; Neural Networks, Computer; Convolutional neural networks; Copying; Inertial confinement fusion; Adversarial networks; Face hallucination; Illumination compensation; Illumination conditions; Magnification factors; Non-uniform illumination; State-of-the-art methods; Uniform illumination; algorithm; face; hallucination; human; Image enhancement","Face hallucination; Generative adversarial network; Illumination normalization; Super-resolution","Article","Final","","Scopus","2-s2.0-85101736853"
"Gu W.; Li B.; Luo J.; Yan Z.; Ta D.; Liu X.","Gu, Wenting (57215775331); Li, Boyi (57984742300); Luo, Jianwen (7404182785); Yan, Zhuangzhi (7402519571); Ta, Dean (57610009100); Liu, Xin (56605531700)","57215775331; 57984742300; 7404182785; 7402519571; 57610009100; 56605531700","Ultrafast Ultrasound Localization Microscopy by Conditional Generative Adversarial Network","2023","IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control","70","1","","25","40","15","10.1109/TUFFC.2022.3222534","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142849295&doi=10.1109%2fTUFFC.2022.3222534&partnerID=40&md5=81da1354286d73853376241d4f4db895","Ultrasound localization microscopy (ULM) overcomes the acoustic diffraction limit and enables the visualization of microvasculature at subwavelength resolution. However, challenges remain in ultrafast ULM implementation, where short data acquisition time, efficient data processing speed, and high imaging resolution need to be considered simultaneously. Recently, deep learning (DL)-based methods have exhibited potential in speeding up ULM imaging. Nevertheless, a certain number of ultrasound (US) data (L frames) are still required to accumulate enough localized microbubble (MB) events, leading to an acquisition time within a time span of tens of seconds. To further speed up ULM imaging, in this article, we present a new DL-based method, termed as ULM-GAN. By using a modified conditional generative adversarial network (cGAN) framework, ULM-GAN is able to reconstruct a superresolution image directly from a temporal mean low-resolution (LR) image generated by averaging l-frame raw US images with l being significantly smaller than L. To evaluate the performance of ULM-GAN, a series of numerical simulations and phantom experiments are both implemented. The results of the numerical simulations demonstrate that when performing ULM imaging, ULM-GAN allows ∼ 40-fold reduction in data acquisition time and ∼ 61 -fold reduction in computational time compared with the conventional Gaussian fitting method, without compromising spatial resolution according to the resolution scaled error (RSE). For the phantom experiments, ULM-GAN offers an implementation of ULM with ultrafast data acquisition time (∼ 0.33 s) and ultrafast data processing speed (∼ 0.60 s) that makes it promising to observe rapid biological activities in vivo.  © 1986-2012 IEEE.","Data acquisition; Data handling; Deep learning; Diffraction; Numerical methods; Numerical models; Optical resolving power; Phantoms; Ultrasonic imaging; Acquisition time; Deep learning; Microscopy imaging; Processing speed; Super-resolution ultrasound imaging; Superresolution; Ultra-fast; Ultrasound imaging; Ultrasound localization; Ultrasound localization microscopy; Generative adversarial networks","Deep learning (DL); generative adversarial networks; superresolution (SR) ultrasound (US) imaging; ultrasound localization microscopy (ULM)","Article","Final","","Scopus","2-s2.0-85142849295"
"Arun Pandian J.; Kanchanadevi K.","Arun Pandian, J. (56398419000); Kanchanadevi, K. (57579207700)","56398419000; 57579207700","An improved deep convolutional neural network for detecting plant leaf diseases","2022","Concurrency and Computation: Practice and Experience","34","28","e7357","","","","10.1002/cpe.7357","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139461196&doi=10.1002%2fcpe.7357&partnerID=40&md5=7d1df58bf928723c23c682824c6ddb31","This research proposes a novel dense convolutional neural network with five dense blocks (5DB-DenseConvNet) for the detection of plant leaf diseases. Five dense blocks and four transition layers were used to develop the 5DB-DenseConvNet. The 5DB-DenseConvNet was trained and tested using a plant leaf disease image dataset. Such advanced data augmentation techniques as color augmentations, geometric transformations, neural style transfer, principal component analysis, color augmentation, random erasing, and super-resolution generative adversarial network were used to increase the dataset size. The dataset contains 97 diseased and healthy image classes of 21 plants and comprises 145,500 images of healthy and diseased plant leaves. The Bayesian searching technique was used to optimize the hyperparameter values of 5DB-DenseConvNet. Compression factor was introduced in the transition layer to improve the compactness of the 5DB-DenseConvNet. The 5DB-DenseConvNet was trained on a graphics processing unit environment for 1000 epochs. The trained 5DB-DenseConvNet performed at an average classification accuracy of 99.36% and at an average precision of 98.83% on the test dataset. The experimental results demonstrate that the performance and reliability of the proposed 5DB-DenseConvNet on plant disease detection are superior to prior transfer learning techniques. © 2022 John Wiley & Sons, Ltd.","Classification (of information); Computer graphics; Computer graphics equipment; Convolution; Deep neural networks; Generative adversarial networks; Graphics processing unit; Multilayer neural networks; Plants (botany); Principal component analysis; Program processors; Statistical tests; Transfer learning; Bayesian optimization; Convolutional neural network; Data augmentation; Dense block; Dense convolutional neural network; Disease detection; Plant disease; Plant disease detection; Transfer learning; Transition layers; Convolutional neural networks","Bayesian optimization; data augmentation; dense block; dense convolutional neural network; plant disease detection; transfer learning; transition layer","Article","Final","","Scopus","2-s2.0-85139461196"
"Li W.; Zhou K.; Qi L.; Lu L.; Lu J.","Li, Wenbo (57221637668); Zhou, Kun (57215777703); Qi, Lu (57201703499); Lu, Liying (57215964225); Lu, Jiangbo (57200684958)","57221637668; 57215777703; 57201703499; 57215964225; 57200684958","Best-Buddy GANs for Highly Detailed Image Super-resolution","2022","Proceedings of the 36th AAAI Conference on Artificial Intelligence, AAAI 2022","36","","","1412","1420","8","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138252551&partnerID=40&md5=72d901e28b614b36f4f6017a3ee237e3","We consider the single image super-resolution (SISR) problem, where a high-resolution (HR) image is generated based on a low-resolution (LR) input. Recently, generative adversarial networks (GANs) become popular to hallucinate details. Most methods along this line rely on a predefined single-LR-single-HR mapping, which is not flexible enough for the ill-posed SISR task. Also, GAN-generated fake details may often undermine the realism of the whole image. We address these issues by proposing best-buddy GANs (Beby-GAN) for rich-detail SISR. Relaxing the rigid one-to-one constraint, we allow the estimated patches to dynamically seek trustworthy surrogates of supervision during training, which is beneficial to producing more reasonable details. Besides, we propose a region-aware adversarial learning strategy that directs our model to focus on generating details for textured areas adaptively. Extensive experiments justify the effectiveness of our method. An ultra-high-resolution 4K dataset is also constructed to facilitate future super-resolution research. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org).","Learning systems; Optical resolving power; Textures; Adversarial learning; Detailed images; High-resolution images; High-resolution mapping; Ill posed; Image super resolutions; Learning strategy; Lower resolution; Single images; Ultrahigh resolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85138252551"
"Wang Q.; Zhou H.; Li G.; Guo J.","Wang, Qiang (56108435800); Zhou, Hongbin (57204351574); Li, Guangyuan (57219101668); Guo, Jiansheng (55681097300)","56108435800; 57204351574; 57219101668; 55681097300","Single Image Super-Resolution Method Based on an Improved Adversarial Generation Network","2022","Applied Sciences (Switzerland)","12","12","6067","","","","10.3390/app12126067","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132693035&doi=10.3390%2fapp12126067&partnerID=40&md5=cc4a98397b885c9697bf490990e45695","Super-Resolution (SR) techniques for image restoration have recently been gaining attention due to their excellent performance. For powerful learning abilities, Generative Adversarial Networks (GANs) have been proven to have achieved great success. In this paper, we propose an Enhanced Generative Adversarial Network (EGAN) for improving its effects for a real-time Super-Resolution task. The main content of this paper are as follows: (1) We adopted the Laplacian pyramid framework as a pre-trained module, which is beneficial for providing multiscale features for our input. (2) At each feature block, a convolutional skip-connections network, which may contain some latent information, was significant for the generative model to reconstruct a plausible-looking image. (3) Considering that the edge details usually play an important role in image generation, a perceptual loss function was defined to train and seek the optimal parameters. Quantitative and qualitative evaluations were demonstrated so that our algorithm not only took full advantage of the Convolutional Neural Networks (CNNs) to improve the image quality, but also performed better than other algorithms in speed and performance for real-time Super-Resolution tasks. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","deep-learning; generative adversarial networks; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132693035"
"Shuting K.; Minghui C.; Zexi Z.; Yuan Y.; Teng W.; Longxi H.; Linjie L.; Hao S.","Shuting, Ke (57968220000); Minghui, Chen (55733208400); Zexi, Zheng (57968220100); Yuan, Yuan (57220641423); Teng, Wang (57967761700); Longxi, He (57967761800); Linjie, Lü (57968220200); Hao, Sun (57968526300)","57968220000; 55733208400; 57968220100; 57220641423; 57967761700; 57967761800; 57968220200; 57968526300","Super-Resolution Reconstruction of Optical Coherence Tomography Retinal Images by Generating Adversarial Network; [生成对抗网络对OCT视网膜图像的超分辨率重建]","2022","Zhongguo Jiguang/Chinese Journal of Lasers","49","15","1507203","","","","10.3788/CJL202249.1507203","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142046700&doi=10.3788%2fCJL202249.1507203&partnerID=40&md5=b6f686a74331440bd58660448e51d5a4","Objective Optical coherence tomography (OCT) imaging shows great potential in clinical practice because of its noninvasive nature. However, two critical issues affect the diagnostic capability of OCT imaging. The first problem is that the interferential nature of OCT imaging produces interference noise, which reduces contrast and obfuscates fine structural features. The second problem is caused by the low spatial sampling rate of OCT. In fact, in clinical diagnosis, the use of a lower spatial sampling rate is a method to achieve a wide field of vision and reduce the impact of unconscious movement. Therefore, most OCT images obtained in reality are not optimal in terms of signal-to-noise ratio and spatial sampling rate. There are significant differences in the texture and brightness of the retinal layer in patients, as well as in the shape and size of the lesion area, so traditional models may not be able to reliably reconstruct the pathological structure. To obtain high peak signal-to-noise ratio (PSNR) and high-resolution B-scan OCT images, it is necessary to develop sufficient methods for super-resolution reconstruction of OCT images. In this paper, an improved OCT super-resolution image reconstruction network structure (PPECA-SRGAN) was proposed. Methods In this paper, a PPECA-SRGAN network based on generative adversarial network (GAN) was proposed. The network model includes a generator and a discriminator. A PA module was added between the residual blocks of the generator to increase the feature extraction capability of OCT retinal image reconstruction. In addition, a PECA module was added to the discriminator, which is an improvement of the pyramid split attention network (PSANet) and can fully capture the spatial information of multi-scale feature maps. First, we used two data sets to test a training set of 1000 images and a test set of 50 images, respectively. The data set was imported into the preprocessing module, and the low-resolution image was obtained through four down-sampling processes. Then, the generator was used to train the model to generate high-resolution images from low-resolution images. When the discriminator could not distinguish the authenticity of the images, it indicated that the generation network generated high-resolution images. Finally, the image quality was evaluated using the structural similarity index measure (SSIM) and PSNR. Results and Discussions The super-resolution index evaluation results of PPECA-SRGAN and the other three models were compared, as well as the final reconstruction effect images. In general, PPECA-SRGAN' s reconstruction effect was better than SRResNet; however, for the restoration of the image details, the image quality of the PPECA-SRGAN network reconstruction was more in line with the satisfaction degree of human vision. Compared with SRResNet, SRGAN, and ESRGAN, the SSIM indexes of PPECA-SRGAN were 0. 090, 0. 028, and 0. 016 higher and the PSNR indexes were 2.15 dB, 0.71 dB, and 0.47 dB higher, respectively. The good reconstruction effect of PPECA-SRGAN was due to the addition of the attention mechanism called path aggregation network (PANet) and the proposed attention mechanism named PECA, both enhancing the capture of OCT retinal image features and the reconstruction of details. The PECA module was composed of pyramid splitting and extracting features, with the use of ECANet to fuse multi-scale information. PANet can effectively reduce image noise, such as compression artifacts. This makes our model better than the SRGAN network and other traditional networks. Therefore, the application of the proposed model in OCT image super-resolution reconstruction has been verified, and its performance has been improved compared with other reconstruction algorithms. Conclusions The PPECA-SRGAN network structure proposed in this paper is an improved model of the SRGAN network for super-resolution reconstruction of retinal OCT B-scan images. We conducted training and verification on MICCAI RETOUCH data set and data collected by Wenzhou Medical University to solve the problems of low-resolution and few details of images collected by OCT. We used advanced GAN to improve the super-resolution reconstruction of OCT images, and the SRGAN network was improved due to the difference in reconstruction between medical images and natural images. Firstly, a PANet module was added between the residual blocks of the generator to extract multi-scale feature relations by pyramid structure and suppress unnecessary artifacts. Then, the PECA module was inserted into the discriminator to effectively combine spatial and channel attentions to learn more image details for the discriminator and obtain richer image pair feature information. The experimental results show that this model is effective and stable in improving the resolution of medical images. Compared with SRResNet, SRGAN, and ESRGAN, the PSNR and SSIM indexes of the reconstructed images were improved by about 3.5% and 5.6%, respectively. In clinical diagnosis, the proposed algorithm can overcome the inherent limitations of low-resolution imaging systems and reconstruct various details lost in the process of image acquisition; the algorithm is easy to integrate and implement. In the future, if higher-quality data sets and lighter algorithms can be obtained, it is possible to further improve the quality of super-resolution reconstruction medical images and make them more applicable in clinical practice. © 2022 Science Press. All rights reserved.","Diagnosis; Generative adversarial networks; Image enhancement; Image quality; Image reconstruction; Ophthalmology; Optical resolving power; Quality control; Signal to noise ratio; Textures; Data set; Peak signal to noise ratio; Retinal image; Sampling rates; Spatial sampling; Structural similarity; Super-resolution reconstruction; Superresolution; Tomography imaging; Unpaired image; Optical tomography","biotechnology; generative adversarial network; optical coherence tomography; super-resolution; unpaired image","Article","Final","","Scopus","2-s2.0-85142046700"
"Chen Y.; Huang M.; Liu H.; Zhang J.; Shao K.","Chen, Yong (55910921300); Huang, Meiyong (57226284431); Liu, Huanlin (15124819100); Zhang, Jinliang (57890005900); Shao, Kaixin (57889295300)","55910921300; 57226284431; 15124819100; 57890005900; 57889295300","GAN-Based Local Lightness-Aware Enhancement Network for Underexposed Images","2022","Journal of Information Processing Systems","18","4","","575","586","11","10.3745/JIPS.02.0179","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137976058&doi=10.3745%2fJIPS.02.0179&partnerID=40&md5=f89254f434604ee0a31e3181f7a26540","Uneven light in real-world causes visual degradation for underexposed regions. For these regions, insufficient consideration during enhancement procedure will result in over-/under-exposure, loss of details and color distortion. Confronting such challenges, an unsupervised low-light image enhancement network is proposed in this paper based on the guidance of the unpaired low-/normal-light images. The key components in our network include super-resolution module (SRM), a GAN-based low-light image enhancement network (LLIEN), and denoising-scaling module (DSM). The SRM improves the resolution of the low-light input images before illumination enhancement. Such design philosophy improves the effectiveness of texture details preservation by operating in high-resolution space. Subsequently, local lightness attention module in LLIEN effectively distinguishes unevenly illuminated areas and puts emphasis on low-light areas, ensuring the spatial consistency of illumination for locally underexposed images. Then, multiple discriminators, i.e., global discriminator, local region discriminator, and color discriminator performs assessment from different perspectives to avoid over- /under-exposure and color distortion, which guides the network to generate images that in line with human aesthetic perception. Finally, the DSM performs noise removal and obtains high-quality enhanced images. Both qualitative and quantitative experiments demonstrate that our approach achieves favorable results, which indicates its superior capacity on illumination and texture details restoration © 2022 KIPS","Color; Discriminators; Gallium nitride; Generative adversarial networks; III-V semiconductors; Textures; Color distortions; De-noising; Gan; Local lightness attention module; Local lightness-aware; Low-light image enhancement; Low-light images; Multiple discriminator; Scalings; Superresolution; Image enhancement","Gan; Local lightness attention module; Local lightness-aware; Low-light image enhancement; Multiple discriminators","Article","Final","","Scopus","2-s2.0-85137976058"
"Salaudeen H.; Çelebi E.","Salaudeen, Habeeb (57481579000); Çelebi, Erbuğ (35101468100)","57481579000; 35101468100","Pothole Detection Using Image Enhancement GAN and Object Detection Network","2022","Electronics (Switzerland)","11","12","1882","","","","10.3390/electronics11121882","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131893482&doi=10.3390%2felectronics11121882&partnerID=40&md5=e4f337cd6b2d8168d35f0dbff1484921","Many datasets used to train artificial intelligence systems to recognize potholes, such as the challenging sequences for autonomous driving (CCSAD) and the Pacific Northwest road (PNW) datasets, do not produce satisfactory results. This is due to the fact that these datasets present complex but realistic scenarios of pothole detection tasks than popularly used datasets that achieve better results but do not effectively represents realistic pothole detection task. In remote sensing, super-resolution generative adversarial networks (GAN), such as enhanced super-resolution generative adversarial networks (ESRGAN), have been employed to mitigate the issues of small-object detection, which has shown remarkable performance in detecting small objects from low-quality images. Inspired by this success in remote sensing, we apply similar techniques with an ESRGAN super-resolution network to improve the image quality of road surfaces, and we use different object detection networks in the same pipeline to detect instances of potholes in the images. The architecture we propose consists of two main components: ESRGAN and a detection network. For the detection network, we employ both you only look once (YOLOv5) and EfficientDet networks. Comprehensive experiments on different pothole detection datasets show better performance for our method compared to similar state-of-the-art methods for pothole detection. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","deep learning; GAN; object detection; pothole detection; small object detection; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131893482"
"Kim M.; Song M.H.","Kim, Mira (58069427600); Song, Myeong Ho (58070140500)","58069427600; 58070140500","High Performing Facial Skin Problem Diagnosis with Enhanced Mask R-CNN and Super Resolution GAN","2023","Applied Sciences (Switzerland)","13","2","989","","","","10.3390/app13020989","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146442552&doi=10.3390%2fapp13020989&partnerID=40&md5=a88dc7ab345a0509469b777dc40d0fa9","Facial skin condition is perceived as a vital indicator of the person’s apparent age, perceived beauty, and degree of health. Machine-learning-based software analytics on facial skin conditions can be a time- and cost-efficient alternative to the conventional approach of visiting facial skin care shops or dermatologist’s offices. However, the conventional CNN-based approach is shown to be limited in the diagnosis performance due to the intrinsic characteristics of facial skin problems. In this paper, the technical challenges in facial skin problem diagnosis are first addressed, and a set of 5 effective tactics are proposed to overcome the technical challenges. A total of 31 segmentation models are trained and applied to the experiments of validating the proposed tactics. Through the experiments, the proposed approach provides 83.38% of the diagnosis performance, which is 32.58% higher than the performance of conventional CNN approach. © 2023 by the authors.","","facial skin problem; Generative Adversarial Network (GAN); mask R-CNN; super resolution; tactics for high performance","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146442552"
"Ye X.; Du N.; Yang D.; Yuan X.; Song R.; Sun S.; Fang D.","Ye, Xiuzhu (35726371700); Du, Naike (57345558500); Yang, Daohan (57935743400); Yuan, Xujin (35276528100); Song, Rencheng (54893870700); Sun, Sheng (57928023300); Fang, Daining (57935732100)","35726371700; 57345558500; 57935743400; 35276528100; 54893870700; 57928023300; 57935732100","Application of Generative Adversarial Network-Based Inversion Algorithm in Imaging 2-D Lossy Biaxial Anisotropic Scatterer","2022","IEEE Transactions on Antennas and Propagation","70","9","","8262","8275","13","10.1109/TAP.2022.3164198","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128262052&doi=10.1109%2fTAP.2022.3164198&partnerID=40&md5=d1c970e1396babbec88674fc86bb3939","An effective quasi-real-time inversion algorithm based on the super-resolution generative adversarial network (SR-GAN) is proposed to quantitatively image the 2-D biaxial anisotropic scatterers. The SR-GAN was originally proposed for the purpose of super-resolution image reconstruction, which exactly fits the need for inverse problem. In addition, Visual Geometry Group (VGG) loss is introduced to extract the high-level features of the object instead of the low-level pixel-wise error measures. The angle-dependent reconstruction effect due to the dipole radiation as in the traditional inversion methods is effectively resolved by the machine learning method. Numerical results using both synthetic data and experimental data are given to validate the effectiveness of the proposed method. Both the imaging quality and resolution are greatly improved by the proposed SR-GAN algorithm, compared to the traditional iterative inversion algorithm. In addition, the computational time is reduced significantly and the quasi-real-time imaging is finally realized, which promises a potential real-time application of the inverse scattering method.  © 1963-2012 IEEE.","Differential equations; Image reconstruction; Interactive computer systems; Inverse problems; Iterative methods; Numerical methods; Real time systems; Images reconstruction; Inversion algorithm; Network-based; Real - Time system; Real- time; Super-resolution image reconstruction; Superresolution; Time inversions; Two-dimensional; Optical resolving power","Image reconstruction; inverse problems","Article","Final","","Scopus","2-s2.0-85128262052"
"Ueki W.; Nishii T.; Umehara K.; Ota J.; Higuchi S.; Ohta Y.; Nagai Y.; Murakawa K.; Ishida T.; Fukuda T.","Ueki, Wataru (57444719100); Nishii, Tatsuya (56015106400); Umehara, Kensuke (57194469605); Ota, Junko (57191333946); Higuchi, Satoshi (57233925400); Ohta, Yasutoshi (57219674155); Nagai, Yasuhiro (57444934700); Murakawa, Keizo (57444496400); Ishida, Takayuki (35406489400); Fukuda, Tetsuya (56784880500)","57444719100; 56015106400; 57194469605; 57191333946; 57233925400; 57219674155; 57444934700; 57444496400; 35406489400; 56784880500","Generative adversarial network-based post-processed image super-resolution technology for accelerating brain MRI: comparison with compressed sensing","2023","Acta Radiologica","64","1","","336","345","9","10.1177/02841851221076330","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124316780&doi=10.1177%2f02841851221076330&partnerID=40&md5=780e0addcc2b76902e3b6e802a4d60e1","Background: It is unclear whether deep-learning–based super-resolution technology (SR) or compressed sensing technology (CS) can accelerate magnetic resonance imaging (MRI). Purpose: To compare SR accelerated images with CS images regarding the image similarity to reference 2D- and 3D gradient-echo sequence (GRE) brain MRI. Material and Methods: We prospectively acquired 1.3× and 2.0× faster 2D and 3D GRE images of 20 volunteers from the reference time by reducing the matrix size or increasing the CS factor. For SR, we trained the generative adversarial network (GAN), upscaling the low-resolution images to the reference images with twofold cross-validation. We compared the structural similarity (SSIM) index of accelerated images to the reference image. The rate of incorrect answers of a radiologist discriminating faster and reference image was used as a subjective image similarity (ISM) index. Results: The SR demonstrated significantly higher SSIM than the CS (SSIM=0.9993–0.999 vs. 0.9947–0.9986; P < 0.001). In 2D GRE, it was challenging to discriminate the SR image from the reference image, compared to the CS (ISM index 40% vs. 17.5% in 1.3×; P = 0.039 and 17.5% vs. 2.5% in 2.0×; P = 0.034). In 3D GRE, the CS revealed a significantly higher ISM index than the SR (22.5% vs. 2.5%; P = 0.011) in 2.0 × faster images. However, the ISM index was identical for the 2.0× CS and 1.3× SR (22.5% vs. 27.5%; P = 0.62) with comparable time costs. Conclusion: The GAN-based SR outperformed CS in image similarity with 2D GRE for MRI acceleration. In addition, CS was more advantageous in 3D GRE than SR. © The Foundation Acta Radiologica 2022.","Brain; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Pressure; Compressed sensing; Deep learning; Engineering education; Generative adversarial networks; Image analysis; Optical resolving power; Compressed-Sensing; Deep learning; Image similarity; Image similarity index; Network-based; Post-processed images; Reference image; Sensing technology; Structural similarity; Superresolution; acceleration; adult; article; brain; clinical article; controlled study; cross validation; deep learning; female; human; human experiment; male; neuroimaging; nuclear magnetic resonance imaging; radiologist; brain; diagnostic imaging; image processing; pressure; procedures; three-dimensional imaging; Magnetic resonance imaging","acceleration; Brain; deep learning; magnetic resonance imaging; super resolution","Article","Final","","Scopus","2-s2.0-85124316780"
"Mukadam S.B.; Patil H.Y.","Mukadam, Sufiyan Bashir (58073049900); Patil, Hemprasad Yashwant (55787156800)","58073049900; 55787156800","Skin Cancer Classification Framework Using Enhanced Super Resolution Generative Adversarial Network and Custom Convolutional Neural Network","2023","Applied Sciences (Switzerland)","13","2","1210","","","","10.3390/app13021210","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146568344&doi=10.3390%2fapp13021210&partnerID=40&md5=774fc59e9bd39bb38520d2b240bc4e17","Melanin skin lesions are most commonly spotted as small patches on the skin. It is nothing but overgrowth caused by melanocyte cells. Skin melanoma is caused due to the abnormal surge of melanocytes. The number of patients suffering from skin cancer is observably rising globally. Timely and precise identification of skin cancer is crucial for lowering mortality rates. An expert dermatologist is required to handle the cases of skin cancer using dermoscopy images. Improper diagnosis can cause fatality to the patient if it is not detected accurately. Some of the classes come under the category of benign while the rest are malignant, causing severe issues if not diagnosed at an early stage. To overcome these issues, Computer-Aided Design (CAD) systems are proposed which help to reduce the burden on the dermatologist by giving them accurate and precise diagnosis of skin images. There are several deep learning techniques that are implemented for cancer classification. In this experimental study, we have implemented a custom Convolution Neural Network (CNN) on a Human-against-Machine (HAM10000) database which is publicly accessible through the Kaggle website. The designed CNN model classifies the seven different classes present in HAM10000 database. The proposed experimental model achieves an accuracy metric of 98.77%, 98.36%, and 98.89% for protocol-I, protocol-II, and protocol-III, respectively, for skin cancer classification. Results of our proposed models are also assimilated with several different models in the literature and were found to be superior than most of them. To enhance the performance metrics, the database is initially pre-processed using an Enhanced Super Resolution Generative Adversarial Network (ESRGAN) which gives a better image resolution for images of smaller size. © 2023 by the authors.","","benign; CAD; ESRGAN; malignant; skin cancer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146568344"
"Akhyar F.; Furqon E.N.; Lin C.-Y.","Akhyar, Fityanul (57209415830); Furqon, Elvin Nur (57321870300); Lin, Chih-Yang (56410127600)","57209415830; 57321870300; 56410127600","Enhancing Precision with an Ensemble Generative Adversarial Network for Steel Surface Defect Detectors (EnsGAN‐SDD)","2022","Sensors","22","11","4257","","","","10.3390/s22114257","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131158379&doi=10.3390%2fs22114257&partnerID=40&md5=09522077b998b083bc3cefc51e9698c1","Defects are the primary problem affecting steel product quality in the steel industry. The specific challenges in developing detect defectors involve the vagueness and tiny size of defects. To solve these problems, we propose incorporating super‐resolution technique, sequential feature pyramid network, and boundary localization. Initially, the ensemble of enhanced super‐resolution generative adversarial networks (ESRGAN) was proposed for the preprocessing stage to generate a more detailed contour of the original steel image. Next, in the detector section, the latest state‐of-the‐art feature pyramid network, known as De‐tectoRS, utilized the recursive feature pyramid network technique to extract deeper multi‐scale steel features by learning the feedback from the sequential feature pyramid network. Finally, Side‐Aware Boundary Localization was used to precisely generate the output prediction of the defect detectors. We named our approach EnsGAN‐ SDD. Extensive experimental studies showed that the proposed methods improved the defect detector’s performance, which also surpassed the accuracy of state‐of‐the‐art methods. Moreover, the proposed EnsGAN achieved better performance and effectiveness in processing time compared with the original ESRGAN. We believe our innovation could significantly contribute to improved production quality in the steel industry. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Image Processing, Computer-Assisted; Steel; Image enhancement; Steelmaking; Surface defects; steel; Boundary localization; Defect detection; Defect detectors; Feature pyramid; Generative adversarial network; Localisation; Performance; Pyramid network; Recursive FPN; Superresolution; image processing; procedures; Generative adversarial networks","boundary localization; defect detection; generative adversarial network (GAN); recursive FPN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131158379"
"Vedavyas Y.; Vasavi S.; Harsha S.S.; Subhash M.S.","Vedavyas, Y. (57643442700); Vasavi, S. (23986854500); Harsha, S. Sri (57643442800); Subhash, M. Sai (57643442900)","57643442700; 23986854500; 57643442800; 57643442900","An FPGA-Based Adaptive Real-Time Quality Enhancement System for Drone Imagery","2023","SN Computer Science","4","1","84","","","","10.1007/s42979-022-01509-y","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143600052&doi=10.1007%2fs42979-022-01509-y&partnerID=40&md5=6379d25722b5caec12c12b804b1adb84","Drones have varieties of applications in the world and extensively used for intelligence, monitoring, and reconnaissance. The low-resolution video has challenges, such as blurriness, over-exposure to sunlight, etc., which are needed to be diagnosed for effective operations. There is a requirement for onboard enhancement of the video stream, which can be achieved using the field-programmable gate array (FPGA). This paper presents the development of real-time and adaptive-quality enhancement system for drone imagery. The enhancement system includes Dynamic scene de-blurring for de-blurring the frame extracted from video, Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) for enhancing, contrast-limited Adaptive histogram equalization (CLAHE) for enhancing the local contrast of an image, sharpening for highlighting the edges of the objects present in the image, Gamma Correction for controlling the brightness of the image, and Saturation Adjustment. This system has an advantage that the computations can be done at the edge location and can be transmitted to the receiver in real time. Performance metrics, such as Mean squared error (MSE), Root Mean squared error (RMSE), Peak signal noise ratio (PSNR), Mean absolute error (MAE), Signal-to-noise ratio (SNR), Image Quality Index (IQI), Similarity Index (SI) and Pearson Correlation Coefficient (r), are used to measure the performance of the proposed system. The paper concluded that CLAHE is the best enhancement technique for quality enhancement during the foggy/smoky environment and ESRGAN is suitable for enhancing the smaller objects present in the drone video. © 2022, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.","","Drone Video; Dynamic Scene De-blurring; ESRGAN; FPGA; Quality Enhancement; Smoothing","Article","Final","","Scopus","2-s2.0-85143600052"
"Wang Y.; Lei W.; Zhang W.; Meng H.; Chen X.; Ye W.; Jing Q.","Wang, Yanwen (57933184500); Lei, Weimin (26026406600); Zhang, Wei (57206986787); Meng, Huan (57933367900); Chen, Xinyi (57206486506); Ye, Wenhui (57933212200); Jing, Qingyang (57933368000)","57933184500; 26026406600; 57206986787; 57933367900; 57206486506; 57933212200; 57933368000","Survey on video image reconstruction method based on generative model; [基于生成模型的视频图像重建方法综述]","2022","Tongxin Xuebao/Journal on Communications","43","9","","194","208","14","10.11959/j.issn.1000-436x.2022178","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140135807&doi=10.11959%2fj.issn.1000-436x.2022178&partnerID=40&md5=7e32939f7da4b59ce77caa9dccd2f300","Traditional video compression technology based on pixel correlation has limited performance improvement space, semantic compression has become the new direction of video compression coding, and video image reconstruction is the key link of semantic compression coding. First, the video image reconstruction methods for traditional coding optimization were introduced, including how to use deep learning to improve prediction accuracy and enhance reconstruction quality with super-resolution techniques. Second, the video image reconstruction methods based on variational auto-encoders, generative adversarial networks, autoregressive models and transformer models were discussed emphatically. Then, the models were classified according to different semantic representations of images. The advantages, disadvantages, and applicable scenarios of various methods were compared. Finally, the existing problems of video image reconstruction were summarized, and the further research directions were prospected. © 2022 Editorial Board of Journal on Communications. All rights reserved.","Deep learning; Image coding; Image compression; Image enhancement; Image reconstruction; Network coding; Semantics; Video signal processing; Auto encoders; Generative model; Image reconstruction methods; Images reconstruction; Semantic compressions; Transformer modeling; Variational auto-encoder; Video compression coding; Video compression technology; Video image; Generative adversarial networks","generative adversarial network; image reconstruction; Transformer model; variational auto-encoder; video compression coding","Article","Final","","Scopus","2-s2.0-85140135807"
"Li J.; Li J.; Wang C.; Zhao X.","Li, Jianhong (57211803980); Li, Jianhua (58102579500); Wang, Chengjun (57192602106); Zhao, Xin (57188568865)","57211803980; 58102579500; 57192602106; 57188568865","Wide & deep generative adversarial networks for recommendation system","2023","Intelligent Data Analysis","27","1","","121","136","15","10.3233/IDA-216400","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148004150&doi=10.3233%2fIDA-216400&partnerID=40&md5=a400ee7667306b7fd70aa285922201ec","Generative Adversarial Networks (GANs) has achieved great success in computer vision like Image Inpainting, Image Super-Resolution. Many researchers apply it to improve the effectiveness of recommendation system. However, GANs-based methods obtain users' preferences using a single Neural Network framework in generative model, which may not be fully mined. Furthermore, most GANs-based algorithms adopt cross-entropy loss to get pair-wise bias, but these methods don't reveal global data distribution loss when data are sparse. Those problems will influence the performance of the algorithm and result in poor accuracy. To address these problems, we introduce Wide & Deep Generative Adversarial Networks for Recommendation System (a.k.a W & DGAN) in this paper. On the one hand, we employ Wide & Deep Learning as a generative model capable of extracting both explicit and implicit information of user preferences. Furthermore, we combine Cross-Entropy loss in G with Wasserstein loss in D to get data distribution, then, the joint loss will be to receive the training information feedback from data distribution. Empirical results on three public benchmarks show that W&DGAN significantly outperforms state-of-the-art methods. © 2023 - IOS Press. All rights reserved.","Deep learning; Entropy; Generative adversarial networks; Cross entropy; Data distribution; Entropy loss; Generative model; Image Inpainting; Image super resolutions; Joint loss; Network-based; User's preferences; Wide & deep learning; Recommender systems","Generative Adversarial Networks; joint loss; recommendation system; Wide & deep learning","Article","Final","","Scopus","2-s2.0-85148004150"
"Pawar M.; Marab S.","Pawar, Meenakshi (55605652600); Marab, Sheetal (57224401323)","55605652600; 57224401323","A novel edge boosting approach for image super-resolution","2022","Evolutionary Intelligence","15","3","","2131","2138","7","10.1007/s12065-021-00625-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107477587&doi=10.1007%2fs12065-021-00625-7&partnerID=40&md5=4a053cf3f29a42f2ca4b0a567c502bbe","Image super-resolution is highly demanding area in both research as well as industry. In this paper, a novel deep network to acquire the high-resolution image from the input low-resolution image has been proposed. The proposed a novel residual-edge module is used to build the network for Single Image Super Resolution. The residual-edge module comprises of traditional residual learning followed by the proposed edge boosting mechanism. The proposed edge boosting mechanism propagates difference between the residual feature maps across the network with dense connections. The proposed edge boosting mechanism enhances the edge content in the feature maps learned by the residual blocks. This study uses conditional generative adversarial networks framework to optimize the weight parameters. Experiments have been carried out on benchmark database to validate the proposed network for image super-resolution. Empirical results show that the proposed network performs better other existing methods for image super-resolution. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Artificial intelligence; Evolutionary algorithms; Adversarial networks; Benchmark database; Boosting approach; Boosting mechanism; High resolution image; Image super resolutions; Low resolution images; Weight parameters; Optical resolving power","Deep network; Edge enhancement; Image super-resolution; Residual learning","Article","Final","","Scopus","2-s2.0-85107477587"
"Tokunaga T.; Mizutani K.","Tokunaga, Tomoki (57490541300); Mizutani, Kimihiro (55695932900)","57490541300; 55695932900","A Comprehensive Evaluation of Generating a Mobile Traffic Data Scheme without a Coarse-Grained Process Using CSR-GAN","2022","Sensors (Basel, Switzerland)","22","5","","","","","10.3390/s22051930","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126490706&doi=10.3390%2fs22051930&partnerID=40&md5=18634bd49a39466dae0fd1640cad842a","Large-scale mobile traffic data analysis is important for efficiently planning mobile base station deployment plans and public transportation plans. However, the storage costs of preserving mobile traffic data are becoming much higher as traffic increases enormously population density of target areas. To solve this problem, schemes to generate a large amount of mobile traffic data have been proposed. In the state-of-the-art of the schemes, generative adversarial networks (GANs) are used to transform a large amount of traffic data into a coarse-grained representation and generate the original traffic data from the coarse-grained data. However, the scheme still involves a storage cost, since the coarse-grained data must be preserved in order to generate the original traffic data. In this paper, we propose a scheme to generate the mobile traffic data by using conditional-super-resolution GAN (CSR-GAN) without requiring a coarse-grained process. Through experiments using two real traffic data, we assessed the accuracy and the amount of storage data needed. The results show that the proposed scheme, CSR-GAN, can reduce the storage cost by up to 45% compared to the traditional scheme, and can generate the original mobile traffic data with 94% accuracy. We also conducted experiments by changing the architecture of CSR-GAN, and the results show an optimal relationship between the amount of traffic data and the model size.","Proteins; protein","conditional GAN; SR-GAN; traffic data management","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126490706"
"Hu H.-T.; Hsu L.-Y.; Wu S.-T.","Hu, Hwai-Tsu (7404097894); Hsu, Ling-Yuan (26654165700); Wu, Shyi-Tsong (58046067500)","7404097894; 26654165700; 58046067500","Blind Watermarking for Hiding Color Images in Color Images with Super-Resolution Enhancement","2023","Sensors","23","1","370","","","","10.3390/s23010370","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145878436&doi=10.3390%2fs23010370&partnerID=40&md5=890a7c9a46e3060bc033a94a1a7cac81","This paper presents a novel approach for directly hiding the pixel values of a small color watermark in a carrier color image. Watermark embedding is achieved by modulating the gap of paired coefficient magnitudes in the discrete cosine transform domain according to the intended pixel value, and watermark extraction is the process of regaining and regulating the gap distance back to the intensity value. In a comparison study of robustness against commonly encountered attacks, the proposed scheme outperformed seven watermarking schemes in terms of zero-normalized cross-correlation (ZNCC). To render a better visual rendition of the recovered color watermark, a generative adversarial network (GAN) was introduced to perform image denoising and super-resolution reconstruction. Except for JPEG compression attacks, the proposed scheme generally resulted in ZNCCs higher than 0.65. The employed GAN contributed to a noticeable improvement in perceptual quality, which is also manifested as high-level ZNCCs of no less than 0.78. © 2022 by the authors.","Algorithms; Computer Security; Data Compression; Discrete cosine transforms; Generative adversarial networks; Image compression; Image denoising; Image enhancement; Image watermarking; Optical resolving power; Pixels; Blind color image watermarking; Blind watermarking; Color image watermarking; Colour image; Gap adjustments; Magnitude gap adjustment; Pixel values; Resolution enhancement; Superresolution; Watermark embedding; algorithm; computer security; information processing; procedures; Color","blind color image watermarking; discrete cosine transform; generative adversarial network; magnitude gap adjustment; super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145878436"
"Zhang W.; Hou Y.; Fan W.; Yang X.; Zhou D.; Zhang Q.; Wei X.","Zhang, Wei (57775666300); Hou, Yaqing (57192664172); Fan, Wanshu (57206253891); Yang, Xin (56203319300); Zhou, Dongsheng (13105639000); Zhang, Qiang (55624487967); Wei, Xiaopeng (7402117019)","57775666300; 57192664172; 57206253891; 56203319300; 13105639000; 55624487967; 7402117019","Perception-oriented Single Image Super-Resolution Network with Receptive Field Block","2022","Neural Computing and Applications","34","17","","14845","14858","13","10.1007/s00521-022-07341-y","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132632228&doi=10.1007%2fs00521-022-07341-y&partnerID=40&md5=237e489b6d89d35f3e2d41c830c8dda7","In recent years, deep learning has been widely applied to single image super-resolution(SISR). However, the majority of deep learning methods employ the Mean Square Error(MSE) loss as the objective optimization function, and the generated results are frequently too smooth and lack of details. In addition, the high-frequency information of the reconstructed image is severely lost, resulting in a generated image with poor visual effects. In order to address the aforementioned issues, this paper proposes a super-resolution network (RRFDB-GAN) with a receptive field module with a generative adversarial network as the main framework. The network adopts the receptive field block (RFB), which enables it to extract the features in multiple scales and to improve the discriminability. In this paper, the Residual in Residual Dense Block (RRDB) and the Residual of Receptive Field Dense Block (RRFDB) are combined into a new module, called Basic block. This module enhances the capability of feature reconstruction for low-resolution images. During the upsampling stage, a combination of Nearest Neighborhood Interpolation and Sub-pixel convolution is used to reduce the computational complexity and provide additional contextual information for super-resolution reconstruction, while achieving satisfactory performance. Finally, the four Basic blocks are integrated with the upsampling module into a simple end-to-end framework. Extensive experimental results demonstrate that the proposed method in this paper shows more details on the five test sets and outperforms other methods in terms of quantitative metrics and perception assessment. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; Mean square error; Signal sampling; Basic blocks; High-frequency informations; Image super resolutions; Learning methods; Means square errors; Receptive field module; Receptive fields; Single images; Upsampling; Upsampling stage; Optical resolving power","High-frequency information; Image super-resolution; Receptive field module; Upsampling stage","Article","Final","","Scopus","2-s2.0-85132632228"
"Zhang J.; Jia Y.; He X.; Han B.; Zhu H.; Du J.","Zhang, Jian (57551209900); Jia, Yuanyuan (55619451100); He, Xiangqian (57193630849); Han, Banru (57735786300); Zhu, Huazheng (56161121500); Du, Jinglong (57189247257)","57551209900; 55619451100; 57193630849; 57735786300; 56161121500; 57189247257","ESRGAN network for super-resolution reconstruction of anisotropic 3D-MRI images; [面向各向异性3D-MRI图像超分辨率重建的ESRGAN网络]","2022","Chongqing Daxue Xuebao/Journal of Chongqing University","45","5","","114","124","10","10.11835/j.issn.1000-582X.2022.05.011","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131768736&doi=10.11835%2fj.issn.1000-582X.2022.05.011&partnerID=40&md5=4eeb5eb02da38b43ad8eba0011243618","High-resolution(HR) magnetic resonance images (MRI) can improve the accuracy of disease diagnosis, but it is very difficult to obtain high-resolution MRI. Image super-resolution (SR) technology based on deep learning can effectively improve image resolution. In recent years, the generative adversarial networks (GANs) have provided new ideas for 3D-MRI SR reconstruction. Compared with the traditional SR algorithm based on deep convolutional neural network (DCNN), the GANs network targets the human visual mechanism and introduces a discriminant function to make the reconstructed 3D-MRI closer to the real image. We introduced the enhanced super-resolution generative adversarial network (ESRGAN) to perform SR reconstruction of 3D-MRI, and used the cross-layer self-similarity of 3D-MRI to reduce the dimensionality of the reconstruction task to 2D. On the basis of ensuring the reconstruction effect, the proposed method can reduce network training time and memory requirements. Compared with other traditional algorithms and DCNN-based techniques, experimental results show that our proposed method can further improve the visual quality of SR 3D-MRI. © 2022, Editorial Board of Journal of Chongqing University, Chongqing University Journals Department. All right reserved.","","Generative adversarial network; Magnetic resonance imaging; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85131768736"
"Kim C.; Bekar O.; Seo H.; Park S.-M.; Lee D.","Kim, Chaewoo (58036876400); Bekar, Oguzcan (58037123300); Seo, Hyunseok (56125023000); Park, Sang-Min (58036998700); Lee, Deukhee (35237776600)","58036876400; 58037123300; 56125023000; 58036998700; 35237776600","Computed tomography vertebral segmentation from multi-vendor scanner data","2022","Journal of Computational Design and Engineering","9","5","","1650","1664","14","10.1093/jcde/qwac072","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145173578&doi=10.1093%2fjcde%2fqwac072&partnerID=40&md5=31271b7dd5423aa87aa617e21d1b3236","Automatic medical image segmentation is a crucial procedure for computer-assisted surgery. Especially, three-dimensional reconstruction of medical images of the surgical targets can be accurate in fine anatomical structures with optimal image segmentation, thus leading to successful surgical results. However, the performance of the automatic segmentation algorithm highly depends on the consistent properties of medical images. To address this issue, we propose a model for standardizing computed tomography (CT) images. Hence, our CT image-to-image translation network enables diverse CT images (non-standard images) to be translated to images with identical features (standard images) for the more precise performance of U-Net segmentation. Specifically, we combine an image-to-image translation network with a generative adversarial network, consisting of a residual block-based generative network and the discriminative network. Also, we utilize the feature extracting layers of VGG-16 to extract the style of the standard image and the content of the non-standard image. Moreover, for precise diagnosis and surgery, the conservation of anatomical information of the non-standard image is also essential during the synthesis of medical images. Therefore, for performance evaluation, largely three evaluation methods are employed: (i) visualization of the geometrical matching between the non-standard (content) and synthesized images to verify the maintenance of the anatomical structures; (ii) measuring numerical results using image similarity evaluation metrics; and (iii) assessing the performance of U-Net segmentation with our synthesized images. Specifically, we investigate that our model network can transfer the texture from standard CT images to diverse CT images (non-standard) scanned by different scanners and scan protocols. Also, we verify that the synthesized images can retain the global pose and fine structures of the non-standard images. We also compare the predicted segmentation result of the non-standard image and the synthesized image generated from its non-standard image via our proposed network. In addition, the performance of our proposed model is compared with the windowing process, where the window parameter of the standard image is applied to the non-standard image to ensure that our model outperforms the windowing process.  © 2022 The Author(s). Published by Oxford University Press on behalf of the Society for Computational Design and Engineering.","Computerized tomography; Diagnosis; Generative adversarial networks; Medical imaging; Numerical methods; Structural optimization; Surgery; Textures; Computed tomography images; Image translation; Image-to-image translation; Performance; Standard images; Style transfer; Superresolution; Synthesized images; U-net; Vertebral segmentation; anatomy; automation; autonomy; computer aided design; image analysis; image resolution; performance assessment; precision; segmentation; tomography; Image segmentation","computed tomography image; generative adversarial networks; image-to-image translation; style transfer; super-resolution; U-Net; vertebral segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145173578"
"Li H.; Xuan Z.; Zhou J.; Hu X.; Yang B.","Li, Hangyu (57848545000); Xuan, Zuxing (15046351200); Zhou, Jianpin (57765399800); Hu, Xiyuan (7404710258); Yang, Bo (57191226169)","57848545000; 15046351200; 57765399800; 7404710258; 57191226169","Fast and accurate super-resolution of MR images based on lightweight generative adversarial network","2023","Multimedia Tools and Applications","82","2","","2465","2487","22","10.1007/s11042-022-13326-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132848554&doi=10.1007%2fs11042-022-13326-9&partnerID=40&md5=59dc41fc7c552b425f81e55deae00520","Single image super-resolution reconstruction (SISR) can effectively and economically improve the spatial resolution of magnetic resonance (MR) images, and it helps more accurate early clinical diagnosis and subsequent analysis. To increase the imaging speed and reduce the patient’s pain and motion artifacts, many studies have moved from only considering the quality of the reconstructed image to proposing some lightweight models. However, the model’s lightweight will limit its performance, and high-resolution MR images are often reconstructed with a single target (LOSS). In this work, we propose a lightweight generative adversarial network to alleviate this problem. The network mainly contains generators and discriminators. The generator uses a global cascade module to extract image features, and multi-scale up sampling of high-frequency and low-frequency features of different depths. As the cascaded modules lead to similar features, a consistent spatial attention module is used to weigh them and share the up-sampling module to reduce network parameters. The discriminator judges the authenticity of the input MR image, and it constructs two losses with the pre-trained VGG network to assist the generator training and provide diversified standards for the generation of MR images. In addition, we use knowledge transfer to train the network to explore the toplimit of network performance. Qualitative and quantitative experiments on the FASTMRI dataset show that the MR images generated by the designed multiple targets (loss) have better visual effects in detail. The proposed network has advantages in running time and parameter memory and achieved the highest precision results compared with state-of-the-art methods. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Diagnosis; Generative adversarial networks; Image enhancement; Image reconstruction; Knowledge management; Magnetic resonance imaging; Optical resolving power; Clinical diagnosis; Image-based; Magnetic resonance image; Multi-scale up sampling; Multi-scales; Scale-up; Single-image super-resolution reconstruction; Spatial resolution; Superresolution; Upsampling; Magnetic resonance","Generative adversarial networks; MR images; Multi-scale up sampling; Super-resolution","Article","Final","","Scopus","2-s2.0-85132848554"
"Xu S.; Qi M.; Wang X.; Zhao H.; Hu Z.; Sun H.","Xu, Shuhua (55185885200); Qi, Mingming (57830295000); Wang, Xianming (36022893900); Zhao, Hanli (14036715800); Hu, Zhongyi (56225755400); Sun, Hongyu (57198748073)","55185885200; 57830295000; 36022893900; 14036715800; 56225755400; 57198748073","A Positive-Unlabeled Generative Adversarial Network for Super-Resolution Image Reconstruction Using a Charbonnier Loss","2022","Traitement du Signal","39","3","","1061","1069","8","10.18280/ts.390333","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135401306&doi=10.18280%2fts.390333&partnerID=40&md5=bac736f10e294f5f30c30b33b21e59d2","Recently, the generative adversarial network (GAN) has been widely used to obtain the real high-frequency details of images. This spurs the application of GAN in super-resolution reconstruction. However, GAN is unstable in the training process, for the following two reasons: Firstly, the discriminator in GAN keeps the positive (true) and negative (false) criteria of the generated samples unchanged throughout the learning process, without considering the gradual quality improvement of the generated samples (Sometimes, the generated samples are even more realistic than the real samples). To solve the above problems, this paper proposes a super-resolution model based on positive-unlabeled (PU)-GAN-Charbon (SRPUGAN-Charbon). The proposed model includes one generator network that synthetizes super-resolution images and one discriminator network trained to distinguish super-resolution images from real high-resolution images. In addition, the Charbonnier loss function was called to handle the outliers in super-resolution images, and retain the low-frequency features of super-resolution images. Extensive experiments were conducted on three benchmark databases, including BSDS500, Set5, and Set14. The results show that the proposed SRPUGAN-Charbon method is superior to the most advanced methods in terms of visual effect, peak signal-to-noise ratio (PSNR), and structural similarity (SSIM). © 2022 Lavoisier. All rights reserved.","Image reconstruction; Learning systems; Optical resolving power; Signal to noise ratio; Charbonnier loss; High frequency HF; Learning process; Positive-unlabeled generative adversarial network; Resolution images; Robustness; Super-resolution image reconstruction; Super-resolution reconstruction; Superresolution; Training process; Generative adversarial networks","charbonnier loss; positive-unlabeled generative adversarial network (PUGAN); robustness; super-resolution reconstruction","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85135401306"
"Zhu L.; Zheng Y.","Zhu, Lianxiang (57221809094); Zheng, Yi (57738981400)","57221809094; 57738981400","Applications of Self-attention SRGAN in Super Resolution Reconstruction of Rock CT Image; [自注意力SRGAN在岩石CT图像超分辨中的应用研究]","2022","Xi'an Shiyou Daxue Xuebao (Ziran Kexue Ban)/Journal of Xi'an Shiyou University, Natural Sciences Edition","37","2","","131","137","6","10.3969/j.issn.1673-064X.2022.02.019","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127838878&doi=10.3969%2fj.issn.1673-064X.2022.02.019&partnerID=40&md5=094eebd8f05d3179dbf23c7f8ca79d9c","Rock microscopic images are of great significance for the study of ore-forming fluids.Aiming at the problems of low resolution and unclear details of rock microscopic images caused by image acquisition equipment and geological environment, a super-resolution reconstruction algorithm of rock microscopic images is proposed based on SRGAN and self-attention mechanism by adjusting generator network and discriminator network, and combining the porosity characteristics of rock micro-images into loss function.Under the magnification factor of four times, the rock micro image data set DRSRD1_2D is tested.The result shows that the proposed algorithm can significantly improve the peak signal to noise ratio (PSNR) of the reconstructed images.Compared with SRGAN algorithm, when running the same number of rounds, the edge details of the reconstructed images are clearer and the brightness information is more accurate, the proposed algorithm can better express the high-frequency features of the images. © 2022, the Editorial Department of Journal of Xi'an Shiyou University. All right reserved.","","Generative adversarial network; Porosity; Rock micro-image; Self-attention mechanism; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85127838878"
"Karwowska K.; Wierzbicki D.","Karwowska, Kinga (57608598400); Wierzbicki, Damian (56835658600)","57608598400; 56835658600","Improving Spatial Resolution of Satellite Imagery Using Generative Adversarial Networks and Window Functions","2022","Remote Sensing","14","24","6285","","","","10.3390/rs14246285","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144608432&doi=10.3390%2frs14246285&partnerID=40&md5=aca3bea7c96713d4cf2678e7208784dc","Dynamic technological progress has contributed to the development of systems imaging of the Earth’s surface as well as data mining methods. One such example is super-resolution (SR) techniques that allow for the improvement of the spatial resolution of satellite imagery on the basis of a low-resolution image (LR) and an algorithm using deep neural networks. The limitation of these solutions is the input size parameter, which defines the image size that is adopted by a given neural network. Unfortunately, the value of this parameter is often much smaller than the size of the images obtained by Earth Observation satellites. In this article, we presented a new methodology for improving the resolution of an entire satellite image, using a window function. In addition, we conducted research to improve the resolution of satellite images acquired with the World View 2 satellite using the ESRGAN network, we determined the number of buffer pixels that will make it possible to obtain the best image quality. The best reconstruction of the entire satellite imagery using generative neural networks was obtained using a Triangular window (for 10% coverage). The Hann-Poisson window worked best when more overlap between images was used. © 2022 by the authors.","Data mining; Deep neural networks; Generative adversarial networks; Image enhancement; Remote sensing; Satellite imagery; Images processing; Network functions; Neural network application; Neural-networks; Remote-sensing; Satellite images; Spatial resolution; Surface mining methods; Technological progress; Window functions; Image resolution","image processing; image resolution; neural network application; remote sensing; satellites","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144608432"
"You A.; Kim J.K.; Ryu I.H.; Yoo T.K.","You, Aram (57701432500); Kim, Jin Kuk (57216274046); Ryu, Ik Hee (57189390195); Yoo, Tae Keun (57226626960)","57701432500; 57216274046; 57189390195; 57226626960","Application of generative adversarial networks (GAN) for ophthalmology image domains: a survey","2022","Eye and Vision","9","1","6","","","","10.1186/s40662-022-00277-3","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126597413&doi=10.1186%2fs40662-022-00277-3&partnerID=40&md5=3854062598279123a9b33768af2ff6f8","Background: Recent advances in deep learning techniques have led to improved diagnostic abilities in ophthalmology. A generative adversarial network (GAN), which consists of two competing types of deep neural networks, including a generator and a discriminator, has demonstrated remarkable performance in image synthesis and image-to-image translation. The adoption of GAN for medical imaging is increasing for image generation and translation, but it is not familiar to researchers in the field of ophthalmology. In this work, we present a literature review on the application of GAN in ophthalmology image domains to discuss important contributions and to identify potential future research directions. Methods: We performed a survey on studies using GAN published before June 2021 only, and we introduced various applications of GAN in ophthalmology image domains. The search identified 48 peer-reviewed papers in the final review. The type of GAN used in the analysis, task, imaging domain, and the outcome were collected to verify the usefulness of the GAN. Results: In ophthalmology image domains, GAN can perform segmentation, data augmentation, denoising, domain transfer, super-resolution, post-intervention prediction, and feature extraction. GAN techniques have established an extension of datasets and modalities in ophthalmology. GAN has several limitations, such as mode collapse, spatial deformities, unintended changes, and the generation of high-frequency noises and artifacts of checkerboard patterns. Conclusions: The use of GAN has benefited the various tasks in ophthalmology image domains. Based on our observations, the adoption of GAN in ophthalmology is still in a very early stage of clinical validation compared with deep learning classification techniques because several problems need to be overcome for practical use. However, the proper selection of the GAN technique and statistical modeling of ocular imaging will greatly improve the performance of each image analysis. Finally, this survey would enable researchers to access the appropriate GAN technique to maximize the potential of ophthalmology datasets for deep learning research. © 2022, The Author(s).","","Data augmentation; Deep learning; Domain transfer; Generative adversarial network; Ophthalmology image","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85126597413"
"Das B.; Roy S.D.","Das, Bishshoy (57224399101); Roy, Sumantra Dutta (57819159700)","57224399101; 57819159700","Edge-Aware Image Super-Resolution Using a Generative Adversarial Network","2023","SN Computer Science","4","2","146","","","","10.1007/s42979-022-01561-8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145897865&doi=10.1007%2fs42979-022-01561-8&partnerID=40&md5=dfb0752b23d96a6fe8a81b6a47c335b1","Edge-awareness is an important factor in the perception of high frequency details. MSE-based single image super-resolution (SISR) algorithms, such as SRResNet do not deliver perceptually sharp images, but maximizes PSNR (Peak Signal-to-Noise Ratio). Edge details are often lost in such algorithms. A variant of SRResNet based on a generative adversarial network (GAN) model, named SRGAN, aims at achieving higher perceptual sharpness by trading in PSNR. This drop in PSNR is often massive and is attributed to the occurrence of unwanted artifacts. We introduce EaSRGAN, an edge-aware generative adversarial network, which reduces artifacts, delivers highly sharp and photorealistic images with PSNR values better than SRGAN. EaSRGAN treats high frequency regions separately from flat regions which brings in awareness of edges in the super-resolution output. Combined with a multi-stage training process separate for edge and flat areas, these loss functions make the generator and the discriminator, ‘edge-aware’. We compare our results with state-of-the-art SISR algorithms. EaSRGAN delivers superior perceptual clarity like that of SRGAN, while maintaining high PSNR by attenuating artifacts. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.","","Edge detection; Generative adversarial network; Image enhancement; Super-resolution","Article","Final","","Scopus","2-s2.0-85145897865"
"Furat O.; Finegan D.P.; Yang Z.; Kirstein T.; Smith K.; Schmidt V.","Furat, Orkun (57203537124); Finegan, Donal P. (56613490600); Yang, Zhenzhen (57189304794); Kirstein, Tom (57575203700); Smith, Kandler (55489781700); Schmidt, Volker (7202394043)","57203537124; 56613490600; 57189304794; 57575203700; 55489781700; 7202394043","Super-resolving microscopy images of Li-ion electrodes for fine-feature quantification using generative adversarial networks","2022","npj Computational Materials","8","1","68","","","","10.1038/s41524-022-00749-z","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128196848&doi=10.1038%2fs41524-022-00749-z&partnerID=40&md5=b7a296b7044c6077c12435c8ac375113","For a deeper understanding of the functional behavior of energy materials, it is necessary to investigate their microstructure, e.g., via imaging techniques like scanning electron microscopy (SEM). However, active materials are often heterogeneous, necessitating quantification of features over large volumes to achieve representativity which often requires reduced resolution for large fields of view. Cracks within Li-ion electrode particles are an example of fine features, representative quantification of which requires large volumes of tens of particles. To overcome the trade-off between the imaged volume of the material and the resolution achieved, we deploy generative adversarial networks (GAN), namely SRGANs, to super-resolve SEM images of cracked cathode materials. A quantitative analysis indicates that SRGANs outperform various other networks for crack detection within aged cathode particles. This makes GANs viable for performing super-resolution on microscopy images for mitigating the trade-off between resolution and field of view, thus enabling representative quantification of fine features. © 2022, The Author(s).","Crack detection; Economic and social effects; Generative adversarial networks; Scanning electron microscopy; Active material; Energy materials; Fine Feature; Functional behaviors; Ions electrodes; Large volumes; Microscopy images; Reduced resolution; Representativity; Trade off; Cathodes","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128196848"
"Singla K.; Pandey R.; Ghanekar U.","Singla, Khushboo (57812145700); Pandey, Rajoo (9246913500); Ghanekar, Umesh (23994982000)","57812145700; 9246913500; 23994982000","A review on Single Image Super Resolution techniques using generative adversarial network","2022","Optik","266","","169607","","","","10.1016/j.ijleo.2022.169607","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134602946&doi=10.1016%2fj.ijleo.2022.169607&partnerID=40&md5=1c92c76c1c00ad7b133eede07c1701a1","Single Image Super Resolution (SISR) is a process to obtain a high pixel density and refined details from a low resolution (LR) image to get upscaled and sharper high-resolution (HR) image. In last decade, SISR based on Convolutional Neural Networks (CNN) have achieved impressive results for generating super-resolved images up to the size of ×3. This technique focuses on minimizing L1/L2 loss between real HR image and generated HR image without considering the perceptual quality of images. To improve upon, SISR based on Generative Adversarial Network (GAN) has gained researchers attention for generating visually pleasing images with reasonably minimizing L1/L2 loss for ×4 size images. The basic idea of GAN is to train two networks simultaneously, a Generator and a Discriminator such that generator can produce the super-resolved image for a given input LR image by learning real HR image distribution. This paper presents an overview of GAN based SISR techniques for further research as there are a few surveys in this area. Different GAN models have been classified in terms of architecture, algorithms, and loss functions including their benefits and limitations. Lastly, the research gaps as well as some possible solutions for the existing methods have been discussed. © 2022 Elsevier GmbH","Convolutional neural networks; Image enhancement; Optical resolving power; Convolutional neural network; High-resolution images; Image distributions; Image super resolutions; Low resolution images; Network-based; Perceptual quality; Resolution techniques; Single image super resolution; Single images; Generative adversarial networks","Generative adversarial networks; Review; Single Image Super Resolution","Article","Final","","Scopus","2-s2.0-85134602946"
"You S.; Shen Y.; Wu G.; Wang S.","You, Senrong (57220890828); Shen, Yanyan (25121829200); Wu, Guocheng (23390775700); Wang, Shuqiang (53872228000)","57220890828; 25121829200; 23390775700; 53872228000","Brain MR Images Super-Resolution with the Consistent Features","2022","ACM International Conference Proceeding Series","","","","501","506","5","10.1145/3529836.3529939","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133462465&doi=10.1145%2f3529836.3529939&partnerID=40&md5=238751cd2ad9b9f612d1a0d24feff018","Magnetic resonance imaging plays an important role in auxiliary diagnosis and brain exploration. However, limited by hardware, scanning time and cost, it's challenging to acquire high-resolution (HR) magnetic resonance (MR) image clinically. In this paper, consistent feature generative adversarial network (CFGAN) is proposed to produce HR MR images from the low-resolution counterparts. Specifically, a consistent-features encoder is employed to extract the multi-scales features and encode them into latent codes. Then, a progressive generator is utilized to decode the latent codes from high-level to low-level features. With the encoder and generator, the shared consistent features between low-resolution and high-resolution can be fully extracted and recovered. Experiments on ADNI dataset demonstrate that CFGAN outperforms the competing methods quantitatively and qualitatively.  © 2022 Owner/Author.","Convolutional neural networks; Diagnosis; Magnetic resonance imaging; Magnetism; Medical imaging; Optical resolving power; Resonance; Signal encoding; Brain magnetic resonance images; Convolutional neural network; High resolution; Image super resolutions; Low-level features; Lower resolution; Magnetic resonance image; Multi-scale features; Scanning time; Superresolution; Generative adversarial networks","Convolutional Neural Network; Magnetic Resonance Images; Super Resolution","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85133462465"
"Li L.","Li, Longzhen (58018070600)","58018070600","Unsupervised Image Style Transfer Based on U-GAT-IT","2022","Proceedings of SPIE - The International Society for Optical Engineering","12456","","124562S","","","","10.1117/12.2660372","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144060666&doi=10.1117%2f12.2660372&partnerID=40&md5=ff93cadbf1e4ade50b5406f584f21955","Thanks to the rapid development of Generative Adversarial Networks (GAN) in recent years, great progress has been made in image translation using GAN. Image-to-image transformation is one of the important applications in the field of computer vision, and its scope includes image inpainting, image colorization, super-resolution and image style transfer. In recent years, there are many classic research on GAN-based image transformation, such as CycleGAN, UNIT, AGGAN, etc. This paper focuses on researching and testing the U-GAT-IT unsupervised image style transfer method. The authors introduced a new attention module and a new learnable normalization function (AdaLIN), which enables flexible control of the amount of change in shape and texture during image conversion. This paper uses a new dataset for testing and verification, trying to achieve bidirectional conversion between real face photos and sketches, and qualitatively and quantitatively analyzes the method by calculating PSNR and SSIM. © 2022 SPIE.","III-V semiconductors; Statistical tests; Textures; Bidirectional networks; Image colorizations; Image Inpainting; Image transformations; Image translation; Network-based; Style transfer; Superresolution; Transfer method; U-GAT-IT; Generative adversarial networks","bidirectional network; GAN; style transfer; U-GAT-IT","Conference paper","Final","","Scopus","2-s2.0-85144060666"
"Kong Y.; Ren X.; Zhou R.","Kong, Yihan (57763349200); Ren, Xuanang (57763349300); Zhou, Renjie (57762402000)","57763349200; 57763349300; 57762402000","Deep Learning for Image Super-Resolution and Its Application in Urban Security","2022","Proceedings of SPIE - The International Society for Optical Engineering","12259","","122595M","","","","10.1117/12.2639314","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132754601&doi=10.1117%2f12.2639314&partnerID=40&md5=45d44e7322f859ecf89d230852398b0e","Improving the image quality of surveillance cameras is highly demanded in urban security, which could greatly ease the workload of policies in identifying criminals. Deep learning, especially for generative adversarial networks (GAN), had been used in increasing image quality by translating low-resolution images into high-resolution images. The technology that converting low-resolution images into high-resolution images is called image super-resolution. GAN consists of a generator and a discriminator in which the discriminator is to discriminate real and fake high-resolution images while the generator is to convert low-resolution images into fake high-resolution images that can not be differentiated by the discriminator. The generator and discriminator work together in a competitive manner that tries to improve each other. This paper aims to explore GAN-based methods for image super-resolution, which is important for urban security. We first provide readers with theories about GAN. Moreover, we introduce four popular methods based on GAN for image super-resolution including super-resolution GAN, enhanced super-resolution GAN, residual channel attention GAN, and super-resolution GAN with ranker. We also list some challenges about applying image super-resolution to urban security and provide the corresponding solution. © 2022 SPIE","Deep learning; Discriminators; Image enhancement; Image quality; Optical resolving power; Security systems; Deep learning; High-resolution images; Identifying criminals; Image super resolutions; ITS applications; Low resolution images; Quality of surveillance; Superresolution; Surveillance cameras; Urban security; Generative adversarial networks","Deep learning; GAN; image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85132754601"
"Zhang L.; Dai H.; Sang Y.","Zhang, Lina (57222095467); Dai, Haidong (57765286200); Sang, Yu (36824201500)","57222095467; 57765286200; 36824201500","Med-SRNet: GAN-Based Medical Image Super-Resolution via High-Resolution Representation Learning","2022","Computational Intelligence and Neuroscience","2022","","1744969","","","","10.1155/2022/1744969","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132923316&doi=10.1155%2f2022%2f1744969&partnerID=40&md5=82eda3317fa5ab8713a0dee16de21413","High-resolution (HR) medical imaging data provide more anatomical details of human body, which facilitates early-stage disease diagnosis. But it is challenging to get clear HR medical images because of the limiting factors, such as imaging systems, imaging environments, and human factors. This work presents a novel medical image super-resolution (SR) method via high-resolution representation learning based on generative adversarial network (GAN), namely, Med-SRNet. We use GAN as backbone of SR considering the advantages of GAN that can significantly reconstruct the visual quality of the images, and the high-frequency details of the images are more realistic in the image SR task. Furthermore, we employ the HR network (HRNet) in GAN generator to maintain the HR representations and repeatedly use multi-scale fusions to strengthen HR representations for facilitating SR. Moreover, we adopt deconvolution operations to recover high-quality HR representations from all the parallel lower resolution (LR) streams with the aim to yield richer aggregated features, instead of simple bilinear interpolation operations used in HRNetV2. When evaluated on a home-made medical image dataset and two public COVID-19 CT datasets, the proposed Med-SRNet outperforms other leading edge methods, which obtains higher peak signal to noise ratio (PSNR) values and structural similarity (SSIM) values, i.e., maximum improvement of 1.75 and minimum increase of 0.433 on the PSNR metric for ""Brain""test sets under 8× and maximum improvement of 0.048 and minimum increase of 0.016 on the SSIM metric for ""Lung""test sets under 8× compared with other methods.  © 2022 Lina Zhang et al.","COVID-19; Humans; Image Processing, Computer-Assisted; Learning; Signal-To-Noise Ratio; Computerized tomography; Diagnosis; Image enhancement; Image resolution; Medical imaging; Signal to noise ratio; Statistical tests; Disease diagnosis; High resolution; Human bodies; Image super resolutions; Imaging data; Network-based; Peak signal to noise ratio; Structural similarity; Superresolution; Test sets; human; image processing; learning; procedures; signal noise ratio; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132923316"
"Zhang J.; Wang X.; Liu J.; Zhang D.; Lu Y.; Zhou Y.; Sun L.; Hou S.; Fan X.; Shen S.; Zhao J.","Zhang, Jun (57476905100); Wang, Xinxin (58078049400); Liu, Jingyan (57211953226); Zhang, Dongfang (57482555600); Lu, Yin (58078804200); Zhou, Yuhong (57927467400); Sun, Lei (57193565866); Hou, Shenglin (58078804300); Fan, Xiaofei (57212384880); Shen, Shuxing (57672098800); Zhao, Jianjun (57821019400)","57476905100; 58078049400; 57211953226; 57482555600; 58078804200; 57927467400; 57193565866; 58078804300; 57212384880; 57672098800; 57821019400","Multispectral Drone Imagery and SRGAN for Rapid Phenotypic Mapping of Individual Chinese Cabbage Plants","2022","Plant Phenomics","2022","","0007","","","","10.34133/plantphenomics.0007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146912072&doi=10.34133%2fplantphenomics.0007&partnerID=40&md5=55237c8471277c0f6451f89809067f39","The phenotypic parameters of crop plants can be evaluated accurately and quickly using an unmanned aerial vehicle (UAV) equipped with imaging equipment. In this study, hundreds of images of Chinese cabbage (Brassica rapa L. ssp. pekinensis) germplasm resources were collected with a low-cost UAV system and used to estimate cabbage width, length, and relative chlorophyll content (soil plant analysis development [SPAD] value). The super-resolution generative adversarial network (SRGAN) was used to improve the resolution of the original image, and the semantic segmentation network Unity Networking (UNet) was used to process images for the segmentation of each individual Chinese cabbage. Finally, the actual length and width were calculated on the basis of the pixel value of the individual cabbage and the ground sampling distance. The SPAD value of Chinese cabbage was also analyzed on the basis of an RGB image of a single cabbage after background removal. After comparison of various models, the model in which visible images were enhanced with SRGAN showed the best performance. With the validation set and the UNet model, the segmentation accuracy was 94.43%. For Chinese cabbage dimensions, the model was better at estimating length than width. The R2 of the visible-band model with images enhanced using SRGAN was greater than 0.84. For SPAD prediction, the R2 of the model with images enhanced with SRGAN was greater than 0.78. The root mean square errors of the 3 semantic segmentation network models were all less than 2.18. The results showed that the width, length, and SPAD value of Chinese cabbage predicted using UAV imaging were comparable to those obtained from manual measurements in the field. Overall, this research demonstrates not only that UAVs are useful for acquiring quantitative phenotypic data on Chinese cabbage but also that a regression model can provide reliable SPAD predictions. This approach offers a reliable and convenient phenotyping tool for the investigation of Chinese cabbage breeding traits.  Copyright © 2022 Jun Zhang et al.","Agricultural robots; Antennas; Crops; Drones; Image enhancement; Mean square error; Regression analysis; Semantic Segmentation; Aerial vehicle; Brassica rapa L; Chinese cabbage; Crop plants; Germplasms; Imaging equipment; Multi-spectral; Plant analysis; Semantic segmentation; Superresolution; Semantics","","Article","Final","","Scopus","2-s2.0-85146912072"
"Baral A.; Koirala A.; Pantha S.; Pokhrel R.; Paudel B.H.","Baral, Anita (57565246700); Koirala, Anupama (57566032000); Pantha, Sanjay (57566420700); Pokhrel, Rewant (57565246800); Paudel, Bishnu Hari (57566420800)","57565246700; 57566032000; 57566420700; 57565246800; 57566420800","Automatic License Plate Recognition for Distorted Images Using SRGAN","2022","Lecture Notes in Electrical Engineering","853","","","121","131","10","10.1007/978-981-16-9885-9_10","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127772334&doi=10.1007%2f978-981-16-9885-9_10&partnerID=40&md5=fb5a4d7da5a5c66f82851053af699f33","The quality of an image is always a limitation for automatic license plate recognition (ALPR) for its practical and accurate functioning. Distorted low-resolution (LR) images are one of the common issues encountered in ALPR systems. Most of the available algorithms in ALPR systems are effective under controlled circumstances or with intricate image capture systems. Authorities are looking for an improved ALPR system that is primarily concerned with dealing with distorted LR images. In this research, an improved ALPR system is proposed to generate high-resolution (HR) images from LR images using the super-resolution generative adversarial network (SRGAN). On a self-created Nepali dataset, both the networks (ALPR and SRGAN) are trained and assessed. Traditionally, for the identification of vehicles, images of license plates are localized, characters are segmented, and recognized. The accuracy of license plate recognition is heavily reliant on segmentation and recognition accuracy. ALPR with SRGAN comprises a super-resolution process in between. Super-resolution aids to improve the performance of the system by recreating detailed variation of the localized image, thus providing better end-to-end recognition. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Image enhancement; Image segmentation; License plates (automobile); Optical character recognition; Optical resolving power; Automatic license plate recognition; Detection; Distorted images; Image captures; License plate recognition systems; Localisation; Localised; Low resolution images; Segmentation; Superresolution; Generative adversarial networks","Automatic license plate recognition; Detection; Generative adversarial network; Localization; Segmentation; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85127772334"
"Abedjooy A.; Ebrahimi M.","Abedjooy, Aref (57776124900); Ebrahimi, Mehran (14833990500)","57776124900; 14833990500","MULTI-MODALITY IMAGE SUPER-RESOLUTION USING GENERATIVE ADVERSARIAL NETWORKS","2022","16th International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing, CGVCVIP 2022, 8th International Conference on Connected Smart Cities, CSC 2022, 7th International Conference on Big Data Analytics, Data Mining and Computational Intelligence, BigDaCI 2022, and 11th International Conference on Theory and Practice in Modern Computing, TPMC 2022 - Held at the 16th Multi Conference on Computer Science and Information Systems, MCCSIS 2022","","","","101","110","9","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142350183&partnerID=40&md5=5fcbf018129412bbff808d3961e5667c","Over the past few years deep learning-based techniques such as Generative Adversarial Networks (GANs) have significantly improved solutions to image super-resolution and image-to-image translation problems. In this paper, we propose a solution to the joint problem of image super-resolution and multi-modality image-to-image translation. The problem can be stated as the recovery of a high-resolution image in a modality, given a low-resolution observation of the same image in an alternative modality. Our paper offers two models to address this problem and will be evaluated on the recovery of high-resolution day images given low-resolution night images of the same scene. Promising qualitative and quantitative results will be presented for each model. © MCCSIS 2022.All rights reserved.","Deep learning; Image enhancement; Optical resolving power; Deep learning; High resolution; High-resolution images; Image super resolutions; Image translation; Image-to-image translation; Lower resolution; Multi modality image; Quantitative result; Super-multi; Generative adversarial networks","Deep Learning; Generative Adversarial Networks; Image Super-Resolution; Image-to-Image Translation","Conference paper","Final","","Scopus","2-s2.0-85142350183"
"Das S.; Guhathakurta R.; Roy D.","Das, S. (55476994500); Guhathakurta, R. (57200145413); Roy, D. (8608757400)","55476994500; 57200145413; 8608757400","Generative Adversarial Network (GAN) based MRI data synthesis with validation of Convolutional Networks, KNN","2022","ECS Transactions","107","1","","14697","14706","9","10.1149/10701.14697ecst","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130537630&doi=10.1149%2f10701.14697ecst&partnerID=40&md5=fe7060944f86681b6aa49350520b00b3","Magnetic Resonance Imaging (MRI) is a technology mainly used for disease prediction and treatment. Practically, due to poor quality of MRI images, sometimes it is advised to repeat the scan test again which causes some unavoidable situations with increase of costs. Therefore, only the improvement of the quality of MRI images can give us the relief from these unnecessary problems. So, we need an automated supervised machine learning algorithm to generate high resolution data without more efforts. In this paper, some computational techniques like convolutional networks, K-Nearest Neighbour classifier and Generative Adversarial Network (GAN) are applied on the MRI images to get the high-resolution based MRI images. The methodology follows medical image localization, detection, segmentation and classification. The validation results on real data of MRI data fundamentally determines its usefulness and demonstrates the effectiveness in compared to state-of-the-art super-resolution techniques. © The Electrochemical Society","Convolution; Generative adversarial networks; Image enhancement; Image segmentation; Learning algorithms; Medical imaging; Nearest neighbor search; Supervised learning; Computational technique; Convolutional networks; Data synthesis; High resolution data; K-nearest neighbors classifiers; Machine learning algorithms; Network-based; Resonance imaging data; Scan tests; Supervised machine learning; Magnetic resonance imaging","","Conference paper","Final","","Scopus","2-s2.0-85130537630"
"Zhang Z.; Li Y.; AlSinan M.; He X.; Kwak H.; Hoteit H.","Zhang, Zhen (57218379894); Li, Yiteng (57194063531); AlSinan, Marwah (57206481444); He, Xupeng (57214912219); Kwak, Hyung (57759108800); Hoteit, Hussein (12764363500)","57218379894; 57194063531; 57206481444; 57214912219; 57759108800; 12764363500","Multiscale Carbonate Rock Reconstruction Using a Hybrid WGAN-GP and Super-Resolution","2022","Proceedings - SPE Annual Technical Conference and Exhibition","2022-October","","","","","","10.2118/210461-MS","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139702369&doi=10.2118%2f210461-MS&partnerID=40&md5=d5088532198ffb3175ff6be81f76c2e5","The X-ray micro-Computed Tomography (μ-CT) is the primary tool for digital rock imaging, which provides the foundation for numerically studying petrophysical properties of reservoir rocks at the pore scale. However, the finite resolution of μ-CT imaging cannot capture the micro-porosity at the sub-micrometer scale in carbonate rocks. The tradeoff between the resolution and field of view (FOV) is a persisting challenge in the industry. The machine-learning-based single-image super-resolution techniques has rapidly developed in the past few years. It is becoming a promising approach to ""super-resolve"" low-resolution carbonate rock images. In this study, we present a fast super-resolution generative adversarial network to enhance the image resolution of carbonate rocks. A pre-trained VGG network is implemented to extract important high-level features, from which the perceptual similarity is evaluated between the generated and ground-truth images. The novelty of this study is two-fold. First, the generator is significantly simplified with a fast super-resolution convolutional neural network. On the other hand, the spatial and channel squeeze-and excitation block is applied to recalibrate nonlinear feature mapping so that the quality of super-resolved images is promising even with much fewer residual blocks. To quantify the quality of the super-resolution images, we compare difference maps between the generated and ground-truth images. Numerical results indicate that the proposed network shows excellent potential in enhancing the resolution of heterogeneous carbonate rocks. In particular, the pixel errors are minor, and the super-resolution images exhibit clear and sharp edges and dissolved mineral texture. This study provides a novel machine-learning-based method using a simple generative adversarial network with squeeze and excitation blocks to super-resolve μ-CT images of carbonate rocks. Copyright © 2022, Society of Petroleum Engineers.","Carbonates; Carbonation; Computerized tomography; Convolutional neural networks; Generative adversarial networks; Image enhancement; Petrophysics; Sedimentary rocks; Textures; Carbonate rock; Digital rock imaging; Ground truth; Machine-learning; Resolution images; Spatial and channel squeeze-and-excitation block; Super-resolution generative adversarial network; Superresolution; VGG19 network; X ray micro-computed tomography; Image resolution","Carbonate rocks; Digital rock imaging; Spatial and channel squeeze-and-excitation block; Super-resolution generative adversarial network; VGG19 network","Conference paper","Final","","Scopus","2-s2.0-85139702369"
"Nisticò S.; Palopoli L.; Romano A.P.","Nisticò, Simona (57208836255); Palopoli, Luigi (7004141918); Romano, Adele Pia (57939902000)","57208836255; 7004141918; 57939902000","Audio Super-Resolution via Vision Transformer","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13515 LNAI","","","378","387","9","10.1007/978-3-031-16564-1_36","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140478729&doi=10.1007%2f978-3-031-16564-1_36&partnerID=40&md5=92a62ad84e84c7ccde0bab403d9eef05","Audio super-resolution refers to techniques that improve the quality of audio signals, usually by exploiting bandwidth extension methods, whereby audio enhancement is obtained by expanding the phase and the spectrogram of the input audio traces. These techniques are therefore much significant for all those cases where audio traces miss relevant parts of the audible spectrum. In many cases, the given input signal contains the low-band frequencies (the easiest to capture with low-quality recording instruments) whereas the high-band must be generated. In this paper, we illustrate a system for bandwidth extension that works on musical tracks and generates the high-band frequencies starting from the low-band ones. The system, called ViT Super-resolution (ViT-SR), features an architecture based on a Generative Adversarial Network and Vision Transformer model. Some experiments, which are accounted for in the paper, serve the purpose to prove the effectiveness of the presented approach. In particular, our purpose was to demonstrate that it is possible to faithfully reconstruct the high-band signal of an audio file having only its low-band spectrum available as the input, therewith including the usually difficult to synthetically generate harmonics associated with the input track which significantly contribute to the final perceived sound quality. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Audio acoustics; Bandwidth; Music; Optical resolving power; Sound reproduction; Audio enhancement; Audio signal; Audio super-resolution; Band frequencies; Bandwidth extension; Extension methods; Music enhancement; Superresolution; Transformer; Vision transformer; Generative adversarial networks","Audio super-resolution; Generative adversarial networks; Music enhancement; Transformers; Vision transformer","Conference paper","Final","","Scopus","2-s2.0-85140478729"
"Hu X.; Zhang Y.; Yang D.","Hu, Xinyi (57221143013); Zhang, Yuxin (57202468961); Yang, Dongxiao (7404800774)","57221143013; 57202468961; 7404800774","A Unified Framework for Super-Resolution Based on Segmentation-Prior and Self-Attention","2022","2022 IEEE 2nd International Conference on Data Science and Computer Application, ICDSCA 2022","","","","80","84","4","10.1109/ICDSCA56264.2022.9988090","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146371231&doi=10.1109%2fICDSCA56264.2022.9988090&partnerID=40&md5=a0c49150a3a884431b5b21d73f334148","Convolutional Neural Network (CNN) is intensively applied to super-resolution (SR) task because of its superior performance. However, the problem of SR task is still challenging due to the lack of prior knowledge and small receptive field of CNN. We propose a unified framework for single image SR based on segmentation-prior and self-attention, named Segmentation-Prior Self-Attention Generative Adversarial Network (SPSAGAN). This combination is led by a carefully designed weighted addition to balance the influence of feature and segmentation attentions. Thus, the SPSAGAN can emphasize textures in the same segmentation category and meanwhile focus on the long-distance feature relationship. Extensive experiments show that SPSAGAN can generate more realistic and visually pleasing textures compared to state-of-the-art SFTGAN [1] and ESRGAN [2] on OST and BSD100 datasets © 2022 IEEE.","Convolutional neural networks; Optical resolving power; Semantic Segmentation; Semantics; Textures; Convolutional neural network; GAN; Performance; Prior-knowledge; Receptive fields; Segmentation prior; Self-attention; Semantic segmentation; Superresolution; Unified framework; Generative adversarial networks","GAN; Self-Attention; Semantic Segmentation; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85146371231"
"Long K.; Jiang B.; Yang Y.","Long, Kun (57944958100); Jiang, Biao (57943736500); Yang, Yubin (24723322700)","57944958100; 57943736500; 24723322700","Unsupervised Unpaired Super-Resolution Using an Active Sampling Strategy Based on Edge Detection","2022","Proceedings of the International Joint Conference on Neural Networks","2022-July","","","","","","10.1109/IJCNN55064.2022.9892530","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140736826&doi=10.1109%2fIJCNN55064.2022.9892530&partnerID=40&md5=7161a8ea02625bd85a107dd81eacdd04","Most existing super-resolution (SR) methods rely on pairs of low resolution (LR) and high resolution (HR) images and predetermined degradation operations (e.g., bicubic), usually trained by supervised learning. However, they often fail in real-world scenarios because of the occurrence of noise and blur. The key reason is that the degradation process is unknown, and no HR-LR pairs can be obtained directly. To address the above issues, this paper explores the optimization of an unsupervised unpaired SR method inspired by generative models such as Generative Adversarial Networks (GAN) and Cycle-Consistent Adversarial Networks (CycleGAN). We propose an active sampling strategy based on edge detection, and introduce a denoising network to construct a novel unsupervised unpaired SR framework. The active sampling strategy can perform image-level feature alignment by sampling image patches actively, thus optimizing the learning direction of the generator. At the same time, the denoising network can reduce the learning difficulty of the generator by preprocessing real-world LR images. Extensive experiments indicate that our method obtains better performance over other existing solutions to the unsupervised unpaired SR challenge, © 2022 IEEE.","Computer vision; Edge detection; Optical resolving power; Active sampling; Adver-sarial learning; De-noising; High-resolution images; Low-high; Lower resolution; Real-world scenario; Sampling strategies; Superresolution; Superresolution methods; Generative adversarial networks","active sampling; adver-sarial learning; denoising; super-resolution; unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85140736826"
"Gashnikov M.","Gashnikov, Mikhail (12644970700)","12644970700","General Structure of an Machine Learning Method for Compression of Images","2022","2022 8th International Conference on Information Technology and Nanotechnology, ITNT 2022","","","","","","","10.1109/ITNT55410.2022.9848719","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137757569&doi=10.1109%2fITNT55410.2022.9848719&partnerID=40&md5=429fc13ac5a38275ffe36cc5f5ad0c3d","The article proposes an architecture of based on machine learning (ML) method for compressing digital images and two-dimensional signals. The method uses a quadtree-based representation of the two-dimensional signal. This representation allows you to use the characteristics of the different quadtree floors to tuning the ML-methods. The representation also allows you to predict two-dimensional signal samples based on less detailed quadtree floors and encode prediction deviations. We use ML-methods at all stages of the compression method. In particular, we use ML-based super-resolution and embedding algorithms for digital two-dimensional signals to predict signal samples: convolutional and generative adversarial neural networks, as well as autoencoders. We perform computational experiments to investigate the efficiency of ML-based prediction at the most detailed floor of the quadtree-based representation of the two-dimensional signal during compression. We experimentally prove that the studied ML-based predictor has a significant advantage in natural digital two-dimensional signals.  © 2022 IEEE.","Deep learning; Floors; Generative adversarial networks; Network architecture; Optical resolving power; Compression of images; General structures; Machine learning methods; Machine-learning; Method architecture; Neural-networks; On-machines; Quad trees; Superresolution; Two-dimensional signals; Forecasting","method architecture; neural networks; prediction; super-resolution; two-dimensional signal","Conference paper","Final","","Scopus","2-s2.0-85137757569"
"Chen Y.-Z.; Liu T.-J.; Liu K.-H.","Chen, Yu-Zhang (57549949400); Liu, Tsung-Jung (36625954800); Liu, Kuan-Hsien (36626017900)","57549949400; 36625954800; 36626017900","SUPER-RESOLUTION OF SATELLITE IMAGES BY TWO-DIMENSIONAL RRDB AND EDGE-ENHANCEMENT GENERATIVE ADVERSARIAL NETWORK","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","1825","1829","4","10.1109/ICASSP43922.2022.9747063","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131229370&doi=10.1109%2fICASSP43922.2022.9747063&partnerID=40&md5=a64f1f69ef4c38d18b85a84a5367272a","With the increasing demand for high-resolution images, image super-resolution (SR) technology has become one of the focuses in related research fields. Generally speaking, high resolution is usually achieved by increasing the density and accuracy of the sensor. However, such an approach is quite expensive for equipment and design. In particular, increasing the density of satellite sensors must be undertaken great risks. Inspired by EEGAN and based on it, the Ultra-Dense Subnet (UDSN) and Edge Enhanced Network (EEN) were modified. Among them, the UDSN is used for feature extraction and obtains high-resolution results that look clear in the intermediate but are deteriorated by artifacts, and the Edge-Enhanced Subnet (EESN) is used to purify, extract and enhance the image contour and use mask processing to eliminate images contaminated by noise. Finally, the restored intermediate image and the enhanced edge are combined to produce a high-resolution image with high credibility and clear content. We use Kaggle and AID open experimental datasets to test and compare the results among different methods. It proves the performance of the proposed model is better than other SR methods. © 2022 IEEE","Image enhancement; Optical resolving power; Satellites; Edge enhancements; Generative adversarial network; High resolution; High-resolution images; Image super resolutions; Residual in residual dense block; Satellite images; Subnets; Superresolution; Two-dimensional; Generative adversarial networks","generative adversarial network (GAN); residual in residual dense block (RRDB); satellite images; Super-resolution(SR)","Conference paper","Final","","Scopus","2-s2.0-85131229370"
"Dharejo F.A.; Zawish M.; Deeba F.; Zhou Y.; Dev K.; Khowaja S.A.; Qureshi N.M.F.","Dharejo, Fayaz Ali (57195487028); Zawish, Muhammad (57206720765); Deeba, Farah (57215997317); Zhou, Yuanchun (55737417400); Dev, Kapal (57190336390); Khowaja, Sunder Ali (56500465400); Qureshi, Nawab Muhammad Faseeh (57191380223)","57195487028; 57206720765; 57215997317; 55737417400; 57190336390; 56500465400; 57191380223","Multimodal-Boost: Multimodal Medical Image Super-Resolution Using Multi-Attention Network With Wavelet Transform","2022","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","","","1","14","13","10.1109/TCBB.2022.3191387","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135246199&doi=10.1109%2fTCBB.2022.3191387&partnerID=40&md5=84f02143d53e698abf71f6c2ac3d7c2a","Multimodal medical images are widely used by clinicians and physicians to analyze and retrieve complementary information from high-resolution images in a non-invasive manner. Loss of corresponding image resolution adversely affects the overall performance of medical image interpretation. Deep learning-based single image super resolution (SISR) algorithms have revolutionized the overall diagnosis framework by continually improving the architectural components and training strategies associated with convolutional neural networks (CNN) on low-resolution images. However, existing work lacks in two ways: i) the SR output produced exhibits poor texture details, and often produce blurred edges, ii) most of the models have been developed for a single modality, hence, require modification to adapt to a new one. This work addresses (i) by proposing generative adversarial network (GAN) with deep multi-attention modules to learn high-frequency information from low-frequency data. Existing approaches based on the GAN have yielded good SR results; however, the texture details of their SR output have been experimentally confirmed to be deficient for medical images particularly. The integration of wavelet transform (WT) and GANs in our proposed SR model addresses the aforementioned limitation concerning textons. While the WT divides the LR image into multiple frequency bands, the transferred GAN uses multi-attention and upsample blocks to predict high-frequency components. Additionally, we present a learning method for training domain-specific classifiers as perceptual loss functions. Using a combination of multi-attention GAN loss and a perceptual loss function results in an efficient and reliable performance. Applying the same model for medical images from diverse modalities is challenging, our work addresses (ii) by training and performing on several modalities via transfer learning. Using two medical datasets, we validate our proposed SR network against existing state-of-the-art approaches and achieve promising results in terms of structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR). IEEE","Deep learning; Diagnosis; Edge detection; Generative adversarial networks; Image analysis; Image compression; Image enhancement; Image reconstruction; Image resolution; Neural networks; Signal to noise ratio; Textures; Wavelet transforms; Attention module; Features extraction; Image edge detection; Images reconstruction; Medical diagnostic imaging; Multi-modality; Multimodality data; Superresolution; Task analysis; Transfer learning; Wavelets transform; Feature extraction","Attention modules; Feature extraction; generative adversarial network; Generative adversarial networks; Image edge detection; Image reconstruction; Medical diagnostic imaging; multimodality data; super-resolution; Task analysis; Training; transfer learning; wavelet transform","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85135246199"
"Zhou X.; Gao L.","Zhou, Xiaohui (58017405600); Gao, Li (57705407900)","58017405600; 57705407900","A super-resolution image reconstruction method based on generative adversarial network","2022","Proceedings of SPIE - The International Society for Optical Engineering","12456","","124562M","","","","10.1117/12.2659326","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144090401&doi=10.1117%2f12.2659326&partnerID=40&md5=e66684c3535dfefc11d7eddfac60ba79","Compared with traditional methods, the quality of reconstructed images has been greatly improved by super-resolution image reconstruction methods based on deep residual networks. However, in order to make full use of shallow image features and solve the problem of instability during network training. This paper proposes a method for image super-resolution reconstruction that can better reconstruct texture details based on generative adversarial network (GAN). Dense blocks containing residual scaling (RRDBs) is used to construct the generative network to extract more image features. WGAN is introduced to construct the discriminative network to solve the problem of unstable training of generative adversarial network. The experimental results show that the proposed model in this paper has better visual effects and improved SSIM and PSNR values compared with other models. © 2022 SPIE.","Image enhancement; Image reconstruction; Optical resolving power; Textures; Image features; Image reconstruction methods; Image super-resolution reconstruction; Images processing; Network training; Quality of reconstructed images; Scalings; Super-resolution image reconstruction; Superresolution; WGAN; Generative adversarial networks","generative adversarial network; Image processing; super-resolution; WGAN","Conference paper","Final","","Scopus","2-s2.0-85144090401"
"Xu L.; Yan Y.; Huang X.","Xu, Long (55660493400); Yan, Yihua (57581281800); Huang, Xin (43361346200)","55660493400; 57581281800; 43361346200","Deep Learning in Solar Image Generation Tasks","2022","SpringerBriefs in Computer Science","","","","59","81","22","10.1007/978-981-19-2746-1_5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131130859&doi=10.1007%2f978-981-19-2746-1_5&partnerID=40&md5=3cc8c46b5ed671973cfc94a13b03efaf","It has been witnessed that deep learning has been applied to classification in previous chapters. In fact, deep learning also demonstrated great ability of image generation which is more challenging than classification. In this chapter, several applications of deep learning in solar image enhancement, reconstruction and processing are presented, including image deconvolution of solar radioheliograph, desaturation of solar imaging, generating magnetogram, image super-resolution. These tasks are all concerned with image generation, by employing generative neural networks. As a representative of generative networks, GAN was widely exploited in image generation tasks. It can generate high fidelity and photo-realistic content mainly owning to an adversarial loss. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Deep learning; Generative adversarial networks; Aperture synthesis; Desaturation; Image deconvolution; Image generations; Image super resolutions; Images processing; Images reconstruction; Magnetograms; Solar images; Solar imaging; Image enhancement","Aperture synthesis; Image deconvolution; Image generation; Magnetogram","Book chapter","Final","","Scopus","2-s2.0-85131130859"
"Li H.-A.; Wang D.; Li Z.; Ma T.","Li, Hong-An (55673317300); Wang, Diao (57988210500); Li, Zhanli (12782257100); Ma, Tian (55490554800)","55673317300; 57988210500; 12782257100; 55490554800","Image Super-Resolution Reconstruction Based on Big Data and Cloud Computing","2022","Proceedings - 2022 IEEE 7th International Conference on Smart Cloud, SmartCloud 2022","","","","177","183","6","10.1109/SmartCloud55982.2022.00035","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143085742&doi=10.1109%2fSmartCloud55982.2022.00035&partnerID=40&md5=d8da9570be94bcf81b1d9629e0b99df3","Image super-resolution reconstruction can reconstruct low-resolution images into high-resolution images, which is an important application of big data combined with cloud computing. Using big data technology can mine the useful information of a large number of images, and cloud computing can reduce the model computation. However, existing super-resolution models are difficult to train and have problems such as artifacts, blurred detail texture and too smooth after image reconstruction. To solve the above problems, we propose the Multi-scale double Attention mechanism based on Residual Dense Generative Adversarial Network (MARDGAN), which uses multi-branch paths to extract image features of different scale sizes, to obtain multi-scale features information. We also design the double attention mechanism block (CSAB) and combine it with the Enhanced Residual Dense Block (ERDB) to form the deep residual dense attention module (DRDAM) to extract multi-level depth feature information. The perceptual capability of the model is improved by adding pixel loss, perceptual loss, and adversarial loss. The experimental results show that our proposed MARDGAN has shorter training time. And it can use the original image information more effectively than other methods on multiple benchmark datasets to recover super-resolution images with clearer details and better realism.  © 2022 IEEE.","Big data; Cloud computing; Image reconstruction; Information use; Optical resolving power; Textures; Attention mechanisms; Cloud-computing; Data technologies; Feature information; Features extraction; High-resolution images; Image super-resolution reconstruction; Low resolution images; Mechanism-based; Super-resolution reconstruction; Generative adversarial networks","Big data; Cloud computing; Feature extraction; Generative adversarial network; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85143085742"
"Mittal H.; Rai V.; Sonawane S.; Mhatre S.","Mittal, Harsh (57782051300); Rai, Vaibhav (57781001700); Sonawane, Swaraj (57781526200); Mhatre, Sneha (57647899400)","57782051300; 57781001700; 57781526200; 57647899400","Image Resolution Enhancer using Deep Learning","2022","Proceedings - International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2022","","","","578","586","8","10.1109/ICAAIC53929.2022.9792975","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133479820&doi=10.1109%2fICAAIC53929.2022.9792975&partnerID=40&md5=c9869f86f002aba5119d002c0dceaca0","Image Super-Resolution is a technique that is used to obtain high-resolution, realistic images from low-resolution input images. Deep learning algorithms such as SRCNN, ESRGAN, RDN, etc. have shown significant results in this field. But these algorithms at times vary in results. To solve this problem, this research study has proposed an image super-resolution by using Patch Extraction on Deep Learning Algorithm, in which the LR image is first divided into patches and then the algorithms like RDN and ESRGAN are applied. Comparing each patch from each algorithm based on PSNR values, the patch with the highest PSNR value will be selected. After picking up all the patches for that image, it will be reconstructed and hence the super-resolution image will be obtained as the output. © 2022 IEEE.","Convolutional neural networks; Deep learning; Extraction; Generative adversarial networks; Image enhancement; Image resolution; Learning algorithms; Learning systems; Convolutional neural network; Deep learning; Dense network; High resolution; Image super resolutions; Input image; Lower resolution; Patch extraction; Realistic images; Residual dense network; Computer vision","Computer Vision; Convolutional Neural Networks; Deep Learning; Generative Adversarial Networks; Image Super-Resolution; Patch Extraction; Residual Dense Networks","Conference paper","Final","","Scopus","2-s2.0-85133479820"
"Wang H.; Sun J.; Diao W.; Li J.; Zhang K.","Wang, Haitao (57665373900); Sun, Jiande (12645161300); Diao, Wenxiu (57406352000); Li, Jing (55441752700); Zhang, Kai (56451954400)","57665373900; 12645161300; 57406352000; 55441752700; 56451954400","TAGAN: Texture and Attention Guided Generative Adversarial Network for Image Super Resolution","2022","Proceedings - IEEE International Symposium on Circuits and Systems","2022-May","","","3269","3273","4","10.1109/ISCAS48785.2022.9937622","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142487754&doi=10.1109%2fISCAS48785.2022.9937622&partnerID=40&md5=eb3bbc1616dd2ddc688f38c82dfc22f3","Super Resolution (SR) methods based on Generative Adversarial Networks (GANs) accomplish predominant execution in visual perception and image quality. These methods are mainly generated by traditional Peak-Signal-to-Noise-Ratio (PSNR)oriented or perceptual-driven. As the reconstruction process usually loses high frequency information, various methods aim to preserve more details. To make the details of the generated image richer, the Gradient Weight (GW) loss is introduced in the proposed method, because the gradient can reflect the texture of the image to a certain extent. The GW loss function is helpful to improve the edge and detailed texture of the generated image. Furthermore, we introduce attention mechanism to the image reconstruction block via Squeeze and Excitation Net (SENet). Attention mechanism can effectively aggregate the global features obtained by the nonlinear mapping network, and improve the channel sensitivity of the model. With the help of GW and attention mechanism, the proposed method can achieve better performance and visual quality in image texture detail restoration. The performance comparison between the state-of-the-art methods and our proposed method verifies the feasibility and reliability of the proposed method. © 2022 IEEE.","Image enhancement; Image quality; Image reconstruction; Image texture; Optical resolving power; Signal to noise ratio; Textures; Attention mechanisms; Gradient weight; Image super resolutions; Peak signal to noise ratio; Single image super resolution; Single images; Superresolution methods; Visual image; Visual perception; Weight loss; Generative adversarial networks","Attention Mechanism; Generative Adversarial Networks; Gradient Weight; Single Image Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85142487754"
"Wang S.; Sun Z.; Li Q.","Wang, Shuang (57929677400); Sun, Zhengxing (7404239784); Li, Qian (57199178362)","57929677400; 7404239784; 57199178362","Image super-resolution based on self-similarity generative adversarial networks","2022","IET Image Processing","","","","","","","10.1049/ipr2.12624","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139975988&doi=10.1049%2fipr2.12624&partnerID=40&md5=e1fe3b92b4f0d1f68ef3bcd41706471a","Self-attention has been successfully leveraged for long-range feature-wise similarities in deep learning super-resolution (SR) methods. However, most of the SR methods only explore the features on the original scale, but do not take full advantage of self-similarities features on different scales especially in generative adversarial networks (GAN). In this paper, self-similarity generative adversarial networks (SSGAN) are proposed as the SR framework. The framework establishes the multi-scale feature correlation by adding two modules to the generative network: downscale attention block (DAB) and upscale attention block (UAB). Specifically, DAB is designed to restore the repetitive details from the corresponding downsampled image, which achieves multi-scale feature restoration through self-similarity. And UAB improves the baseline up-sampling operations and captures low-resolution to high-resolution feature mapping, which enhances the cross-scale repetitive features to reconstruct the high-resolution image. Experimental results demonstrate that the proposed SSGAN achieve better visual performance especially in the similar pattern details. © 2022 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Deep learning; Image enhancement; Image reconstruction; Optical resolving power; Restoration; Feature correlation; Feature mapping; High resolution; Image super resolutions; Lower resolution; Multi-scale features; Self-similarities; Superresolution; Superresolution methods; Upsampling; Generative adversarial networks","","Article","Article in press","","Scopus","2-s2.0-85139975988"
"Yu M.; Wang H.; Liu C.; Lin D.","Yu, Mengbei (57223211542); Wang, Hongjuan (57210918053); Liu, Chang (57240736300); Lin, Deping (55608464000)","57223211542; 57210918053; 57240736300; 55608464000","Super-resolution reconstruction of remote sensing images based on SRGAN","2022","Proceedings of SPIE - The International Society for Optical Engineering","12473","","124730U","","","","10.1117/12.2653847","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144046247&doi=10.1117%2f12.2653847&partnerID=40&md5=cd5db18d4b97c3d4861777f4b69804a5","In the field of remote sensing images, due to the limitations of hardware equipment, image transmission, natural environment and other reasons, the resolution of the obtained remote sensing images cannot reach the desired resolution. The emergence of image super-resolution reconstruction technology can improve the resolution of remote sensing images without increasing the high cost. Image super-resolution reconstruction refers to the fact that low-resolution images can obtain high-resolution images through certain algorithmic techniques. With the rapid development of deep learning ideas, researchers have applied it to the field of image super-resolution reconstruction and achieved good results. Image super-resolution reconstruction also shifts from traditional reconstruction methods to deep learning-based methods. The emergence of the idea of Generative Adversarial Networks has further advanced the field of image super-resolution reconstruction. By using the idea of Generative Adversarial Network (GAN), researchers can obtain more realistic high-resolution images. This paper mainly uses the SRGAN model, the image dataset DIV2K for super-resolution reconstruction, and uses a dense residual structure in the generator network to obtain more image information, so that the effect of image reconstruction is more realistic. Through the experimental verification on the SIRI-WHU remote sensing test data set, the two evaluation indicators of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) are compared, and the effect is improved. Better generation results can also be observed through subjective human vision. © 2022 SPIE.","Deep learning; Generative adversarial networks; Image enhancement; Optical resolving power; Remote sensing; Signal to noise ratio; Statistical tests; Algorithmic techniques; High costs; High-resolution images; Image super-resolution reconstruction; Image-based; Low resolution images; Natural environments; Reconstruction method; Remote sensing images; Super-resolution reconstruction; Image reconstruction","Generative Adversarial Networks; Image Super-Resolution Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85144046247"
"Deng X.; Hua W.; Liu X.; Chen S.; Zhang W.; Duan J.","Deng, Xiaotong (58017228500); Hua, Weihua (58017889400); Liu, Xiuguo (15060266300); Chen, Siying (58017452200); Zhang, Wen (58017889500); Duan, Jianchao (57854897000)","58017228500; 58017889400; 15060266300; 58017452200; 58017889500; 57854897000","D-SRCAGAN : DEM Super-resolution Generative Adversarial Network","2022","IEEE Geoscience and Remote Sensing Letters","","","","1","1","0","10.1109/LGRS.2022.3224296","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144092025&doi=10.1109%2fLGRS.2022.3224296&partnerID=40&md5=b6c3396c778b3e76858b205913a79c8b","High-resolution digital elevation models (DEMs) are widely used in many fields such as mapping, hydrology, meteorology and geology, where they can improve the accuracy and reliability of many geographic analysis applications as an input. However, due to the high cost and difficulty of acquiring high-resolution DEMs, as well as the problems of edge smoothing, data distortion and fractures in reconstructed ground surfaces with traditional super-resolution DEM reconstruction techniques. Inspired by the excellence of generative adversarial neural networks in super-resolution image analysis, this paper investigates an approach for DEM super-resolution reconstruction with deep residual generative adversarial network. An advanced DEM Super-resolution Generative Adversarial Network (D-SRCAGAN) is proposed in this paper, which can reconstruct a quadruple higher resolution DEM by using low-resolution DEM. Compared with the bicubic and SRGAN methods, the D-SRCAGAN method reconstruction results can retain more topographic features and obtain higher RMSE values. IEEE","Digital instruments; Geomorphology; Image reconstruction; Optical resolving power; Reliability analysis; Surveying; Attention mechanisms; Digital elevation model; Edge smoothing; Geographics; High costs; High resolution; Improved super-resolution generative adversarial network (D-SRCAGAN); Super-resolution reconstruction; Superresolution; Generative adversarial networks","Attention mechanism; Digital elevation model (DEM); Improved super-resolution generative adversarial network (D-SRCAGAN); Super-resolution reconstruction","Article","Article in press","","Scopus","2-s2.0-85144092025"
"Jiang P.; Xu W.; Li J.","Jiang, Pengfei (57215331516); Xu, Wangqing (57742605900); Li, Jinping (55976809700)","57215331516; 57742605900; 55976809700","Super-Resolution of Defocus Thread Image Based on Cycle Generative Adversarial Networks","2022","IFIP Advances in Information and Communication Technology","643 IFIP","","","457","472","15","10.1007/978-3-031-03948-5_37","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132030313&doi=10.1007%2f978-3-031-03948-5_37&partnerID=40&md5=7f63eccaad6bfe7023b08e40cfb493e8","The dual camera calibration measurement method can realize low-cost and high-precision bolt dimension measurement by using two microscope cameras. But the height difference between the thread crest and root exceeds the depth of field, and the thread image becomes defocus, which seriously affects the measurement accuracy. For this reason, a super-resolution method for defocus thread image based on cyclic generative adversarial networks is proposed. We collected focus thread images and defocus thread images as training data. Two encoders are used in the generation network to extract image defocus features and content features. And a sub-pixel convolution layer is added to the decoder to achieve image super-resolution. A loss function based on adversarial loss and cycle-consistent loss is constructed to realize unsupervised training of the network, thereby achieve super-resolution of defocus thread images. The experimental results show that, in the simulated defocus images, the method has superiority in image detail preservation, sharpness improvement and peak signal to noise ratio. In the bolt dimension measurement task, it can effectively reconstruct the clear thread image and thus provide the measurement accuracy to 0.01 mm. © 2022, IFIP International Federation for Information Processing.","Bolts; Cameras; Generative adversarial networks; Image enhancement; Signal to noise ratio; Camera calibration; Cyclic generative adversarial network; Defocus; Defocus image; Dimension measurements; Dual cameras; Image-based; Measurement accuracy; Superresolution; Thread images; Optical resolving power","Bolt; Cyclic generative adversarial networks; Defocus image; Dimension measurement; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85132030313"
"Xu K.; Qian Z.; Zhong Y.; Su J.; Gao H.; Li W.","Xu, Kuiwen (55630730500); Qian, Zemin (58064735300); Zhong, Yu (7401808855); Su, Jiangtao (57195545296); Gao, Haijun (57188590133); Li, Wenjun (58064383000)","55630730500; 58064735300; 7401808855; 57195545296; 57188590133; 58064383000","Learning-Assisted Inversion for Solving Nonlinear Inverse Scattering Problem","2022","IEEE Transactions on Microwave Theory and Techniques","","","","1","12","11","10.1109/TMTT.2022.3228945","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146256246&doi=10.1109%2fTMTT.2022.3228945&partnerID=40&md5=04f0fb7559e588e72f493bb3f0751a3d","Solving inverse scattering problems (ISPs) is challenging because of its intrinsic ill-posedness and the nonlinearity. When dealing with highly nonlinear ISPs, i.e., those scatterers with high contrast and/or electrically large size, the traditional iterative nonlinear inversion methods converge slowly and take lots of computation time, even maybe trapped into local wrong solution. To alleviate the above challenges, a learning-assisted (LA) inversion approach termed as the LA inversion method (LAIM) with advanced generative adversarial network (GAN) in virtue of a new recently established contraction integral equation for inversion (CIE-I) is proposed to achieve a good balance between the computational efficiency and the accuracy of solving highly nonlinear ISPs. The preliminary profiles composed of only small amount of low-frequency components can be got efficiently by the Fourier bases expansion of CIE-I inversion (FBE-CIE-I). The physically exacted information can be taken as the input of the neural network to recover super-resolution image with more high-frequency components. A weighted loss function composed of the adversarial loss, mean absolute percentage error (MAPE), and structural similarity (SSIM) is used under the pix2pix GAN framework. In addition, the self-attention module is used at the end of the generator network to capture the physical distance information between two pixels and enhance the inversion accuracy of the feature scatterers. To further improve the inversion efficiency, the data-driven method (DDM) is used to achieve real-time imaging by cascading U-net and pix2pix GAN, where U-net is used to replace FBE-CIE-I in the LAIM. Compared with other LA inversion, both the synthetic and experimental examples have validated the merits of the proposed LAIM and DDM. IEEE","Computational efficiency; Generative adversarial networks; Image reconstruction; Integral equations; Interactive computer systems; Inverse problems; Nonlinear equations; Real time systems; Highly nonlinear; Images reconstruction; Inverse-scattering; Inversion methods; Pix2pix generative adversarial network; Real - Time system; Realtime imaging; Self-attention; Structural similarity; Iterative methods","Generative adversarial networks; Highly nonlinear; Image reconstruction; Imaging; inverse scattering; Iterative methods; Mathematical models; pix2pix generative adversarial network (GAN); real-time imaging; Real-time systems; Scattering; self-attention; structural similarity (SSIM)","Article","Article in press","","Scopus","2-s2.0-85146256246"
"Segawa R.; Hayashi H.","Segawa, Ryo (57217308319); Hayashi, Hitoshi (35408327500)","57217308319; 35408327500","Improved SinGAN's Performance by Changing the Activation Function","2022","IEEJ Transactions on Electrical and Electronic Engineering","17","2","","308","310","2","10.1002/tee.23514","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117219765&doi=10.1002%2ftee.23514&partnerID=40&md5=3fcae7d76dc7ac07d8d132a5b7854a2b","Generative adversarial nets (GANs) perform well on a variety of tasks, but rely on large datasets and expensive computer-based learning. SinGAN was proposed as a GANs that overcomes this problem, but its super-resolution performance for large-scale natural images was not very good. In this study, we aimed to improve the performance of SinGAN by changing the activation function. As a result of the verification, it was found that applying RSwish to the generator and Swish to the discriminator can generate an image with less deterioration as a whole than the default. © 2021 Institute of Electrical Engineers of Japan. Published by Wiley Periodicals LLC. © 2021 Institute of Electrical Engineers of Japan. Published by Wiley Periodicals LLC.","Chemical activation; Deep learning; Deterioration; Large dataset; Optical resolving power; Activation functions; Computer-based learning; Deep learning; Large datasets; Large-scales; Natural images; Performance; Superresolution; Generative adversarial networks","","Letter","Final","","Scopus","2-s2.0-85117219765"
"Shi R.; Zhang J.; Li Y.; Ge S.","Shi, Ruixin (57223361816); Zhang, Junzheng (57725327200); Li, Yong (56259407100); Ge, Shiming (56226219300)","57223361816; 57725327200; 56259407100; 56226219300","REGULARIZED LATENT SPACE EXPLORATION FOR DISCRIMINATIVE FACE SUPER-RESOLUTION","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","2534","2538","4","10.1109/ICASSP43922.2022.9746928","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131234250&doi=10.1109%2fICASSP43922.2022.9746928&partnerID=40&md5=030c63425460a6f5af8cd1d85c21e6d1","Learning face super-resolution models is challenged in many practical scenarios where high-resolution and low-resolution face pairs usually are difficult to collect for training examples. Recent self-supervised approach provides a feasible solution by using low-resolution faces to guide the generation of the corresponding high-resolution ones with a pretrained generator. In this paper, we propose a regularized latent space exploration approach to facilitate self-supervised face super-resolution. In the approach, a pretrained generative adversarial network (GAN) is fully used to control the exploration of high-resolution face generation in an iterative optimization manner for a low-resolution face. During the iteration, super-resolution faces are continually generated from a feasible latent space by the generator and evaluated by the discriminator, while the generator is online finetuned. The generation is evaluated by measuring the semantic loss as well as pixel loss between ground-truth low-resolution faces and the corresponding downsampled super-resolution faces. In this way, the generated faces can be appearance natural and semantic discriminative. Experiments validate the effectiveness of our approach in terms of quantitative metrics and visual quality. © 2022 IEEE","Computer vision; Generative adversarial networks; Optical resolving power; Semantics; Discriminative features; Face super-resolution; High resolution; High-low; Lower resolution; Self-supervised learning; Space explorations; Super-resolution models; Superresolution; Training example; Space research","discriminative features; Face super-resolution; self-supervised learning","Conference paper","Final","","Scopus","2-s2.0-85131234250"
"Kumar L.; Jain M.","Kumar, Loveleen (57219798657); Jain, Manish (56754270200)","57219798657; 56754270200","A Novel Image Super-Resolution Reconstruction Framework Using the AI Technique of Dual Generator Generative Adversarial Network (GAN)","2022","Journal of Universal Computer Science","28","9","","967","983","16","10.3897/jucs.94134","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139967910&doi=10.3897%2fjucs.94134&partnerID=40&md5=d096e28449783b4c261f9383fd43a705","Image superresolution (SR) is the process of enlarging and enhancing a low-resolution image. Image superresolution helps in industrial image enhancement, classification, detection, pattern recognition, surveillance, satellite imaging, medical diagnosis, image analytics, etc. It is of utmost importance to keep the features of the low-resolution image intact while enlarging and enhancing it. In this research paper, a framework is proposed that works in three phases and generates superresolution images while keeping low-resolution image features intact and reducing image blurring and artifacts. In the first phase, image enlargement is done, which enlarges the low-resolution image to the 2x/4x scale using two standard algorithms. The second phase enhances the image using an AI-empowered Generative adversarial network (GAN). We have used a GAN with dual generators and named it EffN-GAN (EfficientNet-GAN). Fusion is done in the last phase, wherein the final improved image is generated by fusing the enlarged image and GAN output image. The fusion phase helps in reducing the artifacts. We have used the DIV2K dataset to train the GAN and further tested the results on the images of Set5, Set14, B100, Urban100, Manga109 datasets with ground truth of size 224x224x3. The obtained results were compared with the state-of-the-art superresolution approach based on important image quality parameters, namely, Peak signal-to--to-noise ratio (PSNR), Structural similarity index (SSIM), Visual information fidelity (VIF) image quality parameters. The results show that the proposed framework for generating super-resolution images from 2x/4x resolution downgraded images improves the aforementioned mentioned image quality parameters significantly. © 2022, IICM. All rights reserved.","","AI-empowered Imaging system; dual Generator; Generative Adversarial Network (GAN); image analytics; image superresolution; Industrial-image enhancement","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139967910"
"Zhang W.; Yang D.; Cheung C.Y.; Chen H.","Zhang, Weiwen (57910998200); Yang, Dawei (57200657615); Cheung, Carol Y. (35276903300); Chen, Hao (56493367600)","57910998200; 57200657615; 35276903300; 56493367600","Frequency-Aware Inverse-Consistent Deep Learning for OCT-Angiogram Super-Resolution","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13432 LNCS","","","645","655","10","10.1007/978-3-031-16434-7_62","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139058982&doi=10.1007%2f978-3-031-16434-7_62&partnerID=40&md5=b9d0f7a9761a288cc3b42d8529425c3f","Optical Coherence Tomography Angiography (OCTA) is a novel imaging modality that captures the retinal and choroidal microvasculature in a non-invasive way. So far, 3 mm × 3 mm and 6 mm × 6 mm scanning protocols have been the two most widely-used field-of-views. Nevertheless, since both are acquired with the same number of A-scans, resolution of 6 mm × 6 mm image is inadequately sampled, compared with 3 mm × 3 mm. Moreover, conventional supervised super-resolution methods for OCTA images are trained with pixel-wise registered data, while clinical data is mostly unpaired. This paper proposes an inverse-consistent generative adversarial network (GAN) for archiving 6 mm × 6 mm OCTA images with super-resolution. Our method is designed to be trained with unpaired 3 mm × 3 mm and 6 mm × 6 mm OCTA image datasets. To further enhance the super-resolution performance, we introduce frequency transformations to refine high-frequency information while retaining low-frequency information. Compared with other state-of-the-art methods, our approach outperforms them on various performance metrics. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep learning; Inverse problems; Medical computing; Medical imaging; Optical resolving power; Optical tomography; Angiography images; Field of views; Frequency-aware; Image super resolutions; Imaging modality; Inverse-consistency; Micro-vasculature; Non-invasive way; Optical coherence tomography angiography; Superresolution; Generative adversarial networks","Frequency-aware; GAN; Image Super-Resolution; Inverse-consistency; OCTA","Conference paper","Final","","Scopus","2-s2.0-85139058982"
"Yang Q.; Chen Y.; Zhang J.; Li Z.","Yang, Qiongqian (57385224000); Chen, Ye (57384989600); Zhang, Jianfeng (57385681200); Li, Zhenting (57226719357)","57385224000; 57384989600; 57385681200; 57226719357","A Comprehensive Survey and Outlook for Cross-Resolution Person Re-Identification","2022","Proceedings - 2022 International Conference on Frontiers of Artificial Intelligence and Machine Learning, FAIML 2022","","","","204","208","4","10.1109/FAIML57028.2022.00047","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144624396&doi=10.1109%2fFAIML57028.2022.00047&partnerID=40&md5=b4fd59eb34e71a22c4da03bec700b231","Person re-identification (Re-ID) is a fundamental task in computer vision which has achieved significant progress in recent years. However, the existing promising algorithms are typically based on the assumption that all the images have the same and sufficiently high resolution (HR), ignoring the fact that the images are often captured with different resolutions. This study intends to present a comprehensive overview of cross-resolution (CR) person Re-ID to promote a deeper understanding of this topic and further research. We first group the current techniques into three categories: dictionary-learning-based, super-resolution-based, and generative-adversarial-network-based methods. The motivation, principles, benefits, and drawbacks of these techniques are extensively discussed. Then, the ways to construct synthetic multi-low-resolution (MLR) datasets and the performance comparisons of the state-of-the-art algorithms on five MLR datasets are demonstrated. Finally, challenges and potential research directions are further discussed. © 2022 IEEE.","Computer vision; Optical resolving power; 'current; Cross-resolution; Dictionary learning; Different resolutions; Generative adversarial gan; High resolution; Lower resolution; Person re identifications; Superresolution; Three categories; Generative adversarial networks","cross-resolution; dictionary learning; generative adversarial gan; person re-identification; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85144624396"
"Mou C.; Wu Y.; Wang X.; Dong C.; Zhang J.; Shan Y.","Mou, Chong (57222314036); Wu, Yanze (57238003600); Wang, Xintao (57195942631); Dong, Chao (56335662200); Zhang, Jian (56459724700); Shan, Ying (57219761074)","57222314036; 57238003600; 57195942631; 56335662200; 56459724700; 57219761074","Metric Learning Based Interactive Modulation for Real-World Super-Resolution","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13677 LNCS","","","723","740","17","10.1007/978-3-031-19790-1_43","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142754429&doi=10.1007%2f978-3-031-19790-1_43&partnerID=40&md5=af58d85f0ed15c23ae739beb1c23d64b","Interactive image restoration aims to restore images by adjusting several controlling coefficients, which determine the restoration strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and restoration performance in real-world super-resolution. Codes are available at https://github.com/TencentARC/MM-RealSR. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Generative adversarial networks; Image reconstruction; Learning systems; Optical resolving power; Restoration; Set theory; Topology; Interactive images; Interactive modulation; Metric learning; Metric spaces; Performance; Real-world; Real-world scenario; Real-world super-resolution; Restore image; Superresolution; Modulation","Generative adversarial network; Interactive modulation; Metric learning; Real-world super-resolution","Conference paper","Final","","Scopus","2-s2.0-85142754429"
"Tu J.; Mei G.; Ma Z.; Piccialli F.","Tu, Jingzhi (57217008206); Mei, Gang (57806891200); Ma, Zhengjing (57219977775); Piccialli, Francesco (42762051900)","57217008206; 57806891200; 57219977775; 42762051900","SWCGAN: Generative Adversarial Network Combining Swin Transformer and CNN for Remote Sensing Image Super-Resolution","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","5662","5673","11","10.1109/JSTARS.2022.3190322","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134297666&doi=10.1109%2fJSTARS.2022.3190322&partnerID=40&md5=7c6271eb172cba28091080f8603bf88a","Easy and efficient acquisition of high-resolution remote sensing images is of importance in geographic information systems. Previously, deep neural networks composed of convolutional layers have achieved impressive progress in super-resolution reconstruction. However, the inherent problems of the convolutional layer, including the difficulty of modeling the long-range dependency, limit the performance of these networks on super-resolution reconstruction. To address the abovementioned problems, we propose a generative adversarial network (GAN) by combining the advantages of the swin transformer and convolutional layers, called SWCGAN. It is different from the previous super-resolution models, which are composed of pure convolutional blocks. The essential idea behind the proposed method is to generate high-resolution images by a generator network with a hybrid of convolutional and swin transformer layers and then to use a pure swin transformer discriminator network for adversarial training. In the proposed method, first, we employ a convolutional layer for shallow feature extraction that can be adapted to flexible input sizes; second, we further propose the residual dense swin transformer block to extract deep features for upsampling to generate high-resolution images; and third, we use a simplified swin transformer as the discriminator for adversarial training. To evaluate the performance of the proposed method, we compare the proposed method with other state-of-the-art methods by utilizing the UCMerced benchmark dataset, and we apply the proposed method to real-world remote sensing images. The results demonstrate that the reconstruction performance of the proposed method outperforms other state-of-the-art methods in most metrics.  © 2008-2012 IEEE.","Benchmarking; Convolution; Deep neural networks; Image reconstruction; Optical resolving power; Remote sensing; Convolutional layer; Generative adversarial network; High-resolution images; High-resolution remote sensing images; Image super resolutions; Performance; Remote sensing images; State-of-the-art methods; Super-resolution reconstruction; Swin transformer; input-output analysis; numerical model; remote sensing; sampling; satellite data; satellite imagery; spatial resolution; Generative adversarial networks","Convolutional layers; generative adversarial network (GAN); remote sensing images; super-resolution reconstruction; swin transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134297666"
"Xiao G.; Zhang L.","Xiao, Guangyi (57328144900); Zhang, Long (57222227721)","57328144900; 57222227721","SAR IMAGE SUPER-RESOLUTION RECONSTRUCTION BASED ON FULL-RESOLUTION DISCRIMINATION","2022","Proceedings - International Conference on Image Processing, ICIP","","","","691","695","4","10.1109/ICIP46576.2022.9897999","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146724735&doi=10.1109%2fICIP46576.2022.9897999&partnerID=40&md5=f43c002c5eef6ad53246ed8460a4b1e1","In image super-resolution reconstruction based on generative adversarial networks (GANs), the discrimination of high-resolution (HR) images enriches texture details. However, solely discriminating HR images limits the reconstruction quality, while discriminating other resolution features can improve the texture structures of the reconstructed HR images. Therefore, this paper proposes a SAR image super-resolution reconstruction algorithm based on full-resolution discrimination (FRD). In the suggested architecture, the full-resolution discriminator network is used to extract the high, medium, and low-resolution features, which are then fused into a full-resolution feature. Finally, the full-resolution feature difference between the authentic and fake images is input to the generator, which reduces the inaccuracy of single-resolution feature discrimination. Experimental results on synthetic aperture radar (SAR) images demonstrate that the proposed FRD algorithm performs better than the state-of-the-art super-resolution algorithms in reconstructing the texture structures of the HR SAR images. © 2022 IEEE.","Fake detection; Image enhancement; Image reconstruction; Image texture; Optical resolving power; Radar imaging; Synthetic aperture radar; Textures; Features fusions; Full resolutions; Full-resolution discrimination; High-resolution images; Image super-resolution reconstruction; Reconstruction algorithms; Reconstruction quality; Superresolution; Synthetic aperture radar images; Texture structure; Generative adversarial networks","feature fusion; full-resolution discrimination; generative adversarial network; super-resolution; Synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85146724735"
"Cheng C.; Dai N.; Tang T.","Cheng, Cheng (56768506500); Dai, Ning (56365979900); Tang, Tao (57209234514)","56768506500; 56365979900; 57209234514","Improvement and Optimization of a 3D Reconstruction Algorithm for SEM Images of Porous Materials","2022","Mathematical Problems in Engineering","2022","","2904178","","","","10.1155/2022/2904178","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135155940&doi=10.1155%2f2022%2f2904178&partnerID=40&md5=d1a73d0b0e589d924a28d5afab2b9a1a","Porous materials have become increasingly common in people's daily lives as the industry has advanced. Porous materials have numerous applications in the petroleum and chemical industries, as well as in everyday life. The study of diffusion, thermal conductivity, and percolation properties of porous materials has an important engineering application background and scientific value. The microstructure of materials affects their properties and attributes, so the description and visualization of the microstructure of porous materials is of great importance in the study of materials science. Due to the specificity of the internal structure of porous materials, many scenarios require 3-dimensional reconstruction of porous materials in practical engineering. In order to improve the effect of 3-dimensional reconstruction of porous materials, a 3D reconstruction method based on the improved generative adversarial neural network (GAN) is proposed in this paper for SEM images of porous materials. First, scanning electron microscope (SEM) images of porous materials are acquired, and then the acquired SEM images are preprocessed, including denoising and determining the boundary. Second, an improved GAN-based image super-resolution reconstruction model (ISRGAN) is used, and then the preprocessed images are fed into the ISRGAN model for training. Thus, multiple intermediate layer images are generated. Third, the 3D reconstruction of the intermediate layer images is performed using the slice combination method. The relationship between the unit cell pixels and the porosity is analyzed in the experiments to verify the effectiveness of the 3D reconstruction method used in this paper, and it is concluded that the porosity tends to be stable when the unit cell pixels converge to 110 and converge to the porosity of the real sample. The experimental results validate the feasibility and effectiveness of the method presented in this paper in the 3D reconstruction process.  © 2022 Cheng Cheng et al.","Chemical industry; Generative adversarial networks; Image acquisition; Image enhancement; Image reconstruction; Pixels; Porosity; Scanning electron microscopy; Solvents; Thermal conductivity; Thermal Engineering; 3-dimensional; 3D reconstruction; Electron microscope images; Intermediate layers; Neural-networks; Optimisations; Property; Reconstruction method; Scanning electrons; Unit cells; Porous materials","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135155940"
"Lu T.; Zhao K.; Wu Y.; Wang Z.; Zhang Y.","Lu, Tao (56406646300); Zhao, Kanghui (57220898298); Wu, Yuntao (55993578900); Wang, Zhongyuan (34973569100); Zhang, Yanduo (55993581700)","56406646300; 57220898298; 55993578900; 34973569100; 55993581700","Structure-Texture Parallel Embedding for Remote Sensing Image Super-Resolution","2022","IEEE Geoscience and Remote Sensing Letters","19","","6516105","","","","10.1109/LGRS.2022.3206348","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139380653&doi=10.1109%2fLGRS.2022.3206348&partnerID=40&md5=3bdfd90776645e24ea1815eb0d85b7c3","The structure and texture of images are crucial for remote sensing image super-resolution (SR). Generative adversarial networks (GANs) recover image details through adversarial training. However, the recovered images always have structural distortions, on the one hand, and GANs are difficult to train, on the other hand. In addition, some methods assist reconstruction by introducing prior information of the image, but this brings additional computational cost. To address this issue, we propose a novel structure-texture parallel embedding (SPE) method for SR of remote sensing images. Our method does not require additional image priors to reconstruct high-quality images. Specifically, we use the global structure information and local texture information of the image in the ascending space to guide the reconstruction result of the image. First, we design a structure preserving block (SPB) to extract global structural features in the ascending space of the image, so as to obtain global structure information for a priori representation. Then, we design a local texture attention module (LTAM) to restore richer texture details. We have conducted lots of experiments on Draper public dataset. Experimental results show that our proposed method not only achieves a better tradeoff between computational cost and performance, but also outperforms the existing several SR methods in terms of objective index evaluation and subjective visual effects.  © 2004-2012 IEEE.","Economic and social effects; Embeddings; Generative adversarial networks; Image texture; Optical resolving power; Remote sensing; Space optics; Textures; Attention mechanisms; Features extraction; Image super resolutions; Images reconstruction; Remote sensing image super-resolution; Remote sensing images; Remote-sensing; Structure-preserving; Superresolution; Texture attention mechanism; data set; image resolution; performance assessment; remote sensing; Image reconstruction","Remote sensing image super-resolution (SR); structure preserving; texture attention mechanism","Article","Final","","Scopus","2-s2.0-85139380653"
"Chen S.; Liu X.; Meng S.","Chen, Siqi (57870097500); Liu, Xiao (57219490472); Meng, Shilong (57869951700)","57870097500; 57219490472; 57869951700","Generative Adversarial Network-Based Methods in Super Resolution","2022","ISCTT 2021 - 6th International Conference on Information Science, Computer Technology and Transportation","","","","375","381","6","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137114222&partnerID=40&md5=e1c785d61b87b993ae88d4ff346fa2c1","Super-resolution (SR) is a crucial image processing technique to optimize the resolution of images and videos. Recent years have witnessed significant development of SR approaches using Generative Adversarial Nets (GAN). Herein, a thorough overview on the latest achievements of SR approaches using GAN are given. Specifically, we firstly define the SR problem. Then, we introduce traditional and deep-learning (DL)-based SR techniques including Convolutional Neural Network (CNN) and GAN methods. Afterward, we explore the background of GAN and pay special attention to GAN-based approaches such as SRGAN, GMGAN, and Cycle-GAN. In addition, we also cover the applications of GAN-based SR approaches in real life, including medical diagnosis and remote sensing. © VDE VERLAG GMBH · Berlin · Offenbach.","Deep learning; Diagnosis; Image processing; Neural networks; Optical resolving power; Remote sensing; Convolutional neural network; Image processing technique; Learning-based super-resolution; Network-based; Remote-sensing; Resolution techniques; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85137114222"
"Balasubramanian A.; Dhanasekaran H.; Raghu B.; Kumarasamy K.","Balasubramanian, Ashwin (58093965500); Dhanasekaran, Haripriya (58093890500); Raghu, Booma (58093890600); Kumarasamy, Kunaraj (58093965600)","58093965500; 58093890500; 58093890600; 58093965600","MRI Super-Resolution using Generative Adversarial Network and Discrete Wavelet Transform","2022","Proceedings - International Conference on Augmented Intelligence and Sustainable Systems, ICAISS 2022","","","","1314","1318","4","10.1109/ICAISS55157.2022.10010995","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147548549&doi=10.1109%2fICAISS55157.2022.10010995&partnerID=40&md5=ae8958e3c6bb5b07fec213d0b0797ec8","Deep Learning Approaches have brought in major advances in super-resolution. Deep Learning architectures like convolutional neural networks or auto-encoders are trained to beget high-resolution (HR) images from the low-resolution (LR) images. Discrete wavelet transformations (DWT) is used to bring out the high frequency (HF) sub-bands of the image, which is further used to reduce the error while synthesizing super-resolution images. The proposed architecture combines the features generated by the DWT and Super-Resolution Generative Adversarial Networks (SRGAN) to produce images of high resolution. The implemented topology initially applies DWT to the image and separates out the image into two fundamental bands (HF and LF sub-bands). The features are then generated by separate generators and then Combined by inverse 2d discrete wavelet transformation (IDWT) which is then passed on to the discriminator to evaluate the generated image. A key feature of this design is that each feature is given importance and is generated by separate generators which makes the reconstruction and improvising the quality of the image more feasible. From the experiments conducted on medical images such as magnetic resonance imaging (MRI), the proposed design is computationally simpler and yet produces competitive and often improved results than state-of-the-art alternatives in terms of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM).  © 2022 IEEE.","Convolutional neural networks; Deep learning; Discrete wavelet transforms; Image enhancement; Magnetic resonance imaging; Medical imaging; Network architecture; Optical resolving power; Signal reconstruction; Signal to noise ratio; Auto encoders; Convolutional neural network; Discrete wavelets transformations; Discrete-wavelet-transform; High frequency HF; High-resolution images; Image super resolutions; Learning approach; Learning architectures; Superresolution; Generative adversarial networks","Biomedical image processing; Generative adversarial network; Image Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85147548549"
"Ye Y.","Ye, Yang (57555960100)","57555960100","Generative adversarial networks","2022","Proceedings of SPIE - The International Society for Optical Engineering","12158","","121580T","","","","10.1117/12.2626949","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127331214&doi=10.1117%2f12.2626949&partnerID=40&md5=1e19c5ecd1deed05dfaf35a35680b4c9","Generative Adversarial Networks has shown impressive achievement in computer graphics applications, and it's now widely used worldwide. GAN is composed of generator and discriminator, which are crucial compositions of GAN. GAN can generate 3D models, graphics that are required in animated movies or video game characters. For example, a generative model could simply generate a new picture that looks like a specific type of animal. In the meantime, the discriminative model could distinguish the picture from a human being or an animal. GAN variants consist of progressive GAN as well as conditional GAN. Since GAN can be applied in so many different areas, in this essay, we are going to talk about how GAN is applied in image and computer vision. More specifically, face synthesis, image to image translation, and super resolution. Conclusively, GAN has a significant contribution to various areas, and it boosts the advancement in the domain of computer graphics.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Animals; Deep learning; Discriminators; Optical resolving power; Three dimensional computer graphics; 3D models; 3d-modeling; Adversarial networks; Computer graphics applications; Deep learning; Generator; Image super resolutions; Image translation; Image-to-image translation; Movie games; Generative adversarial networks","Adversarial Networks; Deep Learning; Discriminator; Generator; Image super-resolution; Image-To-image translation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127331214"
"Shah B.K.; Yadav A.; Dixit A.K.","Shah, Bickey Kumar (57218936815); Yadav, Anshul (57781539300); Dixit, Ashutosh Kumar (57223140759)","57218936815; 57781539300; 57223140759","License Plate Image Super Resolution Using Generative Adversarial Network(GAN)","2022","Proceedings - International Conference on Applied Artificial Intelligence and Computing, ICAAIC 2022","","","","1139","1143","4","10.1109/ICAAIC53929.2022.9792759","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133444181&doi=10.1109%2fICAAIC53929.2022.9792759&partnerID=40&md5=e1eface7b1275d8598f6251e18334192","Super resolution of images in the field of Computer Vision is a widely used for the conversion of images into high resolution without the loss of pixel data into the images. Due to fast movement of vehicles and low quality of camera the image cannot be verified easily so, the techniques of Generative Adversarial network have been applied for the Super resolution of license plate Images which works to recover the loss data of license plate images without loss of pixel data. Earlier, mean square error (MS E) and peak signal to noise ratio (PSNR) was used as content loss to minimize the error but at optimal minimization the images get over smoothen and pixel data were lost. This paper has proposed and applied VGG-19 as pretrained neural network along with MSE and PSNR to minimize the content loss which overall optimizes the perpetual loss, and over smoothness of the images gets controlled which saves pixel data. Later, the pre-trained neural network is integrated with Generative Adversarial Network [GAN] of discriminator and generator to produce high resolution images. Taking the PSNR as an evaluation metrices for the images, it increases from 26.184 to 28.696 and accuracy from 58% to 84%. © 2022 IEEE.","Discriminators; License plates (automobile); Mean square error; Optical resolving power; Pixels; Signal to noise ratio; Fast movement; Generator; High resolution; Image super resolutions; License plate images; Low qualities; Peak signal to noise ratio; Superresolution; Vehicle quality; VGG-19; Generative adversarial networks","Discriminator; Generator; Peak signal to noise ratio; Super Resolution; VGG-19","Conference paper","Final","","Scopus","2-s2.0-85133444181"
"Sun B.","Sun, Binwen (57578673800)","57578673800","Methodology and Application of GAN Algorithms","2022","Proceedings of SPIE - The International Society for Optical Engineering","12168","","121682U","","","","10.1117/12.2631155","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128302986&doi=10.1117%2f12.2631155&partnerID=40&md5=e9ea5573307af61cd8d31146273eba19","GAN, proposed in 2014, has received high attention and extensive research from industry and academia. It is an important node in the development of generative model. With the development of research, GAN has been improved in theory and application. The basic idea of GAN is derived from two-person zero-sum game, which consists of a generator and discriminator, and is iteratively trained through confrontation. The purpose is to learn the distribution of sample data and generate new samples. Firstly, the theoretical framework of GAN is introduced. Secondly, some typical GAN models are discussed. Then four applications of GAN in data generation, image super resolution and so on is shown. Then the advantages and disadvantages of GAN are discussed and the future development direction of GAN is pointed out. Finally, the future prospect is summarized.  © 2022 SPIE.","Deep learning; Iterative methods; Optical resolving power; Data generation; Deep learning; Generative model; Image super resolutions; Image translation; Image-to-image translation; Learn+; Sample data; Theoretical framework; Two-person zero-sum game; Generative adversarial networks","Data generation; Deep learning; Generative Adversarial Network; Image super resolution; Image-to-Image translation","Conference paper","Final","","Scopus","2-s2.0-85128302986"
"Cobelli P.; Nesmachnow S.; Toutouh J.","Cobelli, Patricia (58064881900); Nesmachnow, Sergio (20734981200); Toutouh, Jamal (35362760300)","58064881900; 20734981200; 35362760300","A comparison of Generative Adversarial Networks for image super-resolution","2022","Proceedings - 2022 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2022","","","","","","","10.1109/LA-CCI54402.2022.9981850","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146230540&doi=10.1109%2fLA-CCI54402.2022.9981850&partnerID=40&md5=11aeee97ac6c0d0225e1f99696809551","This article presents a comparison of Generative Adversarial Networks for the image super-resolution problem. This is a relevant problem in several research areas and many real-world applications. The research consists of four steps: selecting successful Generative Adversarial Networks architectures, implementing two promising models, evaluating their image quality results, and analyzing their transfer learning capabilities. The main results indicate that both models are able to compute accurate results, with a reasonable deviation from state-of-the-art results and good transfer capabilities.  © 2022 IEEE.","Computer vision; Image analysis; Optical resolving power; Empirical analysis; Image super resolutions; Images processing; Learning capabilities; Real-world; Research areas; State of the art; Superresolution; Transfer capability; Transfer learning; Generative adversarial networks","empirical analysis; generative adversarial networks; image processing; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85146230540"
"","","","12th IFIP TC 12 International Conference on Intelligent Information Processing, IIP 2022","2022","IFIP Advances in Information and Communication Technology","643 IFIP","","","","","548","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132016256&partnerID=40&md5=745a66aeb0f9052a07ef3856ed5aaaec","The proceedings contain 43 papers. The special focus in this conference is on Intelligent Information Processing. The topics include: A Game-Theoretic Analysis of Impulse Purchase; research on Blockchain Privacy Protection Mechanism in Financial Transaction Services Based on Zero-Knowledge Proof and Federal Learning; a Hybrid Parallel Algorithm With Multiple Improved Strategies; resource Scheduling for Human-Machine Collaboration in Multiagent Systems; pre-loaded Deep-Q Learning; using Multi-level Attention Based on Concept Embedding Enrichen Short Text to Classification; does Large Pretrained Dataset Always Help? On the Effect of Dataset Size on Big Transfer Model; augmenting Context Representation with Triggers Knowledge for Relation Extraction; high-Resolution Remote Sensing Image Semantic Segmentation Method Based on Improved Encoder-Decoder Convolutional Neural Network; preface; classification Between Rumors and Explanations of Rumors Based on Common and Difference Subsequences of Sentences; augmenting Convolution Neural Networks by Utilizing Attention Mechanism for Knowledge Tracing; a Hybrid Multi-objective Optimization Algorithm with Improved Neighborhood Rough Sets for Feature Selection; predicting Student Performance in Online Learning Using a Highly Efficient Gradient Boosting Decision Tree; A Method for AGV Double-Cycling Scheduling at Automated Container Terminals; inductive Light Graph Convolution Network for Text Classification Based on Word-Label Graph; super-Resolution of Defocus Thread Image Based on Cycle Generative Adversarial Networks; a Method on Online Learning Video Recommendation Method Based on Knowledge Graph; A HEp-2 Cell Image Classification Model Based on Deep Residual Shrinkage Network Combined with Dilated Convolution; attention Adaptive Chinese Named Entity Recognition Based on Vocabulary Enhancement; software Defect Prediction Method Based on Cost-Sensitive Random Forest; a Pear Leaf Diseases Image Recognition Model Based on Capsule Network.","","","Conference review","Final","","Scopus","2-s2.0-85132016256"
"Yuan H.; Wang Y.; Xu G.; Wang F.","Yuan, Hongwu (12790399800); Wang, Yiqing (57572801200); Xu, Guoming (55546062900); Wang, Feng (56498041500)","12790399800; 57572801200; 55546062900; 56498041500","Research on super-resolution reconstruction of single-frame image of infrared focal plane polarization imaging","2022","Proceedings of SPIE - The International Society for Optical Engineering","12169","","121692H","","","","10.1117/12.2622890","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128065704&doi=10.1117%2f12.2622890&partnerID=40&md5=e245dec9dff1177825fa604fdff4149e","Since the low resolution of infrared focal plane arrays may degrade the performance of polarization imaging significantly, it is necessary to study the super-resolution reconstruction method for superior image resolution and contrast. Four typical single-frame image reconstruction methods are studied in this paper, and the comparison of reconstruction result of these methods is conducted by using subjective and objective evaluation. The experiments show that the reconstruction method based on generative adversarial network performs poorly in the evaluation indexes such as peak signal-to-noise ratio and structural similarity, but its reconstructed image has good visual effects, rich texture and details, and has a strong ability to suppress background noise, and using the reconstructed images for polarization information parsing can significantly improve the accuracy of polarization information parsing and the effect of polarization image fusion. © 2022 SPIE","Focal plane arrays; Focusing; Image enhancement; Image fusion; Image reconstruction; Image resolution; Infrared detectors; Polarization; Signal to noise ratio; Textures; Information parsing; Infrared focal plane polarization imaging; Infrared focal planes; Plane polarizations; Polarisation informations; Polarization image fusion; Polarization images; Polarization imaging; Polarization information parsing;; Super-resolution reconstruction; Generative adversarial networks","generative adversarial network; infrared focal plane polarization imaging; polarization information parsing; polarization image fusion; super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85128065704"
"Li J.-T.; Bian Z.; Guo L.-X.","Li, Jiang-Ting (37006755000); Bian, Zheng (57208108900); Guo, Li-Xin (56498191400)","37006755000; 57208108900; 56498191400","Optimized complex object classification model: reconstructing the ISAR image of a hypersonic vehicle covered with a plasma sheath using a U-WGAN-GP framework","2022","International Journal of Remote Sensing","43","14","","5306","5323","17","10.1080/01431161.2022.2133578","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140244358&doi=10.1080%2f01431161.2022.2133578&partnerID=40&md5=8d670dc17d22a30cfa02d8a00b7ce710","This study aims to apply generative adversarial networks (GANs) to the effective classification of high/low resolution (HR/LR) image pairs obtained via inverse synthetic aperture radar (ISAR) for hypersonic objects covered with a plasma sheath. We propose a classification training model based on a Wasserstein GAN with a gradient penalty (U-WGAN-GP) framework, wherein a U-Net with an excellent jump connection structure is used as a generator, and a VGG-Net with high robustness is used as a discriminator, to support the reliable classification of HR/LR ISAR image pairs for the enhancement of ISAR image resolution. The WGAN-GP provides a shortcut for the gradient propagation of network parameters during the training stage through the stable encoder and decoder structure of U-Net. It inherits the echo intensity variation and position distribution characteristics of the scattering points between hypersonic objects in LR images and effectively transplants them into the generated super-resolution ISAR images, and it establishes end-to-end mapping between the HR/LR ISAR images. Moreover, the VGG-Net ensures that the generated super-resolution images are stable, controllable, and undistorted. In addition, the WGAN-GP structure combines the content and adversarial losses, optimizes the generator loss function, and uses the GP method to provide a stable and continuous training process. Finally, a traditional VGG-16 network classifier is added at the end of the training model. The proposed model is applied to an HR/LR image pair dataset of hypersonic objects. Compared with existing methods, the proposed method improved the classification and recognition accuracy from 54.4% to 81.8%. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Generative adversarial networks; Hypersonic aerodynamics; Hypersonic vehicles; Image enhancement; Image resolution; Inverse synthetic aperture radar; Plasma sheaths; Radar imaging; High-low; High/low resolution (HR/LR) image pair; Hypersonic object; Image pairs; Inverse synthetic aperture radar; Inverse synthetic aperture radar images; Lower resolution; Training model; U-net; Wasserstein generative adversarial network with gradient penalty; conceptual framework; image analysis; image resolution; inverse problem; remote sensing; synthetic aperture radar; Inverse problems","High/low resolution (HR/LR) image pairs; hypersonic objects; inverse synthetic aperture radar (ISAR); plasma sheath; U-Net; Wasserstein generative adversarial networks with gradient penalty (WGAN-GP)","Article","Final","","Scopus","2-s2.0-85140244358"
"Ren Y.; Li R.; Liu Y.","Ren, Yalu (57887997200); Li, Rui (57204803394); Liu, Yan (57216669633)","57887997200; 57204803394; 57216669633","Super-resolution reconstruction of face images based on iterative upsampling and downsampling layers","2022","Proceedings of SPIE - The International Society for Optical Engineering","12454","","124542A","","","","10.1117/12.2658883","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144058113&doi=10.1117%2f12.2658883&partnerID=40&md5=3c9e191561c38ffb09cc952087ec053b","In order to solve the current problems of insufficient detail extraction and poor visual effect after high magnification reconstruction of face images, a super-resolution method is proposed for single images of faces based on generative adversarial networks. Channel attention is added to the generative network to extract richer facial details, and the idea of iterative up and down sampling layers in the depth inverse projection network is borrowed to make the reconstructed image with good visual effect after high magnification. For the discriminator network, the normalization layer, which would destroy the image contrast, is removed. The experimental results show that the reconstructed images are more realistic and the visual effects are improved compared with Bicubic, SRCNN, LapSRN and SRGAN. © 2022 SPIE.","Computer vision; Image enhancement; Image reconstruction; Iterative methods; Optical resolving power; Signal sampling; Channel attention; Down sampling; Face images; High magnifications; Image-based; Iterative upsampling and downsampling layer; Reconstructed image; Super-resolution reconstruction; Upsampling; Visual effects; Generative adversarial networks","channel attention; generative adversarial networks; iterative upsampling and downsampling layers; super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85144058113"
"Nan F.; Jing W.; Tian F.; Zhang J.; Chao K.-M.; Hong Z.; Zheng Q.","Nan, Fang (57340540700); Jing, Wei (57209529553); Tian, Feng (57218958427); Zhang, Jizhong (57219693951); Chao, Kuo-Ming (7202680681); Hong, Zhenxin (57219689418); Zheng, Qinghua (56113889000)","57340540700; 57209529553; 57218958427; 57219693951; 7202680681; 57219689418; 56113889000","Feature super-resolution based Facial Expression Recognition for multi-scale low-resolution images","2022","Knowledge-Based Systems","236","","107678","","","","10.1016/j.knosys.2021.107678","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119210480&doi=10.1016%2fj.knosys.2021.107678&partnerID=40&md5=4ff2a41871e0c01d4797b1c02b225a70","Facial Expression Recognition (FER) for various low-resolution images is an important task and need in applications of analyzing crowd scenes (station, classroom, etc.). Due to the discriminative feature loss caused by reduced resolution, classifying various low-resolution facial images into the right category is still a challenging task. In this work, we proposed a novel generative adversarial network-based feature level super-resolution method for robust facial expression recognition (FSR-FER), which can reduce the chance of privacy leaking without restoring high-resolution facial images. In particular, a pre-trained FER model was employed as a feature extractor, and a generator network G and a discriminator network D are trained with features extracted from low-resolution and corresponding high-resolution images. Generator network G tries to transform features of low-resolution images to more discriminative ones by making them closer to the ones of corresponding high-resolution images. For better classification performance, we also proposed an effective classification-aware loss reweighting strategy based on the classification probability calculated by a fixed FER model to make our model focus more on samples that are prone to misclassification. Experimental results on the Real-World Affective Faces (RAF) Database and Static Facial Expressions in the Wild (SFEW) 2.0 dataset demonstrate that our method achieves satisfying results on various down-sample factors with a single model and has better performance on low-resolution images compared with methods using image super-resolution and expression recognition separately. © 2021 The Authors","Discriminators; Face recognition; Optical resolving power; Discriminative features; Facial expression recognition; Feature super-resolution; High-resolution images; Low resolution images; Lower resolution; Multi-scales; Recognition models; Reduced resolution; Superresolution; Generative adversarial networks","Facial expression recognition; Feature super-resolution; Generative Adversarial Network; Low-resolution image","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85119210480"
"Yang L.; Liu H.; Li Y.; Zhou W.; Liu Y.; Di X.; Wang L.; Li C.","Yang, Linlin (57394748300); Liu, Hongying (36348639900); Li, Yiming (58025486900); Zhou, Wenhao (57222036102); Liu, Yuanyuan (57857115100); Di, Xiaobiao (58026522800); Wang, Lei (57785927800); Li, Chuanwen (58026261800)","57394748300; 36348639900; 58025486900; 57222036102; 57857115100; 58026522800; 57785927800; 58026261800","Multi Recursive Residual Dense Attention GAN for Perceptual Image Super Resolution","2022","IFIP Advances in Information and Communication Technology","659 IFIP","","","363","377","14","10.1007/978-3-031-14903-0_39","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144549295&doi=10.1007%2f978-3-031-14903-0_39&partnerID=40&md5=f68c85edcc6581b3b16ea61bc69050d7","Single image super-resolution (SISR) has achieved great progress based on convolutional neural networks (CNNs) such as generative adversarial network (GAN). However, most deep learning architectures cannot utilize the hierarchical features in original low-resolution images, which may result in the loss of image details. To recover visually high-quality high-resolution images, we propose a novel Multi-recursive residual dense Attention Generative Adversarial Network (MAGAN). Our MAGAN enjoys the ability to learn more texture details and overcome the weakness of conventional GAN-based models, which easily generate redundant information. In particular, we design a new multi-recursive residual dense network as a module in our generator to take advantage of the information from hierarchical features. We also introduce a multi-attention mechanism to our MAGAN to capture more informative features. Moreover, we present a new convolutional block in our discriminator by utilizing switchable normalization and spectral normalization to stabilize the training and accelerate convergence. Experimental results on benchmark datasets indicate that MAGAN yields finer texture details and does not produce redundant information in comparison with existing methods. © 2022, IFIP International Federation for Information Processing.","Convolution; Convolutional neural networks; Deep learning; Optical resolving power; Textures; Attention mechanisms; Convolutional neural network; Dense network; Hierarchical features; Image details; Image super resolutions; Learning architectures; Low resolution images; Multi-recursive residual dense network; Single images; Generative adversarial networks","Attention mechanism; Generative adversarial networks; Image super-resolution; Multi-recursive residual dense network","Conference paper","Final","","Scopus","2-s2.0-85144549295"
"Chen Y.-Z.; Liu T.-J.; Liu K.-H.","Chen, Yu-Zhang (57549949400); Liu, Tsung-Jung (36625954800); Liu, Kuan-Hsien (36626017900)","57549949400; 36625954800; 36626017900","Super-Resolution of Satellite Images Based on Two-Dimensional RRDB and Edge-Enhanced Generative Adversarial Network","2022","Digest of Technical Papers - IEEE International Conference on Consumer Electronics","2022-January","","","","","","10.1109/ICCE53296.2022.9730339","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127085207&doi=10.1109%2fICCE53296.2022.9730339&partnerID=40&md5=d7ece46361c4fc9bbec1cfda17d19a03","With the increasing demand for high-resolution images, image super-resolution (SR) technology has become one of the focuses in related research fields. Generally speaking, high resolution is usually achieved by increasing the density and accuracy of the sensor. However, such an approach is quite expensive for equipment and design. In particular, increasing the density of satellite sensors must be undertaken great risks. Inspired by EEGAN and based on it, the Ultra-Dense Subnet (UDSN) and Edge Enhanced Network (EEN) were modified. Among them, the UDSN is used for feature extraction and obtains high-resolution results that look clear in the intermediate but are deteriorated by artifacts, and the Edge-Enhanced Subnet (EESN) is used to purify, extract and enhance the image contour and use mask processing to eliminate images contaminated by noise. Finally, the restored intermediate image and the enhanced edge are combined to produce a high-resolution image with high credibility and clear content. We use Kaggle open experimental dataset to test and compare the results among different methods. It proves the performance of the proposed model is better than other SR methods.  © 2022 IEEE.","Generative adversarial networks; Image enhancement; Satellites; Statistical tests; Generative adversarial network; High resolution; High-resolution images; Image super resolutions; Image-based; Residual in residual dense block; Satellite images; Subnets; Superresolution; Two-dimensional; Optical resolving power","generative adversarial network (GAN); residual in residual dense block (RRDB); satellite images; Super-resolution(SR)","Conference paper","Final","","Scopus","2-s2.0-85127085207"
"Khalida R.; Madenda S.; Harmanto S.; Wiryana I.M.","Khalida, Rakhmi (57538485600); Madenda, Sarifuddin (55120215600); Harmanto, Suryadi (55645786700); Wiryana, I Made (55837669800)","57538485600; 55120215600; 55645786700; 55837669800","Concatenate Word Embedding for Text to Image through Generative Adversarial Network","2022","Proceedings - 4th International Conference on Informatics, Multimedia, Cyber and Information System, ICIMCIS 2022","","","","259","264","5","10.1109/ICIMCIS56303.2022.10017727","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147439238&doi=10.1109%2fICIMCIS56303.2022.10017727&partnerID=40&md5=ecd03146c6cc6e87fe8861496adc2f5a","Since the explosion of deep learning, automatic image generation from natural language is highly desirable through artificial intelligence (AI) as it makes it easier for users to create visually rich images through the ease of language. One of the methods used to generate images from text is GAN, in this study using word embedding is to process natural language and develop GAN by concatenate word embedding. In this work, we carry out our early-stage research which is to explore the simple technique of concatenate word embedding as the input of the GAN neural network. We show that this model is a novelty for the GAN model with the concept of multimodal input that is able to generate text to image and is expected to improve performance on a stronger GAN. Based on the explanation of our research method, this model can be implemented and can be developed for various GAN tasks such as style transfer, image to image, face inpainting or image repair semantically, and super resolution.  © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Image enhancement; Repair; Embeddings; GAN; Image; Image generations; Multimodal inputs; Natural languages; Neural-networks; Simple++; Text; Word embedding; Embeddings","GAN; image; text; word embedding","Conference paper","Final","","Scopus","2-s2.0-85147439238"
"Parekh D.; Maiti A.; Jain V.","Parekh, Darshan (57223037877); Maiti, Ankita (57740506900); Jain, Vishal (57192707657)","57223037877; 57740506900; 57192707657","Image Super-Resolution using GAN - A study","2022","2022 6th International Conference on Trends in Electronics and Informatics, ICOEI 2022 - Proceedings","","","","1539","1549","10","10.1109/ICOEI53556.2022.9777129","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131915615&doi=10.1109%2fICOEI53556.2022.9777129&partnerID=40&md5=384faf79ccf565d21d8f011bfdbebb2b","Reconstructing low-resolution images to high-resolution images by building a neural network is quite challenging but can be used in many applications like medical imaging, public surveillance, or old photo recovery. Compared to previous methods, deep learning has a breakthrough in high-resolution accuracy and speed. By applying a deep network with Generative Adversarial Networks, this model aims to enhance low-resolution images to produce high-resolution images. The major focus is to reconstruct the image with high resolution by developing the image with low-resolution in order to preserve the main details in the reconstructed images. © 2022 IEEE.","Deep learning; Image enhancement; Image reconstruction; Medical imaging; Optical resolving power; GAN; High resolution; High-resolution images; Image super resolutions; Low resolution images; Lower resolution; Neural-networks; Photo recovery; Reconstructed image; Superresolution; Generative adversarial networks","GAN; Generative Adversarial Network; High-resolution; Low-resolution; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85131915615"
"Reddy K.S.; Vijayan V.P.; Gupta A.D.; Singh P.; Vidhya R.G.; Kapila D.","Reddy, K. Srinivasa (57200767596); Vijayan, Vinodh P (56406165300); Gupta, Ayan Das (57575881200); Singh, Prabhdeep (57212591722); Vidhya, R.G. (57751176300); Kapila, Dhiraj (57218898268)","57200767596; 56406165300; 57575881200; 57212591722; 57751176300; 57218898268","Implementation of Super Resolution in Images Based on Generative Adversarial Network","2022","8th International Conference on Smart Structures and Systems, ICSSS 2022","","","","","","","10.1109/ICSSS54381.2022.9782170","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132247791&doi=10.1109%2fICSSS54381.2022.9782170&partnerID=40&md5=bd9e15d8bd0def642bd2f64e4c0155e3","A 3D visualization of a microscopic object is provided by the integral imaging microscopy system. A generative-adversarial-network (GAN) relied on super resolution (SR) algorithm is suggested in this research to improve resolution. The generator in GAN network regresses the highresolution (HR) outcome out of the low-resolution (LR) input image, where the discriminator differentiates among the original as well as generated images. It could perhaps recover the edges and boost the resolution besides 2, 4, or indeed 8 times without compromising image quality for different sector in different field. The framework is validated using a variation of decreased microscopic specimen images as well as appropriately develops images with considerable directional view and compared with each other to get the best model among them in different sector. The quantifiable investigation reveals that the suggested framework outperforms the existing algorithms for microscopic images. © 2022 IEEE.","Microlenses; Optical resolving power; Three dimensional computer graphics; 3D Visualization; Generative adversarial network; Image-based; Imaging microscopy; Integral imaging; Micro lens array; Micro-lens arrays; Microscopic objects; Super-resolution; Superresolution; Generative adversarial networks","generative adversarial network (GAN); micro lens array (MLA); super-resolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85132247791"
"Zhou B.; Yan H.; Wang S.","Zhou, Bijun (57944327700); Yan, Huibin (57273356600); Wang, Shuoyao (57201013949)","57944327700; 57273356600; 57201013949","Structure and Texture Preserving Network for Real-World Image Super-Resolution","2022","IEEE Signal Processing Letters","29","","","2173","2177","4","10.1109/LSP.2022.3216116","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140799067&doi=10.1109%2fLSP.2022.3216116&partnerID=40&md5=e11a8ef60a8f429dc6c8c857e7ae6a61","Real-world image super-resolution (Real-SR) is a challenging task due to the unknown complex image degradation. Recent research on Real-SR has achieved remarkable progress by degradation process modeling; however, there are still undesired structural distortions and over-smoothed textures in the recovered images. In this letter, we propose a structure and texture preserving network, towards reducing the structural distortions while refining the perceptual-pleasant textures. Specifically, we propose a structure tensor (ST) branch to guide the restoration of high-resolution images by extracting channel-aggregated structural information. To further adaptively optimize different local texture, we replace the global discriminator with a global-local discriminator. By 'local,' we mean that the discriminator loss, imposed on local areas randomly selected from the generated SR image, is minimized to generate textures with great visual perception in the selected local areas. Experimental results on five real-world datasets demonstrate the superiority of our methods in restoring structures, generating visually realistic SR images, as well as handling images of different degradation levels.  © 1994-2012 IEEE.","Discriminators; Generative adversarial networks; Image texture; Optical resolving power; Restoration; Tensors; Global-local; Global-local discriminator; Images reconstruction; Real-world; Real-world image; Real-world super-resolution; Structure tensors; Superresolution; Task analysis; Image reconstruction","generative adversarial network; global-local discriminator; Real-world super-resolution; structure tensor","Article","Final","","Scopus","2-s2.0-85140799067"
"Tibebu H.; Malik A.; De Silva V.","Tibebu, Haileleol (57218394616); Malik, Aadin (57814016400); De Silva, Varuna (57200569349)","57218394616; 57814016400; 57200569349","Text to Image Synthesis Using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks","2022","Lecture Notes in Networks and Systems","506 LNNS","","","560","580","20","10.1007/978-3-031-10461-9_38","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135048816&doi=10.1007%2f978-3-031-10461-9_38&partnerID=40&md5=11af60d22d7488bf8713e93e2af3d960","Synthesizing a realistic image from textual description is a major challenge in computer vision. Current text to image synthesis approaches falls short of producing a high-resolution image that represent a text descriptor. Most existing studies rely either on Generative Adversarial Networks (GANs) or Variational Auto Encoders (VAEs). GANs has the capability to produce sharper images but lacks the diversity of outputs, whereas VAEs are good at producing a diverse range of outputs, but the images generated are often blurred. Taking into account the relative advantages of both GANs and VAEs, we proposed a new stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture for synthesizing images conditioned on a text description. This study uses Conditional VAEs as an initial generator to produce a high-level sketch of the text descriptor. This high-level sketch output from first stage and a text descriptor is used as an input to the conditional GAN network. The second stage GAN produces a 256 × 256 high resolution image. The proposed architecture benefits from a conditioning augmentation and a residual block on the Conditional GAN network to achieve the results. Multiple experiments were conducted using CUB and Oxford-102 dataset and the result of the proposed approach is compared against state-of-the-art techniques such as StackGAN. The experiments illustrate that the proposed method generates a high-resolution image conditioned on text descriptions and yield competitive results based on Inception and Fréchet Inception Score using both datasets. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","","Conditional GAN; Conditional VAE; Constrained image synthesis; Stacked network; Super-resolution; Text to image synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135048816"
"Li V.; Amponis G.; Nebel J.-C.; Argyriou V.; Lagkas T.; Ouzounidis S.; Sarigiannidis P.","Li, Vladislav (57221683471); Amponis, George (57221555716); Nebel, Jean-Christophe (6602553817); Argyriou, Vasileios (13806485100); Lagkas, Thomas (6507945237); Ouzounidis, Savvas (57795420200); Sarigiannidis, Panagiotis (12445587500)","57221683471; 57221555716; 6602553817; 13806485100; 6507945237; 57795420200; 12445587500","Super Resolution for Augmented Reality Applications","2022","INFOCOM WKSHPS 2022 - IEEE Conference on Computer Communications Workshops","","","","","","","10.1109/INFOCOMWKSHPS54753.2022.9798101","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133922355&doi=10.1109%2fINFOCOMWKSHPS54753.2022.9798101&partnerID=40&md5=c679b8b5e18c881c61588233ce96d866","Latest developments in machine learning (ML), adversarial networks, combined with increasingly powerful IoT devices via the introduction of efficient processors, are bringing about the implementation of near real-time object detection and classification for augmented reality (AR) and virtual reality (VR) applications. This paper intends to explore new object detection and classification technologies leveraging super-resolution (SR), that have the potential to be integrated into small, mobile and low-power ARNR devices. SR in conjunction with novel object detection and classification algorithms are examined in the presented paper, with the ultimate goal of proposing a low-footprint Generative Adversarial Network (GAN)-based framework capable of receiving an LR input and outputting an SR-supported recognition model based on FRRCNN, YOLOv3 or Retina. © 2022 IEEE.","Augmented reality; Generative adversarial networks; Object recognition; Optical resolving power; Virtual reality; Adversarial networks; ARNR application; Augmented reality applications; Detection technology; Latest development; Machine-learning; Near-real time; Object classification; Objects detection; Superresolution; Object detection","ARNR applications; Object Detection and Classification; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85133922355"
"Zhang X.; Chowdhury R.R.; Shang J.; Gupta R.; Hong D.","Zhang, Xiyuan (57477990400); Chowdhury, Ranak Roy (57209513102); Shang, Jingbo (56355265600); Gupta, Rajesh (57211565489); Hong, Dezhi (55204679600)","57477990400; 57209513102; 56355265600; 57211565489; 55204679600","ESC-GAN: Extending spatial coverage of physical sensors","2022","WSDM 2022 - Proceedings of the 15th ACM International Conference on Web Search and Data Mining","","","","1347","1356","9","10.1145/3488560.3498461","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125778113&doi=10.1145%2f3488560.3498461&partnerID=40&md5=acf041483d75bf27e6ccf74c86126c9a","Scientific discoveries and studies about our physical world have long benefited from large-scale and planetary sensing, from weather forecasting to wildfire monitoring. However, the limited deployment of sensors in the environment due to cost or physical access constraints has lagged behind our ever-growing need for increased data coverage and higher resolution, impeding timely and precise monitoring and understanding of the environment. Therefore, we seek to extend the spatial coverage of analysis based on existing sensory data, that is, to ""generate""data for locations where no historical data exists. This problem is fundamentally different and more challenging than the traditional spatio-temporal imputation that assumes data for any particular location are only partially missing across time. Inspired by the success of Generative Adversarial Network (GAN) in imputation, we propose a novel ESC-GAN. We observe that there are local patterns in nearby locations, as well as trends in a global manner (e.g., temperature drops as altitude increases regardless of the location). As local patterns may exhibit at different scales (from meters to kilometers), we employ a multi-branch generator to aggregate information of different granularity. More specifically, each branch in the generator contains 1) randomly masked 3D partial convolutions at different resolutions to capture the local patterns and 2) global attention modules for global similarity. Next, we adversarially train a 3D convolution-based discriminator to distinguish the generator's output from the ground truth. Extensive experiments on three geo-sensor datasets demonstrate that ESC-GAN outperforms state-of-the-art methods on extending spatial coverage and also achieves the best results on a traditional spatio-temporal imputation task. © 2022 Owner/Author.","Convolution; Generative adversarial networks; Sensory analysis; Weather forecasting; Imputation; Local patterns; Physical sensors; Scientific discovery; Scientific studies; Self-attention; Spatial coverage; Spatio-temporal; Spatio-temporal data; Superresolution; Location","Generative adversarial network; Imputation; Self-attention; Spatio-temporal data; Super resolution","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85125778113"
"Xia Y.; Ravikumar N.; Frangi A.F.","Xia, Yan (57219626194); Ravikumar, Nishant (57190258888); Frangi, Alejandro F. (7005249248)","57219626194; 57190258888; 7005249248","Image imputation in cardiac MRI and quality assessment","2022","Biomedical Image Synthesis and Simulation: Methods and Applications","","","","347","367","20","10.1016/B978-0-12-824349-7.00024-4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137570204&doi=10.1016%2fB978-0-12-824349-7.00024-4&partnerID=40&md5=2c4da69e2cead9d6e191a6f1b75e92d1","Missing data is common in medical image research. For instance, corrupted or unusable slices owing to the presence of artifacts such as respiratory or motion ghosting, aliasing, and signal loss in images significantly reduce image quality and diagnostic accuracy. Also, medical image acquisition time is often limited by cost and physical or patient care constraints, resulting in highly under-sampled images, which can be formulated as missing in-between slices. Such clinically acquired scans violate underlying assumptions of many downstream algorithms. Another important application lies in multi-modal/multi-contrast imaging, where different medical images contain complementary information for improving the diagnosis. However, a complete set of different images is often difficult to obtain. All of these can be considered as missing image data, which can lead to a reduced statistical power and potentially biased results, if not handled appropriately. Thanks to the recent advances in deep neural networks and generative adversarial networks (GANs), the problem of missing image imputation can be viewed as an image synthesis problem, and its performance has been remarkably improved. In this chapter, we present cardiac MR imaging as a use case and investigate a robust approach, namely Image Imputation Generative Adversarial Network (I2-GAN), and compare it with several traditional and state-of-the-art image imputation techniques in context of missing slices. © 2022 Elsevier Inc. All rights reserved.","","Cardiac MRI; Generative adversarial network; Image imputation; Super resolution","Book chapter","Final","","Scopus","2-s2.0-85137570204"
"Zhou Q.","Zhou, Qiaoliang (57553798300)","57553798300","Superresolution Reconstruction of Remote Sensing Image Based on Generative Adversarial Network","2022","Wireless Communications and Mobile Computing","2022","","9114911","","","","10.1155/2022/9114911","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127113909&doi=10.1155%2f2022%2f9114911&partnerID=40&md5=3d876711c29468b78ce7006bf16c849c","To recreate high-resolution, more detailed remote sensing images from existing low-resolution photos, this technique is known as remote sensing image superresolution reconstruction, and it has numerous uses. As an important research hotspot of neural networks, generative adversarial network (GAN) has made outstanding progress for image superresolution reconstruction. It solves the computational complexity and low reconstructed image quality of standard superresolution reconstruction algorithms. This research offers a superresolution reconstruction strategy with a self-attention generative adversarial network to improve the quality of reconstructed superresolution remote sensing images. The self-attention strategy as well as residual module is utilized to build a generator in this model that transforms low-resolution remote sensing images into superresolution ones. It aims to determine the discrepancy between a reconstructed picture and a true picture by using a deep convolutional network as a discriminator. For the purpose of enhancing the accuracy, content loss is used. This is done to obtain accurate detail reconstruction. According to the findings of the experiments, this approach is capable of regenerating higher-quality images.  © 2022 Qiaoliang Zhou.","Image enhancement; Image reconstruction; Optical resolving power; Remote sensing; High resolution; Hotspots; Image super-resolution reconstruction; Image-based; Lower resolution; Neural-networks; Reconstructed image; Remote sensing images; Super-resolution reconstruction; Superresolution; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85127113909"
"Ren T.; Xu H.; Jiang G.; Yu M.; Zhang X.; Wang B.; Luo T.","Ren, Tingdi (57605422500); Xu, Haiyong (55462879200); Jiang, Gangyi (7401706697); Yu, Mei (57762346100); Zhang, Xuan (57605927600); Wang, Biao (57888440900); Luo, Ting (54397301800)","57605422500; 55462879200; 7401706697; 57762346100; 57605927600; 57888440900; 54397301800","Reinforced Swin-Convs Transformer for Simultaneous Underwater Sensing Scene Image Enhancement and Super-resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4209616","","","","10.1109/TGRS.2022.3205061","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137878445&doi=10.1109%2fTGRS.2022.3205061&partnerID=40&md5=55fdb6560fa249229efcd66661ed54a6","Underwater image enhancement (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. Meanwhile, the ever-increasing requirement for higher resolution images from a lower resolution in the underwater domain cannot be overlooked. To address these problems, a novel U-Net-based reinforced Swin-Convs Transformer for simultaneous enhancement and superresolution (URSCT-SESR) method is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, the Swin Transformer is embedded into U-Net for improving the ability to capture the global dependence. Then, given the inadequacy of the Swin Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, an ingenious manner is presented for the fusion of convolutions and the core attention mechanism to build a reinforced Swin-Convs Transformer block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the Swin Transformer. Finally, experimental results on available datasets demonstrate that the proposed URSCT-SESR achieves the state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code is publicly available at https://github.com/TingdiRen/URSCT-SESR.  © 1980-2012 IEEE.","Convolution; Generative adversarial networks; Image reconstruction; Image resolution; Light absorption; Neural networks; Reinforcement; Atmospheric modeling; Convolutional neural network; Scene image; Superresolution; Swin-convs transformer; Transformer; U-net; Underwater image enhancements; Underwater image enhancemnet; image classification; image resolution; instrumentation; remote sensing; satellite imagery; Image enhancement","Super-resolution (SR); Swin-Convs Transformer; U-Net; underwater image enhancement (UIE)","Article","Final","","Scopus","2-s2.0-85137878445"
"Ota M.; Yamaguchi K.; Suzuki K.","Ota, Masashi (57454074900); Yamaguchi, Keita (56680482000); Suzuki, Kenya (55421350100)","57454074900; 56680482000; 55421350100","Generative-adversarial-network-based dimensional measurement of optical waveguides","2022","Optics Express","30","4","","6365","6373","8","10.1364/OE.450740","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124676138&doi=10.1364%2fOE.450740&partnerID=40&md5=bdee88563640bb89fa81714209d8b1a4","We propose a high-throughput and precise waveguide-dimensional-measurement method consisting of a generative adversarial network (GAN) and curve-fitting-based dimensional calculator using sidewall functions. The GAN can learn the differences between low-magnification (LM) and high-magnification (HM) optical microscope images taken with different objective lenses at different magnifications over the same area. The LM and HM images of the waveguides are captured using an optical microscope at magnifications of 500× and 2000×, respectively. We obtained a standard deviation of the waveguide widths of approximately 0.8 pixels (∼ 42 nm), and confirmed precise width measurement using super-resolution images at the same imaging throughput as with an LM microscope. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.","Curve fitting; Generative adversarial networks; Lenses; Curves fittings; Dimensional measurements; High magnifications; High-throughput; Learn+; Measurement methods; Measurements of; Microscope images; Network-based; Optical microscopes; Microscopes","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124676138"
"Yang J.; Fu K.; Wu Y.; Diao W.; Dai W.; Sun X.","Yang, Jinze (57865182300); Fu, Kun (7202283802); Wu, Youming (57188724721); Diao, Wenhui (56816620400); Dai, Wei (57864642000); Sun, Xian (34875643000)","57865182300; 7202283802; 57188724721; 56816620400; 57864642000; 34875643000","Mutual-Feed Learning for Super Resolution and Object Detection in Degraded Aerial Imagery","2022","IEEE Transactions on Geoscience and Remote Sensing","","","","1","1","0","10.1109/TGRS.2022.3198083","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136884210&doi=10.1109%2fTGRS.2022.3198083&partnerID=40&md5=4241fcfbd013b4fe0675f27ff9882a88","The resolution degradation poses a huge challenge for object detection (OD) in the aerial imagery. Existing methods utilize super resolution (SR) based on Generative Adversarial Network (GAN) to restore texture details in degraded images. However, constrained detection results are still acquired due to the object feature difference between restored and clear images. Therefore, we propose a simple-yet-effective learning method called Mutual-Feed Learning (MFL) to solve the problem in this paper. A closed-loop structure is designed via building the feedback connection based on the feedforward connection between the two tasks. It effectively delivers the object spatial and feature information from OD to SR, and provides restoration-enhanced images from SR to OD. Specifically, a Feedback of Region of Interest (FROI) module is introduced to realize a region-level discrimination under the guidance of object information. It guides the discrimination process of super resolution. Furthermore, a Multi-Scale Object Information (MSOI) module is developed to implement a feature-level restoration by narrowing differences in object-related features. It improves the generation process of super resolution. Then object detection can be performed in restoration-enhanced images to obtain more accurate results. Extensive experiments over NWPU VHR-10, COWC, and FAIR1M dataset show that the method can achieve state-of-the-art results. IEEE","Aerial photography; Antennas; Feature extraction; Feedback; Image enhancement; Image resolution; Image segmentation; Learning systems; Mapping; Object detection; Object recognition; Restoration; Closed-loop; Closed-loop framework; Features extraction; Feedback of region of interest; Generator; Multi-scale object information; Multi-scales; Mutual-feed learning; Object information; Objects detection; Region-of-interest; Regions of interest; Superresolution; Task analysis; Image reconstruction","Closed-loop framework; Detectors; Feature extraction; Feedback of Region of Interest; Generators; Image resolution; Image restoration; Multi-Scale Object Information; Mutual-Feed Learning; Object detection; Object detection; Super resolution; Task analysis","Article","Article in press","","Scopus","2-s2.0-85136884210"
"Shi S.; Li M.","Shi, Shuaibing (57830511500); Li, Min (57830511600)","57830511500; 57830511600","Image Super Resolution Reconstruction Algorithm Based on Strong Constraint","2022","2022 3rd International Conference on Computer Vision, Image and Deep Learning and International Conference on Computer Engineering and Applications, CVIDL and ICCEA 2022","","","","1171","1175","4","10.1109/CVIDLICCEA56201.2022.9824123","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135405171&doi=10.1109%2fCVIDLICCEA56201.2022.9824123&partnerID=40&md5=1da0f6ce975a69eeab2197caf20285bc","To solve the problem of texture details loss in super-resolution reconstructed images, a novel image super-resolution algorithm (SCGAN) based on strong constraints with better fidelity was proposed. Firstly, in order to reconstruct finer texture details, an enhanced loss function was designed based on L1 function. Secondly, channel attention mechanism is introduced in super-resolution generative adversarial network architecture, and input feature weight parameters are given. Finally, the algorithm is validated on several benchmark data sets. The experimental results show that SCGAN not only has better visual effect, but also improves the objective evaluation index. Perceived similarity (LPIPS) improved at least 2.3% on average across the five test sets. Excluding RRDB algorithm with poor LPIPS, PSNR and SSIM improved at least 1. 2303dB on average and 0.0408 on average. © 2022 IEEE.","Image reconstruction; Network architecture; Optical resolving power; Textures; Attention mechanisms; Channel attention; Image super resolutions; Image super-resolution reconstruction; Loss functions; Reconstructed image; Reconstruction algorithms; Strong constraint; Super resolution algorithms; Superresolution; Generative adversarial networks","channel attention; Generative Adversarial Network; strong constraint; super resolution","Conference paper","Final","","Scopus","2-s2.0-85135405171"
"Nista L.; Schumann C.D.K.; Grenga T.; Attili A.; Pitsch H.","Nista, L. (57209858392); Schumann, C.D.K. (57219595116); Grenga, T. (56113409400); Attili, A. (36597809400); Pitsch, H. (7003265812)","57209858392; 57219595116; 56113409400; 36597809400; 7003265812","Investigation of the generalization capability of a generative adversarial network for large eddy simulation of turbulent premixed reacting flows","2022","Proceedings of the Combustion Institute","","","","","","","10.1016/j.proci.2022.07.244","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139197496&doi=10.1016%2fj.proci.2022.07.244&partnerID=40&md5=b5db55d388bf56bb942ed12056d1e814","In the past decades, Deep Learning (DL) frameworks have demonstrated excellent performance in modeling nonlinear interactions and are a promising technique to move beyond physics-based models. In this context, super-resolution techniques may present an accurate approach as subfilter-scale (SFS) closure model for Large Eddy Simulations (LES) in premixed combustion. However, DL models need to perform accurately in a variety of physical regimes and generalize well beyond their training conditions. In this work, a super-resolution Generative Adversarial Network (GAN) is proposed as closure model for the unresolved subfilter-stress and scalar-flux tensors of the filtered reactive Navier-Stokes equations solved in LES. The model trained on a premixed methane/air jet flame is evaluated a-priori on similar configurations at different Reynolds and Karlovitz numbers. The GAN generalizes well at both lower and higher Reynolds numbers and outperforms existing algebraic models when the ratio between the filter size and the Kolmogorov scale is preserved. Moreover, extrapolation at a higher Karlovitz number is investigated indicating that the ratio between the filter size and the thermal flame thickness may not need to be conserved in order to achieve high correlation in terms of SFS field. Generalization studies obtained on substantially different flame conditions indicate that successful predictive abilities are demonstrated if the generalization criterion is matched. Finally, the reconstruction of a scalar quantity, different from that used during the training, is evaluated, revealing that the model is able to reconstruct scalar fields with large gradients that have not been explicitly used in the training. The a-priori investigations carried out assess whether out-of-sample predictions are even feasible in the first place, providing insights into the quantities that need to be conserved for the model to perform well between different regimes, and represent a crucial step toward future embedding into LES numerical solvers. © 2022 Elsevier Inc. All rights reserved.","Combustion; Computational complexity; Deep learning; Large eddy simulation; Navier Stokes equations; Optical resolving power; Reynolds number; Closure models; Combustion model; Data-driven model; Filter sizes; Generalization capability; Large-eddy simulations; Premixed combustion; Premixed combustion modeling; Subfilter scale; Superresolution; Generative adversarial networks","Data-driven modeling; Generalization capability; Generative adversarial network; Large eddy simulation; Premixed combustion modeling","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85139197496"
"Liu K.-H.; Lin C.-C.; Liu T.-J.","Liu, Kuan-Hsien (36626017900); Lin, Chien-Cheng (57550736600); Liu, Tsung-Jung (36625954800)","36626017900; 57550736600; 36625954800","Image Generation by Residual Block Based Generative Adversarial Networks","2022","Digest of Technical Papers - IEEE International Conference on Consumer Electronics","2022-January","","","","","","10.1109/ICCE53296.2022.9730533","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127030921&doi=10.1109%2fICCE53296.2022.9730533&partnerID=40&md5=42007a1b5f85fab84ffd7c71326890ae","Generative adversarial network is a popular deep learning technique for solving artificial intelligence tasks, and it has been widely studied and applied for processing images, voices, texts and so on. Especially, generative adversarial network is adopted in the field of image processing, such as image style transfer, image restoration, image super-resolution and so on. Although generative adversarial networks show remarkable success in image generation, training process is usually unstable and trained models collapse where many of the generated images may contain the same color or texture pattern. In this paper, the network of generator and discriminator are modified, and the residual block is added to the generative adversarial network architecture to learn better image features. To reduce the loss of image feature during training and get more features to stabilize image generation, we use feature matching to minimize feature loss between the real and generated images for stable training. In the experiment, performance improvement can be obtained by adopting our proposed method, which is also better than some state-of-the-art methods.  © 2022 IEEE.","Deep learning; Image reconstruction; Network architecture; Textures; Block based; Deep learning; Image features; Image gen-eration; Image generations; Image super resolutions; Images processing; Learning techniques; Residual block; Training process; Generative adversarial networks","Artificial intelligence; deep learning; generative adversarial networks; image gen-eration; residual block","Conference paper","Final","","Scopus","2-s2.0-85127030921"
"Sun Y.; Pan B.; Li Q.; Wang J.; Wang X.; Chen H.; Cao Q.; Liu H.; Feng T.; Sun H.; Xiao Y.; Gong N.-J.","Sun, Yao (58064724700); Pan, Boyang (58064374400); Li, Qingchu (58064374500); Wang, JiaChen (57221646529); Wang, Xiang (57211222259); Chen, Honghua (58064019300); Cao, Qing (58064139100); Liu, Hui (56079150500); Feng, Tao (57200808235); Sun, Hongbiao (58064261400); Xiao, Yi (25622657000); Gong, Nan-Jie (49663223200)","58064724700; 58064374400; 58064374500; 57221646529; 57211222259; 58064019300; 58064139100; 56079150500; 57200808235; 58064261400; 25622657000; 49663223200","Clinical ultra-high resolution CT scans enabled by using a generative adversarial network","2022","Medical Physics","","","","","","","10.1002/mp.16172","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146243647&doi=10.1002%2fmp.16172&partnerID=40&md5=483b21b3dc9bc40ee06755cbee8e5887","Background: Ultra-high resolution computed tomography (UHRCT) has shown great potential for the detection of pulmonary diseases. However, UHRCT scanning generally induces increases in scanning time and radiation exposure. Super resolution is a gradually prosperous application in CT imaging despite higher radiation dose. Recent works have proved that the convolution neural network especially the generative adversarial network (GAN) based model could generate high-resolution CT using phantom images or simulated low resolution data without extra dose. Research that used clinical CT particularly lung images are rare due to the difficulty in collecting paired dataset. Purpose: To generate clinical UHRCT in lung from low resolution computed tomography (LRCT) using a GAN model. Methods: 43 clinical scans with LRCT and UHRCT were collected in this study. Paired patches were selected using the structural similarity index measure (SSIM) and the peak signal-to-noise ratio (PSNR) threshold. A relativistic GAN with gradient guidance was trained to learn the mapping from LRCT to UHRCT. The performance of the proposed method was evaluated using PSNR and SSIM. A reader study with five-point Likert score (five for the worst and one for the best) is also applied to assess the proposed method in terms of general quality, diagnostic confidence, sharpness and denoise level. Results: Experimental results show that our method got PSNR 32.60 ± 2.92 and SSIM 0.881 ± 0.057 on our clinical CT dataset, outperforming other state-of-the-art methods based on the simulated scenarios. Moreover, reader study shows that our method reached the good clinical performance in terms of general quality (1.14 ± 0.36), diagnostic confidence (1.36 ± 0.49), sharpness (1.07 ± 0.27) and high denoise level (1.29 ± 0.61) compared to other SR methods. Conclusion: This study demonstrated the feasibility of generating UHRCT images from LRCT without longer scanning time or increased radiation dose. © 2022 American Association of Physicists in Medicine.","Biological organs; Clinical research; Computerized tomography; Deep learning; Diagnosis; Optical resolving power; Signal to noise ratio; Computed tomography; Deep learning; High-resolution computed tomography; Lower resolution; Peak signal to noise ratio; Scanning time; Similarity indices; Structural similarity; Superresolution; Ultrahigh resolution; Generative adversarial networks","computed tomography (CT); deep learning; super resolution","Article","Article in press","","Scopus","2-s2.0-85146243647"
"Zhang T.; Ji X.; Lu F.","Zhang, Ting (56999385700); Ji, Xin (57655937400); Lu, Fangfang (35173394600)","56999385700; 57655937400; 35173394600","3D reconstruction of porous media by combining scaling transformation and multi-scale discrimination using generative adversarial networks","2022","Journal of Petroleum Science and Engineering","209","","109815","","","","10.1016/j.petrol.2021.109815","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119271392&doi=10.1016%2fj.petrol.2021.109815&partnerID=40&md5=20669d596527c8d43ed59d1b51f53246","The modeling and characterization of porous media is quite significant to explore and develop oil and natural gas resources. Traditional numerical simulation methods obtain reconstruction results through the statistical probability in training images (TIs), while the reconstruction process is lengthy and the probability information cannot be reused. The development of deep learning has provided reliable support for the reconstruction of porous media due to its strong ability of learning and extracting characteristics from TIs and the reuse of network parameters. As a common deep learning method, generative adversarial networks (GANs) can obtain images highly similar to the structural characteristics of the TI through adversarial training between the generator and the discriminator. Based on GAN, a three-dimensional multi-scale pattern generative adversarial network (3D-MSPGAN) that can achieve 3D super-resolution reconstruction of porous media is proposed in this paper. The use of scaling transformation allows 3D-MSPGAN to reconstruct super-resolution images of porous media that are much larger than the TI, while the multi-scale discrimination in the discriminator provides a guarantee for the generation of high-quality super-resolution images, by which the discriminator can retain the global structure and local characteristics of the TI simultaneously during the reconstruction process. In addition, due to the multi-scale patch information used in the discriminator, only a single TI is needed to complete the reconstruction of porous media. Experimental comparison with some typical methods proves that 3D-MSPGAN can achieve the reconstruction of 3D porous media with faster speed and higher quality. © 2021 Elsevier B.V.","Deep learning; Energy resources; Image reconstruction; Natural gas; Numerical methods; Optical resolving power; Porous materials; 3D reconstruction; Deep learning; High quality; Multi-scales; Porous medium; Reconstruction; Reconstruction process; Scaling transformation; Superresolution; Training image; experimental study; hydrocarbon exploration; image resolution; machine learning; numerical method; oil well; porous medium; reconstruction; resolution; Generative adversarial networks","Deep learning; Generative adversarial network; Porous media; Reconstruction; Super-resolution","Article","Final","","Scopus","2-s2.0-85119271392"
"Wang C.; Lu T.; Wu G.; Wang Y.; Sun Z.","Wang, Caiyong (57215526589); Lu, Tianhao (57227850700); Wu, Gaosheng (58087478700); Wang, Yunlong (57190767864); Sun, Zhenan (8081773300)","57215526589; 57227850700; 58087478700; 57190767864; 8081773300","D-ESRGAN: A Dual-Encoder GAN with Residual CNN and Vision Transformer for Iris Image Super-Resolution","2022","2022 IEEE International Joint Conference on Biometrics, IJCB 2022","","","","","","","10.1109/IJCB54206.2022.10007938","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147257589&doi=10.1109%2fIJCB54206.2022.10007938&partnerID=40&md5=b9fae19478ab77ddb35ee3089bcf64a5","Iris images captured in less-constrained environments, especially at long distances often suffer from the interference of low resolution, resulting in the loss of much valid iris texture information for iris recognition. In this paper, we propose a dual-encoder super-resolution generative adversarial network (D-ESRGAN) for compensating texture lost of the raw image meanwhile maintaining the newly generated textures more natural. Specifically, the proposed D-ESRGAN not only integrates the residual CNN encoder to extract local features, but also employs an emerging vision transformer encoder to capture global associative information. The local and global features from two encoders are further fused for the subsequent reconstruction of high-resolution features. During the training, we develop a three-stage strategy to alleviate the problem that generative adversarial networks are prone to collapse. Moreover, to boost the iris recognition performance, we introduce a triplet loss to push away the distance of super-resolved iris images with different IDs, and pull the distance of super-resolved iris images with the same ID much closer. Experimental results on the public CASIA-Iris-distance and CASIA-Iris-M1 datasets show that D-ESRGAN archives better performance than state-of-the-art baselines in terms of both super-resolution image quality metrics and iris recognition metric.  © 2022 IEEE.","Biometrics; Computer vision; Image texture; Optical resolving power; Signal encoding; Textures; Global feature; Image super resolutions; Iris images; Iris recognition; Iris texture information; Local feature; Lower resolution; Performance; Raw images; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85147257589"
"Latif H.; Ghuffar S.; Ahmad H.M.","Latif, Hasan (57971676700); Ghuffar, Sajid (14630367300); Ahmad, Hafiz Mughees (57208205069)","57971676700; 14630367300; 57208205069","Super-resolution of Sentinel-2 images using Wasserstein GAN","2022","Remote Sensing Letters","13","12","","1194","1202","8","10.1080/2150704X.2022.2136019","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142262873&doi=10.1080%2f2150704X.2022.2136019&partnerID=40&md5=6ed46ccc6359ea33ed4a46f5cec66add","The Sentinel-2 satellites deliver 13 band multi-spectral imagery with bands having 10 m, 20 m or 60 m spatial resolution. The low-resolution bands can be upsampled to match the high resolution bands to extract valuable information at higher spatial resolution. This paper presents a Wasserstein Generative Adversarial Network (WGAN) based approach named as DSen2-WGAN to super-resolve the low-resolution (i.e., 20 m and 60 m) bands of Sentinel-2 images to a spatial resolution of 10 m. A proposed generator is trained in an adversarial manner using the min-max game to super-resolve the low-resolution bands with the guidance of available high-resolution bands in an image. The performance evaluated using metrics such as Signal Reconstruction Error (SRE) and Root Mean Squared Error (RMSE) shows the effectiveness of the proposed approach as compared to the state-of-the-art method, DSen2 as the DSen2-WGAN reduced RMSE by 14.68% and 7%, while SRE improved by almost 4% and 1.6% for 6 (Formula presented.) and 2 (Formula presented.) super-resolution. Lastly, for further evaluation, we have used trained DSen2-WGAN model to super-resolve the bands of EuroSAT dataset, a satellite image classification dataset based on Sentinel-2 images. The per band classification accuracy of low-resolution bands shows significant improvement after super-resolution using our proposed approach. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Air navigation; Classification (of information); Image resolution; Mean square error; Signal reconstruction; Spectroscopy; High resolution; High spatial resolution; Lower resolution; Multispectral imagery; Network-based approach; Reconstruction error; Root mean squared errors; Signals reconstruction; Spatial resolution; Superresolution; data set; image classification; image resolution; satellite data; Sentinel; spatial resolution; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85142262873"
"Cai F.; Wu K.-Y.; Wang F.","Cai, Feng (57888175100); Wu, Ke-Yu (57324654500); Wang, Feng (56459216100)","57888175100; 57324654500; 56459216100","Remote Sensing Image Super-Resolution Via Attentional Feature Aggregation Generative Adversarial Network","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2598","2601","3","10.1109/IGARSS46834.2022.9884863","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141821238&doi=10.1109%2fIGARSS46834.2022.9884863&partnerID=40&md5=39a0f2d24636b6be3ed4ff7137c98de3","The extraction of high-frequency details is generally neglected in single image super-resolution (SISR) for remote sensing images. In this paper, we propose an attentional feature aggregation generative adversarial network (AFA-GAN) with the capability of strong feature extraction and attentional feature fusion to generate high-resolution remote sensing images. We adopt the residual feature aggregation framework for the feature extraction to make full use of the hierarchical features on the residual branches. To better fuse global and local features with inconsistent scales, an attentional feature fusion mechanism is utilized in residual feature aggregation modules. The comprehensive experiments with state-of-the-art SISR methods on the UC Merced dataset demonstrate the effectiveness and superiority of our AFA-GAN. © 2022 IEEE.","Computer vision; Extraction; Feature extraction; Optical resolving power; Remote sensing; Attentional feature aggregation; Feature aggregation; Features extraction; Features fusions; Generative adversarial network; High frequency HF; Image super resolutions; Remote sensing images; Single image super-resolution; Single images; Generative adversarial networks","attentional feature aggregation (AFA); generative adversarial network (GAN); Remote sensing images; single image super-resolution (SISR)","Conference paper","Final","","Scopus","2-s2.0-85141821238"
"Pattanaik A.; Balabantaray R.C.","Pattanaik, Anmol (57563024000); Balabantaray, Rakesh Chandra (55548207900)","57563024000; 55548207900","Enhancement of license plate recognition performance using Xception with Mish activation function","2022","Multimedia Tools and Applications","","","","","","","10.1007/s11042-022-13922-9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140076968&doi=10.1007%2fs11042-022-13922-9&partnerID=40&md5=0c60d96248b55ad9b9788a75e34ed0ad","The current breakthroughs in the highway research sector have resulted in a greater awareness and focus on the construction of an effective Intelligent Transportation System (ITS). One of the most actively researched areas is Vehicle Licence Plate Recognition (VLPR), concerned with determining the characters contained in a vehicle’s Licence Plate (LP). Many existing methods have been used to deal with different environmental complexity factors but are limited to motion deblurring. The aim of our research is to provide an effective and robust solution for recognizing characters present in license plates in complex environmental conditions. Our proposed approach is capable of handling not only the motion-blurred LPs but also recognizing the characters present in different types of low resolution and blurred license plates, illegible vehicle plates, license plates present in different weather and light conditions, and various traffic circumstances, as well as high-speed vehicles. Our research provides a series of different approaches to execute different steps in the character recognition process. The proposed approach presents the concept of Generative Adversarial Networks (GAN) with Discrete Cosine Transform (DCT) Discriminator (DCTGAN), a joint image super resolution and deblurring approach that uses a discrete cosine transform with low computational complexity to remove various types of blur and complexities from licence plates. License Plates (LPs) are detected using the Improved Bernsen Algorithm (IBA) with Connected Component Analysis(CCA). Finally, with the aid of the proposed Xception model with transfer learning, the characters in LPs are recognised. Here we have not used any segmentation technique to split the characters. Four benchmark datasets such as Stanford Cars, FZU Cars, HumAIn 2019 Challenge datasets, and Application-Oriented License Plate (AOLP) dataset, as well as our own collected dataset, were used for the validation of our proposed algorithm. This dataset includes the images of vehicles captured in different lighting and weather conditions such as sunny, rainy, cloudy, blurred, low illumination, foggy, and night. The suggested strategy does better than the current best practices in both numbers and quality. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Benchmarking; Complex networks; Convolution; Discrete cosine transforms; Image enhancement; Intelligent systems; Learning systems; License plates (automobile); Optical character recognition; 'current; Activation functions; Bernsen; Connected component analysis; Highway research; Improved bernsen algorithm; Licenses plate recognition; Performance; Transfer learning; Xception; Generative adversarial networks","Connected Component Analysis; Generative Adversarial Networks; Improved Bernsen Algorithm; Transfer Learning; Xception","Article","Article in press","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85140076968"
"Jiang S.; Hao J.; Ma J.; Li X.","Jiang, Shangling (57863050300); Hao, Jiafeng (57863174300); Ma, Jiquan (36078806400); Li, Xuliang (57863303200)","57863050300; 57863174300; 36078806400; 57863303200","Two Stage Real Facial Image Super Resolution with Geometric Knowledge Infusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12257","","122571W","","","","10.1117/12.2640272","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136823042&doi=10.1117%2f12.2640272&partnerID=40&md5=877d7b33b0dbf181b8c45b8dbd920aba","Super-resolution(SR) is a classic issue in computer vision. Existing SR methods mainly rely on artificially down-sampling to prepare the pair-wised training data set without considering the blur kernel inconsistency and often result in an unstable performance in the real scenario. Especially for facial images, unacceptable reconstruction seriously affects its downstream applications. In order to improve the quality in real facial image SR, we proposed an end-to-end deep neural network to mimic a realistic process of image degradation and reconstruct a high-resolution (HR) facial image from a low-resolution (LR) one. At the first stage, a generative adversarial network (GAN) is employed to sample more realistic LR images by jointly considering facial landmark and composite blur kernels. At the second stage, pair-wised training samples are fed into a decoder to fit the mapping from LR to HR. Experiments were performed on real LR test set from Widerface and LS3D-W dataset. Results demonstrate that our model outperforms existing competing methods. Furthermore, the ablation studies valid the effectiveness of the proposed components. © 2022 SPIE.","Computer vision; Deep neural networks; Image enhancement; Image reconstruction; Optical resolving power; Statistical tests; Blur kernel; Deep learning; Down sampling; Facial images; Facial landmark; Geometric knowledge; Image super resolutions; Lower resolution; Superresolution; Superresolution methods; Generative adversarial networks","blur kernel; deep learning; facial landmark; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85136823042"
"Nakayama K.; Goto T.","Nakayama, Koki (57870202900); Goto, Tomio (7403938412)","57870202900; 7403938412","High Resolution Image Generation Using Learning Super-Resolution for Low Resolution Images","2022","2022 4th International Conference on Computer Communication and the Internet, ICCCI 2022","","","","83","86","3","10.1109/ICCCI55554.2022.9850252","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137081030&doi=10.1109%2fICCCI55554.2022.9850252&partnerID=40&md5=6011c4df336dfb917ab8497aeccb2331","Cameras have been installed in a variety of locations. This is to record evidence in case of trouble. However, many of these images have low resolution, which reduces the effectiveness of the evidence. In particular, license plate images can identify the parties involved, so it is important to read the characters. In this paper, we aim to improve the resolution of license plate images by using super-resolution with adversarial learning method.  © 2022 IEEE.","Image enhancement; Learning systems; License plates (automobile); Optical character recognition; Optical resolving power; Adversarial learning; High-resolution images; Image generations; Learning methods; License plate images; Low resolution images; Lower resolution; Superresolution; Generative adversarial networks","generative adversarial network; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85137081030"
"Liao J.; Qu J.; Hao Y.; Li J.","Liao, Jianhui (57226654012); Qu, Junle (7201534904); Hao, Yongqi (56115825500); Li, Jia (57201799367)","57226654012; 7201534904; 56115825500; 57201799367","Deep-learning-based methods for super-resolution fluorescence microscopy","2022","Journal of Innovative Optical Health Sciences","","","2230016","","","","10.1142/S1793545822300166","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144520876&doi=10.1142%2fS1793545822300166&partnerID=40&md5=22656ef5ba4fa8f91b0114f203f4ce2d","The algorithm used for reconstruction or resolution enhancement is one of the factors affecting the quality of super-resolution images obtained by fluorescence microscopy. Deep-learning-based algorithms have achieved state-of-the-art performance in super-resolution fluorescence microscopy and are becoming increasingly attractive. We firstly introduce commonly-used deep learning models, and then review the latest applications in terms of the network architectures, the training data and the loss functions. Additionally, we discuss the challenges and limits when using deep learning to analyze the fluorescence microscopic data, and suggest ways to improve the reliability and robustness of deep learning applications.  © 2022 ","Convolutional neural networks; Deep learning; Fluorescence; Fluorescence microscopy; Generative adversarial networks; Image enhancement; Learning systems; Network architecture; Optical resolving power; Convolutional neural network; Deep learning; Images reconstruction; Learning-based algorithms; Learning-based methods; Resolution enhancement; Resolution images; State-of-the-art performance; Super-resolution fluorescence microscopy; Superresolution; convolutional neural network; deep learning; fluorescence microscopy; image reconstruction; loss of function mutation; reliability; review; Image reconstruction","convolutional neural network; deep learning; generative adversarial network; image reconstruction; Super-resolution fluorescence microscopy","Review","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85144520876"
"Haykir A.A.; Oksuz I.","Haykir, Aslan Ahmet (57903966500); Oksuz, Ilkay (55793268700)","57903966500; 55793268700","Transfer Learning Based Super Resolution of Aerial Images; [Öǧrenme Transferi Temelli Hava Araci Görüntülerinin Süper Çözünürlüǧü]","2022","2022 30th Signal Processing and Communications Applications Conference, SIU 2022","","","","","","","10.1109/SIU55565.2022.9864797","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138735957&doi=10.1109%2fSIU55565.2022.9864797&partnerID=40&md5=3a50d88f8bc8e6628259efaa3ddef146","Images created using the Super Resolution method can generate more information compared to their low resolution counterparts. A super-resolved image, which is created using an original image captured by an imaging source is not only more meaningful to human perception but also has advantages on downstream tasks such as object detection and pattern recognition. In this work, we aim to apply the Super Resolution method to the Aerial Images captured for surveillance to enable more information about the original scenes. To achieve this Super Resolution Generative Adversarial Network (SRGAN), which is based on the Generative Adversarial Networks architecture is used. We also applied transfer learning methodology to achieve better image quality. Public xView and DOTA datasets which contain images mostly captured by satellites around the world are used to train a generative model via SRGAN architecture. Furthermore, DIV2K dataset is used to pre-train a generative model, and then the transfer learning technique is used to train separate models on xView and DOTA validation datasets. Perceptual Index (PI) and Root Mean Squared Error (RMSE) which are used on European Conference on Computer Vision -Perceptual Image Restoration and Manipulation Workshop 2018 are computed as the performance metrics. We have seen that the model which gives the best PI results, i.e. better perceptual quality, on xView and DOTA validation datasets is the one trained using the DIV2K dataset and the model which gives the best RMSE results, i.e. better reconstruction quality, is the one trained using the transfer learning technique. © 2022 IEEE.","Antennas; Computer vision; Deep learning; Image reconstruction; Learning algorithms; Learning systems; Mean square error; Network architecture; Object detection; Optical resolving power; Aerial images; Deep learning; Generative model; Learning techniques; Learning-based super-resolution; Lower resolution; Root mean squared errors; Superresolution; Superresolution methods; Transfer learning; Generative adversarial networks","Aerial Images; Deep Learning; Generative Adversarial Networks; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85138735957"
"Dong Y.; Xu X.; Gao C.","Dong, Yan (58068959200); Xu, Xiaoyang (58069097400); Gao, Chongyang (57221686358)","58068959200; 58069097400; 57221686358","Trans-GAN network for image super-resolution reconstruction","2022","2022 IEEE International Conference on Signal Processing, Communications and Computing, ICSPCC 2022","","","","","","","10.1109/ICSPCC55723.2022.9984227","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146420811&doi=10.1109%2fICSPCC55723.2022.9984227&partnerID=40&md5=ee517a9d289e86b3e8bf48e931c59731","In order to solve the problems of network training difficulties and artifacts in the generated image super-resolution reconstruction algorithm based on convolutional neural network, this paper proposes a Trans-GAN super-resolution reconstruction algorithm based on generative adversarial network. By introducing Transformer model, the algorithm proposed a new Trans-GAN network, and improved residual module, using VGG19 network as the discriminator model framework, using adaptive pooling to replace the full connection layer to prevent overfitting, introducing per-pixel mean square error loss, perceived loss, The antagonism loss and total variation loss constitute the total objective loss function of the generator to solve the problem of insufficient feature extraction ability in the super-resolution algorithm based on convolutional neural network. Experimental results show that the proposed network can obtain better detail texture and image quality, and the PSNR of 4-fold image hyperfraction reconstruction is improved by 0.57 on average on five public datasets of Set5, Set14, BSD100, Urban100 and DIV2K.  © 2022 IEEE.","Convolution; Convolutional neural networks; Image enhancement; Image quality; Image reconstruction; Mean square error; Optical resolving power; Textures; Convolutional neural network; Image super-resolution reconstruction; Modelling framework; Multi-scales; Network training; Reconstruction algorithms; Residual network; Super-resolution reconstruction; Transformer; Transformer modeling; Generative adversarial networks","generative adversarial network; multi-scale; residual network; super-resolution reconstruction; Transformer","Conference paper","Final","","Scopus","2-s2.0-85146420811"
"Zadtootaghaj S.","Zadtootaghaj, Saman (57195276053)","57195276053","Future Challenges: Enhancement Techniques","2022","T-Labs Series in Telecommunication Services","","","","133","140","7","10.1007/978-3-030-98249-2_7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129772389&doi=10.1007%2f978-3-030-98249-2_7&partnerID=40&md5=28367cec5dc1322848f4562526bbb87f","This chapter discusses one of the critical future challenges, which is video quality measurement for video quality enhancement techniques. Enhancement techniques have already found their place in the market, with new TVs capable of improving the quality by super-resolution models or new codecs that aim at quality enhancement, e.g., LCEVC. In the gaming context, NVIDIA introduces the Dynamic Super Resolution (DSR) model that runs on Maxwell architectures. NVIDIA claims that it can offer a 4k resolution video gaming experience for 1080p generated (rendered) content. Therefore, this section will discuss the new enhancement codec (LCEVC) and enhancement techniques that might impact the quality assessment models for gaming streaming services. This section is written based on the results of two recent publications (Avanaki et al. Quality enhancement of gaming content using generative adversarial networks. In 2020 twelfth international conference on quality of multimedia experience (QoMEX). IEEE, New York, pp. 1–6, 2020; Barman et al., Evaluation of MPEG-5 Part 2 (LCEVC) for live gaming video streaming applications. In ACM Mile-High Video, 2022, submitted). © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Convolutional neural networks; Image quality; Image reconstruction; Motion Picture Experts Group standards; Network architecture; Optical resolving power; Quality control; Auto encoders; Codec; Convolutional neural network; Dataset; Generative adversarial network; LCEVC; Loss functions; Perceptual quality; Quality enhancement; Reconstruction accuracy; Subjective test; Super-resolution; Superresolution; Generative adversarial networks","Auto-encoders; Codec; Convolutional neural network (CNN); Dataset; Generative adversarial network (GAN); LCEVC; Loss function; Network architecture; Perceptual quality; Quality enhancement; Reconstruction accuracy; Subjective test; Super-resolution (SR)","Book chapter","Final","","Scopus","2-s2.0-85129772389"
"Montanaro A.; Valsesia D.; Magli E.","Montanaro, Antonio (57245937400); Valsesia, Diego (55968886600); Magli, Enrico (7003771643)","57245937400; 55968886600; 7003771643","EXPLORING THE SOLUTION SPACE OF LINEAR INVERSE PROBLEMS WITH GAN LATENT GEOMETRY","2022","Proceedings - International Conference on Image Processing, ICIP","","","","1381","1385","4","10.1109/ICIP46576.2022.9897211","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146709535&doi=10.1109%2fICIP46576.2022.9897211&partnerID=40&md5=430b8c546e5c4a27f6eaa3efaab23c46","Inverse problems consist in reconstructing signals from incomplete sets of measurements and their performance is highly dependent on the quality of the prior knowledge encoded via regularization. While traditional approaches focus on obtaining a unique solution, an emerging trend considers exploring multiple feasibile solutions. In this paper, we propose a method to generate multiple reconstructions that fit both the measurements and a data-driven prior learned by a generative adversarial network. In particular, we show that, starting from an initial solution, it is possible to find directions in the latent space of the generative model that are null to the forward operator, and thus keep consistency with the measurements, while inducing significant perceptual change. Our exploration approach allows to generate multiple solutions to the inverse problem an order of magnitude faster than existing approaches; we show results on image super-resolution and inpainting problems. © 2022 IEEE.","Computer vision; Differential equations; Generative adversarial networks; Data driven; Emerging trends; GAN; Linear inverse problems; Performance; Prior-knowledge; Reconstructing Signals; Regularisation; Solution space; Traditional approaches; Inverse problems","GANs; Inverse problems","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85146709535"
"Xiu J.; Qu X.; Yu H.","Xiu, Jie (57563364600); Qu, Xiujie (7202660844); Yu, Haowei (57564125600)","57563364600; 7202660844; 57564125600","Face Super-Resolution Using Recurrent Generative Adversarial Network","2022","IEEE 6th Information Technology and Mechatronics Engineering Conference, ITOEC 2022","","","","1169","1174","5","10.1109/ITOEC53115.2022.9734461","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127711943&doi=10.1109%2fITOEC53115.2022.9734461&partnerID=40&md5=9a5b5b5acdf1a32bfeabc3344c1319d0","Theface super-resolution (SR) networks based on deep learning have more advanced performance than traditional SR algorithms. However, facial key components are difficult to reconstruct because the adjacent pixels have great change. Moreover, most face SR networks only focus on the performance and ignore the number of parameters. To solve the above problems, we propose the face super-resolution network using recurrent generative adversarial network (FSRRGAN). The generator is the face SR recurrent generator (FSRRG) with dense iterative up-down sampling blocks as the basic unit. It can reduce the number of parameters and effectively improve the reconstruction performance combined with the relativistic average patch discriminator (RAPD). We use the facial perceptual similarity distance (FPSD) loss to replace the traditional perceptual loss. The experimental results show that our network has excellent performance both qualitatively and quantitatively on 4x and 8x face reconstruction. © 2022 IEEE.","Computer vision; Deep learning; Iterative methods; Optical resolving power; Signal sampling; Adjacent pixels; Basic units; Deep learning; Down sampling; Face super-resolution; Network-based; Perceptual quality; Performance; Super resolution algorithms; Superresolution; Generative adversarial networks","deep learning; face super-resolution; generative adversarial network; perceptual quality","Conference paper","Final","","Scopus","2-s2.0-85127711943"
"Li X.; Strasser B.; Neuberger U.; Vollmuth P.; Bendszus M.; Wick W.; Dietrich J.; Batchelor T.T.; Cahill D.P.; Andronesi O.C.","Li, Xianqi (57190585911); Strasser, Bernhard (56374930300); Neuberger, Ulf (57192420231); Vollmuth, Philipp (57222559959); Bendszus, Martin (7006493496); Wick, Wolfgang (7003287906); Dietrich, Jorg (7102567611); Batchelor, Tracy T. (7005788235); Cahill, Daniel P. (57204708663); Andronesi, Ovidiu C. (6506599452)","57190585911; 56374930300; 57192420231; 57222559959; 7006493496; 7003287906; 7102567611; 7005788235; 57204708663; 6506599452","Deep learning super-resolution magnetic resonance spectroscopic imaging of brain metabolism and mutant isocitrate dehydrogenase glioma","2022","Neuro-Oncology Advances","4","1","vdac071","","","","10.1093/noajnl/vdac071","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141706644&doi=10.1093%2fnoajnl%2fvdac071&partnerID=40&md5=48f0bb2a14537f16d5624840e8ddb957","Background: Magnetic resonance spectroscopic imaging (MRSI) can be used in glioma patients to map the metabolic alterations associated with IDH1,2 mutations that are central criteria for glioma diagnosis. The aim of this study was to achieve super-resolution (SR) MRSI using deep learning to image tumor metabolism in patients with mutant IDH glioma. Methods: We developed a deep learning method based on generative adversarial network (GAN) using Unet as generator network to upsample MRSI by a factor of 4. Neural networks were trained on simulated metabolic images from 75 glioma patients. The performance of deep neuronal networks was evaluated on MRSI data measured in 20 glioma patients and 10 healthy controls at 3T with a whole-brain 3D MRSI protocol optimized for detection of d-2-hydroxyglutarate (2HG). To further enhance structural details of metabolic maps we used prior information from high-resolution anatomical MR imaging. SR MRSI was compared to ground truth by Mann-Whitney U-test of peak signal-to-noise ratio (PSNR), structure similarity index measure (SSIM), feature-based similarity index measure (FSIM), and mean opinion score (MOS). Results: Deep learning SR improved PSNR by 17%, SSIM by 5%, FSIM by 7%, and MOS by 30% compared to conventional interpolation methods. In mutant IDH glioma patients proposed method provided the highest resolution for 2HG maps to clearly delineate tumor margins and tumor heterogeneity. Conclusions: Our results indicate that proposed deep learning methods are effective in enhancing spatial resolution of metabolite maps. Patient results suggest that this may have great clinical potential for image guided precision oncology therapy.  © 2022 The Author(s).","","D-2-hydroxyglutarate; deep learning; glioma; isocitrate dehydrogenase; magnetic resonance spectroscopic imaging; super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141706644"
"Jiao C.; Bi C.; Yang L.; Wang Z.; Xia Z.; Ono K.","Jiao, Chenyue (57959138900); Bi, Chongke (36801122300); Yang, Lu (57195471607); Wang, Zhen (57240216900); Xia, Zijun (57959139000); Ono, Kenji (35105793600)","57959138900; 36801122300; 57195471607; 57240216900; 57959139000; 35105793600","ESRGAN-based visualization for large-scale volume data","2022","Journal of Visualization","","","","","","","10.1007/s12650-022-00891-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141638478&doi=10.1007%2fs12650-022-00891-2&partnerID=40&md5=f859a0efb7a7d48b100530225ebe4f0c","Abstract: In the scientific visualization and data analysis workflow, data transmission bandwidth and memory resources have become the main bottlenecks when handling large-scale volume data. As a direct and effective scheme, data reduction is generally used to decrease data movement overhead and memory usage. However, it is still a challenge to obtain visualization results from reduced data without losing too many features. This paper proposes a visualization scheme for large-scale volume data based on enhanced super-resolution generative adversarial networks (ESRGAN) and designs a reduction-restoration workflow. Firstly, in order to reduce memory footprint, we propose an error-controlled data reduction method to delete data in 3D space, which is based on octree. Secondly, rendered images with loss of details are generated by performing volume rendering on reduced data. Lastly, to obtain feature-lossless visualization results, we apply ESRGAN to restore the details of rendered images. Based on the above scheme, the dual goals of data reduction and visual feature retention can be realized. Finally, the effectiveness of the proposed method is demonstrated by evaluating the performance of data reduction and visual restoration. Graphical abstract: [Figure not available: see fulltext.] © 2022, The Visualization Society of Japan.","","Data reduction; ESRGAN; large-scale volume data; visualization","Article","Article in press","","Scopus","2-s2.0-85141638478"
"Zhao S.; Sun J.; Ou H.; Lin Y.","Zhao, Shuai (57302071500); Sun, Jifeng (56005259700); Ou, Hongshi (57202709628); Lin, Yibin (57226879197)","57302071500; 56005259700; 57202709628; 57226879197","A Novel Multi-Task Face Super-Resolution Framework Embedding Degraded Augmented GAN Networks","2022","Journal of Physics: Conference Series","2303","1","012061","","","","10.1088/1742-6596/2303/1/012061","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135294592&doi=10.1088%2f1742-6596%2f2303%2f1%2f012061&partnerID=40&md5=9a2daca35685a4b362b8a2ccc1460dc3","The training method that uses predefined degradation methods to provide training data for deep convolutional neural networks suffers from overfitting and image distortion when performing face super-resolution reconstruction. In this paper, we propose a multi-task facial super-resolution reconstruction framework embedded in a degraded augmented GAN network. Compared with the commonly used method of generating low-quality face images through pixel interpolation, this paper inversely embeds a generative adversarial network composed of residual coding blocks in the network. Through the feature learning ability of the network, the feature fitting of the face images collected in the natural scene is performed to generate training data that can more fully represent the real noise in the natural scene, thereby improving the noise reduction and image reconstruction capabilities of the network. In order to take full advantage of the task-specific features, the semantic features of the dataset are extracted through a pre-trained facial organ semantic detection framework and the data is used to add face semantic information in the super-resolution reconstruction network. The experimental results show that the model proposed in this study can achieve super-resolution reconstruction of ultra-low-resolution face images simply and efficiently, and achieve super-resolution face images with lower visual quality than other advanced methods with lower model complexity. It solves the problem of single style and distortion of face reconstruction images caused by the training method of the data set obtained by the usual predefined degradation. © Published under licence by IOP Publishing Ltd.","Deep neural networks; Generative adversarial networks; Image coding; Image enhancement; Noise abatement; Optical resolving power; Semantics; Convolutional neural network; Embeddings; Face images; Face super-resolution; Multi tasks; Natural scenes; Overfitting; Super-resolution reconstruction; Training data; Training methods; Image reconstruction","","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135294592"
"Xin Y.; Zhu F.; Shi P.; Yang X.; Zhou R.","Xin, Yuanxue (57188986953); Zhu, Fengting (57545145900); Shi, Pengfei (57213385715); Yang, Xin (57778409300); Zhou, Runkang (57545146000)","57188986953; 57545145900; 57213385715; 57778409300; 57545146000","Super-Resolution Reconstruction Algorithm of Images Based on Improved Enhanced Super-Resolution Generative Adversarial Network; [基 于 改 进 增 强 型 超 分 辨 率 生 成 对 抗 网 络 的 图 像超 分 辨 率 重 建 算 法]","2022","Laser and Optoelectronics Progress","59","5","0420002","","","","10.3788/LOP202259.0420002","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126987139&doi=10.3788%2fLOP202259.0420002&partnerID=40&md5=3280a19758324f10e02250036dc5f4cf","To solve the problem of insufficient detail processing in the existing image super-resolution reconstruction algorithm, a super-resolution reconstruction algorithm of images based on improved enhanced super-resolution generative adversarial network (ESRGAN) is proposed. Firstly, the deep information extraction module of the improved ESRGAN generation network is improved using multiscale dense block (MDB) instead of dense block (DB), and by adding channel attention mechanism after MDB to adjust the characteristic response values of different channels. Secondly, the shallow feature extraction module of the improved ESRGAN model is used to extract the original features of the low resolution images, and the deep information extraction module is used to extract the depth residual features of the low resolution images. The original features and the depth residual features are fused by adding the corresponding elements. Finally, the reconstruction module is used to complete the image super-resolution reconstruction. The proposed algorithm’s two and four times super-resolution reconstructions are tested on Set5, Set14, and BSD100 datasets and compared to Bicubic, FSRCNN, and ESRGAN methods. The results show that the proposed algorithm’s reconstructed image has a clearer edge, and it can provide more details, which greatly improves the image’s visual effect. Compared to ESRGAN, the proposed algorithm improves the average peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) of 2-fold super-resolution reconstruction images by 0. 467 dB and 0. 005, respectively; At the same time, the proposed algorithm improves the average PSNR and SSIM of 4-fold super-resolution reconstruction images by 0. 438 dB and 0. 015, respectively. © 2022 Universitat zu Koln. All rights reserved.","","Attention mechanism; Generative adversarial network; Image processing; Multi-scale dense connection; Super-resolution","Article","Final","","Scopus","2-s2.0-85126987139"
"Huang Y.; Wang Q.; Omachi S.","Huang, Yongsong (57214838902); Wang, Qingzhong (57209217850); Omachi, Shinichiro (35501320300)","57214838902; 57209217850; 35501320300","Rethinking Degradation: Radiograph Super-Resolution via AID-SRGAN","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13583 LNCS","","","43","52","9","10.1007/978-3-031-21014-3_5","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144815519&doi=10.1007%2f978-3-031-21014-3_5&partnerID=40&md5=e60e4160b05f60b948044d14194b0518","In this paper, we present a medical AttentIon Denoising Super Resolution Generative Adversarial Network (AID-SRGAN) for diographic image super-resolution. First, we present a medical practical degradation model that considers various degradation factors beyond downsampling. To the best of our knowledge, this is the first composite degradation model proposed for radiographic images. Furthermore, we propose AID-SRGAN, which can simultaneously denoise and generate high-resolution (HR) radiographs. In this model, we introduce an attention mechanism into the denoising module to make it more robust to complicated degradation. Finally, the SR module reconstructs the HR radiographs using the “clean” low-resolution (LR) radiographs. In addition, we propose a separate-joint training approach to train the model, and extensive experiments are conducted to show that the proposed method is superior to its counterparts. e.g., our proposed method achieves 31.90 of PSNR with a scale factor of 4 ×, which is 7.05% higher than that obtained by recent work, SPSR [16]. Our dataset and code will be made available at: https://github.com/yongsongH/AIDSRGAN-MICCAI2022. © 2022, Springer Nature Switzerland AG.","Generative adversarial networks; Optical resolving power; Radiography; De-Noise; De-noising; Degradation factor; Degradation model; Down sampling; High resolution; Image super resolutions; Musculoskeletal radiograph; Radiographic images; Superresolution; Medical imaging","Musculoskeletal radiographs; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85144815519"
"Kim D.-H.; Lee J.-W.; Park S.-H.","Kim, Dong-Hwi (56975581300); Lee, Jun-Won (57641592200); Park, Sang-Hyo (55362514700)","56975581300; 57641592200; 55362514700","A Study on Model Compression Methods for SRGAN","2022","2022 International Conference on Electronics, Information, and Communication, ICEIC 2022","","","","","","","10.1109/ICEIC54506.2022.9748707","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128844325&doi=10.1109%2fICEIC54506.2022.9748707&partnerID=40&md5=f1cda52461f7eaab61bdb29e7d15479f","The construction of SR algorithms by using deep learning model such as super-resolution generative adversarial networks (SRGAN) have become larger and complicated model architectures with requiring a vast amount of memory capacity. However, it is difficult to operate deep learning models which have millions of parameters at the mobile devices. Thus, in this paper, we present a study on lightweight neural network using network pruning method. Through our extensive experiments, pruned network can show similar performance to the original SRGAN model with substantially reduced model size.  © 2022 IEEE.","Computer vision; Deep learning; Distillation; Generative adversarial networks; Deep learning; Fine tuning; Knowledge-distillation; Learning models; Lightweight neural network; Network compression; Network pruning; Neural-networks; Study on models; Superresolution; Optical resolving power","Deep learning; Fine-tuning; Knowledge-distillation; Lightweight neural network; Network compression; Network pruning; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85128844325"
"Aggarwal A.; Bansal M.; Verma T.; Sood A.","Aggarwal, Anirudh (57700358600); Bansal, Mohit (16466939600); Verma, Tanishq (57699109100); Sood, Apoorvi (56126781100)","57700358600; 16466939600; 57699109100; 56126781100","Residual in Residual Cascade Network for Single-Image Super Resolution","2022","Lecture Notes in Networks and Systems","392","","","335","346","11","10.1007/978-981-19-0619-0_30","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130369777&doi=10.1007%2f978-981-19-0619-0_30&partnerID=40&md5=a8e37d3bb14c0300b69485a1ede3f349","Digital images are of great significance in today’s scenario, where almost every single domain relies on them from agriculture to businesses to the military for their specific purposes. To convey the context, a more precise image gives more idealization and personalization to the aspect one wants to build. The precise images connote high clarity and distinct characteristics which is represented by a parameter known as resolution of image. This paper focuses on upscaling the resolution of image using a fast, lightweight artificial neural network (ANN)—RRCasN. The proposed model surpasses the traditional methods and competes with modern ANN-based architectures in terms of size of the model and quality of the high-resolution image produced. This paper also introduces a novel approach for normalization in place of batch normalization. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Artificial neural networks; Generative adversarial networks; Image processing; Residual networks; Resolution enhancement; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85130369777"
"","","","28th International Conference on MultiMedia Modeling, MMM 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13141 LNCS","","","","","1222","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127057690&partnerID=40&md5=2b7bd38618d427bdf703aa26e1eb34ec","The proceedings contain 107 papers. The special focus in this conference is on MultiMedia Modeling. The topics include: Non-Uniform Attention Network for Multi-modal Sentiment Analysis; combining Knowledge and Multi-modal Fusion for Meme Classification; bi-attention Modal Separation Network for Multimodal Video Fusion; Melody Generation from Lyrics Using Three Branch Conditional LSTM-GAN; a-Muze-Net: Music Generation by Composing the Harmony Based on the Generated Melody; Speech Intelligibility Enhancement By Non-Parallel Speech Style Conversion Using CWT and iMetricGAN Based CycleGAN; time-Frequency Attention for Speech Emotion Recognition with Squeeze-and-Excitation Blocks; Real-Time FPGA Design for OMP Targeting 8K Image Reconstruction; one-Stage Image Inpainting with Hybrid Attention; MF-GAN: Multi-conditional Fusion Generative Adversarial Network for Text-to-Image Synthesis; fast Single Image Dehazing Using Morphological Reconstruction and Saturation Compensation; arbitrary Style Transfer with Adaptive Channel Network; point Cloud Upsampling via a Coarse-to-Fine Network; JVCSR: Video Compressive Sensing Reconstruction with Joint In-Loop Reference Enhancement and Out-Loop Super-Resolution; SAM: Self Attention Mechanism for Scene Text Recognition Based on Swin Transformer; A Multiple Positives Enhanced NCE Loss for Image-Text Retrieval; multimodal Embedding for Lifelog Retrieval; prediction of Blood Glucose Using Contextual LifeLog Data; fall Detection Using Multimodal Data; an Investigation into Keystroke Dynamics and Heart Rate Variability as Indicators of Stress; PF-VTON: Toward High-Quality Parser-Free Virtual Try-On Network; joint Re-Detection and Re-Identification for Multi-Object Tracking; Multi-scale Cross-Modal Transformer Network for RGB-D Object Detection; A Complementary Fusion Strategy for RGB-D Face Recognition; double Granularity Relation Network with Self-criticism for Occluded Person Re-identification; Using Explainable AI to Identify Differences Between Clinical and Experimental Pain Detection Models Based on Facial Expressions; rating-Aware Self-Organizing Maps; preface.","","","Conference review","Final","","Scopus","2-s2.0-85127057690"
"Zhang Z.; Chen J.; Xu X.; Liu C.; Han Y.","Zhang, Zichao (57203099484); Chen, Jian (56383105500); Xu, Xinyu (57904642600); Liu, Cunjia (57196115942); Han, Yu (57905242200)","57203099484; 56383105500; 57904642600; 57196115942; 57905242200","Hawk-eye-inspired perception algorithm of stereo vision for obtaining orchard 3D point cloud navigation map","2022","CAAI Transactions on Intelligence Technology","","","","","","","10.1049/cit2.12141","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138734614&doi=10.1049%2fcit2.12141&partnerID=40&md5=c2bd9baedea95d202a79735d36d89581","The binocular stereo vision is the lowest cost sensor for obtaining 3D information. Considering the weakness of long-distance measurement and stability, the improvement of accuracy and stability of stereo vision is urgently required for application of precision agriculture. To address the challenges of stereo vision long-distance measurement and stable perception without hardware upgrade, inspired by hawk eyes, higher resolution perception and the adaptive HDR (High Dynamic Range) were introduced in this paper. Simulating the function from physiological structure of ‘deep fovea’ and ‘shallow fovea’ of hawk eye, the higher resolution reconstruction method in this paper was aimed at accuracy improving. Inspired by adjustment of pupils, the adaptive HDR method was proposed for high dynamic range optimisation and stable perception. In various light conditions, compared with default stereo vision, the accuracy of proposed algorithm was improved by 28.0% evaluated by error ratio, and the stability was improved by 26.56% by disparity accuracy. For fixed distance measurement, the maximum improvement was 78.6% by standard deviation. Based on the hawk-eye-inspired perception algorithm, the point cloud of orchard was improved both in quality and quantity. The hawk-eye-inspired perception algorithm contributed great advance in binocular 3D point cloud reconstruction in orchard navigation map. © 2022 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.","Generative adversarial networks; Orchards; Stereo image processing; 3D point cloud; Adaptive high dynamic range; Binocular-stereo visions; Hawk-eye-inspired perception; High dynamic range; Navigation map; Point cloud of orchard; Point-clouds; Super-resolution generative adversarial network; Superresolution; Stereo vision","adaptive high dynamic range; binocular stereo vision; hawk-eye-inspired perception; point cloud of orchard; super-resolution generative adversarial network","Article","Article in press","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85138734614"
"Bi X.; Guo K.; He J.; Jing X.","Bi, Xingjian (57980528800); Guo, Kaiyi (57980528900); He, Jingze (57980342500); Jing, Xiangyu (57980342600)","57980528800; 57980528900; 57980342500; 57980342600","Application and Review of Generative Adversarial Networks","2022","Proceedings of SPIE - The International Society for Optical Engineering","12348","","1234809","","","","10.1117/12.2641605","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142507303&doi=10.1117%2f12.2641605&partnerID=40&md5=7f2ca7d4c18651944ff1b28e67f82bfc","Generative adversarial networks (GANs) are one of the most popular innovations in machine learning. GANs provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a process involving a pair of networks. GANs are generative models since they are able to create data instances that resemble the training data. Besides, GANs provide a way to learn deep representations without annotated training data. They achieve this by deriving backpropagation signals through a looping process involving multiple networks. Those representations learned by GANs can be used in various fields such as Image Generation, Abnormal Detection, Video Repair, using GAN for Infrared to RGB, Image Inpainting, etc. This review paper provides a clear overview of GANs and their application into the Image Inpainting process. Furthermore, it points out both the advantages and disadvantages of GANs in the machine learning field. © 2022 SPIE. All rights reserved.","Backpropagation; Image processing; Annotated training data; Face synthesis; Generative model; Image Inpainting; Image super resolutions; Learn+; Machine-learning; Multiple networks; Style transfer; Generative adversarial networks","Face Synthesis; Generative Adversarial Networks; Image Super Resolution; NLP; Style Transfer","Conference paper","Final","","Scopus","2-s2.0-85142507303"
"Hanano T.; Seo M.; Chen Y.-W.","Hanano, Tatsuya (57426895600); Seo, Masataka (35280835900); Chen, Yen-Wei (56036268200)","57426895600; 35280835900; 56036268200","Accurate and Efficient Generation of High-Resolution Facial Expression Images by Multi-Task Learning Using Generative Adversarial Networks","2022","GCCE 2022 - 2022 IEEE 11th Global Conference on Consumer Electronics","","","","765","768","3","10.1109/GCCE56475.2022.10014139","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147257325&doi=10.1109%2fGCCE56475.2022.10014139&partnerID=40&md5=20424052cd4ecdd75982892517f276df","Recently, the means to see human face images have increased owing to the spread of smartphones and social networking services. Especially, in the field of face images, the generation of face images using facial expression transformation has already been realized using deep learning-based approaches. However, in the existing deep learning-based models, only low-resolution images can be generated owing to limited computational resources, and the generated images are blur or aliasing. To solve this problem, we proposed to enhance the resolution of the generated facial image by using a super resolution network after the generative model, which can be considered as a serial model in our previous work. In this paper, we newly proposed a parallel model, which train the generative adversarial network and the super-resolution network through multi-task learning. Using the peak signal-to-noise ratio as an evaluation index, image quality was improved by 0.29 dB for the male test data and by 0.24 dB for the female test data compared with the our previous serial model. © 2022 IEEE.","Computer vision; Deep learning; Image enhancement; Learning systems; Optical resolving power; Signal to noise ratio; Face images; Facial Expressions; High resolution; Human face image; Multitask learning; Parallel models; Pix2pix; Serial models; Superresolution; Test data; Generative adversarial networks","Generative Adversarial Nets; multi-task learning; parallel model; Pix2Pix; serial model","Conference paper","Final","","Scopus","2-s2.0-85147257325"
"Khoo J.J.D.; Lim K.H.; Pang P.K.","Khoo, John Julius Danker (57221834507); Lim, King Hann (25031784300); Pang, Po Ken (57209463435)","57221834507; 25031784300; 57209463435","Deep Learning Super Resolution of Sea Surface Temperature on South China Sea","2022","2022 International Conference on Green Energy, Computing and Sustainable Technology, GECOST 2022","","","","176","180","4","10.1109/GECOST55694.2022.10010371","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147041003&doi=10.1109%2fGECOST55694.2022.10010371&partnerID=40&md5=b0456d18133bc085bfce74991040fd90","Surface temperature is one of the key observations to analyse the greenhouse effect on the Earth. The surface of the ocean can be captured using satellite sensors and transmitted to a meteorological center for real-time analysis. The use of the deep learning paradigm in super resolution has its potential in geoscience applications to increase the data transmission latency and enhance low-quality observation from remote sensing data. In this paper, the deployment of Generative Adversarial Network (GAN) architecture is studied to apply resolution reconstruction using the South China Sea sea surface temperature data. In addition, the development of spectral normalization is added to the Enhanced Super Resolution Generative Adversarial Network (ESRGAN) architecture to improve the training mechanism of generator and discriminator. This improved ESRGAN is compared with its super resolution performance against peak signal-to-noise ratio and structural similarity index evaluation metrics. The experiment shows that the low resolution of South China Sea data can be inferred to obtain a higher resolution with a more realistic resolution as compared to the conventional upsampling approaches.  © 2022 IEEE.","Atmospheric temperature; Deep learning; Generative adversarial networks; Greenhouse effect; Network architecture; Oceanography; Optical resolving power; Remote sensing; Signal to noise ratio; Submarine geophysics; Surface waters; Deep learning; Geoscience applications; Learning paradigms; Real time analysis; Satellite sensors; Sea surface temperature; Sea surfaces; South China sea; Superresolution; Surface temperatures; Surface temperature","Deep Learning; Generative Adversarial Networks; Sea Surface Temperature; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85147041003"
"Yan Y.; Nguyen B.T.; Geng Y.; Iwai K.; Nishiura T.","Yan, Yanqiao (58065401000); Nguyen, Binh Thien (57215929341); Geng, Yuting (57213195315); Iwai, Kenta (36760895800); Nishiura, Takanobu (7103179225)","58065401000; 57215929341; 57213195315; 36760895800; 7103179225","Phase-Aware Audio Super-resolution for Music Signals Using Wasserstein Generative Adversarial Network","2022","Proceedings of 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2022","","","","1673","1677","4","10.23919/APSIPAASC55919.2022.9980298","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146262703&doi=10.23919%2fAPSIPAASC55919.2022.9980298&partnerID=40&md5=8d44c571d35deda52ea98fde2ea3686b","Audio super-resolution (ASR) is a complicated task for generating a high-resolution audio signal from a low-resolution signal. To solve this problem, we propose an ASR system for music signals that involves using deep neural networks in the time-frequency domain. The system has two components: a Wasserstein generative adversarial network-based high frequency magnitude generation model and a fully connected network-based corresponding high frequency band phase estimation model. The conventional high frequency band phase estimation methods require large computational complexity, have slow convergence, and reconstruct low quality high-resolution signals. We compare our proposed high frequency band phase estimation model in the ASR system with conventional phase estimation methods. The results show that our proposed phase estimation model outperforms conventional methods in objective evaluations.  © 2022 Asia-Pacific of Signal and Information Processing Association (APSIPA).","Audio acoustics; Deep neural networks; Frequency domain analysis; Frequency estimation; Music; Optical resolving power; Audio signal; Estimation methods; Estimation models; High frequency bands; High-resolution audio; Music signals; Network-based; Phase-estimation; Resolution systems; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85146262703"
"Madi B.; Alaasam R.; El-Sana J.","Madi, Boraq (57219761909); Alaasam, Reem (57204568418); El-Sana, Jihad (6603056280)","57219761909; 57204568418; 6603056280","Text Edges Guided Network for Historical Document Super Resolution","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13639 LNCS","","","18","33","15","10.1007/978-3-031-21648-0_2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144433364&doi=10.1007%2f978-3-031-21648-0_2&partnerID=40&md5=3741468bd4374cfb8e8069d239acf78c","Super-resolution aims to increase the resolution and the clarity of the details in low-resolution images, and document images are no exception. Although significant improvements have been achieved in super-resolution for different domains, historical document images have not been addressed well. Most of the current works in the text domain deal with modern fonts and rely on extracting prior semantic information from a recognizer to super-resolve images. The absence of a reliable handwritten recognizer for Arabic documents, where historical documents have a complex structure and overlapping parts, makes these text-domain works inapplicable. This paper presents a Text-Attention-ed Super Resolution GAN (TASR-GAN) to address this problem. The model deals with historical Arabic documents and does not rely on prior semantic information. Since our input domain documents, text edges are essential for quality and readability; thus, we introduce a new loss function called text edge loss. This loss function provides more attention and weight to text edge information and guides through optimization to super-resolve images with accurate small regions’ details and fine edges to improve image quality. Experiments on six Arabic manuscripts show that the proposed TASR achieves state-of-the-art performance in terms of PSNR/SSIM metrics and significantly improves the visual image quality, mainly the edges of small regions details, and eliminates artifacts noises. Also, a grid search experiment has been conducted to tune the best hyperparameters values for our text edge loss function. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Character recognition; Generative adversarial networks; Image enhancement; Optical resolving power; Semantics; Arabic document; Document images; Handwritten document; Historical documents; Historical handwritten document; Loss functions; Low resolution images; Semantics Information; Small region; Superresolution; Image quality","Generative adversarial networks; Historical handwritten documents; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85144433364"
"Wang J.; Shao Z.; Huang X.; Lu T.; Zhang R.; Li Y.","Wang, Jiaming (57206676342); Shao, Zhenfeng (7202244409); Huang, Xiao (57201292422); Lu, Tao (56406646300); Zhang, Ruiqian (57190385256); Li, Yong (57223768875)","57206676342; 7202244409; 57201292422; 56406646300; 57190385256; 57223768875","From Artifact Removal to Super-Resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5627715","","","","10.1109/TGRS.2022.3196709","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135754017&doi=10.1109%2fTGRS.2022.3196709&partnerID=40&md5=2ab1d77f49794834081a3f5fa02221d3","Deep-learning-based super-resolution (SR) methods have been extensively studied and have achieved significant performance with deep convolutional neural networks. However, the results still suffer from the ringing effect, especially in satellite image SR tasks, due to the loss of image details in the satellite degradation process. In this article, we build a novel satellite SR framework by decomposing a high-resolution image into three components, i.e., low-resolution (LR), artifact, and high-frequency information. Specifically, we propose an artifact removal network with a self-adaption difference convolution (SDC) to fully exploit the structure prior in the LR image and predict the artifact map. Considering that the artifact map and the high-frequency map share a similar pattern, we introduce the supervised structure correction (SSC) block that establishes a bridge between the high-frequency generation process and the artifact removal process. Experimental results on satellite images demonstrate that the proposed method owns an improved tradeoff between the performance and the computational cost compared to existing state-of-the-art satellite and natural SR methods. The source code is available at https://github.com/jiaming-wang/ARSRN.  © 1980-2012 IEEE.","Convolution; Deep neural networks; Edge detection; Generative adversarial networks; Image enhancement; Job analysis; Optical resolving power; Remote sensing; Satellites; Artifact removal; Difference convolution; Image edge detection; Images reconstruction; Performance; Remote-sensing; Superresolution; Superresolution methods; Task analysis; artificial neural network; image analysis; image resolution; remote sensing; satellite data; satellite imagery; Image reconstruction","Artifact removal; difference convolution; remote sensing; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85135754017"
"Fang S.; Guo Q.; Cao Y.; Zhang J.","Fang, Shuai (7402422537); Guo, Qing (57938173300); Cao, Yang (57022583200); Zhang, Jing (57211055913)","7402422537; 57938173300; 57022583200; 57211055913","A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","891","894","3","10.1109/IGARSS46834.2022.9883547","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140364937&doi=10.1109%2fIGARSS46834.2022.9883547&partnerID=40&md5=ca6a8cca3ffe4be9f310ce7f1b26e2b0","Remote sensing image spatiotemporal fusion (STF) algorism plays an important role by supplementing the lack of original high-resolution remote sensing satellite images in the study scenarios of dense time-series data. In recent years, the deep-learning-based STF algorithm has become a research hotspot with comparatively higher accuracy and robustness. However, due to the lack of sufficient high-quality images for training and the huge resolution gap between low-resolution images and high-resolution images, it is difficult to recover detailed information, especially for areas of land-cover change. In this paper, we propose a two-layers super-resolution based generation adversarial spatiotemporal fusion model(TLSRSTF) using smaller inputs to reduce pressure on data requirements and a mutual affine convolution to reduce model parameters. Specifically, we only use a pair of high-resolution and low-resolution images and a high-resolution image at any time. A spatial degradation consistency is constructed to adaptively determine the ratio of two layers of the super-resolution STF model. The quantitative and qualitative experimental results on public spatiotemporal fusion datasets demonstrate our superiority over the state-of-the-art methods. © 2022 IEEE.","Convolution; Deep learning; Image fusion; Optical resolving power; Remote sensing; Fusion model; High resolution remote sensing; High-resolution images; Low resolution images; Mutual affine convolution; Remote sensing images; Remote sensing satellites; Spatio-temporal fusions; Superresolution; Two-layer; Generative adversarial networks","Generative Adversarial Networks(GAN); Mutual Affine Convolution; Spatiotemporal Fusion","Conference paper","Final","","Scopus","2-s2.0-85140364937"
"Meng Y.; Li W.; Lei S.; Zou Z.; Shi Z.","Meng, Yapeng (57984756800); Li, Wenyuan (57204784272); Lei, Sen (57195618353); Zou, Zhengxia (56073977200); Shi, Zhenwei (23398841900)","57984756800; 57204784272; 57195618353; 56073977200; 23398841900","Large-Factor Super-Resolution of Remote Sensing Images With Spectra-Guided Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5634111","","","","10.1109/TGRS.2022.3222360","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142837106&doi=10.1109%2fTGRS.2022.3222360&partnerID=40&md5=7e95902a868ae82cb77d9c293a163791","Large-factor image super-resolution (SR) is a challenging task due to the high uncertainty and incompleteness of the missing details to be recovered. In remote sensing images, the subpixel spectral mixing and semantic ambiguity of ground objects make this task even more challenging. In this article, we propose a novel method for large-factor SR of remote sensing images named spectra-guided generative adversarial networks (SpecGANs). In response to the above problems, we explore whether introducing additional hyperspectral images (HSIs) to GAN as conditional input can be the key to solving the problems. Different from previous approaches that mainly focus on improving the feature representation of a single source input, we propose a dual-branch network architecture to effectively fuse low-resolution (LR) red, green, blue (RGB) images and corresponding HSIs, which fully exploit the rich hyperspectral information as conditional semantic guidance. Due to the spectral specificity of ground objects, the semantic accuracy of the generated images is guaranteed. To further improve the visual fidelity of the generated output, we also introduce the Latent Code Bank with rich visual priors under a generative adversarial training framework so that high-resolution, detailed, and realistic images can be progressively generated. Extensive experiments show the superiority of our method over the state-of-art image SR methods in terms of both quantitative evaluation metrics and visual quality. Ablation experiments also suggest the necessity of adding spectral information and the effectiveness of our designed fusion module. To our best knowledge, we are the first to achieve up to 32x SR of remote sensing images with high visual fidelity under the premise of accurate ground object semantics. Our code can be publicly available at https://github.com/YapengMeng/SpecGAN. © 1980-2012 IEEE.","Computer vision; Deep neural networks; Generative adversarial networks; Hyperspectral imaging; Image enhancement; Network architecture; Optical resolving power; Quality control; Remote sensing; Semantics; Spectroscopy; Uncertainty analysis; Convolutional neural network; Deep convolutional neural network; Ground objects; HyperSpectral; Hyperspectral image; Images reconstruction; Remote sensing images; Superresolution; Task analysis; artificial neural network; image resolution; remote sensing; satellite imagery; spectral analysis; Image reconstruction","Deep convolutional neural networks (CNNs); generative adversarial networks (GANs); hyperspectral image (HSI); remote sensing image; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85142837106"
"Jia M.; Lu M.; Sang Y.","Jia, Mei (57863547000); Lu, Mingde (57747727400); Sang, Yu (36824201500)","57863547000; 57747727400; 36824201500","Advanced Generative Adversarial Network for Image Superresolution","2022","Communications in Computer and Information Science","1628 CCIS","","","193","208","15","10.1007/978-981-19-5194-7_15","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136807170&doi=10.1007%2f978-981-19-5194-7_15&partnerID=40&md5=3f944a02e4a2fbc85da443164ab7b2e4","The superresolution (SR) method based on generative adversarial networks (GANs) cannot adequately capture enough diversity from training data, resulting in misalignment between input low resolution (LR) images and output high resolution (HR) images. GAN training has difficulty converging. Based on this, an advanced GAN-based image SR reconstruction method is presented. First, the dense connection residual block and attention mechanism are integrated into the GAN generator to improve high-frequency feature extraction. Meanwhile, an added discriminator is added into the GAN discriminant network, which forms a dual discriminator to ensure that the process of training is stable. Second, the more robust Charbonnier loss is used instead of the mean square error (MSE) loss to compare similarities between the obtained image and actual image, and the total variation (TV) loss is employed to smooth the training results. Finally, the experimental results indicate that global structures can be better reconstructed using the method of this paper and texture details of images compared with other SOTA methods. The peak signal-to-noise ratio (PSNR) values by the method of this paper are improved by an average of 2.24 dB, and the structural similarity index measure (SSIM) values are improved by an average of 0.07. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Discriminators; Image reconstruction; Mean square error; Optical resolving power; Signal to noise ratio; Textures; Attention mechanisms; Dual discriminator; Generative adversarial network; Image super resolutions; Low resolution images; Residual dense block; Superresolution; Superresolution methods; Training data; Generative adversarial networks","Attention mechanism; Dual discriminator; Generative adversarial networks (GANs); Residual dense block; Superresolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85136807170"
"Fu Y.; Zhang X.; Wang M.","Fu, Yujia (57776361900); Zhang, Xiangrong (37076469400); Wang, Mingyang (57919947700)","57776361900; 37076469400; 57919947700","Super-Resolution Reconstruction of Remote Sensing Images Using Generative Adversarial Network with Shallow Information Enhancement","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","8529","8540","11","10.1109/JSTARS.2022.3209819","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139491442&doi=10.1109%2fJSTARS.2022.3209819&partnerID=40&md5=f967887b97beff71766faa594138489f","The super-resolution (SR) reconstruction method based on deep learning can significantly improve the spatial SR of remote sensing images. However, the current methods make insufficient use of the remote context information and channel information in shallow feature extraction, resulting in the limited effect of SR reconstruction. This article proposed a new SR reconstruction model, SIEGAN, which uses generative adversarial network with shallow information enhancement to improve the effect of SR reconstruction of remote sensing images. Similar to other generative adversarial models, SIEGAN is composed of generator and discriminator. But SIEGAN enhances the generator's ability to extract shallow information by using three different scale convolution operations. Specifically, a depthwise convolution is used to extract the local context information of each band of the image. A depthwise dilation convolution is used to capture the remote context information in the image. Finally, a 1×1 convolution is used to extract the correlation features between different channels in remote sensing images. In addition, SIEGAN uses U-Net network as its discriminator to provide detailed feedback per pixel to the generator to improve the model's ability to identify image details. And the spectral-spatial total variation loss function is introduced to ensure the spectral-spatial reliability of the reconstructed images. The experimental results on Gaofen-1 data proved that compared with the state-of-the-art models, SIEGAN has achieved better SR reconstruction performance. Furthermore, the reconstructed images by SIEGAN demonstrate better performance in land cover classification.  © 2008-2012 IEEE.","Convolution; Data mining; Deep learning; Extraction; Feature extraction; Generative adversarial networks; Image enhancement; Information use; Optical resolving power; Remote sensing; Semantics; Brain modeling; Context information; Features extraction; Images reconstruction; Multi-scale shallow information; Multi-scales; Remote sensing images; Remote-sensing; Super-resolution reconstruction; Superresolution; image classification; image resolution; network analysis; numerical model; remote sensing; satellite data; satellite imagery; spatial resolution; Image reconstruction","Generative adversarial network (GAN); multiscale shallow information; remote sensing images; super-resolution (SR) reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139491442"
"Sun L.; Zhao Y.","Sun, Lihui (16507691300); Zhao, Yiyou (57872336300)","16507691300; 57872336300","Research on Infrared Image Super-Resolution Based on Enhanced Generative Adversarial Network","2022","Proceedings - 2022 International Conference on Computer Engineering and Artificial Intelligence, ICCEAI 2022","","","","334","338","4","10.1109/ICCEAI55464.2022.00076","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137181838&doi=10.1109%2fICCEAI55464.2022.00076&partnerID=40&md5=9443183c6a2e9d97598e9db3445542f3","Infrared image super-resolution reconstruction is an effective way to improve the quality of infrared images. Aiming at the problem that the reconstructed image of the enhanced generative adversarial network Esrgan is prone to artifacts, This paper proposes a multi-branch enhanced generative adversarial network model, which adds an attention mechanism module and a residual module to the backbone of the original enhanced generative adversarial network model generator. The attention mechanism module strengthens the learning of useful channel information and suppresses useless channel information through the learning of global channel information. The residual module is a stack of the original Esrgan model block, which is the normalization of the channel dimension and retains the high-frequency features of the image. Experiments show that, compared with the original Esrgan model, the image texture details generated by the improved model are clearer and the resolution is higher, and the PSNR and SSIM of the image are significantly improved. © 2022 IEEE.","Generative adversarial networks; Image enhancement; Image reconstruction; Image texture; Infrared imaging; Learning systems; Optical resolving power; Attention mechanisms; Channel; Channel information; Detail texture; Esrgan; Image super resolutions; Image super-resolution reconstruction; Network models; Residual module; Super-resolution reconstruction; Textures","Attention mechanism; Channel; Detail texture; Esrgan; Residual module; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85137181838"
"An H.; He L.; Hou Z.; Lai D.","An, Hongyu (57913264200); He, Li (57211949869); Hou, Zhongwei (57912614600); Lai, Dakun (56285568200)","57913264200; 57211949869; 57912614600; 56285568200","Terahertz Image Super-Resolution Reconstruction Using Unpaired Real-World Images","2022","2022 3rd International Conference on Pattern Recognition and Machine Learning, PRML 2022","","","","193","198","5","10.1109/PRML56267.2022.9882221","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139165576&doi=10.1109%2fPRML56267.2022.9882221&partnerID=40&md5=090efb8d8936cdf84bfb750b812bb218","Terahertz imaging technology has been widely used in applications of security detection, defect detection and biomedicine. The in- sufficient resolution is still the main defect of this technology so far. The use of deep neural network for super-resolution reconstruction of image is a mainstream enhancement of resolution methods. Aiming at solving the shortcomings of existing methods to obtaining training sets and the design of networks, this paper proposes to use real- world terahertz images as low-resolution images and real-world optical images as ground truth to organize unpaired training sets, also a cycle adversarial super-resolution network is designed for the characteristics of unpaired training sets and different picture styles. In this paper we trained the network using training sets organized with the terahertz images which was collected through the Internet, and the images suitable for training in the Office-Home Datasets. The obtained results with real terahertz images shown that through the proposed network the resolution of terahertz images improved, which proves the feasibility of this network.  © 2022 IEEE.","Defects; Generative adversarial networks; Geometrical optics; Image enhancement; Image reconstruction; Optical resolving power; Cycle generative adversarial network; Image degradation; Image super-resolution reconstruction; Imaging technology; Real-world; Real-world image; Super-resolution reconstruction; Tera Hertz; Terahertz imaging; Training sets; Deep neural networks","cycle generative adversarial networks; deep neural networks; image degradation; super-resolution reconstruction; Terahertz imaging","Conference paper","Final","","Scopus","2-s2.0-85139165576"
"Shi Y.; Han L.; Han L.; Chang S.; Hu T.; Dancey D.","Shi, Yue (55496938800); Han, Liangxiu (24281092500); Han, Lianghao (55823389000); Chang, Sheng (57215059964); Hu, Tongle (57782578000); Dancey, Darren (17343675200)","55496938800; 24281092500; 55823389000; 57215059964; 57782578000; 17343675200","A Latent Encoder Coupled Generative Adversarial Network (LE-GAN) for Efficient Hyperspectral Image Super-Resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5534819","","","","10.1109/TGRS.2022.3193441","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135759162&doi=10.1109%2fTGRS.2022.3193441&partnerID=40&md5=b9b147ce60673b43236223d3d24947e5","Realistic hyperspectral image (HSI) super-resolution (SR) techniques aim to generate a high-resolution (HR) HSI with higher spectral and spatial fidelity from its low-resolution (LR) counterpart. The generative adversarial network (GAN) has proven to be an effective deep learning framework for image SR. However, the optimization process of existing GAN-based models frequently suffers from the problem of mode collapse, leading to the limited capacity of spectral-spatial invariant reconstruction. This may cause the spectral-spatial distortion to the generated HSI, especially with a large upscaling factor. To alleviate the problem of mode collapse, this work has proposed a novel GAN model coupled with a latent encoder (LE-GAN), which can map the generated spectral-spatial features from the image space to the latent space and produce a coupling component to regularize the generated samples. Essentially, we treat an HSI as a high-dimensional manifold embedded in a latent space. Thus, the optimization of GAN models is converted to the problem of learning the distributions of HR HSI samples in the latent space, making the distributions of the generated SR HSIs closer to those of their original HR counterparts. We have conducted experimental evaluations on the model performance of SR and its capability in alleviating mode collapse. The proposed approach has been tested and validated based on two real HSI datasets with different sensors (i.e., AVIRIS and UHD-185) for various upscaling factors (i.e., $\times 2$ , $\times 4$ , and $\times 8$ ) and added noise levels (i.e., $\infty $ , 40, and 80 dB) and compared with the state-of-the-art SR models (i.e., hyperspectral coupled network (HyCoNet), low tensor-train rank (LTTR), band attention GAN (BAGAN), SR-GAN, and WGAN). Experimental results show that the proposed model outperforms the competitors on the SR quality, robustness, and alleviation of mode collapse. The proposed approach is able to capture spectral and spatial details and generate more faithful samples than its competitors. It has also been found that the proposed model is more robust to noise and less sensitive to the upscaling factor and has been proven to be effective in improving the convergence of the generator and the spectral-spatial fidelity of the SR HSIs.  © 1980-2012 IEEE.","Deep learning; Distortion (waves); Image processing; Network coding; Optical resolving power; Spectroscopy; Deep learning; Generator; High resolution; HyperSpectral; Hyperspectral image super-resolution; Image super resolutions; Optimisations; Spatial resolution; Superresolution; Upscaling; optimization; reconstruction; resolution; Generative adversarial networks","Deep learning (DL); generative adversarial network (GAN); hyperspectral image (HSI) super-resolution (SR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135759162"
"Xu M.; Ding Y.","Xu, Min (55640661800); Ding, Youdong (7404136883)","55640661800; 7404136883","Single-image Super-resolution Based on Generative Adversarial Network with Dual Attention Mechanism","2022","IMCEC 2022 - IEEE 5th Advanced Information Management, Communicates, Electronic and Automation Control Conference","","","","239","250","11","10.1109/IMCEC55388.2022.10020076","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147663703&doi=10.1109%2fIMCEC55388.2022.10020076&partnerID=40&md5=525a47f010e35a3ef6cb719aa72bf165","Most of the single-image (SI) super-resolution (SR) methods based on convolutional neural network (CNN) focus on the broader or deeper model architecture design, ignoring the research on the correlation of features in the middle layer, which hinders the representative power of CNN. However, most of the classical SISR methods based on generative adversarial network (GAN) focus on improving generators and loss functions, which are difficult to balance visual quality and objective evaluation. In order to solve these problems, SRGAN-DAM-SISR is proposed in this paper, which is also a typical GAN structure. However, it is different from the classical SRGAN and ESRGAN. We have made many designs and innovations, including improving the traditional feature extraction module of the generator and the discriminator's discriminant target in the GAN, removing the batch normalization (BN) layer, using many loss functions for weighting and using some tricks to improve the performance of the network. Experimental results show the improvement and innovation to enhance the visual effect of the output image, the resulting image quality is also improved.  © 2022 IEEE.","Convolutional neural networks; Generative adversarial networks; Multilayer neural networks; Optical resolving power; Architecture designs; Attention mechanisms; Convolutional neural network; Image super resolutions; Loss functions; Modeling architecture; Residual learning; Single images; Superresolution; Superresolution methods; Image enhancement","attention mechanisms; generative adversarial networks; image enhancement; residual learning; super resolution","Conference paper","Final","","Scopus","2-s2.0-85147663703"
"Kosana V.; Gunda V.S.P.; Kosana V.","Kosana, Vishalteja (57229941500); Gunda, Venkata Sai Praveen (57973755000); Kosana, Vamshikrishna (57973617500)","57229941500; 57973755000; 57973617500","ADEEC-Multistage Novel Framework For Cattle Identification using Muzzle Prints","2022","2022 2nd International Conference on Computer Science, Engineering and Applications, ICCSEA 2022","","","","","","","10.1109/ICCSEA54677.2022.9936195","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142357019&doi=10.1109%2fICCSEA54677.2022.9936195&partnerID=40&md5=6bf2db7ec12d8cb89a1003d192a0a20e","This research proposed a deep learning approach using muzzle prints for cattle recognition. This method also proves effective for addressing missing or swapped cattle and false insurance claims. A novel hybrid multistage optimal framework ADEEC (Augmentation-Denoising-Enhancement-Extraction-Classification) for cattle identification and detection is proposed in this paper. The proposed multi-stage framework is developed by hybridizing Denoising Autoencoders (DAE), Super-Resolution Generative Adversarial Network (SRGAN), InceptionResnetV2 feature extractor and multilayer perceptron Network (MLP). The process of denoising the noisy images is carried out using DAE. Then, SRGAN is employed to enhance the denoised images, eliminating the residual noise, making feature extraction more optimal. InceptionResnetV2 model is used to extract meaningful and optimal features from the images and the MLP is adopted to route the classification process. To evaluate the proposed approaches in all stages, different benchmark techniques are used for comparative analysis. The proposed framework performance is assessed through various statistical metrics, and its efficacy is evaluated. With multiple test datasets, proposed framework achieved an accuracy of 98.21%.  © 2022 IEEE.","Deep learning; Extraction; Generative adversarial networks; Image denoising; Image enhancement; Insurance; Learning systems; Auto encoders; Cattle identification; Cow muzzle image; De-noising; Enhancement; Features extraction; Learning approach; Multi layer perceptron networks; Multi-stages; Superresolution; Feature extraction","cattle Identification; cow muzzle images; denoising; enhancement; feature extraction","Conference paper","Final","","Scopus","2-s2.0-85142357019"
"Liang J.; Zeng H.; Zhang L.","Liang, Jie (57688439800); Zeng, Hui (57655872100); Zhang, Lei (55115873800)","57688439800; 57655872100; 55115873800","Details or Artifacts: A Locally Discriminative Learning Approach to Realistic Image Super-Resolution","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","5647","5656","9","10.1109/CVPR52688.2022.00557","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141789334&doi=10.1109%2fCVPR52688.2022.00557&partnerID=40&md5=39a466200c1254681999af518b7c38c8","Single image super-resolution (SISR) with generative adversarial networks (GAN) has recently attracted increasing attention due to its potentials to generate rich details. However, the training of GAN is unstable, and it often introduces many perceptually unpleasant artifacts along with the generated details. In this paper, we demonstrate that it is possible to train a GAN-based SISR model which can stably generate perceptually realistic details while inhibiting visual artifacts. Based on the observation that the local statistics (e.g., residual variance) of artifact areas are often different from the areas of perceptually friendly details, we develop a framework to discriminate between GAN-generated artifacts and realistic details, and consequently generate an artifact map to regularize and stabilize the model training process. Our proposed locally discriminative learning (LDL) method is simple yet effective, which can be easily plugged in off-the-shelf SISR methods and boost their performance. Experiments demonstrate that LDL outperforms the state-of-the-art GAN based SISR methods, achieving not only higher reconstruction accuracy but also superior perceptual quality on both synthetic and real-world datasets. Codes and models are available at https://github.com/csjliang/LDL. © 2022 IEEE.","Computer vision; Learning systems; Optical resolving power; Discriminative learning; Image super resolutions; Learning approach; Low-level vision; Network-based; Realistic images; Single images; Super-resolution models; Superresolution methods; Visual artifacts; Generative adversarial networks","Low-level vision","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141789334"
"Kabiraj A.; Pal D.; Ganguly D.; Chatterjee K.; Roy S.","Kabiraj, Anwesh (57222123949); Pal, Debojyoti (57222132945); Ganguly, Debayan (56524176600); Chatterjee, Kingshuk (57213796353); Roy, Sudipta (56406670700)","57222123949; 57222132945; 56524176600; 57213796353; 56406670700","Number plate recognition from enhanced super-resolution using generative adversarial network","2022","Multimedia Tools and Applications","","","","","","","10.1007/s11042-022-14018-0","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138980368&doi=10.1007%2fs11042-022-14018-0&partnerID=40&md5=f6cd97f28a841872864e11db745f3e26","Identification and recognition of number plate is very difficult from low resolution images due to poor boundary and contrast. Our goal is to identify the digits from a low-quality number plate image correctly, but correct detection was exceedingly difficult in some cases due to the low-resolution image. Another goal of this paper was to upscale the image from a very low resolution to high resolution to recover helpful information to improve the accuracy of number plate detection and recognition. We have used Enhanced- Super-Resolution with Generative Adversarial Network (SRGAN). We modified native Dense Blocks of the Generative Adversarial Network with a Residual in Residual Dense Block model. In addition to Convolutional Neural Networks for thresholding. We also used a Rectified Linear Unit (ReLU) activation layer. The plate image is then used for segmentation using the OCR model for detection and recognizing the characters in the number plates. The Optical character recognition (OCR) model reaches an average accuracy of 84% for high resolution, whereas the accuracy is 4% - 7% for low resolution. The model’s accuracy increases with the resolution enhancement of the plate images. ESRGAN provides better enhancement of low-resolution images than SRGAN and Pro-SRGAN, which the OCR model validates. The accuracy significantly increased digit/alphabet detection in the number plate than the original low-resolution image when converted to a high-resolution image using ESRGAN. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolution; Convolutional neural networks; Deep learning; Image enhancement; Image segmentation; Multilayer neural networks; Optical character recognition; Optical resolving power; Plates (structural components); Deep learning; Low resolution images; Number plate detection; Number plates; Plate detections; Recognition models; Residual dense block; Structural similarity; Structural similarity of image; Superresolution; Generative adversarial networks","Deep learning; Number plate detection; Optical character recognition; Residual dense block; Structural similarity of images; Super-resolution","Article","Article in press","","Scopus","2-s2.0-85138980368"
"Ciotola M.; Martinelli A.; Mazza A.; Scarpa G.","Ciotola, M. (57239080700); Martinelli, A. (57937431100); Mazza, A. (57200854745); Scarpa, G. (7004081145)","57239080700; 57937431100; 57200854745; 7004081145","An Adversarial Training Framework for Sentinel-2 Image Super-Resolution","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","3782","3785","3","10.1109/IGARSS46834.2022.9883144","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140358132&doi=10.1109%2fIGARSS46834.2022.9883144&partnerID=40&md5=978c0b9124123088c41e78a34f7b4e22","In this work is presented a new adversarial training framework for deep learning neural networks for super-resolution of Sentinel 2 images, exploiting the data fusion techniques on 10 and 20 meters bands. The proposed scheme is fully convolutional and tries to answer the need for generalization in scale, producing realistic and detailed accurate images. Furthermore, the presence of a mathcal{L}-{1} loss limits the instability of GAN training, limiting possible problems of spectral dis-tortion. In our preliminary experiments, the GAN training scheme has shown comparable results in comparison with the baseline approach. © 2022 IEEE.","Computer vision; Data fusion; Deep learning; Optical resolving power; Convo-lutional neural network; Data fusion technique; Deep learning; Generalisation; Image super resolutions; Learning neural networks; Neural-networks; Sentinel-2; Superresolution; Training framework; Generative adversarial networks","Convo-lutional Neural Network; Data-Fusion; Deep Learning; Generative Adversarial Network; Sentinel-2; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85140358132"
"Kussul N.; Shelestov A.; Yailymova H.; Shumilo L.; Drozd S.","Kussul, Nataliia (6602485938); Shelestov, Andrii (6507365226); Yailymova, Hanna (57202424721); Shumilo, Leonid (57208256914); Drozd, Sophia (57456870300)","6602485938; 6507365226; 57202424721; 57208256914; 57456870300","Agriculture Land Appraisal with Use of Remote Sensing and Infrastructure Data","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2785","2788","3","10.1109/IGARSS46834.2022.9884045","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140357841&doi=10.1109%2fIGARSS46834.2022.9884045&partnerID=40&md5=eb8b51ef384d51205e0438cb2db12049","1st July 2021 the law on the creation of land market start effect in Ukraine. As a result, land appraisal became cornerstone task in Ukrainian agriculture sector. The official methodology on land appraisal includes use of soil fertility characteristics combined with coefficients related to the distance to the infrastructure objects or settlements and placing of field in specific functional areas, like recreational, or areas with high level of radiation pollution. In this study we collected open source infostructure geospatial information and characteristics of fields obtained from remote sensing data-crop types and Normalized Difference Vegetation Index to build land price predictive model trained on the official land market information. This work designed to investigate potential of geo-informational technologies and remote sensing in the land appraisal use. We separated all available ground truth land price data into three groups by fields size-very small, small, medium and big. We found different relationships between field characteristics and prices. For very small fields the most important features are area, altitude, slope, bonitet and distances to elevators, villages and roads. For small fields the most important are bonitet, altitude, area and distances to cities and roads. For medium and big field's area, slope, distance to cities, roads and historical NDVI. © 2022 IEEE.","Agriculture; Commerce; Deep learning; Generative adversarial networks; Agriculture sectors; Deep learning; GAN; Land markets; Land prices; Remote-sensing; Sentinel-2; Soil fertility; Superresolution; Ukraine; Remote sensing","deep learning; GAN; Generative Adversarial Networks; Sentinel-2; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140357841"
"Nandhini P.S.; Srinath P.; Veeramanikandan P.","Nandhini, P.S. (57215548860); Srinath, P. (57795199700); Veeramanikandan, P. (57730730200)","57215548860; 57795199700; 57730730200","Detection of Glaucoma using Convolutional Neural Network (CNN) with Super Resolution Generative Adversarial Network (SRGAN)","2022","3rd International Conference on Smart Electronics and Communication, ICOSEC 2022 - Proceedings","","","","1034","1040","6","10.1109/ICOSEC54921.2022.9951876","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143711536&doi=10.1109%2fICOSEC54921.2022.9951876&partnerID=40&md5=4d1413f425abef605170d767d9d8f359","Eye Disease is one of the major impacts in human life. Glaucoma is dangerous eye disease because it causes permanent blindness by damaging the optic nerves. According to the data, it is one of the primary causes of blindness and the second most common eye illness. Early detection of this disorder is very important to avoid partial or complete visual loss. A high fluid pressure inside the eye occurs when the circulation of a liquid called aqueous humor in the front region of the eyes is obstructed. Therefore, the trabecular meshwork in the eye is blocked. It stop the flow of the fluid and so raise in the pressure causes a change in the size of optic nerve. The communication between the eyes and the brain is lost, resulting in vision loss. Normally, only the most well-prepared physicians perform a laborious physical review on the fundus images. So, CNN-SRGAN is proposed to detect Glaucoma using eye fundus images. Since high resolution image is needed for classification, SRGAN enhancement is done. UNet is deployed for image segmentation. Finally, the images are classified using CNN. The proposed system provides better accuracy in detection of Glaucoma.  © 2022 IEEE.","Convolutional neural networks; Eye protection; Image enhancement; Image segmentation; Ophthalmology; Optical resolving power; Vision; Convolutional neural network; Eye disease; Fluid pressures; Fundus image; Glaucoma; Human lives; Optic nerve; Permanent blindness; Superresolution; Visual loss; Generative adversarial networks","Eye Disease; Fundus Images; Glaucoma; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85143711536"
"Zhang Y.; Yu W.","Zhang, Yifan (57214255854); Yu, Wenhao (56443033900)","57214255854; 56443033900","Comparison of DEM Super-Resolution Methods Based on Interpolation and Neural Networks","2022","Sensors","22","3","745","","","","10.3390/s22030745","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122969431&doi=10.3390%2fs22030745&partnerID=40&md5=fc8ccb8a7d865f83dd19cbee28fd4e45","High-resolution digital elevation models (DEMs) play a critical role in geospatial databases, which can be applied to many terrain-related studies such as facility siting, hydrological analysis, and urban design. However, due to the limitation of precision of equipment, there are big gaps to collect high-resolution DEM data. A practical idea is to recover high-resolution DEMs from easily obtained low-resolution DEMs, and this process is termed DEM super-resolution (SR). However, traditional DEM SR methods (e.g., bicubic interpolation) tend to over-smooth high-frequency regions on account of the operation of averaging local variations. With the recent development of machine learning, image SR methods have made great progress. Nevertheless, due to the complexity of terrain characters (e.g., peak and valley) and the huge difference between elevation field and image RGB (Red, Green, and Blue) value field, there are few works that apply image SR methods to the task of DEM SR. Therefore, this paper investigates the question of whether the state-of-the-art image SR methods are appropriate for DEM SR. More specifically, the traditional interpolation method and three excellent SR methods based on neural networks are chosen for comparison. Experimental results suggest that SRGAN (Super-Resolution with Generative Adversarial Network) presents the best performance on accuracy evaluation over a series of DEM SR experiments. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Databases, Factual; Image Processing, Computer-Assisted; Machine Learning; Neural Networks, Computer; Generative adversarial networks; Interpolation; Landforms; Optical resolving power; Digital elevation model; Geospatial database; High resolution; Image super resolutions; Neural-networks; Resolution process; Super-resolution process; Superresolution; Superresolution methods; Terrain features; factual database; image processing; machine learning; Surveying","DEM; Neural network; Super-resolution process; Terrain features","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122969431"
"Zhao Y.; Liu S.; Luo A.; Peng B.","Zhao, Yachuan (57983430200); Liu, Shuyan (57444617100); Luo, Anguo (57982256400); Peng, Bo (57203991970)","57983430200; 57444617100; 57982256400; 57203991970","Dual Generative Adversarial Network For Ultrasound Localization Microscopy","2022","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2022-October","","","3125","3130","5","10.1109/SMC53654.2022.9945563","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142687965&doi=10.1109%2fSMC53654.2022.9945563&partnerID=40&md5=cf56a163404e5b6bace1af36de5237bf","Ultrasound localization microscopy (ULM) is a new imaging technique that uses microbubbles (MBs) to improve the spatial resolution of ultrasound (US) imaging. For ULM, it is critical to accurately localize MB position. Recently, deep learning-based methods are adopted to acquire MB localization, which shows promising performance and efficient computation. However, detection of high-concentration MBs is still a challenging task. To further improve the localization accuracy, a dual generative adversarial network (DualGAN)-based ULM imaging method (DualGAN-ULM) is proposed in this paper to overcome the problems of long data processing time and low parameter robustness in current ULM imaging methods. This method is trained using simulated data generated by point spread function (PSF) convolution and uses dual generation adversarial strategy to enable the generator to perform accurate localization under high-concentration MB conditions. Meanwhile, the localization and reconstruction capabilities of five ULM methods, namely Centroid, CS-ULM, mUNET-ULM, mSPCN-ULM and DualGAN-ULM, are evaluated in this paper. The experimental results reveal that DL-based ULM methods (DualGAN-ULM, mSPCN-ULM, and mUNET-ULM) outperform compressed sensing-based localization methods (CS-ULM) and Centroid in terms of localization accuracy and localization dependability. DualGAN-ULM performs better than mSPCN-ULM and mUNET-ULM, making it a more realistic ULM method.  © 2022 IEEE.","Data handling; Deep learning; Optical transfer function; Ultrasonic applications; Localisation; Localization accuracy; Microbubble localization; Microbubbles; Microscopy imaging; Point-Spread function; Super resolution imaging; Ultrasound localization; Ultrasound localization microscopy; Ultrasound super-resolution imaging; Generative adversarial networks","Generative Adversarial Networks; Microbubble Localization; Point Spread Function; Ultrasound Localization Microscopy; Ultrasound Super-Resolution Imaging","Conference paper","Final","","Scopus","2-s2.0-85142687965"
"Haykir A.A.; Oksuz I.","Haykir, Aslan Ahmet (57903966500); Oksuz, Ilkay (55793268700)","57903966500; 55793268700","Super-resolution with generative adversarial networks for improved object detection in aerial images","2022","Information Discovery and Delivery","","","","","","","10.1108/IDD-05-2022-0048","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142186605&doi=10.1108%2fIDD-05-2022-0048&partnerID=40&md5=975a946afae4201c07e6b6b6f8d35122","Purpose: Data quality and data resolution are essential for computer vision tasks like medical image processing, object detection, pattern recognition and so on. Super-resolution is a way to increase the image resolution, and super-resolved images contain more information compared to their low-resolution counterparts. The purpose of this study is analyzing the effects of the super resolution models trained before on object detection for aerial images. Design/methodology/approach: Two different models were trained using the Super-Resolution Generative Adversarial Network (SRGAN) architecture on two aerial image data sets, the xView and the Dataset for Object deTection in Aerial images (DOTA). This study uses these models to increase the resolution of aerial images for improving object detection performance. This study analyzes the effects of the model with the best perceptual index (PI) and the model with the best RMSE on object detection in detail. Findings: Super-resolution increases the object detection quality as expected. But, the super-resolution model with better perceptual quality achieves lower mean average precision results compared to the model with better RMSE. It means that the model with a better PI is more meaningful to human perception but less meaningful to computer vision. Originality/value: The contributions of the authors to the literature are threefold. First, they do a wide analysis of SRGAN results for aerial image super-resolution on the task of object detection. Second, they compare super-resolution models with best PI and best RMSE to showcase the differences on object detection performance as a downstream task first time in the literature. Finally, they use a transfer learning approach for super-resolution to improve the performance of object detection. © 2022, Emerald Publishing Limited.","","Aerial images; Data quality; Generative adversarial networks; Object detection; Perceptual quality; Super-resolution","Article","Article in press","","Scopus","2-s2.0-85142186605"
"Lu J.; Shi M.; Lu Y.; Chang C.-C.; Li L.; Bai R.","Lu, Jianfeng (55574231160); Shi, Mengtao (57960022400); Lu, Yuhang (57958922500); Chang, Ching-Chun (56056418200); Li, Li (56179676100); Bai, Rui (57959585100)","55574231160; 57960022400; 57958922500; 56056418200; 56179676100; 57959585100","Multi-Stage Generation of Tile Images Based on Generative Adversarial Network","2022","IEEE Access","10","","","127502","127513","11","10.1109/ACCESS.2022.3218636","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141616992&doi=10.1109%2fACCESS.2022.3218636&partnerID=40&md5=b87a196599ebc0bda5f2af62f4f7089b","Deep learning techniques have been recently widely used in the field of texture image generation. There are still two major problems when applying them to tile image design work. On the one hand, there is still lack of enough diverse ceramic tile images for the training process. On the other hand, the output image is difficult to control and adjust, and cannot meet the designer's requirements of interactivity. Therefore, we propose a multi-stage generation algorithm of tile images based on generative adversarial network(GAN). First, the multi-scale attention GAN is applied to generate controllable texture image. Then, the SWAG texture synthesis GAN is also applied to obtain controllable and diverse image style. And finally, through the style iteration mechanism and the multiple step magnification method based on image super-resolution reconstruction network, the final tile images can be automatically generated with larger-size and higher-precision. The relevant experiments demonstrate that our method can not only generate high-quality tile images in a relatively short period of time, but also consider human interaction to a certain extent, and maintain a certain degree of control over the main texture and style of the final generated tile images. It has good and wide application value.  © 2013 IEEE.","Deep learning; Image reconstruction; Image texture; Iterative methods; Optical resolving power; Quality control; Features extraction; Generator; Image super resolutions; Image super-resolution magnification; Image-based; Images reconstruction; Multi-stages; Style transfer; Superresolution; Tile image; Generative adversarial networks","generative adversarial networks; image super-resolution magnification; style transfer; Tile images","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85141616992"
"Groshev A.; Maltseva A.; Chesakov D.; Kuznetsov A.; Dimitrov D.","Groshev, Alexander (57450186400); Maltseva, Anastasia (57450364900); Chesakov, Daniil (57451087400); Kuznetsov, Andrey (57220713929); Dimitrov, Denis (57837974200)","57450186400; 57450364900; 57451087400; 57220713929; 57837974200","GHOST-A New Face Swap Approach for Image and Video Domains","2022","IEEE Access","10","","","83452","83462","10","10.1109/ACCESS.2022.3196668","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135762465&doi=10.1109%2fACCESS.2022.3196668&partnerID=40&md5=150eaae9ffe4fb0843a64b3dc6bf2144","Deep fake stands for a face swapping algorithm where the source and target can be an image or a video. Researchers have investigated sophisticated generative adversarial networks (GAN), autoencoders, and other approaches to establish precise and robust algorithms for face swapping. However the achieved results are far from perfect in terms of human and visual evaluation. In this study, we propose a new one-shot pipeline for image-to-image and image-to-video face swap solutions-GHOST (Generative High-fidelity One Shot Transfer). We take the FaceShifter (image-to-image) architecture as a baseline approach and propose several major architecture improvements which include a new eye-based loss function, face mask smooth algorithm, a new face swap pipeline for image-to-video face transfer, a new stabilization technique to decrease face jittering on adjacent frames and a super-resolution stage. In the experimental stage, we show that our solution outperforms SoTA face swap architectures in terms of ID retrieval (+1.5% improvement), shape (the second best value) and eye gaze preserving (+1% improvement) metrics. We also established an ablation study for our solution to estimate the contribution of pipeline stages to the overall accuracy, which showed that the eye loss leads to 2% improvement in the ID retrieval and 45% improvement in the eye gaze preserving. © 2013 IEEE.","Face recognition; Fake detection; Generative adversarial networks; Image enhancement; Image reconstruction; Network architecture; Optical resolving power; Pipelines; AEI-net; Deep fake; Deepfake; Eye loss; Face; Face mask smooth; Face masks; Face swap; Images reconstruction; Superresolution; Stabilization","AEI-Net; Deep fake; eye loss; face mask smooth; face swap; GHOST; stabilization; super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135762465"
"Sui Y.; Guo X.; Yu J.; Ta D.; Xu K.","Sui, Yihui (57413494500); Guo, Xingyi (57413775300); Yu, Junjin (57735741100); Ta, Dean (6701758419); Xu, Kailiang (34875930800)","57413494500; 57413775300; 57735741100; 6701758419; 34875930800","Generative Adversarial Nets for Ultrafast Ultrasound Localization Microscopy Reconstruction","2022","IEEE International Ultrasonics Symposium, IUS","2022-October","","","","","","10.1109/IUS54386.2022.9957566","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143832516&doi=10.1109%2fIUS54386.2022.9957566&partnerID=40&md5=2cc6bf200f1aafbd951ee85264aa5ca6","Ultrafast ultrasound localization microscopy (u ULM) can be used to resolve deep vasculature down to a few micrometers. After microbubble localization over hundreds of thousands of images, accurate and efficient tracking of each individual microbubble over consecutive frames is one of the crucial issues for uULM reconstruction. Continuous long acquisition still limits its clinical application. In the study, a generative adversarial nets (GAN) based deep learning approach is developed to facilitate microbubble tracking and further reduce the acquisition time of uULM.  © 2022 IEEE.","Convolutional neural networks; Deep learning; Medical imaging; Ultrasonics; Convolutional neural network; Generative adversarial network; Localisation; Microbubbles; Superresolution; Ultra-fast; Ultrasonic localization; Ultrasonic localization microscopy; Ultrasound localization; Vasculature; Generative adversarial networks","Convolutional neural network; Generative adversarial network (GAN); Super-resolution; Ultrasonic localization microscopy","Conference paper","Final","","Scopus","2-s2.0-85143832516"
"Fu X.; Kouyama T.; Yang H.; Nakamura R.; Yoshikawa I.","Fu, Xuanchao (57937799100); Kouyama, Toru (16162149600); Yang, Hang (57482386400); Nakamura, Ryosuke (56711594400); Yoshikawa, Ichiro (35235779300)","57937799100; 16162149600; 57482386400; 56711594400; 35235779300","Toward Faster and Accurate Post-Disaster Damage Assessment: Development of End-to-End Building Damage Detection Framework with Super-Resolution Architecture","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1588","1591","3","10.1109/IGARSS46834.2022.9883317","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140394652&doi=10.1109%2fIGARSS46834.2022.9883317&partnerID=40&md5=88b4139b5055b2af62dffafb2b08454d","Building damage detection (BDD) with satellite images has been frequently adopted as an essential reference for post-disaster rescue, whereas its timeliness is significantly impacted by the long revisit time of high-resolution remote sensing satellites. Therefore, a reliable super-resolution method which is optimized for accurate and detail BDD is fundamental one for advancing the BDD analysis even when we can use only low-resolution images after a disaster. Based on Super-Resolution Generative Adversarial Network (SRGAN) and U-Net convolutional network, an efficient and novel BDD framework is proposed in this paper for obtaining upsampled BDD results from low-resolution post-disaster images. We trained the framework using two disasters from the xBD dataset and tested three different structures. The results show that our training structure based on an end-to-end framework successfully generated super-resolution BDD maps from low-resolution images, which performed significantly better than those from a two-stage training structure. © 2022 IEEE.","Boolean functions; Deep learning; Disasters; Generative adversarial networks; Optical resolving power; Remote sensing; Building damage; Damage assessments; Deep learning; Detection framework; End to end; End-to-end network; Low resolution images; Post disasters; Superresolution; XBD dataset; Damage detection","building damage; deep learning; end-to-end network; Super-resolution; xBD dataset","Conference paper","Final","","Scopus","2-s2.0-85140394652"
"Aakerberg A.; Nasrollahi K.; Moeslund T.B.","Aakerberg, Andreas (57194071197); Nasrollahi, Kamal (24476795100); Moeslund, Thomas B. (6507267791)","57194071197; 24476795100; 6507267791","Real-world super-resolution of face-images from surveillance cameras","2022","IET Image Processing","16","2","","442","452","10","10.1049/ipr2.12359","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117888761&doi=10.1049%2fipr2.12359&partnerID=40&md5=4c2cdeebaf297254530ac21e21fb40dc","Most existing face image Super-Resolution (SR) methods assume that the Low-Resolution (LR) images were artificially downsampled from High-Resolution (HR) images with bicubic interpolation. This operation changes the natural image characteristics and reduces noise. Hence, SR methods trained on such data most often fail to produce good results when applied to real LR images. To solve this problem, a novel framework for the generation of realistic LR/HR training pairs is proposed. The framework estimates realistic blur kernels, noise distributions, and JPEG compression artifacts to generate LR images with similar image characteristics as the ones in the source domain. This allows to train an SR model using high-quality face images as Ground-Truth (GT). For better perceptual quality, a Generative Adversarial Network (GAN) based SR model is used, where the commonly used VGG-loss [1] is exchanged with LPIPS-loss [2]. Experimental results on both real and artificially corrupted face images show that our method results in more detailed reconstructions with less noise compared to the existing State-of-the-Art (SoTA) methods. In addition, it is shown that the traditional non-reference Image Quality Assessment (IQA) methods fail to capture this improvement and demonstrate that the more recent NIMA metric [3] correlates better with human perception via Mean Opinion Rank (MOR). © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Image compression; Image enhancement; Optical resolving power; Security systems; Face images; Image characteristics; Image super resolutions; Images processing; Low resolution images; Real-world; Super-resolution models; Superresolution; Superresolution methods; Surveillance cameras; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117888761"
"Wang J.; Geng K.; Yin G.; Cheng X.; Sun Y.; Ding P.","Wang, Jinhu (58028483900); Geng, Keke (57202961397); Yin, Guodong (24781523700); Cheng, Xiaolong (58028069200); Sun, Yuxiao (58028483800); Ding, Pengbo (58028653800)","58028483900; 57202961397; 24781523700; 58028069200; 58028483800; 58028653800","Binocular Infrared Depth Estimation Based On Generative Adversarial Network","2022","2022 6th CAA International Conference on Vehicular Control and Intelligence, CVCI 2022","","","","","","","10.1109/CVCI56766.2022.9964565","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144632953&doi=10.1109%2fCVCI56766.2022.9964565&partnerID=40&md5=6b371ffdfb049ec86577b96b3b26e5f0","Dedicated to the fulfillment of the all-weather scene perception of intelligent vehicle, our work focus on depth estimation based on binocular infrared image. First of all, considering the rareness of binocular infrared datasets, we apply a top-down attention and gradient alignment based on generative adversarial network (GAN) to convert binocular visible datasets into pseudo-infrared datasets, which was used to fine-tune the depth estimation network. In this way, the basal function of binocular infrared depth estimation could be achieved. Then, the enhancement and super-resolution (SR) preprocessing method based on high-order degradation modeling process was introduced to enhance the image edge information and enrich the image details. The experiment results showed that the image details are richer after SR, which improves the accuracy and breadth of binocular depth perception by 5%.  © 2022 IEEE.","Binoculars; Depth perception; Image enhancement; Infrared imaging; Intelligent systems; Optical resolving power; Binocular depth estimation; Degradation model; Depth Estimation; High-order; Higher-order; Image details; Infrared image; Pre-processing method; Superresolution; Topdown; Generative adversarial networks","binocular depth estimation; generative adversarial network; Infrared image; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85144632953"
"Ahuja V.R.; Gupta U.; Rapole S.R.; Saxena N.; Hofmann R.; Day-Stirrat R.J.; Prakash J.; Yalavarthy P.K.","Ahuja, Vishal R. (57192089331); Gupta, Utkarsh (57712096100); Rapole, Shivani R. (57712010000); Saxena, Nishank (55450282600); Hofmann, Ronny (12140095100); Day-Stirrat, Ruarri J. (24175931100); Prakash, Jaya (55763790369); Yalavarthy, Phaneendra K. (8717097900)","57192089331; 57712096100; 57712010000; 55450282600; 12140095100; 24175931100; 55763790369; 8717097900","Siamese-SR: A Siamese Super-Resolution Model for Boosting Resolution of Digital Rock Images for Improved Petrophysical Property Estimation","2022","IEEE Transactions on Image Processing","31","","","3479","3493","14","10.1109/TIP.2022.3172211","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130765712&doi=10.1109%2fTIP.2022.3172211&partnerID=40&md5=8f4d101cd81ab0d19524429c7ddfc18e","Digital Rock Physics leverages advances in digital image acquisition and analysis techniques to create 3D digital images of rock samples, which are used for computational modeling and simulations to predict petrophysical properties of interest. However, the accuracy of the predictions is crucially dependent on the quality of the digital images, which is currently limited by the resolution of the micro-CT scanning technology. We have proposed a novel Deep Learning based Super-Resolution model called Siamese-SR to digitally boost the resolution of Digital Rock images whilst retaining the texture and providing optimal de-noising. The Siamese-SR model consists of a generator which is adversarially trained with a relativistic and a siamese discriminator utilizing Materials In Context (MINC) loss estimator. This model has been demonstrated to improve the resolution of sandstone rock images acquired using micro-CT scanning by a factor of 2. Another key highlight of our work is that for the evaluation of the super-resolution performance, we propose to move away from image-based metrics such as Structural Similarity (SSIM) and Peak Signal to Noise Ratio (PSNR) because they do not correlate well with expert geological and petrophysical evaluations. Instead, we propose to subject the super-resolved images to the next step in the Digital Rock workflow to calculate a crucial petrophysical property of interest, viz. porosity and use it as a metric for evaluation of our proposed Siamese-SR model against several other existing super-resolution methods like SRGAN, ESRGAN, EDSR and SPSR. Furthermore, we also use Local Attribution Maps to show how our proposed Siamese-SR model focuses optimally on edge-semantics, which is what leads to improvement in the image-based porosity prediction, the permeability prediction from Multiple Relaxation Time Lattice Boltzmann Method (MRTLBM) flow simulations as well as the prediction of other petrophysical properties of interest derived from Mercury Injection Capillary Pressure (MICP) simulations.  © 1992-2012 IEEE.","Computational fluid dynamics; Computerized tomography; Deep learning; E-learning; Forecasting; Generative adversarial networks; Geology; Image acquisition; Optical resolving power; Porosity; Rocks; Semantics; Signal to noise ratio; Deep learning; Digital rock physic; Generator; Image super resolutions; Local attribution map; Petrophysics; Predictive models; Rock physics; Siamese network; Signal resolution; Superresolution; Image enhancement","deep learning; digital rock physics; generative adversarial networks; geology; Image super-resolution; local attribution maps; micro computed tomography; petrophysics; siamese networks","Article","Final","","Scopus","2-s2.0-85130765712"
"","","","18th International Conference on Frontiers in Handwriting Recognition, ICFHR 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13639 LNCS","","","","","562","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144426137&partnerID=40&md5=3ceefd5585e17ccf961598ede8d6b9c2","The proceedings contain 37 papers. The special focus in this conference is on Frontiers in Handwriting Recognition. The topics include: Progressive Multitask Learning Network for Online Chinese Signature Segmentation and Recognition; musigraph: Optical Music Recognition Through Object Detection and Graph Neural Network; Combining CNN and Transformer as Encoder to Improve End-to-End Handwritten Mathematical Expression Recognition Accuracy; a Vision Transformer Based Scene Text Recognizer with Multi-grained Encoding and Decoding; spatial Attention and Syntax Rule Enhanced Tree Decoder for Offline Handwritten Mathematical Expression Recognition; FPRNet: End-to-End Full-Page Recognition Model for Handwritten Chinese Essay; active Transfer Learning for Handwriting Recognition; recognition-Free Question Answering on Handwritten Document Collections; handwriting Recognition and Automatic Scoring for Descriptive Answers in Japanese Language Tests; text Edges Guided Network for Historical Document Super Resolution; a Weighted Combination of Semantic and Syntactic Word Image Representations; combining Self-training and Minimal Annotations for Handwritten Word Recognition; script-Level Word Sample Augmentation for Few-Shot Handwritten Text Recognition; Towards Understanding and Improving Handwriting with AI; chaCo: Character Contrastive Learning for Handwritten Text Recognition; enhancing Indic Handwritten Text Recognition Using Global Semantic Information; yi Characters Online Handwriting Recognition Models Based on Recurrent Neural Network: RnnNet-Yi and ParallelRnnNet-Yi; self-attention Networks for Non-recurrent Handwritten Text Recognition; an Efficient Prototype-Based Model for Handwritten Text Recognition with Multi-loss Fusion; Urdu Handwritten Ligature Generation Using Generative Adversarial Networks (GANs); curT: End-to-End Text Line Detection in Historical Documents with Transformers; SCUT-CAB: A New Benchmark Dataset of Ancient Chinese Books with Complex Layouts for Document Layout Analysis; a Benchmark Gurmukhi Handwritten Character Dataset: Acquisition, Compilation, and Recognition; synthetic Data Generation for Semantic Segmentation of Lecture Videos; generating Synthetic Styled Chu Nom Characters; UOHTD: Urdu Offline Handwritten Text Dataset; DAZeTD: Deep Analysis of Zones in Torn Documents.","","","Conference review","Final","","Scopus","2-s2.0-85144426137"
"Nneji G.U.; Deng J.; Monday H.N.; Hossin M.A.; Obiora S.; Nahar S.; Cai J.","Nneji, Grace Ugochi (57206902001); Deng, Jianhua (56102351300); Monday, Happy Nkanta (57201671873); Hossin, Md Altab (57202841195); Obiora, Sandra (57218458683); Nahar, Saifun (57215928272); Cai, Jingye (10738940600)","57206902001; 56102351300; 57201671873; 57202841195; 57218458683; 57215928272; 10738940600","COVID-19 Identification from Low-Quality Computed Tomography Using a Modified Enhanced Super-Resolution Generative Adversarial Network Plus and Siamese Capsule Network","2022","Healthcare (Switzerland)","10","2","403","","","","10.3390/healthcare10020403","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125150717&doi=10.3390%2fhealthcare10020403&partnerID=40&md5=e362718613c5245c01fd3c78851e2fa3","Computed Tomography has become a vital screening method for the detection of coron-avirus 2019 (COVID-19). With the high mortality rate and overload for domain experts, radiologists, and clinicians, there is a need for the application of a computerized diagnostic technique. To this effect, we have taken into consideration improving the performance of COVID-19 identification by tackling the issue of low quality and resolution of computed tomography images by introducing our method. We have reported about a technique named the modified enhanced super resolution generative adversarial network for a better high resolution of computed tomography images. Furthermore, in contrast to the fashion of increasing network depth and complexity to beef up imaging performance, we incorporated a Siamese capsule network that extracts distinct features for COVID-19 identifica-tion.The qualitative and quantitative results establish that the proposed model is effective, accurate, and robust for COVID-19 screening. We demonstrate the proposed model for COVID-19 identification on a publicly available dataset COVID-CT, which contains 349 COVID-19 and 463 non-COVID-19 computed tomography images. The proposed method achieves an accuracy of 97.92%, sensitivity of 98.85%, specificity of 97.21%, AUC of 98.03%, precision of 98.44%, and F1 score of 97.52%. Our approach obtained state-of-the-art performance, according to experimental results, which is helpful for COVID-19 screening. This new conceptual framework is proposed to play an influential task in the issue facing COVID-19 and related ailments, with the availability of few datasets. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","Adversarial learning; Computed tomography; Convolutional neural network; Deep learning; Siamese network; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85125150717"
"Deng D.; Tang Y.","Deng, Danni (57965665700); Tang, Yongming (35602406300)","57965665700; 35602406300","A Comprehensive Quality Assessment of Video Super-Resolution for Ultra High-Resolution Application","2022","Digest of Technical Papers - SID International Symposium","53","S1","","388","392","4","10.1002/sdtp.15958","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141895232&doi=10.1002%2fsdtp.15958&partnerID=40&md5=22ef73defd473bc50b73d900b08a9b31","As the performance of video super-resolution (SR) algorithms continues to improve, traditional video quality assessment such as peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), which are widely used for natural images, are gradually failing to meet the evaluation requirements as they do not effectively take all the characteristics of super-resolved images into account. In addition, the newly emerged SR algorithms based on generative adversarial networks (GAN) bring a whole new challenge to quality assessment while improving SR performance. Therefore, this paper will introduce the shortcomings of existing video quality assessment methods and propose a comprehensive SR video quality assessment method including visual saliency, structural fidelity, texture information and temporal metrics. © 2022, John Wiley and Sons Inc. All rights reserved.","Image enhancement; Image quality; Optical resolving power; Signal to noise ratio; Textures; Comprehensive qualities; Peak signal to noise ratio; Performance; Quality assessment; Super resolution algorithms; Superresolution; Ultrahigh resolution; Video quality; Video quality assessment; Video super-resolution; Generative adversarial networks","super-resolution; video quality assessment","Conference paper","Final","","Scopus","2-s2.0-85141895232"
"Zhao J.; Fang C.; Zhou Z.","Zhao, Jianwei (36618500900); Fang, Chenyun (57866530500); Zhou, Zhenghua (55511512300)","36618500900; 57866530500; 55511512300","Enhanced image super-resolution using hierarchical generative adversarial network","2022","International Journal of Computing Science and Mathematics","15","3","","243","257","14","10.1504/IJCSM.2022.10049414","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136940419&doi=10.1504%2fIJCSM.2022.10049414&partnerID=40&md5=ab0a2404d3ee6466214fe349e68ef1b9","Recently, generative adversarial networks (GAN) have been introduced in single-image super-resolution (SISR) to reconstruct more realistic high-resolution (HR) images. In this paper, we propose an effective SISR method, named super-resolution using hierarchical generative adversarial network (SRHGAN), based on the idea of GAN and the prior knowledge. Different from the existing GANs that focus on the depth of networks, our proposed method considers the prior knowledge in addition. That is, we introduce an edge extraction branch and an edge enhancement branch into GAN for considering the edge information. By means of the added edge loss in the loss function, the edge extraction branch and the edge enhancement branch will be trained to reconstruct the sharp edge well. Experimental results on several datasets illustrate that our reconstructed visual effect images are clearer and sharper than some related SISR methods. © 2022 Inderscience Enterprises Ltd.","Deep learning; Extraction; Image enhancement; Image reconstruction; Optical resolving power; Deep learning; Edge enhancements; Edge extraction; Edge prior; Hierarchical network; Image super resolutions; Prior-knowledge; Single image super-resolution; Single images; Superresolution methods; Generative adversarial networks","deep learning; edge prior; generative adversarial network; hierarchical network; single image super-resolution","Article","Final","","Scopus","2-s2.0-85136940419"
"Tincy Thomas C.; Nambiar A.M.; Mittal A.","Tincy Thomas, C. (57734370300); Nambiar, Athira M (36642667900); Mittal, Anurag (7103357341)","57734370300; 36642667900; 7103357341","A GAN-based Super Resolution Model for Efficient Image Enhancement in Underwater Sonar Images","2022","Oceans Conference Record (IEEE)","","","","","","","10.1109/OCEANSChennai45887.2022.9775508","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131678167&doi=10.1109%2fOCEANSChennai45887.2022.9775508&partnerID=40&md5=11341b7242b0b55c787ef07155b68247","Acoustic imaging systems dominate in underwater imaging due to their unique ability to illuminate objects on the seabed, even in dark or turbid water conditions. These systems mounted on an autonomous underwater vehicle (AUV) are being used for a variety of civilian and military applications. Mine detection and classification is a predominant application. The raw images captured using these systems are usually noisy and poor in their resolution. Consequently, methods to enhance sonar images are necessary to aid further processing and classification of these acquired scenes. Inspired by the developments in the field of deep learning in different areas of computer vision, this study explores efficient deep neural networks for acoustic image super resolution. The study is performed on a custom-made sonar image dataset to handle the deficiency of public datasets in the domain. We employ a Generative Adversarial Network (GAN) deep learning model i.e. pre-trained ESRGAN and make use of transfer learning to achieve our goal with limited data samples. We use the model published by the original authors, Xintao Wang et al and experiment with our proposed method in three ways. a) Direct use of pre-trained model b) Fine-tuning the model with VGG-19 feature extractors at the discriminator and c) Finetuning the model with ResNet-34 feature extractors at the discriminator. The super resolved images are validated through image quality assessment metrics like PSNR, SSIM, and Perceptual index.  © 2022 IEEE.","Autonomous underwater vehicles; Deep neural networks; Generative adversarial networks; Military applications; Military photography; Optical resolving power; Underwater imaging; Acoustic image enhancement; ESRGAN; Feature extractor; Image super resolutions; Network-based; Single image super resolution; Single images; Sonar image; Super-resolution models; Underwater sonars; Image enhancement","Acoustic image enhancement; ESRGAN; single image super resolution","Conference paper","Final","","Scopus","2-s2.0-85131678167"
"Karatsiolis S.; Padubidri C.; Kamilaris A.","Karatsiolis, Savvas (55569345400); Padubidri, Chirag (57222362784); Kamilaris, Andreas (36189564000)","55569345400; 57222362784; 36189564000","Exploiting Digital Surface Models for Inferring Super-Resolution for Remotely Sensed Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4414213","","","","10.1109/TGRS.2022.3209340","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139526038&doi=10.1109%2fTGRS.2022.3209340&partnerID=40&md5=4a6e18e6e3566978fe6a4d1b304ca49f","Despite the plethora of successful super-resolution (SR) reconstruction (SRR) models applied to natural images, their application to remote sensing imagery tends to produce poor results. Remote sensing imagery is often more complicated than natural images, has its peculiarities such as being of lower resolution, contains noise, and often depicts large textured surfaces. As a result, applying nonspecialized SRR models like the enhanced SR generative adversarial network (ESRGAN) on remote sensing imagery results in artifacts and poor reconstructions. To address these problems, we propose a novel strategy for enabling an SRR model to output realistic remote sensing images: Instead of relying on feature-space similarities as a perceptual loss, the model considers pixel-level information inferred from the normalized digital surface model (nDSM) of the image. This allows the application of better-informed updates during the training of the model which sources from a task (elevation map inference) that is closely related to remote sensing. Nonetheless, the nDSM auxiliary information is not required during production, i.e., the model infers an SR image without additional data. We assess our model on two remotely sensed datasets of different spatial resolutions that also contain the DSMs of the images: The Data Fusion 2018 Contest (DFC2018) dataset and the dataset containing the national LiDAR flyby of Luxembourg. We compare our model with ESRGAN, and we show that it achieves better performance and does not introduce any artifacts in the results. In particular, the results for the high-resolution DFC2018 dataset are realistic and almost indistinguishable from the ground-truth images.  © 1980-2012 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; Job analysis; Optical resolving power; Personnel training; Deep learning; Digital surface models; Images reconstruction; Loss measurement; Normalized digital surface model; Perceptual loss; Remote-sensing; Spatial resolution; Super-resolution reconstruction; Superresolution; Task analysis; pixel; remote sensing; satellite imagery; spatial resolution; Remote sensing","Deep learning (DL); normalized digital surface model (nDSM); perceptual loss; remote sensing; super-resolution (SR) reconstruction (SRR)","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139526038"
"Geng Y.; Zhou W.","Geng, Yuchen (57940352700); Zhou, Weimin (57223039695)","57940352700; 57223039695","Image Super-Resolution Reconstruction of Pancreatic Carcinoma Based on Edge Repair Generative Adversarial Network","2022","Chinese Control Conference, CCC","2022-July","","","6421","6426","5","10.23919/CCC55666.2022.9901818","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140437910&doi=10.23919%2fCCC55666.2022.9901818&partnerID=40&md5=a207781a054c2c15e08dc0c1f8be98e8","High-resolution medical imaging of pancreatic carcinoma is of great significance for the early diagnosis of the disease. Despite breakthroughs in accuracy and the capable of generating realistic textures, there are still some problems remain unsolved in current medical image super-resolution reconstruction methods: the edge details are often accompanied with unpleasant artifacts, how do we recover finer textures? To achieve this, we propose an Edge Repair Generative Adversarial Network (ERGAN). The model adds an edge repair network on the basis of the generator and the discriminator network, performs edge detection and repair on the input low-resolution images, then fuses the output edge feature map with the shallow feature map generated by the generator. Finally, the reconstructed images are sent to the discriminator through the up-sampling layer to evaluate the reconstruction effect. In this paper, the model is evaluated on the pancreatic carcinoma data set CPTAC-PDA published on the website of the Cancer Imaging Archive (TCIA). The experimental results show that the images reconstructed by the network model proposed in this paper not only improve the evaluation indicators, but also has a clearer cross-sectional outline and more obvious detailed features.  © 2022 Technical Committee on Control Theory, Chinese Association of Automation.","Diagnosis; Discriminators; Edge detection; Image enhancement; Image reconstruction; Medical imaging; Optical resolving power; Textures; 'current; Detection and repairs; Early diagnosis; Edge repair; Feature map; High resolution; Image super-resolution reconstruction; Pancreatic carcinoma; Reconstruction method; Super-resolution reconstruction; Generative adversarial networks","Edge Repair; Generative Adversarial Network; Pancreatic Carcinoma; Super-Resolution Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85140437910"
"Gu Y.; Huang W.","Gu, Yikai (57763790800); Huang, Weiqin (57437383100)","57763790800; 57437383100","Super-resolution algorithm using an improved generative adversarial network","2022","Proceedings of SPIE - The International Society for Optical Engineering","12250","","122500I","","","","10.1117/12.2639522","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132833103&doi=10.1117%2f12.2639522&partnerID=40&md5=3509375cc5cbde24defa35ba7c96e05e","SRGAN applies generative adversarial networks to super-resolution reconstruction, and the advent of this algorithm greatly facilitates image reconstruction based on deep learning. It introduces batch normalization for stable training, and although batch normalization can facilitate training, it destroys the original contrast information of the image and affects the quality of image reconstruction. For this problem, this paper proposes an super-resolution algorithm based on improved SRGAN, which removes the batch normalization in the residual block of the generative network to avoid its bad influence on the reconstructed image and improve the quality of the reconstructed image for better. In addition, the total variation is introduced in the loss function of the discriminative model to constrain the gradient change, which serves to stabilize the network training and accelerate the convergence. Experiments of the proposed super-resolution algorithm on a large number of public datasets show that this method achieves better results under various evaluation metrics compared with other similar algorithms. © 2022 SPIE","Deep learning; Image enhancement; Image reconstruction; Large dataset; Optical resolving power; Batch normalization; Discriminative models; Images reconstruction; Loss functions; Normalisation; Reconstructed image; Super resolution algorithms; Super-resolution reconstruction; Superresolution; Total-variation; Generative adversarial networks","batch normalization; generative adversarial networks; super-resolution; total variation","Conference paper","Final","","Scopus","2-s2.0-85132833103"
"Liu Z.; Yuan T.; Lin Y.; Zeng B.","Liu, Zhelin (57852624600); Yuan, Tingxu (57852840300); Lin, Yaxin (57852624700); Zeng, Botao (57852767000)","57852624600; 57852840300; 57852624700; 57852767000","Recent Advances of Generative Adversarial Networks","2022","2022 IEEE 2nd International Conference on Electronic Technology, Communication and Information, ICETCI 2022","","","","558","562","4","10.1109/ICETCI55101.2022.9832194","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136329835&doi=10.1109%2fICETCI55101.2022.9832194&partnerID=40&md5=83aaa802971b30f0fdf89d0acfa7696a","Generative adversarial networks (GANs) have received great attention recently. GAN is an unsupervised learning method that learns representations without highly relying on annotations. GAN comprises two networks called generator and discriminator that training in a competitive process. GANs have made notable progress and promising performance in various applications including image inpainting, image super-resolution, and face synthesis. This paper aims to introduce an overview of GAN, including fundamentals, several up-to-date variants, applications, and challenges of GAN. Finally, we also provide readers with some solutions to mitigate issues existing in GANs. © 2022 IEEE.","Computer vision; Deep learning; Learning systems; Unsupervised learning; Deep learning; Face synthesis; Image Inpainting; Image super resolutions; Learn+; Performance; Resolution synthesis; Unsupervised learning method; Generative adversarial networks","deep learning; generative adversarial networks; unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85136329835"
"Thenmozhi E.; Karunakaran A.; Arunkumar J.R.; Chinnammal V.; Kalaivanan C.; Anitha G.","Thenmozhi, E. (58069234300); Karunakaran, A. (57957619300); Arunkumar, J.R. (57214600246); Chinnammal, V. (57202858211); Kalaivanan, C. (57211477078); Anitha, G. (57213502273)","58069234300; 57957619300; 57214600246; 57202858211; 57211477078; 57213502273","An Efficient Object Detection and Classification from Restored Thermal Images based on Mask RCNN","2022","6th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud), I-SMAC 2022 - Proceedings","","","","639","645","6","10.1109/I-SMAC55078.2022.9987422","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146417975&doi=10.1109%2fI-SMAC55078.2022.9987422&partnerID=40&md5=2d36e1e42cfa71d8686f98d2f4efecc1","In recent years, thermal cameras are extensively employed in several industries, including biometrics, intelligent surveillance, and health monitoring. The thermal cameras' exorbitant price, meanwhile, makes them difficult to obtain. Furthermore, blurring brought on by camera movement, object movement, and focus settings is a problem with thermal photos. There haven't been many research on thermal image-centered picture restoration that focus on such issues. Additionally, it is critical to accelerate the processing capability of image treatment technologies in order to work in tandem with techniques like object tracking and activity detection that make use of temporal data from thermal recordings. Furthermore, no research has been done on the use of thermal pictures for super-resolution rebuilding and deblurring. Due to the inability to discern reflected on the soil surface or wall caused by the heat emitted by the item, previous research on object recognition using thermal imaging include inaccuracies. This paper suggests a deep learning-based technique for thermal image reconstruction that combines deblurring and super-resolution reconstruction in one step. Recent advances in deep learning have shown that approaches based on generative adversarial networks (GANs) perform well in image-to-image translation challenges because they can maintain texture features in pictures and produce finer, more convincing textures than traditional feed forward encoders. This research study suggests a deblur-SRRGAN for thermal image restoration in light of the benefits of GAN. Additionally, a MR-CNN is also recommended to perform object recognition in the thermal image reconstruction.  © 2022 IEEE.","Cameras; Computer vision; Deep learning; Generative adversarial networks; Image classification; Image enhancement; Infrared devices; Infrared imaging; Object detection; Object recognition; Optical resolving power; Restoration; Security systems; Textures; Deblurring; Deep learning; Image deblurring; Images reconstruction; Objects recognition; Reflection detections; Subject and thermal reflection detection; Thermal; Thermal camera; Thermal images; Image reconstruction","deep learning; Image deblurring; subject and thermal reflection detection; Thermal images","Conference paper","Final","","Scopus","2-s2.0-85146417975"
"Manuel C.; Zehnder P.; Kaya S.; Sullivan R.; Hu F.","Manuel, Cyrus (57925258900); Zehnder, Philip (57490388600); Kaya, Sertan (57202700520); Sullivan, Ruth (57754724200); Hu, Fangyao (46061200400)","57925258900; 57490388600; 57202700520; 57754724200; 46061200400","Impact of color augmentation and tissue type in deep learning for hematoxylin and eosin image super resolution","2022","Journal of Pathology Informatics","13","","100148","","","","10.1016/j.jpi.2022.100148","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139727952&doi=10.1016%2fj.jpi.2022.100148&partnerID=40&md5=a55bc91769a4518a653eab7f701a12d6","Single image super-resolution is an important computer vision task with applications including remote sensing, medical imaging, and surveillance. Modern work on super-resolution utilizes deep learning to synthesize high resolution (HR) images from low resolution images (LR). With the increased utilization of digitized whole slide images (WSI) in pathology workflows, digital pathology has emerged as a promising domain for super-resolution. Despite extensive existing research into super-resolution, there remain challenges specific to digital pathology. Here, we investigated image augmentation techniques for hematoxylin and eosin (H&E) WSI super-resolution and model generalizability across diverse tissue types. In addition, we investigated shortcomings with common quality metrics (peak signal-to-noise ratio (PSNR), structure similarity index (SSIM)) by conducting a perceptual quality survey for super-resolved pathology images. High performing deep super-resolution models were used to generate 20X HR images from LR images (5X or 10X equivalent) for 11 different tissues and 30 human evaluators were asked to score the quality of the generated versus the ground truth 20X HR images. The scores given by a human rater and the PSNR or the SSIM were compared to investigate the correlation between model training parameters. We found that models trained on multiple tissues generalized better than those trained on a single tissue type. We also found that PSNR correlated with perceptual quality (R = 0.26) less accurately than did SSIM (R = 0.64), suggesting that the SSIM quality metric is insufficient. The methods proposed in this study can be used to virtually magnify H&E images with better perceptual quality than interpolation methods (i.e., bicubic interpolation) commonly implemented in digital pathology software. The impact of deep SISR methods is more notable when scaling to 4X is needed, such as in the case of super-resolving a low magnification WSI from 10X to 40X. © 2022 The Authors","","Artificial intelligence; Deep learning; Digital pathology; Generative adversarial networks; Histopathology; Image processing; Whole slide imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139727952"
"Dong S.; Hangel G.; Chen E.Z.; Sun S.; Bogner W.; Widhalm G.; You C.; Onofrey J.A.; de Graaf R.; Duncan J.S.","Dong, Siyuan (57404722800); Hangel, Gilbert (55844844800); Chen, Eric Z. (57219440464); Sun, Shanhui (57729165700); Bogner, Wolfgang (26025582100); Widhalm, Georg (26041237300); You, Chenyu (57203062927); Onofrey, John A. (55823011300); de Graaf, Robin (57203077877); Duncan, James S. (57210775364)","57404722800; 55844844800; 57219440464; 57729165700; 26025582100; 26041237300; 57203062927; 55823011300; 57203077877; 57210775364","Flow-Based Visual Quality Enhancer for Super-Resolution Magnetic Resonance Spectroscopic Imaging","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13609 LNCS","","","3","13","10","10.1007/978-3-031-18576-2_1","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141800629&doi=10.1007%2f978-3-031-18576-2_1&partnerID=40&md5=65b391e285a76e1272faaaadb3e5b9cf","Magnetic Resonance Spectroscopic Imaging (MRSI) is an essential tool for quantifying metabolites in the body, but the low spatial resolution limits its clinical applications. Deep learning-based super-resolution methods provided promising results for improving the spatial resolution of MRSI, but the super-resolved images are often blurry compared to the experimentally-acquired high-resolution images. Attempts have been made with the generative adversarial networks to improve the image visual quality. In this work, we consider another type of generative model, the flow-based model, of which the training is more stable and interpretable compared to the adversarial networks. Specifically, we propose a flow-based enhancer network to improve the visual quality of super-resolution MRSI. Different from previous flow-based models, our enhancer network incorporates anatomical information from additional image modalities (MRI) and uses a learnable base distribution. In addition, we impose a guide loss and a data-consistency loss to encourage the network to generate images with high visual quality while maintaining high fidelity. Experiments on a 1H-MRSI dataset acquired from 25 high-grade glioma patients indicate that our enhancer network outperforms the adversarial networks and the baseline flow-based methods. Our method also allows visual quality adjustment and uncertainty estimation. Our code is available at https://github.com/dsy199610/Flow-Enhancer-SR-MRSI. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Deep learning; Generative adversarial networks; Image enhancement; Image resolution; Magnetic resonance; Magnetic resonance imaging; Medical computing; Medical imaging; Adversarial networks; Brain magnetic resonance spectroscopic imaging; Flow based; Flow-based models; Magnetic resonance spectroscopic imaging; Normalizing flow; Resolution limits; Spatial resolution; Superresolution; Visual qualities; Metabolites","Brain MRSI; Normalizing flow; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141800629"
"Kalvankar S.; Pandit H.; Parwate P.; Patil A.; Kamalapur S.","Kalvankar, Shreyas (57219733967); Pandit, Hrushikesh (57221149901); Parwate, Pranav (57221149434); Patil, Atharva (57221084587); Kamalapur, Snehal (24461499800)","57219733967; 57221149901; 57221149434; 57221084587; 24461499800","Astronomical Image colorization and up-scaling with Conditional Generative Adversarial Networks","2022","Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","P-326","","","489","498","9","10.18420/inf2022_40","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139768949&doi=10.18420%2finf2022_40&partnerID=40&md5=4a363e6d9d27dc6dd02667adba841c84","This research aims to provide an automated approach for the problem of Image colorization and Single Image Super Resolution by focusing on a very specific domain: astronomical images, using Generative Adversarial Networks. We explore the usage of various models in RBG and L*a*b color spaces. We use transfer learning owing to a small data set, using pre-trained ResNet-18 as a backbone encoder and fine-tune it further. The model produces visually appealing images that are high resolution and colorized. We present our results by evaluating the GANs quantitatively using distance metrics such as L1 distance and L2 distance in each of the color spaces across all channels to provide a comparative analysis. We use Fréchet inception distance (FID) to compare the distribution of the generated images and real image to assess the model's performance. © 2022 Gesellschaft fur Informatik (GI). All rights reserved.","Color; Optical resolving power; Astronomical images; Automated approach; Image colorizations; Image super resolutions; L*a*b* color space; Single image super resolution; Single images; Small data set; Transfer learning; Upscaling; Generative adversarial networks","Astronomical images; Generative Adversarial Networks; Image Colorization; Single Image Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85139768949"
"Boby A.; Brown D.","Boby, Alden (57680180600); Brown, Dane (57189031295)","57680180600; 57189031295","Improving Licence Plate Detection Using Generative Adversarial Networks","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13256 LNCS","","","588","601","13","10.1007/978-3-031-04881-4_47","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129813795&doi=10.1007%2f978-3-031-04881-4_47&partnerID=40&md5=9bf5e44219740a5f56e81fff653230da","The information on a licence plate is used for traffic law enforcement, access control, surveillance and parking lot management. Existing licence plate recognition systems work with clear images taken under controlled conditions. In real-world licence plate recognition scenarios, images are not as straightforward as the ‘toy’ datasets used to benchmark existing systems. Real-world data is often noisy as it may contain occlusion and poor lighting, obscuring the information on a licence plate. Cleaning input data before using it for licence plate recognition is a complex problem, and existing literature addressing the issue is still limited. This paper uses two deep learning techniques to improve licence plate visibility towards more accurate licence plate recognition. A one-stage object detector popularly known as YOLO is implemented for locating licence plates under challenging situations. Super-resolution generative adversarial networks are considered for image upscaling and reconstruction to improve the clarity of low-quality input. The main focus involves training these systems on datasets that include difficult to detect licence plates, enabling better performance in unfavourable conditions and environments. © 2022, Springer Nature Switzerland AG.","Access control; Deep learning; Generative adversarial networks; Image enhancement; Information management; Object recognition; Optical character recognition; Complex problems; Controlled conditions; Existing systems; Input datas; License plate detection; License plate recognition systems; Licenses plate recognition; Parking lots; Real-world; Traffic laws; Object detection","Generative adversarial networks; Object detection; Optical character recognition","Conference paper","Final","","Scopus","2-s2.0-85129813795"
"Köprülü M.; Eskil M.T.","Köprülü, Mertali (58068915800); Eskil, M. Taner (57199604490)","58068915800; 57199604490","Analysis of Single Image Super Resolution Models","2022","International Conference on Electrical, Computer, Communications and Mechatronics Engineering, ICECCME 2022","","","","","","","10.1109/ICECCME55909.2022.9988599","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146434856&doi=10.1109%2fICECCME55909.2022.9988599&partnerID=40&md5=cdaaecc941f2e31ec968f62e40eb5aae","Image Super-Resolution (SR) is a set of image processing techniques which improve the resolution of images and videos. Deep learning approaches have made remarkable improvement in image super-resolution in recent years. This article aims and seeks to provide a comprehensive analysis on recent advances of models which has been used in image super-resolution. This study has been investigated over other essential topics of current model problems, such as publicly accessible benchmark data-sets and performance evaluation measures. Finally, The study concluded these analysis by highlighting several weaknesses of existing base models as their feeding strategy and approved that the training technique which is Blind Feeding, which led several model to achieve state-of-the art. © 2022 IEEE.","Benchmarking; Convolutional neural networks; Deep learning; Image analysis; Image enhancement; Optical resolving power; Comprehensive analysis; Convolutional neural network; Current modeling; Image processing technique; Image super resolutions; Images processing; Learning approach; Single image super resolution; Single images; Super-resolution models; Generative adversarial networks","Convolutional Neural Network; Generative Adversarial Networks; Image Processing; Single Image Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85146434856"
"Ambudkar S.; Raj R.; Billa K.; Hukumchand R.","Ambudkar, Shravan (57937361900); Raj, Rahul (57937215100); Billa, Karthik (57937362000); Hukumchand, Richa (57937953900)","57937361900; 57937215100; 57937362000; 57937953900","Super-Resolution for Cross-Sensor Optical Remote Sensing Images","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1880","1883","3","10.1109/IGARSS46834.2022.9883182","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140403342&doi=10.1109%2fIGARSS46834.2022.9883182&partnerID=40&md5=cb1f1f61246b1ec07e09992c1e85c92f","Generative adversarial network (GAN) models are becoming popular in the field of remote sensing for generating high spatial resolution images from their low resolution versions. In this study, four models including two basic Super-resolution GAN models and two non-GAN Deep Learning models were trained and tested to achieve 2.5m, and 5m spatial resolution from their 10m spatial resolution satellite data. The comparison of results showed that the SRGAN model performed better than the other deep learning models. The performance metrics were also found to be consistent with available literature. © 2022 IEEE.","Deep learning; Image enhancement; Image resolution; Learning systems; Optical remote sensing; High spatial resolution images; Learning models; Lower resolution; Network models; Optical remote sensing; Remote sensing images; Remote-sensing; Resolution enhancement; Spatial resolution; Superresolution; Generative adversarial networks","generative adversarial network; resolution enhancement; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85140403342"
"Sui W.-C.; Cheng X.; Chan H.A.","Sui, Wan-Chi (58043552400); Cheng, Xi (57206756990); Chan, H. Anthony (57795253200)","58043552400; 57206756990; 57795253200","Critical Review on Deep Learning and Smart Technologies for Image Super-Resolution","2022","IEEE Region 10 Annual International Conference, Proceedings/TENCON","2022-November","","","","","","10.1109/TENCON55691.2022.9977489","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145660043&doi=10.1109%2fTENCON55691.2022.9977489&partnerID=40&md5=dfe30f7fd2de3a5bd98754588768f51d","Image super-resolution is an extremely useful way to improve the quality of an image. It is miracle that making use of the current signal processing and deep learning technologies, the image can look much appealing after the super-solution. This review paper is to highlight important techniques, especially to point out recent key contributions to make superior success of super-resolution of the recent years, especially on face super-resolution. We will start with a very brief and quick review on using conventional signal processing and classic learning approaches for super-resolution, and then concentrate on giving the advantages of deep learning, in particular, the recent powerful concepts on using latent vector and facial priors to achieve superior performance. Further topics of discussion include generative adversarial network (GAN), StyleGAN, latent space, facial priors and diffusion models. Our concentration is on the reasons for the success of these techniques. Attractive demonstrations on a few state-of-the-art models, including some of our work, are provided © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Image enhancement; Learning systems; Optical resolving power; Critical review; Current signal; Deep learning; Face super-resolution; Image super resolutions; Learning technology; Machine-learning; Signal-processing; Smart technology; Superresolution; Image reconstruction","deep learning; face-super-resolution; Image restoration; machine learning; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85145660043"
"Wang Y.; Li X.; Nan F.; Liu F.; Li H.; Wang H.; Qian Y.","Wang, Yongqiang (57222249068); Li, Xue (57222711280); Nan, Fangzhe (57217150115); Liu, Feng (57221201867); Li, Hua (57222248938); Wang, Haitao (57418857000); Qian, Yurong (26027209500)","57222249068; 57222711280; 57217150115; 57221201867; 57222248938; 57418857000; 26027209500","Image super-resolution reconstruction based on generative adversarial network model with feedback and attention mechanisms","2022","Multimedia Tools and Applications","81","5","","6633","6652","19","10.1007/s11042-021-11679-1","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123105161&doi=10.1007%2fs11042-021-11679-1&partnerID=40&md5=af5d873bcde16cad2224382d7e268d81","Despite the rapid development of single-image super-resolution (SISR) methods of generative adversarial networks (GAN), which can reconstruct visually realistic images, the problem of high discrepancy between the recovered details or textures and the ground truth persists. To address this issue, an SISR reconstruction GAN based on a feedback and attention mechanism (FBSRGAN) is proposed. Specifically, we select a network with a feedback mechanism as the generator, which can gradually create high-resolution images through the feedback connection. The attention mechanism is combined with the feedback block to adaptively select useful feature information and effectively process the feedback stream and enhance the image output quality. We use the relative average least squares GAN loss to reduce the instability of the optimization generator process to guide GAN to obtain more realistic results. The results show that compared with the ESRGAN method, when the amplification factor is 4, PSNR and SSIM of the proposed method increase by 0.386 and 0.0141, respectively, and PI decreases by 0.284, while the number of parameters is only 18.5% of that of ESRGAN, when tested on the Set5 dataset. Compared with existing GAN-based SR methods, FBSRGAN achieves superior performance in terms of both perceptual ability and image distortion. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Feedback control; Image enhancement; Image reconstruction; Optical resolving power; Textures; Attention mechanisms; Channel attention; Feedback mechanisms; Image super resolutions; Image super-resolution reconstruction; Network models; Network-based; RaLSGAN; Single images; Super-resolution reconstruction; Generative adversarial networks","Channel attention; Feedback mechanism; Generative adversarial network; RaLSGAN; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85123105161"
"Cai F.; Wu K.; Jia H.; Wang F.","Cai, Feng (57888175100); Wu, Keyu (57324654500); Jia, Hecheng (57226296459); Wang, Feng (56459216100)","57888175100; 57324654500; 57226296459; 56459216100","Super Resolution of Airplane Target in Remote Sensing Images via A Multi-Degradation Model","2022","2022 IEEE 14th International Conference on Advanced Infocomm Technology, ICAIT 2022","","","","330","333","3","10.1109/ICAIT56197.2022.9862621","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137903222&doi=10.1109%2fICAIT56197.2022.9862621&partnerID=40&md5=c958b4d073294c3a993303cfbb590c6f","In this paper, we argue that the degradation model assumed by existing work on super-resolution of remote sensing images deviates from those in real-world images. To address the above problem, this article proposes a super-resolution reconstruction method for unpaired airplanes in remote sensing images, which consists of a model that simulates the multiple degradations in real-world scenarios and a super-resolution generative adversarial network that generates high-resolution images. Specifically, the novel degradation model can cover a wide range of degradations in natural scenes, which contains diverse factors of additive Gaussian noise, Poisson noise, Brown Gaussian noise, Gaussian blur, etc. Experiments are conducted on the airplane targets in the Gaofen challenge dataset, and the results demonstrate the superiority of our method compared to state-of-the-art methods, especially in raw remote sensing images with multiple noise and blur, which may be preferred in other practical satellite applications with harsh circumstances.  © 2022 IEEE.","Computer vision; Gaussian distribution; Gaussian noise (electronic); Generative adversarial networks; Optical resolving power; Remote sensing; Aircraft target in remote sensing image; Aircraft targets; Degradation model; Image super resolutions; Multiple degradation model; Multiple degradations; Remote sensing image super-resolution; Remote sensing images; Superresolution; Aircraft","aircraft targets in remote sensing images; multiple degradation model; remote sensing image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85137903222"
"Shukla A.; Upadhyay A.; Sharma M.; Chinnusamy V.; Kumar S.","Shukla, Ankit (57196584648); Upadhyay, Avinash (57205628981); Sharma, Manoj (57217742709); Chinnusamy, Viswanathan (7006424678); Kumar, Sudhir (57216090668)","57196584648; 57205628981; 57217742709; 7006424678; 57216090668","HIGH-RESOLUTION NIR PREDICTION FROM RGB IMAGES: APPLICATION TO PLANT PHENOTYPING","2022","Proceedings - International Conference on Image Processing, ICIP","","","","4058","4062","4","10.1109/ICIP46576.2022.9897670","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146723454&doi=10.1109%2fICIP46576.2022.9897670&partnerID=40&md5=018885cfac1752038abef9358c7ff254","In contrast to the conventional RGB cameras, Near-infrared (NIR) spectroscopy provides images with rich information concerning the biological process of plants. However, NIR spectroscopy is a costly affair and produces low-resolution (LR) images. In this context, recently deep learning-based methods have been proposed in computer vision. In addition, the development of phenomics facilities has facilitated the generation of large plant data necessary for the utilization of these deep learning-based methods. Motivated by these developments, we propose a novel attention-based pix-to-pix generative adversarial network (GAN) followed by a super-resolution (SR) module to generate high-resolution (HR) NIR images from corresponding RGB images. An experiment including extraction of phenotypic data based on HR NIR images has also been conducted to evaluate its efficacy from an agricultural perspective. Our proposed architecture achieved state-of-the-art performance in terms of MRAE and RMSE on the Wheat plant multi-modality dataset. © 2022 IEEE.","Computer vision; Deep learning; Generative adversarial networks; Infrared devices; Attention; Deep learning; High resolution; Image applications; Learning-based methods; Near- infrared images; Near-infrared prediction; Pix-to-pix; Plant phenotyping; RGB images; Near infrared spectroscopy","Attention; CNN; Deep learning; NIR Prediction; Pix-to-pix; Plant Phenotyping","Conference paper","Final","","Scopus","2-s2.0-85146723454"
"","","","International Conference on Computer Graphics, Artificial Intelligence, and Data Processing, ICCAID 2021","2022","Proceedings of SPIE - The International Society for Optical Engineering","12168","","","","","749","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128266174&partnerID=40&md5=e4ae1278859b6775788b16ae6148a39c","The proceedings contain 109 papers. The topics discussed include: research on the design of bedroom space environment layout based on computer modeling technology; application of computer modeling technology in furniture design; image inpainting by reducing edge blur and error accumulation; a new depth residual network combined recurrent with residual structure for human action recognition from videos; image super-resolution for endangered animals using refined super-resolution generative adversarial network; transformer in image interpretation; iterative up and downsampling residual networks for single image super-resolution; multivariate prediction of retail sales by multi-task time series learning; and a height inversion scheme of cylindrical oil tank with single high-resolution image.","","","Conference review","Final","","Scopus","2-s2.0-85128266174"
"Farhangfar S.; Baradarani A.; Balafar M.A.; Asadpour M.","Farhangfar, Saghar (57210587570); Baradarani, Aryaz (24460552900); Balafar, Mohammad Ali (25227623000); Asadpour, Mohammad (56667337000)","57210587570; 24460552900; 25227623000; 56667337000","SSTRN: Semantic Style Transfer Reference Network for Face Super-Resolution","2022","International Conference on Systems, Signals, and Image Processing","2022-June","","","","","","10.1109/IWSSIP55020.2022.9854432","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137161982&doi=10.1109%2fIWSSIP55020.2022.9854432&partnerID=40&md5=a69ed3bd550a82dc85cb37b2bf89664f","Reference super-resolution (RefSR) has achieved promising results in the single image super-resolution (SISR) field by providing additional details from the reference images. Existing RefSR methods usually tend to extract similar or aligned features from reference images to further enhance the resolution of the final result. Therefore, the efficiency of RefSR models highly depends on the conformity between extracted features from the low-resolution (LR) and reference images. In this paper, we propose a new reference image generation scheme via semantic style transfer to unleash our model from relevant feature extraction computations. The generated reference images have the most content similarity and identical alignment with the LR input that compensates for the lost details of the LR images. Despite previous RefSR methods that rely on extracting and transferring texture information from the reference image to LR input, provided reference images are enriched with the style information of high-resolution (HR) images. Extensive experiments indicate the effectiveness of the proposed reference images.  © 2022 IEEE.","Image enhancement; Optical resolving power; Semantic Segmentation; Semantic Web; Semantics; Textures; Face images; Face super-resolution; Low resolution images; Lower resolution; Reference image; Reference network; Reference-based super-resolution; Semantic style transfer; Superresolution; Superresolution methods; Generative adversarial networks","Face images; Generative adversarial networks; Reference-based super-resolution; Semantic style transfer","Conference paper","Final","","Scopus","2-s2.0-85137161982"
"Boby A.; Brown D.; Connan J.; Marais M.","Boby, Alden (57680180600); Brown, Dane (57189031295); Connan, James (55876936700); Marais, Marc (57679253500)","57680180600; 57189031295; 55876936700; 57679253500","Investigating the Effects of Image Correction Through Affine Transformations on Licence Plate Recognition","2022","5th International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems, icABCD 2022 - Proceedings","","","","","","","10.1109/icABCD54961.2022.9856380","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138002113&doi=10.1109%2ficABCD54961.2022.9856380&partnerID=40&md5=71154fef20b17a82b0f906226ac8e1f7","Licence plate recognition has many real-world applications, which fall under security and surveillance. Deep learning for licence plate recognition has been adopted to improve existing image-based processing techniques in recent years. Object detectors are a popular choice for approaching this task. All object detectors are some form of a convolutional neural network. The You Only Look Once framework and Region-Based Convolutional Neural Networks are popular models within this field. A novel architecture called the Warped Planar Object Detector is a recent development by Zou et al. that takes inspiration from YOLO and Spatial Network Transformers. This paper aims to compare the performance of the Warped Planar Object Detector and YOLO on licence plate recognition by training both models with the same data and then directing their output to an Enhanced Super-Resolution Generative Adversarial Network to upscale the output image, then lastly using an Optical Character Recognition engine to classify characters detected from the images. © 2022 IEEE.","Convolution; Convolutional neural networks; Deep learning; Image enhancement; Object detection; Object recognition; Optical character recognition; Optical resolving power; Affine transformations; Convolutional neural network; Image-correction; Licenses plate recognition; Object detectors; Objects detection; Planar objects; Spatial network; Spatial network transformer; Superresolution; Generative adversarial networks","generative adversarial network; object detection; optical character recognition; spatial network transformer; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85138002113"
"Thuan N.D.; Dong T.P.; Manh B.Q.; Thai H.A.; Trung T.Q.; Hong H.S.","Thuan, Nguyen Duc (36476808400); Dong, Trinh Phuong (57966148000); Manh, Bui Quang (57966324500); Thai, Hoang Anh (57966177700); Trung, Tran Quang (57966206800); Hong, Hoang Si (57956330600)","36476808400; 57966148000; 57966324500; 57966177700; 57966206800; 57956330600","Edge-Focus Thermal Image Super-Resolution using Generative Adversarial Network","2022","2022 International Conference on Multimedia Analysis and Pattern Recognition, MAPR 2022 - Proceedings","","","","","","","10.1109/MAPR56351.2022.9924742","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141938189&doi=10.1109%2fMAPR56351.2022.9924742&partnerID=40&md5=f9fc2a4800581bcb3031eaaea245aa40","Thermal imaging has played an important role in a wide range of areas of life. However, thermal cameras often produce low-resolution images, which limits the ability to observe objects in thermal imaging applications. Modern thermal cameras often include a built-in high-resolution visible camera to supplement the thermal image information. This paper proposes a method to increase the resolution of thermal images using edge features of corresponding high-resolution visible images. The Canny edge detection and thin-line downscaling algorithms are used to generate edge maps from high-resolution visible images to contribute to the super-resolution network. The proposed super-resolution model is designed based on generative adversarial network architecture for times 2, times3, and times 4 magnification. The KAIST dataset is used to train and test the model. Peak signal-to-noise ratio (PSNR) and structure similarity index (SSIM) are used to evaluate the quality of super-resolution images. After the training process, to prove the effectiveness of edge features, we compare the quality of the super-resolution images generated from the proposed method with other methods. The comparison results show that the proposed method has the highest performance in terms of PSNR and SSIM.  © 2022 IEEE.","Cameras; Image enhancement; Infrared devices; Infrared imaging; Network architecture; Optical resolving power; Signal to noise ratio; Statistical tests; Edge features; Edge map; Enhancement; GAN; High resolution visible; Peak signal to noise ratio; Superresolution; Thermal camera; Thermal images; Visible image; Generative adversarial networks","edge feature; edge map; enhancement; GAN; super-resolution; thermal image","Conference paper","Final","","Scopus","2-s2.0-85141938189"
"Kim Y.; Ha J.; Cho Y.; Kim J.","Kim, Youngsoo (57268546200); Ha, Jeonghyo (57204643677); Cho, Yooshin (57244652200); Kim, Junmo (36015494900)","57268546200; 57204643677; 57244652200; 36015494900","Unsupervised Blur Kernel Estimation and Correction for Blind Super-Resolution","2022","IEEE Access","10","","","45179","45189","10","10.1109/ACCESS.2022.3170053","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129637528&doi=10.1109%2fACCESS.2022.3170053&partnerID=40&md5=af6303982c82902f564c9d8d346ee7f4","Blind super-resolution (blind-SR) is an important task in the field of computer vision and has various applications in real-world. Blur kernel estimation is the main element of blind-SR along with the adaptive SR networks and a more accurately estimated kernel guarantees a better performance. Recently, generative adversarial networks (GANs), comparing recurrence patches across scales, have been the most successful unsupervised kernel estimation methods. However, they still involve several problems. Their sharpness discrimination ability has been noted as being too weak, causing them to focus more on pattern shapes than sharpness. In some cases, kernel correction processes were omitted; however, these are essential because the optimally generated kernel may be narrower than a point spread function (PSF) except when the PSF is ideal low-pass filter. Previous studies also did not consider that GANs are affected by the thickness of edges as well as PSF. Thus, in this paper, 1) we propose a degradation and ranking comparison process designed to induce GAN models to became sensitive to image sharpness, and 2) propose a scale-free kernel correction technique using Gaussian kernel approximation including a thickness parameter. To improve the kernel accuracy further, we 3) propose a combination model of the proposed GAN and DIP(deep image prior) for more supervision, and designed a kernel correction network to propagate gradients through developed correction method. Several experiments demonstrate that our methods enhanced the l2 error and the shape of the kernel significantly. In addition, by combining with ordinary blind-SR algorithms, the best reconstruction accuracy was achieved among unsupervised blur kernel estimation methods.  © 2013 IEEE.","Blind equalization; Edge detection; Electronics packaging; Estimation; Generative adversarial networks; Image enhancement; Low pass filters; Optical transfer function; Blind Super-resolution; Blur kernel estimations; Deep image prior; Electronic Packaging; Image edge detection; Image priors; Kernel; Kernel correction; Kernel estimation; Superresolution; Optical resolving power","Blind super-resolution; deep image prior; generative adversarial networks; kernel correction; kernel estimation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85129637528"
"Jinkua L.; Chenxiang Y.; Abdalla H.B.","Jinkua, Liu (58061069500); Chenxiang, Yang (58061069600); Abdalla, Hemn Barzan (56940231900)","58061069500; 58061069600; 56940231900","Enhanced style transfer with colorization and super-resolution","2022","Proceedings - 2022 7th International Conference on Communication, Image and Signal Processing, CCISP 2022","","","","166","172","6","10.1109/CCISP55629.2022.9974475","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146114495&doi=10.1109%2fCCISP55629.2022.9974475&partnerID=40&md5=57311e3a4cba697008dd2391f3175488","Style transfer is a novel and successful technology in the field of computer vision which allow people to create art pieces without training. This research has combined style transfer, colorization, and super-resolution algorithm to create a method to create art pieces from the black-white image as content and sketch art pieces as style with high resolution. This method could significantly lower the demand for art creation and allow people with little artistic skill to create desired artwork. Moreover, this research compares the impact of the different parameters in style transfer and the influence of the colorization in different processing stages resulting that colorizing the content image before style transfer would create a single style image that is more controllable but colorizing the generated image after style transfer would result in a more unpredictable multi-style image which depends on the training dataset of the image and the ratio of the weight in style transfer. © 2022 IEEE.","Arts computing; Convolutional neural networks; Image enhancement; Optical resolving power; Black-white images; Convolutional neural network; High resolution; Image colorizations; Processing stage; Style transfer; Super resolution algorithms; Superresolution; Training dataset; Generative adversarial networks","Convolutional Neural Network; Generative Adversarial Networks; Image Colorization; Style transfer; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85146114495"
"","","","28th International Conference on MultiMedia Modeling, MMM 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13142 LNCS","","","","","1222","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127121118&partnerID=40&md5=8d5c0b257d59d87c75269789cd2eec7d","The proceedings contain 107 papers. The special focus in this conference is on MultiMedia Modeling. The topics include: Non-Uniform Attention Network for Multi-modal Sentiment Analysis; combining Knowledge and Multi-modal Fusion for Meme Classification; bi-attention Modal Separation Network for Multimodal Video Fusion; Melody Generation from Lyrics Using Three Branch Conditional LSTM-GAN; a-Muze-Net: Music Generation by Composing the Harmony Based on the Generated Melody; Speech Intelligibility Enhancement By Non-Parallel Speech Style Conversion Using CWT and iMetricGAN Based CycleGAN; time-Frequency Attention for Speech Emotion Recognition with Squeeze-and-Excitation Blocks; Real-Time FPGA Design for OMP Targeting 8K Image Reconstruction; one-Stage Image Inpainting with Hybrid Attention; MF-GAN: Multi-conditional Fusion Generative Adversarial Network for Text-to-Image Synthesis; fast Single Image Dehazing Using Morphological Reconstruction and Saturation Compensation; arbitrary Style Transfer with Adaptive Channel Network; point Cloud Upsampling via a Coarse-to-Fine Network; JVCSR: Video Compressive Sensing Reconstruction with Joint In-Loop Reference Enhancement and Out-Loop Super-Resolution; SAM: Self Attention Mechanism for Scene Text Recognition Based on Swin Transformer; A Multiple Positives Enhanced NCE Loss for Image-Text Retrieval; multimodal Embedding for Lifelog Retrieval; prediction of Blood Glucose Using Contextual LifeLog Data; fall Detection Using Multimodal Data; an Investigation into Keystroke Dynamics and Heart Rate Variability as Indicators of Stress; PF-VTON: Toward High-Quality Parser-Free Virtual Try-On Network; joint Re-Detection and Re-Identification for Multi-Object Tracking; Multi-scale Cross-Modal Transformer Network for RGB-D Object Detection; A Complementary Fusion Strategy for RGB-D Face Recognition; double Granularity Relation Network with Self-criticism for Occluded Person Re-identification; Using Explainable AI to Identify Differences Between Clinical and Experimental Pain Detection Models Based on Facial Expressions; rating-Aware Self-Organizing Maps; preface.","","","Conference review","Final","","Scopus","2-s2.0-85127121118"
"Khaliq M.M.; Mumtaz R.","Khaliq, Muhammad Mahad (58040347500); Mumtaz, Rafia (16176156800)","58040347500; 16176156800","Enhancing NDVI Calculation of Low-Resolution Imagery using ESRGANs","2022","2022 24th International Multitopic Conference, INMIC 2022","","","","","","","10.1109/INMIC56986.2022.9972928","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145435609&doi=10.1109%2fINMIC56986.2022.9972928&partnerID=40&md5=cec264ace310a984ca42b802206d05cb","Normalized Difference Vegetation Index (NDVI) has been one of the key scales for monitoring multiple plant parameters, but satellite imagery is never up to date, which makes it difficult to get readings for the recent situation of field crops. Doing so with Unmanned Aerial System, drone, in this case, is an intricate task, but with its advantages which include timely and effective measurements with the least errors to be fixed in post-processing of data. Before this, NDVI has been calculated using an Unmanned Aerial System, but the problem of the low resolution of the imagery always lingers. With the recent advancement of generated adversarial networks, the up-scaling of images has been made possible, which, if done with the right model, rules out the need for upgrading the camera hardware that is never cost-effective. We have come up with the solution of calculating the vegetation index of field crops by implementing Enhanced Super-Resolution Generated Adversarial Networks with drone imagery to calculate the vegetation index of crop fields. A simple near-infrared spectrum camera is usually not capable of producing a higher resolution image, by implementing the aforementioned generated adversarial network, we have been able to calculate vegetation index for a comparably much higher resolution image without upgrading with sophisticated hardware. We were able to perform the calculations for more pixels (12952) against the same area yielded an output value of 0.829 as compared to 0.828 in the case of low-resolution imagery (546416 pixels). The averaged values for red and near-infrared pixels showed changes from 32.337 to 30.264 for red, and from 189.168 to 182.1656 for near-infrared pixels. The results produced with this technique are different from those generated using original images which account for a new gateway in the calculation of the NDVI.  © 2022 IEEE.","Antennas; Cameras; Cost effectiveness; Crops; Data handling; Deep learning; Gateways (computer networks); Generative adversarial networks; Image enhancement; Infrared devices; Optical resolving power; Pixels; Satellite imagery; Vegetation mapping; Adversarial networks; Field crops; High-resolution images; Low-resolution imagery; Machine-learning; Multispectral imagery; Normalized difference vegetation index; Superresolution; Unmanned aerial systems; Vegetation index; Drones","Generative Adversarial Networks; Machine Learning; Multispectral imagery; NDVI; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85145435609"
"Shao J.; Zhuang X.; Wang Z.; Shen W.","Shao, Jie (56418813500); Zhuang, Xuecheng (57971777900); Wang, Zhengqi (57971778000); Shen, Wenzhong (36778235100)","56418813500; 57971777900; 57971778000; 36778235100","Pixel-level self-paced adversarial network with multiple attention in single image super-resolution","2022","Signal, Image and Video Processing","","","","","","","10.1007/s11760-022-02397-8","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142228931&doi=10.1007%2fs11760-022-02397-8&partnerID=40&md5=654cbe0480490e7d0bc1659f664c0168","Image super-resolution (SR) is an important image processing technique in computer vision. Although the convolutional neural network has developed rapidly and made some breakthroughs in the field of super-division, there are still some problems when images are magnified at large upscaling factors. Recently, generative adversarial network is popular, but the structural similarity (SSIM) between the super-resolution (SR) image generated by GAN network and high-resolution (HR) image is always unsatisfactory. In this paper, we propose a pixel-level self-paced adversarial network with multiple attention (PSPA) method to reduce the noise of SR image and increase its structural similarity with HR image. The combination of multiple attentions makes the model grasp the global information and restore the detail texture more accurately. The PSPA network can make the model notice the position with a large difference between the pixel values of SR and HR images and speed up the gradient descent speed. Our method shows excellent performance on Set5, Set14 and BSD100 datasets and overcomes many popular algorithms. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","","Dual attention; GAN; PSPA; Super-resolution","Article","Article in press","","Scopus","2-s2.0-85142228931"
"Rashid S.I.; Shakibapour E.; Ebrahimi M.","Rashid, Shawkh Ibne (57819713800); Shakibapour, Elham (57819713900); Ebrahimi, Mehran (14833990500)","57819713800; 57819713900; 14833990500","SINGLE MR IMAGE SUPER-RESOLUTION USING GENERATIVE ADVERSARIAL NETWORK","2022","15th International Conference on ICT, Society and Human Beings, ICT 2022, 19th International Conference on Web Based Communities and Social Media, WBC 2022 and 14th International Conference on e-Health, EH 2022 - Held at the 16th Multi Conference on Computer Science and Information Systems, MCCSIS 2022","","","","181","188","7","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142234378&partnerID=40&md5=30f38a1627882ca28411af21a65a0176","Spatial resolution of medical images can be improved using super-resolution methods. Real Enhanced Super Resolution Generative Adversarial Network (Real-ESRGAN) is one of the recent effective approaches utilized to produce higher resolution images, given input images of lower resolution. In this paper, we apply this method to enhance the spatial resolution of 2D MR images. In our proposed approach, we slightly modify the structure of the Real-ESRGAN to train 2D Magnetic Resonance images (MRI) taken from the Brain Tumor Segmentation Challenge (BraTS) 2018 dataset. The obtained results are validated qualitatively and quantitatively by computing SSIM (Structural Similarity Index Measure), NRMSE (Normalized Root Mean Square Error), MAE (Mean Absolute Error) and VIF (Visual Information Fidelity) values. © 2022 15th International Conference on ICT, Society and Human Beings, ICT 2022, 19th International Conference on Web Based Communities and Social Media, WBC 2022 and 14th International Conference on e-Health, EH 2022 - Held at the 16th Multi Conference on Computer Science and Information Systems, MCCSIS 2022. All rights reserved.","Deep learning; Generative adversarial networks; Image resolution; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Mean square error; Medical imaging; Deep learning; Effective approaches; Image super resolutions; MR image enhancement; MR-images; Single MR image super resolution; Spatial resolution; Superresolution; Superresolution methods; Image enhancement","Deep learning; Generative Adversarial Network; Imaging; MR Image Enhancement; Single MR Image Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85142234378"
"Cheng J.-R.C.; Stanford C.; Glandon S.R.; Lam A.L.; Williams W.R.","Cheng, Jing-Ru C. (57226796169); Stanford, Corwin (57932754800); Glandon, Steven R. (57191408473); Lam, Anthony L. (57932754900); Williams, Warren R. (57932928100)","57226796169; 57932754800; 57191408473; 57932754900; 57932928100","Macro benchmarking edge devices using enhanced super-resolution generative adversarial networks (ESRGANs)","2023","Journal of Supercomputing","79","5","","5360","5373","13","10.1007/s11227-022-04819-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140115968&doi=10.1007%2fs11227-022-04819-3&partnerID=40&md5=9781df1c34cd0a6a1c1766fac7f54268","In standard machine learning implementations, training and inference takes place on servers located remotely from where data is gathered. With the advent of the Internet of Things (IoT), the groundwork is laid to shift half of that computing burden (inferencing) closer to where data is gathered. This paradigm shift to edge computing can significantly decrease the latency and cost of these tasks. Many small, powerful devices have been developed in recent years with the potential to fulfill that goal. In this paper, we analyze two such devices, the NVIDIA Jetson AGX Xavier Developer Kit and the Microsoft Azure Stack Edge Pro (2 GPUs). In addition, the NVIDIA DGX-1 system containerized in a ruggedized case is also taken for running inference model at the Edge. For comparison, the performance of these devices is compared to more common inferencing devices, including a laptop, desktop, and high performance computing (HPC) system. The inferencing model used for testing is the Enhanced Super-Resolution Generative Adversarial Networks (ESRGANs), which was developed using techniques borrowed primarily from other GAN designs, most notably SRGANs and Relativistic average GANs (RaGANs), along with some novel techniques. Metrics chosen for benchmarking were inferencing time, GPU power consumption, and GPU temperature. We found that inferencing using ESRGANs was approximately 10 to 20 times slower on the Jetson edge device, but used approximately 100 to 300 times less power, and was approximately 2 times cooler than any of the other devices tested. The inferencing using ESRGANs performed very similarly on the Azure device as on the more traditional methods. The Azure device performed with slightly slower speeds and equivalent temperatures to the other devices, but with slightly less power consumption. © 2022, This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply.","Benchmarking; Computing power; Deep learning; Edge computing; Generative adversarial networks; Internet of things; Optical resolving power; Program processors; Benchmark; Deep learning; Edge computing; Inference; Machine-learning; MicroSoft; Paradigm shifts; Powerful devices; Standard machines; Superresolution; Electric power utilization","Benchmark; Deep learning; Edge computing; Generative adversarial network; Inference","Article","Final","","Scopus","2-s2.0-85140115968"
"Zaini M.; Perkins J.; Cheng H.; Gholipour B.","Zaini, M. (57964324900); Perkins, J. (57347126900); Cheng, H. (57963595600); Gholipour, B. (36016553000)","57964324900; 57347126900; 57963595600; 36016553000","Bridging the gap between electron and optical microscopy through neural network-enabled training and imaging","2022","Proceedings of SPIE - The International Society for Optical Engineering","12239","","122390K","","","","10.1117/12.2633249","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141837792&doi=10.1117%2f12.2633249&partnerID=40&md5=828da4dfc33d065bf20ea84d76259a27","Obtaining high-resolution images using an optical microscope is critical when dealing with micro/nanoscale objects. Current techniques use high magnification objective lenses with high numerical apertures to resolve closely spaced objects at the micron/nanoscale. However, these lenses often require additional optics and have a narrow depth of field, preventing ease of use. To date, scanning electron microscopy (SEM) is used for imaging beyond the diffraction limit and has led to various breakthroughs in semiconductor physics and nanotechnology. An alternative to an SEM is using artificial intelligence (AI) to enable super-resolution techniques with correlated image sets. We utilize a convolutional neural network (CNN) and generative adversarial network (GAN) to train correlated images gathered from higher magnification SEM and lower magnification SEM, resulting in a model that enables resolving nanoscale features. We demonstrated that by training a neural network with SEM images, we are able to aid the optical microscope to image beyond the diffraction limit with a resolution closer to the SEM. © 2022 SPIE. All rights reserved.","Convolutional neural networks; Deep learning; Diffraction; Gallium nitride; Generative adversarial networks; III-V semiconductors; Nanotechnology; Optical data storage; Optical microscopy; Optical resolving power; Deep-learning; Diffraction limits; High magnifications; High-resolution images; Machine-learning; Metagrating; Nano scale; Neural-networks; Optical microscopes; Superresolution; Scanning electron microscopy","Deep-learning; Machine Learning; Metagrating; Nanoscale; Optical Microscopy; Scanning Electron Microscopy; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85141837792"
"Pourhadi N.; Shafizadeh B.; Eshghi F.; Kelarestaghi M.","Pourhadi, Navid (57911937900); Shafizadeh, Behnoud (57912138400); Eshghi, Farshad (56007462200); Kelarestaghi, Manoochehr (55486054400)","57911937900; 57912138400; 56007462200; 55486054400","YOLOv5-based ALPR Improvement using Selective SR-GAN","2022","2022 2nd International Conference on Computing and Machine Intelligence, ICMI 2022 - Proceedings","","","","","","","10.1109/ICMI55296.2022.9873675","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139070474&doi=10.1109%2fICMI55296.2022.9873675&partnerID=40&md5=2f08c2523d431dcc75b5c9d3a9ecfc67","Automatic License-Plate Recognition (ALPR) has widespread use in Intelligent Transportation Systems (ITS), security and surveillance systems, and crime investigation. However, the lack of a fixed setup and involvement of cameras with different qualities give rise to the generation of low-resolution (LR) images. Therefore, more recently introduced deep-learning resolution-upgrade algorithms are essential in modern ALPR systems. In this paper, we propose an additional selective Generative-Adversarial-Network (GAN) Super Resolution (SR) step between the two state-of-the-art You-Only-Look-Once (YOLO)v5-based License-Plate Detection (LPD) and Character Recognition (CR) steps. SR-GAN is proposed due to its perceptual information maintenance. Furthermore, selectiveness is suggested to avoid unnecessary high time-complexity impact. The experimental results show a significant accuracy increase of 18% and an average runtime of 86ms, suitable for many real-time applications. © 2022 IEEE.","Automatic vehicle identification; Deep learning; Intelligent systems; License plates (automobile); Optical character recognition; Optical resolving power; Security systems; Automatic license plate recognition; Crime investigation; Deep learning; Intelligent transportation systems; License plate detection; Security and surveillances; Superresolution; Surveillance systems; System investigation; System security; Generative adversarial networks","Deep Learning; Generative Adversarial Network; Intelligent Transportation System; License-Plate Detection; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85139070474"
"Maddala S.; Nara K.; Yerrarapu S.D.; Vanam S.","Maddala, Seetha (57209468066); Nara, Kalyani (35423875300); Yerrarapu, Sravani Devi (58080984600); Vanam, Shubhasri (58080411200)","57209468066; 35423875300; 58080984600; 58080411200","Classification of Fundus Images Captured using D-Eye Smartphone Retinal Imaging System","2022","2022 International Conference on Emerging Trends in Computing and Engineering Applications, ETCEA 2022 - Proceedings","","","","","","","10.1109/ETCEA57049.2022.10009691","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146944193&doi=10.1109%2fETCEA57049.2022.10009691&partnerID=40&md5=dac8f3f6009b098ecddced1db331ee36","Due to the wide range in the accessibility of eye care services, their affordability, and the population's eye care literacy, cataracts, age-related macular degeneration, glaucoma, diabetic retinopathy, etc., are among the top causes of vision impairment worldwide. To increase the standards of eye examination and to detect the most common vision impairment causes early in a hassle-free manner, real time and sophisticated dataset and efficient classification model is essential. Therefore, this paper emphasizes on the portable smartphone-based fundus image capturing system, D-EYE which acts like a visual digital window. The D-EYE system enables routine eye screening, provides information on visible eye illnesses, and captures images for additional analysis of certain medical situations. With the help of the D-EYE device, an efficient dataset of fundus images of glaucoma is generated and subsequently image enhancement is performed through multiple image pre-processing techniques like image sharpening and masking techniques. The fundus images of glaucoma are cropped to look like high resolution fundus images of the eye using the Super-Resolution Generative Adversarial Network (SRGAN). To categorize fundus images as normal or abnormal, or as glaucoma affected or not, a binary classification model was finally developed. The outcomes show that, with an accuracy of 85%, the classification model has performed at its peak level.  © 2022 IEEE.","Classification (of information); Diagnosis; Discriminators; Eye protection; Generative adversarial networks; Image classification; Image enhancement; Medical imaging; Ophthalmology; Classification models; D-eye; Fundus image; GAN; Generator; Pre-processing; Retina; Smart phones; Super-resolution generative adversarial network; Superresolution; Smartphones","CNN; D-Eye; Discriminator; Fundus image; GAN; Generator; Pre-processing; Retina; Smartphone; SRGAN","Conference paper","Final","","Scopus","2-s2.0-85146944193"
"Wu Y.; Li F.; Bai H.; Lin W.; Cong R.; Zhao Y.","Wu, Yixuan (57468699100); Li, Feng (57203119055); Bai, Huihui (18036828600); Lin, Weisi (8574872000); Cong, Runmin (57565241900); Zhao, Yao (35304414700)","57468699100; 57203119055; 18036828600; 8574872000; 57565241900; 35304414700","Bridging Component Learning with Degradation Modelling for Blind Image Super-Resolution","2022","IEEE Transactions on Multimedia","","","","1","16","15","10.1109/TMM.2022.3216115","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140741200&doi=10.1109%2fTMM.2022.3216115&partnerID=40&md5=d725f73a459eb0730bf133a0f205f291","Convolutional Neural Network (CNN)-based image super-resolution (SR) has exhibited impressive success on known degraded low-resolution (LR) images. However, this type of approach is hard to hold its performance in practical scenarios when the degradation process (<italic>i.e.</italic> blur and downsampling) is unknown. Despite existing blind SR methods proposed to solve this problem using blur kernel estimation, the perceptual quality and reconstruction accuracy are still unsatisfactory. In this paper, we analyze the degradation of a high-resolution (HR) image from image intrinsic components according to a degradation-based formulation model. We propose a components decomposition and co-optimization network (CDCN) for blind SR. Firstly, CDCN decomposes the input LR image into structure and detail components in feature space. Then, the mutual collaboration block (MCB) is presented to exploit the relationship between both two components. In this way, the detail component can provide informative features to enrich the structural context and the structure component can carry structural context for better detail revealing via a mutual complementary manner. After that, we present a degradation-driven learning strategy to jointly supervise the HR image detail and structure restoration process. Finally, a multi-scale fusion module followed by an upsampling layer is designed to fuse the structure and detail features and perform SR reconstruction. Empowered by such degradation-based components decomposition, collaboration, and mutual optimization, we can bridge the correlation between component learning and degradation modelling for blind SR, thereby producing SR results with more accurate textures. Extensive experiments on both synthetic SR datasets and real-world images show that the proposed method achieves the state-of-the-art performance compared to existing methods. Author","Convolution; Generative adversarial networks; Learning systems; Neural networks; Optical resolving power; Signal sampling; Blind image super-resolution; Collaboration; Component learning; Convolutional neural network; Degradation model; Degradation-driven learning strategy; Image super resolutions; Images reconstruction; Kernel; Learning strategy; Superresolution; Image reconstruction","blind image super-resolution; Collaboration; component learning; convolutional neural network; Degradation; degradation modelling; degradation-driven learning strategy; Estimation; Generative adversarial networks; Image reconstruction; Kernel; Superresolution","Article","Article in press","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85140741200"
"Zhang T.; Liu Q.; Du Y.","Zhang, Ting (56999385700); Liu, Qingyang (57226335288); Du, Yi (35731460300)","56999385700; 57226335288; 35731460300","Super-Resolution Reconstruction of Porous Media Using Concurrent Generative Adversarial Networks and Residual Blocks","2022","Transport in Porous Media","","","","","","","10.1007/s11242-022-01892-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144880455&doi=10.1007%2fs11242-022-01892-3&partnerID=40&md5=079379c9fd364c342aca5e2c7bcd7c9b","Accurate porous media reconstruction has always been one of the significant research hotspots in the numerical simulation of reservoirs. The traditional methods such as multi-point statistics perform porous media reconstruction based on the statistical features of training images, but the process is possibly cumbersome and the result is less effective. Porous media reconstruction has been greatly developed and benefited by applying current flourishing deep learning to its simulation process thanks to the strong capability of extracting features by deep learning. As a typical branch of deep learning methods, generative adversarial network (GAN) can simulate a two-person zero-sum game through confrontation between a generator and a discriminator. However, in real experiments, constrained by the resolution of physical equipment and the size of samples, it is difficult to physically obtain a large-scale image of porous media with high-resolution (HR) since HR and large field of view are usually contradictory for physical equipment. In this paper, a method is proposed based on multistage concurrent GAN to learn the structural features of porous media from one low-resolution 3D image and then stochastically reconstruct larger-sized porous media images. Experimental comparison with some typical methods proves that this method can reconstruct HR images with favorable quality. © 2022, The Author(s), under exclusive licence to Springer Nature B.V.","Deep learning; Generative adversarial networks; Image reconstruction; Learning systems; Optical resolving power; Deep learning; High resolution; Hotspots; Multi-point statistics; Physical equipment; Porous medium; Reconstruction; Statistical features; Super-resolution reconstruction; Superresolution; Porous materials","Deep learning; Generative adversarial network; Porous media; Reconstruction; Super-resolution","Article","Article in press","","Scopus","2-s2.0-85144880455"
"Qian L.; Liu X.; Wu J.; Xu X.; Zeng H.","Qian, Liuyihui (57465638800); Liu, Xiaojun (57848678600); Wu, Juan (57465348300); Xu, Xiaoqing (57465202900); Zeng, Han (57465788300)","57465638800; 57848678600; 57465348300; 57465202900; 57465788300","360-Degree Image Super-Resolution Based on Single Image Sample and Progressive Residual Generative Adversarial Network","2022","2022 7th International Conference on Image, Vision and Computing, ICIVC 2022","","","","654","661","7","10.1109/ICIVC55077.2022.9886856","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139477595&doi=10.1109%2fICIVC55077.2022.9886856&partnerID=40&md5=420c605d91789a9b5cb2a8788b8aec47","The restriction of network resources has forced cloud Virtual Reality service providers to only transmit low-resolution 360-degree images to Virtual Reality devices, leading to unpleasant user experience. Deep learning-based single image super-resolution approaches are commonly used for transforming low-resolution images into high-resolution versions, but these approaches are unable to deal with a dataset which has an extremely low number of training image samples. Moreover, current single image training models cannot deal with 360-degree images with very large image sizes. Therefore, we propose a 360-degree image super-resolution method which can train a super-resolution model on a single 360-degree image sample by using image patching techniques and a generative adversarial network. We also propose an improved Generative Adversarial Network (GAN) model structure named Progressive Residual GAN (PRGAN), which learns the image in a rough-to-fine way using progressively growing residual blocks and preserves structural and textural information with multi-level skip connections. Experiments on a street view panorama image dataset prove that our image super-resolution method outperforms several baseline methods in multiple image quality evaluation metrics, meanwhile keeping the generator model computational efficient.  © 2022 IEEE.","Deep learning; Image enhancement; Image quality; Optical resolving power; Virtual reality; 360-degree image; Image super resolutions; Lower resolution; Network resource; Service provider; Single images; Single sample; Superresolution; Superresolution methods; Virtual reality devices; Generative adversarial networks","360-degree image; generative adversarial network; single sample; super-resolution; virtual reality","Conference paper","Final","","Scopus","2-s2.0-85139477595"
"Zhang Z.; Jin L.; Gao T.","Zhang, Zhiming (58043290100); Jin, Lina (58043290200); Gao, Tianzhu (58043548800)","58043290100; 58043290200; 58043548800","Research on Underwater Image Enhancement Algorithm Based on SRGAN","2022","Proceedings of the International Conference on Cyber-Physical Social Intelligence, ICCSI 2022","","","","374","379","5","10.1109/ICCSI55536.2022.9970668","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145655362&doi=10.1109%2fICCSI55536.2022.9970668&partnerID=40&md5=48d2f0291fb83862e40fa5480ed00dbb","Due to the limitation of the special underwater imaging environment, underwater images usually have problems such as low contrast, blurred texture features, color distortion and so on. Based on the typical problem of underwater images, this paper improves the network structure and loss function on the basis of the original SRGAN network model, and achieves good results. The generative network reduces the convolutional layers and removes the normalization layer (BN layer), reducing resource consumption. The loss function introduces L1 content loss and VGG19 perceptual loss to improve the stability of training. The experimental results show that the improved SRGAN network model effectively solves the color distortion and blurring of underwater images, and has a good enhancement effect on underwater images. © 2022 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Network layers; Textures; Color distortions; Deep learning; Image enhancement algorithm; Loss functions; Low contrast; Network models; Super resolution generative adversarial(SRGAN); Superresolution; Texture features; Underwater image enhancements; Underwater imaging","deep learning; super resolution generative adversarial(SRGAN); underwater image enhancement","Conference paper","Final","","Scopus","2-s2.0-85145655362"
"Le N.P.L.; Do H.N.; Huynh V.T.D.; Mai L.","Le, Nguyen Phan Long (57915578600); Do, Hung Ngoc (56602353000); Huynh, Vo Trung Dung (57189044314); Mai, Linh (8938093600)","57915578600; 56602353000; 57189044314; 8938093600","Image Super Resolution Using Deep Learning","2022","ICCE 2022 - 2022 IEEE 9th International Conference on Communications and Electronics","","","","369","374","5","10.1109/ICCE55644.2022.9852096","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139229295&doi=10.1109%2fICCE55644.2022.9852096&partnerID=40&md5=e8914d6e8b765d3d7f5f44894653c091","Image upscaling has been applied in many applications in the image processing field. This paper shows a model which is able to perform image upscaling by 4 times using a series of convolutional filters and trained using the generative adversarial network (GAN) training scheme. The GAN training process involves a generator network, which will perform the image upscaling. The results of the generator network will be evaluated by a discriminator network for the realistic score which will be feedback to the generator network for training. The chosen GAN type is the GAN with a relativistic discriminator which calculates how realistic is the generated image compared to the real image. The network also utilizes different structures of dilated convolution filter, inception module and residue connection between the filters to enhance the feature extraction capability. The high-definition image dataset DIV2K is used for the training.  © 2022 IEEE.","Convolution; Deep learning; Discriminators; Image processing; Optical resolving power; Deep learning; Image super resolution.; Image super resolutions; Images processing; Network training; Network types; Relativistics; Training process; Training schemes; Upscaling; Generative adversarial networks","Deep learning; Image processing; Image Super Resolution.","Conference paper","Final","","Scopus","2-s2.0-85139229295"
"Xu J.; Gao Q.","Xu, Jian (57196733946); Gao, Qiannan (57222059875)","57196733946; 57222059875","Image Super-Resolution Based on Frequency Division Generative Adversarial Network","2022","Proceedings - 2022 4th International Conference on Natural Language Processing, ICNLP 2022","","","","266","271","5","10.1109/ICNLP55136.2022.00048","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139397131&doi=10.1109%2fICNLP55136.2022.00048&partnerID=40&md5=be75a6c9bcc7a27aedc2884022abacdf","Most supervised super-resolution (SR) algorithms require paired high-resolution (HR) and low-resolution (LR) images as training samples. However, the network structures trained by supervised algorithms don't adapt to different image degradations and it is difficult to find paired HR and LR images in real world. In this paper, a new unsupervised image SR algorithm based on generative adversarial network (GAN) is proposed. (1) A new network that can be trained with unpaired HR and LR images is proposed. (2) An intermediate process is proposed so that network contains two GANs, the first learns LR degradation, and the second performs SR procedure. (3) The idea of frequency division training is adopted in LR degradation and SR procedures: low frequency of image is preserved and only high frequency is learned. Experimental results on different datasets show that proposed algorithm provide a better balance between visual quality of SR reconstructed images and computational cost when compared with the state-of-the-art SR algorithms.  © 2022 IEEE.","Computer vision; Optical resolving power; Frequency division; Frequency division training; High resolution; High-low; Image super resolutions; Low resolution images; Lower resolution; Super resolution algorithms; Superresolution; Training sample; Generative adversarial networks","frequency division training; generative adversarial network; image super-resolution; unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85139397131"
"Liu L.; Li W.; Shi Z.; Zou Z.","Liu, Liqin (57215536317); Li, Wenyuan (57204784272); Shi, Zhenwei (23398841900); Zou, Zhengxia (56073977200)","57215536317; 57204784272; 23398841900; 56073977200","Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5528215","","","","10.1109/TGRS.2022.3173532","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130678301&doi=10.1109%2fTGRS.2022.3173532&partnerID=40&md5=d0c26ffea5447d5804eb35757065b4df","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as 'Physics-informed Deep Adversarial Spectral Synthesis (PDASS).' On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.  © 1980-2012 IEEE.","Generative adversarial networks; Hyperspectral imaging; Military applications; Optical resolving power; Pixels; Reflection; Remote sensing; Spectroscopy; Adversarial networks; Atmospheric modeling; Generation adversarial network; Images reconstruction; Imaging modeling; Remote-sensing; Spatial resolution; Spectral super-resolution; Superresolution; artificial neural network; image analysis; image resolution; imaging method; physics; pixel; remote sensing; satellite imagery; spectral analysis; Image reconstruction","Generation adversarial networks (GANs); hyperspectral image; imaging model; remote sensing; spectral super-resolution (SSR)","Article","Final","","Scopus","2-s2.0-85130678301"
"Li L.; Liu M.; Sun L.; Li Y.; Li N.","Li, Lina (56158736700); Liu, Minghan (57656757900); Sun, Liyan (57195572136); Li, Yupeng (57488411800); Li, Nianfeng (37043958200)","56158736700; 57656757900; 57195572136; 57488411800; 37043958200","ET-YOLOv5s: Toward Deep Identification of Students' in-Class Behaviors","2022","IEEE Access","10","","","44200","44211","11","10.1109/ACCESS.2022.3169586","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129210710&doi=10.1109%2fACCESS.2022.3169586&partnerID=40&md5=3ecc60633c4663dd35f5439b78c19cf0","Recognizing students' behaviors in classes based on videos of their activity plays an important role in improving teaching quality and paying attention to the healthy growth of students. The existing student behavior recognition methods mainly focus on the behavior of the single student and have low performance and efficiency. To recognize the behaviors of multiple students in the classroom at the same time, we propose a fast and effective solution, called ET-YOLOv5s, which is an improved YOLOv5s with ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) and a tiny object detection module in this paper. First, the ESRGAN is adopted to generate high-definition images from original images of real classroom environments in colleges to improve the recognition accuracy. Then a tiny object detection module is added to the YOLOv5s to detect the smaller objects on high-definition images, such as phones, books, and students sitting at the back of the classroom. Finally, the experimental results show that the proposed solution can detect 11 kinds of students' behaviors in classrooms, including playing mobile phones, having class, sitting with hands on face, sitting turn right, sitting turn left, bowing, sleeping, and so on. Compared with YOLOv4, YOLOv5s, and other tiny target detection algorithms, it has better detection performance and can effectively deal with the behavior recognition of multiple students.  © 2013 IEEE.","Behavioral research; Deep learning; Feature extraction; Generative adversarial networks; Image enhancement; Object detection; Object recognition; Optical resolving power; Telephone sets; Behaviour recognition; Deep learning; Features extraction; Head; Student behavior recognition; Students' behaviors; Superresolution; Target recognition; Tiny object detection; Video; YOLOv5; Students","Deep learning; generative adversarial network; student behavior recognition; tiny object detection; YOLOv5s","Article","Final","","Scopus","2-s2.0-85129210710"
"Wang J.; Song J.; Zhang Y.; Chen H.","Wang, Jiachun (57827550500); Song, Junkui (57827750300); Zhang, Yizhe (57827418900); Chen, Hao (57840297400)","57827550500; 57827750300; 57827418900; 57840297400","Design of 3D Display System for Intangible Cultural Heritage Based on Generative Adversarial Network","2022","Scientific Programming","2022","","2944750","","","","10.1155/2022/2944750","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135274699&doi=10.1155%2f2022%2f2944750&partnerID=40&md5=2844461d52d68f4b90892eae861d381a","This paper designs a three-dimensional display system for intangible cultural heritage based on generative adversarial networks. The system function is realized through four modules: input module, data processing module, 3D model generation module, and model output module. Two 3D model reconstruction methods are used to realize the transformation from 2D images to 3D models. In the low-resolution Nuo surface 3D construction, multiresidual dense blocks are introduced and applied to the deep image super-resolution network. The experimental comparison results show that the quadratic optimization multifusion 3D construction model proposed in this paper can achieve considerable improvement and can improve the reconstruction accuracy by about 6.3%. In the high-resolution 3D construction of the Nuo surface, a generative adversarial network model is used to improve the generator, discriminator, and loss function of the original SRGAN model. Experimental results show that this method can generate super-resolution images with more realistic and natural depth maps. In addition, when it is used for high-resolution 3D Nuo surface sculpting, it can also generate 3D voxel Nuo surfaces with more details.  © 2022 Jiachun Wang et al.","3D modeling; Data handling; Image reconstruction; Optical resolving power; Quadratic programming; Three dimensional computer graphics; Three dimensional displays; 3D construction; 3D display systems; 3D models; 3d-modeling; Display system; High resolution; Intangible cultural heritages; Processing modules; System functions; Three-dimensional display; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135274699"
"de Oliveira R.A.; Scheeren M.H.; Rodrigues P.J.S.; Junior A.C.; de Paula Filho P.L.","de Oliveira, Rafael Augusto (58103024600); Scheeren, Michel Hanzen (58102124700); Rodrigues, Pedro João Soares (56479418900); Junior, Arnaldo Candido (35179585400); de Paula Filho, Pedro Luiz (57256888800)","58103024600; 58102124700; 56479418900; 35179585400; 57256888800","Super-Resolution Face Recognition: An Approach Using Generative Adversarial Networks and Joint-Learn","2022","Communications in Computer and Information Science","1754 CCIS","","","747","762","15","10.1007/978-3-031-23236-7_51","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147987485&doi=10.1007%2f978-3-031-23236-7_51&partnerID=40&md5=1db1c2a4bef2dc56ae6db1fe8c37033b","Face Recognition is a challenging task present in different applications and systems. An existing challenge is to recognize faces when imaging conditions are adverse, for example when images come from low-quality cameras or when the subject and the camera are far apart, thus impacting the accuracy of these recognizing systems. Super-Resolution techniques can be used to improve both image resolution and quality, hopefully improving the accuracy of the face recognition task. Among these techniques, the actual state-of-the-art uses Generative Adversarial Networks. One promising option is to train Super-Resolution and Face Recognition as one single network, conducting the network to learn super resolution features that will improve its capability when recognizing faces. In the present work, we trained a super resolution face recognition model using a jointly-learn approach, combining a generative network for super resolution and a ResNet50 for Face Recognition. The model was trained with a discriminator network, following the generative adversarial training. The images generated by the network were convincing, but we could not converge the face recognition model. We hope that our contributions could help future works on this topic. Code is publicly available at https://github.com/OliRafa/SRFR-GAN. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Cameras; Generative adversarial networks; Image enhancement; Image resolution; Imaging conditions; Learn+; Low qualities; Machine-learning; Recognition models; Resolution techniques; Single-networks; State of the art; Super A*; Superresolution; Face recognition","Face Recognition; Generative Adversarial Networks; Machine learning; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85147987485"
"Zhang H.; Zhu Y.; Sun J.; Zhang Y.","Zhang, Haiyu (57932214200); Zhu, Yu (55723822100); Sun, Jinqiu (17436184700); Zhang, Yanning (56075029000)","57932214200; 55723822100; 17436184700; 56075029000","REAL-WORLD IMAGE SUPER-RESOLUTION VIA KERNEL AUGMENTATION AND STOCHASTIC VARIATION","2022","Proceedings - International Conference on Image Processing, ICIP","","","","2506","2510","4","10.1109/ICIP46576.2022.9897540","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146668695&doi=10.1109%2fICIP46576.2022.9897540&partnerID=40&md5=af62195ab79a8231b8b564c8712820a7","Deep learning (DL) based single image super-resolution (SISR) algorithms have now achieved highly satisfactory evaluation and visualization results on synthetic datasets. However, in some practical applications, especially when restoring some real-world low-resolution (LR) photos, the limitation and unicity of the most commonly used bicubic down-sampling kernel often lead to significant performance degradation of models trained under ideal conditions. Thus, we first propose a kernel augmentation (KA) strategy based on generative adversarial networks (GANs) to improve the generalization ability and robustness of current SISR models. Then, we intend to reconstruct the stochastic variation (SV) features that are widely present in natural images to obtain a more realistic feature representation. In the end, extensive experiments demonstrate the feasibility and effectiveness of our approach in dealing with real-world SISR problems. © 2022 IEEE.","Computer vision; Deep learning; Image reconstruction; Optical resolving power; Stochastic systems; Deep learning; Image super resolutions; Kernel augmentation; Real-world; Real-world image; Single image super-resolution; Single images; Stochastic variation; Super resolution algorithms; Generative adversarial networks","Deep learning (DL); kernel augmentation (KA); real-world; single image super-resolution (SISR); stochastic variation (SV)","Conference paper","Final","","Scopus","2-s2.0-85146668695"
"Prajapati K.; Chudasama V.; Patel H.; Sarvaiya A.; Upla K.; Raja K.; Ramachandra R.; Busch C.","Prajapati, Kalpesh (57217177596); Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Sarvaiya, Anjali (57224979212); Upla, Kishor (53985429600); Raja, Kiran (57188866050); Ramachandra, Raghavendra (57190835798); Busch, Christoph (7101767185)","57217177596; 57202047982; 57209145428; 57224979212; 53985429600; 57188866050; 57190835798; 7101767185","Unsupervised Denoising for Super-Resolution (UDSR) of Real-World Images","2022","IEEE Access","10","","","122329","122346","17","10.1109/ACCESS.2022.3223101","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142788785&doi=10.1109%2fACCESS.2022.3223101&partnerID=40&md5=f9bb623395ad06f11c71ca6cbfb2b787","Single Image Super-Resolution (SISR) using Convolutional Neural Networks (CNNs) for many applications in supervised manner has resulted in significant improvement in state-of-the-art performance. Such supervised models achieve remarkable accuracy; albeit their poor generalization ability for real-world Low-Resolution (LR) images. Supervised training in many SR works involves synthetically generated LR images from its corresponding High-Resolution (HR) images. As the distribution of such LR observation is relatively different from that of real LR image, the supervised training in SISR task results in a degradation when applied on real-world data. SISR has been scaled to real-world data recently by posing the unsupervised problem into a supervised one through learning the distribution of noisy LR observation first, following which supervised training is performed to obtain the SR image. It therefore involves two steps where the accuracy of SR image relies on how closely the LR's distribution is learnt in the first step. In this work, we overcome such limitation by introducing unsupervised denoising network to transform real noisy LR image to clean image and then pre-trained SR network is utilised to increase the spatial resolution of cleaned LR image to generate SR image. Thus, instead of evaluating the denoised image in LR space to train the denoising network, we inspect the denoised image in SR space which allows to overcome the SR network's generalization problem. The proposed Unsupervised Denoising framework for Super-Resolution (UDSR) is validated on real-world datasets (NTIRE-2020 Real-World SR Challenge validation and testing dataset (Track-1)) by comparing it with many recent unsupervised SISR methods. The performance of denoising and SR networks is superior in terms of various perceptual indices such as Perceptual Index (PI) and Ma Score in addition to numerous non-references metrics.  © 2013 IEEE.","Convolution; Generative adversarial networks; Image denoising; Image enhancement; Neural networks; Noise abatement; Optical resolving power; Personnel training; Statistical tests; Convolutional neural network; De-noising; Image super resolutions; Kernel; Noise measurements; Single images; Single-image super-resolution; Superresolution; Task analysis; Image reconstruction","Convolutional neural network; generative adversarial network; image enhancement; image restoration; single-image super-resolution; unsupervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142788785"
"Guo R.","Guo, Run (57578003800)","57578003800","Image Super-resolution for Endangered Animals using Refined SuperResolution Generative Adversarial Network","2022","Proceedings of SPIE - The International Society for Optical Engineering","12168","","1216809","","","","10.1117/12.2631276","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128257158&doi=10.1117%2f12.2631276&partnerID=40&md5=5825250bb29d229f4ffeeb9777c5508b","In the nature and wildlife protection field, biologists want to learn about endangered animals' behavior, environment, and activities. The hope is to get more information about endangered animals, such as their diets, predators, and surroundings. With the help of digital devices, biologists use drones to capture information about the changing environment of endangered animals. Therefore, high-resolution images with precise details are essential in the research process. Recently, there have been many deep learning models for improving image resolution in the computer science field, for example, Super-Resolution Convolutional Neural Networks (SRCNN), Efficient Sub-Pixel Convolutional Neural Network (ESPCN), Deeply-Recursive Convolutional Network (DRCN), and Super-Resolution Generative Adversarial Network (SRGAN). SRGAN is a deep learning network model based on generative adversarial networks. It has a good effect on improving image resolution. Based on the SRGAN model, the paper refined the loss function and structure of the SRGAN networks, which optimized the details of the generated high-resolution images, and provided more accurate high-resolution image results. The refined SRGAN aims to give better results in animal image analysis. We selected DIV2K dataset as the training and testing data and compared the refined SRGAN method with the original SRGAN method. The quantitative evaluation used the peak signal-to-noise ratio (PNSR) and structural similarity index measurement (SSIM). Compared with the SRGAN model with one convolution layer, the model improves the average PSNR value of 0.93dB, and the SSIM value reaches 0.8723. With the help of the improved SRGAN network, biologists can obtain better super-resolution images under the premise that other conditions remain unchanged to protect wild animals.  © 2022 SPIE.","Animals; Conservation; Convolution; Convolutional neural networks; Deep learning; Digital devices; Image enhancement; Image resolution; Signal to noise ratio; Statistical tests; Convolution kernel; Convolutional neural network; High-resolution images; Image super resolutions; Network methods; Network models; Similarity indices; Structural similarity; Super-resolution generative adversarial network; Superresolution; Generative adversarial networks","convolution kernels; image super-resolution; SRGAN","Conference paper","Final","","Scopus","2-s2.0-85128257158"
"Roy S.; Binu D.; Rajakumar B.R.; Talasila V.; Bhatt A.","Roy, Srinjoy (57573644300); Binu, D. (57197290064); Rajakumar, B.R. (55266137500); Talasila, Vamsidhar (57289574900); Bhatt, Abhishek (57930754100)","57573644300; 57197290064; 55266137500; 57289574900; 57930754100","Super Resolved Maize Plant Leaves Disease Detection Using Optimal Generative Adversarial Network","2022","International Journal of Image and Graphics","","","2450003","","","","10.1142/S0219467824500037","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143600026&doi=10.1142%2fS0219467824500037&partnerID=40&md5=0d6bc1bd5b4384d65ac1c8c1274ba43a","Agriculture plays a vital role in the economy and crop disease causes huge financial losses every year. The losses can be reduced by detecting the disease accurately. The variation in light intensity and complex background of the agricultural field in detecting the maize leaves disease are the biggest challenges. An optimization algorithm, named Cat Swarm Political Optimizer Algorithm (CSPOA) has been developed in this research to detect the disease of a maize plant leaf. Our proposed algorithm is an integration of the Cat Swarm Optimization (CSO) and Political Optimizer (PO) algorithm. Anisotropic filtering performs pre-processing for removing noise and the Region of Interest (ROI) extraction for enhancing the image quality. The super resolution image is obtained from the Low Resolution (LR) images using kernel regression model. After obtaining the super resolution image, the salient map extraction has been carried out for representing the saliency. Finally, the maize plant leaves disease classification process is done using General Adversarial Network (GAN) for identifying the maize leaves disease. The training of GAN develops the CSPOA. On comparing with the existing maize plant leaves disease detection approaches, the developed CSPOA-based GAN performed with a maximum accuracy 0.9056, maximum sensitivity 0.9599, and the maximum specificity 0.9592, respectively. © 2024 World Scientific Publishing Company.","Agriculture; Extraction; Generative adversarial networks; Image segmentation; Losses; Optical resolving power; Plants (botany); Regression analysis; Adversarial networks; Cat swarm optimization; General adversarial network; Leaf disease; Leaf disease detections; Low resolution images; Maize plants; Optimizers; Plant leaves; Swarm optimization; Image enhancement","agriculture; cat swarm optimization; general adversarial network; low resolution image; Maize plant","Article","Article in press","","Scopus","2-s2.0-85143600026"
"Zhang Z.; Lu W.; Chen S.; Yang F.; Jingchang P.","Zhang, Zhechen (58030151400); Lu, Weigang (56284716000); Chen, Shuo (57214754635); Yang, Fei (56491869400); Jingchang, Pan (16549451500)","58030151400; 56284716000; 57214754635; 56491869400; 16549451500","Boundary equilibrium SR: effective loss functions for single image super-resolution","2022","Applied Intelligence","","","","","","","10.1007/s10489-022-04162-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144724101&doi=10.1007%2fs10489-022-04162-3&partnerID=40&md5=2ae0c735525394a68be03bbd0915096e","Recently, single image super-resolution (SISR) has made great progress due to the rapid development of deep convolutional neural networks (CNN), and the application of Generative Adversarial Networks (GAN) has made super-resolution networks even more effective. However, GAN-based methods have many problems such as lengthy and unstable convergence. To solve these problems, this paper presents a mechanism that employs boundary equilibrium in the image super-resolution network to balance the convergence of the generator and the discriminator and improve the visual quality of the generated synthetic images. Furthermore, current methods often use perceptual loss based on the VGG network. However, experiments show that the visual quality improvement brought by this perceptual loss is very limited, so we propose an improved perceptual loss based on Learned Perceptual Image Patch Similarity (LPIPS) to acquire better human visual effects rather than adopting the traditional perceptual loss based on VGG. The experimental results clearly show that using our proposed method can considerably improve the performance of image super-resolution and obtain clearer details than state-of-the-arts. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep neural networks; Image enhancement; Optical resolving power; Signal encoding; Auto encoders; Boundary equilibrium; Boundary equilibrium GAN; Deep learning; Image patches; Image super resolutions; Learned perceptual image patch similarity-based perceptual loss; Loss functions; Single images; Visual qualities; Generative adversarial networks","Auto-encoder; Boundary equilibrium GANs; Deep learning; Image super-resolution; LPIPS-based perceptual loss","Article","Article in press","","Scopus","2-s2.0-85144724101"
"Zhou L.; Xu L.; Liu X.-Y.; Zhang X.-Z.; Zhang X.","Zhou, Le (57913325000); Xu, Long (55660493400); Liu, Xiao-Yan (57192258753); Zhang, Xin-Ze (57912672300); Zhang, Xuan-De (54790248800)","57913325000; 55660493400; 57192258753; 57912672300; 54790248800","Gradient-aware based single image super-resolution","2022","Chinese Journal of Liquid Crystals and Displays","37","10","","1334","1344","10","10.37188/CJLCD.2022-0083","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139132732&doi=10.37188%2fCJLCD.2022-0083&partnerID=40&md5=7f3899b03feca8b8dcee05797af0dec4","：With the application of generative adversarial networks in the field of image super-resolution （SR)，some perception-driven SR methods can recover SR images with richer texture details，effectively alleviating the over-smoothing problem of the PSNR dominated SR methods. Gradient information is an important representation of image texture. However，few SR methods can make use of this information accurately and efficiently. In this paper，a gradient-aware single image super-resolution（GASR） is proposed，using gradient information better from two aspects. On the one hand，the feature map of gradient domain is used as the convolution kernel imposing on the feature map of image domain，which can effectively avoid the domain conflict caused by the concatenation of feature map of different domains. On the other hand，by elaborating the network details such as convolution kernel size，etc.，the image fields output at the corresponding positions of the two branches are consistent with the feel fields of the feature maps in the gradient domain. In addition，the proposed GASR algorithm also effectively reduces the number of parameters and the amount of computation due to the increased demand for network lightweight in practical applications. Compared to SPSR，GASR can achieve the same performance at the cost of about 1/6 of the parameters and 1/10 of the computation of SPSR. On Set14 dataset，LPIPS and PSNR increase by 0. 002 2 and 0. 217，respectively. The experimental results show that GASR can achieve a good trade-off between texture details and image smoothness. In addition，GASR can not only reconstruct high fidelity SR image，but also alleviate the generation of messy textures. © 2022, Science Press. All rights reserved.","","deep learning; generative adversarial network; gradient; image super-resolution","Article","Final","","Scopus","2-s2.0-85139132732"
"Terada Y.; Miyasaka T.; Nakao A.; Funayama S.; Ichikawa S.; Takamura T.; Tamada D.; Morisaka H.; Onishi H.","Terada, Yasuhiko (57667138000); Miyasaka, Tomoki (57819130500); Nakao, Ai (57819083000); Funayama, Satoshi (57196044597); Ichikawa, Shintaro (26633759300); Takamura, Tomohiro (57143861200); Tamada, Daiki (35424445700); Morisaka, Hiroyuki (37054478700); Onishi, Hiroshi (57208835117)","57667138000; 57819130500; 57819083000; 57196044597; 26633759300; 57143861200; 35424445700; 37054478700; 57208835117","Clinical evaluation of super-resolution for brain MRI images based on generative adversarial networks","2022","Informatics in Medicine Unlocked","32","","101030","","","","10.1016/j.imu.2022.101030","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134889810&doi=10.1016%2fj.imu.2022.101030&partnerID=40&md5=224d6f890be0c3354cc1e898f8e8b3d4","In magnetic resonance imaging (MRI), reducing long scan times is an urgent issue that could be addressed with super-resolution (SR) techniques. Most of the SR networks using deep neural networks (DNNs) have been evaluated only based on numeric metrics, and the image restoration quality for individual lesions is not evaluated sufficiently. Here, we evaluated the performances of different types of SR networks using DNNs for brain MRI, in terms of diagnostic performance and image quality. We focused on comparing the performance between generative adversarial networks (GANs) and non-GAN networks. There was a trade-off in such restoration quality between GAN- and non-GAN-based SRs, with the GANs being more accurate in restoring images of anatomical structures but less accurate in restoring those of lesions; non-GANs showed the opposite tendency. The non-GAN SRs were preferable in terms of diagnostic performance and image quality. This result suggested that the evaluation of DNN performance for lesions might be changed drastically by adding a clinical evaluation perspective. The dependence of network architecture on network performance obtained in this study will provide guidance for future development of SR DNN for medical images. © 2022 The Authors","article; brain; clinical evaluation; deep neural network; diagnostic test accuracy study; diagnostic value; human; image quality; neuroimaging; nuclear magnetic resonance imaging","Brain MRI; Clinical evaluation; Deep neural networks; Generative adversarial networks; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85134889810"
"Li F.; Li A.; Qin J.; Bai H.; Lin W.; Cong R.; Zhao Y.","Li, Feng (57203119055); Li, Anqi (57807842500); Qin, Jia (57194169239); Bai, Huihui (18036828600); Lin, Weisi (8574872000); Cong, Runmin (57565241900); Zhao, Yao (35304414700)","57203119055; 57807842500; 57194169239; 18036828600; 8574872000; 57565241900; 35304414700","SRInpaintor: When Super-Resolution Meets Transformer for Image Inpainting","2022","IEEE Transactions on Computational Imaging","8","","","743","758","15","10.1109/TCI.2022.3190142","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134315046&doi=10.1109%2fTCI.2022.3190142&partnerID=40&md5=e170d7a80e11ded2077e4971affb9c9b","Recent image inpainting methods have achieved remarkable improvements by using generative adversarial networks (GAN). Most of them have been designed to produce plausible results from high-level semantic features using only high-resolution (HR) supervision. However, because abundant details are lost in large holes, it is difficult to simultaneously synthesize details while preserving structural coherence in HR space. Besides, the correlations between the inside and outside of the missing region play a critical role in transferring relevant known information to generate semantic-coherent textures, especially in patch matching-based methods. In this work, we present SRInpaintor which inherits the merits of super-resolution (SR) and transformer for high-fidelity image inpainting. The SRInpaintor starts from global structure reasoning with low-resolution (LR) input and progressively refines the local textures in HR space, constituting a multi-stage framework with SR supervision. The bottom stage recovers coarse SR results that provide structural information as an appearance prior, and is combined with the higher-resolution corrupted image at the next stage to render available textures for the missing region. Such a design can analyse the image from LR to HR with the increase of stages, enabling coarse-to-fine information propagation and detail refinement. In addition, we propose a hierarchical transformer (HieFormer) to model the long-term correlations between distant contexts and holes. By embedding it into a compact latent space in a cross-scale manner, we can ensure reliable relevant texture transformation and robust appearance consistency. Experimental results demonstrate the superiority of our method compared with recent state-of-the-art methods. Code will be available on https://github.com/lifengshiwo/SRInpaintor. © 2015 IEEE.","Computer vision; Edge detection; Generative adversarial networks; Image analysis; Image enhancement; Image texture; Optical resolving power; Semantics; Textures; Correlation; Generative image inpainting; Image edge detection; Image Inpainting; Images reconstruction; Progressive super-resolution; Superresolution; Task analysis; Transformer; Image reconstruction","Generative image inpainting; progressive super-resolution; transformer","Article","Final","","Scopus","2-s2.0-85134315046"
"Galvan-Hernandez A.; Ticay-Rivas J.R.; Alonso-Eugenio V.; Arana V.; Cabrera F.","Galvan-Hernandez, A. (57818679300); Ticay-Rivas, J.R. (37862263900); Alonso-Eugenio, V. (57217114609); Arana, V. (55368600600); Cabrera, F. (57204446004)","57818679300; 37862263900; 57217114609; 55368600600; 57204446004","Thermographic image super-resolution based on neural networks","2022","2022 3rd URSI Atlantic and Asia Pacific Radio Science Meeting, AT-AP-RASC 2022","","","","","","","10.23919/AT-AP-RASC54737.2022.9814417","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134883835&doi=10.23919%2fAT-AP-RASC54737.2022.9814417&partnerID=40&md5=fd21335990b0bd8a4d3a52c82dcd8882","The continuous development of thermographic technology has led to the overall improvement of instruments used in medicine, surveillance, or military systems. However, thermographic imaging cameras still have a high cost compared to other alternatives on the market, such as visible light cameras and a much lower spatial resolution. Super-resolution is a technique that improves the visual quality of an image through software processing. This work studies three neural networks architectures based on deep learning capable of performing super-resolution of RGB images at x2 and x4 scales: Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network (SRGAN), Enhanced Deep Residual Networks for Single Image Super-Resolution (EDSR), and Wide Activation for Efficient and Accurate Image Super-Resolution (WDSR). These architectures, in this work, have been trained as a super-resolution system using thermographic images as input data. The evaluation was carried out using thermographic images from different thermographic cameras. The performance assessment was carried out using the Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM). In addition, low-resolution images from a low-cost thermographic camera were used as input to the neural networks to study the feasibility of this method.  © 2022 URSI.","Cameras; Deep learning; Image enhancement; Network architecture; Optical resolving power; Security systems; Signal to noise ratio; Continuous development; High costs; Image super resolutions; Imaging camera; Military systems; Neural-networks; Single images; Superresolution; Surveillance systems; Thermographic cameras; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85134883835"
"Li Z.; Zhong Z.; Chen Z.; Yao G.; Chen X.; Huang W.","Li, Zhan (56991881000); Zhong, Ziyi (57943700200); Chen, Zhitao (57943457300); Yao, Gengqi (57945168800); Chen, Xi (57826794300); Huang, Weijian (57943700300)","56991881000; 57943700200; 57943457300; 57945168800; 57826794300; 57943700300","Edge Guided Learning for Image Super-resolution with Realistic Textures","2022","Proceedings of the International Joint Conference on Neural Networks","2022-July","","","","","","10.1109/IJCNN55064.2022.9891974","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140734606&doi=10.1109%2fIJCNN55064.2022.9891974&partnerID=40&md5=a1be9e362ce57e9a45b2e8859609a7bd","Super-resolution (SR) methods are used to reconstruct details in images to obtain an improved resolution. Recently, SR methods based on generative adversarial networks (GANs) have become seminal due to their effectiveness in generating textures. However, a common problem is the presence of unpleasant artifacts. In this paper, an edge-guided SR neural network (Edge-SRN) is proposed by introducing a plug-in edge detection module and incorporating a new edge loss, which increases the reconstruction accuracy and reduces artifacts. We also use the Edge-SRN as a teacher network to a knowledge distillation framework for training a lightweight student SR model. The student model learned from Edge-SRN outperforms its counterparts learned from GAN-based teachers or from the ground-truth HR images in both reconstruction accuracy and perceptual quality, which indicates the ability of reconstructing realistic textures can be transferred well from Edge-SRN to a small model. Extensive experiments on diverse criteria show the promising performance of our method compared with several state-of-the-art SR methods in the qualitative and quantitative evaluations. Our code is available at https://github.com/lizhangray/Edge-SRN © 2022 IEEE.","Computer vision; Distillation; Edge detection; Generative adversarial networks; Image enhancement; Image reconstruction; Optical resolving power; Personnel training; Detection modules; Image super resolutions; Knowledge distillation; Neural-networks; Plug-ins; Reconstruction accuracy; Superresolution; Superresolution methods; Teachers'; Texture quality; Textures","Knowledge distillation; Super-resolution; Texture quality","Conference paper","Final","","Scopus","2-s2.0-85140734606"
"Mei J.Q.; Wen Ding X.; Zheng D.; Page T.","Mei, Jian Qiang (23389666500); Wen Ding, Xue (57809733000); Zheng, Dandan (57809663500); Page, Tom (56047889600)","23389666500; 57809733000; 57809663500; 56047889600","Infrared Image Super-Resolution via Generative Adversarial Network with Gradient Penalty Loss","2022","Conference Record - IEEE Instrumentation and Measurement Technology Conference","","","","","","","10.1109/I2MTC48687.2022.9806485","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134430313&doi=10.1109%2fI2MTC48687.2022.9806485&partnerID=40&md5=923e578b5f67c1ea3bdeb0e732316df7","Infrared thermal imaging technology has been gradually developed and widely applied in measurement and non-destructive testing. However, low-contrast blurred details and expensive acquisition equipment remain as barriers to its further practical applications and widespread adoption. In this paper, a novel framework comprising deep learning techniques is proposed to offer a relatively competitive and compatible solution of infrared image super-resolution. Firstly, radiance information from low-resolution imagery is detected and automatically translated to high-resolution through a Generative Adversarial Network (GAN) with Wasserstein distance. Secondly, a gradient penalty loss function is utilized for the discriminator to guide the generator to achieve reasonable and acceptable convergence. Through evaluation of three widely utilized infrared datasets, the proposed method demonstrates superior performance against the state-of-art method with more accurate Peak Signal-To-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) respectively. The outcome of this study has implications for a real-application of deep learning based infrared non-destructive testing and measurement scenarios.  © 2022 IEEE.","Deep learning; Nondestructive examination; Optical resolving power; Signal to noise ratio; Thermography (imaging); Acquisition equipments; Deep learning; Gradient penalty loss; Image super resolutions; Infrared thermal imaging; Low contrast; Non destructive testing; Penalty loss; Superresolution; Thermal imaging technology; Generative adversarial networks","Deep Learning; Generative Adversarial Network; Gradient Penalty Loss; Infrared Imaging; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85134430313"
"Jia S.; Wang Z.; Li Q.; Jia X.; Xu M.","Jia, Sen (57681305300); Wang, Zhihao (57253694800); Li, Qingquan (55831292900); Jia, Xiuping (7201933692); Xu, Meng (57206827164)","57681305300; 57253694800; 55831292900; 7201933692; 57206827164","Multiattention Generative Adversarial Network for Remote Sensing Image Super-Resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5624715","","","","10.1109/TGRS.2022.3180068","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131761657&doi=10.1109%2fTGRS.2022.3180068&partnerID=40&md5=3ff02c05e0246c450feae23e75918e27","Image super-resolution (SR) methods can generate remote sensing images with high spatial resolution without increasing the cost of acquisition equipment, thereby providing a feasible way to improve the quality of remote sensing images. Clearly, image SR is a severe ill-posed problem. With the development of deep learning, the powerful fitting ability of deep neural networks has solved this problem to some extent. Since the texture information of various remote sensing images are totally different from each other, in this article, we proposed a network based on generative adversarial network (GAN) to achieve high-resolution remote sensing images, named multiattention GAN (MA-GAN). The main body of the generator in MA-GAN contains three blocks: pyramid convolutional residual dense (PCRD) block, attention-based upsampling (AUP) block, and attention-based fusion (AF) block. Specifically, the developed attention pyramid convolutional (AttPConv) operator in the PCRD block combines multiscale convolution and channel attention (CA) to automatically learn and adjust the scale of residuals for better representation. The established AUP block uses pixel attention (PA) to perform arbitrary scales of upsampling. The AF block uses branch attention (BA) to integrate upsampled low-resolution images with high-level features. Besides, the loss function takes both adversarial loss and feature loss into consideration to guide the learning procedure of the generator. We have compared our MA-GAN approach with several state-of-the-art methods on a number of remote sensing scenes, and the experimental results consistently demonstrate the effectiveness of the proposed MA-GAN. For study replication, the source code will be released at: https://github.com/ZhihaoWang1997/MA-GAN.  © 1980-2012 IEEE.","Convolution; Deep neural networks; Image enhancement; Image resolution; Remote sensing; Signal sampling; Generative adversarial network; Generator; Image super resolutions; Remote sensing images; Remote-sensing; Spatial resolution; Super resolution; Superresolution; Task analysis; Upsampling; artificial neural network; experimental study; remote sensing; satellite data; Generative adversarial networks","Generative adversarial network (GAN); remote sensing image; super-resolution (SR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131761657"
"Cao Z.; Liu X.; Wang Z.","Cao, Zhenyue (57830421000); Liu, Xuan (57225965871); Wang, Zhenkun (57830327500)","57830421000; 57225965871; 57830327500","Single image super-resolution via deep learning","2022","2022 3rd International Conference on Computer Vision, Image and Deep Learning and International Conference on Computer Engineering and Applications, CVIDL and ICCEA 2022","","","","425","430","5","10.1109/CVIDLICCEA56201.2022.9824576","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135408034&doi=10.1109%2fCVIDLICCEA56201.2022.9824576&partnerID=40&md5=5736aa0c2c35e9cda395f64e1256f607","Single image Super-resolution (SISR) is a computer vision (CV) problem that aims to acquire a high-resolution (HR) image from a distorted low-resolution (LR) image, making it a valuable technology that could be utilized in various fields such as photography, medical imaging, satellite imaging, etc. As a result of the advancement of computing hardware and richer computational power, deep learning-based image super-resolution models have emerged at an unprecedented rate. This paper reviews SISR and its recent development. Three widely used deep architectures: convolutional neural network (CNN), generative adversarial network (GAN), and transformer are explained. Next, six different deep learning-based models that summarize research on SISR are analyzed. Finally, this review concludes with applications of SR, current challenges SISR models encountered, and potential future research directions. © 2022 IEEE.","Computer hardware; Computer vision; Convolution; Convolutional neural networks; Deep learning; Learning systems; Medical imaging; Optical resolving power; Attention mechanisms; Component; Convolutional neural network; Deep learning; Generative adversarial network; Image super resolutions; Single image super-resolution; Single images; Transformer; Generative adversarial networks","Attention mechanism; component; Computer Vision; Convolutional neural networks (CNN); Deep learning; Generative adversarial networks (GAN); Single image super-resolution; Transformer","Conference paper","Final","","Scopus","2-s2.0-85135408034"
"Uddin M.S.; Pamie-George R.; Wilkins D.; Sousa-Poza A.; Canan M.; Kovacic S.; Li J.","Uddin, Mohammad Shahab (57218626198); Pamie-George, Raphael (57818734500); Wilkins, Daron (57818700200); Sousa-Poza, Andres (6701811948); Canan, Mustafa (57514492400); Kovacic, Samuel (24468007300); Li, Jiang (56226550100)","57218626198; 57818734500; 57818700200; 6701811948; 57514492400; 24468007300; 56226550100","Ship Deck Segmentation in Engineering Document Using Generative Adversarial Networks","2022","2022 IEEE World AI IoT Congress, AIIoT 2022","","","","207","212","5","10.1109/AIIoT54504.2022.9817355","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134883285&doi=10.1109%2fAIIoT54504.2022.9817355&partnerID=40&md5=153305232dd00745d34a070542578345","Generative adversarial networks (GANs) have become very popular in recent years. GANs have proved to be successful in different computer vision tasks including image-Translation, image super-resolution etc. In this paper, we have used GAN models for ship deck segmentation. We have used 2D scanned raster images of ship decks provided by US Navy Military Sealift Command (MSC) to extract necessary information including ship walls, objects etc. Our segmentation results will be helpful to get vector and 3D image of a ship that can be later used for maintenance of the ship. We applied the trained models to engineering documents provided by MSC and obtained very promising results, demonstrating that GANs can be potentially good candidates for this research area.  © 2022 IEEE.","Computer vision; Image segmentation; Military photography; Engineering document; Image super resolutions; Image translation; Military sealift commands; Network models; Pix2pix generative adversarial network; Raster image; Ship deck; Ship deck segmentation; U.S. Navy; Generative adversarial networks","GAN; Pix2Pix GAN; ship deck segmentation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134883285"
"Karim A.A.; Saleh S.M.","Karim, Abdulamir A. (57211411574); Saleh, Suha Mohammed (57726629700)","57211411574; 57726629700","Face Image Animation with Adversarial Learning and Motion Transfer","2022","International Journal of Interactive Mobile Technologies","16","10","","109","121","12","10.3991/ijim.v16i10.30047","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131325660&doi=10.3991%2fijim.v16i10.30047&partnerID=40&md5=44c1ec8aff9e158d20551597e79cceb7","Significant advances have been made in facial image animation from a single image. Nonetheless, generating convincing facial feature movements remains a complex challenge in computer graphics. The purpose of this study is to develop an efficient and effective approach for transferring motion from a source video to a single facial image by governing the position and expression of the face in the video to generate a new video imitating the source image. Compared to prior methods that focus solely on manipulating facial expressions, this model has been trained to distinguish the moving foreground from the background image and to create motions such as facial rotation and translation as well as small local motions such as gaze shift. The proposed technique uses generative adversarial networks GANs with a motion transfer model. The network forecasts photorealistic video frames for a given target image using synthetic input in renderings from a parametric face model. The authenticity in this postprocessing conversion is attained by precise image manipulation. Thorough adversarial training is used to produce greater accuracy in this postprocessing conversion. Although more improvements to face landmark identification on videos and face super-resolution techniques have been made to improve the results, the proposed technique can provide more coherent videos with improved visual quality, resulting in more aligned landmark sequences for training. In addition, experiments indicate that we obtain superior results compared to those obtained by the stateof-the-art image-driven technique with PSNR 30.74 and SSIM 0.90. © 2022","","adversarial learning; face image super-resolution; image-to-video; motion transfer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131325660"
"Kim D.; Kyung R.","Kim, Dohyun (57998461600); Kyung, Richard (55259990900)","57998461600; 55259990900","Improving Image Quality Using Deep Learning Based Super Resolution","2022","2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference, IEMCON 2022","","","","275","279","4","10.1109/IEMCON56893.2022.9946503","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143617168&doi=10.1109%2fIEMCON56893.2022.9946503&partnerID=40&md5=639b9b0f32c9f7fcbc9caa56b72b7d14","Super resolution is one of the important computer vision tasks. A low-definition image can be changed to a high-definition image through super resolution. Likewise, it can be applied to a video. Especially, the popularization of smart devices and the inundation of video-based contents cause a gradual increase in the importance of super resolution tasks. However, the super resolution task is an ill-posed problem without only one correct answer. The reason is that given a low-definition image, there is no only one answer sheet corresponding to the high-definition version of the corresponding image with possible multiple answer sheets. Namely, super resolution is the technology greatly contributing to society but has an ill-posed problem. Hence, super resolution is a very invaluable subject from the research perspective. The study performs the recurrence of the existing methodology for super resolution and presents the new deep learning model called the Boosted Super Resolution Generative Adversarial Nets (BSRGAN) by improving the methodology.  © 2022 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Optical resolving power; Answer sheets; High definition; Ill posed problem; Learning models; Learning-based super-resolution; Low-definition image; Popularization; Smart devices; Superresolution; Video-based contents; Computer vision","computer vision; high-definition; low-definition image; popularization; smart devices; Super resolution; video-based contents","Conference paper","Final","","Scopus","2-s2.0-85143617168"
"Lisha P.P.; Jayasree V.K.","Lisha, P.P. (35292683700); Jayasree, V.K. (6506608483)","35292683700; 6506608483","Novel Deep Learning Technique to Improve Resolution of Low-Quality Finger Print Image for Bigdata Applications","2022","International Journal of Advanced Computer Science and Applications","13","8","","718","724","6","10.14569/IJACSA.2022.0130882","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137164893&doi=10.14569%2fIJACSA.2022.0130882&partnerID=40&md5=591c19d3c0592a745b541af2a8f8e70b","High-resolution images are highly in demand when they are utilized for different analysis purposes and obviously due to their quality aesthetic visual impact. The objective of image super-resolution (SR) is to reconstruct a high-resolution (HR) image from a low-resolution (LR)image. Storing, transferring and processing of high-resolution (HR) images have got many practical issues in big data domain. In the case of finger print images, the data is huge because of the huge number of populations. So instead of transferring or storing these finger print images in its original form (HR images), it cost very low if we choose its low-resolution form. By using sampling technique, we can easily generate LR images, but the main problem is to regenerate HR image from these LR images. So, this paper addresses this problem, a novel method for enhancing resolution of low-resolution fingerprint images of size 50 x 50 to a highresolution image of size 400 x 400 using convolutional neural network (CNN) architecture followed by sub pixel convolution operation for up sampling with no loss of promising features available in low-resolution image has been proposed. The proposed model contains five convolutional layers, each of which has an appropriate number of filter channels, activation functions, and optimization functions. The proposed model was trained using three publicly accessible fingerprint datasets FVC 2004 DB1, DB2, and DB3 after being validation and testing were done using 10 percent of these fingerprint data sets. In terms of performance measures like Peak Signal to Noise Ratio (PSNR), Mean Squared Error (MSE), Structural Similarity Index (SSIM) and loss functions, the quantitative and qualitative results show that the proposed model greatly outperformed existing state-ofthe- art techniques like Enhanced deep residual network (EDSR), wide activation for image and video SR (WDSR), Generative adversarial network(GAN) based models and Auto-encoder-based models. © 2022, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Big data; Biometrics; Chemical activation; Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image enhancement; Image quality; Learning systems; Mean square error; Optical resolving power; Population statistics; Signal to noise ratio; Convolution neural network; Finger print; Fingerprint images; High-resolution images; Image super resolutions; Learning techniques; Low resolution images; Lower resolution; Single image super-resolution; Single images; Convolution","Biometric; Convolution neural network; Fingerprint images; Single image super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137164893"
"Vedavyas Y.; Harsha S.S.; Subhash M.S.; Vasavi S.","Vedavyas, Y. (57643442700); Harsha, S. Sri (57643442800); Subhash, M. Sai (57643442900); Vasavi, S. (23986854500)","57643442700; 57643442800; 57643442900; 23986854500","Quality Enhancement for Drone Based Video using FPGA","2022","Proceedings of the International Conference on Electronics and Renewable Systems, ICEARS 2022","","","","29","34","5","10.1109/ICEARS53579.2022.9751731","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128970209&doi=10.1109%2fICEARS53579.2022.9751731&partnerID=40&md5=72b98934bf9f3f3f445e4c5f4a6ce4bc","Nowadays Drones are being widely used for surveillance and various other activities. The video stream produced by the drone can be disturbing or can contain noise data which might reduce the quality of the video stream. The video stream can be enhanced so that there is no disturbance in the video stream. The video enhancement can be done in real-time with the help of field programmable gate array (FPGA) which reduces the processing time with low energy consumption. Our project mainly focuses on enhancing the quality of the video stream using enhanced super-resolution generative adversarial networks (ESRGAN), contrast-limited Adaptive histogram equalization (CLAHE), Gamma Correction and Saturation Adjustment by integrating the image source in the drone with the FPGA. © 2022 IEEE.","Drones; Energy utilization; Equalizers; Generative adversarial networks; Graphic methods; Image enhancement; Logic gates; Optical resolving power; Signal receivers; Video streaming; Adaptive histograms; Contrast-limited adaptive histogram equalization; Drone video; Enhanced super-resolution generative adversarial network; Gamma correction; Histogram equalizations; Images processing; Quality enhancement; Saturation adjustment; Superresolution; Field programmable gate arrays (FPGA)","Contrast-Limited Adaptive Histogram Equalization; Drone Video; Enhanced Super-Resolution Generative Adversarial Networks; Field Programmable Gate Arrays; Gamma Correction; Image Processing; Quality Enhancement; Saturation Adjustment","Conference paper","Final","","Scopus","2-s2.0-85128970209"
"Zhang Y.; Yu X.; Lu X.; Liu P.","Zhang, Yang (57201701049); Yu, Xin (57191429254); Lu, Xiaobo (14627586600); Liu, Ping (57190793397)","57201701049; 57191429254; 14627586600; 57190793397","Pro-UIGAN: Progressive Face Hallucination From Occluded Thumbnails","2022","IEEE Transactions on Image Processing","31","","","3236","3250","14","10.1109/TIP.2022.3167280","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128696418&doi=10.1109%2fTIP.2022.3167280&partnerID=40&md5=cc9c7b2262268fd5dd1718d7c47e074f","In this paper, we study the task of hallucinating an authentic high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed Pro-UIGAN, which exploits facial geometry priors to replenish and upsample ( $8\times $ ) the occluded and tiny faces ( $16\times 16$ pixels). Pro-UIGAN iteratively (1) estimates facial geometry priors for low-resolution (LR) faces and (2) acquires non-occluded HR face images under the guidance of the estimated priors. Our multi-stage hallucination network upsamples and inpaints occluded LR faces via a coarse-to-fine fashion, significantly reducing undesirable artifacts and blurriness. Specifically, we design a novel cross-modal attention module for facial priors estimation, in which an input face and its landmark features are formulated as queries and keys, respectively. Such a design encourages joint feature learning across the input facial and landmark features, and deep feature correspondences will be discovered by attention. Thus, facial appearance features and facial geometry priors are learned in a mutually beneficial manner. Extensive experiments show that our Pro-UIGAN attains visually pleasing completed HR faces, thus facilitating downstream tasks, i.e., face alignment, face parsing, face recognition as well as expression classification.  © 1992-2012 IEEE.","Algorithms; Face; Hallucinations; Head; Humans; Image Processing, Computer-Assisted; Generative adversarial networks; Geometry; Job analysis; Optical resolving power; Semantic Web; Semantics; Face; Face hallucination; Face inpainting; Facial geometry; High resolution; Inpainting; Multi-stages; Superresolution; Task analysis; algorithm; diagnostic imaging; face; hallucination; head; human; image processing; procedures; Face recognition","face hallucination; Face inpainting; generative adversarial network; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128696418"
"Luo P.; Zhang J.; Zhou J.","Luo, Peixiang (57887386400); Zhang, Jinchao (57216614926); Zhou, Jie (57211746430)","57887386400; 57216614926; 57211746430","High-Resolution and Arbitrary-Sized Chinese Landscape Painting Creation Based on Generative Adversarial Networks","2022","IJCAI International Joint Conference on Artificial Intelligence","","","","5015","5018","3","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137884465&partnerID=40&md5=eec907d43f7d2c2f91412b4127c48460","This paper outlines an automated creation system for Chinese landscape paintings based on generative adversarial networks. The system consists of three cascaded modules: generation, resizing, and super-resolution. The generation module first generates a square-shaped painting, then the resizing module predicts an appropriate aspect ratio for it and performs resizing, and finally the super-resolution module is used to increase the resolution and improve the quality. After training each module with the images we collected from the web, our system can create high-resolution landscape paintings in arbitrary sizes. © 2022 International Joint Conferences on Artificial Intelligence. All rights reserved.","Aspect ratio; Optical resolving power; Aspect-ratio; High resolution; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85137884465"
"Zhuge H.; Ashman K.; Summa B.; Brown J.Q.","Zhuge, Huimin (57795992400); Ashman, Kimberly (57795859600); Summa, Brian (36457959800); Brown, J. Quincy (55500623600)","57795992400; 57795859600; 36457959800; 55500623600","Super-resolution and Focus Recovery in Whole Slide Images using CycleGAN","2022","Optics InfoBase Conference Papers","","","MW4A.4","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136429098&partnerID=40&md5=9cf0f95bae639bb6d65f766cd54047cd","The unsupervised Cycle conditional generative adversarial networks (Cycle GAN) was modified to achieve high-resolution outputs from low-resolution inputs, and dynamic focus detection and recovery, in large- scale whole slide images (WSI). © 2022 The Authors).","Image processing; Dynamic focus; Focus detection; High-resolution output; Large-scales; Lower resolution; Superresolution; Whole slide images; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85136429098"
"Hsu T.-H.; Wu P.-H.; Horng G.-J.","Hsu, Tz-Heng (7401791666); Wu, Po-Hui (57216417068); Horng, Gwo-Jiun (23094553700)","7401791666; 57216417068; 23094553700","Design of a High-Resolution Video Reconstruction and Transcoding System based on Super-Resolution GAN","2022","Proceedings - 2022 IET International Conference on Engineering Technologies and Applications, IET-ICETA 2022","","","","","","","10.1109/IET-ICETA56553.2022.9971690","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145355682&doi=10.1109%2fIET-ICETA56553.2022.9971690&partnerID=40&md5=0c07c68de056aeb2ac0eaf701d6de257","With the popularization of high-definition video devices, more and more users and video streaming service providers desire to restore early classic videos and deliver classic video contents with improved resolution. However, existing video transcoding systems do not support super-resolution video reconstruction. A high-resolution video reconstruction and transcoding system design based on super-resolution image generative adversarial network (GAN) is proposed to perform super-resolution video reconstruction for low resolution videos and then transcode videos with different resolutions to meet the playback requirements of heterogeneous mobile devices in order to provide users with good video viewing quality. Users can have good video viewing quality with the proposed system even in the absence of the original high-resolution videos.  © 2022 IEEE.","Image reconstruction; Optical resolving power; Video recording; Video signal processing; High resolution; High-definition videos; Reconstruction systems; Resolution video; Super resolution video reconstruction; Superresolution; Transcoding systems; Video reconstruction; Video transcoding server; Video-transcoding; Generative adversarial networks","Generative Adversarial Network; Super Resolution Video Reconstruction; Video Transcoding Server","Conference paper","Final","","Scopus","2-s2.0-85145355682"
"","","","15th International Conference on ICT, Society and Human Beings, ICT 2022, 19th International Conference on Web Based Communities and Social Media, WBC 2022 and 14th International Conference on e-Health, EH 2022 - Held at the 16th Multi Conference on Computer Science and Information Systems, MCCSIS 2022","2022","15th International Conference on ICT, Society and Human Beings, ICT 2022, 19th International Conference on Web Based Communities and Social Media, WBC 2022 and 14th International Conference on e-Health, EH 2022 - Held at the 16th Multi Conference on Computer Science and Information Systems, MCCSIS 2022","","","","","","288","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142216119&partnerID=40&md5=3ae28646eacb2d774de0e9c39d86b64e","The proceedings contain 38 papers. The topics discussed include: technological, organizational and personal factors of remote work: an exploratory study; promoting the road safety through the augmented reality: an Italian experience in occupational safety and health; exploring consumer attitude toward sustainable energy-efficient appliance: preliminary findings for augmented reality application; the human in the home: privacy invasion risks of smart home appliances and devices; virtual reality applications in autism spectrum disorder: a systematic review; cybernetic philosophy of digital public governance: modeling recursive sensory systems; leveraging SOCMINT: extrapolating cyber threat intelligence from Russia-Ukraine conflict; expectations of software development education: students vs professionals; the emergence of liminal cyberspace – challenges for the ontological work in cybersecurity; artificial intelligence and gender equality: a systematic mapping study; centralized or de-centralized data and algorithms in the Finnish health care infrastructure; and single MR image super-resolution using generative adversarial network.","","","Conference review","Final","","Scopus","2-s2.0-85142216119"
"Hou Z.; An H.; He L.; Li E.; Lai D.","Hou, Zhongwei (57912614600); An, Hongyu (57913264200); He, Li (57913267300); Li, En (57203282298); Lai, Dakun (56285568200)","57912614600; 57913264200; 57913267300; 57203282298; 56285568200","Super-Resolution Reconstruction Algorithm for Terahertz Images","2022","2022 3rd International Conference on Pattern Recognition and Machine Learning, PRML 2022","","","","180","185","5","10.1109/PRML56267.2022.9882225","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139127363&doi=10.1109%2fPRML56267.2022.9882225&partnerID=40&md5=1323cd3df6956e3b0eac1598ddea2981","Terahertz imaging technology has great prospect in applications of nondestructive testing, biomedicine and security inspection. However, terahertz imaging has not received a satisfactory image quality so far due to problems such as complex noise, artifacts and low resolution. In this work, an image preprocessing algorithm by combining an improved wavelet thresholding function and a generative adversarial network image super-resolution algorithm was proposed. Specifically, the constant deviation between the original wavelet coefficients and the estimated wavelet coefficients could be reduced by improving the threshold function and setting appropriate adjustment parameters to dynamically select a fixed threshold. As such the approximation between of the reconstructed image and the original image could be improved. In addition, the generative network combined with the improved loss function was designed to obtain the detailed information of the images at multiple scales. Moreover, an image database containing 51,300 images was built by degrading and data expanding the original 3650 images collected from the DF2K dataset so as to evaluate the proposed super-resolution reconstruction algorithm of terahertz images. The obtained results show that there were more than 2 dB and at less 1 dB improvement compared with previous median filtering and the hard/soft threshold wavelet, respectively. Consequently, the preliminarily reconstructed terahertz images suggested that the improved threshold wavelet function together with the generative adversarial network may offer the possibility of provide a high quality image especially for nondestructive testing.  © 2022 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; Median filters; Optical resolving power; Terahertz waves; Wavelet transforms; Deep learning; Reconstruction algorithms; Super-resolution reconstruction; Superresolution; Tera Hertz; Terahertz imaging; Threshold functions; THZ imaging; Wavelet threshold; Wavelet threshold function; Nondestructive examination","deep learning; super-resolution; THZ imaging; wavelet threshold function","Conference paper","Final","","Scopus","2-s2.0-85139127363"
"Chen H.-Q.; Xie K.; Li M.-R.; Wen C.; He J.-B.","Chen, Hua-Quan (57222900073); Xie, Kai (8972776100); Li, Mei-Ran (57789296300); Wen, Chang (56811792800); He, Jian-Biao (14032744200)","57222900073; 8972776100; 57789296300; 56811792800; 14032744200","Face Recognition With Masks Based on Spatial Fine-Grained Frequency Domain Broadening","2022","IEEE Access","10","","","75536","75548","12","10.1109/ACCESS.2022.3191113","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135233968&doi=10.1109%2fACCESS.2022.3191113&partnerID=40&md5=99c200b5e859019f379adf51ff0f33ff","Along with social distancing, wearing masks is an effective method of preventing the transmission of COVID-19 in the ongoing pandemic. However, masks occlude a large number of facial features, preventing facial recognition. The recognition rate of existing methods may be significantly reduced by the presence of masks. In this paper, we propose a method to effectively solve the problem of the lack of facial feature information needed to perform facial recognition on people wearing masks. The proposed approach uses image super-resolution technology to perform image preprocessing along with a deep bilinear module to improve EfficientNet. It also combines feature enhancement with frequency domain broadening, fuses the spatial features and frequency domain features of the unoccluded areas of the face, and classifies the fused features. The features of the unoccluded area are increased to improve the accuracy of recognition of masked faces. The results of a cross-validation show that the proposed approach achieved an accuracy of 98% on the RMFRD dataset, as well as a higher recognition rate and faster speed than previous methods. In addition, we also performed an experimental evaluation in an actual facial recognition system and achieved an accuracy of 99%, which demonstrates the effectiveness and practicability of the proposed method. © 2013 IEEE.","Face recognition; Generative adversarial networks; Image enhancement; Neural networks; Bilinear module; Convolutional neural network; Domain broadening; Face recognition with mask; Facial feature; Features extraction; Frequency domain widening; Frequency domains; Frequency-domain analysis; RMFRD dataset; Frequency domain analysis","bilinear module; convolutional neural network; Face recognition with mask; frequency domain widening; RMFRD dataset","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135233968"
"Conmy A.; Mukherjee S.; Schönlieb C.-B.","Conmy, Arthur (57311205200); Mukherjee, Subhadip (57212613876); Schönlieb, Carola-Bibiane (24544878300)","57311205200; 57212613876; 24544878300","STYLEGAN-INDUCED DATA-DRIVEN REGULARIZATION FOR INVERSE PROBLEMS","2022","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2022-May","","","3788","3792","4","10.1109/ICASSP43922.2022.9747632","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131253125&doi=10.1109%2fICASSP43922.2022.9747632&partnerID=40&md5=6dec9ac8b13dd416fe053235a4783e00","Recent advances in generative adversarial networks (GANs) have opened up the possibility of generating high-resolution photo-realistic images that were impossible to produce previously. The ability of GANs to sample from high-dimensional distributions has naturally motivated researchers to leverage their power for modeling the image prior in inverse problems. We extend this line of research by developing a Bayesian image reconstruction framework that utilizes the full potential of a pre-trained StyleGAN2 generator, which is the currently dominant GAN architecture, for constructing the prior distribution on the underlying image. Our proposed approach, which we refer to as learned Bayesian reconstruction with generative models (L-BRGM), entails joint optimization over the style-code and the input latent code, and enhances the expressive power of a pre-trained StyleGAN2 generator by allowing the style-codes to be different for different generator layers. Considering the inverse problems of image inpainting and super-resolution, we demonstrate that the proposed approach is competitive with, and sometimes superior to, state-of-the-art GAN-based image reconstruction methods. © 2022 IEEE","Computer vision; Differential equations; Generative adversarial networks; Image reconstruction; Bayesian image reconstruction; Data driven; Generative prior; High-dimensional; High-resolution photos; Higher-dimensional; Image priors; Photorealistic images; Power; Regularisation; Inverse problems","generative prior; Inverse problems","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85131253125"
"El-Shafai W.; Mohamed E.M.; Zeghid M.; Ali A.M.; Aly M.H.","El-Shafai, Walid (55819860600); Mohamed, Ehab Mahmoud (36169585800); Zeghid, Medien (19934668700); Ali, Anas M. (57198724497); Aly, Moustafa H. (57607518400)","55819860600; 36169585800; 19934668700; 57198724497; 57607518400","Hybrid Single Image Super-Resolution Algorithm for Medical Images","2022","Computers, Materials and Continua","72","3","","4879","4896","17","10.32604/cmc.2022.028364","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128644314&doi=10.32604%2fcmc.2022.028364&partnerID=40&md5=408fe2faea8359df2927fb883b1dbc4e","High-quality medical microscopic images used for diseases detection are expensive and difficult to store. Therefore, low-resolution images are favorable due to their low storage space and ease of sharing, where the images can be enlarged when needed using Super-Resolution (SR) techniques. However, it is important to maintain the shape and size of the medical images while enlarging them. One of the problems facing SR is that the performance of medical image diagnosis is very poor due to the deterioration of the reconstructed image resolution. Consequently, this paper suggests a multi-SR and classification framework based on Generative Adversarial Network (GAN) to generate high-resolution images with higher quality and finer details to reduce blurring. The proposed framework comprises five GAN models: Enhanced SR Generative Adversarial Networks (ESRGAN), Enhanced deep SR GAN (EDSRGAN), Sub-Pixel-GAN, SRGAN, and Efficient Wider Activation-B GAN (WDSR-b-GAN). To train the proposed models, we have employed images from the famous BreakHis dataset and enlarged them by 4× and 16× upscale factors with the ground truth of the size of 256×256×3. Moreover, several evaluation metrics like Peak Signal-to-NoiseRatio (PSNR), Mean Squared Error (MSE), Structural Similarity Index (SSIM), Multiscale Structural Similarity Index (MS-SSIM), and histogram are applied to make comprehensive and objective comparisons to determine the best methods in terms of efficiency, training time, and storage space. The obtained results reveal the superiority of the proposed models over traditional and benchmark models in terms of color and texture restoration and detection by achieving an accuracy of 99.7433%. © 2022 Tech Science Press. All rights reserved.","Deterioration; Diagnosis; Image resolution; Mean square error; Medical imaging; Textures; Medical image; Multiscale structural similarity index; Peak signal-to-noiseratio; Similarity indices; SISR; Structural similarity; Structural similarity index; Superresolution; Generative adversarial networks","GAN; medical images; MS-SSIM; PSNR; SISR; SSIM","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128644314"
"Alshehhi R.","Alshehhi, Rasha (57193553724)","57193553724","Super-Resolution of Solar Active Region Patches Using Generative Adversarial Networks","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13231 LNCS","","","451","462","11","10.1007/978-3-031-06427-2_38","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130978737&doi=10.1007%2f978-3-031-06427-2_38&partnerID=40&md5=5da7dacfe900a2d441d8ce22ab43650e","Monitoring solar active region patches from Helioseismic and Magnetic Imager (HMI) instruments is essential for space weather forecasting. However, recovering small bipolar details in HMI patches requires additional pre-processing steps to obtain better quality. This work uses a generative adversarial network, with transposed convolution and super-pixel convolution up-sampling layers, to generate the higher quality of HMI patches. It trains and validates the network based on binary cross-entropy, mean absolute error and multi-scale dice-coefficient functions. It illustrates the performance of the generative method in two image types (magnetogram and continuum intensity patches) from two instruments (SDO/HMI and SOT/NET). It also compares its performance with state-of-the-art methods. The results demonstrate that the generative method produces high-quality images by increasing polarity contrast and retrieving smaller structures. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Convolution; Generative adversarial networks; Space research; Weather forecasting; Dice coefficient; Generative methods; Helioseismic; Multi-scale dice-coefficient; Multi-scales; Performance; Solar active region patch; Solar active regions; Space weather; Superresolution; Solar energy","Generative adversarial network; Multi-scale dice-coefficient; Solar active region patches; Space weather","Conference paper","Final","","Scopus","2-s2.0-85130978737"
"Lavreniuk M.; Kussul N.; Shelestov A.; Lavrenyuk A.; Shumilo L.","Lavreniuk, Mykola (56667743100); Kussul, Nataliia (6602485938); Shelestov, Andrii (6507365226); Lavrenyuk, Alla (16444915500); Shumilo, Leonid (57208256914)","56667743100; 6602485938; 6507365226; 16444915500; 57208256914","Super Resolution Approach for the Satellite Data Based on the Generative Adversarial Networks","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1095","1098","3","10.1109/IGARSS46834.2022.9884460","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140369300&doi=10.1109%2fIGARSS46834.2022.9884460&partnerID=40&md5=9273c26a2a94b7f402c373876be0b2e1","In the past few years, medium and high-resolution data became freely available for downloading. It provides great opportunity for researchers not to select between solving the task with high-resolution data on small territory or on global scale, but with low-resolution satellite images. Due to high spectral and spatial resolution of the data, Sentinel-1 and Sentinel-2 are very popular sources of information. Nevertheless, in practice if we would like to receive final product in 10 m resolution we should use bands with 10 m resolution. Sentinel-2 has four such bands, but also has other bands, especially red-edge 20 m resolution bands that are useful for vegetation analysis and often are omitted due to lower resolution. Thus, in this study we propose methodology for enhancing resolution (super-resolution) of the existing low-resolution images to higher resolution images. The main idea is to use advanced methods of deep learning-Generative Adversarial Networks (GAN) and train it to increase the resolution for the satellite images. Experimental results for the Sentinel-2 data showed that this approach is efficient and could be used for creating high resolution products. © 2022 IEEE.","Deep learning; Image enhancement; Optical resolving power; Remote sensing; Satellites; Deep learning; Global scale; High resolution data; High spatial resolution; High spectral resolution; Lower resolution; Satellite data; Satellite images; Sentinel-2; Superresolution; Generative adversarial networks","deep learning; GAN; Generative Adversarial Networks; Sentinel-2; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85140369300"
"Wang R.; Zhao R.; Fu W.; Zhang X.; Zhang Y.; Feng R.","Wang, Runhan (58073855300); Zhao, Ruiwei (37021956300); Fu, Weijia (58074371000); Zhang, Xiaobo (37092301200); Zhang, Yuejie (9734634900); Feng, Rui (56611353400)","58073855300; 37021956300; 58074371000; 37092301200; 9734634900; 56611353400","Multi-contrast High Quality MR Image Super-Resolution with Dual Domain Knowledge Fusion","2022","Proceedings - 2022 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2022","","","","2127","2134","7","10.1109/BIBM55620.2022.9995219","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146704027&doi=10.1109%2fBIBM55620.2022.9995219&partnerID=40&md5=bb6484d9a0fae1ff287ecfde3f8a4e20","Multi-contrast high quality high-resolution (HR) Magnetic Resonance (MR) images enrich available information for diagnosis and analysis. Deep convolutional neural network methods have shown promising ability for MR image super-resolution (SR) given low-resolution (LR) MR images. Methods taking HR images as references (Ref) have made progress to enhance the effect of MR images SR. However, existing multi-contrast MR image SR approaches are based on contrasting-expanding backbones, which lose high frequency information of Ref image during downsampling. They also failed to transfer textures of Ref image into target domain. In this paper, we propose Edge Mask Transformer UNet (EMFU) for accelerating MR images SR. We propose Edge Mask Transformer (EMF) to generate global details and texture representation of target domain. Dual domain fusion module in UNet aggregates semantic information of the representation and LR image of target domain. Specifically, we extract and encode edge masks to guide the attention in EMF by re-distributing the embedding tensors, so that the network allocates more attention to image edge area. We also design a dual domain fusion module with self-attention and cross-attention to deeply fuse semantic information of multiple protocols for MRI. Extensive experiments show the effectiveness of our proposed EMFU, which surpasses state-of-the-art methods on benchmarks quantitatively and visually. Codes will be released to the community.  © 2022 IEEE.","Convolutional neural networks; Deep neural networks; Domain Knowledge; Generative adversarial networks; Image enhancement; Image fusion; Magnetic resonance imaging; Medical imaging; Optical resolving power; Quality control; Semantics; Textures; Adversarial learning; Domain knowledge; Dual domain; Dual domain knowledge fusion; Edge mask transformer; Generative adversarial learning; Image super resolutions; Knowledge fusion; MRI super-resolution; Superresolution; Magnetic resonance","Dual Domain Knowledge Fusion; Edge Mask Transformer; Generative Adversarial Learning; MRI Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85146704027"
"Zhang X.","Zhang, Xiaoliang (57211205838)","57211205838","A Dual Regression Scheme to Improve GAN in Low-Dose CT Scan Restoration","2022","Proceedings - 2022 International Conference on Big Data, Information and Computer Network, BDICN 2022","","","","732","737","5","10.1109/BDICN55575.2022.00142","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129676915&doi=10.1109%2fBDICN55575.2022.00142&partnerID=40&md5=bdd6da4529620df9280de79aa52981f8","The application of Super Resolution (SR) technology on recovering low-dose medical image is gaining its importance nowadays. Because it can provide as many contextual details as high-dose CT for physi-cians without harming the patient or spending more on devices. Usage of Deep Learning (DL) on SR tasks has been developed many years, and Generative Adversarial Network (GAN) plays a leading role among them. In this paper, we find that using a special closed-loop scheme, dual network, can strongly improve the performance of GAN model, with about 1.2 point of improvement in PSNR. And the best way to implement this dual network is also evaluated. © 2022 IEEE.","Computerized tomography; Deep learning; Medical imaging; Optical resolving power; Closed-loop; CT-scan; Dual regression; High dose; Low dose; Low-dose CT; Network models; Performance; Superresolution; Generative adversarial networks","CT-scans; Dual regression; GAN; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85129676915"
"Li L.; Xiahou J.","Li, Licheng (57854450300); Xiahou, Jiangbin (36465827100)","57854450300; 36465827100","Game Image Quality Enhancement Algorithm based on Generative Adversarial Network and Knowledge Distillation","2022","IEEE Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","2022-June","","","2570","2575","5","10.1109/ITAIC54216.2022.9836707","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136371951&doi=10.1109%2fITAIC54216.2022.9836707&partnerID=40&md5=40f4f8517ef8f175d844f4c2284fa9db","Nowadays, with the development of computer graphics and the improvement of related hardware, the picture quality of games is getting higher and higher. Today's mainstream games support resolutions up to 4k or above. At the same time, for a large number of excellent games in the past, the picture quality is far behind today due to technical and hardware limitations at the time. Blurred picture quality is becoming more and more unbearable for users. For this reason, the game company will spend a lot of resources to reset the game to meet the needs of these users. The combination of super-resolution technology based on deep learning and generative adversarial networks can effectively improve image resolution. In this paper, we combines ESRGAN(Enhanced Super-Resolution Generative Adversarial Networks)to design a real-time game image quality enhancement algorithm, and distilled the model, reduce parameters for increase the game frames.  © 2022 IEEE.","Computer games; Computer graphics; Computer hardware; Deep learning; Distillation; Image enhancement; Image quality; Image resolution; Enhancement algorithms; Image quality enhancements; Image super resolutions; Knowledge distillation; Picture quality; Real-time games; Superresolution; Technology-based; Generative adversarial networks","Generative Adversarial Network; image super resolution; Knowledge Distillation","Conference paper","Final","","Scopus","2-s2.0-85136371951"
"Nasser S.A.; Shamsi S.; Bundele V.; Garg B.; Sethi A.","Nasser, Sahar Almahfouz (57222990898); Shamsi, Saqib (57203248860); Bundele, Valay (57226650118); Garg, Bhavesh (57432695000); Sethi, Amit (12775689700)","57222990898; 57203248860; 57226650118; 57432695000; 12775689700","Perceptual cGAN for MRI Super-resolution","2022","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2022-July","","","3035","3038","3","10.1109/EMBC48229.2022.9871832","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138126734&doi=10.1109%2fEMBC48229.2022.9871832&partnerID=40&md5=eea362a77d96283b8b97c836fff5f9d9","Capturing high-resolution magnetic resonance (MR) images is a time consuming process, which makes it unsuitable for medical emergencies and pediatric patients. Low-resolution MR imaging, by contrast, is faster than its high-resolution counterpart, but it compromises on fine details necessary for a more precise diagnosis. Super-resolution (SR), when applied to low-resolution MR images, can help increase their utility by synthetically generating high-resolution images with little additional time. In this paper, we present an SR technique for MR images that is based on generative adversarial networks (GANs), which have proven to be quite useful in producing sharp-looking details in SR. We introduce a conditional GAN with perceptual loss, which is conditioned upon the input low-resolution image, which improves the performance for isotropic and anisotropic MRI super-resolution. Clinical Relevance- MR image super-resolution has the potential for improving image acquisition speed to save the time of the clinicians, while guaranteeing high-quality images. © 2022 IEEE.","Anisotropy; Child; Humans; Magnetic Resonance Imaging; Records; Diagnosis; Generative adversarial networks; Image enhancement; Magnetic resonance imaging; Medical imaging; Optical resolving power; Emergency patients; High resolution; High-resolution images; Low resolution images; Lower resolution; Medical emergency; Pediatric patients; Performance; Resolution techniques; Superresolution; anisotropy; child; human; information processing; nuclear magnetic resonance imaging; Magnetic resonance","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85138126734"
"Kalpana A.; John A.","Kalpana, A. (57153137900); John, Anju (58066682300)","57153137900; 58066682300","Pix2Pix GAN Image Synthesis To Detect Electric Vehicle License Plate","2022","Proceedings of 4th International Conference on Cybernetics, Cognition and Machine Learning Applications, ICCCMLA 2022","","","","439","443","4","10.1109/ICCCMLA56841.2022.9989063","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146339089&doi=10.1109%2fICCCMLA56841.2022.9989063&partnerID=40&md5=cc5a27d80b9f7a4fae6a4753e263e68b","The area of image processing is more intensive in development and research activities for decades. The role of image processing is huge in modeling, analytics, communication, computation, information security, information forensics and smart city application. Images are ubiquitous in day to day life and images or videos play dominant role in monitoring applications. But when it comes to development of specific application, collection of data is a very challenging task. Nowadays deep learning plays a significant role for generation of data. Robust technologies like Generative Adversarial Network (GAN) and Cycle GAN play a crucial role for generating realistic images with super resolution. GAN and its associated methods used for image synthesis improve the accuracy of deep learning models. In this paper, we analyze challenges of license plate recognition in realistic situation and experiments demonstrate that GAN can generate realistic images to improve the accuracy of license plate recognition. © 2022 IEEE.","Deep learning; Image enhancement; License plates (automobile); Optical character recognition; Security of data; Deep learning; Development activity; Generator; Images processing; Images synthesis; Information Forensics; Licenses plate recognition; Realistic images; Research activities; Vehicle license plates; Generative adversarial networks","Deep Learning; Generative Adversarial Network; Generator; Image synthesis; License Plate Recognition","Conference paper","Final","","Scopus","2-s2.0-85146339089"
"Liu H.; Liu Q.","Liu, Hangyu (57928272000); Liu, Qicheng (57928116800)","57928272000; 57928116800","Image Creation Based on Transformer and Generative Adversarial Networks","2022","IEEE Access","10","","","108296","108306","10","10.1109/ACCESS.2022.3213079","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139854821&doi=10.1109%2fACCESS.2022.3213079&partnerID=40&md5=2ccc3cbfe3fb543a68b87ddb93fe05d0","To address the problem of low authenticity of generated images in existing generative models, the transformer super-resolution generative adversarial network(TransSRGAN) model based on the generative adversarial network is proposed. The generator of the model uses the transformer encoder sub-module as the basic module. The features of the input vector are extracted. low-definition images are generated through the transformer encoder submodule, and the low-definition image is up-sampled by the convolutional neural network to complete the image generation. The discriminator of this model uses the convolutional neural network as the basic module. To discriminate the real samples from the generated fake samples, the discriminator extracts the image features by the convolutional neural network. The experimental results show that the TransSRGAN model brings the distribution of the generated samples closer to the training samples, effectively raises the quality of the generated samples, improves the authenticity of the generated samples, and enriches the diversity of the generated samples. During the training process, there was no mode collapse or instability.  © 2013 IEEE.","Authentication; Convolution; Extraction; Image processing; Network coding; Neural networks; Optical resolving power; Convolutional neural network; Encodings; Features extraction; Image generations; Images synthesis; Self-attention; Superresolution; Training data; Transformer; Generative adversarial networks","generative adversarial network; Image generation; self-attention; transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85139854821"
"Xiu J.; Qu X.; Yu H.","Xiu, Jie (57563364600); Qu, Xiujie (7202660844); Yu, Haowei (57564125600)","57563364600; 7202660844; 57564125600","Double discriminative face super-resolution network with facial landmark heatmaps","2022","Visual Computer","","","","","","","10.1007/s00371-022-02701-0","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141087210&doi=10.1007%2fs00371-022-02701-0&partnerID=40&md5=0f1d13d2e3a4976ef980ec9238ffd3ca","At present, most of face super-resolution (SR) networks cannot balance the visual quality and the pixel accuracy. The networks with high objective index values often reconstruct too smooth images, while the networks which can restore texture information often introduce too much high-frequency noise and artifacts. Besides, some face super-resolution networks do not consider the mutual promotion between the extracting face prior knowledge part and the super-resolution reconstruction part. To solve these problems, we propose the double discriminative face super-resolution network (DDFSRNet). We propose a collaborative generator and two discriminators. Specifically, the collaborative generator, including the face super-resolution module (FSRM) and the face alignment module (FAM), can strengthen the reconstruction of facial key components, under the restriction of the perceptual similarity loss, the facial heatmap loss and double generative adversarial loss. We design the feature fusion unit (FFU) in FSRM, which integrates the facial heatmap features and SR features. FFU can use the facial landmarks to correct the face edge shape. Moreover, the double discriminators, including the facial SR discriminator (FSRD) and the facial landmark heatmap discriminator (FLHD), are used to judge whether face SR images and face heatmaps are from real data or generated data, respectively. Experiments show that the perceptual effect of our method is superior to other advanced methods on 4x reconstruction and fit the face high-resolution (HR) images as much as possible. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Discriminators; Image reconstruction; Optical resolving power; Textures; Deep learning; Face super-resolution; Facial landmark; Facial landmark heatmap; Features fusions; Fusion units; Heatmaps; Index values; Superresolution; Visual qualities; Generative adversarial networks","Deep learning; Face super-resolution; Facial landmark Heatmaps; Generative adversarial network","Article","Article in press","","Scopus","2-s2.0-85141087210"
"Zhou Y.; Zhang X.; Wang S.; Li L.","Zhou, Yichen (57891157000); Zhang, Xinfeng (57211151919); Wang, Shanshe (36645440900); Li, Lin (57216895582)","57891157000; 57211151919; 36645440900; 57216895582","Multi-Attribute Joint Point Cloud Super-Resolution with Adversarial Feature Graph Networks","2022","ICMEW 2022 - IEEE International Conference on Multimedia and Expo Workshops 2022, Proceedings","","","","","","","10.1109/ICMEW56448.2022.9859457","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138066866&doi=10.1109%2fICMEW56448.2022.9859457&partnerID=40&md5=9cd172ab1f5178dfaead6070c54f3d62","3D point cloud super-resolution (PCSR) plays an important role in many applications, which can infer a dense geometric shape from a sparse one. However, existing PCSR methods only leverage the geometric properties to predict dense geometric coordinates without considering the importance of correlated attributes in the prediction of complex geometric structures. In this paper, we propose a novel PCSR network by leveraging color attributes to improve the reconstruction quality of dense geometric shape. In the proposed network, we utilize graph convolutions to obtain cross-domain structure representation for point cloud from both geometric coordinates and color attributes, which is constructed by aggregating local points based on the similarity of cross-domain features. Furthermore, we propose a shape-aware loss function to cooperate with network training, which constrains the point cloud generation from both overall and detailed aspects. Extensive experimental results show that our proposed method outperforms the state-of-the-art methods from both objective and subjective quality. © 2022 IEEE.","Color; Computer vision; Convolution; Geometry; Optical resolving power; Color attributes; Convolutional networks; Cross-domain; Geometric shape; Graph convolutional network; Graph networks; Multi-attributes; Point cloud super-resolution; Point-clouds; Superresolution; Generative adversarial networks","color attributes; generative adversarial network; graph convolutional network; Point cloud super-resolution","Conference paper","Final","","Scopus","2-s2.0-85138066866"
"Yildiz B.","Yildiz, Beytullah (14632851900)","14632851900","Enhancing Image Resolution with Generative Adversarial Networks","2022","Proceedings - 7th International Conference on Computer Science and Engineering, UBMK 2022","","","","104","109","5","10.1109/UBMK55850.2022.9919520","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141825651&doi=10.1109%2fUBMK55850.2022.9919520&partnerID=40&md5=4d7d1d23663aada61ec4c2a319e5ded7","Super-resolution is the process of generating high-resolution images from low-resolution images. There are a variety of practical applications used in real-world problems such as high-definition content creation, surveillance imaging, gaming, and medical imaging. Super-resolution has been the subject of many researches over the past few decades, as improving image resolution offers many advantages. Going beyond the previously presented methods, Generative Adversarial Networks offers a very promising solution. In this work, we will use the Generative Adversarial Networks-based approach to obtain 4x resolution images that are perceptually better than previous solutions. Our extensive experiments, including perceptual comparison, Peak Signal-to-Noise Ratio, and classification success metrics, show that our approach is quite promising for image super-resolution.  © 2022 IEEE.","Deep learning; Image enhancement; Image resolution; Learning systems; Medical imaging; Signal to noise ratio; Content creation; Deep learning; High-definition contents; High-resolution images; Images processing; Low resolution images; Machine-learning; Real-world problem; Superresolution; Surveillance imaging; Generative adversarial networks","Deep Learning; Generative Adversarial Networks; Image Processing; Machine Learning; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85141825651"
"Liu J.; Sun Y.; Ren K.; Zhao Y.; Deng K.; Wang L.","Liu, Jia (36816241900); Sun, Yongjian (57211860991); Ren, Kaijun (35093112400); Zhao, Yanlai (45662261300); Deng, Kefeng (54583346900); Wang, Lizhe (23029267900)","36816241900; 57211860991; 35093112400; 45662261300; 54583346900; 23029267900","A Spatial Downscaling Approach for WindSat Satellite Sea Surface Wind Based on Generative Adversarial Networks and Dual Learning Scheme","2022","Remote Sensing","14","3","769","","","","10.3390/rs14030769","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124521852&doi=10.3390%2frs14030769&partnerID=40&md5=5769dd6774b23f35947860e44e4eceec","Sea surface wind (SSW) is a crucial parameter for meteorological and oceanographic research, and accurate observation of SSW is valuable for a wide range of applications. However, most existing SSW data products are at a coarse spatial resolution, which is insufficient, especially for regional or local studies. Therefore, in this paper, to derive finer-resolution estimates of SSW, we present a novel statistical downscaling approach for satellite SSW based on generative adversarial networks and dual learning scheme, taking WindSat as a typical example. The dual learning scheme performs a primal task to reconstruct high resolution SSW, and a dual task to estimate the degradation kernels, which form a closed loop and are simultaneously learned, thus introducing an additional constraint to reduce the solution space. The integration of a dual learning scheme as the generator into the generative adversarial network structure further yield better downscaling performance by fine-tuning the generated SSW closer to high-resolution SSW. Besides, a model adaptation strategy was exploited to enhance the capacity for downscaling from low-resolution SSW without high-resolution ground truth. Comprehensive experiments were conducted on both the synthetic paired and unpaired SSW data. In the study areas of the East Coast of North America and the North Indian Ocean, in this work, the downscaling results to 0.25° (high resolution on the synthetic dataset), 0.03125° (8× downscaling), and 0.015625° (16× downscaling) of the proposed approach achieve the highest accuracy in terms of root mean square error and R-Square. The downscaling resolution can be enhanced by increasing the basic blocks in the generator. The highest downscaling reconstruction quality in terms of peak signal-to-noise ratio and structural similarity index was also achieved on the synthetic dataset with high-resolution ground truth. The experimental results demonstrate the effectiveness of the proposed downscaling network and the superior performance compared with the other typical advanced downscaling methods, including bicubic interpolation, DeepSD, dual regression networks, and adversarial DeepSD. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Mean square error; Remote sensing; Satellites; Signal to noise ratio; Space optics; Surface waters; Deep learning; Down-scaling; Dual learning; High resolution; Learning schemes; Satellite remote sensing; Sea surface wind; Statistical downscaling; Superresolution; WindSat; Generative adversarial networks","Deep learning; Dual learning; Generative adversarial network; Satellite remote sensing; Sea surface wind; Statistical downscaling; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124521852"
"Thakoor K.A.; Carter A.; Song G.; Wax A.; Moussa O.; Chen R.W.S.; Hendon C.; Sajda P.","Thakoor, Kaveri A. (55938126400); Carter, Ari (57962214200); Song, Ge (57202228999); Wax, Adam (57203031135); Moussa, Omar (57221782149); Chen, Royce W. S. (22955600500); Hendon, Christine (56096639200); Sajda, Paul (57204342918)","55938126400; 57962214200; 57202228999; 57203031135; 57221782149; 22955600500; 56096639200; 57204342918","Enhancing Portable OCT Image Quality via GANs for AI-Based Eye Disease Detection","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13573 LNCS","","","155","167","12","10.1007/978-3-031-18523-6_15","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141797104&doi=10.1007%2f978-3-031-18523-6_15&partnerID=40&md5=d5d181b75d77e6953e3743cd4e1da0ac","Optical coherence tomography (OCT) is widely used for detection of ophthalmic diseases, such as glaucoma, age-related macular degeneration (AMD), and diabetic retinopathy. Using a low-coherence-length light source, OCT is able to achieve high axial resolution in biological samples; this depth information is used by ophthalmologists to assess retinal structures and characterize disease states. However, OCT systems are often bulky and expensive, costing tens of thousands of dollars and weighing on the order of 50 pounds or more. Such constraints make it difficult for OCT to be accessible in low-resource settings. In the U.S. alone, only 15.3% of diabetic patients meet the recommendation of obtaining annual eye exams; the situation is even worse for minority/under-served populations. In this study, we focus on data acquired with a low-cost, portable OCT (p-OCT) device, characterized by lower resolution, scanning rate, and imaging depth than a commercial OCT system. We use generative adversarial networks (GANs) to enhance the quality of this p-OCT data and then assess the impact of this enhancement on downstream performance of artificial intelligence (AI) algorithms for AMD detection. Using GANs trained on simulated p-OCT data generated from paired commercial OCT data degraded with the point spread function (PSF) of the p-OCT device, we observe improved AI performance on p-OCT data after single-image super-resolution. We also achieve denoising after image-to-image translation. By exhibiting proof-of-principle AI-based AMD detection even on low-quality p-OCT data, this study stimulates future work toward low-cost, portable imaging+AI systems for eye disease detection. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Costs; Eye protection; Image enhancement; Light sources; Medical imaging; Ophthalmology; Optical tomography; Optical transfer function; Age-related macular degeneration; Coherence lengths; Diabetic retinopathy; Disease detection; Eye disease; Low-coherence; Low-costs; Performance; Portable optical coherence tomography; Tomography system; Generative adversarial networks","Eye disease; Generative adversarial networks; Portable optical coherence tomography","Conference paper","Final","","Scopus","2-s2.0-85141797104"
"Xiang N.; Tang B.; Wang L.","Xiang, Nan (57192578347); Tang, Bin (57852825200); Wang, Lu (57852753300)","57192578347; 57852825200; 57852753300","Image Super-Resolution Method Based on Improved Generative Adversarial Network","2022","2022 IEEE 5th International Conference on Electronics Technology, ICET 2022","","","","1207","1212","5","10.1109/ICET55676.2022.9824258","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136332705&doi=10.1109%2fICET55676.2022.9824258&partnerID=40&md5=2dceb4fa398ed6e3266843d46135c662","The image super-resolution algorithm based on generative adversarial network has the problem of imperfect extraction of detailedure features, and the network is difficult to converge in the training process. Therefore, a new image super-resolution method: MR-SRGAN (Multi-branch receptive field dense block improved super-resolution generative adversarial network) is proposed. MR-SRGAN extracts image detailure features through the new MBRS residual block and MRB module, then adjusts the loss function of the model to make the training easier to converge. Finally, in the super-resolution comparison experiment on Set5 and Set14 datasets, under the magnification of 2 and 4 times, the PSNR value of this algorithm is 0.36dB (Set 5 times 2), 0.15dB (Set 14 times 2), 0.63dB (Set 5 times 4), 0.57dB (Set 14 times 4) higher than SRGAN algorithm.  © 2022 IEEE.","Image enhancement; Optical resolving power; Image super resolutions; Loss functions; MBRS residual block; MR-SRGAN; MRB module; Receptive fields; Super resolution algorithms; Superresolution; Superresolution methods; Training process; Generative adversarial networks","MBRS residual block; MR-SRGAN; MRB module; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85136332705"
"Mu J.; Liu Z.; Han F.; Zhou Y.; Li S.","Mu, Jinzhen (57219133143); Liu, Zongming (55925248600); Han, Fei (57683284900); Zhou, Yan (57357707800); Li, Shuang (56288781000)","57219133143; 55925248600; 57683284900; 57357707800; 56288781000","Long-range relative pose estimation and optimization of a failure satellite; [失效卫星远距离相对位姿估计与优化方法]","2021","Hangkong Xuebao/Acta Aeronautica et Astronautica Sinica","42","11","524959","","","","10.7527/S1000-6893.2021.24959","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120304772&doi=10.7527%2fS1000-6893.2021.24959&partnerID=40&md5=959c9e05c5951ce0d4b7b207fdb1c1aa","To improve the accuracy of pose estimation of long-range non-cooperative targets with slow rotation, a method for relative pose estimation is proposed based on fusion of image super-resolution and visual Simultaneous Localization And Mapping (SLAM). The method mainly includes three steps. First, a gradient guidance Generative Adversarial Network (GAN)-based super-resolution model is utilized to improve the quality of images, so as to obtain more and higher quality feature points. Second, a feature database is constructed to match the current frame with the feature database, so as to improve tracking stability of the rotating target. Thirdly, pose graph optimization is carried out in multiple frames to optimize the joint pose, so as to eliminate the cumulative error and obtain more accurate estimation results. To stablize the training of GAN, an evolutionary algorithm is introduced. To enhance the generalization and robustness of the model, the dataset is obtained by semi-physical simulation. Experimental results show that when the imaging distance is equivalent to 25 m and the target is rotating at 25 (°)/s, our algorithm can realize continuous stable measurement after the images are enhanced by the super-resolution model. © 2021, Beihang University Aerospace Knowledge Press. All right reserved.","Image enhancement; Mapping; Optical resolving power; Robotics; Failure rotating satellite; Generative adversarial network; Image super resolutions; Non-cooperative target; Pose-estimation; Relative pose; Relative pose estimation; Simultaneous localization and mapping; Super-resolution models; Generative adversarial networks","Failure rotating satellite; Generative adversarial network (GAN); Image super-resolution; Non-cooperative target; Relative pose estimation; Simultaneous localization and mapping (SLAM)","Article","Final","","Scopus","2-s2.0-85120304772"
"Bianco V.; Priscoli M.D.; Pirone D.; Zanfardino G.; Memmolo P.; Bardozzo F.; Miccio L.; Ciaparrone G.; Ferraro P.; Tagliaferri R.","Bianco, Vittorio (55389283400); Priscoli, Mattia Delli (57219554959); Pirone, Daniele (57221084933); Zanfardino, Gennaro (57226661365); Memmolo, Pasquale (24478818300); Bardozzo, Francesco (57015392700); Miccio, Lisa (15835343900); Ciaparrone, Gioele (57205023704); Ferraro, Pietro (57225688012); Tagliaferri, Roberto (7004695643)","55389283400; 57219554959; 57221084933; 57226661365; 24478818300; 57015392700; 15835343900; 57205023704; 57225688012; 7004695643","Deep Learning-Based, Misalignment Resilient, Real-Time Fourier Ptychographic Microscopy Reconstruction of Biological Tissue Slides","2022","IEEE Journal of Selected Topics in Quantum Electronics","28","4","6800110","","","","10.1109/JSTQE.2022.3154236","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125347043&doi=10.1109%2fJSTQE.2022.3154236&partnerID=40&md5=a3da4b0533fea01524bd1a075c0e5538","Fourier ptychographic microscopy probes label-free samples from multiple angles and achieves super resolution phase-contrast imaging according to a synthetic aperture principle. Thus, it is particularly suitable for high-resolution imaging of tissue slides over a wide field of view. Recently, in order to make the optical setup robust against misalignments-induced artefacts, numerical multi-look has been added to the conventional phase retrieval process, thus allowing the elimination of related phase errors but at the cost of a long computational time. Here we train a generative adversarial network to emulate the process of complex amplitude estimation. Once trained, the network can accurately reconstruct in real-time Fourier ptychographic images acquired using a severely misaligned setup. We benchmarked the network by reconstructing images of animal neural tissue slides. Above all, we show that important morphometric information, relevant for diagnosis on neural tissues, are retrieved using the network output. These are in very good agreement with the parameters calculated from the ground-truth, thus speeding up significantly the quantitative phase-contrast analysis of tissue samples. © 1995-2012 IEEE.","Alignment; Deep learning; Generative adversarial networks; Image enhancement; Light emitting diodes; Medical imaging; Synthetic apertures; Tissue; Deep learning; Fourier; Fourier ptychographic microscopy; Generator; Images reconstruction; Lightemitting diode; Neural tissue; Optical imaging; Phase imaging; Real- time; Image reconstruction","deep learning; Fourier ptychographic microscopy; generative adversarial networks; phase imaging","Article","Final","","Scopus","2-s2.0-85125347043"
"Sun H.; Lu Z.; Fan R.; Xiong W.; Xie K.; Ni X.; Yang J.","Sun, Hongfei (57195072153); Lu, Zhengda (57219032033); Fan, Rongbo (57213263950); Xiong, Wenjun (57213268953); Xie, Kai (57202255067); Ni, Xinye (52063553400); Yang, Jianhua (57188667674)","57195072153; 57219032033; 57213263950; 57213268953; 57202255067; 52063553400; 57188667674","Research on obtaining pseudo ct images based on stacked generative adversarial network","2021","Quantitative Imaging in Medicine and Surgery","11","5","","1983","2000","17","10.21037/qims-20-1019","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102470509&doi=10.21037%2fqims-20-1019&partnerID=40&md5=7c7b94f05c3a0e48f68a6fe5b4242e14","Background: To investigate the feasibility of using a stacked generative adversarial network (sGAN) to synthesize pseudo computed tomography (CT) images based on ultrasound (US) images. Methods: The pre-radiotherapy US and CT images of 75 patients with cervical cancer were selected for the training set of pseudo-image synthesis. In the first stage, labeled US images were used as the first conditional GAN input to obtain low-resolution pseudo CT images, and in the second stage, a super-resolution reconstruction GAN was used. The pseudo CT image obtained in the first stage was used as an input, following which a high-resolution pseudo CT image with clear texture and accurate grayscale information was obtained. Five cross validation tests were performed to verify our model. The mean absolute error (MAE) was used to compare each pseudo CT with the same patient's real CT image. Also, another 10 cases of patients with cervical cancer, before radiotherapy, were selected for testing, and the pseudo CT image obtained using the neural style transfer (NSF) and CycleGAN methods were compared with that obtained using the sGAN method proposed in this study. Finally, the dosimetric accuracy of pseudo CT images was verified by phantom experiments. Results: The MAE metric values between the pseudo CT obtained based on sGAN, and the real CT in five-fold cross validation are 66.82±1.59 HU, 66.36±1.85 HU, 67.26±2.37 HU, 66.34±1.75 HU, and 67.22±1.30 HU, respectively. The results of the metrics, namely, normalized mutual information (NMI), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR), between the pseudo CT images obtained using the sGAN method and the ground truth CT (CTgt) images were compared with those of the other two methods via the paired t-test, and the differences were statistically significant. The dice similarity coefficient (DSC) measurement results showed that the pseudo CT images obtained using the sGAN method were more similar to the CTgt images of organs at risk. The dosimetric phantom experiments also showed that the dose distribution between the pseudo CT images synthesized by the new method was similar to that of the CTgt images. Conclusions: Compared with NSF and CycleGAN methods, the sGAN method can obtain more accurate pseudo CT images, thereby providing a new method for image guidance in radiotherapy for cervical cancer. © 2021 AME Publishing Company. All rights reserved.","adult; Article; controlled study; cross validation; dosimetry; feasibility study; female; human; image processing; major clinical study; radiation dose distribution; signal noise ratio; stacked generative adversarial network; uterine cervix cancer; x-ray computed tomography","Generative adversarial networks (gans) submitted aug 30, 2020. accepted for publication nov 30, 2020; Image synthesis; Pseudo computed tomography (ct)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102470509"
"Khoo J.J.D.; Lim K.H.; Nistah N.N.M.; Basuki T.A.","Khoo, John Julius Danker (57221834507); Lim, King Hann (25031784300); Nistah, Nong Nurnie Mohd (57211211943); Basuki, Thomas Anung (25932001800)","57221834507; 25031784300; 57211211943; 25932001800","Deep Learning Mobile App Based Microscopic Leaf Imaging Disease Classification with Azure Cloud Computing Service","2021","2021 International Conference on Green Energy, Computing and Sustainable Technology, GECOST 2021","","","","","","","10.1109/GECOST52368.2021.9538689","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116268747&doi=10.1109%2fGECOST52368.2021.9538689&partnerID=40&md5=ec6cf4fd3d63987ce81b96bef36cde4a","The growth of rubber trees always suffer from blight disease that can be detected on the leaves. Several studies show that an early detection of blight disease may contribute to a positive turnout recovery rate. In this paper, a mobile app-based microscopic leaf imaging disease classification is proposed to detect infected leaves that provides a probability of disease types, geo-tagging location, and smart reporting with recovery stage to assist productivity workflow. Super Resolution Generative Adversarial Network is applied to upscale a low resolution microscopic leaf imaging while introducing finer texture detail. After super resolution reconstruction, a convolution neural network classifier is performed to classify disease groups with an improved accuracy. The diagnostic solution is designed on the Azure cloud computing service to manage the plant disease database, perform reinforcement learning, host web application, secure authentication and display valuable insight of recovery. © 2021 IEEE.","Cloud computing; Computer system recovery; Diagnosis; Plants (botany); Recovery; Reinforcement learning; Rubber; Textures; Cloud computing services; Cloud-computing; Deep learning; Disease classification; MicroSoft; Microsoft azure; Mobile app; Recovery rate; Rubber tree; Superresolution; Deep learning","Agriculture; Cloud Computing; Deep Learning; Microsoft Azure; Rubber Plantation; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85116268747"
"Jiang M.; Zhi M.; Li Y.; Li T.; Zhang J.","Jiang, Mingfeng (16175216000); Zhi, Minghao (57219910908); Li, Yang (56052616600); Li, Tieqiang (7406372587); Zhang, Jucheng (55581543800)","16175216000; 57219910908; 56052616600; 7406372587; 55581543800","Super-resolution reconstruction of MR image with self-attention based generate adversarial network algorithm; [基于自注意力机制生成对抗网络的超分辨率磁共振图像重建]","2021","Scientia Sinica Informationis","51","6","","959","970","11","10.1360/SSI-2020-0100","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107313321&doi=10.1360%2fSSI-2020-0100&partnerID=40&md5=15e6835aee8df135f2702d7b5455afff","Super-resolution (SR) MRI images can provide fine-grained anatomical information, however it takes a long time to acquire data. In order to accelerate the acquisition of MR images while maintaining high-quality images, extensive research has been performed on image reconstruction through the deep learning method. In this study, a reconstruction framework by using self-attention based super-resolution generative adversarial networks (SA-SR-GAN) is proposed to generate super resolution MR image from low resolution MR image. Moreover, the self-attention mechanism is integrated into super-resolution generative adversarial networks (SR-GAN) framework, which is used to calculate the weight parameters of the input features. At the same time, spectral normalization is added to make the discriminator network training process more stable. The network was trained with 40 3D images (each 3D image contains 256 slices) and tested with 10 images. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed SA-SR-GAN method are higher than the state-of-the-art reconstruction methods. © 2021, Science China Press. All right reserved.","","Generative adversarial network; MR image; Self-attention; Spectral normalization; Super resolution","Article","Final","","Scopus","2-s2.0-85107313321"
"Uddin M.S.; Hoque R.; Islam K.A.; Kwan C.; Gribben D.; Li J.","Uddin, Mohammad Shahab (57218626198); Hoque, Reshad (57215344852); Islam, Kazi Aminul (57201212562); Kwan, Chiman (7201421216); Gribben, David (57205406613); Li, Jiang (56226550100)","57218626198; 57215344852; 57201212562; 7201421216; 57205406613; 56226550100","Converting optical videos to infrared videos using attention gan and its impact on target detection and classification performance","2021","Remote Sensing","13","16","3257","","","","10.3390/rs13163257","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113372575&doi=10.3390%2frs13163257&partnerID=40&md5=ea6a778280a3590d5ec0b9a2ee988332","To apply powerful deep-learning-based algorithms for object detection and classification in infrared videos, it is necessary to have more training data in order to build high-performance models. However, in many surveillance applications, one can have a lot more optical videos than infrared videos. This lack of IR video datasets can be mitigated if optical-to-infrared video conversion is possible. In this paper, we present a new approach for converting optical videos to infrared videos using deep learning. The basic idea is to focus on target areas using attention generative adversarial network (attention GAN), which will preserve the fidelity of target areas. The approach does not require paired images. The performance of the proposed attention GAN has been demonstrated using objective and subjective evaluations. Most importantly, the impact of attention GAN has been demonstrated in improved target detection and classification performance using real-infrared videos. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Classification (of information); Deep learning; Learning algorithms; Object detection; Object recognition; Security systems; Adversarial networks; Learning-based algorithms; New approaches; Objective and subjective evaluations; Performance Model; Surveillance applications; Target detection and classifications; Video datasets; Video signal processing","Attention GAN; Deep learning; Image conversion; Mid-wave infrared (MWIR) videos; ResNet; Target detection and classification; Video super-resolution; YOLO","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113372575"
"Tampubolon H.; Setyoko A.; Purnamasari F.","Tampubolon, Hendrik (57208859230); Setyoko, Aji (57221258159); Purnamasari, Fanindia (57211914529)","57208859230; 57221258159; 57211914529","SNPE-SRGAN: Lightweight Generative Adversarial Networks for Single-Image Super-Resolution on Mobile Using SNPE Framework","2021","Journal of Physics: Conference Series","1898","1","012038","","","","10.1088/1742-6596/1898/1/012038","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109041126&doi=10.1088%2f1742-6596%2f1898%2f1%2f012038&partnerID=40&md5=d9977a2ba0b5b9df9663285bc771260b","An image resulting from a low-resolution (LR) camera on the mobile phone has lower quality than a high-resolution(HR) camera on a DSLR. Meanwhile, the HR camera is pricing if compared with the LR camera. How to achieve a single-image quality on LR camera likewise on HR camera becomes essential research in the past years. Addressing this issue can be done by upscaling a single LR image. Recently, the super-resolution generative adversarial network (SRGAN) model is one of the state-of-the-art super-resolution(SR)models employed on single-image SR. However, implementing a deep learning model like SRGAN on a mobile device is challenging in computation power and resources. This study aims to develop a smaller and lower resources model while preserving single-image SR quality on mobile devices. To meet these objectives, we convert, quantize, and compress the SRGAN model on Snapdragon Neural Processing Engine (SNPE) as an example. We then validate the SRGAN on the DIV2K dataset on which improves the model performances. Besides, we conduct experiments on GPU, DSP environment. The experimental result confirmed that SNPE-SRGAN capable of achieves not only HR images' quality but also low latency by 0.06 second and smaller model by 1.7 Mb size running on DSP. Also, the SRGAN-DLC-Quantized running on GPU has a smaller size by 1.7 Mb and lower latency by 1.151 seconds compared with Non-quantized SRGAN-TensorFlow by 9.1 Mb and 1.608 seconds latency. © Published under licence by IOP Publishing Ltd.","Deep learning; Graphics processing unit; Optical resolving power; Adversarial networks; Computation power; High resolution; Learning models; Model performance; Neural-processing; State of the art; Super resolution; Cameras","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85109041126"
"Dai M.; Xiao G.; Fiondella L.; Shao M.; Zhang Y.S.","Dai, Manna (56457485500); Xiao, Gao (57201647250); Fiondella, Lance (24766252300); Shao, Ming (55000875100); Zhang, Yu Shrike (56191420300)","56457485500; 57201647250; 24766252300; 55000875100; 56191420300","Deep learning-enabled resolution-enhancement in mini- and regular microscopy for biomedical imaging","2021","Sensors and Actuators, A: Physical","331","","112928","","","","10.1016/j.sna.2021.112928","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111256738&doi=10.1016%2fj.sna.2021.112928&partnerID=40&md5=f4e632cba0aeb42e83851f75342a423d","Artificial intelligence algorithms that aid mini-microscope imaging are attractive for numerous applications. In this paper, we optimize artificial intelligence techniques to provide clear, and natural biomedical imaging. We demonstrate that a deep learning-enabled super-resolution method can significantly enhance the spatial resolution of mini-microscopy and regular-microscopy. This data-driven approach trains a generative adversarial network to transform low-resolution images into super-resolved ones. Mini-microscopic images and regular-microscopic images acquired with different optical microscopes under various magnifications are collected as our experimental benchmark datasets. The only input to this generative-adversarial-network-based method are images from the datasets down-sampled by the Bicubic interpolation. We use independent test sets to evaluate this deep learning approach with other deep learning-based algorithms through qualitative and quantitative comparisons. To clearly present the improvements achieved by this generative-adversarial-network-based method, we zoom into the local features to explore and highlight the qualitative differences. We also employ the peak signal-to-noise ratio and the structural similarity, to quantitatively compare alternative super-resolution methods. The quantitative results illustrate that super-resolution images obtained from our approach with interpolation parameter α = 0.25 more closely match those of the original high-resolution images than to those obtained by any of the alternative state-of-the-art method. These results are significant for fields that use microscopy tools, such as biomedical imaging of engineered living systems. We also utilize this generative adversarial network-based algorithm to optimize the resolution of biomedical specimen images and then generate three-dimensional reconstruction, so as to enhance the ability of three-dimensional imaging throughout the entire volumes for spatial-temporal analyses of specimen structures. © 2021 Elsevier B.V.","Deep learning; Image enhancement; Interpolation; Medical imaging; Signal to noise ratio; Adversarial networks; Biomedical imaging; Biomedicine; Deep learning; Microscopic image; Mini-microscopy; Network-based; Optical imaging; Resolution enhancement; Superresolution methods; Optical resolving power","Artificial intelligence; Biomedicine; Deep learning; Mini-microscopy; Optical imaging","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111256738"
"Clabaut É.; Lemelin M.; Germain M.; Bouroubi Y.; St-Pierre T.","Clabaut, Étienne (57211641688); Lemelin, Myriam (55845571900); Germain, Mickaël (7201845387); Bouroubi, Yacine (42961054400); St-Pierre, Tony (57297977400)","57211641688; 55845571900; 7201845387; 42961054400; 57297977400","Model specialization for the use of esrgan on satellite and airborne imagery","2021","Remote Sensing","13","20","4044","","","","10.3390/rs13204044","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117227601&doi=10.3390%2frs13204044&partnerID=40&md5=0490fea79fd11b226c9e096e6bf7063e","Training a deep learning model requires highly variable data to permit reasonable generalization. If the variability in the data about to be processed is low, the interest in obtaining this generalization seems limited. Yet, it could prove interesting to specialize the model with respect to a particular theme. The use of enhanced super-resolution generative adversarial networks (ERSGAN), a specific type of deep learning architecture, allows the spatial resolution of remote sensing images to be increased by “hallucinating” non-existent details. In this study, we show that ESRGAN create better quality images when trained on thematically classified images than when trained on a wide variety of examples. All things being equal, we further show that the algorithm performs better on some themes than it does on others. Texture analysis shows that these performances are correlated with the inverse difference moment and entropy of the images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image enhancement; Optical resolving power; Remote sensing; Satellite imagery; Textures; Airborne imagery; ESRGAN; Generalisation; Haralick; Learning architectures; Learning models; Spatial resolution; Specialisation; Superresolution; Variable data; Generative adversarial networks","ESRGAN; Generative adversarial networks; Haralick; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117227601"
"Li Y.; Zhao H.; Li B.; Wang Y.","Li, Yunhe (55647591200); Zhao, Huiyan (57433791500); Li, Bo (57192830530); Wang, Yi (57433212300)","55647591200; 57433791500; 57192830530; 57433212300","X4 Super-Resolution Analysis of Magnetic Resonance Imaging based on Generative Adversarial Network without Supervised Images","2021","ACM International Conference Proceeding Series","","","","","","","10.1145/3503047.3503064","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123780267&doi=10.1145%2f3503047.3503064&partnerID=40&md5=15b320ca6f0cd9ff6f63766f914dd76a","Magnetic resonance imaging (MRI) is widely used in clinical medical auxiliary diagnosis. In acquiring images by MRI machines, patients usually need to be exposed to harmful radiation. The radiation dose can be reduced by reducing the resolution of MRI images. This paper analyzes the super-resolution of low-resolution MRI images based on a deep learning algorithm to ensure the pixel quality of the MRI image required for medical diagnosis. It then reconstructs high-resolution MRI images as an alternative method to reduce radiation dose. This paper studies how to improve the resolution of low-dose MRI by 4 times through super-resolution analysis based on deep learning technology without other available information. This paper constructs a data set close to the natural low-high resolution image pair through degenerate kernel estimation and noise injection and constructs a two-layer generated countermeasure network based on the design ideas of ESRGAN, PatchGAN, and VGG-19. The test shows that our method is better than EDSR, RCAN, and ESRGAN in comparing non-reference image quality evaluation indexes. © 2021 ACM.","Deep learning; Diagnosis; Generative adversarial networks; Image analysis; Learning algorithms; Magnetic resonance imaging; Medical imaging; Network layers; Quality control; Exposed to; High resolution; Image-based; Imaging machines; Low dose; Lower resolution; Magnetic resonance imaging image; Paper analysis; Resolution analysis; Superresolution; Optical resolving power","Generative Adversarial Network; MRI Images; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85123780267"
"Zhang H.; Shinomiya Y.; Yoshida S.","Zhang, Hongtao (57223028038); Shinomiya, Yuki (56239397700); Yoshida, Shinichi (7406245025)","57223028038; 56239397700; 7406245025","3d mri reconstruction based on 2d generative adversarial network super‐resolution","2021","Sensors","21","9","2978","","","","10.3390/s21092978","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104521486&doi=10.3390%2fs21092978&partnerID=40&md5=608c452d796a878b5b1324371628da41","The diagnosis of brain pathologies usually involves imaging to analyze the condition of the brain. Magnetic resonance imaging (MRI) technology is widely used in brain disorder diagnosis. The image quality of MRI depends on the magnetostatic field strength and scanning time. Scanners with lower field strengths have the disadvantages of a low resolution and high imaging cost, and scanning takes a long time. The traditional super‐resolution reconstruction method based on MRI generally states an optimization problem in terms of prior information. It solves the problem using an iterative approach with a large time cost. Many methods based on deep learning have emerged to replace traditional methods. MRI super‐resolution technology based on deep learning can effec-tively improve MRI resolution through a three‐dimensional convolutional neural network; how-ever, the training costs are relatively high. In this paper, we propose the use of two‐dimensional super‐resolution technology for the super‐resolution reconstruction of MRI images. In the first re-construction, we choose a scale factor of 2 and simulate half the volume of MRI slices as input. We utilize a receiving field block enhanced super‐resolution generative adversarial network (RFB‐ ESRGAN), which is superior to other super‐resolution technologies in terms of texture and frequency information. We then rebuild the super‐resolution reconstructed slices in the MRI. In the second reconstruction, the image after the first reconstruction is composed of only half of the slices, and there are still missing values. In our previous work, we adopted the traditional interpolation method, and there was still a gap in the visual effect of the reconstructed images. Therefore, we propose a noise‐based super‐resolution network (nESRGAN). The noise addition to the network can provide additional texture restoration possibilities. We use nESRGAN to further restore MRI resolution and high‐frequency information. Finally, we achieve the 3D reconstruction of brain MRI images through two super‐resolution reconstructions. Our proposed method is superior to 3D su-per‐resolution technology based on deep learning in terms of perception range and image quality evaluation standards. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Brain; Magnetic Resonance Imaging; Neural Networks, Computer; Convolutional neural networks; Deep learning; Image quality; Iterative methods; Learning systems; Magnetic resonance imaging; Magnetostatics; Restoration; Textures; Adversarial networks; Frequency information; Image quality evaluation; Interpolation method; Optimization problems; Reconstructed image; Reconstruction method; Texture restorations; brain; diagnostic imaging; nuclear magnetic resonance imaging; Image reconstruction","Deep learning; Magnetic resonance imaging; NESRGAN; RFB‐ESRGAN; Super‐resolution; Three‐dimensional recon-struction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85104521486"
"Ge W.; Wang Z.; Wang G.; Tan S.; Zhang J.","Ge, Wenyi (55782079500); Wang, Zhitao (57222554407); Wang, Guigui (57190274143); Tan, Shihan (57190282499); Zhang, Jianwei (57195437554)","55782079500; 57222554407; 57190274143; 57190282499; 57195437554","Remote sensing image super-resolution for the visual system of a flight simulator: Dataset and baseline","2021","Aerospace","8","3","76","","","","10.3390/aerospace8030076","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103208331&doi=10.3390%2faerospace8030076&partnerID=40&md5=6f11f15ea40c56b725a7a9cabd7b53fb","High-resolution remote sensing images are the key data source for the visual system of a flight simulator for training a qualified pilot. However, due to hardware limitations, it is an expensive task to collect spectral and spatial images at very high resolutions. In this work, we try to tackle this issue with another perspective based on image super-resolution (SR) technology. First, we present a new ultra-high-resolution remote sensing image dataset named Airport80, which is captured from the airspace near various airports. Second, a deep learning baseline is proposed by applying the generative and adversarial mechanism, which is able to reconstruct a high-resolution image during a single image super-resolution. Experimental results for our benchmark demonstrate the effectiveness of the proposed network and show it has reached satisfactory performances. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Flight simulator; Generative adversarial network; Remote sensing image; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103208331"
"Liu R.; Yan G.; He H.; An Y.; Wang T.; Huang X.","Liu, Runze (57429453100); Yan, Guangwei (24170361000); He, Hui (56303064100); An, Yubin (57429277300); Wang, Ting (57768879200); Huang, Xile (57361123400)","57429453100; 24170361000; 56303064100; 57429277300; 57768879200; 57361123400","Small Targets Detection for Transmission Tower Based on SRGAN and Faster RCNN","2021","Recent Advances in Electrical and Electronic Engineering","14","8","","812","825","13","10.2174/2352096514666211026143543","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123615673&doi=10.2174%2f2352096514666211026143543&partnerID=40&md5=94f0c71af35b6b64e8ce6441b3e85e62","Background: Power line inspection is essential to ensure the safe and stable operation of the power system. Object detection for tower equipment can significantly improve inspection efficiency. However, due to the low resolution of small targets and limited features, the detection accuracy of small targets is not easy to improve. Objective: This study aimed to improve the tiny targets’ resolution while making the small target's texture and detailed features more prominent to be perceived by the detection model. Methods: In this paper, we propose an algorithm that employs generative adversarial networks to improve small objects' detection accuracy. First, the original image is converted into a super-resolution one by a super-resolution reconstruction network (SRGAN). Then the object detection framework Faster RCNN is utilized to detect objects on the super-resolution images. Results: The experimental results on two small object recognition datasets show that the model proposed in this paper has good robustness. It can especially detect the targets missed by Faster RCNN, which indicates that SRGAN can effectively enhance the detailed information of small targets by improving the resolution. Conclusion: We found that higher resolution data is conducive to obtaining more detailed information of small targets, which can help the detection algorithm achieve higher accuracy. The small object detection model based on the generative adversarial network proposed in this paper is feasible and more efficient. Compared with Faster RCNN, this model has better performance on small object detection. © 2021 Bentham Science Publishers.","Electric power transmission; Feature extraction; Generative adversarial networks; Image segmentation; Object detection; Object recognition; Optical resolving power; Textures; Fast RCNN; Generative adversarial network; Network (RPN); Reconstruction networks; Region of interest; Region proposal; Region-of-interest; Regions of interest; Small object detection; SRGAN; Super-resolution reconstruction; Super-resolution reconstruction network; Computer vision","Computer vision; Faster RCNN; Generative adversarial network (GAN); Network (RPN); Region of interest (ROI); Region proposal; Small object detection; SRGAN; Super-resolution reconstruction network","Article","Final","","Scopus","2-s2.0-85123615673"
"Guo X.; Sang X.; Chen D.; Wang P.; Wang H.; Liu X.; Li Y.; Xing S.; Yan B.","Guo, Xiao (57205526174); Sang, Xinzhu (55148933600); Chen, Duo (55850780300); Wang, Peng (56106381100); Wang, Huachun (57196434999); Liu, Xue (57223272194); Li, Yuanhang (57201424152); Xing, Shujun (55850656300); Yan, Binbin (56975784800)","57205526174; 55148933600; 55850780300; 56106381100; 57196434999; 57223272194; 57201424152; 55850656300; 56975784800","Real-time optical reconstruction for a three-dimensional light-field display based on path-tracing and CNN super-resolution","2021","Optics Express","29","23","","37862","37876","14","10.1364/OE.441714","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118529052&doi=10.1364%2fOE.441714&partnerID=40&md5=7df1236d087a728c22b60edd11f39021","Three-Dimensional (3D) light-field display plays a vital role in realizing 3D display. However, the real-time high quality 3D light-field display is difficult, because super high-resolution 3D light field images are hard to be achieved in real-time. Although extensive research has been carried out on fast 3D light-field image generation, no single study exists to satisfy real-time 3D image generation and display with super high-resolution such as 7680×4320. To fulfill real-time 3D light-field display with super high-resolution, a two-stage 3D image generation method based on path tracing and image super-resolution (SR) is proposed, which takes less time to render 3D images than previous methods. In the first stage, path tracing is used to generate low-resolution 3D images with sparse views based on Monte-Carlo integration. In the second stage, a lite SR algorithm based on a generative adversarial network (GAN) is presented to up-sample the low-resolution 3D images to high-resolution 3D images of dense views with photo-realistic image quality. To implement the second stage efficiently and effectively, the elemental images (EIs) are super-resolved individually for better image quality and geometry accuracy, and a foreground selection scheme based on ray casting is developed to improve the rendering performance. Finally, the output EIs from CNN are used to recompose the high-resolution 3D images. Experimental results demonstrate that real-time 3D light-field display over 30fps at 8K resolution can be realized, while the structural similarity (SSIM) can be over 0.90. It is hoped that the proposed method will contribute to the field of real-time 3D light-field display. © 2021 Optical Society of America under the terms of the OSA Open Access Publishing Agreement","Field emission displays; Generative adversarial networks; Image enhancement; Image reconstruction; Image understanding; Optical resolving power; Rendering (computer graphics); Three dimensional computer graphics; Three dimensional displays; 3-D image; 3D image generation; 3D-images; Field images; High resolution; Light field displays; Light fields; Lower resolution; Path tracing; Real- time; article; diagnostic test accuracy study; geometry; image quality; Image quality","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118529052"
"Harris M.W.; Semwal S.K.","Harris, Mark Wesley (57433249200); Semwal, Sudhanshu Kumar (6603768549)","57433249200; 6603768549","A Multi-Stage Advanced Deep Learning Graphics Pipeline","2021","Proceedings - SIGGRAPH Asia 2021 Technical Communications, SA 2021","","","7","","","","10.1145/3478512.3488609","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123797257&doi=10.1145%2f3478512.3488609&partnerID=40&md5=021da858083aaaf9f4e5ba2c55d38ae1","In this paper we propose the Advanced Deep Learning Graphics Pipeline (ADLGP). ADLGP is a novel approach that uses existing deep learning architectures to convert scene data into rendered images. Our goal of generating frames from semantic data has produced successful renderings with similar structures and composition as target frames. We demonstrate the success of ADLGP with side-by-side comparisons of frames generated through standard rendering procedures. We assert that a fully implemented ADLGP framework would reduce the time spent in visualizing 3D environments, and help selectively offload the requirements of the current graphics rendering pipeline.  © 2021 Owner/Author.","Data handling; Deep learning; Image processing; Interactive computer graphics; Pipeline processing systems; Pipelines; Rendering (computer graphics); Semantics; Three dimensional computer graphics; Graphics pipeline; Images processing; Learning architectures; Multi-stages; Rendering pipelines; Rendering technology; Semantic data; Semantic data processing; Superresolution; Text-to-image; Generative adversarial networks","generative adversarial networks; Graphics pipeline; image processing; machine learning; rendering pipeline; rendering technologies; semantic data processing; super resolution; text-to-image","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85123797257"
"Zha L.; Shi Y.; Wen J.","Zha, Lei (57223856433); Shi, Yangjing (57218326448); Wen, Juan (57202574050)","57223856433; 57218326448; 57202574050","A Lightweight Image Super-Resolution Network Based on ESRGAN for Rapid Tomato Leaf Disease Classification","2022","Lecture Notes in Electrical Engineering","813","","","97","110","13","10.1007/978-981-16-6963-7_9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126371477&doi=10.1007%2f978-981-16-6963-7_9&partnerID=40&md5=1233422abbd35dcd34c072ab62025ffc","The Crop disease identification is an important task in intelligent agriculture. Image resolutions have a large impact on the overall accuracy of classification performance. Some crop diseases are so similar that low-resolution images cannot capture their differences. To reduce the losses caused by crop diseases, it is vital to study crop image super-resolution reconstruction. Recently, with the rapid development of deep learning, various Single Image Super-Resolution (SISR) methods based on convolutional neural network (CNN) have achieved remarkable performance. However, the existing SR networks mainly have large parameter sizes, which require numerous training images and computing resources. In this paper, a lightweight image super-resolution model is constructed and applied to tomato leaf disease identification. We introduce the Shuffle Blocks with an attention mechanism to replace the Residual in Residual Dense Blocks (RRDBs) in the Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN), which is an effective SR model and won first place in the PIRM2018-SR Challenge. By the special structure we designed, our model can significantly reduce the parameter size of ESRGAN while almost maintaining its performance. Besides, we employ the tomato leaf images from the Plant Village dataset to train and test our model. Finally, we use the VGG16 to classify tomato leaf diseases based on the reconstructed images. The experiment results show that our model can effectively reduce the parameter size, computational complexity, and image reconstruction time compared to other chosen SR networks. Furthermore, the accuracy of SR images generated by our model is closer to ESRGAN and higher than other state-of-the-art methods. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Convolutional neural networks; Crops; Deep learning; Fruits; Generative adversarial networks; Image classification; Image reconstruction; Image resolution; Statistical tests; Disease classification; Enhanced super-resolution generative adversarial network; Image super resolutions; Plant village; Single image super-resolution; Single images; Superresolution; Tomato disease; Tomato disease classification; VGG16; Rural areas","ESRGAN; Plant village; SISR; Tomato disease classification; VGG16","Conference paper","Final","","Scopus","2-s2.0-85126371477"
"Park S.-W.; Ko J.-S.; Huh J.-H.; Kim J.-C.","Park, Sung-Wook (57205079478); Ko, Jae-Sub (35253916100); Huh, Jun-Ho (56438784800); Kim, Jong-Chan (49963957400)","57205079478; 35253916100; 56438784800; 49963957400","Review on generative adversarial networks: Focusing on computer vision and its applications","2021","Electronics (Switzerland)","10","10","1216","","","","10.3390/electronics10101216","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106180519&doi=10.3390%2felectronics10101216&partnerID=40&md5=8d5ca3347494a23908b9b448ce19d112","The emergence of deep learning model GAN (Generative Adversarial Networks) is an important turning point in generative modeling. GAN is more powerful in feature and expression learning compared to machine learning-based generative model algorithms. Nowadays, it is also used to generate non-image data, such as voice and natural language. Typical technologies include BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pretrained Transformer-3), and MuseNet. GAN differs from the machine learning-based generative model and the objective function. Training is conducted by two networks: generator and discriminator. The generator converts random noise into a true-to-life image, whereas the discriminator distinguishes whether the input image is real or synthetic. As the training continues, the generator learns more sophisticated synthesis techniques, and the discriminator grows into a more accurate differentiator. GAN has problems, such as mode collapse, training instability, and lack of evaluation matrix, and many researchers have tried to solve these problems. For example, solutions such as one-sided label smoothing, instance normalization, and minibatch discrimination have been proposed. The field of application has also expanded. This paper provides an overview of GAN and application solutions for computer vision and artificial intelligence healthcare field researchers. The structure and principle of operation of GAN, the core models of GAN proposed to date, and the theory of GAN were analyzed. Application examples of GAN such as image classification and regression, image synthesis and inpainting, image-to-image translation, super-resolution and point registration were then presented. The discussion tackled GAN’s problems and solutions, and the future research direction was finally proposed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Artificial intelligence healthcare; Computer vision; Deep learning; Generative adversarial networks","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106180519"
"Imanuel I.; Lee S.","Imanuel, I. (57408814600); Lee, S. (55716387600)","57408814600; 55716387600","Super-resolution with adversarial loss on the feature maps of the generated high-resolution image","2022","Electronics Letters","58","2","","47","49","2","10.1049/ell2.12360","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122694073&doi=10.1049%2fell2.12360&partnerID=40&md5=a7d14fe2ca43302d6d60c5b9a8f07148","Recent studies on image super-resolution make use of Generative Adversarial Networks to generate the high-resolution image counterpart of the low-resolution input. However, while being able to generate sharp high-resolution images, Generative Adversarial Networks based super-resolution methods often fail to produce good results when tested on images having different degradation as the low-resolution images used in the training. Some recent works have tried to mitigate this failure by introducing a degradation network that can replicate the noise of real-world low-resolution images. However, even these methods can produce poor results if a real-world test image differs much from the real-world images in the training data set. This paper proposes the use of adversarial losses on the feature maps extracted by a pre-trained network with the generated high-resolution image as input. This is in contrast to all other Generative Adversarial Networks-based super-resolution methods that directly apply the adversarial loss to the generated high-resolution image. The rationale behind this idea is illustrated, and experimental results confirm that high-resolution images generated by the proposed method achieve better results in both quantitative and qualitative evaluations than methods that directly apply adversarial losses to generated high-resolution images. © 2021 The Authors. Electronics Letters published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Optical resolving power; Statistical tests; Feature map; High-resolution images; Image super resolutions; Low resolution images; Lower resolution; Network-based; Real-world; Real-world tests; Superresolution; Superresolution methods; Generative adversarial networks","","Letter","Final","","Scopus","2-s2.0-85122694073"
"Zhou W.; Wang Z.; Chen Z.","Zhou, Wei (57190390124); Wang, Zhou (55880038400); Chen, Zhibo (56099149800)","57190390124; 55880038400; 56099149800","Image super-resolution quality assessment: Structural fidelity versus statistical naturalness","2021","2021 13th International Conference on Quality of Multimedia Experience, QoMEX 2021","","","9465479","61","64","3","10.1109/QoMEX51781.2021.9465479","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113881915&doi=10.1109%2fQoMEX51781.2021.9465479&partnerID=40&md5=10e6da989400c6dc29de9f428fae08d4","Single image super-resolution (SISR) algorithms reconstruct high-resolution (HR) images with their low-resolution (LR) counterparts. It is desirable to develop image quality assessment (IQA) methods that can not only evaluate and compare SISR algorithms, but also guide their future development. In this paper, we assess the quality of SISR generated images in a two-dimensional (2D) space of structural fidelity versus statistical naturalness. This allows us to observe the behaviors of different SISR algorithms as a tradeoff in the 2D space. Specifically, SISR methods are traditionally designed to achieve high structural fidelity but often sacrifice statistical naturalness, while recent generative adversarial network (GAN) based algorithms tend to create more natural-looking results but lose significantly on structural fidelity. Furthermore, such a 2D evaluation can be easily fused to a scalar quality prediction. Interestingly, we find that a simple linear combination of a straightforward local structural fidelity and a global statistical naturalness measures produce surprisingly accurate predictions of SISR image quality when tested using public subject-rated SISR image datasets. Code of the proposed SFSN model is publicly available at https://github.con/weizhou-geek/SFSN. © 2021 IEEE.","Multimedia systems; Optical resolving power; Quality control; Accurate prediction; Adversarial networks; High resolution image; Image quality assessment (IQA); Image super resolutions; Linear combinations; Quality prediction; Two Dimensional (2 D); Image quality","image decomposition; image super-resolution; quality assessment; statistical naturalness; structural fidelity","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85113881915"
"Küstner T.; Munoz C.; Psenicny A.; Bustin A.; Fuin N.; Qi H.; Neji R.; Kunze K.; Hajhosseiny R.; Prieto C.; Botnar R.","Küstner, Thomas (56549927000); Munoz, Camila (57109884400); Psenicny, Alina (57225170576); Bustin, Aurelien (56610187700); Fuin, Niccolo (42961330700); Qi, Haikun (57188573322); Neji, Radhouene (57194071727); Kunze, Karl (56581386200); Hajhosseiny, Reza (55303798700); Prieto, Claudia (35337695700); Botnar, René (35602500200)","56549927000; 57109884400; 57225170576; 56610187700; 42961330700; 57188573322; 57194071727; 56581386200; 55303798700; 35337695700; 35602500200","Deep-learning based super-resolution for 3D isotropic coronary MR angiography in less than a minute","2021","Magnetic Resonance in Medicine","86","5","","2837","2852","15","10.1002/mrm.28911","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109373143&doi=10.1002%2fmrm.28911&partnerID=40&md5=82d30a7d93ddc2f2ef4b6df83a548d63","Purpose: To develop and evaluate a novel and generalizable super-resolution (SR) deep-learning framework for motion-compensated isotropic 3D coronary MR angiography (CMRA), which allows free-breathing acquisitions in less than a minute. Methods: Undersampled motion-corrected reconstructions have enabled free-breathing isotropic 3D CMRA in ~5-10 min acquisition times. In this work, we propose a deep-learning–based SR framework, combined with non-rigid respiratory motion compensation, to shorten the acquisition time to less than 1 min. A generative adversarial network (GAN) is proposed consisting of two cascaded Enhanced Deep Residual Network generator, a trainable discriminator, and a perceptual loss network. A 16-fold increase in spatial resolution is achieved by reconstructing a high-resolution (HR) isotropic CMRA (0.9 mm3 or 1.2 mm3) from a low-resolution (LR) anisotropic CMRA (0.9 × 3.6 × 3.6 mm3 or 1.2 × 4.8 × 4.8 mm3). The impact and generalization of the proposed SRGAN approach to different input resolutions and operation on image and patch-level is investigated. SRGAN was evaluated on a retrospective downsampled cohort of 50 patients and on 16 prospective patients that were scanned with LR-CMRA in ~50 s under free-breathing. Vessel sharpness and length of the coronary arteries from the SR-CMRA is compared against the HR-CMRA. Results: SR-CMRA showed statistically significant (P <.001) improved vessel sharpness 34.1% ± 12.3% and length 41.5% ± 8.1% compared with LR-CMRA. Good generalization to input resolution and image/patch-level processing was found. SR-CMRA enabled recovery of coronary stenosis similar to HR-CMRA with comparable qualitative performance. Conclusion: The proposed SR-CMRA provides a 16-fold increase in spatial resolution with comparable image quality to HR-CMRA while reducing the predictable scan time to <1 min. © 2021 The Authors. Magnetic Resonance in Medicine published by Wiley Periodicals LLC on behalf of International Society for Magnetic Resonance in Medicine","Coronary Angiography; Coronary Vessels; Deep Learning; Heart; Humans; Imaging, Three-Dimensional; Magnetic Resonance Angiography; Prospective Studies; Retrospective Studies; Angiography; Heart; Image resolution; Motion compensation; Optical resolving power; Acquisition time; Adversarial networks; Coronary arteries; Coronary stenosis; Learning frameworks; Learning-based super-resolution; Spatial resolution; Super resolution; adult; anisotropy; artery diameter; Article; clinical article; cohort analysis; controlled study; coronary artery; coronary artery obstruction; deep learning; female; human; image quality; image reconstruction; in vivo study; information processing; magnetic resonance angiography; male; middle aged; prospective study; retrospective study; spatial analysis; three-dimensional imaging; validation study; coronary angiography; coronary blood vessel; diagnostic imaging; heart; magnetic resonance angiography; three-dimensional imaging; Deep learning","3D whole-heart; coronary MR angiography; deep learning; super resolution","Article","Final","","Scopus","2-s2.0-85109373143"
"Dong W.; Yang Y.; Qu J.; Xie W.; Li Y.","Dong, Wenqian (57196087026); Yang, Yufei (57224191949); Qu, Jiahui (57196097630); Xie, Weiying (56768656200); Li, Yunsong (55986546100)","57196087026; 57224191949; 57196097630; 56768656200; 55986546100","Fusion of Hyperspectral and Panchromatic Images Using Generative Adversarial Network and Image Segmentation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3078711","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107207029&doi=10.1109%2fTGRS.2021.3078711&partnerID=40&md5=ced5e81bab36ff953807405583b9d563","Hyperspectral (HS) image fusion aims at integrating a panchromatic (PAN) image and an HS image, featuring the fused image with the spatial quality of the former and the spectral diversity of the latter. The classic fusion algorithm generally includes three consecutive procedures that are upsampling, detail extraction, and detail injection. In this article, we propose an HS and PAN image fusion method based on generative adversarial network and local estimation of injection gain. Instead of upsampling the HS image by classical interpolation techniques, a generative adversarial super-resolution network (GASN) is designed to obtain the interpolated HS image in the fusion framework. GASN establishes a spectral-information-based discriminator to conduct adversarial learning with the generator, so as to preserve the spectral information of the low-resolution HS image. An image segmentation-based injection gain estimation (ISGE) algorithm is subsequently proposed for HS and PAN images fusion. The injection gain is estimated over image segments obtained by a binary partition tree approach to improve the fusion performance. The proposed GASN and ISGE are implemented into two credible global estimation pansharpening methods, and experimental results prove the performance improvement of the proposed method. The proposed method is also compared with existing state-of-the-art methods, and experiments on several public databases demonstrate that the proposed method is competitive or superior to the state-of-the-art fusion methods.  © 1980-2012 IEEE.","Binary trees; Image enhancement; Image segmentation; Object recognition; Signal sampling; Adversarial learning; Adversarial networks; Binary partition Tree; Image fusion methods; Interpolation techniques; Panchromatic (Pan) image; Spectral information; State-of-the-art methods; artificial neural network; detection method; image analysis; satellite imagery; segmentation; Image fusion","Details injection; hyperspectral (HS) fusion; image segment; injection gains","Article","Final","","Scopus","2-s2.0-85107207029"
"Tan M.; Xu S.; Zhang S.; Chen Q.","Tan, Mingkui (22837202600); Xu, Shoukai (57219487789); Zhang, Shuhai (57223372407); Chen, Qi (57211763557)","22837202600; 57219487789; 57223372407; 57211763557","A review on deep adversarial visual generation; [深度对抗视觉生成综述]","2021","Journal of Image and Graphics","26","12","","2751","2766","15","10.11834/jig.210252","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121325683&doi=10.11834%2fjig.210252&partnerID=40&md5=8b0e1d03705dd87086644829b7b215ed","Deep visual generation has aimed to create synthetic photo-realistic visual contents (such as images and videos) that could fool or please human perceptions according to some specific requirements. In fact, many human activities belong to the field of visual generation, e.g., advertisement making, house designing and film making. However, these tasks normally can only be done by experts with professional skills gained through long-term training and the help of professional software such as Adobe Photoshop. Besides, it may also take a very long time to produce photo-realistic contents since the process can be very tedious and cumbersome. Thus, how to make these processes automated is a very important yet non-trivial problem. Nowadays, deep visual generation has become a significant research direction in computer vision and machine learning, and has been applied in many tasks, such as automatic content generation, beautification, rendering and data augmentation. Thanks to the current deep generative methods can be categorized into two groups: variational auto-encoder (VAE) based methods and generative adversarial networks (GANs) based methods. Based on encoder-decoder architecture, VAE methods first map input data into a latent distribution, and then minimize the distance between the latent distribution and some prior distribution, e.g., Gaussian distribution. A well-trained VAE model could be used in the tasks of dimensionality reduction and image generation. However, an inevitable gap between the latent distribution and prior distribution would make the generated images/videos blurred. Unlike the VAE model, GAN has learned a mapping between input and output distributions to synthesize sharper images/videos. A GAN model has contained two major modules. A generator has aimed to generate the fake data and a discriminator has distinguished whether a sample is fake or not. To produce plausible fake data, the generator has been matched the distribution of real data and synthesized fake data that would fulfill the requirements of reality and diversity. The optimization problem of learning the generator and discriminator has been formulated into a two-player minimax game. During the training, the two modules have been optimized alternately using stochastic gradient methods. At the end of the training, the generator and discriminator have been supposed to reach a Nash Equilibria of the minimax game. Due to the development of GAN model, more deep visual generation applications and tasks have occurred based on GAN model. The six typical tasks for deep visual generation have been presented as follows: 1) Image generation from noises: it is the earliest task of deep visual generation in which GAN model seeks to generate an image (e.g., face image) from random noises. 2) Image generation from images: it tries to transform a given image into a new one (e.g., from black-and-white image to color image). This task can be applied to applications like style transfer and image reconstruction. 3) Image generation from texts: it is a very natural task just like that humans describe the content of a painting and then the painters draw the corresponding images based on the texts. 4) Video generation from images: it aims to turn a static image into a dynamic video, which can be used in time-lapse photography, making animated videos from pictures, etc. 5) Video generation from videos: it is mainly used for video style transfer, video super-resolution and so on. 6) Video generation from texts: it is more difficult than image generation from texts since it needs the generated videos focusing on both semantical alignments with text and consistency among video frames. The challenges in deep visual generation have been analyzed and discussed. First, rather than 2D data, we should try to generate high-quality 3D data, which contains more information and details. Second, we could pay more attention to video generation instead of only image generation. Third, we could conduct some researches on controllable deep visual generation methods, which are more practical in real-world applications. Finally, we could try to expand the style transfer methods from two domains to multiple domains. In this review, we have summarized very recent works on deep adversarial visual generation through a systematic investigation. The review has mainly included an introduction of deep visual generation background, typical generation models, an overview of mainstream deep visual generation tasks and related algorithms. The deep adversarial visual generation research has been conducted further. © 2021, Editorial Office of Journal of Image and Graphics. All right reserved.","","3D-depth image generation; Controllable generation; Deep learning; Generative adversarial networks (GANs); Image generation; Style transfer; Video generation; Visual generation","Review","Final","","Scopus","2-s2.0-85121325683"
"Hu M.-F.; Zuo X.; Liu J.-W.","Hu, Ming-Fei (57268901900); Zuo, Xin (7103216106); Liu, Jian-Wei (37048789500)","57268901900; 7103216106; 37048789500","Survey on Deep Generative Model; [深度生成模型综述]","2022","Zidonghua Xuebao/Acta Automatica Sinica","48","1","","40","74","34","10.16383/j.aas.c190866","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124119003&doi=10.16383%2fj.aas.c190866&partnerID=40&md5=fafa8b3ae96183897d33c4bc539b57d4","The generative model, which can generate samples randomly by learning the probability density of observable data, has been widely concerned for the past few years. It has been successfully applied in a wide range of fields, such as image generation, image restoration, density estimation, natural language and speech recognition, style transfer and super resolution, and so on. Deep generative model with multiple hidden layers in the network structure becomes a research hotspot because of its better generation ability. Depending on the different methods of calculating the maximum likelihood function, we divide the models into three types: the first kind of method is the approximate method, which use the sampling method to calculate approximately the likelihood function, such as Restricted Boltzmann machines (RBM) and Deep belief network (DBN), Deep Boltzmann machines (DBM), helmholtz machine based on RBM. The alternatives are to optimize directly the variational lower bound of likelihood function, it is named as variational autoencoder. The important improvements to these variants include importance weighted autoencoders and auxiliary deep generative models; the second kind is implicit methods, the representative model is generative adversarial nets (GAN), GAN＇s model parameters is optimized by the adversaring behavior between the generator and the discriminator. The principal instantiations of GAN include Wasserstein GAN, deep convolutional generative adversarial networks and BigGAN. The third kind involve flow and neural autoregressive net, the main variations of the flow paradigm include normalizing flow based on nonlinear independent components estimation, invertible residual networks and variational inference with flow. The successful improvements to the neural autoregressive net include neural autoregressive distribution estimation, pixel recurrent neural network, masked autoencoder for distribution estimation and WaveNet. We outline the principle and structure of these deep generative models, and look forward to the future work. Copyright ©2022 Acta Automatica Sinica. All rights reserved.","Image reconstruction; Maximum likelihood estimation; Network layers; Recurrent neural networks; Signal encoding; Speech recognition; Auto encoders; Auto-regressive; Deep generative model; Flow; Generative model; Likelihood functions; Neural autoregressive distribution estimator; Restricted boltzmann machine; Variational auto-encoder; Generative adversarial networks","Deep generative models; Flow; Generative adversarial nets; Neural autoregressive distribution estimator; Restricted Boltzmann machine (RBM); Variational auto-encoder","Article","Final","","Scopus","2-s2.0-85124119003"
"Feng X.; Zhang W.; Su X.; Xu Z.","Feng, Xubin (57201876230); Zhang, Wuxia (54390588100); Su, Xiuqin (18438569800); Xu, Zhengpu (57217677334)","57201876230; 54390588100; 18438569800; 57217677334","Optical remote sensing image denoising and super-resolution reconstructing using optimized generative network in wavelet transform domain","2021","Remote Sensing","13","9","1858","","","","10.3390/rs13091858","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106526544&doi=10.3390%2frs13091858&partnerID=40&md5=e89da686618affee80112bc1b00efb5f","High spatial quality (HQ) optical remote sensing images are very useful for target detection, target recognition and image classification. Due to the influence of imaging equipment accuracy and atmospheric environment, HQ images are difficult to acquire, while low spatial quality (LQ) remote sensing images are very easy to acquire. Hence, denoising and super-resolution (SR) reconstruction technology are the most important solutions to improve the quality of remote sensing images very effectively, which can lower the cost as much as possible. Most existing methods usually only employ denoising or SR technology to obtain HQ images. However, due to the complex structure and the large noise of remote sensing images, the quality of the remote sensing image obtained only by denoising method or SR method cannot meet the actual needs. To address these problems, a method of reconstructing HQ remote sensing images based on Generative Adversarial Network (GAN) named “Restoration Generative Adversarial Network with ResNet and DenseNet” (RRDGAN) is proposed, which can acquire better quality images by incorporating denoising and SR into a unified framework. The generative network is implemented by fusing Residual Neural Network (ResNet) and Dense Convolutional Network (DenseNet) in order to consider denoising and SR problems at the same time. Then, total variation (TV) regularization is used to furthermore enhance the edge details, and the idea of Relativistic GAN is explored to make the whole network converge better. Our RRDGAN is implemented in wavelet transform (WT) domain, since different frequency parts could be handled separately in the wavelet domain. The experimental results on three different remote sensing datasets shows the feasibility of our proposed method in acquiring remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Image acquisition; Image denoising; Image enhancement; Image reconstruction; Optical resolving power; Wavelet transforms; Adversarial networks; Atmospheric environment; Convolutional networks; Optical remote sensing; Remote sensing images; Super resolution reconstruction; Total variation regularization; Wavelet-transform domain; Remote sensing","Denoising; Densely connection network (DenseNet); Generative adversarial network (GAN); Relativistic; Remote sensing; Residual network (ResNet); Super-resolution; Total variation (TV); Wavelet transform (WT)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106526544"
"Nneji G.U.; Cai J.; Jianhua D.; Monday H.N.; Chikwendu I.A.; Oluwasanmi A.; James E.C.; Mgbejime G.T.","Nneji, Grace Ugochi (57206902001); Cai, Jingye (10738940600); Jianhua, Deng (6506091558); Monday, Happy Nkanta (57201671873); Chikwendu, Ijeoma Amuche (57216689292); Oluwasanmi, Ariyo (57210636841); James, Edidiong Christopher (57313225000); Mgbejime, Goodness Temofe (57311893400)","57206902001; 10738940600; 6506091558; 57201671873; 57216689292; 57210636841; 57313225000; 57311893400","Enhancing low quality in radiograph datasets using wavelet transform convolutional neural network and generative adversarial network for COVID-19 identification","2021","2021 4th International Conference on Pattern Recognition and Artificial Intelligence, PRAI 2021","","","","146","151","5","10.1109/PRAI53619.2021.9551043","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117956279&doi=10.1109%2fPRAI53619.2021.9551043&partnerID=40&md5=95ea9dacd91607a5b9dc462f4956c70d","The coronavirus disease of 2019 (COVID-19) pandemic has caused a global public health epidemic since there is no 100% vaccine to cure or prevent the further spread of the virus. With the ever-increasing number of new infections, creating automated methods for COVID-19 identification of Chest X-ray images is critical to aiding clinical diagnosis and reducing the time-consumption for image interpretation. This paper proposes a novel joint framework for accurate COVID-19 identification by integrating an enhanced super-resolution generative adversarial network with a noise reduction filter bank of wavelet transform convolutional neural network on both Chest X-ray and Chest Tomography images for COVID-19 identification. The super-resolution utilized in this study is to enhance the image quality while the wavelet transform Convolutional Neural Network architecture is used to accurately identify COVID-19. Our proposed architecture is very robust to noise and vanishing gradient problem. We used public domain datasets of Chest x-ray images and Chest Tomography to train and check the performance of our COVID-19 identification task. This experiment shows that our system is consistently efficient by accuracy of 0.988, sensitivity of 0.994, and specificity of 0.987, AUC of 0.99, F1-score of 0.982 and 0.989 for precision using the Chest X-ray dataset while for Chest Tomography dataset, an accuracy of 0.978, sensitivity of 0.981, and specificity of 0.979, AUC of 0.985, F1-score of 0.961 and precision of 0.980. These performances have also outweighed other established state-of-the-art learning methods.  © 2021 IEEE.","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Diseases; Image compression; Image enhancement; Image reconstruction; Medical imaging; Noise abatement; Optical resolving power; Tomography; Viruses; Wavelet transforms; Chest tomography; Chest X-ray image; Chest x-rays; Convolutional neural network; Coronavirus disease of 2019; Coronaviruses; Deep learning; Identification; Superresolution; Wavelets transform; Coronavirus","Chest tomography; Chest x-ray; COVID-19; Deep learning; Identification; Super-resolution; Wavelet transform","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85117956279"
"Senalp F.M.; Ceylan M.","Senalp, Fatih M. (57361544200); Ceylan, Murat (56276648900)","57361544200; 56276648900","Deep learning based super resolution and classification applications for neonatal thermal images","2021","Traitement du Signal","38","5","","1361","1368","7","10.18280/ts.380511","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120534778&doi=10.18280%2fts.380511&partnerID=40&md5=48016be3b75664b9c7b3104046c3365a","The thermal camera systems can be used in all kinds of applications that require the detection of heat change, but thermal imaging systems are highly costly systems. In recent years, developments in the field of deep learning have increased the success by obtaining quality results compared to traditional methods. In this paper, thermal images of neonates (healthy - unhealthy) obtained from a high-resolution thermal camera were used and these images were evaluated as high resolution (ground truth) images. Later, these thermal images were downscaled at 1/2, 1/4, 1/8 ratios, and three different datasets consisting of low-resolution images in different sizes were obtained. In this way, super-resolution applications have been carried out on the deep network model developed based on generative adversarial networks (GAN) by using three different datasets. The successful performance of the results was evaluated with PSNR (peak signal to noise ratio) and SSIM (structural similarity index measure). In addition, healthy - unhealthy classification application was carried out by means of a classifier network developed based on convolutional neural networks (CNN) to evaluate the super-resolution images obtained using different datasets. The obtained results show the importance of combining medical thermal imaging with super-resolution methods. © 2021 Lavoisier. All rights reserved.","Cameras; Convolutional neural networks; Deep learning; Generative adversarial networks; Image classification; Imaging systems; Infrared devices; Infrared imaging; Optical resolving power; Signal to noise ratio; Dataset; Deep learning; Heat change; High resolution; Learning-based super-resolution; Superresolution; Thermal camera system; Thermal images; Thermal imaging system; Thermal-imaging; Classification (of information)","Classification; Datasets; Deep learning; Super-resolution; Thermal imaging","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85120534778"
"Du J.; Li S.; Jin X.","Du, Jinlian (8884526700); Li, Shufei (57299487000); Jin, Xueyun (36189733200)","8884526700; 57299487000; 36189733200","Research on the Network of 3D Smoke Flow Super-Resolution Data Generation; [三维烟雾流场超分辨率数据生成网络模型的研究]","2021","Xitong Fangzhen Xuebao / Journal of System Simulation","33","10","","2381","2389","8","10.16182/j.issn1004731x.joss.20-0556","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117307192&doi=10.16182%2fj.issn1004731x.joss.20-0556&partnerID=40&md5=c74f09f3b08d473e3a6566ce109b94b9","Aiming at the problem of low data generation efficiency due to the high complexity of solving the N-S equation of smoke flow field, a deep learning model which can generate high-resolution smoke flow data based on low-resolution smoke flow data solved by N-S equation is explored and designed. Based on the Generative Adversarial Network, the smoke data reconstruction network based on the sub voxel convolution layer is constructed. Considering the fluidity of smoke, time loss based on advection step is introduced into the loss function to realize high-precision smoke simulation. By extending the image super-resolution quality evaluation index, the peak signal-to-noise ratio of smoke density data is constructed to evaluate the data quality of the reconstructed high-resolution three-dimensional smoke flow field. The experimental results show that the smoke data reconstructed by the deep learning model designed in this paper based on the low resolution data generated by the N-S equation of smoke flow field have good performance in numerical distribution, accuracy and visual effects. © 2021, The Editorial Board of Journal of System Simulation. All right reserved.","","Deep learning; GAN; Smoke simulation; Sub voxel convolution; Super-resolution","Article","Final","","Scopus","2-s2.0-85117307192"
"Cheng G.; Zhang F.","Cheng, Guojian (15753589500); Zhang, Fulin (57222111378)","15753589500; 57222111378","Super-resolution Reconstruction of Rock Slice Image Based on SinGAN; [基于SinGAN的岩石薄片图像超分辨率重建]","2021","Xi'an Shiyou Daxue Xuebao (Ziran Kexue Ban)/Journal of Xi'an Shiyou University, Natural Sciences Edition","36","2","","116","121","5","10.3969/j.issn.1673-064X.2021.02.017","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107172557&doi=10.3969%2fj.issn.1673-064X.2021.02.017&partnerID=40&md5=a2e1d32d838e7c4a2923b9a3cb689dc9","The rock slice images are of great significance to the study of petroleum geological characteristics and the exploration of oil and gas. Due to the limitations of various factors, the obtained images of rock slices often have low resolution, which limits the researchers' grasp of their detailed information to some extent. General super-resolution algorithm of neural network will need a large amount of data as a training set, and in order to improve the ability of rock slice image super-resolution reconstruction algorithm to the restoration of the texture detail information of rock slice images, in this paper, the super-resolution reconstruction of single rock slice image is finished using single image generative adversarial network without inputting a large number of data sets. Rock cast thin section images from an oilfield area in Ordos were processed using this method, and the processing results are evaluated by the indexes of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) evaluation. It is shown that the processed images using this method has good visual effects and evaluation indexes. © 2021, the Editorial Department of Journal of Xi'an Shiyou University. All right reserved.","","Neural network; Rock slice image; Single image generation adversarial network; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85107172557"
"Cao M.; Liu Z.; Huang X.; Shen Z.","Cao, Minjie (57223052535); Liu, Zhe (57201597665); Huang, Xueting (57223033698); Shen, Zhuoxuan (57223037792)","57223052535; 57201597665; 57223033698; 57223037792","Research for Face Image Super-Resolution Reconstruction Based on Wavelet Transform and SRGAN","2021","IAEAC 2021 - IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference","","","9390748","448","451","3","10.1109/IAEAC50856.2021.9390748","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104591570&doi=10.1109%2fIAEAC50856.2021.9390748&partnerID=40&md5=59954d98e7329f96ba47ef3ddc2dfe64","Super-resolution face image is the basis of high detection rate in face recognition. In order to meet the requirements of super-resolution image in face recognition, aiming at the problem of texture loss of super-resolution image under high-frequency features, a face image reconstruction method based on wavelet transform and super-resolution generative adversarial network (SRGAN) is proposed to reduce the impact of low-resolution image caused by imaging hardware, network bandwidth and sampling environment on face recognition accuracy. Firstly, the wavelet transform algorithm is used to preprocess the low-resolution face image to extract the detailed texture features of the face image under different frequencies. Then, GAN is used to learn the prior knowledge of wavelet coefficients, and the identity preserving constraint is applied to the output image, and the perceptual loss function of the fusion wavelet coefficients is realized. Finally, the deep learning model based on SRGAN is used to obtain high-resolution face images. Experimental results show that the method can achieve super-resolution restoration of low-resolution face images and meet the requirements of face recognition accuracy. © 2021 IEEE.","Deep learning; Face recognition; Image compression; Image reconstruction; Optical resolving power; Textures; Wavelet transforms; Adversarial networks; Face image reconstruction; Low resolution images; Low-resolution face images; Recognition accuracy; Super-resolution restoration; Wavelet coefficients; Wavelet transform algorithms; Image texture","super-resolution; super-resolution generative adversarial network(SRGAN); wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85104591570"
"Xing H.; Bao M.; Li Y.; Shi L.; Xing M.","Xing, Hantong (57222711667); Bao, Min (36699264600); Li, Yachao (16307232700); Shi, Lin (36190030800); Xing, Mengdao (7005922869)","57222711667; 36699264600; 16307232700; 36190030800; 7005922869","Deep Mutual GAN for Life-Detection Radar Super Resolution","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3065696","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103758001&doi=10.1109%2fLGRS.2021.3065696&partnerID=40&md5=e0be291632952485a16a417db96472aa","To improve the life-detection radar resolution under certain hardware conditions, in this letter, a deep mutual learning generative adversarial network model (Deep Mutual GAN) is proposed. In the proposed model, the generator can improve the angular resolution of the input low-resolution radar image by five times, which is enough to meet our requirements for the resolution of life detection. We innovatively use two generators in GAN with the same network structure and make the two generators learn from each other. In this way, the learning process of a generator is not only achieved by its confrontation with the discriminator but also guided by another generator. As a result, the knowledge of the generator is no longer only obtained through its own learning; each generator learns knowledge from another generator while learning knowledge by itself. The proposed model can effectively make the convergence of GAN more stable and improves the super resolution effect. We also introduce the details of the network structure of generator and discriminator, in which residual learning and a symmetrical network structure are applied. The experimental results show that the proposed method can achieve state-of-the-art imaging effect, which is meaningful for subsequent target detection and recognition.  © 2004-2012 IEEE.","Deep learning; Image enhancement; Optical resolving power; Radar imaging; Tracking radar; Adversarial networks; Angular resolution; Learning process; Life detection radar; Network structures; State of the art; Super resolution; Target detection and recognition; image resolution; instrumentation; radar; Learning systems","Deep mutual learning; generative adversarial network (GAN); life-detection radar imaging; super resolution (SR)","Article","Final","","Scopus","2-s2.0-85103758001"
"Demiray B.Z.; Sit M.; Demir I.","Demiray, Bekir Z. (57219733801); Sit, Muhammed (57199691574); Demir, Ibrahim (56425613600)","57219733801; 57199691574; 56425613600","D-SRGAN: DEM Super-Resolution with Generative Adversarial Networks","2021","SN Computer Science","2","1","48","","","","10.1007/s42979-020-00442-2","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108161142&doi=10.1007%2fs42979-020-00442-2&partnerID=40&md5=c82bede44a9f1411deaaf6b9a8f3026b","Digital elevation model (DEM) is a critical data source for variety of applications such as road extraction, hydrological modeling, flood mapping, and many geospatial studies. The usage of high-resolution DEMs as inputs in many application areas improves the overall reliability and accuracy of the raw dataset. The goal of this study is to develop a machine learning model that increases the spatial resolution of DEM without additional information. In this paper, a GAN based model (D-SRGAN), inspired by single image super-resolution methods, is developed and evaluated to increase the resolution of DEMs. The experiment results show that D-SRGAN produces promising results while constructing 3 feet high-resolution DEMs from 50 feet low-resolution DEMs. It outperforms common statistical interpolation methods and neural network algorithms.This study shows that it is possible to use the power of artificial neural networks to increase the resolution of the DEMs. The study also demonstrates that approaches from single image super-resolution can be applied for DEM super-resolution. © 2021, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. part of Springer Nature.","","Deep learning; DEM; DEM reconstruction; Generative adversarial networks (GANs); Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85108161142"
"Wang C.; Zhang Z.; Zhang Y.; Tian R.; Ding M.","Wang, Cong (57862719900); Zhang, Zian (57226611585); Zhang, Yongqiang (57190277288); Tian, Rui (57226599227); Ding, Mingli (8925275900)","57862719900; 57226611585; 57190277288; 57226599227; 8925275900","GMSRI: A texture-based martian surface rock image dataset","2021","Sensors","21","16","5410","","","","10.3390/s21165410","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112054890&doi=10.3390%2fs21165410&partnerID=40&md5=b88a2940731de7a2944ed73930b76959","CNN-based Martian rock image processing has attracted much attention in Mars missions lately, since it can help planetary rover autonomously recognize and collect high value science targets. However, due to the difficulty of Martian rock image acquisition, the accuracy of the processing model is affected. In this paper, we introduce a new dataset called “GMSRI” that is a mixture of real Mars images and synthetic counterparts which are generated by GAN. GMSRI aims to provide a set of Martian rock images sorted by the texture and spatial structure of rocks. This paper offers a detailed analysis of GMSRI in its current state: Five sub-trees with 28 leaf nodes and 30,000 images in total. We show that GMSRI is much larger in scale and diversity than the current same kinds of datasets. Constructing such a database is a challenging task, and we describe the data collection, selection and generation processes carefully in this paper. Moreover, we evaluate the effectiveness of the GMSRI by an image super-resolution task. We hope that the scale, diversity and hierarchical structure of GMSRI can offer opportunities to researchers in the Mars exploration community and beyond. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Extraterrestrial Environment; Mars; Martian surface analysis; Rocks; Textures; Generation process; Hierarchical structures; Image super resolutions; Mars exploration; Martian surface; Planetary rovers; Processing model; Spatial structure; astronomy; space; Image texture","Generative adversarial network; Mars image dataset; Martian surface rock image","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85112054890"
"Li Y.; Liu J.; Chen Y.","Li, Yuanzhuo (57222116320); Liu, Junjie (57337021700); Chen, Yu (57846716600)","57222116320; 57337021700; 57846716600","Learning Structural Coherence Via Generative Adversarial Network for Single Image Super-resolution","2021","Proceedings - 2021 International Conference on Computer Engineering and Application, ICCEA 2021","","","","184","188","4","10.1109/ICCEA53728.2021.00044","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118975324&doi=10.1109%2fICCEA53728.2021.00044&partnerID=40&md5=ee3edc14cbbd106b2921ff33fca04aa7","Among the major remaining challenges for single image super-resolution (SISR) is the capacity to recover coherent images with global shapes and local details conforming to human vision system. Recent generative adversarial network (GAN) based SISR methods have yielded overall realistic SR images, however, there are always unpleasant textures accompanied with structural distortions in local regions. To target these issues, we introduce the gradient branch into the generator to preserve structural information by restoring high-resolution gradient maps in SR process. In addition, we utilize a U-net based discriminator to consider both the whole image and the detailed per-pixel authenticity, which could encourage the generator to maintain overall coherence of the reconstructed images. Moreover, we have studied objective functions and LPIPS perceptual loss is added to generate more realistic and natural details. Experimental results show that our proposed method outperforms state-of-the-art perceptual-driven SR methods in perception index (PI), and obtains more geometrically consistent and visually pleasing textures in natural image restoration.  © 2021 IEEE.","Computer vision; Image reconstruction; Optical resolving power; Textures; Coherent images; Global shapes; Global-local; Gradient map; Image super resolutions; Perceptual loss; Sing image super-resolution; Single images; Structural coherence; U-net; Generative adversarial networks","generative adversarial network; gradient map; perceptual loss; sing image super-resolution; U-net","Conference paper","Final","","Scopus","2-s2.0-85118975324"
"Du T.; Zhang Y.","Du, Tianwen (57280158900); Zhang, Yifeng (56544995400)","57280158900; 56544995400","Single image super-resolution algorithm based on enhanced generative adversarial network","2021","2021 6th International Conference on Image, Vision and Computing, ICIVC 2021","","","","357","361","4","10.1109/ICIVC52351.2021.9527025","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116142285&doi=10.1109%2fICIVC52351.2021.9527025&partnerID=40&md5=e554267618f9e51040d9e3053ea30d8c","With the advancement of technology, the processing of increasing image data becomes particularly important. Computer vision can process the hidden information behind these data. Image super-resolution aims to use a low-resolution image to generate a corresponding high-resolution image through the convolutional neural network, improving the quality of the image and the display effect. This paper proposes an Enhanced Generative Adversarial Network (EGAN), designing a new loss function, so that the generated image has more texture details than the previous method. On three benchmark datasets, Set5, Set14 and BSD100, the proposed method has a great improvement on the peak signal-to-noise ratio and structural similarity of the generated images. © 2021 IEEE.","Computer vision; Convolutional neural networks; Image enhancement; Optical resolving power; Signal to noise ratio; Textures; Convolutional neural network; Hidden information; High-resolution images; Image data; Image super resolutions; Loss functions; Low resolution images; Single images; Super resolution algorithms; Superresolution; Generative adversarial networks","Generative adversarial network; Loss function; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85116142285"
"Si J.; Xiao X.; Li J.; Ma M.; Mao Y.","Si, Jie (57863512100); Xiao, Xiong (57863131100); Li, Jing (57207737643); Ma, Mingxun (57863259700); Mao, Yuxing (17346073100)","57863512100; 57863131100; 57207737643; 57863259700; 17346073100","Super-Resolution Reconstruction Algorithm with Multi-Frame Defocused Images Based on Generative Adversarial Network","2021","Jisuanji Gongcheng/Computer Engineering","47","9","","266","273","7","10.19678/j.issn.1000-3428.0058848","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136777682&doi=10.19678%2fj.issn.1000-3428.0058848&partnerID=40&md5=af4a3461adac95dcb46ab154652d8c70","In order to improve the quality of the reconstructed images, a super-resolution reconstruction algorithm using multi-frame defocused images is proposed. The algorithm employs an auto-encoder to extract the important features in the defocused images, and the layer structure is transformed based on spatial features to combine the defocused features with the original features, so the super-resolution reconstruction of the image is completed. The experimental results on the Celeb A face data set show that in most cases, the proposed algorithm provides a higher peak signal-to-noise ratio and structural similarity than the traditional interpolation algorithm and the SRGAN algorithm. This super-resolution algorithm based on multi-frame defocused images can generate better reconstructed images. © 2021, Editorial Office of Computer Engineering. All rights reserved.","","Auto encoder; Deep neural network; Generative Adversarial Net (GAN); Image feature extraction; Super Resolution (SR)","Article","Final","","Scopus","2-s2.0-85136777682"
"Wang X.-S.; Chao J.; Cheng Y.-H.","Wang, Xue-Song (56090094900); Chao, Jie (57224168731); Cheng, Yu-Hu (9733966500)","56090094900; 57224168731; 9733966500","Image super-resolution reconstruction based on self-attention GAN; [基于自注意力生成对抗网络的图像超分辨率重建]","2021","Kongzhi yu Juece/Control and Decision","36","6","","1324","1332","8","10.13195/j.kzyjc.2019.1290","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107121608&doi=10.13195%2fj.kzyjc.2019.1290&partnerID=40&md5=bc29ef5d07187aebc18cb4218fd9ce11","Aiming at how to recover the texture details of the reconstructed super-resolution image, an image super-resolution reconstruction based on the self-attention generative adversarial network (SRAGAN) is proposed. In the SRAGAN, a generator based on a combination of the self-attention mechanism and the residual module is used to transform low-resolution into super-resolution images, while a discriminator based on the deep convolutional network tries to distinguish the difference between the reconstructed and real super-resolution images. In terms of loss function construction, on the one hand, the Charbonnier content loss function is used to improve the accuracy of image reconstruction; on the other hand, the eigenvalues before the activation layer in the pre-trained VGG network are used to calculate the perceptual loss to achieve accurate texture detail reconstruction of super-resolution images. Experiments show that the proposed SRAGAN is superior to the current popular algorithms in peak signal-to-noise ratio and structural similarity score, reconstructing more realistic images with clear textures. Copyright ©2021 Control and Decision.","Convolutional neural networks; Eigenvalues and eigenfunctions; Image enhancement; Image texture; Optical resolving power; Signal to noise ratio; Textures; Activation layer; Adversarial networks; Attention mechanisms; Convolutional networks; Image super-resolution reconstruction; Peak signal to noise ratio; Structural similarity; Super resolution; Image reconstruction","Generative adversarial network; Image super-resolution reconstruction; Loss function; Self-attention mechanism","Article","Final","","Scopus","2-s2.0-85107121608"
"Mohamed A.O.A.; Salih K.H.M.; Ali H.H.S.M.","Mohamed, Awad O. A. (57224163757); Salih, Khalid H. M. (57224165856); Ali, Hiba H. S. M. (57195629781)","57224163757; 57224165856; 57195629781","Super Resolution of Medical Images Using Generative Adversarial Networks","2021","Proceedings of: 2020 International Conference on Computer, Control, Electrical, and Electronics Engineering, ICCCEEE 2020","","","9429656","","","","10.1109/ICCCEEE49695.2021.9429656","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107111482&doi=10.1109%2fICCCEEE49695.2021.9429656&partnerID=40&md5=6146c23ba9a26a3208575529642e1b28","This paper aims to present a new model called SRMIGAN that performs super-resolution for MRI and CT medical images to help doctors reach a better diagnosis. This model, SRMIGAN adopts deep learning by applying generative adversarial networks technique. It is developed using MSE loss and by exploiting different optimization techniques. This model is compared to other adopted models by using both objective and subjective metrics. Hence PSNR, SSIM, and mean opinion score results are included. The results show that our model beats the other examined models. © 2021 IEEE.","Computerized tomography; Deep learning; Diagnosis; Magnetic resonance imaging; Optical resolving power; Adversarial networks; Mean opinion scores; Optimization techniques; Super resolution; Medical imaging","Convolutional Neural Networks; Generative Adversarial Networks; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85107111482"
"Huang K.-W.; Chen H.-C.; Lee S.-A.; Fu L.-C.","Huang, Kuan-Wei (57218567377); Chen, Huang-Chih (57203657704); Lee, Sheng-An (57226547566); Fu, Li-Chen (7401812723)","57218567377; 57203657704; 57226547566; 7401812723","Improving 3D Recovery based on Super-Resolution Generative Adversarial Network and Uniform Continuous Trajectory for Atomic Force Microscopy","2021","Proceedings of the American Control Conference","2021-May","","9483059","2601","2606","5","10.23919/ACC50511.2021.9483059","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111912657&doi=10.23919%2fACC50511.2021.9483059&partnerID=40&md5=462fc182a77ebdc1caa2df465d088a8a","Atomic force microscope (AFM) is a powerful nano-scale measurement instrument, which is diffusely applied on different fields, such as biological science, nanomanipulation, semiconductor, Micro Electro Mechanical Systems (MEMS) detection, etc. The well-known advantage of AFM is its high-accuracy 3D topography reconstruction. Different from optical microscopy, which can only obtain 2D image by optical reflection, three kinds of operating principles of AFM respectively maintaining the contact force, amplitude or distance between the tip and sample surface during scanning to collect the sample's height information, and then help us to build a 3D sample topography. However, because of the physical contact with probe, there is a major problem in AFM - imaging speed. In this paper, we propose a new method which applies the Generative Adversarial Networks (GAN) to AFM image reconstruction, which can recover a high-resolution (HR) image from a low-resolution (LR) one with only a quarter of time. While using GAN, data uniformity is most crucial. To address this issue, we propose a new trajectory - Uniform continuous path (UC path) to break the limits on traditional raster scanning and a proposed feature similarity metric is used on comparing the reconstruction results in experiments. © 2021 American Automatic Control Council.","Image reconstruction; MEMS; Nanotechnology; Topography; Adversarial networks; Biological science; Feature similarities; High resolution image; Micro  electromechanical system (MEMS); Nano-scale measurements; Operating principles; Optical reflection; Atomic force microscopy","","Conference paper","Final","","Scopus","2-s2.0-85111912657"
"Su J.-S.; Zhang M.-J.; Yu W.-J.","Su, Jin-Sheng (57474031400); Zhang, Ming-Jun (57204639713); Yu, Wen-Jing (57216693919)","57474031400; 57204639713; 57216693919","A Single Image Super-Resolution Reconstruction Based on Fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12083","","120831J","","","","10.1117/12.2623592","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125562436&doi=10.1117%2f12.2623592&partnerID=40&md5=918c4accfa6aed79e0b638526bc932cd","Image super-resolution is to restore a high-resolution image from a low-resolution image or image sequence. High resolution means that the image has a high pixel density and can provide more details, which often play a key role in the application. Aiming at the application of single-frame low-resolution reconstruction and super-resolution, this paper proposes a method based on image fusion. This method combines two or more methods of super-resolution image reconstruction using generative adversarial neural networks. The reconstructed images are fused. Image fusion uses the integration of two or more images into a new image. Fusion can make use of the temporal and spatial correlation and information complementarity of two or more images, which can make the image obtained after fusion have a more comprehensive and clear description of the scene, which is more conducive to human eye recognition. This paper draws on the idea of ensemble learning, and uses the super-resolution images generated by the three super-resolution reconstruction algorithms of BasicSR, SRGAN and ESRGAN to carry out two-by-two cross fusion for simulation experiments. The experimental results show that this kind of reconstruction using different generation adversarial networks to generate the super-resolution image by fusion is simple and effective. The super-resolution image quality after fusion is generally better than the image quality before fusion in terms of PSNR and SSIM. © 2022 SPIE.","Generative adversarial networks; Image quality; Image reconstruction; Optical resolving power; High resolution; High-resolution images; Image sequence; Image super resolutions; Low resolution images; Resolution images; Single frames; Single-image super-resolution reconstruction; Super-resolution reconstruction; Superresolution; Image fusion","Generative adversarial networks; Image fusion; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85125562436"
"Dharejo F.A.; Deeba F.; Zhou Y.; Das B.; Jatoi M.A.; Zawish M.; Du Y.; Wang X.","Dharejo, Fayaz Ali (57195487028); Deeba, Farah (57215997317); Zhou, Yuanchun (55737417400); Das, Bhagwan (56493885700); Jatoi, Munsif Ali (55822002000); Zawish, Muhammad (57206720765); Du, Yi (35791161600); Wang, Xuezhi (56512501900)","57195487028; 57215997317; 55737417400; 56493885700; 55822002000; 57206720765; 35791161600; 56512501900","TWIST-GAN: Towards Wavelet Transform and Transferred GAN for Spatiooral Single Image Super Resolution","2021","ACM Transactions on Intelligent Systems and Technology","12","6","71","","","","10.1145/3456726","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123956833&doi=10.1145%2f3456726&partnerID=40&md5=423b9bbc6a776996628949b45f5fa79d","Single Image Super-resolution (SISR) produces high-resolution images with fine spatial resolutions from a remotely sensed image with low spatial resolution. Recently, deep learning and generative adversarial networks (GANs) have made breakthroughs for the challenging task of single image super-resolution (SISR). However, the generated image still suffers from undesirable artifacts such as the absence of texture-feature representation and high-frequency information. We propose a frequency domain-based spatiooral remote sensing single image super-resolution technique to reconstruct the HR image combined with generative adversarial networks (GANs) on various frequency bands (TWIST-GAN). We have introduced a new method incorporating Wavelet Transform (WT) characteristics and transferred generative adversarial network. The LR image has been split into various frequency bands by using the WT, whereas the transfer generative adversarial network predicts high-frequency components via a proposed architecture. Finally, the inverse transfer of wavelets produces a reconstructed image with super-resolution. The model is first trained on an external DIV2 K dataset and validated with the UC Merced Landsat remote sensing dataset and Set14 with each image size of 256 × 256. Following that, transferred GANs are used to process spatiooral remote sensing images in order to minimize computation cost differences and improve texture information. The findings are compared qualitatively and qualitatively with the current state-of-art approaches. In addition, we saved about 43% of the GPU memory during training and accelerated the execution of our simplified version by eliminating batch normalization layers. © 2021 Association for Computing Machinery.","Deep learning; Frequency domain analysis; Image compression; Image enhancement; Image reconstruction; Image resolution; Image texture; Remote sensing; Textures; Wavelet transforms; High-resolution images; Image super resolutions; Neural-networks; Remote-sensing; Remotely sensed images; Single images; Spatial resolution; Spatiooral; Superresolution; Wavelets transform; Generative adversarial networks","neural networks; spatiooral; super resolution; Wavelet transform","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123956833"
"Khare Y.; Ramesh A.; Chandran V.; Veerasamy S.; Singh P.; Adarsh S.; Anjali T.","Khare, Yash (57222578730); Ramesh, Abhijit (57442176300); Chandran, Vishwaak (57442555200); Veerasamy, Sevagen (57442055100); Singh, Pranjal (57442055200); Adarsh, S. (57219485297); Anjali, T. (57195522716)","57222578730; 57442176300; 57442555200; 57442055100; 57442055200; 57219485297; 57195522716","Intelligent CCTV Footage Analysis with Sound Source Separation, Object Detection and Super Resolution","2022","Lecture Notes in Networks and Systems","336","","","107","118","11","10.1007/978-981-16-6723-7_9","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124135032&doi=10.1007%2f978-981-16-6723-7_9&partnerID=40&md5=5208e59fb17d7757a409545ee3854921","CCTV cameras are found everywhere nowadays and are used to monitor, secure, and protect your property, or at the very least serves as intelligent CCTV footage analysis with sound source separation, object detection and super resolution. However, according to recent statistics, 80% of CCTV footage is discarded in the case of an investigation and is deemed uninformative. The reason being the grainy and low-quality video feed from CCTV cameras. Nonetheless, people thought about it and created video processing software or forensic tools that can improve the quality of the footage. Despite this, the latter are usually expensive or are only available to the authorities. Here developed is an open-source solution that is cross-platform and offers a seamless user interface for your average consumer. The application uses super-resolution to enhance image quality, object detection using YOLO v3, and sound extraction. Using actual CCTV footage as an example, the overall quality and output satisfying results for every functionality. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Encoder-decoder architecture; Generative adversarial networks; Object detection; Sound source separation; Super-resolution; UNet","Conference paper","Final","","Scopus","2-s2.0-85124135032"
"Huang B.; Xiao H.; Liu W.; Zhang Y.; Wu H.; Wang W.; Yang Y.; Yang Y.; Miller G.W.; Li T.; Cai J.","Huang, Bangyan (57217056573); Xiao, Haonan (57222711088); Liu, Weiwei (57835199300); Zhang, Yibao (35094727300); Wu, Hao (56642105100); Wang, Weihu (57215374101); Yang, Yunhuan (57217058508); Yang, Yidong (57226727250); Miller, G Wilson (7404978855); Li, Tian (57216658368); Cai, Jing (35067810800)","57217056573; 57222711088; 57835199300; 35094727300; 56642105100; 57215374101; 57217058508; 57226727250; 7404978855; 57216658368; 35067810800","MRI super-resolution via realistic downsampling with adversarial learning","2021","Physics in Medicine and Biology","66","20","205004","","","","10.1088/1361-6560/ac232e","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117222487&doi=10.1088%2f1361-6560%2fac232e&partnerID=40&md5=8d13b4aa3bd625b67785fbbe335f703b","Many deep learning (DL) frameworks have demonstrated state-of-the-art performance in the super-resolution (SR) task of magnetic resonance imaging, but most performances have been achieved with simulated low-resolution (LR) images rather than LR images from real acquisition. Due to the limited generalizability of the SR network, enhancement is not guaranteed for real LR images because of the unreality of the training LR images. In this study, we proposed a DL-based SR framework with an emphasis on data construction to achieve better performance on real LR MR images. The framework comprised two steps: (a) downsampling training using a generative adversarial network (GAN) to construct more realistic and perfectly matched LR/high-resolution (HR) pairs. The downsampling GAN input was real LR and HR images. The generator translated the HR images to LR images and the discriminator distinguished the patch-level difference between the synthetic and real LR images. (b) SR training was performed using an enhance4d deep super-resolution network (EDSR). In the controlled experiments, three EDSRs were trained using our proposed method, Gaussian blur, and k-space zero-filling. As for the data, liver MR images were obtained from 24 patients using breath-hold serial LR and HR scans (only HR images were used in the conventional methods). The k-space zero-filling group delivered almost zero enhancement on the real LR images and the Gaussian group produced a considerable number of artifacts. The proposed method exhibited significantly better resolution enhancement and fewer artifacts compared with the other two networks. Our method outperformed the Gaussian method by an improvement of 0.111 0.016 in the structural similarity index and 2.76 0.98 dB in the peak signal-to-noise ratio. The blind/reference-less image spatial quality evaluator metric of the conventional Gaussian method and proposed method were 46.6 4.2 and 34.1 2.4, respectively. © 2021 Institute of Physics and Engineering in Medicine.","Deep learning; Gaussian distribution; Generative adversarial networks; Image enhancement; Magnetic resonance imaging; Quality control; Signal sampling; Signal to noise ratio; Down sampling; High-resolution images; K-space; Low resolution images; Low-high; Lower resolution; MR-images; Performance; Superresolution; Zero filling; Optical resolving power","","Article","Final","","Scopus","2-s2.0-85117222487"
"Ma T.; Tian W.","Ma, Tingsong (57197867477); Tian, Wenhong (13404650300)","57197867477; 13404650300","Back-projection-based progressive growing generative adversarial network for single image super-resolution","2021","Visual Computer","37","5","","925","938","13","10.1007/s00371-020-01843-3","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084197926&doi=10.1007%2fs00371-020-01843-3&partnerID=40&md5=7dcf3436dddad2bab8ab1bfa1b5404b3","Recent advanced deep learning studies have shown the positive role of feedback mechanism in image super-resolution task. However, current feedback mechanism only calculates residual errors of images with the same resolution without considering the useful features that may be carried by different resolution features. In this paper, to explore the potential of feedback mechanism, we design a new network structure (progressive up- and downsampling back-projection units) to construct a generative adversarial network for single image super-resolution and use progressive growing methodologies to train it. Unlike previous feedback structure, we use progressively increasing scale factor to build up- and down-projection units, which aims to learn fruitful features across scales. This method allows us to get more meaningful information from early feature maps. Additionally, we train our network progressively; in the process of training, we start from single layer network structure and add new layers as the training goes on. By this mean, the training process can be greatly accelerated and stabilized. Experiments on benchmark dataset with the state-of-the-art methods show that our network achieves 0.01 dB, 0.11 dB, 0.13 dB and 0.4 dB better PSNR results than that of RDN+, MDSR, D-DBPN and EDSR on 8× enlargement, respectively, and also achieves favorable performance against the state-of-the-art methods on 2× and 4× enlargement. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Benchmarking; Deep learning; Feedback control; Optical resolving power; Adversarial networks; Benchmark datasets; Different resolutions; Feedback mechanisms; Feedback structure; Image super resolutions; Network structures; State-of-the-art methods; Network layers","Back-projection; Generative adversarial networks; Progressive growing; Single image super-resolution","Article","Final","","Scopus","2-s2.0-85084197926"
"Sood R.R.; Shao W.; Kunder C.; Teslovich N.C.; Wang J.B.; Soerensen S.J.C.; Madhuripan N.; Jawahar A.; Brooks J.D.; Ghanouni P.; Fan R.E.; Sonn G.A.; Rusu M.","Sood, Rewa R. (57207113139); Shao, Wei (57193026466); Kunder, Christian (35176176000); Teslovich, Nikola C. (56237761100); Wang, Jeffrey B. (57218170561); Soerensen, Simon J.C. (57218170479); Madhuripan, Nikhil (57193859425); Jawahar, Anugayathri (56566400600); Brooks, James D. (26643370000); Ghanouni, Pejman (6602684777); Fan, Richard E. (57190400724); Sonn, Geoffrey A. (8714058300); Rusu, Mirabela (25628776200)","57207113139; 57193026466; 35176176000; 56237761100; 57218170561; 57218170479; 57193859425; 56566400600; 26643370000; 6602684777; 57190400724; 8714058300; 25628776200","3D Registration of pre-surgical prostate MRI and histopathology images via super-resolution volume reconstruction","2021","Medical Image Analysis","69","","101957","","","","10.1016/j.media.2021.101957","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100438113&doi=10.1016%2fj.media.2021.101957&partnerID=40&md5=83470f8de123268d67dcea410218dcb5","The use of MRI for prostate cancer diagnosis and treatment is increasing rapidly. However, identifying the presence and extent of cancer on MRI remains challenging, leading to high variability in detection even among expert radiologists. Improvement in cancer detection on MRI is essential to reducing this variability and maximizing the clinical utility of MRI. To date, such improvement has been limited by the lack of accurately labeled MRI datasets. Data from patients who underwent radical prostatectomy enables the spatial alignment of digitized histopathology images of the resected prostate with corresponding pre-surgical MRI. This alignment facilitates the delineation of detailed cancer labels on MRI via the projection of cancer from histopathology images onto MRI. We introduce a framework that performs 3D registration of whole-mount histopathology images to pre-surgical MRI in three steps. First, we developed a novel multi-image super-resolution generative adversarial network (miSRGAN), which learns information useful for 3D registration by producing a reconstructed 3D MRI. Second, we trained the network to learn information between histopathology slices to facilitate the application of 3D registration methods. Third, we registered the reconstructed 3D histopathology volumes to the reconstructed 3D MRI, mapping the extent of cancer from histopathology images onto MRI without the need for slice-to-slice correspondence. When compared to interpolation methods, our super-resolution reconstruction resulted in the highest PSNR relative to clinical 3D MRI (32.15 dB vs 30.16 dB for BSpline interpolation). Moreover, the registration of 3D volumes reconstructed via super-resolution for both MRI and histopathology images showed the best alignment of cancer regions when compared to (1) the state-of-the-art RAPSODI approach, (2) volumes that were not reconstructed, or (3) volumes that were reconstructed using nearest neighbor, linear, or BSpline interpolations. The improved 3D alignment of histopathology images and MRI facilitates the projection of accurate cancer labels on MRI, allowing for the development of improved MRI interpretation schemes and machine learning models to automatically detect cancer on MRI. © 2021 The Author(s)","Humans; Magnetic Resonance Imaging; Male; Prostatic Neoplasms; Alignment; Diagnosis; Diseases; Image enhancement; Interpolation; Magnetic resonance imaging; Optical resolving power; Surgery; Urology; Adversarial networks; B-spline interpolations; Interpolation method; Machine learning models; Radical prostatectomy; Super resolution reconstruction; Volume reconstruction; Whole-mount histopathologies; adult; article; cancer model; cancer surgery; controlled study; histopathology; human; machine learning; male; nuclear magnetic resonance imaging; prostatectomy; radiology; surgery; writing; diagnostic imaging; prostate tumor; Image reconstruction","Generative adversarial networks; Mapping cancer from histopathology images onto MRI; Radiology pathology fusion; Super-resolution registration","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85100438113"
"Zhang H.; Zhu T.; Chen X.; Zhu L.; Jin D.; Fei P.","Zhang, Hao (57193765753); Zhu, Tingting (57193757426); Chen, Xiongchao (57219812911); Zhu, Lanxin (57205285011); Jin, Di (57192958536); Fei, Peng (57008844400)","57193765753; 57193757426; 57219812911; 57205285011; 57192958536; 57008844400","Super-resolution generative adversarial network (SRGAN) enabled on-chip contact microscopy","2021","Journal of Physics D: Applied Physics","54","39","394005","","","","10.1088/1361-6463/ac1138","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111236010&doi=10.1088%2f1361-6463%2fac1138&partnerID=40&md5=297e858e0d4755e3c39e7a8dc80af3fc","We demonstrate a deep learning based contact imaging on a CMOS chip to achieve ∼1 μm spatial resolution over a large field of view of ∼24 mm2. By using regular LED illumination, we acquire the single lower-resolution image of the objects placed approximate to the sensor with unit fringe magnification. For the raw contact-mode lens-free image, the pixel size of the sensor chip limits the spatial resolution. We apply a super-resolution generative adversarial networks, a type of deep learning based single-image super-resolution (SR) algorithm, to circumvent this limitation and effectively recover much higher resolution image of the objects, permitting sub-micron spatial resolution to be achieved across the entire sensor chip active area, which is also equivalent to the imaging field-of-view (24 mm2) due to unit magnification. This SR contact imaging approach eliminates the need of either lens or multi-frame acquisition, being very powerful and cost-effective. We demonstrate the success of this approach by imaging the proliferation dynamics of large-scale cells and the Instantaneous behaviors of freely moving Caenorhabditis elegans directly on the chip.  © 2021 IOP Publishing Ltd.","Cost effectiveness; Image resolution; Optical resolving power; Adversarial networks; Caenorhabditis elegans; Contact microscopy; Higher resolution images; Large field of views; LED illumination; Lower resolution; Spatial resolution; Deep learning","biophotonics; cell imaging; contact microscopy; optical imaging","Article","Final","","Scopus","2-s2.0-85111236010"
"Yu T.T.; Ma D.; Cole J.; Jin Ju M.; Faisal Beg M.; Sarunic M.V.","Yu, Timothy T. (57218895653); Ma, Da (55992234300); Cole, Jayden (57295344300); Jin Ju, Myeong (57210181083); Faisal Beg, Mirza (57219825192); Sarunic, Marinko V. (6508176087)","57218895653; 55992234300; 57295344300; 57210181083; 57219825192; 6508176087","Spectral bandwidth recovery of optical coherence tomography images using deep learning","2021","International Symposium on Image and Signal Processing and Analysis, ISPA","2021-September","","","67","71","4","10.1109/ISPA52656.2021.9552122","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117018082&doi=10.1109%2fISPA52656.2021.9552122&partnerID=40&md5=441dfd325fb2e0a71a0a4d45b32dea60","Optical coherence tomography (OCT) is a noninvasive imaging modality utilized by ophthalmologists to acquire volumetric data to characterize the retina, the light-sensitive tissue at the back of the eye. OCT captures cross-sectional data and is used for the screening, monitoring, and treatment planning of retinal diseases. Technological developments to increase the speed of acquisition often results in systems with narrower spectral bandwidth, and hence a lower axial resolution. Traditionally, image-processing-based techniques have been utilized to reconstruct subsampled OCT data and more recently, deep-learning-based methods have been explored. In this study, we simulate reduced axial scan (A-scan) resolution by Gaussian windowing in the spectral domain and investigate the use of a learning-based approach for image feature reconstruction. Our experiment is limited by the size of our current dataset, and we leverage techniques like transfer learning from large natural image databases and image augmentation in our implementation. In anticipation of the reduced resolution that accompanies wide-field OCT systems, we attempt to reconstruct lost features using a pixel-to-pixel approach with an altered super-resolution GAN (SRGAN) architecture. Similar techniques have been used to upscale images of lower image size and resolution in medical images like radiographs. We build upon methods of super-resolution to explore methods of better aiding clinicians in their decision-making to improve patient outcomes.  © 2021 IEEE.","Bandwidth; Decision making; Deep neural networks; Diagnosis; Image reconstruction; Large dataset; Medical imaging; Ophthalmology; Optical data processing; Optical resolving power; Optical tomography; Pixels; Vision; Volumetric analysis; Imaging modality; Non-invasive imaging; Retina; Retinal disease; Signals reconstruction; Spectral bandwidth; Superresolution; Technological development; Treatment planning; Volumetric data; Generative adversarial networks","Deep Neural Network; Generative Adversarial Network; Optical Coherence Tomography; Retina; Signal Reconstruction","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117018082"
"Li Y.; Mavromatis S.; Zhang F.; Du Z.; Sequeira J.; Wang Z.; Zhao X.; Liu R.","Li, Yadong (57226097505); Mavromatis, Sebastien (22433774300); Zhang, Feng (56434720200); Du, Zhenhong (25929119800); Sequeira, Jean (56231992200); Wang, Zhongyi (57216175636); Zhao, Xianwei (57193860330); Liu, Renyi (55809641900)","57226097505; 22433774300; 56434720200; 25929119800; 56231992200; 57216175636; 57193860330; 55809641900","Single-Image Super-Resolution for Remote Sensing Images Using a Deep Generative Adversarial Network with Local and Global Attention Mechanisms","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3093043","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110893958&doi=10.1109%2fTGRS.2021.3093043&partnerID=40&md5=9fc5b609b878823e950bf2c70f6490f1","Super-resolution (SR) technology is an important way to improve spatial resolution under the condition of sensor hardware limitations. With the development of deep learning (DL), some DL-based SR models have achieved state-of-the-art performance, especially the convolutional neural network (CNN). However, considering that remote sensing images usually contain a variety of ground scenes and objects with different scales, orientations, and spectral characteristics, previous works usually treat important and unnecessary features equally or only apply different weights in the local receptive field, which ignores long-range dependencies; it is still a challenging task to exploit features on different levels and reconstruct images with realistic details. To address these problems, an attention-based generative adversarial network (SRAGAN) is proposed in this article, which applies both local and global attention mechanisms. Specifically, we apply local attention in the SR model to focus on structural components of the earth's surface that require more attention, and global attention is used to capture long-range interdependencies in the channel and spatial dimensions to further refine details. To optimize the adversarial learning process, we also use local and global attentions in the discriminator model to enhance the discriminative ability and apply the gradient penalty in the form of hinge loss and loss function that combines $L1$ pixel loss, $L1$ perceptual loss, and relativistic adversarial loss to promote rich details. The experiments show that SRAGAN can achieve performance improvements and reconstruct better details compared with current state-of-the-art SR methods. A series of ablation investigations and model analyses validate the efficiency and effectiveness of our method. © 1980-2012 IEEE.","Convolutional neural networks; Deep learning; Learning systems; Optical resolving power; Remote sensing; Scales (weighing instruments); Adversarial learning; Adversarial networks; Discriminative ability; Long-range dependencies; Remote sensing images; Spectral characteristics; State-of-the-art performance; Structural component; artificial neural network; image resolution; remote sensing; spatial resolution; Image processing","Convolutional neural networks (CNNs); generative adversarial network (GAN); local and global attention module; remote sensing; single-image super super-resolution (SISR)","Article","Final","","Scopus","2-s2.0-85110893958"
"Wang Y.D.; Blunt M.J.; Armstrong R.T.; Mostaghimi P.","Wang, Ying Da (57207002693); Blunt, Martin J. (7005204147); Armstrong, Ryan T. (37025484700); Mostaghimi, Peyman (35759133600)","57207002693; 7005204147; 37025484700; 35759133600","Deep learning in pore scale imaging and modeling","2021","Earth-Science Reviews","215","","103555","","","","10.1016/j.earscirev.2021.103555","49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101397416&doi=10.1016%2fj.earscirev.2021.103555&partnerID=40&md5=3357a014f4be2d5c2f8752f3322019cc","Pore-scale imaging and modeling has advanced greatly through the integration of Deep Learning into the workflow, from image processing to simulating physical processes. In Digital Core Analysis, a common tool in Earth Sciences, imaging the nano- and micro-scale structure of the pore space of rocks can be enhanced past hardware limitations, while identification of minerals and phases can be automated, with reduced bias and high physical accuracy. Traditional numerical methods for estimating petrophysical parameters and simulating flow and transport can be accelerated or replaced by neural networks. Techniques and common neural network architectures used in Digital Core Analysis are described with a review of recent studies to illustrate the wide range of tasks that benefit from Deep Learning. Focus is placed on the use of Convolutional Neural Networks (CNNs) for segmentation in pore-scale imaging, the use of CNNs and Generative Adversarial Networks (GANs) in image quality enhancement and generation, and the use of Artificial Neural Networks (ANNs) and CNNs for pore-scale physics modeling. Current limitations and challenges are discussed, including advances in network implementations, applications to unconventional resources, dataset acquisition and synthetic training, extrapolative potential, accuracy loss from soft computing, and the computational cost of 3D Deep Learning. Future directions of research are also discussed, focusing on the standardization of datasets and performance metrics, integrated workflow solutions, and further studies in multiphase flow predictions, such as CO2 trapping. The use of Deep Learning at the pore-scale will likely continue becoming increasingly pervasive, as potential exists to improve all aspects of the data-driven workflow, with higher image quality, automated processing, and faster simulations. © 2021 Elsevier B.V.","accuracy assessment; artificial neural network; computer simulation; core analysis; data acquisition; Earth science; identification method; imaging method; multiphase flow; numerical model; permeability; segmentation","Deep learning; Permeability; Pore-scale; Reconstruction; Segmentation; Super resolution","Review","Final","","Scopus","2-s2.0-85101397416"
"Liu Y.; Qiao Y.; Li Y.","Liu, Ying (55976865400); Qiao, Yangge (57267115500); Li, YingHua (57191382183)","55976865400; 57267115500; 57191382183","Single Image Super Resolution Reconstruction Based on the Combination of Residual Encoding-Decoding Structure and GAN","2021","ACM International Conference Proceeding Series","","","","144","150","6","10.1145/3488933.3488989","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125863914&doi=10.1145%2f3488933.3488989&partnerID=40&md5=5cf8747c14787e37e60f0193d89db2a9","In recent years, methods based on deep neural networks have achieved excellent performance in the field of image super-resolution. In order to generate more realistic images, based on the super-resolution generative adversarial network (SRGAN), this paper proposes a single-image super-resolution reconstruction model combining residual encoding-decoding structure and generative adversarial network. To further improve the visual perception effect, the encoding-decoding module with residual groups is used as a generator network. Compared with residual blocks, the encoding-decoding module uses a larger receptive field to obtain more effective features and enhance the ability to obtain contextual information of the input image. The residual group is compromised of multiple residual channel attention modules connected with the residual of a single convolutional layer. It can refine the eigenvalues of the network layer and improve the ability to obtain high-frequency detail information in an image. Experimental results have proved that our method is better than existing ones in both objective and subjective visual effects, and the texture details of the reconstructed high resolution image are more realistic.  © 2021 ACM.","Decoding; Deep neural networks; Eigenvalues and eigenfunctions; Encoding (symbols); Image coding; Image enhancement; Image reconstruction; Network coding; Network layers; Optical resolving power; Textures; Decoding module; Encoding-decoding structure; Encoding/decoding; Image super resolutions; Image-based; Performance; Realistic images; Residual encoding; Residual group; Single-image super-resolution reconstruction; Generative adversarial networks","Encoding-Decoding structure; Generative adversarial network; Image super-resolution; Residual groups","Conference paper","Final","","Scopus","2-s2.0-85125863914"
"","","","2021 3rd International Conference on Intelligent Control, Measurement and Signal Processing and Intelligent Oil Field, ICMSP 2021","2021","2021 3rd International Conference on Intelligent Control, Measurement and Signal Processing and Intelligent Oil Field, ICMSP 2021","","","","","","502","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113951632&partnerID=40&md5=0b8e832741331a77eff247dbae0901b5","The proceedings contain 101 papers. The topics discussed include: design of measuring device bracket for distance between elevator car and shaft wall; study on evaluation model of rate grade of soil corrosion for pipeline; the design of ECG preamplifier for wearable device; a low power low noise ECG amplifier design based on OTA; investigation on the application of parabolic reflector in plasma logging source; simulation analysis of EMAT shear wave in pipeline and research of defect detection characteristics; images super-resolution using improved generative adversarial networks; research on location method based on GNSS/IMU/LIDAR multi-source information fusion; anti interference transmission algorithm of wireless communication data in the last kilometer of substation; and research on fuzzy control of centrifugal compressor anti-surge in long-distance natural gas pipeline.","","","Conference review","Final","","Scopus","2-s2.0-85113951632"
"Lin Z.; Gao Z.; Ji H.; Zhai R.; Shen X.; Mei T.","Lin, Zhipeng (57369765100); Gao, Zhi (55256514200); Ji, Hong (57205763449); Zhai, Ruifang (36618390400); Shen, Xiaoqing (57370131200); Mei, Tiancan (8914886000)","57369765100; 55256514200; 57205763449; 36618390400; 57370131200; 8914886000","Classification of cervical cells leveraging simultaneous super-resolution and ordinal regression","2022","Applied Soft Computing","115","","108208","","","","10.1016/j.asoc.2021.108208","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120992227&doi=10.1016%2fj.asoc.2021.108208&partnerID=40&md5=70d02745f104b8fd0b2015ed19043177","Automatic classification of cervical cells plays a critical role in the Computer-assisted Cytology Test (CCT) system. The efficiency of the CCT system can be promoted by sacrificing the microscopic image resolution to speed up the microscopic image acquisition. In this case, the low resolution of the cell image will severely deteriorate the performance of available Convolutional Neural Networks (CNN) based classification methods. Inspired by the positive effect of super-resolution in addressing classification or recognition tasks, we propose a cervical cell classification algorithm leveraging simultaneous super-resolution, which is achieved using Generative Adversarial Network (GAN) techniques. Our framework is designed in an end-to-end manner wherein the classification loss is back-propagated into the super-resolution network during training. Moreover, we perform ordinal regression with smooth L1 loss to further improve the classification results. Extensive experiments have verified the effectiveness of our method. Our simultaneous super-resolution based method achieves 93.5% classification accuracy on the 6-class Heer dataset, outperforming the method using only the state-of-the-art classifier by an obvious margin of 3.2%. Besides, our ordinal regression method significantly improves the MAE (Mean Absolute Error) by 0.0143 and 1-off accuracy by 0.95% on the 4-class Heer dataset. For the Herlev dataset, our method yields the classification accuracy of 98.1% and 97.6% for the 2-class and 7-class problems, which is still competitive even with low-resolution input. © 2021 Elsevier B.V.","Cells; Classification (of information); Convolutional neural networks; Cytology; Deep learning; Image resolution; Regression analysis; Sensitivity analysis; Cell classification; Cervical cell classification; Cervical cells; Computer assisted; Deep-learning; Microscopic image; Ordinal regression; Smooth l1 loss; Superresolution; Test systems; Generative adversarial networks","Cervical cell classification; Deep-learning; GAN; Ordinal regression; Smooth L1 loss; Super-resolution","Article","Final","","Scopus","2-s2.0-85120992227"
"Rafiq M.; Bajwa U.I.; Gilanie G.; Anwar W.","Rafiq, Maimoona (57222506436); Bajwa, Usama Ijaz (24490946700); Gilanie, Ghulam (55143963700); Anwar, Waqas (57206267813)","57222506436; 24490946700; 55143963700; 57206267813","Reconstruction of scene using corneal reflection","2021","Multimedia Tools and Applications","80","14","","21363","21379","16","10.1007/s11042-020-10409-3","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102947850&doi=10.1007%2fs11042-020-10409-3&partnerID=40&md5=fa92dd44dea4fe340b7c40259177311a","Corneal reflection extracted from an eye image identifies the relationship between the subject of the image and the scene in front of the subject. The reconstructed scene from corneal reflection provides detailed information about the environment opposite to the subject. It also provides scrutiny about any critical scenario, a subject is encountered with. This research area has significant applications in computer vision, human-computer interaction, psychology, and image forgery detection. Digital image processing and computer vision techniques have been used to reconstruct the scene from cornea image. The proposed model involved the following steps, i.e., identification of the corneal area in an eye, unnecessary reflection removal from cornea surface, developing eye geometric model to correct the spherical effect of the eye, and implementation of super-resolution (SR) algorithm to reconstruct the lost visual information present in the environment. The proposed study is able to reconstruct the SR scene image from cornea image. The effectiveness of the study is evaluated by using subjective as well as objective evaluation measures. Some useful insights related to cornea reflection construction have been described to make this study more effective. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.","Computer vision; Human computer interaction; Petroleum reservoir evaluation; Cornea surface; Corneal reflection; Geometric modeling; Image forgery detections; Objective evaluation; Reflection removals; Super resolution; Visual information; Image reconstruction","Cornea reflection; Generative adversarial network; Scene reconstruction; Super resolution","Article","Final","","Scopus","2-s2.0-85102947850"
"Mahanty M.; Bhattacharyya D.; Midhunchakkaravarthy D.","Mahanty, Mohan (57220483702); Bhattacharyya, Debnath (57216142572); Midhunchakkaravarthy, Divya (56380232700)","57220483702; 57216142572; 56380232700","SRGAN Assisted Encoder-Decoder Deep Neural Network for Colorectal Polyp Semantic Segmentation","2021","Revue d'Intelligence Artificielle","35","5","","395","401","6","10.18280/RIA.350505","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120419567&doi=10.18280%2fRIA.350505&partnerID=40&md5=6ef29459c6001ffdcb56ebc9e0978950","Colon cancer is thought about as the third most regularly identified cancer after Brest and lung cancer. Most colon cancers are adenocarcinomas developing from adenomatous polyps, grow on the intima of the colon. The standard procedure for polyp detection is colonoscopy, where the success of the standard colonoscopy depends on the colonoscopist experience and other environmental factors. Nonetheless, throughout colonoscopy procedures, a considerable number (8-37%) of polyps are missed due to human mistakes, and these missed polyps are the prospective reason for colorectal cancer cells. In the last few years, many research groups developed deep learning-based computer-aided (CAD) systems that recommended many techniques for automated polyp detection, localization, and segmentation. Still, accurate polyp detection, segmentation is required to minimize polyp miss out rates. This paper suggested a Super-Resolution Generative Adversarial Network (SRGAN) assisted Encoder-Decoder network for fully automated colon polyp segmentation from colonoscopic images. The proposed deep learning model incorporates the SRGAN in the up-sampling process to achieve more accurate polyp segmentation. We examined our model on the publicly available benchmark datasets CVC-ColonDB and Warwick- QU. The model accomplished a dice score of 0.948 on the CVC-ColonDB dataset, surpassed the recently advanced state-of-the-art (SOTA) techniques. When it is evaluated on the Warwick-QU dataset, it attains a Dice Score of 0.936 on part A and 0.895 on Part B. Our model showed more accurate results for sessile and smaller-sized polyps. © 2021 Lavoisier. All rights reserved.","","Colonoscopy; Colorectal polyp segmentation; Computer-aided diagnosis (CAD); Deep convolutional neural network; SRGAN","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85120419567"
"Lyu Q.; Shan H.; Xie Y.; Kwan A.C.; Otaki Y.; Kuronuma K.; Li D.; Wang G.","Lyu, Qing (57213422866); Shan, Hongming (57191481929); Xie, Yibin (56019417700); Kwan, Alan C. (55618998300); Otaki, Yuka (43361672400); Kuronuma, Keiichiro (57132788500); Li, Debiao (35262251100); Wang, Ge (7407148134)","57213422866; 57191481929; 56019417700; 55618998300; 43361672400; 57132788500; 35262251100; 7407148134","Cine Cardiac MRI Motion Artifact Reduction Using a Recurrent Neural Network","2021","IEEE Transactions on Medical Imaging","40","8","9405626","2170","2181","11","10.1109/TMI.2021.3073381","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104581488&doi=10.1109%2fTMI.2021.3073381&partnerID=40&md5=65429c05b7fd67ddd825ddc3b5a541fb","Cine cardiac magnetic resonance imaging (MRI) is widely used for the diagnosis of cardiac diseases thanks to its ability to present cardiovascular features in excellent contrast. As compared to computed tomography (CT), MRI, however, requires a long scan time, which inevitably induces motion artifacts and causes patients' discomfort. Thus, there has been a strong clinical motivation to develop techniques to reduce both the scan time and motion artifacts. Given its successful applications in other medical imaging tasks such as MRI super-resolution and CT metal artifact reduction, deep learning is a promising approach for cardiac MRI motion artifact reduction. In this paper, we propose a novel recurrent generative adversarial network model for cardiac MRI motion artifact reduction. This model utilizes bi-directional convolutional long short-term memory (ConvLSTM) and multi-scale convolutions to improve the performance of the proposed network, in which bi-directional ConvLSTMs handle long-range temporal features while multi-scale convolutions gather both local and global features. We demonstrate a decent generalizability of the proposed method thanks to the novel architecture of our deep network that captures the essential relationship of cardiovascular dynamics. Indeed, our extensive experiments show that our method achieves better image quality for cine cardiac MRI images than existing state-of-the-art methods. In addition, our method can generate reliable missing intermediate frames based on their adjacent frames, improving the temporal resolution of cine cardiac MRI sequences.  © 1982-2012 IEEE.","Artifacts; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Magnetic Resonance Imaging, Cine; Motion; Neural Networks, Computer; Computerized tomography; Convolution; Diagnosis; Heart; Magnetic resonance imaging; Medical imaging; Adversarial networks; Cardiac magnetic resonance imaging; Cardiovascular dynamics; Metal artifact reduction; Motion artifact reduction; Novel architecture; State-of-the-art methods; Temporal resolution; Article; artifact reduction; cardiovascular magnetic resonance; cine magnetic resonance imaging; computer assisted tomography; controlled study; convolution algorithm; deep learning; double blind procedure; feasibility study; feature extraction; human; image quality; image reconstruction; major clinical study; motion; nerve cell network; recurrent neural network; short term memory; artifact; cine magnetic resonance imaging; image processing; nuclear magnetic resonance imaging; Recurrent neural networks","Cardiac magnetic resonance imaging (MRI); deep learning; fast MRI; motion artifact reduction; recurrent neural network","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104581488"
"Liu Z.-S.; Siu W.-C.; Chan Y.-L.","Liu, Zhi-Song (57191052009); Siu, Wan-Chi (57202950625); Chan, Yui-Lam (7403675929)","57191052009; 57202950625; 7403675929","Photo-realistic image super-resolution via variational autoencoders","2021","IEEE Transactions on Circuits and Systems for Video Technology","31","4","9121327","1351","1365","14","10.1109/TCSVT.2020.3003832","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098788742&doi=10.1109%2fTCSVT.2020.3003832&partnerID=40&md5=3cd0f3a5e48fd61467a03c295c1baaf0","There is a great leap in objective accuracy on image super-resolution, which recently brings a new challenge on image super-resolution with larger up-scaling (e.g. × ) using pixel based distortion for measurement. This causes over-smooth effect which cannot grasp well the perceptual similarity. The advent of generative adversarial networks makes it possible super-resolve a low-resolution image to generate photo-realistic images sharing distribution with the high-resolution images. However, generative networks suffer from problems of mode-collapse and unrealistic sample generation. We propose to perform Image Super-Resolution via Variational AutoEncoders (SR-VAE) learning according to the conditional distribution of the high-resolution images induced by the low-resolution images. Given that the Conditional Variational Autoencoders tend to generate blur images, we add the conditional sampling mechanism to narrow down the latent subspace for reconstruction. To evaluate the model generalization, we use KL loss to measure the divergence between latent vectors and standard Gaussian distribution. Eventually, in order to balance the trade-off between super-resolution distortion and perception, not only that we use pixel based loss, we also use the modified deep feature loss between SR and HR images to estimate the reconstruction. In experiments, we evaluated a large number of datasets to make comparison with other state-of-the-art super-resolution approaches. Results on both objective and subjective measurements show that our proposed SR-VAE can achieve good photo-realistic perceptual quality closer to the natural image manifold while maintain low distortion. © 1991-2012 IEEE.","Arts computing; Economic and social effects; Large dataset; Learning systems; Optical resolving power; Pixels; Conditional distribution; High resolution image; Image super resolutions; Low resolution images; Model generalization; Objective and subjective measurements; Perceptual similarity; Photorealistic images; Image reconstruction","distortion; divergence; Image super-resolution; variational autoencoders","Article","Final","","Scopus","2-s2.0-85098788742"
"Wang S.; Guo J.; Zhang Y.; Hu Y.; Ding C.; Wu Y.","Wang, Shihong (57221762372); Guo, Jiayi (57194143247); Zhang, Yueting (57218470544); Hu, Yuxin (56163045500); Ding, Chibiao (7202622015); Wu, Yirong (8403430500)","57221762372; 57194143247; 57218470544; 56163045500; 7202622015; 8403430500","Tomosar 3d reconstruction for buildings using very few tracks of observation: A conditional generative adversarial network approach","2021","Remote Sensing","13","24","5055","","","","10.3390/rs13245055","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121325517&doi=10.3390%2frs13245055&partnerID=40&md5=03418890e0662059d76296bc9ae162f0","SAR tomography (TomoSAR) is an important technology for three-dimensional (3D) reconstruction of buildings through multiple coherent SAR images. In order to obtain sufficient signal-to-noise ratio (SNR), typical TomoSAR applications often require dozens of scenes of SAR images. However, limited by time and cost, the available SAR images are often only 3–5 scenes in practice, which makes the traditional TomoSAR technique unable to produce satisfactory SNR and elevation resolution. To tackle this problem, the conditional generative adversarial network (CGAN) is proposed to improve the TomoSAR 3D reconstruction by learning the prior information of building. Moreover, the number of tracks required can be reduced to three. Firstly, a TomoSAR 3D super-resolution dataset is constructed using high-quality data from the airborne array and low-quality data obtained from a small amount of tracks sampled from all observations. Then, the CGAN model is trained to estimate the corresponding high-quality result from the low-quality input. Airborne data experiments prove that the reconstruction results are improved in areas with and without overlap, both qualitatively and quantitatively. Furthermore, the network pretrained on the airborne dataset is directly used to process the spaceborne dataset without any tuning, and generates satisfactory results, proving the effectiveness and robustness of our method. The comparative experiment with nonlocal algorithm also shows that the proposed method has better height estimation and higher time efficiency. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Image reconstruction; Radar imaging; Signal to noise ratio; Synthetic aperture radar; Tomography; 3D reconstruction; Building 3d reconstruction; Conditional generative adversarial network; High quality data; Prior information; SAR Images; Superresolution; Three-dimensional (3-D) reconstruction; TomoSAR; Very few track; Generative adversarial networks","Building 3D reconstruction; Conditional generative adversarial network; TomoSAR; Very few tracks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121325517"
"Varma K.; Reddy G.S.; Subramanyam N.","Varma, Kavya (57216789277); Reddy, G. Satyabhama (57346903100); Subramanyam, Natarajan (35108329500)","57216789277; 57346903100; 35108329500","Face Image Super Resolution using a Generative Adversarial Network","2021","Proceedings - 1st International Conference on Smart Technologies Communication and Robotics, STCR 2021","","","","","","","10.1109/STCR51658.2021.9588816","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119398888&doi=10.1109%2fSTCR51658.2021.9588816&partnerID=40&md5=d08667ef0c501992aa0009fcceb44efb","Traditional image super resolution centered around purely mathematical models are capable of creating gradient based textures, but fail to render the specific lineaments that would be expected in a realistically upscaled image. This is especially problematic in scenarios involving images of subjects whose recognition is reliant on the presence of specific characteristics, for example, faces. In this paper, we describe a deep learning model that is capable of generating an 8x upscaled photo from a low resolution image of a face. The underlying model is based on the SRGAN architecture that deviates from the conventional GAN approach of the adversarial back and forth between generator and discriminator by incorporating an added content loss whose value is dependent on the detection of the natural features in the generated image by a pre-trained VGG model. The model is trained on the Celeb Faces Attributes dataset with over 1,00,000 data points and can produce upscaled images that are realistic with a coherent presence of the natural attributes of a face. © 2021 IEEE.","Computer vision; Deep learning; Image enhancement; Optical resolving power; Textures; Datapoints; Face images; Facial enhancement; Gradient based; Image super resolutions; Learning models; Low resolution images; Natural features; SRGAN; Generative adversarial networks","Facial Enhancement; Image Super Resolution; SRGAN","Conference paper","Final","","Scopus","2-s2.0-85119398888"
"Lahiri A.; Bairagya S.; Bera S.; Haldar S.; Biswas P.K.","Lahiri, Avisek (56572183500); Bairagya, Sourav (57200148059); Bera, Sutanu (57204668566); Haldar, Siddhant (57205610222); Biswas, Prabir Kumar (7202443668)","56572183500; 57200148059; 57204668566; 57205610222; 7202443668","Lightweight modules for efficient deep learning based image restoration","2021","IEEE Transactions on Circuits and Systems for Video Technology","31","4","9134805","1395","1410","15","10.1109/TCSVT.2020.3007723","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103938138&doi=10.1109%2fTCSVT.2020.3007723&partnerID=40&md5=25f8cf6decb30693e19f3d275337ce67","Low level image restoration is an integral component of modern artificial intelligence (AI) driven camera pipelines. Most of these frameworks are based on deep neural networks which present a massive computational overhead on resource constrained platform like a mobile phone. In this paper, we propose several lightweight low-level modules which can be used to create a computationally low cost variant of a given baseline model. Recent works for efficient neural networks design have mainly focused on classification. However, low-level image processing falls under the 'image-to-image' translation genre which requires some additional computational modules not present in classification. This paper seeks to bridge this gap by designing generic efficient modules which can replace essential components used in contemporary deep learning based image restoration networks. We also present and analyse our results highlighting the drawbacks of applying depthwise separable convolutional kernel (a popular method for efficient classification network) for sub-pixel convolution based upsampling (a popular upsampling strategy for low-level vision applications). This shows that concepts from domain of classification cannot always be seamlessly integrated into 'image-to-image' translation tasks. We extensively validate our findings on three popular tasks of image inpainting, denoising and super-resolution. Our results show that proposed networks consistently output visually similar reconstructions compared to full capacity baselines with significant reduction of parameters, memory footprint and execution speeds on contemporary mobile devices.  © 1991-2012 IEEE.","Convolution; Deep neural networks; Image reconstruction; Neural networks; Restoration; Signal sampling; Classification networks; Computational overheads; Convolutional kernel; Integral components; Low level image processing; Reduction of parameters; Restoration network; Super resolution; Deep learning","adversarial learning; CNN; efficient neural networks; generative adversarial network (GAN); Image denoising; image inpainting; image super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85103938138"
"Cao H.; Mi S.","Cao, Hangpu (57223810545); Mi, Sicheng (57223806629)","57223810545; 57223806629","Weighted SRGAN and Reconstruction Loss Analysis for Accurate Image Super Resolution","2021","Journal of Physics: Conference Series","1903","1","012050","","","","10.1088/1742-6596/1903/1/012050","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106203233&doi=10.1088%2f1742-6596%2f1903%2f1%2f012050&partnerID=40&md5=fb3e80068876e2ff888bef9d5174b208","Super resolution (SR) image generation is to generate a high-resolution image from a given low-resolution image, which can be used in numerous vision tasks, such as small object detection and specific image processing. Generative adversarial network for super resolution (SRGAN) is the mainstream framework in SR task, which utilizes two reconstruction losses, including MSE loss and VGG loss. However, the influence of these losses on model learning is not examined. In this paper, the importance of MSE and VGG losses is analyzed. The loss function of traditional SRGAN is improved, and the weights of MSE and VGG losses are set manually. The weights range from 0 to 1, and the sampling interval is 0.1. Furthermore, a learnable parameter to dynamically adjust the two weights is proposed. Experiments on the datasets, including Set5, Set14, BAS100, and Urban100, show that our method is capable of generating much better images than SRGAN, with higher values of PSNR and SSIM. It is found that the MSE loss makes a greater contribution to learning the discriminative model and the VGG loss plays a supplementary role. Our WSRGAN can apply to most SRGAN-based methods to improve their accuracy. © Published under licence by IOP Publishing Ltd.","Image reconstruction; Intelligent computing; Object detection; Optical resolving power; Adversarial networks; Discriminative models; High resolution image; Image generations; Image super resolutions; Low resolution images; Sampling interval; Small object detection; Learning systems","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85106203233"
"Liu B.; Zhao L.; Li J.; Zhao H.; Liu W.; Li Y.; Wang Y.; Chen H.; Cao W.","Liu, Baodi (16319146900); Zhao, Lifei (57274635800); Li, Jiaoyue (57299552400); Zhao, Hengle (57376940600); Liu, Weifeng (36739405100); Li, Ye (35770968700); Wang, Yanjiang (57223714000); Chen, Honglong (24174425300); Cao, Weijia (55558020600)","16319146900; 57274635800; 57299552400; 57376940600; 36739405100; 35770968700; 57223714000; 24174425300; 55558020600","Saliency-guided remote sensing image super-resolution","2021","Remote Sensing","13","24","5144","","","","10.3390/rs13245144","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121442780&doi=10.3390%2frs13245144&partnerID=40&md5=fca1807ae0681758c0b7de7375f163af","Deep learning has recently attracted extensive attention and developed significantly in remote sensing image super-resolution. Although remote sensing images are composed of various scenes, most existing methods consider each part equally. These methods ignore the salient objects (e.g., buildings, airplanes, and vehicles) that have more complex structures and require more attention in recovery processing. This paper proposes a saliency-guided remote sensing image super-resolution (SG-GAN) method to alleviate the above issue while maintaining the merits of GAN-based methods for the generation of perceptual-pleasant details. More specifically, we exploit the salient maps of images to guide the recovery in two aspects: On the one hand, the saliency detection network in SG-GAN learns more high-resolution saliency maps to provide additional structure priors. On the other hand, the well-designed saliency loss imposes a second-order restriction on the super-resolution process, which helps SG-GAN concentrate more on the salient objects of remote sensing images. Experimental results show that SG-GAN achieves competitive PSNR and SSIM compared with the advanced super-resolution methods. Visual results demonstrate our superiority in restoring structures while generating remote sensing super-resolution images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Computer system recovery; Deep learning; Object detection; Optical resolving power; Remote sensing; Complexes structure; Detection networks; Image super resolutions; Learn+; Remote sensing images; Saliency detection; Salient maps; Salient object detection; Salient objects; Superresolution; Generative adversarial networks","Generative adversarial network; Image super-resolution; Remote sensing image; Salient object detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121442780"
"Yang X.; Wang X.; Wang N.; Gao X.","Yang, Xi (56124410500); Wang, Xiaoqi (57222165966); Wang, Nannan (55694111900); Gao, Xinbo (7403873424)","56124410500; 57222165966; 55694111900; 7403873424","SRDN: A Unified Super-Resolution and Motion Deblurring Network for Space Image Restoration","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3131264","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120576985&doi=10.1109%2fTGRS.2021.3131264&partnerID=40&md5=d845fa7737fda0773d30b18e5c4d92c5","Space target super-resolution (SR) is a domain-specific single image SR problem aiming to help distinguish the satellite and spacecrafts from numerous space debris. Compared to the other object SR problem, images for space target are always in low quality with varies of degradation condition, as a result of long distance and motion blur, which significantly reduces the manual classification reliability, especially for these small targets, e.g., satellite payloads. To address this challenge, we present an end-to-end SR and deblurring network (SRDN). Concretely, focusing on the low-resolution (LR) space target images with blind motion blur, we integrate the SR and deblur function together, improving the image quality by a unified generative adversarial network (GAN)-based framework. We implement a deblur module by using contrastive learning to extract degradation feature and add symmetrical downsampling and upsampling modules to the SR network in order to restore texture information, while shortcut connections are redesigned to maintain the global similarity. Extensive experiments on the public satellite dataset, BUAA-SID-share1.5, demonstrate that our network outperforms the state-of-the-art SR and deblur methods. © 1980-2012 IEEE.","Generative adversarial networks; Image denoising; Image enhancement; Image reconstruction; Image resolution; Restoration; Satellites; Deblurring; Domain specific; Features extraction; High-resolution imaging; Motion blur; Motion deblurring; Space image; Space targets; Superresolution; Target recognition; algorithm; artificial neural network; data set; detection method; image resolution; satellite data; Neural networks","Artificial intelligence; artificial neural networks; high-resolution (HR) imaging; image denoising","Article","Final","","Scopus","2-s2.0-85120576985"
"Dong R.; Zhang L.; Fu H.","Dong, Runmin (57205415789); Zhang, Lixian (57207392945); Fu, Haohuan (8713118400)","57205415789; 57207392945; 8713118400","RRSGAN: Reference-Based Super-Resolution for Remote Sensing Image","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2020.3046045","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099726810&doi=10.1109%2fTGRS.2020.3046045&partnerID=40&md5=9011834a954982a4325984a5efa39e97","Remote sensing image super-resolution (SR) plays an important role by supplementing the lack of original high-resolution (HR) images in the study scenarios of large spatial areas or long time series. However, due to the lack of imagery information in low-resolution (LR) images, single-image super-resolution (SISR) is an inherently ill-posed problem. Especially, it is difficult to reconstruct the fine textures of HR images at large upscaling factors (e.g., four times). In this work, based on Google Earth HR images, we explore the potential of the reference-based super-resolution (RefSR) method on remote sensing images, utilizing rich texture information from HR reference (Ref) images to reconstruct the details in LR images. This method can use existing HR images to help reconstruct the LR images of long time series or a specific time. We build a reference-based remote sensing SR data set (RRSSRD). Furthermore, by adopting the generative adversarial network (GAN), we propose a novel end-to-end reference-based remote sensing GAN (RRSGAN) for SR. RRSGAN can extract the Ref features and align them to the LR features. Eventually, the texture information in the Ref features can be transferred to the reconstructed HR images. In contrast to the existing RefSR methods, we propose a gradient-assisted feature alignment method that adopts the deformable convolutions to align the Ref and LR features and a relevance attention module (RAM) to improve the robustness of the model in different scenarios (e.g., land cover changes and cloud coverage). The experimental results demonstrate that RRSGAN is robust and outperforms the state-of-the-art SISR and RefSR methods in both quantitative evaluation and visual results, which indicates the great potential of the RefSR method for remote sensing tasks.  © 1980-2012 IEEE.","Image reconstruction; Image texture; Optical resolving power; Textures; Time series; Adversarial networks; High resolution image; Ill posed problem; Land-cover change; Low resolution images; Quantitative evaluation; Remote sensing images; Texture information; image resolution; remote sensing; satellite imagery; Remote sensing","Deep learning; Remote sensing imagery; Super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85099726810"
"Chen J.; Zhang Y.; Hu X.; Chen C.Y.-C.","Chen, Jianqiang (57208215976); Zhang, Yali (56600941000); Hu, Xiang (57226041144); Chen, Calvin Yu-Chian (7501963316)","57208215976; 56600941000; 57226041144; 7501963316","Cascading residual–residual attention generative adversarial network for image super resolution","2021","Soft Computing","25","14","","9651","9662","11","10.1007/s00500-021-05730-4","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103413979&doi=10.1007%2fs00500-021-05730-4&partnerID=40&md5=4990ea34458285cb7e2266950e5a6e99","Image super resolution technology plays an important role in the field of computer vision. With the application of deep learning in the field of image super-resolution, the generative adversarial network is applied to image super-resolution and obtains images with great quality. In this paper, we propose a novel generative adversarial network structure called Cascading Residual–Residual Attention Generative Adversarial Network (CRRAGAN). First, this paper proposes a novel and efficient feature extraction module: Cascading Residual–Residual Block, which can extract multi-scale information and low-level cascade information to high-level information. CRRAGAN directly uses the channel attention module to capture low-resolution image key information and fuse it into the next stage feature. Second, a new loss combination function is proposed, a weighted sum of image loss, adversarial loss, perceptual loss, and charbonnier loss, to make the network training more stable. In the end, we compare our proposed method with 15 previous state-of-the-art methods and discuss the performance of different training datasets. Experimental results demonstrate that our model exhibits improved performance. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Adversarial networks; High-level information; Image super resolutions; Low resolution images; Multi-scale informations; Network training; State-of-the-art methods; Training data sets; Optical resolving power","Cascading residual–residual block; Deep learning; Generative adversarial network; Image super resolution","Article","Final","","Scopus","2-s2.0-85103413979"
"Sun Y.; Li L.; Ding Y.; Bai J.; Xin X.","Sun, Ying (57223219687); Li, Lang (57346789100); Ding, Yang (57346789200); Bai, Jiabao (57346627100); Xin, Xiangning (57347110700)","57223219687; 57346789100; 57346789200; 57346627100; 57347110700","Image compression algorithm based on variational autoencoder","2021","Journal of Physics: Conference Series","2066","1","012008","","","","10.1088/1742-6596/2066/1/012008","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119453982&doi=10.1088%2f1742-6596%2f2066%2f1%2f012008&partnerID=40&md5=4560cd6972c8cffecafed93aaf4adf08","Variational Autoencoder (VAE), as a kind of deep hidden space generation model, has achieved great success in performance in recent years, especially in image generation. This paper aims to study image compression algorithms based on variational autoencoders. This experiment uses the image quality evaluation measurement model, because the image super-resolution algorithm based on interpolation is the most direct and simple method to change the image resolution. In the experiment, the first step of the whole picture is transformed by the variational autoencoder, and then the actual coding is applied to the complete coefficient. Experimental data shows that after encoding using the improved encoding method of the variational autoencoder, the number of bits required for the encoding symbol stream required for transmission or storage in the traditional encoding method is greatly reduced, and symbol redundancy is effectively avoided. The experimental results show that the image research algorithm using variational autoencoder for image 1, image 2, and image 3 reduces the time by 3332, 2637, and 1470 bit respectively compared with the traditional image research algorithm of self-encoding. In the future, people will introduce deep convolutional neural networks to optimize the generative adversarial network, so that the generative adversarial network can obtain better convergence speed and model stability. © 2021 Institute of Physics Publishing. All rights reserved.","Convolutional neural networks; Digital storage; Encoding (symbols); Generative adversarial networks; Image compression; Image resolution; Signal encoding; Auto encoders; Encoding methods; Image compression algorithms; Image generations; Image quality evaluation; Performance; Space generation; Traditional autoencoder; Variational autoencoder; Deep neural networks","Image Compression Algorithm; Traditional Autoencoder; Variational Autoencoder","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119453982"
"Streli P.; Holz C.","Streli, Paul (57223400778); Holz, Christian (56070704000)","57223400778; 56070704000","CapContact: Super-resolution Contact Areas from Capacitive Touchscreens","2021","Conference on Human Factors in Computing Systems - Proceedings","","","","","","","10.1145/3411764.3445621","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112126475&doi=10.1145%2f3411764.3445621&partnerID=40&md5=98f8363589a13f34eead1dfbe0529c40","Touch input is dominantly detected using mutual-capacitance sensing, which measures the proximity of close-by objects that change the electric feld between the sensor lines. The exponential dropof in intensities with growing distance enables software to detect touch events, but does not reveal true contact areas. In this paper, we introduce CapContact, a novel method to precisely infer the contact area between the user's fnger and the surface from a single capacitive image. At 8 super-resolution, our convolutional neural network generates refned touch masks from 16-bit capacitive images as input, which can even discriminate adjacent touches that are not distinguishable with existing methods. We trained and evaluated our method using supervised learning on data from 10 participants who performed touch gestures. Our capture apparatus integrates optical touch sensing to obtain ground-truth contact through high-resolution frustrated total internal refection.We compare our method with a baseline using bicubic upsampling as well as the ground truth from FTIR images. We separately evaluate our method's performance in discriminating adjacent touches. CapContact successfully separated closely adjacent touch contacts in 494 of 570 cases (87%) compared to the baseline's 43 of 570 cases (8%). Importantly, we demonstrate that our method accurately performs even at half of the sensing resolution at twice the grid-line pitch across the same surface area, challenging the current industrywide standard of a ~4mm sensing pitch. We conclude this paper with implications for capacitive touch sensing in general and for touch-input accuracy in particular. © 2021 ACM.","Convolutional neural networks; Electric lines; Human engineering; Capacitive touch sensing; Contact areas; High resolution; Mutual capacitance; Sensing resolution; Super resolution; Touch sensing; True contact area; Capacitive sensors","Accuracy; Capacitive sensing; Contact area; Generative adversarial networks; Super-resolution; Touch input","Conference paper","Final","","Scopus","2-s2.0-85112126475"
"Huang C.-E.; Li Y.-H.; Aslam M.S.; Chang C.-C.","Huang, Chi-En (57245274500); Li, Yung-Hui (14121385000); Aslam, Muhammad Saqlain (57209266673); Chang, Ching-Chun (56056418200)","57245274500; 14121385000; 57209266673; 56056418200","Super-resolution generative adversarial network based on the dual dimension attention mechanism for biometric image super-resolution","2021","Sensors","21","23","7817","","","","10.3390/s21237817","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119703438&doi=10.3390%2fs21237817&partnerID=40&md5=e1ecc11c59ca53dae810921c4f0769ae","There exist many types of intelligent security sensors in the environment of the Internet of Things (IoT) and cloud computing. Among them, the sensor for biometrics is one of the most important types. Biometric sensors capture the physiological or behavioral features of a person, which can be further processed with cloud computing to verify or identify the user. However, a low-resolution (LR) biometrics image causes the loss of feature details and reduces the recognition rate hugely. Moreover, the lack of resolution negatively affects the performance of image-based biometric technology. From a practical perspective, most of the IoT devices suffer from hardware constraints and the low-cost equipment may not be able to meet various requirements, particularly for image resolution, because it asks for additional storage to store high-resolution (HR) images, and a high bandwidth to transmit the HR image. Therefore, how to achieve high accuracy for the biometric system without using expensive and high-cost image sensors is an interesting and valua-ble issue in the field of intelligent security sensors. In this paper, we proposed DDA-SRGAN, which is a generative adversarial network (GAN)-based super-resolution (SR) framework using the dual-dimension attention mechanism. The proposed model can be trained to discover the regions of interest (ROI) automatically in the LR images without any given prior knowledge. The experiments were performed on the CASIA-Thousand-v4 and the CelebA datasets. The experimental results show that the proposed method is able to learn the details of features in crucial regions and achieve better performance in most cases. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Biometry; Humans; Image Processing, Computer-Assisted; Research Design; Biometrics; Cloud computing; Costs; Image resolution; Internet of things; Attention mechanisms; Biometric image; Biometric recognition; Cloud-computing; High-resolution images; Intelligent security; Network-based; Performance; Security sensors; Superresolution; biometry; human; image processing; methodology; Generative adversarial networks","Attention mechanism; Biometric recognition; Generative adversarial network; Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85119703438"
"Cao J.; Jia Y.; Yan M.; Tian X.","Cao, Jianfang (36175317900); Jia, Yiming (57226255807); Yan, Minmin (57226240994); Tian, Xiaodong (57226413345)","36175317900; 57226255807; 57226240994; 57226413345","Superresolution reconstruction method for ancient murals based on the stable enhanced generative adversarial network","2021","Eurasip Journal on Image and Video Processing","2021","1","28","","","","10.1186/s13640-021-00569-z","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111543985&doi=10.1186%2fs13640-021-00569-z&partnerID=40&md5=6358a083330ac2ddc4a5d49625c53871","A stable enhanced superresolution generative adversarial network (SESRGAN) algorithm was proposed in this study to address the low-resolution and blurred texture details in ancient murals. This algorithm makes improvements on the basis of GANs, which use dense residual blocks to extract image features. After two upsampling steps, the feature information of the image is input into the high-resolution (HR) image space to realize an improvement in resolution, and the reconstructed HR image is finally generated. The discriminator network uses VGG as its basic framework to judge the authenticity of the input image. This study further optimized the details of the network model. In addition, three loss optimization models, i.e., the perceptual loss, content loss, and adversarial loss models, were integrated into the proposed algorithm. The Wasserstein GAN-gradient penalty (WGAN-GP) theory was used to optimize the adversarial loss of the model when calculating the perceptual loss and when using the preactivation feature information for calculation purposes. In addition, public data sets were used to pretrain the generative network model to achieve a high-quality initialization. The simulation experiment results showed that the proposed algorithm outperforms other related superresolution algorithms in terms of both objective and subjective evaluation indicators. A subjective perception evaluation was also conducted, and the reconstructed images produced by our algorithm were more in line with the general public’s visual perception than those produced by the other compared algorithms. © 2021, The Author(s).","Image reconstruction; Optical resolving power; Textures; Adversarial networks; Feature information; High resolution image; Objective and subjective evaluations; Reconstructed image; Subjective perceptions; Super resolution algorithms; Super-resolution reconstruction; Image enhancement","Dense residual block; Generative adversarial networks; Superresolution reconstruction of murals; WGAN-GP","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111543985"
"Wang H.; Wang J.; Bai K.; Sun Y.","Wang, Hongfeng (57208759829); Wang, Jianzhong (55972836500); Bai, Kemeng (57219715800); Sun, Yong (57222614160)","57208759829; 55972836500; 57219715800; 57222614160","Centered multi-task generative adversarial network for small object detection","2021","Sensors","21","15","5194","","","","10.3390/s21155194","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111604704&doi=10.3390%2fs21155194&partnerID=40&md5=f005a27e142c92e560825f477addc314","Despite the breakthroughs in accuracy and efficiency of object detection using deep neural networks, the performance of small object detection is far from satisfactory. Gaze estimation has developed significantly due to the development of visual sensors. Combining object detection with gaze estimation can significantly improve the performance of small object detection. This paper presents a centered multi-task generative adversarial network (CMTGAN), which combines small object detection and gaze estimation. To achieve this, we propose a generative adversarial network (GAN) capable of image super-resolution and two-stage small object detection. We exploit a generator in CMTGAN for image super-resolution and a discriminator for object detection. We introduce an artificial texture loss into the generator to retain the original feature of small objects. We also use a centered mask in the generator to make the network focus on the central part of images where small objects are more likely to appear in our method. We propose a discriminator with detection loss for two-stage small object detection, which can be adapted to other GANs for object detection. Compared with existing interpolation methods, the super-resolution images generated by CMTGAN are more explicit and contain more information. Experiments show that our method exhibits a better detection performance than mainstream methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Adaptation, Physiological; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Research Design; Deep neural networks; Object recognition; Optical resolving power; Textures; Adversarial networks; Detection loss; Detection performance; Gaze estimation; Image super resolutions; Interpolation method; Small object detection; Super resolution; adaptation; human; image processing; methodology; Object detection","Generative adversarial network; Image super-resolution; Two-stage small object detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111604704"
"Cai Q.; Li J.; Li H.; Yang Y.-H.; Wu F.; Zhang D.","Cai, Qing (57476388900); Li, Jinxing (57191886654); Li, Huafeng (35302951000); Yang, Yee-Hong (57196350724); Wu, Feng (57216133533); Zhang, David (57211730986)","57476388900; 57191886654; 35302951000; 57196350724; 57216133533; 57211730986","TDPN: Texture and Detail-Preserving Network for Single Image Super-Resolution","2022","IEEE Transactions on Image Processing","31","","","2375","2389","14","10.1109/TIP.2022.3154614","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125720641&doi=10.1109%2fTIP.2022.3154614&partnerID=40&md5=fc31c4f83af887bbab4c4a07d0041f18","Single image super-resolution (SISR) using deep convolutional neural networks (CNNs) achieves the state-of-the-art performance. Most existing SISR models mainly focus on pursuing high peak signal-to-noise ratio (PSNR) and neglect textures and details. As a result, the recovered images are often perceptually unpleasant. To address this issue, in this paper, we propose a texture and detail-preserving network (TDPN), which focuses not only on local region feature recovery but also on preserving textures and details. Specifically, the high-resolution image is recovered from its corresponding low-resolution input in two branches. First, a multi-reception field based branch is designed to let the network fully learn local region features by adaptively selecting local region features in different reception fields. Then, a texture and detail-learning branch supervised by the textures and details decomposed from the ground-truth high resolution image is proposed to provide additional textures and details for the super-resolution process to improve the perceptual quality. Finally, we introduce a gradient loss into the SISR field and define a novel hybrid loss to strengthen boundary information recovery and to avoid overly smooth boundary in the final recovered high-resolution image caused by using only the MAE loss. More importantly, the proposed method is model-agnostic, which can be applied to most off-the-shelf SISR networks. The experimental results on public datasets demonstrate the superiority of our TDPN on most state-of-the-art SISR methods in PSNR, SSIM and perceptual quality. We will share our code on https://github.com/tocaiqing/TDPN. © 1992-2012 IEEE.","Computer system recovery; Convolution; Deep neural networks; Image enhancement; Image texture; Optical resolving power; Recovery; Signal to noise ratio; Textures; Convolutional neural network; Detail preserving; Features extraction; High-resolution images; Image super resolutions; Local region; Region feature; Single images; Superresolution; Texture preserving; Agnostic; article; decomposition; human; learning; receptive field; signal noise ratio; Generative adversarial networks","convolutional neural network (CNN); multi-branch network; multi-reception field module; Single image super-resolution (SISR); texture and detail-preserving network (TDPN)","Article","Final","","Scopus","2-s2.0-85125720641"
"de Farias E.C.; di Noia C.; Han C.; Sala E.; Castelli M.; Rundo L.","de Farias, Erick Costa (57322476100); di Noia, Christian (57251023500); Han, Changhee (57192821057); Sala, Evis (7004450805); Castelli, Mauro (36052209100); Rundo, Leonardo (56624605200)","57322476100; 57251023500; 57192821057; 7004450805; 36052209100; 56624605200","Impact of GAN-based lesion-focused medical image super-resolution on the robustness of radiomic features","2021","Scientific Reports","11","1","21361","","","","10.1038/s41598-021-00898-z","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118440548&doi=10.1038%2fs41598-021-00898-z&partnerID=40&md5=5ea1ed537b1b8f72562628199797a198","Robust machine learning models based on radiomic features might allow for accurate diagnosis, prognosis, and medical decision-making. Unfortunately, the lack of standardized radiomic feature extraction has hampered their clinical use. Since the radiomic features tend to be affected by low voxel statistics in regions of interest, increasing the sample size would improve their robustness in clinical studies. Therefore, we propose a Generative Adversarial Network (GAN)-based lesion-focused framework for Computed Tomography (CT) image Super-Resolution (SR); for the lesion (i.e., cancer) patch-focused training, we incorporate Spatial Pyramid Pooling (SPP) into GAN-Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE). At 2 × SR, the proposed model achieved better perceptual quality with less blurring than the other considered state-of-the-art SR methods, while producing comparable results at 4 × SR. We also evaluated the robustness of our model’s radiomic feature in terms of quantization on a different lung cancer CT dataset using Principal Component Analysis (PCA). Intriguingly, the most important radiomic features in our PCA-based analysis were the most robust features extracted on the GAN-super-resolved images. These achievements pave the way for the application of GAN-based image Super-Resolution techniques for studies of radiomics for robust biomarker discovery. © 2021, The Author(s).","Algorithms; Humans; Image Processing, Computer-Assisted; Lung; Lung Neoplasms; Machine Learning; Tomography, X-Ray Computed; algorithm; diagnostic imaging; human; image processing; lung; lung tumor; machine learning; pathology; procedures; x-ray computed tomography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118440548"
"Chao H.-S.; Shiao T.-H.; Chou C.-W.; Lin F.-C.; Wu Y.-T.; Liou D.-M.","Chao, Heng-sheng (37064320800); Shiao, Tsu-Hui (37665974900); Chou, Chung-Wei (7403593600); Lin, Fang-Chi (25632353800); Wu, Yu-Te (7406890264); Liou, Der-Ming (7003450058)","37064320800; 37665974900; 7403593600; 25632353800; 7406890264; 7003450058","Computed Tomography Super-Resolution Using a Generative Adversarial Network in Bronchoscopy: A Clinical Feasibility Study","2021","Journal of Medical and Biological Engineering","41","5","","592","598","6","10.1007/s40846-021-00614-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106481355&doi=10.1007%2fs40846-021-00614-2&partnerID=40&md5=fe155774e0c7ce36958f8e793a5af07c","Purpose: To evaluate the usefulness of applying computed tomography (CT) images reconstructed by a deep learning super-resolution method to the clinical scenario of planning a real bronchoscopy procedure. Methods: We trained a super-resolution generative adversarial network (SRGAN) to reconstruct CT images to high-resolution (SRGANrc). We tasked three pulmonologists with evaluating the quality of the CT images and the derived virtual bronchoscopies. We also compared the number of bronchi that were segmented by an automatic commercial program with the number of bronchi segmented in different processed thin-sectioned CT images. Results: Regarding the human visual score, the original thin-sectioned CT images received more votes than the reconstructed CT images (SRGANrc) (29 votes versus eight votes). As for the human classification of four high-resolution CT images, the majority of images (83.7%) were classified correctly. Four out of 23 virtual bronchoscopies derived from super-resolution CT images were considered superior. The number of automatically segmented bronchi in super-resolution CT images was on average 1.5 less than that in the original thin-sliced CT images (mean bronchi: 15.1 vs. 16.6). Conclusion: The reconstruction of super-resolution CT images through the SRGAN may have limited applications in the clinical scenarios of our study. In addition to improving the deep-learning algorithm, we need more clinical implementation tests to discover its value. © 2021, Taiwanese Society of Biomedical Engineering.","Deep learning; Endoscopy; Image reconstruction; Learning algorithms; Optical resolving power; Respiratory system; Adversarial networks; Clinical feasibility; High resolution; High resolution CT; Human visual; Super resolution; Superresolution methods; Virtual bronchoscopy; algorithm; article; bronchoscopy; bronchus; controlled study; deep learning; feasibility study; high resolution computer tomography; human; human experiment; pulmonologist; Computerized tomography","Bronchoscopy; Super-resolution; Super-resolution generative adversarial network; Virtual bronchoscopy","Article","Final","","Scopus","2-s2.0-85106481355"
"You S.","You, Shuangyu (57547499500)","57547499500","PCB Defect Detection based on Generative Adversarial Network","2022","2022 2nd International Conference on Consumer Electronics and Computer Engineering, ICCECE 2022","","","","557","560","3","10.1109/ICCECE54139.2022.9712737","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126957873&doi=10.1109%2fICCECE54139.2022.9712737&partnerID=40&md5=34c8b8364f44fc29ad2025d6ab643cb3","This paper proposes a PCB defect detection scheme based on the generative confrontation network, which can be applied to the automatic detection system of PCB vision inspection (vision inspection). We use the edge-enhanced super-resolution GAN (EESRGAN) applied in the field of remote sensing to enhance the PCB images and complete the super-resolution detection of the reconstructed picture. And use the PCB pictures of different preprocessing models in an end-to-end manner to compare the recognition of PCB defects after training. Experiments on the PCB data set show that the PCB pictures after sliding cutting are input into the result of EESRGAN training, which can relatively accurately identify the 6 types of defects contained in the data set. Our results show the effectiveness of our data processing methods.  © 2022 IEEE.","Data handling; Defects; Generative adversarial networks; Image enhancement; Organic pollutants; Polychlorinated biphenyls; Remote sensing; Automatic detection systems; Data set; Detection scheme; Edge-enhanced super-resolution GAN; End to end; PCB defects detections; Remote-sensing; Resolution detection; Superresolution; Vision inspection; Optical resolving power","EESRGAN; Generative Adversarial Network; PCB defect detection","Conference paper","Final","","Scopus","2-s2.0-85126957873"
"Prasolov A.; Stirenko S.; Gordienko Y.","Prasolov, Andrii (57282627600); Stirenko, Sergii (54421204800); Gordienko, Yuri (6701855242)","57282627600; 54421204800; 6701855242","Improvement of image super resolution by deep neural networks","2021","EUROCON 2021 - 19th IEEE International Conference on Smart Technologies, Proceedings","","","","140","145","5","10.1109/EUROCON52738.2021.9535575","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116268246&doi=10.1109%2fEUROCON52738.2021.9535575&partnerID=40&md5=b4fc24b9b93977fa2b22a6c21cfd43af","The modern methods and architectures for image super resolution which are based on deep neural networks (DNNs) are considered. Several ways of their improvements were proposed and demonstrated. It was shown that the perception models built on MobileNet and EfficientNet families of DNNs turned out to be faster in training and have a better perception loss rate than previously used VGG family. In the more general context the usage of the smaller DNNs with the higher performance and lower size allow researchers to use and deploy them on devices with the limited computational resources for Edge Computing layer. © 2021 IEEE.","Computer vision; Convolutional neural networks; Deep neural networks; Image enhancement; Optical resolving power; Convolutional neural network; Deep learning; Generative adversarial network; High resolution; High-resolution; Low resolution; Lower resolution; Super-resolution; Superresolution; Generative adversarial networks","Convolutional neural networks (CNNs); Deep learning; Generative adversarial networks (GANs); High-resolution (HR); Low resolution (LR); Super-resolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85116268246"
"Kant M.; Chaurasia S.; Sharma H.","Kant, Moksh (57351950700); Chaurasia, Sandeep (57193862990); Sharma, Harish (56229215400)","57351950700; 57193862990; 56229215400","Contribution Analysis of Scope of SRGAN in the Medical Field","2022","Lecture Notes in Networks and Systems","238","","","341","352","11","10.1007/978-981-16-2641-8_33","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119830420&doi=10.1007%2f978-981-16-2641-8_33&partnerID=40&md5=408651f9a884cb21995f3b1f1f8cf105","This paper focuses on the concept of generative adversarial networks (GANs) and their scope in the medical field. “Generative adversarial networks” has been one of the most prominent research areas in the domain of machine learning in the past few years. It consists of two neural networks competing against each other. One of them is a generator model which generates fake samples of data and the other is a discriminator model which receives both real data (from the training data) and fake data (from the generator model) and tries to identify them as real or fake. Use of generative adversarial networks has been done here for the purpose of “super-resolution” because GANs work on the concept of generative modeling. Since applying super-resolution to an image means adding more data to the image which was not previously there (2017), it would require generation of data which might not be actually real data but is so close to the real one that one cannot know the difference. Hence, when we apply super-resolution using generative adversarial networks it gives us way better results in comparison to many other approaches such as “SRCNN”. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Data; Discriminator; GANs; Generator; Resolution; SRGAN","Conference paper","Final","","Scopus","2-s2.0-85119830420"
"Cap Q.H.; Tani H.; Kagiwada S.; Uga H.; Iyatomi H.","Cap, Quan Huu (57204062603); Tani, Hiroki (57208582152); Kagiwada, Satoshi (57195341156); Uga, Hiroyuki (8364842300); Iyatomi, Hitoshi (7801446758)","57204062603; 57208582152; 57195341156; 8364842300; 7801446758","LASSR: Effective super-resolution method for plant disease diagnosis","2021","Computers and Electronics in Agriculture","187","","106271","","","","10.1016/j.compag.2021.106271","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108644488&doi=10.1016%2fj.compag.2021.106271&partnerID=40&md5=50fe42eaa139de4c9188158ff458c6c9","The collection of high-resolution training data is crucial in building robust plant disease diagnosis systems, since such data have a significant impact on diagnostic performance. However, they are very difficult to obtain and are not always available in practice. Deep learning-based techniques, and particularly generative adversarial networks (GANs), can be applied to generate high-quality super-resolution images, but these methods often produce unexpected artifacts that can lower the diagnostic performance. In this paper, we propose a novel artifact-suppression super-resolution method that is specifically designed for diagnosing leaf disease, called Leaf Artifact-Suppression Super-Resolution (LASSR). Thanks to its own artifact removal module that detects and suppresses artifacts to a considerable extent, LASSR can generate much more pleasing, high-quality images compared to the state-of-the-art ESRGAN model. Experiments based on a five-class cucumber disease (including healthy) discrimination model show that training with data generated by LASSR significantly boosts the performance on an unseen test dataset by over 21% compared with the baseline, and that our approach is more than 2% better than a model trained with images generated by ESRGAN. © 2021 Elsevier B.V.","Cucumis sativus; Deep learning; Diagnosis; Statistical tests; Artifact suppression; Automated plant disease diagnose; Cucumber plant disease; Deep learning; Diagnostic performance; High resolution; Plant disease diagnosis; Super resolution; Superresolution methods; Training data; artifact; data set; detection method; image analysis; network analysis; performance assessment; Optical resolving power","Automated plant disease diagnosis; Cucumber plant diseases; Deep learning; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85108644488"
"Ai J.; Fan G.; Mao Y.; Jin J.; Xing M.; Yan H.","Ai, Jiaqiu (35319586300); Fan, Gaowei (57263273200); Mao, Yuxiang (57221601419); Jin, Jing (57212715723); Xing, Mengdao (7005922869); Yan, He (55550832600)","35319586300; 57263273200; 57221601419; 57212715723; 7005922869; 55550832600","An Improved SRGAN Based Ambiguity Suppression Algorithm for SAR Ship Target Contrast Enhancement","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2021.3111553","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115129298&doi=10.1109%2fLGRS.2021.3111553&partnerID=40&md5=3a36826aa9ab1932d1845423a1b4cc1a","Due to the specific characteristics of synthetic aperture radar (SAR), there will be ambiguity interference in SAR images, resulting in low contrast of the ship target to the clutter. This letter proposes an improved super-resolution generative adversarial network (ISRGAN) based ambiguity suppression algorithm for SAR ship target contrast enhancement. The proposed ISRGAN is the first attempt of using GAN for SAR ambiguity suppression. As a post-processing procedure, it does not need prior information of SAR systems, so it can be applied to various observation scenes and different acquisition modes. The generator of ISRGAN embeds the residual dense network (RDN) to optimally fuse the global and local features of the image, and it effectively improves the completeness of the feature information used for SAR ship target contrast enhancement. The superiority of ISRGAN on ambiguity suppression is validated on the Chinese Gaofen-3 imagery.  © 2004-2012 IEEE.","Image enhancement; Radar imaging; Ships; Acquisition modes; Adversarial networks; Contrast Enhancement; Feature information; Post-processing procedure; Prior information; Super resolution; Suppression algorithm; accuracy assessment; algorithm; remote sensing; synthetic aperture radar; Synthetic aperture radar","Azimuth ambiguity suppression; improved super-resolution generative adversarial network (ISRGAN); synthetic aperture radar (SAR); target contrast enhancement","Article","Final","","Scopus","2-s2.0-85115129298"
"Xu Y.; Zhao S.; Tang H.; Mao X.; Xu T.; Chen E.","Xu, Yifan (57345727400); Zhao, Sirui (56784709300); Tang, Huaying (57345268400); Mao, Xinglong (57345425700); Xu, Tong (56330270200); Chen, Enhong (35228685900)","57345727400; 56784709300; 57345268400; 57345425700; 56330270200; 35228685900","FAMGAN: Fine-grained AUs Modulation based Generative Adversarial Network for Micro-Expression Generation","2021","MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia","","","","4813","4817","4","10.1145/3474085.3479212","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119347063&doi=10.1145%2f3474085.3479212&partnerID=40&md5=2e23256b752aa830edfc4fe4671782f6","Micro-expressions (MEs) are significant and effective clues to reveal the true feelings and emotions of human beings, and thus MEs analysis is widely used in different fields such as medical diagnosis, interrogation and security. However, it is extremely difficult to elicit and label MEs, resulting in a lack of sufficient MEs data for MEs analysis. To address this challenge and inspired by the current face generation technology, in this paper we introduce Generative Adversarial Network based on fine-grained Action Units (AUs) modulation to generate MEs sequence (FAMGAN). Specifically, after comprehensively analyzing the factors that lead to inaccurate AU values detection, we performed fine-grained AUs modulation, which includes carefully eliminating the various noises and dealing with the asymmetry of AUs intensity. Additionally, we incorporate super-resolution into our model to enhance the quality of the generated images. Through experiments, we show that the system achieves very competitive results on the Micro-Expression Grand Challenge (MEGC2021). © 2021 ACM.","Chemical detection; Diagnosis; Image enhancement; Modulation; 'current; Action Unit; Expression analysis; Expression data; Fine grained; Grand Challenge; Human being; Micro-expression grand challenge; Micro-expressions; Generative adversarial networks","action units; generative adversarial network; micro-expression; micro-expression grand challenge","Conference paper","Final","","Scopus","2-s2.0-85119347063"
"Zhang C.; Shao Z.; Jiang C.; Chen F.","Zhang, Chengsheng (57223862538); Shao, Zhenguo (24578049000); Jiang, Changxu (57223870409); Chen, Feixiong (56654554400)","57223862538; 24578049000; 57223870409; 56654554400","A PV generation data reconstruction method based on improved super-resolution generative adversarial network","2021","International Journal of Electrical Power and Energy Systems","132","","107129","","","","10.1016/j.ijepes.2021.107129","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106217767&doi=10.1016%2fj.ijepes.2021.107129&partnerID=40&md5=93ff61cc854de16622384b466b4a744e","With the growing penetration of solar photovoltaic (PV) generation, advanced data analysis methods have been applied to the smart grid operation. However, the low-temporal-resolution PV generation data limits the utilization of the data analysis methods, because the low-temporal-resolution PV generation data contains too little information. On the other hand, the existing data reconstruction methods are less than satisfactory in reconstructing high-temporal-resolution PV generation data from low-temporal-resolution data, since most of them cannot fully capture the characteristics of PV generation data. To address this issue, a PV generation data reconstruction method based on improved super-resolution generative adversarial network is proposed in this paper. First, a data-image construction method is proposed to encode the PV generation data into the so-called data-images. Furthermore, we develop a data-image super-resolution generative adversarial network (DISRGAN) model, and the data-images are used to train the DISRGAN model. Finally, based on the trained DISRGAN model, a general framework is developed to reconstruct high-temporal-resolution PV generation data from low-temporal-resolution data. Numerical experiments have been carried out based on PV generation data from the State Grid Corporation of China, to reconstruct the high-temporal-resolution data from low-temporal-resolution data. The results demonstrate the superior performance of the proposed framework compared with a series of state-of-the-art methods. © 2021 Elsevier Ltd","Data reduction; Information analysis; Photovoltaic cells; Solar power generation; Adversarial networks; Data images; Data reconstruction; Generative adversarial network; High temporal resolution; Photovoltaic generation data; Photovoltaics generations; Reconstruction method; Super-resolution reconstruction; Temporal resolution; Optical resolving power","Data analysis; Generative adversarial network; PV generation data; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85106217767"
"Sharma S.; Kumar V.","Sharma, Shailza (57307338300); Kumar, Vinay (8728969100)","57307338300; 8728969100","An efficient image super resolution model with dense skip connections between complex filter structures in Generative Adversarial Networks","2021","Expert Systems with Applications","186","","115780","","","","10.1016/j.eswa.2021.115780","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113544930&doi=10.1016%2fj.eswa.2021.115780&partnerID=40&md5=bf6cab214f2aba5ae5cfb42064931a13","Generative Adversarial Network (GAN) based models have gained a lot of popularity due to their outstanding performance in image super resolution tasks. However, these networks have few inherent problems; such as, high computational complexity, large depth and vanishing gradient. Present work proposes a novel GAN based architecture, namely, Super Resolution with Inception Network (SRINet) to solve the above mentioned problems. Generator architecture of SRINet uses complex filter structure rather than the linear filter structure to increase the depth and width of network without increasing the computational cost. Complex filter settings, present in the architecture, helps it to attain locally distributed information along with hierarchical global information in an image. Hence, the proposed method approximates the most favorable sparse structures to foster the learning capability of network. Additionally, progressive two stage upscaling approach with dense skip connections are introduced in the generator architecture. This technique helps the proposed network to learn precise mapping to generate an output image from the low resolution image. To measure visual quality of an image, we use human visual system based visual information fidelity metric. Proposed method outperforms all the state-of-the-art methods qualitatively (perceptually) and quantitatively on other GAN based methods. © 2021","Complex networks; Network architecture; Adversarial networks; Complex filters; Convolutional neural network; Filter structures; Generative adversarial network; Image super resolutions; Inception architecture; Subpixel layer; Super resolution; Super-resolution models; Optical resolving power","Convolutional neural networks; Generative adversarial networks; Inception architecture; Subpixel layer; Super resolution","Article","Final","","Scopus","2-s2.0-85113544930"
"Hamdi A.; Chan Y.K.; Koo V.C.","Hamdi, Abdelsalam (57202786230); Chan, Yee Kit (9243982200); Koo, Voon Chet (55663712200)","57202786230; 9243982200; 55663712200","A New Image Enhancement and Super Resolution technique for license plate recognition","2021","Heliyon","7","11","e08341","","","","10.1016/j.heliyon.2021.e08341","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119268910&doi=10.1016%2fj.heliyon.2021.e08341&partnerID=40&md5=5ccd71fc2ba3f371807e115303a491a3","License Plate Recognition (LPR) is an important implemented application of Artificial Intelligence (AI) and deep learning in the past decades. However, due to the low image quality caused by the fast movement of vehicles and low-quality analogue cameras, many plate numbers cannot be recognised accurately by LPR models. To solve this issue, we propose a new deep learning architecture called D_GAN_ESR (Double Generative Adversarial Networks for Image Enhancement and Super Resolution) used for effective image denoising and super-resolution for license plate images. In this paper, we show the limitation of the existing networks for image enhancement and image super-resolution. Furthermore, a feature-based evaluation metric called Peak Signal to Noise Ratio Features (PSNR-F) is used to evaluate and compare performance between different methods. It is shown that the use of PSNR-F has a better performance indicator than the classical PSNR-pixel-to-pixel (PSNR-pixel) evaluation metric. The results show that using D_GAN_ESR to enhance the license plate images increases the LPR accuracy from 30% to 78% when blur images are used and increases the accuracy from 59% to 74.5% when low-quality images are used. © 2021 The Author(s)","","Image Enhancement; Image Super Resolution; LPR","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85119268910"
"Takada N.; Omori T.","Takada, Naomichi (57267972700); Omori, Toshiaki (8626560300)","57267972700; 8626560300","Video frame rate up-conversion via spatio-temporal generative adversarial networks","2021","Journal of Image and Graphics(United Kingdom)","9","3","","87","94","7","10.18178/joig.9.3.87-94","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115403646&doi=10.18178%2fjoig.9.3.87-94&partnerID=40&md5=80f99059facefc63d977f1f85770a30a","Video quality has become more important due to the development of information and communication technology. In this study, we propose a spatio-temporal super-resolution method using a Generative Adversarial Network (GAN) in order to achieve a higher frame rate. In recent years, with the development of machine learning technology such as convolutional neural networks, clearer interpolation frame estimation has been realized. Most of the estimation methods use optimization techniques that minimize the mean squared reconstruction error, and the resulting estimates show a high Peak Signal-to-Noise Ratio (PSNR). However, these Mean Squared Error (MSE)-based methods often lack the high-frequency components of the generated frame, resulting in blurry frames. To address this issue, our study adopts GAN that uses spatiotemporal convolution instead of traditional spatial convolution. We propose a method for video frame rate up-conversion with perceptual loss function, which consists of adversarial loss and mean squared loss. This adversarial loss produces a more natural frame using a discriminator network trained to distinguish between the estimated frame and the original frame. We verified the effectiveness of the proposed method using video data containing complex and large motions such as rotational motion and scaling. © 2021 Journal of Image and Graphics.","","Deep learning; Machine learning; Neural networks; Spatio-temporal data analysis; Video frame interpolation","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85115403646"
"Liu Y.; Yang J.; Han J.; Sun J.","Liu, Yunfeng (57546729200); Yang, Jinbiao (57547114400); Han, Jinfeng (57547493600); Sun, Jingjie (57547144800)","57546729200; 57547114400; 57547493600; 57547144800","Super-resolution reconstruction of thermal imaging of power equipment based on Generative Adversarial Network with Channel Filtering","2022","2022 2nd International Conference on Consumer Electronics and Computer Engineering, ICCECE 2022","","","","275","280","5","10.1109/ICCECE54139.2022.9712763","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126947507&doi=10.1109%2fICCECE54139.2022.9712763&partnerID=40&md5=f112242759fe8ead2191152341009bc1","Non-contact infrared thermal imaging can help people visually see the temperature distribution on the surface of an object, which is of great significance for power equipment status monitoring and health status evaluation. However, the expensive cost and technical barriers of high-resolution infrared imagers limit the application of high-resolution thermal imaging images in the online monitoring of power Internet of Things equipment. Super-resolution reconstruction meets the resolution requirements while reducing costs. This paper realizes the enhancement of thermal imaging of power equipment by constructing a generative confrontation network with channel filtering. This network embeds a threshold filtering module in the generator part of SRGAN (super-resolution generative confrontation network), and uses channel information for autonomous threshold learning. Filtering improves the stability of training on the basis of reducing image noise; at the same time, the peak information of the image is strengthened through edge extraction technology to improve the network's ability to restore the edge of the image. The analysis of the calculation example shows that: using PSNR (peak signal to noise ratio) and SSIM (structural similarity) indicators to analyze the overall data and edge data after reconstruction, the results are significantly improved, and the subjective visual effect after reconstruction is clearer. Higher engineering practical value.  © 2022 IEEE.","Extraction; Generative adversarial networks; Image enhancement; Information filtering; Optical resolving power; Signal to noise ratio; Channel filtering; Edge extraction; Health status; Infrared thermal imaging; Non-contact; Power equipment; Power equipment thermal imaging; Status monitoring; Super-resolution reconstruction; Thermal-imaging; Infrared imaging","channel filtering; Edge Extraction; Power equipment thermal imaging; Super-resolution Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85126947507"
"Zhao Y.; Chen Z.; Gao X.; Song W.; Xiong Q.; Hu J.; Zhang Z.","Zhao, Yafeng (24472424400); Chen, Zhen (57219057694); Gao, Xuan (57219050323); Song, Wenlong (8655780000); Xiong, Qiang (57221966226); Hu, Junfeng (55499436500); Zhang, Zhichao (57221954247)","24472424400; 57219057694; 57219050323; 8655780000; 57221966226; 55499436500; 57221954247","Plant Disease Detection Using Generated Leaves Based on DoubleGAN","2022","IEEE/ACM Transactions on Computational Biology and Bioinformatics","19","3","","1817","1826","9","10.1109/TCBB.2021.3056683","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100791828&doi=10.1109%2fTCBB.2021.3056683&partnerID=40&md5=68830a5705f13353406965baa8739e8a","Plant leaves can be used to effectively detect plant diseases. However, the number of images of unhealthy leaves collected from various plants is usually unbalanced. It is difficult to detect diseases using such an unbalanced dataset. We used DoubleGAN (a double generative adversarial network) to generate images of unhealthy plant leaves to balance such datasets. We proposed using DoubleGAN to generate high-resolution images of unhealthy leaves using fewer samples. DoubleGAN is divided into two stages. In stage 1, we used healthy leaves and unhealthy leaves as inputs. First, the healthy leaf images were used as inputs for the WGAN (Wasserstein generative adversarial network) to obtain the pretrained model. Then, unhealthy leaves were used for the pretrained model to generate 64∗64 pixel images of unhealthy leaves. In stage 2, a superresolution generative adversarial network (SRGAN) was used to obtain corresponding 256∗256 pixel images to expand the unbalanced dataset. Finally, compared with images generated by DCGAN (Deep convolution generative adversarial network). The dataset expanded with DoubleGAN, the generated images are clearer than DCGAN, and the accuracy of plant species and disease recognition reached 99.80 and 99.53 percent, respectively. The recognition results are better than those from the original dataset.  © 2004-2012 IEEE.","Image Processing, Computer-Assisted; Plant Diseases; Plant Leaves; Pixels; Adversarial networks; High resolution image; Leaf images; Pixel images; Plant disease; Plant leaves; Plant species; Super resolution; image processing; plant disease; plant leaf; procedures; Plants (botany)","GAN; leaf; plant disease detection; superresolution","Article","Final","","Scopus","2-s2.0-85100791828"
"Zhao M.; Wei Y.; Wong K.K.L.","Zhao, Ming (57189875649); Wei, Yang (57220668530); Wong, Kelvin K.L. (26434942800)","57189875649; 57220668530; 26434942800","A Generative Adversarial Network technique for high-quality super-resolution reconstruction of cardiac magnetic resonance images","2022","Magnetic Resonance Imaging","85","","","153","160","7","10.1016/j.mri.2021.10.033","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117897339&doi=10.1016%2fj.mri.2021.10.033&partnerID=40&md5=fcd4a35ae46a00a2f6769004e9190644","Purpose: In this paper, we proposed a Denoising Super-resolution Generative Adversarial Network (DnSRGAN) method for high-quality super-resolution reconstruction of noisy cardiac magnetic resonance (CMR) images. Methods: The proposed method is based on feed-forward denoising convolutional neural network (DnCNN) and SRGAN architecture. Firstly, we used a feed-forward denoising neural network to pre-denoise the CMR image to ensure that the input is a clean image. Secondly, we use the gradient penalty (GP) method to solve the problem of the discriminator gradient disappearing, which improves the convergence speed of the model. Finally, a new loss function is added to the original SRGAN loss function to monitor GAN gradient descent to achieve more stable and efficient model training, thereby providing higher perceptual quality for the super-resolution of CMR images. Results: We divided the tested cardiac images into 3 groups, each group of 25 images. Then, we calculated the Peak Signal to Noise Ratio (PSNR) /Structural Similarity (SSIM) between Ground Truth (GT) and the images generated by super-resolution, used them to evaluate our model. We compared with the current widely used method: Bicubic ESRGAN and SRGAN, our method has better reconstruction quality and higher PSNR/SSIM score. Conclusion: We used DnCNN to denoise the CMR image, and then using the improved SRGAN to perform super-resolution reconstruction of the denoised image, we can solve the problem of high noise and artifacts that cause the cardiac image to be reconstructed incorrectly during super-resolution. © 2021","Artifacts; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Signal-To-Noise Ratio; article; artifact; cardiovascular magnetic resonance; controlled study; convolutional neural network; deep learning; heart; human; loss of function mutation; punishment; signal noise ratio; velocity; image processing; nuclear magnetic resonance imaging; procedures","CMR images; Deep learning; Denoising; Generative Adversarial Network; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117897339"
"Jinzhen M.; Shuo Z.; Yu Z.; Yami F.; Yan Z.; Shuqing C.; Zongming L.","Jinzhen, Mu (57363206900); Shuo, Zhang (57363349800); Yu, Zhang (57223864009); Yami, Fang (57363057600); Yan, Zhou (57362767200); Shuqing, Cao (57363207000); Zongming, Liu (57362914400)","57363206900; 57363349800; 57223864009; 57363057600; 57362767200; 57363207000; 57362914400","Image Super-Resolution Using Quality Aware Generative Adversarial Networks","2022","Lecture Notes in Electrical Engineering","644 LNEE","","","1883","1893","10","10.1007/978-981-15-8155-7_158","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120644905&doi=10.1007%2f978-981-15-8155-7_158&partnerID=40&md5=8f29ad45aa44ec3901ff3a8bb7f73573","It has been demonstrated that GAN-based algorithms can generate realistic images in single image super-resolution. However, these methods usually generate undesired artifacts in the accompanied images. We proposes a new GAN-based super resolution method to further improve the performance of super-resolved results. In this fashion, we introduce dense compression unit as our basic unit. Then, we use an additional noise into the generator to enhance the quality of generator network. To enhance the supervision of texture recovery, we use a novel quality aware function that is inspired by the SSIM index as excellent regularizer for GAN objective functions. Finally, we demonstrate our method in extensive experiments that the generated images has more realistic textures and it has a great potential in remote sensing tiny-object detection. © 2022, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Generative adversarial networks; Object detection; Remote sensing; Textures; Basic units; Content loss; Image super resolutions; Performance; Quality aware; Realistic images; Single images; SSIM indices; Superresolution; Superresolution methods; Optical resolving power","Content loss; Generative adversarial networks; Quality aware; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85120644905"
"Dai J.; Zhang Y.; Xie P.; Xu X.","Dai, Jinhui (57250373600); Zhang, Yue (57250373700); Xie, Pengcheng (57250609900); Xu, Xinzhou (56145954300)","57250373600; 57250373700; 57250609900; 56145954300","Super-Resolution for Music Signals Using Generative Adversarial Networks","2021","2021 IEEE 4th International Conference on Big Data and Artificial Intelligence, BDAI 2021","","","","171","175","4","10.1109/BDAI52447.2021.9515219","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114499865&doi=10.1109%2fBDAI52447.2021.9515219&partnerID=40&md5=51a365b6b5008872a99018bf2fe284fe","Super-Resolution (SR) refers to increasing the resolution of a signal in a variety of ways, conventionally employed in the field of image enhancement. Compared with the endeavors for super-resolution in image processing, music signals require supper-resolution to improve their quality or adapt to communication in narrow-band channel, which is also regarded as bandwidth expansion. To this end, we shed light on super-resolution for music signals using the deep learning strategy of Generative Adversarial Networks (GANs). The proposed approach feeds Shot-Time Fourier Transform (STFT) features of low-band signals to the GAN, expecting to obtain their high-band information through jointly considering content and adversarial losses. Then, we carry out experiments on MUSDB18 dataset using mixtures of music sources, in order to show the performance of the proposed approach. The experimental results indicate that the proposed approach achieves better super-resolution performances compared with interpolation and some conventional deep-learning strategies. © 2021 IEEE.","Big data; Image enhancement; Learning systems; Optical resolving power; Adversarial networks; Bandwidth expansions; Learning strategy; Music signals; Narrow band channel; Super resolution; Supper resolutions; Deep learning","bandwidth extension; generative adversarial networks; music processing; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85114499865"
"Lijun Y.; Xiaoming Z.; Fan L.; Gang S.; Zhou C.; Jing Y.; Min Z.; Yongchang C.; Lingling W.; Zelong C.; Lan P.; Fengqing B.; Zifang Y.; Hongqiu X.; Pengjian L.; Zhisheng L.; Qiang T.","Lijun, Yao (57240952400); Xiaoming, Zhu (57240952500); Fan, Luo (57241166500); Gang, Sun (57240952600); Zhou, Chen (57749599900); Jing, Yan (57241385000); Min, Zhou (57240631800); Yongchang, Cai (57240842000); Lingling, Wang (57742817700); Zelong, Cao (57240842100); Lan, Peng (57241168000); Fengqing, Bai (57241168100); Zifang, You (57240634600); Hongqiu, Xiao (57240634700); Pengjian, Li (57240735300); Zhisheng, Lv (57240955000); Qiang, Tang (57234752500)","57240952400; 57240952500; 57241166500; 57240952600; 57749599900; 57241385000; 57240631800; 57240842000; 57742817700; 57240842100; 57241168000; 57241168100; 57240634600; 57240634700; 57240735300; 57240955000; 57234752500","Image super-resolution method based on generative adversarial network","2021","Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021","","","9513030","909","915","6","10.1109/AEMCSE51986.2021.00185","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114040660&doi=10.1109%2fAEMCSE51986.2021.00185&partnerID=40&md5=c4369d68923e85472e90af4332770c53","Although deep convolutional neural networks have made breakthroughs in the accuracy and speed of single-image super-resolution, there are still many unsolved problems: firstly, How to refine the texture problem when performing super-resolution processing at a larger magnification ratio. Secondly, the existing convolutional neural network image super-resolution algorithms are prone to overfitting and insufficient convergence of the loss function. Aiming at two problems, an image super-resolution method based on generative adversarial network is proposed. The feature map is spatially transformed on the network to solve the problem of refined texture, combined with CycleGAN and SRGAN, the network structure is improved and the loss function is optimized, and the SRCICGAN algorithm is proposed to restore the four times down-sampled image to solve the loss function problem. The experiment is compared with the latest six methods on three data sets. The PSNR and SSIM indicators are 1.92% and 5.49% higher in the Flickr2K data set, respectively, and better visual effects can be obtained in terms of detailed texture.  © 2021 IEEE.","Convolution; Convolutional neural networks; Deep neural networks; Image enhancement; Optical resolving power; Software engineering; Textures; Adversarial networks; Image super resolutions; Loss functions; Network structures; Super resolution; Texture problems; Unsolved problems; Visual effects; Image texture","ASSP semantic segmentation probability map; ESFT method; Generating adversarial networks; SRCICGAN","Conference paper","Final","","Scopus","2-s2.0-85114040660"
"Li B.; Qiu S.; Jiang W.; Zhang W.; Le M.","Li, Bin (57102055600); Qiu, Shi (57203679667); Jiang, Wei (57223722655); Zhang, Wei (57196272383); Le, Mingnan (56459627500)","57102055600; 57203679667; 57223722655; 57196272383; 56459627500","A UAV Detection and Tracking Algorithm Based on Image Feature Super-Resolution","2022","Wireless Communications and Mobile Computing","2022","","6526684","","","","10.1155/2022/6526684","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125147314&doi=10.1155%2f2022%2f6526684&partnerID=40&md5=8f6dc23bb280f5f6299433fb03cdd73c","UAV is difficult to detect by visual methods at a long distance, so a UAV detection and tracking algorithm is proposed based on image super-resolution. Firstly, a saliency transformation algorithm is built to focus on the suspected area. Then, a generative adversarial network is established on the basis of ROI to realize the super-resolution of weak targets and restore the high-resolution details of target features. Finally, the cooperative attention module is built to recognize and track UAV. Our experiments show that the proposed algorithm has strong robustness.  © 2022 Bin Li et al.","Generative adversarial networks; Optical resolving power; Unmanned aerial vehicles (UAV); Detection and tracking algorithms; High-Resolution Details; Image features; Image super resolutions; Strong robustness; Superresolution; Target feature; Transformation algorithm; Weak targets; Aircraft detection","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85125147314"
"Shao W.; Zhang M.; Li H.","Shao, Wenze (55959121000); Zhang, Miaomiao (56122160600); Li, Haibo (56375326400)","55959121000; 56122160600; 56375326400","Tiny Face Hallucination via Relativistic Adversarial Learning; [基于相对生成对抗网络的低清小脸幻构]","2021","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","43","9","","2577","2585","8","10.11999/JEIT200362","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115197835&doi=10.11999%2fJEIT200362&partnerID=40&md5=34c1a27e9e0a26c303f0d9889eafc0ed","Considering that previous tiny face hallucination methods either produced visually less pleasant faces or required architecturally more complex networks, this paper advocates a new deep model for tiny face hallucination by borrowing the idea of Relativistic Generative Adversarial Network (tfh-RGAN). Specifically, a hallucination generator and a relativistic discriminator are jointly learned in an alternately iterative training fashion by minimizing the combined pixel loss and relativistic generative adversarial loss. As for the generator, it is mainly structured as concatenation of a few basic modules followed by three 2×up-sampling layers, and each basic module is formulated by coupling the residual blocks, dense blocks, and depthwise separable convolution operators. As such, the generator can be made lightweight while with a considerable depth so as to achieve high quality face hallucination. As for the discriminator, it makes use of VGG128 while removing all its batch normalization layers and embedding a fully connected layer additionally so as to fulfill the capacity limit of relativistic adversarial learning. Experimental results reveal that, the proposed method, though simpler in the network architecture without a need of explicitly imposing any face structural prior, is able to produce better hallucination faces with higher definition and stronger reality. In terms of the quantitative assessment, the peak signal-to-noise ratio of the proposed method can be improved up to 0.25~1.51 dB compared against several previous approaches. © 2021, Science Press. All right reserved.","Complex networks; Iterative methods; Network architecture; Adversarial learning; Adversarial networks; Capacity limit; Convolution operators; Face hallucination; High quality; Peak signal to noise ratio; Quantitative assessments; Signal to noise ratio","Deep learning; Face hallucination; Generative adversarial networks; Image processing; Super-resolution","Article","Final","","Scopus","2-s2.0-85115197835"
"Huang Y.; Fan F.; Syben C.; Roser P.; Mill L.; Maier A.","Huang, Yixing (57190218062); Fan, Fuxin (57221148426); Syben, Christopher (57194397713); Roser, Philipp (57208545060); Mill, Leonid (57201071476); Maier, Andreas (23392966100)","57190218062; 57221148426; 57194397713; 57208545060; 57201071476; 23392966100","Cephalogram synthesis and landmark detection in dental cone-beam CT systems","2021","Medical Image Analysis","70","","102028","","","","10.1016/j.media.2021.102028","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102619349&doi=10.1016%2fj.media.2021.102028&partnerID=40&md5=6e3a2341d714e274ada4e158aee584db","Due to the lack of a standardized 3D cephalometric analysis methodology, 2D cephalograms synthesized from 3D cone-beam computed tomography (CBCT) volumes are widely used for cephalometric analysis in dental CBCT systems. However, compared with conventional X-ray film based cephalograms, such synthetic cephalograms lack image contrast and resolution, which impairs cephalometric landmark identification. In addition, the increased radiation dose applied to acquire the scan for 3D reconstruction causes potential health risks. In this work, we propose a sigmoid-based intensity transform that uses the nonlinear optical property of X-ray films to increase image contrast of synthetic cephalograms from 3D volumes. To improve image resolution, super resolution deep learning techniques are investigated. For low dose purpose, the pixel-to-pixel generative adversarial network (pix2pixGAN) is proposed for 2D cephalogram synthesis directly from two cone-beam projections. For landmark detection in the synthetic cephalograms, an efficient automatic landmark detection method using the combination of LeNet-5 and ResNet50 is proposed. Our experiments demonstrate the efficacy of pix2pixGAN in 2D cephalogram synthesis, achieving an average peak signal-to-noise ratio (PSNR) value of 33.8 with reference to the cephalograms synthesized from 3D CBCT volumes. Pix2pixGAN also achieves the best performance in super resolution, achieving an average PSNR value of 32.5 without the introduction of checkerboard or jagging artifacts. Our proposed automatic landmark detection method achieves 86.7% successful detection rate in the 2 mm clinical acceptable range on the ISBI Test1 data, which is comparable to the state-of-the-art methods. The method trained on conventional cephalograms can be directly applied to landmark detection in the synthetic cephalograms, achieving 93.0% and 80.7% successful detection rate in 4 mm precision range for synthetic cephalograms from 3D volumes and 2D projections, respectively. © 2021 Elsevier B.V.","Cephalometry; Cone-Beam Computed Tomography; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Signal-To-Noise Ratio; Deep learning; Health risks; Image enhancement; Image resolution; Mathematical transformations; Optical properties; Optical resolving power; Pixels; Signal to noise ratio; X ray films; X rays; Adversarial networks; Cephalometric analysis; Cone-beam computed tomography; Landmark identification; Non-linear optical properties; Peak signal to noise ratio; Potential health risks; State-of-the-art methods; anatomic landmark; anatomical concepts; Article; cephalometry; clinical assessment; comparative study; cone beam computed tomography; controlled study; convolutional neural network; deep learning; human; image analysis; optics; outcome assessment; priority journal; radiation dose; signal noise ratio; synthesis; cephalometry; image processing; three-dimensional imaging; Computerized tomography","Cephalogram synthesis; Deep learning; Landmark detection; Super resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102619349"
"Yousif M.Z.; Yu L.; Lim H.-C.","Yousif, Mustafa Z. (57221872306); Yu, Linqi (57271595700); Lim, Hee-Chang (7403094883)","57221872306; 57271595700; 7403094883","Super-resolution reconstruction of turbulent flow fields at various Reynolds numbers based on generative adversarial networks","2022","Physics of Fluids","34","1","015130","","","","10.1063/5.0074724","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124033997&doi=10.1063%2f5.0074724&partnerID=40&md5=37403677c63a1d35ee4ea147e8a25f39","This study presents a deep learning-based framework to recover high-resolution turbulent velocity fields from extremely low-resolution data at various Reynolds numbers by utilizing the concept of generative adversarial networks. A multiscale enhanced super-resolution generative adversarial network is applied as a model to reconstruct the high-resolution velocity fields, and direct numerical simulation data of turbulent channel flow with large longitudinal ribs at various Reynolds numbers are used to evaluate the performance of the model. The model is found to have the capacity to accurately reconstruct the high-resolution velocity fields from data at two different down-sampling factors in terms of the instantaneous velocity fields, two-point correlations, and turbulence statistics. The results further reveal that the model is able to reconstruct high-resolution velocity fields at Reynolds numbers that fall within the range of the training Reynolds numbers.  © 2022 Author(s).","Channel flow; Deep learning; Optical resolving power; Reynolds number; Sampling; Turbulent flow; Velocity; Direct numerical simulations datum; High resolution; High resolution velocity; Lower resolution; Reynold number; Super-resolution reconstruction; Superresolution; Turbulent flow field; Turbulent velocity fields; Velocity field; Generative adversarial networks","","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124033997"
"Zhang L.; Lu W.; Huang Y.; Sun X.; Zhang H.","Zhang, Lize (57233449400); Lu, Wen (55484285200); Huang, Yuanfei (57203161002); Sun, Xiaopeng (57201075215); Zhang, Hongyi (35748167800)","57233449400; 55484285200; 57203161002; 57201075215; 35748167800","Unpaired remote sensing image super-resolution with multi-stage aggregation networks","2021","Remote Sensing","13","16","3167","","","","10.3390/rs13163167","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113645201&doi=10.3390%2frs13163167&partnerID=40&md5=9576728e45d6e5c535405f46b866fb2e","Mainstream image super-resolution (SR) methods are generally based on paired training samples. As the high-resolution (HR) remote sensing images are difficult to collect with a limited imaging device, most of the existing remote sensing super-resolution methods try to down-sample the collected original images to generate an auxiliary low-resolution (LR) image and form a paired pseudo HR-LR dataset for training. However, the distribution of the generated LR images is generally inconsistent with the real images due to the limitation of remote sensing imaging devices. In this paper, we propose a perceptually unpaired super-resolution method by constructing a multi-stage aggregation network (MSAN). The optimization of the network depends on consistency losses. In particular, the first phase is to preserve the contents of the super-resolved results, by constraining the content consistency between the down-scaled SR results and the low-quality low-resolution inputs. The second stage minimizes perceptual feature loss between the current result and LR input to constrain perceptual-content consistency. The final phase employs the generative adversarial network (GAN) to adding photo-realistic textures by constraining perceptual-distribution consistency. Numerous experiments on synthetic remote sensing datasets and real remote sensing images show that our method obtains more plausible results than other SR methods quantitatively and qualitatively. The PSNR of our network is 0.06dB higher than the SOTA method—HAN on the UC Merced test set with complex degradation. © 2021 by the authors.","Imaging techniques; Optical resolving power; Textures; Adversarial networks; Aggregation network; Content consistency; Image super resolutions; Low resolution images; Remote sensing images; Remote sensing imaging; Superresolution methods; Remote sensing","Consistency losses; Multi-stage aggregation network; Remote sensing; Unpaired super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85113645201"
"Qiao C.; Chen X.; Zhang S.; Li D.; Guo Y.; Dai Q.; Li D.","Qiao, Chang (57193796384); Chen, Xingye (57193722787); Zhang, Siwei (57204285455); Li, Di (57201864678); Guo, Yuting (56942610200); Dai, Qionghai (7202735131); Li, Dong (56148369900)","57193796384; 57193722787; 57204285455; 57201864678; 56942610200; 7202735131; 56148369900","3D Structured Illumination Microscopy via Channel Attention Generative Adversarial Network","2021","IEEE Journal of Selected Topics in Quantum Electronics","27","4","9360437","","","","10.1109/JSTQE.2021.3060762","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101747335&doi=10.1109%2fJSTQE.2021.3060762&partnerID=40&md5=7190544daa3359b7a29f5f6762dfcbe6","Three-dimensional (3D) structured illumination microscopy (SIM) plays an important role in biological volumetric imaging with the capabilities of doubling the lateral and axial resolution and optical sectioning. However, 3D-SIM suffers from more photobleaching and phototoxicity compared to other volumetric imaging modalities, such as light-sheet microscopy, because it requires 15 raw images per axial slice, which hampers its widespread application in live cell imaging. Here we report the design of a channel attention generative adversarial network (caGAN) that improves the quality of 3D-SIM reconstruction under low signal-to-noise-ratio (SNR) condition and enables reconstruction using fewer raw images. Compared to the conventional algorithm, caGAN-SIM achieves comparable or higher reconstruction fidelity while using 15-fold less signal level. We demonstrate the superior performance of caGAN-SIM for various subcellular structures and its ability in long-term multi-color 3D super-resolution imaging using the example of dynamic interactions between microtubules and lysosomes in live cells.  © 1995-2012 IEEE.","Image enhancement; Photobleaching; Conventional algorithms; Light-sheet microscopies; Low signal-to-noise ratio; Structured illumination microscopies (SIM); Structured illumination microscopy; Subcellular structure; Super resolution imaging; Threedimensional (3-d); Signal to noise ratio","deep learning; generative adversarial network; super-resolution; Three-dimensional structured illumination microscopy","Article","Final","","Scopus","2-s2.0-85101747335"
"Tang R.; Dore J.; Ma J.; Leong P.H.W.","Tang, Rui (55749712900); Dore, Jonathon (55180818200); Ma, Jin (57221188501); Leong, Philip H.W. (7005928205)","55749712900; 55180818200; 57221188501; 7005928205","Interpolating high granularity solar generation and load consumption data using super resolution generative adversarial network","2021","Applied Energy","299","","117297","","","","10.1016/j.apenergy.2021.117297","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108896066&doi=10.1016%2fj.apenergy.2021.117297&partnerID=40&md5=2582e770ab09eae8d007d0e0d5f9e2d0","The vast majority of commonly accessible photovoltaics (PV) generation and load consumption datasets have low temporal resolutions, leading to inaccuracies in the modeling and optimisation of PV-integrated battery systems. This study addresses this problem by proposing an interpolation model based on a super resolution generative adversarial network (SRGAN) that generates 5-minute PV and load power data from 30-minute/hourly temporal resolutions. The proposed approach is validated by two different datasets including large amounts of residential data and compared to an alternative predictive model. The results indicate that the model can adequately capture the targeted data distributions and temporal characteristics with negligible statistical differences from the measured high resolution data. Moreover, it performs consistently across different types of PV/load profiles and on average it results in 0.32% and 0.28% normalised root mean squared errors (NRMSEs) in daily totals of 5-minute PV and load power values when using hourly data as inputs. Under a time-of-use (ToU) tariff, the interpolated 5-minute data leads to 44.7% and 41.7% error reductions compared to using hourly data for estimating electricity costs and battery saving potentials of a PV battery system. Hence, the proposed model can be potentially applied in a battery sizing tool to obtain more accurate sizing results when only low resolution data is available. © 2021 Elsevier Ltd","Costs; Electric batteries; Interpolation; Large dataset; Mean square error; Optical resolving power; Smart meters; Solar power generation; Adversarial networks; Data interpolation; Energy; High granularity; Load power; Photovoltaic power; Photovoltaics; Super resolution; Temporal resolution; accuracy assessment; data set; detection method; electricity; electricity generation; estimation method; interpolation; model validation; network analysis; performance assessment; power generation; solar power; Solar energy","Data interpolation; Load energy; Smart meter; Solar energy","Article","Final","","Scopus","2-s2.0-85108896066"
"Han Q.; Zhou X.; Song R.; Zhao Y.","Han, Qiaoling (57200303311); Zhou, Xibo (57204582459); Song, Runze (57349693700); Zhao, Yue (57194347501)","57200303311; 57204582459; 57349693700; 57194347501","Super-resolution reconstruction of soil CT images using sequence information; [基于序列信息的土壤CT图像超分辨率重建]","2021","Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering","37","17","","90","96","6","10.11975/j.issn.1002-6819.2021.17.010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119608105&doi=10.11975%2fj.issn.1002-6819.2021.17.010&partnerID=40&md5=728ec9991c2990793dfc77ccb293ac12","Pore boundary is generally blur resulted from the partial volume in the soil CT image. This phenomenon has inevitably posed a great influence on the accuracy of soil pore topology. This study aims to propose a novel Sequence information Generative Adversarial Network (SeqGAN) to realize the Super-Resolution reconstruction of soil CT images. Therefore, the SeqGAN was selected to improve the clarity and accuracy of soil CT images, particularly for the high resolution and feature boundaries. Two improvements of SeqGAN were utilized, including the Sequential Convolution block (SeqConv) structure, and Beginning-to-End Residuals Connection block (BE-Resblock). SeqConv structure involved two convolution block structures. The first convolution block was used to extract the feature of the target image, while the second was used to extract the sequence information of the next and previous image in the sequence, thereby realizing the extraction of sequence information. In the BE-Resblock, more than 8 residual blocks were connected in series to extract the image information. At the same time, the residual blocks of the beginning and end were also connected, where the input information was introduced to reduce the probability of overfitting. Furthermore, twice up-sampling blocks were used to improve the resolution of images, where the final output was a 4x high-definition Super-Resolution image. The experimental samples soil was taken from Keshan Farm in the northwest of Keshan County, Qiqihar City, Heilongjiang Province (125°23'57″E, 48°18'37″N). Soil samples were collected with a cutting ring and stored in a plexiglass tube. A submerging test was also conducted to obtain the soil samples. A spiral CT scanner was then used to capture soil CT images. The test datasets were finally taken as the 440 soil CT sequence images with high sequence Structural Similarity (SSIM). Two datasets were obtained after preprocessed, including the high- and low-resolution images with twice the difference in resolution. Specifically, the low-resolution image dataset contained 220 soil CT images, where each image presented a resolution of 128×128 pixels and a size of 62.17 mm× 62.17 mm. At the same time, the high-resolution image dataset (original image datasets) contained 220 soil CT images, where each image presented a resolution of 256×256 pixels and a size of 62.17 mm×62.17 mm. Three common models were selected to compare with the improved model. Qualitative experiments showed that the improved model well performed a higher resolution, and lower gray difference, thereby constructing most soil pores in detail. Quantitative experiments showed that the Mean Square Error (MSE) of the improved model was 25% lower than that of Generative Adversarial Network. In addition, the Peak Signal to Noise Ratio (PSNR) of improved model was 1.4 higher than that of Generative Adversarial Network. SSIM of super- and high-resolution image was 0.2% higher than that of Generative Adversarial Network. Consequently, the SeqGAN can be expected to realize the super-resolution reconstruction of soil CT images with high accuracy and high definition. The finding can also provide potential data reliability and benefit to follow-up research on soil pore segmentation and soil skeletonization. © 2021, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","Computerized tomography; Convolution; Deep learning; Generative adversarial networks; Image enhancement; Optical resolving power; Soil surveys; CT Image; Deep learning; High definition; High resolution; Image datasets; Images processing; Sequence informations; Soil pores; Super-resolution reconstruction; Superresolution; Soils","CT images; Deep learning; Generative Adversarial Network; Image processing; Soils; Super-resolution","Article","Final","","Scopus","2-s2.0-85119608105"
"Song H.; Wang M.; Zhang L.; Li Y.; Jiang Z.; Yin G.","Song, Hongtao (35113865100); Wang, Minghao (57219377342); Zhang, Liguo (56681766700); Li, Yang (57216684747); Jiang, Zhengyi (57219381411); Yin, Guisheng (55817240200)","35113865100; 57219377342; 56681766700; 57216684747; 57219381411; 55817240200","S 2RGAN : sonar-image super-resolution based on generative adversarial network","2021","Visual Computer","37","8","","2285","2299","14","10.1007/s00371-020-01986-3","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092525718&doi=10.1007%2fs00371-020-01986-3&partnerID=40&md5=7525f4a366a19c0b7e7a0290f09be285","As an important display mode of underwater environments, the sonar image has limitations on the resolution, which often leads to problems with low resolution of underwater objects. Therefore, the image super-resolution algorithm is needed to transform the images from low-resolution to high-resolution. It can improve the visual effect and contribute to subsequent processing such as 3D reconstruction and object recognition. This paper proposes a method for sonar image super-resolution based on generative adversarial networks (GAN). By comparing the super-resolution effects of various interpolation and convolutional neural network algorithms on sonar images, a Residual-in-Residual Dense Block network is employed as the generator of GAN since it has the low distortion and high perceptual quality. Because the sonar image training set does not have enough data, the generator utilizes the transfer learning on the sonar images to produce an optimized network model which is more suitable for super-resolution of sonar image. The VGG19 network is employed as the discriminator. In addition, the perceptual loss is introduced into the loss function of S 2RGAN to further improve the perceptual quality of super-resolution images. The experimental results indicate that the proposed S 2RGAN shows excellent performance. The generated super-resolution images of S 2RGAN have the remarkable advantages of both lower distortion and higher perceptual quality comparing with other methods. Because S 2RGAN focuses more on the reality and overall visual effect of super-resolution sonar images, it is suitable for various underwater situations. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Convolutional neural networks; Learning systems; Object recognition; Optical resolving power; Sonar; Transfer learning; Underwater acoustics; 3D reconstruction; Adversarial networks; Image super resolutions; Network modeling; Perceptual quality; Super resolution; Underwater environments; Underwater objects; Image enhancement","Generative adversarial network; Sonar-image; Super-resolution; Transfer learning","Article","Final","","Scopus","2-s2.0-85092525718"
"Gao W.; Zhou C.; Guo M.-F.","Gao, Wei (57192812316); Zhou, Chen (57370102900); Guo, Mou-Fa (8860925800)","57192812316; 57370102900; 8860925800","Insulator defect identification via improved YOLOv4 and SR-GAN algorithm; [基于改进YOLOv4及SR-GAN的绝缘子缺陷辨识研究]","2021","Dianji yu Kongzhi Xuebao/Electric Machines and Control","25","11","","93","104","11","10.15938/j.emc.2021.11.011","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120971415&doi=10.15938%2fj.emc.2021.11.011&partnerID=40&md5=72bb0d806e9bc78037f71010195ee0dd","In order to accurately identify small target insulators and defects in UAV inspection graphics, a defect detection method for transmission line insulators based on improved deep learning target detection network namely YOLOv4 was proposed. First, sufficient insulator images were collected by the means of drone and data enhancement to construct insulator dataset. Secondly, insulator image data set was used to train YOLOv4 network. During the training process, multi-stage transfer learning strategy and cosine annealing attenuation learning rate method were adopted to improve the training speed and overall performance of the network. Finally, in the test process, the super-resolution reconstruction generative adversarial network was introduced to generate high-quality images for low-confidence images, and then the test was carried out again to improve identification ability of small objects. The experimental results show that, compared with Faster R-CNN and YOLOv3, the average classification accuracy and detection rate per frame of the proposed algorithm are greatly improved and the performance is excellent. © 2021, Harbin University of Science and Technology Publication. All right reserved.","Aircraft detection; Deep learning; Defects; Image enhancement; Optical resolving power; Unmanned aerial vehicles (UAV); Data enhancement; Defect detection; Insulator; Insulator images; Multi-stage transfer learning; Multi-stages; Super-resolution reconstruction; Super-resolution reconstruction generative adversarial network; Transfer learning; YOLOv4; Generative adversarial networks","Data enhancement; Defect detection; Insulator; Multi-stage transfer learning; Super-resolution reconstruction generative adversarial network (SR-GAN); YOLOv4","Article","Final","","Scopus","2-s2.0-85120971415"
"Lin M.; Liu L.; Wang F.; Li J.; Pan J.","Lin, Mianfen (57226597997); Liu, Liangxin (57226602411); Wang, Fei (56366666900); Li, Jingcong (57200533404); Pan, Jiahui (55355956000)","57226597997; 57226602411; 56366666900; 57200533404; 55355956000","License plate image reconstruction based on generative adversarial networks","2021","Remote Sensing","13","15","3018","","","","10.3390/rs13153018","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112037286&doi=10.3390%2frs13153018&partnerID=40&md5=eaad1cda55268f23b55f98619cc406a7","License plate image reconstruction plays an important role in Intelligent Transportation Systems. In this paper, a super-resolution image reconstruction method based on Generative Adversarial Networks (GAN) is proposed. The proposed method mainly consists of four parts: (1) pretreatment for the input image; (2) image features extraction using residual dense network; (3) introduction of progressive sampling, which can provide larger receptive field and more information details; (4) discriminator based on markovian discriminator (PatchGAN) can make a more accurate judgment, which guides the generator to reconstruct images with higher quality and details. Regarding the Chinese City Parking Dataset (CCPD) dataset, compared with the current better algorithm, the experiment results prove that our model has a higher peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) and less reconstruction time, which verifies the feasibility of our approach. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Image segmentation; Intelligent systems; License plates (automobile); Plates (structural components); Signal to noise ratio; Adversarial networks; Chinese cities; Intelligent transportation systems; Peak signal to noise ratio; Progressive sampling; Receptive fields; Structural similarity; Super-resolution image reconstruction; Image reconstruction","Generative adversarial network; Python; Residual dense network; Super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112037286"
"Shang C.; Li X.; Yin Z.; Li X.; Wang L.; Zhang Y.; Du Y.; Ling F.","Shang, Cheng (57209801137); Li, Xinyan (36523257300); Yin, Zhixiang (57205742028); Li, Xiaodong (55878368700); Wang, Lihui (57141007100); Zhang, Yihang (55658053900); Du, Yun (56420121700); Ling, Feng (56278268300)","57209801137; 36523257300; 57205742028; 55878368700; 57141007100; 55658053900; 56420121700; 56278268300","Spatiotemporal Reflectance Fusion Using a Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3065418","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103284108&doi=10.1109%2fTGRS.2021.3065418&partnerID=40&md5=29addda27e77aa23f573ebe809fcefef","The spatiotemporal reflectance fusion method is used to blend high-temporal and low-spatial resolution images with their low-temporal and high-spatial resolution counterparts that were previously acquired by various satellite sensors. Recently, a wide variety of learning-based solutions have been developed, but challenges remain. These solutions usually require two sets of data acquired before and after the prediction time, making them unsuitable for near-real-time predicting. The solutions are always trained band by band and thus do not consider the spectral correlation. High-resolution temporal changes are difficult to reconstruct accurately with the network structure used, which lowers the accuracy of the fusion result. To address these problems, this study proposes a novel spatiotemporal adaptive reflectance fusion model using a generative adversarial network (GASTFN). In GASTFN, an end-to-end network, including a generative and discriminative network, is simultaneously trained for all spectral bands. The proposed model can be applied to the one-pair case, consider the spectral correlation of each band, and improve the process of producing super-resolution imagery by adopting the discriminative network for image reflectance values rather than temporal changes in reflectance. The proposed model has been verified with two actual satellite data sets acquired in heterogeneous landscapes and areas with abrupt changes, with a comparison of the state-of-art methods. The results show that GASTFN can generate the most accurate fusion images with more detailed textures, more realistic spatial shapes, and higher accuracy, demonstrating that the GASTFN is effective for predicting near-real-time changes in image reflectance and preserves the most valuable spatial information. © 1980-2012 IEEE.","Forecasting; Image enhancement; Image resolution; Reflection; Textures; Adversarial networks; Discriminative networks; Heterogeneous landscapes; High spatial resolution; Spatial informations; Spatial resolution images; Spectral correlation; State-of-art methods; correlation; image resolution; network analysis; reflectance; satellite data; satellite imagery; spatiotemporal analysis; Image fusion","Generative adversarial network (GAN); spatiotemporal fusion; super-resolution; temporal changes","Article","Final","","Scopus","2-s2.0-85103284108"
"Cho S.-Y.; Kim D.-Y.; Oh S.-Y.; Sohn C.-B.","Cho, Soo-Young (57202996708); Kim, Dae-Yeol (57202988942); Oh, Su-Yeong (57216928453); Sohn, Chae-Bong (8972923200)","57202996708; 57202988942; 57216928453; 8972923200","Reducing system load of effective video using a network model","2021","Applied Sciences (Switzerland)","11","20","9665","","","","10.3390/app11209665","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117326060&doi=10.3390%2fapp11209665&partnerID=40&md5=addad5d3b2b48498ecd76104d7cc637b","Recently, as non-face-to-face work has become more common, the development of streaming services has become a significant issue. As these services are applied in increasingly diverse fields, various problems are caused by the overloading of systems when users try to transmit high-quality images. In this paper, SRGAN (Super Resolution Generative Adversarial Network) and DAIN (Depth-Aware Video Frame Interpolation) deep learning were used to reduce the overload that occurs during real-time video transmission. Images were divided into a FoV (Field of view) region and a non-FoV (Non-Field of view) region, and SRGAN was applied to the former, DAIN to the latter. Through this process, image quality was improved and system load was reduced. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","DAIN; FBF; Front on the backward frame; Segmentation; SST","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117326060"
"Hu Y.; Jing M.; Jiao Y.; Sun K.","Hu, Yuliang (57240017700); Jing, Mingli (36170854900); Jiao, Yao (57240336000); Sun, Kun (57223847898)","57240017700; 36170854900; 57240336000; 57223847898","Images super-resolution using improved generative adversarial networks","2021","2021 3rd International Conference on Intelligent Control, Measurement and Signal Processing and Intelligent Oil Field, ICMSP 2021","","","9513225","254","258","4","10.1109/ICMSP53480.2021.9513225","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113988686&doi=10.1109%2fICMSP53480.2021.9513225&partnerID=40&md5=3c0f27144f9842d77be96248a7b108a0","Image super-resolution (ISR) is an important image processing technology to improve image resolution in computer vision tasks. The purpose of this paper is to study the super-resolution reconstruction of single image based on the depth learning method. Aiming at the problem that the existing pixel loss-based super-resolution image reconstruction algorithms have poor reconstruction effect on high-frequency details, such as textures, a lighter algorithm is proposed on the basis of the existing deep learning method (SRGAN). Firstly, the feedback structure is applied in the generator to process the feedback information and enhance the high frequency information of the image. Secondly, a general residual feature aggregation framework (RFA), is applied to make full use of the residual information of each layer to improve the quality of the SR image. Finally, the solution space of the function is further reduced and the image quality is improved by using a new loss function. The algorithms are implemented on pytorch framework. The experimental results on VOC2012 data sets show that, compared with the original SRGAN algorithm, the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) of the proposed algorithm on the benchmark data set Set5 are improved by 0.83dB and 0.028, respectively, on Set14, the PSNR and SSIM of the proposed algorithm are improved by 0.56dB and 0.009, on Urban100, the PSNR and SSIM of the proposed algorithm are improved by 0.51dB and 0.031, on BSD100, the PSNR and SSIM of the proposed algorithm are improved by 0.33dB and 0.014, and compared with other improved algorithms, the effect of this algorithm is also better than other algorithms. © 2021 IEEE.","Deep learning; Feedback; Image quality; Image reconstruction; Image resolution; Learning systems; Oil fields; Optical resolving power; Signal to noise ratio; Textures; Feed back information; High-frequency informations; Image processing technology; Image super resolutions; Peak signal to noise ratio; Structural similarity; Super resolution image reconstruction algorithm; Super resolution reconstruction; Image enhancement","Convolutional neural networks (CNN); Deep learning; Generative adversarial network (GAN); Image super-resolution; Pytorch","Conference paper","Final","","Scopus","2-s2.0-85113988686"
"Ding L.; Ding S.-F.; Zhang J.; Zhang Z.-C.","Ding, Ling (57218532895); Ding, Shi-Fei (24314525600); Zhang, Jian (56637434200); Zhang, Zi-Chen (57195331210)","57218532895; 24314525600; 56637434200; 57195331210","Single Image Super-Resolution Reconstruction Based on VGG Energy Loss; [使用VGG能量损失的单图像超分辨率重建]","2021","Ruan Jian Xue Bao/Journal of Software","32","11","","3659","3668","9","10.13328/j.cnki.jos.006053","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118928719&doi=10.13328%2fj.cnki.jos.006053&partnerID=40&md5=c2347a22128981c8a81e7e12b27ffe54","Single image super-resolution (SR) is an important task in image synthesis. Based on neural nets, the loss function in the SR task commonly contains a content-based reconstruction loss and a generative adversarial network (GAN) based regularization loss. However, due to the instability of GAN training, the generated discriminative signal of a high-resolution image from the GAN loss is not stable in the SRGAN model. In order to alleviate this problem, based on the commonly used VGG reconstruction loss, this study designs a stable energy-based regularization loss, which is called VGG energy loss. The proposed VGG energy loss in this study uses the VGG encoder in the reconstruction loss as an encoder, and designs the corresponding decoder to build a VGG-U-Net auto encoder: VGG-UAE; by using the VGG-UAE as the energy function, which can provide gradients for the generator, the generated high-resolution samples track the energy flow of real data. Experiments verify that a generative model using the proposed VGG energy loss can generate more effective high-resolution images. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","Energy dissipation; Image reconstruction; Optical resolving power; Signal encoding; Auto encoders; Energy functions; High-resolution images; Image super resolutions; Images synthesis; Loss functions; Regularisation; Single images; Single-image super-resolution reconstruction; Superresolution; Generative adversarial networks","Auto encoder; Energy function; Generative adversarial net; Single image super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85118928719"
"Sharma A.; Jindal N.","Sharma, Akanksha (57188975329); Jindal, Neeru (8571819900)","57188975329; 8571819900","Cross-Modality Breast Image Translation with Improved Resolution Using Generative Adversarial Networks","2021","Wireless Personal Communications","119","4","","2877","2891","14","10.1007/s11277-021-08376-5","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103387032&doi=10.1007%2fs11277-021-08376-5&partnerID=40&md5=b781249d21a3a01a0a9937edc5fc437b","Unpaired cross domain medical image translation is a challenging problem as the target image modality cannot be mapped directly from the input data distribution. The best approach used till date, was by Cycle generative adversarial network, which utilized the cycle consistency loss to perform the task. Although efficient, still the resultant image size was small and blurry. Recent trends show that due to change in lifestyle and increased exposure to carcinogens in different forms has increased occurrence of cancer. Statistics show that every one in eight women might develop breast cancer at some stage of her life. Hence, this paper focuses on a combination of two GANs, CycleGAN and Super resolution GAN is used in two stages to obtain translated breast images with improved resolution. The proposed model is tested on images of breast cancer patients to obtain CT Scan using PET scan and vice versa so that the patients are not exposed to an extremely potent dose of radiation. In order to ensure the presence of tumour in the estimated image, a simplified U-net feature extractor is also used. Quantitative studies are carried out for both the stages of simulation to establish the efficiency of the proposed model. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Diseases; Image enhancement; Medical imaging; Adversarial networks; Breast Cancer; Breast images; Cross modality; Feature extractor; Image translation; Quantitative study; Super resolution; Computerized tomography","Breast cancer; Computed tomography; Cross-modality; Generative adversarial networks; Improved resolution; Medical image translation; Positron emission tomography","Article","Final","","Scopus","2-s2.0-85103387032"
"Jammes-Floreani M.; Laine A.F.; Angelini E.D.","Jammes-Floreani, Martin (57224203963); Laine, Andrew F. (26643433600); Angelini, Elsa D. (56878282500)","57224203963; 26643433600; 56878282500","Enhanced-quality gan (EQ-GAN) on lung CT scans: Toward truth and potential hallucinations","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9433996","20","23","3","10.1109/ISBI48211.2021.9433996","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107215063&doi=10.1109%2fISBI48211.2021.9433996&partnerID=40&md5=0cac4aa712ce477eb9327febe999d251","Lung Computed Tomography (CT) scans are extensively used to screen lung diseases. Strategies such as large slice spacing and low-dose CT scans are often preferred to reduce radiation exposure and therefore the risk for patients' health. The counterpart is a significant degradation of image quality and/or resolution. In this work we investigate a generative adversarial network (GAN) for lung CT image enhanced-quality (EQ). Our EQ-GAN is trained on a high-quality lung CT cohort to recover the visual quality of scans degraded by blur and noise. The capability of our trained GAN to generate EQ CT scans is further illustrated on two test cohorts. Results confirm gains in visual quality metrics, remarkable visual enhancement of vessels, airways and lung parenchyma, as well as other enhancement patterns that require further investigation. We also compared automatic lung lobe segmentation on original versus EQ scans. Average Dice scores vary between lobes, can be as low as 0.3 and EQ scans enable segmentation of some lobes missed in the original scans. This paves the way to using EQ as pre-processing for lung lobe segmentation, further research to evaluate the impact of EQ to add robustness to airway and vessel segmentation, and to investigate anatomical details revealed in EQ scans.  © 2021 IEEE.","Biological organs; Health risks; Image enhancement; Image quality; Medical imaging; Adversarial networks; Computed tomography scan; Lung lobe segmentations; Pre-processing; Radiation Exposure; Vessel segmentation; Visual qualities; Visual quality metrics; Computerized tomography","Generative Adversarial Network (GAN); Image enhancement; Lung CT; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85107215063"
"Moustafa M.S.; Sayed S.A.","Moustafa, Marwa S. (57239502500); Sayed, Sayed A. (57224931387)","57239502500; 57224931387","Satellite Imagery Super-Resolution Using Squeeze-and-Excitation-Based GAN","2021","International Journal of Aeronautical and Space Sciences","22","6","","1481","1492","11","10.1007/s42405-021-00396-6","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108651959&doi=10.1007%2fs42405-021-00396-6&partnerID=40&md5=0800068e86f272ecf5eafddc0b06c0b5","Single Image Super Resolution (SISR) elevates spectral and spatial image resolution beyond the sensor capabilities. Convolutional Neural Networks (CNNs) have dominated current mainstream approaches. However, the utilization of pixel-based loss function hinders achieving realistic perceptual results at large upscale factors. Recently, Generative Adversarial Network (GAN) attained more realistic, crisp results in natural image, but the complex nature of satellite images limits the performance. In this work, we address these challenges equipped with the promising results of Squeeze-and-Excitation (SE) in classification tasks. A Spatial and Channel Squeeze-and-Excitation GAN (SCSE-GAN) is introduced. The proposed generator stacked SCSE block after each residual block to recalibrate and ensure features flow and amplify high-frequency details. In addition, skip/residual connection was utilized in the GAN generator network to further boost the performance. Wasserstein distance with gradient penalty (WGAN-GP) was adopted to stabilize training and avoid gradient vanishing phenomena. Finally, we conducted various experiments on two open-source benchmarks namely: RSSCN7 and Kaggle datasets, to systematically evaluate the proposed framework performance. According to the obtained results, the proposed approach excels other approaches quantitatively and visually. Quantitatively, the results show a boost by a considerable margin of 2%, and 3% in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), respectively. Visually, the proposed method shows a sharper, less smooth image compared with benchmark SISR approaches. © 2021, The Korean Society for Aeronautical & Space Sciences.","Benchmarking; Convolutional neural networks; Image resolution; Open systems; Optical resolving power; Satellite imagery; Adversarial networks; Classification tasks; High frequency HF; Peak signal to noise ratio; Sensor capability; Structural similarity index measures (SSIM); Super resolution; Wasserstein distance; Signal to noise ratio","Deep Learning (DL); Generative Adversarial Network (GAN); Gradient penalty (WGAN-GP); Single Image Super Resolution (SISR); Spatial and Channel Squeeze-and-Excitation (SCSE); Wasserstein distance","Article","Final","","Scopus","2-s2.0-85108651959"
"Wang Y.-L.; Li X.-J.; Ma H.-B.; Ding Q.; Pirouz M.; Ma Q.-T.","Wang, Ying-Li (54791667900); Li, Xiao-Jing (57224586324); Ma, Hong-Bin (55609308700); Ding, Qun (37043420200); Pirouz, Matin (57193432915); Ma, Qi-Tao (57216810791)","54791667900; 57224586324; 55609308700; 37043420200; 57193432915; 57216810791","Image super-resolution reconstruction based on improved generative adversarial network","2021","Journal of Network Intelligence","6","2","","155","163","8","10.1109/ICIAI.2019.8850808","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108010926&doi=10.1109%2fICIAI.2019.8850808&partnerID=40&md5=f27275329a0096c18bebb63337c83b29","This article introduces the super-resolution reconstruction of images based on an improved generative confrontation network, improves the network structure of the generator, and proposes a super-resolution reconstruction algorithm for the recursive residual generation confrontation network. Its discriminator uses PatchGAN as the discriminator. The network solves the bottleneck of low feature information utilization and slow convergence of the generation countermeasure network in the super-resolution algorithm based on convolutional neural network. The reconstruction algorithm is compared with mainstream super-resolution reconstruction algorithms on standard data sets such as Set5 and Set14. The data shows that the algorithm can effectively improve the use of feature information, restore the details of low-resolution images, and improve the quality of image reconstruction. © 2021, Taiwan Ubiquitous Information. All rights reserved.","Convolutional neural networks; Image enhancement; Optical resolving power; Adversarial networks; Feature information; Image super-resolution reconstruction; Low resolution images; Reconstruction algorithms; Residual generation; Super resolution algorithms; Super resolution reconstruction; Image reconstruction","Convolutional neural network; Generative adversarial network; Image super-resolution; Recursive residual network","Article","Final","","Scopus","2-s2.0-85108010926"
"Datta S.; Dandapat S.; Deka B.","Datta, Sumit (57014095700); Dandapat, Samarendra (15922221700); Deka, Bhabesh (49663267700)","57014095700; 15922221700; 49663267700","A deep framework for enhancement of diagnostic information in CSMRI reconstruction","2022","Biomedical Signal Processing and Control","71","","103117","","","","10.1016/j.bspc.2021.103117","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114127570&doi=10.1016%2fj.bspc.2021.103117&partnerID=40&md5=8d600c5f228a155aff14d3887a991eb1","In compressed sensing (CS)-based magnetic resonance imaging (MRI), it is very challenging to maintain the diagnostic quality due to limited measurements. Diagnostically critical information, like fine anatomical details, edges, and boundaries are distorted due to the leakage of energy and artifacts during CS-reconstruction. In this paper, we have proposed a deep learning architecture to reconstruct high-resolution diagnostically enhanced MR images from comprehensively sensed k-space data with reduced scan time. Proposed network is based on deep back-projection and generative adversarial network (GAN) architectures. For the performance evaluation, we have considered a pathological dataset having 3064 MR images with three types of brain tumors. The performance of the proposed technique is also compared with some of the well-known image super-resolution techniques. It has been observed that the proposed technique outperforms some of the recent image super-resolution techniques, both quantitatively and qualitatively. © 2021 Elsevier Ltd","Compressed sensing; Deep learning; Diagnosis; Image enhancement; Image reconstruction; Network architecture; Optical resolving power; Back-projection network; Compressed-Sensing; Deep learning; Diagnostics informations; Generative adversarial network; Image super resolutions; MR-images; Pathological magnetic resonance imaging; Resolution techniques; Super resolution; Article; brain tumor; clinical practice; comparative study; compressed sensing based magnetic resonance imaging; controlled study; deep learning; human; image artifact; image reconstruction; nuclear magnetic resonance imaging; patient comfort; Magnetic resonance imaging","Back-projection network; Compressed sensing; Deep learning; Diagnostic information; GAN; Pathological MRI; Super-resolution","Article","Final","","Scopus","2-s2.0-85114127570"
"Alam M.S.; Kwon K.-C.; Erdenebat M.-U.; Abbass M.Y.; Alam M.A.; Kim N.","Alam, Md. Shahinur (57205751357); Kwon, Ki-Chul (7201503212); Erdenebat, Munkh-Uchral (36166588400); Abbass, Mohammed Y. (56730458700); Alam, Md. Ashraful (57213590057); Kim, Nam (35494120000)","57205751357; 7201503212; 36166588400; 56730458700; 57213590057; 35494120000","Super-resolution enhancement method based on generative adversarial network for integral imaging microscopy","2021","Sensors","21","6","2164","1","17","16","10.3390/s21062164","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102686707&doi=10.3390%2fs21062164&partnerID=40&md5=a1cee5f3b17961ce955ffc1ca20c797d","The integral imaging microscopy system provides a three-dimensional visualization of a microscopic object. However, it has a low-resolution problem due to the fundamental limitation of the F-number (the aperture stops) by using micro lens array (MLA) and a poor illumination environment. In this paper, a generative adversarial network (GAN)-based super-resolution algorithm is proposed to enhance the resolution where the directional view image is directly fed as input. In a GAN network, the generator regresses the high-resolution output from the low-resolution input image, whereas the discriminator distinguishes between the original and generated image. In the generator part, we use consecutive residual blocks with the content loss to retrieve the photo-realistic original image. It can restore the edges and enhance the resolution by ×2, ×4, and even ×8 times without seriously hampering the image quality. The model is tested with a variety of low-resolution microscopic sample images and successfully generates high-resolution directional view images with better illumination. The quantitative analysis shows that the proposed model performs better for microscopic images than the existing algorithms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Microlenses; Optical resolving power; Three dimensional computer graphics; Adversarial networks; Fundamental limitations; High-resolution output; Micro-lens arrays; Microscopic image; Microscopic objects; Super resolution algorithms; Three dimensional visualization; algorithm; article; artificial intelligence; deep learning; human tissue; illumination; image quality; microscopy; quantitative analysis; Image enhancement","Deep learning; Generative adversarial network; Integral imaging microscopy; Machine intelligence; Microscopy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102686707"
"Zhang C.; Shao Z.; Chen F.","Zhang, Chengsheng (57223862538); Shao, Zhenguo (24578049000); Chen, Feixiong (56654554400)","57223862538; 24578049000; 56654554400","A Power Data Reconstruction Method Based on Super-Resolution Generative Adversarial Network","2021","Proceedings - 2021 6th Asia Conference on Power and Electrical Engineering, ACPEE 2021","","","9437116","300","304","4","10.1109/ACPEE51499.2021.9437116","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107507851&doi=10.1109%2fACPEE51499.2021.9437116&partnerID=40&md5=5b9a94d2be00c34c25c8a65ca029c614","The smart grid is rapidly developing to become highly connected and automated. These advancements have been mainly attributed to the ubiquitous data communication in the grid. However, low sampling frequency will limit the utilization degree of data because low frequency measurement power data contains little information. The existing methods of reconstructing the low-frequency sampling data into the high-frequency sampling data have poor accuracy of data reconstruction since most of them failed to capture the characteristics of power data. This paper proposes a novel method based on super-resolution generative adversarial network (SRGAN) to address this issue. First, we convert power data into data-images. Furthermore, the data-images are used to train the SRGAN model. Finally, the trained generator can be used to reconstruct the low-frequency sampling data into the high-frequency sampling data. Numerical experiments have been carried out based on photovoltaic (PV) power generation time-series data from the State Grid Corporation of China with separately reconstruction of the irradiance and PV power datas. The results demonstrate the superior performance of the proposed method compared with a series of state-of-the-art methods. © 2021 IEEE.","Data handling; Electric power transmission networks; Optical resolving power; Photovoltaic cells; Adversarial networks; Data reconstruction; High-frequency sampling; Low frequency measurements; Numerical experiments; Sampling frequencies; State-of-the-art methods; Super resolution; Solar power generation","data-driven; generative adversarial network; power data; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85107507851"
"Tiddeman B.; Ghahremani M.","Tiddeman, Bernard (6602496513); Ghahremani, Morteza (55912181900)","6602496513; 55912181900","Principal component wavelet networks for solving linear inverse problems","2021","Symmetry","13","6","1083","","","","10.3390/sym13061083","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108988178&doi=10.3390%2fsym13061083&partnerID=40&md5=ef56bee27c7fa7bb1fe7c6cf3382fe9f","In this paper we propose a novel learning-based wavelet transform and demonstrate its utility as a representation in solving a number of linear inverse problems—these are asymmetric problems, where the forward problem is easy to solve, but the inverse is difficult and often ill-posed. The wavelet decomposition is comprised of the application of an invertible 2D wavelet filter-bank comprising symmetric and anti-symmetric filters, in combination with a set of 1 × 1 convolution filters learnt from Principal Component Analysis (PCA). The 1 × 1 filters are needed to control the size of the decomposition. We show that the application of PCA across wavelet subbands in this way produces an architecture equivalent to a separable Convolutional Neural Network (CNN), with the principal components forming the 1 × 1 filters and the subtraction of the mean forming the bias terms. The use of an invertible filter bank and (approximately) invertible PCA allows us to create a deep autoencoder very simply, and avoids issues of overfitting. We investigate the construction and learning of such networks, and their application to linear inverse problems via the Alternating Direction of Multipliers Method (ADMM). We use our network as a drop-in replacement for traditional discrete wavelet transform, using wavelet shrinkage as the projection operator. The results show good potential on a number of inverse problems such as compressive sensing, in-painting, denoising and super-resolution, and significantly close the performance gap with Generative Adversarial Network (GAN)-based methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","ADMM; Deep learning; PCA; Wavelet networks","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85108988178"
"He W.; Wang C.; Sun Z.","He, Wenlei (57921370500); Wang, Chaoli (57907091700); Sun, Zhanquan (57215661166)","57921370500; 57907091700; 57215661166","Super-resolution Reconstruction of Satellite Imagery Based on Generative Adversarial Network","2021","Information and Control","50","2","","195","203","8","10.13976/j.cnki.xk.2021.0181","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137653769&doi=10.13976%2fj.cnki.xk.2021.0181&partnerID=40&md5=797224bbccebfd2cbeac7b49a69bbc27","Although most indices associated with the super-resolution reconstruction of a single remote sensing image based on deep earning have been significantly mproved, the effect observed by the human eyes is not obvious. Previous methods for creating low-resolution mages cause some information losses. To avoid this problem, we use different scales to obtain high-and low-resolution remote sensing image pairs as training data sets. Through this method, we can effectively avoid the loss of original image information caused by downsampling. We use a generative adversarial network (GAN) image super-resolution model based on deep residual blocks so that the model can better learn a priori information. Thus, the quality of the mage generated by the algorithm and the efficiency improve. We also add the spatial position information between mage features to the contextual loss function, thereby reducing mage artifacts caused by feature matching errors. Then, we add a relative discriminator to evaluate the relative authenticity of the obtained image and optimize the super-resolution effect. Experimental results on MWPU-RESISG45 dataset verify that the proposed method greatly enhances PSNR (peak signal to noise ratio), SSIM (structural similarity), and AG(average gradient) indicators. The human eye observation reveals that the network outputs a good super-resolution effect map. © 2021 The Authors. All rights reserved.","","contextual loss function; convolutional neural network (CNN); generative adversarial network (GAN); remote sensing magery; super-resolution","Article","Final","","Scopus","2-s2.0-85137653769"
"Zhu J.; Tan C.; Yang J.; Yang G.; Lio' P.","Zhu, Jin (57202160439); Tan, Chuan (57217106261); Yang, Junwei (57225187397); Yang, Guang (57216243504); Lio', Pietro (7004223170)","57202160439; 57217106261; 57225187397; 57216243504; 7004223170","Arbitrary Scale Super-Resolution for Medical Images","2021","International Journal of Neural Systems","31","10","2150037","","","","10.1142/S0129065721500374","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111242960&doi=10.1142%2fS0129065721500374&partnerID=40&md5=af0ad73ba74599a029e8137225ed083d","Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalize over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in (1, 4]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation.  © 2021 World Scientific Publishing Company.","Algorithms; COVID-19; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; SARS-CoV-2; Brain mapping; Computerized tomography; Deep learning; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Optical resolving power; Transfer learning; Adversarial networks; Cardiac mr images; Computed tomography images; High spatial resolution images; High-resolution output; Image quality enhancements; Low resolution images; Perceptual quality; algorithm; human; image processing; nuclear magnetic resonance imaging; Image enhancement","generative adversarial networks; image processing; medical image analysis; meta learning; Super-resolution; transfer learning","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85111242960"
"Maqsood M.H.; Mumtaz R.; Haq I.U.; Shafi U.; Zaidi S.M.H.; Hafeez M.","Maqsood, Muhammad Hassan (57351290400); Mumtaz, Rafia (16176156800); Haq, Ihsan Ul (57532173800); Shafi, Uferah (57202601301); Zaidi, Syed Mohammad Hassan (25031385500); Hafeez, Maryam (24765929600)","57351290400; 16176156800; 57532173800; 57202601301; 25031385500; 24765929600","Super resolution generative adversarial network (Srgans) for wheat stripe rust classification","2021","Sensors","21","23","7903","","","","10.3390/s21237903","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119719751&doi=10.3390%2fs21237903&partnerID=40&md5=6c3ce4bfe475ba43de291d554e9e53d1","Wheat yellow rust is a common agricultural disease that affects the crop every year across the world. The disease not only negatively impacts the quality of the yield but the quantity as well, which results in adverse impact on economy and food supply. It is highly desired to develop methods for fast and accurate detection of yellow rust in wheat crop; however, high-resolution images are not always available which hinders the ability of trained models in detection tasks. The approach presented in this study harnesses the power of super-resolution generative adversarial networks (SRGAN) for upsampling the images before using them to train deep learning models for the detection of wheat yellow rust. After preprocessing the data for noise removal, SRGANs are used for upsampling the images to increase their resolution which helps convolutional neural network (CNN) in learning high-quality features during training. This study empirically shows that SRGANs can be used effectively to improve the quality of images and produce significantly better results when compared with models trained using low-resolution images. This is evident from the results obtained on upsampled images, i.e., 83% of overall test accuracy, which are substantially better than the overall test accuracy achieved for low-resolution images, i.e., 75%. The proposed approach can be used in other real-world scenarios where images are of low resolution due to the unavailability of high-resolution camera in edge devices. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Agriculture; Basidiomycota; Image Processing, Computer-Assisted; Neural Networks, Computer; Triticum; Convolutional neural networks; Crops; Deep learning; Food supply; Generative adversarial networks; Image enhancement; Signal sampling; Deep learning; GAN; Low resolution images; SRGAN; Stripe rust; Superresolution; Test accuracy; Upsampling; Wheat stripe rust; Yellow rust; agriculture; Basidiomycetes; image processing; wheat; Optical resolving power","Deep learning; GANs; SRGANs; Super resolution; Wheat stripe rust","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119719751"
"Hu L.; Wang Z.; Chen T.; Zhang Y.","Hu, Lei (56489119100); Wang, Zugen (57273102100); Chen, Tian (57273843300); Zhang, Yongmei (7601330280)","56489119100; 57273102100; 57273843300; 7601330280","An Improved SRGAN Infrared Image Super-Resolution Reconstruction Algorithm; [一种改进的SRGAN红外图像超分辨率重建算法]","2021","Xitong Fangzhen Xuebao / Journal of System Simulation","33","9","","2109","2118","9","10.16182/j.issn1004731x.joss.20-0450","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115759068&doi=10.16182%2fj.issn1004731x.joss.20-0450&partnerID=40&md5=21e0b377a90ab35866d336f59a08c4d3","Aiming at the low resolution of infrared images, an improved SRGAN super-resolution reconstruction algorithm is designed. In the generative network, the method of applying the residual dense network to obtain the image features extracted from each network layer so as to retain more high-frequency information of the image, and adopting a progressive upsampling method to improve the super-resolution reconstruction effect under a large scaling factor. In terms of the loss function, the perceptual loss that is more in line with human senses is adopted to make the generated image being closer to the real high-resolution image of senses and content. Experimental results show that the quality of reconstructed infrared image is better than that of the current representative methods in the subjective and objective evaluation. © 2021, The Editorial Board of Journal of System Simulation. All right reserved.","","Generative adversarial network; Infrared image; Residual dense network; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85115759068"
"Xie J.; Fang L.; Zhang B.; Chanussot J.; Li S.","Xie, Jie (57210957012); Fang, Leyuan (57218451012); Zhang, Bob (57217861698); Chanussot, Jocelyn (6602159365); Li, Shutao (7409240361)","57210957012; 57218451012; 57217861698; 6602159365; 7409240361","Super Resolution Guided Deep Network for Land Cover Classification from Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3120891","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117777912&doi=10.1109%2fTGRS.2021.3120891&partnerID=40&md5=3c92c3e9453a108dbed15f797fcad970","The low resolution of remote sensing images often limits the land cover classification (LCC) performance. Super resolution (SR) can improve the image resolution, while greatly increasing the computational burden for the LCC due to the larger size of the input image. In this article, the SR-guided deep network (SRGDN) framework is proposed, which can generate meaningful structures from higher resolution images to improve the LCC performance without consuming more computational costs. In general, the SRGDN consists of two branches (i.e., SR branch and LCC branch) and a guidance module. The SR branch aims to increase the resolution of remote sensing images. Since high- and low-resolution image pairs cannot be directly provided by imaging sensors to train the SR branch, we introduce a self-supervised generative adversarial network (GAN) to estimate the downsampling kernel that can produce these image pairs. The LCC branch adopts the high-resolution network (HRNet) to retain as much resolution information with a few downsampling operations as possible. The guidance module teaches the LCC branch to learn the high-resolution information from the SR branch without the utilization of the higher-resolution images as the inputs. Furthermore, the guidance module introduces spatial pyramid pooling (SPP) to match the feature maps of different sizes in the two branches. In the testing stage, the guidance module and SR branch can be removed, and therefore do not create additional computational costs. Experimental results on three real datasets demonstrate the superiority of the proposed method over several well-known LCC approaches.  © 1980-2012 IEEE.","Image classification; Image resolution; Remote sensing; Signal sampling; Classification performance; Computational costs; Down sampling; Guidance; High resolution; High-resolution images; Image pairs; Land cover classification; Remote sensing images; Superresolution; artificial neural network; image analysis; land cover; remote sensing; satellite imagery; Image enhancement","Guidance; land cover classification (LCC); remote sensing image; super resolution (SR)","Article","Final","","Scopus","2-s2.0-85117777912"
"Liu Y.; Liu Y.; Vanguri R.; Litwiller D.; Liu M.; Hsu H.-Y.; Ha R.; Shaish H.; Jambawalikar S.","Liu, Yucheng (57858913700); Liu, Yulin (57212010859); Vanguri, Rami (57214347841); Litwiller, Daniel (22135244300); Liu, Michael (57203289420); Hsu, Hao-Yun (57258480500); Ha, Richard (36137024800); Shaish, Hiram (56638530300); Jambawalikar, Sachin (6507408536)","57858913700; 57212010859; 57214347841; 22135244300; 57203289420; 57258480500; 36137024800; 56638530300; 6507408536","3D Isotropic Super-resolution Prostate MRI Using Generative Adversarial Networks and Unpaired Multiplane Slices","2021","Journal of Digital Imaging","34","5","","1199","1208","9","10.1007/s10278-021-00510-w","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114900224&doi=10.1007%2fs10278-021-00510-w&partnerID=40&md5=1429fdf167917c020a1a8a5f9f8bd907","We developed a deep learning–based super-resolution model for prostate MRI. 2D T2-weighted turbo spin echo (T2w-TSE) images are the core anatomical sequences in a multiparametric MRI (mpMRI) protocol. These images have coarse through-plane resolution, are non-isotropic, and have long acquisition times (approximately 10–15 min). The model we developed aims to preserve high-frequency details that are normally lost after 3D reconstruction. We propose a novel framework for generating isotropic volumes using generative adversarial networks (GAN) from anisotropic 2D T2w-TSE and single-shot fast spin echo (ssFSE) images. The CycleGAN model used in this study allows the unpaired dataset mapping to reconstruct super-resolution (SR) volumes. Fivefold cross-validation was performed. The improvements from patch-to-volume reconstruction (PVR) to SR are 80.17%, 63.77%, and 186% for perceptual index (PI), RMSE, and SSIM, respectively; the improvements from slice-to-volume reconstruction (SVR) to SR are 72.41%, 17.44%, and 7.5% for PI, RMSE, and SSIM, respectively. Five ssFSE cases were used to test for generalizability; the perceptual quality of SR images surpasses the in-plane ssFSE images by 37.5%, with 3.26% improvement in SSIM and a higher RMSE by 7.92%. SR images were quantitatively assessed with radiologist Likert scores. Our isotropic SR volumes are able to reproduce high-frequency detail, maintaining comparable image quality to in-plane TSE images in all planes without sacrificing perceptual accuracy. The SR reconstruction networks were also successfully applied to the ssFSE images, demonstrating that high-quality isotropic volume achieved from ultra-fast acquisition is feasible. © 2021, Society for Imaging Informatics in Medicine.","Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Male; Prostate; Deep learning; Image reconstruction; Magnetic resonance imaging; Optical resolving power; Spin polarization; Three dimensional computer graphics; Urology; Volume measurement; 3D reconstruction; Adversarial networks; High frequency HF; Perceptual quality; Reconstruction networks; Super resolution; Super-resolution models; Volume reconstruction; article; cross validation; deep learning; diagnostic test accuracy study; human; image quality; Likert scale; male; multiparametric magnetic resonance imaging; prostate; quantitative analysis; radiologist; diagnostic imaging; nuclear magnetic resonance imaging; three-dimensional imaging; Image enhancement","Generative adversarial network; Image quality; Prostate MRI; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85114900224"
"Hui Z.; Li J.; Gao X.; Wang X.","Hui, Zheng (57200213383); Li, Jie (7410068291); Gao, Xinbo (7403873424); Wang, Xiumei (35786636600)","57200213383; 7410068291; 7403873424; 35786636600","Progressive perception-oriented network for single image super-resolution","2021","Information Sciences","546","","","769","786","17","10.1016/j.ins.2020.08.114","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090978981&doi=10.1016%2fj.ins.2020.08.114&partnerID=40&md5=c0dd45019a0fd463fa609b53a6cbb864","Recently, it has been demonstrated that deep neural networks can significantly improve the performance of single image super-resolution (SISR). Numerous studies have concentrated on raising the quantitative quality of super-resolved (SR) images. However, these methods that target PSNR maximization usually produce blurred images at large upscaling factor. The introduction of generative adversarial networks (GANs) can mitigate this issue and show impressive results with synthetic high-frequency textures. Nevertheless, these GAN-based approaches always have a tendency to add fake textures and even artifacts to make the SR image of visually higher-resolution. In this paper, we propose a novel perceptual image super-resolution method that progressively generates visually high-quality results by constructing a stage-wise network. Specifically, the first phase concentrates on minimizing pixel-wise error, and the second stage utilizes the features extracted by the previous stage to pursue results with better structural retention. The final stage employs fine structure features distilled by the second phase to produce more realistic results. In this way, we can maintain the pixel, and structural level information in the perceptual image as much as possible. It is useful to note that the proposed method can build three types of images in a feed-forward process. Also, we explore a new generator that adopts multi-scale hierarchical features fusion. Extensive experiments on benchmark datasets show that our approach is superior to the state-of-the-art methods. Code is available at https://github.com/Zheng222/PPON. © 2020 Elsevier Inc.","Deep neural networks; Image resolution; Optical resolving power; Pixels; Textures; Adversarial networks; Benchmark datasets; Hierarchical features; Higher resolution; Image super resolutions; State-of-the-art methods; Structural levels; Structural retention; Image enhancement","Multi-scale hierarchical fusion; Perceptual image super-resolution; Progressive related works learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090978981"
"Huang C.-E.; Chang C.-C.; Li Y.-H.","Huang, Chi-En (57245274500); Chang, Ching-Chun (56056418200); Li, Yung-Hui (14121385000)","57245274500; 56056418200; 14121385000","Mask attention-SRGAN for mobile sensing networks","2021","Sensors","21","17","5973","","","","10.3390/s21175973","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114214526&doi=10.3390%2fs21175973&partnerID=40&md5=747354b81aef9255d9457f9cdbe49dc0","Biometrics has been shown to be an effective solution for the identity recognition problem, and iris recognition, as well as face recognition, are accurate biometric modalities, among others. The higher resolution inside the crucial region reveals details of the physiological characteristics which provides discriminative information to achieve extremely high recognition rate. Due to the growing needs for the IoT device in various applications, the image sensor is gradually integrated in the IoT device to decrease the cost, and low-cost image sensors may be preferable than high-cost ones. However, low-cost image sensors may not satisfy the minimum requirement of the resolution, which definitely leads to the decrease of the recognition accuracy. Therefore, how to maintain high accuracy for biometric systems without using expensive high-cost image sensors in mobile sensing networks becomes an interesting and important issue. In this paper, we proposed MA-SRGAN, a single image super-resolution (SISR) algorithm, based on the mask-attention mechanism used in Generative Adversarial Network (GAN). We modified the latest state-of-the-art (nESRGAN+) in the GAN-based SR model by adding an extra part of a discriminator with an additional loss term to force the GAN to pay more attention within the region of interest (ROI). The experiments were performed on the CASIA-Thousand-v4 dataset and the Celeb Attribute dataset. The experimental results show that the proposed method successfully learns the details of features inside the crucial region by enhancing the recognition accuracies after image super-resolution (SR). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Algorithms; Image Processing, Computer-Assisted; Iris; Research Design; Biometrics; Costs; Face recognition; Image enhancement; Image segmentation; Image sensors; Optical resolving power; Adversarial networks; Attention mechanisms; Identity recognition; Minimum requirements; Mobile sensing networks; Physiological characteristics; Recognition accuracy; The region of interest (ROI); algorithm; image processing; iris; methodology; Internet of things","Attention mechanism; Biometric authentication; Biometric identification; Generative Adversarial Network; Mobile sensing network; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114214526"
"El-Shal I.H.; Fahmy O.M.; Elattar M.A.","El-Shal, Ibrahim H. (57219396114); Fahmy, Omar M. (16315847600); Elattar, Mustafa A. (55951661000)","57219396114; 16315847600; 55951661000","License Plate Image Analysis Empowered by Generative Adversarial Neural Networks (GANs)","2022","IEEE Access","10","","","30846","30857","11","10.1109/ACCESS.2022.3157714","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126275144&doi=10.1109%2fACCESS.2022.3157714&partnerID=40&md5=c374e14970684b7814b3639ccda60e1c","Although the majority of existing License Plate (LP) recognition techniques have significant improvements in accuracy, they are still limited to ideal situations in which training data is correctly annotated with restricted scenarios. Moreover, images or videos are frequently used in monitoring systems that have Low Resolution (LR) quality. In this work, the problem of LP detection in digital images is addressed in the images of a naturalistic environment. Single-stage character segmentation and recognition are combined with adversarial Super-Resolution (SR) approaches to improve the quality of the LP by processing the LR images into High-Resolution (HR) images. This work proposes effective changes to the SRGAN network regarding the number of layers, an activation function, and the appropriate loss regularization using Total Variation (TV) loss. The main paper contribution can be summarized into presenting an end-to-end deep learning framework based on generative adversarial networks (GAN), which is able to generate realistic super-resolution images. Also, proposed adding a TV regularization to the loss function to help the model enhance the resolution of images. The proposed SRGAN can handle tiny $72\times 72$ images of LPs. The paper explores how SRGAN performed over different datasets from many aspects, such as visual analysis, PSNR, SSIM, and Optical Character Recognition (OCR). The experiments demonstrate that the suggested SRGAN can generate high-resolution images that improve the accuracy of the license plate recognition stage compared to other systems.  © 2013 IEEE.","Computer vision; Deep learning; Edge detection; Image analysis; Image enhancement; Image reconstruction; Image segmentation; License plates (automobile); Optical character recognition; Optical resolving power; Deep learning; Features extraction; Image color analysis; Image edge detection; Image super resolutions; Images reconstruction; License; Licenses plate recognition; Optical character recognition software; Single image super-resolution; Single images; Superresolution; Total variation loss; Total-variation; Generative adversarial networks","Computer vision; deep learning; generative adversarial networks; image reconstruction; license plate recognition; single image super-resolution; total variation loss","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126275144"
"","","","ICCAI 2021 - Conference Proceedings of 2021 7th International Conference on Computing and Artificial Intelligence","2021","ACM International Conference Proceeding Series","","","","","","501","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116233362&partnerID=40&md5=6360f753ace9e8cc6a9d2e597ca02bf0","The proceedings contain 75 papers. The topics discussed include: a survey of preprocessing methods for marine ship target detection based on video surveillance; defect detection for mobile phone cases based on improved yolo model; dried Robusta coffee bean quality classification using convolutional neural network algorithm; research on apple variety classification based on the combination of hyperspectral and deep learning; binary auto-encoders hashing with manifold similarity-preserving for image retrieval; research on instrument image generation method based on conditional generative adversarial network; medical images super resolution reconstruction based on residual network; and phase extraction of electronic speckle interference fringe image based on convolutional neural network.","","","Conference review","Final","","Scopus","2-s2.0-85116233362"
"Kantipudi M.V.V.P.; Vemuri S.; Sanipini V.K.","Kantipudi, M.V.V. Prasad (57158399800); Vemuri, Sailaja (57207945856); Sanipini, Venkata Kiran (57224307383)","57158399800; 57207945856; 57224307383","Deep Learning-based Image Super-resolution Algorithms – A Survey","2022","International Journal of Computing and Digital Systems","11","1","","413","421","8","10.12785/ijcds/110134","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123536492&doi=10.12785%2fijcds%2f110134&partnerID=40&md5=f685807372f4a34e103133af77b631b1","Image super-resolution (SR) is a technique of enhancing image by increasing spatial resolution of the image. By performing image SR, the pixel intensity in an image is increased. Based number of input images, the SR technique is categorized into two types as single-image SR (SISR) and multi-image SR (MISR). This work analyses different characteristics of SISR on different image datasets like medical and real-world images by using different quality factors such as peak signal to noise ratio (PSNR), structural similarity index (SSIM), and perceptual index (PI). Also, it examines the complexity of SISR schemes based on their computation time. Based on the detailed study it is identified that the deep-leaning-based SR using component learning is a best method in terms of quantitative, qualitative and computation time of generating SR image. Further, it is suggested that to improve the quality a greater number of convolutional network layers can be used in SR algorithms. © 2022 University of Bahrain. All rights reserved.","","Convolutional neural networks; Ensemble learning; Generative adversarial network (GAN); Single image super resolution (SSIR); Spatial transform","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123536492"
"Alqahtani H.; Kavakli-Thorne M.; Kumar G.","Alqahtani, Hamed (57194574403); Kavakli-Thorne, Manolya (6602420178); Kumar, Gulshan (35932222600)","57194574403; 6602420178; 35932222600","Applications of Generative Adversarial Networks (GANs): An Updated Review","2021","Archives of Computational Methods in Engineering","28","2","","525","552","27","10.1007/s11831-019-09388-y","73","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077091121&doi=10.1007%2fs11831-019-09388-y&partnerID=40&md5=5d2a51b74ad6a862ea25cb97896be7bf","Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This paper aims to present an overview of GANs, its different variants, and potential application in various domains. The paper attempts to identify GANs’ advantages, disadvantages and significant challenges to the successful implementation of GAN in different application areas. The main intention of this paper is to explore and present a comprehensive review of the crucial applications of GANs covering a variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects. © 2019, CIMNE, Barcelona, Spain.","Neural networks; Semantics; Supervised learning; Unsupervised learning; Adversarial networks; Annotated training data; Application area; Image super resolutions; Image synthesis; Real-world; Semantic images; Backpropagation","Generative adversarial networks; Neural networks; Supervised learning; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85077091121"
"Wang Y.; Sun G.; Guo S.","Wang, Yuwu (57217011318); Sun, Guobing (57197195443); Guo, Shengwei (57297857800)","57217011318; 57197195443; 57297857800","Target detection method for low-resolution remote sensing image based on ESRGAN and ReDet","2021","Photonics","8","10","431","","","","10.3390/photonics8100431","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117216653&doi=10.3390%2fphotonics8100431&partnerID=40&md5=59e6d2953b206376c2513f1de03862d1","With the widespread use of remote sensing images, low-resolution target detection in remote sensing images has become a hot research topic in the field of computer vision. In this paper, we propose a Target Detection on Super-Resolution Reconstruction (TDoSR) method to solve the problem of low target recognition rates in low-resolution remote sensing images under foggy con-ditions. The TDoSR method uses the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) to perform defogging and super-resolution reconstruction of foggy low-resolution remote sensing images. In the target detection part, the Rotation Equivariant Detector (ReDet) algo-rithm, which has a higher recognition rate at this stage, is used to identify and classify various types of targets. While a large number of experiments have been carried out on the remote sensing image dataset DOTA-v1.5, the results of this paper suggest that the proposed method achieves good results in the target detection of low-resolution foggy remote sensing images. The principal result of this paper demonstrates that the recognition rate of the TDoSR method increases by roughly 20% when compared with low-resolution foggy remote sensing images. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.","","ESRGAN; ReDet; Remote sensing images; Super-resolution reconstruction; Target detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117216653"
"Chen S.; Wu H.; Chen Y.","Chen, Siwen (57299734300); Wu, Huaiyu (9434844600); Chen, Yang (57196277752)","57299734300; 9434844600; 57196277752","Multi-Domain Image Super-Resolution Generative Adversarial Network for Low-Resolution Person Re-Identification","2021","Chinese Control Conference, CCC","2021-July","","","8353","8359","6","10.23919/CCC52363.2021.9549785","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117349864&doi=10.23919%2fCCC52363.2021.9549785&partnerID=40&md5=985a99695cbb7233c69e1fb0efa5ea56","Person re-identification (ReID) is an important task in video surveillance application. To address the issue that various low-resolutions and scale mismatching always exist in the real world, a multi-domain image-to-image translation network, termed Multi-Domain image Super-Resolution Generative Adversarial Network (MSRGAN), is proposed to learn the mapping relationship between the various low-resolution domains and the high-resolution domain. MSRGAN can ensure that the transferred image has a similar resolution as in the target domain. It is also able to keep the identity information of images from low-resolution domain during the translation. In addition, a novel ReID model, termed CSA-ReID in which channel attention and spatial attention module are introduced, is designed to learn resolution-invariant deep representations. The proposed method achieves 90.7% rank-1 accuracy and 96.4% rank-5 accuracy on multiple low-resolutions Market-1501 dataset. The experimental results prove that the proposed method achieves promising generalization ability and accuracy compared with the state-of-the-art methods. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.","Image processing; Optical resolving power; Security systems; Attention; Cross-resolution; Image super resolutions; Learn+; Lower resolution; Multi-domains; Person re identifications; Re identifications; Superresolution; Video-surveillance applications; Generative adversarial networks","attention; cross-resolution; person re-identification; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85117349864"
"Zhang X.; Kelkar V.A.; Granstedt J.; Li H.; Anastasio M.A.","Zhang, Xiaohui (57225101058); Kelkar, Varun A. (57204540505); Granstedt, Jason (57190435103); Li, Hua (57000055400); Anastasio, Mark A. (7006769220)","57225101058; 57204540505; 57190435103; 57000055400; 7006769220","Impact of deep learning-based image super-resolution on binary signal detection","2021","Journal of Medical Imaging","8","6","065501","","","","10.1117/1.JMI.8.6.065501","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122618006&doi=10.1117%2f1.JMI.8.6.065501&partnerID=40&md5=981b5770d41d6a10ae4b30cfdb2ea4f6","Purpose: Deep learning-based image super-resolution (DL-SR) has shown great promise in medical imaging applications. To date, most of the proposed methods for DL-SR have only been assessed using traditional measures of image quality (IQ) that are commonly employed in the field of computer vision. However, the impact of these methods on objective measures of IQ that are relevant to medical imaging tasks remains largely unexplored. We investigate the impact of DL-SR methods on binary signal detection performance. Approach: Two popular DL-SR methods, the super-resolution convolutional neural network and the super-resolution generative adversarial network, were trained using simulated medical image data. Binary signal-known-exactly with background-known-statistically and signal-known-statistically with background-known-statistically detection tasks were formulated. Numerical observers (NOs), which included a neural network-approximated ideal observer and common linear NOs, were employed to assess the impact of DL-SR on task performance. The impact of the complexity of the DL-SR network architectures on task performance was quantified. In addition, the utility of DL-SR for improving the task performance of suboptimal observers was investigated. Results: Our numerical experiments confirmed that, as expected, DL-SR improved traditional measures of IQ. However, for many of the study designs considered, the DL-SR methods provided little or no improvement in task performance and even degraded it. It was observed that DL-SR improved the task performance of suboptimal observers under certain conditions. Conclusions: Our study highlights the urgent need for the objective assessment of DL-SR methods and suggests avenues for improving their efficacy in medical imaging applications.  © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 International License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.","Deep learning; Generative adversarial networks; Image quality; Network architecture; Neural networks; Optical resolving power; Signal detection; Binary signals; Deep learning-based image super-resolution; Detection tasks; Image super resolutions; Numerical observer; Objective image quality assessment; Rayleigh; Rayleigh detection task; Superresolution methods; Task performance; article; controlled study; convolutional neural network; deep learning; diagnostic imaging; human; image quality; signal detection; simulation; task performance; Medical imaging","deep learning-based image super-resolution; numerical observers; objective image quality assessment; Rayleigh detection task","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85122618006"
"Han J.; Zheng H.; Chen D.Z.; Wang C.","Han, Jun (57209633331); Zheng, Hao (57207262621); Chen, Danny Z. (7405453271); Wang, Chaoli (57203797020)","57209633331; 57207262621; 7405453271; 57203797020","STNet: An End-to-End Generative Framework for Synthesizing Spatiotemporal Super-Resolution Volumes","2022","IEEE Transactions on Visualization and Computer Graphics","28","1","","270","280","10","10.1109/TVCG.2021.3114815","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118661225&doi=10.1109%2fTVCG.2021.3114815&partnerID=40&md5=8411fded2bcdd71ea74c443e0c2bf499","We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal super-resolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions (\mathsf{SSR}+\mathsf{TSF}, STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet.  © 1995-2012 IEEE.","Computer vision; Deep learning; Digital storage; Interpolation; Job analysis; Optical resolving power; Deep learning; End to end; Features extraction; Lower resolution; Spatiotemporal phenomenon; Spatiotemporal super-resolution; Superresolution; Task analysis; Time-varying data; article; compression; deep learning; information storage; short term memory; simulation; Generative adversarial networks","generative adversarial network; spatiotemporal super-resolution; Time-varying data","Article","Final","","Scopus","2-s2.0-85118661225"
"Zhu X.; Liu X.; Wang L.; Guo Z.; Wang J.; Wang R.; Sun Y.","Zhu, Xuan (55964217700); Liu, Xin (57769862600); Wang, Lin (56769098400); Guo, Zhenpeng (57210821386); Wang, Jun (55352702100); Wang, Rongzhi (57221058954); Sun, Yifei (57327783000)","55964217700; 57769862600; 56769098400; 57210821386; 55352702100; 57221058954; 57327783000","Temporal-spatial feature compensation combines with multi-feature discriminators for video super-resolution perceptual quality improvement","2021","Journal of Electronic Imaging","30","5","053005","","","","10.1117/1.JEI.30.5.053005","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118775399&doi=10.1117%2f1.JEI.30.5.053005&partnerID=40&md5=603467c894fb0f72306f11043097a174","Generative adversarial network (GAN) for super-resolution (SR) has attracted enormous interest in recent years. It has been widely used to solve the single-image super-resolution (SISR) task and made superior performance. However, GAN is rarely used for video super-resolution (VSR). VSR aims to improve video resolution by exploiting the temporal continuity and spatial similarity of video sequence frames. We design a GAN with multi-feature discriminators and combine it with optical flow estimation compensation to construct an end-To-end VSR framework OFC-MFGAN. Optical flow estimation compensation makes use of temporal continuity and spatial similarity features of adjacent frames to provide rich detailed information for GAN. Multi-feature discriminators based on visual attention mechanism include the pixel discriminator, edge discriminator, gray discriminator, and color discriminator. GAN with multi-feature discriminators makes the data distribution and visually sensitive features (edge, texture, and color) of SR frames similar to high-resolution frames. OFC-MFGAN effectively integrates the time, space, and visually sensitive features of videos. Extensive experiments on public video datasets and surveillance videos show the effectiveness and robustness of the proposed method. Compared with several state-of-The-Art VSR methods and SISR methods, the proposed method can not only recover prominent edges, clear textures, and realistic colors but also make a pleasant visual feeling and competitive perceptual index. © 2021 SPIE and IS&T.","Color; Data visualization; Deep learning; Generative adversarial networks; Image segmentation; Optical flows; Optical resolving power; Security systems; Textures; Deep learning; Image super resolutions; Multi-feature discriminator; Multifeatures; Perceptual quality; Single images; Spatial similarity; Superresolution; Temporal continuity; Video super-resolution; Behavioral research","deep learning; multi-feature discriminators; perceptual quality; video super-resolution","Article","Final","","Scopus","2-s2.0-85118775399"
"Li C.; Liu Y.; Sun J.","Li, Chang (57775800600); Liu, Yu (57221874222); Sun, Jinglin (57204103351)","57775800600; 57221874222; 57204103351","Optimization Method for Infrared Eye Movement Image Segmentation; [用 于 红 外 眼 动 图 像 分 割 的 优 化 方 法]","2022","Laser and Optoelectronics Progress","59","2","0210016","","","","10.3788/LOP202259.0210016","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124383909&doi=10.3788%2fLOP202259.0210016&partnerID=40&md5=8569b7cbaac0ebd1f563d34239374632","When the eye tracker is collecting infrared eye movement data, due to the rapid movement of the subject’s eyeballs or the inability to keep relatively still with the instrument, some of the collected eye area images are defocused and blurred. This paper proposes a semantic segmentation optimization system, which is called super real-time semantic segmentation network (S-RITnet). First, a pixel-level annotation data set with a 4∶ 1∶ 1 ratio of images in the training set, validation set, and test set is created. Then, the enhance super-resolution generative adversarial network and contrast-limited adaptive histogram enhancement algorithm are used to repair the blurred eye area data set image. Finally, based on real-time semantic segmentation net and the autonomous data set (including the repair data set), perform network training to realize the semantic segmentation of the eye area image and evaluate the obtained segmentation module. The experimental results show that the optimization scheme can effectively optimize the quality of eye area images. Compared with the low-quality eye images training module, the mean intersection over union and F1-score evaluation of S-RITnet increased by 0. 0247 and 0. 024 respectively. © 2022 Universitat zu Koln. All rights reserved.","","Generative adversarial network; Image enhancement; Image processing; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85124383909"
"Xie Q.; Yang T.; Pei S.; Xie J.; Lü F.","Xie, Qing (36085372600); Yang, Tianchi (57310685000); Pei, Shaotong (55899839800); Xie, Jun (56669053600); Lü, Fangcheng (55836118800)","36085372600; 57310685000; 55899839800; 56669053600; 55836118800","Super-Resolution Identification Method of Electrical Equipment Fault Based on Multi-Scale Cooperation Model; [基于多尺度协作模型的电气设备红外图像超分辨率故障辨识方法]","2021","Diangong Jishu Xuebao/Transactions of China Electrotechnical Society","36","21","","4608","4616","8","10.19595/j.cnki.1000-6753.tces.210273","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118717106&doi=10.19595%2fj.cnki.1000-6753.tces.210273&partnerID=40&md5=08b8aca6b8b7ba086a69f6ba547a2624","Aiming at the shortcomings of existing infrared images, such as low resolution and poor definition, which can easily affect the fault detection effect of electrical equipment based on infrared images, a super resolution fault identification method of infrared images of electrical equipment based on multi-scale collaboration model is proposed. Firstly, a super resolution reconstruction network for infrared images of electrical equipment is constructed based on multi-scale collaboration model. The network is based on generative adversarial network. By introducing multi-scale collaboration module and two-channel structure, the adaptability of the super resolution reconstruction network to infrared images is improved, and the effect of image feature extraction is optimized. On the basis of infrared image super-resolution reconstruction, combined with deep learning target detection method, a super-resolution fault identification model of infrared image of electrical equipment is established. Experimental verification of the proposed method is carried out. The experimental results show that the infrared image quality can be significantly improved after the proposed super resolution reconstruction network, and the PSNR and SSIM values can be increased to 27.26dB and 0.8283 respectively. The proposed infrared image super-resolution fault identification model can significantly improve the fault identification effect, mAP, mAR, mAC and mAIOU increased by 19.34%, 19.14%, 11.83% and 25.03% on average. © 2021, Electrical Technology Press Co. Ltd. All right reserved.","Deep learning; Fault detection; Image enhancement; Image reconstruction; Optical resolving power; Collaboration models; Electrical equipment; Fault identifications; Identification method; Identification modeling; Multi-scale collaboration; Multi-scales; Reconstruction networks; Super-resolution reconstruction; Superresolution; Infrared imaging","Fault identification; Infrared image; Multi-scale collaboration; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85118717106"
"Liu Y.; Qiao Y.; Hao Y.; Wang F.; Rashid S.F.","Liu, Ying (55976865400); Qiao, Yangge (57267115500); Hao, Yu (57192159081); Wang, Fuping (57020457000); Rashid, Sheikh Faisal (35747019000)","55976865400; 57267115500; 57192159081; 57020457000; 35747019000","Single image super resolution techniques based on deep learning: Status, applications and future directions","2021","Journal of Image and Graphics(United Kingdom)","9","3","","74","86","12","10.18178/joig.9.3.74-86","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115436323&doi=10.18178%2fjoig.9.3.74-86&partnerID=40&md5=6b61d8c61bbda66731da7414f312380a","Single Image Super Resolution (SISR) reconstruction aims to recover high-resolution images from corresponding Low-Resolution (LR) versions, which is essentially an ill-posed inverse problem. In recent years, learning-based methods have been frequently exploited to tackle this problem, which correspond to promising calculation efficiency and performance, especially in image sharpening processing based on deep neural networks. Learning-based methods can be generally categorized as conventional methods and deep learning-based methods. This survey aims to review deep learning-based image super-resolution methods, including Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN) based on internal network structure. Furthermore, this paper describes the applications of single-frame image super resolution in various practical fields. In addition, a few future research directions of image super resolution techniques are identified. © 2021 Journal of Image and Graphics.","","Convolutional neural networks; Deep learning; Generative adversarial networks; Single image super-resolution reconstruction","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85115436323"
"Bashir S.M.A.; Wang Y.","Bashir, Syed Muhammad Arsalan (56385198200); Wang, Yi (12763268500)","56385198200; 12763268500","Small object detection in remote sensing images with residual feature aggregation-based super-resolution and object detector network","2021","Remote Sensing","13","9","1854","","","","10.3390/rs13091854","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106517229&doi=10.3390%2frs13091854&partnerID=40&md5=98b765460da6dde1bfa9f0ccc129d0ab","This paper deals with detecting small objects in remote sensing images from satellites or any aerial vehicle by utilizing the concept of image super-resolution for image resolution enhancement using a deep-learning-based detection method. This paper provides a rationale for image super-resolution for small objects by improving the current super-resolution (SR) framework by incorporating a cyclic generative adversarial network (GAN) and residual feature aggregation (RFA) to improve detection performance. The novelty of the method is threefold: first, a framework is proposed, independent of the final object detector used in research, i.e., YOLOv3 could be replaced with Faster R-CNN or any object detector to perform object detection; second, a residual feature aggregation network was used in the generator, which significantly improved the detection performance as the RFA network detected complex features; and third, the whole network was transformed into a cyclic GAN. The image super-resolution cyclic GAN with RFA and YOLO as the detection network is termed as SRCGAN-RFA-YOLO, which is compared with the detection accuracies of other methods. Rigorous experiments on both satellite images and aerial images (ISPRS Potsdam, VAID, and Draper Satellite Image Chronology datasets) were performed, and the results showed that the detection performance increased by using super-resolution methods for spatial resolution enhancement; for an IoU of 0.10, AP of 0.7867 was achieved for a scale factor of 16. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aircraft detection; Antennas; Deep learning; Feature extraction; Image enhancement; Image resolution; Object recognition; Optical resolving power; Remote sensing; Small satellites; Adversarial networks; Detection performance; Image resolution enhancements; Image super resolutions; Remote sensing images; Small object detection; Spatial-resolution enhancement; Superresolution methods; Object detection","Deep learning; Generative adversarial networks; Image classification; Object detection in satellite images; Remote sensing; Residual feature aggregation (RFA); Vehicle detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85106517229"
"Hua J.; Liu M.; Wang S.","Hua, Jin (57201946275); Liu, Mengzhao (57219183129); Wang, Shujia (57318454000)","57201946275; 57219183129; 57318454000","A super-resolution reconstruction method of underwater target detection image by side scan sonar","2021","ACM International Conference Proceeding Series","","","3483869","135","140","5","10.1145/3483845.3483869","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118292648&doi=10.1145%2f3483845.3483869&partnerID=40&md5=51f13c0ef4d87fbb36bdd06e7f6ca532","However, the scope and distance of optical imaging were limited, especially in the case of muddy water, the propagation of optical information was seriously interfered, and imaging became more difficult. Due to the complex and changeable underwater environment and the nature of acoustic imaging, sonar image has noise, low resolution and fuzzy details, which has a great impact on the recognition and interpretation of sonar image. On the basis of the original SRGAN network, this paper improves and optimates its network structure and loss function. Replace the ordinary convolution layer with the void convolution layer in the residual block structure of the generated network, delete the batch normalization layer (BN layer), reduce the resource consumption and expand the receiver field, so as to improve the training efficiency of the network; A gradient penalty term is added to the improved discriminant network loss function to accelerate the convergence of the network and improve the stability of training. Four classical image super resolution algorithms are compared with the improved SRGAN algorithm under the verification of sonar dataset. The experimental results show that the improved SRGAN network is superior to the traditional super resolution method in the reconstruction of sonar image in terms of rich texture and details, and improves the quality of sonar image super resolution reconstruction. © 2021 ACM.","Convolution; Deep learning; Image denoising; Image enhancement; Image reconstruction; Optical resolving power; Sonar; Textures; Underwater imaging; Deep learning; Image super-resolution reconstruction; Loss functions; Muddy waters; Network loss; Reconstruction method; Side scan sonar; Sonar image; Super-resolution reconstruction; Underwater target detection; Generative adversarial networks","Deep learning; Generative Adversarial Network; Image denoising; Image super-resolution reconstruction; Sonar image","Conference paper","Final","","Scopus","2-s2.0-85118292648"
"Farooq M.; Dailey M.N.; Mahmood A.; Moonrinta J.; Ekpanyapong M.","Farooq, Muhammad (57212081048); Dailey, Matthew N. (56187964300); Mahmood, Arif (55636036300); Moonrinta, Jednipat (36460177500); Ekpanyapong, Mongkol (6506112110)","57212081048; 56187964300; 55636036300; 36460177500; 6506112110","Human face super-resolution on poor quality surveillance video footage","2021","Neural Computing and Applications","33","20","","13505","13523","18","10.1007/s00521-021-05973-0","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104103307&doi=10.1007%2fs00521-021-05973-0&partnerID=40&md5=10c866b5f78fe21082b5f3990b8d1892","Most super-resolution (SR) methods proposed to date do not use real ground-truth high-resolution (HR) and low-resolution (LR) image pairs; instead, the vast majority of methods use synthetic LR images generated from the HR images. This approach yields excellent performance on synthetic datasets, but on real-world poor quality surveillance video footage, they suffer from performance degradation. A promising alternative is to apply recent advances in style transfer for unpaired datasets, but state-of-the-art work along these lines has used LR images and HR images from completely different datasets, introducing more variation between the HR and LR domains than necessary. In this paper, we propose methods that overcome both of these shortcomings, applying unpaired style transfer learning methods to face SR but using HR and LR datasets that share important properties. The key is to acquire roughly paired training data from a high-quality main stream and a lower-quality sub-stream of the same IP camera. Based on this principle, we have constructed four datasets comprising more than 400 people, with 1–15 weakly aligned real HR–LR pairs for each subject. We adopt a cycle generative adversarial networks (Cycle GANs) approach that produces impressive super-resolved images for low-quality test images never seen during training. Experiments prove the efficacy of the method. The approach to face SR advocated for in this paper makes possible many real-world applications requiring the extraction of high-quality face images from low-resolution video streams such as those produced by security cameras. Developers of diverse applications such as face recognition, 3D face reconstruction, face alignment, face parsing, human–computer interaction, remote sensing, and access control will benefit from the methods introduced in this work. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Access control; Cameras; Data streams; Formal languages; Human computer interaction; Learning systems; Optical resolving power; Remote sensing; Security systems; Transfer learning; 3D face reconstruction; Adversarial networks; Computer interaction; Diverse applications; Low resolution images; Low resolution video; Performance degradation; Transfer learning methods; Face recognition","Cycle GANs; Face hallucination; Super-resolution","Article","Final","","Scopus","2-s2.0-85104103307"
"Wang Y.","Wang, Yin (57280413400)","57280413400","Single Image Super-Resolution with U-Net Generative Adversarial Networks","2021","IMCEC 2021 - IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference","","","","1835","1840","5","10.1109/IMCEC51613.2021.9482317","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116137089&doi=10.1109%2fIMCEC51613.2021.9482317&partnerID=40&md5=703283af8929460b2900e7123ea92c0b","It is very challenging to recover a High Resolution (HR) image with real texture from a single Low Resolution (LR) image. SRGAN[1] first applied GAN(Generative Adversarial Network) in the field of image Super-Resolution, and restored a relatively real texture HR. However, SRGAN's reconstructed HR often contains unreal artifacts and distortions. Subsequently ESRGAN[2] has improved this problem, but it still has the shortcomings of not sharp edges of objects and slow reconstruction speed. To further improve the perception quality and accelerate the reconstruction speed, we proposed an image super-resolution algorithm (SR) based on U-Net GAN [3]. As the basic block, we introduced the Residual-in-Residual Self-calibrated Convolution with Pixel Attention block(RRSCPA) [4]. From [5] to heuristic, we design the discriminator into a U-shaped structure, which can provide per-pixel feedback to the generator and promote the generator to generate a more realistic HR. Finally, we replaced the VGG-based perceptual loss[6] with the LPIPS perceptual loss[7] function. Our proposed U-Net SRGAN achieves consistently better visual quality with more realistic and natural textures than ESRGAN. © 2021 IEEE.","Computer vision; Image enhancement; Image reconstruction; Image texture; Optical resolving power; Pixels; Textures; High resolution; High-resolution images; Image super resolutions; Low resolution images; Perceived loss; Reconstruction speed; Sharp edges; Single images; Superresolution; U-net; Generative adversarial networks","Generative Adversarial Network; perceived loss; Super-Resolution; U-Net","Conference paper","Final","","Scopus","2-s2.0-85116137089"
"Mirchandani K.; Chordiya K.","Mirchandani, Kapil (57220022676); Chordiya, Kushal (57220033469)","57220022676; 57220033469","DPSRGAN: Dilation Patch Super-Resolution Generative Adversarial Networks","2021","2021 6th International Conference for Convergence in Technology, I2CT 2021","","","9417903","","","","10.1109/I2CT51068.2021.9417903","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106501798&doi=10.1109%2fI2CT51068.2021.9417903&partnerID=40&md5=818248b6a44e1d11e0d12be17402d9ed","Single Image Super-Resolution (SISR) has proven itself as a highly challenging and ill-posed problem. Multiple methods have been applied to this problem in the past, with varying degrees of success. Recently, methods using deep learning such as Generative Adversarial Networks (GAN) and Variational Auto-Encoders (VAE) in particular have proven to be extremely effective. However, most of the present methods either create a blurry output, lacking fine details, or use extremely heavy models to achieve better results. We introduce a novel, lightweight GAN architecture for 4× super-resolution of images, which builds on previous methods, showing high quality of features both qualitatively and quantitatively. To achieve this, we use dilated convolutions in our generator architecture, a Markovian discriminator, a modified loss function and a training process more typical of a conditional GAN (cGAN). For testing our results qualitatively, we use Mean Opinion Score (MOS). The obtained MOS show the effectiveness of our model at generating visually superior images. Our code is available at https://www.github.com/kushalchordiya216/Super-Resolution.  © 2021 IEEE.","Deep learning; Network architecture; Optical resolving power; Adversarial networks; Auto encoders; Ill posed problem; Loss functions; Mean opinion scores; Multiple methods; Super resolution; Training process; Learning systems","Generative Adversarial Network; SRGAN; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85106501798"
"Wang J.; Shao Z.; Huang X.; Lu T.; Zhang R.; Ma J.","Wang, Jiaming (57206676342); Shao, Zhenfeng (57203905559); Huang, Xiao (57201292422); Lu, Tao (56406646300); Zhang, Ruiqian (57190385256); Ma, Jiayi (26638975600)","57206676342; 57203905559; 57201292422; 56406646300; 57190385256; 26638975600","Enhanced image prior for unsupervised remoting sensing super-resolution","2021","Neural Networks","143","","","400","412","12","10.1016/j.neunet.2021.06.005","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109071626&doi=10.1016%2fj.neunet.2021.06.005&partnerID=40&md5=70dcc8ff5c43455549965e9d7e5d66ca","Numerous approaches based on training low-high resolution image pairs have been proposed to address the super-resolution (SR) task. Despite their success, low-high resolution image pairs are usually difficult to obtain in certain scenarios, and these methods are limited in the actual scene (unknown or non-ideal image acquisition process). In this paper, we proposed a novel unsupervised learning framework, termed Enhanced Image Prior (EIP), which achieves SR tasks without low/high resolution image pairs. We first feed random noise maps into a designed generative adversarial network (GAN) for satellite image SR reconstruction. Then, we convert the reference image to latent space as the enhanced image prior. Finally, we update the input noise in the latent space with a recurrent updating strategy, and further transfer the texture and structured information from the reference image. Results on extensive experiments on the Draper dataset show that EIP achieves significant improvements over state-of-the-art unsupervised SR methods both quantitatively and qualitatively. Our experiments on satellite (SuperView-1) images reveal the potential of the proposed approach in improving the resolution of remote sensing imagery compared with the supervised algorithms. Source code is available at https://github.com/jiaming-wang/EIP. © 2021 Elsevier Ltd","Algorithms; Image Processing, Computer-Assisted; Image enhancement; Optical resolving power; Remote sensing; Textures; Unsupervised learning; High-resolution images; Ideal images; Image pairs; Image priors; Latent space; Low-high; Nonideal; Prior enhancement; Reference image; Super resolution; algorithm; article; human; human experiment; learning; noise; quantitative analysis; satellite imagery; image processing; Satellite imagery","Latent space; Prior enhancement; Satellite imagery; Super resolution; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85109071626"
"Du J.; Cheng K.; Yu Y.; Wang D.; Zhou H.","Du, Juan (57208501019); Cheng, Kuanhong (57188822983); Yu, Yue (55897991900); Wang, Dabao (25722876900); Zhou, Huixin (7404743079)","57208501019; 57188822983; 55897991900; 25722876900; 7404743079","Panchromatic image super-resolution via self attention-augmented wasserstein generative adversarial network","2021","Sensors","21","6","2158","1","15","14","10.3390/s21062158","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102680979&doi=10.3390%2fs21062158&partnerID=40&md5=1dc79c0439252253e08b521b763a2b82","Panchromatic (PAN) images contain abundant spatial information that is useful for earth observation, but always suffer from low-resolution (LR) due to the sensor limitation and large-scale view field. The current super-resolution (SR) methods based on traditional attention mechanism have shown remarkable advantages but remain imperfect to reconstruct the edge details of SR images. To address this problem, an improved SR model which involves the self-attention augmented Wasserstein generative adversarial network (SAA-WGAN) is designed to dig out the reference information among multiple features for detail enhancement. We use an encoder-decoder network followed by a fully convolutional network (FCN) as the backbone to extract multi-scale features and reconstruct the High-resolution (HR) results. To exploit the relevance between multi-layer feature maps, we first integrate a convolutional block attention module (CBAM) into each skip-connection of the encoder-decoder subnet, generating weighted maps to enhance both channel-wise and spatial-wise feature representation automatically. Besides, considering that the HR results and LR inputs are highly similar in structure, yet cannot be fully reflected in traditional attention mechanism, we, therefore, designed a self augmented attention (SAA) module, where the attention weights are produced dynamically via a similarity function between hidden features; this design allows the network to flexibly adjust the fraction relevance among multi-layer features and keep the long-range inter information, which is helpful to preserve details. In addition, the pixel-wise loss is combined with perceptual and gradient loss to achieve comprehensive supervision. Experiments on benchmark datasets demonstrate that the proposed method outperforms other SR methods in terms of both objective evaluation and visual effect. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Channel coding; Convolution; Decoding; Image resolution; Optical resolving power; Signal encoding; Adversarial networks; Attention mechanisms; Convolutional networks; Feature representation; Multi-scale features; Objective evaluation; Panchromatic (Pan) image; Spatial informations; article; attention; Convolutional neural networks","Attention-augmented convolution; Panchromatic images; Super resolution; WGAN","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102680979"
"Umer R.M.; Munir A.; Micheloni C.","Umer, Rao Muhammad (57211291072); Munir, Asad (57195483746); Micheloni, Christian (6507976201)","57211291072; 57195483746; 6507976201","A deep residual star generative adversarial network for multi-domain image super-resolution","2021","2021 6th International Conference on Smart and Sustainable Technologies, SpliTech 2021","","","","","","","10.23919/SpliTech52315.2021.9566406","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118460346&doi=10.23919%2fSpliTech52315.2021.9566406&partnerID=40&md5=4893c19c9491420b7152e6a7f93529a4","Recently, most of state-of-the-art single image super-resolution (SISR) methods have attained impressive performance by using deep convolutional neural networks (DCNNs). The existing SR methods have limited performance due to a fixed degradation settings, i.e. usually a bicubic downscaling of low-resolution (LR) image. However, in real-world settings, the LR degradation process is unknown which can be bicubic LR, bilinear LR, nearest-neighbor LR, or real LR. Therefore, most SR methods are ineffective and inefficient in handling more than one degradation settings within a single network. To handle the multiple degradation, i.e. refers to multi-domain image super-resolution, we propose a deep Super-Resolution Residual StarGAN (SR2*GAN), a novel and scalable approach that super-resolves the LR images for the multiple LR domains using only a single model. The proposed scheme is trained in a StarGAN like network topology with a single generator and discriminator networks. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments compared to other state-of-the-art methods. © 2021 University of Split, FESB.","Convolutional neural networks; Deep neural networks; Optical resolving power; Deep learning; GAN; Image super resolutions; Low resolution images; Lower resolution; Multi-domain SR; Multi-domains; Performance; Single image super-resolution; Single images; Generative adversarial networks","Deep Learning; GAN; Multi-domain SR; Single Image Super-Resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85118460346"
"Han S.; Lee T.B.; Heo Y.S.","Han, Sujy (57236496900); Lee, Tae Bok (57221229677); Heo, Yong Seok (57201765185)","57236496900; 57221229677; 57201765185","Deep image prior for super resolution of noisy image","2021","Electronics (Switzerland)","10","16","2014","","","","10.3390/electronics10162014","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113733800&doi=10.3390%2felectronics10162014&partnerID=40&md5=ca8557174306ac5d0d0896df1351056c","Single image super-resolution task aims to reconstruct a high-resolution image from a low-resolution image. Recently, it has been shown that by using deep image prior (DIP), a single neural network is sufficient to capture low-level image statistics using only a single image without data-driven training such that it can be used for various image restoration problems. However, super-resolution tasks are difficult to perform with DIP when the target image is noisy. The super-resolved image becomes noisy because the reconstruction loss of DIP does not consider the noise in the target image. Furthermore, when the target image contains noise, the optimization process of DIP becomes unstable and sensitive to noise. In this paper, we propose a noise-robust and stable framework based on DIP. To this end, we propose a noise-estimation method using the generative adversarial network (GAN) and self-supervision loss (SSL). We show that a generator of DIP can learn the distribution of noise in the target image with the proposed framework. Moreover, we argue that the optimization process of DIP is stabilized when the proposed self-supervision loss is incorporated. The experiments show that the proposed method quantitatively and qualitatively outperforms existing single image super-resolution methods for noisy images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Deep image prior; Image restoration; Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113733800"
"Zhu Y.; Ma H.; Peng J.; Liu D.; Xiong Z.","Zhu, Yunan (57278459400); Ma, Haichuan (57200651920); Peng, Jialun (57222956588); Liu, Dong (56597085700); Xiong, Zhiwei (57207267772)","57278459400; 57200651920; 57222956588; 56597085700; 57207267772","Recycling Discriminator: Towards Opinion-Unaware Image Quality Assessment Using Wasserstein GAN","2021","MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia","","","","116","125","9","10.1145/3474085.3479234","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119359418&doi=10.1145%2f3474085.3479234&partnerID=40&md5=b4739bbab7acf2f7ca7b39efdfbfc0a5","Generative adversarial networks (GANs) have been extensively used for training networks that perform image generation. After training, the discriminator in GAN was not used anymore. We propose to recycle the trained discriminator for another use: no-reference image quality assessment (NR-IQA). We are motivated by twofold facts. First, in Wasserstein GAN (WGAN), the discriminator is designed to calculate the distance between the distribution of generated images and that of real images; thus, the trained discriminator may encode the distribution of real-world images. Second, NR-IQA often needs to leverage the distribution of real-world images for assessing image quality. We then conjecture that using the trained discriminator for NR-IQA may help get rid of any human-labeled quality opinion scores and lead to a new opinion-unaware (OU) method. To validate our conjecture, we start from a restricted NR-IQA problem, that is IQA for artificially super-resolved images. We train super-resolution (SR) WGAN with two kinds of discriminators: one is to directly evaluate the entire image, and the other is to work on small patches. For the latter kind, we obtain patch-wise quality scores, and then have the flexibility to fuse the scores, e.g., by weighted average. Moreover, we directly extend the trained discriminators for authentically distorted images that have different kinds of distortions. Our experimental results demonstrate that the proposed method is comparable to the state-of-the-art OU NR-IQA methods on SR images and is even better than them on authentically distorted images. Our method provides a better interpretable approach to NR-IQA. Our code and models are available at https://github.com/YunanZhu/RecycleD. © 2021 ACM.","Discriminators; Image quality; Quality control; Recycling; Distorted images; Image generations; Image quality assessment; No-reference; No-reference images; Opinion-unaware; Real-world image; Superresolution; Training network; Wasserstein gan .; Generative adversarial networks","image quality assessment; no-reference; opinion-unaware; wasserstein gan (wgan).","Conference paper","Final","","Scopus","2-s2.0-85119359418"
"Wang J.; Teng G.; An P.","Wang, Jialu (57221968952); Teng, Guowei (8681431300); An, Ping (35242112900)","57221968952; 8681431300; 35242112900","Video super-resolution based on generative adversarial network and edge enhancement","2021","Electronics (Switzerland)","10","4","459","1","19","18","10.3390/electronics10040459","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100748750&doi=10.3390%2felectronics10040459&partnerID=40&md5=c5b53077924c991ab0e498955cc138b0","With the help of deep neural networks, video super-resolution (VSR) has made a huge breakthrough. However, these deep learning-based methods are rarely used in specific situations. In addition, training sets may not be suitable because many methods only assume that under ideal circumstances, low-resolution (LR) datasets are downgraded from high-resolution (HR) datasets in a fixed manner. In this paper, we proposed a model based on Generative Adversarial Network (GAN) and edge enhancement to perform super-resolution (SR) reconstruction for LR and blur vid-eos, such as closed-circuit television (CCTV). The adversarial loss allows discriminators to be trained to distinguish between SR frames and ground truth (GT) frames, which is helpful to produce realistic and highly detailed results. The edge enhancement function uses the Laplacian edge mod-ule to perform edge enhancement on the intermediate result, which helps further improve the final results. In addition, we add the perceptual loss to the loss function to obtain a higher visual experi-ence. At the same time, we also tried training network on different datasets. A large number of experiments show that our method has advantages in the Vid4 dataset and other LR videos. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Edge enhancement; Generative adversarial networks; Video super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85100748750"
"Yang S.","Yang, Shuguang (57226444553)","57226444553","Perceptually enhanced super-resolution reconstruction model based on deep back projection; [基于深度反向投影的感知增强超分辨率重建模型]","2021","Journal of Applied Optics","42","4","","691","697and716","697025","10.5768/JAO202142.0402009","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111574035&doi=10.5768%2fJAO202142.0402009&partnerID=40&md5=5761fcf14facdbf99804e7f87da99043","The super-resolution reconstruction models represented by the super-resolution convolutional neural network (SRCNN) models usually have high peak signal to noise ratio (PSNR) and structural similarity index measure (SSIM) values, but its visual perception is not satisfactory. And the generative adversarial networks (GAN) models represented by the super-resolution generative adversarial networks (SRGAN) having high perceptual quality is prone to produce a lot of false details, which is manifested in its low PSNR and SSIM values. To solve the above problems, a perceptually enhanced super-resolution reconstruction model based on deep back projection was proposed. The dual-scale self-adaptive weighted fusion feature extraction module was adopted by this model for feature extraction, then the sampling was carried out by the deep back projection, and finally the final output was obtained after the enhanced module was enhanced. The residual connections and dense connections were adopted by the model, which facilitated the features sharing and the effective training of the model. In the index evaluation, the learned perceptual image patch similarity (LPIPS) metric based on the learning was introduced as a new quality evaluation index of image perception, together with PSNR and SSIM as the model evaluation index. The experimental results show that the average values of PSNR, SSIM, and LPIPS of the model on the test data set are 27.84, 0.7320, and 0.1258, respectively, and all the indicators are better than the comparison algorithm. Copyright ©2021 Journal of Applied Optics. All rights reserved.","","Deep back projection; Learned perceptual image patch similarity metric; Perceived quality; Super-resolution reconstruction","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85111574035"
"Gowramma G.S.; Kishor Kumar R.; Manish Kumar S.; Monusha D.E.","Gowramma, G.S. (57218830731); Kishor Kumar, R. (57351649900); Manish Kumar, S. (57352399500); Monusha, D.E. (57351956300)","57218830731; 57351649900; 57352399500; 57351956300","Single Picture Super-Resolution Using Generative Adversarial Network","2022","Lecture Notes in Electrical Engineering","783","","","1255","1265","10","10.1007/978-981-16-3690-5_120","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119869754&doi=10.1007%2f978-981-16-3690-5_120&partnerID=40&md5=dc52cddcba98ffd20c9d770cacd89973","Picture Super-resolution is a by and large analysed issue in computer vision, where the objective is to change over a low-resolution picture to a high-resolution picture. As of now, deep learning methods such as convolution neural systems and generative adversarial networks are being utilized to perform super-resolution with results competitive to the best in class. In this paper, a generative adversarial network, SRGAN, is proposed for super-resolution with a perceptual loss work comprising of an adversarial loss, mean squared loss, and content loss. The target of our usage is to get familiar with an end-to-end mapping between the low and high-resolution pictures furthermore, enhance the up-scaled picture for quantitative measurements as well as absolute quality. We at that point think about our outcomes with the present cutting-edge strategies in super-resolution, lead proof of idea division study to show that super-resolved pictures can be utilized as a compelling pre-processing step before division and approve the findings measurably. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Cutting; Deep learning; Generative adversarial networks; Optical resolving power; Deep learning; End to end; High resolution; Learning methods; Lower resolution; MSE; Neural systems; PSNR; SSIM; Superresolution; Python","Deep learning; Generative adversarial networks; MSE; PSNR; Python; SSIM; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85119869754"
"Li H.; Prasad R.G.N.; Sekuboyina A.; Niu C.; Bai S.; Hemmert W.; Menze B.","Li, Hongwei (57007362000); Prasad, Rameshwara G. N. (57222529034); Sekuboyina, Anjany (57191334803); Niu, Chen (55207626200); Bai, Siwei (56818421500); Hemmert, Werner (6601954688); Menze, Bjoern (35299840300)","57007362000; 57222529034; 57191334803; 55207626200; 56818421500; 6601954688; 35299840300","Micro-Ct synthesis and inner ear super resolution via generative adversarial networks and bayesian inference","2021","Proceedings - International Symposium on Biomedical Imaging","2021-April","","9434061","1500","1504","4","10.1109/ISBI48211.2021.9434061","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107199725&doi=10.1109%2fISBI48211.2021.9434061&partnerID=40&md5=e24eb1bbcee858dd009e1c6f4ed18dc3","Existing medical image super-resolution methods rely on pairs of low-and high-resolution images to learn a mapping in a fully supervised manner. However, such image pairs are often not available in clinical practice. In this paper, we address super resolution problem in a real-world scenario using unpaired data and synthesize linearly eight times higher resolved Micro-CT images of temporal bone structure embedded in the inner ear. We explore cycle-consistency generative adversarial networks for super-resolution and equip the model with Bayesian inference. We further introduce Hu Moments distance as the evaluation metric to quantify the shape of the temporal bone. We evaluate our method on a public inner ear CT dataset and have seen both visual and quantitative improvement over state-of-the-art supervised deep-learning based methods. Further, we conduct a multi-rater visual evaluation experiment and find that three inner-ear researchers consistently rate our method highest quality scores among three methods. Furthermore, we are able to quantify uncertainty in the unpaired translation task and the uncertainty map can provide structural information of the temporal bone.  © 2021 IEEE.","Bayesian networks; Deep learning; Inference engines; Medical imaging; Optical resolving power; Quality control; Adversarial networks; Clinical practices; Evaluation metrics; High resolution image; Image super resolutions; Learning-based methods; Real-world scenario; Structural information; Computerized tomography","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107199725"
"Hui B.; Liu Y.; Qiu J.; Cao L.; Ji L.; He Z.","Hui, Bei (16506742000); Liu, Yanbo (57203151282); Qiu, Jiajun (56939647100); Cao, Likun (57200412759); Ji, Lin (36650670800); He, Zhiqiang (57218792740)","16506742000; 57203151282; 56939647100; 57200412759; 36650670800; 57218792740","Study of texture segmentation and classification for grading small hepatocellular carcinoma based on CT images","2021","Tsinghua Science and Technology","26","2","9147157","199","207","8","10.26599/TST.2019.9010058","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090283217&doi=10.26599%2fTST.2019.9010058&partnerID=40&md5=890a7cf2e0c4e09b0e16169caf9c55a1","To grade Small Hepatocellular CarCinoma (SHCC) using texture analysis of CT images, we retrospectively analysed 68 cases of Grade II (medium-differentiation) and 37 cases of Grades III and IV (high-differentiation). The grading scheme follows 4 stages: (1) training a Super Resolution Generative Adversarial Network (SRGAN) migration learning model on the Lung Nodule Analysis 2016 Dataset, and employing this model to reconstruct Super Resolution Images of the SHCC Dataset (SR-SHCC) images; (2) designing a texture clustering method based on Gray-Level Co-occurrence Matrix (GLCM) to segment tumour regions, which are Regions Of Interest (ROIs), from the original and SR-SHCC images, respectively; (3) extracting texture features on the ROIs; (4) performing statistical analysis and classifications. The segmentation achieved accuracies of 0.9049 and 0.8590 in the original SHCC images and the SR-SHCC images, respectively. The classification achived an accuracy of 0.838 and an Area Under the ROC Curve (AUC) of 0.84. The grading scheme can effectively reduce poor impacts on the texture analysis of SHCC ROIs. It may play a guiding role for physicians in early diagnoses of medium-differentiation and high-differentiation in SHCC. © 1996-2012 Tsinghua University Press.","Classification (of information); Cluster analysis; Diagnosis; Grading; Image analysis; Image classification; Image segmentation; Image texture; Optical resolving power; Textures; Adversarial networks; Area under the ROC curve; Gray level co occurrence matrix(GLCM); Hepatocellular carcinoma; Regions of interest; Texture analysis; Texture clustering; Texture segmentation; Computerized tomography","grading of Small Hepatocellular CarCinoma (SHCC); Gray-Level Co-occurrence Matrix (GLCM); super-resolution reconstruction; texture clustering","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85090283217"
"Shen P.; Zhang L.; Wang M.; Yin G.","Shen, Pengyang (57224172924); Zhang, Liguo (56681766700); Wang, Minghao (57219377342); Yin, Guisheng (55817240200)","57224172924; 56681766700; 57219377342; 55817240200","Deeper super-resolution generative adversarial network with gradient penalty for sonar image enhancement","2021","Multimedia Tools and Applications","80","18","","28087","28107","20","10.1007/s11042-021-10888-y","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107128291&doi=10.1007%2fs11042-021-10888-y&partnerID=40&md5=23e291f701031e0fd3628b2a385b93e3","In the field of underwater wireless communication, the acoustic signal has the advantages of low attenuation, long propagation distance, and high fidelity compared with electromagnetic wave signals. Therefore, as an important acoustic sensor, the sonar has been widely used in underwater topographical surveying, underwater search and rescue, ship navigation, etc. The sonar image also faces challenge of low-resolution due to its imaging mechanism, like ultrasonic image and synthetic aperture radar image. In this paper, we propose a deeper super-resolution generative adversarial network (DGP-SRGAN) with gradient penalty. It can be used to produce the sonar image with high-resolution. The main contribution of our method is that the gradient penalty is added to the loss function for a more stable and faster training network. The deep of the generator network is doubled from the original 16 layers to 32 layers to make the network more expressive, achieving its better performance. The loss function of the discriminator network increases the gradient penalty term. It can insure a faster network converge and then reach a stable state in less time. Thus the proposed network model achieves a better super-resolution reconstruction effect. The experimental results show that DGP-SRGAN can control the output of super-resolution images well based on input conditions. Meanwhile, the quality of the output image has improved significantly when compared with the other methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Acoustic wave propagation; Electromagnetic waves; Marine communication; Optical resolving power; Radar imaging; Sonar; Surveying; Synthetic aperture radar; Ultrasonic imaging; Underwater acoustic communication; Underwater imaging; Adversarial networks; Imaging mechanism; Propagation distances; Super resolution; Super resolution reconstruction; Training network; Underwater searches; Wireless communications; Image enhancement","Generative adversarial network; Gradient penalty; Sonar image; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85107128291"
"Liu P.-W.; Gao Y.; Qin P.-L.; Yin Z.; Wang L.-F.","Liu, Peng-Wei (57337262400); Gao, Yuan (57221270944); Qin, Pin-Le (23393704900); Yin, Zhe (57336290000); Wang, Li-Fang (57142669800)","57337262400; 57221270944; 23393704900; 57336290000; 57142669800","Medical MRI Image Super-Resolution Reconstruction Network Based on Multi-Scale Residual Generative Adversarial Network; [基于多尺度残差的生成对抗网络医学MRI影像超分辨率重建]","2021","Zhongbei Daxue Xuebao (Ziran Kexue Ban)/Journal of North University of China (Natural Science Edition)","42","5","","449","459","10","10.3969/j.issn.1673-3193.2021.05.010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119019879&doi=10.3969%2fj.issn.1673-3193.2021.05.010&partnerID=40&md5=0b2b0689e954b9e61a607c08e156067f","In order to solve the problems of missing details and unclear texture of medical MRI image caused by noise, imaging technology and other interference factors in the process of medical imaging, a super-resolution reconstruction network of medical MRI image based on multi-scale residuals generative adversarial network was proposed. Firstly, the multi-scale residual group was used to improve the residual blocks in the network; the local residual feature aggregation module aggregated the residual groups together to realize the non-local use of residual features and reduce the loss of local features in the process of network transmission. Then, the attention module aimed to enable the network to obtain channel and spatial feature information with a higher degree of response to the key information, so as to improve the detail texture effect of reconstructed image. Next, the gradient image of the low resolution image was transformed into the gradient image of the high resolution image to assist the reconstruction of the super-resolution image. Finally, the restored gradient image was integrated into the super resolution branch to provide structural prior information for super resolution reconstruction, so as to clearly guide the generation of high quality super resolution image. Compared with the super-resolution algorithm based on structure-preserving super resolution with gradient guidance(SPSR), the peak signal-to-noise ratio of the proposed algorithm at ×2, ×3 and ×4 scales were increased by an average of 0.72 dB, and the experimental results show that the texture details of medical MRI images reconstructed by the proposed algorithm are richer and the visual effect is more realistic. © 2021, Editorial Department of Journal of North University of China (Natural Science Edition). All right reserved.","","Attention mechanism; Gradient map; Local residual aggregation; Multi-scale residuals; Super resolution","Article","Final","","Scopus","2-s2.0-85119019879"
"Aishwarya G.; Krishnan R.K.","Aishwarya, G. (57200596578); Krishnan, Raghesh K. (55568643200)","57200596578; 55568643200","Generative adversarial networks for facial image inpainting and super-resolution","2021","Journal of Physics: Conference Series","2070","1","012103","","","","10.1088/1742-6596/2070/1/012103","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120426493&doi=10.1088%2f1742-6596%2f2070%2f1%2f012103&partnerID=40&md5=59c62e82455976b12aba882409209ba9","Inpainting helps to fill in the lost data in visual images. Inpainting techniques also refer to unusual image editing in distorted regions. These include areas that are noisy, blurred and watery areas. The most appropriate pixel values must be replaced in these regions to achieve good performance. Artists used to play it, and still now, pieces that are not in the picture can be inpainted in the same manner, though it takes more time. In the present age of automation, inpainting can be automated to obtain quicker and better outcomes by deep learning technologies. In this area, many of the latest techniques have been created, however, many methods produce blurred findings and data loss. Two adversarial networks are used to achieve this task, where first network aims at inpainting and the second network aims at super-resolution. The input generated as a part of first stage network is passed on to the second stage super-resolution network to overcome blurriness that is caused in the initial inpainting network. The network efficiency is determined in terms of increased PSNR obtained which is 28.19 dB with less training period of approximately 14 hours in comparison with other network models which performs similar task. © Content from this work may be used under the terms of the Creative Commons Attribution 3.0 licence.","Deep learning; Generative adversarial networks; Facial images; Image editing; Image Inpainting; Inpainting; Inpainting techniques; Learning technology; Performance; Pixel values; Superresolution; Visual image; Optical resolving power","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85120426493"
"Zhang Z.; Zhang C.; Wu M.; Han Y.; Yin H.; Kong A.; Chen F.","Zhang, Ziyun (57215773184); Zhang, Chengming (16835435800); Wu, Menxin (7405594817); Han, Yingjuan (57225052187); Yin, Hao (57215779384); Kong, Ailing (57215534548); Chen, Fangfang (57225034051)","57215773184; 16835435800; 7405594817; 57225052187; 57215779384; 57215534548; 57225034051","Super-resolution method using generative adversarial network for Gaofen wide-field-view images","2021","Journal of Applied Remote Sensing","15","2","028506","","","","10.1117/1.JRS.15.028506","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109017050&doi=10.1117%2f1.JRS.15.028506&partnerID=40&md5=1e874a476e14df20afcb3c7a1509cd13","Accurate information on the spatial distribution of crops is of great significance for scientific research and production practices. Such accurate information can be extracted from high-spatial-resolution optical remote sensing images. However, acquiring these images with a wide coverage is difficult. We established a model named multispectral super-resolution generative adversarial network (MS_SRGAN) for generating high-resolution 4-m images using Gaofen 1 wide-field-view (WFV) 16-m images. The MS_SRGAN model contains a generator and a discriminator. The generator network is composed of feature extraction units and feature fusion units with a symmetric structure, and the attention mechanism is introduced to constrain the spectral value of the feature map during feature extraction. The generator loss introduces feature loss to describe the feature difference of the image. This is realized using pre-trained discriminator parameters and a partial discriminator network. In addition to realizing feature loss, the discriminator network, which is a simple convolutional neural network, also realizes adversarial loss. Adversarial loss can provide some fake high frequency details to the generator to get a more sharpened image. In the Gaofen 1 WFV image test, the performance of MS_SRGAN was compared with that of Bicubic, EDSR, SRGAN, and ESRGAN. The results show that the spectral angle mapper (3.387) and structural similarity index measure (0.998) of MS_SRGAN are higher than those of the other models. In addition, the image obtained by MS_SRGAN is more realistic; its texture details and color distribution are closer to the reference image to a greater extent.  © The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.","Convolutional neural networks; Extraction; Feature extraction; Image acquisition; Optical resolving power; Remote sensing; Textures; Attention mechanisms; High spatial resolution; Optical remote sensing; Scientific researches; Spectral angle mappers; Structural similarity index measures; Superresolution methods; Symmetric structures; Discriminators","convolutional neural network; Gaofen 1 WFV image; Gaofen 2 image; generative adversarial network; multispectral image; super-resolution","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85109017050"
"Jiang M.; Zhi M.; Wei L.; Yang X.; Zhang J.; Li Y.; Wang P.; Huang J.; Yang G.","Jiang, Mingfeng (16175216000); Zhi, Minghao (57219910908); Wei, Liying (57224524874); Yang, Xiaocheng (55971485400); Zhang, Jucheng (55581543800); Li, Yongming (56457768700); Wang, Pin (56456764000); Huang, Jiahao (57223238396); Yang, Guang (57216243504)","16175216000; 57219910908; 57224524874; 55971485400; 55581543800; 56457768700; 56456764000; 57223238396; 57216243504","FA-GAN: Fused attentive generative adversarial networks for MRI image super-resolution","2021","Computerized Medical Imaging and Graphics","92","","101969","","","","10.1016/j.compmedimag.2021.101969","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112746586&doi=10.1016%2fj.compmedimag.2021.101969&partnerID=40&md5=674f2c377dd390e8e183aa9d379eafcf","High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super- resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods. © 2021 The Author(s)","Algorithms; Attention; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Image enhancement; Image fusion; Magnetism; Optical resolving power; Resonance; Adversarial networks; Anatomical information; Convolution kernel; Important features; Local fusion features; Reconstruction method; Spectral normalization; Super resolution; article; attention; human; nuclear magnetic resonance imaging; algorithm; attention; image processing; Magnetic resonance imaging","Attention; Generative adversarial networks; Mechanism; MRI; Super-resolution","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85112746586"
"Li X.; Zou C.; Yang G.; Liu H.","Li, Xinli (26662670600); Zou, Changming (57224903219); Yang, Guotian (22735958900); Liu, He (55719344400)","26662670600; 57224903219; 22735958900; 55719344400","Research of Super-resolution Processing of Invoice Image Based on Generative Adversarial Network; [基于生成式对抗网络的发票图像超分辨率研究]","2021","Xitong Fangzhen Xuebao / Journal of System Simulation","33","6","","1307","1314","7","10.16182/j.issn1004731x.joss.20-0095","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108569156&doi=10.16182%2fj.issn1004731x.joss.20-0095&partnerID=40&md5=0ea554fc9ab8c017aa6bd7f1d232ea87","Automatic identification of invoices can effectively improve financial efficiency. But low-resolution invoice image reduces the accuracy of automatic identification, an ESRGAN (Encoder Super-resolution Generative Adversarial Network) network for super-resolution processing of invoice images is proposed. The ESRGAN network is based on a conditional generative adversarial network. An auxiliary encoder is designed to guide the network to generate a more realistic super-resolution image. Based on the actual invoice image, the ESRGAN network and the conventional image processing, SRCNN (Super-resolution Convolutional Neural Networks) network and SRGAN (Super-resolution Generative Adversarial Network) network. The model is evaluated through two evaluation indicators of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). The experimental results show that the images processed based on ESRGAN super-resolution are better on visual effects and evaluation indicators. © 2021, The Editorial Board of Journal of System Simulation. All right reserved.","","ESRGAN; Evaluation indicator; Generative adversarial networks; Invoice image; Super-resolution","Article","Final","","Scopus","2-s2.0-85108569156"
"Wu S.; Dong C.; Qiao Y.","Wu, Shixiang (57744442400); Dong, Chao (57683629300); Qiao, Yu (57685445200)","57744442400; 57683629300; 57685445200","Blind Image Restoration Based on Cycle-Consistent Network","2022","IEEE Transactions on Multimedia","","","","","","","10.1109/TMM.2021.3139209","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122597481&doi=10.1109%2fTMM.2021.3139209&partnerID=40&md5=5077f4255e08081156d31dc27455901e","This paper studies the blind image restoration where the ground truth is unavailable and the downsampling process is unknown. This complicated setting makes supervised learning and accurate kernel estimation impossible. Inspired by the recent success of image-to-image translation, this paper resorts to the unsupervised Cycle-consistent based framework to tackle this challenging problem. Different from the image-to-image task, the fidelity of reconstructed image is important for image restoration. Therefore, to improve the reconstruction ability of the Cycle-consistent network, we make explorations from the following aspects. First, we constrain low-frequency content in data to preserve the content of output from LR input. Second, we impose constraint on the content of training data to provide better supervision for discriminator, helping to suppress high-frequency artifacts or fake textures. Third, we average model parameters to further improve the generated image quality and help with model selection for GAN-based methods. Since GAN-based methods tend to produce various artifacts with different models, model average could realize a smoother control of balancing artifacts and fidelity. We have conducted extensive experiments on real noise and super resolution datasets to validate the effectiveness of the above techniques. The proposed ECycleGAN also demonstrates superior performance to SOTA methods in two applications -- blind SR and blind denoising. IEEE","Computer vision; Image enhancement; Image reconstruction; Optical resolving power; Restoration; Blind image restoration; Consistent network; Down sampling; Ground truth; Image translation; Kernel estimation; Lower frequencies; Reconstructed image; Superresolution; Unsupervised; Generative adversarial networks","Blind image restoration; Generative Adversarial Network; Super-resolution; Unsupervised","Article","Article in press","","Scopus","2-s2.0-85122597481"
"Cheng J.; Liu J.; Kuang Q.; Xu Z.; Shen C.; Liu W.; Zhou K.","Cheng, Jianxin (57215698381); Liu, Jin (55978402400); Kuang, Qiuming (57191916860); Xu, Zhou (57212062746); Shen, Chenkai (57215694763); Liu, Wang (57215687140); Zhou, Kang (57222580088)","57215698381; 55978402400; 57191916860; 57212062746; 57215694763; 57215687140; 57222580088","DeepDT: Generative Adversarial Network for High-Resolution Climate Prediction","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3041760","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103268391&doi=10.1109%2fLGRS.2020.3041760&partnerID=40&md5=6d7f5308e267664012dd76cd1f72b99e","Climate prediction is susceptible to a variety of meteorological factors, and downscaling technology is used for high-resolution climate prediction. This technology can generate small-scale regional climate prediction from large-scale climate output information. Inspired by the concept of image super resolution, we propose to apply the convolutional neural network (CNN) to downscaling technology. However, some unpleasant artifacts always appear in the final climate images generated by existing CNN-based models. To further eliminate these unpleasant artifacts, we present a new training strategy for the generative adversarial network, termed DeepDT. The key idea of our DeepDT is to train a generator and a discriminator separately. More specifically, we apply the residual-in-residual dense block as the basic frame structure to fully extract the features of the input. Additionally, we innovatively use a CNN model to fuse multiple climate elements to generate trainable climate images, and build a high-quality climate data set. Finally, we evaluate the DeepDT using the proposed climate data sets, and the experiments indicate that DeepDT performs best compared to most CNN-based models in climate prediction.  © 2004-2012 IEEE.","Climatology; Convolutional neural networks; Forecasting; Adversarial networks; Climate elements; Climate prediction; High resolution; Image super resolutions; Meteorological factors; Regional climate; Training strategy; climate change; image resolution; satellite imagery; Climate models","Climate prediction; generative adversarial network; image super resolution","Article","Final","","Scopus","2-s2.0-85103268391"
"Zhou X.; Yang D.; Feng J.","Zhou, Xun (57770957500); Yang, Degang (56138575800); Feng, Ji (56405833600)","57770957500; 56138575800; 56405833600","Single Image Super-Resolution Reconstruction Algorithm with Generative Adversarial Network Based on Multi-Size Convolution and Laplacian Filtering; [基于多感受野拉普拉斯生成对抗网络的单幅图像超分辨率重建算法]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","10","","1504","1513","9","10.3724/SP.J.1089.2021.18763","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118278535&doi=10.3724%2fSP.J.1089.2021.18763&partnerID=40&md5=c185f731242958a0489977e8079e9815","At present, the image super-resolution reconstruction algorithm using deep learning still faces problems such as insufficient texture perception and insufficient realism of the reconstructed image. In order to improve the quality of the reconstructed image, a single image super-resolution reconstruction algorithm with generative ad-versarial network based on multi-size convolution and Laplacian filtering is proposed. First, multi-size convolu-tion feature extraction, separable Laplacian filtering and composite residual dense block are used to build a generation network which can extract more comprehensive image information. Secondly, multi-dimensional soft labels are utilized to construct the adversarial network, which can train the generative adversarial network easily and reconstruct image texture richly. Finally, the L1 loss function and the VGG low-level features are taken to obtain the overall features in the pre-training stage and the VGG high-level features, Charbonnier loss, and generative loss are used to make the reconstruction result more meticulously during training. Div2k and Flickr2K are chosen for model training, and Set5 and other data sets are used for testing. The experiment results show that the network size of this algorithm is 40% less than USRNet and other related algorithms, and the per-ceptual index is 0.76% lower than USRNet on average. The reconstruction result has more details and is more authentic. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Convolution; Deep learning; Image enhancement; Image reconstruction; Image texture; Information filtering; Laplace transforms; Optical resolving power; Textures; Depth convolution; Image super resolutions; Image super-resolution reconstruction; Laplacian filtering; Laplacians; Multi sizes; Network-based; Reconstructed image; Reconstruction algorithms; Single-image super-resolution reconstruction; Generative adversarial networks","Depth convolution; Generative adversarial network; Image super-resolution; Laplacian filtering","Article","Final","","Scopus","2-s2.0-85118278535"
"Sharma N.; Sharma R.; Jindal N.","Sharma, Neha (57191615950); Sharma, Reecha (57001029600); Jindal, Neeru (8571819900)","57191615950; 57001029600; 8571819900","Prediction of face age progression with generative adversarial networks","2021","Multimedia Tools and Applications","80","25","","33911","33935","24","10.1007/s11042-021-11252-w","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113808440&doi=10.1007%2fs11042-021-11252-w&partnerID=40&md5=36928996fa5d8326f08e4451927e0f45","Face age progression, goals to alter the individual’s face from a given face image to predict the future appearance of that image. In today’s world that demands more security and a touchless unique identification system, face aging attains tremendous attention. The existing face age progression approaches have the key problem of unnatural modifications of facial attributes due to insufficient prior knowledge of input images and nearly visual artifacts in the generated output. Research has been continuing in face aging to handle the challenge to generate aged faces accurately. So, to solve the issue, the proposed work focuses on the realistic face aging method using AttentionGAN and SRGAN. AttentionGAN uses two separate subnets in a generator. One subnet for generating multiple attention masks and the other for generating multiple content masks. Then attention mask is multiplied with the corresponding content mask along with an input image to finally achieve the desired results. Further, the regex filtering process is performed to separates the synthesized face images from the output of AttentionGAN. Then image sharpening with edge enhancement is done to give high-quality input to SRGAN, which further generates the super-resolution face aged images. Thus, presents more detailed information in an image because of its high quality. Moreover, the experimental results are obtained from five publicly available datasets: UTKFace, CACD, FGNET, IMDB-WIKI, and CelebA. The proposed work is evaluated with quantitative and qualitative methods, produces synthesized face aged images with a 0.001% error rate, and is also evaluated with the comparison to prior methods. The paper focuses on the various practical applications of super-resolution face aging using Generative Adversarial Networks (GANs). © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Optical resolving power; Adversarial networks; Edge enhancements; Filtering process; Future appearance; Image sharpening; Quantitative and qualitative methods; Super resolution; Unique identifications; Image enhancement","Age estimation; Face age progression; Face super-resolution; Generative adversarial networks (GANs)","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85113808440"
"Li X.; Liu J.; Dai S.","Li, Xiaoqiang (55718219400); Liu, Jitao (57212485053); Dai, Songmin (57845084400)","55718219400; 57212485053; 57845084400","Point cloud super-resolution based on geometric constraints","2021","IET Computer Vision","15","4","","312","321","9","10.1049/cvi2.12045","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135912461&doi=10.1049%2fcvi2.12045&partnerID=40&md5=b11b8526a2b947602c3f3fe901bcdb73","Among all digital representations we have for real physical objects, three-dimensional (3D) is arguably the most expressive encoding. But due to the limitations of 3D scanning equipment, point cloud often becomes sparse or partially missing. A point cloud super-resolution (PCSR) method based on geometric constraints is proposed to solve the sparse problem of point clouds: it allows dense point clouds to be generated by sparse point clouds. The method is based on the conditional generative adversarial network including redesigned generator and discriminator for point cloud data specially. Moreover, the method can maintain the shape of the dense point cloud by adding geometric constraints. The contributions of our work are as follows: (1) a PCSR method based on geometric constraints is proposed; (2) add a module for obtaining point cloud neighbourhood information in the generator, called K-nn operation module; and (3) feature aggregation is performed using the weighted pooling to process the neighbourhood information obtained by the K-nn operation module. Extensive experimental results demonstrate the effectiveness of the proposed method. © 2021 The Authors. IET Computer Vision published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.","Generative adversarial networks; Geometry; Image representation; Image resolution; Nearest neighbor search; All digital; Geometric constraint; Image representations; Images reconstruction; Neighborhood information; Operation module; Point-clouds; Solid modelling; Superresolution; Superresolution methods; Image reconstruction","image reconstruction; image representation; image resolution; solid modelling","Article","Final","","Scopus","2-s2.0-85135912461"
"Sun Y.; Yang Z.; Tao B.; Jiang G.; Hao Z.; Chen B.","Sun, Ying (57191647189); Yang, Zhiwen (57224217535); Tao, Bo (54785420900); Jiang, Guozhang (55600320900); Hao, Zhiqiang (56849852200); Chen, Baojia (23110036300)","57191647189; 57224217535; 54785420900; 55600320900; 56849852200; 23110036300","Multiscale generative adversarial network for real-world super-resolution","2021","Concurrency and Computation: Practice and Experience","33","21","e6430","","","","10.1002/cpe.6430","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107314175&doi=10.1002%2fcpe.6430&partnerID=40&md5=c6850e91915fcdda5c42e4fe2bcac8d5","Recently, most deep convolutional neural networks used for image super-resolution have achieved impressive performance on ideal datasets. However, these methods always fail in real-world super-resolution, and the results are blurred and structurally deformed. In this paper, a multiscale generative adversarial network (MGAN) is proposed to alleviate these issues. The model's multiscale loss function can effectively reduce the solution space and obtain the best features to reconstruct the image. The degraded framework based on kernel estimation and noise injection is mainly applied to obtain LR images that share the same domain with real-world pictures. Moreover, the gradient branch is presented to provide other structural priors for SR processing. Simultaneously, to obtain better visual effects, LPIPS is used for perceptual losses instead of Visual Geometry Group (VGG). The competitive results show that our MGAN model outperforms the state-of-the-art methods, resulting in lower noise and better visual quality, and reflects the superiority in image structure restoration. © 2021 John Wiley & Sons, Ltd.","Convolutional neural networks; Deep neural networks; Optical resolving power; Adversarial networks; Image Structures; Image super resolutions; Kernel estimation; Noise injection; State-of-the-art methods; Super resolution; Visual qualities; Image reconstruction","gradient branch; loss function; real-world super-resolution; visual quality","Article","Final","","Scopus","2-s2.0-85107314175"
"Stumpo V.; Kernbach J.M.; van Niftrik C.H.B.; Sebök M.; Fierstra J.; Regli L.; Serra C.; Staartjes V.E.","Stumpo, Vittorio (57204320430); Kernbach, Julius M. (57194019689); van Niftrik, Christiaan H. B. (57087287200); Sebök, Martina (57201197649); Fierstra, Jorn (35777972500); Regli, Luca (7004240836); Serra, Carlo (50263014800); Staartjes, Victor E. (57190836125)","57204320430; 57194019689; 57087287200; 57201197649; 35777972500; 7004240836; 50263014800; 57190836125","Machine Learning Algorithms in Neuroimaging: An Overview","2022","Acta Neurochirurgica, Supplementum","134","","","125","138","13","10.1007/978-3-030-85292-4_17","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120867819&doi=10.1007%2f978-3-030-85292-4_17&partnerID=40&md5=079f2756ced47ad0681d6fc3847d9630","Machine learning (ML) and artificial intelligence (AI) applications in the field of neuroimaging have been on the rise in recent years, and their clinical adoption is increasing worldwide. Deep learning (DL) is a field of ML that can be defined as a set of algorithms enabling a computer to be fed with raw data and progressively discover—through multiple layers of representation—more complex and abstract patterns in large data sets. The combination of ML and radiomics, namely the extraction of features from medical images, has proven valuable, too: Radiomic information can be used for enhanced image characterization and prognosis or outcome prediction. This chapter summarizes the basic concepts underlying ML application for neuroimaging and discusses technical aspects of the most promising algorithms, with a specific focus on Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs), in order to provide the readership with the fundamental theoretical tools to better understand ML in neuroimaging. Applications are highlighted from a practical standpoint in the last section of the chapter, including: image reconstruction and restoration, image synthesis and super-resolution, registration, segmentation, classification, and outcome prediction. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Algorithms; Artificial Intelligence; Image Processing, Computer-Assisted; Machine Learning; Neural Networks, Computer; architecture; back propagation; classification algorithm; clinical assessment; clinical classification; convolutional neural network; data availability; deep learning; developing country; echography; feature extraction; feature selection; generative adversarial network; human; hyperparameter optimization; image analysis; image reconstruction; image registration; image segmentation; information processing; kernel method; machine learning; medical education; medical student; network training; neuroimaging; neuroscience; nuclear magnetic resonance imaging; outcome assessment; physician; positron emission tomography; privacy; process optimization; radiomics; residual neural network; segmentation algorithm; standardization; synthesis; transfer of learning; workflow; x-ray computed tomography; algorithm; artificial intelligence; image processing","Classification; Convolutional neural network; Deep learning; Generative adversarial network; Machine learning; Segmentation","Book chapter","Final","","Scopus","2-s2.0-85120867819"
"Mao F.; Guan X.; Wang R.; Yue W.","Mao, Fuqi (57207246458); Guan, Xiaohan (18133434000); Wang, Ruoyu (57223268580); Yue, Wen (57201302772)","57207246458; 18133434000; 57223268580; 57201302772","Super-resolution based on generative adversarial network for HRTEM images","2021","International Journal of Pattern Recognition and Artificial Intelligence","35","10","2154027","","","","10.1142/S0218001421540276","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106907088&doi=10.1142%2fS0218001421540276&partnerID=40&md5=03d7cb783fb23080ac69fd88a26c7d4f","As an important tool to study the microstructure and properties of materials, High Resolution Transmission Electron Microscope (HRTEM) images can obtain the lattice fringe image (reflecting the crystal plane spacing information), structure image and individual atom image (which reflects the configuration of atoms or atomic groups in crystal structure). Despite the rapid development of HTTEM devices, HRTEM images still have limited achievable resolution for human visual system. With the rapid development of deep learning technology in recent years, researchers are actively exploring the Super-resolution (SR) model based on deep learning, and the model has reached the current best level in various SR benchmarks. Using SR to reconstruct high-resolution HRTEM image is helpful to the material science research. However, there is one core issue that has not been resolved: most of these super-resolution methods require the training data to exist in pairs. In actual scenarios, especially for HRTEM images, there are no corresponding HR images. To reconstruct high quality HRTEM image, a novel Super-Resolution architecture for HRTEM images is proposed in this paper. Borrowing the idea from Dual Regression Networks (DRN), we introduce an additional dual regression structure to ESRGAN, by training the model with unpaired HRTEM images and paired nature images. Results of extensive benchmark experiments demonstrate that the proposed method achieves better performance than the most resent SISR methods with both quantitative and visual results.  © 2021 World Scientific Publishing Company.","Atoms; Benchmarking; Crystal atomic structure; Deep learning; Optical resolving power; Adversarial networks; Benchmark experiments; High-resolution transmission electron microscopes; Human Visual System; Learning technology; Material science; Microstructure and properties; Superresolution methods; Image reconstruction","DCNN; GAN; HRTEM; SISR","Article","Final","","Scopus","2-s2.0-85106907088"
"Lee I.H.; Chung W.Y.; Park C.G.","Lee, In Ho (57477440400); Chung, Won Yeung (57337915700); Park, Chan Gook (35230911800)","57477440400; 57337915700; 35230911800","Super-resolution Thermal Generative Adversarial Networks for Infrared Image Enhancement","2022","Journal of Institute of Control, Robotics and Systems","28","2","","153","160","7","10.5302/J.ICROS.2022.21.0228","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125776707&doi=10.5302%2fJ.ICROS.2022.21.0228&partnerID=40&md5=43facf019c42c9804dc3b8b586c3272f","Thermal infrared cameras create IR (infrared) images, thus enabling the recognition of objects regardless of weather, illuminance, or ambient color. With the recent development of deep learning, research interest in image conversion and super-resolution techniques has increased. This paper proposes an algorithm that converts electro-optical (EO) images to IR images using super-resolution techniques based on generative adversarial networks. ThermalWorld data were used as the learning data. Additionally, drones of EO and IR images were added using thermal infrared cameras. The proposed super-resolution technique adapts the loss function and neural network structure to generate a high-resolution IR image. The loss function learns the neural network by utilizing the difference between the actual image and the generated image, thereby generating an image while maintaining the shape of the object on the image. The resolution is further improved by densely connecting the generator neural network structure and removing batch normalization. Finally, the structure of the discriminator is changed, and the stability of learning improved using the spectral norm. Images are generated according to each change item and quantitatively verified through performance indicators of improved image quality. This study analyzes images made with super-resolution techniques by considering the results of performance indicators and discusses the possibility of using IR images; and presents future research directions. © ICROS 2022.","Benchmarking; Deep learning; Generative adversarial networks; Image enhancement; Infrared devices; Infrared imaging; Learning algorithms; Temperature indicating cameras; Deep learning method; Infrared image enhancement; Infrared thermal image; Learning methods; Loss functions; Neural networks structure; Performance indicators; Resolution techniques; Superresolution; Thermal; Optical resolving power","Deep learning method; Generative adversarial network; Imaging infrared; Infrared thermal image; Learning algorithm; Super resolution","Article","Final","","Scopus","2-s2.0-85125776707"
"Wu H.; Zhang L.; Ma J.","Wu, Hanlin (57221263814); Zhang, Libao (35325855000); Ma, Jie (57205916758)","57221263814; 35325855000; 57205916758","Remote Sensing Image Super-Resolution via Saliency-Guided Feedback GANs","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2020.3042515","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098788589&doi=10.1109%2fTGRS.2020.3042515&partnerID=40&md5=315a94441bc03b62dc3b253c3b5166c6","In remote sensing images (RSIs), the visual characteristics of different regions are versatile, which poses a considerable challenge to single image super-resolution (SISR). Most existing SISR methods for RSIs ignore the diverse reconstruction needs of different regions and thus face a serious contradiction between high perception quality and less spatial distortion. The mean square error (MSE) optimization-based methods produce results of unsatisfactory visual quality, while generative adversarial networks (GANs) can produce photo-realistic but severely distorted results caused by pseudotextures. In addition, increasingly deeper networks, although providing powerful feature representations, also face problems of overfitting and occupying too much storage space. In this article, we propose a new saliency-guided feedback GAN (SG-FBGAN) to address these problems. The proposed SG-FBGAN applies different reconstruction principles for areas with varying levels of saliency and uses feedback (FB) connections to improve the expressivity of the network while reducing parameters. First, we propose a saliency-guided FB generator with our carefully designed paired-feedback block (PFBB). The PFBB uses two branches, a salient and a nonsalient branch, to handle the FB information and generate powerful high-level representations for salient and nonsalient areas, respectively. Then, we measure the visual perception quality of salient areas, nonsalient areas, and the global image with a saliency-guided multidiscriminator, which can dramatically eliminate pseudotextures. Finally, we introduce a curriculum learning strategy to enable the proposed SG-FBGAN to handle complex degradation models. Comprehensive evaluations and ablation studies validate the effectiveness of our proposal.  © 1980-2012 IEEE.","Learning systems; Mean square error; Optical resolving power; Space optics; Adversarial networks; Comprehensive evaluation; Degradation model; Feature representation; Learning strategy; Optimization based methods; Remote sensing images; Spatial distortion; algorithm; image resolution; remote sensing; satellite imagery; Remote sensing","Deep learning (DL); generative adversarial network (GAN); remote sensing; saliency detection; super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85098788589"
"Tao Y.; Muller J.-P.","Tao, Yu (56539197700); Muller, Jan-Peter (7404871794)","56539197700; 7404871794","Super-resolution restoration of spaceborne ultra-high-resolution images using the ucl optigan system","2021","Remote Sensing","13","12","2269","","","","10.3390/rs13122269","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108893859&doi=10.3390%2frs13122269&partnerID=40&md5=5a4502223f127ddc550ac2a938414e15","We introduce a robust and light-weight multi-image super-resolution restoration (SRR) method and processing system, called OpTiGAN, using a combination of a multi-image maximum a posteriori approach and a deep learning approach. We show the advantages of using a combined twostage SRR processing scheme for significantly reducing inference artefacts and improving effective resolution in comparison to other SRR techniques. We demonstrate the optimality of OpTiGAN for SRR of ultra-high-resolution satellite images and video frames from 31 cm/pixel WorldView-3, 75 cm/pixel Deimos-2 and 70 cm/pixel SkySat. Detailed qualitative and quantitative assessments are provided for the SRR results on a CEOS-WGCV-IVOS geo-calibration and validation site at Baotou, China, which features artificial permanent optical targets. Our measurements have shown a 3.69 times enhancement of effective resolution from 31 cm/pixel WorldView-3 imagery to 9 cm/pixel SRR. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Deep learning; Image enhancement; Optical resolving power; Restoration; Calibration and validations; Effective resolutions; Learning approach; Maximum a posteriori; Processing systems; Qualitative and quantitative assessments; Super-resolution restoration; Ultrahigh resolution; Image reconstruction","Deimos-2; Earth observation; EarthDaily Analytics®; Generative adversarial network; HD video; Maxar® WorldView-3; OpTiGAN; Planet® SkySat; Remote sensing; Satellite; Super-resolution restoration; Ultra-high resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85108893859"
"Hou M.; Liu S.; Zhou J.; Zhang Y.; Feng Z.","Hou, Mingzheng (55258843800); Liu, Song (57217014489); Zhou, Jiliu (21234416400); Zhang, Yi (57203829244); Feng, Ziliang (7403442867)","55258843800; 57217014489; 21234416400; 57203829244; 7403442867","Extreme low-resolution activity recognition using a super-resolution-oriented generative adversarial network","2021","Micromachines","12","6","670","","","","10.3390/mi12060670","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108376989&doi=10.3390%2fmi12060670&partnerID=40&md5=8c6a0410ecc067d45c908437dcdfb21e","Activity recognition is a fundamental and crucial task in computer vision. Impressive results have been achieved for activity recognition in high-resolution videos, but for extreme low-resolution videos, which capture the action information at a distance and are vital for preserving privacy, the performance of activity recognition algorithms is far from satisfactory. The reason is that extreme low-resolution (e.g., 12 × 16 pixels) images lack adequate scene and appearance information, which is needed for efficient recognition. To address this problem, we propose a super-resolution-driven generative adversarial network for activity recognition. To fully take advantage of the latent information in low-resolution images, a powerful network module is employed to super-resolve the extremely low-resolution images with a large scale factor. Then, a general activity recognition network is applied to analyze the super-resolved video clips. Extensive experiments on two public benchmarks were conducted to evaluate the effectiveness of our proposed method. The results demonstrate that our method outperforms several state-of-the-art low-resolution activity recognition approaches. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Optical resolving power; Privacy by design; Activity recognition; Adversarial networks; High resolution; Latent information; Low resolution images; Low resolution video; State of the art; Super resolution; Pattern recognition","Activity recognition; Extreme low-resolution activity recognition; Generative network; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85108376989"
"Mu J.; Li S.; Liu Z.; Zhou Y.","Mu, Jinzhen (57219133143); Li, Shuang (56288781000); Liu, Zongming (57869062600); Zhou, Yan (57188815143)","57219133143; 56288781000; 57869062600; 57188815143","Integration of gradient guidance and edge enhancement into super-resolution for small object detection in aerial images","2021","IET Image Processing","15","13","","3037","3052","15","10.1049/ipr2.12288","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108818533&doi=10.1049%2fipr2.12288&partnerID=40&md5=a24cfb4a97fbcc4a87bdf786f63f3dd2","Detecting small objects are difficult because of their poor-quality appearance and small size, and such issues are especially pronounced for aerial images of great importance. To address the small object detection (SOD) problem, a united architecture that tries to upsample small objects into super-resolved versions, achieving characteristics similar to those large objects and thus resulting in more discriminative detection is used. For this purpose, a new end-to-end multi-task generative adversarial network (GAN) is proposed. In the architecture, the generator is a super-resolution (SR) network, and the discriminator is a multi-task network. In the generator, a gradient guide and an edge-enhancement strategy are introduced to alleviate structural distortions. In the discriminator, a faster region-based convolutional neural network (FRCNN) is incorporated for the task of object detection. Specifically, the discriminator outputs a distribution scalar to measure the realness. Then, each super-resolved image passes through the discriminator with a realness distribution, classification scores, and bounding box regression offsets. Furthermore, the losses of the detection task are backpropagated into the generator during training rather than being optimized independently. Extensive experiments on the challenging cars overhead with context dataset (COWC), detectIon in optical remote sensing images (DIOR), vision meets drones (VisDrone), and dataset for object detection in aerial images (DOTA) demonstrate the effectiveness of the proposed method in reconstructing structures while generating natural super-resolved images and show the superiority of the proposed method in detecting small objects over state-of-the-art detectors. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Air navigation; Aircraft detection; Antennas; Backpropagation; Convolutional neural networks; Discriminators; Image enhancement; Image segmentation; Network architecture; Object recognition; Optical resolving power; Remote sensing; Adversarial networks; Detection tasks; Edge enhancements; Optical remote sensing; Small object detection; State of the art; Structural distortions; Super resolution; Object detection","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85108818533"
"Han Y.; Wang Y.; Ma Y.","Han, Yunfei (57204434261); Wang, Yi (57200058985); Ma, Yupeng (56297073400)","57204434261; 57200058985; 56297073400","Generative difference image for blind image quality assessment","2021","Proceedings - 2021 International Conference on Computer Engineering and Artificial Intelligence, ICCEAI 2021","","","","108","115","7","10.1109/ICCEAI52939.2021.00021","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117578508&doi=10.1109%2fICCEAI52939.2021.00021&partnerID=40&md5=8fae7adecdb3ddd4e97a4e22cff2058f","Image quality usually refers to the degree of error of the distorted image relative to the reference image in the human visual perception system. Image quality assessment is to score the image quality objectively. No-reference image quality assessment is limited to distorted image information, which is more challenging in the field of computer vision. In this paper, we proposed an approach based on difference image generation to address this problem. First, by removing the up-sampling layer and batch normalization layer in the Super-Resolution Generative Adversarial Network (SRGAN) to build a difference image generation model, and applying the content loss function to optimize the model. Then, the regression network is constructed based on the convolutional neural network (CNN). The regression network contains 4 convolutional layers and 2 fully connected layers and learns the correlation between the generated difference image and the image quality score to predict the distorted image quality. Finally, comparative experiments were evaluated on three public datasets. Compared with the previous state-of-the-art methods, our method obtains similar results on the LIVE dataset and achieves significant improvement on the TID2013 and CSIQ datasets. The results demonstrate that our proposed approach achieves state-of-the-art image quality prediction. © 2021 IEEE.","Computer vision; Convolution; Convolutional neural networks; Image quality; Multilayer neural networks; Difference image generation; Difference images; Distorted images; Human visual perception; Image generations; Image information; Image quality assessment; No-reference images; Perception systems; Reference image; Generative adversarial networks","Difference image generation; Generative adversarial networks; Image quality assessment","Conference paper","Final","","Scopus","2-s2.0-85117578508"
"Lu E.; Hu X.","Lu, Enmin (57224449030); Hu, Xiaoxiao (55496193600)","57224449030; 55496193600","Image super-resolution via channel attention and spatial attention","2022","Applied Intelligence","52","2","","2260","2268","8","10.1007/s10489-021-02464-6","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107557409&doi=10.1007%2fs10489-021-02464-6&partnerID=40&md5=6311029f8a530c04f9c3b87f4b99ebc8","Deep convolutional networks have been widely applied in super-resolution (SR) tasks and have achieved excellent performance. However, even though the self-attention mechanism is a hot topic, has not been applied in SR tasks. In this paper, we propose a new attention-based network for more flexible and efficient performance than other generative adversarial network(GAN)-based methods. Specifically, we employ a convolutional block attention module(CBAM) and embed it into a dense block to efficiently exchange information throughout feature maps. Furthermore, we construct our own spatial module with respect to the self-attention mechanism, which not only captures long-distance spatial connections, but also provides more stability for feature extraction. Experimental results demonstrate that our attention-based network improves the performance of visual quality and quantitative evaluations. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolution; Optical resolving power; Adversarial networks; Attention mechanisms; Convolutional networks; Image super resolutions; Quantitative evaluation; Spatial attention; Super resolution; Visual qualities; Convolutional neural networks","Attention mechanism; Spatial attention","Article","Final","","Scopus","2-s2.0-85107557409"
"Chudasama V.; Upla K.","Chudasama, Vishal (57202047982); Upla, Kishor (53985429600)","57202047982; 53985429600","Computationally efficient progressive approach for single-image super-resolution using generative adversarial network","2021","Journal of Electronic Imaging","30","2","021003","","","","10.1117/1.JEI.30.2.021003","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105537162&doi=10.1117%2f1.JEI.30.2.021003&partnerID=40&md5=179171ffb86c5a0cd43cd2f4fccc226d","Single-image super-resolution (SISR) refers to reconstructing a high-resolution image from given low-resolution observation. Recently, convolutional neural network (CNN)-based SISR methods have achieved remarkable results in terms of peak-signal-to-noise ratio and structural similarity measures. These models use pixel-wise loss functions to optimize their models, which results in blurry images. However, the generative adversarial network (GAN) has the ability to generate visually plausible solutions. The different GAN-based SISR methods obtain perceptually better SR results when compared to that with the existing CNN-based methods. However, the existing GAN-based SISR methods need a large number of training parameters in the architecture to obtain better SR performance, which makes them unsuitable for many real-world applications. We propose a computationally efficient enhanced progressive approach for SISR task using GAN, which we referred as E-ProSRGAN. In the proposed method, we introduce a novel design of residual block called enhanced parallel densely connected residual network, which helps to obtain better SR performance with less number of training parameters. The quantitative performance of the proposed E-ProSRNet (i.e., generator network of E-ProSRGAN) model is better for higher upscaling factors ×3 and ×4 for most of datasets when compared to the same obtained using different CNN-based methods whose trainable parameters are less than 7 M. In the case of upscaling factor ×2, E-ProSRNet obtains second highest structural similarity index measure values for Set5 and BSD100 datasets. The proposed E-ProSRGAN model generates SR samples with better high-frequency details and perception measures than that of the other existing GAN-based SISR methods with significant reduction in the number of training parameters for larger upscaling factor.  © 2021 SPIE and IS&T.","Computational efficiency; Optical resolving power; Signal to noise ratio; Adversarial networks; Computationally efficient; High frequency HF; High resolution image; Peak signal to noise ratio; Structural similarity; Structural similarity index measures; Training parameters; Convolutional neural networks","convolutional neural network; generative adversarial network; learned perceptual image patch similarity; perceptual image-error assessment through pairwise preference; perceptual index-root mean squared error; super-resolution","Article","Final","","Scopus","2-s2.0-85105537162"
"Yousif M.Z.; Yu L.; Lim H.-C.","Yousif, Mustafa Z. (57221872306); Yu, Linqi (57271595700); Lim, Hee-Chang (7403094883)","57221872306; 57271595700; 7403094883","High-fidelity reconstruction of turbulent flow from spatially limited data using enhanced super-resolution generative adversarial network","2021","Physics of Fluids","33","12","125119","","","","10.1063/5.0066077","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121294390&doi=10.1063%2f5.0066077&partnerID=40&md5=e27cd726f35fc6c96a65a4d317a0c4bc","In this study, a deep learning-based approach is applied with the aim of reconstructing high-resolution turbulent flow fields using minimal flow field data. A multi-scale enhanced super-resolution generative adversarial network with a physics-based loss function is introduced as a model to reconstruct the high-resolution flow fields. The model capability to reconstruct high-resolution laminar flows is examined using direct numerical simulation data of laminar flow around a square cylinder. The results reveal that the model can accurately reproduce the high-resolution flow fields even when limited spatial information is provided. The DNS data of turbulent channel flow at two friction Reynolds numbers R e τ = 180 and 550 are used to assess the ability of the model to reconstruct the high-resolution wall-bounded turbulent flow fields. The instantaneous and statistical results obtained from the model agree well with the ground truth data, indicating that the model can successfully learn to map the coarse flow fields to the high-resolution ones. Furthermore, the possibility of performing transfer learning for the case of turbulent channel flow is thoroughly examined. The results indicate that the amount of the training data and the time required for training can be effectively reduced without affecting the performance of the model. The computational cost of the proposed model is also found to be effectively low. These results demonstrate that using high-fidelity training data with physics-guided generative adversarial network-based models can be practically efficient in reconstructing high-resolution turbulent flow fields from extremely coarse data. © 2021 Author(s).","Channel flow; Deep learning; Flow fields; Laminar flow; Optical resolving power; Reynolds number; Turbulent flow; Field data; High resolution; High-fidelity; Learning-based approach; Limited data; Minimal flows; Superresolution; Training data; Turbulent channel flows; Turbulent flow field; Generative adversarial networks","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85121294390"
"Wen M.; Park J.; Cho K.","Wen, Mingyun (56982444400); Park, Jisun (57192682916); Cho, Kyungeun (14833735900)","56982444400; 57192682916; 14833735900","Textured mesh generation using multi-view and multi-source supervision and generative adversarial networks","2021","Remote Sensing","13","21","4254","","","","10.3390/rs13214254","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118145666&doi=10.3390%2frs13214254&partnerID=40&md5=c5c0894af9cf41a98e9ff979af54830b","This study focuses on reconstructing accurate meshes with high-resolution textures from single images. The reconstruction process involves two networks: a mesh-reconstruction network and a texture-reconstruction network. The mesh-reconstruction network estimates a deformation map, which is used to deform a template mesh to the shape of the target object in the input image, and a lowresolution texture. We propose reconstructing a mesh with a high-resolution texture by enhancing the low-resolution texture through use of the super-resolution method. The architecture of the texturereconstruction network is like that of a generative adversarial network comprising a generator and a discriminator. During the training of the texture-reconstruction network, the discriminator must focus on learning high-quality texture predictions and to ignore the difference between the generated mesh and the actual mesh. To achieve this objective, we used meshes reconstructed using the mesh-reconstruction network and textures generated through inverse rendering to generate pseudo-ground-truth images. We conducted experiments using the 3D-Future dataset, and the results prove that our proposed approach can be used to generate improved three-dimensional (3D) textured meshes compared to existing methods, both quantitatively and qualitatively. Additionally, through our proposed approach, the texture of the output image is significantly improved. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Convolutional neural networks; Image enhancement; Image reconstruction; Image texture; Inverse problems; Mesh generation; MESH networking; Optical resolving power; Textures; Convolutional neural network; High-resolution textures; Low-resolution textures; Mesh reconstruction; Multi-views; Reconstruction networks; Single image textured mesh reconstruction; Single images; Superresolution; Texture reconstruction; Generative adversarial networks","Convolutional neural networks; Generative adversarial network; Single image textured mesh reconstruction; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118145666"
"Deng L.; Zhang Y.; Wang X.","Deng, Liwei (51663261700); Zhang, Yuanzhi (57839766400); Wang, Xiaofei (57219132572)","51663261700; 57839766400; 57219132572","High-definition processing of remote sensing images based on CUT-CycleGAN","2021","Chinese Control Conference, CCC","2021-July","","","8158","8162","4","10.23919/CCC52363.2021.9549656","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117263619&doi=10.23919%2fCCC52363.2021.9549656&partnerID=40&md5=e289a72501a1a8d376e7030485de4cf6","High-definition remote sensing images are more and more widely used in research and life. However, due to hardware conditions and transmission rate limitations, it is too expensive to directly obtain high-definition original images. So, it has become a research hotspot on how to use algorithms to receive high-definition remote sensing images from low-resolution images. In view of the existing super-resolution methods for remote sensing images, the dependence on a large number of matching low-resolution and high-resolution(LR-HR) data sets and the slow network training time. In this paper, contrast learning is used for unpaired image-to-image conversion model (CUT-CycleGAN), which uses cyclic consistency to achieve the purpose of training using unpaired images, and adds a contrast learning framework to effectively shorten CycleGAN's training time and to improve efficiency. The experiment selects SRGAN, CycleGAN, EDSR, and FSRCNN four existing super-resolution methods to compare with the method in this paper. The results show that the training time of CUT-CycleGAN is reduced by nearly 55.7%, and after training with unpaired images, the quality of the generated high-definition images is good enough. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.","Image enhancement; Learning systems; Optical resolving power; Remote sensing; Comparative learning; Condition; High definition; Image-based; Remote sensing images; Remote-sensing; Superresolution; Superresolution methods; Training time; Transmission rates; Generative adversarial networks","Comparative learning; Generative adversarial network; Remote sensing; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85117263619"
"Ahmad M.; Abdullah M.; Han D.","Ahmad, Mobeen (57210116830); Abdullah, Muhammad (57626921000); Han, Dongil (13005656200)","57210116830; 57626921000; 13005656200","Video Quality Enhancement using Generative Adversarial Networks-based Super-Resolution and Noise Removal","2021","2021 36th International Technical Conference on Circuits/Systems, Computers and Communications, ITC-CSCC 2021","2021-June","","","","","","10.1109/ITC-CSCC52171.2021.9568313","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122520442&doi=10.1109%2fITC-CSCC52171.2021.9568313&partnerID=40&md5=f2e1e6bce9abba705864b74b82774e9b","Video quality enhancement is a challenging task as it not only involves super-resolution but also there is underlying noise in most of the real-world videos recorded almost a decade ago. Existing literature focuses on image super-resolution-based methods which fail to deliver satisfactory results in real-world scenario due to lack of high-resolution and low-resolution pairs. We propose a method based on image translation methodology coupled with super-resolution architecture. This does not require high-resolution, low-resolution pair, and learns the underlying noise automatically, Furthermore, it can learn the style of a HighDefinition video and apply it on a low-resolution video. We present qualitative results that show excellent performance on unseen dataset. © ROCLING 2008. All rights reserved.","Generative adversarial networks; Quality control; GAN; High resolution; Image super resolutions; Learn+; Lower resolution; Network-based; Noises removal; Real world videos; Superresolution; Video quality enhancements; Optical resolving power","GAN; Noise removal; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85122520442"
"You S.; Lei B.; Wang S.; Chui C.K.; Cheung A.C.; Liu Y.; Gan M.; Wu G.; Shen Y.","You, Senrong (57220890828); Lei, Baiying (26422280400); Wang, Shuqiang (53872228000); Chui, Charles K. (24297847200); Cheung, Albert C. (57215432085); Liu, Yong (55954392300); Gan, Min (57208454607); Wu, Guocheng (23390775700); Shen, Yanyan (25121829200)","57220890828; 26422280400; 53872228000; 24297847200; 57215432085; 55954392300; 57208454607; 23390775700; 25121829200","Fine Perceptive GANs for Brain MR Image Super-Resolution in Wavelet Domain","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2022.3153088","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126291613&doi=10.1109%2fTNNLS.2022.3153088&partnerID=40&md5=be92c9518844c8d0cec134e0efdb67a9","Magnetic resonance (MR) imaging plays an important role in clinical and brain exploration. However, limited by factors such as imaging hardware, scanning time, and cost, it is challenging to acquire high-resolution MR images clinically. In this article, fine perceptive generative adversarial networks (FP-GANs) are proposed to produce super-resolution (SR) MR images from the low-resolution counterparts. By adopting the divide-and-conquer scheme, FP-GANs are designed to deal with the low-frequency (LF) and high-frequency (HF) components of MR images separately and parallelly. Specifically, FP-GANs first decompose an MR image into LF global approximation and HF anatomical texture subbands in the wavelet domain. Then, each subband generative adversarial network (GAN) simultaneously concentrates on super-resolving the corresponding subband image. In generator, multiple residual-in-residual dense blocks are introduced for better feature extraction. In addition, the texture-enhancing module is designed to trade off the weight between global topology and detailed textures. Finally, the reconstruction of the whole image is considered by integrating inverse discrete wavelet transformation in FP-GANs. Comprehensive experiments on the MultiRes_7T and ADNI datasets demonstrate that the proposed model achieves finer structure recovery and outperforms the competing methods quantitatively and qualitatively. Moreover, FP-GANs further show the value by applying the SR results in classification tasks. IEEE","Discrete wavelet transforms; Economic and social effects; Generative adversarial networks; Image enhancement; Image fusion; Image reconstruction; Inverse problems; Magnetism; Optical resolving power; Resonance; Discrete wavelets transformations; Generative adversarial network; Images reconstruction; Lower frequencies; Subbands; Super-resolution; Superresolution; Task analysis; Texture enhance.; Wavelet domain; Magnetic resonance imaging","Discrete wavelet transformation; Discrete wavelet transforms; generative adversarial network (GAN); Generative adversarial networks; Hafnium; Image reconstruction; magnetic resonance (MR) imaging; Magnetic resonance imaging; super-resolution (SR); Task analysis; textures enhance.; Wavelet domain","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85126291613"
"","","","6th International Conference on Cognitive Systems and Signal Processing, ICCSIP 2021","2022","Communications in Computer and Information Science","1515 CCIS","","","","","544","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123603409&partnerID=40&md5=009754b8036f5c30ad83d974961706b0","The proceedings contain 41 papers. The special focus in this conference is on Cognitive Systems and Signal Processing. The topics include: Functional Primitive Library and Movement Sequence Reasoning Algorithm; constrained Control for Systems on Lie Groups with Uncertainties via Tube-Based Model Predictive Control on Euclidean Spaces; social-Transformer: Pedestrian Trajectory Prediction in Autonomous Driving Scenes; gridPointNet: Grid and Point-Based 3D Object Detection from Point Cloud; depth Image Super-resolution via Two-Branch Network; EBANet: Efficient Boundary-Aware Network for RGB-D Semantic Segmentation; camouflaged Object Segmentation with Transformer; DGrid: Dense Grid Network for Salient Object Detection; a Multi-frame Lane Detection Method Based on Deep Learning; unsupervised Semantic Segmentation with Contrastive Translation Coding; ensemble Deep Learning Based Single Finger-Vein Recognition; hand-Dorsa Vein Recognition Based on Local Deep Feature; Detection Method of Apple Based on Improved Lightweight YOLOv5; scene Graph Prediction with Concept Knowledge Base; a Discussion of Data Sampling Strategies for Early Action Prediction; sensor Fusion Based Weighted Geometric Distance Data Association Method for 3D Multi-object Tracking; multiple Granularities with Gradual Transition Network for Person Re-identification; generative Adversarial Networks and Improved Efficientnet for Imbalanced Diabetic Retinopathy Grading; sample-Efficient Reinforcement Learning Based on Dynamics Models via Meta-policy Optimization; from Human Oral Instructions to General Representations of Knowledge: A New Paradigm for Industrial Robots Skill Teaching; multi-class Feature Selection Based on Softmax with L2, 0 -Norm Regularization; 3D Grasping Pose Detection Method Based on Improved PointNet Network; MCTS-Based Robotic Exploration for Scene Graph Generation; predictive Maintenance Estimation of Aircraft Health with Survival Analysis.","","","Conference review","Final","","Scopus","2-s2.0-85123603409"
"Shang C.; Li X.; Foody G.M.; Du Y.; Ling F.","Shang, Cheng (57209801137); Li, Xiaodong (55878368700); Foody, Giles M. (7007014233); Du, Yun (56420121700); Ling, Feng (56278268300)","57209801137; 55878368700; 7007014233; 56420121700; 56278268300","Superresolution Land Cover Mapping Using a Generative Adversarial Network","2022","IEEE Geoscience and Remote Sensing Letters","19","","","","","","10.1109/LGRS.2020.3020395","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106494034&doi=10.1109%2fLGRS.2020.3020395&partnerID=40&md5=1b486b4ea819e31935afe7592de1c4d6","Superresolution mapping (SRM) is a commonly used method to cope with the problem of mixed pixels when predicting the spatial distribution within low-resolution pixels. Central to the popular SRM method is the spatial pattern model, which is utilized to represent the land cover spatial distribution within mixed pixels. The use of an inappropriate spatial pattern model limits such SRM analyses. Alternative approaches, such as deep-learning-based algorithms, which learn the spatial pattern from training data through a convolutional neural network, have been shown to have considerable potential. Deep learning methods, however, are limited by issues such as the way the fraction images are utilized. Here, a novel SRM model based on a generative adversarial network (GAN), GAN-SRM, is proposed that uses an end-to-end network to address the main limitations of existing SRM methods. The potential of the proposed GAN-SRM model was assessed using four land cover subsets and compared to hard classification and several popular SRM methods. The experimental results show that of the set of methods explored, the GAN-SRM model was able to generate the most accurate high-resolution land cover maps. © 2004-2012 IEEE.","Deep learning; Neural networks; Optical resolving power; Pixels; Spatial distribution; Deep learning; Generative adversarial network; Land cover; Mapping method; Mapping modeling; Mixed pixel; Spatial patterns; Super-resolution mapping; Superresolution; Superresolution mapping; artificial neural network; image classification; land cover; mapping method; pixel; spatial distribution; Generative adversarial networks","Deep learning; generative adversarial network (GAN); super-resolution mapping (SRM)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85106494034"
"Guo C.; Zhao Q.; Liu G.; Hao S.; Gao C.; Sun J.; Liu C.; Chen Y.","Guo, Chao (57190872674); Zhao, Qianping (56094478400); Liu, Gang (57221077017); Hao, Shiyan (26661875100); Gao, Chao (56205403600); Sun, Jianbo (55500800900); Liu, Chao (56969873200); Chen, Yiyi (57223712669)","57190872674; 56094478400; 57221077017; 26661875100; 56205403600; 55500800900; 56969873200; 57223712669","Super-resolution imaging of thin sections for lacustrine shale reservoirs; [陆相页岩储层薄片超分辨率增强方法]","2021","Oil and Gas Geology","42","5","","1202","1209","7","10.11743/ogg20210517","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117904471&doi=10.11743%2fogg20210517&partnerID=40&md5=9bb02df839279494e120c260e3a08b4b","Despite a great progress made in the exploration and development of shale gas and oil in recent years, the study on the pores, organic matter and mineral composition of shale reservoirs at a microscopic scale is still a challenge with thin section analysis for conventional reservoirs. In order to solve this problem, this study introduces a super-resolution technology to improve thin section image quality for revealing micro-characteristics of shale reservoirs. A set of super-resolution models are established based on generative adversarial networks and corresponding content loss functions are also set up for thin-section images. Application of the technology to the processing of actual data from lacustrine shale gas reservoirs in the Yanchang Formation in Ordos Basin has yielded positive results, demonstrating quantitatively and qualitatively its applicability, accuracy and reliability for unconventional reservoir assessment. © 2021, OIL & GAS GEOLOGY Editorial Board. All right reserved.","Data handling; Image enhancement; Metamorphic rocks; Petroleum prospecting; Data enhancement; Exploration and development; Microscopic physical property; Ordos Basin; Section image; Shale gas and oil; Shale reservoir; Super resolution imaging; Superresolution; Thin-sections; Optical resolving power","Data enhancement; Microscopic physical property; Ordos Basin; Shale reservoir; Super-resolution; Thin section","Article","Final","","Scopus","2-s2.0-85117904471"
"Kim T.-G.; Yun B.-J.; Kim T.-H.; Lee J.-Y.; Park K.-H.; Jeong Y.; Kim H.D.","Kim, Tae-Gu (57203390635); Yun, Byoung-Ju (7006416932); Kim, Tae-Hun (57226162925); Lee, Jae-Young (57226163919); Park, Kil-Houm (35776805000); Jeong, Yoosoo (57193450818); Kim, Hyun Deok (55663858300)","57203390635; 7006416932; 57226162925; 57226163919; 35776805000; 57193450818; 55663858300","Recognition of vehicle license plates based on image processing","2021","Applied Sciences (Switzerland)","11","14","6292","","","","10.3390/app11146292","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110674047&doi=10.3390%2fapp11146292&partnerID=40&md5=79073a3ebe123e534feeda5805d26d9d","In this study, we have proposed an algorithm that solves the problems which occur during the recognition of a vehicle license plate through closed-circuit television (CCTV) by using a deep learning model trained with a general database. The deep learning model which is commonly used suffers with a disadvantage of low recognition rate in the tilted and low-resolution images, as it is trained with images acquired from the front of the license plate. Furthermore, the vehicle images acquired by using CCTV have issues such as limitation of resolution and perspective distortion. Such factors make it difficult to apply the commonly used deep learning model. To improve the recognition rate, an algorithm which is a combination of the super-resolution generative adversarial network (SRGAN) model, and the perspective distortion correction algorithm is proposed in this paper. The accuracy of the proposed algorithm was verified with a character recognition algorithm YOLO v2, and the recognition rate of the vehicle license plate image was improved 8.8% from the original images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","CCTV image; Deep learning; Image processing; License plate detection; SRGAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110674047"
"Zhong X.; Wang Y.; Cai A.; Liang N.; Li L.; Yan B.","Zhong, Xinyi (57267183200); Wang, Yizhong (57221871192); Cai, Ailong (55750377700); Liang, Ningning (57203978839); Li, Lei (56108914100); Yan, Bin (35188444100)","57267183200; 57221871192; 55750377700; 57203978839; 56108914100; 35188444100","Dual-energy CT image super-resolution via generative adversarial network","2021","Proceedings - 2021 International Conference on Artificial Intelligence and Electromechanical Automation, AIEA 2021","","","","343","347","4","10.1109/AIEA53260.2021.00079","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115402605&doi=10.1109%2fAIEA53260.2021.00079&partnerID=40&md5=7bcb429798a793efa437dfd619adbffe","Photon counting detector obtains CT images from multiple energy bins, and acquires X-ray intensity data of different energy bins through one X-ray exposure. However, the spatial resolution of the reconstructed image will decrease, and the image will be blurred due to the low photon count in the narrow energy box width, quantum noise and the response problem of detector cells. Deep learning is gradually applied to medical images to reduce noise or improve resolution, which has exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images to high-resolution (HR) images. Inspired by the cycle-GAN, we propose a novel network model which realize the mapping of HR images to LR images for Dual-Energy CT (DECT) reconstruction. Experimental results show that the reconstructed image has significant improvements in peak signal-to-noise ratio (PSNR) and root mean square error (RMSE). Compared with the traditional super-resolution reconstruction method, this method has better experimental results. © 2021 IEEE.","Computerized tomography; Deep learning; Image denoising; Image enhancement; Image reconstruction; Mapping; Mean square error; Medical imaging; Optical resolving power; Photons; Quantum noise; Signal to noise ratio; CT Image; Deep learning; Dual-energy CT; Energy; High-resolution images; Image super resolutions; Low resolution images; Photon counting detectors; Reconstructed image; Superresolution; Generative adversarial networks","Deep learning; Dual-Energy CT; Generative adversarial network; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85115402605"
"Liu Q.-M.; Jia R.-S.; Liu Y.-B.; Sun H.-B.; Yu J.-Z.; Sun H.-M.","Liu, Qing-Ming (57212619890); Jia, Rui-Sheng (25927894300); Liu, Yan-Bo (57205368195); Sun, Hai-Bin (57207025120); Yu, Jian-Zhi (46061757000); Sun, Hong-Mei (55729286100)","57212619890; 25927894300; 57205368195; 57207025120; 46061757000; 55729286100","Infrared image super-resolution reconstruction by using generative adversarial network with an attention mechanism","2021","Applied Intelligence","51","4","","2018","2030","12","10.1007/s10489-020-01987-8","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093985631&doi=10.1007%2fs10489-020-01987-8&partnerID=40&md5=74cf794f0c81878280f13f33b270a096","Due to the limitations of infrared imaging principles and imaging systems, many problems are typically encountered with collected infrared images, such as low resolution, insufficient detail information, and blurred edges. In response to these problems, a method of infrared image super-resolution reconstruction that uses recursive attention and is based on a generative adversarial network is proposed. First, according to the characteristics of low-resolution infrared images such as uniform pixel distributions, low contrast, and poor perceived quality, a deep generator structure with a recursive-attention network is designed in this article. The recursive-attention module is used to extract high-frequency information from the feature maps, suppress useless information, and enhance the expressiveness of the features, which facilitates the reconstruction of texture details of infrared images. Then, to better distinguish the reconstructed images from the original high-resolution images, we designed a discriminator that was composed of a deep convolutional neural network. In addition, targeted improvements were made to the content loss function of GAN. We used the pre-trained VGG-19 network features before activation to calculate the perceptual loss, which helps recover the texture details of the infrared images. The experimental results on infrared image datasets demonstrated that the reconstruction performance of the proposed method is higher than those of several typical methods, and it realizes higher image visual quality. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep neural networks; Image enhancement; Image resolution; Image texture; Optical resolving power; Textures; Thermography (imaging); Adversarial networks; Attention mechanisms; High resolution image; High-frequency informations; Image super-resolution reconstruction; Imaging principle; Pixel distribution; Reconstructed image; Image reconstruction","Attention mechanism; Generative adversarial network; Infrared images; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85093985631"
"Li J.; Liu W.; Zhang K.; Liu B.","Li, Jiaoyue (57299552400); Liu, Weifeng (57835192100); Zhang, Kai (55769748056); Liu, Baodi (16319146900)","57299552400; 57835192100; 55769748056; 16319146900","Edge Loss for Remote Sensing Image Super-Resolution","2021","Frontiers in Artificial Intelligence and Applications","345","","","262","267","5","10.3233/FAIA210411","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123641314&doi=10.3233%2fFAIA210411&partnerID=40&md5=19ad01fe50cbbe874f489bddd25ce1e1","Remote sensing image super-resolution (SR) plays an essential role in many remote sensing applications. Recently, remote sensing image super-resolution methods based on deep learning have shown remarkable performance. However, directly utilizing the deep learning methods becomes helpless to recover the remote sensing images with a large number of complex objectives or scene. So we propose an edge-based dense connection generative adversarial network (SREDGAN), which minimizes the edge differences between the generated image and its corresponding ground truth. Experimental results on NWPU-VHR-10 and UCAS-AOD datasets demonstrate that our method improves 1.92 and 0.045 in PSNR and SSIM compared with SRGAN, respectively. © 2022 The authors and IOS Press.","Deep learning; Optical resolving power; Remote sensing; Edge difference; Edge loss; Edge-based; Ground truth; Image super resolutions; Learning methods; Performance; Remote sensing applications; Remote sensing images; Superresolution methods; Generative adversarial networks","Edge loss; Generative adversarial network; Image super-resolution; Remote sensing image","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85123641314"
"Xie H.; Zhang T.; Song W.; Wang S.; Zhu H.; Zhang R.; Zhang W.; Yu Y.; Zhao Y.","Xie, Hongqiang (57207933712); Zhang, Tongtong (57312648700); Song, Weiwei (57205408898); Wang, Shoujun (56039962000); Zhu, Hongchang (57212217689); Zhang, Rumin (56039039300); Zhang, Weiping (57212222559); Yu, Yong (57212216642); Zhao, Yan (57825224000)","57207933712; 57312648700; 57205408898; 56039962000; 57212217689; 56039039300; 57212222559; 57212216642; 57825224000","Super-resolution of Pneumocystis carinii pneumonia CT via self-attention GAN","2021","Computer Methods and Programs in Biomedicine","212","","106467","","","","10.1016/j.cmpb.2021.106467","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117904319&doi=10.1016%2fj.cmpb.2021.106467&partnerID=40&md5=c73f0fcb4d3154c6f710720863be7d2e","Background and objective: Computed tomography (CT) examination plays an important role in screening suspected and confirmed patients in pneumocystis carinii pneumonia (PCP), and the efficient acquisition of high-quality medical CT images is essential for the clinical application of computer-aided diagnosis technology. Therefore, improving the resolution of CT images of pneumonia is a very important task. Methods: Aiming at the problem of how to recover the texture details of the reconstructed PCP CT super-resolution image, we propose the image super-resolution reconstruction model based on self-attention generation adversarial network (SAGAN). In the SAGAN algorithm, a generator based on self-attention mechanism and residual module is used to transform a low-resolution image into a super-resolution image. A discriminator based on depth convolution network tries to distinguish the difference between the reconstructed super-resolution image and the real super-resolution image. In terms of loss function construction, on the one hand, the Charbonnier content loss function is used to improve the accuracy of image reconstruction, and on the other hand, the feature value before activation of the pre-trained VGGNet is used to calculate the perceptual loss to achieve accurate texture detail reconstruction of super-resolution images. Results: Experimental results show that our SAGAN algorithm is superior to other state-of-the-art algorithms in both peak signal-to-noise ratio (PSNR) and structural similarity score (SSIM). Specifically, our SAGAN method can obtain 31.94 dB which is 1.53 dB better than SRGAN on Set5 dataset for 4 enlargements. Conclusion: Our SAGAN method can reconstruct more realistic PCP CT images with clear texture, which can help experts diagnose the condition of PCP. © 2021","Algorithms; Humans; Image Processing, Computer-Assisted; Pneumonia, Pneumocystis; Signal-To-Noise Ratio; Tomography, X-Ray Computed; Computer aided diagnosis; Computerized tomography; Convolution; Convolutional neural networks; Image enhancement; Image reconstruction; Image texture; Medical computing; Medical imaging; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Attention mechanisms; Computed tomography images; Convolutional neural network; Loss functions; Network algorithms; Pneumocysti carinii pneumonia; Resolution images; Self-attention mechanism; Superresolution; Article; discriminant analysis; feature extraction; human; kernel method; Pneumocystis pneumonia; reconstruction algorithm; self attention generation adversarial network; signal noise ratio; x-ray computed tomography; algorithm; diagnostic imaging; image processing; x-ray computed tomography; Generative adversarial networks","Convolutional neural network; Generative adversarial network; Pneumocystis carinii pneumonia; Self-attention mechanism; Super-resolution","Article","Final","","Scopus","2-s2.0-85117904319"
"Xiao G.; Dong Z.; Yang X.","Xiao, Guangyi (57328144900); Dong, Zhangyu (37005375600); Yang, Xuezhi (7406505132)","57328144900; 37005375600; 7406505132","SAR image super-resolution reconstruction based on cross-resolution discrimination","2021","Journal of Electronic Imaging","30","5","053018","","","","10.1117/1.JEI.30.5.053018","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118798396&doi=10.1117%2f1.JEI.30.5.053018&partnerID=40&md5=b01a0850a73609b6fe00de8dfaf63446","High-resolution (HR) synthetic aperture radar (SAR) images play an important role in people's daily life and military applications. However, due to the interference of speckle noise, the texture details of the SAR images become relatively blurred. The fine texture details can be reconstructed by increasing the resolution of the SAR images. Generative adversarial networks achieve high performance in image super-resolution (SR) reconstruction, but the existing generative adversarial networks only pay attention to the discrimination of HR images without that of the low-resolution (LR) images. If the reconstructed HR image is sufficiently realistic, the LR image obtained from downsampled super-resolved images should also be the same as the original LR image. To take advantage of the LR image, an SAR image SR reconstruction algorithm based on cross-resolution discrimination (CRD) using teacher-student network is proposed. First, the teacher discriminator network (TD-Net) discriminates the HR images, which enriches the reconstructed HR images with more high-frequency texture details. Second, the student discriminator network (SD-Net) discriminates the LR images, which enables the reconstructed HR images to be accurately downsampled to the original LR image. Finally, the TD-Net guides the training of the SD-Net by transmitting distillation knowledge to the SD-Net, which further improves the discriminative performance of the SD-Net. Experiments on the SAR image dataset demonstrate that the performance of the proposed CRD algorithm is better than other algorithms when both the objective evaluations and subjective effects are considered. © 2021 SPIE and IS&T.","Distillation; Image reconstruction; Military applications; Optical resolving power; Radar imaging; Synthetic aperture radar; Textures; Cross-resolution discrimination; High resolution synthetic aperture radar images; High-resolution images; Image super-resolution reconstruction; Knowledge distillation; Low resolution images; Performance; Superresolution; Synthetic aperture radar images; Teachers'; Generative adversarial networks","cross-resolution discrimination; generative adversarial network; knowledge distillation; super-resolution; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85118798396"
"Wang J.; Liu Z.; Foster I.; Chang W.; Kettimuthu R.; Kotamarthi V.R.","Wang, Jiali (55803438700); Liu, Zhengchun (57189354719); Foster, Ian (35572232000); Chang, Won (55813857400); Kettimuthu, Rajkumar (6507364595); Kotamarthi, V. Rao (6603968694)","55803438700; 57189354719; 35572232000; 55813857400; 6507364595; 6603968694","Fast and accurate learned multiresolution dynamical downscaling for precipitation","2021","Geoscientific Model Development","14","10","","6355","6372","17","10.5194/gmd-14-6355-2021","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118230292&doi=10.5194%2fgmd-14-6355-2021&partnerID=40&md5=791598258d7aece138ce16b352f54f30","This study develops a neural-network-based approach for emulating high-resolution modeled precipitation data with comparable statistical properties but at greatly reduced computational cost. The key idea is to use combination of low- and high-resolution simulations (that differ not only in spatial resolution but also in geospatial patterns) to train a neural network to map from the former to the latter. Specifically, we define two types of CNNs, one that stacks variables directly and one that encodes each variable before stacking, and we train each CNN type both with a conventional loss function, such as mean square error (MSE), and with a conditional generative adversarial network (CGAN), for a total of four CNN variants. We compare the four new CNN-derived high-resolution precipitation results with precipitation generated from original high-resolution simulations, a bilinear interpolater and the state-of-the-art CNN-based super-resolution (SR) technique. Results show that the SR technique produces results similar to those of the bilinear interpolator with smoother spatial and temporal distributions and smaller data variabilities and extremes than the original high-resolution simulations. While the new CNNs trained by MSE generate better results over some regions than the interpolator and SR technique do, their predictions are still biased from the original high-resolution simulations. The CNNs trained by CGAN generate more realistic and physically reasonable results, better capturing not only data variability in time and space but also extremes such as intense and long-lasting storms. The new proposed CNN-based downscaling approach can downscale precipitation from 50 to 12gkm in 14gmin for 30 years once the network is trained (training takes 4gh using 1 GPU), while the conventional dynamical downscaling would take 1gmonth using 600 CPU cores to generate simulations at the resolution of 12gkm over the contiguous United States. © 2021 Jiali Wang et al.","United States; accuracy assessment; detection method; downscaling; precipitation (climatology); satellite data; simulation; spatial resolution","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118230292"
"Yan Y.; Liu C.; Chen C.; Sun X.; Jin L.; Peng X.; Zhou X.","Yan, Yitong (57208862449); Liu, Chuangchuang (57208860339); Chen, Changyou (25624426900); Sun, Xianfang (7405626287); Jin, Longcun (35110541800); Peng, Xinyi (7401594040); Zhou, Xiang (55743301100)","57208862449; 57208860339; 25624426900; 7405626287; 35110541800; 7401594040; 55743301100","Fine-Grained Attention and Feature-Sharing Generative Adversarial Networks for Single Image Super-Resolution","2022","IEEE Transactions on Multimedia","24","","","1473","1487","14","10.1109/TMM.2021.3065731","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102678901&doi=10.1109%2fTMM.2021.3065731&partnerID=40&md5=0368b14ecb0b37954e4fb03f235a200f","Traditional super-resolution (SR) methods by minimize the mean square error usually produce images with over-smoothed and blurry edges, due to the lack of high-frequency details. In this paper, we propose two novel techniques within the generative adversarial network framework to encourage generation of photo-realistic images for image super-resolution. Firstly, instead of producing a single score to discriminate real and fake images, we propose a variant, called Fine-grained Attention Generative Adversarial Network (FASRGAN), to discriminate each pixel of real and fake images. FASRGAN adopts a UNet-like network as the discriminator with two outputs: an image score and an image score map. The score map has the same spatial size as the HR/SR images, serving as the fine-grained attention to represent the degree of reconstruction difficulty for each pixel. Secondly, instead of using different networks for the generator and the discriminator, we introduce a feature-sharing variant (denoted as Fs-SRGAN) for both the generator and the discriminator. The sharing mechanism can maintain model express power while making the model more compact, and thus can improve the ability of producing high-quality images. Quantitative and visual comparisons with state-of-the-art methods on benchmark datasets demonstrate the superiority of our methods. We further apply our super-resolution images for object recognition, which further demonstrates the effectiveness of our proposed method. The code is available at https://github.com/Rainyfish/FASRGAN-and-Fs-SRGAN. © 1999-2012 IEEE.","Discriminators; Mean square error; Object recognition; Optical resolving power; Pixels; Adversarial networks; Benchmark datasets; High frequency HF; High quality images; Image super resolutions; Photorealistic images; State-of-the-art methods; Visual comparison; Image enhancement","Feature-sharing; fine-grained attention; generative adversarial network; image super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102678901"
"Vassilo K.; Taha T.; Mehmood A.","Vassilo, Kyle (57218712112); Taha, Tarek (23013518500); Mehmood, Asif (36133731600)","57218712112; 23013518500; 36133731600","Infrared Image Super Resolution with Deep Neural Networks","2021","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2021-March","","9484045","","","","10.1109/WHISPERS52202.2021.9484045","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112856133&doi=10.1109%2fWHISPERS52202.2021.9484045&partnerID=40&md5=f61b4cd0953a391f10c4c0ff47aecdd8","Recent studies have shown that Deep Learning (DL) algorithms can significantly improve Super Resolution (SR) performance. Single image SR is useful in producing High Resolution (HR) images from their Low Resolution (LR) counterparts. The motivation for SR is the potential to assist algorithms such as object detection, localization, and classification. Insufficient work has been conducted using Generative Adversarial Networks (GANs) for SR on infrared (IR) images despite its promising ability to increase object detection accuracy by extracting more precise features from a given image. This work adopts the idea of a relativistic GAN that utilizes Residual in Residual Dense blocks (RRDBs) for feature ex- traction, a novel residual image addition, and a Pixel Transposed Convolutional Layer (PixelTCL) for up-sampling. Recent work has validated the use of GANs for Visible Light (VL) images, making them a strong candidate. The inclusion of these components produce more realistic and natural features while also receiving superior metric values.  © 2021 IEEE.","Deep learning; Deep neural networks; Infrared imaging; Neural networks; Object detection; Object recognition; Optical resolving power; Remote sensing; Signal receivers; Spectroscopy; Adversarial networks; Detection accuracy; High resolution image; Image super resolutions; Low resolution; Natural features; Residual images; Super resolution; Hyperspectral imaging","Deep Learning; Generative Adversarial Network; Infrared Imaging; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85112856133"
"Ma Y.; Liu K.; Xiong H.; Fang P.; Li X.; Chen Y.; Yan Z.; Zhou Z.; Liu C.","Ma, Yuan (57219415397); Liu, Kewen (35812165500); Xiong, Hongxia (8580026900); Fang, Panpan (57219522112); Li, Xiaojun (57219817692); Chen, Yalei (57219815696); Yan, Zejun (55461584800); Zhou, Zhijun (57219527213); Liu, Chaoyang (36863772300)","57219415397; 35812165500; 8580026900; 57219522112; 57219817692; 57219815696; 55461584800; 57219527213; 36863772300","Medical image super-resolution using a relativistic average generative adversarial network","2021","Nuclear Instruments and Methods in Physics Research, Section A: Accelerators, Spectrometers, Detectors and Associated Equipment","992","","165053","","","","10.1016/j.nima.2021.165053","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100206765&doi=10.1016%2fj.nima.2021.165053&partnerID=40&md5=f57973eef229faeb9ead6c56675885ad","The medical imaging technique, e.g., positron emission tomography (PET), X-ray computed tomography (CT), and magnetic resonance imaging (MRI) is essential for clinical diagnosis and nuclear medicine. However, due to the hardware limitations of scanners, it is always clinically challenging to obtain high-resolution (HR) medical images. With the development of artificial intelligence, image super-resolution has been an effective technique to enhance the spatial resolution of medical images. In this paper, we propose a novel medical image super-resolution method using a relativistic average generative adversarial network (GAN), which consists of a generator and a discriminator for enhancing medical imaging quality in terms of both numerical criteria and visual results. The generator is trained to reconstruct HR images according to low-resolution (LR) counterparts. In contrast, the discriminator is trained to discriminate the probability of whether real HR images are more realistic than reconstructed images, further enhancing visual results. We apply our proposed method to two different public medical datasets, and experimental results show that our proposed method outperforms in terms of visual results, peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), model complexity and an additional non-reference image quality assessment metric, compared with other state-of-the-art medical image super-resolution methods. © 2021 Elsevier B.V.","Artificial intelligence; Diagnosis; Encoding (symbols); Image enhancement; Image quality; Image reconstruction; Magnetic resonance imaging; Medical imaging; Nuclear medicine; Numerical methods; Optical resolving power; Positron emission tomography; Signal to noise ratio; Adversarial networks; Clinical diagnosis; Image super resolutions; Peak signal to noise ratio; Positron emission tomography (PET); Reconstructed image; Structural similarity indices (SSIM); X-ray computed tomography; Computerized tomography","Attention mechanism; Medical image super-resolution; Relativistic average generative adversarial network; Residual neural network","Article","Final","","Scopus","2-s2.0-85100206765"
"Gong Y.; Liao P.; Zhang X.; Zhang L.; Chen G.; Zhu K.; Tan X.; Lv Z.","Gong, Yuanfu (57200512932); Liao, Puyun (57208162705); Zhang, Xiaodong (57192504939); Zhang, Lifei (57207389916); Chen, Guanzhou (56181390800); Zhu, Kun (57200511212); Tan, Xiaoliang (57202231708); Lv, Zhiyong (23111268400)","57200512932; 57208162705; 57192504939; 57207389916; 56181390800; 57200511212; 57202231708; 23111268400","Enlighten-gan for super resolution reconstruction in mid-resolution remote sensing images","2021","Remote Sensing","13","6","1104","","","","10.3390/rs13061104","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103080085&doi=10.3390%2frs13061104&partnerID=40&md5=5d1cb69627959e3776eec7312ce71b58","Previously, generative adversarial networks (GAN) have been widely applied on super resolution reconstruction (SRR) methods, which turn low-resolution (LR) images into high-resolution (HR) ones. However, as these methods recover high frequency information with what they observed from the other images, they tend to produce artifacts when processing unfamiliar images. Optical satellite remote sensing images are of a far more complicated scene than natural images. Therefore, applying the previous networks on remote sensing images, especially mid-resolution ones, leads to unstable convergence and thus unpleasing artifacts. In this paper, we propose Enlighten-GAN for SRR tasks on large-size optical mid-resolution remote sensing images. Specifically, we design the enlighten blocks to induce network converging to a reliable point, and bring the Self-Supervised Hierarchical Perceptual Loss to attain performance improvement overpassing the other loss functions. Furthermore, limited by memory, large-scale images need to be cropped into patches to get through the network separately. To merge the reconstructed patches into a whole, we employ the internal inconsistency loss and cropping-and-clipping strategy, to avoid the seam line. Experiment results certify that Enlighten-GAN outperforms the state-of-the-art methods in terms of gradient similarity metric (GSM) on mid-resolution Sentinel-2 remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Global system for mobile communications; Optical resolving power; Remote sensing; Adversarial networks; High-frequency informations; Low resolution images; Optical satellites; Remote sensing images; Similarity metrics; State-of-the-art methods; Super resolution reconstruction; Image reconstruction","Generative adversarial network; Mid-resolution remote sensing images; Super resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103080085"
"Nneji G.U.; Cai J.; Jianhua D.; Monday H.N.; Ejiyi C.J.; James E.C.; Mgbejime G.T.; Oluwasanmi A.","Nneji, Grace Ugochi (57206902001); Cai, Jingye (10738940600); Jianhua, Deng (6506091558); Monday, Happy Nkanta (57201671873); Ejiyi, Chukwuebuka Joseph (57216692640); James, Edidiong Christopher (57313225000); Mgbejime, Goodness Temofe (57311893400); Oluwasanmi, Ariyo (57210636841)","57206902001; 10738940600; 6506091558; 57201671873; 57216692640; 57313225000; 57311893400; 57210636841","A super-resolution generative adversarial network with siamese CNN based on low quality for breast cancer identification","2021","2021 4th International Conference on Pattern Recognition and Artificial Intelligence, PRAI 2021","","","","218","223","5","10.1109/PRAI53619.2021.9551033","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117956871&doi=10.1109%2fPRAI53619.2021.9551033&partnerID=40&md5=370888615955690f63a7ef6ada16265d","Breast cancer is a chronic illness leading to the death of millions of people yearly. Despite the fact that successful identification of benign and malignant images is dependent on radiologists' long-term knowledge, specialists occasionally disagree with their decisions. An automatic system provides an alternative choice for the image diagnosis, thereby helping the expert to make more reliable decisions efficiently, less prone to errors and make diagnosis more scalable. Another issue based with the diagnosis of breast cancer identification is the poor quality of the image which poses a challenge in identification performance. An enhanced super-resolution generative adversarial network has been implemented in this paper to produce super-resolution images of breast cancer from a low-resolution counterpart with higher quality and finer details using an upscale factor of 4. Additionally, siamese convolutional neural network was utilized for the features extraction and classification of breast cancer. The proposed model provides an effective classification performance in terms of accuracy and ROC-AUC scores of 98.87% and 98.76% respectively as compared to other existing approaches.  © 2021 IEEE.","Convolutional neural networks; Deep learning; Diseases; Image enhancement; Medical imaging; Optical resolving power; Automatic systems; Breast Cancer; Chronic illness; Deep learning; Image diagnosis; Low qualities; Performance; Resolution images; Siamese network; Superresolution; Generative adversarial networks","Breast cancer; Deep learning; Generative adversarial network; Siamese network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85117956871"
"Kigure H.","Kigure, Hiromitsu (57288849900)","57288849900","Application of Video-to-Video Translation Networks to Computational Fluid Dynamics","2021","Frontiers in Artificial Intelligence","4","","670208","","","","10.3389/frai.2021.670208","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117912299&doi=10.3389%2ffrai.2021.670208&partnerID=40&md5=5006d532e63f0e5089606e67c1f7334a","In recent years, the evolution of artificial intelligence, especially deep learning, has been remarkable, and its application to various fields has been growing rapidly. In this paper, I report the results of the application of generative adversarial networks (GANs), specifically video-to-video translation networks, to computational fluid dynamics (CFD) simulations. The purpose of this research is to reduce the computational cost of CFD simulations with GANs. The architecture of GANs in this research is a combination of the image-to-image translation networks (the so-called “pix2pix”) and Long Short-Term Memory (LSTM). It is shown that the results of high-cost and high-accuracy simulations (with high-resolution computational grids) can be estimated from those of low-cost and low-accuracy simulations (with low-resolution grids). In particular, the time evolution of density distributions in the cases of a high-resolution grid is reproduced from that in the cases of a low-resolution grid through GANs, and the density inhomogeneity estimated from the image generated by GANs recovers the ground truth with good accuracy. Qualitative and quantitative comparisons of the results of the proposed method with those of several super-resolution algorithms are also presented. © Copyright © 2021 Kigure.","","computational fluid dynamics (CFD); deep learning; generative adversarial networks (GANs); image-to-image translation networks (pix2pix); long short-term memory (LSTM)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85117912299"
"Panchal P.; Raman V.C.; Baraskar T.; Sinha S.; Purohit S.; Modi J.","Panchal, Poojan (57216312490); Raman, Vignesh Charan (57216314498); Baraskar, Trupti (36871711400); Sinha, Shambhavi (57336512200); Purohit, Swaraj (57337347400); Modi, Jaynam (57337347500)","57216312490; 57216314498; 36871711400; 57336512200; 57337347400; 57337347500","Reconstruction of Missing Data in Satellite Imagery Using SN-GANs","2022","Lecture Notes in Networks and Systems","286","","","629","638","9","10.1007/978-981-16-4016-2_60","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119013878&doi=10.1007%2f978-981-16-4016-2_60&partnerID=40&md5=d26cfbaf737f3013a7a3d9b000135b61","In the field of remote sensing satellite imagery, malfunctions in the available raw data are prominent. Especially in Short-Wave Infrared (SWIR) detectors used in satellite imaging cameras, which suffer from dropouts in pixel and line direction in raw data. With the recent development in generative adversarial networks and its vast application in inpainting the missing data, the possibility to predict and fill in the missing data accurately with contextual attention has become prevalent. This paper presents SN-GANs (SN-generative adversarial networks) which is two-staged architecture, and it is based on the concept of feed forward neural networks with contextual attention layers. While reconstructing the corrupted part of the images, the model takes surrounding pixels into consideration. Moreover, this architecture is adept enough to fill in the multiple lines and pixel dropouts efficiently even in super-resolution satellite images. The available traditional methods fail to address the loss of data that incurs, while inpainting a 16-bit raw image because they are effective enough for 8-bit RGB images. SN-GANs have effectively resolved this issue with a lossless image inpainting method for 16-bit satellite images as it retains the features of non-corrupted data. The performance of the model is evaluated using similarity metrics like structural similarity index measure (SSIM), peak signal-to-noise ratio (PSNR) and mean-squared error (MSE). © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","Column-dropouts; Deep learning; GANS; Image reconstruction; Satellite imagery; SN-GANs; SWIR","Conference paper","Final","","Scopus","2-s2.0-85119013878"
"Yang Z.; Wang Y.","Yang, Zhiwei (57225126998); Wang, Yunyan (55734131800)","57225126998; 55734131800","Image Enhancement and Improvement Algorithm Based on Esrgan Singal Frame Remote Sensing Image","2021","Journal of Physics: Conference Series","1952","2","022012","","","","10.1088/1742-6596/1952/2/022012","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109215707&doi=10.1088%2f1742-6596%2f1952%2f2%2f022012&partnerID=40&md5=0923ff999a706afabade40947471dc47","Traditional spline interpolation algorithm for reconstruction of visual effects are not good, based on Super Resolution against Network (Super - Resolution Generative Adversarial Network, SRGAN) edge character to deal with such problems as imperfect, using a generated based on the enhanced Super Resolution against Network to improve the Resolution of ordinary optical remote sensing images, the first Network generated by high Resolution data sets to train (G network), then lower Resolution test data model test, The test results and real results are put into the discrimination network (D network) to get the adversarial loss, and then the generated network is modified according to the adversarial loss. The superiority of the network is improved by introducing dense residuals to SRGAN, modifying the judgment object of the discriminator to be relatively real, and using the eigenvalue before activation to improve the perceived loss. The desert, farmland, forest and mountain data were tested on AID data set, and the algorithm in this paper could obtain the recomposition of the real image more closely. Compared with SRResNet and SRGAN algorithms, PSNR improved by about 4.0db and SSIM improved by about 0.14. This method improves the feature comprehensiveness by increasing the network fineness degree, and USES the modified perception loss to get the brightness closer to the real image, which is beneficial to improve the quality of single frame remote sensing image.  © Published under licence by IOP Publishing Ltd.","Discriminators; Edge detection; Eigenvalues and eigenfunctions; Interpolation; Optical data processing; Optical resolving power; Remote sensing; Adversarial networks; High resolution data; Lower resolution; Optical remote sensing; Remote sensing images; Spline interpolation; Super resolution; Visual effects; Image enhancement","Confrontation; image; network; Reconstruction error; Remote; sensing; Super resolution","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85109215707"
"Güemes A.; Discetti S.; Ianiro A.; Sirmacek B.; Azizpour H.; Vinuesa R.","Güemes, A. (57209686139); Discetti, S. (37123621500); Ianiro, A. (36600822100); Sirmacek, B. (25652523500); Azizpour, H. (55431173400); Vinuesa, R. (55553485100)","57209686139; 37123621500; 36600822100; 25652523500; 55431173400; 55553485100","From coarse wall measurements to turbulent velocity fields through deep learning","2021","Physics of Fluids","33","7","075121","1ENG","","","10.1063/5.0058346","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110958305&doi=10.1063%2f5.0058346&partnerID=40&md5=66c02c7459e32f222317757695aab7db","This work evaluates the applicability of super-resolution generative adversarial networks (SRGANs) as a methodology for the reconstruction of turbulent-flow quantities from coarse wall measurements. The method is applied both for the resolution enhancement of wall fields and the estimation of wall-parallel velocity fields from coarse wall measurements of shear stress and pressure. The analysis has been carried out with a database of a turbulent open-channel flow with a friction Reynolds number. We first show that SRGAN can be used to enhance the resolution of coarse wall measurements. If compared with the direct reconstruction from the sole coarse wall measurements, SRGAN provides better instantaneous reconstructions, in terms of both mean-squared error and spectral-fractional error. Even though lower resolutions in the input wall data make it more challenging to achieve highly accurate predictions, the proposed SRGAN-based network yields very good reconstruction results. Furthermore, it is shown that even for the most challenging cases, the SRGAN is capable of capturing the large-scale structures that populate the flow. The proposed novel methodology has a great potential for closed-loop control applications relying on non-intrusive sensing. © 2021 Author(s).","Closed loop control systems; Mean square error; Open channel flow; Reynolds equation; Reynolds number; Shear flow; Shear stress; Velocity; Adversarial networks; Closed-loop control; Large scale structures; Mean squared error; Novel methodology; Resolution enhancement; Turbulent open channel flow; Turbulent velocity fields; Deep learning","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85110958305"
"Molahasani Majdabadi M.; Choi Y.; Deivalakshmi S.; Ko S.","Molahasani Majdabadi, Mahdiyar (57216440558); Choi, Younhee (56173112600); Deivalakshmi, S. (36600200500); Ko, Seokbum (7403326100)","57216440558; 56173112600; 36600200500; 7403326100","Capsule GAN for prostate MRI super-resolution","2022","Multimedia Tools and Applications","81","3","","4119","4141","22","10.1007/s11042-021-11697-z","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120182061&doi=10.1007%2fs11042-021-11697-z&partnerID=40&md5=8b2916c219fec092ddafe054c935c1d4","Prostate cancer is a prevalent disease among adult men. One in seven Canadian men is diagnosed with this cancer in their lifetime. Super-Resolution (SR) can facilitate early diagnosis and potentially save many lives. In this paper, a robust and accurate model is proposed for prostate MRI SR. For the first time, MSG-GAN and CapsGAN are utilized simultaneously for high-scale medical SR. The model is trained on the Prostate-Diagnosis and PROSTATEx datasets. The proposed model outperformed the state-of-the-art prostate SR model in all similarity metrics with substantial margins. For 8 × SR, 19.77, 0.60, and 0.79 are achieved for Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index metric (SSIM), and Multi-Scale Structural SIMilarity index metric (MS-SSIM), respectively. A new task-specific similarity assessment is introduced as well. A classifier is trained for severe cancer detection. The drop in the accuracy of this model when dealing with super-resolved images is used to evaluate the ability of medical detail reconstruction of the SR models. The proposed model surpassed state-of-the-art work with a 6% margin. The model is also more compact in comparison with the related architecture and has 45% less number of trainable parameters. The proposed SR model is a step towards an efficient and accurate general medical SR platform. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Diagnosis; Generative adversarial networks; Medical imaging; Optical resolving power; Signal to noise ratio; Urology; Capsule network; Early diagnosis; Generative adversarial network; Prostate cancers; Robust modeling; Similarity indices; State of the art; Structural similarity; Super-resolution models; Superresolution; Diseases","Capsule network; Generative Adversarial Network (GAN); MRI; Prostate cancer; Super resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85120182061"
"Li Z.; Tao R.; Wang J.; Li F.; Niu H.; Yue M.; Li B.","Li, Ziqiang (56857136700); Tao, Rentuo (57190389018); Wang, Jie (57479579700); Li, Fu (57942274500); Niu, Hongjing (57219755657); Yue, Mingdao (57226112647); Li, Bin (57819851500)","56857136700; 57190389018; 57479579700; 57942274500; 57219755657; 57226112647; 57819851500","Interpreting the Latent Space of GANs via Measuring Decoupling","2021","IEEE Transactions on Artificial Intelligence","2","1","","58","70","12","10.1109/TAI.2021.3071642","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129045759&doi=10.1109%2fTAI.2021.3071642&partnerID=40&md5=3d7b8baa9919133cfd34c8231f17421d","With the success of generative adversarial networks (GANs) on various real-world applications, the controllability and security of GANs have raised more and more concerns from the community. Specifically, understanding the latent space of GANs, i.e., obtaining the completely decoupled latent space, is essential for applications in some secure scenarios. At present, there is no quantitative method to measure the decoupling of latent space, which is not conducive to the development of the community. In this article, we propose two methods to measure the sensitivity of latent dimensions: one is a sequential intervention method, and the other is an optimization-based method that measures the sensitivity in both the value and the direction. With these two methods, the decoupling of latent space can be measured by the sparsity of the sensitivity vector obtained. The effectiveness of the proposed methods has been verified by experiments on the representative GANs. Code will be available at https://github.com/iceli1007/latent-analysis-of. Impact Statement-Generative adversarial networks (GANs) is a popular technology in image generation. Benefiting from the development of neural networks, GANs have been widely used in many real-world tasks, such as super-resolution, image translation, and image inpainting. However, the understanding of the generator in GANs as a map from latent space (represented in a multidimensional vector) to image space is incomplete. To make better use of GANs, more insight into the generation process is required. The motivation of this article is to develop methods to analyze the influence of each dimension of latent space on the generated results, and to measure the decoupling of latent space, the knowledge obtained will be useful for the development of more precise controllable image generation technology. © 2021 IEEE.","Vector spaces; Correlation analysis; Decouplings; Generative adversarial net; Image generations; Interpreting network; Intervention methods; Latent space; Optimization based methods; Quantitative method; Real-world; Generative adversarial networks","Correlation analysis; generative adversarial nets (GANs); interpreting networks; latent space","Article","Final","","Scopus","2-s2.0-85129045759"
"Li X.; Dong N.; Huang J.; Zhuo L.; Li J.","Li, Xiaoguang (36600859900); Dong, Ning (57207940049); Huang, Jianglu (57222391530); Zhuo, Li (55568078600); Li, Jiafeng (35111290700)","36600859900; 57207940049; 57222391530; 55568078600; 35111290700","A discriminative self-attention cycle GAN for face super-resolution and recognition","2021","IET Image Processing","15","11","","2614","2628","14","10.1049/ipr2.12250","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105124256&doi=10.1049%2fipr2.12250&partnerID=40&md5=e1893882a36dee7a029e31cd6a03d167","Face image captured via surveillance videos in an open environment is usually of low quality, which seriously affects the visual quality and recognition accuracy. Most image super-resolution methods adopt paired high-quality and its interpolated low-resolution version to train the super-resolution network. It is difficult to achieve contented visual quality and restoring discriminative features in real scenarios. A discriminative self-attention cycle generative adversarial network is proposed for real-world face image super-resolution. Based on the cycle GAN framework, unpaired samples are adopted to train a degradation network and a reconstruction network simultaneously. A self-attention mechanism is employed to capture the contextual information for details restoring. A Siamese face recognition network is introduced to provide a constraint on identify consistency. In addition, an asymmetric perceptual loss is introduced to handle the imbalance between the degradation model and the reconstruction model. Experimental results show that the observation model achieved more realistic low-quality face images, and the super-resolved face images have shown better subjective quality and higher face recognition performance. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology","Optical resolving power; Security systems; Adversarial networks; Attention mechanisms; Contextual information; Discriminative features; Face super-resolution; Image super resolutions; Recognition accuracy; Reconstruction networks; Face recognition","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105124256"
"Hou Z.; Cao D.; Ji S.; Cui R.; Liu Q.","Hou, Zhiyu (57273002000); Cao, Danping (36873885500); Ji, Siqi (57226359900); Cui, Rongang (57223201849); Liu, Qiang (57212729570)","57273002000; 36873885500; 57226359900; 57223201849; 57212729570","Enhancing digital rock image resolution with a GAN constrained by prior and perceptual information","2021","Computers and Geosciences","157","","104939","","","","10.1016/j.cageo.2021.104939","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115743055&doi=10.1016%2fj.cageo.2021.104939&partnerID=40&md5=5d6c55249a5e06a01428dd82b6ae90cc","In digital rock physics, pore distribution and microstructure of rocks affect the flow properties and elastic characteristic of rocks. It is difficult to accurately characterize these parameters in low-resolution images due to rock heterogeneity, and improving with instrument is costly. Traditional interpolation method to enhance the image resolution only enriches the low frequency information of the image without focusing on the boundary which is rich in high frequency information, and conventional deep-learning methods cannot perform a satisfying result in different-level instrument noise. To address these issues, a generative adversarial network of an image segmentation network as discriminator constrained by perspective information and prior information (SCPGAN) is proposed to improve images resolution by enhancing medium-high frequency information of images and improving network anti-noise capacity. The perceptual information is extracted by VGG19 to enrich boundary and texture information and the porosity is built by discriminator as prior information to guide generator to improve noise immunity. Traditional interpolation and CNN are used to verify that perceptual information can be augmented with medium-high frequency information, and GAN-based models are used to verify that prior information can improve network performance. The result shows the network with perceptual information can significantly improve images resolution and outperform the traditional interpolation on structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), and multiple-point connectivity (MPC), while GAN-based model with prior information shows an excellent anti-noise capacity, especially high-level noise, and has similar physical properties to the high-resolution model in the permeability simulation. © 2021 Elsevier Ltd","Deep learning; Image enhancement; Image resolution; Image segmentation; Interpolation; Rocks; Signal to noise ratio; Textures; Anti-noise capacity; Digital rock image; Flow properties; High-frequency informations; Perceptual information; Pore microstructures; Pores distribution; Prior information; Rock physics; Superresolution; artificial neural network; image processing; image resolution; microstructure; permeability; porosity; rock property; Generative adversarial networks","Digital rock image; Enhancing image; Generative adversarial network; Prior and perceptual information; Super resolution","Article","Final","","Scopus","2-s2.0-85115743055"
"Ozcelik F.; Alganci U.; Sertel E.; Unal G.","Ozcelik, Furkan (57208564493); Alganci, Ugur (24281315400); Sertel, Elif (21934838300); Unal, Gozde (57220534209)","57208564493; 24281315400; 21934838300; 57220534209","Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs","2021","IEEE Transactions on Geoscience and Remote Sensing","59","4","9153037","3486","3501","15","10.1109/TGRS.2020.3010441","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103340930&doi=10.1109%2fTGRS.2020.3010441&partnerID=40&md5=4547984f760413e2dede1e82f04b7766","Convolutional neural network (CNN)-based approaches have shown promising results in the pansharpening of the satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared with the existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas the CNN-based methods provide a reduced-resolution panchromatic image as the input to their model along with the reduced-resolution multispectral images and, hence, learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as the input and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization generative adversarial network (PanColorGAN) framework, help overcome the spatial-detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods, as demonstrated in our experiments. © 1980-2012 IEEE.","Electrical engineering; Geology; Adversarial networks; Full resolutions; Multispectral images; Noise injection; Panchromatic images; Reduced resolution; Satellite images; Super resolution; data set; image analysis; image resolution; multispectral image; panchromatic image; satellite imagery; Convolutional neural networks","AI; colorization; convolutional neural networks (CNNs); deep learning; generative adversarial networks (GANs); image fusion; PanColorization generative adversarial network (PanColorGAN); pansharpening; self-supervised learning; super-resolution (SR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85103340930"
"Wei Y.; Hou W.","Wei, Yangjie (35175095700); Hou, Weihan (57393381700)","35175095700; 57393381700","Blurring kernel extraction and super-resolution image reconstruction based on style generative adersarial networks","2021","Optics Express","29","26","","44024","44044","20","10.1364/OE.441507","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122062237&doi=10.1364%2fOE.441507&partnerID=40&md5=22019ee988d21cb9ded570cc0c3b6c6f","The point spread function (PSF) is the main index used to evaluate the imaging resolution and further improve the quality of an optical image. Its measurement is significant for system development and pattern recognition. However, the precision of current measurement methods is low owing to a complicated modelling process, the pairing of various camera parameters, and disturbances by external factors. In this paper, we propose a method to extract blurring kernels and reconstruct super-resolution images based on style generative adversarial networks (StyleGANs). First, an improved StyleGAN model is introduced and an ideal blurry image generation model based on StyleGAN is trained to obtain a series of ideal Gaussian light-source images with a regular Airy disk; as the intensity distribution in the Airy disk is closer to its theoretical distribution. Second, the blurring kernels are extracted at different depth positions from the generated Gaussian light-source images to replace the PSF. This allows the evaluation of the blurry property of the optical system and effectively avoids the enrolment of noise in parameter identification or curve fitting in PSF representation. Finally, both the blurring kernels are used to deblur the blurry images of the Gaussian light source with a single wavelength and the blurry images of microbeads under visual light illumination at different depths based on the learnable convolutional half-quadratic splitting and convolutional preconditioned Richardson (LCHQS-CPCR) model. Compared to other image deblurring methods, our proposed method achieves high-resolution image reconstruction with blurring kernels from the generated optical images of the Gaussian light source. © 2021 Optical Society of America under the terms of the OSA Open Access Publishing Agreement","Convolution; Curve fitting; Gaussian distribution; Gaussian noise (electronic); Geometrical optics; Image enhancement; Image reconstruction; Optical resolving power; Optical systems; Optical transfer function; Parameter estimation; Pattern recognition; Quality control; Airy disks; Blurring kernels; Blurry images; Imaging resolutions; Measurement methods; Optical image; Point-Spread function; Source images; Super-resolution image reconstruction; System development; Light sources","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122062237"
"Zhang S.; Fu G.; Wang H.; Zhao Y.","Zhang, Shaolei (57208773089); Fu, Guangyuan (7202722322); Wang, Hongqiao (56322051800); Zhao, Yuqing (57209912605)","57208773089; 7202722322; 56322051800; 57209912605","Degradation learning for unsupervised hyperspectral image super-resolution based on generative adversarial network","2021","Signal, Image and Video Processing","15","8","","1695","1703","8","10.1007/s11760-021-01902-9","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112365846&doi=10.1007%2fs11760-021-01902-9&partnerID=40&md5=ca8613e5e6475f4fd4f092f96ac72af6","High-resolution is essential to achieve remarkable performance in many applications using hyperspectral images. However, the acquired hyperspectral images are often low-resolution due to the limitations of sensors. Recently, deep learning-based methods have been widely studied to address the super-resolution problem, and most super-resolution methods are learned with the simulated dataset. However, the pre-defined down-sampling function employed in the simulation is simple, thus the trained model often suffers from the poor generalization for real-world images. In this paper, we propose an unsupervised super-resolution approach that does not require the paired dataset. The method is conducted in two stages: unsupervised image generation and supervised image super-resolution. Specifically, the image generation model generates a low-resolution image through a generative adversarial network in an unsupervised manner. Then, the generated low-resolution images are used to train the super-resolution model with high-resolution images in a supervised manner. To explore the high correlation of hyperspectral image in spectral and spatial domain, group convolution, attention mechanism, and multi-level features are employed in the super-resolution model. Experiments on the public dataset CAVE and Harvard show that the proposed model provides outperforming super-resolution ability over the compared methods. Also, the results on images with unknown degradation show a promising generalization of the proposed model. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Deep learning; Spectroscopy; Adversarial networks; Attention mechanisms; High resolution image; Image super resolutions; Learning-based methods; Low resolution images; Super-resolution models; Superresolution methods; Optical resolving power","Attention mechanism; Hyperspectral image super-resolution; Multi-level feature; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85112365846"
"Zhang X.; Feng C.; Wang A.; Yang L.; Hao Y.","Zhang, Xiong (57192503819); Feng, Congli (57219441081); Wang, Anhong (7404620368); Yang, Linlin (57212655994); Hao, Yawen (57216565786)","57192503819; 57219441081; 7404620368; 57212655994; 57216565786","CT super-resolution using multiple dense residual block based GAN","2021","Signal, Image and Video Processing","15","4","","725","733","8","10.1007/s11760-020-01790-5","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092693874&doi=10.1007%2fs11760-020-01790-5&partnerID=40&md5=44d8dee42cbb160740c9d18880b996df","High-resolution computed tomography (CT) can provide accurate diagnostic information for clinical applications. However, using CT scanning equipment to obtain high-resolution CT directly may cause significant radiation damage to human body. Low-dose CT super-resolution using generative adversarial network (GAN) can improve the visual quality of CT while maintaining a low radiation dose to human body. The existing GAN networks for super-resolution still suffer from the issues such as weak feature expression and network redundancy. This work proposes a novel lightweight multiple dense residual block structure-based GAN network for CT super-resolution. The new structure reduces the number of residual units and establishes a dense link among all residual blocks, which can reduce network redundancy and ensure maximum information transmission. In addition, in order to avoid the gradient vanishing phenomena, the Wasserstein distance is introduced into the loss function. Experimental results show that the presented method achieved a more desirable visual quality with more high-frequency details for different upscaling factors than other popular methods did. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Optical resolving power; Radiation damage; Redundancy; Adversarial networks; Clinical application; Feature expression; High resolution CT; High-resolution computed tomography; Information transmission; Network redundancy; Wasserstein distance; Computerized tomography","CT; Generative adversarial network; Multiple dense residual block; Super-resolution; Wasserstein distance","Article","Final","","Scopus","2-s2.0-85092693874"
"Santosh K.C.; Ghosh S.; Bose M.","Santosh, K.C. (14831502300); Ghosh, Sourodip (57219051225); Bose, Moinak (57223086009)","14831502300; 57219051225; 57223086009","Ret-GAN: Retinal image enhancement using generative adversarial networks","2021","Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June","","9474782","79","84","5","10.1109/CBMS52027.2021.00082","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110829489&doi=10.1109%2fCBMS52027.2021.00082&partnerID=40&md5=f95d3f97055fdec437b0822cccdc8ca6","With over 200K cases in the U.S. alone, retinal disorders are the most common cause of irreversible blindness. This serves as a primary aim to analyze automated screening tools to detect retinal disorders. We analyze the OCT dataset (84, 484 images) and enhance the images by using Generative Adversarial Networks (GANs). This work specifically focuses on enhancing the quality of source (training) images for better algorithm validatiorr/testing11Authors contributed equally to the work.. We synthesize super resolution-based images using generators, discriminators and the adversarial nature of the GANs. The performance of the Ret-GAN is validated by PSNR, SSIM, and loss functions. To test the Ret-GAN generated images, we train a convolutional neural network (CNN) with the original dataset images and super-resolution images. We achieve an accuracy of 0.9825 on Ret-GAN generated image data, and 0.9525 on the original data. We statistically analyze the CNN with a number of evaluation metrics to further validate the results. The proposed scheme is compared to benchmark research findings on the same dataset. Our results are encouraging.  © 2021 IEEE.","Automation; Convolutional neural networks; Diagnosis; Ophthalmology; Optical resolving power; Statistical tests; Adversarial networks; Automated screening; Benchmark researches; Evaluation metrics; Image data; Loss functions; Super resolution; Image enhancement","Generative Adversarial Networks; Ret-GAN; Retinal disorders","Conference paper","Final","","Scopus","2-s2.0-85110829489"
"Shao J.; Chen L.; Wu Y.","Shao, Jun (57223030190); Chen, Liang (57059516200); Wu, Yi (57215898068)","57223030190; 57059516200; 57215898068","SRWGANTV: Image Super-Resolution through Wasserstein Generative Adversarial Networks with Total Variational Regularization","2021","2021 IEEE 13th International Conference on Computer Research and Development, ICCRD 2021","","","9386518","21","26","5","10.1109/ICCRD51685.2021.9386518","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104555262&doi=10.1109%2fICCRD51685.2021.9386518&partnerID=40&md5=cb6206261ae743b06ce33fd05ee5e6a6","The study of generative adversarial networks (GAN) has enormously promoted the research work on single image super-resolution (SISR) problem. SRGAN firstly apply GAN to SISR reconstruction, which has achieved good results. However, SRGAN sacrifices the fidelity. At the same time, it is well known that the GANs are difficult to train and the improper training fails the SISR results easily. Recently, Wasserstein Generative Adversarial Network with gradient penalty (WGAN-GP) has been proposed to alleviate these issues at the expense of performance of the model with a relatively simple training process. However, we find that applying WGAN-GP to SISR still suffers from training instability, leading to failure to obtain a good SR result. To address this problem, we present an image super resolution framework base on enhanced WGAN (SRWGAN-TV). We introduce the total variational (TV) regularization term into the loss function of WGAN. The total variational (TV) regularization term can stabilize the network training and improve the quality of generated images. Experimental results on public datasets show that the proposed method achieves superior performance in both quantitative and qualitative measurements. © 2021 IEEE.","Optical resolving power; Adversarial networks; Image super resolutions; Loss functions; Network training; Qualitative measurements; Regularization terms; Training process; Variational regularization; Image enhancement","gradient penalty; single image super-resolution; total variational regularization; Wasserstein Generative Adversarial Network (WGAN)","Conference paper","Final","","Scopus","2-s2.0-85104555262"
"Atienza A.J.; Calabano A.; Manalo M.L.; Salandanan S.B.; Lucas C.R.; De Leon F.; Ambatali C.; Tolentino C.T.","Atienza, Allysa Joy (57219411212); Calabano, Andrei (57415116100); Manalo, Marie Lourdes (57414479700); Salandanan, Sophia Beatrice (57414635800); Lucas, Crisron Rudolf (57203033384); De Leon, Franz (16678716900); Ambatali, Charleston (57194210395); Tolentino, Carl Timothy (57209858882)","57219411212; 57415116100; 57414479700; 57414635800; 57203033384; 16678716900; 57194210395; 57209858882","Decompression of Bluetooth-transmitted Audio using Super Resolution for Low-Latency Applications","2021","International Conference on ICT Convergence","2021-October","","","530","534","4","10.1109/ICTC52510.2021.9620765","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122957612&doi=10.1109%2fICTC52510.2021.9620765&partnerID=40&md5=2f559ccd13f21b5820431ef7d926054d","Bluetooth devices experience a common trade-off between quality and latency. This affects applications that require fast and accurate transmission of audio signals, especially in the medical and musical industry. In this paper, the use of super resolution techniques such as the Convolutional Neural Networks (CNN) and Generative Adversarial Networks (GAN) were utilized to improve the quality of Bluetooth-transmitted audio signals. We have shown that these two models were able to improve a number of speech and non speech audio signals based on our performance metrics: (1) Signal-to-Noise Ratio (SNR); (2) Log Spectral Distance (LSD); (3) Perceptual Evaluation of Audio Quality (PEAQ); and (4) Multi Stimulus test with Hidden Reference and Anchor (MUSHRA). Both models were able to improve the Bluetooth-transmitted audio signals, although the GAN model produced better results on both the objective and subjective evaluation tests compared to the CNN model. For the SNR, LSD, PEAQ, and MUSHRA, the GAN model averaged 20.4170 dB, 2.1408 dB, -2.7190, and 42.6500 respectively, while the CNN model averaged -1.7190 dB, 1.9410 dB, -2.7190, and 17.1600 respectively. For this project, the subjective test MUSHRA bear more weight in the results as the objective tests shows inconsistencies and cannot be heavily relied on.  © 2021 IEEE.","Audio acoustics; Bluetooth; Convolutional neural networks; Deep neural networks; Economic and social effects; Music; Optical resolving power; Quality control; Signal to noise ratio; Audio decompression; Audio signal; Convolutional neural network; Deep learning; Latency; Network models; Neural network model; Perceptual evaluation of audio qualities; Spectral distances; Superresolution; Generative adversarial networks","audio decompression; bluetooth; deep learning; latency; super resolution","Conference paper","Final","","Scopus","2-s2.0-85122957612"
"Carver W.; Nwogu I.","Carver, William (57223278072); Nwogu, Ifeoma (24177267900)","57223278072; 24177267900","Facial Expression Neutralization with StoicNet","2021","Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision Workshops, WACVW 2021","","","9407808","201","208","7","10.1109/WACVW52041.2021.00026","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105450295&doi=10.1109%2fWACVW52041.2021.00026&partnerID=40&md5=bb384713199535a9303a0de5cf6702e1","Expression neutralization is the process of synthetically altering an image of a face so as to remove any facial expression from it without changing the face's identity. Facial expression neutralization could have a variety of applications, particularly in the realms of facial recognition, in action unit analysis, or even improving the quality of identification pictures for various types of documents. Our proposed model, StoicNet, combines the robust encoding capacity of variational autoencoders, the generative power of generative adversarial networks, and the enhancing capabilities of super resolution networks with a learned encoding transformation to achieve compelling expression neutralization, while preserving the identity of the input face. Objective experiments demonstrate that StoicNet successfully generates realistic, identity-preserved faces with neutral expressions, regardless of the emotion or expression intensity of the input face. © 2021 IEEE.","Computer vision; Encoding (symbols); Signal encoding; Action Unit; Adversarial networks; Autoencoders; Encoding capacity; Expression intensities; Facial Expressions; Facial recognition; Super resolution; Face recognition","","Conference paper","Final","","Scopus","2-s2.0-85105450295"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","11","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120169160&partnerID=40&md5=e2fef5a2a9652d1f060f19fbd0129cc6","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85120169160"
"Nguyen-Truong H.; Nguyen K.N.A.; Cao S.","Nguyen-Truong, Hai (57222052718); Nguyen, Khoa N. A. (57222051103); Cao, San (57222051871)","57222052718; 57222051103; 57222051871","SRGAN with Total Variation Loss in Face Super-Resolution","2020","Proceedings - 2020 7th NAFOSTED Conference on Information and Computer Science, NICS 2020","","","9335836","292","297","5","10.1109/NICS51282.2020.9335836","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101115077&doi=10.1109%2fNICS51282.2020.9335836&partnerID=40&md5=844f82712c77702f7d7b7def65226ab7","Facial image super-resolution is a crucial preprocessing for facial image analysis, face recognition, and image-based 3D face reconstruction. Convolutional neural networks were earlier used to produce high-resolution images that train quicker and shown excellent performance by learning mapping relation using pairs of low-resolution and high-resolution images. However, in some cases, they are incapable of recovering finer details and often generate blurry images. In this paper, we evaluate a method of applying Generative adversarial networks in generating realistic super-resolution images from low-resolution ones by using three typical losses for super-resolution: Content Loss, Adversarial Loss, Perceptual Loss, and proposed to use Total Variation Loss. We try different pre-trained famous Convolutional neural networks models (VGG19, FaceNet, and EfficientNet) in Perceptual Loss to have a general view with different backbones. Our network gains 32.67 of Peak signal-to-noise ratio (PSNR) and 0.89 of Structural similarity index (SSIM) in 100 random samples from the Flickr-Faces-HQ dataset. © 2020 IEEE.","Convolution; Convolutional neural networks; Optical character recognition; Optical resolving power; Signal to noise ratio; 3D face reconstruction; Adversarial networks; Face super-resolution; High resolution image; Mapping relation; Peak signal to noise ratio; Structural similarity indices (SSIM); Super resolution; Face recognition","","Conference paper","Final","","Scopus","2-s2.0-85101115077"
"","","","2021 9th E-Health and Bioengineering Conference, EHB 2021","2021","2021 9th E-Health and Bioengineering Conference, EHB 2021","","","","","","924","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124521367&partnerID=40&md5=623881e08a90798d2cd6292f4c8860d5","The proceedings contain 214 papers. the topics discussed include: anisotropic diffusion-based multiscale medical image analysis technique for COVID-19 detection; generative adversarial networks for medical image super-resolution; physicochemical and sensory characteristics of red wines; machine learning analysis for cervical cancer prediction, a systematic review of the literature; acute infarct volume prediction based on CT perfusion metrics derived from an automated software package using machine learning models; two-channel bioradar system for sleep-disordered breathing detection; a new respiratory rate sensing principle based on magnetic amorphous wire microtransformers; printing of dental works from cobalt-chrome metal powder and composite material; and a new step in detecting and preventing hereditary cancer: the hope project and mobile application.","","","Conference review","Final","","Scopus","2-s2.0-85124521367"
"Deng W.; Zhu Q.; Sun X.; Lin W.; Guan Q.","Deng, Weihuan (57224183913); Zhu, Qiqi (56420945500); Sun, Xiongli (57213190878); Lin, Weihua (57224195800); Guan, Qingfeng (55838509944)","57224183913; 56420945500; 57213190878; 57224195800; 55838509944","EML-GAN: GENERATIVE ADVERSARIAL NETWORK-BASED END-TO-END MULTI-TASK LEARNING ARCHITECTURE FOR SUPER-RESOLUTION RECONSTRUCTION AND SCENE CLASSIFICATION OF LOW-RESOLUTION REMOTE SENSING IMAGERY","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","5397","5400","3","10.1109/IGARSS47720.2021.9554060","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129901449&doi=10.1109%2fIGARSS47720.2021.9554060&partnerID=40&md5=0fe7a3c2eada4f8431bd4807afd84b84","High spatial resolution remote sensing images (HSR-RSIs) are critical to providing fine land cover/land use information for scene classification. The global low spatial resolution remote sensing images (LSR-RSIs) can be easily obtained at present, whereas it is still a challenge to acquire large-scale HSR-RSIs. In this paper, an algorithmic-based architecture is proposed to improve the spatial resolution of RSIs beyond the limits of imaging sensors. The generative adversarial network-based end-to-end multi-task learning architecture (EML-GAN) is proposed for LSR-RSIs super-resolution reconstruction and scene classification simultaneously. In EML-GAN, the generator network is used to recover the fine geometric structures of LSR-RSIs by fusing the deep contextual, structure, and edge information. In addition, the discriminator network is designed to predict the scene label and distinguish the real/fake of the input data. The proposed architecture is evaluated on a public dataset and two self-made dataset. The experimental results show that the proposed architecture improves the visual effect and classification performance of LSR-RSIs. © 2021 IEEE","Classification (of information); Image classification; Image reconstruction; Image resolution; Network architecture; Remote sensing; End to end; High spatial resolution; Learning architectures; Multitask learning; Network-based; Proposed architectures; Remote sensing images; Scene classification; Super-resolution reconstruction; Generative adversarial networks","generative adversarial network; remote sensing images; Scene classification; super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85129901449"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","8B-2021","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119957524&partnerID=40&md5=71785029ac2d98f19d88462fda5fe407","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119957524"
"Prajapati K.; Chudasama V.; Patel H.; Upla K.; Raja K.; Ramachandra R.; Busch C.","Prajapati, Kalpesh (57217177596); Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Upla, Kishor (53985429600); Raja, Kiran (57188866050); Ramachandra, Raghavendra (57190835798); Busch, Christoph (7101767185)","57217177596; 57202047982; 57209145428; 53985429600; 57188866050; 57190835798; 7101767185","Direct Unsupervised Super-Resolution Using Generative Adversarial Network (DUS-GAN) for Real-World Data","2021","IEEE Transactions on Image Processing","30","","","8251","8264","13","10.1109/TIP.2021.3113783","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115757041&doi=10.1109%2fTIP.2021.3113783&partnerID=40&md5=c3a8d2fd9c43ff1ce6a28ef93f80b153","The deep learning models for the Single Image Super-Resolution (SISR) task have found success in recent years. However, one of the prime limitations of existing deep learning-based SISR approaches is that they need supervised training. Specifically, the Low-Resolution (LR) images are obtained through known degradation (for instance, bicubic downsampling) from the High-Resolution (HR) images to provide supervised data as an LR-HR pair. Such training results in a domain shift of learnt models when real-world data is provided with multiple degradation factors not present in the training set. To address this challenge, we propose an unsupervised approach for the SISR task using Generative Adversarial Network (GAN), which we refer to hereafter as DUS-GAN. The novel design of the proposed method accomplishes the SR task without degradation estimation of real-world LR data. In addition, a new human perception-based quality assessment loss, i.e., Mean Opinion Score (MOS), has also been introduced to boost the perceptual quality of SR results. The pertinence of the proposed method is validated with numerous experiments on different reference-based (i.e., NTIRE Real-world SR Challenge validation dataset) and no-reference based (i.e., NTIRE Real-world SR Challenge Track-1 and Track-2) testing datasets. The experimental analysis demonstrates committed improvement from the proposed method over the other state-of-The-Art unsupervised SR approaches, both in terms of subjective and quantitative evaluations on different reference metrics (i.e., LPIPS, PI-RMSE graph) and no-reference quality measures such as NIQE, BRISQUE and PIQE. We also provide the implementation of the proposed approach (https://github.com/kalpeshjp89/DUSGAN) to support reproducible research. © 1992-2012 IEEE.","Deep learning; Job analysis; Optical resolving power; Quality control; Statistical tests; Image super resolutions; Learning models; Lower resolution; No-reference; Quality assessment; Real-world; Single images; Superresolution; Supervised trainings; Task analysis; article; human; perception; quality control; quantitative analysis; Generative adversarial networks","artificial neural networks; image enhancement; image quality; image reconstruction; interpolation; spatial resolution; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85115757041"
"Zhao S.; Fang Y.; Qiu L.","Zhao, Siqiang (57345201800); Fang, Yuan (57203373086); Qiu, Ling (7201537277)","57345201800; 57203373086; 7201537277","Deep learning-based channel estimation with SRGAN in OFDM systems","2021","IEEE Wireless Communications and Networking Conference, WCNC","2021-March","","","","","","10.1109/WCNC49053.2021.9417242","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119341273&doi=10.1109%2fWCNC49053.2021.9417242&partnerID=40&md5=f757e086c23ca5244bd7a34f759b4901","In this paper, we propose a novel deep learning–based channel estimation scheme in an orthogonal frequency division multiplexing (OFDM) system. The channel response with known pilot positions can be treated as a low-resolution image. Then, we explore a generative adversarial network (GAN) for channel super-resolution (SR) to estimate the whole channel state information (CSI). For previous deep learning-based channel estimators recovered by a single model, high-frequency details are missing and they fail to match the fidelity expected at the higher resolution. The scheme we proposed is more consistent with the real channel by adding a discriminator to recover more details of the channel. The simulation results show that our scheme is superior to other SR–based channel estimation methods and close to the linear minimum mean square error (LMMSE) performance. © 2021 IEEE.","Channel estimation; Channel state information; Deep learning; Frequency estimation; Mean square error; Optical resolving power; Orthogonal frequency division multiplexing; Channel estimator; Channel response; Channel-state information; Deep learning; Estimation schemes; Generative adversarial network; Low resolution images; Orthogonal frequency division multiplexing systems; Single models; Superresolution; Generative adversarial networks","Channel estimation; Deep learning; Generative adversarial network (GAN); Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85119341273"
"Qiao J.; Song H.; Zhang K.; Zhang X.","Qiao, Jiaojiao (57210157638); Song, Huihui (36572623600); Zhang, Kaihua (55475000500); Zhang, Xiaolu (57208245743)","57210157638; 36572623600; 55475000500; 57208245743","Conditional generative adversarial network with densely-connected residual learning for single image super-resolution","2021","Multimedia Tools and Applications","80","3","","4383","4397","14","10.1007/s11042-020-09817-2","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091728856&doi=10.1007%2fs11042-020-09817-2&partnerID=40&md5=dbcfe0a45392892d9c2e008a35c40288","Recently, generative adversarial network (GAN) has been widely employed in single image super-resolution (SISR), achieving favorably good perceptual effects. However, the SR outputs generated by GAN still have some fictitious details, which are quite different from the ground-truth images, resulting in a low PSNR value. In this paper, we leverage the ground-truth high-resolution (HR) image as a useful guide to learn an effective conditional GAN (CGAN) for SISR. Among it, we design the generator network via residual learning, which introduces dense connections to the residual blocks to effectively fuse low and high-level features across different layers. Extensive evaluations show that our proposed SR method performs much better than state-of-the-art methods in terms of PSNR, SSIM, and visual perception. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Multimedia systems; Adversarial networks; Different layers; High resolution image; High-level features; Perceptual effects; Single images; State-of-the-art methods; Visual perception; Optical resolving power","Conditional generative adversarial network; Deep convolutional neural network; Residual network; Super-resolution","Article","Final","","Scopus","2-s2.0-85091728856"
"Balaji Prabhu B.V.; Narasipura O.S.J.","Balaji Prabhu, B.V. (57202716108); Narasipura, Omkar Subburao Jois (55626396900)","57202716108; 55626396900","Improved Image Super-resolution Using Enhanced Generative Adversarial Network a Comparative Study","2021","Lecture Notes on Data Engineering and Communications Technologies","61","","","181","193","12","10.1007/978-981-33-4582-9_15","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106422361&doi=10.1007%2f978-981-33-4582-9_15&partnerID=40&md5=bdaeb8e707bfa17933c6d2c8e6274bbb","Super-resolution using generative adversarial networks is an approach for improving the quality of imaging system. With the advances in deep learning, convolutional neural networks-based models are becoming a favorite choice of researchers in image processing and analysis as it generates more accurate results compared to conventional methods. Recent works on image super-resolution have mainly focused on minimizing the mean squared reconstruction error and able to get high signal-to-noise ratios. But, they often lack high-frequency details and are not as accurate at producing high-resolution images as expected. With the aim of generating perceptually better images, this paper implements the enhanced generative adversarial model and compares with super-resolution generative adversarial model. The qualitative measures such as peak signal-to-noise ratio and structural similarity indices were used to assess the quality of the super-resolved images. The results obtained prove that, enhanced GAN model is able to recover more texture details when compared to super-resolution GAN models. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Convolutional neural networks; Deep learning; Image resolution; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Conventional methods; High resolution image; High signal-to-noise ratio; Image processing and analysis; Image super resolutions; Peak signal to noise ratio; Structural similarity indices; Image enhancement","Convolutional neural network; Deep learning; Enhanced generative adversarial network; Generative adversarial network; Peak signal-to-noise ratio; Structural similarity; Super-resolution","Book chapter","Final","","Scopus","2-s2.0-85106422361"
"Toriya H.; Dewan A.; Kitahara I.","Toriya, Hisatoshi (56125898000); Dewan, Ashraf (15925234800); Kitahara, Itaru (6603549979)","56125898000; 15925234800; 6603549979","Adaptive Image Scaling for Corresponding Points Matching between Images with Differing Spatial Resolutions","2020","Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020","","","9377754","3088","3095","7","10.1109/BigData50022.2020.9377754","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103823868&doi=10.1109%2fBigData50022.2020.9377754&partnerID=40&md5=f03edab89d2e00adf33f9c66ad0611dc","In this study, an image scaling method to improve the accuracy of the image registration between images using different imaging devices is proposed. It is known that conventional keypoint detection, description, and matching methods do not work well between images with different spatial resolutions, such as those captured by drones and satellites. Thus, we propose a method to improve the geometric accuracy of image registration through an adaptive combination of super-resolution and low- resolution images and downscaling to high-resolution images based on the assumption that artificial structures are perceived as relatively simple shapes in top-view images. If the superresolution factor is too high, artifacts are generated, and the accuracy of the corresponding matching points will be decreased. Thus, estimating the highest super-resolution factor while avoiding the artifacts is necessary. Using the super-resolution factor, super-resolution processing of satellite images and downscaling to drone images are simultaneously performed. This is followed by corresponding points matching to achieve high estimation accuracy in the image registration process. Through quantitative evaluation experiments performed using pairs of images with 12 times difference in spatial resolutions, we demonstrated that high-accuracy image registration is possible by applying super-resolution processing at a factor of 4 to 6. © 2020 IEEE.","Aircraft detection; Big data; Drones; Image registration; Optical resolving power; Adaptive combinations; Adaptive image scaling; Artificial structures; High resolution image; Low resolution images; Quantitative evaluation; Registration process; Spatial resolution; Image enhancement","drones; generative adversarial networks; image registration; satellite images; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85103823868"
"Patil P.W.; Dudhane A.; Murala S.","Patil, Prashant W. (57204924942); Dudhane, Akshay (57201668421); Murala, Subrahmanyam (26639647100)","57204924942; 57201668421; 26639647100","End-to-End Recurrent Generative Adversarial Network for Traffic and Surveillance Applications","2020","IEEE Transactions on Vehicular Technology","69","12","9288640","14550","14562","12","10.1109/TVT.2020.3043575","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097945088&doi=10.1109%2fTVT.2020.3043575&partnerID=40&md5=f6fa4f503f689448cca1726d5d7926c5","In video frame segmentation, many existing deep networks and contemporary approaches give a remarkable performance with the assumption that the only foreground is moving, and the background is stationary. However, in the presence of infrequent motion of foreground objects, sudden illumination changes in the background, bad weather, and dynamic background, the accurate foreground object(s) segmentation is a challenging task. Generative adversarial networks (GAN) based training shows fruitful results in various fields like image-to-image style transfer, image enhancement, semantic segmentation, image super-resolution, etc. The limited results of hand-crafted approaches for moving object segmentation (MOS) and the robustness of adversarial training for a given task inspired us to propose a novel approach for moving object segmentation (MOS). In this context, an end-to-end generative adversarial network (two generators) with recurrent technique is proposed for MOS and is named as RMS-GAN. The proposed RMS-GAN is able to incorporate foreground probability knowledge with residual and weight sharing based recurrent technique for accurate segmentation. The recurrent technique helps us to exhibit the temporal behavior between successive video frames, which is more prominent for any video processing applications. Also, to enhance the spatial coherence of the obtained foreground probability map using the generator-1 network, the cascaded architecture of two generators is proposed. The effectiveness of the proposed approach is evaluated both qualitatively and quantitatively on three benchmark video datasets for MOS. Experimental result analysis shows that the proposed network outperforms the existing state-of-the-art methods on three benchmark datasets for MOS.  © 1967-2012 IEEE.","Image enhancement; Image segmentation; Semantics; Video signal processing; Adversarial networks; Illumination changes; Image super resolutions; Moving object segmentation; Semantic segmentation; State-of-the-art methods; Surveillance applications; Video processing applications; Recurrent neural networks","Generative adversarial networks; motion; recurrent; video frame segmentation","Article","Final","","Scopus","2-s2.0-85097945088"
"He Y.; Wang L.; Yang F.; Clarysse P.; Zhu Y.","He, Yunlong (57193382175); Wang, Lihui (57141686200); Yang, Feng (56408792200); Clarysse, Patrick (6701664073); Zhu, Yuemin (55629946300)","57193382175; 57141686200; 56408792200; 6701664073; 55629946300","Deep Group-Wise Angular Translation of Cardiac Diffusion MRI in q-space via Manifold Regularized GAN","2020","International Conference on Signal Processing Proceedings, ICSP","2020-December","","9320925","511","515","4","10.1109/ICSP48669.2020.9320925","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100272190&doi=10.1109%2fICSP48669.2020.9320925&partnerID=40&md5=9a6b4ca7efc09d97e5c6ba5f2f9c3c99","Diffusion magnetic resonance imaging (dMRI) has become an indispensable tool for non-invasive characterization of fiber structures of tissues. Clinical applicability of dMRI is often shackled by trade-off between image quality and long acquisition time. We propose a novel group-wise image translation method to improve the angular resolution of cardiac dMRI data. It consists in using a generative adversarial network (GAN) model to estimate a sequence of images from given DW images acquired in a limited number of diffusion gradient directions. We embed a supervised manifold regularized term in the GAN loss function to exploit the correlation between multiple DW images acquired in different gradient directions. Experimental results on cardiac dMRI data demonstrated that our method can significantly improve the quality of diffusion tensor imaging (DTI) reconstruction.  © 2020 IEEE.","Diffusion; Economic and social effects; Image acquisition; Image enhancement; Magnetic resonance imaging; Tensors; Adversarial networks; Angular resolution; Characterization of fibers; Diffusion gradients; Diffusion magnetic resonance imaging; Gradient direction; Indispensable tools; Sequence of images; Diffusion tensor imaging","cardiac DTI; deep learning; diffusion MRI; image synthesis; spatial-angular information; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85100272190"
"Yoon K.","Yoon, Kwangjin (37076240000)","37076240000","Simple and Efficient Unpaired Real-world Super-Resolution using Image Statistics","2021","Proceedings of the IEEE International Conference on Computer Vision","2021-October","","","1983","1990","7","10.1109/ICCVW54120.2021.00225","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123054318&doi=10.1109%2fICCVW54120.2021.00225&partnerID=40&md5=2e8137e7f084f54255d838c0e5cd812c","Learning super-resolution (SR) network without the paired low resolution (LR) and high resolution (HR) image is difficult because direct supervision through the corresponding HR counterpart is unavailable. Recently, many real-world SR researches take advantage of the unpaired image-to-image translation technique. That is, they used two or more generative adversarial networks (GANs), each of which translates images from one domain to another domain, e.g., translates images from the HR domain to the LR domain. However, it is not easy to stably learn such a translation with GANs using unpaired data. In this study, we present a simple and efficient method of training of real-world SR network. To stably train the network, we use statistics of an image patch, such as means and variances. Our real-world SR framework consists of two GANs, one for translating HR images to LR images (degradation task) and the other for translating LR to HR (SR task). We argue that the unpaired image translation using GANs can be learned efficiently with our proposed data sampling strategy, namely, variance matching. We test our method on the NTIRE 2020 real-world SR dataset. Our method outperforms the current state-of-the-art method in terms of the SSIM metric as well as produces comparable results on the LPIPS metric.  © 2021 IEEE.","Computer vision; Generative adversarial networks; Statistical tests; High resolution; High-resolution images; Image statistics; Image translation; Learn+; Low-high; Lower resolution; Real-world; Simple++; Superresolution; Optical resolving power","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123054318"
"Chan K.C.K.; Wang X.; Xu X.; Gu J.; Loy C.C.","Chan, Kelvin C.K. (57216374468); Wang, Xintao (57195942631); Xu, Xiangyu (57195934971); Gu, Jinwei (55661597500); Loy, Chen Change (25522308800)","57216374468; 57195942631; 57195934971; 55661597500; 25522308800","Glean: Generative latent bank for large-factor image super-resolution","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","14240","14249","9","10.1109/CVPR46437.2021.01402","54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123193414&doi=10.1109%2fCVPR46437.2021.01402&partnerID=40&md5=1ba070cbb64b588d97174d2359a8d879","We show that pre-trained Generative Adversarial Networks (GANs), e.g., StyleGAN, can be used as a latent bank to improve the restoration quality of large-factor image super-resolution (SR). While most existing SR approaches attempt to generate realistic textures through learning with adversarial loss, our method, Generative LatEnt bANk (GLEAN), goes beyond existing practices by directly leveraging rich and diverse priors encapsulated in a pre-trained GAN. But unlike prevalent GAN inversion methods that require expensive image-specific optimization at runtime, our approach only needs a single forward pass to generate the upscaled image. GLEAN can be easily incorporated in a simple encoder-bank-decoder architecture with multi-resolution skip connections. Switching the bank allows the method to deal with images from diverse categories, e.g., cat, building, human face, and car. Images upscaled by GLEAN show clear improvements in terms of fidelity and texture faithfulness in comparison to existing methods as shown in Fig. 1. © 2021 IEEE","Computer vision; Image enhancement; Optical resolving power; Textures; Decoder architecture; Human faces; Image super resolutions; Inversion methods; Network inversion; Optimisations; Restoration quality; Runtimes; Simple++; Superresolution; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123193414"
"Moran M.B.H.; Faria M.D.B.; Giraldi G.A.; Bastos L.F.; Conci A.","Moran, Maira B.H. (57203835713); Faria, Marcelo D.B. (57209275928); Giraldi, Gilson A. (6601977723); Bastos, Luciana F. (57216344803); Conci, Aura (6701460323)","57203835713; 57209275928; 6601977723; 57216344803; 6701460323","Using super-resolution generative adversarial network models and transfer learning to obtain high resolution digital periapical radiographs","2021","Computers in Biology and Medicine","129","","104139","","","","10.1016/j.compbiomed.2020.104139","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097643145&doi=10.1016%2fj.compbiomed.2020.104139&partnerID=40&md5=88b3c45cb4bbdf7a4644388f4aa0bb7f","Periapical Radiographs are commonly used to detect several anomalies, like caries, periodontal, and periapical diseases. Even considering that digital imaging systems used nowadays tend to provide high-quality images, external factors, or even system limitations can result in a vast amount of radiographic images with low quality and resolution. Commercial solutions offer tools based on interpolation methods to increase image resolution. However, previous literature shows that these methods may create undesirable effects in the images affecting the diagnosis accuracy. One alternative is using deep learning-based super-resolution methods to achieve better high-resolution images. Nevertheless, the amount of data for training such models is limited, demanding transfer learning approaches. In this work, we propose the use of super-resolution generative adversarial network (SRGAN) models and transfer learning to achieve periapical images with higher quality and resolution. Moreover, we evaluate the influence of using the transfer learning approach and the datasets selected for it in the final generated images. For that, we performed an experiment comparing the performance of the SRGAN models (with and without transfer learning) with other super-resolution methods. Considering Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Mean Opinion Score (MOS), the results of SRGAN models using transfer learning were better on average. This superiority was also verified statistically using the Wilcoxon paired test. In the visual analysis, the high quality achieved by the SRGAN models, in general, is visible, resulting in more defined edges details and fewer blur effects. © 2020 Elsevier Ltd","Image Processing, Computer-Assisted; Machine Learning; Signal-To-Noise Ratio; Deep learning; Diagnosis; E-learning; Image resolution; Learning systems; Mean square error; Optical resolving power; Quality control; Radiography; Signal to noise ratio; Adversarial networks; Digital imaging system; High resolution image; Interpolation method; Learning-based super-resolution; Peak signal to noise ratio; Structural similarity indices (SSIM); Superresolution methods; Article; convolutional neural network; deep learning; digital radiography; feature learning (machine learning); image enhancement; image quality; image resolution; imaging algorithm; imaging and display; intermethod comparison; mean opinion score; mean square error; peak signal to noise ratio; periapical radiography; periapical tissue; priority journal; qualitative analysis; quantitative analysis; rank sum test; structural similarity index; super resolution convolutional neural network; super resolution generative adversarial network; thorax radiography; tooth radiography; transfer learning (algorithm); transfer of learning; image processing; machine learning; signal noise ratio; Transfer learning","Generative adversarial networks; Image enhancement; Periapical radiography; Super-resolution; Transfer learning","Article","Final","","Scopus","2-s2.0-85097643145"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","3A-2021","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119978450&partnerID=40&md5=5638cb57306673d2cf9481e93263b332","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119978450"
"Gao X.W.; Wen X.; Li D.; Liu W.; Xiong J.; Xu B.; Liu J.; Zhang H.; Liu X.","Gao, Xiaohong W. (36170243000); Wen, Xuesong (14021841000); Li, Dong (16507471600); Liu, Weiping (57193308352); Xiong, Jichun (57201890669); Xu, Bin (57198895238); Liu, Juan (57222018443); Zhang, Heng (57277520000); Liu, Xuefeng (56517944400)","36170243000; 14021841000; 16507471600; 57193308352; 57201890669; 57198895238; 57222018443; 57277520000; 56517944400","Evaluation of GAN Architectures for Visualisation of HPV Viruses from Microscopic Images","2021","Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021","","","","829","833","4","10.1109/ICMLA52953.2021.00137","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125855090&doi=10.1109%2fICMLA52953.2021.00137&partnerID=40&md5=0cdc33be271b7d3f6479e5480ffdbc58","Human papillomavirus (HPV) remains a leading cause of virus-induced cancers and has a typical size of 52 to 55nm in diameter. Hence conventional light microscopy that usually sustains a resolution at sim 100nm per pixel falls short of detecting it. This study explores four state of the art generative adversarial networks (GANs) for visualising HPV. The evaluation is achieved by counting the HPV clusters that are corrected identified as well as drug treated cultured cells, i.e. no HPVs. The average sensitivity and specificity are 78.81%, 76.37%, 76.62% and 84.71% for CycleGAN, Pix2pix, ESRGAN and Pix2pixHD respectively. For ESRGAN, the training takes place by matching pairs between low and high resolution (x4) images. For the other three networks, the translation is performed from original raw images to their coloured maps that have undertaken Gaussian filtering in order to discern HPV clusters visually. Pix2pixHD appears to perform the best. © 2021 IEEE.","Petroleum reservoir evaluation; Viruses; Cultured cell; Cyclegan; Generative adversarial network; Human papilloma virus; Human papilloma virus like particle; Microscopic image; Pix2pixhd; State of the art; Superresolution; Virus-like particles; Generative adversarial networks","CycleGAN; Generative adversarial network (GAN); Human papilloma virus like particles (HPVLPs); Pix2pixHD; Super resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125855090"
"Kim J.-G.; Shim M.; Bae E.; Moon Y.; Choi J.","Kim, Jin-Gyu (57223980788); Shim, Myounghoon (57191886202); Bae, Eunkyung (57211680821); Moon, Youngjin (57215071137); Choi, Jaesoon (55903819300)","57223980788; 57191886202; 57211680821; 57215071137; 55903819300","Patient posture estimation using super-resolution reconstruction of pressure distribution image for pressure ulcer prevention","2021","Journal of Institute of Control, Robotics and Systems","27","5","","342","348","6","10.5302/J.ICROS.2021.21.0024","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106621381&doi=10.5302%2fJ.ICROS.2021.21.0024&partnerID=40&md5=901f1dc75f5de33c5c074c2095578d8e","In this study, to improve the prediction of pressure ulcer spots, we have developed super-resolution (SR) techniques to reconstruct a high-resolution (HR) pressure image from a low-resolution (LR) body pressure image to overcome the limitations of sensor resolution. We implemented a super-resolution generative adversarial network (SRGAN) to reconstruct pressure images and a convolution neural network (CNN) to predict posture. To evaluate the similarity between the original pressure image and the 4× rescaled LR body pressure image restored using SR technology, we used image quality assessment (IQA) technology, peak signal-to-noise ratio (PSNR), and structural similarity (SSIM). The reconstructed pressure images were classified into four patient postures (supine, right side, left side, and others) with 98.37% accuracy showing the feasibility of practical implementation. © ICROS 2021.","Agricultural robots; Diseases; Image enhancement; Image quality; Optical resolving power; Signal to noise ratio; Adversarial networks; Convolution neural network; Image quality assessment (IQA); Peak signal to noise ratio; Posture estimation; Pressure ulcer preventions; Structural similarity; Super resolution reconstruction; Image reconstruction","Generative adversarial network; Posture detection; Pressure ulcer; Super-resolution","Article","Final","","Scopus","2-s2.0-85106621381"
"Song G.; Nguyen T.-D.; Bum J.; Yi H.; Son C.-H.; Choo H.","Song, Geonhak (57478582800); Nguyen, Tien-Dung (57209167458); Bum, Junghyun (57190274322); Yi, Hwijong (57194603587); Son, Chang-Hwan (7005976985); Choo, Hyunseung (57559407800)","57478582800; 57209167458; 57190274322; 57194603587; 7005976985; 57559407800","Super Resolution with Sparse Gradient-Guided Attention for Suppressing Structural Distortion","2021","Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021","","","","885","890","5","10.1109/ICMLA52953.2021.00146","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125862970&doi=10.1109%2fICMLA52953.2021.00146&partnerID=40&md5=71963f623f89fbd9af56c8c62c545a42","Generative adversarial network (GAN)-based methods recover perceptually pleasant details in super resolution (SR), but they pertain to structural distortions. Recent study alleviates such structural distortions by attaching a gradient branch to the generator. However, this method compromises the perceptual details. In this paper, we propose a sparse gradient-guided attention generative adversarial network (SGAGAN), which incorporates a modified residual-in-residual sparse block (MRRSB) in the gradient branch and gradient-guided self-attention (GSA) to suppress structural distortions. Compared to the most frequently used block in GAN-based SR methods, i.e., residual-in-residual dense block (RRDB), MRRSB reduces computational cost and avoids gradient redundancy. In addition, GSA emphasizes the highly correlated features in the generator by guiding sparse gradient. It captures the semantic information by connecting the global interdependencies of the sparse gradient features in the gradient branch and the features in the SR branch. Experimental results show that SGAGAN relieves the structural distortions and generates more realistic images compared to state-of-the-art SR methods. Qualitative and quantitative evaluations in the ablation study show that combining GSA and MRRSB together has a better perceptual quality than combining self-attention alone. © 2021 IEEE.","Computer vision; Optical resolving power; Semantics; Computational costs; Gradient branch; Gradient feature; Highly-correlated; Network-based; Self-attention; Semantics Information; Structural distortions; Superresolution; Superresolution methods; Generative adversarial networks","Generative Adversarial Network; Gradient Branch; Self-Attention; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85125862970"
"Sathyabama B.; Arunesh A.; SynthiyaVinothini D.; Anupriyadharsini S.; Md. Mansoor Roomi S.","Sathyabama, B. (57220039118); Arunesh, A. (57226305011); SynthiyaVinothini, D. (57226308909); Anupriyadharsini, S. (57226317775); Md. Mansoor Roomi, S. (57218706622)","57220039118; 57226305011; 57226308909; 57226317775; 57218706622","Recognition of Obscure Objects Using Super Resolution-Based Generative Adversarial Networks","2021","Lecture Notes in Electrical Engineering","749 LNEE","","","459","472","13","10.1007/978-981-16-0289-4_34","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111137113&doi=10.1007%2f978-981-16-0289-4_34&partnerID=40&md5=e92989b563c9a506399e36348f70a074","Object recognition has achieved a good progress in computer vision, but still it is a difficult task in case of low-resolution images, because traditional discriminant features in high resolution usually disappear in low resolution. In surveillance systems, the region of interests gets blurred due to the distance between the camera and object and also due to illumination effects. In low-resolution images, objects appear very small and blurred, thus making recognition of those objects tedious. Super resolution for natural images is a classic and difficult problem in image and video processing. But rapid developments in deep learning have recently sparked interest in super resolution of images. In this paper, generative adversarial network (GAN) that has been successfully employed in generating images and realistic textures with fine details has been extended to the application of image super resolution. The paper aims at improving the resolution of the obscure objects to improve the classification accuracy of the system. This is done by detecting obscure objects using RCNN, then improves its resolution using GAN network and finally classifies the improved images using AlexNet. The experiment is conducted using a MSCOCO dataset and collected shoe database in Google COLAB. The super resolved images increase the classification accuracy by 16%. © 2021, Springer Nature Singapore Pte Ltd.","Deep learning; Image classification; Intelligent computing; Learning systems; Object detection; Object recognition; Optical resolving power; Security systems; Textures; Video signal processing; Adversarial networks; Classification accuracy; Illumination effect; Image and video processing; Image super resolutions; Low resolution images; Region of interest; Surveillance systems; Image enhancement","Generative adversarial network (GAN); Region-based convolutional neural network (RCNN); Super resolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85111137113"
"Krishnan K.S.; Krishnan K.S.","Krishnan, Koushik Sivarama (57313435400); Krishnan, Karthik Sivarama (57313640500)","57313435400; 57313640500","SwiftSRGAN - Rethinking Super-Resolution for Efficient and Real-time Inference","2021","2021 International Conference on Intelligent Cybernetics Technology and Applications, ICICyTA 2021","","","","46","51","5","10.1109/ICICyTA53712.2021.9689188","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126632394&doi=10.1109%2fICICyTA53712.2021.9689188&partnerID=40&md5=e70232cda05e63df83f5eb90ee42e4aa","In recent years, there have been several advancements in the task of image super-resolution using the state of the art Deep Learning-based architectures. Many super-resolution-based techniques previously published, require high-end and top-of-the-line Graphics Processing Unit (GPUs) to perform image super-resolution. With the increasing advancements in Deep Learning approaches, neural networks have become more and more compute hungry. We took a step back and, focused on creating a real-time efficient solution. We present an architecture that is faster and smaller in terms of its memory footprint. The proposed architecture uses Depth-wise Separable Convolutions to extract features and, it performs on-par with other super-resolution GANs (Generative Adversarial Networks) while maintaining real-time inference and a low memory footprint. A real-time super-resolution enables streaming high resolution media content even under poor bandwidth conditions. We need a real-time and efficient solution for tasks like cloud gaming and media streaming but also not at the cost of using a high-end top-of-the-line GPU. While maintaining an efficient trade-off between the accuracy and latency, we are able to produce a comparable performance model which is one-eighth (1/8) the size of super-resolution GANs and computes 74 times faster than super-resolution GANs. This significant reduction in inference time enables us to perform super-resolution in real-time. © 2021 IEEE.","Computer graphics; Computer graphics equipment; Computer vision; Convolution; Deep learning; Economic and social effects; Generative adversarial networks; Graphics processing unit; Memory architecture; Network architecture; Program processors; Adversarial loss; Content loss; Depth-wise separable convolution; Image super resolutions; Loss functions; Mobilenet loss; Perceptual loss function; PSNR; SSIM; Swift-SRGAN; Upsampling; Optical resolving power","Adversarial Loss; Content Loss; Depth-wise Separable Convolutions; GANs; Image Super-Resolution; MobileNet Loss; Perceptual Loss Function; PSNR; SSIM; Swift-SRGAN; up-sampling","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126632394"
"Roslan Z.; Long Z.A.; Ismail R.","Roslan, Zhafri (57203641140); Long, Zalizah Awang (56580430100); Ismail, Roslan (57195320416)","57203641140; 56580430100; 57195320416","Individual Tree Crown Detection using GAN and RetinaNet on Tropical Forest","2021","Proceedings of the 2021 15th International Conference on Ubiquitous Information Management and Communication, IMCOM 2021","","","9377360","","","","10.1109/IMCOM51814.2021.9377360","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103739841&doi=10.1109%2fIMCOM51814.2021.9377360&partnerID=40&md5=4663e56cc2a2d7bb547497510de9d8a5","The detection performance of tree crowns in forest environment has not been satisfactory compared to common objects, especially using aerial RGB imagery. Previous methods regarding Individual Tree Crown Detection (ITCD) utilizes different data sources to improve the detection rate due to the noisy image. Image enhancement methods such as super-resolution provide a solution to the noisy image by reconstructing the image using the low-resolution image. Generative Adversarial Network (GAN)-based model has shown success in super-resolution techniques. However, the GAN-based model created artefacts that may hinder the accuracy of the detection. In this paper, a noise-cancelling GAN-based model is proposed by averaging the weights of a compressed image and non-compressed image. The proposed method forces the network to discriminate the noise to generate a more photorealistic image. This method is inspired by super-resolution GAN (SRGAN) architecture with Residual Dense Network as the generator network. A two-stage object detection RetinaNet model is then used to detect the individual tree crowns in a sequential fashion. Extensive experiments have been conducted on a self-assembled tree crown dataset which showed the proposed model is more superior than a non-enhanced model with 0.6017 and 0.5908 respectively. Based on the results of the proposed method, the super-resolution technique can be used in conjunction with object detection algorithm to improve the detection in ITCD to improve the detection rate. © 2021 IEEE.","Antennas; Image enhancement; Information management; Object detection; Object recognition; Optical resolving power; Tropics; Adversarial networks; Compressed images; Detection performance; Forest environments; Individual tree crown; Low resolution images; Object detection algorithms; Photorealistic images; Forestry","aerial imagery; noise-cancelling; RetinaNet; super-resolution; tree crown detection","Conference paper","Final","","Scopus","2-s2.0-85103739841"
"","","","15th Conference on Image and Graphics Technology and Applications, IGTA 2020","2021","Communications in Computer and Information Science","1314 CCIS","","","","","324","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107336660&partnerID=40&md5=732561f30fcff4a8e043e84759d04152","The proceedings contain 24 papers. The special focus in this conference is on Image and Graphics Technology and Applications. The topics include: Infrared Small Target Recognition with Improved Particle Filtering Based on Feature Fusion; single Image Super-Resolution Based on Generative Adversarial Networks; Simplifying Sketches with Conditional GAN; Improved Method of Target Tracking Based on SiamRPN; An Improved Target Tracking Method Based on DIMP; target Recognition Framework and Learning Mode Based on Parallel Images; view Consistent 3D Face Reconstruction Using Siamese Encoder-Decoders; an Angle-Based Smoothing Method for Triangular and Tetrahedral Meshes; AUIF: An Adaptive User Interface Framework for Multiple Devices; A Striping Removal Method Based on Spectral Correlation in MODIS Data; deep Attention Network for Remote Sensing Scene Classification; thin Cloud Removal Using Cirrus Spectral Property for Remote Sensing Images; accurate Estimation of Motion Blur Kernel Based on Genetic Algorithms; graph Embedding Discriminant Analysis and Semi-Supervised Extension for Face Recognition; 3D Human Body Reconstruction from a Single Image; abnormal Crowd Behavior Detection Based on Movement Trajectory; image Recognition Method of Defective Button Battery Base on Improved MobileNetV1; preface; control and on-Board Calibration Method for in-Situ Detection Using the Visible and Near-Infrared Imaging Spectrometer on the Yutu-2 Rover; full Convolutional Color Constancy with Attention; fast and Accurate Face Alignment Algorithm Based on Deep Knowledge Distillation; multi-modal 3-D Medical Image Fusion Based on Tensor Robust Principal Component Analysis.","","","Conference review","Final","","Scopus","2-s2.0-85107336660"
"Kang J.K.; Lee M.B.; Yoon H.S.; Park K.R.","Kang, Jin Kyu (57202771289); Lee, Min Beom (57194500507); Yoon, Hyo Sik (57191035819); Park, Kang Ryoung (8983316300)","57202771289; 57194500507; 57191035819; 8983316300","AS-RIG: Adaptive selection of reconstructed input by generator or interpolation for person re-identification in cross-modality visible and thermal images","2021","IEEE Access","9","","9324824","12055","12066","11","10.1109/ACCESS.2021.3051637","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723352&doi=10.1109%2fACCESS.2021.3051637&partnerID=40&md5=de1d80c99855a8a1ba7153d8b9d92fcc","Multimodal camera-based person re-identification (ReID) is important in the field of intelligent surveillance. Thermal cameras can solve the problem in that visible-light cameras cannot acquire the valid feature information of a person under poor illumination conditions. However, thermal cameras usually have lower frame resolution than visible-light cameras. To overcome this problem, we propose an adaptive selection of reconstructed input by generator or interpolation (AS-RIG) method, which can adaptively select the generative adversarial network (GAN), or an interpolation method (bi-linear or bi-cubic). AS-RIG automatically selects a resolution-model using the mean-squared error (MSE), feature distance (FD), and structural similarity (SSIM). To verify the performance of our proposed method, two open databases are used: the DBPerson-Recog-DB1 and Sun Yat-set University multiple modality Re-ID (SYSU-MM01). Infrared frames from both databases are resized to be smaller than the original ones for experimentation. Experimental results show that our generator outperforms traditional interpolation methods. In addition, the person ReID experimental results demonstrate that AS-RIG outperforms non-adaptive selection methods and state-of-the-art methods. © 2013 IEEE.","Cameras; Image reconstruction; Infrared devices; Interpolation; Light; Mean square error; Adversarial networks; Feature information; Illumination conditions; Intelligent surveillance; Interpolation method; Person re identifications; State-of-the-art methods; Structural similarity; Security systems","convolutional neural network (CNN); GAN; Person Re-ID; super-resolution (SR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099723352"
"Mu J.; Cao S.; Sheng Y.; Zhou Y.; Liu Z.","Mu, Jinzhen (57219133143); Cao, Shuqing (56724966900); Sheng, Yanping (57572596500); Zhou, Yan (57188815143); Liu, Zongming (55925248600)","57219133143; 56724966900; 57572596500; 57188815143; 55925248600","Detection and Mapping of an Uncooperative Spinning Target under Low-light Illumination Condition","2021","Proceeding - 2021 China Automation Congress, CAC 2021","","","","3885","3890","5","10.1109/CAC53003.2021.9727717","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128027983&doi=10.1109%2fCAC53003.2021.9727717&partnerID=40&md5=b135b12ad9d409335ade74cf5dded7ad","This paper investigates the simultaneous detection and mapping problem for inspecting an unknown and uncooperative target that is spinning in space. First, we apply a new unsupervised generative adversarial network (GAN) to enhance the low contrast and poor visibility images which are captured in low-light space illumination conditions. Second, due to the captured low-resolution (LR) images of the target contain small size of key-components, so that we propose a new small object detection network that combines a GAN-based super-resolution (SR) network and a FRCNN-based detection network to locate these objects. The SR network was used to reconstruct super-resolved images from the original LR images. Third, we utilize a SLAM-based algorithm to map and estimate the pose of the spinning target based on previous image enhancement. In summary, the integrated architecture has three components: a low-light enhancement GAN, a small object detection network, and a real-time SLAM system. The experimental results show that the integrated architecture achieves better visual quality and improves the awareness of an uncooperative spinning target. © 2021 IEEE","Generative adversarial networks; Mapping; Network architecture; Object detection; Object recognition; Optical resolving power; Detection networks; Illumination conditions; Low light; Low resolution images; Low-light image enhancement; Low-light images; Non-cooperative; Non-cooperative spinning target; Pose determination; Superresolution; Image enhancement","Generative Adversarial Networks; Low-light Image Enhancement; Non-cooperative spinning target; Pose determination; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85128027983"
"Li J.; Bare B.; Zhou S.; Yan B.; Li K.","Li, Jichun (57210590086); Bare, Bahetiyaer (56433101200); Zhou, Shili (57219594336); Yan, Bo (36764113400); Li, Ke (55322485200)","57210590086; 56433101200; 57219594336; 36764113400; 55322485200","ORGAN-BRANCHED CNN FOR ROBUST FACE SUPER-RESOLUTION","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428152","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126483418&doi=10.1109%2fICME51207.2021.9428152&partnerID=40&md5=3e6c7c645839206215dbbe9a5314716a","In this paper, we present a novel organ-branched CNN method for face super-resolution, named OBC-FSR. It is the first work focusing on facial-part-specific face SR, which consists of a local (facial part) network and a global network. Specifically, local network enhances the five key regions of human faces separately by Wasserstein generative adversarial networks (WGAN). Simultaneously, it also predicts five key regions' masks, namely, eyes, eyebrows, mouth, nose, and other parts. The output of the local network is obtained by merging super-resolved five key regions. In order to alleviate boundary effects and distortions in the result of local network, our proposed network also includes a global network, which learns the direct mapping between LR and HR human faces. The final HR result of our FSR method is a fusion of the outputs of local and global networks. Experimental results verify the superior performance of our method compared to the state-of-the-art. © 2021 IEEE","Computer vision; Optical resolving power; Boundary effects; Face key region; Face super-resolution; Facial parts; Global networks; Learn+; Local networks; Organ-branched-CNN; Region of human faces; Wasserstein generative adversarial network; Generative adversarial networks","Face key regions; Face super-resolution; Organ-branched-CNN; WGAN","Conference paper","Final","","Scopus","2-s2.0-85126483418"
"Hou X.; Liu T.; Wang S.; Zhang L.","Hou, Xuzhe (57543729700); Liu, Tong (57743623100); Wang, Shuaijie (57544789300); Zhang, Linmin (57544433700)","57543729700; 57743623100; 57544789300; 57544433700","Image Quality Improve by Super Resolution Generative Adversarial Networks","2021","Proceedings - 2021 2nd International Conference on Intelligent Computing and Human-Computer Interaction, ICHCI 2021","","","","117","121","4","10.1109/ICHCI54629.2021.00031","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126882662&doi=10.1109%2fICHCI54629.2021.00031&partnerID=40&md5=7e9c637d84215a0cfcd28eaa78fe0c87","In this research, we analyze the methods to improve the quality of the pictures. Machine learning technique is used to achieve the effect of converting low-pixeled pictures into high pixeled ones. It is convenient and can be used in many circumstances, such as engineering projects and medical surveys. This study uses the SRGAN to train the model, which has the generator network and the discriminator network. The generator network is used to generate high-resolution images, and the discriminator network is used to judge the authenticity of the image generated. Our network has trained the network with 8× upscaling factors and eventually obtains predominant achievement for dealing with detailed textures like trees, cars, and animal fur. Our network can recover low-quality pictures perpetual satisfying, and the loss of this model is low. Our research provides model-based strategic support for image quality improvement and gets a good picture resolution increase in the tests. © 2021 IEEE.","Discriminators; Image enhancement; Image quality; Optical resolving power; Textures; Animal fur; Engineering program; High-resolution images; Image super resolutions; Images processing; Low qualities; Machine learning techniques; Super resolution generative adversarial network; Superresolution; Upscaling; Generative adversarial networks","Image Processing; Image Super Resolution; Super Resolution Generative Adversarial Networks (SRGAN)","Conference paper","Final","","Scopus","2-s2.0-85126882662"
"Cheng J.; Jiang N.; Tang J.; Deng X.; Yu W.; Zhang P.","Cheng, Jiarui (57388629900); Jiang, Ning (57212426361); Tang, Jialiang (57212377052); Deng, Xin (57220164716); Yu, Wenxin (36610960300); Zhang, Peng (57750992600)","57388629900; 57212426361; 57212377052; 57220164716; 36610960300; 57750992600","Using a Two-Stage GAN to Learn Image Degradation for Image Super-Resolution","2021","Communications in Computer and Information Science","1516 CCIS","","","222","230","8","10.1007/978-3-030-92307-5_26","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121901266&doi=10.1007%2f978-3-030-92307-5_26&partnerID=40&md5=4f1f644981e18e9c4055d95e61071316","Recent super-resolution (SR) methods based on generative adversarial networks (GANs) almost assume that the degradation process is known. Most of these works are to use bicubic or bilinear down-sampling to obtain low-resolution (LR) images, but for real-world images, these methods often can not recover the details well. Thus affecting the performance. In this paper, we propose to first build a self-attention gradient degradation GAN, then build a self-attention gradient super-resolution GAN to alleviate the above problem. Specifically, first of all, we learn the down-sampling process by self-attention gradient degradation GAN to approximate the real-world degradation of high-resolution (HR) images, and use unpaired HR and LR images in the training process. Then, we get the LR images by self-attention gradient degradation GAN, and send them into the self-attention gradient super-resolution GAN together with the corresponding original HR images to get the SR images. The experimental results show that our method is superior to other state-of-the-art methods in terms of FID and we get competitive results on PSNR. It also potentially means that our method can be used for other categories of images. © 2021, Springer Nature Switzerland AG.","Optical resolving power; Signal sampling; Attention gradients; Down sampling; Generative adversarial network; High-resolution images; Image degradation; Image super resolutions; Learn+; Low resolution images; Superresolution; Superresolution methods; Generative adversarial networks","Generative adversarial network (GAN); Image degradation; Image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85121901266"
"Chen W.; Ma Y.; Liu X.; Yuan Y.","Chen, Weimin (57207768741); Ma, Yuqing (57197842290); Liu, Xianglong (36100195100); Yuan, Yi (57688124000)","57207768741; 57197842290; 36100195100; 57688124000","Hierarchical generative adversarial networks for single image super-resolution","2021","Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021","","","","355","364","9","10.1109/WACV48630.2021.00040","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116168238&doi=10.1109%2fWACV48630.2021.00040&partnerID=40&md5=2507d2406dea05ed52650c410a7e001e","Recently, deep convolutional neural network (CNN) have achieved promising performance for single image super-resolution (SISR). However, they usually extract features on a single scale and lack sufficient supervision information, leading to undesired artifacts and unpleasant noise in super-resolution (SR) images. To address this problem, we first propose a hierarchical feature extraction module (HFEM) to extract the features in multiple scales, which helps concentrate on both local textures and global semantics. Then, a hierarchical guided reconstruction module (HGRM) is introduced to reconstruct more natural structural textures in SR images via intermediate supervisions in a progressive manner. Finally, we integrate HFEM and HGRM in a simple yet efficient end-to-end framework named hierarchical generative adversarial networks (HSR-GAN) to recover consistent details, and thus obtain the semantically reasonable and visually realistic results. Extensive experiments on five common datasets demonstrate that our method shows favorable visual quality and superior quantitative performance compared to state-of-the-art methods for SISR. © 2021 IEEE.","Computer vision; Convolutional neural networks; Deep neural networks; Optical resolving power; Semantics; Textures; Hierarchical feature extraction; Image super resolutions; Local Texture; Multiple scale; Performance; Resolution images; Simple++; Single images; Structural textures; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85116168238"
"Ikuta M.; Zhang J.","Ikuta, Masaki (57219794190); Zhang, Jun (56013488600)","57219794190; 56013488600","TextureWGAN: Texture preserving WGAN with MLE regularizer for inverse problems","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11596","","1159618","","","","10.1117/12.2580434","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103611093&doi=10.1117%2f12.2580434&partnerID=40&md5=7fcf1aad54ce410bef02d1bd4e79273b","Many algorithms and methods have been proposed for inverse problems particularly with the recent surge of interest in machine learning and deep learning methods. Among all proposed methods, the most popular and effective method is the convolutional neural network (CNN) with mean square error (MSE). This method has been proven effective in super-resolution, image de-noising, and image reconstruction. However, this method is known to over-smooth images due to the nature of MSE. MSE based methods minimize Euclidean distance for all pixels between a baseline image and a generated image by CNN and ignore the spatial information of the pixels such as image texture. In this paper, we proposed a new method based on Wasserstein GAN (WGAN) for inverse problems. We showed that the WGAN-based method was effective to preserve image texture. It also used a maximum likelihood estimation (MLE) regularizer to preserve pixel fidelity. Maintaining image texture and pixel fidelity is the most important requirement for medical imaging. We used Peak Signal to Noise Ratio (PSNR) and Structure Similarity (SSIM) to evaluate the proposed method quantitatively. We also conducted the first-order and the second-order statistical image texture analysis to assess image texture.  © 2021 SPIE.","Convolutional neural networks; Deep learning; Differential equations; Image denoising; Image reconstruction; Image segmentation; Inverse problems; Learning systems; Maximum likelihood estimation; Mean square error; Medical image processing; Pixels; Signal to noise ratio; Textures; Euclidean distance; Learning methods; Peak signal to noise ratio; Spatial informations; Statistical images; Structure similarity; Super resolution; Texture preserving; Image texture","Convolutional Neural Network (CNN); Generative Adversarial Networks (GANs); Inverse problems; Maximum Likelihood Estimation (MLE); Statistical texture analysis; Wasserstein distance","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85103611093"
"Batchuluun G.; Kang J.K.; Nguyen D.T.; Pham T.D.; Arsalan M.; Park K.R.","Batchuluun, Ganbayar (57188649020); Kang, Jin Kyu (57202771289); Nguyen, Dat Tien (35608738000); Pham, Tuyen Danh (55808639500); Arsalan, Muhammad (57203178070); Park, Kang Ryoung (8983316300)","57188649020; 57202771289; 35608738000; 55808639500; 57203178070; 8983316300","Deep Learning-Based Thermal Image Reconstruction and Object Detection","2021","IEEE Access","9","","9311732","5951","5971","20","10.1109/ACCESS.2020.3048437","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099111790&doi=10.1109%2fACCESS.2020.3048437&partnerID=40&md5=1e6a3f60850e23d81a704400df480ff9","Recently, thermal cameras are being widely used in various fields, such as intelligent surveillance, biometrics, and health monitoring. However, the high cost of the thermal cameras poses a challenge in terms of purchase. Additionally, thermal images have an issue pertaining to blurring caused by object movement, camera movement, and camera focus settings. There have been very few studies on image restoration centered around thermal images to address such problems. Moreover, it is important to increase the processing speed of image restoration methods to jointly conduct with methods such as action recognition and object tracking that use temporal information from thermal videos. However, no study has been conducted on simultaneously performing super-resolution reconstruction and deblurring using thermal images. Furthermore, existing studies on object detection using thermal images have errors owing to the incapability in distinguishing reflections on the surrounding ground or wall due to the heat radiated from the object. To address such issues, this study proposes a deep learning-based thermal image restoration method that simultaneously performs super-resolution reconstruction and deblurring. According to recent development of deep learning, generative adversarial network (GAN)-based methods which have ability to preserve texture details in images, and yield sharper and more plausible textures than classical feed forward encoders show success in image-to-image translation tasks. Considering the advantages of GAN, we propose a deblur-SRRGAN for thermal image reconstruction. In addition, we propose a light-weighted Mask R-CNN for object detection in the reconstructed thermal image. For the input, we employ an image processing method that converts 1-channel thermal images (often used in the existing studies) into 3-channel images. The results of the experiments conducted using self-collected databases and an open database demonstrate that our method outperforms the state-of-the-art methods. © 2013 IEEE.","Cameras; Deep learning; Image enhancement; Infrared devices; Learning systems; Object detection; Object recognition; Object tracking; Optical resolving power; Processing; Restoration; Security systems; Textures; Action recognition; Adversarial networks; Image processing - methods; Intelligent surveillance; Restoration methods; State-of-the-art methods; Super resolution reconstruction; Temporal information; Image reconstruction","deep learning; image deblurring; object and thermal reflection detection; super-resolution reconstruction; Thermal image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099111790"
"Xiong F.; Liu J.; Zhao M.; Yao M.; Guo R.","Xiong, Fang (57223438103); Liu, Jian (55836908900); Zhao, Min (57191699245); Yao, Min (35276619200); Guo, Ruipeng (57218922008)","57223438103; 55836908900; 57191699245; 35276619200; 57218922008","Positron Image Super-Resolution Using Generative Adversarial Networks","2021","IEEE Access","9","","9527232","121329","121343","14","10.1109/ACCESS.2021.3109634","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117602220&doi=10.1109%2fACCESS.2021.3109634&partnerID=40&md5=ae5a6a34da6ff3e76720784f35de31e0","Positron images generated by positron non-destructive testing technology under rapid detection scenes such as low concentration dose, low exposure time and short imaging time, which have some problems like low-resolution and poor definition. These issues cannot be solved for the being time. To solves these problems, this research super-resolves the low-resolution positron images to generate images with high-resolution and clear details. To make the generated super-resolution images more capable of restoring the features of low- resolution images, this research proposed a positron image super-resolution reconstruction method based on generative adversarial networks. In order to improve the input information utilization rate, long skip connections were added into the generator. In addition, the discriminant model, where composed of an image discriminator and a feature discriminator, can stimulate the generator to generate clearer super-resolution images which contain more details. In attempting to solve the problem of dataset matching, a special positron image super-resolution dataset is constructed for network application scenarios. In the adversarial training stage, perceptual similarity loss and adversarial loss are used to replace the traditional mean squared error loss to improve the images perception quality. Experimental results show that the proposed model can reconstruct low-resolution images by four times super-resolution in 0.16 seconds. The super-resolution images obtained are superior to other algorithms in visual effect, which have clearer detail structure and higher objective performance values. Hence this model can meet the requirements of rapid non-destructive testing of industrial parts.  © 2013 IEEE.","Deep learning; Electrons; Image enhancement; Image reconstruction; Mean square error; Nondestructive examination; Optical resolving power; Deep learning; Image super resolutions; Low concentrations; Low resolution images; Lower resolution; Positron image; Rapid detection; Resolution images; Super-resolution reconstruction; Superresolution; Generative adversarial networks","deep learning; generative adversarial networks; positron image; Super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117602220"
"Hanano T.; Seo M.; Chen Y.-W.","Hanano, Tatsuya (57426895600); Seo, Masataka (35280835900); Chen, Yen-Wei (56036268200)","57426895600; 35280835900; 56036268200","Automatic Generation of High-Resolution Facial Expression Images with End-to-End Models Using Pix2Pix and Super-Resolution Convolutional Neural Network","2021","2021 IEEE 10th Global Conference on Consumer Electronics, GCCE 2021","","","","798","801","3","10.1109/GCCE53005.2021.9622042","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123493079&doi=10.1109%2fGCCE53005.2021.9622042&partnerID=40&md5=ce5d425a82ec6c2977d1fba8c93cd779","Recently, the means to see human face images have increased owing to the spread of smartphones and social networking services. Therefore, research on facial image generation, such as facial expression transformation, has been actively conducted. Especially, in the field of face images, the generation of face images using facial expression transformation has already been realized using generative adversarial networks (i.e., pix2pix). However, in the conventional models, only low-resolution images can be generated owing to limited computational resources, and the generated images are blur or aliasing. To solve this problem, we improved the resolution of generated images by training the Pix2Pix and super-resolution convolutional neural network methods as one model end-to-end instead of training them separately. Using the peak signal-to-noise ratio as an evaluation index, image quality was improved by 0.391 dB compared with the conventional model. © 2021 IEEE.","Convolution; Convolutional neural networks; Image enhancement; Optical resolving power; Signal to noise ratio; Adversarial networks; Conventional modeling; Convolutional neural network; End to end; Face images; Facial Expressions; Image translation; Image-to-image translation with conditional adversarial network; Super-resolution convolutional neural network; Superresolution; Generative adversarial networks","end-to-end; Generative Adversarial Nets; Image-to-Image Translation with Conditional Adversarial Networks; Super-Resolution Convolutional Neural Network","Conference paper","Final","","Scopus","2-s2.0-85123493079"
"Shahidi F.","Shahidi, Faezehsadat (57215214079)","57215214079","Breast Cancer Histopathology Image Super-Resolution Using Wide-Attention GAN with Improved Wasserstein Gradient Penalty and Perceptual Loss","2021","IEEE Access","9","","2476","32795","32809","14","10.1109/ACCESS.2021.3057497","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101444253&doi=10.1109%2fACCESS.2021.3057497&partnerID=40&md5=6f29a3878bc3bfe2951ab6513f7ba763","In the realm of image processing, enhancing the quality of the images is known as a super-resolution problem (SR). Among SR methods, a super-resolution generative adversarial network, or SRGAN, has been introduced to generate SR images from low-resolution images. As it is of the utmost importance to keep the size and the shape of the images, while enlarging the medical images, we propose a novel super-resolution model with a generative adversarial network to generate SR images with finer details and higher quality to encourage less blurring. By widening residual blocks and using a self-attention layer, our model becomes robust and generalizable as it is able to extract the most important part of the images before up-sampling. We named our proposed model as wide-attention SRGAN (WA-SRGAN). Moreover, we have applied improved Wasserstein with a Gradient penalty to stabilize the model while training. To train our model, we have applied images from Camylon 16 database and enlarged them by 2×, 4×, 8×, and 16× upscale factors with the ground truth of the size of 256× 256× 3. Furthermore, two normalization methods, including batch normalization, and weight normalization have been applied and we observed that weight normalization is an enabling factor to improve metric performance in terms of SSIM. Moreover, several evaluation metrics, such as PSNR, MSE, SSIM, MS-SSIM, and QILV have been applied for having a comprehensive objective comparison with other methods, including SRGAN, A-SRGAN, and bicubial. Also, we performed the job of classification by using a deep learning model called ResNeXt-101 (32 × 8d) for super-resolution, high-resolution, and low-resolution images and compared the outcomes in terms of accuracy score. Finally, the results on breast cancer histopathology images show the superiority of our model by using weight normalization and a batch size of one in terms of restoration of the color and the texture details.  © 2013 IEEE.","Deep learning; Diseases; Image resolution; Importance sampling; Medical imaging; Optical resolving power; Signal receivers; Textures; Adversarial networks; Evaluation metrics; High resolution; Image super resolutions; Low resolution images; Normalization methods; Super resolution; Super-resolution models; Image enhancement","breast cancer histopathology medical images; classification; perceptual loss; SRGAN; Wasserstein gradient penalty; weight and batch normalization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85101444253"
"Chen X.; Jia C.","Chen, Xin (57225125522); Jia, Caiyan (8919313200)","57225125522; 8919313200","An Overview of Image-to-Image Translation Using Generative Adversarial Networks","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12666 LNCS","","","366","380","14","10.1007/978-3-030-68780-9_31","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103293493&doi=10.1007%2f978-3-030-68780-9_31&partnerID=40&md5=70708edcf92f6da1cb25d4e5fa13a1a8","Image-to-image translation is an important and challenging problem in computer vision. It aims to learn the mapping between two different domains, with applications ranging from data augmentation, style transfer, to super-resolution, etc. With the success of deep learning methods in visual generative tasks, researchers have applied deep generative models, especially generative adversarial networks (GANs), to image-to-image translation since the year of 2016 and gained fruitful progress. In this survey, we have conducted a comprehensive review of the literature in this field, covering supervised and unsupervised methods, among which unsupervised approaches include one-to-one, one-to-many, many-to-many categories and some latest theories. We highlight the innovation aspect of these methods and analyze different models employed and their components. Besides, we summarized some commonly used normalization techniques and evaluation metrics, and finally, present several challenges and future research directions in this area. © 2021, Springer Nature Switzerland AG.","Deep learning; Pattern recognition; Adversarial networks; Data augmentation; Different domains; Evaluation metrics; Future research directions; Image translation; Unsupervised approaches; Unsupervised method; Learning systems","Deep learning; Evaluation metrics; Generative adversarial networks; Image-to-image translation; Normalization","Conference paper","Final","","Scopus","2-s2.0-85103293493"
"Cherian A.K.; Poovammal E.","Cherian, Aswathy K. (57216656961); Poovammal, E. (26639841900)","57216656961; 26639841900","A novel alphasrgan for underwater image super resolution","2021","Computers, Materials and Continua","69","2","","1537","1552","15","10.32604/cmc.2021.018213","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110494580&doi=10.32604%2fcmc.2021.018213&partnerID=40&md5=85b95203e0e0a3aa484021b7f07e4d56","Obtaining clear images of underwater scenes with descriptive details is an arduous task. Conventional imaging techniques fail to provide clear cut features and attributes that ultimately result in object recognition errors. Consequently, a need for a system that produces clear images for underwater image study has been necessitated. To overcome problems in resolution and to make better use of the Super-Resolution (SR) method, this paper introduces a novel method that has been derived from the Alpha Generative Adversarial Network (AlphaGAN) model, named Alpha Super Resolution Generative Adversarial Network (AlphaSRGAN). The model put forth in this paper helps in enhancing the quality of underwater imagery and yields images with greater resolution and more concise details. Images undergo pre-processing before they are fed into a generator network that optimizes and reforms the structure of the network while enhancing the stability of the network that acts as the generator.After the images are processed by the generator network, they are passed through an adversarial method for training models. The dataset used in this paper to learn Single Image Super Resolution (SISR) is the USR 248 dataset. Training supervision is performed by an unprejudiced function that simultaneously scrutinizes and improves the image quality. Appraisal of images is done with reference to factors like local style information, global content and color. The dataset USR 248 which has a huge collection of images has been used for the study is composed of three collections of images-high (640×480) and low (80×60, 160×120, and 320×240). Paired instances of different sizes-2×, 4× and 8×-are also present in the dataset. Parameters likeMean Opinion Score (MOS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) and Underwater Image Quality Measure (UIQM) scores have been compared to validate the improved efficiency of our model when compared to existing works. © 2021 Tech Science Press. All rights reserved.","Image quality; Imaging techniques; Object recognition; Optical resolving power; Signal to noise ratio; Underwater acoustic communication; Adversarial networks; Conventional imaging; Global contents; Image super resolutions; Peak signal to noise ratio; Structural similarity; Super resolution; Underwater scenes; Image enhancement","Generative adversarial network; Image super resolution; Perceptual quality; Single image super-resolution; Underwater imagery","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110494580"
"","","","Proceedings of the 2021 36th International Conference on Image and Vision Computing New Zealand, IVCNZ 2021","2021","International Conference Image and Vision Computing New Zealand","2021-December","","","","","287","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124411804&partnerID=40&md5=967524bd1be5e7ac984e29a41777d033","The proceedings contain 48 papers. The topics discussed include: virtual sawing using generative adversarial networks; a new coefficient estimation method when using PCA for spectral super-resolution; thermographic identification of hidden corrosion; robust human instance segmentation in a challenging forest environment; predictive state estimation of invasive predators using low resolution thermal cameras; deep convolutional neural networks for detecting dolphin echolocation clicks; perceptual improvements for super-resolution of satellite imagery; and likelihood-free bayesian inference framework for sizing kiwifruit from orchard imaging surveys.","","","Conference review","Final","","Scopus","2-s2.0-85124411804"
"An Z.; Zhang J.; Sheng Z.; Er X.; Lv J.","An, Zeyu (57226176575); Zhang, Junyuan (57226199294); Sheng, Ziyu (57219799861); Er, Xuanhe (57226177574); Lv, Junjie (57226192039)","57226176575; 57226199294; 57219799861; 57226177574; 57226192039","RBDN: Residual Bottleneck Dense Network for Image Super-Resolution","2021","IEEE Access","9","","9481260","103440","103451","11","10.1109/ACCESS.2021.3096548","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110855396&doi=10.1109%2fACCESS.2021.3096548&partnerID=40&md5=5150e3c987be3de38706da56d9c07207","Recent studies have shown that a super-resolution generative adversarial network (SRGAN) can significantly improve the quality of single-image super-resolution. However, existing SRGAN methods also have certain drawbacks, such as an insufficient feature utilization, a large number of parameters. To further enhance the visual quality, we thoroughly studied three key components of SRGAN, i.e., the network architecture, adversarial loss, and perceptual loss, and propose a DenseNet with Residual-in-Residual Bottleneck Block (RRBB), called a residual bottleneck dense network (RBDN), for single-image super-resolution. First, to improve the utilization of features between the various layers of the network, we adopted a dense cascading connection between layers. At the same time, to reduce the computational cost, we added a bottleneck structure to each layer, greatly reducing the number of network parameters and accelerating the convergence speed of the training process. Second, the proposed RRBB, as the basic network building unit, removes the batch normalization (BN) layer and employs the ELU function to reduce the opposite effects in the absence of BN. In addition, we applied an improved overall loss function during the model training process to stably train the model and further improve the realism of the reconstructed high-resolution image. To prove the superiority of our proposed model, we conducted a comprehensive and objective evaluation of the Peak Signal-to-Noise Ratio, structural similarity, learned perceptual image patch similarity, and other evaluation indicators obtained from the three test sets, i.e., Set5, Set14, and BSD100, from the recent state-of-the-art model. Finally, we conducted qualitative and quantitative analyses of the results obtained in terms of the evaluation indicators, the authenticity of the restored HR images, and textural details, which show the superiority of the RBDN model.  © 2013 IEEE.","Function evaluation; Image reconstruction; Network architecture; Network layers; Optical resolving power; Signal to noise ratio; Adversarial networks; Evaluation indicators; High resolution image; Image super resolutions; Objective evaluation; Peak signal to noise ratio; Qualitative and quantitative analysis; Structural similarity; Image enhancement","bottleneck; residual bottleneck dense network; Residual-in-residual bottleneck block; ResNet","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85110855396"
"Chen C.; Gong D.; Wang H.; Li Z.; Wong K.-Y.K.","Chen, Chaofeng (57203595932); Gong, Dihong (55925199600); Wang, Hao (57202272670); Li, Zhifeng (55707159500); Wong, Kwan-Yee K. (57216110773)","57203595932; 55925199600; 57202272670; 55707159500; 57216110773","Learning Spatial Attention for Face Super-Resolution","2021","IEEE Transactions on Image Processing","30","","9293182","1219","1231","12","10.1109/TIP.2020.3043093","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098119837&doi=10.1109%2fTIP.2020.3043093&partnerID=40&md5=9c3ce08ca6482415fd054df2cc94560c","General image super-resolution techniques have difficulties in recovering detailed face structures when applying to low resolution face images. Recent deep learning based methods tailored for face images have achieved improved performance by jointly trained with additional task such as face parsing and landmark prediction. However, multi-task learning requires extra manually labeled data. Besides, most of the existing works can only generate relatively low resolution face images (e.g., $128\times 128$ ), and their applications are therefore limited. In this paper, we introduce a novel SPatial Attention Residual Network (SPARNet) built on our newly proposed Face Attention Units (FAUs) for face super-resolution. Specifically, we introduce a spatial attention mechanism to the vanilla residual blocks. This enables the convolutional layers to adaptively bootstrap features related to the key face structures and pay less attention to those less feature-rich regions. This makes the training more effective and efficient as the key face structures only account for a very small portion of the face image. Visualization of the attention maps shows that our spatial attention network can capture the key face structures well even for very low resolution faces (e.g., $16\times 16$ ). Quantitative comparisons on various kinds of metrics (including PSNR, SSIM, identity similarity, and landmark detection) demonstrate the superiority of our method over current state-of-the-arts. We further extend SPARNet with multi-scale discriminators, named as SPARNetHD, to produce high resolution results (i.e., $512\times 512$ ). We show that SPARNetHD trained with synthetic data can not only produce high quality and high resolution outputs for synthetically degraded face images, but also show good generalization ability to real world low quality face images. Codes are available at https://github.com/chaofengc/Face-SPARNet. © 1992-2012 IEEE.","Deep learning; Learning systems; Multi-task learning; Optical resolving power; Face super-resolution; Generalization ability; High-resolution output; Image super resolutions; Landmark detection; Learning-based methods; Low-resolution face images; Quantitative comparison; article; attention network; bootstrapping; learning; nonhuman; quantitative analysis; Vanilla; Image enhancement","Face super-resolution; generative adversarial networks; spatial attention","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098119837"
"Yang B.; Xie K.; Yang Z.; Yang M.","Yang, Bin (57226054930); Xie, Kai (57212154137); Yang, Zepeng (57217100700); Yang, Mengyao (57222146135)","57226054930; 57212154137; 57217100700; 57222146135","Super-resolution Generative Adversarial Networks Based on Attention Model","2020","2020 IEEE 6th International Conference on Computer and Communications, ICCC 2020","","","9345044","781","786","5","10.1109/ICCC51575.2020.9345044","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101719124&doi=10.1109%2fICCC51575.2020.9345044&partnerID=40&md5=96fdaebd35307a2fe0f3a4669e1369de","The purpose of super-resolution reconstruction is to reconstruct a high-resolution image from one or more low resolution images. In this paper, we propose an improved super-resolution generative adversarial networks based on the attention model. The attention model can be used to extract the important features and suppress the unimportant features, so as to ensure the quality of network reconstruction and optimize the network structure of the generator in the generative adversarial networks (GAN). The experimental results show that on the Set5 datasets, we could use less residual block to train and the spending time of reconstruction is less by using the attention model. That also means the fewer parameters and the less calculation. The images selected on Set14 and BSD100 datasets are reconstructed, and the reconstructed time is also shortened. © 2020 IEEE.","Optical resolving power; Adversarial networks; High resolution image; Important features; Low resolution images; Network reconstruction; Network structures; Super resolution; Super resolution reconstruction; Image reconstruction","attention model; generative adversarial networks; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85101719124"
"","","","11th International Symposium on Parallel Architectures, Algorithms and Programming, PAAP 2020","2021","Communications in Computer and Information Science","1362","","","","","438","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102515545&partnerID=40&md5=f082ca71ae9d9ac9e7e9eba4c9b5a43c","The proceedings contain 37 papers. The special focus in this conference is on Parallel Architectures, Algorithms and Programming. The topics include: Analysing and Forecasting Electricity Demand and Price Using Deep Learning Model During the COVID-19 Pandemic; cross-database Micro Expression Recognition Based on Apex Frame Optical Flow and Multi-head Self-attention; GPS Intelligent Solution of Aerial Image Target in State Grid EIA Survey; Encryption and Decryption in Conic Curves Cryptosystem Over Finite Field GF(2n) Using Tile Self-assembly; optimizing Embedding-Related Quantum Annealing Parameters for Reducing Hardware Bias; a Behavioural Network Traffic Novelty Detection for the Internet of Things Infrastructures; a Fast Algorithm for Image Segmentation Based on Global Cosine Fitting Energy Model; household Garbage Classification: A Transfer Learning Based Method and a Benchmark; lightweight Neural Network Based Garbage Image Classification Using a Deep Mutual Learning; on the Decycling Problem in a Torus; VBSSR: Variable Bitrate Encoded Video Streaming with Super-Resolution on HPC Education Platform; An Investigation on the Performance of Highly Congested Home WiFi Networks During the COVID-19 Pandemic; using Feed-Forward Network for Fast Arbitrary Style Transfer with Contextual Loss; enhancing Underwater Image Using Multi-scale Generative Adversarial Networks; Inferring Prerequisite Relationships Among Learning Resources for HPC Education; research on Bank Knowledge Transaction Coverage Model Based on Innovation Capacity Analysis; deep Deterministic Policy Gradient Based Resource Allocation in Internet of Vehicles; a Pufferfish Privacy Mechanism for the Trajectory Clustering Task; a Novel Attention Model of Deep Learning in Image Classification; FDRA: Fully Distributed Routing Architecture for Private Virtual Network in Public Cloud.","","","Conference review","Final","","Scopus","2-s2.0-85102515545"
"Altakrouri S.; Usman S.B.; Ahmad N.B.; Justinia T.; Noor N.M.","Altakrouri, Saleh (57538714000); Usman, Sahnius Bt (36007713400); Ahmad, Norulhusna Binti (35316795700); Justinia, Taghreed (56201860900); Noor, Norliza Mohd (7003593814)","57538714000; 36007713400; 35316795700; 56201860900; 7003593814","Image to Image Translation Networks using Perceptual Adversarial Loss Function","2021","Proceedings of the 2021 IEEE International Conference on Signal and Image Processing Applications, ICSIPA 2021","","","","","","","10.1109/ICSIPA52582.2021.9576815","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126707464&doi=10.1109%2fICSIPA52582.2021.9576815&partnerID=40&md5=eb0617b8da99b81f71cbeaf1fba87257","Image to image translation based on deep learning models is a subject of immense importance in the disciplines of Artificial Intelligence (AI) and Computer Vision (CV). A variety of traditional tasks such as image colorization, image denoising and image inpainting, are categorized as typical paired image translation tasks. In computer vision, super-resolution regeneration is particularly important field. We proposed an improved algorithm to mitigate the issues that arises during the reconstruction using super resolution based on generative adversarial network. It is difficult to train in reconstruction of results. The generated images and the corresponding ground-truth images should share the same fundamental structure in order to output the required resultant images. The shared basic structure between the input and the corresponding output image is not as optimal as assumed for paired image translation tasks, which can greatly impact the generating model performance. The traditional GAN based model used in image-to-image translation tasks used a pre-trained classification network. The pre-trained networks perform well on the classification tasks compared to image translation tasks because they were trained on features that contribute to better classification. We proposed the perceptual loss based efficient net Generative Adversarial Network (PLE-GAN) for super resolution tasks. Unlike other state of the art image translation models, the PL-E-GAN offers a generic architecture for image super-resolution tasks. PL-E-GAN is constituted of two convolutional neural networks (CNNs) that are the Generative network and Discriminator network Gn and Dn, respectively. PL-E-GAN employed both the generative adversarial loss and perceptual adversarial loss as objective function to the network. The integration of these loss function undergoes an adversarial training and both the networks Gn and Dn trains alternatively. The feasibility and benefits of the PL-E-GAN over several image translation models are shown in studies and tested on many image-to-image translation tasks. © 2021 IEEE","Computer vision; Convolution; Convolutional neural networks; Deep learning; Image denoising; Optical resolving power; Convolutional neural network; Efficientnet; Image colorizations; Image translation; Image-to-image translation; Learning models; Loss functions; Perceptual loss; Superresolution; Translation models; Generative adversarial networks","convolutional neural network; EfficientNet; Generative adversarial network; image-to-image translation; perceptual loss; superresolution","Conference paper","Final","","Scopus","2-s2.0-85126707464"
"Prajapati K.; Chudasama V.; Patel H.; Upla K.; Raja K.; Ramachandra R.; Busch C.","Prajapati, Kalpesh (57217177596); Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Upla, Kishor (53985429600); Raja, Kiran (57188866050); Ramachandra, Raghavendra (57190835798); Busch, Christoph (7101767185)","57217177596; 57202047982; 57209145428; 53985429600; 57188866050; 57190835798; 7101767185","Unsupervised real-world super-resolution using variational auto-encoder and generative adversarial network","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12662 LNCS","","","703","718","15","10.1007/978-3-030-68790-8_54","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110734691&doi=10.1007%2f978-3-030-68790-8_54&partnerID=40&md5=a4aad9ecbbce2676e5fbcd823aef97c7","Convolutional Neural Networks (CNNs) have shown promising results on Single Image Super-Resolution (SISR) task. A pair of Low-Resolution (LR) and High-Resolution (HR) images are typically used in the CNN models to train them to super-resolve LR images in a fully supervised manner. Owing to non-availability of true LR-HR pairs, the LR images are generally synthesized from HR data by applying synthetic degradation such as bicubic downsampling. Such networks under-perform when used on real-world data where degradation is different from the synthetically generated LR image. As obtaining true LR-HR pair is a tedious and resource (time and effort) consuming task, we propose a new approach and architecture to super-resolve the real-world LR images in an unsupervised manner by using a Generative Adversarial Network (GAN) framework with Variational Auto-Encoder (VAE). Along with a new network architecture, we also introduce a novel loss metric based on no-reference quality scores of SR images to improve the perceptual fidelity of the SR images. Through the experiments on NTIRE-2020 Real-World SR Challenge dataset, we demonstrate the superiority of the proposed approach over the other competing state-of-the-art methods. © Springer Nature Switzerland AG 2021.","Convolutional neural networks; Image resolution; Learning systems; Network architecture; Network coding; Optical resolving power; Pattern recognition; Adversarial networks; High resolution image; Low resolution; New approaches; Perceptual fidelity; Single images; State-of-the-art methods; Super resolution; Image enhancement","Convolutional Neural Network; Generative Adversarial Network; No-reference image quality assessment; Unsupervised single image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85110734691"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","2","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119967935&partnerID=40&md5=7710817d7f6cdbc020a086b49babeb59","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119967935"
"","","","38th Computer Graphics International Conference, CGI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13002 LNCS","","","","","718","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118320399&partnerID=40&md5=37dbe8b76877ed7053a38aeb9e82e79f","The proceedings contain 53 papers. The special focus in this conference is on Computer Graphics. The topics include: Light-Weight Multi-view Topology Consistent Facial Geometry and Reflectance Capture; real-Time Fluid Simulation with Atmospheric Pressure Using Weak Air Particles; reinforcement Learning for Quadruped Locomotion; partially Occluded Skeleton Action Recognition Based on Multi-stream Fusion Graph Convolutional Networks; social-Scene-Aware Generative Adversarial Networks for Pedestrian Trajectory Prediction; cecid Fly Defect Detection in Mangoes Using Object Detection Frameworks; twin-Channel Gan: Repair Shape with Twin-Channel Generative Adversarial Network and Structural Constraints; coPaint: Guiding Sketch Painting with Consistent Color and Coherent Generative Adversarial Networks; multi-Stream Fusion Network for Multi-Distortion Image Super-Resolution; the Impact of Animations in the Perception of a Simulated Crowd; generative Face Parsing Map Guided 3D Face Reconstruction Under Occluded Scenes; Compact Double Attention Module Embedded CNN for Palmprint Recognition; M2M: Learning to Enhance Low-Light Image from Model to Mobile FPGA; character Flow Detection and Rectification for Scene Text Spotting; a Deep Learning Method for 2D Image Stippling; in Silico Heart Versatile Graphical Interface with Systole and Diastole Phases Customizable for Diversified Arrhythmias Simulations; ADD-Net:Attention U-Net with Dilated Skip Connection and Dense Connected Decoder for Retinal Vessel Segmentation; BDFNet: Boundary-Assisted and Discriminative Feature Extraction Network for COVID-19 Lung Infection Segmentation; a Classification Network for Ocular Diseases Based on Structure Feature and Visual Attention; DSNet: Dynamic Skin Deformation Prediction by Recurrent Neural Network; virtual Haptic System for Shape Recognition Based on Local Curvatures; curvature Analysis of Sculpted Hair Meshes for Hair Guides Generation; temporal-Consistency-Aware Video Color Transfer.","","","Conference review","Final","","Scopus","2-s2.0-85118320399"
"Chen Z.-H.; Hu H.-L.; Yao J.-M.; Yan Q.; Lin Z.-X.","Chen, Zong-Hang (57224578472); Hu, Hai-Long (57198978159); Yao, Jian-Min (55265981800); Yan, Qun (57217136691); Lin, Zhi-Xian (26421249000)","57224578472; 57198978159; 55265981800; 57217136691; 26421249000","Single frame image super-resolution reconstruction based on improved generative adversarial network","2021","Chinese Journal of Liquid Crystals and Displays","36","5","1007-2780(2021)05-0705-08","705","712","7","10.37188/CJLCD.2020-0250","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107949406&doi=10.37188%2fCJLCD.2020-0250&partnerID=40&md5=14033b645ee24f2b2f8784e159230fd5","Abstract: In order to obtain better image super-resolution reconstruction quality and improve the stability of network training, the generation of confrontation networks and loss functions are studied. Firstly- SRG AN and DenseNet are introduced, a generation network is designed to generate image based on DenseNet, and the sub-pixel convolution module is added to DenseNet. Then, the redundant BN layer in the original DenseNet is removed to improve the training efficiency of the model. Finally, the loss function of SRGAN is introduced and the loss function is redesigned based on the Earth- Mover distance, and the SmoothLl loss is used to replace the MSE loss to calculate the VGG feature map to prevent MSE from amplifying the gap between the maximum error and the minimum error. Experiments prove that the model can achieve a stable convergence state during the network training process. The quality of the reconstructed image is compared with SRGAN, thc average PSNR on the three benchmark tesl sets SET5, SET14, and BSD100 is about 2.02 dB higher, and SSIM is about 0.042 (5.6%) higher. The reconstructed image not only has improved indicators, but also has better definition and richer high-frequency details. © 2021, Science Press. All rights reserved.","","Deep learning; Generative adversarial networks; Image super-resolution","Article","Final","","Scopus","2-s2.0-85107949406"
"Pala F.; Mhiri I.; Rekik I.","Pala, Furkan (57290763300); Mhiri, Islem (57211785783); Rekik, Islem (55546384900)","57290763300; 57211785783; 55546384900","Template-Based Inter-modality Super-Resolution of Brain Connectivity","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12928 LNCS","","","70","82","12","10.1007/978-3-030-87602-9_7","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116834172&doi=10.1007%2f978-3-030-87602-9_7&partnerID=40&md5=2a0119d01983860f940bfa9b998d190e","Brain graph synthesis becomes a challenging task when generating brain graphs across different modalities. Although promising, existing multimodal brain graph synthesis frameworks based on deep learning have several limitations. First, they mainly focus on predicting intra-modality graphs, overlooking the rich multimodal representations of brain connectivity (inter-modality). Second, while few techniques work on super-resolving low-resolution brain graphs within a single modality (i.e., intra), inter-modality graph super-resolution remains unexplored though this avoids the need for costly data collection and processing. More importantly, all these works need large amounts of training data which is not always feasible due to the scarce neuroimaging datasets especially for low resource clinical facilities and rare diseases. To fill these gaps, we propose an inter-modality super-resolution brain graph synthesis (TIS-Net) framework trained on one population driven atlas namely–connectional brain template (CBT). Our TIS-Net is grounded in three main contributions: (i) predicting a target representative template from a source one based on a novel graph generative adversarial network, (ii) generating high-resolution representative brain graph without resorting to the time consuming and expensive MRI processing steps, and (iii) training our framework using one shot learning where we estimate a representative and well centered CBT (i.e., one shot) with shared common traits across subjects. Moreover, we design a new Target Preserving loss function to guide the generator in learning the structure of target the CBT more accurately. Our comprehensive experiments on predicting a target CBT from a source CBT using our method showed the outperformance of TIS-Net in comparison with its variants. TIS-Net presents the first work for graph synthesis based on one representative shot across varying modalities and resolutions, which handles graph size, and structure variations. Our Python TIS-Net code is available on BASIRA GitHub at https://github.com/basiralab/TIS-Net. © 2021, Springer Nature Switzerland AG.","Data handling; Forecasting; Large dataset; Neuroimaging; Optical resolving power; Brain connectivity; Brain templates; Data collection; Intermodality; Large amounts; Lower resolution; Multi-modal; Superresolution; Template-based; Training data; Deep learning","","Conference paper","Final","","Scopus","2-s2.0-85116834172"
"Niu Y.; Wang Y.D.; Mostaghimi P.; Swietojanski P.; Armstrong R.T.","Niu, Yufu (7202225505); Wang, Ying Da (57207002693); Mostaghimi, Peyman (35759133600); Swietojanski, Pawel (37125204800); Armstrong, Ryan T. (37025484700)","7202225505; 57207002693; 35759133600; 37125204800; 37025484700","An Innovative Application of Generative Adversarial Networks for Physically Accurate Rock Images With an Unprecedented Field of View","2020","Geophysical Research Letters","47","23","e2020GL089029","","","","10.1029/2020GL089029","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096687591&doi=10.1029%2f2020GL089029&partnerID=40&md5=ecf77a81ee37e63d24b261149872218f","High-resolution X-ray microcomputed tomography (micro-CT) data are used for the accurate determination of rock petrophysical properties. High-resolution data, however, result in a small field of view, and thus, the representativeness of a simulation domain can be brought into question when dealing with geophysical applications. This paper applies a cycle-in-cycle generative adversarial network (CinCGAN) to improve the resolution of 3-D micro-CT data and create a super-resolution image using unpaired training images. Effective porosity, Euler characteristic, pore size distribution, and absolute permeability are measured on super-resolution and high-resolution ground-truth images to evaluate the physical accuracy of the proposed CinCGAN. The results demonstrate that CinCGAN provides physically accurate images with an order of magnitude larger field of view when compared to typical micro-CT methods. This unlocks new pathways for the geophysical characterization of subsurface rocks with broad implications for flow modeling in highly heterogeneous rocks or fundamental studies on nonlocal forces that extend beyond domain sizes typically used for pore-scale simulation. ©2020. American Geophysical Union. All Rights Reserved.","Image enhancement; Optical resolving power; Petrophysics; Pore size; Rocks; Absolute permeability; Euler characteristic; Geophysical applications; Geophysical characterization; High resolution data; Petrophysical properties; Pore-scale simulation; X ray micro-computed tomography; computer system; field of view; flow modeling; image resolution; innovation; magnitude; microstructure; permeability; porosity; three-dimensional modeling; tomography; Computerized tomography","digital rock; generative adversarial network; super resolution; X-ray microcomputed tomography","Article","Final","","Scopus","2-s2.0-85096687591"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","10","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119989448&partnerID=40&md5=e7df3dd0d2038fdbe8ecda5a520642f1","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119989448"
"Lu X.; Wang M.; Wu H.; Hui F.","Lu, Xin (57221221349); Wang, Mingqun (57210220589); Wu, Hangyu (57222593897); Hui, Fang (57221232375)","57221221349; 57210220589; 57222593897; 57221232375","Deep learning for fast image reconstruction of Fourier ptychographic microscopy with expanded frequency spectrum","2021","Proceedings of SPIE - The International Society for Optical Engineering","11781","","117810M","","","","10.1117/12.2591381","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103356324&doi=10.1117%2f12.2591381&partnerID=40&md5=4d083468f436b8b8603da3d476b071a6","Fourier Ptychographic Microscopy (FPM) is a super-resolution microscopy technology, in which a set of low-resolution images containing different frequency components of the sample can be obtained by changing the angle of the light source in this technology, and then the iterative algorithm is used to reconstruct high-resolution intensity and phase information. The reconstruction usually takes a long time and is not suitable for real-time FPM imaging. It has been recognized recently that the potential fast image reconstruction algorithm is the use of deep learning algorithms. We designed a conditional generative adversarial network (cGAN) which has multi-branch input and multi-branch output which can expand the frequency spectrum of the reconstructed image very well. Based on the convolutional neural network (CNN), the brightfield and darkfield images obtained by FPM imaging can be regarded as different image features obtained by different convolutional kernel, and the skip connection of U-net can effectively utilize this information. The brightfield and darkfield images in FPM imaging are input to different branches, which can avoid missing the darkfield signal information. Importantly, the neural network we designed will continue to perform simulation process of FPM imaging from the recovered high-resolution intensity and phase to obtain low-resolution images and make them correspond one-to-one with the input low-resolution images. These corresponded images will enter loss function, making it easier for the neural network to learn relation between the low-resolution images and the high-resolution images. We validated the deep learning algorithm through simulated experimental research on biological cell imaging. © SPIE. Downloading of the abstract is permitted for personal use only.","Bioinformatics; Convolution; Convolutional neural networks; Deep learning; Iterative methods; Learning algorithms; Light sources; Spectroscopy; Adversarial networks; Convolutional kernel; Different frequency; Experimental research; Fast image reconstruction; High resolution image; Low resolution images; Super-resolution microscopy; Image reconstruction","Computational Imaging; Deep Learning; Dense net; Fourier ptychographic Microscopy","Conference paper","Final","","Scopus","2-s2.0-85103356324"
"Jeihouni P.; Dehzangi O.; Amireskandari A.; Rezai A.; Nasrabadi N.M.","Jeihouni, Paria (57215874065); Dehzangi, Omid (23396296800); Amireskandari, Annahita (55135990500); Rezai, Ali (35479361100); Nasrabadi, Nasser M. (7006312852)","57215874065; 23396296800; 55135990500; 35479361100; 7006312852","GAN-BASED SUPER-RESOLUTION AND SEGMENTATION OF RETINAL LAYERS IN OPTICAL COHERENCE TOMOGRAPHY SCANS","2021","Proceedings - International Conference on Image Processing, ICIP","2021-September","","","46","50","4","10.1109/ICIP42928.2021.9506291","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125564702&doi=10.1109%2fICIP42928.2021.9506291&partnerID=40&md5=21a4d6b1bd69496018823aabf0e78083","In this paper, we design a Generative Adversarial Network (GAN)-based solution for super-resolution and segmentation of optical coherence tomography (OCT) scans of the retinal layers. OCT has been identified as a non-invasive and inexpensive modality of imaging to discover potential biomarkers for the diagnosis and progress determination of neurodegenerative diseases, such as Alzheimer’s Disease (AD). Current hypotheses presume the thickness of the retinal layers, which are analyzable within OCT scans, can be effective biomarkers. As a logical first step, this work concentrates on the challenging task of retinal layer segmentation and also super-resolution for higher clarity and accuracy. We propose a GAN-based segmentation model and evaluate incorporating popular networks, namely, U-Net and ResNet, in the GAN architecture with additional blocks of transposed convolution and sub-pixel convolution for the task of upscaling OCT images from low to high resolution by a factor of four. We also incorporate the Dice loss as an additional reconstruction loss term to improve the performance of this joint optimization task. Our best model configuration empirically achieved the Dice coefficient of 0.867 and mIOU of 0.765. © 2021 IEEE.","Biomarkers; Convolution; Deep learning; Diagnosis; Generative adversarial networks; Image segmentation; Neurodegenerative diseases; Ophthalmology; Optical resolving power; 'current; Alzheimer; Conditional generative adversarial network; Dice loss; Layer segmentation; Network-based; Network-based solutions; Retinal layer segmentation; Retinal layers; Superresolution; Optical tomography","Conditional GAN; Dice loss; Optical coherence tomography; Retinal layer segmentation; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125564702"
"Su J.; Wang Y.; Finkelstein A.; Jin Z.","Su, Jiaqi (57209881410); Wang, Yunyun (57261963800); Finkelstein, Adam (7101962340); Jin, Zeyu (57189597728)","57209881410; 57261963800; 7101962340; 57189597728","Bandwidth extension is all you need","2021","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2021-June","","","696","700","4","10.1109/ICASSP39728.2021.9413575","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115077319&doi=10.1109%2fICASSP39728.2021.9413575&partnerID=40&md5=191e26ebd4cd38e12afc2b8c8fb6c84a","Speech generation and enhancement have seen recent breakthroughs in quality thanks to deep learning. These methods typically operate at a limited sampling rate of 16-22kHz due to computational complexity and available datasets. This limitation imposes a gap between the output of such methods and that of high-fidelity (≥44kHz) real-world audio applications. This paper proposes a new bandwidth extension (BWE) method that expands 8-16kHz speech signals to 48kHz. The method is based on a feed-forward WaveNet architecture trained with a GAN-based deep feature loss. A mean-opinion-score (MOS) experiment shows significant improvement in quality over state-of-the-art BWE methods. An AB test reveals that our 16-to-48kHz BWE is able to achieve fidelity that is typically indistinguishable from real high-fidelity recordings. We use our method to enhance the output of recent speech generation and denoising methods, and experiments demonstrate significant improvement in sound quality over these baselines. We propose this as a general approach to narrow the gap between generated speech and recorded speech, without the need to adapt such methods to higher sampling rates. © 2021 IEEE","Bandwidth; Deep learning; Signal processing; Speech; Audio applications; Bandwidth extension; Denoising methods; Mean opinion scores; Sampling rates; Speech generation; Speech signals; State of the art; Speech communication","Audio super resolution; Bandwidth extension; Deep features; Generative adversarial networks; Speech enhancement","Conference paper","Final","","Scopus","2-s2.0-85115077319"
"Emad M.; Peemen M.; Corporaal H.","Emad, Mohammad (57196009556); Peemen, Maurice (48861712000); Corporaal, Henk (7003345585)","57196009556; 48861712000; 7003345585","DualSR: Zero-shot dual learning for real-world super-resolution","2021","Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021","","","","1629","1638","9","10.1109/WACV48630.2021.00167","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102848896&doi=10.1109%2fWACV48630.2021.00167&partnerID=40&md5=bf9196ff88242c8bea115ab27faaac88","Advanced methods for single image super-resolution (SISR) based upon Deep learning have demonstrated a remarkable reconstruction performance on downscaled images. However, for real-world low-resolution images (e.g. images captured straight from the camera) they often generate blurry images and highlight unpleasant artifacts. The main reason is the training data that does not reflect the real-world super-resolution problem. They train the net-work using images downsampled with an ideal (usually bicubic) kernel. However, for real-world images the degradation process is more complex and can vary from image to image. This paper proposes a new dual-path architecture (DualSR) that learns an image-specific low-to-high resolution mapping using only patches of the input test image. For every image, a downsampler learns the degradation process using a generative adversarial network, and an up-sampler learns to super-resolve that specific image. In the DualSR architecture, the upsampler and downsampler are trained simultaneously and they improve each other using cycle consistency losses. For better visual quality and eliminating undesired artifacts, the upsampler is constrained by a masked interpolation loss. On standard benchmarks with unknown degradation kernels, DualSR outperforms recent blind and non-blind super-resolution methods in term of SSIM and generates images with higher perceptual quality. On real-world LR images it generates visually pleasing and artifact-free results. © 2021 IEEE.","Benchmarking; Computer vision; Deep learning; Generative adversarial networks; Network architecture; Blurry images; Degradation process; Image super resolutions; Learn+; Low resolution images; Performance; Real-world; Single images; Superresolution; Training data; Optical resolving power","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102848896"
"Cho H.-W.; Kim W.; Choi S.; Eo M.; Khang S.; Kim J.","Cho, Hyun-Woong (57192870277); Kim, Woosuk (57214365232); Choi, Sungdo (57209533654); Eo, Minsung (57214364940); Khang, Seungtae (57516781600); Kim, Jongseok (57209542538)","57192870277; 57214365232; 57209533654; 57214364940; 57516781600; 57209542538","Guided Generative Adversarial Network for Super Resolution of Imaging Radar","2021","EuRAD 2020 - 2020 17th European Radar Conference","","","9337325","144","147","3","10.1109/EuRAD48048.2021.00046","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100887546&doi=10.1109%2fEuRAD48048.2021.00046&partnerID=40&md5=b2d21c1c03cbd94e6ef6fa00423c2a6b","We propose a super-resolution algorithm for radar data that combines the ability to generate realistic results based on adversarial loss with the accurate estimation performance of subspace-type direction-of-arrival technique. The proposed algorithm forces the generator to completely incorporate information in multiple channels through a guide for the discriminator in the adversarial learning architecture, enabling reliable super resolution of radar data. In addition, the proposed algorithm can be generalized and applied to all dimensions of radar data, thereby contributing toward overcoming the physical limitations in obtaining 4D high-resolution radar data.  © 2021 EuMA.","Optical resolving power; Radar; Accurate estimation; Adversarial learning; Adversarial networks; High resolution radar; Multiple channels; Physical limitations; Super resolution; Super resolution algorithms; Radar imaging","adversarial learning; MIMO radar; radar imaging; super resolution","Conference paper","Final","","Scopus","2-s2.0-85100887546"
"Gautam I.; Arjun Rajesh K.P.; Sagere K.S.; Badri Prasad K.V.","Gautam, Iruvanti (57310686200); Arjun Rajesh, K.P. (57310686300); Sagere, Krishna Sidharth (57310866800); Badri Prasad, K.V. (57310866900)","57310686200; 57310686300; 57310866800; 57310866900","Image super resolution and deblurring using generative adversarial networks","2021","12th International Conference on Advances in Computing, Control, and Telecommunication Technologies, ACT 2021","2021-August","","","110","115","5","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117774960&partnerID=40&md5=1f34418acb0151593a9ab65a0686fa7b","As deep learning methodologies exhibit strong advantages in the vast domain of feature extraction, it has been used widely in the computer vision area, and gradually replaced traditional machine learning algorithms. This paper describes in detail the development, creation and Analytics of a Deep learning pipeline which is comprised of two components firstly a Deblur Generative Adversarial Network (DeblurGAN) and secondly a Super Resolution Generative Adversarial Net-work (SRGAN) which work in unison with one another in order to convert and transform Low Quality Blurred images into a higher quality Super resolved de-blurred image. In our solution we are aiming to tackle various types of blurs such as linear blurring, Gaussian Blurring and Media Blurring. Generative adversarial networks (GANs) are algorithmic architectures that primarily use two neural networks, pitting one against the other (hence the”adversarial”) in order to produce new,enhanced, synthetic data instances that can pass for real data, using this principle we are generating de-blurred and super-resolved images. The Trained Discriminator acts as an accurate classifier and has the main functionality of differentiating and distinguishing between the multiple generated images of the GAN and the real images. At the specific point in time when the discriminator component is unable to differentiate between the images that are created and the actual images, it is safe to say that we have generated the most de-blurred and Super-resolved image possible. Afterwards the results (i.e. Peak Signal to Noise Ratio aka PSNR) are compared in different scenarios to observe which type of blur is best tackled by the deep learning pipeline. © Grenze Scientific Society, 2021.","Deep learning; Image enhancement; Learning algorithms; Optical resolving power; Pipelines; Signal to noise ratio; Adversarial loss; Blurred image; Content loss; Deblurring; Features extraction; Image super resolutions; Machine learning algorithms; PSNR value; Superresolution; Two-component; Generative adversarial networks","Adversarial loss; Content loss; Deblurring; Generative adversarial networks; PSNR value; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85117774960"
"Li X.; Wu Y.; Zhang W.; Wang R.; Hou F.","Li, Xiaofang (55718269300); Wu, Yirui (56093586700); Zhang, Wen (57221431547); Wang, Ruichao (56128411600); Hou, Feng (57220668737)","55718269300; 56093586700; 57221431547; 56128411600; 57220668737","Deep learning methods in real-time image super-resolution: a survey","2020","Journal of Real-Time Image Processing","17","6","","1885","1909","24","10.1007/s11554-019-00925-3","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075357831&doi=10.1007%2fs11554-019-00925-3&partnerID=40&md5=3a34821560f1aa29a2ecb1114b4ea372","Super-resolution is generally defined as a process to obtain high-resolution images form inputs of low-resolution observations, which has attracted quantity of attention from researchers of image-processing community. In this paper, we aim to analyze, compare, and contrast technical problems, methods, and the performance of super-resolution research, especially real-time super-resolution methods based on deep learning structures. Specifically, we first summarize fundamental problems, perform algorithm categorization, and analyze possible application scenarios that should be considered. Since increasing attention has been drawn in utilizing convolutional neural networks (CNN) or generative adversarial networks (GAN) to predict high-frequency details lost in low- resolution images, we provide a general overview on background technologies and pay special attention to super-resolution methods built on deep learning architectures for real-time super-resolution, which not only produce desirable reconstruction results, but also enlarge possible application scenarios of super resolution to systems like cell phones, drones, and embedding systems. Afterwards, benchmark datasets with descriptions are enumerated, and performance of most representative super-resolution approaches is provided to offer a fair and comparative view on performance of current approaches. Finally, we conclude the paper and suggest ways to improve usage of deep learning methods on real-time image super-resolution. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.","Benchmarking; Convolution; Convolutional neural networks; Deep neural networks; Image enhancement; Learning systems; Optical resolving power; Real time systems; Adversarial networks; Application scenario; High resolution image; Image super resolutions; Learning architectures; Low resolution images; Realtime processing; Superresolution methods; Deep learning","Convolutional neural network; Deep learning; Generative adversarial network; Image super-resolution; Real-time processing","Conference paper","Final","","Scopus","2-s2.0-85075357831"
"Yamagishi R.; Sciazko A.; Ouyang Z.; Komatsu Y.; Katsuhiko N.; Shikazono N.","Yamagishi, Rena (57226458884); Sciazko, Anna (56499521400); Ouyang, Zhufeng (57226477038); Komatsu, Yosuke (56239831800); Katsuhiko, Nishimura (7403202987); Shikazono, Naoki (7005214534)","57226458884; 56499521400; 57226477038; 56239831800; 7403202987; 7005214534","Super-Resolved in-Operando Observation of SOFC Pattern Electrodes","2021","ECS Transactions","103","1","","2087","2098","11","10.1149/10301.2087ecst","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111700121&doi=10.1149%2f10301.2087ecst&partnerID=40&md5=e14346c947abf5e84a76b6d7b4c32781","In order to investigate Nickel (Ni) movements during operation, in-operando observation of Ni-pattern electrode is conducted with confocal laser microscope. In-operando observations have limitation of quantitative analysis due to the low resolution of the laser microscope. In this study, two machine learning algorithms for super resolving laser microscope images are proposed. Firstly, semantic segmentation algorithm based on convolutional neural network (CNN) is introduced for automatic material identification. U-net architecture is incorporated and compared with simple encoder-decoder network. The phase fractions and triple phase boundary (TPB) density estimated from the automatically segmented image agreed well with the ground truth data. The proposed U-net can detect fine features of the microstructure. Secondly, super-resolution generative adversarial network (GAN) is proposed. The developed network applies pix2pix GAN architecture and aims to synthesize SEM-quality image from low resolution laser image. The generated images achieved very high resolution equivalent to SEM images. © 2021 Electrochemical Society Inc.. All rights reserved.","Convolutional neural networks; Electrodes; Image segmentation; Learning algorithms; Machine learning; Microscopes; Network architecture; Nickel metallography; Semantics; Adversarial networks; Confocal laser microscopes; Ground truth data; Material identification; Pattern electrodes; Semantic segmentation; Triple phase boundary; Very high resolution; Solid oxide fuel cells (SOFC)","","Conference paper","Final","","Scopus","2-s2.0-85111700121"
"Shim J.; Kim J.; Kim J.","Shim, JooYong (57221962428); Kim, Joongheon (8866990900); Kim, JongKook (7601381739)","57221962428; 8866990900; 7601381739","On the Tradeoff between Computation-Time and Learning-Accuracy in GAN-based Super-Resolution Deep Learning","2021","International Conference on Information Networking","2021-January","","9333991","422","424","2","10.1109/ICOIN50884.2021.9333991","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100795277&doi=10.1109%2fICOIN50884.2021.9333991&partnerID=40&md5=1147e4e21153ce1bedcb9371cbf8536d","The trade-off between accuracy and computation should be considered when applying generative adversarial network (GAN)-based image generation to real-world applications. This paper presents a simple yet efficient method based on Progressive Growing of GANs (PGGAN) to exploit the trade-off for image generation. The scheme is evaluated using the LSUN dataset. © 2021 IEEE.","Economic and social effects; Adversarial networks; Computation time; GaN based; Image generations; Learning accuracy; Real-world; Super resolution; Trade off; Deep learning","","Conference paper","Final","","Scopus","2-s2.0-85100795277"
"Granstedt J.L.; Kelkar V.A.; Zhou W.; Anastasio M.A.","Granstedt, Jason L. (57190435103); Kelkar, Varun A. (57204540505); Zhou, Weimin (57193382198); Anastasio, Mark A. (7006769220)","57190435103; 57204540505; 57193382198; 7006769220","SlabGAN: A method for generating efficient 3D anisotropic medical volumes using generative adversarial networks","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11596","","1159617","","","","10.1117/12.2581380","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103628174&doi=10.1117%2f12.2581380&partnerID=40&md5=e6f2edca40714be58a31ff591a2d213b","Generative adversarial networks (GANs) have proven useful for several medical imaging tasks, including image reconstruction and stochastic object model generation. Thus far, most of the work with GANs has been constrained to twodimensional images. Considering that medical imaging data are often inherently three-dimensional (3D), a 3D GAN would be a more principled way to synthesize realistic volumes. Training a 3D GAN is both computationally and memory intensive. However, prior work has not considered the anisotropic nature of many medical imaging systems. In this paper, the SlabGAN is proposed to reduce the inefficiencies associated with training a 3D GAN. The SlabGAN uses the progressive GAN architecture extended to 3D, but removes the requirement of the three dimensions being equal sizes. This permits the generation of anisotropic 3D volumes with large x and y dimensions. The SlabGAN is trained on MRI brain images from the fastMRI dataset to generate images of dimension 256×256×16. The x and y dimensions of these images are comparable to previously published results while requiring significantly fewer computational resources to generate. The trained SlabGAN is applicable to tasks such as 3D medical image reconstruction and thin-slice MR super resolution.  © 2021 SPIE.","Anisotropy; Brain mapping; Magnetic resonance imaging; Medical image processing; Stochastic models; Stochastic systems; 3D medical image; Adversarial networks; Brain images; Computational resources; Super resolution; Three dimensions; Threedimensional (3-d); Two-dimensional images; Image reconstruction","3D; Anisotropic; Deep learning; GAN; Generative adversarial network; Image synthesis; MRI","Conference paper","Final","","Scopus","2-s2.0-85103628174"
"Do H.; Bourdon P.; Helbert D.; Naudin M.; Guillevin R.","Do, Huy (57413750800); Bourdon, Pascal (7005784278); Helbert, David (12761288800); Naudin, Mathieu (57204172140); Guillevin, Remy (57225705908)","57413750800; 7005784278; 12761288800; 57204172140; 57225705908","7T MRI super-resolution with Generative Adversarial Network","2021","IS and T International Symposium on Electronic Imaging Science and Technology","2021","18","","","","","10.2352/ISSN.2470-1173.2021.18.3DIA-106","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119140910&doi=10.2352%2fISSN.2470-1173.2021.18.3DIA-106&partnerID=40&md5=fa596f7471988c2b91a2aaca8d64f263","The high-resolution magnetic resonance image (MRI) provides detailed anatomical information critical for clinical application diagnosis. However, high-resolution MRI typically comes at the cost of long scan time, small spatial coverage, and low signal-to-noise ratio. The benefits of the convolutional neural network (CNN) can be applied to solve the super-resolution task to recover high-resolution generic images from low-resolution inputs. Additionally, recent studies have shown the potential to use the generative advertising network (GAN) to generate high-quality super-resolution MRIs using learned image priors. Moreover, existing approaches require paired MRI images as training data, which is difficult to obtain with existing datasets when the alignment between high and low-resolution images has to be implemented manually. This paper implements two different GAN-based models to handle the super-resolution: Enhanced super-resolution GAN (ESRGAN) and CycleGAN. Different from the generic model, the architecture of CycleGAN is modified to solve the super-resolution on unpaired MRI data, and the ESRGAN is implemented as a reference to compare GAN-based methods performance. The results of GAN-based models provide generated high-resolution images with rich textures compared to the ground-truth. Moreover, results from experiments are performed on both 3T and 7T MRI images in recovering different scales of resolution. © 2021, Society for Imaging Science and Technology","Convolutional neural networks; Diagnosis; Generative adversarial networks; Medical applications; Optical resolving power; Signal to noise ratio; Textures; Advertising networks; Anatomical information; Clinical application; High resolution; High-resolution images; Image super resolutions; Network-based modeling; Scan time; Spatial coverage; Magnetic resonance imaging","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119140910"
"Chong C.K.; Ho E.T.W.","Chong, Chee Keong (57223342690); Ho, Eric Tatt Wei (56659125700)","57223342690; 56659125700","Synthesis of 3D MRI Brain Images with Shape and Texture Generative Adversarial Deep Neural Networks","2021","IEEE Access","9","","9416434","64747","64760","13","10.1109/ACCESS.2021.3075608","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105081647&doi=10.1109%2fACCESS.2021.3075608&partnerID=40&md5=5c39496c68c2aa1fe6f32708d097e373","Generative Adversarial Networks (GAN) are emerging as an exciting training paradigm which promises a step improvement to the impressive feature learning capabilities of deep neural networks. Unlike supervised learning approaches, GAN learns generalizable features without requiring labeled images to achieve new capabilities like distinguishing previously unseen anomalies, creating novel instances of data and factorizing learned features into explainable dimensions in fully unsupervised fashion. The advanced feature learning property of GAN will enable the next generation of computational image understanding tasks. However, GAN models are difficult to train to converge towards good models, especially for high resolution and high dimensional datasets like image volumes. We develop a GAN approach to learn a generative model of T1-contrast 3D MRI image volumes of the healthy human brain by training on 1112 MRI images from the Human Connectome Project. Our method utilizes a first unconditional Super-Resolution GAN, dubbed the shape network, to learn the 3D shape variations in adult brains and a second conditional pix2pix GAN, dubbed the texture network, to upgrade image slices with realistic local contrast patterns. Novel 3D MRI images are synthesized by first applying the 3D voxel-wise deformation map which is generated from the shape network to deform the Montreal Neurological Institute (MNI) brain template and subsequently performing style transfer on axial-wise slices using the texture network. The Maximum Mean Discrepancy (MMD) and Multi-scale Structural Similarity Index Measure (MS-SSIM) scores of MRI image volumes synthesized using our GAN approach are competitive with state-of-art GAN methods. Our work establishes the feasibility of an alternative approach to high-dimensional GAN learning-splitting the type of information content learned among several GANs can be an effective form of regularization and complementary to latent code shaping or super-resolution approaches in state-of-the-art methods.  © 2013 IEEE.","Brain mapping; Deep learning; Deep neural networks; Magnetic resonance imaging; Neural networks; Optical resolving power; Textures; Adversarial networks; High dimensional datasets; Human Connectome; Information contents; Shape and textures; State-of-the-art methods; Structural similarity index measures; Supervised learning approaches; Image texture","brain morphometry; computational brain anatomy; deep neural networks; Generative adversarial networks; MRI brain images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85105081647"
"","","","7th International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2021","2021","Communications in Computer and Information Science","1452 CCIS","","","","","1055","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115713811&partnerID=40&md5=f06be6e5abb6d7c68eba6453168663ef","The proceedings contain 81 papers. The special focus in this conference is on Pioneering Computer Scientists, Engineers and Educators. The topics include: MA Mask R-CNN: MPR and AFPN Based Mask R-CNN; improved Non-negative Matrix Factorization Algorithm for Sparse Graph Regularization; a Blockchain-Based Scheme of Data Sharing for Housing Provident Fund; intelligent Storage System of Machine Learning Model Based on Task Similarity; predicting Stock Price Movement with Multiple Data Sources and Machine Learning Models; channel Context and Dual-Domain Attention Based U-Net for Skin Lesion Attributes Segmentation; study on the Protection and Product Development of Intangible Cultural Heritage with Computer Virtual Reality Technology; ECG-Based Arrhythmia Detection Using Attention-Based Convolutional Neural Network; Quantum Color Image Scaling on QIRHSI Model; WSN Data Compression Model Based on K-SVD Dictionary and Compressed Sensing; human Body Pose Recognition System Based on Teaching Interaction; adaptive Densely Residual Network for Image Super-Resolution; Real-Time Image and Video Artistic Style Rendering System Based on GPU; semantic Segmentation of High Resolution Remote Sensing Images Based on Improved ResU-Net; Exploring Classification Capability of CNN Features; generative Adversarial Network Based Status Generation Simulation Approach; the Construction of Case Event Logic Graph for Judgment Documents; anti-obfuscation Binary Code Clone Detection Based on Software Gene; thread Private Variable Access Optimization Technique for Sunway High-Performance Multi-core Processors; parallel Region Reconstruction Technique for Sunway High-Performance Multi-core Processors; research on Route Optimization of Battlefield Collection Equipment Based on Improved Ant Algorithm; a Collaborative Cache Strategy Based on Utility Optimization; integrating Local Closure Coefficient into Weighted Networks for Link Prediction.","","","Conference review","Final","","Scopus","2-s2.0-85115713811"
"Hyatt J.S.; Lee M.S.","Hyatt, John S. (57516417500); Lee, Michael S. (55744663400)","57516417500; 55744663400","Multi-modal generative adversarial networks make realistic and diverse but untrustworthy predictions when applied to ILL-posed problems","2021","CEUR Workshop Proceedings","2808","","","","","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101285244&partnerID=40&md5=95872d82b17a21d3f8171feaa346d9d8","Ill-posed problems can have a distribution of possible solutions rather than a unique one, where each solution incorporates significant features not present in the initial input. We investigate whether cycle-consistent generative neural network models based on generative adversarial networks (GANs) and variational autoencoders (VAEs) can properly sample from this distribution, testing on super-resolution of highly down-sampled images. We are able to produce diverse and plausible predictions, but, looking deeper, we find that the statistics of the generated distributions are substantially wrong. This is a critical flaw in applications that require any kind of uncertainty quantification. We trace this to the fact that these models cannot easily learn a bijective, invertible map between the latent space and the target distribution. Additionally, we describe a simple method for constraining the distribution of a deterministic encoder’s outputs via the Kullback-Leibler divergence without the reparameterization trick used in VAEs. Copyright © 2021 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Adversarial networks; Critical flaw; Ill posed problem; Kullback Leibler divergence; Neural network model; Reparameterization; Super resolution; Uncertainty quantifications; Artificial intelligence","","Conference paper","Final","","Scopus","2-s2.0-85101285244"
"Wang T.; Lei Y.; Tian Z.; Tang X.; Curran W.J.; Liu T.; Yang X.","Wang, Tonghe (57189639465); Lei, Yang (57202715941); Tian, Zhen (35320361000); Tang, Xiangyang (7404101165); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Yang, Xiaofeng (36712893800)","57189639465; 57202715941; 35320361000; 7404101165; 57203070877; 26643332700; 36712893800","Synthesizing high-resolution ct from low-resolution ct using self-learning","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11595","","115952N","","","","10.1117/12.2581080","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103690019&doi=10.1117%2f12.2581080&partnerID=40&md5=36c7ad2a7ae1eca849a9615bf5a7b944","We propose a learning-based method to synthesize high-resolution (HR) CT images from low-resolution (LR) CT images. A self-super-resolution framework with cycle consistent generative adversarial network (CycleGAN) is proposed. As an ill-posed problem, recent super-resolution methods rely on the presence of external/training atlases to learn the transform LR images to HR images, which is often not available for CT imaging to have high resolution for slice thickness. To circumvent the lack of HR training data in z-axis, the network learns the mapping from LR 2D transverse plane slices to HR 2D transverse plane slices via CycleGAN and inference HR 2D sagittal and coronal plane slices by feeding these sagittal and coronal slices into the trained CycleGAN. The 3D HR CT image is then reconstructed by collecting these HR 2D sagittal and coronal slices and image fusion. In addition, in order to force the ill-posed LR to HR mapping to be close to a one-to-one mapping, CycleGAN is used to model the mapping. To force the network focusing on learning the difference between LR and HR image, residual network is integrated into the CycleGAN. To evaluate the proposed method, we retrospectively investigate 20 brain datasets. For each dataset, the original CT image volume was served as ground truth and training target. Low-resolution CT volumes were simulated by downsampling the original CT images at slice thickness direction. The MAE is 17.9±2.9 HU and 25.4±3.7 HU for our results at downsampling factor of 3 and 5, respectively. The proposed method has great potential in improving the image resolution for low pitch scan without hardware modification. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Image enhancement; Image fusion; Image resolution; Mapping; Medical imaging; Optical resolving power; Signal sampling; Adversarial networks; Hardware modifications; High resolution CT; Ill posed problem; Learning-based methods; One-to-one mappings; Superresolution methods; Transverse planes; Computerized tomography","Ct; High-resolution.; Self-learning","Conference paper","Final","","Scopus","2-s2.0-85103690019"
"Wang C.; Zhang Y.; Zhang Y.; Tian R.; Ding M.","Wang, Cong (57862719900); Zhang, Yin (57216699083); Zhang, Yongqiang (57190277288); Tian, Rui (57226599227); Ding, Mingli (8925275900)","57862719900; 57216699083; 57190277288; 57226599227; 8925275900","Mars image super-resolution based on generative adversarial network","2021","IEEE Access","9","","9503382","108889","108898","9","10.1109/ACCESS.2021.3101858","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112612309&doi=10.1109%2fACCESS.2021.3101858&partnerID=40&md5=dba09169201b0e35ac9da8fce2f5f3dd","High-resolution (HR) Mars images have great significance for studying the land-form features of Mars and analyzing the climate on Mars. Nowadays, the mainstream image super-resolution methods are based on deep learning or CNNs, which are better than traditional methods. However, these deep learning based methods obtain low-resolution(LR) images usually by using an ideal down-sampling method (e.g. bicubic interpolation). There are two limitations in the existing SR methods: 1) The paired LR-HR data by using such methods can achieve a satisfactory results when tested on an ideal datasets. But, these methods always fail in real Mars image super-resolution, since real Mars images rarely obey an ideal down-sampling rule. 2) The LR images obtained by ideal down-sampling methods have no noise while real Mars images usually have noise, which leads to the super-resolved images are not realistic in texture details. To solve the above-mentioned problems, in this article, we propose a novel two-step framework for Mars image super-resolution. Specifically, to address limitation 1), we focus on designing a new degradation framework by estimating blur-kernels. To address limitation 2), a Generative Adversarial Network (GAN) is trained to generate noise distribution. Extensive experiments on the Mars32k dataset demonstrate the effectiveness of the proposed method, and we achieve better qualitative and quantitative results compared to other SOTA methods. © 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Deep learning; Interpolation; Learning systems; Optical resolving power; Textures; Adversarial networks; Bicubic interpolation; High resolution; Image super resolutions; Learning-based methods; Low resolution images; Noise distribution; Quantitative result; Image sampling","Generative adversarial network; Kernel estimation; Mars image super-resolution; Noise model","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112612309"
"Tibo Z.; Lin L.; Kai Y.; Yu Z.; Jinlong L.","Tibo, Zha (57223845779); Lin, Luo (57195903367); Kai, Yang (55434982000); Yu, Zhang (57753508100); Jinlong, Li (57197732774)","57223845779; 57195903367; 55434982000; 57753508100; 57197732774","Image reconstruction algorithm based on improved super-resolution generative adversarial network","2021","Laser and Optoelectronics Progress","58","8","0810005","","","","10.3788/LOP202158.0810005","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106309318&doi=10.3788%2fLOP202158.0810005&partnerID=40&md5=c7d8a9ad47762168c2793fafe557bd7f","Aiming at the problem that the existing pixel loss-based super-resolution image reconstruction algorithms have poor reconstruction effect on high-frequency details, such as textures, an image reconstruction algorithm based on an improved super-resolution generative adversarial network (SRGAN) is proposed in this paper. First, remove the batch normalization layers in the generator, combine the multi-level residual network and dense connections, and use the residual-in-residual dense blocks to improve the network's ability for feature extraction. Then, the mean square error and perceptual loss are combined as the loss function to guide the generator training, which preserves the image's high-frequency details and avoids the artifacts' appearance. Finally, the last Sigmoid layer of the discriminator is removed to better converge the training process, and the relativistic loss function is used to guide the discriminator training. The experimental results on the COCO dataset show that compared with the original SRGAN algorithm, the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) of the algorithm in the Set5 data set are increased by 0.86 dB and 0.0123, respectively, in the Set14 data set, the PSNR and SSIM of the algorithm are improved by 0.69 dB and 0.0090, respectively. The mean opinion index and visual effect of the algorithm are far better than other algorithms. © 2021 Universitat zu Koln. All rights reserved.","","Deep learning; Generative adversarial network; Image processing; Residual network; Super-resolution","Article","Final","","Scopus","2-s2.0-85106309318"
"Fatima N.","Fatima, Noor (57221559177)","57221559177","AI in Photography: Scrutinizing Implementation of Super-Resolution Techniques in Photo-Editors","2020","International Conference Image and Vision Computing New Zealand","2020-November","","9290737","","","","10.1109/IVCNZ51579.2020.9290737","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099485496&doi=10.1109%2fIVCNZ51579.2020.9290737&partnerID=40&md5=8ec797f3597305fd203d4bc94a5b1924","Judging the quality of a photograph from the perspective of a photographer we can ascertain resolution, symmetry, content, location, etc. as some of the factors that influence the proficiency of a photograph. The exponential growth in the allurement for photography impels us to discover ways to perfect an input image in terms of the aforesaid parameters. Where content and location are the immutable ones, attributes like symmetry and resolution can be worked upon. In this paper, I prioritized resolution as our cynosure and there can be multiple ways to refine it. Image super-resolution is progressively becoming a prerequisite in the fraternity of computer graphics, computer vision, and image processing. It's the process of obtaining high-resolution images from their low-resolution counterparts. In my work, image super-resolution techniques like Interpolation, SRCNN (Super-Resolution Convolutional Neural Network), SRResNet (Super Resolution Residual Network), and GANs (Generative Adversarial Networks: Super-Resolution GAN-SRGAN and Conditional GAN-CGAN) were studied experimentally for post-enhancement of images in photography as employed by photo-editors, establishing the most coherent approach for attaining optimized super-resolution in terms of quality.  © 2020 IEEE.","Computer graphics; Convolutional neural networks; Optical resolving power; Photography; Adversarial networks; Exponential growth; High resolution image; Image super resolutions; Input image; Low resolution; Photo editors; Super resolution; Image enhancement","computer vision; gan; image processing; interpolation; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85099485496"
"Durall R.; Frolov S.; Hees J.; Raue F.; Pfreundt F.-J.; Dengel A.; Keuper J.","Durall, Ricard (57212194172); Frolov, Stanislav (57221158423); Hees, Jörn (35229784500); Raue, Federico (57141992100); Pfreundt, Franz-Josef (6507544948); Dengel, Andreas (6603764314); Keuper, Janis (57142459800)","57212194172; 57221158423; 35229784500; 57141992100; 6507544948; 6603764314; 57142459800","Combining Transformer Generators with Convolutional Discriminators","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12873 LNAI","","","67","79","12","10.1007/978-3-030-87626-5_6","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116905959&doi=10.1007%2f978-3-030-87626-5_6&partnerID=40&md5=132ebda50a27fac715f6ed4b46bd9da0","Transformer models have recently attracted much interest from computer vision researchers and have since been successfully employed for several problems traditionally addressed with convolutional neural networks. At the same time, image synthesis using generative adversarial networks (GANs) has drastically improved over the last few years. The recently proposed TransGAN is the first GAN using only transformer-based architectures and achieves competitive results when compared to convolutional GANs. However, since transformers are data-hungry architectures, TransGAN requires data augmentation, an auxiliary super-resolution task during training, and a masking prior to guide the self-attention mechanism. In this paper, we study the combination of a transformer-based generator and convolutional discriminator and successfully remove the need of the aforementioned required design choices. We evaluate our approach by conducting a benchmark of well-known CNN discriminators, ablate the size of the transformer-based generator, and show that combining both architectural elements into a hybrid model leads to better results. Furthermore, we investigate the frequency spectrum properties of generated images and observe that our model retains the benefits of an attention based generator. © 2021, Springer Nature Switzerland AG.","Convolution; Convolutional neural networks; Discriminators; Image enhancement; Network architecture; Architectural element; Attention mechanisms; Convolutional neural network; Data augmentation; Hybrid model; Images synthesis; Superresolution; Transformer; Transformer generators; Transformer modeling; Generative adversarial networks","Generative adversarial networks; Hybrid models; Image synthesis; Transformers","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116905959"
"Han S.; Remedios S.; Carass A.; Schär M.; Prince J.L.","Han, Shuo (57202847922); Remedios, Samuel (57201854184); Carass, Aaron (15061054500); Schär, Michael (14065126500); Prince, Jerry L. (56600943200)","57202847922; 57201854184; 15061054500; 14065126500; 56600943200","MR Slice Profile Estimation by Learning to Match Internal Patch Distributions","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12729 LNCS","","","108","119","11","10.1007/978-3-030-78191-0_9","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111434820&doi=10.1007%2f978-3-030-78191-0_9&partnerID=40&md5=4e1e46b1c2f997b6cbec718a079e510c","To super-resolve the through-plane direction of a multi-slice 2D magnetic resonance (MR) image, its slice selection profile can be used as the degeneration model from high resolution (HR) to low resolution (LR) to create paired data when training a supervised algorithm. Existing super-resolution algorithms make assumptions about the slice selection profile since it is not readily known for a given image. In this work, we estimate a slice selection profile given a specific image by learning to match its internal patch distributions. Specifically, we assume that after applying the correct slice selection profile, the image patch distribution along HR in-plane directions should match the distribution along the LR through-plane direction. Therefore, we incorporate the estimation of a slice selection profile as part of learning a generator in a generative adversarial network (GAN). In this way, the slice selection profile can be learned without any external data. Our algorithm was tested using simulations from isotropic MR images, incorporated in a through-plane super-resolution algorithm to demonstrate its benefits, and also used as a tool to measure image resolution. Our code is at https://github.com/shuohan/espreso2. © 2021, Springer Nature Switzerland AG.","Image resolution; Magnetic resonance; Magnetic resonance imaging; Optical resolving power; Adversarial networks; High resolution; In-plane direction; Low resolution; Plane directions; Profile estimation; Super resolution algorithms; Supervised algorithm; Medical imaging","GAN; MRI; Slice profile; Super resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111434820"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","7","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119980895&partnerID=40&md5=f777c74b1952d07fbb261558f490f660","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119980895"
"Angeline R.; Vani R.","Angeline, R. (36709325200); Vani, R. (57207565087)","36709325200; 57207565087","A Novel Unified Scheme for Missing Image Data Suggestion Based on Collaborative Generative Adversarial Network","2021","Lecture Notes on Data Engineering and Communications Technologies","61","","","463","471","8","10.1007/978-981-33-4582-9_36","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106398107&doi=10.1007%2f978-981-33-4582-9_36&partnerID=40&md5=8ecd7bf5c299656b362239e8e357f431","An immense number of the applications are available that necessitate numerous system input data to bring about the wanted or desired results or system outputs. In this case, in case of any data that is lost or absent it announces a huge amount of favoritism or unfairness in the spectrum of the output produced from within the method. Here, we are presenting one algorithm that aids in creating more precise anatomically credible imageries of high dimensions according to acquired medical brain scans having a large gap between the spacing of two corresponding inner image slices. Even in spite of the fact that vast databases containing anatomical images which store a copious amount of data, anatomical procurement parameters produce a result in the form of scattered scans that tend to lose a large part of the anatomical image. The main ambition of this system is to be able to apply previously developed algorithms that were developed for fine resolution scans used for research purposes, to be applied on poorly sampled images. The algorithm alters the problem of anatomical image imputation to an image to image illustration translation task over multiple domains for the purpose that the generator part and the discriminator part of the network is able to recover the lost data from the remaining pure and unsoiled data accumulation. In this envisioned form of the system in place of producing common and general results, the generator part of the network trains itself to learn to generate a counterfeit sample result that is specifically parameterized along with particular conditions. Our goal is to enable application of existing algorithms that were originally developed for high resolution research scans to significantly under-sampled scans. The models we introduce capture fine-scale anatomical similarity across subjects in clinical image collections and use it to fill in the missing data in scans with large slice spacing with good quality as suitable for medical image-based applications. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Engineering; Industrial engineering; Adversarial networks; Anatomical images; Anatomical similarity; Data accumulation; Image-based application; Numerous systems; Particular condition; Research purpose; Medical imaging","Brain scans; Clinical scans; CT prediction; Imputation; Nonlinear descriptor; Super-resolution","Book chapter","Final","","Scopus","2-s2.0-85106398107"
"Wang B.; Zhang S.; Feng Y.; Mei S.; Jia S.; Du Q.","Wang, Baorui (57255138300); Zhang, Shun (56198001500); Feng, Yan (36696426500); Mei, Shaohui (25822578400); Jia, Sen (7202859948); Du, Qian (7202060063)","57255138300; 56198001500; 36696426500; 25822578400; 7202859948; 7202060063","Hyperspectral Imagery Spatial Super-Resolution Using Generative Adversarial Network","2021","IEEE Transactions on Computational Imaging","7","","","948","960","12","10.1109/TCI.2021.3110103","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114738560&doi=10.1109%2fTCI.2021.3110103&partnerID=40&md5=ae42111f52deef50449fba0ec8cd0b9c","Hyperspectral imagery contains both spatial structure information and abundant spectral features of imaged objects. However, due to sensor limitations, abundant spectral information always comes at the sacrifice of low spatial resolution, which brings about difficulties with object analysis and identification. The super-resolution (SR) of HSIs, restored by the traditional interpolation algorithms or the network models trained with the mean-square-error-based loss function, tends to produce over-smoothed images. In this paper, we propose a novel Hyperspectral imagery Spatial Super-Resolution algorithm based on a Generative Adversarial Network (HSSRGAN). The generator network in HSSRGAN consists of two interacting part, i.e., a spatial feature enhanced network (SFEN) and a spectral refined network (SRN), while the discriminator network is employed to predict the probability that the authentic HR image is comparatively more similar than the forged generated image. Concretely, SFEN with the special dense residual blocks is designed to fully extract and enhance more deep hierarchical spatial features of hyperspectral imagery, while SRN is constructed to capture spectral interrelationships and refine the spatial context information so as to increase spatial resolution and alleviate spectral distortion. Moreover, SFEN and SRN are trained by the least-absolute-deviation based loss function to investigate spatial context and the spectral-angle-mapper based loss function to refine spectral information. We validate two versions of our proposed algorithm, 3D-HSSRGAN and 2D-HSSRGAN, on Pavia Centre dataset and Cuprite dataset. Experimental results show that the presented approach is superior to several existing state-of-the-art works.  © 2015 IEEE.","Image resolution; Mean square error; Optical resolving power; Oxide minerals; Remote sensing; Spectroscopy; Adversarial networks; Hyper-spectral imageries; Interpolation algorithms; Least absolute deviations; Spatial structure information; Spectral angle mappers; Spectral information; Super resolution algorithms; Image enhancement","Generative Adversarial Network(GAN); Hyperspectral Image; Spatial Resolution; Spatial Super-resolution","Article","Final","","Scopus","2-s2.0-85114738560"
"Wang J.; Shao Z.; Lu T.; Huang X.; Zhang R.; Wang Y.","Wang, Jiaming (57206676342); Shao, Zhenfeng (57203905559); Lu, Tao (56406646300); Huang, Xiao (57201292422); Zhang, Ruiqian (57190385256); Wang, Yu (57221510339)","57206676342; 57203905559; 56406646300; 57201292422; 57190385256; 57221510339","UNSUPERVISED REMOTING SENSING SUPER-RESOLUTION VIA MIGRATION IMAGE PRIOR","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428093","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126450445&doi=10.1109%2fICME51207.2021.9428093&partnerID=40&md5=897edebf97036f7257f62a583c26d842","Recently, satellites with high temporal resolution have fostered wide attention in various practical applications. Due to limitations of bandwidth and hardware cost, however, the spatial resolution of such satellites is considerably low, largely limiting their potentials in scenarios that require spatially explicit information. To improve image resolution, numerous approaches based on training low-high resolution pairs have been proposed to address the super-resolution (SR) task. Despite their success, however, low/high spatial resolution pairs are usually difficult to obtain in satellites with a high temporal resolution, making such approaches in SR impractical to use. In this paper, we proposed a new unsupervised learning framework, called “MIP”, which achieves SR tasks without low/high resolution image pairs. First, random noise maps are fed into a designed generative adversarial network (GAN) for reconstruction. Then, the proposed method converts the reference image to latent space as the migration image prior. Finally, we update the input noise via an implicit method, and further transfer the texture and structured information from the reference image. Extensive experimental results on the Draper dataset show that MIP achieves significant improvements over state-of-the-art methods both quantitatively and qualitatively. The proposed MIP is open-sourced at https://github.com/jiaming-wang/MIP. © 2021 IEEE","Generative adversarial networks; Image enhancement; Image resolution; Satellites; Textures; Unsupervised learning; Explicit information; Hardware cost; High temporal resolution; Image priors; Latent space; Low-high; Reference image; Spatial resolution; Spatially explicit; Superresolution; Deep neural networks","deep neural networks; latent space; Super-resolution; unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85126450445"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","9","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120444024&partnerID=40&md5=d33fcc85d21c8c2181ca49635ff89853","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85120444024"
"Yamamoto H.; Kitahara D.; Hirabayashi A.","Yamamoto, Hiroya (57221497257); Kitahara, Daichi (55644647300); Hirabayashi, Akira (56036793800)","57221497257; 55644647300; 56036793800","Image super-resolution via generative adversarial network using an orthogonal projection","2021","European Signal Processing Conference","2021-January","","9287515","660","664","4","10.23919/Eusipco47968.2020.9287515","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099276250&doi=10.23919%2fEusipco47968.2020.9287515&partnerID=40&md5=2f8b36107355f8d8f2cbe9fa25e330a7","In this paper, we propose a simple but powerful idea to improve super-resolution (SR) methods based on convolutional neural networks (CNNs). We consider a linear manifold, which is the set of all SR images whose downsampling results are the same as the input image, and apply the orthogonal projection onto this linear manifold in the output layers of the CNNs. The proposed method can guarantee the consistency between the SR image and the input image and reduce the mean squared error. The proposed method is especially effective for SR methods based on generative adversarial networks (GANs), composed of one generator and one discriminator, since the generator can learn high-frequency components while maintaining low-frequency ones. Experiments show the effectiveness of the proposed technique for a GAN-based SR method. Finally we introduce an idea of extension to noisy images. © 2021 European Signal Processing Conference, EUSIPCO. All rights reserved.","Convolutional neural networks; Image resolution; Mean square error; Optical resolving power; Adversarial networks; High frequency components; Image super resolutions; Linear manifold; Low-frequency; Mean squared error; Orthogonal projection; Super resolution; Signal processing","Constrained learning; Generative adversarial network; Orthogonal projection; Single image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85099276250"
"Liu Y.; Zhu L.","Liu, Ying (55976865400); Zhu, Li (57576243200)","55976865400; 57576243200","Face Image Super-resolution Based On Relative Average Generative Adversarial Networks","2021","Proceedings - 2021 2nd Asia Symposium on Signal Processing, ASSP 2021","","","","38","43","5","10.1109/ASSP54407.2021.00014","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128222227&doi=10.1109%2fASSP54407.2021.00014&partnerID=40&md5=7023b11ede64ce0b9702bc35c4305093","Face image plays an important role in visual perception and various computer vision tasks. However, due to the influence factors of equipment and environment, the image often has the problem of low resolution. In order to relieve this problem, this paper proposes a face image super-resolution reconstruction algorithm based on Relative average Generative Adversarial Networks. Different from the standard discriminator D in Generative Adversarial Networks(GAN), which estimates the probability that one input image is real and natural, a relativistic average discriminator tries to predict on average the probability that a real image is relatively more realistic than a fake one. At the same time, the loss function used to measure the spatial similarity of image pixels is combined with the perceptual loss function used to measure the similarity of image feature spaces, so that the network pays attention to the reconstruction of image pixel information while taking into account the feature information of the image. Experimental results demonstrate the effectiveness of the proposed method in multi-scale face image super-resolution, and the evaluation indicators (PSNR and SSIM) tested on common test sets are better than those of the contrast algorithm, it also has better visual perception and more detailed information.  © 2021 IEEE.","Convolutional neural networks; Deep neural networks; Image reconstruction; Optical resolving power; Pixels; Vision; Deep residual network; Face image super-resolution; Face images; Image pixels; Image super resolutions; Image super-resolution reconstruction; Loss functions; Lower resolution; Relative average generative adversarial network; Visual perception; Generative adversarial networks","Deep convolutional neural network; Deep residual network; Face image super-resolution; Relative average generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85128222227"
"Zhou L.; Xia Y.; Liu Z.","Zhou, Luo (57443314600); Xia, Yan (57696663900); Liu, Zhen (57443504500)","57443314600; 57696663900; 57443504500","Super-Resolution Reconstruction of Remote Sensing Images Based on GAN","2021","Proceedings of 2021 IEEE International Conference on Data Science and Computer Application, ICDSCA 2021","","","","270","274","4","10.1109/ICDSCA53499.2021.9649727","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124205370&doi=10.1109%2fICDSCA53499.2021.9649727&partnerID=40&md5=eeb613b46ef8e793b5294be68161c987","In recent years, the methods of super-resolution image reconstruction that based on deep learning have become a hot topic in research of computer vision. The methods of super-resolution image reconstruction that based on the Generative Adversarial Network (GAN) are not controlled in network generation, the models are easy to collapse, the generalization ability is undesirable, and the time complexity degree is too high. To fill these gaps, we propose a super-resolution image reconstruction method based on the GAN of encoding and decoding, which improves the quality of image reconstruction. First of all, our approach uses a design network with regularized structure to avoid model collapse. Then we build a generation network structure that based on encoding and decoding to suppress the uncontrollable defects of GAN network generated images. Finally, in the last layer of the generator, NN convolutional feature layer is included to replace the Softmax layer, which speeds up the training of the model. The experimental results show that the super-resolution remote sensing image reconstructed by the proposed method has higher reconstruction quality and better generalization ability in the DOTA training data sets. At the same time, the image reconstruction process can take much less time. © 2021 IEEE.","Decoding; Deep learning; Encoding (symbols); Image coding; Image enhancement; Image reconstruction; Optical resolving power; Remote sensing; Signal encoding; Coding and decoding; Encoding and decoding; Generalization ability; Hot topics; Image-based; Images reconstruction; In networks; Remote sensing images; Super-resolution image reconstruction; Super-resolution reconstruction; Generative adversarial networks","Coding and Decoding; GAN; Remote Sensing Images; Super Resolution Reconstruction","Conference paper","Final","","Scopus","2-s2.0-85124205370"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","8A-2021","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119984220&partnerID=40&md5=65ce947f5e6f04d5270ca89dc6a03110","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119984220"
"Nan F.; Zeng Q.; Xing Y.; Qian Y.","Nan, Fangzhe (57217150115); Zeng, Qingliang (57217145945); Xing, Yanni (57217145065); Qian, Yurong (26027209500)","57217150115; 57217145945; 57217145065; 26027209500","Single Image Super-Resolution Reconstruction based on the ResNeXt Network","2020","Multimedia Tools and Applications","79","45-46","","34459","34470","11","10.1007/s11042-020-09053-8","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086453079&doi=10.1007%2fs11042-020-09053-8&partnerID=40&md5=da1637decae62f8815ca9eaabf1d14e8","To solve the complex computation, unstable network and slow learning speed problems of a generative adversarial network for image super-resolution (SRGAN), we proposed a single image super-resolution reconstruction model called the Res_WGAN based on ResNeXt. The generator is constructed by the ResNeXt network, which reduced the computational complexity of the model generator to 1/8 that of the SRGAN. The discriminator was constructed by the Wasserstein GAN(WGAN), which solved the SRGAN’s instability. By removing the normalization operation in the residual network, the learning rate is improved. The experimental results from the Res_WGAN demonstrated that the proposed model achieved better performance in the subjective and objective evaluations using four public data sets compared with other state-of-the-art models. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Complex networks; Optical resolving power; Adversarial networks; Complex computation; Image super resolutions; Learning speed; Model generator; Single-image super-resolution reconstruction; State of the art; Subjective and objective evaluations; Image reconstruction","Deep learning; ResNeXt; Single image super-resolution reconstruction; WGAN","Article","Final","","Scopus","2-s2.0-85086453079"
"Lee J.; Lee K.M.","Lee, Jaerin (57221140886); Lee, Kyoung Mu (26642943400)","57221140886; 26642943400","STRUCTURE-RESONANT DISCRIMINATOR FOR IMAGE SUPER-RESOLUTION","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428359","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126437222&doi=10.1109%2fICME51207.2021.9428359&partnerID=40&md5=051c3ea5d29e73878a7cd465fd3986d5","Convolutional neural networks are data models. Their design should embrace the structural properties of the data being modeled, e.g., the natural images. We argue that this also holds for discriminators of adversarial training frameworks for photo-realistic image restoration. We develop this idea to highlight three essential structural features of natural images: translation equivariance, rotation invariance, and hierarchy of scale. The analysis leads to a new discriminator, Structure-Resonant Discriminator (SRD), which can capture image structures in need. The proposed SRD is demonstrated in the perceptual single image super-resolution task. By replacing only the discriminator, our method restores more visually pleasing high-resolution images than the previous state-of-the-art techniques, while exhibits the least distortions. © 2021 IEEE","Computer vision; Convolutional neural networks; Discriminators; Generative adversarial networks; Image reconstruction; Optical resolving power; Restoration; Adversarial training; Convolutional neural network; Equivariance; Group equivariant convolution; Image super resolutions; Image translation; Natural images; Photorealistic images; Structural feature; Training framework; Convolution","adversarial training; discriminator; group equivariant convolution; image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85126437222"
"Pyatigoudar S.; Meena S.M.; Gurlahosur S.V.; Kulkarni U.","Pyatigoudar, Sheetal (57226303004); Meena, S.M. (23987606800); Gurlahosur, Sunil V. (57221379763); Kulkarni, Uday (57192095069)","57226303004; 23987606800; 57221379763; 57192095069","2D-Image Super-Resolution on Heritage Site","2021","Lecture Notes in Electrical Engineering","736 LNEE","","","409","420","11","10.1007/978-981-33-6987-0_34","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111151793&doi=10.1007%2f978-981-33-6987-0_34&partnerID=40&md5=1ecdce208f16b2cd8d952ef244e829c8","One proposed method for image enhancement is single image super-resolution. For this task, many convolutional neural networks-based models were designed. These convolutional neural networks-based models perform better than the other approaches in quality measurements like structural similarity and peak signal-to-noise ratio (PSNR). Resulting super-resolved image quality is dependent on choice of a loss function. Ongoing work is to a great extent dependent on advancing mean squared reconstruction error. But PSNR and structural similarity values cannot give fine details in an image and provide higher values with unsatisfying quality. Hence, generative adversarial networks model was introduced for this problem in recent years. In this paper, image super-resolution (SR) is done with a generative adversarial network (GAN). It is the first method used for 4 × upscaling factors. Proposed approach calculates loss function which is combination of two loss functions like content and adversarial loss. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Convolution; Image enhancement; Image quality; Optical resolving power; Signal to noise ratio; Adversarial networks; Heritage sites; Image super resolutions; Loss functions; Peak signal to noise ratio; Quality measurements; Reconstruction error; Structural similarity; Convolutional neural networks","Generative adversarial network; High-resolution image; Low-resolution image; MSE; PSNR; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85111151793"
"Li G.; Lv J.; Tong X.; Wang C.; Yang G.","Li, Guangyuan (57224164788); Lv, Jun (57222371240); Tong, Xiangrong (56270028700); Wang, Chengyan (56737554800); Yang, Guang (57216243504)","57224164788; 57222371240; 56270028700; 56737554800; 57216243504","High-Resolution Pelvic MRI Reconstruction Using a Generative Adversarial Network with Attention and Cyclic Loss","2021","IEEE Access","9","","9493874","105951","105964","13","10.1109/ACCESS.2021.3099695","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112614872&doi=10.1109%2fACCESS.2021.3099695&partnerID=40&md5=71c3192853aab19aebaf3f4a4b0188fb","Magnetic resonance imaging (MRI) is an important medical imaging modality, but its acquisition speed is quite slow due to the physiological limitations. Recently, super-resolution methods have shown excellent performance in accelerating MRI. In some circumstances, it is difficult to obtain high-resolution images even with prolonged scan time. Therefore, we proposed a novel super-resolution method that uses a generative adversarial network with cyclic loss and attention mechanism to generate high-resolution MR images from low-resolution MR images by upsampling factors of 2\times and 4\times . We implemented our model on pelvic images from healthy subjects as training and validation data, while those data from patients were used for testing. The MR dataset was obtained using different imaging sequences, including T2, T2W SPAIR, and mDIXON-W. Four methods, i.e., BICUBIC, SRCNN, SRGAN, and EDSR were used for comparison. Structural similarity, peak signal to noise ratio, root mean square error, and variance inflation factor were used as calculation indicators to evaluate the performances of the proposed method. Various experimental results showed that our method can better restore the details of the high-resolution MR image as compared to the other methods. In addition, the reconstructed high-resolution MR image can provide better lesion textures in the tumor patients, which is promising to be used in clinical diagnosis. © 2013 IEEE.","Diagnosis; Image reconstruction; Mean square error; Medical imaging; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Attention mechanisms; Clinical diagnosis; High resolution image; Peak signal to noise ratio; Root mean square errors; Structural similarity; Superresolution methods; Magnetic resonance imaging","attention; cyclic loss; generative adversarial network; pelvic; Super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85112614872"
"Ouyang X.; Chen Y.; Agam G.","Ouyang, Xu (57202459862); Chen, Ying (57207798471); Agam, Gady (7005725869)","57202459862; 57207798471; 7005725869","Accelerated WGAN update strategy with loss change rate balancing","2021","Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021","","","","2545","2554","9","10.1109/WACV48630.2021.00259","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116119354&doi=10.1109%2fWACV48630.2021.00259&partnerID=40&md5=c2def36ec2ee9464c4fbd9c777817da6","Optimizing the discriminator in Generative Adversarial Networks (GANs) to completion in the inner training loop is computationally prohibitive, and on finite datasets would result in overfitting. To address this, a common update strategy is to alternate between k optimization steps for the discriminator D and one optimization step for the generator G. This strategy is repeated in various GAN algorithms where k is selected empirically. In this paper, we show that this update strategy is not optimal in terms of accuracy and convergence speed, and propose a new update strategy for networks with Wasserstein GAN (WGAN) group related loss functions (e.g. WGAN, WGAN-GP, Deblur GAN, and Super resolution GAN). The proposed update strategy is based on a loss change ratio comparison of G and D. We demonstrate that the proposed strategy improves both convergence speed and accuracy. © 2021 IEEE.","Computer vision; Change ratio; Convergence speed; Loss functions; Network algorithms; Optimisations; Overfitting; Rate balancing; Superresolution; Generative adversarial networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116119354"
"Ghadekar P.; Joshi S.; Kokate Y.; Kude H.","Ghadekar, Premanand (56637017300); Joshi, Shaunak (35932516400); Kokate, Yogini (57223098954); Kude, Harshada (57223096475)","56637017300; 35932516400; 57223098954; 57223096475","Unsupervised Image Generation and Manipulation Using Deep Convolutional Adversarial Networks","2021","Advances in Intelligent Systems and Computing","1311 AISC","","","33","43","10","10.1007/978-981-33-4859-2_4","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104744812&doi=10.1007%2f978-981-33-4859-2_4&partnerID=40&md5=7d28639e8c9d12f743812e05633919ba","In recent years, there has been an outburst in the field of Computer Vision due to the introduction of Convolutional Neural Networks. However, Convolutional Neural Networks have been sparsely used for unsupervised learning. The advancement of computational power and large datasets provide large opportunities to apply deep learning for image processing. This paper proposes a new architecture based on Deep Convolutional Generative Adversarial Network (DCGAN) for unsupervised image generation, its usage for image manipulation tasks such as denoising, super-resolution, and deconvolution. This proposed model demonstrates that the learned features can be used for image processing tasks—demonstrating their applications for general use as DCGAN learns from large datasets and adds high-level image details and features where traditional methods cannot be used. While the image results from the proposed network architecture and the original DCGAN architecture are similar in terms of performance, they are visually better when viewed by humans. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Convolution; Convolutional neural networks; Deep learning; Large dataset; Learning systems; Network architecture; Unsupervised learning; Adversarial networks; Architecture-based; Computational power; Image details; Image generations; Image manipulation; Large datasets; Super resolution; Image processing","Convolutional neural networks; Generative adversarial networks; Image processing; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85104744812"
"Hu X.; Wang Z.; Liu X.; Li X.; Cheng G.; Gong J.","Hu, Xiaoyan (55496214500); Wang, Zechen (57215133842); Liu, Xiangjun (57215136288); Li, Xinran (57215137323); Cheng, Guang (7401862789); Gong, Jian (55901043400)","55496214500; 57215133842; 57215136288; 57215137323; 7401862789; 55901043400","Exploring real-time super-resolution generative adversarial networks","2021","International Journal of Sensor Networks","36","2","","85","96","11","10.1504/IJSNET.2021.115917","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120720599&doi=10.1504%2fIJSNET.2021.115917&partnerID=40&md5=eab0bb032ce4eac77723f52ba9288374","Image super-resolution is an essential technology for improving user quality of experience of internet videos. As the state-of-the-art deep learning-based super-resolution technology, the enhanced super-resolution generative adversarial networks (ESRGAN) has the best performance in the perceptual quality of reconstructed images, and the efficient sub-pixel convolutional neural network (ESPCN) has the best real-time performance. This work proposes real-time super-resolution generative adversarial network (RTSRGAN). RTSRGAN takes the advantages of ESRGAN and ESPCN so as to simultaneously satisfy the demands on the real-time performance and the resulting pleasant artifacts of super-resolution at the client side. Our experimental studies demonstrate our proposed RTSRGAN can be used for super-resolution at the client side to enhance the real-time performance as well as ensure the image perceptual quality. We also find that RTSRGAN is suitable for restoring imageswith regularly changing texture featureswithout requiring training for individual image categories. Copyright © 2021 Inderscience Enterprises Ltd.","Convolutional neural networks; Deep learning; Generative adversarial networks; Image enhancement; Image reconstruction; Optical resolving power; Pixels; Quality of service; Textures; Convolutional neural network; Efficient sub-pixel convolutional; Efficient sub-pixel convolutional neural network' efficient sub-pixel convolutional neural network; Enhanced super-resolution generative adversarial network; Image super resolutions; Network efficient; Network structures; Real- time; Sub-pixels; Superresolution; Convolution","Efficient sub-pixel convolutional; Enhanced super-resolution generative adversarial networks; ESPCN' efficient sub-pixel convolutional neural network; ESRGAN; Image super-resolution; Network Structure; Real-time","Article","Final","","Scopus","2-s2.0-85120720599"
"Shahsavari A.; Ranjbari S.; Khatibi T.","Shahsavari, Ali (57222147080); Ranjbari, Sima (57221226712); Khatibi, Toktam (56904244000)","57222147080; 57221226712; 56904244000","Corrigendum to “Proposing a novel cascade ensemble super resolution generative adversarial network (CESR-GAN) method for the reconstruction of super-resolution skin lesion images” [Inf Med Unlocked 24 (2021) 100628](S2352914821001180)(10.1016/j.imu.2021.100628)","2021","Informatics in Medicine Unlocked","25","","100667","","","","10.1016/j.imu.2021.100667","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122823269&doi=10.1016%2fj.imu.2021.100667&partnerID=40&md5=84bc36c3e764f0f96bcc5f7561bc07c8","The authors regret for the inconvenience. After reviewing the article, we found problem with Figs. 4 and 5. Fig. 4 on page 7 is correct and should be remained, but Fig. 4. (continued) on page 8 should be eliminated. Moreover, Fig. 5 on page 9 needs modification. It includes two groups of figs, and only the top 15 should be remained, and the other 15 ones at the bottom should be eliminated. The figs in the following show the correct figures. [Figure presented] [Figure presented] The authors would like to apologise for any inconvenience caused. © 2021","erratum","","Erratum","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122823269"
"An R.; Yan F.; Deng T.","An, Rui (57572778200); Yan, Fei (57191609224); Deng, Tao (57357217500)","57572778200; 57191609224; 57357217500","A Generative Adversarial Network with Attention Module for Unpaired Image-to-Image Translation","2021","Proceeding - 2021 China Automation Congress, CAC 2021","","","","2526","2531","5","10.1109/CAC53003.2021.9727816","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128060500&doi=10.1109%2fCAC53003.2021.9727816&partnerID=40&md5=556c8e7e560d5577153e40534c282cae","The Image-to-Image Translation (I2IT) is a challenging image processing task that can be applied to many aspects, such as super-resolution and style transfer. Although several image translation algorithms based on Generative Adversarial Network (GAN) have been proposed, achieving better translation effects still remains a problem worthy of attention. This work proposes a model that fuses an attention module and ResNet-based generator to enhance the performance of I2IT. Using an attention module after the first downsampling, our model can focus more on important low-level semantic features. After the downsampling, the residual blocks provide contextual supplementary information of the photos. The qualitative and quantitative experimental results on unpaired datasets show that our model is better than the SOTA methods, which further confirms the robustness and effectiveness of the proposed model. © 2021 IEEE","Image processing; Semantics; Signal sampling; Attention mechanisms; Down sampling; Image translation; Image-to-image translation; Images processing; Performance; Semantic features; Superresolution; Supplementary information; Translation algorithms; Generative adversarial networks","attention mechanism; generative adversarial network; image-to-image translation","Conference paper","Final","","Scopus","2-s2.0-85128060500"
"Cherian A.K.; Poovammal E.; Rathi Y.","Cherian, Aswathy K. (57216656961); Poovammal, E. (26639841900); Rathi, Yash (57226383373)","57216656961; 26639841900; 57226383373","Improving Image Resolution on Surveillance Images Using SRGAN","2021","Lecture Notes in Networks and Systems","204 LNNS","","","61","76","15","10.1007/978-981-16-1395-1_6","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111384883&doi=10.1007%2f978-981-16-1395-1_6&partnerID=40&md5=15b60e9854d81c03cc7fa0e4d7c6e5ee","The process of generating an image from a single low-resolution input to a high-resolution image is known as single image super-resolution. Image resolution finds its application in images where there involves a blurriness or high brightness in the pictures. The same can be corrected to produce a higher resolution image using generative adversarial networks (GAN), specifically super-resolution GAN (SRGAN). This method uses perceptual losses instead of the traditional peak signal-to-noise ratio (PSNR). The application of SRGAN in the field of CCTV footage is immense as the video from a CCTV camera produces low-resolution images. The low-resolution images are passed through a window where it is sliced into smaller images, overlapping, and then passed through the SRGAN. All the SRGAN produced images are then stitched together to make the complete image. Finally, the contrast limited adaptive histogram equalization (CLAHE) filter is applied to the created image with contrast equalization. The final produced images have high resolution and increase visibility through the pictures’ hazy, foggy parts. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","","CLAHE; Descriptor; Generative adversarial network; Generator; Peak signal-to-noise ratio; SRGAN","Conference paper","Final","","Scopus","2-s2.0-85111384883"
"Chen B.; Li J.; Jin Y.","Chen, Bin (57210117458); Li, Jing (57207737643); Jin, Yufang (7404457584)","57210117458; 57207737643; 7404457584","Deep learning for feature-level data fusion: Higher resolution reconstruction of historical landsat archive","2021","Remote Sensing","13","2","167","1","23","22","10.3390/rs13020167","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099179435&doi=10.3390%2frs13020167&partnerID=40&md5=16cb0d20fb4cfacbe0f3984caadbbf53","Long-term record of fine spatial resolution remote sensing datasets is critical for monitoring and understanding global environmental change, especially with regard to fine scale processes. However, existing freely available global land surface observations are limited by medium to coarse resolutions (e.g., 30 m Landsat) or short time spans (e.g., five years for 10 m Sentinel-2). Here we developed a feature-level data fusion framework using a generative adversarial network (GAN), a deep learning technique, to leverage the overlapping Landsat and Sentinel-2 observations during 2016–2019, and reconstruct 10 m Sentinel-2 like imagery from 30 m historical Landsat archives. Our tests with both simulated data and actual Landsat/Sentinel-2 imagery showed that the GANbased fusion method could accurately reconstruct synthetic Landsat data at an effective resolution very close to that of the real Sentinel-2 observations. We applied the GAN-based model to two dynamic systems: (1) land over dynamics including phenology change, cropping rotation, and water inundation; and (2) human landscape changes such as airport construction, coastal expansion, and urbanization, via historical reconstruction of 10 m Landsat observations from 1985 to 2018. The resulting comparison further validated the robustness and efficiency of our proposed framework. Our pilot study demonstrated the promise of transforming 30 m historical Landsat data into a 10 m Sentinel-2-like archive with advanced data fusion. This will enhance Landsat and Sentinel-2 data science, facilitate higher resolution land cover and land use monitoring, and global change research. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Data fusion; Data Science; Forestry; Image reconstruction; Land use; Metadata; Remote sensing; Adversarial networks; Airport construction; Effective resolutions; Global environmental change; Global land surface; Historical reconstruction; Learning techniques; Spatial resolution; Deep learning","Data fusion; Data reconstruction; GAN; Machine learning; Super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099179435"
"Wang W.; Zhang H.; Yuan Z.; Wang C.","Wang, Wei (57225120529); Zhang, Haochen (57864344000); Yuan, Zehuan (55322757000); Wang, Changhu (17347274000)","57225120529; 57864344000; 55322757000; 17347274000","Unsupervised Real-World Super-Resolution: A Domain Adaptation Perspective","2021","Proceedings of the IEEE International Conference on Computer Vision","","","","4298","4307","9","10.1109/ICCV48922.2021.00428","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124788644&doi=10.1109%2fICCV48922.2021.00428&partnerID=40&md5=99159feab4c9cdf10d0501b07f393f95","Most existing convolution neural network (CNN) based super-resolution (SR) methods generate their paired training dataset by artificially synthesizing low-resolution (LR) images from the high-resolution (HR) ones. However, this dataset preparation strategy harms the application of these CNNs in real-world scenarios due to the inherent domain gap between the training and testing data. A popular attempts towards the challenge is unpaired generative adversarial networks, which generate “real” LR counterparts from real HR images using image-to-image translation and then perform super-resolution from “real” LR→SR. Despite great progress, it is still difficult to synthesize perfect “real” LR images for super-resolution. In this paper, we firstly consider the real-world SR problem from the traditional domain adaptation perspective. We propose a novel unpaired SR training framework based on feature distribution alignment, with which we can obtain degradation-indistinguishable feature maps and then map them to HR images. In order to generate better SR images for target LR domain, we introduce several regularization losses to force the aligned feature to locate around the target domain. Our experiments indicate that our SR network obtains the state-of-the-art performance over both blind and unpaired SR methods on diverse datasets. © 2021 IEEE","Computer vision; Generative adversarial networks; Statistical tests; Convolution neural network; Domain adaptation; High-resolution images; Low resolution images; Lower resolution; Network-based; Real-world; Superresolution; Superresolution methods; Training dataset; Optical resolving power","","Conference paper","Final","","Scopus","2-s2.0-85124788644"
"Zhang W.; Wang G.; Huang M.; Wang H.; Wen S.","Zhang, Weichao (57221865933); Wang, Guanjun (55738664700); Huang, Mengxing (25623006400); Wang, Hongyu (57192213271); Wen, Shaoping (57255459000)","57221865933; 55738664700; 25623006400; 57192213271; 57255459000","Generative Adversarial Networks for Abnormal Event Detection in Videos Based on Self-Attention Mechanism","2021","IEEE Access","9","","","124847","124860","13","10.1109/ACCESS.2021.3110798","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114712754&doi=10.1109%2fACCESS.2021.3110798&partnerID=40&md5=c78e0160a488b133999ee96d18210333","Unsupervised anomaly detection defines an abnormal event as an event that does not conform to expected behavior. In the field of unsupervised anomaly detection, it is a pioneering work that leverages the difference between a future frame predicted by a generative adversarial network and its ground truth to detect an abnormal event. Based on the work, we improve the ability of video prediction framework to detect abnormal events by enhancing the difference between prediction results for normal and abnormal events. We incorporate super-resolution and self-attention mechanism to design a generative adversarial network. We propose an auto-encoder as a generator, which incorporates dense residual networks and self-attention. Moreover, we propose a new discriminator, which introduces self-attention on the basis of a relativistic discriminator. To predict a future frame with higher quality for normal events, we impose a constraint on the motion in video prediction by fusing optical flow and gradient difference between frames. We also introduce a perception constraint in video prediction to enrich the texture details of a frame. The AUC of our method on CUHK Avenue and Shanghai Tech datasets reaches 89.2% and 75.7% respectively, which is better than most existing methods. In addition, we propose a processing flow that can realize real-time anomaly detection in videos. The average running time of our video prediction framework is 37 frames per second. Among all real-time methods for abnormal event detection in videos, our method is competitive with the state-of-the-art methods.  © 2013 IEEE.","Forecasting; Optical flows; Textures; Abnormal event detections; Adversarial networks; Attention mechanisms; Average running time; Frames per seconds; Real-time anomaly detections; State-of-the-art methods; Unsupervised anomaly detection; Anomaly detection","Abnormal event detection; generative adversarial networks (GANs); self-attention; video understanding","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114712754"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","5","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119974675&partnerID=40&md5=df06e0397cf0c08fed1792dd7af93631","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119974675"
"Huang Y.; Jiang Z.; Wang Q.; Jiang Q.; Pang G.","Huang, Yongsong (57214838902); Jiang, Zetao (24512367900); Wang, Qingzhong (57209217850); Jiang, Qi (57216755792); Pang, Guoming (57271443900)","57214838902; 24512367900; 57209217850; 57216755792; 57271443900","Infrared Image Super-Resolution via Heterogeneous Convolutional WGAN","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13032 LNAI","","","461","472","11","10.1007/978-3-030-89363-7_35","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119336194&doi=10.1007%2f978-3-030-89363-7_35&partnerID=40&md5=76d0ba3806088d01010ba8bc8d086b7a","Image super-resolution is important in many fields, such as surveillance and remote sensing. However, infrared (IR) images normally have low resolution since the optical equipment is relatively expensive. Recently, deep learning methods have dominated image super-resolution and achieved remarkable performance on visible images; however, IR images have received less attention. IR images have fewer patterns, and hence, it is difficult for deep neural networks (DNNs) to learn diverse features from IR images. In this paper, we present a framework that employs heterogeneous convolution and adversarial training, namely, heterogeneous kernel-based super-resolution Wasserstein GAN (HetSRWGAN), for IR image super-resolution. The HetSRWGAN algorithm is a lightweight GAN architecture that applies a plug-and-play heterogeneous kernel-based residual block. Moreover, a novel loss function that employs image gradients is adopted, which can be applied to an arbitrary model. The proposed HetSRWGAN achieves consistently better performance in both qualitative and quantitative evaluations. According to the experimental results, the whole training process is more stable. © 2021, Springer Nature Switzerland AG.","Deep neural networks; Generative adversarial networks; Infrared imaging; Optical data processing; Optical resolving power; Remote sensing; Heterogeneous kernel-based convolution; Image super resolutions; Images processing; Learning methods; Lower resolution; Optical equipment; Performance; Remote-sensing; Superresolution; Visible image; Convolution","Generative adversarial networks; Heterogeneous kernel-based convolution; Image processing; Infrared image; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119336194"
"","","","International Conference on Machine Learning, Deep Learning and Computational Intelligence for Wireless Communication, MDCWC 2020","2021","Lecture Notes in Electrical Engineering","749 LNEE","","","","","643","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111156298&partnerID=40&md5=6d1566063848fd1f58c4cb8ae7723c83","The proceedings contain 46 papers. The special focus in this conference is on Machine Learning, Deep Learning and Computational Intelligence for Wireless Communication. The topics include: Secure, Efficient, Lightweight Authentication in Wireless Sensor Networks; performance Evaluation of Logic Gates Using Magnetic Tunnel Junction; medical IoT—Automatic Medical Dispensing Machine; Performance Analysis of Digital Modulation Formats in FSO; High-Level Synthesis of Cellular Automata–Belousov Zhabotinsky Reaction in FPGA; ioT-Based Calling Bell; development of an Ensemble Gradient Boosting Algorithm for Generating Alerts About Impending Soil Movements; seam Carving Detection and Localization Using Two-Stage Deep Neural Networks; LSTM Network for Hotspot Prediction in Traffic Density of Cellular Network; a Machine Learning-Based Approach to Password Authentication Using Keystroke Biometrics; Attention-Based SRGAN for Super Resolution of Satellite Images; detection of Acute Lymphoblastic Leukemia Using Machine Learning Techniques; computer-Aided Classifier for Identification of Renal Cystic Abnormalities Using Bosniak Classification; recognition of Obscure Objects Using Super Resolution-Based Generative Adversarial Networks; low-Power U-Net for Semantic Image Segmentation; electrocardiogram Signal Classification for the Detection of Abnormalities Using Discrete Wavelet Transform and Artificial Neural Network Back Propagation Algorithm; performance Analysis of Optimizers for Glaucoma Diagnosis from Fundus Images Using Transfer Learning; machine Learning based Early Prediction of Disease with Risk Factors Data of the Patient Using Support Vector Machines; scene Classification of Remotely Sensed Images using Ensembled Machine Learning Models; generative Adversarial Network and Reinforcement Learning to Estimate Channel Coefficients; fuzziness and Vagueness in Natural Language Quantifiers: Searching and Systemizing Few Patterns in Predicate Logic; a Novel Deep Hybrid Spectral Network for Hyperspectral Image Classification.","","","Conference review","Final","","Scopus","2-s2.0-85111156298"
"Sun J.; Yang Z.; Huang S.","Sun, Jinguang (24723042300); Yang, Zhongwei (57221524119); Huang, Sheng (57226043940)","24723042300; 57221524119; 57226043940","Image inpainting model with consistent global and local attributes; [全局与局部属性一致的图像修复模型]","2020","Journal of Image and Graphics","25","12","","2505","2516","11","10.11834/jig.190681","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099386957&doi=10.11834%2fjig.190681&partnerID=40&md5=4355e1c1002edf9ca56261934c6fd270","Objective: Image inpainting is a hot research topic in computer vision. In recent years, this task has been considered a conditional pattern generation problem in deep learning that has received much attention from researchers. Compared with traditional algorithms, deep-learning-based image inpainting methods can be used in more extensive scenarios with better inpainting effects. Nevertheless, these methods have limitations. For instance, their image inpainting results need to be improved in terms of semantic rationality, structural coherence, and detail accuracy when processing the close association among global and local attributed images, especially when dealing with images involving a large defect area. This paper proposes a novel image inpainting model based on the fully convolutional neural network and the idea of generative adversarial network to solve the above problems. This model optimizes the network structure, loss constraints, and training strategies to obtain improved image inpainting effects. Method: First, this paper proposes a novel image inpainting network as a generator to repair defective images by using effective methods in the field of image processing. A network framework based on a fully convolutional neural network is then built in the form of an encoder-decoder. For instance, we replace part of convolutional layers in the network decoding stage with dilated convolution. We also apply dilated convolution superposition with multiple dilation rates to obtain a larger input image area compared with ordinary convolution in small-size feature graphs and then effectively increase the receptive field of the convolution kernel without increasing the calculation amount to develop a better understanding of images. We also set long-skip connections in the corresponding stage of encoding-decoding. This connection strengthens the structural information by transmitting low-level features to the decoding stage. The setting enhances the correlation among deep features and reduces the difficulties in network training. Second, we introduce structural similarity (SSIM) as the reconstruction loss of image inpainting. This image quality evaluation index is built from the perspective of the human visual perception system and differs from the common mean square error (MSE) loss per pixel. This index comprehensively evaluates via an experiment the similarity between two images in their brightness, contrast, and structure. Structural similarity, as the reconstruction loss of an image, can effectively improve the visual effects of image inpainting results. We use the improved global and local context discriminator as a two-way discriminator to determine the authenticity of the inpainting results. The global context discriminator guarantees the consistency of attributes between the image inpainting area and the entire image, whereas the local context discriminator improves the detailed performance of the image inpainting area. Combined with adversarial loss, this paper proposes a joint loss to improve the performance of the model and reduce the difficulties in its training. By drawing lessons from the training mode of generative adversarial networks, we presents a novel method to alternately train image inpainting network and image discriminative network, which obtains an ideal result. In practical applications, we only use image inpainting network to repair defective images. Result: To verify the effectiveness of the proposed image inpainting model, we compare the image inpainting effect of this model with that of mainstream image inpainting algorithms on the CelebA-HQ dataset by using subjective perception and objective indicators. To achieve the best inpainting effect in controlled experiments, we use official versions of codes and examples. The image inpainting result is taken from loading pre-training files or online demos. We place the specific defect mask onto 50 randomly selected images as test cases and then apply different image inpainting algorithms to repair and collect statistics for the comparison. The CelebA-HQ dataset is a cropped and super-resolution reconstructed version of the CelebA dataset, which contains 30 000 high-resolution face images. The human face represents a special image that not only contains specific features but also an infinite amount of details. Therefore, face images can fully test the expressiveness of the image inpainting method. Considering the algorithm consistent attribute of the global and local images in the controlled experiment, experiment results show that the image inpainting model demonstrates some improvements in its semantic rationality, structural coherence, and detail performance compared with other algorithms. Subjectively, this model has a natural edge transition and a very detailed image inpainting area. Objectively, this model has a peak signal-to-noise ratio(PSNR), and SSIM of 31.30 dB and 90.58% on average, respective, both of which exceed those of mainstream deep learning-based image inpainting algorithms. To verify its generality, we test the image inpainting model on the Places2 dataset. Conclusion: This paper proposes a novel image inpainting model that shows improvements in terms of network structure, cost, training strategy, and image inpainting results. This model also provides a better understanding of the high-level semantics of images. Given its highly accurate context and details, the proposed model obtains better image inpainting results from human visual perception. We will continue to improve the effect of image inpainting and explore the conditional image inpainting task in the future. Our plan is to improve and optimize this model in terms of network structure and loss constraint to reduce losses in an image during the feature extraction process under a controllable network training setup. We shall also try to make the defect mask do more work with channel domain attention mechanism to further improve the quality of image inpainting. We also plan to analyze the relationship between image boundary structure and feature reconstruction. We aim to improve the convergence speed of network training and the quality of image inpainting by using an accurate and effective loss function. Furthermore, we would use human-computer interaction or presupposed condition to affect the results of image inpainting, which explores more practical values of the model. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","Adversarial loss; Dilated convolution; Fully convolutional neural network; Image inpainting; Skip connection","Article","Final","","Scopus","2-s2.0-85099386957"
"Niu L.; Duan L.; Dong X.; Zhou Z.","Niu, Liming (57260592800); Duan, Liya (15750270200); Dong, Xiangliang (57260486700); Zhou, Zhonghai (55496548700)","57260592800; 15750270200; 57260486700; 55496548700","Underwater image enhancement and super-resolution restoration based on generative adversarial networks","2021","Proceedings of the International Offshore and Polar Engineering Conference","","","","153","160","7","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115006310&partnerID=40&md5=10a690880010a9d958b2acb3487151b6","As an important carrier and presentation form of underwater information, underwater images play an irreplaceable role in underwater environment detection. However, due to the impact of the ocean, underwater images often have quality degradation such as low resolution, blurred details, color distortion, and poor clarity. In response to the above problems, we designed a model based on Generative Adversarial Networks, which fuse underwater white balance images to achieve color correction and super-resolution of underwater images. We have conducted qualitative and quantitative experiments compared with other methods and proved that the method proposed in this article is superior to other methods in terms of visual effects. © 2021 by the International Society of Offshore and Polar Engineers (ISOPE).","Arctic engineering; Image reconstruction; Optical resolving power; Adversarial networks; Color correction; Color distortions; Quality degradation; Quantitative experiments; Super resolution; Super-resolution restoration; Underwater environments; Image enhancement","Color correction; Neural networks; ResNeXt; Super resolution; Underwater image enhancement","Conference paper","Final","","Scopus","2-s2.0-85115006310"
"Gao L.; Sun H.-M.; Cui Z.; Du Y.-B.; Sun H.-B.; Jia R.-S.","Gao, Li (57221482568); Sun, Hong-Mei (55729286100); Cui, Zhe (57218325391); Du, Yan-Bin (57221413033); Sun, Hai-Bin (57207025120); Jia, Rui-Sheng (25927894300)","57221482568; 55729286100; 57218325391; 57221413033; 57207025120; 25927894300","Super-resolution reconstruction of single remote sensing images based on residual channel attention","2021","Journal of Applied Remote Sensing","15","1","016513","","","","10.1117/1.JRS.15.016513","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103623109&doi=10.1117%2f1.JRS.15.016513&partnerID=40&md5=4933992be2b6797c799b6325aa5ce724","The existing methods of remote sensing image super-resolution reconstruction based on deep learning have some problems, such as insufficient feature extraction abilities, blurred image edges, and difficult model training. To solve these problems, a super-resolution reconstruction method combining residual channel attention (CA) is proposed. Based on the framework of generative adversarial networks, the residual structure is designed to enhance the ability of deep feature extraction ability for remote sensing images. The CA module is added to extract the deep feature information of remote sensing images, and the shallow features and deep features are fused using the skip connection. The perceptual loss function is combined with the loss function represented by the Wasserstein distance to improve the stability of model training. The experimental results show that this method is superior to the comparison algorithms in the objective evaluation criteria of the peak-signal-To-noise ratio and structural similarity of the reconstructed remote sensing images. After optimizing the model training process, the reconstructed remote sensing images are visually clearer and have sharper edges. © 2021 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Extraction; Feature extraction; Image enhancement; Image reconstruction; Optical resolving power; Signal to noise ratio; Adversarial networks; Feature information; Objective evaluation criteria; Peak signal to noise ratio; Remote sensing images; Structural similarity; Super resolution reconstruction; Wasserstein distance; Remote sensing","generative adversarial network; remote sensing image; residual channel attention; super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85103623109"
"Zhu H.; Han G.; Peng Y.; Zhang W.; Lin C.; Zhao H.","Zhu, Hongbo (56925720900); Han, Guangjie (7202923237); Peng, Yan (36617766500); Zhang, Wenbo (8716402400); Lin, Chuan (56158751000); Zhao, Hai (57210249766)","56925720900; 7202923237; 36617766500; 8716402400; 56158751000; 57210249766","Functional-realistic CT image super-resolution for early-stage pulmonary nodule detection","2021","Future Generation Computer Systems","115","","","475","485","10","10.1016/j.future.2020.09.020","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092273392&doi=10.1016%2fj.future.2020.09.020&partnerID=40&md5=78f1cf576a6530922e51eb70b5bdb8e1","Early-stage pulmonary nodule detection is challenging for Computer-aided Diagnosis systems (CADs) in clinical practice. It always relies on large-scale annotated pathological images. Unfortunately, the limited voxels of earlier-stage nodules can aggravate the risk of escaping diagnosis. Due to high-dose CT and bronchoscope potential threats, CT image super-resolution has become a suboptimal way to tackle the problem. Therefore, we proposed a deep generative adversarial network (GAN) architecture based on a deep grammar model, called FRGAN (Functional-Realistic GAN). By using region proposal network (RPN), the bottom semantic features are recommended and classified as the basic units of functional structure. Local pathological images can be hierarchically aggregated with corresponding to different semantic patterns as parse trees. Refer to their EMRs, and we use TreeGAN to generate the correct syntax patterns for each early-stage pulmonary nodule candidates. We report the generated results of the super-resolution images and feed them into a convolutional network to assess the functional loss of the generated results along to different parse trees. Within the contextual and generative losses, we rebuild a novel objective function paralleling with TreeGAN. The aim is to boost the sensibility of pulmonary nodule detection with more functional-realistic data augmentation. Experimental results show that our generator can faster generate more realistic SR images with pathological features. Moreover, it could be a data augmentation tool for some deep architecture to overcome sample imbalance. © 2020","Computer aided diagnosis; Convolutional neural networks; Forestry; Image resolution; Network architecture; Optical resolving power; Semantics; Adversarial networks; Clinical practices; Computer aided diagnosis systems; Convolutional networks; Functional structure; Objective functions; Pathological images; Pulmonary nodule detection; Computerized tomography","CADs; Data augmentation; Pulmonary nodule detection; Semantic patterns; Super resolution","Article","Final","","Scopus","2-s2.0-85092273392"
"Chen X.; Yang J.","Chen, Xikun (57395533600); Yang, Junmei (16302434000)","57395533600; 16302434000","Speech Bandwidth Extension Based on Wasserstein Generative Adversarial Network","2021","International Conference on Communication Technology Proceedings, ICCT","2021-October","","","1356","1362","6","10.1109/ICCT52962.2021.9658055","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124420393&doi=10.1109%2fICCT52962.2021.9658055&partnerID=40&md5=f4bdf9523d0af18a67ed0dec172714c9","Artificial bandwidth extension (ABE) algorithms have been developed to improve the quality of narrowband calls before devices are upgraded to wideband calls. Most methods use deep neural networks (DNN) to establish a nonlinear relationship between narrowband components and wideband components, so as to predict high frequency components from narrowband. Although traditional convolutional deep neural networks (CNN) trained on the minimum mean square error (MSE) can bring high peak Signal to Noise Ratio (SNR), it usually lacks high frequency details and has poor generalization. In this paper, we propose a speech signal super-resolution Wasserstein generative adversarial network (SRWGAN). In this paper, we propose a new speech signal super resolution method based on Wasserstein generated adversarial network, where a network joined with adversarial learning is designed and a perceptual loss function including adversarial loss and regression loss is derived. The simulation results shows that the proposed scheme is better than the traditional minimum mean square error training network in predicting high frequency components. © 2021 IEEE.","Bandwidth; Convolutional neural networks; Deep neural networks; Mean square error; Optical resolving power; Signal to noise ratio; Speech communication; Audio super-resolution; Bandwidth extension; Deep learning; High frequency components; Higher-frequency components; Narrow bands; Speech quality; Speech signals; Superresolution; Wide-band; Generative adversarial networks","audio super-resolution; bandwidth extension; deep learning; generative adversarial network; speech quality","Conference paper","Final","","Scopus","2-s2.0-85124420393"
"","","","2021 IEEE International Workshop on Information Forensics and Security, WIFS 2021","2021","2021 IEEE International Workshop on Information Forensics and Security, WIFS 2021","","","","","","163","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124164113&partnerID=40&md5=102de6655f81b771ea175222582af8f1","The proceedings contain 27 papers. The topics discussed include: fusing multiscale texture and residual descriptors for multilevel 2D barcode rebroadcasting detection; robust image hashing for detecting small tampering using a hyperrectangular region; differentially private generative adversarial networks with model inversion; mobile authentication of copy detection patterns: how critical is to know fakes?; quality of service guarantees for physical unclonable functions; spoofing speaker verification with voice style transfer and reconstruction loss; impact of super-resolution and human identification in drone surveillance; and compare before you buy: privacy-preserving selection of threat intelligence providers.","","","Conference review","Final","","Scopus","2-s2.0-85124164113"
"El-Shafai W.; Ali A.M.; El-Rabaie E.-S.M.; Soliman N.F.; Algarni A.D.; Abd El-Samie F.E.","El-Shafai, Walid (55819860600); Ali, Anas M. (57198724497); El-Rabaie, El-Sayed M. (42161378200); Soliman, Naglaa F. (7003625335); Algarni, Abeer D. (57204971671); Abd El-Samie, Fathi E. (12785222000)","55819860600; 57198724497; 42161378200; 7003625335; 57204971671; 12785222000","Automated COVID-19 detection based on single-image super-resolution and CNN models","2021","Computers, Materials and Continua","70","1","","1141","1157","16","10.32604/cmc.2022.018547","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114556475&doi=10.32604%2fcmc.2022.018547&partnerID=40&md5=395095b7b30ce53e2f8beb602af95f8c","In developing countries, medical diagnosis is expensive and time consuming. Hence, automatic diagnosis can be a good cheap alternative. This task can be performed with artificial intelligence tools such as deep Convolutional Neural Networks (CNNs). These tools can be used on medical images to speed up the diagnosis process and save the efforts of specialists. The deep CNNs allow direct learning from the medical images. However, the accessibility of classified data is still the largest challenge, particularly in the field of medical imaging. Transfer learning can deliver an effective and promising solution by transferring knowledge from universal object detection CNNs to medical image classification. However, because of the inhomogeneity and enormous overlap in intensity between medical images in terms of features in the diagnosis of Pneumonia and COVID-19, transfer learning is not usually a robust solution. Single-Image Super-Resolution (SISR) can facilitate learning to enhance computer vision functions, apart from enhancing perceptual image consistency. Consequently, it helps in showing the main features of images. Motivated by the challenging dilemma of Pneumonia and COVID-19 diagnosis, this paper introduces a hybrid CNN model, namely SIGTra, to generate super-resolution versions of X-ray and CT images. It depends on a Generative Adversarial Network (GAN) for the super-resolution reconstruction problem. Besides, Transfer learning with CNN (TCNN) is adopted for the classification of images. Three different categories of chest X-ray and CT images can be classified with the proposed model. A comparison study is presented between the proposed SIGTra model and the other related CNN models for COVID-19 detection in terms of precision, sensitivity, and accuracy. © 2021 Tech Science Press. All rights reserved.","Computerized tomography; Convolutional neural networks; Deep neural networks; Developing countries; Diagnosis; Image classification; Medical imaging; Object detection; Optical resolving power; Transfer learning; X rays; Adversarial networks; Artificial intelligence tools; Automatic diagnosis; Comparison study; Robust solutions; Super resolution; Super resolution reconstruction; Vision functions; Image enhancement","CT and X-ray images; GAN; Medical images; SIGTra; SISR; TCNN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114556475"
"Guo L.; Zhang Y.; Li Y.","Guo, Lantu (57208854972); Zhang, Yan (55961814300); Li, Yue (57207782058)","57208854972; 55961814300; 57207782058","An intelligent electromagnetic environment reconstruction method based on super-resolution generative adversarial network","2021","Physical Communication","44","","101253","","","","10.1016/j.phycom.2020.101253","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097357098&doi=10.1016%2fj.phycom.2020.101253&partnerID=40&md5=7315139a55caf2801331fa5530914c20","In this paper, an intelligent electromagnetic environment reconstruction method is proposed based on a super-resolution generative adversarial network (SRGAN). The altitude matrices together with the low-resolution matrices obtained by measured power values are employed as inputs. Then, an electromagnetic environment reconstruction method capable of generating the high-resolution power coverage matrix in the selected area is designed. Data enhancement is employed to expand the dataset and a modified generator network with squeeze and excitation modules is used during the training process. To validate the proposed method, the simulation analyses are carried out in typical suburb environments based on a ray-tracing tool. Numerical results indicate that, compared with classical methods such as nearest-neighbor interpolation, bilinear interpolation, and bicubic interpolation, the proposed method can provide more accurate reconstruction results for power coverage. In addition, the peak signal to noise ratio (PSNR) of the proposed method is higher than those of the classical methods. The proposed intelligent electromagnetic environment reconstruction method can be useful for the planning, deployment, and optimization of wireless networks. © 2020","Interpolation; Optical resolving power; Signal to noise ratio; Adversarial networks; Bicubic interpolation; Bilinear interpolation; Electromagnetic environments; Nearest neighbor interpolation; Numerical results; Peak signal to noise ratio; Simulation analysis; Numerical methods","Electromagnetic environment; Intelligent reconstruction; Network planning; Power coverage; Super-resolution generative adversarial network (SRGAN)","Article","Final","","Scopus","2-s2.0-85097357098"
"Almalioglu Y.; Bengisu Ozyoruk K.; Gokce A.; Incetan K.; Irem Gokceler G.; Ali Simsek M.; Ararat K.; Chen R.J.; Durr N.J.; Mahmood F.; Turan M.","Almalioglu, Yasin (57197764273); Bengisu Ozyoruk, Kutsev (57219768168); Gokce, Abdulkadir (57219759606); Incetan, Kagan (57214363192); Irem Gokceler, Guliz (57219760594); Ali Simsek, Muhammed (57220125290); Ararat, Kivanc (57218291464); Chen, Richard J. (57202356119); Durr, Nicholas J. (16303771500); Mahmood, Faisal (56647751100); Turan, Mehmet (56585517200)","57197764273; 57219768168; 57219759606; 57214363192; 57219760594; 57220125290; 57218291464; 57202356119; 16303771500; 56647751100; 56585517200","EndoL2H: Deep Super-Resolution for Capsule Endoscopy","2020","IEEE Transactions on Medical Imaging","39","12","9167261","4297","4309","12","10.1109/TMI.2020.3016744","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097003176&doi=10.1109%2fTMI.2020.3016744&partnerID=40&md5=8b1a27dc112c1bd22901627cacfd0ca4","Although wireless capsule endoscopy is the preferred modality for diagnosis and assessment of small bowel diseases, the poor camera resolution is a substantial limitation for both subjective and automated diagnostics. Enhanced-resolution endoscopy has shown to improve adenoma detection rate for conventional endoscopy and is likely to do the same for capsule endoscopy. In this work, we propose and quantitatively validate a novel framework to learn a mapping from low-to-high-resolution endoscopic images. We combine conditional adversarial networks with a spatial attention block to improve the resolution by up to factors of 8×, 10×, 12×, respectively. Quantitative and qualitative studies demonstrate the superiority of EndoL2H over state-of-the-art deep super-resolution methods Deep Back-Projection Networks (DBPN), Deep Residual Channel Attention Networks (RCAN) and Super Resolution Generative Adversarial Network (SRGAN). Mean Opinion Score (MOS) tests were performed by 30 gastroenterologists qualitatively assess and confirm the clinical relevance of the approach. EndoL2H is generally applicable to any endoscopic capsule system and has the potential to improve diagnosis and better harness computational approaches for polyp detection and characterization. Our code and trained models are available at https://github.com/CapsuleEndoscope/EndoL2H.  © 1982-2012 IEEE.","Capsule Endoscopy; Optical resolving power; Adversarial networks; Automated diagnostics; Camera resolutions; Computational approach; Enhanced resolutions; Mean opinion scores; Superresolution methods; Wireless capsule endoscopy; Article; capsule endoscopy; controlled study; diagnostic procedure; gastroenterologist; high resolution endoscopy; human; qualitative research; quantitative analysis; Endoscopy","Capsule endoscopy; conditional generative adversarial network; spatial attention network; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097003176"
"Wang C.; Ruifei Z.; Bai Y.; Zhang P.; Fan H.","Wang, Chao (57222581465); Ruifei, Zhu (56646483700); Bai, Yang (57695846100); Zhang, Peng (57750475600); Fan, Haiyang (57297816400)","57222581465; 56646483700; 57695846100; 57750475600; 57297816400","Single-frame super-resolution for high resolution optical remote-sensing data products","2021","International Journal of Remote Sensing","42","21","","8099","8123","24","10.1080/01431161.2021.1971790","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117227562&doi=10.1080%2f01431161.2021.1971790&partnerID=40&md5=57a39bea9cd92f2d85fce3864c3bf395","The resolution of remote-sensing images is directly related to the application value of data products. Due to differences in imaging characteristics between digital cameras and remote-sensing cameras, the existing network models cannot get the best super resolution (SR) results of satellite images. In response to the requirements of remote-sensing image production, we propose a single image super-resolution (SISR) reconstruction method for specific type of remote-sensing satellite. First, we measure and model the imaging degradation phenomenon of remote-sensing satellite, including the image blur and noise model. Then, high-quality aerial images are down-sampled and degraded to construct paired training image datasets. We chose the most popular Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) as the basic structure and optimized the number of Residual-in-Residual Dense Block (RRDB) modules to further improve the processing efficiency. Finally, we perform a series of quantitative measurements of the SR image results, including image interpretation capability, reconstruction accuracy, ground resolution distance, and data processing efficiency, using higher resolution remote-sensing images as the benchmark. The experimental results demonstrate that our proposed method has higher interpretation capability and reconstruction accuracy for the SR processing of specific type remote-sensing satellite. Our proposed method is evaluated within a real satellite image product, that demonstrated it has the capability of pipeline super-resolution processing of remote sensing data products. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Antennas; Data handling; Image reconstruction; Optical resolving power; Pipeline processing systems; Remote sensing; Satellites; Data products; Imaging characteristics; Optical remote sensing data; Reconstruction accuracy; Remote sensing cameras; Remote sensing images; Remote sensing satellites; Satellite images; Single frame super resolutions; Superresolution; digital image; image resolution; quantitative analysis; remote sensing; satellite data; satellite imagery; Efficiency","","Article","Final","","Scopus","2-s2.0-85117227562"
"","","","2021 4th International Conference on Robotics, Control and Automation Engineering, RCAE 2021","2021","2021 4th International Conference on Robotics, Control and Automation Engineering, RCAE 2021","","","","","","453","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123787059&partnerID=40&md5=9f0cf2833388fd89f269be7e103c2e21","The proceedings contain 82 papers. The topics discussed include: research on mouse detection method in infrared night vision; image recognition based on improved recurrent attention network; fish recognition based on deep residual shrinkage network; butterfly image generation and recognition based on improved generative adversarial networks; cotton boll growth status recognition method under complex background based on semantic segmentation; enhanced lightweight neural networks for plant disease classification; face super-resolution via meta-learning optimization network; and video image stabilization algorithm for remote control driving.","","","Conference review","Final","","Scopus","2-s2.0-85123787059"
"Jiang X.; Zhao R.; Song W.; Liu Y.","Jiang, Xu (57868330400); Zhao, Rongcai (57738358200); Song, Wenqi (57869183000); Liu, Yongjie (57737776800)","57868330400; 57738358200; 57869183000; 57737776800","Research on Image Super-resolution Reconstruction Algorithm Based on Improved Generative Adversarial Network","2021","ICMLCA 2021 - 2nd International Conference on Machine Learning and Computer Application","","","","442","449","7","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137016773&partnerID=40&md5=89b39e17375012a78d94ede894e2cc73","Aiming at the problem that existing image super-resolution reconstruction algorithm, the problems of network training difficulties and unclear texture details of the generated image appear too smooth and lack of authenticity. This paper proposes an improved image super-resolution reconstruction algorithm based on the super-resolution algorithm for generating confrontation networks (SRGAN). First, for generation network structure, the original residual block is replaced with a densely connected residual block, the batch normalization layer removed reduce computational complexity. Second, VGG-19 network used as the basic structure of the discriminant network, adopt average pooling instead of the original fully connected layer to prevent overfitting. In the third loss function: the perceptual loss function, the adversarial loss function, and the content loss function are introduced to form the total objective function of the generator to optimize the model. The improved algorithm uses Charbonnier loss as the content loss function to evaluate the similarity between the generated image and the real image, and uses WGAN-GP theory to optimize model's anti-loss to accelerate the convergence. Experimental results shows the super-resolution reconstruction algorithm proposed our paper superior to current representative algorithms in subjective and objective image evaluation indexes, and it can better synthesize the texture details of the image. © VDE VERLAG GMBH · Berlin · Offenbach.","Function evaluation; Image enhancement; Image reconstruction; Image texture; Optical resolving power; Restoration; Textures; Basic structure; Image super-resolution reconstruction; Loss functions; Network structures; Network training; Normalisation; Objective functions; Overfitting; Reconstruction algorithms; Super resolution algorithms; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85137016773"
"Kandarpa V.S.S.; Bousse A.; Benoit D.; Visvikis D.","Kandarpa, V.S.S. (57221631873); Bousse, Alexandre (15842886700); Benoit, Didier (55334728100); Visvikis, Dimitris (7003382412)","57221631873; 15842886700; 55334728100; 7003382412","DUG-RECON: A Framework for Direct Image Reconstruction Using Convolutional Generative Networks","2021","IEEE Transactions on Radiation and Plasma Medical Sciences","5","1","9235522","44","53","9","10.1109/TRPMS.2020.3033172","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109775506&doi=10.1109%2fTRPMS.2020.3033172&partnerID=40&md5=d54b7aed9819975a64d5afe8f3302bb5","This article explores convolutional generative networks as an alternative to iterative reconstruction algorithms in medical image reconstruction. The task of medical image reconstruction involves mapping of projection domain data collected from the detector to the image domain. This mapping is done typically through iterative reconstruction algorithms which are time consuming and computationally expensive. Trained deep learning networks provide faster outputs as proven in various tasks across computer vision. In this work, we propose a direct reconstruction framework exclusively with deep learning architectures. The proposed framework consists of three segments, namely, denoising, reconstruction, and super resolution (SR). The denoising and the SR segments act as processing steps. The reconstruction segment consists of a novel double U-Net generator (DUG) which learns the sinogram-to-image transformation. This entire network was trained on positron emission tomography (PET) and computed tomography (CT) images. The reconstruction framework approximates 2-D mapping from the projection domain to the image domain. The architecture proposed in this proof-of-concept work is a novel approach to direct image reconstruction; further improvement is required to implement it in a clinical setting.  © 2017 IEEE.","Computerized tomography; Convolution; Convolutional neural networks; Deep learning; Image enhancement; Image segmentation; Iterative methods; Mapping; Medical image processing; Network architecture; Optical resolving power; Positron emission tomography; Clinical settings; Image transformations; Iterative reconstruction algorithms; Learning architectures; Positron emission tomography (PET); Projection domain; Reconstruction frameworks; Two Dimensional (2 D); Image reconstruction","Deep learning; generative adversarial networks (GANs); medical image reconstruction","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85109775506"
"Su K.; Li X.; Li X.","Su, Kai (57214230200); Li, Xueming (56031046700); Li, Xuewei (57204037258)","57214230200; 56031046700; 57204037258","Single Image Super-Resolution Based on Generative Adversarial Networks","2021","Communications in Computer and Information Science","1314 CCIS","","","3","16","13","10.1007/978-981-33-6033-4_1","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107333579&doi=10.1007%2f978-981-33-6033-4_1&partnerID=40&md5=e8d0f05af1eae79eafb3d9b12fa37ac2","Traditional super-resolution algorithms are computationally intensive and the quality of generated images is not high. In recent years, due to the superiority of GAN in generating image details, it has begun to attract researchers’ attention. This paper proposes a new GAN-based image super-resolution algorithm, which solves the problems of the current GAN-based super-resolution algorithm that the generated image quality is not high and the multi-scale feature information is not processed enough. In the proposed algorithm, a densely connected dilated convolution network module with different dilation rate is added to enhance the network’s ability to generate image features at different scale levels; a channel attention mechanism is introduced in the network to adaptively select the generated image features, which improves the quality of the generated image on the network. After conducting experiments on classic test datasets, the proposed algorithm has improved PSNR and SSIM compared to ESRGAN. © 2020, Springer Nature Singapore Pte Ltd.","Optical resolving power; Adversarial networks; Attention mechanisms; Different scale levels; Image features; Image super resolutions; Multi-scale features; Single images; Super resolution algorithms; Image enhancement","Channel attention; Dilated convolution; GAN; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85107333579"
"","","","4th International Workshop on Machine Learning for Medical Image Reconstruction, MLMIR 2021 held in Conjunction with 24th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12964 LNCS","","","","","139","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116927311&partnerID=40&md5=3879fa8d364b466bf5d0b237c3e57a59","The proceedings contain 13 papers. The special focus in this conference is on Machine Learning for Medical Image Reconstruction. The topics include: Real-Time Video Denoising to Reduce Ionizing Radiation Exposure in Fluoroscopic Imaging; a Frequency Domain Constraint for Synthetic and Real X-ray Image Super Resolution; semi- and Self-supervised Multi-view Fusion of 3D Microscopy Images Using Generative Adversarial Networks; efficient Image Registration Network for Non-Rigid Cardiac Motion Estimation; Evaluation of the Robustness of Learned MR Image Reconstruction to Systematic Deviations Between Training and Test Data for the Models from the fastMRI Challenge; Self-supervised Dynamic MRI Reconstruction; A Simulation Pipeline to Generate Realistic Breast Images for Learning DCE-MRI Reconstruction; Deep MRI Reconstruction with Generative Vision Transformers; Distortion Removal and Deblurring of Single-Shot DWI MRI Scans; One Network to Solve Them All: A Sequential Multi-task Joint Learning Network Framework for MR Imaging Pipeline; Physics-Informed Self-supervised Deep Learning Reconstruction for Accelerated First-Pass Perfusion Cardiac MRI.","","","Conference review","Final","","Scopus","2-s2.0-85116927311"
"Zhou W.; Hong C.; Wang X.; Zeng Z.","Zhou, Wengang (57220996549); Hong, Chaoqun (55457850200); Wang, Xiaodong (57192623684); Zeng, Zhiqiang (8646582400)","57220996549; 55457850200; 57192623684; 8646582400","FSRGAN-DB: Super-resolution Reconstruction Based on Facial Prior Knowledge","2020","Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020","","","9378242","3380","3386","6","10.1109/BigData50022.2020.9378242","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103841910&doi=10.1109%2fBigData50022.2020.9378242&partnerID=40&md5=ff8fe86a25131fc83dcc05e425cb58ca","Face super-resolution (SR) reconstruction is a method of reconstructing a high-resolution (HR) face image from a low-resolution (LR) face image with more facial details. How-ever, most of the SR methods do not account for facial structures and suffer from loss of face details. In this paper, we propose a method that explicitly incorporates structural information of faces into the face super resolution network. We firstly use Super-Resolution Generative Adversarial Network(SRGAN) as the basic network and improve it by replacing residual blocks with dense blocks(SRGAN-DB). Then, we use the heat map of facial key points to describe the facial structure and the trend of facial contours. When the resolution of low-resolution images is very low, it is difficult to obtain fine facial features. But it is easy to get the information of facial key points. Specifically, the LR image is sent to two network branches, one obtains the fine features of the image, and the other generates a heat map of the facial key point positions. In order to enhance the feature expression of facial key points information, we convert the facial key points heat map into a binary image and connect it to the face fine encoder network. Extensive experiments prove that the proposed method is superior to existing network methods to face super-resolution reconstruction. © 2020 IEEE.","Big data; Binary images; Optical resolving power; Adversarial networks; Face super-resolution; Facial structure; Feature expression; Low resolution images; Structural information; Super resolution; Super resolution reconstruction; Image enhancement","Dense Blocks; facial key points; facial prior knowledge; heat map; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85103841910"
"Bashir S.M.A.; Wang Y.; Khan M.; Niu Y.","Bashir, Syed Muhammad Arsalan (56385198200); Wang, Yi (12763268500); Khan, Mahrukh (57197808732); Niu, Yilong (18234173100)","56385198200; 12763268500; 57197808732; 18234173100","A comprehensive review of deep learningbased single image super-resolution","2021","PeerJ Computer Science","7","","","1","56","55","10.7717/PEERJ-CS.621","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111509541&doi=10.7717%2fPEERJ-CS.621&partnerID=40&md5=e94d6e56850e0ad3c02f273599bbf8b9","Image super-resolution (SR) is one of the vital image processing methods that improve the resolution of an image in the field of computer vision. In the last two decades, significant progress has been made in the field of super-resolution, especially by utilizing deep learning methods. This survey is an effort to provide a detailed survey of recent progress in single-image super-resolution in the perspective of deep learning while also informing about the initial classical methods used for image super-resolution. The survey classifies the image SR methods into four categories, i.e., classical methods, supervised learning-based methods, unsupervised learningbased methods, and domain-specific SR methods. We also introduce the problem of SR to provide intuition about image quality metrics, available reference datasets, and SR challenges. Deep learning-based approaches of SR are evaluated using a reference dataset. Some of the reviewed state-of-the-art image SR methods include the enhanced deep SR network (EDSR), cycle-in-cycle GAN (CinCGAN), multiscale residual network (MSRN), meta residual dense network (Meta-RDN), recurrent back-projection network (RBPN), second-order attention network (SAN), SR feedback network (SRFBN) and the wavelet-based residual attention network (WRAN). Finally, this survey is concluded with future directions and trends in SR and open problems in SR to be addressed by the researchers. © 2021 Bashir et al.","Image enhancement; Optical resolving power; Recurrent neural networks; Surveys; Classical methods; Feedback networks; Image processing - methods; Image quality metrics; Image super resolutions; Learning-based approach; Learning-based methods; State of the art; Learning systems","Artificial intelligence; Convolutional neural networks (CNN); Deep learning; Generative adversarial networks (GAN); Image super-resolution; Neural networks; Single-image super-resolution (SISR); Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85111509541"
"Shomee H.H.; Sams A.","Shomee, Homaira Huda (57444754200); Sams, Ataher (57444532800)","57444754200; 57444532800","License Plate Detection and Recognition System for All Types of Bangladeshi Vehicles Using Multi-step Deep Learning Model","2021","DICTA 2021 - 2021 International Conference on Digital Image Computing: Techniques and Applications","","","","","","","10.1109/DICTA52665.2021.9647284","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124329255&doi=10.1109%2fDICTA52665.2021.9647284&partnerID=40&md5=12afd526c7fc64536fd6df80c62e1e11","A robust license plate (LP) detection and recognition system can extract the license plate information from a still image or video of a moving or stationary vehicle. Bangla license plate recognition is a complicated subject of study due to no publicly available dataset and its specific characteristics with over 100 unique classes, including words, letters, and digits. This paper proposes a robust multi-step deep learning system based on You Only Look Once (YOLO) architecture that can extract license plate information from a real-world image. The resulting system localizes license plates using YOLOv4 object detector model, automatically crops the license plates using bounding box coordinates, enhances the extracted license plate image quality using Enhanced Super Resolution Generative Adversarial Networks (ESRGAN), and then recognizes the classes using YOLOv4 without segmenting the characters. Synthetic images have been used to make proposed method capable of recognizing the classes in unfavorable and complicated conditions. A complete two-part dataset named 'Bangla LPDB-A' is created in this study. This dataset includes Bangladeshi vehicle images with manually annotated license plates and cropped license plates with manually annotated words, letters, and digits. The proposed system is tested on this dataset that has achieved mean average precision (mAP) of 98.35% and 98.09% for final detection and recognition model, which has an average prediction time of 23 ms and 35 ms. © 2021 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Image segmentation; License plates (automobile); Object recognition; Optical character recognition; Bangla license plate; Deep learning; Detection system; Images processing; Learning models; License plate detection; Licenses plate recognition; Multisteps; Recognition systems; You only look once; Object detection","Bangla license plate; Deep learning; Generative adversarial networks; Image processing; Object detection; YOLO","Conference paper","Final","","Scopus","2-s2.0-85124329255"
"Kim J.S.; Chang D.S.; Choi Y.S.","Kim, Ji Seong (57221479336); Chang, Doo Soo (57207186032); Choi, Yong Suk (56055020700)","57221479336; 57207186032; 56055020700","Enhancement of multi-target tracking performance via image restoration and face embedding in dynamic environments","2021","Applied Sciences (Switzerland)","11","2","649","1","21","20","10.3390/app11020649","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099179061&doi=10.3390%2fapp11020649&partnerID=40&md5=a6a406e13911b207f0b5a9e20b115483","In this paper, we propose several methods to improve the performance of multiple object tracking (MOT), especially for humans, in dynamic environments such as robots and autonomous vehicles. The first method is to restore and re-detect unreliable results to improve the detection. The second is to restore noisy regions in the image before the tracking association to improve the identification. To implement the image restoration function used in these two methods, an image inference model based on SRGAN (super-resolution generative adversarial networks) is used. Finally, the third method includes an association method using face features to reduce failures in the tracking association. Three distance measurements are designed so that this method can be applied to various environments. In order to validate the effectiveness of our proposed methods, we select two baseline trackers for comparative experiments and construct a robotic environment that interacts with real people and provides services. Experimental results demonstrate that the proposed methods efficiently overcome dynamic situations and show favorable performance in general situations. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Computer vision; Data association; Image restoration; Multiple object tracking; Online object tracking; Visual embedding","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099179061"
"Elkarazle K.; Raman V.; Then P.","Elkarazle, Khaled (57485325100); Raman, Valliappan (25923097200); Then, Patrick (24492593000)","57485325100; 25923097200; 24492593000","Towards Accuracy Enhancement of Age Group Classification Using Generative Adversarial Networks","2021","Journal of Integrated Design and Process Science","25","1","","8","24","16","10.3233/JID-210019","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129223319&doi=10.3233%2fJID-210019&partnerID=40&md5=f8ebb1cf8998588a90e596fa321230b7","Age estimation models can be employed in many applications, including soft biometrics, content access control, targeted advertising, and many more. However, as some facial images are taken in unrestrained conditions, the quality relegates, which results in the loss of several essential ageing features. This study investigates how introducing a new layer of data processing based on a super-resolution generative adversarial network (SRGAN) model can influence the accuracy of age estimation by enhancing the quality of both the training and testing samples. Additionally, we introduce a novel convolutional neural network (CNN) classifier to distinguish between several age classes. We train one of our classifiers on a reconstructed version of the original dataset and compare its performance with an identical classifier trained on the original version of the same dataset. Our findings reveal that the classifier which trains on the reconstructed dataset produces better classification accuracy, opening the door for more research into building data-centric machine learning systems.  © 2021-IOS Press. All rights reserved.","Access control; Classification (of information); Convolutional neural networks; Deep learning; Face recognition; Optical resolving power; Accuracy enhancement; Age estimation; Age-group classifications; Automatic age estimation; Content access controls; Deep learning; Estimation models; Facial recognition; Soft biometrics; Superresolution; Generative adversarial networks","Automatic age estimation; Data processing; Deep learning; Facial recognition; Generative adversarial networks; Super resolution","Article","Final","","Scopus","2-s2.0-85129223319"
"Yu L.; Xian W.; Jiang M.","Yu, Li (57139015300); Xian, Wenhao (57271380100); Jiang, Mengyao (57271101900)","57139015300; 57271380100; 57271101900","Research on Face Degraded Image Generation Algorithm for Practical Application Scenes","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12878 LNCS","","","428","436","8","10.1007/978-3-030-86608-2_47","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115700782&doi=10.1007%2f978-3-030-86608-2_47&partnerID=40&md5=b27c12d8a1c9e3f303e1be22b6ce02ca","In general face super-resolution (FSR) networks, the degraded model usually adopts a simple bicubic down-sampling, without considering more complex degradation conditions and actual scenes. Although this can verify the basic effectiveness of the model, it makes the model lack of generalization. Therefore, in order to solve the problem of poor generalization ability, this paper constructs a Degraded Generative Adversarial Network (DGAN), which replaces the method of bicubic down-sampling to realize implicit modeling of degradation process, and generates degraded images in real scene. Combining DGAN and face super-resolution networks DIC/DICGAN, a new FSR network is trained to recognize the low- resolution (LR) face of real scene. The face recognition test under FSR reconstruction is carried out using SCFace dataset constructed by actual monitoring camera. It is proved that DGAN can effectively simulate the degradation conditions of actual scenes, and improve the generalization of face reconstruction model, so that the SR algorithm can better adapt to face recognition in actual scenes. © 2021, Springer Nature Switzerland AG.","Face recognition; Optical resolving power; Signal sampling; Statistical tests; Bicubic; Degradation condition; Degraded images; Down sampling; Face super-resolution; GAN; Generalisation; Generation algorithm; Image generations; Simple++; Generative adversarial networks","Bicubic; Face recognition; Face super-resolution; GAN","Conference paper","Final","","Scopus","2-s2.0-85115700782"
"Chen X.; Li B.; Jiang S.; Zhang T.; Zhang X.; Qin P.; Yuan X.; Zhang Y.; Zheng G.; Ji X.","Chen, Xu (57247855900); Li, Bowen (57223938952); Jiang, Shaowei (57094397000); Zhang, Terrance (57226502970); Zhang, Xu (57775059000); Qin, Peiwu (57117292500); Yuan, Xi (57218572539); Zhang, Yongbing (7601315649); Zheng, Guoan (57202554945); Ji, Xiangyang (7402837796)","57247855900; 57223938952; 57094397000; 57226502970; 57775059000; 57117292500; 57218572539; 7601315649; 57202554945; 7402837796","Accelerated Phase Shifting for Structured Illumination Microscopy Based on Deep Learning","2021","IEEE Transactions on Computational Imaging","7","","9468949","700","712","12","10.1109/TCI.2021.3093788","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111721682&doi=10.1109%2fTCI.2021.3093788&partnerID=40&md5=a964da56cd97d1534979455772fc687e","Structured illumination microscopy (SIM) enhances spatial resolution by projecting sinusoidal patterns with various orientations and lateral phase shifts. Here, we report a framework, termed DNN-SIM, powered by a deep neural network that learns the physical relationship between images with different lateral phase shifts. This approach captures one image per sinusoidal pattern orientation and infers the remaining phase-shifted images by the network, reducing the acquisition time to one-third of conventional 2D-SIM. We further extend the DNN-SIM to 3D applications and reduce the acquisition time to one-fifth of the conventional method without sacrificing the resolution. The reported DNN-SIM framework is not sample-specific, and can be used to handle new samples with features that the network has not previously encountered or learned. DNN-SIM is compatible with most existing SIM setups and reconstruction algorithms. It has the potential to address challenges associated with phototoxicity and photobleaching in super-resolution fluorescence microscopy. © 2015 IEEE.","Deep neural networks; Fluorescence microscopy; Photobleaching; Acquisition time; Conventional methods; Phase shifted images; Reconstruction algorithms; Sinusoidal patterns; Spatial resolution; Structured illumination microscopies (SIM); Structured illumination microscopy; Deep learning","Deep learning; generative adversarial network; image reconstruction; structured illumination microscopy; super resolution","Article","Final","","Scopus","2-s2.0-85111721682"
"Peng Y.; Zhang P.; Gao Y.; Zi L.","Peng, Yanfei (36731459600); Zhang, Pingjia (57301144500); Gao, Yi (57872597000); Zi, Lingling (8439186700)","36731459600; 57301144500; 57872597000; 8439186700","Attention fusion generative adversarial network for single-image super-resolution reconstruction; [融合注意力的生成式对抗网络单图像超分辨率重建]","2021","Laser and Optoelectronics Progress","58","20","2010012","","","","10.3788/LOP202158.2010012","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117415491&doi=10.3788%2fLOP202158.2010012&partnerID=40&md5=eb12ea4b39bac9de87671b3a5a3cdaec","Deep learning-based single-image super-resolution reconstruction method has been relatively perfect. The reconstructed image has a high objective evaluation value or a good visual effect; however, the image perception effect and objective evaluation value cannot be improved in a balanced manner. To address this problem, this paper proposes a single-image super-resolution reconstruction method based on an attention fusion generative adversarial network. In the proposed method, first, the batch layer that destroys the original image contrast information and affects the quality of image generation in the residual network is removed. Then, the residual block of the attention convolutional neural network, which can effectively perform adaptive feature refinement in the feature map, is constructed. To improve the reconstruction results that lack high-frequency information and texture details under large-scale factors, a pixel-loss function is constructed to replace the mean squared error-loss function with a more robust Charbonnier loss function, and a total variation regular term is used to smooth the training results. The experimental results show that compared with other methods on the Set5, Set14, Urban100, and BSDS1 00 test sets under 4 × magnification factor, the average peak signal-to-noise ratio and average structure similarity increased by 2.88 dB and 0.078, respectively. The experimental data and renderings demonstrate that the proposed method is subjectively rich in details, objectively has a high peak signal-to-noise ratio and structural similarity value, and achieves a balanced improvement of visual effects and objective evaluation index values. © 2021 Universitat zu Koln. All rights reserved.","","Attention; Convolutional neural network; Generative adversarial network; Image processing; Residual learning; Super-resolution","Article","Final","","Scopus","2-s2.0-85117415491"
"Kim J.; Lee C.","Kim, J. (8866990900); Lee, C. (55700543700)","8866990900; 55700543700","Reliable Perceptual Loss Computation for GAN-Based Super-Resolution with Edge Texture Metric","2021","IEEE Access","9","","9524635","120127","120137","10","10.1109/ACCESS.2021.3108394","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113895117&doi=10.1109%2fACCESS.2021.3108394&partnerID=40&md5=a6e49cc18c24b6ad1a1e7c06ce541499","Super-resolution (SR) is an ill-posed problem. Generating high-resolution (HR) images from low-resolution (LR) images remains a major challenge. Recently, SR methods based on deep convolutional neural networks (DCN) have been developed with impressive performance improvement. DCN-based SR techniques can be largely divided into peak signal-to-noise ratio (PSNR)-oriented SR networks and generative adversarial networks (GAN)-based SR networks. In most current GAN-based SR networks, the perceptual loss is computed from the feature maps of a single layer or several fixed layers using a differentiable feature extractor such as VGG. This limited layer utilization may produce overly textured artifacts. In this paper, a new edge texture metric (ETM) is proposed to quantify the characteristics of images and then it is utilized only in the training phase to select an appropriate layer when calculating the perceptual loss. We present experimental results showing that the GAN-based SR network trained with the proposed method achieves qualitative and quantitative perceptual quality improvements compared to many of the existing methods. © 2013 IEEE.","Deep neural networks; Optical resolving power; Signal processing; Signal to noise ratio; Textures; Adversarial networks; Feature extractor; High resolution image; Ill posed problem; Low resolution images; Peak signal to noise ratio; Perceptual quality; Super resolution; Convolutional neural networks","Artificial neural networks; computer vision; image enhancement; image resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85113895117"
"Lezine E.M.D.; Kyzivat E.D.; Smith L.C.","Lezine, Ekaterina M. D. (57223931025); Kyzivat, Ethan D. (57209692081); Smith, Laurence C. (57208193133)","57223931025; 57209692081; 57208193133","Super-Resolution Surface Water Mapping on the Canadian Shield Using Planet CubeSat Images and a Generative Adversarial Network; [Cartographie des eaux de surface à très haute résolution sur le Bouclier canadien à l’aide d’images Planet CubeSat et d’un réseau antagoniste génératif]","2021","Canadian Journal of Remote Sensing","47","2","","261","275","14","10.1080/07038992.2021.1924646","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106535322&doi=10.1080%2f07038992.2021.1924646&partnerID=40&md5=4e2d6dcab84f05212e7e49da213efc9c","The Canadian Shield, the world’s largest exposure of glaciated crystalline bedrock, is the most lake-rich region on Earth. Recent studies using high-resolution CubeSat satellite imagery have revealed its surface water hydrology to be surprisingly dynamic at fine spatial scales. Here we test whether super-resolution (SR), the resampling of coarse imagery to a finer-than-native resolution, can detect such changes. We degrade high-resolution Planet CubeSat images of the Shield, then resample the coarsened imagery back to its native resolution using both traditional cubic resampling and a generative adversarial network, a type of neural network often used for SR. To test classification accuracy from the generated SR imagery, we apply the same water classification to both resampling methods and find similar performance based on confusion matrices with the control case of high-resolution imagery. Next, we compare fine-scale shoreline mapping in SR imagery, cubic resampling, and in-situ field surveys. SR shorelines outperform those from cubic resampling, with an increase in the modified kappa coefficient from −0.070 to 0.073. Potential applications include improved mapping of Shield lakes and retroactive application of SR to coarser-resolution satellite datasets to infer historical changes in fine-scale surface water dynamics. ©, Copyright © CASI.","Lakes; Mapping; Optical resolving power; Adversarial networks; Classification accuracy; Coarser resolution; Confusion matrices; Crystalline bedrocks; High resolution imagery; Historical changes; Surface water hydrology; Satellite imagery","","Article","Final","","Scopus","2-s2.0-85106535322"
"Parekh J.; Turakhia P.; Bhinderwala H.; Dhage S.N.","Parekh, Jinay (57220004453); Turakhia, Poojan (57220005316); Bhinderwala, Hussain (57216222801); Dhage, Sudhir N. (39861355400)","57220004453; 57220005316; 57216222801; 39861355400","A Survey of Image Enhancement and Object Detection Methods","2021","Advances in Intelligent Systems and Computing","1158","","","1035","1047","12","10.1007/978-981-15-4409-5_91","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096499928&doi=10.1007%2f978-981-15-4409-5_91&partnerID=40&md5=06e06e9a6e9316dc895e5eeb4122c3cc","Image enhancement is a classical problem in computer vision. Image enhancement techniques aim to enlarge the size and quality of low-resolution (LR) images to high-resolution (HR) image. Various techniques have been developed over the years, for example, the traditional methods of upscaling to applying neural networks generating output using trained models and datasets. On the other hand, object detection has vast use cases in modern inference systems like face detection to text detection. This paper surveys the various techniques of image enhancement and object detection along with their contributions and methodologies. © 2021, Springer Nature Singapore Pte Ltd.","Face recognition; Object detection; Object recognition; Surveys; Classical problems; High resolution image; Inference systems; Low resolution images; Object detection method; Paper surveys; Text detection; Upscaling; Image enhancement","Convolutional neural networks; Deep learning methods; Generative adversarial networks (GAN’s); Image enhancement; Image enhancement; Image quality; Image super resolution; Object detection","Conference paper","Final","","Scopus","2-s2.0-85096499928"
"","","","Proceedings - 2021 2nd Asia Symposium on Signal Processing, ASSP 2021","2021","Proceedings - 2021 2nd Asia Symposium on Signal Processing, ASSP 2021","","","","","","264","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128232748&partnerID=40&md5=d5fa5faf3ed6b6d89bfe650dd606e83b","The proceedings contain 36 papers. The topics discussed include: research on a graph reachability algorithm based on block optimization; realistic image-to-image translation with enhanced texture; application of color block code in image scaling; face image super-resolution based on relative average generative adversarial networks; integrating residual network and channel attention mechanism for tire pattern image retrieval; position-based vehicular clustering for emergency messages dissemination in zone of interest; support forward secure smart grid data deduplication and deletion mechanism; trusted model based on multi-dimensional attributes in edge computing; and fine classification method of product image based on multi-level convolutional neural networks.","","","Conference review","Final","","Scopus","2-s2.0-85128232748"
"Bao S.","Bao, Shuai (57490243900)","57490243900","Review on Generative Adversarial Network in Computer Vision: Methods and Metrics","2021","Proceedings - 2021 2nd International Conference on Big Data and Artificial Intelligence and Software Engineering, ICBASE 2021","","","","535","542","7","10.1109/ICBASE53849.2021.00105","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126451069&doi=10.1109%2fICBASE53849.2021.00105&partnerID=40&md5=ee0c0565774b0becdaf3ddb92cff87df","Generate adversarial networks (GAN) is a popular method, which can be widely used in numerous research areas, such as computer vision, natural language processing, and time series synthesis. However, few references are proposed to give a comprehensive review on GAN based methods. This paper aims to provide a detailed review of GAN based algorithms on computer vision tasks, such as image style transfer, image/video generation, image matting, and image super-resolution. Furthermore, we conclude the evaluation metrics for those tasks to show the effectiveness of GAN based on method. Our review can help beginners recognize the GAN based methods and give a brief introduction on how to apply them to their tasks.  © 2021 IEEE.Allrights reserved","Generative adversarial networks; Natural language processing systems; Optical resolving power; Adversarial networks; Generate adversarial network; Image generations; Image matting; Image style transfer; Image super resolutions; Network-based; Research areas; Time series synthesis; Video generation; Computer vision","GAN; Image generation; Image matting; Image style transfer; Image super-resolution; Video generation","Conference paper","Final","","Scopus","2-s2.0-85126451069"
"Pham V.-D.; Bui Q.-T.","Pham, Vu-Dong (57202350777); Bui, Quang-Thanh (57189899688)","57202350777; 57189899688","Spatial resolution enhancement method for Landsat imagery using a Generative Adversarial Network","2021","Remote Sensing Letters","12","7","","654","665","11","10.1080/2150704X.2021.1918789","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105127268&doi=10.1080%2f2150704X.2021.1918789&partnerID=40&md5=92fd70c5ee8e4a888827caa02442244e","Landsat and Sentinel-2 are two freely accessible satellite data that are relevant for global land cover monitoring. However, the uses of the latter data set are growing because of its higher spatial resolutions and the availability of benchmark data sets for deep learning applications. In this study, we integrate a style transfer (perceptual loss estimation from Sentinel 2 benchmark data) into a Generative Adversarial Network (GAN) to construct a single image super-resolution model. The proposed model upscales Landsat 8 images (using red, green, blue, and near-infrared bands at 30 m and Panchromatic band 15 m for high-resolution features exploiting) to 10 m (with Sentinel-2 as reference). Compared to pan-sharpening and other upscaling methods, the proposed method can produce more realistic, spatial convincing images at 10 m resolution and more similar to Sentinel-2 images than the other commonly used super-resolution imaging algorithms. As a result, the proposed method extends the usage of high-resolution benchmark data sets for lower resolution imagery to enrich supplement data sources for land cover classification. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Benchmarking; Classification (of information); Deep learning; Infrared devices; Optical resolving power; Adversarial networks; Land cover classification; Near infrared band; Panchromatic bands; Spatial resolution; Spatial-resolution enhancement; Super resolution imaging; Upscaling methods; algorithm; image resolution; land cover; Landsat; satellite data; satellite imagery; Sentinel; spatial resolution; Image enhancement","","Article","Final","","Scopus","2-s2.0-85105127268"
"","","","2021 6th International Conference on Advances in Biomedical Engineering, ICABME 2021","2021","International Conference on Advances in Biomedical Engineering, ICABME","2021-October","","","","","244","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122877699&partnerID=40&md5=66dc65c4be0cade92d0f102c2dd9a210","The proceedings contain 49 papers. The topics discussed include: MRI super-resolution using 3D cycle-consistent generative adversarial network; reconstruction and topological cleaning of brain MR images; bringing AI to automatic diagnosis of diabetic retinopathy from optical coherence tomography angiography; artificial intelligence framework for COVID19 patients monitoring; sensitivity analysis of a realistic electrical model of the uterine activity; CoBra: towards adaptive robotized prostate brachytherapy under MRI guidance; identifying physical worsening in elderly using objective and self-reported measures; anti-fog face mask while wearing glasses in the coronavirus pandemic; and development of a robust mathematical model to estimate COVID-19 cases in Lebanon based on SEIRDV modified model.","","","Conference review","Final","","Scopus","2-s2.0-85122877699"
"Bui T.-A.; Lee P.-J.; Lee K.-M.; Wang W.; Shiu S.-H.","Bui, Trong-An (57196244866); Lee, Pei-Jun (7406120709); Lee, Kuan-Min (57282469400); Wang, Walter (57417109300); Shiu, Shiual-Hal (57215352203)","57196244866; 7406120709; 57282469400; 57417109300; 57215352203","Infrared imagery super-resolution by using a generative adversarial network","2021","2021 IEEE International Conference on Consumer Electronics-Taiwan, ICCE-TW 2021","","","","","","","10.1109/ICCE-TW52618.2021.9603172","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123059044&doi=10.1109%2fICCE-TW52618.2021.9603172&partnerID=40&md5=d97a7b4493b896e0092ead2bf99a2086","The thermal camera often has a limited spatial resolution compared to the RGB camera with typically provides megapixels of resolution. This study presents a super-resolution architecture for infrared (IR) imagery base on a generative adversarial network. The up-sampling in this proposed network's design generates a new super-resolution image by four times. Moreover, in this paper, generative network and discriminative models for IR images are presented. The small-object features in super-resolution IR images are shown in the simulation section with high quality.  © 2021 IEEE.","Cameras; Computer vision; Infrared imaging; Optical resolving power; Discriminative networks; Infrared imagery; Megapixels; Network design; Resolution images; RGB cameras; Spatial resolution; Superresolution; Thermal camera; Upsampling; Generative adversarial networks","discriminative network; generative adversarial network; IR images; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85123059044"
"Wei X.; Wang F.; Chen X.; Yan Y.; Chen P.; Liu S.","Wei, Xiang (57218453401); Wang, Feng (56482454000); Chen, Xi (57218454332); Yan, Yongjie (57224724021); Chen, Ping (57218454350); Liu, Sheng (57218450368)","57218453401; 56482454000; 57218454332; 57224724021; 57218454350; 57218450368","Application of conditional generative adversarial network in image super-resolution reconstruction","2021","Advances in Intelligent Systems and Computing","1234 AISC","","","256","265","9","10.1007/978-3-030-51556-0_37","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089209760&doi=10.1007%2f978-3-030-51556-0_37&partnerID=40&md5=c83ae1e87f1c6b84effc5e17a6875d6d","In the process of image processing, the higher the resolution of the image, the richer the data information contained in the image. Aiming at the problem of low image resolution, through the research of generating countermeasure network, the condition generating countermeasure network is used to solve this problem. We use a symmetric convolutional neural network as the generator network, each of which has 6 layers. At the same time, by entering the labels into the discriminator, the connection between the original image and the model-generated image can be derived. In the loss function during the experiment, perceptual loss and mean square error are added, and good results are obtained in the actual experiment, and the visual effect of the generated image is more ideal. Compared with other algorithms, the proposed algorithm achieves the visual effect of SRGAN, and the effect on some pictures has been greatly improved. © Springer Nature Switzerland AG 2021.","Convolutional neural networks; Image resolution; Intelligent systems; Mean square error; Multilayer neural networks; Actual experiments; Adversarial networks; Data informations; Image super-resolution reconstruction; Loss functions; Original images; Visual effects; Image reconstruction","Conditional generative adversarial network; Deep learning; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85089209760"
"Zhang M.; Ling Q.","Zhang, Menglei (57195432433); Ling, Qiang (7005827443)","57195432433; 7005827443","Supervised Pixel-Wise GAN for Face Super-Resolution","2021","IEEE Transactions on Multimedia","23","","9132630","1938","1950","12","10.1109/TMM.2020.3006414","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087482505&doi=10.1109%2fTMM.2020.3006414&partnerID=40&md5=5aed8b4e34bebe72895e47e7a4401b99","For many face-related multimedia applications, low-resolution face images may greatly degrade the face recognition performance and necessitate face super-resolution (SR). Among the current SR methods, MSE-oriented SR methods often produce over-smoothed outputs and could miss some texture details while GAN-oriented SR methods may generate artifacts which are harmful to face recognition. To resolve the above issues, this paper presents a supervised pixel-wise Generative Adversarial Network (SPGAN) that can resolve a very low-resolution face image of 16× 16 or smaller pixel-size to its larger version of multiple scaling factors (2×, 4×, 8× and even 16×) in a unified framework. Being different from traditional unsupervised discriminators which generate a single number to represent the likelihood whether the input image is real or fake, the proposed supervised pixel-wise discriminator mainly focus on whether each pixel of the generated SR face image is as photo-realistic as its corresponding pixel in the ground-truth HR (high-resolution) face image. To further improve the face recognition performance of SPGAN, we take advantage of the face identity prior by sending two inputs to the discriminator, including an input face image (either a real HR face image or its corresponding SR face image) and its face features which are extracted from a pre-trained face recognition model. Due to the introduced face identity prior, the identity-based discriminator can pay more attention to texture details which are closely related to face recognition. Extensive experiments demonstrate that the proposed SPGAN can achieve more photo-realistic SR images and higher face recognition accuracy than some state-of-the-art methods. © 1999-2012 IEEE.","Image enhancement; Optical resolving power; Pixels; Textures; Adversarial networks; Face recognition performance; Face super-resolution; Low-resolution face images; Multimedia applications; Recognition accuracy; Recognition models; State-of-the-art methods; Face recognition","Face image super-resolution; face recognition; generative adversarial nets (GAN); pixel-wise GAN; supervised","Article","Final","","Scopus","2-s2.0-85087482505"
"Deepak S.; Sahoo S.; Patra D.","Deepak, Shashikant (57216807543); Sahoo, Sanuj (57537052800); Patra, Dipti (23985620900)","57216807543; 57537052800; 23985620900","Super-Resolution of Thermal Images Using GAN Network","2021","2021 Advanced Communication Technologies and Signal Processing, ACTS 2021","","","","","","","10.1109/ACTS53447.2021.9708340","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126712677&doi=10.1109%2fACTS53447.2021.9708340&partnerID=40&md5=2efe862c37c21a0009befd05798ac4d4","Super-resolution (SR) reconstruction of thermal images has been one of the most active research areas specifically for industrial applications. However, most of the conventional RGB SR models available in the literature are not necessarily applicable to thermal images due to their difference in characteristics when compared to normal camera images. The recent advancement in the field of deep learning-based SR has helped achieve unbelievable results. Despite the advancement in models like deep convolution neural networks (CNN) and Generative adversarial networks, there remain multiple problems unsolved that will help improve the spatial resolution of thermal images. Not only the developed model should be computationally efficient but also easily implementable in industrial applications. Motivated to overcome the said limitations, in this work a generative adversarial network (GAN) based single images super-resolution architecture is proposed for thermal camera images. The developed model not only generates at par results with the other model but also is easy to implement and computationally efficient. The modified architecture has an identical layout inspired by SRGAN. In order to make the model faster to train while having less training parameters, the number of residual blocks was reduced to 5. The batch normalization layers were excluded from the residual blocks of both the Generator and Discriminator networks to remove the redundancy. Before each convolution layer, reflective padding is utilized at the edges to preserve the size of the feature maps. The comparative results revealed that the proposed network trained on thermal images produced high-quality images with enhanced details, while still maintaining image features and perspective throughout. The experimental results show that the proposed model has achieved a reduction in computation time compared to the State-of-the-Art method. The suggested strategy has outperformed the SOTA methods with the improvement of approximately 2dB in PSNR along with 0.9825 of SSIM. © 2021 IEEE.","Cameras; Computational efficiency; Convolution; Deep learning; Generative adversarial networks; Image enhancement; Industrial research; Network architecture; Camera images; Computationally efficient; Convolution neural network; Developed model; Learning-based super-resolution; Research areas; Super-resolution models; Super-resolution reconstruction; Superresolution; Thermal images; Optical resolving power","Generative Adversarial Networks; Super Resolution; Thermal Images","Conference paper","Final","","Scopus","2-s2.0-85126712677"
"Shahsavari A.; Ranjbari S.; Khatibi T.","Shahsavari, Ali (57222147080); Ranjbari, Sima (57221226712); Khatibi, Toktam (56904244000)","57222147080; 57221226712; 56904244000","Proposing a novel Cascade Ensemble Super Resolution Generative Adversarial Network (CESR-GAN) method for the reconstruction of super-resolution skin lesion images","2021","Informatics in Medicine Unlocked","24","","100628","","","","10.1016/j.imu.2021.100628","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108179793&doi=10.1016%2fj.imu.2021.100628&partnerID=40&md5=8374f1b3b0242810f4ead603cf7863ef","Background: Skin cancer is one of the most malignant cancers worldwide. Its early detection plays a prominent role in the patients' treatment. The quality of skin lesion images to ease the diagnosis of skin cancer is highly regarded. One of the most common technologies to take the skin lesion images is through a dermoscopy device. However, it is not accessible to all people. Capturing the images via other technologies such as mobile devices, is available everywhere, although they suffer from poor quality. Materials and methods: In this paper, a novel Cascade Ensemble Super Resolution Generative Adversarial Network (CESR-GAN) method is proposed to reconstruct super-resolution skin lesion images using low-resolution counterparts. Specifically, a novel feature-based measurement loss function is designed to obtain more details as much as possible and generate higher quality images. Results: Experimental results from quantitative and qualitative comparisons between our CESR-GAN model and other state-of-the-art methods show that our proposed method outperforms the compared methods on ISIC, and PH2 datasets, respectively. Conclusion: The CESR-GANs method can be used to generate super resolution skin images of skin lesions with highly notable performances. © 2021","Article; basal cell carcinoma; brightness; cancer diagnosis; cascade ensemble super resolution generative adversarial network method; classification algorithm; computer assisted diagnosis; contrast; DC-GAN method; deep learning; epiluminescence microscopy; feature extraction; FRCNN method; image analysis; image processing; image quality; image reconstruction; intermethod comparison; LAP-GAN method; learning algorithm; melanoma; nevus; progressive generative adversarial network method; qualitative analysis; quantitative analysis; reconstruction algorithm; seborrheic keratosis; skin cancer; skin defect; super resolution generative adversarial network method","Deep learning; Generative adversarial network; Medical image analysis; Skin cancer; Super resolution","Article","Final","","Scopus","2-s2.0-85108179793"
"He Z.; Jin Z.; Xu X.; Luo L.","He, Zongyao (57226651682); Jin, Zhi (57199936921); Xu, Xiao (55819455400); Luo, Lei (35275273000)","57226651682; 57199936921; 55819455400; 35275273000","Degradation Reconstruction Loss: A Perceptual-Oriented Super-Resolution Framework for Multi-downsampling Degradations","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12890 LNCS","","","437","449","12","10.1007/978-3-030-87361-5_36","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117151375&doi=10.1007%2f978-3-030-87361-5_36&partnerID=40&md5=272596abeb28eb335251dd1a66719613","Recent years have witnessed the great success of deep learning-based single image super-resolution (SISR) methods. However, most of the existing SR methods assume that low-resolution (LR) images are purely bicubic downsampled from high-resolution (HR) images. Once the actual degradation is not bicubic, their outstanding performance is hard to maintain. Although several SR methods have super-resolved LR images with multiple blur kernels and noise levels, they still follow the bicubic downsampling assumption. To address this issue, we propose a novel degradation reconstruction loss (DRL) to capture the degradation-wise differences between HR images and SR images based on a degradation simulator. By involving the proposed degradation simulator and the loss, a perceptual-oriented SR framework for multi-downsampled images is formed. Extensive experimental results demonstrate that our method outperforms the state-of-the-art perceptual-oriented SR methods on both multi-downsampled datasets and bicubic downsampled datasets. © 2021, Springer Nature Switzerland AG.","Deep learning; Image reconstruction; Optical resolving power; Signal sampling; Degradation reconstruction loss; Down sampling; High-resolution images; Image super resolutions; Low resolution images; Multiple downsampling degradation; Performance; Single images; Superresolution; Superresolution methods; Generative adversarial networks","Degradation reconstruction loss; Generative adversarial network; Image super-resolution; Multiple downsampling degradations","Conference paper","Final","","Scopus","2-s2.0-85117151375"
"Li L.; Zhou Z.; Cui S.","Li, Linhao (57204549892); Zhou, Zhiqiang (55728196000); Cui, Saijia (57299898400)","57204549892; 55728196000; 57299898400","Boosting Small Ship Detection in Optical Remote Sensing Images via Image Super-Resolution","2021","Proceedings of the 33rd Chinese Control and Decision Conference, CCDC 2021","","","","1508","1512","4","10.1109/CCDC52312.2021.9601674","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125184752&doi=10.1109%2fCCDC52312.2021.9601674&partnerID=40&md5=649763eca9e4925aad8eef3363ab157c","Small ships in optical remote sensing images are hard to detect due to the lack of sufficient detail information. In this paper, we adopt the image super-resolution technology to solve this problem. Specifically, an effective super-resolution network is designed to generate clear super-resolution ship images from small blurry ones produced by the ship detector. Inspired by the idea of generative adversarial network (GAN), the super-resolution network is trained together with a discriminator network in an adversarial way, aiming at generating more realistic super-resolution images. Moreover, to eliminate false detections, the discriminator network is also used to distinguish ship and non-ship images via an additional classification branch. Experimental results demonstrate the effectiveness of the proposed method.  © 2021 IEEE.","Discriminators; Geology; Optical resolving power; Remote sensing; Ships; False detections; Image super resolutions; Optical remote sensing; Remote sensing images; Resolution images; Ship detection; Superresolution; Generative adversarial networks","Generative adversarial network; Image super-resolution; Ship detection","Conference paper","Final","","Scopus","2-s2.0-85125184752"
"Wang J.; Li X.; Lv P.; Shi C.","Wang, Jinke (56897294900); Li, Xiang (57235898500); Lv, Peiqing (57222868206); Shi, Changfa (56875086300)","56897294900; 57235898500; 57222868206; 56875086300","SERR-U-Net: Squeeze-and-Excitation Residual and Recurrent Block-Based U-Net for Automatic Vessel Segmentation in Retinal Image","2021","Computational and Mathematical Methods in Medicine","2021","","5976097","","","","10.1155/2021/5976097","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113793569&doi=10.1155%2f2021%2f5976097&partnerID=40&md5=d4861458f630f9b5ccc95aa7d536b20c","Background and Objective. Accurate segmentation of retinal vessels is considered as an important prerequisite for computer-aided diagnosis of ophthalmic diseases, diabetes, glaucoma, and other diseases. Although current learning-based methods have greatly improved the performance of retinal vessel segmentation, the accuracy could not meet the requirements of clinical assistance yet. Methods. A new SERR-U-Net framework for retinal vessel segmentation is proposed, which leverages technologies including Squeeze-and-Excitation (SE), residual module, and recurrent block. First, the convolution layers of encoder and decoder are modified on the basis of U-Net, and the recurrent block is used to increase the network depth. Second, the residual module is utilized to alleviate the vanishing gradient problem. Finally, to derive more specific vascular features, we employed the SE structure to introduce attention mechanism into the U-shaped network. In addition, enhanced super-resolution generative adversarial networks (ESRGANs) are also deployed to remove the noise of retinal image. Results. The effectiveness of this method was tested on two public datasets, DRIVE and STARE. In the experiment of DRIVE dataset, the accuracy and AUC (area under the curve) of our method were 0.9552 and 0.9784, respectively, and for SATRE dataset, 0.9796 and 0.9859 were achieved, respectively, which proved a high accuracy and promising prospect on clinical assistance. Conclusion. An improved U-Net network combining SE, ResNet, and recurrent technologies is developed for automatic vessel segmentation from retinal image. This new model enables an improvement on the accuracy compared to learning-based methods, and its robustness in circumvent challenging cases such as small blood vessels and intersection of vessels is also well demonstrated and validated. © 2021 Jinke Wang et al.","Algorithms; Computational Biology; Databases, Factual; Deep Learning; Humans; Image Interpretation, Computer-Assisted; Neural Networks, Computer; Retinal Vessels; Retinoscopy; Blood vessels; Computer aided diagnosis; Eye protection; Image enhancement; Learning systems; Ophthalmology; Recurrent neural networks; Adversarial networks; Area under the curves; Attention mechanisms; Learning-based methods; Retinal vessel segmentations; Vanishing gradient; Vascular features; Vessel segmentation; area under the curve; Article; computer aided design; diabetic retinopathy; enhanced super resolution generative adversarial network; human; image processing; image segmentation; noise reduction; qualitative analysis; quantitative analysis; recurrent neural network; residual neural network; retina blood vessel; retina image; squeeze and excitation residual and recurrent block based  unit network; algorithm; anatomy and histology; biology; computer assisted diagnosis; factual database; pathology; procedures; retinoscopy; Image segmentation","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85113793569"
"Shahbakhsh M.B.; Hassanpour H.","Shahbakhsh, Mostafa Balouchzehi (57559069200); Hassanpour, Hamid (56919429700)","57559069200; 56919429700","Enhancing Face Super-Resolution via Improving the Edge and Identity Preserving Network","2021","Proceedings - 2021 7th International Conference on Signal Processing and Intelligent Systems, ICSPIS 2021","","","","","","","10.1109/ICSPIS54653.2021.9729372","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127418003&doi=10.1109%2fICSPIS54653.2021.9729372&partnerID=40&md5=74c0bc5353c5a189f15e0eeb812cb3e8","Face super-resolution, known as face hallucination, is a domain-specific image super-resolution problem, which refers to generating high resolution face images from their low resolution. State-of-The-Art face super-resolution methods used deep convolutional neural networks. However, due to significant pose changes and difficulty in recovering high-frequency details in facial areas, most of these methods do not deploy facial structures and identity information well, and it is tough for them to reconstruct super-resolved face images. According to previous researches, proper use of low-resolution image edges can be a solution for these problems. EIPNet (Edge and Identity Preserving Network) is one of the newest methods to achieve outstanding results in this area. In the EIPNet method, the authors used a lightweight edge extraction block in the proposed GAN structure. In this research, we intend to improve the performance of the EIPNet method by presenting a simple but efficient technique. Our proposed technique divides the face images into upper and lower parts. We train a separate network for each area. This technique reduces the number of face components to train from each area, and the networks can better be trained from their components. The results show that this technique can have an excellent effect on visual quality and quantitative measurements in face super-resolution.  © 2021 IEEE.","Computer vision; Convolutional neural networks; Deep neural networks; Optical resolving power; Domain specific; Face hallucination; Face images; Face super-resolution; Facial components; High resolution; Image super resolutions; Lower resolution; Network methods; Superresolution; Generative adversarial networks","Face hallucination; Facial components; Generative Adversarial Network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85127418003"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","4","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120418130&partnerID=40&md5=5ec8713a4b73bd15a712ef1dbe92c18c","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85120418130"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","1","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119979812&partnerID=40&md5=d19fd33f23ebc4a72addac20423557de","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119979812"
"Zhang C.; He Y.; Qi X.; Yao Y.; Mei X.","Zhang, Chang (57575378100); He, Yi (57575593100); Qi, Xuan (57576007800); Yao, Yanyin (57462879300); Mei, Xue (23091706700)","57575378100; 57575593100; 57576007800; 57462879300; 23091706700","Real-Scenes Face Super-Resolution using a Lightweight Generative Adversarial Network","2021","Proceedings - 2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence, MLBDBI 2021","","","","572","576","4","10.1109/MLBDBI54094.2021.00114","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128194359&doi=10.1109%2fMLBDBI54094.2021.00114&partnerID=40&md5=34bea03463e23cbad88804a4f0fbdb4a","Although deep learning-based methods have achieved great success in the field of image super-resolution (SR), the SR algorithms still have some shortcomings, such as unsatisfactory results in reconstructing face images collected in real scenes, the model has a large amount of calculation and serious delays when running in a real-time scene. To solve the above problems, this paper proposes a SR algorithm for face images based on a lightweight Generative Adversarial Network (GAN). The algorithm in this paper constructs a real-scenes face datasets for training. Based on the ESRGAN, this study completes the lightweight reconstruction of the basic network inspired by MobileNetV3. Experimental results show that the algorithm is better than the existing mainstream SR algorithms in terms of image reconstruction time, model volume, and parameters amount. When the test image is a face image with an unknown degradation method collected in a real scene, the algorithm in this paper can also achieve the ideal reconstruction effect. © 2021 IEEE.","Deep learning; Image reconstruction; Optical resolving power; Deep separable convolution; Face images; Face super-resolution; Image super resolutions; Large amounts; Learning-based methods; Lightweight; Real scene; Super resolution algorithms; Superresolution; Generative adversarial networks","deep separable convolution; Generative Adversarial Network; lightweight; real scenes; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85128194359"
"Li M.; McComb C.","Li, Matthew (57211535115); McComb, Christopher (56400780700)","57211535115; 56400780700","Using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations","2021","Proceedings of the ASME Design Engineering Technical Conference","3A-2021","","V03AT03A001","","","","10.1115/DETC2021-66719","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119951007&doi=10.1115%2fDETC2021-66719&partnerID=40&md5=31ce784d4e09277959744a00925ebe21","Computational Fluid Dynamics (CFD) simulations are useful to the field of engineering design as they provide deep insights on product or system performance without the need to construct and test physical prototypes. However, they can be very computationally intensive to run. Machine learning methods have been shown to reconstruct high-resolution single-phase turbulent fluid flow simulations from low-resolution inputs. This offers a potential avenue towards alleviating computational cost in iterative engineering design applications. However, little work thus far has explored the application of machine learning image super-resolution methods to multiphase fluid flow (which is important for important for emerging fields such as marine hydrokinetic energy conversion). In this work, we apply a modified version of the Super-Resolution Generative Adversarial Network (SRGAN) model to a multiphase turbulent fluid flow problem, specifically to reconstruct fluid phase fraction at a higher resolution. Two models were created in this work, one with a simple physics-constrained loss function and one without, and the results are discussed and analyzed. We found that both models were able to significantly outperform non-machine learning upsampling methods and can preserve an impressive amount of detail and nuance, showing the versatility of the SRGAN model for upsampling fluid simulations. However, the difference in accuracy between the two models is quite minimal. This indicates that, for these contexts studied here, the additional complexity of a physics-informed approach may not be justified. Copyright © 2021 by ASME.","Computational fluid dynamics; Cost engineering; Energy conversion; Generative adversarial networks; Iterative methods; Multiphase flow; Product design; Signal sampling; Computational fluid dynamics simulations; Engineering design; Fluid simulations; High resolution; Multiphase fluids; Network models; Product performance; Superresolution; Turbulent fluid flow; Upsampling; Optical resolving power","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119951007"
"Zhao Y.; Fu G.; Wang H.; Zhang S.; Yue M.","Zhao, Yuqing (57209912605); Fu, Guangyuan (7202722322); Wang, Hongqiao (56322051800); Zhang, Shaolei (57208773089); Yue, Min (57209911167)","57209912605; 7202722322; 56322051800; 57208773089; 57209911167","Generative Adversarial Network-Based Edge-Preserving Superresolution Reconstruction of Infrared Images","2021","International Journal of Digital Multimedia Broadcasting","2021","","5519508","","","","10.1155/2021/5519508","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112061583&doi=10.1155%2f2021%2f5519508&partnerID=40&md5=4a5bb549fe64c12e9dea310283e2b29d","The convolutional neural network has achieved good results in the superresolution reconstruction of single-frame images. However, due to the shortcomings of infrared images such as lack of details, poor contrast, and blurred edges, superresolution reconstruction of infrared images that preserves the edge structure and better visual quality is still challenging. Aiming at the problems of low resolution and unclear edges of infrared images, this work proposes a two-stage generative adversarial network model to reconstruct realistic superresolution images from four times downsampled infrared images. In the first stage of the generative adversarial network, it focuses on recovering the overall contour information of the image to obtain clear image edges; the second stage of the generative adversarial network focuses on recovering the detailed feature information of the image and has a stronger ability to express details. The infrared image superresolution reconstruction method proposed in this work has highly realistic visual effects and good objective quality evaluation results.  © 2021 Yuqing Zhao et al.","Convolutional neural networks; Infrared imaging; Optical resolving power; Adversarial networks; Contour information; Feature information; Image super-resolution reconstruction; Objective qualities; Single frame image; Super resolution; Super-resolution reconstruction; Image reconstruction","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112061583"
"Meng H.; Guo F.","Meng, Han (57576671600); Guo, Fangru (57576463600)","57576671600; 57576463600","Image Classification and Generation Based on GAN Model","2021","Proceedings - 2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence, MLBDBI 2021","","","","180","183","3","10.1109/MLBDBI54094.2021.00042","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128183211&doi=10.1109%2fMLBDBI54094.2021.00042&partnerID=40&md5=8925ece7954d20bc0e0b7a2b3b6b610d","The topic of image processing is becoming more and more popular in the field of artificial intelligence, and it can be applied to fields of biology, medicine, video games, art, and etc. In order to have a deeper understanding of how to optimize the image processing, this paper mainly proposed the Generative Adversarial Network (GAN), which is an emerging deep learning model with the ability to continuously improve modeling under the game, and there are already many applications related to image processing, such as video prediction, 3-dimensional object generation, image super-resolution and etc. In this paper, we mainly implement image generation and image classification based on GAN model. In order to indicate the performance of GAN model in image generation in detail, GAN models with linear layers and with convolution layers are trained and compared based on MNIST datasets. Furthermore, we train GAN model with linear layers, and GAN model with convolution layers, Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Residual Network (ResNet) models in image classification based on the Mixed National Institute of Standards and Technology database (MNIST), and receive the training loss and testing accuracy of these models for different epochs in image classification. The experimental results demonstrated that GAN model with convolution layers performs best in both image generation and image classification. © 2021 IEEE.","Arts computing; Classification (of information); Computer vision; Convolution; Convolutional neural networks; Deep learning; Image classification; Image enhancement; Multilayer neural networks; Experimental analysis; Games art; Generative adversarial network model; Image generations; Images classification; Images processing; Learning models; National Institute of Standards and Technology; Network models; Video-games; Generative adversarial networks","Experimental analysis; GAN model; Image classification; Image generation","Conference paper","Final","","Scopus","2-s2.0-85128183211"
"Velumani K.; Lopez-Lozano R.; Madec S.; Guo W.; Gillet J.; Comar A.; Baret F.","Velumani, K. (57196024403); Lopez-Lozano, R. (16047865200); Madec, S. (57197871882); Guo, W. (54414194300); Gillet, J. (57216281128); Comar, A. (55053583500); Baret, F. (7006967825)","57196024403; 16047865200; 57197871882; 54414194300; 57216281128; 55053583500; 7006967825","Estimates of maize plant density from UAV RGB images using faster-RCNN detection model: Impact of the spatial resolution","2021","Plant Phenomics","2021","","9824843","","","","10.34133/2021/9824843","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114380677&doi=10.34133%2f2021%2f9824843&partnerID=40&md5=e74bd65e20ac8cec2586b1f01fcbdfa9","Early-stage plant density is an essential trait that determines the fate of a genotype under given environmental conditions and management practices. The use of RGB images taken from UAVs may replace the traditional visual counting in fields with improved throughput, accuracy, and access to plant localization. However, high-resolution images are required to detect the small plants present at the early stages. This study explores the impact of image ground sampling distance (GSD) on the performances of maize plant detection at three-to-five leaves stage using Faster-RCNN object detection algorithm. Data collected at high resolution (GSD ≈ 0:3 cm) over six contrasted sites were used for model training. Two additional sites with images acquired both at high and low (GSD ≈ 0:6 cm) resolutions were used to evaluate the model performances. Results show that Faster-RCNN achieved very good plant detection and counting (rRMSE = 0:08) performances when native high-resolution images are used both for training and validation. Similarly, good performances were observed (rRMSE = 0:11) when the model is trained over synthetic low-resolution images obtained by downsampling the native training high-resolution images and applied to the synthetic low-resolution validation images. Conversely, poor performances are obtained when the model is trained on a given spatial resolution and applied to another spatial resolution. Training on a mix of high- and low-resolution images allows to get very good performances on the native high-resolution (rRMSE = 0:06) and synthetic low-resolution (rRMSE = 0:10) images. However, very low performances are still observed over the native low-resolution images (rRMSE = 0:48), mainly due to the poor quality of the native low-resolution images. Finally, an advanced super resolution method based on GAN (generative adversarial network) that introduces additional textural information derived from the native high-resolution images was applied to the native low-resolution validation images. Results show some significant improvement (rRMSE = 0:22) compared to bicubic upsampling approach, while still far below the performances achieved over the native high-resolution images. Copyright © 2021 K. Velumani et al.","Aircraft detection; Image resolution; Object detection; Plants (botany); Signal sampling; Unmanned aerial vehicles (UAV); Environmental conditions; Ground sampling distances; High resolution image; Low resolution images; Management practices; Object detection algorithms; Superresolution methods; Textural information; Image enhancement","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85114380677"
"Tayba M.; Rivas P.","Tayba, Mahee (57796084700); Rivas, Pablo (57207780751)","57796084700; 57207780751","Enhancing the Resolution of Satellite Imagery Using a Generative Model","2021","Proceedings - 2021 International Conference on Computational Science and Computational Intelligence, CSCI 2021","","","","20","25","5","10.1109/CSCI54926.2021.00010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133933088&doi=10.1109%2fCSCI54926.2021.00010&partnerID=40&md5=5a84748693b9323aef2ab6b51e11d02b","Recent breakthroughs in deep learning algorithms introduced the image super-resolution technique that maps the low-resolution image to generate a high-resolution image. These techniques increase various surveillance applications by providing finer spatial details than data from original sensors. Satellite images obtained from Moderate Resolution Imaging Spectroradiometer (MODIS) observation offer essential information about the earth's landscape, ocean, and ecosystem, contributing to monitoring various applications in the scientific field. The spatial resolution of satellite images has a significant impact on image accuracy. This paper focuses on improving image resolution by training a convolutional neural network to produce super-resolution images from low-resolution images. We present an implementation of Super Resolution Generative Adversarial Network (SRGAN), a GAN-based approach that uses a perceptual loss function that includes an adversarial loss and a content loss. Using a discriminator network that is designed for discerning between super-resolved images and original photo-realistic images, the adversarial loss drives the solution of this architecture to natural images. Moreover, the content loss is driven by perceptual similarity rather than pixel space similarity. We used this architecture to satellite images collected from NASA MODIS devices and found satisfactory results. Our key finding is that our system's result can now be used to improve a variety of low-resolution images. © 2021 IEEE.","Convolutional neural networks; Deep neural networks; Digital storage; Generative adversarial networks; Image enhancement; NASA; Network architecture; Radiometers; Satellite imagery; GAN; Generative model; High-resolution images; Image super resolutions; Low resolution images; Moderate-resolution imaging spectroradiometers; Resolution techniques; Satellite images; Superresolution; Surveillance applications; Image resolution","deep neural network; GAN; image resolution; satellite image; super resolution","Conference paper","Final","","Scopus","2-s2.0-85133933088"
"Mitra S.; Rammer D.; Pallickara S.; Pallickara S.L.","Mitra, Saptashwa (57204012893); Rammer, Daniel (57147757400); Pallickara, Shrideep (6602168326); Pallickara, Sangmi Lee (8982898100)","57204012893; 57147757400; 6602168326; 8982898100","Glance: A Generative Approach to Interactive Visualization of Voluminous Satellite Imagery","2021","Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021","","","","359","367","8","10.1109/BigData52589.2021.9671537","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125295563&doi=10.1109%2fBigData52589.2021.9671537&partnerID=40&md5=31d82a0ea4f11cab8aef3c901705ad01","Challenges in interactive visualizations over satellite data collections stem primarily from their inherent data volumes. Enabling interactive visualizations of such data results in both processing and I/O (network and disk) on the server side. These are further exacerbated by multiple, concurrent requests issued by different clients. Hotspots may also arise when multiple users are interested in a particular geographical extent. We propose a novel methodology to support interactive visualizations over voluminous satellite imagery. Our system, codenamed Glance, generates models that once installed on the client side, substantially alleviate resource requirements on the server side. Our system dynamically generates imagery during zoom-in operations. Glance also supports image refinements using partial high-resolution information when available. Glance is based broadly on a deep Generative Adversarial Network, and our model is space-efficient to facilitate memory-residency at the clients. We supplement Glance with a module to estimate rendering errors when using the model to generate imagery as opposed to a resource-intensive query-and-retrieve operation to the server. Benchmarks to profile our methodology show substantive improvements in interactivity with up to 23x reduction in time lags without utilizing GPU and 297x-6627x reduction while harnessing GPU. Further, the perceptual quality of the images from our generative model is robust with PSNR values ranging from 32.2-40.5, depending on the scenario and upscale factor. © 2021 IEEE.","Computer vision; Digital storage; Generative adversarial networks; Graphics processing unit; Image enhancement; Satellite imagery; % reductions; Data volume; Edge enhancements; In-memory storage; Interactive visualizations; Memory storage; Satellite data collection; Server sides; Superresolution; Visual analytics; Visualization","edge enhancement; generative adversarial networks; in-memory storage; super-resolution; visual analytics","Conference paper","Final","","Scopus","2-s2.0-85125295563"
"Li X.; Chen Z.","Li, Xiaomeng (57209272730); Chen, Zhaoxue (55647617500)","57209272730; 55647617500","Research on face super-resolution reconstruction algorithm based on GAN; [基于GAN的人脸超分辨率重建算法研究]","2021","Guangxue Jishu/Optical Technique","47","1","","101","106","5","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101484879&partnerID=40&md5=fff494c869f9d32dc7c1e1f3cfc1580b","Aiming at the problems of insufficient detail and over smooth in current face super-resolution algorithms, an algorithm for the Single Image Super-Resolution Reconstruction based on the Generative Adversarial Network(GAN) is proposed. The algorithm connects the edge detection network in parallel in the generation network, extracting abundant face contour details to assist in feature extraction, optimizing the network training process through the Ranger optimizer. Finally, establish a mathematical model to comprehensively evaluate the reconstruction effect combining objective assessment and subjective assessment indicators. The experimental results show that the algorithm has better subjective and objective effects than the Cubic Spline Method, SRGAN, FSRCNN, etc. It is proved that the algorithm improves the reconstruction ability of facial details and has a better reconstruction effect. © 2021, Editorial Board of Optical Technique. All right reserved.","","Deep learning; Edge enhanced; Generative adversarial network; Super resolution","Article","Final","","Scopus","2-s2.0-85101484879"
"Lin H.; Fan J.; Zhang Y.; Peng D.","Lin, Hong (57198047109); Fan, Jing (57222593888); Zhang, Yangyi (57213175059); Peng, Dewei (7202530516)","57198047109; 57222593888; 57213175059; 7202530516","Generative adversarial image super-resolution network for multiple degradations","2020","IET Image Processing","14","17","","4520","4527","7","10.1049/iet-ipr.2020.1176","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103352716&doi=10.1049%2fiet-ipr.2020.1176&partnerID=40&md5=678db114e474fee98623696340a61f2e","The existing single image super-resolution methods based on deep learning cannot handle multiple degradations well, and the generated image tends to be blurred and over-smoothed due to poor generalisation ability. In this study, the authors propose a method based on a generative adversarial network (GAN) to deal with multiple degradations. In the generator network, blur kernel and noise level are used as input through dimensionality stretching strategy preprocessing to make full use of prior knowledge. In addition, three discriminators with different scales are used in the discriminator network to pay attention to the reconstruction of image details while focusing on the global consistency of the image. For the problems of vanishing gradient and mode collapse existing in GAN-based methods, a gradient penalty term is added in the loss function. Extensive experiments demonstrate that the proposed method not only can handle multiple degradations to obtain state-of-the-art performance, but also deliver visually credible results in real scenes. © The Institution of Engineering and Technology 2020.","Deep learning; Adversarial networks; Global consistency; Image super resolutions; Loss functions; Multiple degradations; Prior knowledge; State-of-the-art performance; Vanishing gradient; Optical resolving power","","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85103352716"
"Tao Y.; Conway S.J.; Mulle J.-P.; Putri A.R.D.; Thomas N.; Cremonese G.","Tao, Yu (56539197700); Conway, Susan J. (56468905300); Mulle, Jan-Peter (57223337753); Putri, Alfiah R. D. (56348927300); Thomas, Nicolas (7401830482); Cremonese, Gabriele (56240953900)","56539197700; 56468905300; 57223337753; 56348927300; 7401830482; 56240953900","Single image super-resolution restoration of tgo cassis colour images: Demonstration with perseverance rover landing site and mars science targets","2021","Remote Sensing","13","9","1777","","","","10.3390/rs13091777","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105613199&doi=10.3390%2frs13091777&partnerID=40&md5=3f0d040f86a11fa63f4bf131fb23cbaf","The ExoMars Trace Gas Orbiter (TGO)’s Colour and Stereo Surface Imaging System (CaSSIS) provides multi-spectral optical imagery at 4-5m/pixel spatial resolution. Improving the spatial resolution of CaSSIS images would allow greater amounts of scientific information to be extracted. In this work, we propose a novel Multi-scale Adaptive weighted Residual Super-resolution Generative Adversarial Network (MARSGAN) for single-image super-resolution restoration of TGO CaSSIS images, and demonstrate how this provides an effective resolution enhancement factor of about 3 times. We demonstrate with qualitative and quantitative assessments of CaSSIS SRR results over the Mars2020 Perseverance rover’s landing site. We also show examples of similar SRR performance over 8 science test sites mainly selected for being covered by HiRISE at higher resolution for comparison, which include many features unique to the Martian surface. Application of MARSGAN will allow high resolution colour imagery from CaSSIS to be obtained over extensive areas of Mars beyond what has been possible to obtain to date from HiRISE. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Calcium compounds; Color; Image enhancement; Image reconstruction; Image resolution; Optical resolving power; Restoration; Scales (weighing instruments); Adversarial networks; Effective resolutions; Higher resolution; Qualitative and quantitative assessments; Scientific information; Spatial resolution; Super resolution; Weighted residuals; Stereo image processing","CaSSIS; Frost; GAN; Generative Adversarial Network; Gullies; Jezero Crater; Mars2020; Perseverance; RSL; Slope streaks; SRR; Super-resolution restoration; TGO","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85105613199"
"Yu H.; Liu Y.; He S.; Jiang P.; Xin J.; Wen J.","Yu, Hufei (57217385849); Liu, Yifeng (57203151266); He, Shiwen (36496789400); Jiang, Pei (57219940657); Xin, Jiang (57219938163); Wen, Jingxi (57219942417)","57217385849; 57203151266; 36496789400; 57219940657; 57219938163; 57219942417","A practical generative adversarial network architecture for restoring damaged character photographs","2021","Neurocomputing","423","","","590","600","10","10.1016/j.neucom.2020.10.065","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096175503&doi=10.1016%2fj.neucom.2020.10.065&partnerID=40&md5=417dc5140cea376bf619b36c40dae8d5","Recently, deep learning has been applied to many image restoration tasks. In this work, we focus on studying an efficient deep learning architecture to restore damaged character photographs (DCPs) which are spoiled by natural or human factors including creases, spots, cracks, light, etc. A large amount of work has focused on image restoration such as super-resolution, image inpainting, image deblurring, and image denoising. However, few studies focus on restoring DCPs based on deep learning since DCPs are varied and complex, along with the difficulty of getting paired training dataset. In this work, we propose a new generative adversarial network (GAN) architecture to restore DCPs. Specifically, a residual U-Net (ResU-Net) GAN (RUGAN) is firstly constructed to generate fake DCPs by employing real DCPs, clear character photographs (CCPs), and dirty masks. Then, a ResU-Net conditional GAN (RUCGAN) is built to restore DCPs by exploiting paired CCPs and fake DCPs. To further improve the quality of restored character photographs, a weighted multi-features loss function is adopted in RUCGAN. Finally, numerical results show that our approach can restore spots, creases, cracks, and other spoiled manners in DCPs. © 2020 Elsevier B.V.","Deep learning; Image denoising; Image enhancement; Network architecture; Photography; Restoration; Adversarial networks; Image deblurring; Image Inpainting; Learning architectures; Loss functions; Numerical results; Super resolution; Training dataset; Article; artificial neural network; controlled study; convolutional neural network; deep learning; generative adversarial network; information processing; mathematical parameters; measurement accuracy; peak signal noise ratio; priority journal; process optimization; restoring damaged character photograph; signal noise ratio; structural similarity index; Image reconstruction","Damaged photographs restoration; Deep learning; GAN","Article","Final","","Scopus","2-s2.0-85096175503"
"Chen S.; Wang Y.; Wang M.","Chen, Shuwang (34869274700); Wang, Yun (57360174900); Wang, Meng (57360318100)","34869274700; 57360174900; 57360318100","Medical CT image amplification and reconstruction system based on deep learning","2021","Proceedings of SPIE - The International Society for Optical Engineering","11897","","118970B","","","","10.1117/12.2599491","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120480762&doi=10.1117%2f12.2599491&partnerID=40&md5=bfeb0d0e2b4f121732fec9d39ec5c9cc","Medical CT image amplification and reconstruction system based on deep learning.Shu wang Chen, Yun Wang, Meng Wang.Institute of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang. SRGAN (Super-Resolution Generative Adversarial Networks) algorithm is used in the system and the medical CT image super-resolution reconstruction is completed. The medical CT image processed by this method can better reflect the various details of the image, which is conducive to the observation and correct diagnosis for the doctors.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Computerized tomography; Deep learning; Diagnosis; Engineering education; Generative adversarial networks; Medical image processing; Optical resolving power; Image amplification; Image magnification; Images reconstruction; Medical CT image; Network algorithms; Reconstruction systems; Science and engineering; Super-resolution generative adversarial network algorithm; Super-resolution reconstruction; Superresolution; Image reconstruction","image magnification; image reconstruction; medical CT image; SRGAN algorithm; super resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85120480762"
"Xia L.; Zhu J.; Yu Z.","Xia, Limin (7201955974); Zhu, Jiahui (57222713110); Yu, Zhimin (57222713541)","7201955974; 57222713110; 57222713541","Real-World Person Re-Identification via Super-Resolution and Semi-Supervised Methods","2021","IEEE Access","9","","9366508","35834","35845","11","10.1109/ACCESS.2021.3063000","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103759716&doi=10.1109%2fACCESS.2021.3063000&partnerID=40&md5=8762087e2960f9cea95ace281729c74f","Person re-identification has made great progress over the years. However, due to the problem of super-resolution and few labeled samples, it is difficult to apply in practice. In this paper, we propose a semi-supervised super-resolution person re-identification method based on soft multi-labels. Firstly, a Mixed-Space Super-Resolution model (MSSR) is constructed based on Generative Adversarial Networks (GAN), which aims to convert low-resolution person images into high-resolution images. Secondly, a Part-based Graph Convolutional Network (PGCN) is proposed to extract discriminative feature by exploring the relationship of local features within person. Finally, to solve the problem of label limitation, we use the PGCN trained with a small amount of labeled samples to predict the soft multi-labels of unlabeled samples, and further train PGCN with unlabeled samples based on a novel multi-label similarity loss. Experiments have been conducted on the Market1501, CUHK03, and MSMT17 datasets to evaluate this method, which show that it outperforms other semi-supervised methods.  © 2013 IEEE.","Convolutional neural networks; Optical resolving power; Adversarial networks; Convolutional networks; Discriminative features; High resolution image; Person re identifications; Semi-supervised method; Super-resolution models; Unlabeled samples; Semi-supervised learning","GAN; multi-labels; Person re-identification; semi-supervised; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103759716"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","6","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119981158&partnerID=40&md5=0e21d108a21fc4c36bf987472299c1d9","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119981158"
"Xu W.; Zhang Z.; You X.; Zhang C.","Xu, Weihong (57200276544); Zhang, Zaichen (55617298400); You, Xiaohu (7202297685); Zhang, Chuan (55703891600)","57200276544; 55617298400; 7202297685; 55703891600","Reconfigurable and Low-Complexity Accelerator for Convolutional and Generative Networks over Finite Fields","2020","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","39","12","8994049","4894","4907","13","10.1109/TCAD.2020.2973355","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079644267&doi=10.1109%2fTCAD.2020.2973355&partnerID=40&md5=f4a96de5945cd31ff0d3c49ae08d83b3","Convolutional neural networks (CNNs) have gained great success in various fields, such as computer vision and natural language processing. Besides, with the breakthrough in unsupervised learning, generative adversarial network (GAN) is recently utilized to generate virtual data from limited data sets. The generative model of GAN has impressive applications, such as style transfer and image super-resolution. However, the promising performance of CNN and GAN comes at the cost of prohibitive computation complexity. The convolution (CONV) in CNN and the transposed CONV (TCONV) in GAN are the two operations that dominant the overall complexity. The prior works exploit the fast algorithms, Winograd and fast Fourier transform (FFT), to reduce the complexity of spatial CONV. However, Winograd only supports fixed filter size while FFT has high transform overhead. Moreover, very few works apply fast algorithms to accelerate GAN models. In this article, a reconfigurable and low-complexity accelerator on ASIC for both CNN and GAN is proposed to address these problems. First, by exploiting Fermat number transform (FNT), we propose two FNT-based fast algorithms to reduce the complexity of CONV and TCONV computations, respectively. Then the architectures of the FNT-based accelerator are presented to implement the proposed fast algorithms. The methodology to determine the design parameters and optimize the dataflow is also described for obtaining maximum performance and optimal efficiency. Moreover, we implement the proposed accelerator on 65 nm 1P9M technology and evaluate it on various CNN and GAN models. The post-layout results show that our design achieves a throughput of 288.0 GOP/s on VGG-16 with 25.11 GOP/s/mm2 area efficiency, which is superior to the state-of-the-art CNN accelerators. Furthermore, at least 1.7times speedup over the existing accelerators is obtained on GAN. The resulting energy efficiency is 275.3times and 12.5times of CPU and GPU. © 1982-2012 IEEE.","Acceleration; Complex networks; Convolution; Energy efficiency; Fast Fourier transforms; Natural language processing systems; Network architecture; Number theory; Reconfigurable architectures; Adversarial networks; Computation complexity; Design parameters; Fast convolution; Fermat number transform; Image super resolutions; NAtural language processing; Optimal efficiency; Convolutional neural networks","Convolutional neural network (CNN); fast convolution (CONV); Fermat number transform (FNT); generative network; reconfigurable architectures","Article","Final","","Scopus","2-s2.0-85079644267"
"Mitra S.; Rammer D.; Pallickara S.; Pallickara S.L.","Mitra, Saptashwa (57204012893); Rammer, Daniel (57147757400); Pallickara, Shrideep (6602168326); Pallickara, Sangmi Lee (8982898100)","57204012893; 57147757400; 6602168326; 8982898100","A Generative Approach to Visualizing Satellite Data","2021","Proceedings - IEEE International Conference on Cluster Computing, ICCC","2021-September","","","815","816","1","10.1109/Cluster48925.2021.00079","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125994216&doi=10.1109%2fCluster48925.2021.00079&partnerID=40&md5=38841712ba9313e3a702011c690848ac","We propose EVOKE, a model based on progressive Generative Adversarial Networks, that dynamically reconstructs high-resolution imagery during zoom-in operations using inmemory historical low-resolution images and is space-efficient to facilitate memory-residency at the clients.  ©2021 IEEE.","Computer vision; Digital storage; Visualization; High resolution imagery; In-memory storage; Low resolution images; Memory storage; Model-based OPC; Satellite data; Space efficient; Superresolution; Visual analytics; Generative adversarial networks","Generative adversarial networks; In-memory storage; Super-resolution; Visual analytics","Conference paper","Final","","Scopus","2-s2.0-85125994216"
"Yoon J.; Kim T.; Choe Y.","Yoon, Jongsu (57197796640); Kim, Taehyeon (57204395308); Choe, Yoonsik (7102998299)","57197796640; 57204395308; 7102998299","GAN based Single Image Super-Resolution via Spatially Adaptive De-normalization","2021","Transactions of the Korean Institute of Electrical Engineers","70","2","","402","407","5","10.5370/KIEE.2021.70.2.402","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102338554&doi=10.5370%2fKIEE.2021.70.2.402&partnerID=40&md5=75ee2200d42d856a10629252350440c9","Despite recent advances in technologies on single image super-resolution using deep neural networks, the key question still remains how to recover finer textures and edges. To solve this super-resolution problem, many recent researches have been using conditional generative adversarial network. However, restoring high resolution images using conditional generative adversarial network is disadvantageous in expressing fine textures and edges because there occurs the loss of spatial and high frequency informations. In this paper, informations on images in different scales are added hierarchically by using a spatially adaptive de-normalization method. This method can restore fine textures and edges of an image by inserting different scale informations for each layers in pyramid structure. In experimental results, the efficiency of the proposed method is proved by showing better performance to restore textures and edges in high quality, comparing with other state-of-the art techniques. © 2021 Korean Institute of Electrical Engineers. All rights reserved.","Deep neural networks; Image reconstruction; Optical resolving power; Restoration; Textures; Adversarial networks; High resolution image; High-frequency informations; Normalization methods; Pyramid structure; Recent researches; Spatially adaptive; State-of-the-art techniques; Image texture","Conditional generative adversarial network; Spatially adaptive de-normalization; Super-resolution","Article","Final","","Scopus","2-s2.0-85102338554"
"Gauding M.; Bode M.","Gauding, Michael (26221578700); Bode, Mathis (56135121000)","26221578700; 56135121000","Using Physics-Informed Enhanced Super-Resolution Generative Adversarial Networks to Reconstruct Mixture Fraction Statistics of Turbulent Jet Flows","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12761 LNCS","","","138","153","15","10.1007/978-3-030-90539-2_9","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119821410&doi=10.1007%2f978-3-030-90539-2_9&partnerID=40&md5=23cbb5086a0be04001d288e10e5e6959","This work presents the full reconstruction of coarse-grained turbulence fields in a planar turbulent jet flow by a deep learning framework for large-eddy simulations (LES). Turbulent jet flows are characterized by complex phenomena such as intermittency and external interfaces. These phenomena are strictly non-universal and conventional LES models have shown only limited success in modeling turbulent mixing in such configurations. Therefore, a deep learning approach based on physics-informed enhanced super-resolution generative adversarial networks (Bode et al., Proceedings of the Combustion Institute, 2021) is utilized to reconstruct turbulence and mixture fraction fields from coarse-grained data. The usability of the deep learning model is validated by applying it to data obtained from direct numerical simulations (DNS) with more than 78 Billion degrees of freedom. It is shown that statistics of the mixture fraction field can be recovered from coarse-grained data with good accuracy. © 2021, Springer Nature Switzerland AG.","Deep learning; Degrees of freedom (mechanics); Large eddy simulation; Mixtures; Optical resolving power; Turbulence; Coarse-grained; Flow byes; Intermittency; Jetflows; Large-eddy simulations; Learning frameworks; Mixture fraction; Superresolution; Turbulence fields; Turbulent jet; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85119821410"
"Zhang T.; Wang H.; Cheng R.; Li Z.; Ma Y.; Wu L.","Zhang, Tong (57226045067); Wang, Huajun (55689029400); Cheng, Ruihong (57225129322); Li, Zhongyu (57225184353); Ma, Yu (57308634200); Wu, Linfeng (57225127032)","57226045067; 55689029400; 57225129322; 57225184353; 57308634200; 57225127032","Super-resolution reconstruction of image based on generative adversarial network with attention module","2021","Proceedings of SPIE - The International Society for Optical Engineering","11848","","1184805","","","","10.1117/12.2600169","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109211316&doi=10.1117%2f12.2600169&partnerID=40&md5=8c59928866382cae1575baa439c30ab4","In order to solve the problems of unstable training and texture blurring of generated images, we proposed a generative adversarial network combining residual and attention block. The attention module is added to the network, which reduces the dependence on the network depth and reduces the depth of the model. The dense connection in the residual module can extract richer image details. The number of parameters is reduced and the calculation efficiency is greatly improved. Generative adversarial network is used to further improve the texture details of the image. Generator loss functions include a content loss, a perceptual loss, a texture loss and an adversarial loss. The texture loss is used to enhance the matching degree of local information, and the perceptual loss is used to obtain more detailed features by using the feature information before an activation layer. The experimental results show that the peak signal to noise ratio is 32.10 dB, and the structural similarity is 0.92. Compared with bicubic, SRCNN, VDSR and SRGAN, the proposed algorithm improves the texture details of reconstructed images.  © 2021 SPIE.","Image enhancement; Image reconstruction; Signal to noise ratio; Textures; Adversarial networks; Calculation efficiency; Feature information; Local information; Peak signal to noise ratio; Reconstructed image; Structural similarity; Super resolution reconstruction; Image texture","Attention module; Generative adversarial network; Loss function; Super-resolution; Texture feature","Conference paper","Final","","Scopus","2-s2.0-85109211316"
"","","","7th International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2021","2021","Communications in Computer and Information Science","1451","","","","","1055","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115417831&partnerID=40&md5=8e3ecb39f537ed5de5801347839e6843","The proceedings contain 81 papers. The special focus in this conference is on Pioneering Computer Scientists, Engineers and Educators. The topics include: MA Mask R-CNN: MPR and AFPN Based Mask R-CNN; improved Non-negative Matrix Factorization Algorithm for Sparse Graph Regularization; a Blockchain-Based Scheme of Data Sharing for Housing Provident Fund; intelligent Storage System of Machine Learning Model Based on Task Similarity; predicting Stock Price Movement with Multiple Data Sources and Machine Learning Models; channel Context and Dual-Domain Attention Based U-Net for Skin Lesion Attributes Segmentation; study on the Protection and Product Development of Intangible Cultural Heritage with Computer Virtual Reality Technology; ECG-Based Arrhythmia Detection Using Attention-Based Convolutional Neural Network; Quantum Color Image Scaling on QIRHSI Model; WSN Data Compression Model Based on K-SVD Dictionary and Compressed Sensing; human Body Pose Recognition System Based on Teaching Interaction; adaptive Densely Residual Network for Image Super-Resolution; Real-Time Image and Video Artistic Style Rendering System Based on GPU; semantic Segmentation of High Resolution Remote Sensing Images Based on Improved ResU-Net; Exploring Classification Capability of CNN Features; generative Adversarial Network Based Status Generation Simulation Approach; the Construction of Case Event Logic Graph for Judgment Documents; anti-obfuscation Binary Code Clone Detection Based on Software Gene; thread Private Variable Access Optimization Technique for Sunway High-Performance Multi-core Processors; parallel Region Reconstruction Technique for Sunway High-Performance Multi-core Processors; research on Route Optimization of Battlefield Collection Equipment Based on Improved Ant Algorithm; a Collaborative Cache Strategy Based on Utility Optimization; integrating Local Closure Coefficient into Weighted Networks for Link Prediction.","","","Conference review","Final","","Scopus","2-s2.0-85115417831"
"Gajera B.V.; Kapil S.R.; Ziaei D.; Mangalagiri J.; Siegel E.; Chapman D.","Gajera, Binit Vasant (57226288398); Kapil, Siddhant Raj (57224851018); Ziaei, Dorsa (57212306453); Mangalagiri, Jayalakshmi (57221142562); Siegel, Eliot (57613992900); Chapman, David (57203382252)","57226288398; 57224851018; 57212306453; 57221142562; 57613992900; 57203382252","CT-Scan Denoising Using a Charbonnier Loss Generative Adversarial Network","2021","IEEE Access","9","","9448108","84093","84109","16","10.1109/ACCESS.2021.3087424","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111029823&doi=10.1109%2fACCESS.2021.3087424&partnerID=40&md5=04223992aee2d2a7cd8fed25527d3260","We propose a Generative Adversarial Network (GAN) optimized for noise reduction in CT-scans. The objective of CT scan denoising is to obtain higher quality imagery using a lower radiation exposure to the patient. Recent work in computer vision has shown that the use of Charbonnier distance as a term in the perceptual loss of a GAN can improve the performance of image reconstruction and video super-resolution. However, the use of a Charbonnier structural loss term has not yet been applied or evaluated for the purpose of CT scan denoising. Our proposed GAN makes use of a Wasserstein adversarial loss, a pretrained VGG19 perceptual loss, as well as a Charbonnier distance structural loss. We evaluate our approach using both applied Poisson noise distribution in order to simulate low-dose CT imagery, as well as using an anthropomorphic thoracic phantom at different exposure levels. Our evaluation criteria are Peek Signal to Noise (PSNR) as well as Structured Similarity (SSIM) of the denoised images, and we compare the results of our method versus recent state of the art deep denoising GANs. In addition, we report global noise through uniform soft tissue mediums. Our findings show that the incorporation of the Charbonnier Loss with the VGG-19 network improves the performance of the denoising as measured with the PSNR and SSIM, and that the method greatly reduces soft tissue noise to levels comparable to the NDCT scan.  © 2013 IEEE.","Image enhancement; Image reconstruction; Petroleum reservoir evaluation; Poisson distribution; Signal to noise ratio; Tissue; Adversarial networks; Evaluation criteria; Exposure level; Poisson noise; Radiation Exposure; Signal to noise; Structural loss; Video super-resolution; Computerized tomography","computed tomography; CT-scan denoising; generative adversarial network; machine learning; medical diagnostic imaging","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111029823"
"","","","43rd DAGM German Conference on Pattern Recognition, DAGM GCPR 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13024 LNCS","","","","","724","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124255688&partnerID=40&md5=80f2d2283daf27f975494fc5681a7bbf","The proceedings contain 46 papers. The special focus in this conference is on Pattern Recognition. The topics include: Sublabel-Accurate Multilabeling Meets Product Label Spaces; investigating the Consistency of Uncertainty Sampling in Deep Active Learning; scaleNet: An Unsupervised Representation Learning Method for Limited Information; a New Split for Evaluating True Zero-Shot Action Recognition; video Instance Segmentation with Recurrent Graph Neural Networks; distractor-Aware Video Object Segmentation; (SP)2 Net for Generalized Zero-Label Semantic Segmentation; contrastive Representation Learning for Hand Shape Estimation; Fusion-GCN: Multimodal Action Recognition Using Graph Convolutional Networks; FIFA: Fast Inference Approximation for Action Segmentation; Hybrid SNN-ANN: Energy-Efficient Classification and Object Detection for Event-Based Vision; infoSeg: Unsupervised Semantic Image Segmentation with Mutual Information Maximization; a Comparative Study of PnP and Learning Approaches to Super-Resolution in a Real-World Setting; Merging-ISP: Multi-exposure High Dynamic Range Image Signal Processing; spatiotemporal Outdoor Lighting Aggregation on Image Sequences; AttrLostGAN: Attribute Controlled Image Synthesis from Reconfigurable Layout and Style; learning Conditional Invariance Through Cycle Consistency; CAGAN: Text-To-Image Generation with Combined Attention Generative Adversarial Networks; txT: Crossmodal End-to-End Learning with Transformers; diverse Image Captioning with Grounded Style; leveraging Group Annotations in Object Detection Using Graph-Based Pseudo-labeling; quantifying Uncertainty of Image Labelings Using Assignment Flows; sampling-Free Variational Inference for Neural Networks with Multiplicative Activation Noise; implicit and Explicit Attention for Zero-Shot Learning; self-supervised Learning for Object Detection in Autonomous Driving; Assignment Flows and Nonlocal PDEs on Graphs; viewpoint-Tolerant Semantic Segmentation for Aerial Logistics; t6D-Direct: Transformers for Multi-object 6D Pose Direct Regression; tetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases; detecting Slag Formations with Deep Convolutional Neural Networks; virtual Temporal Samples for Recurrent Neural Networks: Applied to Semantic Segmentation in Agriculture; weakly Supervised Segmentation Pretraining for Plant Cover Prediction; how Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?.","","","Conference review","Final","","Scopus","2-s2.0-85124255688"
"Gupta S.; Venkata Suryanarayana K.; Kudavelly S.R.; Ramaraju G.A.","Gupta, Saumya (57222665868); Venkata Suryanarayana, K. (57222666170); Kudavelly, Srinivas Rao (24829192800); Ramaraju, G.A. (45061232500)","57222665868; 57222666170; 24829192800; 45061232500","Ovarian assessment using deep learning-based 3D ultrasound super resolution","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11597","","115970K","","","","10.1117/12.2581286","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103691199&doi=10.1117%2f12.2581286&partnerID=40&md5=99e22dc823fda015b75ce2517c00b7d8","Ovarian volume assessment is the measurement of the size of ovaries during an Ultrasound (US) in order to estimate the ovarian reserve. Since the ovarian reserve is used in calculating a woman's reproductive age and is also a diagnostic criterion for polycystic ovary syndrome (PCOS), it is imperative that it is measured accurately. Furthermore, ovarian rendering has clinical significance in terms of assessing ovarian anomalies (ovarian surface epithelial cells). Thus if the spacing in the US volume is high along one direction, reducing the spacing would greatly help in both the accurate measurement of the ovarian volume as well as surface assessment. In this paper, we aim to address this problem by developing a deep learning method for super-resolving 3D US data along the axial direction. On the collected dataset, our method has achieved high PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index) values, and has also resulted in a 54% improvement in ovarian volume computation accuracy. Furthermore, our solution has improved the quality of the 3D rendering of the ovary, and has also reduced the problem of fused follicles in segmentation. This proves the viability of our approach for clinical diagnostic assessment. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Computer aided diagnosis; Learning systems; Medical imaging; Rendering (computer graphics); Signal to noise ratio; Three dimensional computer graphics; Ultrasonics; Accurate measurement; Clinical diagnostics; Computation accuracy; Diagnostic criterion; Polycystic ovary syndromes; PSNR (peak signal to noise ratio); Structural similarity indices; Surface assessment; Deep learning","3D Ultrasound (US); Deep learning; Generative Adversarial Networks (GAN); Medical Imaging; Ovarian Assessment; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85103691199"
"Li S.; Qian P.; Zhang X.; Chen A.","Li, Size (57415839400); Qian, Pengjiang (36598989000); Zhang, Xin (57774448700); Chen, Aiguo (55447313100)","57415839400; 36598989000; 57774448700; 55447313100","Research on Image Denoising and Super-Resolution Reconstruction Technology of Multiscale-Fusion Images","2021","Mobile Information Systems","2021","","5184688","","","","10.1155/2021/5184688","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123000907&doi=10.1155%2f2021%2f5184688&partnerID=40&md5=85e9f9226314e878894303c4ec9c48ee","Image denoising and image super-resolution reconstruction are two important techniques for image processing. Deep learning is used to solve the problem of image denoising and super-resolution reconstruction in recent years, and it usually has better results than traditional methods. However, image denoising and super-resolution reconstruction are studied separately by state-of-the-art work. To optimally improve the image resolution, it is necessary to investigate how to integrate these two techniques. In this paper, based on Generative Adversarial Network (GAN), we propose a novel image denoising and super-resolution reconstruction method, i.e., multiscale-fusion GAN (MFGAN), to restore the images interfered by noises. Our contributions reflect in the following three aspects: (1) the combination of image denoising and image super-resolution reconstruction simplifies the process of upsampling and downsampling images during the model learning, avoiding repeated input and output images operations, and improves the efficiency of image processing. (2) Motivated by the Inception structure and introducing a multiscale-fusion strategy, our method is capable of using the multiple convolution kernels with different sizes to expand the receptive field in parallel. (3) The ablation experiments verify the effectiveness of each employed loss measurement in our devised loss function. And our experimental studies demonstrate that the proposed model can effectively expand the receptive field and thus reconstruct images with high resolution and accuracy and that the proposed MFGAN method performs better than a few state-of-the-art methods. Copyright © 2021 Size Li et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.","Deep learning; Generative adversarial networks; Image denoising; Image enhancement; Image fusion; Image resolution; Signal sampling; Art work; Fusion image; Image super-resolution reconstruction; Images processing; Multiscale fusion; Receptive fields; Reconstruction method; State of the art; Super-resolution reconstruction; Upsampling; Image reconstruction","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123000907"
"Jiang Y.; Li J.; Zhao J.","Jiang, Yuning (57863674900); Li, Jinhua (57196156927); Zhao, Junli (55903934600)","57863674900; 57196156927; 55903934600","Image Super-Resolution Reconstruction Algorithm Based on Generative Adversarial Networks","2021","Jisuanji Gongcheng/Computer Engineering","47","3","","249","255","6","10.19678/j.issn.1000-3428.0057168","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121859292&doi=10.19678%2fj.issn.1000-3428.0057168&partnerID=40&md5=0ed75d7cfc26161d78001f0e4615dd2d","The existing image Super-Resolution(SR)reconstruction algorithms have difficulty in network training and cause artifacts in the generated images. To address the problem, this paper proposes a SR reconstruction algorithm based on Generative Adversarial Networks(GAN). The Batch Normalization(BN)layer of the Generative Adversarial Networks (SRGAN)is removed to reduce the computational complexity, and the Residual Block(RB) is replaced by Residual Dense Block(RDB)to form the generative network.VGG19 network is used as the basic framework of the discrimination network. The Global Average Pooling(GAP)is used to replace the full connection layer to prevent over fitting. Texture loss function, perceptual loss function, adversarial loss function and content loss function are introduced to form the overall objective function of the generator. Texture loss is used to enhance the matching degree of local information, and the feature information in front of the activation layer is used to calculate the perceptual loss to obtain more detailed features. Wasserstein Generative Adversarial Nets-Gradient Penalty(WGAN-GP)theory is used to optimize the adversarial loss of the network model to accelerate the convergence, and content loss is used to improve the accuracy of low-frequency information of the image. Experimental results show that the average Peak Signal to Noise Ratio(PSNR)of the image reconstructed by the proposed algorithm is 27.97 dB, and its average Structural Similarity(SSIM)is 0.777.Compared with SRGAN and EDSR algorithms, the proposed algorithm improves the texture details and brightness of the reconstructed image, making it more in line with the requirements of visual sensory evaluation without prolonging much running time. © 2021, Editorial Office of Computer Engineering. All rights reserved.","","Dense Convolutional Networks(DenseNet); Generative Adversarial Networks(GAN); Super-Resolution(SR) reconstruction; Texture loss; Wasserstein Generative Adversarial Nets-Gradient Penalty(WGAN-GP)","Article","Final","","Scopus","2-s2.0-85121859292"
"Wang R.; Zhang D.; Li Q.; Zhou X.-Y.; Lo B.","Wang, Ruoxi (57220950431); Zhang, Dandan (57207943701); Li, Qingbiao (57219255846); Zhou, Xiao-Yun (57226235646); Lo, Benny (57532626500)","57220950431; 57207943701; 57219255846; 57226235646; 57532626500","Real-time Surgical Environment Enhancement for Robot-Assisted Minimally Invasive Surgery Based on Super-Resolution","2021","Proceedings - IEEE International Conference on Robotics and Automation","2021-May","","","3434","3440","6","10.1109/ICRA48506.2021.9561393","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125444013&doi=10.1109%2fICRA48506.2021.9561393&partnerID=40&md5=c64774d7014e011c1cd89c24d3aa58ef","In Robot-Assisted Minimally Invasive Surgery (RAMIS), a camera assistant is normally required to control the position and the zooming ratio of the laparoscope, following the surgeon's instructions. However, moving the laparoscope frequently may lead to unstable and suboptimal views, while the adjustment of zooming ratio may interrupt the workflow of the surgical operation. To this end, we propose a multi-scale Generative Adversarial Network (GAN)-based video super-resolution method to construct a framework for automatic zooming ratio adjustment. It can provide automatic real-time zooming for high-quality visualization of the Region of Interest (ROI) during the surgical operation. In the pipeline of the framework, the Kernel Correlation Filter (KCF) tracker is used for tracking the tips of the surgical tools, while the Semi-Global Block Matching (SGBM)-based depth estimation and Recurrent Neural Network (RNN)-based context-awareness are employed to determine the upscaling ratio for zooming. The framework is validated with the JIGSAW dataset and Hamlyn Centre Laparoscopic/Endoscopic Video Datasets, with results demonstrating its practicability. © 2021 IEEE","Generative adversarial networks; Image segmentation; Laparoscopy; Optical resolving power; Robotic surgery; Surgical equipment; Transplantation (surgical); Minimally-invasive surgery; Multi-scales; Network-based; Real- time; Superresolution; Superresolution methods; Surgical environment; Surgical operation; Video super-resolution; Work-flows; Recurrent neural networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125444013"
"","","","1st CAAI International Conference on Artificial Intelligence, CICAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13069 LNAI","","","","","1226","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122596270&partnerID=40&md5=ab79fbba11953344bcc6a78171203e19","The proceedings contain 101 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Diagnosis of Childhood Autism Using Multi-modal Functional Connectivity via Dynamic Hypergraph Learning; CARNet: Automatic Cerebral Aneurysm Classification in Time-of-Flight MR Angiography by Leveraging Recurrent Neural Networks; White-Box Attacks on the CNN-Based Myoelectric Control System; MMG-HCI: A Non-contact Non-intrusive Real-Time Intelligent Human-Computer Interaction System; DSGSR: Dynamic Semantic Generation and Similarity Reasoning for Image-Text Matching; phase Partition Based Virtual Metrology for Material Removal Rate Prediction in Chemical Mechanical Planarization Process; SAR Target Recognition Based on Model Transfer and Hinge Loss with Limited Data; Neighborhood Search Acceleration Based on Deep Reinforcement Learning for SSCFLP; GBCI: Adaptive Frequency Band Learning for Gender Recognition in Brain-Computer Interfaces; DiffGNN: Capturing Different Behaviors in Multiplex Heterogeneous Networks for Recommendation; hybrid Domain Convolutional Neural Network for Memory Efficient Training; brightening the Low-Light Images via a Dual Guided Network; learning Multi-scale Underexposure Image Correction; optimizing Loss Function for Uni-modal and Multi-modal Medical Registration; registration of 3D Point Clouds Based on Voxelization Simplify and Accelerated Iterative Closest Point Algorithm; few-shot Weighted Style Matching for Glaucoma Detection; Lightweight Convolutional SNN for Address Event Representation Signal Recognition; in-the-Wild Facial Highlight Removal via Generative Adversarial Networks; A Cross-Layer Fusion Multi-target Detection and Recognition Method Based on Improved FPN Model in Complex Traffic Environment; various Plug-and-Play Algorithms with Diverse Total Variation Methods for Video Snapshot Compressive Imaging; graph-Based Exercise- and Knowledge-Aware Learning Network for Student Performance Prediction; EEG Signals Classification in Time-Frequency Images by Fusing Rotation-Invariant Local Binary Pattern and Gray Level Co-occurrence Matrix Features; reduced-reference Perceptual Discrepancy Learning for Image Restoration Quality Assessment; EFENet: Reference-Based Video Super-Resolution with Enhanced Flow Estimation.","","","Conference review","Final","","Scopus","2-s2.0-85122596270"
"Zhang Y.; Tsang I.W.; Li J.; Liu P.; Lu X.; Yu X.","Zhang, Yang (57201701049); Tsang, Ivor W. (7004570429); Li, Jun (57191109857); Liu, Ping (57190793397); Lu, Xiaobo (14627586600); Yu, Xin (57191429254)","57201701049; 7004570429; 57191109857; 57190793397; 14627586600; 57191429254","Face Hallucination with Finishing Touches","2021","IEEE Transactions on Image Processing","30","","9318504","1728","1743","15","10.1109/TIP.2020.3046918","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099554810&doi=10.1109%2fTIP.2020.3046918&partnerID=40&md5=765fbdde0884f86a4acbdd12b72d2077","Obtaining a high-quality frontal face image from a low-resolution (LR) non-frontal face image is primarily important for many facial analysis applications. However, mainstreams either focus on super-resolving near-frontal LR faces or frontalizing non-frontal high-resolution (HR) faces. It is desirable to perform both tasks seamlessly for daily-life unconstrained face images. In this paper, we present a novel Vivid Face Hallucination Generative Adversarial Network (VividGAN) for simultaneously super-resolving and frontalizing tiny non-frontal face images. VividGAN consists of coarse-level and fine-level Face Hallucination Networks (FHnet) and two discriminators, i.e., Coarse-D and Fine-D. The coarse-level FHnet generates a frontal coarse HR face and then the fine-level FHnet makes use of the facial component appearance prior, i.e., fine-grained facial components, to attain a frontal HR face image with authentic details. In the fine-level FHnet, we also design a facial component-aware module that adopts the facial geometry guidance as clues to accurately align and merge the frontal coarse HR face and prior information. Meanwhile, two-level discriminators are designed to capture both the global outline of a face image as well as detailed facial characteristics. The Coarse-D enforces the coarsely hallucinated faces to be upright and complete while the Fine-D focuses on the fine hallucinated ones for sharper details. Extensive experiments demonstrate that our VividGAN achieves photo-realistic frontal HR faces, reaching superior performance in downstream tasks, i.e., face recognition and expression classification, compared with other state-of-the-art methods.  © 1992-2012 IEEE.","Automated Facial Recognition; Face; Female; Humans; Image Processing, Computer-Assisted; Machine Learning; Male; Image processing; Mathematical models; Adversarial networks; Face hallucination; Facial analysis; Facial components; Facial geometry; Photo-realistic; Prior information; State-of-the-art methods; anatomy and histology; diagnostic imaging; face; female; human; image processing; machine learning; male; procedures; Face recognition","face frontalization; Face hallucination; generative adversarial network; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099554810"
"Li H.; Zheng Q.; Yan W.; Tao R.; Qi X.; Wen Z.","Li, Hongan (55673317300); Zheng, Qiaoxue (57217311439); Yan, Wenjing (57219986275); Tao, Ruolin (57226796018); Qi, Xin (57219853462); Wen, Zheng (56438635500)","55673317300; 57217311439; 57219986275; 57226796018; 57219853462; 56438635500","Image super-resolution reconstruction for secure data transmission in Internet of Things environment","2021","Mathematical Biosciences and Engineering","18","5","","6652","6671","19","10.3934/mbe.2021330","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112713026&doi=10.3934%2fmbe.2021330&partnerID=40&md5=6254490b000fa0bfe77237eafb7475a7","The image super-resolution reconstruction method can improve the image quality in the Internet of Things (IoT). It improves the data transmission efficiency, and is of great significance to data transmission encryption. Aiming at the problem of low image quality in image super-resolution using neural networks, a self-attention-based image reconstruction method is proposed for secure data transmission in IoT environment. The network model is improved, and the residual network structure and sub-pixel convolution are used to extract the feature of the image. The self-attention module is used extract detailed information in the image. Using generative confrontation method and image feature perception method to improve the image reconstruction effect. The experimental results on the public data set show that the improved network model improves the quality of the reconstructed image and can effectively restore the details of the image. © 2021 the Author(s), licensee AIMS Press.","Image Processing, Computer-Assisted; Internet of Things; Neural Networks, Computer; Cryptography; Data transfer; Image coding; Image enhancement; Image quality; Internet of things; Optical resolving power; Data transmission efficiency; Image reconstruction methods; Image super resolutions; Image super-resolution reconstruction; Internet of thing (IOT); Network modeling; Network structures; Reconstructed image; image processing; Image reconstruction","Data encryption; Generative adversarial networks; Image super-resolution; Internet of Things; Self-attention","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112713026"
"Ong D.S.; Chan C.S.; Ng K.W.; Fan L.; Yang Q.","Ong, Ding Sheng (57222191063); Chan, Chee Seng (57194450557); Ng, Kam Woh (57218717926); Fan, Lixin (57226253075); Yang, Qiang (57195665626)","57222191063; 57194450557; 57218717926; 57226253075; 57195665626","Protecting Intellectual Property of Generative Adversarial Networks from Ambiguity Attacks","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","3629","3638","9","10.1109/CVPR46437.2021.00363","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111484905&doi=10.1109%2fCVPR46437.2021.00363&partnerID=40&md5=17a9f4feac4714f8982e2ea323947d71","Ever since Machine Learning as a Service emerges as a viable business that utilizes deep learning models to generate lucrative revenue, Intellectual Property Right (IPR) has become a major concern because these deep learning models can easily be replicated, shared, and re-distributed by any unauthorized third parties. To the best of our knowledge, one of the prominent deep learning models - Generative Adversarial Networks (GANs) which has been widely used to create photorealistic image are totally unprotected despite the existence of pioneering IPR protection methodology for Convolutional Neural Networks (CNNs). This paper therefore presents a complete protection framework in both black-box and white-box settings to enforce IPR protection on GANs. Empirically, we show that the proposed method does not compromise the original GANs performance (i.e. image generation, image super-resolution, style transfer), and at the same time, it is able to withstand both removal and ambiguity attacks against embedded watermarks. Codes are available at https://github.com/dingsheng-ong/ipr-gan. © 2021 IEEE","Computer vision; Convolutional neural networks; Deep learning; Intellectual property; Black boxes; Convolutional neural network; Intellectual property rights; Intellectual property rights protections; Learning models; Machine-learning; Photorealistic images; Protection methodology; Third parties; White box; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85111484905"
"","","","Proceedings of ASME 2021 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, IDETC-CIE 2021","2021","Proceedings of the ASME Design Engineering Technical Conference","3B-2021","","","","","4704","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119955422&partnerID=40&md5=3f3c5c8c90f3fb4a146fb6b2cc9e30e5","The proceedings contain 545 papers. The topics discussed include: a mixed sideslip yaw rate stability controller for over-actuated vehicles; a single-card GPU implementation of peridynamics; using physics-informed generative adversarial networks to perform super-resolution for multiphase fluid simulations; multiscale topology optimization with gaussian process regression models; the inspiration design toolkit: a human-centered design tool for a system engineering course; printability and fidelity of protein-enriched 3d printed foods: a case study using cricket and pea protein powder; validating perceived sustainable design features using a novel collage approach; digital twin technology for modelling, simulation and control of a mechatronic system; design of a new piezoelectrically actuated compliant microgripper with high area usage efficiency; cost of controls for multi-rotor drones; linear stability analysis of a waveboard multibody model with a minimal set of equations; frequency tunable phononic crystal flat lens for subwavelength imaging; and detection of electrolytes based on solid-state ion-selective electrode.","","","Conference review","Final","","Scopus","2-s2.0-85119955422"
"Do H.; Helbert D.; Bourdon P.; Naudin M.; Guillevin C.; Guillevin R.","Do, Huy (57413750800); Helbert, David (12761288800); Bourdon, Pascal (7005784278); Naudin, Mathieu (57204172140); Guillevin, Carole (56344201200); Guillevin, Remy (57225705908)","57413750800; 12761288800; 7005784278; 57204172140; 56344201200; 57225705908","MRI super-resolution using 3D cycle-consistent generative adversarial network","2021","International Conference on Advances in Biomedical Engineering, ICABME","2021-October","","","85","88","3","10.1109/ICABME53305.2021.9604810","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122883005&doi=10.1109%2fICABME53305.2021.9604810&partnerID=40&md5=730ae64886f63c4e1234b239c671e3a6","High-resolution magnetic resonance imaging (MRI) provides detailed anatomical information critical for clinical application diagnosis. However, current MRIs are acquired at clinical resolutions due to the limit of physical, technological, and economic considerations. On the other hand, existing approaches require paired MRI images as training data, which are difficult to obtain on existing datasets when the alignment between high and low-resolution images has to be implemented manually.Within the scope of project, we aim to provide an end-to-end system to solve the super-resolution method on 3D MRI. Our proposed method derives from recent neural network developments and does not require paired data for efficient training. By integrating different models with separated functions, our 3D super-resolution CycleGAN (SRCycleGAN) achieved compelling results on MRI volumes. The output is close with ground-truth, showing a low distortion on different scaling factors. Besides, we also compare our method against different GAN-based methods in this field to highlight the performance.  © 2021 IEEE.","Computer vision; Deep learning; Diagnosis; Generative adversarial networks; Medical imaging; Optical resolving power; 'current; Anatomical information; Clinical application; Deep learning; Economic considerations; Generative model; High resolution; Medical image analysis; Neural-networks; Superresolution; Magnetic resonance imaging","Deep learning; Generative model; Medical image analysis; MRI; Neural network; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122883005"
"Prajapati K.; Chudasama V.; Patel H.; Upla K.; Ramachandra R.; Raja K.; Busch C.","Prajapati, Kalpesh (57217177596); Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Upla, Kishor (53985429600); Ramachandra, Raghavendra (57190835798); Raja, Kiran (57188866050); Busch, Christoph (7101767185)","57217177596; 57202047982; 57209145428; 53985429600; 57190835798; 57188866050; 7101767185","Unsupervised Single Image Super-Resolution Using Cycle Generative Adversarial Network","2021","Communications in Computer and Information Science","1382","","","359","370","11","10.1007/978-3-030-71711-7_30","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103512897&doi=10.1007%2f978-3-030-71711-7_30&partnerID=40&md5=9dacbdce8c56b101d762fff6cb50b73a","The current state-of-the-art deep learning based Single Image Super-Resolution (SISR) techniques employ supervised learning in the training process. In this learning, the Low-Resolution (LR) images are prepared by applying known degradation such as bicubic downsampling to the High-Resolution (HR) images. Unfortunately, bicubic down-sampling eliminates the natural image characteristics such as sensor noise, degradation due to built-in hardware, etc., and generates smooth images and hence generated images are different from the real-world data. When deep learning model is trained using such artificially generated LR-HR pairs, they often are prone to generate better SR results for real-world images. To circumvent this problem, we propose an SR framework that can train in an unsupervised manner using Generative Adversarial Networks (GANs). It contains mainly couple of networks called SR network and degradation network which work on an unpaired data of LR-HR images. The SR network learns to eliminate noise present in the LR image and super-resolve it. While, degradation network performs inverse of SR network (i.e. down-sampling and adding degradation from real-world images). We demonstrate the effectiveness of the proposed method by conducting extensive experiments on NTIRE-2020 Real-world SR challenge dataset where it demonstrates the superior performance over state-of-the-art methods in terms of both quantitative and qualitative assessments. © 2021, Springer Nature Switzerland AG.","Inverse problems; Optical resolving power; Signal sampling; Adversarial networks; Built-in-hardware; High resolution image; Low resolution images; Quantitative and qualitative assessments; State of the art; State-of-the-art methods; Training process; Deep learning","Convolutional Neural Network; Deep learning; Generative Adversarial Network; Image restoration; No-reference quality assessment; Single Image Super-Resolution; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85103512897"
"Zhao M.; Naderian A.; Sanei S.","Zhao, Min (57305343200); Naderian, Amirkhashayar (57450912600); Sanei, Saeid (56212338600)","57305343200; 57450912600; 56212338600","Generative Adversarial Networks for Medical Image Super-resolution","2021","2021 9th E-Health and Bioengineering Conference, EHB 2021","","","","","","","10.1109/EHB52898.2021.9657651","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124503973&doi=10.1109%2fEHB52898.2021.9657651&partnerID=40&md5=05ed6fb1de7c1208482c7458beeb082c","Super-resolution (SR) techniques are very useful in enhancing low resolution images. This becomes even more effective when the clinicians and radiologists need to detect tiny bone fractures in some low-resolution medical images such as X-Rays. In this short paper, the application of new deep learning single-image SR techniques to medical bone X-Ray images have been investigated. The quality of the results, when applied to plain hand X-Rays, are assessed based on peak signal-to-noise ratio and mean opinion score (MOS) and the superiority of generative adversarial networks (GANs), particularly in terms of MOS, has been verified for such applications.  © 2021 IEEE.","Deep learning; Image enhancement; Medical imaging; Molybdenum; Optical resolving power; Signal to noise ratio; Bone fracture; Image super resolutions; Low resolution images; Lower resolution; Mean opinion scores; Resolution techniques; Single images; Superresolution; X-ray image; X-ray super-resolution; Generative adversarial networks","GAN; generative adversarial networks; MOS; X-Ray super-resolution","Conference paper","Final","","Scopus","2-s2.0-85124503973"
"Tang C.; Zhang W.; Wang L.; Cai A.; Liang N.; Li L.; Yan B.","Tang, Chao (57211755310); Zhang, Wenkun (56073931100); Wang, Linyuan (36667643700); Cai, Ailong (55750377700); Liang, Ningning (57203978839); Li, Lei (56108914100); Yan, Bin (35188444100)","57211755310; 56073931100; 36667643700; 55750377700; 57203978839; 56108914100; 35188444100","Generative adversarial network-based sinogram super-resolution for computed tomography imaging","2020","Physics in Medicine and Biology","65","23","235006","","","","10.1088/1361-6560/abc12f","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098542075&doi=10.1088%2f1361-6560%2fabc12f&partnerID=40&md5=76ab4bd6e85781c3c93dcf694e221221","Compared with the conventional 1 1 acquisition mode of projection in computed tomography (CT) image reconstruction, the 2 2 acquisition mode improves the collection efficiency of the projection and reduces the x-ray exposure time. However, the collected projection based on the 2 2 acquisition mode has low resolution (LR) and the reconstructed image quality is poor, thus limiting the use of this mode in CT imaging systems. In this study, a novel sinogram-super-resolution (SR) generative adversarial network model is proposed to obtain high-resolution (HR) sinograms from LR sinograms, thereby improving the reconstruction image quality under the 2 2 acquisition mode. The proposed generator is based on the residual network for LR sinogram feature extraction and SR sinogram generation. A relativistic discriminator is designed to render the network capable of obtaining more realistic SR sinograms. Moreover, we combine the cycle consistency loss, sinogram domain loss, and reconstruction image domain loss in the total loss function to supervise SR sinogram generation. Then, a trained model can be obtained by inputting the paired LR/HR sinograms into the network. Finally, the classic filtered-back-projection reconstruction algorithm is used for CT image reconstruction based on the generated SR sinogram. The qualitative and quantitative results of evaluations on digital and real data illustrate that the proposed model not only obtains clean SR sinograms from noisy LR sinograms but also outperforms its counterparts.  © 2020 Institute of Physics and Engineering in Medicine.","Image enhancement; Image quality; Image reconstruction; Image resolution; Optical resolving power; Adversarial networks; Collection efficiency; Ct image reconstruction; Filtered back projection reconstruction; Quantitative result; Reconstructed image; Reconstruction image; Reconstruction image quality; Computerized tomography","CT image reconstruction; generative adversarial network; projection domain; super resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098542075"
"Liu H.; Liu J.; Hou S.; Tao T.; Han J.","Liu, Heng (57022065500); Liu, Jianyong (57218596201); Hou, Shudong (36508758900); Tao, Tao (55420313200); Han, Jungong (14522692900)","57022065500; 57218596201; 36508758900; 55420313200; 14522692900","Perception consistency ultrasound image super-resolution via self-supervised CycleGAN","2021","Neural Computing and Applications","","","","","","","10.1007/s00521-020-05687-9","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100089587&doi=10.1007%2fs00521-020-05687-9&partnerID=40&md5=435bceb6046002b6a285e689a44682aa","Due to the limitations of sensors, the transmission medium, and the intrinsic properties of ultrasound, the quality of ultrasound imaging is always not ideal, especially its low spatial resolution. To remedy this situation, deep learning networks have been recently developed for ultrasound image super-resolution (SR) because of the powerful approximation capability. However, most current supervised SR methods are not suitable for ultrasound medical images because the medical image samples are always rare, and usually, there are no low-resolution (LR) and high-resolution (HR) training pairs in reality. In this work, based on self-supervision and cycle generative adversarial network, we propose a new perception consistency ultrasound image SR method, which only requires the LR ultrasound data and can ensure the re-degenerated image of the generated SR one to be consistent with the original LR image, and vice versa. We first generate the HR fathers and the LR sons of the test ultrasound LR image through image enhancement, and then make full use of the cycle loss of LR–SR–LR and HR–LR–SR and the adversarial characteristics of the discriminator to promote the generator to produce better perceptually consistent SR results. The evaluation of PSNR/IFC/SSIM, inference efficiency and visual effects under the benchmark CCA-US and CCA-US datasets illustrate our proposed approach is effective and superior to other state-of-the-art methods. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.","","CycleGAN; Self-supervision; Ultrasound image super-resolution","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85100089587"
"Bode M.; Gauding M.; Lian Z.; Denker D.; Davidovic M.; Kleinheinz K.; Jitsev J.; Pitsch H.","Bode, Mathis (56135121000); Gauding, Michael (26221578700); Lian, Zeyu (57219522345); Denker, Dominik (57194765373); Davidovic, Marco (57193561056); Kleinheinz, Konstantin (55770386500); Jitsev, Jenia (26023272900); Pitsch, Heinz (7003265812)","56135121000; 26221578700; 57219522345; 57194765373; 57193561056; 55770386500; 26023272900; 7003265812","Using physics-informed enhanced super-resolution generative adversarial networks for subfilter modeling in turbulent reactive flows","2021","Proceedings of the Combustion Institute","38","2","","2617","2625","8","10.1016/j.proci.2020.06.022","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100528952&doi=10.1016%2fj.proci.2020.06.022&partnerID=40&md5=4fd85d0c1c561515f63df53689371eda","Turbulence is still one of the main challenges in accurate prediction of reactive flows. Therefore, the development of new turbulence closures that can be applied to combustion problems is essential. Over the last few years, data-driven modeling has become popular in many fields as large, often extensively labeled datasets are now available and training of large neural networks has become possible on graphics processing units (GPUs) that speed up the learning process tremendously. However, the successful application of deep neural networks in fluid dynamics, such as in subfilter modeling in the context of large-eddy simulations (LESs), is still challenging. Reasons for this are the large number of degrees of freedom in natural flows, high requirements of accuracy and error robustness, and open questions, for example, regarding the generalization capability of trained neural networks in such high-dimensional, physics-constrained scenarios. This work presents a novel subfilter modeling approach based on a generative adversarial network (GAN), which is trained with unsupervised deep learning (DL) using adversarial and physics-informed losses. A two-step training method is employed to improve the generalization capability, especially extrapolation, of the network. The novel approach gives good results in a priori and a posteriori tests with decaying turbulence including turbulent mixing, and the importance of the physics-informed continuity loss term is demonstrated. The applicability of the network in complex combustion scenarios is furthermore discussed by employing it in reactive and inert LESs of the Spray A case defined by the Engine Combustion Network (ECN). © 2020 The Authors","Combustion; Computer graphics; Deep learning; Deep neural networks; Degrees of freedom (mechanics); Graphics processing unit; Large dataset; Large eddy simulation; Learning systems; Program processors; Turbulence; Accurate prediction; Adversarial networks; Combustion problems; Decaying turbulence; Generalization capability; Number of degrees of freedom; Trained neural networks; Turbulence closures; Neural networks","ECN spray A; Generative adversarial networks; Large-eddy simulation; Physics-informed neural networks; Turbulence","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85100528952"
"Wadhawan R.; Drall T.; Singh S.; Chakraverty S.","Wadhawan, Rohan (57223773738); Drall, Tanuj (57243845800); Singh, Shubham (57244661300); Chakraverty, Shampa (7005011462)","57223773738; 57243845800; 57244661300; 7005011462","Multi-attributed and structured text-to-face synthesis","2020","Proceedings of 2020 IEEE International Conference on Technology, Engineering, Management for Societal Impact Using Marketing, Entrepreneurship and Talent, TEMSMET 2020","","","","","","","10.1109/TEMSMET51618.2020.9557583","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117482304&doi=10.1109%2fTEMSMET51618.2020.9557583&partnerID=40&md5=00e6ede9d5b25f509c5c3a22856bc473","Generative Adversarial Networks (GANs) have revolutionized image synthesis through many applications like face generation, photograph editing, and image super-resolution. Image synthesis using GANs has predominantly been uni-modal, with few approaches that can synthesize images from text or other data modes. Text-to-image synthesis, especially text-to-face synthesis, has promising use cases of robust face-generation from eye witness accounts and augmentation of the reading experience with visual cues. However, only a couple of datasets provide consolidated face data and textual descriptions for text-to-face synthesis. Moreover, these textual annotations are less extensive and descriptive, which reduces the diversity of faces generated from it. This paper empirically proves that increasing the number of facial attributes in each textual description helps GANs generate more diverse and real-looking faces. To prove this, we propose a new methodology that focuses on using structured textual descriptions. We also consolidate a Multi-Attributed and Structured Text-to-face (MAST) dataset consisting of high-quality images with structured textual annotations and make it available to researchers to experiment and build upon. Lastly, we report benchmark Fréchet's Inception Distance (FID), Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for the MAST dataset.  © 2020 IEEE.","Computer vision; Crowdsourcing; Machine learning; Semantics; Face generation; Face synthesis; Frechet; Frechet inception distance; Images synthesis; Machine-learning; Multi-attributed and structured text-to-face dataset; Structured text; Text-to-face synthesis; Textual description; Generative adversarial networks","Crowdsourcing; Face Generation; Frechet's Inception Distance; Generative Adversarial Network; Machine Learning; MAST dataset; Text-to-face synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117482304"
"Balaji Prabhu B.V.; Salian N.P.; Nikhil B.M.; Narasipura O.S.J.","Balaji Prabhu, B.V. (57202716108); Salian, Nikith P. (57223916625); Nikhil, B.M. (57209534063); Narasipura, Omkar Subbaram Jois (55626396900)","57202716108; 57223916625; 57209534063; 55626396900","Super-Resolution of Level-17 Images Using Generative Adversarial Networks","2021","Lecture Notes on Data Engineering and Communications Technologies","61","","","379","392","13","10.1007/978-981-33-4582-9_29","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106443712&doi=10.1007%2f978-981-33-4582-9_29&partnerID=40&md5=75752fd3e89aa1dc4645635fe587090f","The generative adversarial networks (GAN) deep learning models are being widely used in the field of image processing and its applications such as image generation, feature extraction, image recovery and image super-resolution to name a few. Image super-resolution has a board range of applications like satellite and aerial image analysis, medical image processing, compressed image/video enhancement, etc. This work implements an image super-resolution using generative adversarial network for super-resolution of level-17 low-resolution geospatial images obtained from Indian Remote Sensing (IRS) imagery. The results show that the generated super-resolution image can recuperate photo-realistic textures from low-resolution input pictures. The performance of the model is evaluated with qualitative measure indices such as structural similarity (SSIM) and peak signal-to-noise ratio (PSNR). The performance metric demonstrates that the model can generate images as close to that of the high-resolution image and it also has finer details. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Antennas; Deep learning; Image compression; Medical imaging; Optical resolving power; Remote sensing; Satellite imagery; Signal to noise ratio; Textures; Adversarial networks; Compressed images; High resolution image; Image super resolutions; Peak signal to noise ratio; Performance metrices; Satellite and aerial images; Structural similarity; Image enhancement","Deep learning; GAN; Indian remote sensing; Satellite image; Super-resolution","Book chapter","Final","","Scopus","2-s2.0-85106443712"
"Li B.; Keikhosravi A.; Loeffler A.G.; Eliceiri K.W.","Li, Bin (57210320152); Keikhosravi, Adib (57194721690); Loeffler, Agnes G. (24466957400); Eliceiri, Kevin W. (6602334481)","57210320152; 57194721690; 24466957400; 6602334481","Single image super-resolution for whole slide image using convolutional neural networks and self-supervised color normalization","2021","Medical Image Analysis","68","","101938","","","","10.1016/j.media.2020.101938","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098461150&doi=10.1016%2fj.media.2020.101938&partnerID=40&md5=bac0e2bdc2a8a1288d3c2db6213a5a5e","High-quality whole slide scanners used for animal and human pathology scanning are expensive and can produce massive datasets, which limits the access to and adoption of this technique. As a potential solution to these challenges, we present a deep learning-based approach making use of single image super-resolution (SISR) to reconstruct high-resolution histology images from low-resolution inputs. Such low-resolution images can easily be shared, require less storage, and can be acquired quickly using widely available low-cost slide scanners. The network consists of multi-scale fully convolutional networks capable of capturing hierarchical features. Conditional generative adversarial loss is incorporated to penalize blurriness in the output images. The network is trained using a progressive strategy where the scaling factor is sampled from a normal distribution with an increasing mean. The results are evaluated with quantitative metrics and are used in a clinical histopathology diagnosis procedure which shows that the SISR framework can be used to reconstruct high-resolution images with clinical level quality. We further propose a self-supervised color normalization method that can remove staining variation artifacts. Quantitative evaluations show that the SISR framework can generalize well on unseen data collected from other patient tissue cohorts by incorporating the color normalization method. © 2020","Histological Techniques; Humans; Magnetic Resonance Imaging; Neural Networks, Computer; Color; Convolution; Deep learning; Digital storage; Image reconstruction; Normal distribution; Optical resolving power; Quality control; Scanning; Convolutional networks; Diagnosis procedure; Hierarchical features; High resolution image; Learning-based approach; Low resolution images; Quantitative evaluation; Quantitative metrics; adult; article; artifact; cohort analysis; convolutional neural network; deep learning; female; histology; histopathology; human; human tissue; male; normal distribution; quantitative analysis; histology; nuclear magnetic resonance imaging; Convolutional neural networks","Convolutional neural network; Digital pathology; Generative adversarial networks; Super-resolution","Article","Final","","Scopus","2-s2.0-85098461150"
"Naik D.A.; Sangeetha V.; Sandhya G.","Naik, Darshana A. (57208920461); Sangeetha, V. (57189683778); Sandhya, G. (57211584484)","57208920461; 57189683778; 57211584484","Generative Adversarial Networks based method for Generating Photo-Realistic Super Resolution Images","2021","2021 IEEE International Conference on Emerging Trends in Industry 4.0, ETI 4.0 2021","","","","","","","10.1109/ETI4.051663.2021.9619393","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123445820&doi=10.1109%2fETI4.051663.2021.9619393&partnerID=40&md5=173205afe3239518a04faebf48c539a8","Since the word picture was coined, resolution has always been a challenge. Many studies have been conducted to generate high-resolution photographs, but none have been able to develop a process that is both time and quality effective. As a result, the super resolution issue is discussed in this paper using single-processing techniques. Deep learning methods are used to solve the same problem. The method suggested here will transform a low-resolution image into a high-resolution image of a pleasant and satisfactory quality. This can be accomplished using GANs (Generative Adversarial Networks) with significant up scaling factors.  © 2021 IEEE.","Deep learning; Generative adversarial networks; Optical resolving power; Adversarial process; Generative model; Generator; High resolution; Network-based; Photo-realistic; Processing technique; Resolution images; Super resolution image; Superresolution; Discriminators","adversarial process; Discriminator; Generative Models; Generator; Super resolution image","Conference paper","Final","","Scopus","2-s2.0-85123445820"
"Ning K.; Zhang Z.; Han K.; Han S.; Zhang X.","Ning, Keqing (36926065800); Zhang, Zhihao (57192639278); Han, Kai (57141420300); Han, Siyu (57226693080); Zhang, Xiqing (55662831100)","36926065800; 57192639278; 57141420300; 57226693080; 55662831100","Multi-Frame Super-Resolution Algorithm Based on a WGAN","2021","IEEE Access","9","","9452141","85839","85851","12","10.1109/ACCESS.2021.3088128","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112369897&doi=10.1109%2fACCESS.2021.3088128&partnerID=40&md5=20ec45b1c01d9c2afdda262c5f9ef38b","Image super-resolution reconstruction has been widely used in remote sensing, medicine and other fields. In recent years, due to the rise of deep learning research and the successful application of convolutional neural networks in the image field, the super-resolution reconstruction technology based on deep learning has also achieved great development. However, there are still some problems that need to be solved. For example, the current mainstream image super-resolution algorithms based on single or multiple frames pursue high performance indicators such as PSNR and SSIM, while the reconstructed image is relatively smooth and lacks many high-frequency details. It is not conducive to application in a real environment. To address such problem, this paper proposes a super-resolution reconstruction model of sequential images based on Generative Adversarial Networks (GAN). The proposed approach combines the registration module to fuse adjacent frames, effectively use the detailed information in multiple consecutive frames, and enhances the spatio-temporality of low-resolution images in sequential images. While the GAN was used to improve the effect of image high-frequency texture detail reconstruction, WGAN was introduced to optimize model training. The reconstruction results not only improved the PSNR and SSIM indexes but also reconstructed more high-frequency detail textures. Finally, in order to further improve the perception effect, an additional registration loss item RLT is introduced in the GAN network perception loss. Through extensive experiments, it shows that the model proposed in this paper effectively obtains the information between the sequence images. When the PSNR and SSIM indicators are improve, it can reconstruct better high-frequency texture details than the current advanced multi-frame algorithms. © 2013 IEEE.","Convolutional neural networks; Deep learning; Image reconstruction; Optical resolving power; Remote sensing; Textures; Adversarial networks; Image super resolutions; Image super-resolution reconstruction; Low resolution images; Performance indicators; Spatio temporalities; Super resolution algorithms; Super resolution reconstruction; Image enhancement","convolutional neural network; sequential images; Super-resolution reconstruction; Wasserstein generative adversarial network (WGAN)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112369897"
"Zhang N.; Wang Y.-C.; Zhang X.; Xu D.-D.","Zhang, Ning (57188816117); Wang, Yong-Cheng (56437944700); Zhang, Xin (57774426900); Xu, Dong-Dong (56299205100)","57188816117; 56437944700; 57774426900; 56299205100","A Review of Single Image Super-resolution Based on Deep Learning; [基于深度学习的单幅图片超分辨率重构研究进展]","2020","Zidonghua Xuebao/Acta Automatica Sinica","46","12","","2479","2499","20","10.16383/j.aas.c190031","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099758255&doi=10.16383%2fj.aas.c190031&partnerID=40&md5=f0d32269aea2e5b3e689fff8be7e2be9","Super-resolution (SR) refers to an estimation of high resolution (HR) image from one or more low resolution (LR) observations of the same scene, usually employing digital image processing and machine learning techniques. This technique can effectively improve image resolution without upgrading hardware devices. In recent years, deep learning has developed rapidly in the image field, and it has brought promising prospects for single-image super-resolution (SISR). This paper summarizes the research status and development tendency of the current SISR methods based on deep learning. First, we introduce a series of networks characteristics for SISR, and analysis of these networks in the structure, input, loss function, scale factors and evaluation criterion are given. Then according to the experimental results, we discuss the existing problems and solutions. Finally, the future development and challenges of the SISR methods based on deep learning are presented. Copyright © 2020 Acta Automatica Sinica. All rights reserved.","Image enhancement; Image resolution; Learning systems; Optical resolving power; Development and challenges; Development tendency; Evaluation criteria; Existing problems; Hardware devices; High resolution image; Machine learning techniques; Super resolution; Deep learning","Convolutional neural network (CNN); Deep learning; Generative adversarial network; Single-image super-resolution (SISR)","Review","Final","","Scopus","2-s2.0-85099758255"
"Wang W.; Hu Y.; Luo Y.; Zhang T.","Wang, Wei (56948645300); Hu, Yihui (57216589586); Luo, Yanhong (57214072303); Zhang, Tong (57037334400)","56948645300; 57216589586; 57214072303; 57037334400","Brief Survey of Single Image Super-Resolution Reconstruction Based on Deep Learning Approaches","2020","Sensing and Imaging","21","1","21","","","","10.1007/s11220-020-00285-4","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083974443&doi=10.1007%2fs11220-020-00285-4&partnerID=40&md5=9b4d9f6644efd5089b44f1dea404f374","With the presentation of super-resolution convolutional neural network, deep learning approach was applied to image super-resolution reconstruction for the first time. By using convolutional neural network, the deep learning approaches can directly learn the mapping relationship between the low-resolution image and high-resolution image, and have achieved better reconstruction effects than the traditional image super-resolution reconstruction methods. Subsequently, a series of improved deep learning approaches have been proposed, and the reconstruction effects have been improved continuously. This paper systematically summa rizes the image super-resolution reconstruction approaches based on deep learning, analyzes the characteristics of different models, and compares the main deep learning models based on the experiments. Furthermore, based on deep learning model, the future research directions of the image super-resolution reconstruction methods based on deep learning models are reasonably predicted. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Convolution; Convolutional neural networks; Image reconstruction; Learning systems; Optical resolving power; Future research directions; High resolution image; Image super-resolution reconstruction; Learning approach; Low resolution images; Mapping relationships; Single-image super-resolution reconstruction; Super resolution; Deep learning","Convolutional neural network (CNN); Dense network; Generative adversarial networks (GANs); Residual learning; Single image super-resolution (SISR) reconstruction","Article","Final","","Scopus","2-s2.0-85083974443"
"Nayak R.; Balabantaray B.K.","Nayak, Rajashree (56045389900); Balabantaray, Bunil Ku. (56178407400)","56045389900; 56178407400","Generative Adversarial Network for Heritage Image Super Resolution","2021","Communications in Computer and Information Science","1377 CCIS","","","161","173","12","10.1007/978-981-16-1092-9_14","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107370749&doi=10.1007%2f978-981-16-1092-9_14&partnerID=40&md5=c5557520740ec575fc595d7f55ebed35","In this paper we have proposed Generative Adversarial Network (GAN) based Super resolution reconstruction (SRR) method for the estimation of high resolution (HR) heritage images. The proposed SRR method via the GAN model (SRRGAN) is modeled as a min-max optimization process by integrating some new loss functions which will enable to maintain spatial and textural homogeneity in the estimated HR images. Initially, we have divided the input test image into Textured and Non-Textured patches based on their inherent characteristic followed by the optimization of the GAN model via loss functions lossTeXP and lossNonTeXP respectively to yield textured and non-textured patches of the HR image. Loss function lossTeXP helps to preserve realistic and diverse textural content of heritage images along multi-scale and multi-orientations. Whereas, lossNonTeXP efficiently preserves the complete high-frequency map of the heritage images. Consequently, the estimated HR images preserve sufficient image details with realistic textural content and reduced artifacts compared with the existing state-of-the-art SRR methods. © 2021, Springer Nature Singapore Pte Ltd.","Optical resolving power; Textures; Adversarial networks; High frequency HF; High resolution; Image super resolutions; Inherent characteristics; Min-max optimization; State of the art; Super resolution reconstruction; Computer vision","GAN; Loss<sub>NonTeXP</sub>; Loss<sub>TeXP</sub>; Min-max optimization; SRR","Conference paper","Final","","Scopus","2-s2.0-85107370749"
"","","","1st CAAI International Conference on Artificial Intelligence, CICAI 2021","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13070 LNAI","","","","","1226","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122582092&partnerID=40&md5=9721a83a263d65a3ae1a42e37275dbc4","The proceedings contain 101 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Diagnosis of Childhood Autism Using Multi-modal Functional Connectivity via Dynamic Hypergraph Learning; CARNet: Automatic Cerebral Aneurysm Classification in Time-of-Flight MR Angiography by Leveraging Recurrent Neural Networks; White-Box Attacks on the CNN-Based Myoelectric Control System; MMG-HCI: A Non-contact Non-intrusive Real-Time Intelligent Human-Computer Interaction System; DSGSR: Dynamic Semantic Generation and Similarity Reasoning for Image-Text Matching; phase Partition Based Virtual Metrology for Material Removal Rate Prediction in Chemical Mechanical Planarization Process; SAR Target Recognition Based on Model Transfer and Hinge Loss with Limited Data; Neighborhood Search Acceleration Based on Deep Reinforcement Learning for SSCFLP; GBCI: Adaptive Frequency Band Learning for Gender Recognition in Brain-Computer Interfaces; DiffGNN: Capturing Different Behaviors in Multiplex Heterogeneous Networks for Recommendation; hybrid Domain Convolutional Neural Network for Memory Efficient Training; brightening the Low-Light Images via a Dual Guided Network; learning Multi-scale Underexposure Image Correction; optimizing Loss Function for Uni-modal and Multi-modal Medical Registration; registration of 3D Point Clouds Based on Voxelization Simplify and Accelerated Iterative Closest Point Algorithm; few-shot Weighted Style Matching for Glaucoma Detection; Lightweight Convolutional SNN for Address Event Representation Signal Recognition; in-the-Wild Facial Highlight Removal via Generative Adversarial Networks; A Cross-Layer Fusion Multi-target Detection and Recognition Method Based on Improved FPN Model in Complex Traffic Environment; various Plug-and-Play Algorithms with Diverse Total Variation Methods for Video Snapshot Compressive Imaging; graph-Based Exercise- and Knowledge-Aware Learning Network for Student Performance Prediction; EEG Signals Classification in Time-Frequency Images by Fusing Rotation-Invariant Local Binary Pattern and Gray Level Co-occurrence Matrix Features; reduced-reference Perceptual Discrepancy Learning for Image Restoration Quality Assessment; EFENet: Reference-Based Video Super-Resolution with Enhanced Flow Estimation.","","","Conference review","Final","","Scopus","2-s2.0-85122582092"
"Liu Y.-W.; Niu H.-J.; Yin H.-X.; Xia J.-J.; Ren P.-L.; Zhang T.-T.; Li J.; Lv H.; Ding H.-Y.; Ren J.-L.; Wang Z.-C.","Liu, Ya-Wen (57202608540); Niu, Hai-Jun (57204770601); Yin, Hong-Xia (56681613600); Xia, Jing-Jing (57406232200); Ren, Peng-Ling (57038023600); Zhang, Ting-Ting (57193739211); Li, Jing (57209631682); Lv, Han (55977922500); Ding, He-Yu (57202101610); Ren, Jian-Liang (57406238200); Wang, Zhen-Chang (12647315500)","57202608540; 57204770601; 56681613600; 57406232200; 57038023600; 57193739211; 57209631682; 55977922500; 57202101610; 57406238200; 12647315500","Feasibility of Brain Imaging Using a Digital Surround Technology Body Coil: A Study Based on SRGAN-VGG Convolutional Neural Networks","2021","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","","3734","3737","3","10.1109/EMBC46164.2021.9630816","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122551131&doi=10.1109%2fEMBC46164.2021.9630816&partnerID=40&md5=fcffbfb7f3e2442cbc4d95ee874cd134","Brain imaging using conventional head coils presents several problems in routine magnetic resonance (MR) examination, such as anxiety and claustrophobic reactions during scanning with a head coil, photon attenuation caused by the MRI head coil in positron emission tomography (PET)/MRI, and coil constraints in intraoperative MRI or MRI-guided radiotherapy. In this paper, we propose a super resolution generative adversarial (SRGAN-VGG) network-based approach to enhance low-quality brain images scanned with body coils. Two types of T1 fluid-attenuated inversion recovery (FLAIR) images scanned with different coils were obtained in this study: joint images of the head-neck coil and digital surround technology body coil (H+B images) and body coil images (B images). The deep learning (DL) model was trained using images acquired from 36 subjects and tested in 4 subjects. Both quantitative and qualitative image quality assessment methods were performed during evaluation. Wilcoxon signed-rank tests were used for statistical analysis. Quantitative image quality assessment showed an improved structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR) in gray matter and cerebrospinal fluid (CSF) tissues for DL images compared with B images (P <.01), while the mean square error (MSE) was significantly decreased (P <.05). The analysis also showed that the natural image quality evaluator (NIQE) and blind image quality index (BIQI) were significantly lower for DL images than for B images (P <.0001). Qualitative scoring results indicated that DL images showed an improved SNR, image contrast and sharpness (P<.0001). The outcomes of this study preliminarily indicate that body coils can be used in brain imaging, making it possible to expand the application of MR-based brain imaging. © 2021 IEEE.","Brain; Brain mapping; Cerebrospinal fluid; Convolutional neural networks; Deep learning; Generative adversarial networks; Image analysis; Image enhancement; Image quality; Image segmentation; Magnetic resonance; Mean square error; Positron emission tomography; Quality control; Signal to noise ratio; Brain images; Brain imaging; Convolutional neural network; Image quality assessment; Intra-operative; Low qualities; Network-based approach; Photon attenuation; Quantitative images; Superresolution; Magnetic resonance imaging","","Conference paper","Final","","Scopus","2-s2.0-85122551131"
"Chudasama V.; Upla K.","Chudasama, Vishal (57202047982); Upla, Kishor (53985429600)","57202047982; 53985429600","RSRGAN: computationally efficient real-world single image super-resolution using generative adversarial network","2021","Machine Vision and Applications","32","1","3","","","","10.1007/s00138-020-01135-9","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092273445&doi=10.1007%2fs00138-020-01135-9&partnerID=40&md5=07aecc45227ff0796b6a1441e40dc436","Recently, convolutional neural network has been employed to obtain better performance in single image super-resolution task. Most of these models are trained and evaluated on synthetic datasets in which low-resolution images are synthesized with known bicubic degradation and hence they perform poorly on real-world images. However, by stacking more convolution layers, the super-resolution (SR) performance can be improved. But, such idea increases the number of training parameters and it offers a heavy computational burden on resources which makes them unsuitable for real-world applications. To solve this problem, we propose a computationally efficient real-world image SR network referred as RSRN. The RSRN model is optimized using pixel-wise L1 loss function which produces overly-smooth blurry images. Hence, to recover the perceptual quality of SR image, a real-world image SR using generative adversarial network called RSRGAN is proposed. Generative adversarial network has an ability to generate perceptual plausible solutions. Several experiments have been conducted to validate the effectiveness of the proposed RSRGAN model, and it shows that the proposed RSRGAN generates SR samples with more high-frequency details and better perception quality than that of recently proposed SRGAN and SRFeat IF models, while it sets comparable performance with the ESRGAN model with significant less number of training parameters. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Computational efficiency; Convolution; Image resolution; Optical resolving power; Adversarial networks; Computational burden; Computationally efficient; High frequency HF; Low resolution images; Perceptual quality; Synthetic datasets; Training parameters; Convolutional neural networks","Generative adversarial network; Learned perceptual image patch similarity; Perceptual index; Real-world image super-resolution","Article","Final","","Scopus","2-s2.0-85092273445"
"Huang Y.; Jiang Z.; Lan R.; Zhang S.; Pi K.","Huang, Yongsong (57214838902); Jiang, Zetao (24512367900); Lan, Rushi (35146229200); Zhang, Shaoqin (39262873600); Pi, Kui (57223430284)","57214838902; 24512367900; 35146229200; 39262873600; 57223430284","Infrared Image Super-Resolution via Transfer Learning and PSRGAN","2021","IEEE Signal Processing Letters","28","","9424970","982","986","4","10.1109/LSP.2021.3077801","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105884039&doi=10.1109%2fLSP.2021.3077801&partnerID=40&md5=cb4106635fa1e6367b6aed26f422596a","Recent advances in single image super-resolution (SISR) demonstrate the power of deep learning for achieving better performance. Because it is costly to recollect the training data and retrain the model for infrared (IR) image super-resolution, the availability of only a few samples for restoring IR images presents an important challenge in the field of SISR. To solve this problem, we first propose the progressive super-resolution generative adversarial network (PSRGAN) that includes the main path and branch path. The depthwise residual block (DWRB) is used to represent the features of the IR image in the main path. Then, the novel shallow lightweight distillation residual block (SLDRB) is used to extract the features of the readily available visible image in the other path. Furthermore, inspired by transfer learning, we propose the multistage transfer learning strategy for bridging the gap between different high-dimensional feature spaces that can improve the PSGAN performance. Finally, quantitative and qualitative evaluations of two public datasets show that PSRGAN can achieve better results compared to the SR methods.  © 1994-2012 IEEE.","Deep learning; Distillation; Infrared imaging; Optical resolving power; Adversarial networks; High-dimensional feature space; Image super resolutions; Qualitative evaluations; Single images; Super resolution; Training data; Visible image; Transfer learning","generative dversarial networks; image processing; infrared image; knowledge distillation; Super-resolution; transfer learning","Article","Final","","Scopus","2-s2.0-85105884039"
"Jeon W.-S.; Rhee S.-Y.","Jeon, Wang-Su (57195959701); Rhee, Sang-Yong (55415481500)","57195959701; 55415481500","A Restoration Method of Single Image Super Resolution Using Improved Residual Learning with Squeeze and Excitation Blocks","2021","International Journal of Fuzzy Logic and Intelligent Systems","21","3","","222","232","10","10.5391/IJFIS.2021.21.3.222","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117941050&doi=10.5391%2fIJFIS.2021.21.3.222&partnerID=40&md5=e03bcff5eb1070ce9ef8ada3ba941a03","Techniques for single-image super-resolution have been developed through deep learning. In this paper, we propose a method using advanced residual learning and squeeze and excitation (SE) blocks for such resolution. Improving the residual learning increases the similarity between pixels by adding one skip to the existing residual, and it is possible to improve the performance while slightly increasing the number of calculations by applying the SE block of SENet. The performance evaluation was tested as part of the super-resolution generative adversarial network (SRGAN) and using three proposed modules, and the effect of the residual and the SE blocks on the super-resolution and the change in performance was confirmed. Although the results vary slightly from image, SE and residual blocks have been found to help improve the performance and are best used with blocks. © 2021. The Korean Institute of Intelligent Systems This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (http://creativecommons.org/licenses/by-nc/3.0/) which permits unrestricted noncommercial use, distribution, and reproduction in any medium, provided the original work is properly cited.","","CNN; Residual learning; Squeeze and excitation; SRGAN; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85117941050"
"Qian X.; Ding H.; Li F.; Nie S.; Yuan C.; Feng S.","Qian, Xin (57361147200); Ding, Hao (57226563287); Li, Fajing (57226562929); Nie, Shouping (7102656246); Yuan, Caojin (25635027900); Feng, Shaotong (8643747900)","57361147200; 57226563287; 57226562929; 7102656246; 25635027900; 8643747900","Super-resolution quantitative phase imaging of out-of-focus images based on deep learning","2021","Proceedings of SPIE - The International Society for Optical Engineering","11898","","1189811","","","","10.1117/12.2602460","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120482343&doi=10.1117%2f12.2602460&partnerID=40&md5=60f4addd57ec1454ed9d74b17a270b8d","Quantitative phase information which can reflect the internal structure and refractive index distribution of the object is able to be obtained by diffractive and interferometry techniques. However, the phase resolution achieved by the diffraction method is lower than that of interferometry method; while the setup for interferometry method is more complex. To obtain high-resolution phase images without reference beam path, we propose an end-to-end DL based super resolved quantitative phase imaging method (AF-SRQPI) based on generative adversarial network (GAN) to transform low-resolution amplitude images into super-resolved phase images. Meanwhile, considering the inevitable out-focusing during the long hours of observing, autofocusing function is also included by the network. In the training process, out-of-focus low-resolution amplitude images are used as the inputs and corresponding super-resolved phase images obtained by structured illumination digital holographic microscopy (SI-DHM) are used as the ground truth labels. The well-trained network can reconstruct the high-resolution phase image at high speed (20fps) from a single-shot out-of-focus amplitude image. Comparing with other DL-based reconstruction schemes, the proposed method can perform autofocusing and superresolution phase imaging simultaneously. The simulation results verify that the high-resolution quantitative phase images of different biological samples can be reconstructed by using AF-SRQPI.  © 2021 SPIE.","Bioinformatics; Deep learning; Generative adversarial networks; Image reconstruction; Interferometry; Microscopic examination; Refractive index; Auto-focusing; Deep learning; High-resolution phasis; Image-based; Lower resolution; Out-of focus image; Out-of-focus; Phase image; Quantitative phase imaging; Superresolution; Optical resolving power","Deep learning; Quantitative phase imaging; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85120482343"
"Gupta M.; Kumari M.; Jain R.; Lakshay","Gupta, Meenu (55255409400); Kumari, Meet (57210432969); Jain, Rachna (57207292813); Lakshay (57870121200)","55255409400; 57210432969; 57207292813; 57870121200","Super-resolution-based GAN for image processing: Recent advances and future trends","2021","Generative Adversarial Networks for Image-to-Image Translation","","","","1","15","14","10.1016/B978-0-12-823519-5.00030-0","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126736257&doi=10.1016%2fB978-0-12-823519-5.00030-0&partnerID=40&md5=070ea11d2d67818d75a95033fda06806","Humans can analyze the relationship of data collected from different domains without any supervision, but automatic learning of discovering data is a challenging task. Image processing is one of the examples where conversion from day to night and night to day is difficult. To avoid drawbacks in image processing, Generative adversarial network (GAN) came into existence, which helps to generate networks without losing properties of attributes such as face identity and orientation. Again, super-resolution generative adversarial network (SRGAN) is a formative task that is capable of producing realistic textures in single image super-resolution. Still, hallucinated descriptions are generally attained with no desirable artifacts. Furthermore, to improve visual quality, we significantly studied the prime components of SRGAN-architecture, perceptual loss, adversarial loss, and training algorithm. Moreover, various potential open challenges, along with the future scope, are also discussed. © 2021 Elsevier Inc. All rights reserved.","","Deep learning; EEGAN; GAN; Image processing; Neural network; SRGAN","Book chapter","Final","","Scopus","2-s2.0-85126736257"
"Tang W.; Deng C.; Han Y.; Huang Y.; Zhao B.","Tang, Wei (57219250261); Deng, Chenwei (25958671000); Han, Yuqi (57195983797); Huang, Yun (57218183818); Zhao, Baojun (7403059245)","57219250261; 25958671000; 57195983797; 57218183818; 7403059245","SRARNet: A Unified Framework for Joint Superresolution and Aircraft Recognition","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9254000","327","336","9","10.1109/JSTARS.2020.3037225","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098793454&doi=10.1109%2fJSTARS.2020.3037225&partnerID=40&md5=c78d7f87941ced8270ff13be6a7ce407","Aircraft recognition in high-resolution remote sensing images has rapidly progressed with the advance of convolutional neural networks (CNNs). However, the previous CNN-based methods may not work well for recognizing aircraft in low-resolution remote sensing images because the blurred aircraft in these images offer insufficient details to distinguish them from similar types of targets. An intuitive solution is to introduce superresolution preprocessing. However, conventional superresolution methods mainly focus on reconstructing natural images with detailed texture rather than constructing a high-resolution object with strong discriminative information for the recognition task. To address these problems, we propose a unified framework for joint superresolution and aircraft recognition (Joint-SRARNet) that tries to improve the recognition performance by generating discriminative, high-resolution aircraft from low-resolution remote sensing images. Technically, this network integrates superresolution and recognition tasks into the generative adversarial network (GAN) framework through a joint loss function. The generator is constructed as a joint superresolution and refining subnetwork that can upsample small blurred images into high-resolution ones and restore high-frequency information. In the discriminator, we introduce a new classification loss function that forces the discriminator to distinguish between real and fake images while recognizing the type of aircraft. In addition, the classification loss function is back-propagated to the generator to obtain high-resolution images with discriminative information for easier recognition. Extensive experiments on the challenging multitype aircraft of remote sensing images (MTARSI) dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a small blurred image and significant improvement in the recognition performance. To our knowledge, this is the first work on joint superresolution and aircraft recognition tasks.  © 2008-2012 IEEE.","Aircraft; Classification (of information); Image reconstruction; Optical resolving power; Remote sensing; Textures; Vehicle performance; Adversarial networks; High resolution image; High resolution remote sensing images; High-frequency informations; Remote sensing images; Super resolution; Superresolution methods; Unified framework; aerial photography; aircraft; artificial neural network; back propagation; image analysis; image resolution; remote sensing; Image enhancement","Aircraft recognition; multitask GAN; superresolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85098793454"
"Liu J.; Yun L.; Jin X.; Zhang C.","Liu, Jie (57867635500); Yun, Lijun (12790726500); Jin, Xuesong (57292519300); Zhang, Chunjie (57219989633)","57867635500; 12790726500; 57292519300; 57219989633","Image Recognition Based on Super-Resolution Wasserstein Generative Adversarial Nets with Gradient Penalty","2021","Smart Innovation, Systems and Technologies","234","","","19","28","9","10.1007/978-981-16-3391-1_3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116872115&doi=10.1007%2f978-981-16-3391-1_3&partnerID=40&md5=ec160789eccb7d4ce02baf550ba4a30e","Generative adversarial networks (GAN) are currently a hotly debated research topic in the field of machine vision; however, they possess various shortcomings that cannot be overlooked, such as unstable generated samples, collapsed modes, and slow convergence. The present paper combines the advantages of the super-resolution GAN model (SRGAN) and Wasserstein GAN with gradient penalty (WGAN-GP) and proposes a hybrid model, known as super-resolution Wasserstein generative adversarial network with gradient penalty (SRWGAN-GP). The generator of SRWGAN-GP utilizes the residuals block and Wasserstein distance to increase the resolution of the generated samples and solve the shortcomings of the unstable GAN training. Besides, the discriminator of SRWGAN-GP consists of a local discriminator and a global discriminator to enhance image recognition capacity. Finally, the discriminator is extracted for image recognition. The results of comparative experiments indicate that this method can effectively enhance the accuracy of image recognition. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Discriminators; Image enhancement; Image recognition; Optical resolving power; Gradient penalty; Hybrid model; Machine-vision; Network models; Network training; Research topics; Slow convergences; Superresolution; Wasserstein; Wasserstein distance; Generative adversarial networks","GAN; Gradient penalty; Image recognition; Super-resolution; Wasserstein","Conference paper","Final","","Scopus","2-s2.0-85116872115"
"Zhou S.; Zhang Y.; Cao G.; Wang J.","Zhou, Shunkai (57387230600); Zhang, Yueling (57201132340); Cao, Guitao (35253286200); Wang, Jiangtao (55878720800)","57387230600; 57201132340; 35253286200; 55878720800","Generating Adversarial Examples by Distributed Upsampling","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13108 LNCS","","","177","189","12","10.1007/978-3-030-92185-9_15","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121786896&doi=10.1007%2f978-3-030-92185-9_15&partnerID=40&md5=985d8e67e3b13df09b6977429b1bdd7a","The development of neural networks provides state-of-the-art results for many tasks. However, much research has shown that deep neural networks are vulnerable to adversarial attacks that fool deep models by adding small perturbations into original images. The defense of the neural networks also benefits from a better understanding of the attacks. In consequence, generating adversarial examples with a high attack success rate is worth researching. Inspired by single image super-resolution, this paper treats adversarial attacks as an image generation task and designs a new model based on generative adversarial networks (GANs). The latent feature maps have been divided into low-level and high-level in this research. We exploit low-level features with noises to add perturbations during the upsampling process. To further generate perturbed images, we reconsider and make use of checkerboard artifacts caused by deconvolution. We illustrate the performance of our method using experiments conducted on MNIST and CIFAR-10. The experiment results prove that adversarial examples generated by our method achieve a higher attack success rate and better transferability. © 2021, Springer Nature Switzerland AG.","Generative adversarial networks; Signal sampling; Adversarial example; Deconvolutions; Image generations; Image super resolutions; Neural-networks; Original images; Single images; Small perturbations; State of the art; Upsampling; Deep neural networks","Adversarial example; Deconvolution; Deep neural network; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85121786896"
"Mungarwadi P.; Rane S.; Raut R.; Pattanshetti T.","Mungarwadi, Prathamesh (57218492143); Rane, Shubham (57218491786); Raut, Ritu (57218491487); Pattanshetti, Tanuja (57194703904)","57218492143; 57218491786; 57218491487; 57194703904","Improved image deblurring using gans","2021","Advances in Intelligent Systems and Computing","1167","","","581","588","7","10.1007/978-981-15-5285-4_58","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089316995&doi=10.1007%2f978-981-15-5285-4_58&partnerID=40&md5=3a23efa5dbf3bdf54b7c2437907ec097","This paper proposes a new generative adversarial network that attains statistically significant improvements over some current image restoration techniques and traditional image restoration algorithms. Most of the existing image restoration techniques depend on prior information about the image. More flexibility can be obtained by using data-driven techniques to restore the image. Data-driven techniques like deep learning (DL) algorithms along with generative adversarial networks (GAN) have been showing great potential in the domain of computer vision (CV). However, it still has a scope of improvement in image denoising, deblurring, and super-resolution as the basic formulation of generative adversarial networks (GANs) cannot be directly used to generate realistic high-resolution images and some structures of the estimated images are usually not preserved well. Images from domains such as medicine and crime highly depend on the reality of the image and cannot entertain false positives and false negatives in their analysis. Such domains are still facing challenges to obtain true better quality images with the existing techniques. This paper also intends to solve some of these problems by giving better outputs with the proposed method. The improvised GAN proposed in this paper uses image quality parameter SSIM to improve restoration. © Springer Nature Singapore Pte Ltd 2021.","Big data; Deep learning; Image denoising; Image reconstruction; Restoration; Adversarial networks; Basic formulation; Data driven technique; High resolution image; Image quality parameters; Image restoration algorithms; Image restoration techniques; Prior information; Image enhancement","Convolutional neural network; Deep learning; Generative adversarial networks; Image restoration","Conference paper","Final","","Scopus","2-s2.0-85089316995"
"Deng Y.; Zhou Y.; Lan J.; Huang Y.; Gao Q.; Tong T.","Deng, Yanglin (57221604098); Zhou, Yuanbo (57218714138); Lan, Junlin (57221596476); Huang, Yuxiu (57221612304); Gao, Qinquan (55598017400); Tong, Tong (55625986800)","57221604098; 57218714138; 57221596476; 57221612304; 55598017400; 55625986800","Face Beauty: Improving Quality of Face with Semantic Segmentation Prior and Style Encoder","2020","2020 Cross Strait Radio Science and Wireless Technology Conference, CSRSWTC 2020 - Proceedings","","","9372584","","","","10.1109/CSRSWTC50769.2020.9372584","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103165913&doi=10.1109%2fCSRSWTC50769.2020.9372584&partnerID=40&md5=1680f62eccae6f5946543cae9e1144c9","Despite image super-resolution has great progress in recent years, state-of-the-art face super-resolution still has much potential to promote visual quality. Most of these methods utilize a deep convolutional neural network to explore a mapping between low resolution and high resolution, but it cannot well explore facial structures and local knowledge. In this work, we propose a face hallucination method that explicitly incorporates semantic segmentation prior and style encoder to improve the quality of low resolution face images. To enhance the feature mapping and color mapping of the face, we focus on transferring the prior information extracted from the segmentation mask to the super-resolution process. Furthermore, we add the color attention residual block as a color fidelity unit to preserve the color information of the mapped area. In this way, we can input the latent code generated by the style encoder as parameters into the network to improve the image quality. Experimental results demonstrate that our proposed model achieves superior performance over state-of-the-art approaches including the enhanced super-resolution generative adversarial networks (ESRGAN) and Residual Channel Attention Networks (RCAN). © 2020 IEEE.","Color; Convolutional neural networks; Deep neural networks; Image segmentation; Mapping; Optical resolving power; Semantics; Signal encoding; Adversarial networks; Face hallucination; Face super-resolution; Image super resolutions; Low-resolution face images; Segmentation masks; Semantic segmentation; State-of-the-art approach; Image enhancement","face hallucination; semantic segmentation; style encoder","Conference paper","Final","","Scopus","2-s2.0-85103165913"
"Mhiri I.; Mahjoub M.A.; Rekik I.","Mhiri, Islem (57211785783); Mahjoub, Mohamed Ali (14034185700); Rekik, Islem (55546384900)","57211785783; 14034185700; 55546384900","StairwayGraphNet for Inter- and Intra-modality Multi-resolution Brain Graph Alignment and Synthesis","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12966 LNCS","","","140","150","10","10.1007/978-3-030-87589-3_15","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116472426&doi=10.1007%2f978-3-030-87589-3_15&partnerID=40&md5=cfaae8ffdcb2528563ee868f71c14e0d","Synthesizing multimodality medical data provides complementary knowledge and helps doctors make precise clinical decisions. Although promising, existing multimodal brain graph synthesis frameworks have several limitations. First, they mainly tackle only one problem (intra- or inter-modality), limiting their generalizability to synthesizing inter- and intra-modality simultaneously. Second, while few techniques work on super-resolving low-resolution brain graphs within a single modality (i.e., intra), inter-modality graph super-resolution remains unexplored though this would avoid the need for costly data collection and processing. More importantly, both target and source domains might have different distributions, which causes a domain fracture between them. To fill these gaps, we propose a multi-resolution StairwayGraphNet (SG-Net) framework to jointly infer a target graph modality based on a given modality and super-resolve brain graphs in both inter and intra domains. Our SG-Net is grounded in three main contributions: (i) predicting a target graph from a source one based on a novel graph generative adversarial network in both inter (e.g., morphological-functional) and intra (e.g., functional-functional) domains, (ii) generating high-resolution brain graphs without resorting to the time consuming and expensive MRI processing steps, and (iii) enforcing the source distribution to match that of the ground truth graphs using an inter-modality aligner to relax the loss function to optimize. Moreover, we design a new Ground Truth-Preserving loss function to guide both generators in learning the topological structure of ground truth brain graphs more accurately. Our comprehensive experiments on predicting target brain graphs from source graphs using a multi-resolution stairway showed the outperformance of our method in comparison with its variants and state-of-the-art method. SG-Net presents the first work for graph alignment and synthesis across varying modalities and resolutions, which handles graph size, distribution, and structure variations. Our Python TIS-Net code is available on BASIRA GitHub at https://github.com/basiralab/SG-Net. © 2021, Springer Nature Switzerland AG.","Data handling; Medical computing; Medical imaging; Topology; Clinical decision; Data collection; Ground truth; Intermodality; Loss functions; Lower resolution; Medical data; Multi-modal; Multi-modality; Superresolution; Graphic methods","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116472426"
"Synthiya Vinothini D.; Sathya Bama B.","Synthiya Vinothini, D. (57198896392); Sathya Bama, B. (36024410500)","57198896392; 36024410500","Attention-Based SRGAN for Super Resolution of Satellite Images","2021","Lecture Notes in Electrical Engineering","749 LNEE","","","407","423","16","10.1007/978-981-16-0289-4_31","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111077794&doi=10.1007%2f978-981-16-0289-4_31&partnerID=40&md5=7e4ade9b9370f568ff913d8127b58b53","Single image super resolution plays a vital role in satellite image processing as the observed satellite image generally has low resolution due to the bottleneck in imaging sensor equipment and the communication bandwidth. Deep learning provides a better solution to improve its resolution compared to many sophisticated algorithms; hence, a deep attention-based SRGAN network is proposed. The GAN network consists of an attention-based SR generator to hallucinate the missing fine texture detail, a discriminator to guess how realistic is the generated image. The SR generator consists of a feature reconstruction network and attention mechanism. Feature reconstruction network consists of residually connected RDB blocks to reconstruct HR feature. The attention mechanism acts as a feature selector to enhance high-frequency details and suppress undesirable components in uniform region. The reconstructed HR feature and enhanced high-frequency information are fused together for better visual perception. The experiment is conducted on WorldView-2 satellite data using Googles free cloud computing GPU, Google colab. The proposed deep network performs better than the other conventional methods. © 2021, Springer Nature Singapore Pte Ltd.","Image processing; Intelligent computing; Learning systems; Optical resolving power; Satellites; Textures; Attention mechanisms; Communication bandwidth; Conventional methods; Feature reconstruction; High frequency HF; High-frequency informations; Satellite image processing; Visual perception; Deep learning","Deep learning; Generative adversarial network; Satellite image; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85111077794"
"Zhang Y.; Liu G.; Zhao Y.; Zha D.; Xin W.; Zhao L.; Wang L.","Zhang, Yuzhen (57222002222); Liu, Guanqun (57212515892); Zhao, Yanyan (57225044180); Zha, Daren (36028747500); Xin, Wang (57437470300); Zhao, Lin (57198899227); Wang, Lei (57070597700)","57222002222; 57212515892; 57225044180; 36028747500; 57437470300; 57198899227; 57070597700","FRAGAN-VSR: Frame-Recurrent Attention Generative Adversarial Network for Video Super-Resolution","2021","Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","2021-November","","","753","757","4","10.1109/ICTAI52525.2021.00119","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123947258&doi=10.1109%2fICTAI52525.2021.00119&partnerID=40&md5=ac72f31627992e854310ae4f8581c503","Video super resolution (SR) is an important task, which recovers high-resolution (HR) frames from consecutive low-resolution (LR) couterparts. The most advanced works achieved good performance to this day. However, most of them has largely focussed on making a breakthrough in accuracy and speed, which has neglect that how to recover the finer texture details. Therefore, in this paper, we first present an Video SR model combined generative adversarial network and recurrent neural network (GAN-RNN) structure. It is forced by the self-attention mechanism to pay great attention to the high-frequency information of the LR frames. The perceptual loss is introduced to retain the high-frequency detail which is different from other video SR network. A great deal of evaluations and comparisons with previous methods have confirmed the merits of the proposed framework which can significantly outperform the current state of the art.  © 2021 IEEE.","Computer vision; Optical resolving power; Recurrent neural networks; Textures; Attention mechanisms; High frequency HF; High resolution; High-frequency informations; Lower resolution; Neural networks structure; Performance; Self-attention mechanism; Super-resolution models; Video super-resolution; Generative adversarial networks","Generative Adversarial Network; Recurrent Neural Network; Self-attention Mechanism; Video Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85123947258"
"Wu Z.; Zhao Z.; Ma P.; Huang B.","Wu, Zherong (57221169067); Zhao, Zhuoyi (57226041416); Ma, Peifeng (56411541500); Huang, Bo (55388074800)","57221169067; 57226041416; 56411541500; 55388074800","Real-World DEM Super-Resolution Based on Generative Adversarial Networks for Improving InSAR Topographic Phase Simulation","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9516888","8373","8385","12","10.1109/JSTARS.2021.3105123","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113241559&doi=10.1109%2fJSTARS.2021.3105123&partnerID=40&md5=eabce4e4a2de8860fad9057abb94da1a","Topographic phase simulation is important for deformation estimation in differential synthetic aperture radar (SAR) interferometry. The most commonly used 30 m resolution shuttle radar topography mission (SRTM) digital elevation model (DEM) is usually required to be resampled due to its relatively low resolution (LR) comparing to the high resolution (HR) SAR images. Although the WorldDEM with a 12 m resolution achieves global coverage, it is not available freely. Consequently, it is useful to evaluate the practicability of the super-resolution (SR) from LR SRTM DEMs to HR WorldDEM ones, which has not been investigated. Most existing DEM SR models are trained with synthetic datasets in which the LR DEMs are downsampled from their HR counterparts. However, these models become less effective when applied to real-world scenarios due to the domain gap between the synthetic and real LR DEMs. In this article, we constructed a real-world DEM SR dataset, where the LR and HR DEMs were collected from SRTM and WorldDEM, respectively. An enhanced SR generative adversarial network model was adapted to train on the dataset. Considering that the real LR-HR pairs may suffer from misalignment, we introduced the perceptual loss for better optimizing the model. Moreover, a logarithmic normalization was proposed to compress the wide elevation range and adjust the uneven distribution. We also pretrained the model using natural images since collecting sufficient HR DEMs is costly. Experiments demonstrate that the proposed method achieves near 0.69 dB improvement of peak signal-to-noise ratio. In addition, our method is also validated to improve the topographic phase simulation by 23.42% of MSE.  © 2008-2012 IEEE.","Optical resolving power; Signal to noise ratio; Space-based radar; Surveying; Tracking radar; Adversarial networks; Differential synthetic aperture radar interferometry (DInSAR); Digital elevation model; Global coverage; Peak signal to noise ratio; Real-world scenario; Super resolution; Synthetic datasets; computer simulation; digital elevation model; image resolution; machine learning; radar imagery; Shuttle Radar Topography Mission; synthetic aperture radar; topography; Synthetic aperture radar","Deep learning; digital elevation model; generative adversarial network; InSAR topographic phase simulation; super resolution (SR)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85113241559"
"","","","4th International Symposium on Artificial Intelligence and Robotics, ISAIR2019","2021","Studies in Computational Intelligence","917","","","","","260","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097407433&partnerID=40&md5=ecd9fdc8d36b2190df4814b6951406e9","The proceedings contain 19 papers. The special focus in this conference is on Artificial Intelligence and Robotics. The topics include: Classification of Hyperspectral Image Based on Shadow Enhancement by Dynamic Stochastic Resonance; generative Image Inpainting; rice Growth Prediction Based on Periodic Growth; a Multi-scale Progressive Method of Image Super-Resolution; track Related Bursty Topics in Weibo; terrain Classification Algorithm for Lunar Rover Based on Visual Convolutional Neural Network; an Automatic Evaluation Method for Modal Logic Combination Formula; Research on CS-Based Channel Estimation Algorithm for UWB Communications; A New Unambiguous Acquisition Algorithm for BOC(n, n) Signals; building Label-Balanced Emotion Corpus Based on Active Learning for Text Emotion Classification; arbitrary Perspective Crowd Counting via Local to Global Algorithm; a Semi-supervised Learning Method for Automatic Nuclei Segmentation Using Generative Adversarial Networks; research on Image Encryption Based on Wavelet Transform Integrating with 2D Logistic; high Level Video Event Modeling, Recognition and Reasoning via Petri Net; question Generalization in Conversation; optimal Scheduling of IoT Tasks in Cloud-Fog Computing Networks; context-Aware Based Discriminative Siamese Neural Network for Face Verification.","","","Conference review","Final","","Scopus","2-s2.0-85097407433"
"Yamashita K.; Markov K.","Yamashita, Koki (57217698952); Markov, Konstantin (7102521956)","57217698952; 7102521956","Medical image enhancement using super resolution methods","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12141 LNCS","","","496","508","12","10.1007/978-3-030-50426-7_37","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087533334&doi=10.1007%2f978-3-030-50426-7_37&partnerID=40&md5=36ef8fd4d67133c7f6badda7bd76a5da","Deep Learning image processing methods are gradually gaining popularity in a number of areas including medical imaging. Classification, segmentation, and denoising of images are some of the most demanded tasks. In this study, we aim at enhancing optic nerve head images obtained by Optical Coherence Tomography (OCT). However, instead of directly applying noise reduction techniques, we use multiple state-of-the-art image Super-Resolution (SR) methods. In SR, the low-resolution (LR) image is upsampled to match the size of the high-resolution (HR) image. With respect to image enhancement, the upsampled LR image can be considered as low quality, noisy image, and the HR image would be the desired enhanced version of it. We experimented with several image SR architectures, such as super-resolution Convolutional Neural Network (SRCNN), very deep Convolutional Network (VDSR), deeply recursive Convolutional Network (DRCN), and enhanced super-resolution Generative Adversarial Network (ESRGAN). Quantitatively, in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM), the SRCNN, VDSR, and DRCN significantly improved the test images. Although the ERSGAN showed the worst PSNR and SSIM, qualitatively, it was the best one. © Springer Nature Switzerland AG 2020.","Convolution; Convolutional neural networks; Deep learning; Eye protection; Image denoising; Image segmentation; Medical imaging; Optical data processing; Optical resolving power; Optical tomography; Signal to noise ratio; Convolutional networks; High resolution image; Image processing - methods; Low resolution images; Noise reduction technique; Peak signal to noise ratio; Structural similarity indices (SSIM); Superresolution methods; Image enhancement","Image super resolution; Medical image processing; OCT image enhancement","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85087533334"
"Alotaibi A.","Alotaibi, Aziz (55364546500)","55364546500","Deep generative adversarial networks for image-to-image translation: A review","2020","Symmetry","12","10","1705","1","26","25","10.3390/sym12101705","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093676941&doi=10.3390%2fsym12101705&partnerID=40&md5=3d64681ac980645aced1375edb03a30a","Many image processing, computer graphics, and computer vision problems can be treated as image-to-image translation tasks. Such translation entails learning to map one visual representation of a given input to another representation. Image-to-image translation with generative adversarial networks (GANs) has been intensively studied and applied to various tasks, such as multimodal image-to-image translation, super-resolution translation, object transfiguration-related translation, etc. However, image-to-image translation techniques suffer from some problems, such as mode collapse, instability, and a lack of diversity. This article provides a comprehensive overview of image-to-image translation based on GAN algorithms and its variants. It also discusses and analyzes current state-of-the-art image-to-image translation techniques that are based on multimodal and multidomain representations. Finally, open issues and future research directions utilizing reinforcement learning and three-dimensional (3D) modal translation are summarized and discussed. © 2020 by the author. Licensee MDPI, Basel, Switzerland.","","Adversarial learning; Deep generative model; Deep learning; Generative adversarial networks; Image-to-image translation","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85093676941"
"Amruth Gowtham N.; Deepakq S.; Patra D.","Amruth Gowtham, N. (57221785904); Deepakq, Shashikanth (57221789920); Patra, Dipti (23985620900)","57221785904; 57221789920; 23985620900","Super-Resolution Generative Adversarial Network with Modified Architecture for Single Image Super-Resolution","2020","4th International Conference on Computer, Communication and Signal Processing, ICCCSP 2020","","","9315285","","","","10.1109/ICCCSP49186.2020.9315285","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100133512&doi=10.1109%2fICCCSP49186.2020.9315285&partnerID=40&md5=980db8c84e48023bafabf0d2294d8e11","Recently, Single image Super-Resolution (SISR) has become an attractive research area in Image processing which generates a High-Resolution (HR) image by using Single Low-Resolution (LR) image. Deep learningbased SISR approaches have achieved better Super-Resolved (SR) results by using mean squared error (MSE) as an objective function that increases the quality of SR results over performance metrics like peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM). Nevertheless, MSE based approaches lead to generate over smoothed images with less high-frequency texture information at larger upscaling factors. Recent experiments have proved that Generative Adversarial Networks (GAN) generates perceptually convincing SR images through efficient extraction of high-frequency information from single LR image. In this paper, we propose a GAN based approach for SISR with modified deep-residual network architecture. In our proposed technique, we introduce the bottle-neck convolutional (CN) layer in the network structure of the Generator. Adding bottle-neck layers improves the network performance through 1 x 1 convolutional layers which extract complex features from the input and also reduces the computational complexity compared to 3 x 3 convolution layers. We further improve the model performance by removing batch normalization layer from the entire generator to overcome the unpleasant artifacts and improves GPU usage while training.  © 2020 IEEE.","Bottles; Complex networks; Convolution; Image processing; Image resolution; Mean square error; Network layers; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; High resolution image; High-frequency informations; Low resolution images; Modified architecture; Peak signal to noise ratio; Structural similarity indices (SSIM); Texture information; Network architecture","Adversarial training; GAN; Gang Plank; Nash Equilibrium; Resource Provisioning; Scalar chain; SISR; SRGAN; Super-resolution; Trust","Conference paper","Final","","Scopus","2-s2.0-85100133512"
"Avanaki N.J.; Zadtootaghaj S.; Barman N.; Schmidt S.; Martini M.G.; Moller S.","Avanaki, Nasim Jamshidi (57217847335); Zadtootaghaj, Saman (57195276053); Barman, Nabajeet (57193016561); Schmidt, Steven (7401845235); Martini, Maria G. (7202427977); Moller, Sebastian (24076818400)","57217847335; 57195276053; 57193016561; 7401845235; 7202427977; 24076818400","Quality Enhancement of Gaming Content using Generative Adversarial Networks","2020","2020 12th International Conference on Quality of Multimedia Experience, QoMEX 2020","","","9123074","","","","10.1109/QoMEX48832.2020.9123074","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087682901&doi=10.1109%2fQoMEX48832.2020.9123074&partnerID=40&md5=619d0e113c554522b3c5f07b4190a965","Recently, streaming of gameplay scenes has gained much attention, as evident with the rise of platforms such as Twitch.tv and Facebook Gaming. These streaming services have to deal with many challenges due to the low quality of source materials caused by client devices, network limitations such as bandwidth and packet loss, as well as low delay requirements. Spatial video artifact such as blockiness and blurriness as a result of as video compression or up-scaling algorithms can significantly impact the Quality of Experience of end-users of passive gaming video streaming applications. In this paper, we investigate solutions to enhance the video quality of compressed gaming content. Recently, several super-resolution enhancement techniques using Generative Adversarial Network (e.g., SRGAN) have been proposed, which are shown to work with high accuracy on non-gaming content. Towards this end, we improved the SRGAN by adding a modified loss function as well as changing the generator network such as layer levels and skip connections to improve the flow of information in the network, which is shown to improve the perceived quality significantly. In addition, we present a performance evaluation of improved SRGAN for the enhancement of frame quality caused by compression and rescaling artifacts for gaming content encoded in multiple resolution-bitrate pairs. © 2020 IEEE.","Image compression; Multimedia systems; Network layers; Quality of service; Adversarial networks; Multiple resolutions; Perceived quality; Quality enhancement; Quality of experience (QoE); Streaming service; Super resolution; Video Streaming Applications; User experience","Gaming Content; GAN; QoE; Quality Assessment; Quality Enhancement","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85087682901"
"He X.; Lei Y.; Fu Y.; Mao H.; Curran W.J.; Liu T.; Yang X.","He, Xiuxiu (57209105956); Lei, Yang (57202715941); Fu, Yabo (57193681581); Mao, Hui (7201795917); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Yang, Xiaofeng (36712893800)","57209105956; 57202715941; 57193681581; 7201795917; 57203070877; 26643332700; 36712893800","Super-resolution magnetic resonance imaging reconstruction using deep attention networks","2020","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11313","","2549604","","","","10.1117/12.2549604","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092602700&doi=10.1117%2f12.2549604&partnerID=40&md5=15bff567dd0969d437621bf8b5336754","We propose a deep-learning-based method to reconstruct super-resolution images from routinely captured MRI images. We propose to integrate a deeply supervised attention model into a generative adversarial network (GAN)-based framework to improve MRI image resolution. Deep attention GANs are introduced to enable end-to-end encoding-and-decoding learning. Next, an attention model is used to retrieve the most relevant information from the encoder. The residual network is used to learn the difference between low- and high-resolution images. This technique was validated with 20 patients. We performed a leave-one-out cross-validation method to evaluate the proposed algorithm and further tested it with a down-sampling rate 1/3 and 1/6. Our reconstructed high-resolution MRI images from down-sampling images were compared with the original image to evaluate the performance quantitatively. The reconstructed super-resolution images were compared to a high-resolution reference scan using and the mean absolute error (MAE) and peak signal-to-noise ratio (PSNR) of image intensity profiles. The MAE and PSNR between reconstructed and original images for 1/3 down-sampling rate were 3.86±1.53 and 41.95 ± 5.06dB, and 9.45 ± 1.73, 35.05 ± 3.64dB for 1/6 respectively, which demonstrates the accuracy of the proposed method. © 2020 SPIE. All rights reserved.","Deep learning; Image enhancement; Image reconstruction; Image resolution; Magnetic resonance imaging; Optical resolving power; Signal encoding; Signal sampling; Signal to noise ratio; Adversarial networks; Encoding and decoding; High resolution image; Image intensities; Learning-based methods; Leave-one-out cross validations; Mean absolute error; Peak signal to noise ratio; Medical image processing","Attention gate; Deep learning; MRI; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85092602700"
"Zhang Z.","Zhang, Zhongwei (57218528857)","57218528857","Research Progress on Generative Adversarial Network with its Applications","2020","Proceedings of 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference, ITOEC 2020","","","9141685","396","399","3","10.1109/ITOEC49072.2020.9141685","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091064846&doi=10.1109%2fITOEC49072.2020.9141685&partnerID=40&md5=60057b269897baded0026d512a4789b8","As a new unsupervised learning algorithm framework, generative adversarial networks(GAN) have been favored by more and more researchers, and it has become a research hotspot now. GAN is inspired by the two-person zero-sum game theory in game theory. Its unique adversarial training idea can generate high-quality samples and has more powerful feature learning and feature expression capabilities than traditional machine learning algorithms. At present, GAN has achieved remarkable success in the field of computer vision, especially in the field of sample generation. Every year, a large number of GAN-related research papers are produced, reflecting the fiery degree of research on GAN model. Aiming at the hot model of GAN, first introduce the research status of GAN; then introduce the theory and framework of GAN, which analyzes in detail why the gradient disappears and the mode collapses during the training of GAN; then discussed some typical GAN improvement models, and summarized their theoretical improvements, advantages, limitations, application scenarios and implementation costs; Finally, the application results of GAN in data generation, image super-resolution, and image style conversion are shown, and the current challenges and future research directions of GAN are discussed.  © 2020 IEEE.","Game theory; Image enhancement; Machine learning; Adversarial networks; Application scenario; Feature expression; Future research directions; Image super resolutions; Implementation cost; Sample generations; Two-person zero-sum game; Learning algorithms","generative adversarial networks; machine learning; unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85091064846"
"Zhao M.; Liu X.; Liu H.; Wong K.K.L.","Zhao, Ming (57189875649); Liu, Xinhong (57213175673); Liu, Hui (55072479500); Wong, Kelvin K.L. (26434942800)","57189875649; 57213175673; 55072479500; 26434942800","Super-resolution of cardiac magnetic resonance images using Laplacian Pyramid based on Generative Adversarial Networks","2020","Computerized Medical Imaging and Graphics","80","","101698","","","","10.1016/j.compmedimag.2020.101698","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077662699&doi=10.1016%2fj.compmedimag.2020.101698&partnerID=40&md5=b34e53c28435b06ee4c7fee237132753","Background and objective: Cardiac magnetic resonance imaging (MRI) can assist in both functional and structural analysis of the heart, but due to hardware and physical limitations, high-resolution MRI scans is time consuming and peak signal-to-noise ratio (PSNR) is low. The existing super-resolution methods attempt to resolve this issue, but there are still shortcomings, such as hallucinate details after super-resolution, low precision after reconstruction, etc. To dispose these problems, we propose the Laplacian Pyramid Generation Adversarial Network (LSRGAN) in order to generate visually better cardiovascular ultrasound images so as to aid physician diagnosis and treatment. Methods and results: In order to address the problem of low image resolution, we used the Laplacian Pyramid to analyze the high-frequency detail features of super-resolution (SR) reconstruction of images with different pixel sizes. To eliminate gradient disappearance, we implemented the least squares loss function as the discriminator, we introduce the residual-dense block (RDB) as the basic network building unit is used to generate higher quality images. The experimental results show that the LSRGAN can effectively avoid the illusion details after super-resolution and has the best reconstruction quality. Compared with the state-of-the-art methods, our proposed algorithm generates higher quality super-resolution images that comes with higher peak signal-to-noise ratio and structural similarity (SSIM) scores. Conclusion: We implemented a novel LSRGAN network model, which solves reduces insufficient resolution and hallucinate details of MRI after super-resolution. Our research presents a superior super-resolution method for medical experts to diagnose and treat myocardial ischemia and myocardial infarction. © 2020","Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Myocardial Infarction; Myocardial Ischemia; Neural Networks, Computer; Signal-To-Noise Ratio; Biomedical signal processing; Diagnosis; Heart; Image enhancement; Image reconstruction; Image resolution; Laplace transforms; Magnetism; Optical resolving power; Resonance; Signal to noise ratio; Adversarial networks; Cardiac magnetic resonance images; Cardiac magnetic resonance imaging; Laplacian Pyramid; Peak signal to noise ratio; Single images; State-of-the-art methods; Super resolution reconstruction; Article; cardiovascular magnetic resonance; data visualization; human; image analysis; image quality; image reconstruction; intermethod comparison; laplacian pyramid generation adversarial network; peak signal to noise ratio; priority journal; reconstruction algorithm; signal noise ratio; structural similarity score; diagnostic imaging; heart infarction; heart muscle ischemia; image enhancement; image processing; nuclear magnetic resonance imaging; procedures; Magnetic resonance imaging","Cardiac magnetic resonance imaging; Generative Adversarial Networks; Image enhancement; Laplacian Pyramid; Single image super-resolution","Article","Final","","Scopus","2-s2.0-85077662699"
"Umer R.M.; Micheloni C.","Umer, Rao Muhammad (57211291072); Micheloni, Christian (6507976201)","57211291072; 6507976201","Deep Cyclic Generative Adversarial Residual Convolutional Networks for Real Image Super-Resolution","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12537 LNCS","","","484","498","14","10.1007/978-3-030-67070-2_29","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101541016&doi=10.1007%2f978-3-030-67070-2_29&partnerID=40&md5=c3452000d26da234c7cc05a200d08ad6","Recent deep learning based single image super-resolution (SISR) methods mostly train their models in a clean data domain where the low-resolution (LR) and the high-resolution (HR) images come from noise-free settings (same domain) due to the bicubic down-sampling assumption. However, such degradation process is not available in real-world settings. We consider a deep cyclic network structure to maintain the domain consistency between the LR and HR data distributions, which is inspired by the recent success of CycleGAN in the image-to-image translation applications. We propose the Super-Resolution Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a generative adversarial network (GAN) framework for the LR to HR domain translation in an end-to-end manner. We demonstrate our proposed approach in the quantitative and qualitative experiments that generalize well to the real image super-resolution and it is easy to deploy for the mobile/embedded devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge datasets demonstrate that the proposed SR approach achieves comparable results as the other state-of-art methods. © 2020, Springer Nature Switzerland AG.","Arts computing; Computer vision; Deep learning; Optical resolving power; Adversarial networks; Convolutional networks; Degradation process; High resolution image; Network structures; Qualitative experiments; Real world setting; State-of-art methods; Convolutional neural networks","Convex optimization; Cyclic GAN; Deep convolutional neural networks; Image restoration; Real image super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101541016"
"Molahasani Majdabadi M.; Ko S.-B.","Molahasani Majdabadi, Mahdiyar (57216440558); Ko, Seok-Bum (7403326100)","57216440558; 7403326100","Capsule GAN for robust face super resolution","2020","Multimedia Tools and Applications","79","41-42","","31205","31218","13","10.1007/s11042-020-09489-y","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089556743&doi=10.1007%2fs11042-020-09489-y&partnerID=40&md5=3e520de9f022839c40bba1d8d7c236ff","Face hallucination is an emerging sub-field of Super-Resolution (SR) which aims to reconstruct the High-Resolution (HR) facial image given its Low-Resolution (LR) counterpart. The task becomes more challenging when the LR image is extremely small due to the image distortion in the super-resolved results. A variety of deep learning-based approaches has been introduced to address this issue by using attribute domain information. However, a more complex dataset or even further networks is required for training these models. In order to avoid these complexities and yet preserve the precision in reconstructed output, a robust Multi-Scale Gradient capsule GAN for face SR is proposed in this paper. A novel similarity metric called Feature SIMilarity (FSIM) is introduced as well. The proposed network surpassed state-of-the-art face SR systems in all metrics and demonstrates more robust performance while facing image transformations. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Deep learning; Domain informations; Face hallucination; Face super-resolution; Feature similarity(FSIM); Image transformations; Learning-based approach; Robust performance; Similarity metrics; Optical resolving power","Capsule network; Face hallucination; Generative Adversarial Network (GAN); Super resolution","Article","Final","","Scopus","2-s2.0-85089556743"
"Chadha A.; Britto J.; Roja M.M.","Chadha, Aman (56039988800); Britto, John (57202434175); Roja, M. Mani (24480198300)","56039988800; 57202434175; 24480198300","iSeeBetter: Spatio-temporal video super-resolution using recurrent generative back-projection networks","2020","Computational Visual Media","6","3","","307","317","10","10.1007/s41095-020-0175-7","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088298317&doi=10.1007%2fs41095-020-0175-7&partnerID=40&md5=430ea1db05f47d5d6b5d4f8a7c880b03","Recently, learning-based models have enhanced the performance of single-image super-resolution (SISR). However, applying SISR successively to each video frame leads to a lack of temporal coherency. Convolutional neural networks (CNNs) outperform traditional approaches in terms of image quality metrics such as peak signal to noise ratio (PSNR) and structural similarity (SSIM). On the other hand, generative adversarial networks (GANs) offer a competitive advantage by being able to mitigate the issue of a lack of finer texture details, usually seen with CNNs when super-resolving at large upscaling factors. We present iSeeBetter, a novel GAN-based spatio-temporal approach to video super-resolution (VSR) that renders temporally consistent super-resolution videos. iSeeBetter extracts spatial and temporal information from the current and neighboring frames using the concept of recurrent back-projection networks as its generator. Furthermore, to improve the “naturality” of the super-resolved output while eliminating artifacts seen with traditional algorithms, we utilize the discriminator from super-resolution generative adversarial network. Although mean squared error (MSE) as a primary loss-minimization objective improves PSNR/SSIM, these metrics may not capture fine details in the image resulting in misrepresentation of perceptual quality. To address this, we use a four-fold (MSE, perceptual, adversarial, and total-variation loss function. Our results demonstrate that iSeeBetter offers superior VSR fidelity and surpasses state-of-the-art performance. © 2020, The Author(s).","Competition; Convolutional neural networks; Image enhancement; Image quality; Mean square error; Optical resolving power; Signal to noise ratio; Textures; Competitive advantage; Learning Based Models; Peak signal to noise ratio; Spatio-temporal approach; State-of-the-art performance; Structural similarity; Traditional approaches; Video super-resolution; Recurrent neural networks","convolutional neural networks; frame recurrence; generative adversarial networks; optical flow; super resolution; video upscaling","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088298317"
"Sathya K.; Sangavi D.; Sridharshini P.; Manobharathi M.; Jayapriya G.","Sathya, K. (57214675768); Sangavi, D. (57247159700); Sridharshini, P. (57248243700); Manobharathi, M. (57248243800); Jayapriya, G. (57248458200)","57214675768; 57247159700; 57248243700; 57248243800; 57248458200","Improved image based super resolution and concrete crack prediction using pre-trained deep learning models","2020","Journal of Soft Computing in Civil Engineering","4","3","","40","51","11","10.22115/SCCE.2020.229355.1219","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114350644&doi=10.22115%2fSCCE.2020.229355.1219&partnerID=40&md5=5d728d7bd37f6b4038b94ce32098d349","Detection and prediction of cracks play a vital role in the maintenance of concrete structures. The manual instructions result in having images captured from different sources wherein the acquisition of such images into the network may cause an error. The errors are rectified by a method to increase the resolution of those images and are imposed through Super-Resolution Generative Adversarial Network (SRGAN) with a pre-trained model of VGG19. After increasing the resolution then comes the prediction of crack from high resolution images through Convolutional Neural Network (CNN) with a pre-trained model of ResNet50 that trains a dataset of 40,000 images which consists of both crack and non-crack images. This work makes a comparative analysis of predicting the crack after and before the super-resolution method and their performance measure is compared. Compared with other methods on super-resolution and prediction, the proposed method appears to be more stable, faster and highly effective. For the dataset used in this work, the model yields an accuracy of 98.2%, proving the potential of using deep learning for concrete crack detection. © 2020 The Authors. Published by Pouyan Press.","","Crack and non-crack images; Crack prediction; Generative adversarial network (GAN); Highly resolution image; ResNet50; Super-resolution generative adversarial network (SRGAN); VGG19","Article","Final","","Scopus","2-s2.0-85114350644"
"Batchuluun G.; Lee Y.W.; Nguyen D.T.; Pham T.D.; Park K.R.","Batchuluun, Ganbayar (57188649020); Lee, Young Won (57195539045); Nguyen, Dat Tien (35608738000); Pham, Tuyen Danh (55808639500); Park, Kang Ryoung (8983316300)","57188649020; 57195539045; 35608738000; 55808639500; 8983316300","Thermal Image Reconstruction Using Deep Learning","2020","IEEE Access","8","","9136691","126839","126858","19","10.1109/ACCESS.2020.3007896","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089475791&doi=10.1109%2fACCESS.2020.3007896&partnerID=40&md5=d0734f1276e7b72d088090b38d631ca4","A high-resolution thermal camera is very expensive and is thus difficult to be used. Furthermore, thermal images become blurred in various cases of object motion, camera shaking, and camera defocusing. To solve these problems, a previous super-resolution restoration (SRR) technique converting a thermal image acquired by a low-resolution camera into a high-resolution one, and a thermal image deblurring method have been researched. However, existing studies were performed based on 1-channel (grayscale) images. In addition, a large-sized and whole image has been used in the existing thermal image deblurring methods, which causes lower deblurring performance. In this study, we propose novel SRR and deblurring methods. The proposed deblurring method is conducted based on small region images. The proposed methods are also conducted using 3-channel (color) thermal images and generative adversarial networks. In addition, the performances of this method are compared in various color spaces (RGB, Gray, HLS, HSV, Lab, Luv, XYZ, YCrCb), image sizes, and thermal databases. Through experiments using self-collected databases and open databases, it was confirmed that the proposed methods show better performance than the state-of-the-art methods. © 2013 IEEE.","Cameras; Color; Database systems; Deep learning; Image reconstruction; Adversarial networks; High resolution; Low resolution; Object motion; State-of-the-art methods; Super-resolution restoration; Thermal camera; Thermal images; Image enhancement","deep learning; generative adversarial network; image deblurring; super-resolution reconstruction; Thermal image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85089475791"
"Takano N.; Alaghband G.","Takano, Nao (57219750949); Alaghband, Gita (6603532928)","57219750949; 6603532928","Generator from Edges: Reconstruction of Facial Images","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12510 LNCS","","","430","443","13","10.1007/978-3-030-64559-5_34","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098177293&doi=10.1007%2f978-3-030-64559-5_34&partnerID=40&md5=079e556db0c1d9a3c831ad48a778e782","Applications that involve supervised training require paired images. Researchers of single image super-resolution (SISR) create such images by artificially generating blurry input images from the corresponding ground truth. Similarly we can create paired images with the canny edge. We propose Generator From Edges (GFE) [Fig. 1]. Our aim is to determine the best architecture for GFE, along with reviews of perceptual loss [1, 2]. To this end, we conducted three experiments. First, we explored the effects of the adversarial loss often used in SISR. In particular, we uncovered that it is not an essential component to form a perceptual loss. Eliminating adversarial loss will lead to a more effective architecture from the perspective of hardware resource. It also means that considerations for the problems pertaining to generative adversarial network (GAN) [3], such as mode collapse, are not necessary. Second, we reexamined VGG loss and found that the mid-layers yield the best results. By extracting the full potential of VGG loss, the overall performance of perceptual loss improves significantly. Third, based on the findings of the first two experiments, we reevaluated the dense network to construct GFE. Using GFE as an intermediate process, reconstructing a facial image from a pencil sketch can become an easy task. © 2020, Springer Nature Switzerland AG.","Network architecture; Adversarial networks; Canny edges; Dense network; Facial images; Ground truth; Hardware resources; Single images; Supervised trainings; Image reconstruction","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098177293"
"Gu J.; Shen Y.; Zhou B.","Gu, Jinjin (57212042330); Shen, Yujun (57207766466); Zhou, Bolei (36697366200)","57212042330; 57207766466; 36697366200","Image processing using multi-code GaN prior","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9157000","3009","3018","9","10.1109/CVPR42600.2020.00308","120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094101942&doi=10.1109%2fCVPR42600.2020.00308&partnerID=40&md5=5a2c3cdd2b3066979fb422431c0959c4","Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.1 © 2020 IEEE.","Backpropagation; Codes (symbols); Gallium nitride; III-V semiconductors; Image enhancement; Pattern recognition; Semantics; Adversarial networks; Image colorizations; Image Inpainting; Image synthesis; Intermediate layers; Multiple features; Reconstruction quality; Super resolution; Image reconstruction","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094101942"
"Yang H.; Li S.; Deng Z.; Ma Y.; Yu B.; Young E.F.Y.","Yang, Haoyu (57193136289); Li, Shuhe (57211649209); Deng, Zihao (57210919596); Ma, Yuzhe (57198591067); Yu, Bei (54939654000); Young, Evangeline F. Y. (13308439200)","57193136289; 57211649209; 57210919596; 57198591067; 54939654000; 13308439200","GAN-OPC: Mask Optimization with Lithography-Guided Generative Adversarial Nets","2020","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","39","10","8823939","2822","2834","12","10.1109/TCAD.2019.2939329","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071862025&doi=10.1109%2fTCAD.2019.2939329&partnerID=40&md5=242347ec426532c9f35f5b5c0c339f3a","Mask optimization has been a critical problem in the VLSI design flow due to the mismatch between the lithography system and the continuously shrinking feature sizes. Optical proximity correction (OPC) is one of the prevailing resolution enhancement techniques (RETs) that can significantly improve mask printability. However, in advanced technology nodes, the mask optimization process consumes more and more computational resources. In this article, we develop a generative adversarial network (GAN) model to achieve better mask optimization performance. We first develop an OPC-oriented GAN flow that can learn target-mask mapping from the improved architecture and objectives, which leads to satisfactory mask optimization results. To facilitate the training process and ensure better convergence, we propose a pretraining scheme that jointly trains the neural network with inverse lithography technique (ILT). We also propose an enhanced generator design with a U-Net architecture and a subpixel super-resolution structure that promise a better convergence and a better mask quality, respectively. At convergence, the generative network is able to create quasi-optimal masks for given target circuit patterns and fewer normal OPC steps are required to generate high quality masks. The experimental results show that our flow can facilitate the mask optimization process as well as ensure a better printability. © 1982-2012 IEEE.","Gallium nitride; Gas generators; III-V semiconductors; Inverse problems; Lithography; Mathematical models; Optimization; Personnel training; Photolithography; Semiconductor device models; Semiconductor devices; Advanced technology; Adversarial networks; Computational resources; Inverse lithography; Lithography systems; Optical proximity corrections; Resolution enhancement technique; Shrinking feature sizes; Network architecture","Convolutional neural networks; generative model; inverse lithography; optical proximity correction","Article","Final","","Scopus","2-s2.0-85071862025"
"Wu K.; Qiang Y.; Song K.; Ren X.; Yang W.K.; Zhang W.; Hussain A.; Cui Y.","Wu, Kun (57211681436); Qiang, Yan (26639724500); Song, Kai (57211684200); Ren, Xueting (57211682430); Yang, WenKai (57211025809); Zhang, Wanjun (57169366800); Hussain, Akbar (57211683833); Cui, Yanfen (53877122400)","57211681436; 26639724500; 57211684200; 57211682430; 57211025809; 57169366800; 57211683833; 53877122400","Image synthesis in contrast MRI based on super resolution reconstruction with multi-refinement cycle-consistent generative adversarial networks","2020","Journal of Intelligent Manufacturing","31","5","","1215","1228","13","10.1007/s10845-019-01507-7","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074847809&doi=10.1007%2fs10845-019-01507-7&partnerID=40&md5=d195d6dadb7a7f5210c9aa99f851fba0","In the field of medical image processing represented by magnetic resonance imaging (MRI), synthesizing the complementary target contrast of the target patient from the existing contrast has obvious medical significance for assisting doctors in making clinical diagnoses. To satisfy the image translation problem between different MRI contrasts (T1 and T2), a generative adversarial network is proposed that works in an end-to-end manner at image level. The low-frequency and high-frequency information of the image is preserved by using multi-stage optimization learning aided by adversarial loss, the loss of perceptual consistency and the loss of cyclic consistency, as it results in preserving the same contrast anatomical structure of the source domain supervisely when the perceptual pixel distribution of the target contrast is learned perfectly. To integrate different penalties (L1 and L2) organically, adaptive weights are set for the error sensitivity of the penalty function in the present total loss function, the aim being to achieve adaptive optimization of each stage of generating high-resolution images. In addition, a new net structure called multi-skip connection residual net is proposed to refine medical image details step by step with multi-stage optimization. Compared with the existing technology, the present method is more advanced. The contrast conversion of T1 and T2 in MRI is validated, which can help to shorten the imaging time, improve the imaging quality, and effectively assist doctors with diagnoses. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Diagnosis; Image reconstruction; Medical imaging; Synthesis (chemical); Adversarial networks; Anatomical structures; Cyclic consistency; High-frequency informations; Multi stage; Multi-skip; Multi-stage optimization; Super resolution reconstruction; Magnetic resonance imaging","Contrast MRI; Cyclic consistency; Generative adversarial network; Multi-skip; Multi-stage; Synthesis","Article","Final","","Scopus","2-s2.0-85074847809"
"Dittimi T.V.; Suen C.Y.","Dittimi, Tamarafinide V. (57194283160); Suen, Ching Y. (7102317250)","57194283160; 7102317250","Single Image Super-Resolution for Medical Image Applications","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12068 LNCS","","","660","666","6","10.1007/978-3-030-59830-3_57","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092928174&doi=10.1007%2f978-3-030-59830-3_57&partnerID=40&md5=14141c71f0e80b1b45d190ebb1527766","In medical imaging, high-resolution images are expected to have the ability to deliver a more precise diagnosis with the practical application of high-resolution displays. This research proposes a deep learning method for single image super-resolution that learns an end-to-end mapping between the low and high-resolution images. It redesigns the SRGAN, using VGG19 network for feature extraction, setting discriminator network’s working space as feature space, and adding the loss function based on the mean square error of pixel space, gaining more details by incorporating SRCNN layers to increase the PSNR in the reconstruction at the same time. To thoroughly investigate the system, we compared the performance with other architectures on MNIST and CIFAR-10 dataset with a further evaluation conducted on Chest x-ray. © 2020, Springer Nature Switzerland AG.","Deep learning; Diagnosis; Learning systems; Mean square error; Optical resolving power; Pattern recognition; Feature space; High resolution display; High resolution image; Learning methods; Loss functions; Medical image applications; Single images; Working space; Medical imaging","Chest radiography; Deep learning; Generative adversarial network; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85092928174"
"Xia H.; Yang Y.; Hu X.","Xia, Hongrui (57215558846); Yang, Yingyun (25655601100); Hu, Xiao (57218589712)","57215558846; 25655601100; 57218589712","Laplacian Generative Adversarial Networks for Multi-Scale Super-Resolution","2020","Proceedings of 2020 IEEE 5th Information Technology and Mechatronics Engineering Conference, ITOEC 2020","","","9141731","1543","1547","4","10.1109/ITOEC49072.2020.9141731","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091078919&doi=10.1109%2fITOEC49072.2020.9141731&partnerID=40&md5=ec11e63fe5a1779a2b0c2c55f564a67c","Single-image super-resolution, aims to reconstruct images from low-resolution to high-resolution and super-high-resolution, so as to improve image clarity and visual effects. Based on the principles of generating adversarial networks and image pyramids, a LapSRGAN is proposed to compromise the merits of Laplacian Pyramid Generative Adversarial Network(LapGAN) and Laplacian Pyramid Super-Resolution Network(LapSRN). LapSRGAN is an end-to-end image reconstruction network, which can achieve double and quadruple high-quality, high-resolution reconstruction of the original image. The proposed LapSRGAN is trained with multiscale discriminators and perceptual loss which calculated on feature maps of image. Extensive quantitative and qualitative evaluations on bench mark datasets confirm the effectiveness of the proposed algorithm.  © 2020 IEEE.","Image enhancement; Laplace transforms; Optical resolving power; Adversarial networks; High resolution; High-resolution reconstruction; Laplacian Pyramid; Original images; Qualitative evaluations; Reconstruction networks; Super resolution; Image reconstruction","generative adversarial network; image reconstruction network; image super-resolution; Laplace pyramid","Conference paper","Final","","Scopus","2-s2.0-85091078919"
"Li X.; You X.","Li, Xu (57214905155); You, Xiangdong (57216282024)","57214905155; 57216282024","End-to-End Image Super-Resolution via Generative Adversarial Network","2020","IOP Conference Series: Materials Science and Engineering","768","7","072063","","","","10.1088/1757-899X/768/7/072063","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083016899&doi=10.1088%2f1757-899X%2f768%2f7%2f072063&partnerID=40&md5=dd65f0a85c14c56eaa19ee80c7cbbbb5","Image super-resolution is to extract information from single or more low- resolution images and use this information to get the corresponding high-resolution image. The ability to capture high texture details of low-resolution images is one of the impressive advantages of generative adversarial networks (GANs). This paper mainly studies the image reconstruction method based on a single low-resolution image, and builds an end-to-end image Super-Resolution method via generative adversarial networks to improve the image resolution. Convolution neural networks use the mean square error (MSE) as loss function. For such loss function, structural similarity (SSIM) and peak signal to noise ratio (PSNR) can be obtained, but high texture details of original images are often lost. The generative adversarial network mainly contains two parts: generator network and discriminator network. Adversarial loss uses a discriminator network to push our solution to a natural image manifold, which is trained to distinguish super-resolution images from photo-realistic images. Additionally, content loss of our network was indicated by perceptual similarity rather than similarity of pixel-wised space. © Published under licence by IOP Publishing Ltd.","","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85083016899"
"Goswami S.; Aakanksha; Rajagopalan A.N.","Goswami, Saurabh (57222081684); Aakanksha (57497719300); Rajagopalan, A.N. (16647440100)","57222081684; 57497719300; 16647440100","Robust Super-Resolution of Real Faces Using Smooth Features","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12535 LNCS","","","169","185","16","10.1007/978-3-030-66415-2_11","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101303514&doi=10.1007%2f978-3-030-66415-2_11&partnerID=40&md5=2491e8dad634305273bc72a2a169c1aa","Real low-resolution (LR) face images contain degradations which are too varied and complex to be captured by known downsampling kernels and signal-independent noises. So, in order to successfully super-resolve real faces, a method needs to be robust to a wide range of noise, blur, compression artifacts etc. Some of the recent works attempt to model these degradations from a dataset of real images using a Generative Adversarial Network (GAN). They generate synthetically degraded LR images and use them with corresponding real high-resolution (HR) image to train a super-resolution (SR) network using a combination of a pixel-wise loss and an adversarial loss. In this paper, we propose a two module super-resolution network where the feature extractor module extracts robust features from the LR image, and the SR module generates an HR estimate using only these robust features. We train a degradation GAN to convert bicubically downsampled clean images to real degraded images, and interpolate between the obtained degraded LR image and its clean LR counterpart. This interpolated LR image is then used along with it’s corresponding HR counterpart to train the super-resolution network from end to end. Entropy Regularized Wasserstein Divergence is used to force the encoded features learnt from the clean and degraded images to closely resemble those extracted from the interpolated image to ensure robustness. © 2020, Springer Nature Switzerland AG.","Computer vision; Adversarial networks; Compression artifacts; Degraded images; Feature extractor; High resolution image; Independent noise; Interpolated images; Super resolution; Optical resolving power","","Conference paper","Final","","Scopus","2-s2.0-85101303514"
"Chen Y.; Zhao Y.; Jia W.; Cao L.; Liu X.","Chen, Yuan (56438348100); Zhao, Yang (56472249000); Jia, Wei (57190406993); Cao, Li (57201873044); Liu, Xiaoping (35230365400)","56438348100; 56472249000; 57190406993; 57201873044; 35230365400","Adversarial-learning-based image-to-image transformation: A survey","2020","Neurocomputing","411","","","468","486","18","10.1016/j.neucom.2020.06.067","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087430431&doi=10.1016%2fj.neucom.2020.06.067&partnerID=40&md5=427f51da1cf335ec582ff2ccf3c2ee5b","Recently, the generative adversarial network (GAN) has attracted wide attention for various computer vision tasks. GAN provides a novel concept for image-to-image transformation by means of adversarial learning. In recent years, numerous adversarial-learning-based methods have been proposed, and impressive results have been achieved. Related reviews have mainly focused on the basic GAN model and its general variants; in contrast, this survey aims to provide an overview of adversarial-learning-based methods by focusing on the image-to-image transformation scenario. First, a brief review of basic GAN is presented; next, the related approaches are roughly divided into adversarial style transfer and adversarial image restoration, e.g., super-resolution, image inpainting, and de-raining. The network architectures of generative models and loss functions are introduced and discussed in detail. Finally, we conclude the survey with an analysis of the trends and challenges. © 2020 Elsevier B.V.","Learning systems; Network architecture; Surveys; Adversarial learning; Adversarial networks; Generative model; Image Inpainting; Image transformations; Loss functions; Novel concept; Super resolution; article; image reconstruction; learning; loss of function mutation; Image reconstruction","Adversarial learning; Generative adversarial network; Image-to-image transformation","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85087430431"
"Dhawan S.; Kumar S.","Dhawan, Sumit (57219119393); Kumar, Shailender (57200330136)","57219119393; 57200330136","Improving resolution of images using Generative Adversarial Networks","2020","Proceedings of the 4th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2020","","","9297414","880","887","7","10.1109/ICECA49313.2020.9297414","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099540906&doi=10.1109%2fICECA49313.2020.9297414&partnerID=40&md5=b552ded7ac8308919722a28b984e3bf3","Even with all the achievements in precision and speed of various image super-resolution models, such as better and more accurate Convolutional Neural Networks (CNN), the results have not been satisfactory. The high-resolution images produced are generally missing the finer and frequent texture details. The majority of the models in this area focus on such objective functions which minimize the Mean Square Error (MSE). Although, this produces images with better Peak Signal to Noise Ratio (PSNR) such images are perceptually unsatisfying and lack the fidelity and high-frequency details when seen at a high-resolution. Generative Adversarial Networks (GAN), a deep learningmodel, can be usedfor such problems. In this article, the working of the GAN is shown and described about the production satisfying images with decent PSNR score as well as good Perceptual Index (P1) when compared to other models. In contrast to the existing Super Resolution GAN model, various modifications have been introduced to improve the quality of images, like replacing batch normalization layer with weight normalization layer, modified the dense residual block, taking features for comparison before they are fed in activation layer, using the concept of a relativistic discriminator instead of a normal discriminator that is used in vanilla GAN and finally, using Mean Absolute Error in the model. © 2020 IEEE.","Convolutional neural networks; Mean square error; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; High frequency HF; High resolution image; Image super resolutions; Mean absolute error; Objective functions; Peak signal to noise ratio; Super resolution; Image enhancement","Deep learning; Generative adversarial networks; Image super resolution; Neural network","Conference paper","Final","","Scopus","2-s2.0-85099540906"
"Huang J.-H.; Wang H.-K.; Liao Z.-W.","Huang, Jun-Hong (57221331274); Wang, Hai-Kun (57221331082); Liao, Zhi-Wu (7203032583)","57221331274; 57221331082; 7203032583","HFD-SRGAN: Super-Resolution Generative Adversarial Network with High-frequency discriminator","2020","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2020-October","","9282980","3148","3153","5","10.1109/SMC42975.2020.9282980","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098892439&doi=10.1109%2fSMC42975.2020.9282980&partnerID=40&md5=586d2231353c28d3364912e968be5726","The high-frequencies of images is very important both in keeping the edges and suppressing artifacts. To improve the performance of single image super-resolution (SISR) based on the SRGAN framework, we propose Super-Resolution Generative Adversarial Networks with high-frequency discriminator (HFD- SRGAN) by designing an additional discriminator for image's high-frequencies extracted by wavelets. Based on SRGAN, the image's high frequencies extracted by discrete wavelet transformations (DWT) were then introduced into GAN. Moreover, an additional discriminator for these high frequencies was built. Since the proposed model provides a direct and efficient way to locates and estimates the high frequencies of the reconstruction image, the visual effects of reconstructed the images can be improved with fewer computation costs. Experiments show that HFD-SRGAN has improved the visual effects of SRGAN when using the same generator network as SRGAN. The evaluation results show the performance of our method is equal to the state-of-the-art methods. © 2020 IEEE.","Discrete wavelet transforms; Image enhancement; Optical resolving power; Adversarial networks; Computation costs; Discrete wavelet transformation; Evaluation results; High frequency HF; Reconstruction image; State-of-the-art methods; Super resolution; Discriminators","","Conference paper","Final","","Scopus","2-s2.0-85098892439"
"Wei Z.; Wang Y.; Li Z.; Zheng L.","Wei, Zhenhua (8056680800); Wang, Yuqi (57222486983); Li, Zhihong (57191496330); Zheng, Ling (7403406419)","8056680800; 57222486983; 57191496330; 7403406419","Inversion of smoke black concentration field in a tangentially fired furnace based on super-resolution reconstruction","2020","IEEE Access","8","","","165827","165836","9","10.1109/ACCESS.2020.3019713","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102791167&doi=10.1109%2fACCESS.2020.3019713&partnerID=40&md5=bccf6be5633cd85cfecc1bb7ff54a1da","The tangentially fired furnace has the advantages of sufficient combustion and low NOx emission, and the smoke black concentration in the furnace reflects the combustion situation of combustion equipment. The flame field and smoke black concentration field in the furnace are complex, so it is meaningful to study the flame field and the smoke black concentration field of the tangential furnace. In this article, the visual method was used to reconstruct the flame and smoke black concentration fields in the furnace. Due to the low resolution of industrial cameras, the final reconstruction effect is limited. Given this situation, this article proposes a strengthen edge characteristic super-resolution network (SECSR) algorithm suitable for the tangentially fired furnace flame images. The flame edge processing is added to the depth neural network, which enhances the ability of flame edge feature extraction. It constructs a generative adversarial network which can greatly improve the resolution of furnace flame image, to obtain high-resolution tangential furnace flame image. Secondly, based on the high-quality flame image, this article proposes an inversion algorithm of the black concentration field of the tangentially fired furnace smoke. The algorithm obtains the high-precision tangentially furnace flame temperature field through an inversion calculation to calculate the soot concentration field. In the future work, we will predict the variation trend of smoke black concentration in the furnace through the results of the inversion calculation model, to understand the combustion situation in the furnace and control the fuel consumption to reduce smoke black emission, which has important guiding significance for protecting the environment and saving resources. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Combustion; Combustion equipment; Edge detection; Heat treating furnaces; Image enhancement; Optical resolving power; Smoke abatement; Adversarial networks; Concentration fields; Flame temperature field; Guiding significances; Inversion algorithm; Inversion calculations; Super resolution reconstruction; Tangentially fired furnaces; Smoke","Smoke black concentration field; Super-resolution reconstruction; Tangentially fired furnace; Three-dimensional reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102791167"
"Ciolino M.; Noever D.; Kalin J.","Ciolino, Matthew (57218126114); Noever, David (55955470100); Kalin, Josh (57218127569)","57218126114; 55955470100; 57218127569","Training set effect on super resolution for automated target recognition","2020","Proceedings of SPIE - The International Society for Optical Engineering","11394","","113940P","","","","10.1117/12.2557845","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087930621&doi=10.1117%2f12.2557845&partnerID=40&md5=82672ac17807da7720af1c8c9b1a5836","Single Image Super Resolution (SISR) is the process of mapping a low-resolution image to a high-resolution image. This inherently has applications in remote sensing as a way to increase the spatial resolution in satellite imagery. This suggests a possible improvement to automated target recognition in image classification and object detection. We explore the effect that different training sets have on SISR with the network, Super Resolution Generative Adversarial Network (SRGAN). We train 5 SRGANs on different land-use classes (e.g. agriculture, cities, ports) and test them on the same unseen dataset. We attempt to find the qualitative and quantitative differences in SISR, binary classification, and object detection performance. We find that curated training sets that contain objects in the test ontology perform better on both computer vision tasks while having a complex distribution of images allows object detection models to perform better. However, Super Resolution (SR) might not be beneficial to certain problems and will see a diminishing amount of returns for datasets that are closer to being solved. © 2020 SPIE.","Agricultural robots; Automatic target recognition; Image enhancement; Land use; Object recognition; Optical resolving power; Remote sensing; Satellite imagery; Statistical tests; Adversarial networks; Automated target recognition; Binary classification; Detection performance; High resolution image; Low resolution images; Spatial resolution; Super resolution; Object detection","Deep learning; Image classification; Object detection; Satellite imagery; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85087930621"
"Ma J.; Zhang L.; Zhang J.","Ma, Jie (57205916758); Zhang, Libao (35325855000); Zhang, Jue (56513505100)","57205916758; 35325855000; 56513505100","SD-GAN: Saliency-Discriminated GAN for Remote Sensing Image Superresolution","2020","IEEE Geoscience and Remote Sensing Letters","17","11","8933080","1973","1977","4","10.1109/LGRS.2019.2956969","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095797768&doi=10.1109%2fLGRS.2019.2956969&partnerID=40&md5=815e545e039bd3a0e0768e85794499f2","Recently, convolutional neural networks have shown superior performance in single-image superresolution. Although existing mean-square-error-based methods achieve high peak signal-to-noise ratio (PSNR), they tend to generate oversmooth results. Generative adversarial network (GAN)-based methods can provide high-resolution (HR) images with higher perceptual quality, but produce pseudotextures in images, which generally leads to lower PSNR. Besides, different regions in remote sensing images (RSIs) reflect discrepant surface topography and visual characteristics. This means a uniform reconstruction strategy may not be suitable for all targets in RSIs. To solve these problems, we propose a novel saliency-discriminated GAN for RSI superresolution. First, hierarchical weakly supervised saliency analysis is introduced to compute a saliency map, which is subsequently employed to distinguish the diverse demands of regions in the following generator and discriminator part. Different from previous GANs, the proposed residual dense saliency generator takes saliency maps as a supplementary condition in the generator. Simultaneously, combining the characteristic of RSIs, we design a new paired discriminator to enhance the perceptual quality, which measures the distance between generated images and HR images in salient areas and nonsalient areas, respectively. Comprehensive evaluations validate the superiority of the proposed model.  © 2004-2012 IEEE.","Convolutional neural networks; Image segmentation; Mean square error; Optical resolving power; Remote sensing; Signal to noise ratio; Topography; Adversarial networks; Comprehensive evaluation; High resolution image; Peak signal to noise ratio; Perceptual quality; Remote sensing images; Saliency analysis; Super resolution; image analysis; image resolution; machine learning; remote sensing; satellite altimetry; signal-to-noise ratio; topographic mapping; Image enhancement","Deep learning; generative adversarial network (GAN); remote sensing; saliency analysis; superresolution","Article","Final","","Scopus","2-s2.0-85095797768"
"Pinto F.; Romanoni A.; Matteucci M.; Torr P.H.S.","Pinto, Francesco (57226129852); Romanoni, Andrea (55949082600); Matteucci, Matteo (7005216953); Torr, Philip H.S. (56821543600)","57226129852; 55949082600; 7005216953; 56821543600","SECI-GAN: Semantic and edge completion for dynamic objects removal","2020","Proceedings - International Conference on Pattern Recognition","","","9413320","10441","10448","7","10.1109/ICPR48806.2021.9413320","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110556493&doi=10.1109%2fICPR48806.2021.9413320&partnerID=40&md5=48a11d808817d0ea20025c9aa40de992","Image inpainting aims at synthesizing the missing content of damaged or corrupted images to produce visually realistic restorations; typical applications are in image restoration, automatic scene editing, super-resolution, and dynamic object removal. In this paper, we propose Semantic and Edge Conditioned Inpainting Generative Adversarial Network (SECI-GAN), an architecture that jointly exploits the high-level cues extracted by semantic segmentation and the fine-grained details captured by edge extraction to condition the image inpainting process. SECI-GAN is designed with a particular focus on recovering big regions belonging to the same object (e.g. cars or pedestrians) in the context of dynamic object removal from complex street views. To demonstrate the effectiveness of SECI-GAN, we evaluate our results on the Cityscapes dataset, showing that SECI-GAN is better than competing state-of-the-art models at recovering the structure and the content of the missing parts while producing consistent predictions. © 2020 IEEE","Image segmentation; Pattern recognition; Restoration; Semantics; Adversarial networks; Corrupted images; Dynamic objects; Image Inpainting; Semantic segmentation; State of the art; Super resolution; Typical application; Image reconstruction","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85110556493"
"He J.; Zheng J.; Shen Y.; Guo Y.; Zhou H.","He, Jijun (57189212888); Zheng, Jinjin (8518127200); Shen, Yuan (55492339400); Guo, Yutang (56137496300); Zhou, Hongjun (7404742428)","57189212888; 8518127200; 55492339400; 56137496300; 7404742428","Facial Image Synthesis and Super-Resolution With Stacked Generative Adversarial Network","2020","Neurocomputing","402","","","359","365","6","10.1016/j.neucom.2020.03.107","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083672039&doi=10.1016%2fj.neucom.2020.03.107&partnerID=40&md5=4ee6bc2391c804fddc825286d07b2c39","Image synthesis and super-resolution (SR) have always been a hot spot for computer vision and image processing research. Since the development of Deep Learning, especially after the Deep Convolutional Generative Adversarial Network (DC-GAN) methods, facial image synthesis and SR problem had been solved in many circumstances. But most of the existing works were focused on natural-looking of the synthesized result rather than keeping facial information of the original image. Our paper presented an end-to-end method of getting high-resolution photo-realistic facial images from low-resolution (LR) in-the-wild images without losing the facial identity details. The pipeline used a flexible stacked GAN structure for the SR process with different target image resolutions on different upscaling factors. To avoid getting blur or nonsensical image output and realize the flexibility, “U-Net” architecture and upsampling layers with residual learning blocks were stacked. The stacked network structure makes applying different loss functions in different parts of the network possible, which helps to solve the two problems of keeping identical facial details of the LR input image and generating high-quality output images simultaneously. By using 3 different loss functions in different positions of the stacked network separately, through experimental comparison, we found the best stacked residual block parameters which could get the best output image quality. Experimental results also explicated that the network had a good SR ability compare to state of the art methods in different resolution and upscaling factor. © 2020","Deep learning; Image resolution; Optical resolving power; Adversarial networks; Different resolutions; Experimental comparison; Facial Image synthesis; High-resolution photos; Network structures; State-of-the-art methods; Super resolution; Article; convolutional neural network; deep learning; facial recognition; identity; image analysis; image display; image enhancement; image processing; image quality; mathematical computing; priority journal; stacked generative adversarial network; Image processing","Face hallucination; Generative Adversarial Network; Image synthesis; Super Resolution","Article","Final","","Scopus","2-s2.0-85083672039"
"Qian Z.; Huang K.; Wang Q.-F.; Xiao J.; Zhang R.","Qian, Zhuang (57214458405); Huang, Kaizhu (13403476100); Wang, Qiu-Feng (56097694800); Xiao, Jimin (53980810800); Zhang, Rui (56024777800)","57214458405; 13403476100; 56097694800; 53980810800; 56024777800","Generative adversarial classifier for handwriting characters super-resolution","2020","Pattern Recognition","107","","107453","","","","10.1016/j.patcog.2020.107453","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086010943&doi=10.1016%2fj.patcog.2020.107453&partnerID=40&md5=6c50f9c44115bc8f6344602396d85db2","Generative Adversarial Networks (GAN) receive great attention recently due to its excellent performance in image generation, transformation, and super-resolution. However, less emphasis or study has been put on GAN for classification with super-resolution. Moreover, though GANs may fabricate images which perceptually looks realistic, they usually fabricate some fake details especially in character data; this would impose further difficulties when they are input for classification. In this paper, we propose a novel Generative Adversarial Classifier (GAC) for low-resolution handwriting character recognition. Specifically, we design an additional classifier component in GAC, leading to a novel three-player GAN model which is not only able to generate high-quality super-resolved images, but also favorable for classification. Experimental results show that our proposed method can obtain remarkable performance in handwriting characters with 8 × super-resolution, achieving new state-of-the-art on benchmark dataset CASIA-HWDB1.1, and MNIST. © 2020 Elsevier Ltd","Benchmarking; Optical resolving power; Adversarial networks; Benchmark datasets; Character datum; High quality; Image generations; Low resolution; State of the art; Super resolution; Character recognition","Generative adversarial networks (GAN); Handwriting characters recognition; Super-Resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086010943"
"Zhao M.; Liu X.; Yao X.; He K.","Zhao, Ming (57189875649); Liu, Xinhong (57213175673); Yao, Xin (55351554500); He, Kun (57216367254)","57189875649; 57213175673; 55351554500; 57216367254","Better visual image super-resolution with laplacian pyramid of generative adversarial networks","2020","Computers, Materials and Continua","64","3","","1601","1614","13","10.32604/cmc.2020.09754","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090861734&doi=10.32604%2fcmc.2020.09754&partnerID=40&md5=7563f351e0b7497388f1726ffadaf90f","Although there has been a great breakthrough in the accuracy and speed of super-resolution (SR) reconstruction of a single image by using a convolutional neural network, an important problem remains unresolved: how to restore finer texture details during image super-resolution reconstruction? This paper proposes an Enhanced Laplacian Pyramid Generative Adversarial Network (ELSRGAN), based on the Laplacian pyramid to capture the high-frequency details of the image. By combining Laplacian pyramids and generative adversarial networks, progressive reconstruction of super-resolution images can be made, making model applications more flexible. In order to solve the problem of gradient disappearance, we introduce the Residual-in-Residual Dense Block (RRDB) as the basic network unit. Network capacity benefits more from dense connections, is able to capture more visual features with better reconstruction effects, and removes BN layers to increase calculation speed and reduce calculation complexity. In addition, a loss of content driven by perceived similarity is used instead of content loss driven by spatial similarity, thereby enhancing the visual effect of the super-resolution image, making it more consistent with human visual perception. Extensive qualitative and quantitative evaluation of the baseline datasets shows that the proposed algorithm has higher mean-sort-score (MSS) than any state-of-the-art method and has better visual perception. © 2020 Tech Science Press. All rights reserved.","Convolutional neural networks; Image enhancement; Laplace transforms; Optical resolving power; Textures; Vision; Adversarial networks; Human visual perception; Image super-resolution reconstruction; Laplacian Pyramid; Quantitative evaluation; Spatial similarity; State-of-the-art methods; Super resolution reconstruction; Image reconstruction","Generative adversarial networks; Laplacian pyramid; Single image super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090861734"
"Xiao Y.; Peters K.R.; Fox W.C.; Rees J.H.; Rajderkar D.A.; Arreola M.M.; Barreto I.; Bolch W.E.; Fang R.","Xiao, Yao (57715894400); Peters, Keith R. (7402786780); Fox, W. Christopher (15765332200); Rees, John H. (57216653329); Rajderkar, Dhanashree A. (12140225500); Arreola, Manuel M. (6603383809); Barreto, Izabella (57200420994); Bolch, Wesley E. (35232422500); Fang, Ruogu (36720287600)","57715894400; 7402786780; 15765332200; 57216653329; 12140225500; 6603383809; 57200420994; 35232422500; 36720287600","Transfer-Gan: Multimodal Ct Image Super-Resolution Via Transfer Generative Adversarial Networks","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098322","195","198","3","10.1109/ISBI45749.2020.9098322","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085861212&doi=10.1109%2fISBI45749.2020.9098322&partnerID=40&md5=f650dd535bea8e72f7ee98b71ba8e365","Multimodal CT scans, including non-contrast CT, CT perfusion, and CT angiography, are widely used in acute stroke diagnosis and therapeutic planning. While each imaging modality has its advantage in brain cross-sectional feature visualizations, the varying image resolution of different modalities hinders the ability of the radiologist to discern consistent but subtle suspicious findings. Besides, higher image quality requires a high radiation dose, leading to increases in health risks such as cataract formation and cancer induction. In this work, we propose a deep learning-based method Transfer-GAN that utilizes generative adversarial networks and transfer learning to improve multimodal CT image resolution and to lower the necessary radiation exposure. Through extensive experiments, we demonstrate that transfer learning from multimodal CT provides substantial visualization and quantity enhancement compare to the training without learning the prior knowledge. © 2020 IEEE.","Deep learning; Health risks; Image enhancement; Image resolution; Medical imaging; Transfer learning; Visualization; Adversarial networks; Cancer induction; Cataract formation; CT angiography; Imaging modality; Learning-based methods; Prior knowledge; Radiation Exposure; Computerized tomography","Generative Adversarial Network; Image Super-Resolution; Multimodal CT; Transfer Learning","Conference paper","Final","","Scopus","2-s2.0-85085861212"
"Gao F.; Zhu J.; Jiang H.; Niu Z.; Han W.; Yu J.","Gao, Fei (56984952300); Zhu, Jingjie (57206729598); Jiang, Hanliang (57192431124); Niu, Zhenxing (36679259700); Han, Weidong (56415556800); Yu, Jun (56145221000)","56984952300; 57206729598; 57192431124; 36679259700; 56415556800; 56145221000","Incremental focal loss GANs","2020","Information Processing and Management","57","3","102192","","","","10.1016/j.ipm.2019.102192","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078088693&doi=10.1016%2fj.ipm.2019.102192&partnerID=40&md5=1c8b2f8e6ab3211b8c62ad87eacd0fe2","Generative Adversarial Networks (GANs) have achieved inspiring performance in both unsupervised image generation and conditional cross-modal image translation. However, how to generate quality images at an affordable cost is still challenging. We argue that it is the vast number of easy examples that disturb training of GANs, and propose to address this problem by down-weighting losses assigned to easy examples. Our novel Incremental Focal Loss (IFL) progressively focuses training on hard examples and prevents easy examples from overwhelming the generator and discriminator during training. In addition, we propose an enhanced self-attention (ESA) mechanism to boost the representational capacity of the generator. We apply IFL and ESA to a number of unsupervised and conditional GANs, and conduct experiments on various tasks, including face photo-sketch synthesis, map↔aerial-photo translation, single image super-resolution reconstruction, and image generation on CelebA, LSUN, and CIFAR-10. Results show that IFL boosts learning of GANs over existing loss functions. Besides, both IFL and ESA make GANs produce quality images with realistic details in all these tasks, even when no task adaptation is involved. © 2020 Elsevier Ltd","Antennas; Optical resolving power; Adversarial networks; Image generations; Image super-resolution reconstruction; Image translation; Loss functions; Quality image; Single-image super-resolution reconstruction; Task adaptation; Image reconstruction","Face photo-sketch synthesis; Generative adversarial networks; Image generation; Image super-resolution reconstruction; Image-to-image translation","Article","Final","","Scopus","2-s2.0-85078088693"
"Ji H.; Gao Z.; Liu X.; Zhang Y.; Mei T.","Ji, Hong (57205763449); Gao, Zhi (55256514200); Liu, Xiaodong (56420642200); Zhang, Yongjun (55577971100); Mei, Tiancan (8914886000)","57205763449; 55256514200; 56420642200; 55577971100; 8914886000","Small object detection leveraging on simultaneous super-resolution","2020","Proceedings - International Conference on Pattern Recognition","","","9413058","9054","9061","7","10.1109/ICPR48806.2021.9413058","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110485870&doi=10.1109%2fICPR48806.2021.9413058&partnerID=40&md5=f0b28d82151c3dfaf7af0c2683cd264e","Despite the impressive advancement achieved in object detection, the detection performance of small object is still far from satisfactory due to the lack of sufficient detailed appearance to distinguish it from similar objects. Inspired by the positive effects of super-resolution for object detection, we propose a framework that can be incorporated with detector networks to improve the performance of small object detection, in which the low-resolution image is super-resolved via generative adversarial network (GAN) in an unsupervised manner. In our method, the super-resolution network and the detection network are trained jointly. In particular, the detection loss is back-propagated into the super-resolution network during training to facilitate detection. Compared with available simultaneous super-resolution and detection methods which heavily rely on low-/high-resolution image pairs, our work breaks through such restriction via applying the CycleGAN strategy, achieving increased generality and applicability, while remaining an elegant structure. Extensive experiments on datasets from both computer vision and remote sensing communities demonstrate that our method obtains competitive performance on a wide range of complex scenarios. © 2020 IEEE","Image enhancement; Object recognition; Optical resolving power; Remote sensing; Adversarial networks; Competitive performance; Detection methods; Detection networks; Detection performance; Detector networks; Low resolution images; Small object detection; Object detection","","Conference paper","Final","","Scopus","2-s2.0-85110485870"
"Chu M.; Xie Y.; Mayer J.; Leal-Taixé L.; Thuerey N.","Chu, Mengyu (57196000654); Xie, You (57204697503); Mayer, Jonas (57221031994); Leal-Taixé, Laura (35758586600); Thuerey, Nils (26024848800)","57196000654; 57204697503; 57221031994; 35758586600; 26024848800","Learning temporal coherence via self-supervision for GAN-based video generation","2020","ACM Transactions on Graphics","39","4","75","","","","10.1145/3386569.3392457","53","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090384401&doi=10.1145%2f3386569.3392457&partnerID=40&md5=9ce613f631dcdd77f59ac9b10e4c4c88","Our work explores temporal self-supervision for GAN-based video generation tasks. While adversarial training successfully yields generative models for a variety of areas, temporal relationships in the generated data are much less explored. Natural temporal changes are crucial for sequential generation tasks, e.g. video super-resolution and unpaired video translation. For the former, state-of-the-art methods often favor simpler norm losses such as L2 over adversarial training. However, their averaging nature easily leads to temporally smooth results with an undesirable lack of spatial detail. For unpaired video translation, existing approaches modify the generator networks to form spatio-temporal cycle consistencies. In contrast, we focus on improving learning objectives and propose a temporally self-supervised algorithm. For both tasks, we show that temporal adversarial learning is key to achieving temporally coherent solutions without sacrificing spatial detail. We also propose a novel Ping-Pong loss to improve the long-term temporal consistency. It effectively prevents recurrent networks from accumulating artifacts temporally without depressing detailed features. Additionally, we propose a first set of metrics to quantitatively evaluate the accuracy as well as the perceptual quality of the temporal evolution. A series of user studies confirm the rankings computed with these metrics. Code, data, models, and results are provided at https://github.com/thunil/TecoGAN.  © 2020 ACM.","Adversarial learning; Sequential generation; State-of-the-art methods; Supervised algorithm; Temporal consistency; Temporal evolution; Temporal relationships; Video super-resolution; Recurrent neural networks","generative adversarial network; self-supervision; temporal cycle-consistency; unpaired video translation; video super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090384401"
"Alqahtani H.; Kavakli-Thorne M.; Kumar G.","Alqahtani, Hamed (57194574403); Kavakli-Thorne, Manolya (57219506272); Kumar, Gulshan (35932222600)","57194574403; 57219506272; 35932222600","Generative adversarial networks - an introduction","2020","Image Recognition: Progress, Trends and Challenges","","","","107","134","27","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145342974&partnerID=40&md5=84df3d7970cb88a2f6e8b618de0c7ac0","Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This chapter aims to present an overview of GANs, and its different variants. The chapter attempts to identify GANs' advantages, disadvantages and significant challenges to the successful implementation of GAN in different application areas. Finally, the chapter ends with the conclusion and future aspects. © 2020 by Nova Science Publishers, Inc. All rights reserved.","","Generative adversarial networks; Neural networks; Supervised learning; Unsupervised learning","Book chapter","Final","","Scopus","2-s2.0-85145342974"
"Kashihara K.","Kashihara, Koji (7006657805)","7006657805","Iris Recognition for Biometrics Based on CNN with Super-resolution GAN","2020","IEEE Conference on Evolving and Adaptive Intelligent Systems","2020-May","","9122757","","","","10.1109/EAIS48028.2020.9122757","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088133301&doi=10.1109%2fEAIS48028.2020.9122757&partnerID=40&md5=09105f8090adec9612ad2828aea06c4c","Biometric technologies can realize high-security systems. Although camera performance and photographic environment cause a low signal-to-noise ratio, super-resolution (SR) techniques, such as generative adversarial networks (SRGANs) and deep convolutional neural networks (DCNNs), can improve image quality. This study aimed to investigate the effect of the SRGANs on individual identification by DCNNs, assuming external image noise and a prefiltering process in actual cases of iris recognition. After downgraded iris images were improved by the SRGANs, a DCNN classifier predicted the individuals from the restored images. The accuracies of the DCNN classifier were higher in the SR images using the Bicubic method or squared mean errors than the SRGANs focusing on perceptual loss. This result suggests that it may be easier for the DCNN classifier to create image features based on the pixel-based differences (i.e., high peak signal-to-noise ratio), rather than on the perceptual image differences. In the future, a robust security system based on SR methods may be capable of assessing a health condition using the images obtained for individual certification.  © 2020 IEEE.","Adaptive systems; Biometrics; Convolutional neural networks; Deep neural networks; Image quality; Intelligent systems; Optical resolving power; Security systems; Signal to noise ratio; Adversarial networks; Biometric technology; Health condition; Image difference; Individual identification; Iris recognition; Low signal-to-noise ratio; Peak signal to noise ratio; Image enhancement","biometrics; deep learning; image analysis; iris recognition; neural networks","Conference paper","Final","","Scopus","2-s2.0-85088133301"
"Sengupta S.; Wong A.; Singh A.; Zelek J.; Lakshminarayanan V.","Sengupta, Sourya (57212026321); Wong, Alexander (15073608800); Singh, Amitojdeep (57208907697); Zelek, John (6603746225); Lakshminarayanan, Vasudevan (7004723358)","57212026321; 15073608800; 57208907697; 6603746225; 7004723358","DeSupGAN: Multi-scale Feature Averaging Generative Adversarial Network for Simultaneous De-blurring and Super-Resolution of Retinal Fundus Images","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12069 LNCS","","","32","41","9","10.1007/978-3-030-63419-3_4","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097431164&doi=10.1007%2f978-3-030-63419-3_4&partnerID=40&md5=4f9ba179aff5737931d39bb724d10919","Image quality is of utmost importance for image-based clinical diagnosis. In this paper, a generative adversarial network-based retinal fundus quality enhancement network is proposed. With the advent of different cheaper, affordable and lighter point-of-care imaging or telemedicine devices, the chances of making a better and more accessible healthcare system in developing countries become higher. But these devices often lack the quality of images. This single network simultaneously takes into account two different image degradation problems that are common i.e. blurring and low spatial resolution. A novel convolutional multi-scale feature averaging block (MFAB) is proposed which can extract feature maps with different kernel sizes and fuse them together. Both local and global feature fusion are used to get a stable training of wide network and to learn the hierarchical global features. The results show that this network achieves better results in terms of peak-signal-to-noise ratio (PSNR) and structural similarity index (SSIM) metrics compared with other super-resolution, de-blurring methods. To the best of our knowledge, this is the first work that has combined multiple degradation models simultaneously for retinal fundus images analysis. © 2020, Springer Nature Switzerland AG.","Computer aided analysis; Developing countries; Diagnosis; Image analysis; Image quality; Ophthalmology; Optical resolving power; Signal to noise ratio; Adversarial networks; Health-care system; Multi-scale features; Multiple degradations; Peak signal to noise ratio; Quality enhancement; Retinal fundus images; Structural similarity indices (SSIM); Medical imaging","Generative adversarial network; Image de-blurring; Image quality; Image super-resolution; Retinal fundus image","Conference paper","Final","","Scopus","2-s2.0-85097431164"
"Xu X.; Ye Y.; Li X.","Xu, Xuan (57203638498); Ye, Yanfang (23037530700); Li, Xin (36063742800)","57203638498; 23037530700; 36063742800","Joint Demosaicing and Super-Resolution (JDSR): Network Design and Perceptual Optimization","2020","IEEE Transactions on Computational Imaging","6","","9109718","968","980","12","10.1109/TCI.2020.2999819","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100543319&doi=10.1109%2fTCI.2020.2999819&partnerID=40&md5=b32c4dc652fa11dd4f47cda32e226df9","Image demosaicing and super-resolution are two important tasks in color imaging pipeline. So far they have been mostly independently studied in the open literature of deep learning; little is known about the potential benefit of formulating a joint demosaicing and super-resolution (JDSR) problem. In this article, we propose an end-to-end optimization solution to the JDSR problem and demonstrate its practical significance in computational imaging. Our technical contributions are mainly two-fold. On network design, we have developed a Residual-Dense Squeeze-and-Excitation Networks (RDSEN) supported by a pre-demosaicing network (PDNet) as the pre-processing step. We address the issue of spatio-spectral attention for color-filter-array (CFA) data and discuss how to achieve better information flow by concatenating Residue-Dense Squeeze-and-Excitation Blocks (RDSEBs) for JDSR. Experimental results have shown that significant PSNR/SSIM gain can be achieved by RDSEN over previous network architectures including state-of-the-art RCAN. On perceptual optimization, we propose to leverage the latest ideas including relativistic discriminator and pre-excitation perceptual loss function to further improve the visual quality of textured regions in reconstructed images. Our extensive experiment results have shown that Texture-enhanced Relativistic average Generative Adversarial Network (TRaGAN) can produce both subjectively more pleasant images and objectively lower perceptual distortion scores than standard GAN for JDSR. Finally, we have verified the benefit of JDSR to high-quality image reconstruction from real-world Bayer pattern data collected by NASA Mars Curiosity. © 2015 IEEE.","Deep learning; Image reconstruction; NASA; Network architecture; Optical resolving power; Textures; Adversarial networks; Color filter arrays; Computational imaging; Optimization solution; Perceptual distortion; Perceptual optimization; Reconstructed image; Technical contribution; Image enhancement","Color imaging; joint image demosaicing and super-resolution (JDSR); perceptual optimization; residual-dense squeeze-and-excitation network (RDSEN)","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85100543319"
"Li Y.; Huang H.; Zhang L.; Wang G.; Zhang H.; Zhou W.","Li, Yunling (57217028548); Huang, Hui (57218083599); Zhang, Lijuan (57210157984); Wang, Guangyi (56521941700); Zhang, Honglai (55264879200); Zhou, Wu (55925370500)","57217028548; 57218083599; 57210157984; 56521941700; 55264879200; 55925370500","Super-Resolution and Self-Attention with Generative Adversarial Network for Improving Malignancy Characterization of Hepatocellular Carcinoma","2020","Proceedings - International Symposium on Biomedical Imaging","2020-April","","9098705","1556","1560","4","10.1109/ISBI45749.2020.9098705","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085865806&doi=10.1109%2fISBI45749.2020.9098705&partnerID=40&md5=54cd7214ed4a697caefffec52b78ab45","The slice thickness of MR imaging may remarkably degrade the clarity of 3D lesion images within through-plane slices (coronal or sagittal views) so as to influence the performance of lesion characterization. To alleviate the problem, we propose an end-to-end super-resolution and self-attention framework based on Generative adversarial networks (GAN) for improving the malignancy characterization of hepatocellular carcinoma (HCC). Specifically, a super-resolution subnetwork is designed to enhance the low-resolution patches of coronal or sagittal views based on the high resolution patches of the axial view, and then the enhanced patches are fed into the classification subnetwork for malignancy characterization. Furthermore, a self-attention mechanism is utilized to extract multi-level features for better super-resolution and lesion characterization. Experimental results of clinical HCCs demonstrate the superior performance of the proposed method compared with conventional CNN-based methods and show the potential in clinical practice. © 2020 IEEE.","Magnetic resonance imaging; Medical imaging; Adversarial networks; Attention mechanisms; Clinical practices; Hepatocellular carcinoma; High resolution; Low resolution; Slice thickness; Super resolution; Optical resolving power","Convolutional Neural Network; hepatocellular carcinoma; malignancy; self-attention; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85085865806"
"Prajapati K.; Chudasama V.; Patel H.; Upla K.; Ramachandra R.; Raja K.; Busch C.","Prajapati, Kalpesh (57217177596); Chudasama, Vishal (57202047982); Patel, Heena (57209145428); Upla, Kishor (53985429600); Ramachandra, Raghavendra (57190835798); Raja, Kiran (57188866050); Busch, Christoph (7101767185)","57217177596; 57202047982; 57209145428; 53985429600; 57190835798; 57188866050; 7101767185","Unsupervised single image super-resolution network (USISResNet) for real-world data using generative adversarial network","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9151093","1904","1913","9","10.1109/CVPRW50498.2020.00240","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090132650&doi=10.1109%2fCVPRW50498.2020.00240&partnerID=40&md5=1aa11c07fbdadafc66d10def6abf590e","Current state-of-the-art Single Image Super-Resolution (SISR) techniques rely largely on supervised learning where Low-Resolution (LR) images are synthetically generated with known degradation (e.g., bicubic downsampling). The deep learning models trained with such synthetic dataset generalize poorly on the real-world or natural data where the degradation characteristics cannot be fully modelled. As an implication, the super-resolved images obtained for real LR images do not produce optimal Super-Resolution (SR) images. We propose a new SR approach to mitigate such an issue using unsupervised learning in Generative Adversarial Network (GAN) framework - USISResNet. In an attempt to provide high quality SR image for perceptual inspection, we also introduce a new loss function based on the Mean Opinion Score (MOS). The effectiveness of the proposed architecture is validated with extensive experiments on NTIRE-2020 Real-world SR Challenge validation (Track-1) set along with testing datasets (Track-1 and Track-2). We demonstrate the generalizable nature of proposed network by evaluating real-world images as against other state-of-the-art methods which employ synthetically downsampled LR images. The proposed network has further been evaluated on NTIRE 2020 Real-world SR Challenge dataset where the approach has achieved reliable accuracy. © 2020 IEEE.","Computer vision; Image resolution; Optical resolving power; Adversarial networks; Degradation characteristics; Low resolution images; Mean opinion scores; Proposed architectures; Real-world image; State-of-the-art methods; Super resolution; Deep learning","","Conference paper","Final","","Scopus","2-s2.0-85090132650"
"Ha A.; Sun S.; Kim Y.K.; Lee J.; Jeoung J.W.; Kim H.C.; Park K.H.","Ha, Ahnul (57189002120); Sun, Sukkyu (57211841629); Kim, Young Kook (55908959500); Lee, Jinho (55010774100); Jeoung, Jin Wook (9640226000); Kim, Hee Chan (57213118053); Park, Ki Ho (56276582800)","57189002120; 57211841629; 55908959500; 55010774100; 9640226000; 57213118053; 56276582800","Deep-learning-based enhanced optic-disc photography","2020","PLoS ONE","15","10 October","e0239913","","","","10.1371/journal.pone.0239913","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092258377&doi=10.1371%2fjournal.pone.0239913&partnerID=40&md5=7b007733e53ddb29950f1e480d913ca6","Optic-disc photography (ODP) has proven to be very useful for optic nerve evaluation in glaucoma. In real clinical practice, however, limited patient cooperation, small pupils, or media opacities can limit the performance of ODP. The purpose of this study was to propose a deep-learning approach for increased resolution and improved legibility of ODP by contrast, color, and brightness compensation. Each high-resolution original ODP was transformed into two counterparts: (1) down-scaled ‘low-resolution ODPs’, and (2) ‘compensated high-resolution ODPs’ produced via enhancement of the visibility of the optic disc margin and surrounding retinal vessels using a customized image post-processing algorithm. Then, the differences between these two counterparts were directly learned through a super-resolution generative adversarial network (SR-GAN). Finally, by inputting the high-resolution ODPs into SR-GAN, 4-times-up-scaled and overall-color-and-brightness-transformed ‘enhanced ODPs’ could be obtained. General ophthalmologists were instructed (1) to assess each ODP’s image quality, and (2) to note any abnormal findings, at 1-month intervals. The image quality score for the enhanced ODPs was significantly higher than that for the original ODP, and the overall optic disc hemorrhage (DH)-detection accuracy was significantly higher with the enhanced ODPs. We expect that this novel deep-learning approach will be applied to various types of ophthalmic images. © 2020 Ha et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Deep Learning; Glaucoma; Humans; Image Processing, Computer-Assisted; Limit of Detection; Optic Disk; Photography; algorithm; Article; artificial neural network; brightness; color; contrast enhancement; controlled study; data analysis software; deep learning; diagnostic accuracy; eye photography; generative adversarial network; human; image post processing algorithm; image processing; image quality; intraocular hemorrhage; ophthalmologist; optic disc hemorrhage; optic disc photography; optic disk; retina blood vessel; super resolution generative adversarial network; validation study; diagnostic imaging; glaucoma; limit of detection; optic disk; photography; procedures","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092258377"
"Murali L.K.; Lutnick B.; Ginley B.; Tomaszewski J.E.; Sarder P.","Murali, Leema Krishna (57222381303); Lutnick, Brendon (57194460212); Ginley, Brandon (57191404935); Tomaszewski, John E. (35371876300); Sarder, Pinaki (13407502800)","57222381303; 57194460212; 57191404935; 35371876300; 13407502800","Generative modeling for renal microanatomy","2020","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11320","","113200F","","","","10.1117/12.2549891","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102518230&doi=10.1117%2f12.2549891&partnerID=40&md5=f646d1cd731b6418a68a23ec8056fd1d","Generative adversarial networks (GANs) have received immense attention in the field of machine learning for their potential to learn high-dimensional and real data distribution. These methods do not rely on any assumptions about the data distribution of the input sample and can generate real-like samples from latent vector space based on unsupervised learning. In the medical field, particularly, in digital pathology expert annotation and availability of a large set of training data is costly and the study of manifestations of various diseases is based on visual examination of stained slides. In clinical practice, various staining information is required to improve the pathological diagnosis process. But when the sampled tissue to be examined is limited, the final diagnosis made by the pathologist is based on limited stain styles. These limitations can be overcome by studying the usability and reliability of generative models in the field of digital pathology. To understand the usability of the generative models, we synthesize in an unsupervised manner, high resolution renal microanatomical structures like renal glomerulus in thin tissue histology images using state-of-art architectures like Deep Convolutional Generative Adversarial Network (DCGAN) and Enhanced Super-Resolution Generative Adversarial Network (ESRGAN). Successful generation of such structures will lead to obtaining a large set of labeled data for further developing supervised algorithms for disease classification and understanding progression. Our study suggests while GAN is able to attain formalin fixed and paraffin embedded tissue image quality, GAN requires further prior knowledge as input to model intrinsic micro-anatomical details, such as capillary wall, urinary pole, nuclei placement, suggesting developing semi-supervised architectures by using these above details as prior information. Also, the generative models can be used to create an artificial effect of staining without physically tampering the histopathological slide. To demonstrate this, we use a CycleGAN network to transform Hematoxylin and eosin (H&E) stain to Periodic acid-Schiff (PAS) stain and Jones methenamine silver (JMS) stain to PAS stain. In this way GAN can be employed to translate different renal pathology stain styles when the relevant staining information is not available in the clinical settings. © 2020 SPIE. All rights reserved.","Diagnosis; E-learning; Generative adversarial networks; Image enhancement; Learning systems; Medical imaging; Network architecture; Pathology; Silver; Tissue; Vector spaces; Data distribution; Digital pathologies; Generative model; Glomerulus; Haematoxylin; Hematoxylin and eosin; Jones methenamine silver; Machine-learning; Periodic acid; Periodic acid-schiff; Unsupervised learning","Digital pathology; Generative adversarial network; Glomeruli; Hematoxylin and eosin; Jones methenamine silver; Machine learning; Periodic acid-Schiff; Unsupervised learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85102518230"
"Das V.; Dandapat S.; Bora P.K.","Das, Vineeta (57190376515); Dandapat, Samarendra (15922221700); Bora, Prabin Kumar (7005489822)","57190376515; 15922221700; 7005489822","Unsupervised Super-Resolution of OCT Images Using Generative Adversarial Network for Improved Age-Related Macular Degeneration Diagnosis","2020","IEEE Sensors Journal","20","15","9055188","8746","8756","10","10.1109/JSEN.2020.2985131","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088649191&doi=10.1109%2fJSEN.2020.2985131&partnerID=40&md5=9bcc123e9d25cbb4abee206269479dc4","Age-related macular degeneration (AMD) is the leading cause of progressive vision loss in the elderly. Optical coherence tomography (OCT) is a promising diagnostic tool for early detection and management of AMD. However, the speckle noise and low resolution (LR) of the OCT images affect its diagnostic viabilities. Therefore, denoising and super-resolution (SR) techniques present a potential solution to improve the quality of the OCT images. Recent methods rely on example-based approaches that require paired LR and high resolution (HR) images for training. However, the large scale acquisition of paired LR-HR images presents pertinent challenges in clinical settings. Therefore, this work proposes an unsupervised framework using the generative adversarial network (GAN) to perform fast and reliable SR without the requirement of aligned LR-HR pairs. We use adversarial learning with cycle consistency and identity mapping priors to preserve the spatial correlation, color and texture details in the generated clean HR images. Experimental results on clinical-grade OCT images show that the proposed method outperforms the existing methods both in terms of SR performance and computational time. Improved classification accuracy of 96.54% is obtained when the generated images are used for automated AMD diagnosis. The enhanced generalizability and faithful reconstruction attributes make the proposed method suitable for assisting ophthalmologists in better diagnosis and treatment planning. © 2001-2012 IEEE.","Computer aided diagnosis; Ophthalmology; Optical resolving power; Optical tomography; Textures; Adversarial learning; Adversarial networks; Age-related macular degeneration; Classification accuracy; Color and textures; High resolution image; Spatial correlations; Treatment planning; Image enhancement","Age-related macular degeneration (AMD); generative adversarial network (GAN); optical coherence tomography (OCT); super-resolution","Article","Final","","Scopus","2-s2.0-85088649191"
"Liu M.; Shi Q.; Liu P.; Wan C.","Liu, Mengxi (57208160778); Shi, Qian (55286447700); Liu, Penghua (57191632651); Wan, Cheng (57222241570)","57208160778; 55286447700; 57191632651; 57222241570","Siamese Generative Adversarial Network for Change Detection under Different Scales","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323499","2543","2546","3","10.1109/IGARSS39084.2020.9323499","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101998421&doi=10.1109%2fIGARSS39084.2020.9323499&partnerID=40&md5=422672dc13ab48da34f0895a22057648","Change detection methods based on low-resolution (LR) images with higher temporal resolution often lead to fuzzy results, while high-resolution images (HRIs) can provide more detailed information to solve this problem. However, it's hard to obtain two tiles of HRIs with high-quality for rapid change detection in actual production due to low temporal resolution and high cost. Therefore, it is necessary to explore a change detection method combing low- and high-resolution images to acquire urban change areas more accurately and quickly. In this paper, an end-to-end siamese generative adversarial network (SiamGAN) integrating a super resolution network and the siamese structure was proposed for change detection under different scales. The super-resolution network is used to reconstruct low-resolution images into high-resolution images, while the siamese structure is adopted as the classification network to detect changes. In the experiments, SiamGAN achieved an F1 of 76.06% and an IoU of 61.52% in the test set, which is respectively 5.68% and 6.92% higher than the CNN-based methods using LR images after bicubic interpolation. The results show that our proposed method can effectively overcome difference in scale between low- and high-resolution images and perform change detection more precisely and rapidly. © 2020 IEEE.","Geology; Optical resolving power; Remote sensing; Adversarial networks; Bicubic interpolation; Change detection; Classification networks; High resolution image; Low resolution images; Super resolution; Temporal resolution; Chemical detection","Change detection; high resolution images; siamese network; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85101998421"
"Hussein S.A.; Tirer T.; Giryes R.","Hussein, Shady Abu (57219622556); Tirer, Tom (57192950135); Giryes, Raja (33767616100)","57219622556; 57192950135; 33767616100","Image-adaptive gan based reconstruction","2020","AAAI 2020 - 34th AAAI Conference on Artificial Intelligence","","","","3121","3129","8","","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094613574&partnerID=40&md5=82c99371b0deadd24666f3eb4b0859cb","In the recent years, there has been a significant improvement in the quality of samples produced by (deep) generative models such as variational auto-encoders and generative adversarial networks. However, the representation capabilities of these methods still do not capture the full distribution for complex classes of images, such as human faces. This deficiency has been clearly observed in previous works that use pre-trained generative models to solve imaging inverse problems. In this paper, we suggest to mitigate the limited representation capabilities of generators by making them image-adaptive and enforcing compliance of the restoration with the observations via back-projections. We empirically demonstrate the advantages of our proposed approach for image super-resolution and compressed sensing.  © 2020, Association for the Advancement of Artificial Intelligence.","Artificial intelligence; Inverse problems; Adversarial networks; Auto encoders; Back projection; Complex class; GaN based; Generative model; Human faces; Image super resolutions; Image reconstruction","","Conference paper","Final","","Scopus","2-s2.0-85094613574"
"Dou X.; Li C.; Shi Q.; Liu M.","Dou, Xinyu (57216671300); Li, Chenyu (57216672432); Shi, Qian (55286447700); Liu, Mengxi (57208160778)","57216671300; 57216672432; 55286447700; 57208160778","Super-resolution for hyperspectral remote sensing images based on the 3D Attention-SRGAN Network","2020","Remote Sensing","12","7","1204","","","","10.3390/rs12071204","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084251625&doi=10.3390%2frs12071204&partnerID=40&md5=371b3af26f6fab6f3ac50bce7b220ac9","Hyperspectral remote sensing images (HSIs) have a higher spectral resolution compared to multispectral remote sensing images, providing the possibility for more reasonable and effective analysis and processing of spectral data. However, rich spectral information usually comes at the expense of low spatial resolution owing to the physical limitations of sensors, which brings difficulties for identifying and analyzing targets in HSIs. In the super-resolution (SR) field, many methods have been focusing on the restoration of the spatial information while ignoring the spectral aspect. To better restore the spectral information in the HSI SR field, a novel super-resolution (SR) method was proposed in this study. Firstly, we innovatively used three-dimensional (3D) convolution based on SRGAN (Super-Resolution Generative Adversarial Network) structure to not only exploit the spatial features but also preserve spectral properties in the process of SR. Moreover, we used the attention mechanism to deal with the multiply features from the 3D convolution layers, and we enhanced the output of our model by improving the content of the generator's loss function. The experimental results indicate that the 3DASRGAN (3D Attention-based Super-Resolution Generative Adversarial Network) is both visually quantitatively better than the comparison methods, which proves that the 3DASRGAN model can reconstruct high-resolution HSIs with high efficiency. © 2020, by the authors.","Convolution; Data handling; Optical resolving power; Restoration; Three dimensional computer graphics; Adversarial networks; Attention mechanisms; Hyperspectral Remote Sensing Image; Multispectral remote sensing image; Physical limitations; Spatial informations; Spectral information; Three-dimensional (3D) convolution; Remote sensing","3D convolution; Generative adversarial networks; Hyperspectral image; Spectral angle; Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084251625"
"Jaiswal D.P.; Kumar S.; Badr Y.","Jaiswal, Devendra Prakash (57219488412); Kumar, Srishti (57220592234); Badr, Youakim (23481220900)","57219488412; 57220592234; 23481220900","Towards an artificial intelligence aided design approach: Application to anime faces with generative adversarial networks","2020","Procedia Computer Science","168","","","57","64","7","10.1016/j.procs.2020.02.257","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093118026&doi=10.1016%2fj.procs.2020.02.257&partnerID=40&md5=c1fb90388d7fd83fdb32825cd8ee2df4","Ever since the inception of Machine Learning and Artificial Intelligence, the basic motto for most of research works has been to bring the machines at par with human intelligence. Designing new products and artifacts is one of the many fields where it is very difficult to enable computing machines replicate human creativity and innovativeness. Design processes in engineering fields as well as in arts follow methodical series of steps to create new products. Due to high demands of customized products and services, competitors tend to shorten the time-to-market periods, using advanced Computer-Aided Design programs. These programs play important roles to assist designers in digitizing blueprints and automating repetitive tasks. However, they fail to boost designer creativity by generating or suggesting new ideas or designs based on existing products or their variants. In order to boost creativity in the entertainment industry, we propose in this paper a new approach based on unsupervised learning techniques to create variants of a given artifact or product blueprints. Within the field of designing new cartoon characters, our proposed approach relies on Generative Adversarial Neural Networks [1] to create new anime or cartoon faces on their own without any human intervention. It learns features and characteristics from an image training dataset and combines them to create new features and thus builds a new image which is not present in the training dataset. This applied approach attempts to not only help artists and designers to have a preview of the possible new and unique avatars but also would prevent any copyright infringements.  © 2020 The Authors.","Adaptive systems; Blueprints; Complex networks; Computer aided design; Entertainment industry; Learning systems; Product design; Adversarial networks; Cartoon characters; Computing machines; Copyright infringement; Customized products; Engineering fields; Human intelligence; Human intervention; Artificial intelligence","Artificial Neural Networks; Engineering Design; Generative Adversarial Networks; Image Generation; Super-Resolution","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85093118026"
"Ferdous S.N.; Dabouei A.; Dawson J.; Nasrabadi N.M.","Ferdous, Syeda Nyma (57211070813); Dabouei, Ali (57203221299); Dawson, Jeremy (57213992881); Nasrabadi, Nasser M. (7006312852)","57211070813; 57203221299; 57213992881; 7006312852","Super-resolution guided pore detection for fingerprint recognition","2020","Proceedings - International Conference on Pattern Recognition","","","9413043","2025","2032","7","10.1109/ICPR48806.2021.9413043","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110475260&doi=10.1109%2fICPR48806.2021.9413043&partnerID=40&md5=2cac13709db559a7c43e77c44cca47fe","Performance of fingerprint recognition algorithms substantially rely on fine features extracted from fingerprints. Apart from minutiae and ridge patterns, pore features have proven to be usable for fingerprint recognition. Although features from minutiae and ridge patterns are quite attainable from low-resolution images, using pore features is practical only if the fingerprint image is of high resolution which necessitates a model that enhances the image quality of the conventional 500 ppi legacy fingerprints preserving the fine details. To find a solution for recovering pore information from low-resolution fingerprints, we adopt a joint learning-based approach that combines both super-resolution and pore detection networks. Our modified single image Super-Resolution Generative Adversarial Network (SRGAN) framework helps to reliably reconstruct high-resolution fingerprint samples from low-resolution ones assisting the pore detection network to identify pores with a high accuracy. The network jointly learns a distinctive feature representation from a real low-resolution fingerprint sample and successfully synthesizes a high-resolution sample from it. To add discriminative information and uniqueness for all the subjects, we have integrated features extracted from a deep fingerprint verifier with the SRGAN quality discriminator. We also add ridge reconstruction loss, utilizing ridge patterns to make the best use of extracted features. Our proposed method solves the recognition problem by improving the quality of fingerprint images. High recognition accuracy of the synthesized samples that is close to the accuracy achieved using the original high-resolution images validate the effectiveness of our proposed model. © 2020 IEEE","Image enhancement; Optical resolving power; Adversarial networks; Feature representation; Fingerprint Recognition; Fingerprint recognition algorithm; High resolution image; Low resolution images; Recognition accuracy; Ridge reconstruction; Palmprint recognition","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85110475260"
"","","","1st Automatization of Cranial Implant Design in Cranioplasty Challenge, AutoImplant 2020","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12439 LNCS","","","","","114","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097809963&partnerID=40&md5=9a4bf02a64f1736431a218cc1b3a2e2f","The proceedings contain 12 papers. The special focus in this conference is on Automatization of Cranial Implant Design in Cranioplasty Challenge. The topics include: High-Resolution Cranial Implant Prediction via Patch-Wise Training; learning Volumetric Shape Super-Resolution for Cranial Implant Design; dataset Descriptor for the AutoImplant Cranial Implant Design Challenge; automated Virtual Reconstruction of Large Skull Defects using Statistical Shape Models and Generative Adversarial Networks; cranial Implant Design Through Multiaxial Slice Inpainting Using Deep Learning; cranial Implant Design via Virtual Craniectomy with Shape Priors; deep Learning Using Augmentation via Registration: 1st Place Solution to the AutoImplant 2020 Challenge; Cranial Defect Reconstruction Using Cascaded CNN with Alignment; Shape Completion by U-Net: An Approach to the AutoImplant MICCAI Cranial Implant Design Challenge; cranial Implant Prediction Using Low-Resolution 3D Shape Completion and High-Resolution 2D Refinement.","","","Conference review","Final","","Scopus","2-s2.0-85097809963"
"Chun D.; Kim T.S.; Lee K.; Lee H.-J.","Chun, Dayoung (57210795044); Kim, Tae Sung (36072158600); Lee, Kyujoong (50661645000); Lee, Hyuk-Jae (8161276300)","57210795044; 36072158600; 50661645000; 8161276300","Compressed Video restoration using a generative adversarial network for subjective quality enhancement","2020","IEIE Transactions on Smart Processing and Computing","9","1","","1","16","15","10.5573/IEIESPC.2020.9.1.001","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087973457&doi=10.5573%2fIEIESPC.2020.9.1.001&partnerID=40&md5=4a1d0d0ed7fd409bad8904ce337b0d39","High Efficiency Video Coding (HEVC) is a widely used video compression standard that minimizes the sacrifice in visual quality. Convolutional neural networks (CNNs) are being used as a post-processing tool for video restoration degraded by compression. Improving on CNN-based video restoration, this paper attempts a new generative adversarial network (GAN)-based video restoration called a compressed video restoration generative adversarial network (CVRGAN). Although a GAN is widely used for perceptual image enhancement in super-resolution and noise reduction, it is not yet used for compressed video restoration. The proposed CVRGAN is the first attempt to utilize a GAN to create the texture of a degraded image, and consequently, to generate detailed textures that were lost due to compression. In order to avoid the side effect of a GAN boosting the blocking and ringing artifacts incurred by compression, the CVRGAN employs a new content loss that is a combination of VGG feature difference, which represents a perceptual loss, and an objective loss, such as mean squared error (MSE) or mean absolute error (MAE). The new loss function is effective in the enhancement of subjective image quality while suppressing artifact boosting. An extensive mean opinion score (MOS) test shows that the CVRGAN achieves an improvement in perceptual quality over previous CNN-based video restoratio. © 2020 Institute of Electronics and Information Engineers. All rights reserved.","Convolutional neural networks; Image compression; Image enhancement; Mean square error; Noise abatement; Restoration; Textures; Video signal processing; Adversarial networks; Feature differences; High-efficiency video coding; Mean absolute error; Mean opinion scores; Mean squared error; Subjective image quality; Video compression standards; Image reconstruction","CNN; Compressed video restoration; Deep neural network; GAN; High efficiency video coding","Article","Final","","Scopus","2-s2.0-85087973457"
"Zhu Y.; Zhou Z.; Liao G.; Yuan K.","Zhu, Yongpei (57209279791); Zhou, Zicong (57209286486); Liao, Guojun (7102948461); Yuan, Kehong (10240004000)","57209279791; 57209286486; 7102948461; 10240004000","Csrgan: Medical Image Super-Resolution Using A Generative Adversarial Network","2020","ISBI Workshops 2020 - International Symposium on Biomedical Imaging Workshops, Proceedings","","","9153436","","","","10.1109/ISBIWorkshops50223.2020.9153436","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090410174&doi=10.1109%2fISBIWorkshops50223.2020.9153436&partnerID=40&md5=ec09330e6623676d9d5a618bbd7b1631","Super-resolution medical image is vital for doctor's diagnosis and quantitative analysis. In this work we propose a novel super-resolution generative adversarial network which combine conditional GAN (CGAN) and SRGAN, refer to it as CSRGAN to generate super-resolution (SR) images. We use differential geometric information including Jacobian determinant (JD) and curl vector (CV) as conditional inputs of both the discriminator and generator of SRGAN, which make full use of the idea of using GANs to learn a mapping from one manifold to another. In addition, we proposed a content loss motivated by CV feature information instead of VGG loss in SRGAN. We trained our model on a large-scale dataset CelebFaces Attributes, tested it on medical ultrasound image dataset. The experimental results show the method can achieve better performance in SR image generation with higher average peak signal-tonoise ratio (PSNR), Structural Similarity (SSIM) and Mean Opinion Score (MOS) compared with SRGAN. © 2020 IEEE.","Diagnosis; Image resolution; Large dataset; Optical resolving power; Adversarial networks; Geometric information; Image super resolutions; Jacobian determinants; Mean opinion scores; Medical ultrasound images; Signaltonoise ratio (SNR); Structural similarity; Medical imaging","CSRGAN; Differential geometric information; Manifold; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85090410174"
"Kemas M.A.N.; P. A.S.H.; Widhiyasana Y.; Syakrani N.","Kemas, M. Alfin N. (57221684270); P., Ariq Suryo Hadi (57221682989); Widhiyasana, Yudi (57221682609); Syakrani, Nurjannah (53265170900)","57221684270; 57221682989; 57221682609; 53265170900","Facial Image Super Resolution on 3 Architectures of Generative Adversarial Network","2020","7th International Conference on ICT for Smart Society: AIoT for Smart Society, ICISS 2020 - Proceeding","","","9307573","","","","10.1109/ICISS50791.2020.9307573","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099784916&doi=10.1109%2fICISS50791.2020.9307573&partnerID=40&md5=9748096fae7825ec8e9cc8128f992c17","Facial images are widely used for biometric recognition such as facial recognition, analysis or reconstruction. Consequently, it is necessary to increase the resolution of facial images from low resolution to high resolution or super resolution. Generative Adversarial Network (GAN) is a machine-learning-based method that can be used to increase image resolution. GAN is composed of two CNN, namely Generator and Discriminator. This research focuses on examining the effect of Generator configuration, specifically the number of feature extractor blocks named basic blocks and filters on Generator, which are built based on three different architectures, which are Inception, Resnet, and Inception-Resnet. Our finding shows that Resnet with 128 number of filter and 32 number of basic blocks gives the best result. © 2020 IEEE.","Image resolution; Network architecture; Optical resolving power; Turing machines; Adversarial networks; Biometric recognition; Facial recognition; Feature extractor; High resolution; Low resolution; Research focus; Super resolution; Face recognition","Generative Adversarial Network; Inception; Inception-Resnet; Number of Basic Blocks; Number of Filters; Quality of facial image enhancement; Resnet","Conference paper","Final","","Scopus","2-s2.0-85099784916"
"Li J.; Zhou Y.; Ding J.; Chen C.; Yang X.","Li, Jinning (57203539613); Zhou, Yichen (57219390187); Ding, Jie (57191904975); Chen, Cen (57191481439); Yang, Xulei (7406500681)","57203539613; 57219390187; 57191904975; 57191481439; 7406500681","ID preserving face super-resolution generative adversarial networks","2020","IEEE Access","8","","9146819","138373","138381","8","10.1109/ACCESS.2020.3011699","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092529095&doi=10.1109%2fACCESS.2020.3011699&partnerID=40&md5=f6067cc50412b4a4e71d3a49d16cb4a3","We propose an ID Preserving Face Super-Resolution Generative Adversarial Networks (IP-FSRGAN) to reconstruct realistic super-resolution face images from low-resolution ones. Inspired by the success of generative adversarial networks (GAN), we introduce a novel ID preserving module to help the generator learn to infer the facial details and synthesize more realistic super-resolution faces. Our method produces satisfactory visual results and also quantitatively outperforms state-of-the-art super-resolution methods on the face datasets including CASIA-Webface, CelebA, and LFW datasets under the metrics of PSNR, SSIM, and cosine similarity. In addition, we propose a framework to apply IP-FSRGAN model to address the face verification task on low-resolution face images. The synthesized 4 × super-resolution faces achieve a verification accuracy of 97.6%, improved from 92.8% of low resolution faces. We also prove by experiments that the proposed IP-FSRGAN model demonstrates excellent robustness under different downsample scaling factors and extensibility to various face verification models.  © 2013 IEEE.","Internet protocols; Optical resolving power; Adversarial networks; Cosine similarity; Face super-resolution; Face Verification; Low-resolution face images; State of the art; Super resolution; Superresolution methods; Face recognition","face super-resolution; face verification; generative adversarial networks; ID preserving","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092529095"
"Li G.; Li J.; Fan H.","Li, Guihui (57201908517); Li, Jinjiang (25638923500); Fan, Hui (36647626600)","57201908517; 25638923500; 36647626600","Edge-guided multispectral image fusion algorithm","2020","Journal of Applied Remote Sensing","14","4","046515","","","","10.1117/1.JRS.14.046515","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098642871&doi=10.1117%2f1.JRS.14.046515&partnerID=40&md5=1cfd168ef028c7b969d79143cb468b45","Most existing multispectral fusion algorithms often suffer from spectral or spatial information distortion. Driven by this motivation, we propose an edge-guided multispectral (MS) image fusion algorithm. In particular, it combines the advantages of generative adversarial networks and improved fusion frameworks, so the merged image can better preserve the spectral information of the original multispectral image while injecting spatial detail information. Specifically, first, an MS image with more image detail is generated using the generated confrontation network for preliminary reconstruction. The panchromatic image edge information and the antagonistic learning strategy are introduced for the robust multispectral image reconstruction. Then, using the reconstructed MS image and the general component substitution image fusion framework, the whole fusion system of this paper is constructed. An enhancement operator is introduced to inject spatial details. Our extensive dataset evaluations show that our approach performs better in terms of high objective quality and human visual perception than several of the most advanced fusion methods.  © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Edge detection; Image enhancement; Image reconstruction; Adversarial networks; Component substitution; Human visual perception; Multi-spectral image fusions; Multispectral fusion; Multispectral images; Spatial informations; Spectral information; Image fusion","edge enhancement; generative adversarial network; multispectral image fusion; super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85098642871"
"Jiang Y.; Li J.","Jiang, Yuning (36836028000); Li, Jinhua (57196156927)","36836028000; 57196156927","Generative adversarial network for image super-resolution combining texture loss","2020","Applied Sciences (Switzerland)","10","5","1729","","","","10.3390/app10051729","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081950513&doi=10.3390%2fapp10051729&partnerID=40&md5=25ba0903e283fce8f6a9181228190549","Objective: Super-resolution reconstruction is an increasingly important area in computer vision. To alleviate the problems that super-resolution reconstruction models based on generative adversarial networks are difficult to train and contain artifacts in reconstruction results, we propose a novel and improved algorithm. Methods: This paper presented TSRGAN (Super-Resolution Generative Adversarial Networks Combining Texture Loss) model which was also based on generative adversarial networks. We redefined the generator network and discriminator network. Firstly, on the network structure, residual dense blocks without excess batch normalization layers were used to form generator network. Visual Geometry Group (VGG)19 network was adopted as the basic framework of discriminator network. Secondly, in the loss function, the weighting of the four loss functions of texture loss, perceptual loss, adversarial loss and content loss was used as the objective function of generator. Texture loss was proposed to encourage local information matching. Perceptual loss was enhanced by employing the features before activation layer to calculate. Adversarial loss was optimized based on WGAN-GP (Wasserstein GAN with Gradient Penalty) theory. Content loss was used to ensure the accuracy of low-frequency information. During the optimization process, the target image information was reconstructed from different angles of high and low frequencies. Results: The experimental results showed that our method made the average Peak Signal to Noise Ratio of reconstructed images reach 27.99 dB and the average Structural Similarity Index reach 0.778 without losing too much speed, which was superior to other comparison algorithms in objective evaluation index. What is more, TSRGAN significantly improved subjective visual evaluations such as brightness information and texture details. We found that it could generate images with more realistic textures and more accurate brightness, which were more in line with human visual evaluation. Conclusions: Our improvements to the network structure could reduce the model's calculation amount and stabilize the training direction. In addition, the loss function we present for generator could provide stronger supervision for restoring realistic textures and achieving brightness consistency. Experimental results prove the effectiveness and superiority of TSRGAN algorithm. © 2020 by the authors.","","Dense convolutional networks; Generative adversarial networks; Super-resolution reconstruction; Texture loss; WGAN-GP","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081950513"
"Fan M.; Liu Z.; Xu M.; Wang S.; Zeng T.; Gao X.; Li L.","Fan, Ming (56683907900); Liu, Zuhui (57213431478); Xu, Maosheng (57208615272); Wang, Shiwei (57207088646); Zeng, Tieyong (25423412800); Gao, Xin (55712115900); Li, Lihua (7501447027)","56683907900; 57213431478; 57208615272; 57207088646; 25423412800; 55712115900; 7501447027","Generative adversarial network-based super-resolution of diffusion-weighted imaging: Application to tumour radiomics in breast cancer","2020","NMR in Biomedicine","33","8","e4345","","","","10.1002/nbm.4345","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086170157&doi=10.1002%2fnbm.4345&partnerID=40&md5=0c71c734a06c381292a5e3fd96de933d","Diffusion-weighted imaging (DWI) is increasingly used to guide the clinical management of patients with breast tumours. However, accurate tumour characterization with DWI and the corresponding apparent diffusion coefficient (ADC) maps are challenging due to their limited resolution. This study aimed to produce super-resolution (SR) ADC images and to assess the clinical utility of these SR images by performing a radiomic analysis for predicting the histologic grade and Ki-67 expression status of breast cancer. To this end, 322 samples of dynamic enhanced magnetic resonance imaging (DCE-MRI) and the corresponding DWI data were collected. A SR generative adversarial (SRGAN) and an enhanced deep SR (EDSR) network along with the bicubic interpolation were utilized to generate SR-ADC images from which radiomic features were extracted. The dataset was randomly separated into a development dataset (n = 222) to establish a deep SR model using DCE-MRI and a validation dataset (n = 100) to improve the resolution of ADC images. This random separation of datasets was performed 10 times, and the results were averaged. The EDSR method was significantly better than the SRGAN and bicubic methods in terms of objective quality criteria. Univariate and multivariate predictive models of radiomic features were established to determine the area under the receiver operating characteristic curve (AUC). Individual features from the tumour SR-ADC images showed a higher performance with the EDSR and SRGAN methods than with the bicubic method and the original images. Multivariate analysis of the collective radiomics showed that the EDSR- and SRGAN-based SR-ADC images performed better than the bicubic method and original images in predicting either Ki-67 expression levels (AUCs of 0.818 and 0.801, respectively) or the tumour grade (AUCs of 0.826 and 0.828, respectively). This work demonstrates that in addition to improving the resolution of ADC images, deep SR networks can also improve tumour image-based diagnosis in breast cancer. © 2020 John Wiley & Sons, Ltd.","Adult; Aged; Aged, 80 and over; Biomarkers, Tumor; Breast Neoplasms; Deep Learning; Diffusion Magnetic Resonance Imaging; Female; Humans; Image Processing, Computer-Assisted; Ki-67 Antigen; Middle Aged; Neoplasm Staging; Reproducibility of Results; ROC Curve; Computerized tomography; Diagnosis; Diseases; Image analysis; Magnetic resonance imaging; Medical imaging; Multivariant analysis; Optical resolving power; Patient monitoring; Surface diffusion; Tumors; gadobutrol; Ki 67 antigen; Ki 67 antigen; tumor marker; Adversarial networks; Apparent diffusion coefficient; Bicubic interpolation; Clinical management; Diffusion weighted imaging; Image-based diagnosis; Multi variate analysis; Receiver operating characteristic curves; adult; aged; antigen expression; apparent diffusion coefficient; Article; breast cancer; cancer diagnosis; deep learning; diffusion weighted imaging; female; human; human tissue; image analysis; image processing; major clinical study; priority journal; radiomics; breast tumor; cancer staging; chemistry; diagnostic imaging; diffusion weighted imaging; middle aged; pathology; procedures; receiver operating characteristic; reproducibility; very elderly; Image enhancement","apparent diffusion coefficient; breast cancer; enhanced deep super-resolution; histological grade; Ki-67; super-resolution generative adversarial network","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086170157"
"Rad M.S.; Bozorgtabar B.; Musat C.; Marti U.-V.; Basler M.; Ekenel H.K.; Thiran J.-P.","Rad, Mohammad Saeed (57196089451); Bozorgtabar, Behzad (37109488100); Musat, Claudiu (36615733800); Marti, Urs-Viktor (57225422555); Basler, Max (57212377637); Ekenel, Hazım Kemal (55958877400); Thiran, Jean-Philippe (35554798200)","57196089451; 37109488100; 36615733800; 57225422555; 57212377637; 55958877400; 35554798200","Benefiting from multitask learning to improve single image super-resolution","2020","Neurocomputing","398","","","304","313","9","10.1016/j.neucom.2019.07.107","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076532983&doi=10.1016%2fj.neucom.2019.07.107&partnerID=40&md5=2b9169dcc680bb779005454bbd785b37","Despite significant progress toward super resolving more realistic images by deeper convolutional neural networks (CNNs), reconstructing fine and natural textures still remains a challenging problem. Recent works on single image super resolution (SISR) are mostly based on optimizing pixel and content wise similarity between recovered and high-resolution (HR) images and do not benefit from recognizability of semantic classes. In this paper, we introduce a novel approach using categorical information to tackle the SISR problem; we present an encoder architecture able to extract and use semantic information to super-resolve a given image by using multitask learning, simultaneously for image super-resolution and semantic segmentation. To explore categorical information during training, the proposed decoder only employs one shared deep network for two task-specific output layers. At run-time only layers resulting HR image are used and no segmentation label is required. Extensive perceptual experiments and a user study on images randomly selected from COCO-Stuff dataset demonstrate the effectiveness of our proposed method and it outperforms the state-of-the-art methods. © 2019 Elsevier B.V.","Image segmentation; Image texture; Information use; Learning systems; Neural networks; Optical resolving power; Semantics; Textures; Adversarial networks; Convolutional neural network; High resolution image; Image super resolutions; Multitask learning; Semantic segmentation; Single images; State-of-the-art methods; article; controlled study; learning; randomized controlled trial; Image enhancement","Generative adversarial network; Multitask learning; Recovering realistic textures; Semantic segmentation; Single image super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076532983"
"Fu K.; Peng J.; Zhang H.; Wang X.; Jiang F.","Fu, Kui (57218768440); Peng, Jiansheng (54781664200); Zhang, Hanxiao (57216951117); Wang, Xiaoliang (35075334400); Jiang, Frank (51061074400)","57218768440; 54781664200; 57216951117; 35075334400; 51061074400","Image super-resolution based on generative adversarial networks: A brief review","2020","Computers, Materials and Continua","64","3","","1977","1997","20","10.32604/cmc.2020.09882","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090834279&doi=10.32604%2fcmc.2020.09882&partnerID=40&md5=b64c9cbe3780d4372859f1eca4df304f","Single image super resolution (SISR) is an important research content in the field of computer vision and image processing. With the rapid development of deep neural networks, different image super-resolution models have emerged. Compared to some traditional SISR methods, deep learning-based methods can complete the super-resolution tasks through a single image. In addition, compared with the SISR methods using traditional convolutional neural networks, SISR based on generative adversarial networks (GAN) has achieved the most advanced visual performance. In this review, we first explore the challenges faced by SISR and introduce some common datasets and evaluation metrics. Then, we review the improved network structures and loss functions of GAN-based perceptual SISR. Subsequently, the advantages and disadvantages of different networks are analyzed by multiple comparative experiments. Finally, we summarize the paper and look forward to the future development trends of GAN-based perceptual SISR. © 2020 Tech Science Press. All rights reserved.","Deep learning; Deep neural networks; Image processing; Optical resolving power; Adversarial networks; Comparative experiments; Development trends; Evaluation metrics; Image super resolutions; Learning-based methods; Network structures; Visual performance; Convolutional neural networks","Computer vision; Deep learning; Generative adversarial networks; Single image super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85090834279"
"Zhang C.; Wang J.; Yen G.G.; Zhao C.; Sun Q.; Tang Y.; Qian F.; Kurths J.","Zhang, Chongzhen (57217133261); Wang, Jianrui (57219634877); Yen, Gary G. (7102240014); Zhao, Chaoqiang (57212172735); Sun, Qiyu (57208803697); Tang, Yang (57200288115); Qian, Feng (53663472900); Kurths, Jürgen (57216068531)","57217133261; 57219634877; 7102240014; 57212172735; 57208803697; 57200288115; 53663472900; 57216068531","When Autonomous Systems Meet Accuracy and Transferability through AI: A Survey","2020","Patterns","1","4","100050","","","","10.1016/j.patter.2020.100050","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099327654&doi=10.1016%2fj.patter.2020.100050&partnerID=40&md5=655ff1300148cb2cd47100adc67ea56b","With widespread applications of artificial intelligence (AI), the capabilities of the perception, understanding, decision-making, and control for autonomous systems have improved significantly in recent years. When autonomous systems consider the performance of accuracy and transferability, several AI methods, such as adversarial learning, reinforcement learning (RL), and meta-learning, show their powerful performance. Here, we review the learning-based approaches in autonomous systems from the perspectives of accuracy and transferability. Accuracy means that a well-trained model shows good results during the testing phase, in which the testing set shares a same task or a data distribution with the training set. Transferability means that when a well-trained model is transferred to other testing domains, the accuracy is still good. Firstly, we introduce some basic concepts of transfer learning and then present some preliminaries of adversarial learning, RL, and meta-learning. Secondly, we focus on reviewing the accuracy or transferability or both of these approaches to show the advantages of adversarial learning, such as generative adversarial networks, in typical computer vision tasks in autonomous systems, including image style transfer, image super-resolution, image deblurring/dehazing/rain removal, semantic segmentation, depth estimation, pedestrian detection, and person re-identification. We furthermore review the performance of RL and meta-learning from the aspects of accuracy or transferability or both of them in autonomous systems, involving pedestrian tracking, robot navigation, and robotic manipulation. Finally, we discuss several challenges and future topics for the use of adversarial learning, RL, and meta-learning in autonomous systems. Accuracy and transferability are critical to the perception and decision-making tasks of autonomous systems. The focus of several learning-based perception and decision-making methods has gradually evolved from accuracy to transferability. This survey summarizes the perception and decision-making tasks of autonomous systems from the perspectives of accuracy and transferability. We introduce transfer learning and some preliminaries of adversarial learning, reinforcement learning, and meta-learning. Then, we review several perception and decision tasks of autonomous systems from the perspectives of accuracy or transferability or both. Last but not least, we discuss several challenges and future works for using adversarial learning, reinforcement learning, and meta-learning in autonomous systems. Accuracy and transferability are critical to the perception and decision-making tasks of autonomous systems. This paper reviews the perception and decision-making tasks of autonomous systems from the perspectives of accuracy and transferability. This survey summarizes some learning-based methods and discusses several challenges and future topics for complex multi-task, domain adaptation and model transferability in autonomous systems. © 2020 The Authors","Decision making; Decision support systems; Image enhancement; Image segmentation; Object recognition; Reinforcement learning; Robots; Semantics; Surveys; Transfer learning; Visual servoing; Well testing; Adversarial learning; Decision-making method; Image super resolutions; Learning-based approach; Learning-based methods; Model transferabilities; Person re identifications; Semantic segmentation; Learning systems","artificial intelligence; autonomous systems; deep learning; generative adversarial networks; meta-learning; reinforcement learning; transferability","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85099327654"
"Hazra D.; Byun Y.-C.","Hazra, Debapriya (57191375984); Byun, Yung-Cheol (8897891700)","57191375984; 8897891700","Upsampling real-time, low-resolution cctv videos using generative adversarial networks","2020","Electronics (Switzerland)","9","8","1312","1","20","19","10.3390/electronics9081312","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090254666&doi=10.3390%2felectronics9081312&partnerID=40&md5=b5220e9230605d2f57eb4a0f34b2ead1","Video super-resolution has become an emerging topic in the field of machine learning. The generative adversarial network is a framework that is widely used to develop solutions for low-resolution videos. Video surveillance using closed-circuit television (CCTV) is significant in every field, all over the world. A common problem with CCTV videos is sudden video loss or poor quality. In this paper, we propose a generative adversarial network that implements spatio-temporal generators and discriminators to enhance real-time low-resolution CCTV videos to high-resolution. The proposed model considers both foreground and background motion of a CCTV video and effectively models the spatial and temporal consistency from low-resolution video frames to generate high-resolution videos. Quantitative and qualitative experiments on benchmark datasets, including Kinetics-700, UCF101, HMDB51 and IITH_Helmet2, showed that our model outperforms the existing GAN models for video super-resolution. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","","Closed-circuit television (CCTV); Generative adversarial networks (GAN); High-resolution videos; Low-resolution videos; Spatio-temporal network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85090254666"
"Feng X.; Li Z.; Wu S.","Feng, Xiaodong (57218626238); Li, Zongshu (57222006719); Wu, Shaobo (57222048286)","57218626238; 57222006719; 57222048286","Super-resolution Reconstruction of Infrared Images of Internal Defective Metal Plates Based on Generative Adversarial Networks","2020","2020 IEEE 5th International Conference on Signal and Image Processing, ICSIP 2020","","","9339408","292","297","5","10.1109/ICSIP49896.2020.9339408","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101114445&doi=10.1109%2fICSIP49896.2020.9339408&partnerID=40&md5=7e9c82da456e8e0df157ee86e34f438c","Infrared thermal imaging non-destructive testing technology has made great progress and has been used in the field of defect detection. However, due to the internal noise of the infrared imaging equipment and the influence of the surrounding environment interference, the infrared images used for defect recognition have the disadvantages of low contrast, low resolution, and low signal-to-noise ratio. Our article first briefly introduces the basic principle of infrared thermal imaging detection technology and the development status at home and abroad, then we builds an infrared image acquisition system that uses long-pulsed thermal method to collect internal defects of a metal plate. In order to solve the problem of noises, our paper first adopts Gaussian blur and homomorphic filtering on the acquired infrared images. Then we use Laplacian operator to process the filtered images and obtained second-order differential images. Finally, we use GAN for super-resolution reconstruction of the filtered second-order differential images. The results show that the super-resolution reconstructed images have higher PSNR and SSIM. What's more, it retains detailed information about defects in the original infrared images.  © 2020 IEEE.","Defects; Image acquisition; Mathematical operators; Nondestructive examination; Optical resolving power; Plate metal; Signal to noise ratio; Thermography (imaging); Adversarial networks; Homomorphic filtering; Infrared thermal imaging; Low signal-to-noise ratio; Non destructive testing; Reconstructed image; Super resolution reconstruction; Surrounding environment; Image reconstruction","GAN; homomorphic filtering; infrared non-destructive testing; laplacian operator; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85101114445"
"Bucci A.; Topini A.; Franchi M.; Zacchini L.; Secciani N.; Ridolfi A.","Bucci, Alessandro (57215203770); Topini, Alberto (57220959519); Franchi, Matteo (57195063794); Zacchini, Leonardo (57209645356); Secciani, Nicola (57204239646); Ridolfi, Alessandro (55179249500)","57215203770; 57220959519; 57195063794; 57209645356; 57204239646; 55179249500","Underwater Acoustic Image Enhancement by Using Fast Super-Resolution with Generative Adversarial Networks","2020","2020 Global Oceans 2020: Singapore - U.S. Gulf Coast","","","9389055","","","","10.1109/IEEECONF38699.2020.9389055","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104598903&doi=10.1109%2fIEEECONF38699.2020.9389055&partnerID=40&md5=75aba342bce8243ed4d121667bf8a66d","Acoustic sensors play a fundamental role in underwater applications. They are used to perform a wide variety of tasks: from the perception of the surrounding environment to the support of inertial sensors in navigation strategies. The quality of the acquired images deeply affects the obtained results and, consequently, image enhancement approaches need to be developed and tested. Super-Resolution (SR) techniques are employed to reconstruct one high-resolution image by composing a sequence of low-resolution ones. By applying these strategies, the information content of an image can be considerably increased, but the required computational time is incompatible for real-time employment. Due to this limitation, an SR Generative Adversarial Network (SRGAN) approach has been developed in the presented work, where the SR images are used during the training phase of the GAN framework. The proposed approach, which has been developed for images provided by a Forward-Looking Sonar (FLS), can guarantee a solid trade-off between the quality of the generated high-resolution image and the run-time execution. © 2020 IEEE.","Economic and social effects; Optical resolving power; Underwater acoustic communication; Adversarial networks; Forward looking sonars; High resolution image; Information contents; Navigation strategies; Run-time execution; Surrounding environment; Underwater application; Image enhancement","Generative Adversarial Network; Marine Robotics; Signal Processing; Sonar Imaging; Super Resolution; Underwater Imaging","Conference paper","Final","","Scopus","2-s2.0-85104598903"
"Li L.; Fan Z.; Zhao M.; Wang X.; Wang Z.; Wang Z.; Guo L.","Li, Li (57040445600); Fan, Zijia (57216897787); Zhao, Mingyang (57216897430); Wang, Xinlei (57204980960); Wang, Zhongyang (57215071558); Wang, Zhiqiong (25522764100); Guo, Longxiang (27170453200)","57040445600; 57216897787; 57216897430; 57204980960; 57215071558; 25522764100; 27170453200","Super-Resolution Reconstruction of Underwater Image Based on Image Sequence Generative Adversarial Network","2020","Mathematical Problems in Engineering","2020","","8472875","","","","10.1155/2020/8472875","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085205099&doi=10.1155%2f2020%2f8472875&partnerID=40&md5=c63f4a41471c2318f02075422e8553f5","Since the underwater image is not clear and difficult to recognize, it is necessary to obtain a clear image with the super-resolution (SR) method to further study underwater images. The obtained images with conventional underwater image super-resolution methods lack detailed information, which results in errors in subsequent recognition and other processes. Therefore, we propose an image sequence generative adversarial network (ISGAN) method for super-resolution based on underwater image sequences collected by multifocus from the same angle, which can obtain more details and improve the resolution of the image. At the same time, a dual generator method is used in order to optimize the network architecture and improve the stability of the generator. The preprocessed images are, respectively, passed through the dual generator, one of which is used as the main generator to generate the SR image of sequence images, and the other is used as the auxiliary generator to prevent the training from crashing or generating redundant details. Experimental results show that the proposed method can be improved on both peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) compared to the traditional GAN method in underwater image SR. © 2020 Li Li et al.","Image reconstruction; Image resolution; Network architecture; Optical resolving power; Signal to noise ratio; Underwater acoustics; Adversarial networks; Image sequence; Image super resolutions; Peak signal to noise ratio; Sequence images; Structural similarity; Super resolution; Super resolution reconstruction; Image enhancement","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85085205099"
"Dobrovolny M.; Mls K.; Krejcar O.; Mambou S.; Selamat A.","Dobrovolny, Michal (57216899296); Mls, Karel (55696848000); Krejcar, Ondrej (14719632500); Mambou, Sebastien (57201070480); Selamat, Ali (24468984100)","57216899296; 55696848000; 14719632500; 57201070480; 24468984100","Medical Image Data Upscaling with Generative Adversarial Networks","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12108 LNBI","","","739","749","10","10.1007/978-3-030-45385-5_66","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085201299&doi=10.1007%2f978-3-030-45385-5_66&partnerID=40&md5=df08fd705817a43c59988e754e27a4d6","Super-resolution is one of the frequently investigated methods of image processing. The quality of the results is a constant problem in the methods used to obtain high resolution images. Interpolation-based methods have blurry output problems, while non-interpolation methods require a lot of training data and high computing power. In this paper, we present a supervised generative adversarial network system that accurately generates high resolution images from a low resolution input while maintaining pathological invariance. The proposed solution is optimized for small sets of input data. Compared to existing models, our network also provides faster learning. Another advantage of our approach is its versatility for various types of medical imaging methods. We used peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) as the output image quality evaluation method. The results of our test show an improvement of 5.76% compared to optimizer Adam used in the original paper [10]. For faster training of the neural network model, calculations on the graphic card with the CUDA architecture were used. © Springer Nature Switzerland AG 2020.","Bioinformatics; Biomedical engineering; Image quality; Interpolation; Signal to noise ratio; Smart cards; Adversarial networks; High resolution image; Image quality evaluation; Interpolation method; Neural network model; Peak signal to noise ratio; Structural similarity; Super resolution; Medical imaging","Adam; CUDA; Deep neural network; Parallel processing; RMSprop; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85085201299"
"Romero L.S.; Marcello J.; Vilaplana V.","Romero, Luis Salgueiro (57218455911); Marcello, Javier (6602158797); Vilaplana, Verónica (23394280500)","57218455911; 6602158797; 23394280500","Super-resolution of Sentinel-2 imagery using generative adversarial networks","2020","Remote Sensing","12","15","2424","","","","10.3390/RS12152424","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089853089&doi=10.3390%2fRS12152424&partnerID=40&md5=6bb6fc362a861fc8952daf3ca71d5b36","Sentinel-2 satellites provide multi-spectral optical remote sensing images with four bands at 10 m of spatial resolution. These images, due to the open data distribution policy, are becoming an important resource for several applications. However, for small scale studies, the spatial detail of these images might not be sufficient. On the other hand, WorldView commercial satellites offer multi-spectral images with a very high spatial resolution, typically less than 2 m, but their use can be impractical for large areas or multi-temporal analysis due to their high cost. To exploit the free availability of Sentinel imagery, it is worth considering deep learning techniques for single-image super-resolution tasks, allowing the spatial enhancement of low-resolution (LR) images by recovering high-frequency details to produce high-resolution (HR) super-resolved images. In this work, we implement and train a model based on the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) with pairs of WorldView-Sentinel images to generate a super-resolved multispectral Sentinel-2 output with a scaling factor of 5. Our model, named RS-ESRGAN, removes the upsampling layers of the network to make it feasible to train with co-registered remote sensing images. Results obtained outperform state-of-the-art models using standard metrics like PSNR, SSIM, ERGAS, SAM and CC. Moreover, qualitative visual analysis shows spatial improvements as well as the preservation of the spectral information, allowing the super-resolved Sentinel-2 imagery to be used in studies requiring very high spatial resolution. © 2020 by the authors.","Deep learning; Image analysis; Image resolution; Network layers; Open Data; Optical resolving power; Remote sensing; Spectroscopy; Commercial satellites; Data distribution policies; Low resolution images; Multi-temporal analysis; Optical remote sensing; Remote sensing images; Spectral information; Very high spatial resolutions; Image enhancement","Deep learning; Generative adversarial network; Sentinel-2; Super-resolution; WorldView","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089853089"
"Sustika R.; Suksmono A.B.; Danudirdjo D.; Wikantika K.","Sustika, Rika (56523358200); Suksmono, Andriyan Bayu (6602490139); Danudirdjo, Donny (14019254300); Wikantika, Ketut (6603393238)","56523358200; 6602490139; 14019254300; 6603393238","Generative Adversarial Network with Residual Dense Generator for Remote Sensing Image Super Resolution","2020","Proceeding - 2020 International Conference on Radar, Antenna, Microwave, Electronics and Telecommunications, ICRAMET 2020","","","9298648","34","39","5","10.1109/ICRAMET51080.2020.9298648","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099336277&doi=10.1109%2fICRAMET51080.2020.9298648&partnerID=40&md5=d15bcbb688d091181df293460aec5d83","Improving image resolution, especially spatial resolution, has been one of the most important concerns on remote sensing research communities. An efficient solution for improving spatial resolution is by using algorithm, known as super-resolution (SR). The super-resolution technique that received special attention recently is super-resolution based on deep learning. In this paper, we propose deep learning approach based on generative adversarial network (GAN) for remote sensing images super resolution. We used residual dense network (RDN) as generator network. Generally, deep learning with residual dense network (RDN) gives high performance on classical (objective) evaluation metrics meanwhile generative adversarial network (GAN) based approach shows a high perceptual quality. Experiment results show that combination of residual dense network generator with generative adversarial network training is found to be effective. Our proposed method outperforms the baseline method in terms of objective and perceptual quality evaluation metrics. © 2020 IEEE.","Deep learning; Image enhancement; Image resolution; Optical resolving power; Quality control; Radar; Adversarial networks; Evaluation metrics; Learning approach; Perceptual quality; Remote sensing images; Research communities; Spatial resolution; Super resolution; Remote sensing","convolutional neural network; generative adversarial network; image; remote sensing; residual dense network; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85099336277"
"Maeda S.","Maeda, Shunta (57218709006)","57218709006","Unpaired Image Super-Resolution using Pseudo-Supervision","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156545","288","297","9","10.1109/CVPR42600.2020.00037","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094325727&doi=10.1109%2fCVPR42600.2020.00037&partnerID=40&md5=092d1c2d433b4863eda79731ec9a91fd","In most studies on learning-based image super-resolution (SR), the paired training dataset is created by downscaling high-resolution (HR) images with a predetermined operation (e.g., bicubic). However, these methods fail to super-resolve real-world low-resolution (LR) images, for which the degradation process is much more complicated and unknown. In this paper, we propose an unpaired SR method using a generative adversarial network that does not require a paired/aligned training dataset. Our network consists of an unpaired kernel/noise correction network and a pseudo-paired SR network. The correction network removes noise and adjusts the kernel of the inputted LR image; then, the corrected clean LR image is upscaled by the SR network. In the training phase, the correction network also produces a pseudo-clean LR image from the inputted HR image, and then a mapping from the pseudo-clean LR image to the inputted HR image is learned by the SR network in a paired manner. Because our SR network is independent of the correction network, well-studied existing network architectures and pixel-wise loss functions can be integrated with the proposed framework. Experiments on diverse datasets show that the proposed method is superior to existing solutions to the unpaired SR problem. © 2020 IEEE","Network architecture; Optical resolving power; Adversarial networks; Degradation process; High resolution image; Image super resolutions; Loss functions; Low resolution images; Training dataset; Training phase; Pattern recognition","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094325727"
"Zhang Z.; Zhang L.; Chen X.; Xu Y.","Zhang, Zhen (57221228789); Zhang, Liuyang (55838493400); Chen, Xuefeng (57205480544); Xu, Yafei (57214226974)","57221228789; 55838493400; 57205480544; 57214226974","Modified Generative Adversarial Network for Super-Resolution of Terahertz Image","2020","International Conference on Sensing, Measurement and Data Analytics in the Era of Artificial Intelligence, ICSMD 2020 - Proceedings","","","9261734","602","605","3","10.1109/ICSMD50554.2020.9261734","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098556966&doi=10.1109%2fICSMD50554.2020.9261734&partnerID=40&md5=22c3b03b063143238f8f19e557fab8cd","Terahertz (THz) images have low spatial resolution, blurring contour features and high background noise owing to the limitation of terahertz (THz) wavelengths and the THz imaging systems. We have proposed a modified Generative Adversarial Network (GAN) for super-resolution (SR) purpose. To fit the THz images, we design a kind of image degradation model to generate low-resolution images with Gaussian blur and white Gaussian noise. We establish a dataset of damage images in the field of non-destructive testing (NDT) for training and testing. The experimental results on THz images demonstrate that the improved GAN model can improve the quality of THz images effectively. Our method can be beneficial to improve the accuracy of THz NDT with low resolution. © 2020 IEEE.","Artificial intelligence; Gaussian noise (electronic); Image resolution; Nondestructive examination; Optical resolving power; Statistical tests; Adversarial networks; Image degradation model; Low resolution images; Non destructive testing; Spatial resolution; Super resolution; Training and testing; White Gaussian Noise; Image enhancement","deep learning; degradation model; super-resolution; THz image","Conference paper","Final","","Scopus","2-s2.0-85098556966"
"Heydari A.A.; Mehmood A.","Heydari, A. Ali (57217013173); Mehmood, Asif (36133731600)","57217013173; 36133731600","SRVAE: Super resolution using variational autoencoders","2020","Proceedings of SPIE - The International Society for Optical Engineering","11400","","114000U","","","","10.1117/12.2559808","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085735169&doi=10.1117%2f12.2559808&partnerID=40&md5=7bd0a5015f4a334e519c441e99b61bfe","The emergence of Generative Adversarial Network (GAN)-based single-image super-resolution (SISR) has allowed for finer textures in the super-resolved images, thus making them seem realistic to humans. However, GANbased models may depend on extensive high-quality data and are known to be very costly and unstable to train. On the other hand, Variational Autoencoders (VAEs) have inherent mathematical properties, and they are relatively cheap and stable to train; but VAEs produce blurry images that prevent them from being used for super-resolution. In this paper, we propose a first of its kind SISR method that takes advantage of a selfevaluating Variational Autoencoder (IntroVAE). Our network, called SRVAE, judges the quality of generated high-resolution (HR) images with the target images in an adversarial manner, which allows for high perceptual image generation. First, the encoder and the decoder of our introVAE-based method learn the manifold of HR images. In parallel, another encoder and decoder are simultaneously learning the reconstruction of the lowresolution (LR) images. Next, reconstructed LR images are fed to the encoder of the HR network to learn a mapping from LR images to corresponding HR versions. Using the encoder as a discriminator allows SRVAE to be a fast single-stream framework that performs super-resolution through generating photo-realistic images. Moreover, SRVAE has the same training stability and ""nice"" latent manifold structure as of VAEs, while playing a max-min adversarial game between the generator and the encoder like GANs. Our experiments show that our super-resolved images are comparable to the state-of-the-art GAN-based super-resolution. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Decoding; Learning systems; Optical resolving power; Pattern recognition; Signal encoding; Textures; Adversarial networks; High quality data; High resolution image; Image generations; Low resolution images; Manifold structures; Mathematical properties; Photorealistic images; Image reconstruction","Deep Learning; Generative Models; Single-Image Super-Resolution; Super-Resolution; Variational Autoencoders","Conference paper","Final","","Scopus","2-s2.0-85085735169"
"Wang Z.; Jiang K.; Yi P.; Han Z.; He Z.","Wang, Zhongyuan (57203515592); Jiang, Kui (57203871718); Yi, Peng (57203880354); Han, Zhen (56415515700); He, Zheng (53881225400)","57203515592; 57203871718; 57203880354; 56415515700; 53881225400","Ultra-dense GAN for satellite imagery super-resolution","2020","Neurocomputing","398","","","328","337","9","10.1016/j.neucom.2019.03.106","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075425341&doi=10.1016%2fj.neucom.2019.03.106&partnerID=40&md5=5268eacbe62512668065597c63767082","Image super-resolution (SR) techniques improve various remote sensing applications by allowing for finer spatial details than those captured by the original acquisition sensors. Recent advances in deep learning bring a new opportunity for SR by learning the mapping from low to high resolution. The most used convolutional neural networks (CNN) based approaches are prone to excessive smoothing or blurring due to the optimization objective in mean squared error (MSE). Instead, generative adversarial network (GAN) based approaches can achieve more perceptually acceptable results. However, the preliminary design of GANs generator with simple direct- or skip-connection residual blocks compromises its SR potential. Emerging dense convolutional network (DenseNet) equipped with dense connections has shown a promising prospect in classification and super-resolution. An intuitive idea to introduce DenseNet into GAN is expected to boost SR performance. However, because convolutional kernels in the existing residual block are arranged into a one-dimensional flat structure, the formation of dense connections highly relies on skip connections (linking the current layer to all subsequent layers with a shortcut path). In order to increase connection density, the depth of the layer has to be accordingly expanded, which in turn results in training difficulties such as vanishing gradient and information propagation loss. To this end, this paper proposes an ultra-dense GAN (udGAN) for image SR, where we reform the internal layout of the residual block into a two-dimensional matrix topology. This topology can provide additional diagonal connections so that we can still accomplish enough pathways with fewer layers. In particular, the pathways are almost doubled compared to previous dense connections under the same number of layers. The achievable rich connections are flexibly adapted to the diversity of image content, thus leading to improved SR performance. Extensive experiments on public benchmark datasets and real-world satellite imagery show that our model outperforms state-of-the-art counterparts in both subjective and quantitative assessments, especially those related to perception. © 2019 Elsevier B.V.","Backpropagation; Convolution; Deep learning; Image enhancement; Information dissemination; Mean square error; Neural networks; Optical resolving power; Remote sensing; Topology; Convolutional networks; Convolutional neural network; Image super resolutions; Information propagation; Quantitative assessments; Remote sensing applications; Super resolution; Ultra-dense residual block; Article; convolutional neural network; correlation analysis; data base; deep recursive residual network; dense convolutional network; generative adversarial network; image analysis; image processing; image quality; image reconstruction; priority journal; process optimization; satellite imagery; super resolution convolutional neural network; Satellite imagery","GAN; Satellite imagery; Super-resolution; Ultra-dense residual block","Article","Final","","Scopus","2-s2.0-85075425341"
"Wang L.; Yang S.; Jia J.","Wang, Lin (57219301799); Yang, Siqi (57218167235); Jia, Jingqian (57219132726)","57219301799; 57218167235; 57219132726","A super-resolution reconstruction algorithm based on feature fusion","2020","Chinese Control Conference, CCC","2020-July","","9188443","3060","3065","5","10.23919/CCC50068.2020.9188443","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091394741&doi=10.23919%2fCCC50068.2020.9188443&partnerID=40&md5=ecbd3aff234fbbc1c5d5e4968a92fd8c","Super-resolution reconstruction is a technique for recovering corresponding high-resolution images from low-resolution images. Aiming at the problem of insufficient learning ability of the generative network in the SRGAN, a super-resolution reconstruction algorithm of generative adversarial network based on feature fusion is proposed. Recursive residual networks and prior knowledge are used to extract edge and texture features, while densely connected networks are used to fully utilize the features. Aiming at the problem that the input of the discriminative network of the SRGAN has a high computational complexity for the entire image, a method for discriminant residual discrimination is proposed. Two difference maps are used as input images of the discriminative network, and two discriminative methods are used for training. The test data set was used to verify the performance of the algorithm. The subjective index was significantly better than the original algorithm. The objective index PSNR increased by 0.77db, and SSIM increased by 2.8%. © 2020 Technical Committee on Control Theory, Chinese Association of Automation.","Statistical tests; Textures; Adversarial networks; Densely connected networks; Discriminative methods; Discriminative networks; High resolution image; Low resolution images; Original algorithms; Super resolution reconstruction; Optical resolving power","feature fusion; GAN; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85091394741"
"Guo J.; Yang Q.; Fu S.; Boyles R.; Turner S.; Clarke K.","Guo, Jingda (57210584211); Yang, Qing (57043785400); Fu, Song (35078327800); Boyles, Ryan (57219270676); Turner, Shavon (57219266780); Clarke, Kenzie (57219265828)","57210584211; 57043785400; 35078327800; 57219270676; 57219266780; 57219265828","Towards Trustworthy Perception Information Sharing on Connected and Autonomous Vehicles","2020","Proceedings - 2020 International Conference on Connected and Autonomous Driving, MetroCAD 2020","","","9138639","85","90","5","10.1109/MetroCAD48866.2020.00021","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091989830&doi=10.1109%2fMetroCAD48866.2020.00021&partnerID=40&md5=c55183f2eedd4849189946ceefdda274","Sharing perception data among autonomous vehicles is extremely useful to extending the line of sight and field of view of autonomous vehicles, which otherwise suffer from blind spots and occlusions. However, the security of using data from a random other car in making driving decisions is an issue. Without the ability of assessing the trustworthiness of received information, it will be too risky to use them for any purposes. On the other hand, when information is exchanged between vehicles, it provides a golden opportunity to quantitatively study a vehicle's trust. In this paper, we propose a trustworthy information sharing framework for connected and autonomous vehicles in which vehicles measure each other's trust using the Dirichlet-Categorical (DC) model. To increase a vehicle's capability of assessing received data's trust, we leverage the Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) model to increase the resolution of blurry images. As a result, a vehicle is able to evaluate the trustworthiness of received data that contain distant objects. Based on the KITTI dataset, we evaluate the proposed solution and discover that vehicle's trust assessment capability can be increased by 11 - 37%, using the ESRGAN model.  © 2020 IEEE.","Image enhancement; Information analysis; Information dissemination; Network security; Adversarial networks; Blind spots; Blurry images; Field of views; Information sharing; Line of Sight; Super resolution; Trust assessments; Autonomous vehicles","autonomous vehicles; Connect vehicles; image super-resolution; object detection; trustworthy information sharing","Conference paper","Final","","Scopus","2-s2.0-85091989830"
"Pashaei M.; Starek M.J.; Kamangir H.; Berryhill J.","Pashaei, Mohammad (57213189326); Starek, Michael J. (23494042900); Kamangir, Hamid (57196243463); Berryhill, Jacob (56542823200)","57213189326; 23494042900; 57196243463; 56542823200","Deep learning-based single image super-resolution: An investigation for dense scene reconstruction with UAS photogrammetry","2020","Remote Sensing","12","11","1757","","","","10.3390/rs12111757","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086470516&doi=10.3390%2frs12111757&partnerID=40&md5=2371fcfa424e180cf1f098715b7cd45d","The deep convolutional neural network (DCNN) has recently been applied to the highly challenging and ill-posed problem of single image super-resolution (SISR), which aims to predict high-resolution (HR) images from their corresponding low-resolution (LR) images. In many remote sensing (RS) applications, spatial resolution of the aerial or satellite imagery has a great impact on the accuracy and reliability of information extracted from the images. In this study, the potential of a DCNN-based SISR model, called enhanced super-resolution generative adversarial network (ESRGAN), to predict the spatial information degraded or lost in a hyper-spatial resolution unmanned aircraft system (UAS) RGB image set is investigated. ESRGAN model is trained over a limited number of original HR (50 out of 450 total images) and virtually-generated LR UAS images by downsampling the original HR images using a bicubic kernel with a factor x 4. Quantitative and qualitative assessments of super-resolved images using standard image quality measures (IQMs) confirm that the DCNN-based SISR approach can be successfully applied on LR UAS imagery for spatial resolution enhancement. The performance of DCNN-based SISR approach for the UAS image set closely approximates performances reported on standard SISR image sets with mean peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) index values of around 28 dB and 0.85 dB, respectively. Furthermore, by exploiting the rigorous Structure-from-Motion (SfM) photogrammetry procedure, an accurate task-based IQM for evaluating the quality of the super-resolved images is carried out. Results verify that the interior and exterior imaging geometry, which are extremely important for extracting highly accurate spatial information from UAS imagery in photogrammetric applications, can be accurately retrieved from a super-resolved image set. The number of corresponding keypoints and dense points generated from the SfM photogrammetry process are about 6 and 17 times more than those extracted from the corresponding LR image set, respectively. © 2020 by the authors.","Antennas; Convolutional neural networks; Deep learning; Deep neural networks; Image quality; Image reconstruction; Image resolution; Optical resolving power; Photogrammetry; Remote sensing; Satellite imagery; Signal to noise ratio; Unmanned aerial vehicles (UAV); High resolution image; Peak signal to noise ratio; Quantitative and qualitative assessments; Reliability of information; Spatial-resolution enhancement; Structural similarity indices (SSIM); Structure from motion; Unmanned aircraft system; Image enhancement","Convolutional neural network (CNN); Deep learning; Generative adversarial network (GAN); Photogrammetry; Remote sensing; Structure-from-motion; Super-resolution (SR); Unmanned aircraft system (UAS)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086470516"
"Lei D.; Zhang C.; Li Z.; Wu Y.","Lei, Dajiang (36627356400); Zhang, Ce (57216935063); Li, Zhixing (56181457200); Wu, Yu (56468297100)","36627356400; 57216935063; 56181457200; 56468297100","Remote Sensing Image Fusion Based on Generative Adversarial Network with Multi-stream Fusion Architecture; [基于多流融合生成对抗网络的遥感图像融合方法]","2020","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","42","8","","1942","1949","7","10.11999/JEIT17_190273","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089700151&doi=10.11999%2fJEIT17_190273&partnerID=40&md5=45ddf43c40faef4e31b10888e507a93d","The generative adversarial network receives extensive attention in the study of computer vision such as image fusion and image super-resolution, due to its strong ability of generating high quality images. At present, the remote sensing image fusion method based on generative adversarial network only learns the mapping between the images, and lacks the unique Pan-sharpening domain knowledge. This paper proposes a remote sensing image fusion method based on optimized generative adversarial network with the integration of the spatial structure information of panchromatic image. The proposed algorithm extracts the spatial structure information of the panchromatic image by the gradient operator. The extracted feature would be added to both the discriminator and the generator which uses a multi-stream fusion architecture. The corresponding optimization objective and fusion rules are then designed to improve the quality of the fused image. Experiments on images acquired by WorldView-3 satellites demonstrate that the proposed method can generate high quality fused images, which is better than the most of advanced remote sensing image fusion methods in both subjective visual and objective evaluation indicators.","Image enhancement; Network architecture; Remote sensing; Adversarial networks; Fusion architecture; High quality images; Image super resolutions; Objective evaluation; Panchromatic images; Remote sensing images; Spatial structure information; Image fusion","Computer vision; Generative adversarial network; Multi-stream fusion architecture; Remote sensing image fusion","Article","Final","","Scopus","2-s2.0-85089700151"
"Chai Y.; Xu B.; Zhang K.; Lepore N.; Wood J.C.","Chai, Yaqiong (56519050700); Xu, Botian (57216846313); Zhang, Kangning (57216846624); Lepore, Natasha (15042356000); Wood, John C. (55371399600)","56519050700; 57216846313; 57216846624; 15042356000; 55371399600","MRI restoration using edge-guided adversarial learning","2020","IEEE Access","8","","9086007","83858","83870","12","10.1109/ACCESS.2020.2992204","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084947905&doi=10.1109%2fACCESS.2020.2992204&partnerID=40&md5=e43d847437d2aceb2d4c03c4bc755ded","Magnetic resonance imaging (MRI) images acquired as multislice two-dimensional (2D) images present challenges when reformatted in orthogonal planes due to sparser sampling in the through-plane direction. Restoring the 'missing' through-plane slices, or regions of an MRI image damaged by acquisition artifacts can be modeled as an image imputation task. In this work, we consider the damaged image data or missing through-plane slices as image masks and proposed an edge-guided generative adversarial network to restore brain MRI images. Inspired by the procedure of image inpainting, our proposed method decouples image repair into two stages: edge connection and contrast completion, both of which used general adversarial networks (GAN). We trained and tested on a dataset from the Human Connectome Project to test the application of our method for thick slice imputation, while we tested the artifact correction on clinical data and simulated datasets. Our Edge-Guided GAN had superior PSNR, SSIM, conspicuity and signal texture compared to traditional imputation tools, the Context Encoder and the Densely Connected Super Resolution Network with GAN (DCSRN-GAN). The proposed network may improve utilization of clinical 2D scans for 3D atlas generation and big-data comparative studies of brain morphometry. © 2013 IEEE.","Image acquisition; Image reconstruction; Restoration; Statistical tests; Textures; Adversarial learning; Adversarial networks; Artifact correction; Comparative studies; Orthogonal plane; Simulated datasets; Super resolution; Two dimensional (2D) image; Magnetic resonance imaging","Artifact correction; edge; generative adversarial network; image restoration; imputation; magnetic resonance imaging","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084947905"
"Cao M.; Ji H.; Gao Z.; Mei T.","Cao, Min (57226775285); Ji, Hong (57205763449); Gao, Zhi (55256514200); Mei, Tincan (8914886000)","57226775285; 57205763449; 55256514200; 8914886000","Vehicle Detection in Remote Sensing Images Using Deep Neural Networks and Multi-Task Learning","2020","ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","5","2","","797","804","7","10.5194/isprs-annals-V-2-2020-797-2020","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091110780&doi=10.5194%2fisprs-annals-V-2-2020-797-2020&partnerID=40&md5=daec2d2a13004100f9a2033c9127ef4c","Vehicle detection in remote sensing image has been attracting remarkable attention over past years for its applications in traffic, security, military, and surveillance fields. Due to the stunning success of deep learning techniques in object detection community, we consider to utilize CNNs for vehicle detection task in remote sensing image. Specifically, we take advantage of deep residual network, multi-scale feature fusion, hard example mining and homography augmentation to realize vehicle detection, which almost integrates all the advanced techniques in deep learning community. Furthermore, we simultaneously address super-resolution (SR) and detection problems of low-resolution (LR) image in an end-to-end manner. In consideration of the absence of paired low-/highresolution data which are generally time-consuming and cumbersome to collect, we leverage generative adversarial network (GAN) for unsupervised SR. Detection loss is back-propagated to SR generator to boost detection performance. We conduct experiments on representative benchmark datasets and demonstrate that our model yields significant improvements over state-of-the-art methods in deep learning and remote sensing areas. © 2020 Copernicus GmbH. All rights reserved.","Deep neural networks; Learning systems; Military applications; Military photography; Military vehicles; Multi-task learning; Neural networks; Object detection; Remote sensing; Adversarial networks; Benchmark datasets; Detection performance; Learning techniques; Low resolution images; Multi-scale features; Remote sensing images; State-of-the-art methods; Deep learning","GAN; hard example mining; homography augmentation; multi-scale feature fusion; Remote sensing images; super-resolution; Vehicle detection","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091110780"
"Lambhate D.; Subramani D.N.","Lambhate, Devyani (57223045780); Subramani, Deepak N. (57022054400)","57223045780; 57022054400","Super-Resolution of Sea Surface Temperature Satellite Images","2020","2020 Global Oceans 2020: Singapore - U.S. Gulf Coast","","","9389030","","","","10.1109/IEEECONF38699.2020.9389030","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104654759&doi=10.1109%2fIEEECONF38699.2020.9389030&partnerID=40&md5=70748e74bcee07795666f6a8fe6f3351","Availability of high-resolution maps of geophysical fields, devoid of data loss due to clouds, is an urgent requirement for operational forecasting. We develop a Bayesian algorithm for super-resolution (or downscaling) of lower resolution geophysical fields observed by satellites. The key novelty in the present algorithm is the development and use of a Generative Adversarial Network (GAN) to learn the prior probability distribution of the high-resolution geophysical fields from historical data and/or model forecasts. The trained GAN is used to sample from the high-resolution prior and a particle filter along with the low-resolution data (as observation) is used to obtain the posterior high-resolution geophysical field. The resultant algorithm has been named the Particle Filter Generative Adversarial Network super-resolution (PF-GAN-SR) algorithm. The new algorithm is applied to downscale sea surface temperature fields in the northwest Atlantic Ocean. Results show consistent performance across different downscaling ratios. Notably, the high-resolution fields obtained from the new algorithm has better similarity score with the true high-resolution field compared to those from bi-cubic interpolation (commonly used in the geophysical community) and the SR-GAN algorithm (used in the computer vision community). © 2020 IEEE.","Atmospheric temperature; Monte Carlo methods; Optical resolving power; Probability distributions; Submarine geophysics; Surface properties; Surface waters; Adversarial networks; Bayesian algorithms; Bicubic interpolation; Consistent performance; High resolution maps; Operational forecasting; Sea surface temperature (SST); Vision communities; Oceanography","Downscaling; Generative Adversarial Network; particle filter; PF-GAN-SR","Conference paper","Final","","Scopus","2-s2.0-85104654759"
"Pineda F.; Ayma V.; Beltran C.","Pineda, F. (57216822078); Ayma, V. (56566776600); Beltran, C. (55602499700)","57216822078; 56566776600; 55602499700","A generative adversarial network approach for super-resolution of sentinel-2 satellite images","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B1","","9","14","5","10.5194/isprs-archives-XLIII-B1-2020-9-2020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091133545&doi=10.5194%2fisprs-archives-XLIII-B1-2020-9-2020&partnerID=40&md5=6de901c895119a77b97c4ee7b956660b","High-resolution satellite images have always been in high demand due to the greater detail and precision they offer, as well as the wide scope of the fields in which they could be applied; however, satellites in operation offering very high-resolution (VHR) images has experienced an important increase, but they remain as a smaller proportion against existing lower resolution (HR) satellites. Recent models of convolutional neural networks (CNN) are very suitable for applications with image processing, like resolution enhancement of images; but in order to obtain an acceptable result, it is important, not only to define the kind of CNN architecture but the reference set of images to train the model. Our work proposes an alternative to improve the spatial resolution of HR images obtained by Sentinel-2 satellite by using the VHR images from PeruSat1, a peruvian satellite, which serve as the reference for the super-resolution approach implementation based on a Generative Adversarial Network (GAN) model, as an alternative for obtaining VHR images. The VHR PeruSat-1 image dataset is used for the training process of the network. The results obtained were analyzed considering the Peak Signal to Noise Ratios (PSNR) and the Structural Similarity (SSIM). Finally, some visual outcomes, over a given testing dataset, are presented so the performance of the model could be analyzed as well. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Convolutional neural networks; Optical resolving power; Satellites; Signal to noise ratio; Statistical tests; Well testing; Adversarial networks; High resolution satellite images; Peak signal to noise ratio; Resolution enhancement; Satellite images; Spatial resolution; Structural similarity; Very high resolution (VHR) image; Image enhancement","GAN; PeruSat-1; Sentinel-2; Super-Resolution","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091133545"
"Kang S.; Han D.; Lee J.; Im D.; Kim S.; Kim S.; Yoo H.-J.","Kang, Sanghoon (57193747501); Han, Donghyeon (57200015202); Lee, Juhyoung (57194786540); Im, Dongseok (57217265088); Kim, Sangyeob (57201900920); Kim, Soyeon (57220895009); Yoo, Hoi-Jun (7201373390)","57193747501; 57200015202; 57194786540; 57217265088; 57201900920; 57220895009; 7201373390","GANPU: A 135TFLOPS/W Multi-DNN Training Processor for GANs with Speculative Dual-Sparsity Exploitation","2020","Digest of Technical Papers - IEEE International Solid-State Circuits Conference","2020-February","","9062989","140","142","2","10.1109/ISSCC19947.2020.9062989","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083829186&doi=10.1109%2fISSCC19947.2020.9062989&partnerID=40&md5=0e35ad32f2bf258ced2f63b3fa5aa3db","Generative adversarial networks (GAN) have a wide range of applications, from image style transfer to synthetic voice generation [1]. GAN applications on mobile devices, such as face-to-Emoji conversion and super-resolution imaging, enable more engaging user interaction. As shown in Fig. 7.4.1, a GAN consists of 2 competing deep neural networks (DNN): a generator and a discriminator. The discriminator is trained, while the generator is fixed, to distinguish whether the generated image is real or fake. On the other hand, the generator is trained to generate fake images to fool the discriminator. The minimax rivalry between the 2 sub-DNNs enables the model to generate high-quality images, difficult even for humans to distinguish. © 2020 IEEE.","Networks (circuits); Solid state devices; Adversarial networks; Dual sparsities; High quality images; Minimax; Super resolution imaging; Synthetic voices; User interaction; Deep neural networks","","Conference paper","Final","","Scopus","2-s2.0-85083829186"
"Gu Y.; Zeng Z.; Chen H.; Wei J.; Zhang Y.; Chen B.; Li Y.; Qin Y.; Xie Q.; Jiang Z.; Lu Y.","Gu, Yuchong (57211429900); Zeng, Zitao (57194624192); Chen, Haibin (56571624500); Wei, Jun (57201973656); Zhang, Yaqin (35801521300); Chen, Binghui (57200370980); Li, Yingqin (57202364469); Qin, Yujuan (57218491833); Xie, Qing (57685529100); Jiang, Zhuoren (36173693000); Lu, Yao (57190289782)","57211429900; 57194624192; 56571624500; 57201973656; 35801521300; 57200370980; 57202364469; 57218491833; 57685529100; 36173693000; 57190289782","MedSRGAN: medical images super-resolution using generative adversarial networks","2020","Multimedia Tools and Applications","79","29-30","","21815","21840","25","10.1007/s11042-020-08980-w","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089317565&doi=10.1007%2fs11042-020-08980-w&partnerID=40&md5=6cdacf0bd3f0b064b6a9512245438de6","Super-resolution (SR) in medical imaging is an emerging application in medical imaging due to the needs of high quality images acquired with limited radiation dose, such as low dose Computer Tomography (CT), low field magnetic resonance imaging (MRI). However, because of its complexity and higher visual requirements of medical images, SR is still a challenging task in medical imaging. In this study, we developed a deep learning based method called Medical Images SR using Generative Adversarial Networks (MedSRGAN) for SR in medical imaging. A novel convolutional neural network, Residual Whole Map Attention Network (RWMAN) was developed as the generator network for our MedSRGAN in extracting the useful information through different channels, as well as paying more attention on meaningful regions. In addition, a weighted sum of content loss, adversarial loss, and adversarial feature loss were fused to form a multi-task loss function during the MedSRGAN training. 242 thoracic CT scans and 110 brain MRI scans were collected for training and evaluation of MedSRGAN. The results showed that MedSRGAN not only preserves more texture details but also generates more realistic patterns on reconstructed SR images. A mean opinion score (MOS) test on CT slices scored by five experienced radiologists demonstrates the efficiency of our methods. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Deep learning; Magnetic resonance imaging; Medical imaging; Optical resolving power; Textures; Adversarial networks; Emerging applications; High quality images; Learning-based methods; Loss functions; Low field magnetic resonance; Mean opinion scores; Super resolution; Computerized tomography","Deep learning; Generative adversarial networks (GAN); Medical images; Super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85089317565"
"Zhang S.; Wang L.; Zhang J.; Gu L.; Jiang X.; Zhai X.; Sha X.; Chang S.","Zhang, Siqi (57222471756); Wang, Lei (57225162507); Zhang, Jie (57225122105); Gu, Ling (56024017800); Jiang, Xiran (57211957570); Zhai, Xiaoyue (7006359359); Sha, Xianzheng (7005501923); Chang, Shijie (56697663200)","57222471756; 57225162507; 57225122105; 56024017800; 57211957570; 7006359359; 7005501923; 56697663200","Consecutive context perceive generative adversarial networks for serial sections inpainting","2020","IEEE Access","8","","","190417","190430","13","10.1109/ACCESS.2020.3031973","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102763335&doi=10.1109%2fACCESS.2020.3031973&partnerID=40&md5=4fffd5a69d094b9ff31e0506215d0df9","Image inpainting is a hot topic in computer vision research and has been successfully applied to both traditional and digital mediums, such as oil paintings or old photos mending, image or video denoising and super-resolution. With the introduction of artificial intelligence (AI), a series of algorithms, represented by semantic inpainting, have been developed and better results were achieved. Medical image inpainting, as one of the most demanding applications, needs to meet both the visual effects and strict content correctness. 3D reconstruction of microstructures, based on serial sections, could provide more spatial information and help us understand the physiology or pathophysiology mechanism in histology study, in which extremely high-quality continuous images without any defects are required. In this article, we proposed a novel Consecutive Context Perceive Generative Adversarial Networks (CCPGAN) for serial sections inpainting. Our method can learn semantic information from its neighboring image, and restore the damaged parts of serial sectioning images to maximum extent. Validated with 2 sets of serial sectioning images of mouse kidney, qualitative and quantitative results suggested that our method could robustly restore breakage of any size and location while achieving near realtime performance. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Artificial intelligence; Mammals; Medical imaging; Restoration; Semantics; 3D reconstruction; Adversarial networks; Image Inpainting; Quantitative result; Real time performance; Semantic information; Serial sectioning; Spatial informations; Image reconstruction","Consecutive context perceive GAN; Generative adversarial network; Serial sectioning images","Article","Final","","Scopus","2-s2.0-85102763335"
"Ma J.; Wu H.; Zhang J.; Zhang L.","Ma, Jie (57205916758); Wu, Hanlin (57221263814); Zhang, Jue (56513505100); Zhang, Libao (35325855000)","57205916758; 57221263814; 56513505100; 35325855000","SD-FB-GAN: Saliency-Driven Feedback Gan for Remote Sensing Image Super-Resolution Reconstruction","2020","Proceedings - International Conference on Image Processing, ICIP","2020-October","","9190835","528","532","4","10.1109/ICIP40778.2020.9190835","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098652564&doi=10.1109%2fICIP40778.2020.9190835&partnerID=40&md5=d13a6e4f699e22b71ce072e504a572b0","The visual characteristics of different regions in remote sensing images are significantly versatile, which poses a huge challenge to single image super-resolution. Although generative adversarial network (GAN) has shown great potential in generating photo-realistic results, it provides unsatisfactory performance in objective metrics owning to pseudo textures brought by adversarial learning. In this paper, we propose a new saliency-driven feedback GAN to cope with these problems. We design a saliency-driven feedback generator based on paired-feedback blocks (PFBBs) and recurrent structure to provide strong reconstruction ability. In the PFBB, the saliency map serves as an indicator to reflect the texture complexity, so different reconstruction principles can be applied to restore areas with varying levels of saliency. Besides, we propose to measure the visual quality of salient areas, non-salient areas, and the whole image with multi-discriminators, which can dramatically eliminate pseudo textures. Comprehensive evaluations and ablation studies validate the superiority of our proposal. © 2020 IEEE.","Optical resolving power; Recurrent neural networks; Remote sensing; Textures; Adversarial learning; Adversarial networks; Comprehensive evaluation; Objective metrics; Photo-realistic; Remote sensing images; Texture complexity; Visual qualities; Image reconstruction","deep learning; GAN; Image reconstruction; saliency analysis; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85098652564"
"Jiang F.; Norlund P.","Jiang, F. (35200780700); Norlund, P. (42361565100)","35200780700; 42361565100","Super resolution of fault plane prediction by a generative adversarial network","2020","1st EAGE Digitalization Conference and Exhibition","","","ThDIGI17","","","","10.3997/2214-4609.202032011","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092664017&doi=10.3997%2f2214-4609.202032011&partnerID=40&md5=661504e6c6fe085e9b119726415e5fd2","Interpreting seismic data enhances understanding of subsurface geological features, particularly for assisted fault interpretation. The results of assisted fault interpretation workflows can provide valuable information to optimize hydrocarbon production during drilling and stimulation treatments. However, given the complexity of seismic data such workflows can generate incorrect or misleading interpretations, such as discontinuous fault segments and mispositioned fault planes, particularly when deep-learning convolutional neural networks are used. Fault extraction results often face difficulties locating the fault plane where low reflectivity or signal-to-noise ratio exists. In this abstract, a novel approach is introduced that implements a super-resolution generative adversarial network to help improve the resolution of fault prediction results. Synthetic fault data were generated to train an adversarial model, which was then applied to different field data sets. This approach could serve as a standard post-processing workflow to decrease the uncertainty as part of an assisted fault interpretation approach and provides an efficient method of helping improve the fidelity of fault prediction results. © EAGE 2019.","Deep learning; Deep neural networks; Forecasting; Geophysical prospecting; Hydrocarbon refining; Optical resolving power; Seismic response; Seismic waves; Signal to noise ratio; Adversarial networks; Fault interpretation; Fault prediction; Field data sets; Geological features; Hydrocarbon production; Stimulation treatments; Super resolution; Convolutional neural networks","","Conference paper","Final","","Scopus","2-s2.0-85092664017"
"Saquib Sarfraz M.; Seibold C.; Khalid H.; Stiefelhagen R.","Saquib Sarfraz, M. (36761552400); Seibold, Constantin (57217523260); Khalid, Haroon (57190814259); Stiefelhagen, Rainer (6602180348)","36761552400; 57217523260; 57190814259; 6602180348","Content and colour distillation for learning image translations with the spatial profile loss","2020","30th British Machine Vision Conference 2019, BMVC 2019","","","","","","","","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087330546&partnerID=40&md5=a68febdaa0099602eb270a2cfa3c02c8","Generative adversarial networks has emerged as a defacto standard for image translation problems. To successfully drive such models, one has to rely on additional networks e.g., discriminators and/or perceptual networks. Training these networks with pixel based losses alone are generally not sufficient to learn the target distribution. In this paper, we propose a novel method of computing the loss directly between the source and target images that enable proper distillation of shape/content and colour/style. We show that this is useful in typical image-to-image translations allowing us to successfully drive the generator without relying on additional networks. We demonstrate this on many difficult image translation problems such as image-to-image domain mapping, single image super-resolution and photo realistic makeup transfer. Our extensive evaluation shows the effectiveness of the proposed formulation and its ability to synthesize realistic images. © 2019. The copyright of this document resides with its authors.","Computer vision; Distillation; Adversarial networks; De facto standard; Image translation; Photo-realistic; Realistic images; Single images; Spatial profiles; Target images; Distilleries","","Conference paper","Final","","Scopus","2-s2.0-85087330546"
"Li J.; Chen Z.; Zhao X.; Shao L.","Li, Jingtao (57217013418); Chen, Zhanlong (26322275400); Zhao, Xiaozhen (57217013673); Shao, Lijia (57217014471)","57217013418; 26322275400; 57217013673; 57217014471","MAPGAN: An intelligent generation model for network tile maps","2020","Sensors (Switzerland)","20","11","3119","","","","10.3390/s20113119","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085729281&doi=10.3390%2fs20113119&partnerID=40&md5=11c206c75755ee4a1045444f289c8a7b","In recent years, the generative adversarial network (GAN)-based image translation model has achieved great success in image synthesis, image inpainting, image super-resolution, and other tasks. However, the images generated by these models often have problems such as insufficient details and low quality. Especially for the task of map generation, the generated electronic map cannot achieve effects comparable to industrial production in terms of accuracy and aesthetics. This paper proposes a model called Map Generative Adversarial Networks (MapGAN) for generating multitype electronic maps accurately and quickly based on both remote sensing images and render matrices. MapGAN improves the generator architecture of Pix2pixHD and adds a classifier to enhance the model, enabling it to learn the characteristics and style differences of different types of maps. Using the datasets of Google Maps, Baidu maps, and Map World maps, we compare MapGAN with some recent image translation models in the fields of one-to-one map generation and one-to-many domain map generation. The results show that the quality of the electronic maps generated by MapGAN is optimal in terms of both intuitive vision and classic evaluation indicators. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Maps; Remote sensing; Adversarial networks; Evaluation indicators; Image Inpainting; Image super resolutions; Image synthesis; Image translation; Industrial production; Remote sensing images; article; classifier; human; human experiment; remote sensing; vision; Image processing","Deep generation model; Image translation; Map generation; Network tile map","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085729281"
"Lu T.; Chen C.; Xu R.; Zhang Y.","Lu, Tao (56406646300); Chen, Chong (57210596817); Xu, Ruobo (57213520889); Zhang, Yanduo (55993581700)","56406646300; 57210596817; 57213520889; 55993581700","Face hallucination based on edge enhanced generative adversarial network; [基于边缘增强生成对抗网络的人脸超分辨率重建]","2020","Huazhong Keji Daxue Xuebao (Ziran Kexue Ban)/Journal of Huazhong University of Science and Technology (Natural Science Edition)","48","1","","87","92","5","10.13245/j.hust.200116","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079605582&doi=10.13245%2fj.hust.200116&partnerID=40&md5=2336635ec0c02384fae7e816fe23f3a1","Aiming at the imperfection of the countermeasure generation neural network in the restoration of facial contour details, an edge enhancement generation countermeasure network was proposed to enhance the super-resolution reconstruction performance of human face based on the prior structural information of face images.First, a parallel network was designed by using the consistency relationship between face images and their edge images.The network extracted facial and edge detail features, and then high-resolution generated images were obtained by feature fusion network.Finally, the authenticity of generated images was distinguished by discriminant network.Experimental results of face super-resolution reconstruction on face image database by the proposed algorithm show that the proposed edge enhancement generates confrontation network can improve the ability of facial detail reconstruction, and the subjective and objective evaluation indexes are superior to the existing frontier face super-resolution algorithms. © 2020, Editorial Board of Journal of Huazhong University of Science and Technology. All right reserved.","Optical resolving power; Reconstruction (structural); Adversarial networks; Edge fusion; Face hallucination; Face super-resolution; Parallel network; Structural information; Subjective and objective evaluations; Super resolution reconstruction; Image enhancement","Edge enhanced network; Edge fusion; Face hallucination; Generative adversarial network; Parallel network","Article","Final","","Scopus","2-s2.0-85079605582"
"Huang J.","Huang, Jinchao (57218480648)","57218480648","Image super-resolution reconstruction based on generative adversarial network model with double discriminators","2020","Multimedia Tools and Applications","79","39-40","","29639","29662","23","10.1007/s11042-020-09524-y","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089297551&doi=10.1007%2fs11042-020-09524-y&partnerID=40&md5=d6226a60e296468df4c4a574ae7e74ee","To improve the reconstruction accuracy and efficiency for image super-resolution, this paper proposes a novel image super-resolution reconstruction algorithm based on generative adversarial network model with double discriminators (SRGAN-DD). For the proposed super-resolution reconstruction algorithm, we add a new discriminator based on SRGAN model, and combine the Kullback-Leibler (KL) divergence and reverse KL divergence as the uniform objective function to train such two discriminators. By using the complementary statistical characteristics from such two KL divergences, the proposed SRGAN-DD model will effectively disperse the estimated density in multiple modes, and the problem of network collapsed during reconstruction will be effectively avoided, so the robustness and efficiency of the model training is improved. For the part of model loss function design, the loss function to construct content loss by Charbonnier loss function is applied. Then, we design the perception loss and style loss by using the feature maps from middle layers of deep neural network models to achieve a combination loss function. At last, the deconvolutional operation is introduced into the network model for image reconstruction to reduce the reconstruction time complexity. To validate the feasibility and effectiveness, three groups of experiments are conducted to compare the proposed SRGAN-DD model with state-of-the-arts algorithms. Experimental results have shown that the proposed algorithm achieves the best performance on both objective and subjective judgment indicators. With the combination of loss function, the reconstructed images show less effect of artifacts and less influence of noises. The proposed SRGAN-DD model shows significant gains in perceived quality in reconstructing images. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Deep neural networks; Discriminators; Efficiency; Image enhancement; Multilayer neural networks; Optical resolving power; Adversarial networks; Image super resolutions; Image super-resolution reconstruction; Kullback-Leibler divergence; Neural network model; Reconstruction accuracy; Statistical characteristics; Super resolution reconstruction; Image reconstruction","Combination loss function; Double discriminators; Generative adversarial network; Image super-resolution reconstruction; KL divergence","Article","Final","","Scopus","2-s2.0-85089297551"
"Zhu Q.; Fan X.; Zhong Y.; Guan Q.; Zhang L.; Li D.","Zhu, Qiqi (56420945500); Fan, Xin (57222248289); Zhong, Yanfei (12039673900); Guan, Qingfeng (55838509944); Zhang, Liangpei (8359720900); Li, Deren (57212271839)","56420945500; 57222248289; 12039673900; 55838509944; 8359720900; 57212271839","Super Resolution Generative Adversarial Network Based Image Augmentation for Scene Classification of Remote Sensing Images","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324043","573","576","3","10.1109/IGARSS39084.2020.9324043","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102001018&doi=10.1109%2fIGARSS39084.2020.9324043&partnerID=40&md5=2c6d3f95bad091ad3ea96988131867ed","High spatial resolution remote sensing image (RSI) scene classification, aimed at automatically labelling images with the given semantic categories, has been a hot issue. As it's difficult for RSI to quickly obtain a large number of training samples from a specific area. Traditional scene classification researches were mainly using deep learning models to transfer natural images to RSI. Considering the differences between natural images and RSI, we trained several Super Resolution GAN models by using different resolution RSI data from Google earth image. This paper proposed a novel SRGAN-CNN framework. Through transferring the data with scene classification dataset to obtain high resolution fake RSI. The experimental results demonstrate that the proposed framework can enhance transfer effect and help improve the accuracy of scene classification using low resolution RSI. © 2020 IEEE.","Classification (of information); Deep learning; Geology; Optical resolving power; Remote sensing; Semantics; Adversarial networks; Different resolutions; High spatial resolution; Remote sensing images; Scene classification; Semantic category; Super resolution; Training sample; Image classification","deep learning; Image super resolution; remote sensing images; scene classification","Conference paper","Final","","Scopus","2-s2.0-85102001018"
"Ma C.; Rao Y.; Cheng Y.; Chen C.; Lu J.; Zhou J.","Ma, Cheng (8914125600); Rao, Yongming (57200621657); Cheng, Yean (57219598722); Chen, Ce (57219595794); Lu, Jiwen (14065346100); Zhou, Jie (56939394300)","8914125600; 57200621657; 57219598722; 57219595794; 14065346100; 56939394300","Structure-preserving super resolution with gradient guidance","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156994","7766","7775","9","10.1109/CVPR42600.2020.00779","151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094109658&doi=10.1109%2fCVPR42600.2020.00779&partnerID=40&md5=77b4b30bac27fe43a9ab2c55db11091a","Structures matter in single image super resolution (SISR). Recent studies benefiting from generative adversarial network (GAN) have promoted the development of SISR by recovering photo-realistic images. However, there are always undesired structural distortions in the recovered images. In this paper, we propose a structure-preserving super resolution method to alleviate the above issue while maintaining the merits of GAN-based methods to generate perceptual-pleasant details. Specifically, we exploit gradient maps of images to guide the recovery in two aspects. On the one hand, we restore high-resolution gradient maps by a gradient branch to provide additional structure priors for the SR process. On the other hand, we propose a gradient loss which imposes a second-order restriction on the super-resolved images. Along with the previous image-space loss functions, the gradient-space objectives help generative networks concentrate more on geometric structures. Moreover, our method is model-agnostic, which can be potentially used for off-the-shelf SR networks. Experimental results show that we achieve the best PI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with state-of-the-art perceptual-driven SR methods. Visual results demonstrate our superiority in restoring structures while generating natural SR images. © 2020 IEEE.","Image resolution; Pattern recognition; Recovery; Additional structures; Adversarial networks; Geometric structure; Photorealistic images; Structural distortions; Structure-preserving; Super resolution; Superresolution methods; Optical resolving power","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094109658"
"Yurt M.; Cukur T.","Yurt, Mahmut (57211187325); Cukur, Tolga (23034054800)","57211187325; 23034054800","Multi-Image Super Resolution in Multi-Contrast MRI; [Coklu Kontrast MRG'de Coklu Super Cozunurlugu]","2020","2020 28th Signal Processing and Communications Applications Conference, SIU 2020 - Proceedings","","","9302325","","","","10.1109/SIU49456.2020.9302325","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100296594&doi=10.1109%2fSIU49456.2020.9302325&partnerID=40&md5=3d2cebb6aac639a5885ee4dfebfb9356","Acquisition of high-resolution magnetic resonance images (MRI) under distinct contrasts can enhance diagnostic information required in clinical diagnosis. Yet, acquiring highresolution images might be impractical due to increased noise, prolonged scan durations and hardware costs. In such situations, an alternative solution can be the synthesis of high-resolution images from low-resolution images. Common methods perform super resolution of a single image. However, in multi-contrast MRI, the images of a single contrast might not contain sufficient prior information required for a successful deblurring. To enhance the required prior information, complementary prior information available in other contrasts can be used. Here, a multi-contrast MRI super resolution method is proposed to simultaneously deblur the images of multiple distinct contrasts. The proposed method relies on generative adversarial networks that can produce as realistic images as possible by better recovering high-frequency details. Qualitative and quantitative evaluations on a multi-contrast MRI dataset demonstrated that the proposed method outperforms the alternative single image MRI super resolution method.  © 2020 IEEE.","Diagnosis; Magnetic resonance; Magnetic resonance imaging; Optical resolving power; Adversarial networks; Alternative solutions; Clinical diagnosis; High resolution image; Low resolution images; Magnetic resonance images (MRI); Quantitative evaluation; Superresolution methods; Image enhancement","deblurring; multi-contrast MRI; super resolution","Conference paper","Final","","Scopus","2-s2.0-85100296594"
"Song L.; Lam E.Y.","Song, Li (57220187984); Lam, Edmund Y. (55694417300)","57220187984; 55694417300","MBD-GAN: Model-based image deblurring with a generative adversarial network","2020","Proceedings - International Conference on Pattern Recognition","","","9411979","7306","7313","7","10.1109/ICPR48806.2021.9411979","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110422752&doi=10.1109%2fICPR48806.2021.9411979&partnerID=40&md5=f867c49e0835531398321fd4a658e0bf","This paper presents a methodology to tackle inverse imaging problems by leveraging the synergistic power of imaging model and deep learning. The premise is that while learning-based techniques have quickly become the methods of choice in various applications, they often ignore the prior knowledge embedded in imaging models. Incorporating the latter has the potential to improve the image estimation. Specifically, we first provide a mathematical basis of using generative adversarial network (GAN) in inverse imaging through considering an optimization framework. Then, we develop the specific architecture that connects the generator and discriminator networks with the imaging model. While this technique can be applied to a variety of problems, from image reconstruction to super-resolution, we take image deblurring as the example here, where we show in detail the implementation and experimental results of what we call the model-based deblurring GAN (MBD-GAN). © 2020 IEEE","Deep learning; Image reconstruction; Inverse problems; Learning systems; Pattern recognition; Adversarial networks; Image deblurring; Image estimation; Imaging problems; Mathematical basis; Model-based OPC; Optimization framework; Super resolution; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85110422752"
"Sim B.; Oh G.; Kim J.; Jung C.; Ye J.C.","Sim, Byeongsu (57217028250); Oh, Gyutaek (57217029219); Kim, Jeongsol (57221447940); Jung, Chanyong (57200548859); Ye, Jong Chul (57223117835)","57217028250; 57217029219; 57221447940; 57200548859; 57223117835","Optimal transport driven CycleGAN for unsupervised learning in inverse problems","2020","SIAM Journal on Imaging Sciences","13","4","","2281","2306","25","10.1137/20M1317992","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098825056&doi=10.1137%2f20M1317992&partnerID=40&md5=d0e5ae92fe454ba7b6e7a6ea27080635","To improve the performance of classical generative adversarial networks (GANs), Wasserstein generative adversarial networks (WGANs) were developed as a Kantorovich dual formulation of the optimal transport (OT) problem using Wasserstein-1 distance. However, it was not clear how CycleGANtype generative models can be derived from the OT theory. Here we show that a novel CycleGAN architecture can be derived as a Kantorovich dual OT formulation if a penalized least squares (PLS) cost with deep learning--based inverse path penalty is used as a transportation cost. One of the most important advantages of this formulation is that depending on the knowledge of the forward problem, distinct variations of CycleGAN architecture can be derived: for example, one with two pairs of generators and discriminators, and the other with only a single pair of generator and discriminator. Even for the two generator cases, we show that the structural knowledge of the forward operator can lead to a simpler generator architecture which significantly simplifies the neural network training. The new CycleGAN formulation, which we call the OT-CycleGAN, has been applied for various biomedical imaging problems, such as accelerated magnetic resonance imaging (MRI), super-resolution microscopy, and low-dose X-ray computed tomography (CT). Experimental results confirm the efficacy and flexibility of the theory. © 2020 Society for Industrial and Applied Mathematics.","Computerized tomography; Deep learning; Magnetic resonance imaging; Medical imaging; Network architecture; Personnel training; Adversarial networks; Biomedical imaging; Neural network training; Penalized least-squares; Structural knowledge; Super-resolution microscopy; Transportation cost; X-ray computed tomography; Inverse problems","CycleGAN; Inverse problems; Optimal transport; Penalized least squares; Unsupervised learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098825056"
"Zhang S.; Cheng D.; Jiang D.; Kou Q.","Zhang, Sanyou (57208152094); Cheng, Deqiang (23388602000); Jiang, Daihong (53663736200); Kou, Qiqi (57200163455)","57208152094; 23388602000; 53663736200; 57200163455","Least squares relativistic generative adversarial network for perceptual super-resolution imaging","2020","IEEE Access","8","","","185198","185208","10","10.1109/ACCESS.2020.3030044","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102802777&doi=10.1109%2fACCESS.2020.3030044&partnerID=40&md5=a3743bb7aa7ca63307220b219f938f0f","Currently, deep-learning-based methods have been the most popular super-resolution techniques owing to the improvement of super-resolution performance. However, they are still lack perceptual fine details and thus result in unsatisfying visual quality. This article proposes a novel method for highquality perceptual super-resolution imaging, named SRLRGAN-SN. It aims to recovery visually plausible images with perceptual texture details by using the least squares relativistic generative adversarial network (GAN). The method applies the spectral normalization on the network with the target of enhancing the performance of GAN for super-resolution task. The least squares relativistic discriminator is designed to drive reconstruction images approximating high-quality perceptual manifold. Besides, a novel perceptual loss assembly is proposed to preserve structural texture details as much as possible. Results of experiment showthat our method can not only recovery more visually realistic details, but also outperforms other popular methods regarding to quantitative metrics and perceptual evaluations. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Computer system recovery; Deep learning; Optical resolving power; Textures; Adversarial networks; Learning-based methods; Perceptual evaluation; Quantitative metrics; Reconstruction image; Spectral normalization; Structural textures; Super resolution imaging; Image processing","Generative adversarial network; Perceptual quality; Relativistic discriminator; Spectral normalization; Super-resolution imaging","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102802777"
"Chen H.; He X.; Teng Q.; Sheriff R.E.; Feng J.; Xiong S.","Chen, Honggang (57051696700); He, Xiaohai (9237988800); Teng, Qizhi (7005503530); Sheriff, Raymond E. (7004255264); Feng, Junxi (57194450356); Xiong, Shuhua (8666446800)","57051696700; 9237988800; 7005503530; 7004255264; 57194450356; 8666446800","Super-resolution of real-world rock microcomputed tomography images using cycle-consistent generative adversarial networks","2020","Physical Review E","101","2","023305","","","","10.1103/PhysRevE.101.023305","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082098865&doi=10.1103%2fPhysRevE.101.023305&partnerID=40&md5=32804a4868dbb12686c0ec96cbdd6289","Digital rock imaging plays an important role in studying the microstructure and macroscopic properties of rocks, where microcomputed tomography (MCT) is widely used. Due to the inherent limitations of MCT, a balance should be made between the field of view (FOV) and resolution of rock MCT images a large FOV at low resolution (LR) or a small FOV at high resolution (HR). However, large FOV and HR are both expected for reliable analysis results in practice. Super-resolution (SR) is an effective solution to break through the mutual restriction between the FOV and resolution of rock MCT images, for it can reconstruct an HR image from a LR observation. Most of the existing SR methods cannot produce satisfactory HR results on real-world rock MCT images. One of the main reasons for this is that paired images are usually needed to learn the relationship between LR and HR rock images. However, it is challenging to collect such a dataset in a real scenario. Meanwhile, the simulated datasets may be unable to accurately reflect the model in actual applications. To address these problems, we propose a cycle-consistent generative adversarial network (CycleGAN)-based SR approach for real-world rock MCT images, namely, SRCycleGAN. In the off-line training phase, a set of unpaired rock MCT images is used to train the proposed SRCycleGAN, which can model the mapping between rock MCT images at different resolutions. In the on-line testing phase, the resolution of the LR input is enhanced via the learned mapping by SRCycleGAN. Experimental results show that the proposed SRCycleGAN can greatly improve the quality of simulated and real-world rock MCT images. The HR images reconstructed by SRCycleGAN show good agreement with the targets in terms of both the visual quality and the statistical parameters, including the porosity, the local porosity distribution, the two-point correlation function, the lineal-path function, the two-point cluster function, the chord-length distribution function, and the pore size distribution. Large FOV and HR rock MCT images can be obtained with the help of SRCycleGAN. Hence, this work makes it possible to generate HR rock MCT images that exceed the limitations of imaging systems on FOV and resolution. © 2020 American Physical Society.","Computerized tomography; Distribution functions; Image reconstruction; Image resolution; Mapping; Optical resolving power; Pore size; Rocks; Adversarial networks; Chord length distribution; Different resolutions; Local porosity distribution; Macroscopic properties; Microcomputed tomography; Statistical parameters; Two point correlation functions; Image enhancement","","Article","Final","","Scopus","2-s2.0-85082098865"
"Kim H.; Kim J.; Won S.; Lee C.","Kim, Hyojin (57219793321); Kim, Junhyuk (57191686104); Won, Sungjin (57216970226); Lee, Changhoon (7410152282)","57219793321; 57191686104; 57216970226; 7410152282","Unsupervised deep learning for super-resolution reconstruction of turbulence","2020","Journal of Fluid Mechanics","","","","","","","10.1017/jfm.2020.1028","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099471559&doi=10.1017%2fjfm.2020.1028&partnerID=40&md5=774bfc402dc4cefd7c9e5015cfd3225d","Recent attempts to use deep learning for super-resolution reconstruction of turbulent flows have used supervised learning, which requires paired data for training. This limitation hinders more practical applications of super-resolution reconstruction. Therefore, we present an unsupervised learning model that adopts a cycle-consistent generative adversarial network (CycleGAN) that can be trained with unpaired turbulence data for super-resolution reconstruction. Our model is validated using three examples: (i) recovering the original flow field from filtered data using direct numerical simulation (DNS) of homogeneous isotropic turbulence; (ii) reconstructing full-resolution fields using partially measured data from the DNS of turbulent channel flows; and (iii) generating a DNS-resolution flow field from large-eddy simulation (LES) data for turbulent channel flows. In examples (i) and (ii), for which paired data are available for supervised learning, our unsupervised model demonstrates qualitatively and quantitatively similar performance as that of the best supervised learning model. More importantly, in example (iii), where supervised learning is impossible, our model successfully reconstructs the high-resolution flow field of statistical DNS quality from the LES data. Furthermore, we find that the present model has almost universal applicability to all values of Reynolds numbers within the tested range. This demonstrates that unsupervised learning of turbulence data is indeed possible, opening a new door for the wide application of super-resolution reconstruction of turbulent fields. © 2021 CSIRO. All rights reserved.","Channel flow; Flow fields; Internet protocols; Large eddy simulation; Learning systems; Optical resolving power; Reynolds number; Supervised learning; Turbulence; Unsupervised learning; Adversarial networks; Full resolutions; High resolution; Homogeneous isotropic turbulence; Super resolution reconstruction; Turbulent channel flows; Turbulent fields; Deep learning","turbulence simulation","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85099471559"
"Hu S.; Zhang B.; Liang B.; Zhao E.; Lui S.","Hu, Shichao (57221078400); Zhang, Bin (57221086652); Liang, Beici (57199753576); Zhao, Ethan (57221091844); Lui, Simon (55204479500)","57221078400; 57221086652; 57199753576; 57221091844; 55204479500","Phase-aware music super-resolution using generative adversarial networks","2020","Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH","2020-October","","","4074","4078","4","10.21437/Interspeech.2020-2605","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098119999&doi=10.21437%2fInterspeech.2020-2605&partnerID=40&md5=99e128f7b0784380eb13558232d8faa5","Audio super-resolution is a challenging task of recovering the missing high-resolution features from a low-resolution signal. To address this, generative adversarial networks (GAN) have been used to achieve promising results by training the mappings between magnitudes of the low and high-frequency components. However, phase information is not well-considered for waveform reconstruction in conventional methods. In this paper, we tackle the problem of music super-resolution and conduct a thorough investigation on the importance of phase for this task. We use GAN to predict the magnitudes of the high-frequency components. The corresponding phase information can be extracted using either a GAN-based waveform synthesis system or a modified Griffin-Lim algorithm. Experimental results show that phase information plays an important role in the improvement of the reconstructed music quality. Moreover, our proposed method significantly outperforms other state-of-the-art methods in terms of objective evaluations. Copyright © 2020 ISCA","Speech communication; Adversarial networks; Conventional methods; High frequency components; Low and high frequencies; Objective evaluation; State-of-the-art methods; Waveform reconstruction; Waveform synthesis; Optical resolving power","Bandwidth expansion; Generative adversarial network; Music super-resolution; Phase estimation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85098119999"
"Wu Y.; Lan L.; Long H.; Kong G.; Duan X.; Xu C.","Wu, Yun (35096974000); Lan, Lin (57220188017); Long, Huiyun (35096515000); Kong, Guangqian (23389692200); Duan, Xun (22634238200); Xu, Changzhuan (57220189620)","35096974000; 57220188017; 35096515000; 23389692200; 22634238200; 57220189620","Image Super-Resolution Reconstruction Based on a Generative Adversarial Network","2020","IEEE Access","8","","9269971","215133","215144","11","10.1109/ACCESS.2020.3040424","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097199170&doi=10.1109%2fACCESS.2020.3040424&partnerID=40&md5=5f674563deb6af647e013be1008699fb","In the field of computer vision, super-resolution reconstruction techniques based on deep learning have undergone considerable advancement; however, certain limitations remain, such as insufficient feature extraction and blurred image generation. To address these problems, we propose an image super-resolution reconstruction model based on a generative adversarial network. First, we employ a dual network structure in the generator network to solve the problem of insufficient feature extraction. The dual network structure is divided into an upsample subnetwork and a refinement subnetwork, which upsample and optimize a low-resolution image, respectively. In a scene with large upscaling factors, this structure can reduce the negative effect of noise and enhance the utilization of high-frequency details, thereby generating high-quality reconstruction results. Second, to generate sharper super-resolution images, we use the perceptual loss, which exhibits a fast convergence and excellent visual effect, to guide the generator network training. We apply the ResNeXt-50-32 × 4d network, which has few parameters and a large depth, to calculate the loss to obtain a reconstructed super-resolution image that is highly realistic. Finally, we introduce the Wasserstein distance into the discriminator network to enhance the discrimination ability and stability of the model. Specifically, this distance is employed to eliminate the activation function in the last layer of the network and avoid the use of the logarithm in calculating the loss function. Extensive experiments on the DIV2K, Set5, Set14, and BSD100 datasets demonstrate the effectiveness of the proposed model.  © 2020 IEEE.","Deep learning; Extraction; Feature extraction; Network layers; Optical resolving power; Activation functions; Adversarial networks; Discrimination ability; High quality reconstruction; Image super-resolution reconstruction; Low resolution images; Super-resolution reconstruction techniques; Wasserstein distance; Image reconstruction","Deep learning; dual network structure; generative adversarial network; perceptual loss; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097199170"
"Ma J.; Yu J.; Liu S.; Chen L.; Li X.; Feng J.; Chen Z.; Zeng S.; Liu X.; Cheng S.","Ma, Jiabo (57211744322); Yu, Jingya (57211746028); Liu, Sibo (57218717636); Chen, Li (57214203134); Li, Xu (56193336100); Feng, Jie (57218715595); Chen, Zhixing (57218717933); Zeng, Shaoqun (55446488900); Liu, Xiuli (13005404300); Cheng, Shenghua (57191169469)","57211744322; 57211746028; 57218717636; 57214203134; 56193336100; 57218715595; 57218717933; 55446488900; 13005404300; 57191169469","PathSRGAN: Multi-Supervised Super-Resolution for Cytopathological Images Using Generative Adversarial Network","2020","IEEE Transactions on Medical Imaging","39","9","9036984","2920","2930","10","10.1109/TMI.2020.2980839","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090172152&doi=10.1109%2fTMI.2020.2980839&partnerID=40&md5=3d6db4313d4b3b3f0c99ff54cf34cf68","In the cytopathology screening of cervical cancer, high-resolution digital cytopathological slides are critical for the interpretation of lesion cells. However, the acquisition of high-resolution digital slides requires high-end imaging equipment and long scanning time. In the study, we propose a GAN-based progressive multi-supervised super-resolution model called PathSRGAN (pathology super-resolution GAN) to learn the mapping of real low-resolution and high-resolution cytopathological images. With respect to the characteristics of cytopathological images, we design a new two-stage generator architecture with two supervision terms. The generator of the first stage corresponds to a densely-connected U-Net and achieves 4\times to 10\times super resolution. The generator of the second stage corresponds to a residual-in-residual DenseBlock and achieves 10\times to 20\times super resolution. The designed generator alleviates the difficulty in learning the mapping from 4\times images to 20\times images caused by the great numerical aperture difference and generates high quality high-resolution images. We conduct a series of comparison experiments and demonstrate the superiority of PathSRGAN to mainstream CNN-based and GAN-based super-resolution methods in cytopathological images. Simultaneously, the reconstructed high-resolution images by PathSRGAN improve the accuracy of computer-aided diagnosis tasks effectively. It is anticipated that the study will help increase the penetration rate of cytopathology screening in remote and impoverished areas that lack high-end imaging equipment. © 1982-2012 IEEE.","Image Processing, Computer-Assisted; Computer aided diagnosis; Computer graphics; Mapping; Optical resolving power; Adversarial networks; Cervical cancers; High resolution image; Imaging equipment; Numerical aperture; Penetration rates; Super-resolution models; Superresolution methods; Article; cancer screening; computer assisted diagnosis; controlled study; convolutional neural network; cytology; cytopathology; deep learning; diagnostic accuracy; diagnostic test accuracy study; digital imaging; human; image analysis; image quality; image reconstruction; image registration; learning algorithm; pathology super resolution generative adversarial network; supervised machine learning; vision; image processing; Image enhancement","Cervical cancer; cytopathological images; generative adversarial learning; super resolution","Article","Final","","Scopus","2-s2.0-85090172152"
"Peng X.; Wang C.","Peng, Xujun (35198739900); Wang, Chao (57215737970)","35198739900; 57215737970","Building super-resolution image generator for OCR accuracy improvement","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12116 LNCS","","","145","160","15","10.1007/978-3-030-57058-3_11","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090099900&doi=10.1007%2f978-3-030-57058-3_11&partnerID=40&md5=78adc71b3ff3647b916d9b7fb5125e61","Super-resolving a low resolution (LR) document image can not only enhance the visual quality and readability of the text, but improve the optical character recognition (OCR) accuracy. However, even despite the ill-posed nature of image super-resolution (SR) problem, how do we treat the finer details of text with large upscale factors and suppress noises and artifacts at the same time, especially for low quality document images is still a challenging task. Thus, in order to boost the OCR accuracy, we propose a generative adversarial network (GAN) based framework in this paper, where a SR image generator and a document image quality discriminator are constructed. To obtain high quality SR document image, multiple losses are designed to encourage the generator to learn the structural properties of texts. Meanwhile, the quality discriminator is trained based on a relativistic loss function. Based on the proposed framework, the obtained SR document images not only maintain the details of textures but remove the background noises, which achieve better OCR performance on the public databases. The source codes and pre-trained models are available at https://gitlab.com/xujun.peng/doc-super-resolution. © Springer Nature Switzerland AG 2020.","Image resolution; Optical character recognition; Optical resolving power; Textures; Accuracy Improvement; Adversarial networks; Ill-posed natures; Image generators; Image super resolutions; Optical character recognition (OCR); Relativistic loss; Visual qualities; Image enhancement","GAN; OCR; Structural similarity; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85090099900"
"Han X.; He T.; Ong Y.-S.; Zhong Y.","Han, Xiaobing (55267754400); He, Tiantian (56403933800); Ong, Yew-Soon (7006735298); Zhong, Yanfei (12039673900)","55267754400; 56403933800; 7006735298; 12039673900","Precise object detection using adversarially augmented local/global feature fusion","2020","Engineering Applications of Artificial Intelligence","94","","103710","","","","10.1016/j.engappai.2020.103710","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088228053&doi=10.1016%2fj.engappai.2020.103710&partnerID=40&md5=7c98cb2fc7e7c624b2319b95047dca33","Object detection, which aims at recognizing or locating the objects of interest in remote sensing imagery with high spatial resolutions (HSR), plays a significant role in many real-world scenarios, e.g., environment monitoring, urban planning, civil infrastructure construction, disaster rescuing, and geographic image retrieval. As a long-lasting challenging problem in both machine learning and geoinformatics communities, many approaches have been proposed to tackle it. However, previous methods always overlook the abundant information embedded in the HSR remote sensing images. The effectiveness of these methods, e.g., accuracy of detection, is therefore limited to some extent. To overcome the mentioned challenge, in this paper, we propose a novel two-phase deep framework, dubbed GLGOD-Net, to effectively detect meaningful objects in HSR images. GLGOD-Net firstly attempts to learn the enhanced deep representations from super-resolution image data. Fully utilizing the augmented image representations, GLGOD-Net then learns the fused representations into which both local and global latent features are implanted. Such fused representations learned by GLGOD-Net can be used to precisely detect different objects in remote sensing images. The proposed framework has been extensively tested on a real-world HSR image dataset for object detection and has been compared with several strong baselines. The remarkable experimental results validate the effectiveness of GLGOD-Net. The success of GLGOD-Net not only advances the cutting-edge of image data analytics, but also promotes the corresponding applicability of deep learning in remote sensing imagery. © 2020","Data Analytics; Deep learning; Image enhancement; Image retrieval; Object recognition; Remote sensing; Civil infrastructures; Environment monitoring; Geographic image retrieval; High spatial resolution; Real-world scenario; Remote sensing imagery; Remote sensing images; Super resolution; Object detection","Data augmentation; Geospatial object detection; High spatial resolution (HSR) remote sensing imagery; Local/global feature fusion; Super resolution generative adversarial network","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85088228053"
"Xu W.; Luo W.; Wang Y.; You Y.","Xu, Wenjiang (56818150300); Luo, Weiyi (57217669303); Wang, Yu (57217672132); You, Yancheng (14219921700)","56818150300; 57217669303; 57217672132; 14219921700","Data-driven three-dimensional super-resolution imaging of a turbulent jet flame using a generative adversarial network","2020","Applied Optics","59","19","","5729","5736","7","10.1364/AO.392803","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087473464&doi=10.1364%2fAO.392803&partnerID=40&md5=8e78a3b4cb190e856cc5e9d998d4cfc5","Three-dimensional (3D) computed tomography (CT) is becoming a well-established tool for turbulent combustion diagnostics. However, the 3D CT technique suffers from contradictory demands of spatial resolution and domain size. This work therefore reports a data-driven 3D super-resolution approach to enhance the spatial resolution by two times along each spatial direction. The approach, named 3D super-resolution generative adversarial network (3D-SR-GAN), builds a generator and a discriminator network to learn the topographic information and infer high-resolution3Dturbulent flame structure with a given low-resolution counterpart. This work uses numerically simulated3Dturbulent jet flame structures as training data to update model parameters of theGANnetwork. Extensive performance evaluations are then conducted to show the superiority of the proposed 3D-SR-GAN network, compared with other direct interpolation methods. The results show that a convincing super-resolution (SR) operation with the overall error of ~4% and the peak signal-to-noise ratio of 37 dB can be reached with an upscaling factor of 2, representing an eight times enhancement of the total voxel number.Moreover, the trained network can predict the SR structure of the jet flame with a differentReynolds number without retraining the network parameters. ©2020 Optical Society of America.","Computerized tomography; Image resolution; Optical resolving power; Signal to noise ratio; Training aircraft; Adversarial networks; Direct interpolation method; Peak signal to noise ratio; Super resolution imaging; Three-dimensional (3D) computed tomography; Topographic information; Turbulent combustion; Turbulent jet flames; article; flame; signal noise ratio; simulation; Combustion","","Article","Final","","Scopus","2-s2.0-85087473464"
"Kumar N.P.; Kruthi Y.M.; George P.A.; Anouksha K.S.; Likhitha A.","Kumar, Niharika P. (55464661700); Kruthi, Y.M. (57217856943); George, P. Anjali (57217859290); Anouksha, K.S. (57217850047); Likhitha, A. (57554578900)","55464661700; 57217856943; 57217859290; 57217850047; 57554578900","Sketch to image generation using generative adversarial network","2020","Journal of Critical Reviews","7","11","","767","771","4","10.31838/jcr.07.11.137","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087684491&doi=10.31838%2fjcr.07.11.137&partnerID=40&md5=c917c47fcf5f76d60851d49db81fdd1b","A Generative Adversarial Network is primarily based on the competition between the two neural networks, namely the generator and the discriminator. This technique is utilized to develop a system that produces realistic images based on the black and white sketch that is provided by the user. Sketches made by novice artists can be converted to colorful images with this system. Each time the user uploads the same sketch, slight changes in the color perspective can be achieved. The model has been trained and tested for the generation of shoes and handbags with an epoch range of 100 for training the model. Since it is a versatile network, given any dataset it can generate images accordingly. The model takes 50 seconds on an average to generate an image from a sketch. The quality of the images has been tested based on Peak Signal to Noise Ratio and Structural Similarity Index. This paper proposes an idea of including Super Resolution Convolutional Neural Network (SRCNN) to the deployed model. The model is presented with the help of a Graphical User Interface (GUI).  © 2020 by Advance Scientific Research.","","Convolutional Neural Network; Generative Adversarial Network; Neural Network; Sketch to Image","Article","Final","","Scopus","2-s2.0-85087684491"
"Zareapoor M.; Zhou H.; Yang J.","Zareapoor, Masoumeh (56349635100); Zhou, Huiyu (23062556900); Yang, Jie (15039078800)","56349635100; 23062556900; 15039078800","Perceptual image quality using dual generative adversarial network","2020","Neural Computing and Applications","32","18","","14521","14531","10","10.1007/s00521-019-04239-0","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066027567&doi=10.1007%2fs00521-019-04239-0&partnerID=40&md5=3ee4734b2afd71462c508d6971702721","Generative adversarial networks have received a remarkable success in many computer vision applications for their ability to learn from complex data distribution. In particular, they are capable to generate realistic images from latent space with a simple and intuitive structure. The main focus of existing models has been improving the performance; however, there is a little attention to make a robust model. In this paper, we investigate solutions to the super-resolution problems—in particular perceptual quality—by proposing a robust GAN. The proposed model unlike the standard GAN employs two generators and two discriminators in which, a discriminator determines that the samples are from real data or generated one, while another discriminator acts as classifier to return the wrong samples to its corresponding generators. Generators learn a mixture of many distributions from prior to the complex distribution. This new methodology is trained with the feature matching loss and allows us to return the wrong samples to the corresponding generators, in order to regenerate the real-look samples. Experimental results in various datasets show the superiority of the proposed model compared to the state of the art methods. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Classification (of information); Image processing; Adversarial networks; Complex data distributions; Computer vision applications; Data distribution; Perceptual image quality; Perceptual quality; Realistic images; State-of-the-art methods; Complex networks","Classification; Data distribution; Generative adversarial network; Image processing; Perceptual quality","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85066027567"
"Dou H.; Chen C.; Hu X.; Xuan Z.; Hu Z.; Peng S.","Dou, Hao (57209881441); Chen, Chen (57194185980); Hu, Xiyuan (7404710258); Xuan, Zuxing (15046351200); Hu, Zhisen (57219739735); Peng, Silong (7403590415)","57209881441; 57194185980; 7404710258; 15046351200; 57219739735; 7403590415","PCA-SRGAN: Incremental Orthogonal Projection Discrimination for Face Super-resolution","2020","MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia","","","","1891","1899","8","10.1145/3394171.3413590","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101222044&doi=10.1145%2f3394171.3413590&partnerID=40&md5=7b0163255bad1fff2b72562990bdc8e8","Generative Adversarial Networks (GANs) have been employed for face super resolution but they bring distorted facial details easily and still have weakness on recovering realistic texture. To further improve the performance of GAN-based models on super-resolving face images, we propose PCA-SRGAN which pays attention to the cumulative discrimination in the orthogonal projection space spanned by PCA projection matrix of face data. By feeding the principal component projections ranging from structure to details into the discriminator, the discrimination difficulty will be greatly alleviated and the generator can be enhanced to reconstruct clearer contour and finer texture, helpful to achieve the high perception and low distortion eventually. This incremental orthogonal projection discrimination has ensured a precise optimization procedure from coarse to fine and avoids the dependence on the perceptual regularization. We conduct experiments on CelebA and FFHQ face datasets. The qualitative visual effect and quantitative evaluation have demonstrated the overwhelming performance of our model over related works.  © 2020 ACM.","Optical resolving power; Textures; Adversarial networks; Coarse to fine; Face super-resolution; Optimization procedures; Orthogonal projection; Principal Components; Projection matrix; Quantitative evaluation; Image enhancement","cumulative learning; face super-resolution; GAN; PCA","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101222044"
"Hall J.; Bocanegra M.G.; Haddad R.J.","Hall, Justin (57215930273); Bocanegra, Maria Gonzalez (57222759462); Haddad, Rami J. (56245996200)","57215930273; 57222759462; 56245996200","Optimizing generative adversarial networks for low-resolution image enhancement","2020","Conference Proceedings - IEEE SOUTHEASTCON","2","","9368265","","","","10.1109/SoutheastCon44009.2020.9368265","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104011896&doi=10.1109%2fSoutheastCon44009.2020.9368265&partnerID=40&md5=1533882f35b31b2eeb2863917553e44f","Current high-resolution camera and video systems require expensively complex equipment and an excessive amount of digital storage to function, effectively limiting their practicality and availability. The research seeks to address this issue by optimizing Generative Adversarial Networks (GANs) for image super-resolution using an evolutionary-based scheme for successive network modification. The capability of GANs to selectively enhance the resolution of a desired image without increasing costs is extremely significant for a substantial range of industries and practices. The network's capabilities are expressed in terms of produced image quality and network metrics, with preference given to increases in image quality. Using the DIV2K dataset as common input, the highest performing network achieved a comparative increase of 2.22 dB PSNR (29.1%) over the base model, the Super-Resolution Generative Adversarial Network (SRGAN). The most notable contribution to network enhancement with regards to image quality was caused by removing the batch normalization layers from the Generator network. Network performance with regards to subjective image quality was most affected by the inclusion of a second convolutional layer in each residual block of the modified SRGAN Generator. Possible applications of the improved system include enhancing images of license plates for traffic monitoring systems and improving still images from body camera footage to reveal crucial details. Beyond the scope of surveillance, resolution-enhancing GANs may be applied to develop media content or national defense capabilities. © 2020 IEEE","Cameras; Digital storage; Image quality; Optical resolving power; Adversarial networks; Complex equipment; High resolution camera; Image super resolutions; Increasing costs; Low resolution images; Subjective image quality; Traffic monitoring systems; Image enhancement","Deep learning; DIV2K; GAN; Image enhancement; Network architecture; Optimization; PSNR; Pytorch; SRGAN; SSIM; Super resolution; Training parameters","Conference paper","Final","","Scopus","2-s2.0-85104011896"
"Cao J.; Zhang Z.; Zhao A.","Cao, Jianfang (36175317900); Zhang, Zibang (55795137100); Zhao, Aidi (57214235963)","36175317900; 55795137100; 57214235963","Application of a Modified Generative Adversarial Network in the Superresolution Reconstruction of Ancient Murals","2020","Computational Intelligence and Neuroscience","2020","","6670976","","","","10.1155/2020/6670976","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099291345&doi=10.1155%2f2020%2f6670976&partnerID=40&md5=8eb9c5ccedebc180451ec9d42cd49da5","Considering the problems of low resolution and rough details in existing mural images, this paper proposes a superresolution reconstruction algorithm for enhancing artistic mural images, thereby optimizing mural images. The algorithm takes a generative adversarial network (GAN) as the framework. First, a convolutional neural network (CNN) is used to extract image feature information, and then, the features are mapped to the high-resolution image space of the same size as the original image. Finally, the reconstructed high-resolution image is output to complete the design of the generative network. Then, a CNN with deep and residual modules is used for image feature extraction to determine whether the output of the generative network is an authentic, high-resolution mural image. In detail, the depth of the network increases, the residual module is introduced, the batch standardization of the network convolution layer is deleted, and the subpixel convolution is used to realize upsampling. Additionally, a combination of multiple loss functions and staged construction of the network model is adopted to further optimize the mural image. A mural dataset is set up by the current team. Compared with several existing image superresolution algorithms, the peak signal-to-noise ratio (PSNR) of the proposed algorithm increases by an average of 1.2-3.3 dB and the structural similarity (SSIM) increases by 0.04 = 0.13; it is also superior to other algorithms in terms of subjective scoring. The proposed method in this study is effective in the superresolution reconstruction of mural images, which contributes to the further optimization of ancient mural images.  © 2020 Jianfang Cao et al.","Algorithms; Image Processing, Computer-Assisted; Neural Networks, Computer; Signal-To-Noise Ratio; Tomography, X-Ray Computed; Convolution; Convolutional neural networks; Image reconstruction; Network layers; Optical resolving power; Signal to noise ratio; Adversarial networks; High resolution image; Image feature extractions; Image super-resolution; Peak signal to noise ratio; Staged construction; Structural similarity; Super-resolution reconstruction; algorithm; image processing; signal noise ratio; x-ray computed tomography; Image enhancement","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85099291345"
"","","","6th Conference on Signal and Information Processing, Networking and Computers, ICSINC 2019","2020","Lecture Notes in Electrical Engineering","628 LNEE","","","","","917","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085187386&partnerID=40&md5=a7fad39c9e7484dbc5cd77685e1940ec","The proceedings contain 109 papers. The special focus in this conference is on Signal and Information Processing, Networking and Computers. The topics include: Security Service Chain Based on SDN&NFV Construction Technology; a Fast Prediction Mode Selection Algorithm Based on Spatio-Temporal Correlation of Inter-prediction and Intra-prediction Blocks; malicious Network Traffic Recognition Method Based on Deep Learning; Optimizing Method of AVS2 Decoder for Mobile Terminal; optimizing Technology in Video Coding and Decoding; Overview of eMBMS Technique; rate Control Technology in Video Coding; the Quantization Parameter Adjustment Scheme Based on Prediction Mode of Coded Prediction Block; person Re-identification Based on Semantic Segmentation; ILL-Dataset: A Dataset Under Different Illumination Conditions for Face Recognition; a Secure and Credible Supply Chain System Based on Blockchain; super-Resolution Direction-of-Arrival Estimation with Atomic Norm Minimization; research on Face Recognition Algorithms Based on Deep Convolution Generative Adversarial Networks; mask Neural Network for Predicting Flight Ticket Price; Adaptive High-Resolution Channel Estimation Approach for Millimeter Wave MIMO Systems; practical Application of Requirements Engineering Management in Aerospace Ground Software; experimental Research on Discharge Mode of Helicon Plasma Thruster; modeling the Attitude of Satellite Which Has Unfolding Procedure of the Loop Antenna; adjustment Method of Parallel Tanks Balanced Consumption on Satellite; imbalance-Eliminate Loss in Siamese Networks for Arbitrary Object Tracking; research on On-orbit Acceptable Service Satellite Control System Based on the Time-Trigger; conversion of Shock Spectra with Different Q Values; the Design and Analysis on Structure of Helicon Wave Electric Propulsion System on Satellite; Development of UAV Traceability System Based on Data Analysis.","","","Conference review","Final","","Scopus","2-s2.0-85085187386"
"Muhammad Umer R.; Luca Foresti G.; Micheloni C.","Muhammad Umer, Rao (57211291072); Luca Foresti, Gian (7006427233); Micheloni, Christian (6507976201)","57211291072; 7006427233; 6507976201","Deep generative adversarial residual convolutional networks for real-world super-resolution","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9150742","1769","1777","8","10.1109/CVPRW50498.2020.00227","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090136181&doi=10.1109%2fCVPRW50498.2020.00227&partnerID=40&md5=d201296a4232a3bc6043e113c20b1a7b","Most current deep learning based single image super-resolution (SISR) methods focus on designing deeper / wider models to learn the non-linear mapping between low-resolution (LR) inputs and the high-resolution (HR) outputs from a large number of paired (LR/HR) training data. They usually take as assumption that the LR image is a bicubic down-sampled version of the HR image. However, such degradation process is not available in real-world settings i.e. inherent sensor noise, stochastic noise, compression artifacts, possible mismatch between image degradation process and camera device. It reduces significantly the performance of current SISR methods due to real-world image corruptions. To address these problems, we propose a deep Super-Resolution Residual Convolutional Generative Adversarial Network (SRResCGAN1) to follow the real-world degradation settings by adversarial training the model with pixel-wise supervision in the HR domain from its generated LR counterpart. The proposed network exploits the residual learning by minimizing the energy-based objective function with powerful image regularization and convex optimization techniques. We demonstrate our proposed approach in quantitative and qualitative experiments that generalize robustly to real input and it is easy to deploy for other downscaling operators and mobile/embedded devices. © 2020 IEEE.","Computer vision; Convex optimization; Convolution; Deep learning; Optical resolving power; Stochastic systems; Adversarial networks; Compression artifacts; Convex optimization techniques; Convolutional networks; Image degradation process; Image regularization; Objective functions; Qualitative experiments; Convolutional neural networks","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090136181"
"Mostofa M.; Ferdous S.N.; Nasrabadi N.M.","Mostofa, Moktari (57211073809); Ferdous, Syeda Nyma (57211070813); Nasrabadi, Nasser M. (7006312852)","57211073809; 57211070813; 7006312852","A joint cross-modal super-resolution approach for vehicle detection in aerial imagery","2020","Proceedings of SPIE - The International Society for Optical Engineering","11413","","114130O","","","","10.1117/12.2558275","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089178482&doi=10.1117%2f12.2558275&partnerID=40&md5=9b48aa3a03ef2e24b93a8926836e9aae","Vehicle detection in aerial imagery is still an open research challenge although it has received some breakthroughs in the computer vision research community. Most of the existing state-of-the-art vehicle detection algorithms have ignored to consider some major factors which may have a great influence on the detection task. The low-resolution characteristic of aerial images is considered one of the major factors. Although the super-resolution technique can resolve this problem which learns a mapping between the low-resolution (LR) images and their corresponding high-resolution (HR) counterparts, however, the problem still remains when detection needs to take place at night or in a dark environment. Therefore, RGB-based detection can be another vital problem specifically for the detection task in a dark environment. For such environment infrared (IR) imaging becomes necessary which again may not be available during training an IR detector. To address these challenges, we propose a joint cross-modal and super-resolution framework based on the Generative Adversarial Network (GAN) for vehicle detection in aerial images. Our proposed joint network consists of two deep sub-networks. The first sub-network utilizes the GAN architecture to generate super-resolved (SR) images across two different domains (cross-domain translation). The second sub-network performs detection on these cross-domain translated and super-resolved images using one of the state-of-the-art object detectors i.e., You Only Look Once version 3 (YOLOv3). To evaluate the efficacy of our proposed model, we conduct several experiments on a publicly available Vehicle Detection in Aerial Imagery (VEDAI) dataset. We further compare our proposed network with state-of-the-art image generation methods to show the adequacy of our model. © 2020 SPIE.","Aerial photography; Antennas; Machine learning; Optical resolving power; Vehicles; Adversarial networks; Different domains; Low resolution images; Object detectors; Research challenges; State of the art; Super resolution; Vehicle detection; Object detection","Generative Adversarial Network (GAN); Infrared (IR); Super-resolved (SR); You Only Look Onceversion3 (YOLOv3)","Conference paper","Final","","Scopus","2-s2.0-85089178482"
"Anwar S.; Khan S.; Barnes N.","Anwar, Saeed (57189657913); Khan, Salman (56415451600); Barnes, Nick (55515924300)","57189657913; 56415451600; 55515924300","A Deep Journey into Super-resolution: A Survey","2020","ACM Computing Surveys","53","3","60","","","","10.1145/3390462","110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088644323&doi=10.1145%2f3390462&partnerID=40&md5=40860209b6646078d0170013c7f3dc9b","Deep convolutional networks-based super-resolution is a fast-growing field with numerous practical applications. In this exposition, we extensively compare more than 30 state-of-the-art super-resolution Convolutional Neural Networks (CNNs) over three classical and three recently introduced challenging datasets to benchmark single image super-resolution. We introduce a taxonomy for deep learning-based super-resolution networks that groups existing methods into nine categories including linear, residual, multi-branch, recursive, progressive, attention-based, and adversarial designs. We also provide comparisons between the models in terms of network complexity, memory footprint, model input and output, learning details, the type of network losses, and important architectural differences (e.g., depth, skip-connections, filters). The extensive evaluation performed shows the consistent and rapid growth in the accuracy in the past few years along with a corresponding boost in model complexity and the availability of large-scale datasets. It is also observed that the pioneering methods identified as the benchmarks have been significantly outperformed by the current contenders. Despite the progress in recent years, we identify several shortcomings of existing techniques and provide future research directions towards the solution of these open problems. Datasets and codes for evaluation are publicly available at https://github.com/saeed-anwar/SRsurvey.  © 2020 ACM.","Complex networks; Convolution; Deep learning; Large dataset; Learning systems; Optical resolving power; Convolutional networks; Future research directions; Large-scale datasets; Learning-based super-resolution; Memory footprint; Model complexity; Network complexity; Super resolution; Convolutional neural networks","convolutional neural networks (CNNs); deep learning; generative adversarial networks (GANs); high-resolution (HR); Super-resolution (SR); survey","Review","Final","","Scopus","2-s2.0-85088644323"
"Tan C.; Zhu J.; Lio’ P.","Tan, Chuan (57693172100); Zhu, Jin (57202160439); Lio’, Pietro (7004223170)","57693172100; 57202160439; 7004223170","Arbitrary scale super-resolution for brain MRI images","2020","IFIP Advances in Information and Communication Technology","583 IFIP","","","165","176","11","10.1007/978-3-030-49161-1_15","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086261810&doi=10.1007%2f978-3-030-49161-1_15&partnerID=40&md5=9ad3d0092944419b5010e864ba61ba70","Recent attempts at Super-Resolution for medical images used deep learning techniques such as Generative Adversarial Networks (GANs) to achieve perceptually realistic single image Super-Resolution. Yet, they are constrained by their inability to generalise to different scale factors. This involves high storage and energy costs as every integer scale factor involves a separate neural network. A recent paper has proposed a novel meta-learning technique that uses a Weight Prediction Network to enable Super-Resolution on arbitrary scale factors using only a single neural network. In this paper, we propose a new network that combines that technique with SRGAN, a state-of-the-art GAN-based architecture, to achieve arbitrary scale, high fidelity Super-Resolution for medical images. By using this network to perform arbitrary scale magnifications on images from the Multimodal Brain Tumor Segmentation Challenge (BraTS) dataset, we demonstrate that it is able to outperform traditional interpolation methods by up to 20% on SSIM scores whilst retaining generalisability on brain MRI images. We show that performance across scales is not compromised, and that it is able to achieve competitive results with other state-of-the-art methods such as EDSR whilst being fifty times smaller than them. Combining efficiency, performance, and generalisability, this can hopefully become a new foundation for tackling Super-Resolution on medical images. © IFIP International Federation for Information Processing 2020.","Deep learning; Image segmentation; Learning algorithms; Learning systems; Medical imaging; Optical resolving power; Scales (weighing instruments); Adversarial networks; Brain mri images; Brain tumor segmentation; Combining efficiency; Interpolation method; Learning techniques; Meta-learning techniques; State-of-the-art methods; Magnetic resonance imaging","Image processing; Medical image analysis; Meta-Learning; Super-Resolution","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85086261810"
"Delannoy Q.; Pham C.-H.; Cazorla C.; Tor-Díez C.; Dollé G.; Meunier H.; Bednarek N.; Fablet R.; Passat N.; Rousseau F.","Delannoy, Quentin (57219717935); Pham, Chi-Hieu (57194836445); Cazorla, Clément (57211803565); Tor-Díez, Carlos (57204032876); Dollé, Guillaume (55955807000); Meunier, Hélène (57209603238); Bednarek, Nathalie (6602112368); Fablet, Ronan (6603508732); Passat, Nicolas (57207528590); Rousseau, François (57188780886)","57219717935; 57194836445; 57211803565; 57204032876; 55955807000; 57209603238; 6602112368; 6603508732; 57207528590; 57188780886","SegSRGAN: Super-resolution and segmentation using generative adversarial networks — Application to neonatal brain MRI","2020","Computers in Biology and Medicine","120","","103755","","","","10.1016/j.compbiomed.2020.103755","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083080289&doi=10.1016%2fj.compbiomed.2020.103755&partnerID=40&md5=1798926c53a7831291c373a508913711","Background and objective: One of the main issues in the analysis of clinical neonatal brain MRI is the low anisotropic resolution of the data. In most MRI analysis pipelines, data are first re-sampled using interpolation or single image super-resolution techniques and then segmented using (semi-)automated approaches. In other words, image reconstruction and segmentation are then performed separately. In this article, we propose a methodology and a software solution for carrying out simultaneously high-resolution reconstruction and segmentation of brain MRI data. Methods: Our strategy mainly relies on generative adversarial networks. The network architecture is described in detail. We provide information about its implementation, focusing on the most crucial technical points (whereas complementary details are given in a dedicated GitHub repository). We illustrate the behavior of the proposed method for cortex analysis from neonatal MR images. Results: The results of the method, evaluated quantitatively (Dice, peak signal-to-noise ratio, structural similarity, number of connected components) and qualitatively on a research dataset (dHCP) and a clinical one (Epirmex), emphasize the relevance of the approach, and its ability to take advantage of data-augmentation strategies. Conclusions: Results emphasize the potential of our proposed method/software with respect to practical medical applications. The method is provided as a freely available software tool, which allows one to carry out his/her own experiments, and involve the method for the super-resolution reconstruction and segmentation of arbitrary cerebral structures from any MR image dataset. © 2020 Elsevier Ltd","Clinical research; Image analysis; Image reconstruction; Image segmentation; Medical applications; Network architecture; Optical resolving power; Signal to noise ratio; Adversarial networks; Cerebral structures; Connected component; Freely available software; High-resolution reconstruction; Peak signal to noise ratio; Structural similarity; Super resolution reconstruction; article; brain cortex; controlled study; female; human; male; newborn; nuclear magnetic resonance imaging; quantitative analysis; signal noise ratio; software; Magnetic resonance imaging","3D generative adversarial networks; Cortex; Neonatal brain MRI; Segmentation; Super-resolution","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85083080289"
"Kim G.; Park J.; Lee K.; Lee J.; Min J.; Lee B.; Han D.K.; Ko H.","Kim, Gwantae (57218709502); Park, Jaihyun (57189090323); Lee, Kanghyu (57202578714); Lee, Junyeop (57206736110); Min, Jeongki (57214804771); Lee, Bokyeung (57218645150); Han, David K. (7403219841); Ko, Hanseok (35069749800)","57218709502; 57189090323; 57202578714; 57206736110; 57214804771; 57218645150; 7403219841; 35069749800","Unsupervised real-world super resolution with cycle generative adversarial network and domain discriminator","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9150730","1862","1871","9","10.1109/CVPRW50498.2020.00236","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090164893&doi=10.1109%2fCVPRW50498.2020.00236&partnerID=40&md5=8c529904b0f2de90aa8c755c6d4f219f","This paper proposes an unsupervised single-image Super-Resolution(SR) model using cycleGAN and domain discriminator to solve the problem of SR with unknown degradation using unpaired dataset. In previous approaches, paired dataset is required for training with assumed levels of image degradation. In real world SR applications, however, training sets are typically not of low and high resolution image pairs, but only low resolution images with unknown degradation are provided as inputs. To address the problem, we introduce a cycle-in-cycle GAN based unsupervised learning model using an unpaired dataset. In addition, we combine several losses attributed to image contents, such as pixel-wise loss, VGG feature loss and SSIM loss, for stable learning and performance improvement. We also propose a domain discriminator, which consists of noise discriminator, texture discriminator and color discriminator, to guide generated images to follow target domain distribution rather than source domain. We validate effectiveness of our model in quantitative and qualitative experiments using NTIRE2020 real-world SR challenge dataset. © 2020 IEEE.","Computer vision; Discriminators; Optical resolving power; Textures; Adversarial networks; High resolution image; Image degradation; Low resolution images; Qualitative experiments; Single images; Super resolution; Target domain; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85090164893"
"Dehzangi O.; Gheshlaghi S.H.; Amireskandari A.; Nasrabadi N.M.; Rezai A.","Dehzangi, Omid (23396296800); Gheshlaghi, Saba Heidari (57219544954); Amireskandari, Annahita (55135990500); Nasrabadi, Nasser M. (7006312852); Rezai, Ali (35479361100)","23396296800; 57219544954; 55135990500; 7006312852; 35479361100","OCT image segmentation using neural architecture search and SRGAN","2020","Proceedings - International Conference on Pattern Recognition","","","9412818","6425","6430","5","10.1109/ICPR48806.2021.9412818","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110468496&doi=10.1109%2fICPR48806.2021.9412818&partnerID=40&md5=7d7eab0975b4fe44b45f35a8becf43ff","Medical image segmentation is a critical field in the domain of computer vision and with the growing acclaim of deep learning based models, research in this field is constantly expanding. Optical coherence tomography (OCT) is a non-invasive method that scans the human's retina with depth. It has been hypothesized that the thickness of the retinal layers extracted from OCTs could be an efficient and effective biomarker for early diagnosis of AD. In this work, we aim to design a self-training model architecture for the task of segmenting the retinal layers in OCT scans. Neural architecture search (NAS) is a subfield of AutoML domain, which has a significant impact on improving the accuracy of machine vision tasks. We integrate the NAS algorithm with a Unet auto-encoder architecture as its backbone. Then, we employ our proposed model to segment the retinal nerve fiber layer in our preprocessed OCT images with the aim of AD diagnosis. In this work, we trained a super-resolution generative adversarial network on the raw OCT scans to improve the quality of the images before the modeling stage. In our architecture search strategy, different primitive operations suggested to find down- & up-sampling Unet cell blocks and the binary gate method has been applied to make the search strategy more practical. Our architecture search method is empirically evaluated by training on the Unet and NAS-Unet from scratch. Specifically, the proposed NAS-Unet training significantly outperforms the baseline human-designed architecture by achieving 95.1% in the mean Intersection over Union metric and 79.1% in the Dice similarity coefficient. © 2020 IEEE","Computer vision; Deep learning; Diagnosis; Image enhancement; Learning systems; Medical imaging; Network architecture; Noninvasive medical procedures; Ophthalmology; Optical tomography; Adversarial networks; Learning Based Models; Neural architectures; Noninvasive methods; Primitive operations; Retinal nerve fiber layers; Search strategies; Similarity coefficients; Image segmentation","","Conference paper","Final","","Scopus","2-s2.0-85110468496"
"","","","Proceedings - 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPRW 2020","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","","","","4505","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090119600&partnerID=40&md5=0e8678d14df7ef8327e194e385b7f4d8","The proceedings contain 523 papers. The topics discussed include: latent fingerprint image enhancement based on progressive generative adversarial network; zero-shot learning in the presence of hierarchically coarsened labels; multivariate confidence calibration for object detection; context-guided super-class inference for zero-shot detection; learning sparse ternary neural networks with entropy-constrained trained ternarization (EC2T); now that i can see, i can improve: enabling data-driven finetuning of CNNs on the edge; enhancing facial data diversity with style-based face aging; a simplified framework for zero-shot cross-modal sketch data retrieval; unsupervised single image super-resolution network (USISResNet) for real-world data using generative adversarial network; cross-regional oil palm tree detection; and leaf spot attention network for apple leaf disease identification.","","","Conference review","Final","","Scopus","2-s2.0-85090119600"
"Li C.; Wang L.; Cheng S.; Ao N.","Li, Can (57215847640); Wang, Liejun (16833826600); Cheng, Shuli (57203415051); Ao, Naixiang (43360940100)","57215847640; 16833826600; 57203415051; 43360940100","Generative adversarial network-based super-resolution considering quantitative and perceptual quality","2020","Symmetry","12","3","449","","","","10.3390/sym12030449","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082030007&doi=10.3390%2fsym12030449&partnerID=40&md5=dd35cd4f2c21dc9c90fdf09c20f4d1b4","In recent years, the common algorithms for image super-resolution based on deep learning have been increasingly successful, but there is still a large gap between the results generated by each algorithm and the ground-truth. Even some algorithms that are dedicated to image perception produce more textures that do not exist in the original image, and these artefacts also affect the visual perceptual quality of the image. We believe that in the existing perceptual-based image superresolution algorithm, it is necessary to consider Super-Resolution (SR) image quality, which can restore the important structural parts of the original picture. This paper mainly improves the Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) algorithm in the following aspects: adding a shallow network structure, adding the dual attention mechanism in the generator and the discriminator, including the second-order channel mechanism and spatial attention mechanism and optimizing perceptual loss by adding second-order covariance normalization at the end of feature extractor. The results of this paper ensure image perceptual quality while reducing image distortion and artefacts, improving the perceived similarity of images and making the images more in line with human visual perception. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","","Attention mechanism; Generative adversarial networks; Shallow network; Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85082030007"
"Liu D.; Zhao Z.-Q.; Tian W.-D.","Liu, Dian (57204761866); Zhao, Zhong-Qiu (55726118500); Tian, Wei-Dong (54788569600)","57204761866; 55726118500; 54788569600","TFPGAN: Tiny Face Detection with Prior Information and GAN","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12465 LNAI","","","62","73","11","10.1007/978-3-030-60796-8_6","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093933449&doi=10.1007%2f978-3-030-60796-8_6&partnerID=40&md5=9a6c2acd9b3525d60e09aa938948ffcb","This paper addresses two challenging tasks: detecting small faces in unconstrained conditions and improving the quality of very low-resolution facial images. Tiny faces are so fuzzy that the facial patterns are not clear or even ambiguous resulting in greatly reduced detection. In this paper, we proposed an algorithm to directly generate a clear high-resolution face from a blurry small one by adopting a generative adversarial network (GAN). Besides, we also designed a prior information estimation network which extracts the facial image features, and estimates landmark heatmaps respectively. By combining these two networks, we propose the end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the tiny faces. Extensive experiments on the challenging dataset WIDER FACE demonstrate the effectiveness of our proposed method in restoring a clear high-resolution face from a blurry small one, and show that the detection performance outperforms other state-of-the-art methods. © 2020, Springer Nature Switzerland AG.","Image enhancement; Intelligent computing; Adversarial networks; Detection performance; End-to-end systems; Facial images; High resolution; Low resolution; Prior information; State-of-the-art methods; Face recognition","Face detection; GAN; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85093933449"
"Fan Z.; Wang Z.; Xin J.; Wang Z.; Liu L.; Zhang X.; Liu J.","Fan, Zijia (57216897787); Wang, Zhongyang (57215071558); Xin, Junchang (23399268700); Wang, Zhiqiong (25522764100); Liu, Lu (57211252281); Zhang, Xia (57213541995); Liu, Jiren (56366377300)","57216897787; 57215071558; 23399268700; 25522764100; 57211252281; 57213541995; 56366377300","Dual-Enhanced Registration for Field of View Ultrasound Sonography","2020","IEEE Access","8","","9138393","128602","128612","10","10.1109/ACCESS.2020.3008525","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089491985&doi=10.1109%2fACCESS.2020.3008525&partnerID=40&md5=89111d86e9e0e8c1774910eb24c3f38a","Extended Field of View Ultrasound Sonography (EFOV-US) uses the existing ultrasound images for image stitching, so as to display the shape and scope of organ occupation and the relationship with surrounding tissues comprehensively. However, there are still some problems in Extended Field of View Ultrasound Sonography, such as matching error and unstable quality of image stitching. In view of these problems, we propose Dual-enhanced EFOV-US method that overcomes the limitation and produces higher quality results. Firstly, the gray enhancement method is used to improve the image contrast and reduce the noise interference. Then the super-resolution method based on the generative adversarial network is used to improve the resolution of the ultrasonic image further and increase the number of feature point matching between stitching images. The high quality ultrasound wide-range image is gotten by stitching and fusing the double enhanced image. The experimental results show that the proposed method is effective and practical.  © 2013 IEEE.","Ultrasonography; Adversarial networks; Extended field of views; Feature point matching; Image contrasts; Noise interference; Superresolution methods; Ultrasound images; Ultrasound sonographies; Image enhancement","Extended field of view ultrasound sonography; generative adversarial network; gray enhancement; image registration; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85089491985"
"Li L.; Li Y.; Wu C.; Dong H.; Jiang P.; Wang F.","Li, Ling (57203247307); Li, Yaochen (47761911900); Wu, Chuan (57211018583); Dong, Hang (56896569000); Jiang, Peilin (14024293700); Wang, Fei (56835831600)","57203247307; 47761911900; 57211018583; 56896569000; 14024293700; 56835831600","Detail fusion GaN: High-quality translation for unpaired images with GAN-based data augmentation","2020","Proceedings - International Conference on Pattern Recognition","","","9412542","1731","1736","5","10.1109/ICPR48806.2021.9412542","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110463367&doi=10.1109%2fICPR48806.2021.9412542&partnerID=40&md5=b3a3c10cae150a1a92a7d61a50388119","Image-to-image translation, a task to learn the mapping relation between two different domains, is a rapid-growing research field in deep learning. Although existing Generative Adversarial Network (GAN)-based methods have achieved decent results in this field, there are still some limitations in generating high-quality images for practical applications (e.g., data augmentation and image inpainting). In this work, we aim to propose a GAN-based network for data augmentation which can generate translated images with more details and less artifacts. The proposed Detail Fusion Generative Adversarial Network (DFGAN) consists of a detail branch, a transfer branch, a filter module, and a reconstruction module. The detail branch is trained by a super-resolution loss and its intermediate features can be used to introduce more details to the transfer branch by the filter module. Extensive evaluations demonstrate that our model generates more satisfactory images against the state-of-the-art approaches for data augmentation. © 2020 IEEE","Deep learning; Gallium nitride; III-V semiconductors; Adversarial networks; Data augmentation; Different domains; High quality images; Image translation; Mapping relation; State-of-the-art approach; Super resolution; Pattern recognition","","Conference paper","Final","","Scopus","2-s2.0-85110463367"
"Wan D.; Shen F.; Liu L.; Zhu F.; Huang L.; Yu M.; Shen H.T.; Shao L.","Wan, Diwen (57204392864); Shen, Fumin (55272525500); Liu, Li (56102788900); Zhu, Fan (58037232700); Huang, Lei (55264827400); Yu, Mengyang (56729527000); Shen, Heng Tao (7404523209); Shao, Ling (55643855000)","57204392864; 55272525500; 56102788900; 58037232700; 55264827400; 56729527000; 7404523209; 55643855000","Deep quantization generative networks","2020","Pattern Recognition","105","","107338","","","","10.1016/j.patcog.2020.107338","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083704490&doi=10.1016%2fj.patcog.2020.107338&partnerID=40&md5=d839a90b19ccec9a99574b9557769639","Equipped with powerful convolutional neural networks (CNNs), generative models have achieved tremendous success in various vision applications. However, deep generative networks suffer from high computational and memory costs in both model training and deployment. While many efforts have been devoted to accelerate discriminative models by quantization, effectively reducing the costs for deep generative models is more challenging and remains unexplored. In this work, we investigate applying quantization technology to deep generative models. We find that keeping as much information as possible for quantized activations is key to obtain high-quality generative models. With this in mind, we propose Deep Quantization Generative Networks (DQGNs) to effectively accelerate and compress deep generative networks. By expanding the dimensions of the quantization basis space, DQGNs can achieve lower quantization error and are highly adaptive to complex data distributions. Various experiments on two powerful frameworks (i.e., variational auto-encoders, and generative adversarial networks) and two practical applications (i.e., style transfer, and super-resolution) demonstrate our findings and the effectiveness of our proposed approach. © 2020 Elsevier Ltd","Pattern recognition; Software engineering; Adversarial networks; Complex data distributions; Discriminative models; Generative model; Quantization errors; Quantization technologies; Super resolution; Vision applications; Convolutional neural networks","Acceleration; Compression; Generative models; Network quantization","Article","Final","","Scopus","2-s2.0-85083704490"
"Wang Y.; Lan X.; Zhang Y.; Miao R.; Tian Z.","Wang, Yudiao (57218188835); Lan, Xuguang (7006709790); Zhang, Yinshu (57208785307); Miao, Ruixue (57218836464); Tian, Zhiqiang (35194645000)","57218188835; 7006709790; 57208785307; 57218836464; 35194645000","Multiresolution mixture generative adversarial network for image super-resolution","2020","Proceedings - IEEE International Conference on Multimedia and Expo","2020-July","","9102972","","","","10.1109/ICME46284.2020.9102972","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090388867&doi=10.1109%2fICME46284.2020.9102972&partnerID=40&md5=5d4130536f7ca18ae4001b5f184609ad","With regard to the problem of image super-resolution (SR), generative adversarial network (GAN) can make generated images have more details and better effect on perceptual quality than other methods. However, GAN-based methods may lose the contour of object in some texture-intensive areas. In order to recover contour better and further enhance perceptual quality, we propose a Multiresolution Mixture Generative Adversarial Network for Image Super-Resolution (MRMGAN), which employs a multiresolution mixture network (MRMNet) for image super-resolution. The MRMNet is able to have multiple resolution feature maps at the same time when training. Meanwhile, we propose a residual fluctuation loss, which aims to reduce the overall fluctuation of residual between SR image and high-resolution (HR) image. We evaluated the proposed method on benchmark datasets. Experimental results show that the proposed MRMGAN can get satisfactory performance. © 2020 IEEE.","Image resolution; Mixtures; Optical resolving power; Textures; Adversarial networks; Benchmark datasets; Feature map; High resolution image; Image super resolutions; Multiple resolutions; Multiresolution; Perceptual quality; Image enhancement","Generative adversarial network; Multi-resolution mixture network; Residual fluctuation loss; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85090388867"
"Jinjin G.; Haoming C.; Haoyu C.; Xiaoxing Y.; Ren J.S.; Chao D.","Jinjin, Gu (57212042330); Haoming, Cai (57219784807); Haoyu, Chen (35725324500); Xiaoxing, Ye (57219787505); Ren, Jimmy S. (37117544000); Chao, Dong (56335662200)","57212042330; 57219784807; 35725324500; 57219787505; 37117544000; 56335662200","PIPAL: A Large-Scale Image Quality Assessment Dataset for Perceptual Image Restoration","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12356 LNCS","","","633","651","18","10.1007/978-3-030-58621-8_37","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097653101&doi=10.1007%2f978-3-030-58621-8_37&partnerID=40&md5=6694ade989e1d42851df2fc98983d731","Image quality assessment (IQA) is the key factor for the fast development of image restoration (IR) algorithms. The most recent IR methods based on Generative Adversarial Networks (GANs) have achieved significant improvement in visual performance, but also presented great challenges for quantitative evaluation. Notably, we observe an increasing inconsistency between perceptual quality and the evaluation results. Then we raise two questions: (1) Can existing IQA methods objectively evaluate recent IR algorithms? (2) When focus on beating current benchmarks, are we getting better IR algorithms? To answer these questions and promote the development of IQA methods, we contribute a large-scale IQA dataset, called Perceptual Image Processing Algorithms (PIPAL) dataset. Especially, this dataset includes the results of GAN-based methods, which are missing in previous datasets. We collect more than 1.13 million human judgments to assign subjective scores for PIPAL images using the more reliable “Elo system”. Based on PIPAL, we present new benchmarks for both IQA and super-resolution methods. Our results indicate that existing IQA methods cannot fairly evaluate GAN-based IR algorithms. While using appropriate evaluation methods is important, IQA methods should also be updated along with the development of IR algorithms. At last, we improve the performance of IQA networks on GAN-based distortions by introducing anti-aliasing pooling. Experiments show the effectiveness of the proposed method. © 2020, Springer Nature Switzerland AG.","Anti-aliasing; Computer vision; Image reconstruction; Large dataset; Quality control; Restoration; Adversarial networks; Evaluation results; Image quality assessment; Image quality assessment (IQA); Perceptual image processing; Quantitative evaluation; Superresolution methods; Visual performance; Image quality","Generative adversarial network; Image quality assessment; Perceptual image restoration; Perceptual super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097653101"
"Zhang X.; Song H.; Zhang K.; Qiao J.; Liu Q.","Zhang, Xiaolu (57208245743); Song, Huihui (36572623600); Zhang, Kaihua (55475000500); Qiao, Jiaojiao (57210157638); Liu, Qingshan (36063739200)","57208245743; 36572623600; 55475000500; 57210157638; 36063739200","Single image super-resolution with enhanced Laplacian pyramid network via conditional generative adversarial learning","2020","Neurocomputing","398","","","531","538","7","10.1016/j.neucom.2019.04.097","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069689459&doi=10.1016%2fj.neucom.2019.04.097&partnerID=40&md5=dc551a3c9afbd9df02859e5f08ec9cf0","Despite much progress has been made by applying generative adversarial network (GAN) to single image super-resolution (SISR), obvious difference remains between the details of reconstructed high-frequency and ground-truth because GAN is unstable that has a very high degree of freedom. To address this issue, we exploit conditional GAN (CGAN) for SISR, which leverages the ground-truth high-resolution (HR) image as its conditional variable to guide to learn a more stable model. To better reconstruct image with a large-scale factor, we further design an enhanced Laplacian pyramid network (ELapN) as the generator model of CGAN, which progressively reconstructs HR images at multiple pyramid levels. The proposed ELapN fuses low-and high-level features for the residual image learning achieves better generalization than those only based on high-level information. Finally, we train the proposed network via deep supervision using a combination of multi-level CGAN, VGG and robust Charbonnier loss functions to obtain high-quality SR results. Extensive evaluations on three benchmark datasets including Set5, Set14, B100 demonstrate superiority of the proposed method over state-of-the-art methods in terms of PSNR, SSIM and visual effect. © 2019 Elsevier B.V.","Degrees of freedom (mechanics); Image reconstruction; Laplace transforms; Optical resolving power; Adversarial networks; Conditional GAN; High Degree of Freedom; High resolution image; High-level information; Laplacian Pyramid; Single images; State-of-the-art methods; article; learning; loss of function mutation; Image enhancement","Conditional GAN; GAN; Laplacian pyramid; Single image super-resolution","Article","Final","","Scopus","2-s2.0-85069689459"
"Li X.; Dong J.; Li B.; Zhang Y.; Zhang Y.; Veeraraghavan A.; Ji X.","Li, Xiu (7501701944); Dong, Jiuyang (57217176339); Li, Bowen (57223938952); Zhang, Yi (57839850300); Zhang, Yongbing (7601315649); Veeraraghavan, Ashok (6506177844); Ji, Xiangyang (7402837796)","7501701944; 57217176339; 57223938952; 57839850300; 7601315649; 6506177844; 7402837796","Fast confocal microscopy imaging based on deep learning","2020","IEEE International Conference on Computational Photography, ICCP 2020","","","9105215","","","","10.1109/ICCP48838.2020.9105215","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086628353&doi=10.1109%2fICCP48838.2020.9105215&partnerID=40&md5=f07120b1bc6e19df7ef2f9657d119595","Confocal microscopy is the de-facto standard technique in bio-imaging for acquiring 3D images in the presence of tissue scattering. However, the point-scanning mechanism inherent in confocal microscopy implies that the capture speed is much too slow for imaging dynamic objects at sufficient spatial resolution and signal to noise ratio(SNR). In this paper, we propose an algorithm for super-resolution confocal microscopy that allows us to capture high-resolution, high SNR confocal images at an order of magnitude faster acquisition speed. The proposed Back-Projection Generative Adversarial Network (BPGAN) consists of a feature extraction step followed by a back-projection feedback module (BPFM) and an associated reconstruction network, these together allow for super-resolution of low-resolution confocal scans. We validate our method using real confocal captures of multiple biological specimens and the results demonstrate that our proposed BPGAN is able to achieve similar quality to high-resolution confocal scans while the imaging speed can be up to 64 times faster. © 2020 IEEE.","Color photography; Confocal microscopy; Optical resolving power; Signal to noise ratio; Adversarial networks; Biological specimens; Confocal microscopy imaging; De facto standard; Reconstruction networks; Scanning mechanisms; Spatial resolution; Super resolution; Deep learning","Confocal microscopy; Deep learning; Single image super-resoultion","Conference paper","Final","","Scopus","2-s2.0-85086628353"
"Wen J.; Shi Y.; Zhou X.; Xue Y.","Wen, Juan (57202574050); Shi, Yangjing (57218326448); Zhou, Xiaoshi (57208736762); Xue, Yiming (57202586201)","57202574050; 57218326448; 57208736762; 57202586201","Crop disease classification on inadequate low-resolution target images","2020","Sensors (Switzerland)","20","16","4601","1","17","16","10.3390/s20164601","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089801160&doi=10.3390%2fs20164601&partnerID=40&md5=d6601508ac415634909c03e0e093c884","Currently, various agricultural image classification tasks are carried out on high-resolution images. However, in some cases, we cannot get enough high-resolution images for classification, which significantly affects classification performance. In this paper, we design a crop disease classification network based on Enhanced Super-Resolution Generative adversarial networks (ESRGAN) when only an insufficient number of low-resolution target images are available. First, ESRGAN is used to recover super-resolution crop images from low-resolution images. Transfer learning is applied in model training to compensate for the lack of training samples. Then, we test the performance of the generated super-resolution images in crop disease classification task. Extensive experiments show that using the fine-tuned ESRGAN model can recover realistic crop information and improve the accuracy of crop disease classification, compared with the other four image super-resolution methods. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Agriculture; Crops, Agricultural; Plant Diseases; Agricultural robots; Classification (of information); Crops; Image enhancement; Optical resolving power; Transfer learning; Adversarial networks; Classification performance; High resolution image; Image super resolutions; Low resolution images; Model training; Super resolution; Training sample; agriculture; crop; plant disease; Image classification","Convolutional Neural Networks; Disease classification; Generative Adversarial Networks; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85089801160"
"Ren H.; Kheradmand A.; El-Khamy M.; Wang S.; Bai D.; Lee J.","Ren, Haoyu (57212480009); Kheradmand, Amin (37013378600); El-Khamy, Mostafa (6506791951); Wang, Shuangquan (57218714168); Bai, Dongwoon (24780306900); Lee, Jungwon (57144162400)","57212480009; 37013378600; 6506791951; 57218714168; 24780306900; 57144162400","Real-world super-resolution using generative adversarial networks","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9150661","1760","1768","8","10.1109/CVPRW50498.2020.00226","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090137043&doi=10.1109%2fCVPRW50498.2020.00226&partnerID=40&md5=4340b7a7f9d335748683509a89593a03","Robust real-world super-resolution (SR) aims to generate perception-oriented high-resolution (HR) images from the corresponding low-resolution (LR) ones, without access to the paired LR-HR ground-truth. In this paper, we investigate how to advance the state of the art in real-world SR. Our method involves deploying an ensemble of generative adversarial networks (GANs) for robust real-world SR. The ensemble deploys different GANs trained with different adversarial objectives. Due to the lack of knowledge about the ground-truth blur and noise models, we design a generic training set with the LR images generated by various degradation models from a set of HR images. We achieve good perceptual quality by super resolving the LR images whose degradation was caused by unknown image processing artifacts. For real-world SR on images captured by mobile devices, the GANs are trained by weak supervision of a mobile SR training set having LR-HR image pairs, which we construct from the DPED dataset which provides registered mobile-DSLR images at the same scale. Our ensemble of GANs uses cues from the image luminance and adjusts to generate better HR images at low-illumination. Experiments on the NTIRE 2020 real-world super-resolution dataset show that our proposed SR approach achieves good perceptual quality. © 2020 IEEE.","Optical resolving power; Adversarial networks; Degradation model; High resolution image; Image luminance; Low illuminations; Perceptual quality; State of the art; Super resolution; Computer vision","","Conference paper","Final","","Scopus","2-s2.0-85090137043"
"López-Tapia S.; Lucas A.; Molina R.; Katsaggelos A.K.","López-Tapia, Santiago (57196006294); Lucas, Alice (57200297854); Molina, Rafael (34870201500); Katsaggelos, Aggelos K. (7102711302)","57196006294; 57200297854; 34870201500; 7102711302","A single video super-resolution GAN for multiple downsampling operators based on pseudo-inverse image formation models","2020","Digital Signal Processing: A Review Journal","104","","102801","","","","10.1016/j.dsp.2020.102801","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087921670&doi=10.1016%2fj.dsp.2020.102801&partnerID=40&md5=14598a69db15ae5911a2285b297170fb","The popularity of high and ultra-high definition displays has led to the need for methods to improve the quality of videos already obtained at much lower resolutions. A large amount of current CNN-based Video Super-Resolution methods are designed and trained to handle a specific degradation operator (e.g., bicubic downsampling) and are not robust to mismatch between training and testing degradation models. This causes their performance to deteriorate in real-life applications. Furthermore, many of them use the Mean-Squared-Error as the only loss during learning, causing the resulting images to be too smooth. In this work we propose a new Convolutional Neural Network for video super resolution which is robust to multiple degradation models. During training, which is performed on a large dataset of scenes with slow and fast motions, it uses the pseudo-inverse image formation model as part of the network architecture in conjunction with perceptual losses and a smoothness constraint that eliminates the artifacts originating from these perceptual losses. The experimental validation shows that our approach outperforms current state-of-the-art methods and is robust to multiple degradations. © 2020 Elsevier Inc.","Convolutional neural networks; Large dataset; Mean square error; Network architecture; Optical resolving power; Personnel training; Signal sampling; Experimental validations; Image formation models; Multiple degradations; Real-life applications; Smoothness constraints; State-of-the-art methods; Ultra high definitions; Video super-resolution; Inverse problems","Convolutional neuronal networks; Generative adversarial networks; Perceptual loss functions; Super-resolution; Video","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85087921670"
"Tej A.R.; Sukanta Halder S.; Shandeelya A.P.; Pankajakshan V.","Tej, Akella Ravi (57207764881); Sukanta Halder, Shirsendu (57209322139); Shandeelya, Arunav Pratap (57219547545); Pankajakshan, Vinod (6506890403)","57207764881; 57209322139; 57219547545; 6506890403","Enhancing Perceptual Loss with Adversarial Feature Matching for Super-Resolution","2020","Proceedings of the International Joint Conference on Neural Networks","","","9207102","","","","10.1109/IJCNN48605.2020.9207102","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093835200&doi=10.1109%2fIJCNN48605.2020.9207102&partnerID=40&md5=20a1a0bc831d6ae292194061868713c1","Single image super-resolution (SISR) is an ill-posed problem with an indeterminate number of valid solutions. Solving this problem with neural networks would require access to extensive experience, either presented as a large training set over natural images or a condensed representation from another pre-trained network. Perceptual loss functions, which belong to the latter category, have achieved breakthrough success in SISR and several other computer vision tasks. While perceptual loss plays a central role in the generation of photo-realistic images, it also produces undesired pattern artifacts in the super-resolved outputs. In this paper, we show that the root cause of these pattern artifacts can be traced back to a mismatch between the pre-training objective of perceptual loss and the super-resolution objective. To address this issue, we propose to augment the existing perceptual loss formulation with a novel content loss function that uses the latent features of a discriminator network to filter the unwanted artifacts across several levels of adversarial similarity. Further, our modification has a stabilizing effect on non-convex optimization in adversarial training. The proposed approach offers notable gains in perceptual quality based on an extensive human evaluation study and a competent reconstruction fidelity when tested on objective evaluation metrics. © 2020 IEEE.","Convex optimization; Function evaluation; Optical resolving power; Quality control; Condensed representations; Human evaluation; Ill posed problem; Nonconvex optimization; Objective evaluation; Perceptual quality; Photorealistic images; Stabilizing effects; Neural networks","Generative Adversarial Networks; Perceptual Loss Functions; Single Image Super-Resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85093835200"
"Cao S.; Yao X.; Koirala N.; Brott B.; Litovsky S.; Ling Y.; Gan Y.","Cao, Shengting (57215206633); Yao, Xinwen (55360009900); Koirala, Nischal (39861514700); Brott, Brigitta (6602736319); Litovsky, Silvio (6701538499); Ling, Yuye (57193400990); Gan, Yu (55871143500)","57215206633; 55360009900; 39861514700; 6602736319; 6701538499; 57193400990; 55871143500","Super-resolution technology to simultaneously improve optical & digital resolution of optical coherence tomography via deep learning","2020","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","2020-July","","9175777","1879","1882","3","10.1109/EMBC44109.2020.9175777","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091044744&doi=10.1109%2fEMBC44109.2020.9175777&partnerID=40&md5=44e6f21aa975c8a95229f4517e257e69","Optical coherence tomography (OCT) has stimulated a wide range of medical image-based diagnosis and treatment. In cardiac imaging, OCT has been used in assessing plaques before and after stenting. While needed in many scenarios, high resolution comes at the costs of demanding optical design and data storage/transmission. In OCT, there are two types of resolutions to characterize image quality: optical and digital resolutions. Although multiple existing works have heavily emphasized on improving the digital resolution, the studies on improving optical resolution or both resolutions remain scarce. In this paper, we focus on improving both resolutions. In particular, we investigate a deep learning method to address the problem of generating a high-resolution (HR) OCT image from a low optical and low digital resolution (L2R) image. To this end, we have modified the existing super-resolution generative adversarial network (SR-GAN) for OCT image reconstruction. Experimental results from the human coronary OCT images have demonstrated that the reconstructed images from highly compressed data could achieve high structural similarity and accuracy in comparison with the HR images. Besides, our method has obtained better denoising performance than the block-matching and 3D filtering (BM3D) and Denoising Convolutional Neural Networks (DnCNN) denoising method. © 2020 IEEE.","Convolutional neural networks; Deep learning; Diagnosis; Digital storage; E-learning; Engineering education; Image reconstruction; Learning systems; Medical imaging; Optical design; Optical resolving power; Optical tomography; Tomography; Adversarial networks; Block matching and 3d filtering; Denoising methods; Digital resolution; Image-based diagnosis; Optical resolution; Reconstructed image; Structural similarity; Optical data storage","","Conference paper","Final","","Scopus","2-s2.0-85091044744"
"Hongtao Z.; Shinomiya Y.; Yoshida S.","Hongtao, Zhang (55685572800); Shinomiya, Yuki (56239397700); Yoshida, Shinichi (7406245025)","55685572800; 56239397700; 7406245025","3D Brain MRI Reconstruction based on 2D Super-Resolution Technology","2020","Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics","2020-October","","9283444","18","23","5","10.1109/SMC42975.2020.9283444","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098891195&doi=10.1109%2fSMC42975.2020.9283444&partnerID=40&md5=24ac3887737c3a46ba684c1ed3c5a262","Magnetic resonance imaging (MRI) is one of the most important diagnostic imaging methods, which is widely used in diagnosis and image-guided therapy, especially imaging diagnosis of the brain. However, MRI images have the characteristics of low resolution, and there are limitations such as long imaging time and noise. Super-resolution techniques have been studied on three-dimensional MRI images using three-dimensional convolutional neural network. Based on some related techniques of super-resolution reconstruction of two-dimensional MRI slices, we evaluated the capability of several super-resolution technologies. We utilize the super-resolution algorithm based on generative adversarial network ESRGAN to realize super-resolution reconstruction of two-dimensional MRI slices, and then we further demonstrate that frequent details can be obtained from ESRGAN. In the aspect of two-dimensional to three-dimensional reconstruction, we use the technique of two-dimensional super-resolution on slices from three different latitudes. We rebuild reconstructed two-dimensional images into a three-dimensional form. Then based on the principle of linear interpolation, we use the surrounding effective pixel values to interpolate the null value of each slice, and realize the reconstruction of three-dimensional brain MRI. © 2020 IEEE.","Convolutional neural networks; Image reconstruction; Optical resolving power; Adversarial networks; Diagnostic imaging; Image guided therapy; Linear Interpolation; Super resolution algorithms; Super resolution reconstruction; Three-dimensional reconstruction; Two dimensional images; Magnetic resonance imaging","3D-reconstruction; deep-learning; ESRGAN; interpolation; magnetic resonance imaging; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85098891195"
"Wei W.; Liu S.","Wei, Wenguo (57219314951); Liu, Shiguang (36621189700)","57219314951; 36621189700","Interpolating Frames for Super-Resolution Smoke Simulation with GANs","2020","Communications in Computer and Information Science","1300","","","14","21","7","10.1007/978-3-030-63426-1_2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097641269&doi=10.1007%2f978-3-030-63426-1_2&partnerID=40&md5=9321b8e02acaee7f62effd71762774fe","Deep neural networks have enabled super-resolution of fluid data, which can successfully expand data from 2D to 3D. However, it is non-trivial to solve the incoherence between the super-resolution frames. In this paper, we introduce a new frame-interpolation method based on a conditional generative adversarial network for smoke simulation. Our model generates several intermediate frames between the original two consecutive frames to remove the incoherence. Specifically, we design a new generator that consists of residual blocks and a U-Net architecture. The generator with residual blocks is able to accurately recover high-resolution volumetric data from down-sampled one. We then input the two recovered frames and their corresponding velocity fields to the U-Net, warping and linearly fusing to generate several intermediate frames. Additionally, we propose a slow-fusion model to design our temporal discriminator. This model allows our adversarial network to progressively merge a series of consecutive frames step by step. The experiments demonstrate that our model could produce high-quality intermediate frames for smoke simulation, which efficiently remove the incoherence from the original fluid data. © 2020, Springer Nature Switzerland AG.","Animation; Arts computing; Deep neural networks; Interpolation; Optical resolving power; Velocity; Volumetric analysis; Adversarial networks; Frame interpolation; High resolution; NET architecture; Smoke simulation; Super resolution; Velocity field; Volumetric data; Smoke","Deep learning; Frame interpolation; Smoke simulation; Temporal coherence","Conference paper","Final","","Scopus","2-s2.0-85097641269"
"Yilin J.; Ran S.; Sanqiang T.","Yilin, Jiang (6507783321); Ran, Shao (57221631903); Sanqiang, Tang (57221634984)","6507783321; 57221631903; 57221634984","Generative adversarial networks for hyperspectral image spatial super resolution","2020","Journal of China Universities of Posts and Telecommunications","27","4","","8","16","8","10.19682/j.cnki.1005-8885.2020.0032","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099647992&doi=10.19682%2fj.cnki.1005-8885.2020.0032&partnerID=40&md5=7f8028c7e45d33c8366b0f4cced98055","It is becoming increasingly easier to obtain more abundant supplies for hyperspectral images (HSIs). Despite this, achieving high resolution is still critical. In this paper, a method named hyperspectral images super resolution generative adversarial network (HIS SRGAN) is proposed to enhance the spatial resolution of HSI without decreasing its spectral resolution. Different from existing methods with the same purpose, which are based on convolutional neural networks (CNNs) and driven by a pixel level loss function, the new generative adversarial network (GAN) has a redesigned framework and a targeted loss function. Specifically, the discriminator uses the structure of the relativistic discriminator, which provides feedback on how much the generated HSI looks like the ground truth. The generator achieves more authentic details and textures by removing the place of the pooling layer and the batch normalization layer and presenting smaller filter size and two step upsampling layers. Furthermore, the loss function is improved to specially take spectral distinctions into account to avoid artifacts and minimize potential spectral distortion, which may be introduced by neural networks. Furthermore, pre training with the visual geometry group (VGG) network helps the entire model to initialize more easily. Benefiting from these changes, the proposed method obtains significant advantages compared to the original GAN. Experimental results also reveal that the proposed method performs better than several state of the art methods. © 2020, Beijing University of Posts and Telecommunications. All rights reserved.","Image enhancement; Optical resolving power; Spectroscopy; Textures; Adversarial networks; Ground truth; High resolution; Loss functions; Spatial resolution; Spectral distortions; State-of-the-art methods; Super resolution; Convolutional neural networks","HIS; Hyperspectral image; SRGAN; Super resolution","Article","Final","","Scopus","2-s2.0-85099647992"
"Dai Q.; Cheng X.; Qiao Y.; Zhang Y.","Dai, Qiang (57650365900); Cheng, Xi (57206756990); Qiao, Yan (36988851000); Zhang, Youhua (36784200800)","57650365900; 57206756990; 36988851000; 36784200800","Agricultural pest super-resolution and identification with attention enhanced residual and dense fusion generative and adversarial network","2020","IEEE Access","8","","9082695","81943","81959","16","10.1109/ACCESS.2020.2991552","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084951493&doi=10.1109%2fACCESS.2020.2991552&partnerID=40&md5=3f1cbbacd7f0d7d5121cf14c96b30b98","The growth of the most significant field crops such as rice, wheat, maize, and soybean are influenced because of various pests. And crop production is decreased due to various categories of insects. Deep learning technologies significantly increased the efficiency of identifying and controlling agricultural pests attack. However, agricultural pests images obtained are often obscure and unclear because of the sparse density of cameras deployed in the real farmland. This always makes pests difficult to recognize and monitor. Additionally, the existing classification and segmentation methods are not satisfying for the identification of low-resolution images because they are pre-trained on the clear and high-resolution datasets. Therefore, it is crucial to restore and upscale the obtained low-resolution pest images in order to improve classification accuracy and the recall rate of the instance segmentation. In this paper, we propose a generative adversarial network (GAN) with quadra-attention and residual and dense fusion mechanisms to transform low-resolution pest images. Compared with previous state-of-the-art PSNR-oriented super-resolution methods, our proposed method is more powerful in image reconstruction and achieves the state of the art performance. The experiment results show that after reconstructing with our proposed gan, the recall rate increased by 182.89% and classification accuracy also improved a lot. Besides, our proposed method could decrease the density of the camera layout in the agricultural Internet of Things (IOT) monitor systems and the cost of infrastructure, which is practical for real-world applications. © 2013 IEEE.","Agricultural robots; Cameras; Classification (of information); Crops; Cultivation; Deep learning; Image reconstruction; Image segmentation; Optical resolving power; Adversarial networks; Classification accuracy; High-resolution datasets; Internet of Things (IOT); Low resolution images; Segmentation methods; State-of-the-art performance; Superresolution methods; Image enhancement","Agricultural pests; classification; deep learning; object instance segmentation; quadra-attention; residual and dense fusion; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084951493"
"Huang Y.; Zheng F.; Wang D.; Jiang J.; Wang X.; Shao L.","Huang, Yawen (57190131740); Zheng, Feng (57210574274); Wang, Danyang (57840084100); Jiang, Junyu (57220544387); Wang, Xiaoqian (56355253400); Shao, Ling (55643855000)","57190131740; 57210574274; 57840084100; 57220544387; 56355253400; 55643855000","Super-resolution and inpainting with degraded and upgraded generative adversarial networks","2020","IJCAI International Joint Conference on Artificial Intelligence","2021-January","","","645","651","6","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097340315&partnerID=40&md5=78f7dd5573a1180aff7c62ef3b163c89","Image super-resolution (SR) and image inpainting are two topical problems in medical image processing. Existing methods for solving the problems are either tailored to recovering a high-resolution version of the low-resolution image or focus on filling missing values, thus inevitably giving rise to poor performance when the acquisitions suffer from multiple degradations. In this paper, we explore the possibility of super-resolving and inpainting images to handle multiple degradations and therefore improve their usability. We construct a unified and scalable framework to overcome the drawbacks of propagated errors caused by independent learning. We additionally provide improvements over previously proposed super-resolution approaches by modeling image degradation directly from data observations rather than bicubic downsampling. To this end, we propose HLH-GAN, which includes a high-to-low (H-L) GAN together with a low-to-high (L-H) GAN in a cyclic pipeline for solving the medical image degradation problem. Our comparative evaluation demonstrates that the effectiveness of the proposed method on different brain MRI datasets. In addition, our method outperforms many existing super-resolution and inpainting approaches. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.","Artificial intelligence; Magnetic resonance imaging; Medical imaging; Medical problems; Optical resolving power; Adversarial networks; Comparative evaluations; Image degradation; Image Inpainting; Image super resolutions; Independent learning; Low resolution images; Multiple degradations; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85097340315"
"Tran D.T.; Robinson H.; Rasheed A.; San O.; Tabib M.; Kvamsdal T.","Tran, Duy Tan (57220007692); Robinson, Haakon (57215829021); Rasheed, Adil (36835813200); San, Omer (26424808900); Tabib, Mandar (24437021300); Kvamsdal, Trond (6602498949)","57220007692; 57215829021; 36835813200; 26424808900; 24437021300; 6602498949","GANs enabled super-resolution reconstruction of wind field","2020","Journal of Physics: Conference Series","1669","1","012029","","","","10.1088/1742-6596/1669/1/012029","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096469013&doi=10.1088%2f1742-6596%2f1669%2f1%2f012029&partnerID=40&md5=49f0f5d379cb3d7ab681743bfe8f48e5","Atmospheric flows are governed by a broad variety of spatio-temporal scales, thus making real-time numerical modeling of such turbulent flows in complex terrain at high resolution computationally unmanageable. In this paper, we demonstrate a novel approach to address this issue through a combination of fast coarse scale physics based simulator and a family of advanced machine learning algorithm called the Generative Adversarial Networks. The physics-based simulator generates a coarse wind field in a real wind farm and then ESRGANs enhance the result to a much finer resolution. The method outperforms state of the art bicubic interpolation methods commonly utilized for this purpose.  © Published under licence by IOP Publishing Ltd.","Learning algorithms; Offshore oil well production; Wind power; Adversarial networks; Atmospheric flows; Bicubic interpolation; Complex terrains; High resolution; Spatio-temporal scale; State of the art; Super resolution reconstruction; Machine learning","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85096469013"
"Lee R.; Dudziak Ł.; Abdelfattah M.; Venieris S.I.; Kim H.; Wen H.; Lane N.D.","Lee, Royson (57219565579); Dudziak, Łukasz (57210637324); Abdelfattah, Mohamed (39661059600); Venieris, Stylianos I. (57188695783); Kim, Hyeji (56338564400); Wen, Hongkai (55320758300); Lane, Nicholas D. (23135333200)","57219565579; 57210637324; 39661059600; 57188695783; 56338564400; 55320758300; 23135333200","Journey Towards Tiny Perceptual Super-Resolution","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12371 LNCS","","","85","102","17","10.1007/978-3-030-58574-7_6","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097286027&doi=10.1007%2f978-3-030-58574-7_6&partnerID=40&md5=0f043364856822aeff6405d7983a773b","Recent works in single-image perceptual super-resolution (SR) have demonstrated unprecedented performance in generating realistic textures by means of deep convolutional networks. However, these convolutional models are excessively large and expensive, hindering their effective deployment to end devices. In this work, we propose a neural architecture search (NAS) approach that integrates NAS and generative adversarial networks (GANs) with recent advances in perceptual SR and pushes the efficiency of small perceptual SR models to facilitate on-device execution. Specifically, we search over the architectures of both the generator and the discriminator sequentially, highlighting the unique challenges and key observations of searching for an SR-optimized discriminator and comparing them with existing discriminator architectures in the literature. Our tiny perceptual SR (TPSR) models outperform SRGAN and EnhanceNet on both full-reference perceptual metric (LPIPS) and distortion metric (PSNR) while being up to 26.4× more memory efficient and 33.6× more compute efficient respectively. © 2020, Springer Nature Switzerland AG.","Computer vision; Convolution; Convolutional neural networks; Optical resolving power; Textures; Adversarial networks; Convolutional model; Convolutional networks; Distortion metrics; Memory efficient; Neural architectures; Perceptual metrics; Super resolution; Network architecture","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85097286027"
"Jang S.S.; Hwang K.H.; Ha Y.G.","Jang, Sung Su (57205337608); Hwang, Kyu Hong (57216708295); Ha, Young Guk (23389377100)","57205337608; 57216708295; 23389377100","High quality training set collection using generative adversarial network","2020","Proceedings - 2020 IEEE International Conference on Big Data and Smart Computing, BigComp 2020","","","9070346","455","458","3","10.1109/BigComp48618.2020.00-27","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084362593&doi=10.1109%2fBigComp48618.2020.00-27&partnerID=40&md5=7b8f142b9241f45687b526044511e3c6","Image classification and object detection using deep learning have evolved with continuous research. In particular, the development of big data and the improvement of computer hardware performance have contributed extremely to the development of deep learning. Deep learning technologies like Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) can train models through training data. In other words, the optimal value of the weight parameter is automatically obtained from the training data. In this way, training data is the most important in deep learning. The quality and amount of training data affects the learning performance of deep learning models. In order to obtain high quality training data, it is difficult for the user to collect it directly. And using a web crawler to collect images for search keywords is also in problem. The images collected by the crawler include many low-resolution images and irrelevant images. These image data are bad for training deep learning model. Therefore, this paper purposes to collect high quality learning data by automatically applying SRGAN to low resolution images after image collection through image crawler and converting high quality images. © 2020 IEEE.","Big data; Computer hardware; Convolutional neural networks; Learning systems; Object detection; Web crawler; Adversarial networks; Hardware performance; High quality images; Learning performance; Learning technology; Low resolution images; Recurrent neural network (RNN); Weight parameters; Recurrent neural networks","Crawler; ESRGAN; Low resolution; Super-resolution; Training data set","Conference paper","Final","","Scopus","2-s2.0-85084362593"
"Cai J.; Meng Z.; Ho C.M.","Cai, Jie (57195222772); Meng, Zibo (57222589286); Ho, Chiu Man (57218713605)","57195222772; 57222589286; 57218713605","Residual channel attention generative adversarial network for image super-resolution and noise reduction","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9150804","1852","1861","9","10.1109/CVPRW50498.2020.00235","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090165840&doi=10.1109%2fCVPRW50498.2020.00235&partnerID=40&md5=2aed20362fed34993cee3b9892cbe8d2","Image super-resolution is one of the important computer vision techniques aiming to reconstruct high-resolution images from corresponding low-resolution ones. Most recently, deep learning based approaches have been demonstrated for image super-resolution. However, as the deep networks go deeper, they become more difficult to train and more difficult to restore the finer texture details, especially under real-world settings. In this paper, we propose a Residual Channel Attention-Generative Adversarial Network (RCA-GAN) to solve these problems. Specifically, a novel residual channel attention block is proposed to form RCA-GAN, which consists of a set of residual blocks with shortcut connections, and a channel attention mechanism to model the interdependence and interaction of the feature representations among different channels. Besides, a generative adversarial network (GAN) is employed to further produce realistic and highly detailed results. Benefiting from these improvements, the proposed RCA-GAN yields consistently better visual quality with more detailed and natural textures than baseline models; and achieves comparable or better performance compared with the state-of-the-art methods for real-world image super-resolution. © 2020 IEEE.","Computer vision; Deep learning; Image denoising; Noise abatement; Optical resolving power; Textures; Attention mechanisms; Computer vision techniques; Feature representation; High resolution image; Image super resolutions; Learning-based approach; Short-cut connection; State-of-the-art methods; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090165840"
"Kumar R.; Maji S.K.","Kumar, Rajdeep (57217697342); Maji, Suman Kumar (48361548500)","57217697342; 48361548500","A novel framework for denoised high resolution generative adversarial network-DHRGAN","2020","2020 7th International Conference on Signal Processing and Integrated Networks, SPIN 2020","","","9071211","1033","1038","5","10.1109/SPIN48934.2020.9071211","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084279141&doi=10.1109%2fSPIN48934.2020.9071211&partnerID=40&md5=20bd7fd7ceec8330873f6166829e30b0","The advent of deeper convolutional neural networks and related methodologies have made significant achievements in the area of single image super-resolution (SISR). However, none of these techniques are equipped to handle noisy images, i.e., denoising the image as well as enhancing its spatial resolution. In this paper, we propose a denoised high resolution generative adversarial network (DHRGAN), capable of handling noise removal from given sample images while trying to super-resolve it to the desired magnification. As per our knowledge, this is the first GAN framework equipped to remove noise while simultaneously trying to magnify images. To accomplish this, we propose a two-layered generator network in addition to the discriminator network. In the upper layer of the generator network (also called UDCNN), we make use of the mean square error function for training samples while in the lower layer of generator network (called LSRResNet) we make use of a perceptual loss function (comprising of content loss function and adversarial loss function) for the same. We train the discriminator network, which is responsible for separating super-resolved images and ground truth images, using the adversarial loss function, thereby making the generator more robust. The content loss function that we employ in the generator actuates perceptual similarity rather than pixel similarity, which further strengthens its robustness. The resulting DHRGAN network can recover more realistic textures from heavily downsampled noisy images. We have used speckle noise, a common noise observed in natural scenes captured by airborne acquisition devices, to check the performance of DHRGAN. Experiments performed on standard image database, both visually and quantitatively, justify the superior performance of DHRGAN over existing similar networks. © 2020 IEEE.","Convolutional neural networks; Discriminators; Image enhancement; Mean square error; Network layers; Textures; Adversarial networks; Airborne acquisition; High resolution; Perceptual similarity; Pixel similarities; Spatial resolution; Standard images; Training sample; Image denoising","Denoising; DHRGAN; Perceptual similarity; Speckle noise; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85084279141"
"Chen B.; Ruan L.; Lam M.-L.","Chen, Bin (56742504100); Ruan, Lingyan (57205468831); Lam, Miu-Ling (7202630301)","56742504100; 57205468831; 7202630301","LFGAN: 4D Light Field Synthesis from a Single RGB Image","2020","ACM Transactions on Multimedia Computing, Communications and Applications","16","1","2","","","","10.1145/3366371","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083081718&doi=10.1145%2f3366371&partnerID=40&md5=95c5b1d7f1de2e0f790241c7f6d33992","We present a deep neural network called the light field generative adversarial network (LFGAN) that synthesizes a 4D light field from a single 2D RGB image. We generate light fields using a single image super-resolution (SISR) technique based on two important observations. First, the small baseline gives rise to the high similarity between the full light field image and each sub-aperture view. Second, the occlusion edge at any spatial coordinate of a sub-aperture view has the same orientation as the occlusion edge at the corresponding angular patch, implying that the occlusion information in the angular domain can be inferred from the sub-aperture local information. We employ the Wasserstein GAN with gradient penalty (WGAN-GP) to learn the color and geometry information from the light field datasets. The network can generate a plausible 4D light field comprising 8×8 angular views from a single sub-aperture 2D image. We propose new loss terms, namely epipolar plane image (EPI) and brightness regularization (BRI) losses, as well as a novel multi-stage training framework to feed the loss terms at different time to generate superior light fields. The EPI loss can reinforce the network to learn the geometric features of the light fields, and the BRI loss can preserve the brightness consistency across different sub-aperture views. Two datasets have been used to evaluate our method: in addition to an existing light field dataset capturing scenes of flowers and plants, we have built a large dataset of toy animals consisting of 2,100 light fields captured with a plenoptic camera. We have performed comprehensive ablation studies to evaluate the effects of individual loss terms and the multi-stage training strategy, and have compared LFGAN to other state-of-the-art techniques. Qualitative and quantitative evaluation demonstrates that LFGAN can effectively estimate complex occlusions and geometry in challenging scenes, and outperform other existing techniques. © 2020 ACM.","Deep neural networks; Geometry; Large dataset; Luminance; Adversarial networks; Epipolar plane images; Geometry information; Plenoptic cameras; Quantitative evaluation; Spatial coordinates; State-of-the-art techniques; Training framework; Image processing","generative adversarial network; Light field synthesis; single image super-resolution","Article","Final","","Scopus","2-s2.0-85083081718"
"Choi Y.; Kim M.; Kim Y.; Han S.","Choi, Yeonju (57215967828); Kim, Minsik (57223883024); Kim, Yongwoo (57202143770); Han, Sanghyuck (37101680300)","57215967828; 57223883024; 57202143770; 37101680300","A Study of CNN-based Super-Resolution Method for Remote Sensing Image","2020","Korean Journal of Remote Sensing","36","3","","449","460","11","10.7780/kjrs.2020.36.3.5","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106324741&doi=10.7780%2fkjrs.2020.36.3.5&partnerID=40&md5=d10c8da1971505cfb3328e5823c91add","Super-resolution is a technique used to reconstruct an image with low-resolution into that of high-resolution. Recently, deep-learning based super resolution has become the mainstream, and applications of these methods are widely used in the remote sensing field. In this paper, we propose a super-resolution method based on the deep back-projection network model to improve the satellite image resolution by the factor of four. In the process, we customized the loss function with the edge loss to result in a more detailed feature of the boundary of each object and to improve the stability of the model training using generative adversarial network based on Wasserstein distance loss. Also, we have applied the detail preserving image down-scaling method to enhance the naturalness of the training output. Finally, by including the modified-residual learning with a panchromatic feature in the final step of the training process. Our proposed method is able to reconstruct fine features and high frequency information. Comparing the results of our method with that of the others, we propose that the super-resolution method improves the sharpness and the clarity of WorldView-3 and KOMPSAT-2 images. © 2020 Annals of Laparoscopic and Endoscopic Surgery. All rights reserved.","","DPID; Edge loss; Remote sensing image; SISR; Super resolution","Article","Final","","Scopus","2-s2.0-85106324741"
"Rakotonirina N.C.; Rasoanaivo A.","Rakotonirina, Nathanael Carraz (57218454140); Rasoanaivo, Andry (57210159479)","57218454140; 57210159479","ESRGAN+ : Further Improving Enhanced Super-Resolution Generative Adversarial Network","2020","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2020-May","","9054071","3637","3641","4","10.1109/ICASSP40776.2020.9054071","75","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088313073&doi=10.1109%2fICASSP40776.2020.9054071&partnerID=40&md5=29985910f1fc6664575b93514fda779a","Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) is a perceptual-driven approach for single image super-resolution that is able to produce photorealistic images. Despite the visual quality of these generated images, there is still room for improvement. In this fashion, the model is extended to further improve the perceptual quality of the images. We have designed a network architecture with a novel basic block to replace the one used by the original ESRGAN. Moreover, we introduce noise inputs to the generator network in order to exploit stochastic variation. The resulting images present more realistic textures. © 2020 IEEE.","Audio signal processing; Network architecture; Optical resolving power; Speech communication; Stochastic systems; Textures; Adversarial networks; Basic blocks; Perceptual quality; Photorealistic images; Single images; Stochastic variation; Super resolution; Visual qualities; Image enhancement","Generative adversarial network; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85088313073"
"Jiang X.; Xu Y.; Wei P.; Zhou Z.","Jiang, Xuhao (57203923337); Xu, Yifei (55767138300); Wei, Pingping (57211200298); Zhou, Zhuming (57217676175)","57203923337; 55767138300; 57211200298; 57217676175","CT image super resolution based on improved SRGAN","2020","2020 5th International Conference on Computer and Communication Systems, ICCCS 2020","","","9118497","363","367","4","10.1109/ICCCS49078.2020.9118497","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087494056&doi=10.1109%2fICCCS49078.2020.9118497&partnerID=40&md5=2197b64cd7a9e398a33c582de9110ab5","CT images are commonly used in medical clinical diagnosis. However, due to factors such as hardware and scanning time, CT images in real scenes are limited by spatial resolution so that doctors cannot perform accurate disease analysis on tiny lesion areas and pathological features. An image super-resolution (SR) method based on deep learning is a good way to solve this problem. Although many excellent networks have been proposed, but they all pay more attention to image quality indicators than image visual perception quality. Unlike other networks that focus more on image evaluation metrics, the super resolution generative adversarial network (SRGAN) has achieved tremendous improvements in image perception quality. Based on the above, this paper proposes a CT image super-resolution algorithm based on improved SRGAN. In order to improve the visual quality of CT images, a dilated convolution module is introduced. At the same time, in order to improve the overall visual effect of the image, the mean structural similarity (MSSIM) loss is also introduced to improve the perceptual loss function. Experimental results on the public CT image dataset demonstrate that our model is better than the baseline method SRGAN not only in mean opinion score(MOS), but also in peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) values. © 2020 IEEE.","Deep learning; Diagnosis; Image enhancement; Image quality; Medical imaging; Optical resolving power; Quality control; Signal to noise ratio; Adversarial networks; Clinical diagnosis; Image super resolutions; Mean opinion scores; Mean structural similarity; Peak signal to noise ratio; Quality indicators; Structural similarity; Computerized tomography","Deep Learning; Generative Adversarial Network; Medical Image Reconstruction; Srgan; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85087494056"
"Zhang Y.; Bai Y.; Ding M.; Ghanem B.","Zhang, Yongqiang (57190277288); Bai, Yancheng (57206596265); Ding, Mingli (8925275900); Ghanem, Bernard (24331436200)","57190277288; 57206596265; 8925275900; 24331436200","Multi-task Generative Adversarial Network for Detecting Small Objects in the Wild","2020","International Journal of Computer Vision","128","6","","1810","1828","18","10.1007/s11263-020-01301-6","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079766125&doi=10.1007%2fs11263-020-01301-6&partnerID=40&md5=0250dd1000e6f4952846c35fcdd13d1c","Object detection results have been rapidly improved over a short period of time with the development of deep convolutional neural networks. Although impressive results have been achieved on large/medium sized objects, the performance on small objects is far from satisfactory and one of remaining open challenges is detecting small object in unconstrained conditions (e.g. COCO and WIDER FACE benchmarks). The reason is that small objects usually lack sufficient detailed appearance information, which can distinguish them from the backgrounds or similar objects. To deal with the small object detection problem, in this paper, we propose an end-to-end multi-task generative adversarial network (MTGAN), which is a general framework. In the MTGAN, the generator is a super-resolution network, which can up-sample small blurred images into fine-scale ones and recover detailed information for more accurate detection. The discriminator is a multi-task network, which describes each inputted image patch with a real/fake score, object category scores, and bounding box regression offsets. Furthermore, to make the generator recover more details for easier detection, the classification and regression losses in the discriminator are back-propagated into the generator during training process. Extensive experiments on the challenging COCO and WIDER FACE datasets demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods by a large margin. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Benchmarking; Convolutional neural networks; Deep neural networks; Face recognition; Image enhancement; Large dataset; Object recognition; Optical resolving power; Adversarial networks; COCO; Small object detection; Super resolution; WIDER FACE; Object detection","COCO; Multi-task generative adversarial network; Small face detection; Small object detection; Super-resolution network; WIDER FACE","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85079766125"
"Lei S.; Shi Z.; Zou Z.","Lei, Sen (57195618353); Shi, Zhenwei (23398841900); Zou, Zhengxia (56073977200)","57195618353; 23398841900; 56073977200","Coupled Adversarial Training for Remote Sensing Image Super-Resolution","2020","IEEE Transactions on Geoscience and Remote Sensing","58","5","8946581","3633","3643","10","10.1109/TGRS.2019.2959020","54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083899027&doi=10.1109%2fTGRS.2019.2959020&partnerID=40&md5=7215beb42d0cc9dcb4e96f78e3b6c307","Generative adversarial network (GAN) has made great progress in recent natural image super-resolution tasks. The key to its success is the integration of a discriminator which is trained to classify whether the input is a real high-resolution (HR) image or a generated one. Arguably, learning a strong discriminative prior is essential for generating high-quality images. However, in remote sensing images, we discover, through extensive statistical analysis, that there are more low-frequency components than natural images, which may lead to a 'discrimination-ambiguity' problem, i.e., the discriminator will become 'confused' to tell whether its input is real or not when dealing with those low-frequency regions, and therefore, the quality of generated HR images may be deeply affected. To address this problem, we propose a novel GAN-based super-resolution algorithm named coupled-discriminated GANs (CDGANs) for remote sensing images. Different from the previous GAN-based super-resolution models in which their discriminator takes in a single image at one time, in our model, the discriminator is specifically designed to take in a pair of images: a generated image and its HR ground truth, to make better discrimination of the inputs. We further introduce a dual pathway network architecture, a random gate, and a coupled adversarial loss to learn better correspondence between the discriminative results and the paired inputs. Experimental results on two public data sets demonstrate that our model can obtain more accurate super-resolution results in terms of both visual appearance and local details compared with other state of the arts. Our code will be made publicly available. © 1980-2012 IEEE.","Network architecture; Optical resolving power; Adversarial networks; High quality images; High resolution image; Low frequency regions; Low-frequency components; Remote sensing images; Super resolution algorithms; Super-resolution models; algorithm; data set; image resolution; remote sensing; satellite imagery; visual analysis; Remote sensing","Coupled adversarial training; deep convolutional neural networks; generative adversarial networks (GANs); remote sensing images; super-resolution","Article","Final","","Scopus","2-s2.0-85083899027"
"Cui J.; Gong K.; Han P.; Liu H.; Li Q.","Cui, Jianan (57195733866); Gong, Kuang (57146819700); Han, Paul (56514598400); Liu, Huafeng (7409750199); Li, Quanzheng (7405862484)","57195733866; 57146819700; 56514598400; 7409750199; 7405862484","Super Resolution of Arterial Spin Labeling MR Imaging Using Unsupervised Multi-scale Generative Adversarial Network","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12436 LNCS","","","50","59","9","10.1007/978-3-030-59861-7_6","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092720145&doi=10.1007%2f978-3-030-59861-7_6&partnerID=40&md5=d137417323bbf5709b97179ad12b778e","Arterial spin labeling (ASL) magnetic resonance imaging (MRI) is a powerful imaging technology that can measure cerebral blood flow (CBF) quantitatively. However, since only a small portion of blood is labeled compared to the whole tissue volume, conventional ASL suffers from low signal-to-noise ratio (SNR), poor spatial resolution, and long acquisition time. In this paper, we proposed a super-resolution method based on a multi-scale generative adversarial network (GAN) through unsupervised training. The network only needs the low-resolution (LR) ASL image itself for training and the T1-weighted image as the anatomical prior. No training pairs or pre-training are needed. A low-pass filter guided item was added as an additional loss to suppress the noise interference from the LR ASL image. After the network was trained, the super-resolution (SR) image was generated by supplying the upsampled LR ASL image and corresponding T1-weighted image to the generator of the last layer. Performance of the proposed method was evaluated by comparing the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) using normal-resolution (NR) ASL image (5.5 min acquisition) and high-resolution (HR) ASL image (44 min acquisition) as the ground truth. Compared to the nearest, linear, and spline interpolation methods, the proposed method recovers more detailed structure information, reduces the image noise visually, and achieves the highest PSNR and SSIM when using HR ASL image as the ground-truth. © 2020, Springer Nature Switzerland AG.","Blood; Computer aided instruction; Interpolation; Low pass filters; Machine learning; Magnetic resonance imaging; Medical imaging; Optical resolving power; Adversarial networks; Arterial spin labeling; Low signal-to-noise ratio; Peak signal to noise ratio; Structural similarity indices (SSIM); Structure information; Superresolution methods; Unsupervised training; Signal to noise ratio","Arterial spin labeling MRI; Generative adversarial network; Multi-scale; Super resolution; Unsupervised deep learning","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092720145"
"Tian S.; Lin S.; Lei H.; Li D.; Wang L.","Tian, Songwang (57219971327); Lin, Suzhen (7407607523); Lei, Haiwei (57171469000); Li, Dawei (37013419900); Wang, Lifang (57142669800)","57219971327; 7407607523; 57171469000; 37013419900; 57142669800","Multi-Band Image Synchronous Super-Resolution and Fusion Method Based on Improved WGAN-GP; [基于改进WGAN-GP的多波段图像同步超分与融合方法]","2020","Guangxue Xuebao/Acta Optica Sinica","40","20","2010001","","","","10.3788/AOS202040.2010001","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096351228&doi=10.3788%2fAOS202040.2010001&partnerID=40&md5=fa419a873859f65b09baaca6aeb66f33","Aiming at the problem that the fused results of low resolution source images are not good for the subsequent target extraction, a multi-band image synchronous super-resolution and fusion method based on Wasserstein generative adversarial network with gradient penalty (WGAN-GP) is proposed. Firstly, the multi-band low-resolution source images are enlarged to the target size respectively based on the bicubic interpolation method. Secondly, the enlarged results are input to a feature extraction (encoding) network to extract features respectively and combine them in a high-level feature space. Then, the initial fused images are reconstructed by decoding network. Finally, a high-resolution fused image is obtained through a dynamic game between the generator and the discriminator. The experimental results show that the proposed method can not only achieve multi-band images super-resolution and fusion simultaneously, but also the information amount, clarity, and visual quality of the fused images are significantly higher than other representative methods. © 2020, Chinese Lasers Press. All right reserved.","Extraction; Image fusion; Optical resolving power; Adversarial networks; Bicubic interpolation; High-level features; High-resolution fused images; Information amount; Multi-band images; Super resolution; Target extraction; Image enhancement","Generative adversarial network; Image fusion; Image processing; Image super-resolution; Multi-band image","Article","Final","","Scopus","2-s2.0-85096351228"
"Fu Y.; Chen W.; Wang H.; Li H.; Lin Y.; Wang Z.","Fu, Yonggan (57219270287); Chen, Wuyang (57214459624); Wang, Haotao (57218716903); Li, Haoran (57221546916); Lin, Yingyan (7406588846); Wang, Zhangyang (56288839400)","57219270287; 57214459624; 57218716903; 57221546916; 7406588846; 56288839400","AutoGAN-Distiller: Searching to compress generative adversarial networks","2020","37th International Conference on Machine Learning, ICML 2020","PartF168147-5","","","3250","3261","11","","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105185233&partnerID=40&md5=d2760bfd7540b99d16b60bc5885f3337","The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at: https://github.com/TAMU-VITA/AGD. Copyright 2020 by the author(s).","Distillation; Machine learning; Mobile telecommunication systems; Signaling; Adversarial networks; Compression algorithms; Computational resources; End to end; Image translation; Search spaces; Super resolution; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85105185233"
"Ruiz-Munoz J.F.; Nimmagadda J.K.; Dowd T.G.; Baciak J.E.; Zare A.","Ruiz-Munoz, Jose F. (54403857700); Nimmagadda, Jyothier K. (57205081836); Dowd, Tyler G. (57203682309); Baciak, James E. (6602911656); Zare, Alina (14069307000)","54403857700; 57205081836; 57203682309; 6602911656; 14069307000","Super resolution for root imaging","2020","Applications in Plant Sciences","8","7","e11374","","","","10.1002/aps3.11374","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087155288&doi=10.1002%2faps3.11374&partnerID=40&md5=7c9146c2da61cd118a8a145040cc0312","Premise: High-resolution cameras are very helpful for plant phenotyping as their images enable tasks such as target vs. background discrimination and the measurement and analysis of fine above-ground plant attributes. However, the acquisition of high-resolution images of plant roots is more challenging than above-ground data collection. An effective super-resolution (SR) algorithm is therefore needed for overcoming the resolution limitations of sensors, reducing storage space requirements, and boosting the performance of subsequent analyses. Methods: We propose an SR framework for enhancing images of plant roots using convolutional neural networks. We compare three alternatives for training the SR model: (i) training with non-plant-root images, (ii) training with plant-root images, and (iii) pretraining the model with non-plant-root images and fine-tuning with plant-root images. The architectures of the SR models were based on two state-of-the-art deep learning approaches: a fast SR convolutional neural network and an SR generative adversarial network. Results: In our experiments, we observed that the SR models improved the quality of low-resolution images of plant roots in an unseen data set in terms of the signal-to-noise ratio. We used a collection of publicly available data sets to demonstrate that the SR models outperform the basic bicubic interpolation, even when trained with non-root data sets. Discussion: The incorporation of a deep learning–based SR model in the imaging process enhances the quality of low-resolution images of plant roots. We demonstrate that SR preprocessing boosts the performance of a machine learning system trained to separate plant roots from their background. Our segmentation experiments also show that high performance on this task can be achieved independently of the signal-to-noise ratio. We therefore conclude that the quality of the image enhancement depends on the desired application. © 2020 Ruiz-Munoz et al. Applications in Plant Sciences is published by Wiley Periodicals LLC on behalf of the Botanical Society of America","","convolutional neural networks; generative adversarial networks; plant phenotyping; root phenotyping; super resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85087155288"
"Courtrai L.; Pham M.-T.; Lefèvre S.","Courtrai, Luc (6507861482); Pham, Minh-Tan (56070990300); Lefèvre, Sébastien (57203070803)","6507861482; 56070990300; 57203070803","Small object detection in remote sensing images based on super-resolution with auxiliary generative adversarial networks","2020","Remote Sensing","12","19","3152","1","19","18","10.3390/rs12193152","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092761573&doi=10.3390%2frs12193152&partnerID=40&md5=514218afa5c2d6b706c213d8d17edd87","This article tackles the problem of detecting small objects in satellite or aerial remote sensing images by relying on super-resolution to increase image spatial resolution, thus the size and details of objects to be detected. We show how to improve the super-resolution framework starting from the learning of a generative adversarial network (GAN) based on residual blocks and then its integration into a cycle model. Furthermore, by adding to the framework an auxiliary network tailored for object detection, we considerably improve the learning and the quality of our final super-resolution architecture, and more importantly increase the object detection performance. Besides the improvement dedicated to the network architecture, we also focus on the training of super-resolution on target objects, leading to an object-focused approach. Furthermore, the proposed strategies do not depend on the choice of a baseline super-resolution framework, hence could be adopted for current and future state-of-the-art models. Our experimental study on small vehicle detection in remote sensing data conducted on both aerial and satellite images (i.e., ISPRS Potsdam and xView datasets) confirms the effectiveness of the improved super-resolution methods to assist with the small object detection tasks. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Antennas; Image enhancement; Network architecture; Object recognition; Optical resolving power; Remote sensing; Small satellites; Adversarial networks; Aerial remote sensing; Detection performance; Image spatial resolution; Remote sensing data; Remote sensing images; Small object detection; Superresolution methods; Object detection","Auxiliary network; Cycle GAN; Deep learning; Generative adversarial network (GAN); Remote sensing; Small object detection; Super-resolution; Wasserstein GAN","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092761573"
"Bhattacharjee A.; Das S.","Bhattacharjee, Avishek (57209588102); Das, Sukhendu (55476994500)","57209588102; 55476994500","D2SC-GAN: Dual deep-shallow channeled generative adversarial network, for resolving low-resolution faces for recognition in classroom scenarios","2020","IEEE Transactions on Biometrics, Behavior, and Identity Science","2","3","9050650","223","234","11","10.1109/TBIOM.2020.2983524","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122047219&doi=10.1109%2fTBIOM.2020.2983524&partnerID=40&md5=4628a462934204c91d07d3bc693c3998","Face Recognition using convolutional neural networks have achieved considerable success in constrained environments in the recent past. However, the performance of these methods deteriorates in case of mismatch of training and test distributions, under classroom/surveillance scenarios. These test (probe) samples suffer from degradations such as noise, poor illumination, pose variations, occlusion, low-resolution (LR), blur as well as aliasing, when compared to the crisp, rich training (gallery) set, comprising mostly of high-resolution (HR) mugshot images captured in laboratory settings. To cope with this scenario, we propose a novel dual deep-shallow channeled generative adversarial network (D2SC-GAN) which performs supervised domain adaptation (DA) by mapping LR degraded probe samples to their corresponding HR gallery-like counterparts to perform closed-set face recognition. D2SC-GAN uses a multi-component loss function comprising of multi-resolution patchwise MSE and normalized chi-squared distance loss functions, along with a Kullback-Leibler divergence based loss function. Moreover, we propose a novel classroom face dataset called the Indian Classroom Face Dataset (ICFD), which, to the best of our knowledge, is a first of its kind and will be helpful to explore the challenges of face recognition when used for automatically recording the attendance in classroom conditions. The proposed network achieves superior results on five real-world face datasets when compared with recent state-of-the-art deep as well as shallow supervised domain adaptation (DA), super-resolution (SR), and degraded face recognition (DFR) methods, which show the effectiveness of our proposed method. © 2020 IEEE.","Convolutional neural networks; Probes; Adversarial networks; Domain adaptation; High resolution; Kullback Leibler divergence; Low resolution; Multicomponents; Pose variation; Super resolution; Face recognition","Classroom FR; Convolutional neural networks; Face generation; Face recognition; Generative adversarial networks","Article","Final","","Scopus","2-s2.0-85122047219"
"Yun J.U.; Jo B.; Park I.K.","Yun, Jung Un (57219115761); Jo, Byungho (57219109805); Park, In Kyu (8612277600)","57219115761; 57219109805; 8612277600","Joint Face Super-Resolution and Deblurring Using Generative Adversarial Network","2020","IEEE Access","8","","9181507","159661","159671","10","10.1109/ACCESS.2020.3020729","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091315432&doi=10.1109%2fACCESS.2020.3020729&partnerID=40&md5=31d8c055ab1e6488804ffcd63462d187","Facial image super-resolution (SR) is an important aspect of facial analysis, and it can contribute significantly to tasks such as face alignment, face recognition, and image-based 3D reconstruction. Recent convolutional neural network (CNN) based models have exhibited significant advancements by learning mapping relations using pairs of low-resolution (LR) and high-resolution (HR) facial images. However, because these methods are conventionally aimed at increasing the PSNR and SSIM metrics, the reconstructed HR images might be blurry and have an overall unsatisfactory perceptual quality even when state-of-the-art quantitative results are achieved. In this study, we address this limitation by proposing an adversarial framework intended to reconstruct perceptually high-quality HR facial images while simultaneously removing blur. To this end, a simple five-layer CNN is employed to extract feature maps from LR facial images, and this feature information is provided to two-branch encoder-decoder networks that generate HR facial images with and without blur. In addition, local and global discriminators are combined to focus on the reconstruction of HR facial structures. Both qualitative and quantitative results demonstrate the effectiveness of the proposed method for generating photorealistic HR facial images from a variety of LR inputs. Moreover, it was also verified, through a use case scenario that the proposed method can contribute more to the field of face recognition than existing approaches. © 2013 IEEE.","Convolutional neural networks; Image reconstruction; Optical character recognition; Optical resolving power; 3D reconstruction; Adversarial networks; Face super-resolution; Feature information; Mapping relation; Perceptual quality; Quantitative result; Use case scenario; Face recognition","deblurring; face recognition; Facial image super-resolution; generative adversarial network","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85091315432"
"Mostofa M.; Ferdous S.N.; Riggan B.S.; Nasrabadi N.M.","Mostofa, Moktari (57211073809); Ferdous, Syeda Nyma (57211070813); Riggan, Benjamin S. (56406510300); Nasrabadi, Nasser M. (7006312852)","57211073809; 57211070813; 56406510300; 7006312852","Joint-SRVDNet: Joint super resolution and vehicle detection network","2020","IEEE Access","8","","9079855","82306","82319","13","10.1109/ACCESS.2020.2990870","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084948518&doi=10.1109%2fACCESS.2020.2990870&partnerID=40&md5=576db9c0407e261502df5c6e8facb234","In many domestic and military applications, aerial vehicle detection and super-resolution algorithms are frequently developed and applied independently. However, aerial vehicle detection on super-resolved images remains a challenging task due to the lack of discriminative information in the super-resolved images. To address this problem, we propose a Joint Super-Resolution and Vehicle Detection Network (Joint-SRVDNet) that tries to generate discriminative, high-resolution images of vehicles from low-resolution aerial images. First, aerial images are up-scaled by a factor of 4x using a Multi-scale Generative Adversarial Network (MsGAN), which has multiple intermediate outputs with increasing resolutions. Second, a detector is trained on super-resolved images that are upscaled by factor 4x using MsGAN architecture and finally, the detection loss is minimized jointly with the super-resolution loss to encourage the target detector to be sensitive to the subsequent super-resolution training. The network jointly learns hierarchical and discriminative features of targets and produces optimal super-resolution results. We perform both quantitative and qualitative evaluation of our proposed network on VEDAI, xView and DOTA datasets. The experimental results show that our proposed framework achieves better visual quality than the state-of-the-art methods for aerial super-resolution with 4x up-scaling factor and improves the accuracy of aerial vehicle detection. © 2013 IEEE.","Antennas; Military applications; Military vehicles; Optical resolving power; Adversarial networks; Discriminative features; High resolution image; Qualitative evaluations; State-of-the-art methods; Super resolution algorithms; Target detectors; Vehicle detection; Aircraft detection","Aerial images; multi-scale generative adversarial network (MsGAN); super-resolution; vehicle detection","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85084948518"
"Alyafi B.; Diaz O.; Elangovan P.; Vilanova J.C.; Del Riego J.; Marti R.","Alyafi, Basel (57211473139); Diaz, Oliver (36172316800); Elangovan, Premkumar (19640003600); Vilanova, Joan C. (17836985700); Del Riego, Javier (37064464700); Marti, Robert (14048757500)","57211473139; 36172316800; 19640003600; 17836985700; 37064464700; 14048757500","Quality analysis of DCGAN-generated mammography lesions","2020","Proceedings of SPIE - The International Society for Optical Engineering","11513","","115130B","","","","10.1117/12.2560473","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086139155&doi=10.1117%2f12.2560473&partnerID=40&md5=053029122f57f8d4bce89b51d35da48f","Medical image synthesis has gained a great focus recently, especially after the introduction of Generative Adversarial Networks (GANs). GANs have been used widely to provide anatomically-plausible and diverse samples for augmentation and other applications, including segmentation and super resolution. In our previous work, Deep Convolutional GANs were used to generate synthetic mammogram lesions, masses mainly, that could enhance the classification performance in imbalanced datasets. In this new work, a deeper investigation was carried out to explore other aspects of the generated images evaluation, i.e., realism, feature space distribution, and observer studies. t-Stochastic Neighbor Embedding (t-SNE) was used to reduce the dimensionality of real and fake images to enable 2D visualisations. Additionally, two expert radiologists performed a realism-evaluation study. Visualisations showed that the generated images have a similar feature distribution of the real ones, avoiding outliers. Moreover, the Receiver Operating Characteristic (ROC) study showed that the radiologists could not, in many cases, distinguish between synthetic and real lesions, giving accuracies between 51% and 59% using a balanced sample set. © 2020 SPIE.","Classification (of information); Medical imaging; Stochastic systems; Visualization; Adversarial networks; Classification performance; Evaluation study; Feature distribution; Imbalanced Data-sets; Mammogram lesions; Receiver operating characteristics; Stochastic neighbor embedding; Quality control","Breast lesions; GANs; Image synthesis; Observer study; ROC curve; T-SNE","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086139155"
"Li J.; Cui R.; Li B.; Song R.; Li Y.; Dai Y.; Du Q.","Li, Jiaojiao (55934244200); Cui, Ruxing (57211523782); Li, Bo (57188584536); Song, Rui (36460216100); Li, Yunsong (55986546100); Dai, Yuchao (24829251300); Du, Qian (7202060063)","55934244200; 57211523782; 57188584536; 36460216100; 55986546100; 24829251300; 7202060063","Hyperspectral Image Super-Resolution by Band Attention through Adversarial Learning","2020","IEEE Transactions on Geoscience and Remote Sensing","58","6","8960413","4304","4318","14","10.1109/TGRS.2019.2962713","45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085605482&doi=10.1109%2fTGRS.2019.2962713&partnerID=40&md5=618f3799f9d8c01788e94a6aaa7b623e","Hyperspectral image (HSI) super-resolution (SR) is a challenging task due to the problems of texture blur and spectral distortion when the upscaling factor is large. To meet these two challenges, band attention through the adversarial learning method is proposed in this article. First, we put the SR process in a generative adversarial network (GAN) framework, so that the resulted high-resolution HSI can keep more texture details. Second, different from the other band-by-band SR method, the input of our method is of full bands. In order to explore the correlation of spectral bands and avoid the spectral distortion, a band attention mechanism is proposed in our generative network. A series of spatial-spectral constraints or loss functions is imposed to guide the training of our generative network so as to further alleviate spectral distortion and texture blur. The experiments on the Pavia and Cave data sets demonstrate that the proposed GAN-based SR method can yield very high-quality results, even under large upscaling factor (e.g., 8×). More importantly, it can outperform the other state-of-the-art methods by a margin which demonstrates its superiority and effectiveness. © 1980-2012 IEEE.","Italy; Lombardy; Pavia; Optical resolving power; Spectroscopy; Textures; Adversarial learning; Adversarial networks; Attention mechanisms; Image super resolutions; Spectral constraints; Spectral distortions; State-of-the-art methods; Super resolution; data assimilation; data interpretation; image analysis; image resolution; multispectral image; qualitative analysis; spectral analysis; upscaling; Learning systems","Adversarial learning; band attention; hyperspectral image (HSI) super-resolution (SR)","Article","Final","","Scopus","2-s2.0-85085605482"
"Feng C.-H.; Hung Y.-H.; Yang C.-K.; Chen L.-C.; Hsu W.-C.; Lin S.-H.","Feng, Chia-Hui (57195480525); Hung, Yu-Hsiu (35181495100); Yang, Chao-Kuang (57218312028); Chen, Liang-Chi (57218310770); Hsu, Wen-Cheng (57218310761); Lin, Shih-Hao (57218312081)","57195480525; 35181495100; 57218312028; 57218310770; 57218310761; 57218312081","Applying Holo360 Video and Image Super-Resolution Generative Adversarial Networks to Virtual Reality Immersion","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12181 LNCS","","","569","584","15","10.1007/978-3-030-49059-1_42","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088752451&doi=10.1007%2f978-3-030-49059-1_42&partnerID=40&md5=20c60e38027c6e8f22a0933637ed4dab","Super-resolution deep learning methods focus on image processing solutions and discussions in two-dimensional super-resolution image processing NOT for 360 equirectangular images. Therefore, the motivation of this research is to establish the deep learning network model Holo360 SRGAN and data set of 360 equirectangular images, and observe whether the sharpness and noise of Holo360 SRGAN compared with the original image reach the optical verification standard. The results of this study point out two significant points: 1) For a convolution training core neuron with the best model architecture of Holo360 SRGAN with 360 images 8 K (8192 × 4096 px), FOV: 360°, the expanded the convolution core neuron size as 5 × 5 to contains more learning features. And 2) Holo360 SRGAN image experiment results, 6 ROI optical analysis clarity increased by 27%, and sharpness increased by 42%. The experimental original image noise SNR is 28.2 dB, and the Holo360 SRGAN (×2) noise SNR is 36.8 dB, so it is increased by +8.6 dB, and the amount of image detail is also increased. Contributions enhance the super-resolution visual experience of equirectangular video or image. © 2020, Springer Nature Switzerland AG.","Computer architecture; Convolution; Deep learning; Human computer interaction; Learning systems; Optical data processing; Optical resolving power; Signal to noise ratio; User experience; Virtual reality; Adversarial networks; Image super resolutions; Optical analysis; Optical verification; Significant points; Super resolution; Super-resolution image processing; Visual experiences; Image enhancement","360 equirectangular; Super resolution GAN; Tensorlayer; Virtual reality","Conference paper","Final","","Scopus","2-s2.0-85088752451"
"An L.; Dai F.; Yuan Y.","An, Lingran (57224969154); Dai, Fengzhi (22733481500); Yuan, Yasheng (57219433875)","57224969154; 22733481500; 57219433875","Research on image super-resolution reconstruction based on deep learning","2020","Proceedings of International Conference on Artificial Life and Robotics","2020","","","640","643","3","10.5954/ICAROB.2020.OS9-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108803432&doi=10.5954%2fICAROB.2020.OS9-7&partnerID=40&md5=90e08df0d11f1d75217d8958fdd25c85","This paper mainly applies the relevant theories of deep learning to image super-resolution reconstruction technology. By comparing four classical network models used for image super-resolution (SR), finally a generative adversarial network (GAN) is selected to implement image super-resolution, which is called SRGAN. SRGAN consists of a generator and a discriminator that uses both perceived loss and counter loss to enhance the realism of the output image in detail. The data sets used by the training network are partly from the network and partly from the artificial. Compared with other network models, the final trained SRGAN network is above average in PSNR and SSIM values. Although it is not optimal, the output high-resolution images are the best in the subjective feelings of human eyes, and the reconstruction effect in the image details is far higher than that of other networks. © The 2020 International Conference on Artificial Life and Robotics (ICAROB2020).","","Deep learning; Generative Adversarial Networks; Neural network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85108803432"
"Zhang K.; Sumbul G.; Demir B.","Zhang, Kexin (57221087691); Sumbul, Gencer (57196192158); Demir, Begum (15131434800)","57221087691; 57196192158; 15131434800","An Approach to Super-Resolution of Sentinel-2 Images Based on Generative Adversarial Networks","2020","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium, M2GARSS 2020 - Proceedings","","","9105165","69","72","3","10.1109/M2GARSS47143.2020.9105165","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086704517&doi=10.1109%2fM2GARSS47143.2020.9105165&partnerID=40&md5=9f39cc5fcc004c7aac1a60ab0390b2aa","This paper presents a generative adversarial network based super-resolution (SR) approach (which is called as S2GAN) to enhance the spatial resolution of Sentinel-2 spectral bands. The proposed approach consists of two main steps. The first step aims to increase the spatial resolution of the bands with 20m and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To this end, we introduce a generator network that performs SR on the lower resolution bands with the guidance of the bands associated to 10m spatial resolution by utilizing the convolutional layers with residual connections and a long skip-connection between inputs and outputs. The second step aims to distinguish SR bands from their ground truth bands. This is achieved by the proposed discriminator network, which alternately characterizes the high level features of the two sets of bands and applying binary classification on the extracted features. Then, we formulate the adversarial learning of the generator and discriminator networks as a min-max game. In this learning procedure, the generator aims to produce realistic SR bands as much as possible so that the discriminator incorrectly classifies SR bands. Experimental results obtained on different Sentinel-2 images show the effectiveness of the proposed approach compared to both conventional and deep learning based SR approaches. © 2020 IEEE.","Deep learning; Geology; Image resolution; Optical resolving power; Remote sensing; Adversarial learning; Adversarial networks; Binary classification; High-level features; Learning procedures; Lower resolution; Spatial resolution; Super resolution; Discriminators","generative adversarial network; remote sensing; Sentinel-2 images; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85086704517"
"Lin Z.; Jia J.; Gao W.; Huang F.","Lin, Zhongqi (57203061357); Jia, Jingdun (55349327200); Gao, Wanlin (8931933300); Huang, Feng (56643632000)","57203061357; 55349327200; 8931933300; 56643632000","A novel quadruple generative adversarial network for semi-supervised categorization of low-resolution images","2020","Neurocomputing","415","","","266","285","19","10.1016/j.neucom.2020.05.050","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089265312&doi=10.1016%2fj.neucom.2020.05.050&partnerID=40&md5=7953e8c8d1458717a37fd553212b8c03","In order to make utilization of unlabeled low-resolution (LR) images to shape discriminative models, we present quadruple generative adversarial network (Q-GAN), a game-theoretical framework for implementing semi-supervised categorization of LR images. It can realize photo-realistic image super-resolution (SR) and semi-supervised pattern recognition simultaneously. We consider our pipeline as a four-player optimization-based formulation, which consists of four vital components, i.e., a refiner for image SR and generation, a discriminator for identifying high-resolution (HR) samples and another for identifying true (original) samples, a classifier for label prediction. The refiner and two discriminators characterize the conditional distributions between images and labels, whilst the classifier solely focuses on predicting real image-label pairs. We select those high-quality super-solved images with ground-truth labels for data supplement. We customize the global optimization objective function as well as the training procedure to ensure model approximates the posterior distribution of latent variables given true data in a semi-supervised manner. Experimental results demonstrate that Q-GAN can simultaneously (1) deliver the promising categorization performance among state-of-the-arts, i.e., validation accuracy achieves 92.18% and testing accuracy achieves 90.63%, and (2) recover fine-grained textures with high peak signal-to-noise ratios (PNSRs) and structural similarities (SSIMs) from heavily downsampled testing images of hand-crafted dataset and public benchmarks. © 2020","Arts computing; Benchmarking; Game theory; Global optimization; Pattern recognition; Refining; Statistical tests; Textures; Conditional distribution; Discriminative models; Game-theoretical framework; Low resolution images; Optimization objective function; Peak signal to noise ratio; Photorealistic images; Posterior distributions; article; classifier; deep learning; human; signal noise ratio; Signal to noise ratio","Deep learning; Generative adversarial networks; Image categorization; Image super-resolution; Semi-supervised learning","Article","Final","","Scopus","2-s2.0-85089265312"
"","","","11th International Workshop on Machine Learning in Medical Imaging, MLMI 2020, held in conjunction with the 23rd International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2020","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12436 LNCS","","","","","682","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709915&partnerID=40&md5=bbf0611524d2ce2a9d3fd5b51530ce73","The proceedings contain 68 papers. The special focus in this conference is on Machine Learning in Medical Imaging. The topics include: Out-of-Distribution Detection for Skin Lesion Images with Deep Isolation Forest; A 3D+2D CNN Approach Incorporating Boundary Loss for Stroke Lesion Segmentation; Linking Adolescent Brain MRI to Obesity via Deep Multi-cue Regression Network; robust Multiple Sclerosis Lesion Inpainting with Edge Prior; segmentation to Label: Automatic Coronary Artery Labeling from Mask Parcellation; GSR-Net: Graph Super-Resolution Network for Predicting High-Resolution from Low-Resolution Functional Brain Connectomes; anatomy-Aware Cardiac Motion Estimation; division and Fusion: Rethink Convolutional Kernels for 3D Medical Image Segmentation; LDGAN: Longitudinal-Diagnostic Generative Adversarial Network for Disease Progression Prediction with Missing Structural MRI; Unsupervised MRI Homogenization: Application to Pediatric Anterior Visual Pathway Segmentation; error Attention Interactive Segmentation of Medical Image Through Matting and Fusion; boundary-Aware Network for Kidney Tumor Segmentation; o-Net: An Overall Convolutional Network for Segmentation Tasks; label-Driven Brain Deformable Registration Using Structural Similarity and Nonoverlap Constraints; eczemaNet: Automating Detection and Severity Assessment of Atopic Dermatitis; deep Distance Map Regression Network with Shape-Aware Loss for Imbalanced Medical Image Segmentation; Joint Appearance-Feature Domain Adaptation: Application to QSM Segmentation Transfer; exploring Functional Difference Between Gyri and Sulci via Region-Specific 1D Convolutional Neural Networks; detection of Ischemic Infarct Core in Non-contrast Computed Tomography; bayesian Neural Networks for Uncertainty Estimation of Imaging Biomarkers; extended Capture Range of Rigid 2D/3D Registration by Estimating Riemannian Pose Gradients; A Novel fMRI Representation Learning Framework with GAN; structural Connectivity Enriched Functional Brain Network Using Simplex Regression with GraphNet.","","","Conference review","Final","","Scopus","2-s2.0-85092709915"
"Ji H.; Gao Z.; Mei T.; Ramesh B.","Ji, Hong (57205763449); Gao, Zhi (55256514200); Mei, Tiancan (8914886000); Ramesh, Bharath (56442158400)","57205763449; 55256514200; 8914886000; 56442158400","Vehicle Detection in Remote Sensing Images Leveraging on Simultaneous Super-Resolution","2020","IEEE Geoscience and Remote Sensing Letters","17","4","8792159","676","680","4","10.1109/LGRS.2019.2930308","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081677087&doi=10.1109%2fLGRS.2019.2930308&partnerID=40&md5=5a95ef4a661454d292e0a2fe01bbb40d","Owing to the relatively small size of vehicles in remote sensing images, lacking sufficient detailed appearance to distinguish vehicles from similar objects, the detection performance is still far from satisfactory compared with the detection results on everyday images. Inspired by the positive effects of super-resolution convolutional neural network (SRCNN) for object detection and the stunning success of deep CNN techniques, we apply generative adversarial network frameworks to realize simultaneous SRCNN and vehicle detection in an end-to-end manner, and the detection loss is backpropagated into the SRCNN during training to facilitate detection. In particular, our work is unsupervised and bypasses the requirement of low-/high-resolution image pairs during the training stage, achieving increased generality and applicability. Extensive experiments on representative data sets demonstrate that our method outperforms the state-of-the-art detectors. (The source code will be made available after the review process. © 2004-2012 IEEE.","Convolution; Object detection; Optical resolving power; Remote sensing; Vehicles; Feature fusion; Region-based; Remote sensing images; Super resolution; Vehicle detection; artificial neural network; detection method; image resolution; remote sensing; satellite imagery; Convolutional neural networks","Faster region-based convolutional neural network (R-CNN); feature fusion; remote sensing images; super-resolution convolutional neural network (SRCNN); vehicle detection","Article","Final","","Scopus","2-s2.0-85081677087"
"Liu B.; Li H.; Zhou Y.; Peng Y.; Elazab A.; Wang C.","Liu, Bo (57221214127); Li, Heng (57220036007); Zhou, Yutao (57220033102); Peng, Yuqing (57220025030); Elazab, Ahmed (56523141500); Wang, Changmiao (57226651647)","57221214127; 57220036007; 57220033102; 57220025030; 56523141500; 57226651647","A super resolution method for remote sensing images based on cascaded conditional wasserstein GANs","2020","2020 3rd IEEE International Conference on Information Communication and Signal Processing, ICICSP 2020","","","9232066","284","289","5","10.1109/ICICSP50920.2020.9232066","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096514768&doi=10.1109%2fICICSP50920.2020.9232066&partnerID=40&md5=8a44523392137bdab79644c5f0436c29","High-resolution (HR) remote sensing imagery is quite beneficial for subsequent interpretation. Obtaining HR images can be achieved by upgrading the imaging device. Yet, the cost to perform this task is very huge. Thus, it is necessary to obtain HR images from low-resolution (LR) ones. In the literature, the super-resolution image reconstruction methods based on deep learning have unparalleled advantages in comparison to traditional reconstruction methods. This work is inspired by these current mainstream methods and proposes a novel cascaded conditional Wasserstein generative adversarial network (CCWGAN) architecture with the residual dense block to generate high quality remote sensing images. We validate the proposed method on the NWPU VHR-10 dataset. Experimental results show our CCWGAN method has superior performance compared with the state-of-the-art GAN methods. © 2020 IEEE.","Deep learning; Image reconstruction; Optical resolving power; Adversarial networks; High resolution; Reconstruction method; Remote sensing imagery; Remote sensing images; State of the art; Super-resolution image reconstruction; Superresolution methods; Remote sensing","Cascaded conditional generative adversarial networks; Remote sensing images; Residual dense block; Wasserstein generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85096514768"
"Hou Y.; Zhang J.","Hou, Yangshuan (36494442100); Zhang, Jishuai (57218298226)","36494442100; 57218298226","Unsupervised remote sensing image super-resolution method based on adaptive domain distance measurement network","2020","Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020","","","9131243","256","259","3","10.1109/AEMCSE50948.2020.00062","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088658770&doi=10.1109%2fAEMCSE50948.2020.00062&partnerID=40&md5=1f3e8c001c401b0f7b410d6433581906","Compared with supervised learning, unsupervised learning is more practical; however, the associated training process is more difficult and complex. To solve the problems of unstable training and insufficient diversity of generative adversarial networks (GAN), which are widely used to realize unsupervised learning, we propose a novel unsupervised remote sensing image super-resolution method based on a reverse generating network module and the adaptive domain distance measurement network. The discriminant network of GAN is considered as a tool to measure a certain image attribute instead of the original GAN binary classification network. Furthermore, the adaptive domain distance measurement network is used to back feed the information of a high-resolution image to guide the optimization of the generating network. The results of experiments performed on various datasets demonstrate the effectiveness of the proposed method. © 2020 IEEE.","Distance measurement; Optical resolving power; Software engineering; Unsupervised learning; Adversarial networks; Binary classification; High resolution image; Image attributes; Remote sensing images; Training process; Remote sensing","Domain; GAN; Remote senseng; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85088658770"
"Liu J.; Liu H.; Zheng X.; Han J.","Liu, Jianyong (57218596201); Liu, Heng (57022065500); Zheng, Xiaoyu (57212537068); Han, Jungong (14522692900)","57218596201; 57022065500; 57212537068; 14522692900","Exploring multi-scale deep encoder-decoder and patchgan for perceptual ultrasound image super-resolution","2020","Communications in Computer and Information Science","1265 CCIS","","","47","59","12","10.1007/978-981-15-7670-6_5","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089723406&doi=10.1007%2f978-981-15-7670-6_5&partnerID=40&md5=72b17a9f4dd7dadb2328bb3f897bfef3","Ultrasound visual imaging is currently one of three mainstream image diagnosis technologies in the medical industry, but due to the limitations of sensors, transmission media and ultrasound characteristics, the quality of ultrasound imaging may be poor, especially its low spatial resolution. We incorporate, in this paper, a new multi-scale deep encoder-decoder structure into a PatchGAN (patch generative adversarial network) based framework for fast perceptual ultrasound image super-resolution (SR). Specifically, the entire algorithm is carried out in two stages: ultrasound SR image generation and image refinement. In the first stage, a multi-scale deep encoder-decoder generator is employed to accurately super-resolve the LR ultrasound images. In the second stage, we advocate the confrontational characteristics of the discriminator to impel the generator such that more realistic high-resolution (HR) ultrasound images can be produced. The assessments in terms of PSNR/IFC/SSIM, inference efficiency and visual effects demonstrate its effectiveness and superiority, when compared to the most state-of-the-art methods. © Springer Nature Singapore Pte Ltd. 2020.","Decoding; Diagnosis; Image resolution; Optical resolving power; Signal encoding; Ultrasonic imaging; Adversarial networks; Inference efficiency; Medical industries; Spatial resolution; State-of-the-art methods; Transmission media; Ultrasound images; Ultrasound imaging; Medical imaging","Deep encoder-decoder; Multi-scale; PatchGAN; Ultrasound image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85089723406"
"Cheng J.; Liu J.; Xu Z.; Shen C.; Kuang Q.","Cheng, Jianxin (57215698381); Liu, Jin (55978402400); Xu, Zhou (57212062746); Shen, Chenkai (57215694763); Kuang, Qiuming (57191916860)","57215698381; 55978402400; 57212062746; 57215694763; 57191916860","Generating High-Resolution Climate Prediction through Generative Adversarial Network","2020","Procedia Computer Science","174","","","123","127","4","10.1016/j.procs.2020.06.067","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122189025&doi=10.1016%2fj.procs.2020.06.067&partnerID=40&md5=a0674beea9754d809c1b9d881188c281","Downscaling technology is always used for high-resolution climate prediction, and this technology can generate small-scale regional climate prediction from large-scale climate output information. Inspired by image super resolution, we propose to apply the super-resolution models to downscaling technology. However, some unpleasant artifacts always appear in the high-resolution climate images generated by exiting super-resolution models. To further eliminate these unpleasant artifacts, we innovatively apply the super-resolution generative adversarial network (SRGAN) to climate prediction. SRGAN adopts a perceptual loss function which consists of an adversarial loss and a content loss, which can recover more high-frequency details in the generated high-resolution images. Besides, we propose a method to fuse climate data with related meteorological factors to generate climate images, this measure can improve the accuracy of climate prediction. Finally, extensive experimental results on climate datasets show that SRGAN performs better than most super-resolution approaches in climate prediction. © 2020 The Authors. Published by Elsevier B.V.","Climate models; Forecasting; Image enhancement; Optical resolving power; Climate prediction; Down-scaling; High resolution; Image super resolutions; Large-scales; Loss functions; Regional climate; Small scale; Super-resolution models; Superresolution; Generative adversarial networks","Climate Prediction; Generative Adversarial Network; Image Super Resolution","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122189025"
"Song T.-A.; Chowdhury S.R.; Yang F.; Dutta J.","Song, Tzu-An (57211240200); Chowdhury, Samadrita Roy (57205075263); Yang, Fan (57224324302); Dutta, Joyita (24832908700)","57211240200; 57205075263; 57224324302; 24832908700","PET image super-resolution using generative adversarial networks","2020","Neural Networks","125","","","83","91","8","10.1016/j.neunet.2020.01.029","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079343344&doi=10.1016%2fj.neunet.2020.01.029&partnerID=40&md5=91d92a3462937de016f1cdcda500daf9","The intrinsically low spatial resolution of positron emission tomography (PET) leads to image quality degradation and inaccurate image-based quantitation. Recently developed supervised super-resolution (SR) approaches are of great relevance to PET but require paired low- and high-resolution images for training, which are usually unavailable for clinical datasets. In this paper, we present a self-supervised SR (SSSR) technique for PET based on dual generative adversarial networks (GANs), which precludes the need for paired training data, ensuring wider applicability and adoptability. The SSSR network receives as inputs a low-resolution PET image, a high-resolution anatomical magnetic resonance (MR) image, spatial information (axial and radial coordinates), and a high-dimensional feature set extracted from an auxiliary CNN which is separately-trained in a supervised manner using paired simulation datasets. The network is trained using a loss function which includes two adversarial loss terms, a cycle consistency term, and a total variation penalty on the SR image. We validate the SSSR technique using a clinical neuroimaging dataset. We demonstrate that SSSR is promising in terms of image quality, peak signal-to-noise ratio, structural similarity index, contrast-to-noise ratio, and an additional no-reference metric developed specifically for SR image quality assessment. Comparisons with other SSSR variants suggest that its high performance is largely attributable to simulation guidance. © 2020 Elsevier Ltd","Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neural Networks, Computer; Positron-Emission Tomography; Signal-To-Noise Ratio; Image resolution; Magnetic resonance; Neuroimaging; Optical resolving power; Polyethylene terephthalates; Positron emission tomography; Signal to noise ratio; High dimensional feature; Image quality assessment; Multi-modality imaging; Peak signal to noise ratio; Positron emission tomography (PET); Self-supervised; Structural similarity indices; Super resolution; article; contrast to noise ratio; image quality; loss of function mutation; neuroimaging; nuclear magnetic resonance imaging; positron emission tomography; punishment; signal noise ratio; simulation; image processing; positron emission tomography; procedures; Image quality","CNN; GAN; Multimodality imaging; PET; Self-supervised; Super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85079343344"
"Zeng H.; Zhang X.; Yu Z.; Wang Y.","Zeng, Huimin (57220805373); Zhang, Xinliang (57220804585); Yu, Zhibin (36999020600); Wang, Yubo (42662312900)","57220805373; 57220804585; 36999020600; 42662312900","SR-ITM-GAN: Learning 4K UHD HDR with a generative adversarial network","2020","IEEE Access","8","","","182815","182827","12","10.1109/ACCESS.2020.3028584","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102800865&doi=10.1109%2fACCESS.2020.3028584&partnerID=40&md5=dc40579329fa35312e07a7d1425c14f9","Currently, high dynamic range (HDR) videos with high resolution (HR) have become popular due to the display and the rendered technological advancements. However, making ultra-high definition (UHD) with HDR videos is expensive. The legacy low-resolution (LR) standard dynamic range (SDR) format is still largely used in practice. It is necessary to search for a solution to transform LR SDR videos into UHD HDR format. In this paper, we consider joint super resolution and learning inverse tone mapping an issue of high-frequency reconstruction and local contrast enhancement, and we propose an architecture based on a generative adversarial network to apply joint SR-ITM learning. Specifically, we include the residual ResNeXt block (RRXB) as a basic module to better capture high-frequency textures and adopt YUV interpolation to achieve local contrast enhancement. By adopting a generative adversarial network as a pivotal training mechanism, our designs show advantages in both integration and performance. Our code is now available on GitHub: SR-ITM-GAN. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Adversarial networks; Architecture-based; High dynamic range; High frequency reconstruction; Inverse tone mappings; Local Contrast Enhancement; Technological advancement; Ultra high definition (UHD); Textures","Generative adversarial network; High dynamic range; Inverse tone mapping; Super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85102800865"
"Vassilo K.; Heatwole C.; Taha T.; Mehmood A.","Vassilo, Kyle (57218712112); Heatwole, Cory (57218710322); Taha, Tarek (23013518500); Mehmood, Asif (36133731600)","57218712112; 57218710322; 23013518500; 36133731600","Multi-step reinforcement learning for single image super-resolution","2020","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2020-June","","9150927","2160","2168","8","10.1109/CVPRW50498.2020.00264","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090158212&doi=10.1109%2fCVPRW50498.2020.00264&partnerID=40&md5=7422e25ad7176960e4fac418a9cc1084","Deep Learning (DL) has become prevalent in today's image processing research due to its power and versatility. It has dominated the Single Image Super-Resolution (SISR) field with its ability to obtain High-Resolution (HR) images from their Low-Resolution (LR) counterparts, particularly using Generative Adversarial Networks (GANs). Interest in SISR comes from its potential to increase the performance of supplementary image processing tasks such as object detection, localization, and classification. This research applies a multi-agent Reinforcement Learning (RL) algorithm to SISR, creating an advanced ensemble approach for combining powerful GANs. In our implementation each agent chooses a particular action from a fixed action set comprised of results from existing GAN SISR algorithms to update its pixel values. The pixel-wise or patch-wise arrangement of agents and rewards encourages the algorithm to learn a strategy to increase the resolution of an image by choosing the best pixel values from each option. © 2020 IEEE.","Computer vision; Deep learning; Multi agent systems; Object detection; Optical resolving power; Pixels; Software agents; Adversarial networks; Ensemble approaches; High resolution image; Low resolution; Multi-agent reinforcement learning; Multi-step; Pixel values; Single images; Reinforcement learning","","Conference paper","Final","","Scopus","2-s2.0-85090158212"
"Ji Leong W.; Joseph Horgan H.","Ji Leong, Wei (57219904637); Joseph Horgan, Huw (16205109500)","57219904637; 16205109500","DeepBedMap: A deep neural network for resolving the bed topography of Antarctica","2020","Cryosphere","14","11","","3687","3705","18","10.5194/tc-14-3687-2020","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096017317&doi=10.5194%2ftc-14-3687-2020&partnerID=40&md5=1a4344613cbc69353ef3ed92854b30ba","To resolve the bed elevation of Antarctica, we present DeepBedMap - a novel machine learning method that can produce Antarctic bed topography with adequate surface roughness from multiple remote sensing data inputs. The super-resolution deep convolutional neural network model is trained on scattered regions in Antarctica where high-resolution (250 m) ground-truth bed elevation grids are available. This model is then used to generate high-resolution bed topography in less surveyed areas. DeepBedMap improves on previous interpolation methods by not restricting itself to a low-spatial-resolution (1000 m) BEDMAP2 raster image as its prior image. It takes in additional high-spatialresolution datasets, such as ice surface elevation, velocity and snow accumulation, to better inform the bed topography even in the absence of ice thickness data from direct icepenetrating-radar surveys. The DeepBedMap model is based on an adapted architecture of the Enhanced Super-Resolution Generative Adversarial Network, chosen to minimize perpixel elevation errors while producing realistic topography. The final product is a four-times-upsampled (250 m) bed elevation model of Antarctica that can be used by glaciologists interested in the subglacial terrain and by ice sheet modellers wanting to run catchment- or continent-scale ice sheet model simulations. We show that DeepBedMap offers a rougher topographic profile compared to the standard bicubically interpolated BEDMAP2 and BedMachine Antarctica and envision it being used where a high-resolution bed elevation model is required.  © Author(s) 2020.","Antarctica; artificial neural network; cryosphere; topography","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096017317"
"Hu X.; Cai Y.; Wang H.; Peng Y.; Zhang Y.","Hu, Xiaowan (57220078037); Cai, Yuanhao (57219633524); Wang, Haoqian (56516241700); Peng, Yanbin (57217187289); Zhang, Yulun (56928496400)","57220078037; 57219633524; 56516241700; 57217187289; 56928496400","EG2N: Enhanced gradient guiding network for single MR image super-resolution","2020","Proceedings of SPIE - The International Society for Optical Engineering","11550","","115500I","","","","10.1117/12.2575261","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096775369&doi=10.1117%2f12.2575261&partnerID=40&md5=52f89547c94eca1f2d04def0b7d9f743","During magnetic resonance imaging (MRI), the strong response to the signal is usually displayed as structural edges and textures, which is important for distinguishing different tissues and lesions. In the current super-resolution (SR) methods with the usage of deep learning, some low-level structural information tends to gradually disappear as the network deepens, resulting in excessive smoothness in high-frequency regions. This phenomenon is particularly noticeable in MRI with poor brightness contrast and small gray dynamic range. Although the generative adversarial network (GAN) can repair structured textures well in natural images, it is likely to learn patterns that do not exist in the images, which poses risks to the reconstruction of medical images. Therefore, we propose an enhanced gradient guiding network (EG2N) to alleviate these problems. On the one hand, for improving the contrast and suppress the noise effectively, we use a multi-scale wavelet enhancement for preprocessing, where the enhanced gradient map is considered as the structural prior. On the other hand, blindly using dense connections in the feed-forward network will bring about redundancy, so structural features from an additional branch are added to specific layers as a supplement to high-level features and constrain optimization. We add a feedback mechanism to promote cross-layer flow between low-level and high-level features. In addition, the perceptual loss is added to avoid distortion caused by excessive smoothing. The experimental results show that our method achieves the best visual results and excellent performance compared with state-of-the-art methods on most popular MR images SR benchmarks. © 2020 SPIE","Benchmarking; Deep learning; Feedforward neural networks; Magnetic resonance imaging; Medical imaging; Multimedia systems; Optical resolving power; Textures; Adversarial networks; Feed-forward network; Feedback mechanisms; High-level features; Multi-scale wavelet; State-of-the-art methods; Structural feature; Structural information; Image enhancement","Enhanced Gradient; Feedback Mechanism; Magnetic Resonance; Super-Resolution; Wavelet Transform","Conference paper","Final","","Scopus","2-s2.0-85096775369"
"Abdelmaksoud M.; Nabil E.; Farag I.; Hameed H.A.","Abdelmaksoud, Mohamed (57221995486); Nabil, Emad (55351831400); Farag, Ibrahim (35811859100); Hameed, Hala Abdel (57217204944)","57221995486; 55351831400; 35811859100; 57217204944","A Novel Neural Network Method for Face Recognition with a Single Sample per Person","2020","IEEE Access","8","","9105006","102212","102221","9","10.1109/ACCESS.2020.2999030","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086710846&doi=10.1109%2fACCESS.2020.2999030&partnerID=40&md5=ffc751f255915aa730b777e76e73e446","Face Recognition (FR) problem is one of the significant fields in computer vision. FR is used to identify the faces that appear over distributed cameras over the network. The problem of face recognition can be divided into two categories, the first is recognition with more than one sample per person, which can be called traditional face recognition problem. The second is the recognition of faces using only a Single Sample Per Person (SSPP). The efficiency of face recognition systems decreases because of limited references especially (SSPP) and faces taken in the Operational Domain (OD) different from faces in the Enrollment Domain (ED) in illumination, pose, low-resolution, and blurriness. This paper proposed a method that deals with all problems related to face recognition with SSPP. 3D face reconstruction is used to increase the reference gallery set with different poses and generate a design domain dictionary to overcome the problem of limited reference. Besides, the design domain dictionary is used to feed different deep learning models. Face illumination transfer techniques are utilized to overcome the illumination problem. Labeled Faces in the Wild (LFW) dataset is used to train Super-Resolution Generative Adversarial Network (SRGAN) to overcome the low-resolution problem. Deblur Generative Adversarial Network (DeblurGAN) is trained on the LFW dataset to overcome the problem of blurriness. The proposed method is evaluated using the Chokepoint dataset and COX-S2V dataset. The final results confirm an overall enhancement in accuracy compared to techniques that use SSPP for face recognition (generic learning and face synthesizing approaches). Also, the proposed method outperforms of Traditional and Deep Learning (TDL) method accuracy, which uses SSPP for face recognition. © 2013 IEEE.","Deep learning; Learning systems; Neural networks; 3D face reconstruction; Adversarial networks; Face recognition systems; Face synthesizing; Labeled faces in the wilds (LFW); Novel neural network; Operational domains; Transfer technique; Face recognition","Deep learning; Face recognition; Generative adversarial network (GAN); Illumination transferring; Single sample per person (SSPP)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85086710846"
"Fang Y.; Ran Q.; Li Y.","Fang, Yuchun (55469302300); Ran, Qicai (57220039970); Li, Yifan (57196350085)","55469302300; 57220039970; 57196350085","Fractal Residual Network for Face Image Super-Resolution","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12396 LNCS","","","15","26","11","10.1007/978-3-030-61609-0_2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096536301&doi=10.1007%2f978-3-030-61609-0_2&partnerID=40&md5=55efe242cb53017df71b2f9adf6af080","Recently, many Convolutional Neural Network (CNN) algorithms have been proposed for image super-resolution, but most of them aim at architecture or natural scene images. In this paper, we propose a new fractal residual network model for face image super-resolution, which is very useful in the domain of surveillance and security. The architecture of the proposed model is composed of multi-branches. Each branch is incrementally cascaded with multiple self-similar residual blocks, which makes the branch appears as a fractal structure. Such a structure makes it possible to learn both global residual and local residual sufficiently. We propose a multi-scale progressive training strategy to enlarge the image size and make the training feasible. We propose to combine the loss of face attributes and face structure to refine the super-resolution results. Meanwhile, adversarial training is introduced to generate details. The results of our proposed model outperform other benchmark methods in qualitative and quantitative analysis. © 2020, Springer Nature Switzerland AG.","Convolutional neural networks; Fractals; Optical resolving power; Face structure; Fractal structures; Image super resolutions; Natural scene images; Network modeling; Qualitative and quantitative analysis; Super resolution; Training strategy; Network architecture","Face super-resolution; Fractal block; Generative adversarial networks","Conference paper","Final","","Scopus","2-s2.0-85096536301"
"Chen W.; Liu C.; Yan Y.; Jin L.; Sun X.; Peng X.","Chen, Wenhui (57217059491); Liu, Chuangchuang (57208860339); Yan, Yitong (57208862449); Jin, Longcun (35110541800); Sun, Xianfang (7405626287); Peng, Xinyi (7401594040)","57217059491; 57208860339; 57208862449; 35110541800; 7405626287; 7401594040","Guided dual networks for single image super-resolution","2020","IEEE Access","8","","9097227","93608","93620","12","10.1109/ACCESS.2020.2995175","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085986645&doi=10.1109%2fACCESS.2020.2995175&partnerID=40&md5=3c75c85f35c480dba62c7f114959589f","The PSNR-oriented super-resolution (SR) methods pursue high reconstruction accuracy, but tend to produce over-smoothed results and lose plenty of high-frequency details. The GAN-based SR methods aim to generate more photo-realistic images, but the hallucinatory details are often accompanied with unsatisfying artifacts and noise. To address these problems, we propose a guided dual super-resolution network (GDSR), which exploits the advantages of both the PSNR-oriented and the GAN-based methods to achieve a good trade-off between reconstruction accuracy and perceptual quality. Specifically, our network contains two branches, where one is trained to extract global information and the other to focus on detail information. In this way, our network simultaneously generates SR images with high accuracy and satisfactory visual quality. To obtain more high-frequency features, we use the global features extracted from the low-frequency branch to guide the training of the high-frequency branch. Besides, our method utilizes a mask network to adaptively recover the final super-resolved image. Extensive experiments on several standard benchmarks show that our proposed method achieves better performance compared with state-of-the-art methods. The source code and the results of our GDSR are available at https://github.com/wenchen4321/GDSR. © 2013 IEEE.","Economic and social effects; Image resolution; Optical resolving power; Global informations; High frequency HF; Perceptual quality; Photorealistic images; Reconstruction accuracy; State-of-the-art methods; Super resolution; Visual qualities; Benchmarking","Convolutional neural network; dual network; generative adversarial network; single image super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85085986645"
"Shi J.; Wang L.; Wang S.; Chen Y.; Wang Q.; Wei D.; Liang S.; Peng J.; Yi J.; Liu S.; Ni D.; Wang M.; Zhang D.; Shen D.","Shi, Jun (7404495816); Wang, Linlin (57215963826); Wang, Shanshan (36761739900); Chen, Yanxia (57212010273); Wang, Qian (57192157811); Wei, Dongming (57211428968); Liang, Shujun (57197835138); Peng, Jialin (55265406900); Yi, Jiajin (57217028521); Liu, Shengfeng (57207199099); Ni, Dong (26023577500); Wang, Mingliang (57195685852); Zhang, Daoqiang (7405356869); Shen, Dinggang (7401738392)","7404495816; 57215963826; 36761739900; 57212010273; 57192157811; 57211428968; 57197835138; 55265406900; 57217028521; 57207199099; 26023577500; 57195685852; 7405356869; 7401738392","Applications of deep learning in medical imaging: a survey; [深度学习在医学影像中的应用综述]","2020","Journal of Image and Graphics","25","10","","1953","1981","28","10.11834/jig.200255","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093986045&doi=10.11834%2fjig.200255&partnerID=40&md5=c4454f4e3a3ade717502b5f4e92d8b48","Deep learning can automatically learn from a large amount of data to obtain effective feature representations, thereby effectively improving the performance of various machine learning tasks. It has been widely used in various fields of medical imaging. Smart healthcare has become an important application area of deep learning, which is an effective approach to solve the following clinical problems: 1) given the limited medical resources, the experienced radiologists are not fully available, which cannot satisfy the fast development of the clinical requirement; 2) the lack of experienced radiologists, which cannot satisfy the fast increase of medical demand. At present, deep learning-based intelligent medical imaging systems are the typical scenarios in smart healthcare. This paper primarily reviews the applications of deep learning methods in various applications using four major clinical imaging techniques (i.e., X-ray, ultrasound, computed tomography(CT), and magnetic resonance imaging(MRI)). These works cover the whole pipeline of medical imaging, including reconstruction, detection, segmentation, registration, and computer-aided diagnosis (CAD). The reviews on medical image reconstruction focus on both MRI reconstruction and low-dose CT reconstruction on the basis of deep learning. Deep learning methods for MRI reconstruction can be divided into two categories: 1) data-driven end-to-end methods, 2) model-based methods. The low-dose CT reconstruction primarily introduces methods on the basis of convolutional neural networks and generative adversarial networks. In addition, deep learning methods for ultrasound imaging, medical image synthesis, and medical image super-resolution are reviewed. The reviews on lesion detection primarily focuses on the deep learning methods for lung lesions detection using CT, the deep learning detection model for tumor lesions, and the deep learning methods for the general lesion area detection. At present, deep learning has been widely used in medical image segmentation tasks, and its performance is significantly improved compared with traditional image segmentation methods. Most deep learning segmentation methods are typical data-driven machine learning models. We review supervised models, semi-supervised models, and self-supervised models with regard to the amount of labeled data and annotation. Medical images contain rich anatomical information, which enhances the performance of deep learning models with different supervision. Deep learning models incorporating prior knowledge are also reviewed. Medical image registration consistency is a difficult task in the field of medical image analysis. Deep learning has become a breakthrough to improve the performance of medical image registration. The end-to-end network structures produce high-precision registration results and have become a hotspot in the field of image registration. Compared with the conventional methods, the deep learning methods for medical image registration have a significant improvement in registration performance. According to the different supervision in the training procedure, this paper divides the deep learning methods for medical image registration into three modes: fully supervised methods, unsupervised methods, and weakly supervised methods. Computer-aided diagnosis is another application of deep learning in the field of medical imaging. This paper summarizes the deep learning methods on CAD with different supervision and the CAD works on the basis of multi-modality medical images. Notably, although deep learning methods have been applied in medical imaging, several challenges are still identified. For example, the small-sample size problem is common in medical imaging analysis. Advanced machine learning methods, including weakly supervised learning, transfer learning, few-shot learning, self-supervised learning, and increase learning, can help alleviate this problem. In addition, the data annotation of medical images is a problem that seriously restricts the extensive and in-depth application of deep learning, and extensive research on automatic data labeling must be carried out. Interpretability of the deep neural networks is also important in medical image analysis. Improving the interpretability of a deep neural network has always been a difficult point, and in-depth research must be carried out in this area. Furthermore, carrying out human-computer collaboration in medical care is important. The lightweight deep neural network is easy to deploy into portable medical devices, giving portable devices more powerful functions, which is also an important research direction. Deep learning has been successful in various tasks in medical imaging analysis. New methods must be developed for its further application in intelligent medical products. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","Computer-aided diagnosis(CAD); Deep learning; Image reconstruction; Image registration; Image segmentation; Lesion detection; Medical imaging","Review","Final","","Scopus","2-s2.0-85093986045"
"Rabbi J.; Ray N.; Schubert M.; Chowdhury S.; Chao D.","Rabbi, Jakaria (56606267600); Ray, Nilanjan (7102751487); Schubert, Matthias (55605776884); Chowdhury, Subir (35619919300); Chao, Dennis (55967460400)","56606267600; 7102751487; 55605776884; 35619919300; 55967460400","Small-object detection in remote sensing images with end-to-end edge-enhanced GAN and object detector network","2020","Remote Sensing","12","9","1432","","","","10.3390/RS12091432","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085963738&doi=10.3390%2fRS12091432&partnerID=40&md5=930f9b94ac65cd2662d76c77627aaa66","The detection performance of small objects in remote sensing images has not been satisfactory compared to large objects, especially in low-resolution and noisy images. A generative adversarial network (GAN)-based model called enhanced super-resolution GAN (ESRGAN) showed remarkable image enhancement performance, but reconstructed images usually miss high-frequency edge information. Therefore, object detection performance showed degradation for small objects on recovered noisy and low-resolution remote sensing images. Inspired by the success of edge enhanced GAN (EEGAN) and ESRGAN, we applied a new edge-enhanced super-resolution GAN (EESRGAN) to improve the quality of remote sensing images and used different detector networks in an end-to-end manner where detector loss was backpropagated into the EESRGAN to improve the detection performance. We proposed an architecture with three components: ESRGAN, EEN, and Detection network. We used residual-in-residual dense blocks (RRDB) for both the ESRGAN and EEN, and for the detector network, we used a faster region-based convolutional network (FRCNN) (two-stage detector) and a single-shot multibox detector (SSD) (one stage detector). Extensive experiments on a public (car overhead with context) dataset and another self-assembled (oil and gas storage tank) satellite dataset showed superior performance of our method compared to the standalone state-of-the-art object detectors. © 2020 by the author.","Image enhancement; Object recognition; Optical resolving power; Remote sensing; Signal receivers; Adversarial networks; Convolutional networks; Detection networks; Detection performance; High frequency HF; Reconstructed image; Remote sensing images; Small object detection; Object detection","Edge enhancement; Faster region-based convolutional neural network (FRCNN); Object detection; Remote sensing imagery; Satellites; Single-shot multibox detector (SSD); Super-resolution","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85085963738"
"Luo F.; Wu X.","Luo, Fangzhou (57201578891); Wu, Xiaolin (35212443100)","57201578891; 35212443100","Maximum a Posteriori on a Submanifold: A General Image Restoration Method with GAN","2020","Proceedings of the International Joint Conference on Neural Networks","","","9207162","","","","10.1109/IJCNN48605.2020.9207162","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093817559&doi=10.1109%2fIJCNN48605.2020.9207162&partnerID=40&md5=1245b7d6a8575eb043ab1c4b6fcf2cd1","We propose a general method for various image restoration problems, such as denoising, deblurring, super-resolution and inpainting. The problem is formulated as a constrained optimization problem. Its objective is to maximize a posteriori probability of latent variables, and its constraint is that the image generated by these latent variables must be the same as the degraded image. We use a Generative Adversarial Network (GAN) as our density estimation model. Convincing results are obtained on MNIST dataset. © 2020 IEEE.","Constrained optimization; Image enhancement; Neural networks; Restoration; A-posteriori probabilities; Adversarial networks; Constrained optimi-zation problems; Density estimation; Image restoration problem; Maximum a posteriori; Restoration methods; Super resolution; Image reconstruction","deep learning; image restoration; optimization","Conference paper","Final","","Scopus","2-s2.0-85093817559"
"Tian Z.; Wang Y.; Du S.; Lan X.","Tian, Zhiqiang (35194645000); Wang, Yudiao (57218188835); Du, Shaoyi (57215374469); Lan, Xuguang (56114033200)","35194645000; 57218188835; 57215374469; 56114033200","A multiresolution mixture generative adversarial network for video super-resolution","2020","PLoS ONE","15","7","e0235352","","","","10.1371/journal.pone.0235352","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088205538&doi=10.1371%2fjournal.pone.0235352&partnerID=40&md5=c623b6cbc23281d2eeccce27da27b941","Generative adversarial networks (GANs) have been used to obtain super-resolution (SR) videos that have improved visual perception quality and more coherent details. However, the latest methods perform poorly in areas with dense textures. To better recover the areas with dense textures in video frames and improve the visual perception quality and coherence in videos, this paper proposes a multiresolution mixture generative adversarial network for video super-resolution (MRMVSR). We propose a multiresolution mixture network (MRMNet) as the generative network that can simultaneously generate multiresolution feature maps. In MRMNet, the high-resolution (HR) feature maps can continuously extract information from low-resolution (LR) feature maps to supplement information. In addition, we propose a residual fluctuation loss function for video super-resolution. The residual fluctuation loss function is used to reduce the overall residual fluctuation on SR and HR video frames to avoid a scenario where local differences are too large. Experimental results on the public benchmark dataset show that our method outperforms the state-of-the-art methods for the majority of the test sets.  © 2020 Tian et al.","Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Video Recording; Visual Perception; article; loss of function mutation; videorecording; vision; human; image processing; physiology; procedures; videorecording","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088205538"
"Sharma N.; Sharma R.; Jindal N.","Sharma, Neha (57191615950); Sharma, Reecha (57001029600); Jindal, Neeru (8571819900)","57191615950; 57001029600; 8571819900","An Improved Technique for Face Age Progression and Enhanced Super-Resolution with Generative Adversarial Networks","2020","Wireless Personal Communications","114","3","","2215","2233","18","10.1007/s11277-020-07473-1","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085286787&doi=10.1007%2fs11277-020-07473-1&partnerID=40&md5=c5dd6d5ad4c74c8e55bc83ddac3367a6","Several techniques are available for face age progression still identity preservation as well age estimation accuracy are big challenges and need attention. So, the proposed work is focused on these key issues using Generative Adversarial Networks (GANs). To produce a realistic appearance with an enhanced vision of face image, a fusion-based Generative Adversarial Network approach is used. GAN has a generator and a discriminator network. The generator produces fake images which are further differentiated by discriminator whether the image is real or fake. Initially, Cycle-Generative Adversarial Network (CycleGAN) achieves the face age progression, further Enhanced Super-resolution Generative Adversarial Network (ESRGAN) automatically enhance the aged face image to improve the visibility. Simulation results on five face datasets, namely IMDB-WIKI, CACD and UTKFace, FGNET, Celeb A are evaluated. The proposed work efficacy is observed in comparison to previous techniques using a quantitative Face ++ research toolkit with parameters confidence score number and age estimation value. It is observed that the proposed work produces the aged face precisely with an error rate of 0.001%, with a a confidence score 95.13 to 95.39 on datasets. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.","Optical resolving power; Adversarial networks; Age estimation; Age progression; Confidence score; Enhanced vision; Face images; Improved techniques; Super resolution; Image enhancement","Age estimation; Enhanced super-resolution; Face age progression; Generative adversarial network","Article","Final","","Scopus","2-s2.0-85085286787"
"Cai J.; Han H.; Shan S.; Chen X.","Cai, Jiancheng (57210356235); Han, Hu (30267595700); Shan, Shiguang (22235341500); Chen, Xilin (57215374943)","57210356235; 30267595700; 22235341500; 57215374943","FCSR-GAN: Joint Face Completion and Super-Resolution via Multi-Task Learning","2020","IEEE Transactions on Biometrics, Behavior, and Identity Science","2","2","8890717","109","121","12","10.1109/TBIOM.2019.2951063","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122046311&doi=10.1109%2fTBIOM.2019.2951063&partnerID=40&md5=e9d28e6c198bf2ffbe64e3a5771be7b2","Combined variations containing low-resolution and occlusion often present in face images in the wild, e.g., under the scenario of video surveillance. While most of the existing face image recovery approaches can handle only one type of variation per model, in this work, we propose a deep generative adversarial network (FCSR-GAN) for performing joint face completion and face super-resolution via multi-task learning. The generator of FCSR-GAN aims to recover a high-resolution face image without occlusion given an input low-resolution face image with occlusion. The discriminator of FCSR-GAN uses a set of carefully designed losses (an adversarial loss, a perceptual loss, a pixel loss, a smooth loss, a style loss, and a face prior loss) to assure the high quality of the recovered high-resolution face images without occlusion. The whole network of FCSR-GAN can be trained end-to-end using our two-stage training strategy. Experimental results on the public-domain CelebA and Helen databases show that the proposed approach outperforms the state-of-the-art methods in jointly performing face super-resolution (up to 8×) and face completion, and shows good generalization ability in cross-database testing. Our FCSR-GAN is also useful for improving face identification performance when there are low-resolution and occlusion in face images. The code of FCSR-GAN is available at: https://github.com/swordcheng/FCSR-GAN. © 2019 IEEE.","Ability testing; Image enhancement; Learning systems; Optical resolving power; Security systems; Adversarial networks; Face identification; Face super-resolution; Generalization ability; Low-resolution face images; State-of-the-art methods; Super resolution; Video surveillance; Multi-task learning","generative adversarial network; Joint face completion and super-resolution; multi-task learning; two-stage training","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122046311"
"Li F.; Lin D.; Yu T.","Li, Fusheng (57209174818); Lin, Dan (57214818467); Yu, Tao (56454665900)","57209174818; 57214818467; 56454665900","Improved Generative Adversarial Network-Based Super Resolution Reconstruction for Low-Frequency Measurement of Smart Grid","2020","IEEE Access","8","","9087855","85257","85270","13","10.1109/ACCESS.2020.2992836","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085160602&doi=10.1109%2fACCESS.2020.2992836&partnerID=40&md5=bb28b6b7e2503d8c7c842552118306c0","There is a universal trend toward a data-driven smart grid, which aims to realize two-way communication of energy flow and data flow between various agents across power generation side, transmissiondistribution side, electricity retailors and end users. However, the low frequency electrical measurement data accumulated over a long period of time is insignificant for intelligent agents. This paper presents a machine learning method for reconstructing the low frequency electrical measurement data in smart grid. Firstly, the electrical measurement data is converted into electrical images, and then the low frequency electrical measurement data is reconstructed into high frequency electrical measurement data by generative adversarial network to improve the training stability, Wasserstein distance is introduced into the reconstruction mechanism. In addition, by designing the deep residual network based generator, the deep convolutional network based discriminator as well as the perception loss function, the reconstruction accuracy and the high-frequency detail reduction ability are improved. The proposed method is tested on three publicly available datasets and compared with the traditional data reconstruction method, justifying that this method not only can restore high-frequency details with less error, but also can be generalized to different datasets at one location and to datasets at different locations with satisfactory accuracy. © 2013 IEEE.","Electric power transmission networks; Electric variables measurement; Image enhancement; Intelligent agents; Learning systems; Convolutional networks; Electrical measurement; Low frequency measurements; Machine learning methods; Reconstruction accuracy; Super resolution reconstruction; Two way communications; Wasserstein distance; Smart power grids","Data-driven; electrical measurement data; generative adversarial network; super-resolution reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85085160602"
"Gao H.; Chen Z.; Huang B.; Chen J.; Li Z.","Gao, Hongxia (55473285100); Chen, Zhanhong (57201683843); Huang, Binyang (57220154883); Chen, Jiahe (57204147660); Li, Zhifu (55706982900)","55473285100; 57201683843; 57220154883; 57204147660; 55706982900","Image super-resolution based on conditional generative adversarial network","2020","IET Image Processing","14","13","","3076","3083","7","10.1049/iet-ipr.2018.5767","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097041184&doi=10.1049%2fiet-ipr.2018.5767&partnerID=40&md5=75a7db12c0c8258232e99fae329b66a9","Generative adversarial network (GAN) is one of the most prevalent generative models that can synthesise realistic high-frequency details. However, a mismatch between the input and the output may arise when GAN is directly applied to image super-resolution. To alleviate this issue, the authors adopted a conditional GAN (cGAN) in this study. The cGAN discriminator attempted to guess whether the unknown high-resolution (HR) image was produced by the generator with the aid of the original low-resolution (LR) image. They propose a novel discriminator that only penalises at the scale of the patch and, thus, has relatively few parameters to train. The generator of cGAN is an encoder-decoder with skip connections to shuttle the shared low-level information directly across the network. To better maintain the low-frequency information and recover the high-frequency information, they designed a generator loss function combining adversarial loss term and L1 loss term. The former term is beneficial to the synthesis of fine-grained textures, while the latter is responsible for learning the overall structure of the LR input. The experiments revealed that the proposed method could generate HR images with richer details and less over-smoothness. © The Institution of Engineering and Technology 2020","Textures; Adversarial networks; Encoder-decoder; Generative model; High frequency HF; High resolution image; High-frequency informations; Image super resolutions; Low resolution images; Optical resolving power","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85097041184"
"Wang J.; Chen Y.; Wu Y.; Shi J.; Gee J.","Wang, Jiancong (57195777016); Chen, Yuhua (57190346094); Wu, Yifan (57206750971); Shi, Jianbo (55252537400); Gee, James (57202708522)","57195777016; 57190346094; 57206750971; 55252537400; 57202708522","Enhanced generative adversarial network for 3D brain MRI super-resolution","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093603","3616","3625","9","10.1109/WACV45572.2020.9093603","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085496064&doi=10.1109%2fWACV45572.2020.9093603&partnerID=40&md5=1b6d716cf2164d49a9a4ec31e277fca2","Single image super-resolution (SISR) reconstruction for magnetic resonance imaging (MRI) has generated significant interest because of its potential to not only speed up imaging but to improve quantitative processing and analysis of available image data. Generative Adversarial Networks (GAN) have proven to perform well in image recovery tasks. In this work, we followed the GAN framework and developed a generator coupled with discriminator to tackle the task of 3D SISR on T1 brain MRI images. We developed a novel 3D memory-efficient residual-dense block generator (MRDG) that achieves state-of-the-art performance in terms of SSIM (Structural Similarity), PSNR (Peak Signal to Noise Ratio) and NRMSE (Normalized Root Mean Squared Error) metrics. We also designed a pyramid pooling discriminator (PPD) to recover details on different size scales simultaneously. Finally, we introduced model blending, a simple and computational efficient method to balance between image and texture quality in the final output, to the task of SISR on 3D images. © 2020 IEEE.","Computer vision; Image enhancement; Mean square error; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Brain mri images; PSNR (peak signal to noise ratio); Quantitative processing; Root mean squared errors; State-of-the-art performance; Structural similarity; Super resolution; Magnetic resonance imaging","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085496064"
"Xue X.; Zhang X.; Li H.; Wang W.","Xue, Xiangyu (57219313682); Zhang, Xiangnan (57219320238); Li, Haibing (57219320302); Wang, Wenyong (7501758663)","57219313682; 57219320238; 57219320302; 7501758663","Research on GAN-based Image Super-Resolution Method","2020","Proceedings of 2020 IEEE International Conference on Artificial Intelligence and Computer Applications, ICAICA 2020","","","9182617","602","605","3","10.1109/ICAICA50127.2020.9182617","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092136503&doi=10.1109%2fICAICA50127.2020.9182617&partnerID=40&md5=6a7639aefe374e4024d997c3db7c0cd3","Super-Resolution (SR) refers to the reconstruction of high-resolution image from low-resolution image, which has important application value in object detection, medical imaging, satellite remote sensing and other fields. In recent years, with the rapid development of deep learning, the image super-resolution reconstruction method based on deep learning has made remarkable progress. In this paper, R-SRGAN (Residual Super-Resolution Generative Adversarial Networks) is used to build the model and realize image super-resolution. By adding residual blocks between adjacent convolutional layers of the GAN generator, more detailed information is retained. At the same time, the Wassertein distance is used as a loss function to enhance the training effect and achieve image super-resolution. © 2020 IEEE.","Deep learning; Image enhancement; Medical imaging; Object detection; Optical resolving power; Remote sensing; Adversarial networks; High resolution image; Image super resolutions; Image super-resolution reconstruction; Low resolution images; Satellite remote sensing; Super resolution; Training effects; Image reconstruction","Generative Adversarial Networks; Image Processing; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85092136503"
"Gupta R.; Sharma A.; Kumar A.","Gupta, Rohit (57220802732); Sharma, Anurag (57753188300); Kumar, Anupam (57210413959)","57220802732; 57753188300; 57210413959","Super-Resolution using GANs for Medical Imaging","2020","Procedia Computer Science","173","","","28","35","7","10.1016/j.procs.2020.06.005","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089996978&doi=10.1016%2fj.procs.2020.06.005&partnerID=40&md5=cbdfd16ec62d031b5b782fb9261ea6f8","Generative Adversarial Models (GANs) have been quite popular and are currently and active area of research. They can be used for generative new data and study adversarial samples and attacks. We have used the similar approach to apply super-resolution to medical images. In Radiology MRI is a commonly used method to produce medical imaging but the limitations of lab equipment and health hazard of being in an MRI radiation environment to obtain good quality scans lead to lower quality scans and also it takes a lot of time to get a high-resolution data. This problem can be solved by using super-resolution using deep learning as a post-processing step to improve the resolution of the scans. Super-resolution is a process of generating higher resolution images from lower resolution data. For this, we are proposing a generative adversarial network architecture which is a dual neural network designed to generate lifelike images. In this deep learning algorithm, two neural networks compete with each other to improve alternatively. Given a training set, this technique learns to generate new data with the same statistics as the training set. To apply this technique to our problem statement we are using generator as the network to improve the resolution and discriminator as a network to train generator better. We used transfer learning in our generative neural network and training our discriminator from scratch and using the perceptual loss [1] to train our network. This will help in improving the performance of the network. We are using Lung MRI scans of tuberculosis with a set of 216 MRI samples containing around 60-130 channels each and each channel having 512x512 dimensions. © 2020 The Authors. Published by Elsevier B.V.","Deep learning; Discriminators; Health hazards; Intelligent computing; Learning algorithms; Magnetic resonance imaging; Medical imaging; Network architecture; Optical resolving power; Transfer learning; Dual neural networks; High resolution data; Higher resolution images; Lower resolution; Post processing; Problem statement; Radiation environments; Super resolution; Neural networks","Artificial Intelligence; Computer Vision; Deep Learning; Generative Adversarial Network; Magnetic resonance imaging; Super resolution; Transfer Learning","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85089996978"
"Li T.; Zhang S.; Xia J.","Li, Tong (57221125303); Zhang, Shibin (55788282300); Xia, Jinyue (57209545549)","57221125303; 55788282300; 57209545549","Quantum generative adversarial network: A survey","2020","Computers, Materials and Continua","64","1","","401","438","37","10.32604/CMC.2020.010551","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090893167&doi=10.32604%2fCMC.2020.010551&partnerID=40&md5=8eaaf6f60994b9bc3021b56a6208f7c5","Generative adversarial network (GAN) is one of the most promising methods for unsupervised learning in recent years. GAN works via adversarial training concept and has shown excellent performance in the fields image synthesis, image super-resolution, video generation, image translation, etc. Compared with classical algorithms, quantum algorithms have their unique advantages in dealing with complex tasks, quantum machine learning (QML) is one of the most promising quantum algorithms with the rapid development of quantum technology. Specifically, Quantum generative adversarial network (QGAN) has shown the potential exponential quantum speedups in terms of performance. Meanwhile, QGAN also exhibits some problems, such as barren plateaus, unstable gradient, model collapse, absent complete scientific evaluation system, etc. How to improve the theory of QGAN and apply it that have attracted some researcher. In this paper, we comprehensively and deeply review recently proposed GAN and QAGN models and their applications, and we discuss the existing problems and future research trends of QGAN. © 2020 Tech Science Press. All rights reserved.","Quantum theory; Adversarial networks; Existing problems; Image super resolutions; Image translation; Quantum algorithms; Quantum machines; Quantum technologies; Scientific evaluations; Machine learning","Generative adversarial network; Mode collapse; Quantum generative adversarial network; Quantum machine learning","Review","Final","","Scopus","2-s2.0-85090893167"
"Niu X.; Yan B.; Tan W.; Wang J.","Niu, Xuejing (56682465700); Yan, Bo (36764113400); Tan, Weimin (57188575769); Wang, Junyi (57210590159)","56682465700; 36764113400; 57188575769; 57210590159","Effective image restoration for semantic segmentation","2020","Neurocomputing","374","","","100","108","8","10.1016/j.neucom.2019.09.063","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073018765&doi=10.1016%2fj.neucom.2019.09.063&partnerID=40&md5=c2d24f03828f493610bf2d1ef5685a0a","Recent semantic segmentation algorithms are greatly accelerated by deep convolutional neural networks (DCNNs). Although most of them perform well on normal images, they are not robust to the degenerations of images. To boost the performance of semantic segmentation on degraded images, we present an effective image restoration framework based on generative adversarial network (GAN). Firstly, we propose to jointly minimize pixel-wise cross entropy loss and our semantic-aware mean square error loss for the optimization of generative restoration network, which ensures correct semantic predictions at each pixel location. Secondly, we introduce GAN into our framework, which can benefit high-order consistency of semantic predictions. Comprehensive experimental results on image super-resolution and denoising demonstrate that our approach is able to achieve the best segmentation accuracy as well as maintaining pleasing image quality compared to other common image restoration approaches. © 2019","Deep neural networks; Image denoising; Image reconstruction; Mean square error; Neural networks; Optical resolving power; Pixels; Restoration; Semantics; Adversarial networks; Convolutional neural network; Degraded images; Image super resolutions; Pixel location; Restoration network; Segmentation accuracy; Semantic segmentation; article; convolutional neural network; diagnostic test accuracy study; entropy; image quality; image reconstruction; prediction; segmentation algorithm; Image segmentation","Image denoising; Image restoration; Image super-resolution; Semantic segmentation","Article","Final","","Scopus","2-s2.0-85073018765"
"Yue H.; Cheng J.; Liu Z.; Chen W.","Yue, Haosong (37014128300); Cheng, Jiaxiang (57207733866); Liu, Zhong (57190886072); Chen, Weihai (35776127700)","37014128300; 57207733866; 57190886072; 35776127700","Remote-sensing image super-resolution using classifier-based generative adversarial networks","2020","Journal of Applied Remote Sensing","14","4","046514","","","","10.1117/1.JRS.14.046514","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098650782&doi=10.1117%2f1.JRS.14.046514&partnerID=40&md5=6fb617d08f5f403af88d154f6511eb61","The rapid development of the aerospace industry has significantly increased the demand for remote-sensing images with high resolution and quality. Generating images with expected resolution from the samples obtained by common acquisition devices is a challenging task as the trade-off between cost and efficiency must be considered. We propose a super-resolution (SR) algorithm especially for remote-sensing images that is based on generative adversarial networks optimized by a classifier, which is called classifier-based super-resolution generative adversarial network (CSRGAN). We hypothesize that the confidence scores of classification can be a critical factor for representing the features in target remote-sensing images. To sufficiently take this factor into account during training, we add the class-score as an error into the loss function in addition to mean square error and high-dimensional features extracted from deep neural networks. Then, the classifier is utilized for both better SR performance and more precise classification. The classifier-testing branch of our system can also be flexibly combined with other network architectures to optimize SR performance on remote-sensing images. We validate the model on the NWPU-RESISC45 dataset considering both SR and classification performance. The final analysis is also provided and shows that the proposed CSRGAN outperforms existing algorithms.  © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Aerospace industry; Classification (of information); Deep neural networks; Economic and social effects; Image classification; Mean square error; Network architecture; Optical resolving power; Acquisition device; Adversarial networks; Classification performance; Confidence score; Critical factors; High dimensional feature; Remote sensing images; Super resolution; Remote sensing","classifier; generative adversarial networks; remote-sensing image; super-resolution","Article","Final","","Scopus","2-s2.0-85098650782"
"Chen Y.; Cai L.; Cheng W.; Wang H.","Chen, Yanjie (57215338789); Cai, Likun (57217194755); Cheng, Wei (57218967449); Wang, Hao (57201885995)","57215338789; 57217194755; 57218967449; 57201885995","Super-resolution coding defense against adversarial examples","2020","ICMR 2020 - Proceedings of the 2020 International Conference on Multimedia Retrieval","","","","189","197","8","10.1145/3372278.3390689","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086901503&doi=10.1145%2f3372278.3390689&partnerID=40&md5=6b9a7f7ad214110d135f2aed92c75817","Deep neural networks have achieved state-of-the-art performance in many fields including image classification. However, recent studies show these models are vulnerable to adversarial examples formed by adding small but intentional perturbations to clean examples. In this paper, we introduce a significant defense method against adversarial examples. The key idea is to leverage a super-resolution coding (SR-coding) network to eliminate noise from adversarial examples. Furthermore, to boost the effect of defending noise, we propose a novel hybrid approach that incorporates SR-coding and adversarial training to train robust neural networks. Experiments on benchmark datasets demonstrate the effectiveness of our method against both the state-of-the-art white-box attacks and black-box attacks. The proposed approach significantly improves defense performance and achieves up to 41.26% improvement based on the accuracy by ResNet18 on PGD white-box attack. © 2020 ACM.","Deep neural networks; Optical resolving power; Benchmark datasets; Black boxes; Hybrid approach; State of the art; State-of-the-art performance; Super resolution; White box; Network security","Adversarial attack; Deep learning; Generative adversarial network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85086901503"
"Le-Tien T.; Nguyen-Thanh T.; Xuan H.-P.; Nguyen-Truong G.; Ta-Quoc V.","Le-Tien, Thuong (6603402351); Nguyen-Thanh, Tuan (57191332664); Xuan, Hanh-Phan (57205620525); Nguyen-Truong, Giang (57219417666); Ta-Quoc, Vinh (57219423916)","6603402351; 57191332664; 57205620525; 57219417666; 57219423916","Deep learning based approach implemented to image super-resolution","2020","Journal of Advances in Information Technology","11","4","","209","216","7","10.12720/jait.11.4.209-216","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092657220&doi=10.12720%2fjait.11.4.209-216&partnerID=40&md5=f87244380d7abc4c9f377efbb58807e0","The aim of this research is about application of deep learning approach to the inverse problem, which is one of the most popular issues that has been concerned for many years about, the image Super-Resolution (SR). From then on, many fields of machine learning and deep learning have gained a lot of momentum in solving such imaging problems. In this article, we review the deep-learning techniques for solving the image super-resolution especially about the Generative Adversarial Network (GAN) technique and discuss other ways to use the GAN for an efficient solution on the task. More specifically, we review about the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) and Residual in Residual Dense Network (RRDN) that are introduced by ‘idealo’ team and evaluate their results for image SR, they had generated precise results that gained the high rank on the leader board of state-of-the-art techniques with many other datasets like Set5, Set14 or DIV2K, etc. To be more specific, we will also review the Single-Image Super-Resolution using Generative Adversarial Network (SRGAN) and the Enhanced SuperResolution Generative Adversarial Networks (ESRGAN), two famous state-of-the-art techniques, by re-train the proposed model with different parameter and comparing with their result. So that can be helping us understand the working of announced model and the different when we choose others parameter compared to theirs. © 2020 J. Adv. Inf. Technol.","","Deep learning; Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN); Generative Adversarial Network (GAN); Image super-resolution; Inverse problems; Residual in Residual Dense Network (RRDN)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85092657220"
"Zhang J.; Gong L.-R.; Yu K.; Qi X.; Wen Z.; Hua Q.; Myint S.H.","Zhang, Jing (57196378642); Gong, Ling-Rui (57218488253); Yu, Keping (56316023300); Qi, Xin (57219853462); Wen, Zheng (56438635500); Hua, Qiaozhi (57201726103); Myint, San Hlaing (57207827720)","57196378642; 57218488253; 56316023300; 57219853462; 56438635500; 57201726103; 57207827720","3D Reconstruction for Super-Resolution CT Images in the Internet of Health Things Using Deep Learning","2020","IEEE Access","8","","9133099","121513","121525","12","10.1109/ACCESS.2020.3007024","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089306059&doi=10.1109%2fACCESS.2020.3007024&partnerID=40&md5=ecc328c38eb4f12efcbd1c0d2dc5ebde","The Internet of Health Things (IoHT) enables health devices to connect to the Internet and communicate with each other, which provides the high-accuracy and high-security diagnosis result in the medical area. As essential parts of the IoHT, computed tomography (CT) images help doctors diagnose disease. In the traditional disease diagnosing process, low-resolution medical CT images produce low-accuracy diagnosis results for microlesions. Moreover, CT images can only provide 2D information about organs, and doctors should estimate the 3D shape of a lesion based on experience. To solve these problems, we propose a 3D reconstruction method for secure super-resolution computed tomography (SRCT) images in the IoHT using deep learning. First, we use deep learning to obtain secure SRCT images from low-resolution images in the IoHT. To this end, we adopt a conditional generative adversarial network (CGAN) based on the edge detection loss function (EDLF) in the deep learning process, namely EDLF-CGAN algorithm. In this algorithm, the CGAN is employed to generate SRCT images with luminance and contrast as the input auxiliary conditions, which can improve the accuracy of super-resolution (SR) images. An EDLF is proposed to consider the edge features in the generated SRCT images, which reduces the deformation of generated image. Second, we apply the secure SR images generated from the deep learning method to perform 3D reconstruction. An advanced ray casting 3D reconstruction algorithm that can reduce the number of rays by selecting the appropriate bounding box is proposed. Compared with the traditional algorithm, the proposed ray casting 3D reconstruction algorithm can reduce the time and memory cost. The experimental results show that our EDLF-CGAN has a better SR reconstruction effect than other algorithms via the indicators of the peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). In addition, our advanced ray casting 3D reconstruction algorithm greatly improves the efficiency compared with the traditional ray casting algorithm. © 2013 IEEE.","Computerized tomography; Deep learning; Diagnosis; Edge detection; Image enhancement; Image segmentation; Learning algorithms; Learning systems; Medical imaging; Optical resolving power; Rendering (computer graphics); Signal to noise ratio; 3-D reconstruction algorithms; 3D reconstruction; Adversarial networks; Learning process; Low resolution images; Peak signal to noise ratio; Ray casting algorithm; Structural similarity; Image reconstruction","3D reconstruction; Conditional generative adversarial network; edge detection; Internet of Health Things; super-resolution image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85089306059"
"Alqahtani H.; Kavakli-Thorne M.; Kumar G.","Alqahtani, Hamed (57194574403); Kavakli-Thorne, Manolya (57219506272); Kumar, Gulshan (35932222600)","57194574403; 57219506272; 35932222600","Generative adversarial networks - application domains","2020","Image Recognition: Progress, Trends and Challenges","","","","227","272","45","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145342380&partnerID=40&md5=6a14d606c8774956491cbae1afcf7f48","Generative adversarial networks (GANs) present a way to learn deep representations without extensively annotated training data. These networks achieve learning through deriving back propagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in several applications. GANs have made significant advancements and tremendous performance in numerous applications. The essential applications include semantic image editing, style transfer, image synthesis, image super-resolution and classification. This chapter aims to present an overview of GANs, and potential application in various domains. The main intention of this chapter is to explore and present a comprehensive review of the crucial applications of GANs covering a variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the chapter ends with the conclusion and future aspects. © 2020 by Nova Science Publishers, Inc. All rights reserved.","","Generative adversarial networks; Neural networks; Supervised learning; Unsupervised learning","Book chapter","Final","","Scopus","2-s2.0-85145342380"
"Moschoglou S.; Ploumpis S.; Nicolaou M.A.; Papaioannou A.; Zafeiriou S.","Moschoglou, Stylianos (57193191974); Ploumpis, Stylianos (56648048200); Nicolaou, Mihalis A. (36622278100); Papaioannou, Athanasios (55908612300); Zafeiriou, Stefanos (8883680000)","57193191974; 56648048200; 36622278100; 55908612300; 8883680000","3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation","2020","International Journal of Computer Vision","128","10-11","","2534","2551","17","10.1007/s11263-020-01329-8","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091264734&doi=10.1007%2fs11263-020-01329-8&partnerID=40&md5=b686b8d429c59cf97a91e1251ba2613b","Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face—such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation. © 2020, The Author(s).","Distributed database systems; MESH networking; 3D shape analysis; 3D shape representation; Adversarial networks; High frequency components; High frequency HF; Image generations; Quantitative experiments; State-of-the-art methods; Principal component analysis","3D; Face; GAN; Generation; Representation; Translation","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85091264734"
"Bin L.; Lu M.","Bin, Li (57220090018); Lu, Ma (57216204056)","57220090018; 57216204056","Super-resolution reconstruction of densely connected generative adversarial network images","2020","Laser and Optoelectronics Progress","57","22","221011","","","","10.3788/LOP57.221011","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096823725&doi=10.3788%2fLOP57.221011&partnerID=40&md5=90015e3a34c27d7cbfe82ae4fafe9c5e","Aiming at the problems of blurred edge details and loss of image features in the process of image superresolution reconstruction a super-resolution reconstruction algorithm based on dense connection generative adversarial network is proposed This algorithm consists of a generative network and a discriminative network In the generative network structure the original low-resolution image is used as the input of the network In order to make full use of the features the features of the shallow network are transferred to each layer of the deep network structure using dense connection so as to effectively avoid the loss of image features Sub-pixel convolution is performed at the end and the image is deconvolved to complete the final super-resolution reconstruction of the image which greatly reduces the training time In the discriminative network structure 6 convolutional modules and a fully connected layer are used to identify true and false images and the idea of adversarial games is used to improve the quality of reconstructed images Experimental results show that the proposed algorithm has greatly improved the visual effect assessment peak signal to noise ratio value structural similarity value time-consuming and indicators It has restored richer image detail information and achieved better visual effects and comprehensive characteristic. © 2020 Universitat zu Koln. All rights reserved.","","Dense connection; Generative adversarial network; Image processing; Super-resolution technology","Article","Final","","Scopus","2-s2.0-85096823725"
"","","","6th International Workshop on Ophthalmic Medical Image Analysis, OMIA 2020, held in conjunction with 23rd International Conference on Medical Imaging and Computer-Assisted Intervention, MICCAI 2020","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12069 LNCS","","","","","216","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097418205&partnerID=40&md5=8cee8fcf37260e35223ed7146bf0e027","The proceedings contain 21 papers. The special focus in this conference is on Ophthalmic Medical Image Analysis. The topics include: Multi-level Light U-Net and Atrous Spatial Pyramid Pooling for Optic Disc Segmentation on Fundus Image; an Interactive Approach to Region of Interest Selection in Cytologic Analysis of Uveal Melanoma Based on Unsupervised Clustering; Retinal OCT Denoising with Pseudo-Multimodal Fusion Network; deep-Learning-Based Estimation of 3D Optic-Nerve-Head Shape from 2D Color Fundus Photographs in Cases of Optic Disc Swelling; Weakly Supervised Retinal Detachment Segmentation Using Deep Feature Propagation Learning in SD-OCT Images; A Framework for the Discovery of Retinal Biomarkers in Optical Coherence Tomography Angiography (OCTA); an Automated Aggressive Posterior Retinopathy of Prematurity Diagnosis System by Squeeze and Excitation Hierarchical Bilinear Pooling Network; weakly-Supervised Lesion-Aware and Consistency Regularization for Retinitis Pigmentosa Detection from Ultra-Widefield Images; a Conditional Generative Adversarial Network-Based Method for Eye Fundus Image Quality Enhancement; DR Detection Using Optical Coherence Tomography Angiography (OCTA): A Transfer Learning Approach with Robustness Analysis; construction of Quantitative Indexes for Cataract Surgery Evaluation Based on Deep Learning; hybrid Deep Learning Gaussian Process for Diabetic Retinopathy Diagnosis and Uncertainty Quantification; what is the Optimal Attribution Method for Explainable Ophthalmic Disease Classification?; DeSupGAN: Multi-scale Feature Averaging Generative Adversarial Network for Simultaneous De-blurring and Super-Resolution of Retinal Fundus Images; encoder-Decoder Networks for Retinal Vessel Segmentation Using Large Multi-scale Patches; retinal Image Quality Assessment via Specific Structures Segmentation; cascaded Attention Guided Network for Retinal Vessel Segmentation; self-supervised Denoising via Diffeomorphic Template Estimation: Application to Optical Coherence Tomography.","","","Conference review","Final","","Scopus","2-s2.0-85097418205"
"Wang M.; Chen Z.; Wu Q.M.J.; Jian M.","Wang, Mengxue (57216344107); Chen, Zhenxue (55861302600); Wu, Q. M. Jonathan (55613291700); Jian, Muwei (22634073600)","57216344107; 55861302600; 55613291700; 22634073600","Improved face super-resolution generative adversarial networks","2020","Machine Vision and Applications","31","4","22","","","","10.1007/s00138-020-01073-6","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083244708&doi=10.1007%2fs00138-020-01073-6&partnerID=40&md5=c80883641cba270ed347fa17311736b4","The face super-resolution method is used for generating high-resolution images from low-resolution ones for better visualization. The super-resolution generative adversarial network (SRGAN) can generate a single super-resolution image with realistic textures, which is a groundbreaking work. Based on SRGAN, we proposed improved face super-resolution generative adversarial networks. The super-resolution image details generated by SRGAN usually have undesirable artifacts. To further improve visual quality, we delve into the key components of the SRGAN network architecture and improve each part to achieve a more powerful SRGAN. First, the SRGAN employs residual blocks as the core of the very deep generator network G. In this paper, we decide to employ dense convolutional network blocks (dense blocks), which connect each layer to every other layer in a feed-forward fashion as our very deep generator networks. Moreover, in the past few years, generative adversarial networks (GANs) have been applied to solve various problems. Despite its superior performance, it is difficult to train. A simple and effective regularization method called spectral normalization GAN is used to solve this problem. We have experimentally confirmed that our proposed method is superior to the other existing method in training stability and visual improvements. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Network architecture; Textures; Adversarial networks; Convolutional networks; Face super-resolution; High resolution image; Regularization methods; Spectral normalization; Visual improvements; Visual qualities; Optical resolving power","Dense blocks; Face super-resolution; GAN; Spectral normalization","Article","Final","","Scopus","2-s2.0-85083244708"
"Cheng G.; Zhang F.; Qiang X.","Cheng, Guojian (15753589500); Zhang, Fulin (57222111378); Qiang, Xinjian (36447998500)","15753589500; 57222111378; 36447998500","Super-resolution reconstruction of rock thin-section image based on SinGAN","2020","","","","9338894","786","790","4","10.1109/ITAIC49862.2020.9338894","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101472867&doi=10.1109%2fITAIC49862.2020.9338894&partnerID=40&md5=7c76ba25f68a615adc529c2c5ab34b5f","The rock thin section images are of great significance to the study of petroleum geological characteristics and oil and gas exploration. Due to the limitations of various factors, the obtained images of rock thin-sections often have low resolution, which limits the researchers' grasp of their detailed information to some extent. The traditional super-resolution algorithm of neural network will need a large amount of data as a training set, in order to improve rock thin-section image super-resolution reconstruction algorithm texture detail information reduction ability, the paper using single image generative adversarial network for rock thin-section image super-resolution reconstruction, does not need to input a large number of data sets, of single image super-resolution reconstruction image. Rock cast thin section images from an oil field area in Ordos basin were used for training, and peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) evaluation indexes were used for model evaluation. The experimental results show that the super-resolution image processing based on this method has good visual effects and evaluation indexes. © 2020 IEEE.","Artificial intelligence; Image enhancement; Oil fields; Optical resolving power; Petroleum prospecting; Rocks; Signal to noise ratio; Textures; Geological characteristics; Oil and gas exploration; Peak signal to noise ratio; Single-image super-resolution reconstruction; Structural similarity; Super resolution algorithms; Super resolution reconstruction; Super-resolution image processing; Image reconstruction","Neural network; Rock thin-section image; Single image generative confrontatio n network; Superresolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85101472867"
"Courtrai L.; Pham M.-T.; Friguet C.; Lefevre S.","Courtrai, Luc (6507861482); Pham, Minh-Tan (56070990300); Friguet, Chloe (35071291100); Lefevre, Sebastien (57203070803)","6507861482; 56070990300; 35071291100; 57203070803","Small Object Detection from Remote Sensing Images with the Help of Object-Focused Super-Resolution Using Wasserstein GANs","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323236","260","263","3","10.1109/IGARSS39084.2020.9323236","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101963148&doi=10.1109%2fIGARSS39084.2020.9323236&partnerID=40&md5=8a904d6ebf4966e659c174dfc6e1c802","In this paper, we investigate and improve the use of a super-resolution approach to benefit the detection of small objects from aerial and satellite remote sensing images. The main idea is to focus the super-resolution on target objects within the training phase. Such a technique requires a reduced number of network layers depending on the desired scale factor and the reduced size of the target objects. The learning of our super-resolution network is performed using deep residual blocks integrated in a Wasserstein Generative adversarial network. Then, detection task is performed by exploiting two state-of-the-art detectors including Faster-RCNN and YOLOv3. Experiments were conducted on small vehicle detection from both aerial and satellite images from the VEDAI and xView data sets. Results showed that object-focused super-resolution improves the detection performance and facilitates the transfer learning from one data set to another. © 2020 IEEE.","Antennas; Geology; Image enhancement; Network layers; Optical resolving power; Remote sensing; Small satellites; Transfer learning; Adversarial networks; Detection performance; Detection tasks; Remote sensing images; Satellite images; Satellite remote sensing; Small object detection; Super resolution; Object detection","deep learning; remote sensing imagery; Small object detection; super-resolution; Wasserstein GANs","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101963148"
"Chrysos G.G.; Kossaifi J.; Zafeiriou S.","Chrysos, Grigorios G. (57188639985); Kossaifi, Jean (56040739900); Zafeiriou, Stefanos (8883680000)","57188639985; 56040739900; 8883680000","RoCGAN: Robust Conditional GAN","2020","International Journal of Computer Vision","128","10-11","","2665","2683","18","10.1007/s11263-020-01348-5","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087975271&doi=10.1007%2fs11263-020-01348-5&partnerID=40&md5=fb6261758ed8660e3a59c3a85ecddef3","Conditional image generation lies at the heart of computer vision and conditional generative adversarial networks (cGAN) have recently become the method of choice for this task, owing to their superior performance. The focus so far has largely been on performance improvement, with little effort in making cGANs more robust to noise. However, the regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGANs unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Specifically, we augment the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold, even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and establish with both synthetic and real data the merits of our model. We perform a thorough experimental validation on large scale datasets for natural scenes and faces and observe that our model outperforms existing cGAN architectures by a large margin. We also empirically demonstrate the performance of our approach in the face of two types of noise (adversarial and Bernoulli). © 2020, The Author(s).","Artificial intelligence; Software engineering; Adversarial networks; Experimental validations; Image generations; Large margins; Large-scale datasets; Natural scenes; Synthetic and real data; Target space; Large dataset","Adversarial attacks; Autoencoder; Conditional GAN; Cross-noise experiments; Robust regression; Super-resolution; Unsupervised learning","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85087975271"
"Çelik G.; Talu M.F.","Çelik, Gaffari (57200140120); Talu, Muhammed Fatih (35732416100)","57200140120; 35732416100","Resizing and cleaning of histopathological images using generative adversarial networks","2020","Physica A: Statistical Mechanics and its Applications","554","","122652","","","","10.1016/j.physa.2019.122652","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083291576&doi=10.1016%2fj.physa.2019.122652&partnerID=40&md5=83147b73fd016b7fbf0fa7e1d8e0443a","Bilinear and Bicubic interpolation techniques are frequently used to increase image resolution. These techniques with data modeling approach are replaced by intelligent systems that can learn automatically from data. SRGAN is a modern Generative Adversarial Network developed as an alternative to classical interpolation techniques. His ability to produce images in super resolution has attracted the attention of many researchers. In this study, noise elimination performance of super resolution generative adversarial network (SRGAN) with image magnification was investigated. The results of the noise cleaning were compared with the classical approaches (mean, median, adaptive filters). SSIM, PSNR and FFT_MSE metrics were evaluated in experimental studies using images in the data set Camelyon17. When the results were evaluated, it was observed that SRGAN was superior to the classical approaches not only in increasing the resolution but also in the noise cleaning area. © 2019","Adaptive filtering; Adaptive filters; Image resolution; Intelligent systems; Interpolation; Median filters; Optical resolving power; Adversarial networks; Bicubic interpolation; Classical approach; Histopathological images; Image magnification; Interpolation techniques; Noise elimination; Super resolution; Cleaning","Bicubic; Camelyon17; Image resizing; Noise cleaning; SRGAN","Article","Final","","Scopus","2-s2.0-85083291576"
"Jiang X.; Liu M.; Zhao F.; Liu X.; Zhou H.","Jiang, Xin (57198959704); Liu, Mingzhe (8921200600); Zhao, Feixiang (57208685453); Liu, Xianghe (57207259491); Zhou, Helen (56139009800)","57198959704; 8921200600; 57208685453; 57207259491; 56139009800","A novel super-resolution CT image reconstruction via semi-supervised generative adversarial network","2020","Neural Computing and Applications","32","18","","14563","14578","15","10.1007/s00521-020-04905-8","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085154189&doi=10.1007%2fs00521-020-04905-8&partnerID=40&md5=38c3388b68d491e94733a61a73102005","Reconstruction of super-resolution CT images using deep learning requires a large number of high-resolution images. However, high-resolution images are often limited to access due to CT performance and operation factors. In this paper, a new semi-supervised generative adversarial network is presented to accurately recover high-resolution CT images from low-resolution counterparts. We use a deep unsupervised network of 16 residual blocks to design the generator and build a discriminator based on a supervised network. We also apply a parallel 1 × 1 convolution operation to reduce the dimensionality of each hidden layer’s output. Four types of loss functions are presented to build a new one for enforcing the mappings between the generator and discriminator. The bulk specification layer in the commonly used residual network is removed to construct a new type of residual network. In terms of experiments, we conduct an objective and subjective comprehensive evaluation with several state-of-the-art methods. The comparison results show that our proposed network has better advantages in super-resolution image reconstruction. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Computerized tomography; Deep learning; Optical resolving power; Adversarial networks; Comprehensive evaluation; Ct image reconstruction; High resolution CT; High resolution image; State-of-the-art methods; Super-resolution image reconstruction; Unsupervised network; Image reconstruction","Computed tomography images; Generative adversarial network; Residual blocks; Super-resolution","Article","Final","","Scopus","2-s2.0-85085154189"
"Dai Q.; Cheng X.; Qiao Y.; Zhang Y.","Dai, Qiang (57650365900); Cheng, Xi (57206756990); Qiao, Yan (36988851000); Zhang, Youhua (36784200800)","57650365900; 57206756990; 36988851000; 36784200800","Crop leaf disease image super-resolution and identification with dual attention and topology fusion generative adversarial network","2020","IEEE Access","8","","9042295","55724","55735","11","10.1109/ACCESS.2020.2982055","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082681001&doi=10.1109%2fACCESS.2020.2982055&partnerID=40&md5=1c861fefba7350a46336a4b379f13292","For agricultural disease image identification, obtained images are typically unclear, which can lead to poor identification results in real production environments. The quality of an image has a significant impact on the identification accuracy of pre-trained image classifiers. To address this problem, we propose a generative adversarial network with dual-attention and topology-fusion mechanisms called DATFGAN. This network can effectively transform unclear images into clear and high-resolution images. Additionally, the weight sharing scheme in our proposed network can significantly reduce the number of parameters. Experimental results demonstrate that DATFGAN yields more visually pleasing results than state-of-the-art methods. Additionally, treated images are evaluated based on identification tasks. The results demonstrate that the proposed method significantly outperforms other methods and is sufficiently robust for practical use. © 2013 IEEE.","Agricultural robots; Crops; Identification (control systems); Optical resolving power; Topology; Adversarial networks; attention; Identification accuracy; Image super resolutions; Leaf disease; Production environments; State-of-the-art methods; Super resolution; Image processing","attention; Crop leaf disease; generative adversarial networks; identification; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082681001"
"","","","International Conference on Communications, Signal Processing, and Systems, CSPS 2018","2020","Lecture Notes in Electrical Engineering","516","","","","","1120","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071468441&partnerID=40&md5=0ad6f5ad312d388b4f7af7ddd316df81","The proceedings contain 131 papers. The special focus in this conference is on Communications, Signal Processing, and Systems. The topics include: Performance Research of Multiuser Interference in Chaotic UWB System; dual-Feature Spectrum Sensing Exploiting Eigenvalue and Eigenvector of the Sampled Covariance Matrix; adaptive Scale Mean-Shift Tracking with Gradient Histogram; Improved Performance of CDL Algorithm Using DDELM-AE and AK-SVD; Body Gestures Recognition Based on CNN-ELM Using Wi-Fi Long Preamble; evaluation of Local Features Using Convolutional Neural Networks for Person Re-Identification; A Modulation Recognition Method Based on Bispectrum and DNN; Image-to-Image Local Feature Translation Using Double Adversarial Networks Based on CycleGAN; Real-Time Vehicle Color Recognition Based on YOLO9000; evaluation Embedding Features for Ground-Based Cloud Classification; A Gradient Invariant DCT-Based Image Watermarking Scheme for Object Detection; a Method for Under-Sampling Modulation Pattern Recognition in Satellite Communication; sequential Modeling for Polyps Identification from the Vocal Data; audio Tagging With Connectionist Temporal Classification Model Using Sequentially Labelled Data; Implementation of AdaBoost Face Detection Using Vivado HLS; research on Rolling Bearing On-Line Fault Diagnosis Based on Multi-dimensional Feature Extraction; multi-pose Face Recognition Based on Contour Symmetric Constraint-Generative Adversarial Network; flight Target Recognition via Neural Networks and Information Fusion; specific Emitter Identification Based on Feature Selection; improved Video Reconstruction Basing on Single-Pixel Camera By Dual-Fiber Collecting; nonlinear Dynamical System Analysis for Continuous Gesture Recognition; feature Wave Recognition-Based Signal Processing Method for Transit-Time Ultrasonic Flowmeter; realization of Unmanned Cruise Boat for Water Quality; improved K-Means Clustering for Target Activity Regular Pattern Extraction with Big Data Mining; single-Image Super-Resolution: A Survey.","","","Conference review","Final","","Scopus","2-s2.0-85071468441"
"Pineda F.; Ayma V.; Aduviri R.; Beltran C.","Pineda, Ferdinand (57216822078); Ayma, Victor (56566776600); Aduviri, Robert (57207467513); Beltran, Cesar (55602499700)","57216822078; 56566776600; 57207467513; 55602499700","Super Resolution Approach Using Generative Adversarial Network Models for Improving Satellite Image Resolution","2020","Communications in Computer and Information Science","1070 CCIS","","","291","298","7","10.1007/978-3-030-46140-9_27","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084819739&doi=10.1007%2f978-3-030-46140-9_27&partnerID=40&md5=9fe165b60f94c8fbbadd58a0c7da0341","Recently, the number of satellite imaging sensors deployed in space has experienced a considerable increase, but most of these sensors provide low spatial resolution images, and only a small proportion contribute with images at higher resolutions. This work proposes an alternative to improve the spatial resolution of Landsat-8 images to the reference of Sentinel-2 images, by applying a Super Resolution (SR) approach based on the use of Generative Adversarial Network (GAN) models for image processing, as an alternative to traditional methods to achieve higher resolution images, hence, remote sensing applications could take advantage of this new information and improve its outcomes. We used two datasets to train and validate our approach, the first composed by images from the DIV2K open access dataset and the second by images from Sentinel-2 satellite. The experimental results are based on the comparison of the similarity between the Landsat-8 images obtained by the super resolution processing by our approach (for both datasets), against its corresponding reference from Sentinel-2 satellite image, computing the Peak Signal-to-Noise Ratio (PSNR) and the Structural Similarity (SSIM) as metrics for this purpose. In addition, we present a visual report in order to compare the performance of each trained model, analysis that shows interesting improvements of the resolution of Landsat-8 satellite images. © Springer Nature Switzerland AG 2020.","Big data; Image resolution; Information management; Optical resolving power; Remote sensing; Signal to noise ratio; Small satellites; Adversarial networks; Higher resolution images; Peak signal to noise ratio; Remote sensing applications; Satellite imaging; Spatial resolution; Spatial resolution images; Structural similarity; Image enhancement","Landsat-8; Sentinel-2; SR-GAN; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85084819739"
"Liu Z.; Li L.; Wu Y.; Zhang C.","Liu, Zhilei (56101484100); Li, Le (57212493997); Wu, Yunpeng (57215430466); Zhang, Cuicui (56097274400)","56101484100; 57212493997; 57215430466; 56097274400","Facial Expression Restoration Based on Improved Graph Convolutional Networks","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11962 LNCS","","","527","539","12","10.1007/978-3-030-37734-2_43","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080967619&doi=10.1007%2f978-3-030-37734-2_43&partnerID=40&md5=0ae0e4cba880ba375d25b5c09bb9f81b","Facial expression analysis in the wild is challenging when the facial image is with low resolution or partial occlusion. Considering the correlations among different facial local regions under different facial expressions, this paper proposes a novel facial expression restoration method based on generative adversarial network by integrating an improved graph convolutional network (IGCN) and region relation modeling block (RRMB). Unlike conventional graph convolutional networks taking vectors as input features, IGCN can use tensors of face patches as inputs. It is better to retain the structure information of face patches. The proposed RRMB is designed to address facial generative tasks including inpainting and super-resolution with facial action units detection, which aims to restore facial expression as the ground-truth. Extensive experiments conducted on BP4D and DISFA benchmarks demonstrate the effectiveness of our proposed method through quantitative and qualitative evaluations. © 2020, Springer Nature Switzerland AG.","Convolution; Adversarial networks; Convolutional networks; Facial action; Facial expression analysis; Facial Expressions; Qualitative evaluations; Restoration methods; Structure information; Restoration","Facial action units; Facial expression restoration; Generative adversarial network; Graph convolutional network","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85080967619"
"Chen H.; Zhang X.; Liu Y.; Zeng Q.","Chen, Hongguang (57211015334); Zhang, Xing (57211014828); Liu, Yintian (14041952200); Zeng, Qiangyu (55425592500)","57211015334; 57211014828; 14041952200; 55425592500","Generative adversarial networks capabilities for super-resolution reconstruction of weather radar echo images","2019","Atmosphere","10","9","555","","","","10.3390/atmos10090555","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072241158&doi=10.3390%2fatmos10090555&partnerID=40&md5=28b85010879225bc66b6b7857d0bec96","Improving the resolution of degraded radar echo images of weather radar systems can aid severe weather forecasting and disaster prevention. Previous approaches to this problem include classical super-resolution (SR) algorithms such as iterative back-projection (IBP) and a recent nonlocal self-similarity sparse representation (NSSR) that exploits the data redundancy of radar echo data, etc. However, since radar echoes tend to have rich edge information and contour textures, the textural detail in the reconstructed echoes of traditional approaches is typically absent. Inspired by the recent advances of faster and deeper neural networks, especially the generative adversarial networks (GAN), which are capable of pushing SR solutions to the natural image manifold, we propose using GAN to tackle the problem of weather radar echo super-resolution to achieve better reconstruction performance (measured in peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM)). Using authentic weather radar echo data, we present the experimental results and compare its reconstruction performance with the above-mentioned methods. The experimental results showed that the GAN-based method is capable of generating perceptually superior solutions while achieving higher PSNR/SSIM results. © 2019 by the authors.","Deep learning; Disaster prevention; Image enhancement; Image reconstruction; Iterative methods; Meteorological radar; Optical resolving power; Radar measurement; Signal to noise ratio; Textures; Weather forecasting; Adversarial networks; Image super resolutions; Iterative back projections; Peak signal to noise ratio; Sparse representation; Structural similarity indices (SSIM); Super resolution reconstruction; Traditional approaches; algorithm; artificial neural network; image resolution; meteorological hazard; radar; weather forecasting; Radar imaging","Deep learning; Generative adversarial networks; Image super-resolution; Weather radar","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85072241158"
"Eskimez S.E.; Koishida K.","Eskimez, Sefik Emre (57189600042); Koishida, Kazuhito (6601990788)","57189600042; 6601990788","Speech Super Resolution Generative Adversarial Network","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8682215","3717","3721","4","10.1109/ICASSP.2019.8682215","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068990002&doi=10.1109%2fICASSP.2019.8682215&partnerID=40&md5=1828ca6f20465d919c9ef1ff70949550","The goal of speech super-resolution (SSR) or speech bandwidth expansion is to generate the missing high-frequency components for a given low-resolution speech signal. It has the potential to improve the quality of telecommunications. We propose a new method for SSR that leverages the generative adversarial networks (GANs) and a regularization method for stabilizing the GAN training. The generator network is a convolutional autoencoder with 1D convolution kernels, operating along time-axis and generating the high-frequency log-power spectra from the low-frequency log-power spectra input. We employ two recent deep neural network (DNN) based approaches to compare them with our proposed method, including both objective speech quality metrics and subjective perceptual tests. We show that our proposed method outperforms the baseline methods in terms of both objective and subjective evaluations. © 2019 IEEE.","Bandwidth; Convolution; Deep neural networks; Optical resolving power; Power spectrum; Speech; Speech communication; Adversarial networks; Bandwidth expansions; Bandwidth extension; High frequency components; Objective and subjective evaluations; Objective speech quality; Regularization methods; Super resolution; Audio signal processing","artificial speech bandwidth extension; generative adversarial networks; speech super-resolution","Conference paper","Final","","Scopus","2-s2.0-85068990002"
"Wu Y.; Yang F.; Huang J.; Liu Y.","Wu, Yangyang (57215824161); Yang, Feng (57198992554); Huang, Jing (7407189188); Liu, Yaqin (56422566000)","57215824161; 57198992554; 7407189188; 56422566000","Super-resolution construction of intravascular ultrasound images using generative adversarial networks","2019","Nan fang yi ke da xue xue bao = Journal of Southern Medical University","39","1","","82","87","5","10.12122/j.issn.1673-4254.2019.01.13","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081947770&doi=10.12122%2fj.issn.1673-4254.2019.01.13&partnerID=40&md5=3a20a11687992d06086260902aa57507","The low-resolution ultrasound images have poor visual effects. Herein we propose a method for generating clearer intravascular ultrasound images based on super-resolution reconstruction combined with generative adversarial networks. We used the generative adversarial networks to generate the images by a generator and to estimate the authenticity of the images by a discriminator. Specifically, the low-resolution image was passed through the sub-pixel convolution layer r2-feature channels to generate r2-feature maps in the same size, followed by realignment of the corresponding pixels in each feature map into r ×r sub-blocks, which corresponded to the sub-block in a high-resolution image; after amplification, an image with a r2-time resolution was generated. The generative adversarial networks can obtain a clearer image through continuous optimization. We compared the method (SRGAN) with other methods including Bicubic, super-resolution convolutional network (SRCNN) and efficient sub-pixel convolutional network (ESPCN), and the proposed method resulted in obvious improvements in the peak signal-to-noise ratio (PSNR) by 2.369 dB and in structural similarity index by 1.79% to enhance the diagnostic visual effects of intravascular ultrasound images.","Blood Vessels; Endosonography; Image Enhancement; Image Processing, Computer-Assisted; Signal-To-Noise Ratio; blood vessel; diagnostic imaging; endoscopic ultrasonography; image enhancement; image processing; procedures; signal noise ratio","generative adversarial network; intravascular ultrasound; sub-pixel convolution layer; super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85081947770"
"Wang M.; He L.; Chang X.; Cheng Y.","Wang, Mi (57205635094); He, Luxiao (57201260628); Chang, Xueli (56421263200); Cheng, Yufeng (56861501800)","57205635094; 57201260628; 56421263200; 56861501800","Corrections to: Superresolution of single gaofen-4 visible-light and near-infrared (VNIR) image based on texture image extraction (IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing DOI: 10.1109/JSTARS.2019.2926490)","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","8","8823074","3149","","","10.1109/JSTARS.2019.2937427","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072749190&doi=10.1109%2fJSTARS.2019.2937427&partnerID=40&md5=f37eaf1a7aff60b63f26d90894a244bc","In the above article [1], production errors are corrected as follows. Funding information should be added as follows: This work was supported in part by the China National Funds for Distinguished Young Scientists under Grant 61825103 and in part by the National Natural Science Foundation of China under Grant 91838303. On page 3, Ref. [36] should be cited in the sentence: ""For panchromatic and multispectral images of the same scene, the two have strong correlation [34]-[36]."" On page 5, the sentence ""For referenced frame construction, it is more important to choose which LR texture image."" should be ""For referenced frame construction, it is important to choose LR texture image."" On page 5, the sentence ""Local or global motion can be ignored since the data a single GF-4 VNIR image."" should be ""Local or global motion can be ignored since the input data is a single image."" On page 7, a sentence should be added as follows. In Tables III-V, the evaluation parameters of SR results in urban, lake, and mountain area, respectively, have been presented. The bold values in Tables III-V are the best parameters of the five methods."" An acknowledgement should be added as follows: ""The authors would like to thank CRESDA for providing the experimental data. Corrected references are as follows. [15] M. Irani and S. Peleg, ""Improving resolution by image registration,"" CVGIP, Graphical Models Image Process., vol. 53, no. 3, pp. 231-239, 1991.[19] R. R. Schultz and R. L. Stevenson, ""Extraction of HR frames from video sequences,"" IEEE Trans. Image Process., vol. 5, no. 6, pp. 996-1011, Jun. 1996. [28] C. Ledig et al., ""Photo-realistic single image super-resolution using a generative adversarial network,"" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., Jul. 2017, pp. 105-114. [30] C. Dong, C. C. Loy, K. He, and X. Tang, ""Learning a deep convolutional network for image super-resolution,"" in Proc. Eur. Conf. Comput. Vision, 2014, pp. 184-199. [33] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, ""Residual dense network for image super-resolution,"" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2018, pp. 2472-2481. [34] X. U. Qi-Zhi and G. Feng, ""High fidelity panchromatic and multispectral image fusion based on ratio transform,"" Comput. Sci., vol. 41, no. 10, pp. 19-22, 2014. [37] R. C. Gonzalez and R. E. Woods, Digital Image Processing. Beijing, China: Publishing House Electron. Ind., 2007. © 2008-2012 IEEE.","","","Erratum","Final","","Scopus","2-s2.0-85072749190"
"Yan B.; Bare B.; Ma C.; Li K.; Tan W.","Yan, Bo (36764113400); Bare, Bahetiyaer (56433101200); Ma, Chenxi (57210451402); Li, Ke (55322485200); Tan, Weimin (57188575769)","36764113400; 56433101200; 57210451402; 55322485200; 57188575769","Deep Objective Quality Assessment Driven Single Image Super-Resolution","2019","IEEE Transactions on Multimedia","21","11","8705363","2957","2971","14","10.1109/TMM.2019.2914883","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071765236&doi=10.1109%2fTMM.2019.2914883&partnerID=40&md5=da3ee0f15be34985262cd0f0d1b0b869","Single-image super-resolution (SISR) is a classic problem in the image processing community, which aims at generating a high-resolution image from a low-resolution one. In recent years, deep learning based SISR methods emerged and achieved a performance leap than previous methods. However, because the evaluation metrics of SISR methods is peak signal-to-noise ratio (PSNR), previous methods usually choose L2-norm as the loss function. This leads to a significant improvement in the final PSNR value but little improvement in perceptual quality. In this paper, in order to achieve better results in both perceptual quality and PSNR values, we propose an objective quality assessment driven SISR method. First, we propose a novel full-reference image quality assessment approach for SISR and employ it as a loss function, namely super-resolution image quality assessment (SR-IQA) loss. Then, we combine SR-IQA loss with L2-norm to guide our proposed SISR method to achieve better results. Besides that, our proposed SISR method consists of several proposed highway units. Furthermore, in order to verify the generalization ability of our new kind of loss function, we integrate SR-IQA loss to generative adversarial networks based SR method and achieve better perceptual quality. Experimental results prove that our proposed SISR method achieves better performance than other methods both qualitatively and quantitatively in most of the cases. © 1999-2012 IEEE.","Deep learning; Image enhancement; Optical resolving power; Signal to noise ratio; Adversarial networks; Evaluation metrics; Full reference quality assessment; Generalization ability; High resolution image; Objective quality assessment; Peak signal to noise ratio; Single images; Image quality","full-reference quality assessment; generative adversarial networks; image enhancement; Single image super-resolution","Article","Final","","Scopus","2-s2.0-85071765236"
"Chen Y.; Sun J.; Jiao W.; Zhong G.","Chen, Yang (57211908333); Sun, Jinxuan (57211905384); Jiao, Wencong (57205205588); Zhong, Guoqiang (56393836400)","57211908333; 57211905384; 57205205588; 56393836400","Recovering super-resolution generative adversarial network for underwater images","2019","Communications in Computer and Information Science","1142 CCIS","","","75","83","8","10.1007/978-3-030-36808-1_9","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083662266&doi=10.1007%2f978-3-030-36808-1_9&partnerID=40&md5=a04b397267a5f86c6482fca9cec20067","In this paper, we propose an end-to-end Recovering Super-Resolution Generative Adversarial Network (RSRGAN) to automatically learn super-resolution underwater images. RSRGAN mainly includes two parts. The first part is a Recovering GAN, aiming at color correction and removing noise in the images. The generator of Recovering GAN is based on an encoder-decoder network with self-attention on the global feature. The second part is a Super-Resolution GAN, which adopts the residual-in-residual dense block in its generator, to add details onto the results fed from the Recovering GAN. Both qualitative and quantitative experimental results show the advantage of RSRGAN over the state-of-the-art approaches for underwater image super-resolution. © Springer Nature Switzerland AG 2019.","Recovery; Adversarial networks; Color correction; Encoder-decoder; Global feature; Image super resolutions; Removing noise; State-of-the-art approach; Super resolution; Optical resolving power","Generative adversarial network; Super-resolution; Underwater images","Conference paper","Final","","Scopus","2-s2.0-85083662266"
"Zhang C.; Du F.; Zhang Y.","Zhang, Cai (57210917000); Du, Fei (57210910756); Zhang, Yungang (36761729500)","57210917000; 57210910756; 36761729500","A Brief Review of Image Restoration Techniques Based on Generative Adversarial Models","2020","Lecture Notes in Electrical Engineering","590","","","169","175","6","10.1007/978-981-32-9244-4_24","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071880345&doi=10.1007%2f978-981-32-9244-4_24&partnerID=40&md5=49d7b73abbf1d9b1bd3a06ff2592179b","Images are possibly degraded by various reasons, the typical forms of degradation are: blur, noise, low resolution, and etc. Image restoration techniques try to recover the degraded images to the original images with maximum fidelity. Image restoration is a challenging task and also an import research area in image processing. During the decades, researchers have proposed many restoration methods such as inverse filter, Weiner filter, wavelet analysis, support vector machine, and etc. Recently, deep learning has been increasingly popular among researchers and has obtained remarkable results. In this paper, we briefly review the approaches based on generative adversarial networks (GANs) for image restoration. The typical GANs based restoration methods for image super-resolution, image denoising, image inpainting and image deblurring are introduced and discussed. © 2020, Springer Nature Singapore Pte Ltd.","Deep learning; Deep neural networks; Image denoising; Image enhancement; Inverse problems; Neural networks; Restoration; Support vector machines; Adversarial networks; Degraded images; Image deblurring; Image Inpainting; Image restoration techniques; Image super resolutions; Original images; Restoration methods; Image reconstruction","Deep learning; Degraded image; Generative adversarial networks; Image restoration; Neural network","Conference paper","Final","","Scopus","2-s2.0-85071880345"
"Lee O.-Y.; Shin Y.-H.; Kim J.-O.","Lee, Oh-Young (56022244300); Shin, Yoon-Ho (57214139522); Kim, Jong-Ok (36118022200)","56022244300; 57214139522; 36118022200","Multi-Perspective Discriminators-Based Generative Adversarial Network for Image Super Resolution","2019","IEEE Access","7","","8845591","136496","136510","14","10.1109/ACCESS.2019.2942779","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078228188&doi=10.1109%2fACCESS.2019.2942779&partnerID=40&md5=4d68679a155e2499363fade66b1e51dd","Recently, generative adversarial network-based image super resolution has been investigated, and it has been shown to lead to overwhelming improvements in subjective quality. However, it also leads to checkerboard artifacts and the unpleasing high-frequency (HF) components. In this paper, we propose a multi-discriminators-based image super resolution method that distinguishes those artifacts from various perspectives. First, the DCT perspective discriminator is proposed because the checkerboard artifacts are easily separated on the frequency domain. Second, the gradient perspective discriminator is proposed, because the unpleasing HF components can be discriminated on the gradient magnitude distribution. These proposed multi-perspective discriminators can easily identify artifacts, and they can help the generator reproduce artifact-less SR images. The experimental results show that the proposed SR-GAN with multi-perspective discriminators achieves objective and subjective quality improvements in terms of PSNR, SSIM, PI and MOS, as compared to the conventional SR-GAN by reducing the aforementioned artifacts. © 2013 IEEE.","Deep learning; Discriminators; Frequency domain analysis; Image resolution; Optical resolving power; Adversarial networks; Frequency domains; Gradient magnitude; High frequency components; Image super resolutions; Multi-perspective; Subjective quality; Super resolution; Image enhancement","deep learning for super resolution; Image super-resolution; multi-discriminators; SR GAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078228188"
"Kim J.Y.; Lee J.W.; Song K.H.; Kim Y.-S.","Kim, Joo Youn (57211615480); Lee, Joung Woo (57212136977); Song, Kwang Ho (57192083394); Kim, Yoo-Sung (57219694617)","57211615480; 57212136977; 57192083394; 57219694617","Vehicle model recognition using SRGAN for low-resolution vehicle images","2019","ACM International Conference Proceeding Series","","","","42","45","3","10.1145/3357254.3357284","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075975767&doi=10.1145%2f3357254.3357284&partnerID=40&md5=4cbb390e026be77b8b40f7380a9bef5b","An enhanced vehicle model recognizer for low-resolution images is proposed in where SRGAN (Super Resolution Generative Adversarial Network) is used to enhance the image quality and CNN (Convolutional Neural Network) is used to classify the vehicle model from the enhanced images. Many previous vehicle model classifiers trained with only the high-resolution front-images of vehicles have low accuracy against the low-quality images captured by CCTV cameras in real environments. To correctly classify the vehicle model from the low-quality images of arbitrary directions, SRGAN is first used to transform the low-resolution image into the corresponding high-resolution image. Then the direction of the vehicle in the image is determined and the vehicle model is recognized based on the pre-determined direction. The accuracy of the proposed vehicle model classifier is evaluated as 78%, higher than that of the classification without SRGAN. © 2019 Association for Computing Machinery.","Convolution; Convolutional neural networks; Optical resolving power; Pattern recognition; Vehicles; Video cameras; Adversarial networks; Arbitrary direction; High resolution; High resolution image; Low resolution images; Real environments; Super resolution; Vehicle model; Image enhancement","CCTV camera; Convolutional neural network; Low-resolution image; Super resolution generative adversarial network; Vehicle model recognizer","Conference paper","Final","","Scopus","2-s2.0-85075975767"
"Huo D.; Wang R.; Ding J.","Huo, Dongqi (57211431863); Wang, Rong (56193225300); Ding, Jianwei (37017026900)","57211431863; 56193225300; 37017026900","Attention-Based GAN for Single Image Super-Resolution","2019","Communications in Computer and Information Science","1043","","","360","369","9","10.1007/978-981-13-9917-6_35","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073899702&doi=10.1007%2f978-981-13-9917-6_35&partnerID=40&md5=44fd85df9beae1fb48f34ab38f4320df","Single Image Super-Resolution task based on GANs has shown a great improvement in all methods, but still has the optimization problems of texture details and distortion of local regions in super-resolved images. In this paper, we proposed an attention-based GAN architecture to solve preceding problems. Specifically, we first implemented attention mechanism in both Generator and Discriminator. Secondly, we adopted a three-step training for all architecture models and adjusted the adoption frequency of attention implement to make pre-trained model perform better. Extensive experiments on Set5, Set14 and BSD100 showed that the better pre-trained model of ours not only remedied the distortion of local regions, but also achieved the better perceptual quality than the original architecture. © 2019, Springer Nature Singapore Pte Ltd.","Network architecture; Optical resolving power; Textures; Adversarial networks; Architecture models; Attention mechanisms; Local region; Optimization problems; Perceptual quality; Single images; Super resolution; Image enhancement","Attention mechanism; Generative Adversarial Network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85073899702"
"Xu L.; Zeng X.; Huang Z.; Li W.; Zhang H.","Xu, Liming (57210788847); Zeng, Xianhua (54920639100); Huang, Zhiwei (57193565509); Li, Weisheng (36067507500); Zhang, He (57210801226)","57210788847; 54920639100; 57193565509; 36067507500; 57210801226","Low-dose chest X-ray image super-resolution using generative adversarial nets with spectral normalization","2020","Biomedical Signal Processing and Control","55","","101600","","","","10.1016/j.bspc.2019.101600","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071427447&doi=10.1016%2fj.bspc.2019.101600&partnerID=40&md5=6a484e49fe7b0069e2ce1fa04ce66199","Chest X-ray (CXR) imaging is one of the most widely-used and cost-effective technology for chest screening and diagnosis of Pulmonary diseases. An always concerned improvement about CXR is to reduce X-ray radiation while achieving ultra-high quality imaging with fine structural details since CXR involves ionizing radiation and tolerance of different populations. In this paper, we present a supervised generative adversarial nets approach to accurately recover high-resolution (HR) CXR images from low-resolution (LR) counterparts while keep pathological invariance. Specifically, the auxiliary label information is introduced to constrain the feature generation to attack the potential risk of pathological variance. Then, spectral normalization is designed to control the performance of discriminative network with the guarantee of theoretical demonstration in controlling Lipschitz bound of discriminator. Results from quantitative and qualitative evaluations demonstrate that our method delivers more authentic improvement for CXR super-resolution (SR) compared to recent state-of-the-art methods. The proposed method has outperformed average 13.0%, 12.2% in FSIM and 13.7%, 12.5% in MSIM on two datasets, respectively. Besides, the index of generative performance GAN-train and GAN-test have achieved average increment 9.3% and 10.5% on CXR2 dataset. Subjective evaluation on SR CXR has outperformed average score 0.425 and 0.525 in terms of pathological invariance and acceptability, respectively. © 2019","Cost effectiveness; Diagnosis; Optical resolving power; Statistical tests; Auxiliary information; Cost-effective technology; Discriminative networks; Feature generation; Generative adversarial nets; Qualitative evaluations; Spectral normalization; Subjective evaluations; Article; artificial neural network; cost effectiveness analysis; generative adversarial network; human; image processing; image quality; nuclear magnetic resonance imaging; priority journal; radiation dose; thorax radiography; X rays","Auxiliary information; Generative adversarial nets; Pathological invariance; Spectral normalization","Article","Final","","Scopus","2-s2.0-85071427447"
"Huang Z.-X.; Jing C.-W.","Huang, Zhi-Xing (57215537930); Jing, Chang-Wei (57542839000)","57215537930; 57542839000","Super-Resolution Reconstruction Method of Remote Sensing Image Based on Multi-Feature Fusion","2020","IEEE Access","8","","8963714","18764","18771","7","10.1109/ACCESS.2020.2967804","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081112726&doi=10.1109%2fACCESS.2020.2967804&partnerID=40&md5=dde8f91a5cb3bdf1f9a0c3c688eb09c8","The acquisition of remote sensing images is affected by imaging equipment and environmental conditions. Usually on lower performance devices, the resolution of the acquired images is also low. Among many methods, the super-resolution reconstruction method based on generative adversarial networks has obvious advantages over previous network models in reconstructing image texture details. However, it is found in experiments that not all of these reconstructed textures exist in the image itself. Aiming at the problem of whether the texture details of the reconstructed image are accurate and clear, we propose a super-resolution reconstruction method combining wavelet transform and generative adversarial network. Using wavelet multi-resolution analysis, training wavelet decomposition coefficients in the generative adversarial network can effectively improve the local detail information of the reconstructed image. Experimental results show that our method can effectively reconstruct more natural image textures and make the images more visually clear. In the remote sensing image test set, the four indicators of the algorithm, peak signal to noise ratio (PSNR), structural similarity (SSIM), Feature Similarity (FSIM) and Universal Image Quality (UIQ) are slightly better than the algorithms mentioned in the article. © 2020 IEEE.","Image acquisition; Image enhancement; Image quality; Image reconstruction; Optical resolving power; Remote sensing; Signal to noise ratio; Textures; Wavelet decomposition; Decomposition coefficient; Feature similarity(FSIM); Peak signal to noise ratio; Remote sensing images; Self correlation; Super resolution; Super resolution reconstruction; Wavelet multi-resolution analysis; Image texture","image texture; Remote sensing image; self-correlation; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081112726"
"Li J.; Cui R.; Li Y.; Li B.; Du Q.; Ge C.","Li, Jiaojiao (55934244200); Cui, Ruxing (57211523782); Li, Yunsong (55986546100); Li, Bo (57188584536); Du, Qian (7202060063); Ge, Chiru (57189461615)","55934244200; 57211523782; 55986546100; 57188584536; 7202060063; 57189461615","Multitemporal Hyperspectral Image Super-Resolution through 3D Generative Adversarial Network","2019","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images, MultiTemp 2019","","","8866956","","","","10.1109/Multi-Temp.2019.8866956","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074259539&doi=10.1109%2fMulti-Temp.2019.8866956&partnerID=40&md5=b367650ef774fd46193554b7d9860789","The super-resolution of multitemporal hyperspectral imagery is considered, wherein a 3D generative adversarial network (GAN) is promoted and employed. Firstly, we put the SR process in a generative adversarial network (GAN) framework, so that the resulted high resolution HSI can keep more texture details. Secondly, the input of our method is of full bands due to 3D kernel exploited. Furthermore, a series of spatial-spectral constraints or loss functions are imposed to guide the training of our generative network so as to further alleviate spectral distortion and texture blur. The experiments on the houston datasets demonstrate that the proposed GAN-based SR method with the best generalization ability can yield very high quality results. © 2019 IEEE.","Image analysis; Optical resolving power; Spectroscopy; Textures; Adversarial networks; Generalization ability; High resolution; Hyper-spectral imageries; Image super resolutions; Spectral constraints; Spectral distortions; Super resolution; Remote sensing","GAN; generalization ability; hyperspectral super-resolution; Multitemporal Hyperspectral imagery","Conference paper","Final","","Scopus","2-s2.0-85074259539"
"Zuo F.; Liu X.","Zuo, Fang (57220495383); Liu, Xiaofang (57213521179)","57220495383; 57213521179","DPGAN: Prelu used in deep convolutional generative adversarial networks","2019","ACM International Conference Proceeding Series","","","","56","61","5","10.1145/3366715.3366728","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077819161&doi=10.1145%2f3366715.3366728&partnerID=40&md5=c97c21a234e9123543727773408280cb","This paper is directed against the image super resolution problem that is an extraordinary topic in the field of computer vision. We proposed a new model PReLU Used in Deep Convolutional Generative Adversarial Networks (DPGAN) which has designed a pre-training structure, and the generator and the discriminator are cross-optimized to form a stable network structure. In the proposed model, the activation function in the generator uses the PReLU[1] innovatively. The experimental results demonstrate that the images generated by the proposed model have higher resolution, which is compared with previously studied models. © 2019 Association for Computing Machinery.","Chemical activation; Convolution; Optical resolving power; Robotics; Activation functions; Adversarial networks; DPGAN; Higher resolution; Image super resolutions; Pre-training; PReLU; Super resolution; Convolutional neural networks","Activation function; DPGAN; PReLU; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85077819161"
"Konig P.; Aigner S.; Korner M.","Konig, Peter (57211625735); Aigner, Sandra (57211639840); Korner, Marco (57190168095)","57211625735; 57211639840; 57190168095","Enhancing Traffic Scene Predictions with Generative Adversarial Networks","2019","2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019","","","8917046","1768","1775","7","10.1109/ITSC.2019.8917046","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076819069&doi=10.1109%2fITSC.2019.8917046&partnerID=40&md5=011334e6a6e17746ab2b8f2996afe67c","We present a new two-stage pipeline for predicting frames of traffic scenes where relevant objects can still reliably be detected. Using a recent video prediction network, we first generate a sequence of future frames based on past frames. A second network then enhances these frames in order to make them appear more realistic. This ensures the quality of the predicted frames to be sufficient to enable accurate detection of objects, which is especially important for autonomously driving cars. To verify this two-stage approach, we conducted experiments on the Cityscapes dataset. For enhancing, we trained two image-to-image translation methods based on generative adversarial networks, one for blind motion deblurring and one for image super-resolution. All resulting predictions were quantitatively evaluated using both traditional metrics and a state-of-the-art object detection network showing that the enhanced frames appear qualitatively improved. While the traditional image comparison metrics, i.e., MSE, PSNR, and SSIM, failed to confirm this visual impression, the object detection evaluation resembles it well. The best performing prediction-enhancement pipeline is able to increase the average precision values for detecting cars by about 9% for each prediction step, compared to the non-enhanced predictions. © 2019 IEEE.","Forecasting; Intelligent systems; Intelligent vehicle highway systems; Object detection; Object recognition; Petroleum reservoir evaluation; Pipelines; Adversarial networks; Detection networks; Image comparison; Image super resolutions; Image translation; Motion deblurring; Two stage approach; Video prediction; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076819069"
"Tran K.; Panahi A.; Adiga A.; Sakla W.; Krim H.","Tran, Kenneth (57052318100); Panahi, Ashkan (36470876800); Adiga, Aniruddha (36447297900); Sakla, Wesam (14069004700); Krim, Hamid (26643057600)","57052318100; 36470876800; 36447297900; 14069004700; 26643057600","Nonlinear Multi-scale Super-resolution Using Deep Learning","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8682354","3182","3186","4","10.1109/ICASSP.2019.8682354","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069000640&doi=10.1109%2fICASSP.2019.8682354&partnerID=40&md5=7553e8638c9a937e4390ebb9c7d1e70f","We propose a deep learning architecture capable of performing up to 8× single image super-resolution. Our architecture incorporates an adversarial component from the super-resolution generative adversarial networks (SRGANs) and a multi-scale learning component from the multiple scale super-resolution network (MSSRNet), which only together can recover smaller structures inherent in satellite images. To further enhance our performance, we integrate progressive growing and training to our network. This, aided by feed forwarding connections in the network to move along and enrich information from previous inputs, produces super-resolved images at scaling factors of 2, 4, and 8. To ensure and enhance the stability of GANs, we employ Wasserstein GANs (WGANs) during training. Experimentally, we find that our architecture can recover small objects in satellite images during super-resolution whereas previous methods cannot. © 2019 IEEE.","Audio signal processing; Network architecture; Optical resolving power; Remote sensing; Small satellites; Speech communication; Adversarial networks; GANs; Learning architectures; Multiple scale; Remote sensing data; Satellite images; Scaling factors; Super resolution; Deep learning","dilated convolutions; GANs; remote sensing data; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85069000640"
"Seo M.; Kitajima T.; Chen Y.-W.","Seo, Masataka (35280835900); Kitajima, Toshihiro (56810615800); Chen, Yen-Wei (56036268200)","35280835900; 56810615800; 56036268200","High-resolution gaze-corrected image generation based on combined conditional GAN and residual dense network","2020","Digest of Technical Papers - IEEE International Conference on Consumer Electronics","2020-January","","9043159","","","","10.1109/ICCE46568.2020.9043159","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082600490&doi=10.1109%2fICCE46568.2020.9043159&partnerID=40&md5=ce413e9129d71f026c6ebc176b0f0408","In recent years, the ease of taking selfie photos has improved due to widespread use of smartphones and social networking services (SNSs). In a typical smartphone, the camera layout and display are different, rendering the gaze often not front-facing. Although various gaze-correction methods have been proposed for this problem, many of them cannot generate sufficiently natural images. Furthermore, no improvement can typically be seen in the unnatural images generated with these methods. In this paper, we propose a gaze-correction method using a deep generative model. This model can determine the naturalness of the resulting images and learn to provide natural results. In addition, recent smartphones have the ability to take large-sized photos. However, it is difficult for this model to generate large-sized images. In this paper, we use super-resolution techniques to generate images that have a larger size and higher resolution than those generated by conventional methods. In our experiments, to reduce the size of our neural network, we input a small-sized image and convert it back to its original size after performing gaze-correction. © 2020 IEEE.","Smartphones; Adversarial networks; Conventional methods; Correction method; Generative model; Higher resolution; Residual dense block; Social networking services; Super resolution; Image enhancement","Deep generative model; Gaze correction; Generative adversarial network; Residual dense block","Conference paper","Final","","Scopus","2-s2.0-85082600490"
"Shao W.-Z.; Xu J.-J.; Chen L.; Ge Q.; Wang L.-Q.; Bao B.-K.; Li H.-B.","Shao, Wen-Ze (55959121000); Xu, Jing-Jing (57208769839); Chen, Long (57208005338); Ge, Qi (55226556800); Wang, Li-Qian (36895613500); Bao, Bing-Kun (24922856800); Li, Hai-Bo (56375326400)","55959121000; 57208769839; 57208005338; 55226556800; 36895613500; 24922856800; 56375326400","On potentials of regularized Wasserstein generative adversarial networks for realistic hallucination of tiny faces","2019","Neurocomputing","364","","","1","15","14","10.1016/j.neucom.2019.07.046","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071384887&doi=10.1016%2fj.neucom.2019.07.046&partnerID=40&md5=eb9d56a246e86af0fef165c2e33fc659","Super-resolution of facial images, a.k.a. face hallucination, has been intensively studied in the past decades due to the increasingly emerging analysis demands in video surveillance, e.g., face detection, verification, identification. However, the actual performance of most previous hallucination approaches will drop dramatically when a very low-res tiny face is provided, due to the challenging multimodality of the problem as well as lack of an informative prior as a strong semantic guidance. Inspired by the latest progress in deep unsupervised learning, this paper focuses on tiny faces of size 16 × 16 pixels, hallucinating them to their 8 × upsampling versions by exploring the potentials of Wasserstein generative adversarial networks (WGAN). Besides a pixel-wise L2 regularization term imposed to the generative model, it is found that our advocated autoencoding generator with both residual and skip connections is a critical component for WGAN representing the facial contour and semantic content to a reasonable precision. With the additional Lipschitz penalty and architectural considerations for the critic in WGAN, the proposed approach finally achieves state-of-the-art hallucination performance in terms of both visual perception and objective assessment. The cropped CelebA face dataset is primarily used to aid the tuning and analysis of the new method, termed as tfh-WGAN. Experimental results demonstrate that the proposed approach not only achieves realistic hallucination of tiny faces, but also adapts to pose, expression, illuminance and occluded variations to a great degree. © 2019 Elsevier B.V.","Deep learning; Optical resolving power; Pixels; Security systems; Semantics; Autoencoding; Face hallucination; ResNet; Super resolution; Wasserstein GAN; Article; artificial neural network; controlled study; data base; face; human; image analysis; learning algorithm; measurement accuracy; priority journal; semantics; tiny face hallucination; vision; visual hallucination; Wasserstein generative adversarial network; Face recognition","Autoencoding; Face hallucination; ResNet; Skip connections; Super-resolution; Wasserstein GAN","Article","Final","","Scopus","2-s2.0-85071384887"
"San-You Z.; De-Qiang C.; Dai-Hong J.; Qi-Qi K.; Lu M.","San-You, Zhang (57216210226); De-Qiang, Cheng (35217537600); Dai-Hong, Jiang (57216201386); Qi-Qi, Kou (57216204488); Lu, Ma (57216204056)","57216210226; 35217537600; 57216201386; 57216204488; 57216204056","Adaptive diagonal total-variation generative adversarial network for super-resolution imaging","2020","IEEE Access","8","","9040511","57517","57526","9","10.1109/ACCESS.2020.2981726","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082822292&doi=10.1109%2fACCESS.2020.2981726&partnerID=40&md5=6ca3b7b08818a84e5c551b98fe7f2d7c","To address problems that the loss function does not correlate well with perceptual vision in super-resolution methods based on the convolutional neural network(CNN), a novel model called the ADTV-SRGAN is designed based on the adaptive diagonal total-variation generative adversarial network. Combined with global perception and the local structure adaptive method, spatial loss based on the diagonal variation model is proposed to make the loss function can be adjusted according to the spatial features. Pixel loss and characteristic loss are in combination with the spatial loss for the fusing optimization of the total loss function such that high-frequency details of the images are maintained to improve their quality. The results of experiment show that the proposed method can obtain competitive results in objective evaluations. In subjective assessment, images reconstructed by it are clear, delicate, and natural, and it preserved edge- and texture-related details. © 2020 IEEE.","Image enhancement; Image reconstruction; Optical resolving power; Textures; Adversarial networks; High frequency HF; Loss functions; Objective evaluation; Subjective assessments; Super resolution imaging; Superresolution methods; Total variation; Convolutional neural networks","Generative adversarial network; Image reconstruction; Loss function; Super-resolution imaging; Total variation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082822292"
"Zhu X.; Zhang L.; Zhang L.; Liu X.; Shen Y.; Zhao S.","Zhu, Xining (57207256346); Zhang, Lin (54796088600); Zhang, Lijun (39863815100); Liu, Xiao (57195942480); Shen, Ying (56146148600); Zhao, Shengjie (56328424800)","57207256346; 54796088600; 39863815100; 57195942480; 56146148600; 56328424800","Generative Adversarial Network-based Image Super-Resolution with a Novel Quality Loss","2019","Proceedings - 2019 International Symposium on Intelligent Signal Processing and Communication Systems, ISPACS 2019","","","8986250","","","","10.1109/ISPACS48206.2019.8986250","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081091253&doi=10.1109%2fISPACS48206.2019.8986250&partnerID=40&md5=da87c972a7ba132b1cebbf878ec54afa","Single Image Super-Resolution (SISR) has been a very attractive research topic in recent years. Breakthroughs in SISR have been achieved due to deep learning and Generative Adversarial Networks (GANs). However, the generated image still suffers from undesired artifacts. In this paper, we propose a new method named GMGAN for SISR tasks. In this method, to generate images more in line with Human Vision System (HVS), we design a quality loss by integrating an IQA metric named Gradient Magnitude Similarity Deviation (GMSD). To our knowledge, it is the first time to truly integrate an IQA metric into SISR. Moreover, to overcome the instability of the original GAN, we use a variation of GANs named WGAN-GP. Experiments show that GMGAN with quality loss and WGAN-GP can generate visually appealing results and set a new state-of-art. © 2019 IEEE.","Deep learning; Image quality; Optical resolving power; Adversarial networks; Gradient magnitude; Human vision systems; Image quality assessment; Image super resolutions; Quality loss; Research topics; Single images; Quality management","Generative Adversarial Network; Image Quality Assessment; Single Image Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85081091253"
"Liu S.; Yang Y.; Li Q.; Feng H.; Xu Z.; Chen Y.; Liu L.","Liu, Shaowen (57215336831); Yang, Yifan (57215379537); Li, Qi (55584762200); Feng, Huajun (7401736382); Xu, Zhihai (8574374100); Chen, Yueting (55949361600); Liu, Lei (57216228887)","57215336831; 57215379537; 55584762200; 7401736382; 8574374100; 55949361600; 57216228887","Infrared image super resolution using GAN with infrared image prior","2019","2019 IEEE 4th International Conference on Signal and Image Processing, ICSIP 2019","","","8868566","1004","1009","5","10.1109/SIPROCESS.2019.8868566","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074436105&doi=10.1109%2fSIPROCESS.2019.8868566&partnerID=40&md5=0c97cf33ea67045f09f9f71b7a2c5bd6","Super-resolution reconstruction technology based on deep-learning is rarely used in the field of infrared image. This paper will apply the Generative Adversarial Network super-resolution approach to the infrared super-resolution task. The natural image gradient prior is introduced into the super-resolution algorithm, and the visible image of the corresponding scene and the field of view is innovatively used as the style map, and the corresponding shallow network perceptual loss and deep network perceptual loss are added to the super-resolution objective function. The reconstructed image is more abundant and more detailed in the subjective visual reconstruction of the image texture than the existing algorithm in the simulation experiment. © 2019 IEEE.","Deep learning; Image reconstruction; Infrared imaging; Optical resolving power; Textures; Adversarial networks; Component; Image super resolutions; Natural images; Super resolution; Super resolution algorithms; Super resolution reconstruction; Visual reconstruction; Image texture","Component; Generative Adversarial Network; Infrared super-resolution; Natural image prior","Conference paper","Final","","Scopus","2-s2.0-85074436105"
"Chen X.; Shen H.; Bian Q.; Wang Z.; Tian X.","Chen, Xiaofan (57210846207); Shen, Haijie (57210840812); Bian, Qian (57208691694); Wang, Zhenduo (57210842585); Tian, Xinzhi (57208682932)","57210846207; 57210840812; 57208691694; 57210842585; 57208682932","Face image super-resolution with an attention mechanism; [结合注意力机制的人脸超分辨率重建]","2019","Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University","46","3","","148","153","5","10.19665/j.issn1001-2400.2019.03.022","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071667702&doi=10.19665%2fj.issn1001-2400.2019.03.022&partnerID=40&md5=2f706ebee14d06d88263c5b833179baf","Because of the limitation of the imaging equipment, the face images captured by it usually have the problem of low resolution and low quality. This paper proposes a method based on the generative adversarial network and attention mechanism for the multi-scale super-resolution of face images. In this paper, the deep residual network and the deep convolutional neural network (VGG-net) are used as the generator and the discriminator, respectively. The attention modules are combined with the residual blocks in the deep residual network to reconstruct face images which are highly similar to the high-resolution images and difficult for the discriminator to distinguish. Experimental results demonstrate the effectiveness of the proposed method in multi-scale face image super-resolution and the important role of the attention mechanism in image detail reconstruction. © 2019, The Editorial Board of Journal of Xidian University. All right reserved.","Convolution; Discriminators; Image reconstruction; Neural networks; Optical resolving power; Adversarial networks; Attention mechanisms; Convolutional neural network; High resolution image; Imaging equipment; Low qualities; Low resolution; Super resolution; Deep neural networks","Attention mechanism; Deep convolutional neural network; Deep residual network; Generative adversarial network; Super-resolution","Article","Final","","Scopus","2-s2.0-85071667702"
"Gu F.; Zhang H.; Wang C.; Wu F.","Gu, Feng (57205776909); Zhang, Hong (56179236500); Wang, Chao (55141316100); Wu, Fan (57104120200)","57205776909; 56179236500; 55141316100; 57104120200","SAR Image Super-Resolution Based on Noise-Free Generative Adversarial Network","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8899202","2575","2578","3","10.1109/IGARSS.2019.8899202","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077718212&doi=10.1109%2fIGARSS.2019.8899202&partnerID=40&md5=307d0f20e5820821cb10bd710579743c","Deep learning has been successfully applied to the ordinary image super-resolution (SR). However, since the synthetic aperture radar (SAR) images are often disturbed by multiplicative noise known as speckle and more blurry than ordinary images, there are few deep learning methods for the SAR image SR. In this paper, a deep generative adversarial network (DGAN) is proposed to reconstruct the pseudo high-resolution (HR) SAR images. First, a generator network is constructed to remove the noise of low-resolution SAR image and generate HR SAR image. Second, a discriminator network is used to differentiate between the pseudo super-resolution images and the realistic HR images. The adversarial objective function is introduced to make the pseudo HR SAR images closer to real SAR images. The experimental results show that our method can maintain the SAR image content with high-level noise suppression. The performance evaluation based on peak signal-to-noise-ratio and structural similarity index shows the superiority of the proposed method to the conventional CNN baselines. © 2019 IEEE.","Deep learning; Geology; Learning systems; Optical resolving power; Remote sensing; Signal to noise ratio; Synthetic aperture radar; Adversarial networks; Image super resolutions; Multiplicative noise; Objective functions; Peak signal to noise ratio; Structural similarity indices; Super resolution; Synthetic aperture radar (SAR) images; Radar imaging","generative adversarial network; super-resolution; Synthetic aperture radar","Conference paper","Final","","Scopus","2-s2.0-85077718212"
"Ha Y.; Tian J.; Miao Q.; Yang Q.; Guo J.; Jiang R.","Ha, Yan (15622806900); Tian, Junfeng (9735968600); Miao, Qiaowei (57216255595); Yang, Qi (57216252349); Guo, Jiaao (57216250613); Jiang, Ruohui (57216252600)","15622806900; 9735968600; 57216255595; 57216252349; 57216250613; 57216252600","Part-Based Enhanced Super Resolution Network for Low-Resolution Person Re-Identification","2020","IEEE Access","8","","8984370","57594","57605","11","10.1109/ACCESS.2020.2971612","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082952400&doi=10.1109%2fACCESS.2020.2971612&partnerID=40&md5=865a9089d53d0f592f55e7e84d830b06","Person re-identification (REID) is an important task in video surveillance and forensics applications. Many previous works often build models on the assumption that they have same resolution cross different camera views, while it is divorced from reality. To increase the adaptability of person REID models, this paper focuses on the low-resolution person REID task to relax the impractical assumption when traditional low-resolution person REID models are under pixel-to-pixel supervision in low and high resolution pedestrian image pairs. In addition, they are easily influenced by the global background, illumination or pose variations across camera views. Therefore, we propose a Part-based Enhanced Super Resolution (PESR) network by employing a part division strategy and an enhanced generative adversarial network to boost the unpaired pedestrian image super resolution process. Specifically, the part-based super resolution network transforms low resolution image in probe into high resolution without any pixel-to-pixel supervision and the part-based synthetic feature extractor module can learn discriminative pedestrian feature representation for the generated high resolution images, which employ a part feature connection loss as constraint to conduct matching for person re-identification. Furthermore, evaluations on four public person REID datasets demonstrate the advantages of our method over the state-of-the-art ones. © 2013 IEEE.","Cameras; Optical resolving power; Pixels; Security systems; Adversarial networks; Feature representation; High resolution image; Image super resolutions; Low resolution images; Part based; Person re identifications; Super resolution; Image enhancement","enhanced super resolution; Low-resolution person re-identification; part based; realistic discriminator","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082952400"
"Ko M.; Kim D.; Kim K.","Ko, Myeongseob (57206516754); Kim, Donghyun (57204820907); Kim, Kwangtaek (55315840300)","57206516754; 57204820907; 55315840300","Accurate depth estimation of skin surface using a light-field camera toward dynamic haptic palpation","2019","Skin Research and Technology","25","4","","469","481","12","10.1111/srt.12675","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059684218&doi=10.1111%2fsrt.12675&partnerID=40&md5=581946258eb6f42b15fd5efa55d59550","Background: Haptic skin palpation with three-dimensional skin surface reconstruction from in vivo skin images in order to acquire both tactile and visual information has been receiving much attention. However, the depth estimation of skin surface, using a light field camera that creates multiple images with a micro-lens array, is a difficult problem due to low-resolution images resulting in erroneous disparity matching. Methods: Multiple low-resolution images decoded from a light field camera have limitations to accurate 3D surface reconstruction needed for haptic palpation. To overcome this, a deep learning method, Generative Adversarial Networks, was employed to generate super-resolved skin images that preserve surface detail without blurring, and then, accurate skin depth was estimated by taking multiple subsequent steps including lens distortion correction, sub-pixel shifted image generation using phase shift theorem, cost-volume building, multi-label optimization, and hole filling and refinement, which is a new approach for 3D skin surface reconstruction. Results: Experimental results of the deep-learning-based super-resolution method demonstrated that the textural detail (wrinkles) of super-resolved skin images is well preserved, unlike other super-resolution methods. In addition, the depth maps computed with our proposed algorithm verify that our method can produce more accurate and robust results compared to other state-of-the-art depth map computation methods. Conclusion: Herein, we first proposed depth map estimation of skin surfaces using a light field camera and subsequently tested it with several skin images. The experimental results established the superiority of the proposed scheme. © 2019 John Wiley & Sons A/S. Published by John Wiley & Sons Ltd","Algorithms; Evaluation Studies as Topic; Humans; Image Processing, Computer-Assisted; Imaging, Three-Dimensional; Palpation; Skin; Statistics as Topic; Touch; Cameras; Computation theory; Deep learning; Dermatology; Historic preservation; Image coding; Image reconstruction; Microlenses; Optical resolving power; Three dimensional computer graphics; Depth Estimation; Disparity estimations; haptic palpation; Light fields; Skin imaging; article; controlled study; dermatology; learning; palpation; skin surface; wrinkle; algorithm; anatomy and histology; comparative study; devices; diagnostic imaging; evaluation study; human; image processing; palpation; physiology; procedures; skin; statistics; three-dimensional imaging; touch; Surface reconstruction","depth estimation; dermatology; disparity estimation; haptic palpation; light field camera; skin imaging","Article","Final","","Scopus","2-s2.0-85059684218"
"Liu J.; Chen F.; Wang X.; Liao H.","Liu, Jia (57196290743); Chen, Fang (56903313400); Wang, Xianyu (57211981517); Liao, Hongen (7201507586)","57196290743; 56903313400; 57211981517; 7201507586","An Edge Enhanced SRGAN for MRI Super Resolution in Slice-Selection Direction","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11846 LNCS","","","12","20","8","10.1007/978-3-030-33226-6_2","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075558034&doi=10.1007%2f978-3-030-33226-6_2&partnerID=40&md5=494397b9ded1403f2b1946f04aabff99","The low resolution MRI in slice-select direction will lead to information loss and artifacts in 2D multi-slices MRI, which is not conducive to the diagnosis and treatment of diseases. Therefore, we proposed an edge enhanced super-resolution generative adversarial networks (EE-SRGAN) for MRI super resolution in slice-select direction. Firstly, a two-stage super-resolution generator network (TSSR) for solving the problem that the down-sampling ratio of MRI resolution in single direction reached 12 times. In addition, in order to overcome the problem of image smoothness caused by high peak signal-to-noise ratio (PSNR) and improve the visual reality of reconstruction image, we construct a generative adversarial networks based on TSSR. Finally, in order to achieve more texture details, we proposed an edge enhanced loss function to optimize the generator network. From the experimental results, we find that our TSSR is better (increased 1.78 dB PSNR), EE-SRGAN provides more satisfactory visual effect and beneficial to segmentation task (increased 2.14% Dice index) than state-of-art super-resolution network. © Springer Nature Switzerland AG 2019.","Arts computing; Brain mapping; Computer aided analysis; Diagnosis; Image analysis; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Edge enhanced; Information loss; Loss functions; Peak signal to noise ratio; Reconstruction image; Super resolution; Visual realities; Image enhancement","Edge enhanced; MRI Slice-Selection; Two-Stage Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85075558034"
"Huang Y.; Lu Z.; Shao Z.; Ran M.; Zhou J.; Fang L.; Zhang Y.I.","Huang, Yongqiang (57208471294); Lu, Zexin (57208471749); Shao, Zhimin (57208469061); Ran, Maosong (57208473625); Zhou, Jiliu (21234416400); Fang, Leyuan (36739090900); Zhang, Y.I. (57208468563)","57208471294; 57208471749; 57208469061; 57208473625; 21234416400; 36739090900; 57208468563","Simultaneous denoising and super-resolution of optical coherence tomography images based on a generative adversarial network","2019","Optics Express","27","9","","12289","12307","18","10.1364/OE.27.012289","68","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064863790&doi=10.1364%2fOE.27.012289&partnerID=40&md5=36f689081f13345abb349592e2ffb717","Optical coherence tomography (OCT) has become a very promising diagnostic method in clinical practice, especially for ophthalmic diseases. However, speckle noise and low sampling rates have intensively reduced the quality of OCT images, which prevents the development of OCT-assisted diagnosis. Therefore, we propose a generative adversarial network-based approach (named SDSR-OCT) to simultaneously denoise and super-resolve OCT images. Moreover, we trained three different super-resolution models with different upscale factors (2 ×, 4 × and 8 ×) to adapt to the corresponding downsampling rates. We also quantitatively and qualitatively compared our proposed method with some well-known algorithms. The experimental results show that our approach can effectively suppress speckle noise and can super-resolve OCT images at different scales. © 2019 Optical Society of America under the terms of the OSA Open Access Publishing Agreement","Diagnosis; Optical resolving power; Speckle; Tomography; Adversarial networks; Clinical practices; Diagnostic methods; Downsampling; Sampling rates; Speckle noise; Super resolution; Super-resolution models; Optical tomography","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85064863790"
"Liu S.; Li X.","Liu, Shuangshuang (57209243296); Li, Xiaoling (57209890106)","57209243296; 57209890106","A novel image super-resolution reconstruction algorithm based on improved GANs and gradient penalty","2019","International Journal of Intelligent Computing and Cybernetics","12","3","","400","413","13","10.1108/IJICC-10-2018-0135","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071261218&doi=10.1108%2fIJICC-10-2018-0135&partnerID=40&md5=f964c99d11baf734311ba036ac633d56","Purpose: Conventional image super-resolution reconstruction by the conventional deep learning architectures suffers from the problems of hard training and gradient disappearing. In order to solve such problems, the purpose of this paper is to propose a novel image super-resolution algorithm based on improved generative adversarial networks (GANs) with Wasserstein distance and gradient penalty. Design/methodology/approach: The proposed algorithm first introduces the conventional GANs architecture, the Wasserstein distance and the gradient penalty for the task of image super-resolution reconstruction (SRWGANs-GP). In addition, a novel perceptual loss function is designed for the SRWGANs-GP to meet the task of image super-resolution reconstruction. The content loss is extracted from the deep model’s feature maps, and such features are introduced to calculate mean square error (MSE) for the loss calculation of generators. Findings: To validate the effectiveness and feasibility of the proposed algorithm, a lot of compared experiments are applied on three common data sets, i.e. Set5, Set14 and BSD100. Experimental results have shown that the proposed SRWGANs-GP architecture has a stable error gradient and iteratively convergence. Compared with the baseline deep models, the proposed GANs models have a significant improvement on performance and efficiency for image super-resolution reconstruction. The MSE calculated by the deep model’s feature maps gives more advantages for constructing contour and texture. Originality/value: Compared with the state-of-the-art algorithms, the proposed algorithm obtains a better performance on image super-resolution and better reconstruction results on contour and texture. © 2019, Emerald Publishing Limited.","Deep learning; Image enhancement; Iterative methods; Mean square error; Network architecture; Optical resolving power; Textures; Adversarial networks; Design/methodology/approach; Feature map; Image super resolutions; Image super-resolution reconstruction; Learning architectures; State-of-the-art algorithms; Wasserstein distance; Image reconstruction","Deep model’s feature maps; Generative adversarial networks; Gradient penalty; Image super-resolution; Wasserstein distance","Article","Final","","Scopus","2-s2.0-85071261218"
"Zhang W.; Liu Y.; Dong C.; Qiao Y.","Zhang, Wenlong (57200855630); Liu, Yihao (57206486742); Dong, Chao (56335662200); Qiao, Yu (36086392600)","57200855630; 57206486742; 56335662200; 36086392600","RankSRGAN: Generative adversarial networks with ranker for image super-resolution","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9008788","3096","3105","9","10.1109/ICCV.2019.00319","164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081671925&doi=10.1109%2fICCV.2019.00319&partnerID=40&md5=f0420f59dff83fa6378bd92d533f2ee5","Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single image super-resolution (SISR). To further improve the visual quality of super-resolved results, PIRM2018-SR Challenge employed perceptual metrics to assess the perceptual quality, such as PI, NIQE, and Ma. However, existing methods cannot directly optimize these indifferentiable perceptual metrics, which are shown to be highly correlated with human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (RankSRGAN) to optimize generator in the direction of perceptual metrics. Specifically, we first train a Ranker which can learn the behavior of perceptual metrics and then introduce a novel rank-content loss to optimize the perceptual quality. The most appealing part is that the proposed method can combine the strengths of different SR methods to generate better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches state-of-the-art performance in perceptual metrics. Project page: Https://wenlongzhang0724.github.io/Projects/RankSRGAN. © 2019 IEEE.","Optical resolving power; Adversarial networks; Highly-correlated; Image super resolutions; Perceptual metrics; Perceptual quality; State-of-the-art performance; Super resolution; Visual qualities; Computer vision","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081671925"
"Hu X.; Ma P.; Mai Z.; Peng S.; Yang Z.; Wang L.","Hu, Xiao (55614059800); Ma, Peirong (57208902557); Mai, Zhuohao (57208905429); Peng, Shaohu (25927484000); Yang, Zhao (55930090800); Wang, Li (57189384041)","55614059800; 57208902557; 57208905429; 25927484000; 55930090800; 57189384041","Face hallucination from low quality images using definition-scalable inference","2019","Pattern Recognition","94","","","110","121","11","10.1016/j.patcog.2019.05.027","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066067833&doi=10.1016%2fj.patcog.2019.05.027&partnerID=40&md5=d0ece98318160ecb41d471f88fbd59ac","To hallucinate super-resolution (super-res) face from a real low-quality face, a super-resolution technique based on definition-scalable inference (SRDSI) is proposed in this paper. In the proposed strategy, all high-res labeled faces are first decomposed into basic faces and enhanced faces to train a basic face and an enhanced face inferring model, and then two inferring models are used to hallucinate super-res basic face with low-definition and enhanced faces with high-frequency information from a single low-res face. Finally, the basic face is merged with its enhanced face into a super-res face with high-definition. In addition, this paper employs SIFT key-points to evaluate the similarity between the super-res face and its high-res labeled face. Experimental results show that SRDSI can effectively recover more structural information as well as SIFT key-points from real low-res faces and achieves better performance than state-of-the-art super-resolution techniques in terms of both visual and objective quality. © 2019 Elsevier Ltd","Deep learning; Adversarial networks; Face hallucination; High-frequency informations; Objective qualities; SIFT; Sparse representation; Structural information; Super resolution; Optical resolving power","Deep learning; Generative adversarial networks; PCA; SIFT; Sparse representation","Article","Final","","Scopus","2-s2.0-85066067833"
"Hamis S.; Zaharia T.; Rousseau O.","Hamis, Sébastien (57211998698); Zaharia, Titus (6601999900); Rousseau, Olivier (57211999286)","57211998698; 6601999900; 57211999286","Image Compression at Very Low Bitrate Based on Deep Learned Super-Resolution","2019","2019 IEEE 23rd International Symposium on Consumer Technologies, ISCT 2019","","","8901038","128","133","5","10.1109/ISCE.2019.8901038","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075631563&doi=10.1109%2fISCE.2019.8901038&partnerID=40&md5=f6bd8d6dba14886bcbfcb0a949dd249e","The problem of data storage and transmission on mobile devices is constantly growing up. Smartphones are nearly by default equipped with HD cameras that are taking high quality pictures, which can be instantly stored and easily uploaded on cloud platforms. Such a behavior favors the creation of a massive amount of data. In order to reduce the size of such data, it is mandatory to dispose of efficient compression techniques that can take into account the actual usage of such image data. For example, most of the pictures acquired by phone cameras are often displayed on a small screen and this, for a little amount of time. A solution to manage this kind of oversized data would be to store them in a lower resolution, additionally to a standard image compression. The downside of such an approach is that restoring an image to its original resolution is a challenging task, notably in the presence of complex compression artifacts, such as those introduced by sophisticated compression methods. In order to deal with such an issue, in this paper we propose a new model, specifically trained to perform super-resolution after compression with the BPG state-of-the-art codec. An advantage of the proposed approach comes from the fact that the underlying process can be interpreted as a postprocessing step, which can be easily added to any compression scheme without modifying the codec. Experimental results show that our model perceptually outperforms the state-of-the-art compression standards even for very low bitrates. © 2019 IEEE.","Cameras; Digital storage; Digital television; Optical resolving power; Adversarial networks; BPG format; Compression artifacts; Compression methods; Compression standards; Compression techniques; Super resolution; Very low bitrate; Image compression","BPG format; generative adversarial networks; super-resolution; very low bitrate compression","Conference paper","Final","","Scopus","2-s2.0-85075631563"
"Li J.; Wu L.; Wang S.; Wu W.; Song F.; Zheng G.","Li, Junchao (57211998888); Wu, Liming (55533193200); Wang, Shiman (57211998444); Wu, Wenhao (57211998050); Song, Feiyang (57211999408); Zheng, Gengzhe (57211998713)","57211998888; 55533193200; 57211998444; 57211998050; 57211999408; 57211998713","Super resolution image reconstruction of textile based on SRGAN","2019","Proceedings - 2019 IEEE International Conference on Smart Internet of Things, SmartIoT 2019","","","8896586","436","439","3","10.1109/SmartIoT.2019.00078","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075634270&doi=10.1109%2fSmartIoT.2019.00078&partnerID=40&md5=11516940eb3bcc5066d8b8fef89383d0","For the problem of image distortion in textile flaw detection, a super-resolution image reconstruction technique based on GAN (Generative adversarial network) can reconstruct the obtained low-pixel image into a high-pixel image. The generative adversarial network consists of a discriminative network and a generative network. Generative network is responsible for generate high-resolution images, discriminative network is responsible for identifying the authenticity of the image. the generative loss and discriminative loss continuously optimize the network and guide the generation of high-quality images. The experimental results show that, the PNSR of SRGAN is 0.83 higher than that of the Bilinear, and the SSIM is higher than 0.0819. SRGAN can get a clearer image and reconstruct a richer texture, more high-frequency details, and easier to identify defects, which is important in the flaw detection of fabrics. © 2019 IEEE.","Internet of things; Optical resolving power; Pixels; Textiles; Textures; Adversarial networks; Discriminative networks; High frequency HF; High quality images; High resolution image; Image distortions; Super resolution; Super-resolution image reconstruction; Image reconstruction","Generative adversarial network; Super resolution; Textile","Conference paper","Final","","Scopus","2-s2.0-85075634270"
"Tsagkatakis G.; Aidini A.; Fotiadou K.; Giannopoulos M.; Pentari A.; Tsakalides P.","Tsagkatakis, Grigorios (34870845200); Aidini, Anastasia (57203761304); Fotiadou, Konstantina (25824915900); Giannopoulos, Michalis (57205378265); Pentari, Anastasia (56442653900); Tsakalides, Panagiotis (6701848334)","34870845200; 57203761304; 25824915900; 57205378265; 56442653900; 6701848334","Survey of deep-learning approaches for remote sensing observation enhancement","2019","Sensors (Switzerland)","19","18","3929","","","","10.3390/s19183929","82","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072555757&doi=10.3390%2fs19183929&partnerID=40&md5=857b0d03e174ccd53dc03eb73ca52949","Deep Learning, and Deep Neural Networks in particular, have established themselves as the new norm in signal and data processing, achieving state-of-the-art performance in image, audio, and natural language understanding. In remote sensing, a large body of research has been devoted to the application of deep learning for typical supervised learning tasks such as classification. Less yet equally important effort has also been allocated to addressing the challenges associated with the enhancement of low-quality observations from remote sensing platforms. Addressing such channels is of paramount importance, both in itself, since high-altitude imaging, environmental conditions, and imaging systems trade-offs lead to low-quality observation, as well as to facilitate subsequent analysis, such as classification and detection. In this paper, we provide a comprehensive review of deep-learning methods for the enhancement of remote sensing observations, focusing on critical tasks including single and multi-band super-resolution, denoising, restoration, pan-sharpening, and fusion, among others. In addition to the detailed analysis and comparison of recently presented approaches, different research avenues which could be explored in the future are also discussed. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Chemical detection; Data handling; Deep learning; Deep neural networks; Economic and social effects; Fusion reactions; Neural networks; Optical resolving power; Quality control; Adversarial networks; Convolutional neural network; De-noising; Earth observations; Pan-sharpening; Satellite imaging; Super resolution; Remote sensing","Convolutional neural networks; Deep learning; Denoising; Earth observations; Fusion; Generative adversarial networks; Pan-sharpening; Satellite imaging; Super-resolution","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85072555757"
"Zhang Y.; Ding M.; Bai Y.; Ghanem B.","Zhang, Yongqiang (57190277288); Ding, Mingli (8925275900); Bai, Yancheng (57206596265); Ghanem, Bernard (24331436200)","57190277288; 8925275900; 57206596265; 24331436200","Detecting small faces in the wild based on generative adversarial network and contextual information","2019","Pattern Recognition","94","","","74","86","12","10.1016/j.patcog.2019.05.023","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065927985&doi=10.1016%2fj.patcog.2019.05.023&partnerID=40&md5=42c9cc570bf4e6255575db3ae378fe6d","Face detection techniques have been developed for decades, and one of the remaining open challenges is detecting small faces in unconstrained conditions. The reason is that tiny faces are often lacking detailed information and blurry. In this paper, we proposed an algorithm to directly generate a clear high-resolution face from a small blurry one by adopting a generative adversarial network (GAN). Toward this end, the basic GAN formulation achieves it by super-resolving and refining sequentially (e.g. SR-GAN and Cycle-GAN). However, we design a novel network to address the problem of super-resolving and refining jointly. Moreover, we also introduce new training losses (i.e. classification loss and regression loss) to promote the generator network to recover fine details of the small faces and to guide the discriminator network to distinguish face vs. non-face and to refine location simultaneously. Additionally, considering the importance of contextual information when detecting tiny faces in crowded cases, the context around face regions is combined to train the proposed GAN-based network for mining those very small faces from unconstrained scenarios. Extensive experiments on the challenging datasets WIDER FACE and FDDB demonstrate the effectiveness of the proposed method in restoring a clear high-resolution face from a small blurry one, and show that the achieved performance outperforms previous state-of-the-art methods by a large margin. © 2019 Elsevier Ltd","Large dataset; Refining; Adversarial networks; Contextual information; Face detection technique; High resolution; Large margins; State-of-the-art methods; Super resolution; Tiny faces; Face recognition","Contextual information; Face detection; Generative adversarial network; Super-resolution; Tiny faces","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065927985"
"Su J.; Peng Y.; Yin H.","Su, Jingwen (57212414266); Peng, Yao (57191414220); Yin, Hujun (55155712700)","57212414266; 57191414220; 55155712700","Image quality constrained GAN for super-resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11871 LNCS","","","247","256","9","10.1007/978-3-030-33607-3_27","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076643145&doi=10.1007%2f978-3-030-33607-3_27&partnerID=40&md5=9e541afffa5272145b000c1b430305b5","As one of the most important research topics in image processing, super-resolution aims to estimate high resolution images from single or multiple low resolution images taken from the same scene. With the advent of deep learning techniques, generative adversarial networks are widely adopted for solving various image processing problems including super resolution. We investigate the effect of introducing image quality constraints into the training objective function of a generative adversarial network for super resolution. Experiment results demonstrate that network performance has great potential to be improved with such constraints. © 2019, Springer Nature Switzerland AG.","Deep learning; Image quality; Adversarial networks; High resolution image; Image processing problems; Learning techniques; Low resolution images; Objective functions; Quality constraints; Super resolution; Optical resolving power","Generative adversarial networks; Image quality constraints; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85076643145"
"Zhang Y.; Liu S.; Dong C.; Zhang X.; Yuan Y.","Zhang, Yongbing (7601315649); Liu, Siyuan (55578796600); Dong, Chao (56335662200); Zhang, Xinfeng (57211151919); Yuan, Yuan (57220641312)","7601315649; 55578796600; 56335662200; 57211151919; 57220641312","Multiple cycle-in-cycle generative adversarial networks for unsupervised image super-resolution","2020","IEEE Transactions on Image Processing","29","","8825849","1101","1112","11","10.1109/TIP.2019.2938347","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072209655&doi=10.1109%2fTIP.2019.2938347&partnerID=40&md5=f2a549468724273c41b4273944003b3a","With the help of convolutional neural networks (CNN), the single image super-resolution problem has been widely studied. Most of these CNN based methods focus on learning a model to map a low-resolution (LR) image to a high-resolution (HR) image, where the LR image is downsampled from the HR image with a known model. However, in a more general case when the process of the down-sampling is unknown and the LR input is degraded by noises and blurring, it is difficult to acquire the LR and HR image pairs for traditional supervised learning. Inspired by the recent unsupervised image-style translation applications using unpaired data, we propose a multiple Cycle-in-Cycle network structure to deal with the more general case using multiple generative adversarial networks (GAN) as the basis components. The first network cycle aims at mapping the noisy and blurry LR input to a noise-free LR space, then a new cycle with a well-trained × 2 network model is orderly introduced to super-resolve the intermediate output of the former cycle. The number of total cycles depends on the different up-sampling factors ( × 2 , × 4 , × 8 ). Finally, all modules are trained in an end-to-end manner to get the desired HR output. Quantitative indexes and qualitative results show that our proposed method achieves comparable performance with the state-of-the-art supervised models. © 1992-2012 IEEE.","Machine learning; Neural networks; Signal receivers; Signal sampling; Unsupervised learning; Adversarial networks; Convolutional neural network; High resolution image; Image super resolutions; Low resolution images; Network structures; State of the art; Super resolution; Optical resolving power","generative adversarial networks; Super resolution; unsupervised learning","Article","Final","","Scopus","2-s2.0-85072209655"
"Shamsolmoali P.; Zareapoor M.; Wang R.; Jain D.K.; Yang J.","Shamsolmoali, Pourya (56350053200); Zareapoor, Masoumeh (56349635100); Wang, Ruili (55825442900); Jain, Deepak Kumar (56206778300); Yang, Jie (15039078800)","56350053200; 56349635100; 55825442900; 56206778300; 15039078800","G-GANISR: Gradual generative adversarial network for image super resolution","2019","Neurocomputing","366","","","140","153","13","10.1016/j.neucom.2019.07.094","49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070239901&doi=10.1016%2fj.neucom.2019.07.094&partnerID=40&md5=adb92a02a17d8992fbc725a7537c4b6b","Adversarial methods have demonsterated to be signifiant at generating realistic images. However, these approaches have a challenging training process which partially attributed to the performance of discriminator. In this paper, we proposed an efficient super-resolution model based on generative adversarial network (GAN), to effectively generate reprehensive information and improve the test quality of the real-world images. To overcome the current issues, we designed the discriminator of our model based on the Least Square Loss function. The proposed network is organized by a gradual learning process from simple to advanced, which means from the small upsampling factors to the large upsampling factor that helps to improve the overall stability of the training. In particular, to control the model parameters and mitigate the training difficulties, dense residual learning strategy is adopted. Indeed, the key idea of proposed methodology is (i) fully exploit all the image details without losing information by gradually increases the task of discriminator, where the output of each layer is gradually improved in the next layer. In this way the model efficiently generates a super-resolution image even up to high scaling factors (e.g. × 8). (ii) The model is stable during the learning process, as we use least square loss instead of cross-entropy. In addition, the effects of different objective function on training stability are compared. To evaluate the model we conducted two sets of experiments, by using the proposed gradual GAN and the regular GAN to demonstrate the efficiency and stability of the proposed model for both quantitative and qualitative benchmarks. © 2019","Image enhancement; Optical resolving power; Signal sampling; Adversarial networks; Gradual learning; Image super resolutions; Learning strategy; Loss functions; Objective functions; Overall stabilities; Super-resolution models; article; entropy; learning; least square analysis; loss of function mutation; quantitative analysis; Learning systems","CNN; GAN; Gradual learning; Image super-resolution; Loss functions","Article","Final","","Scopus","2-s2.0-85070239901"
"Wei Z.; Bai H.; Zhao Y.","Wei, Zhensong (57202577273); Bai, Huihui (18036828600); Zhao, Yao (35304414700)","57202577273; 18036828600; 35304414700","Stage-GAN with semantic maps for large-scale image super-resolution","2019","KSII Transactions on Internet and Information Systems","13","8","","3942","3961","19","10.3837/tiis.2019.08.007","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075077880&doi=10.3837%2ftiis.2019.08.007&partnerID=40&md5=ae964ab333257448163854918e460fc2","Recently, the models of deep super-resolution networks can successfully learn the non-linear mapping from the low-resolution inputs to high-resolution outputs. However, for large scaling factors, this approach has difficulties in learning the relation of low-resolution to high-resolution images, which lead to the poor restoration. In this paper, we propose Stage Generative Adversarial Networks (Stage-GAN) with semantic maps for image super-resolution (SR) in large scaling factors. We decompose the task of image super-resolution into a novel semantic map based reconstruction and refinement process. In the initial stage, the semantic maps based on the given low-resolution images can be generated by Stage-0 GAN. In the next stage, the generated semantic maps from Stage-0 and corresponding low-resolution images can be used to yield high-resolution images by Stage-1 GAN. In order to remove the reconstruction artifacts and blurs for high-resolution images, Stage-2 GAN based post-processing module is proposed in the last stage, which can reconstruct high-resolution images with photo-realistic details. Extensive experiments and comparisons with other SR methods demonstrate that our proposed method can restore photo-realistic images with visual improvements. For scale factor ×8, our method performs favorably against other methods in terms of gradients similarity. © 2019 KSII.","Image reconstruction; Optical resolving power; Restoration; Semantics; Adversarial networks; High resolution image; High-resolution output; Image super resolutions; Reconstruction artifacts; Scaling factors; Semantic map; Super resolution; Image enhancement","Generative adversarial networks; Large scaling factors; Semantic maps; Stage-GAN; Super-resolution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85075077880"
"Qiao J.; Song H.; Zhang K.; Zhang X.; Liu Q.","Qiao, Jiaojiao (57210157638); Song, Huihui (36572623600); Zhang, Kaihua (55475000500); Zhang, Xiaolu (57208245743); Liu, Qingshan (36063739200)","57210157638; 36572623600; 55475000500; 57208245743; 36063739200","Image super-resolution using conditional generative adversarial network","2019","IET Image Processing","13","14","","2673","2679","6","10.1049/iet-ipr.2018.6570","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077458654&doi=10.1049%2fiet-ipr.2018.6570&partnerID=40&md5=c40b4b7a8317546ccdd205dc1e75368c","Recently, extensive studies on a generative adversarial network (GAN) have made great progress in single image super-resolution (SISR). However, there still exists a significant difference between the reconstructed high-frequency and the real high-frequency details. To address this issue, this study presents an SISR approach based on conditional GAN (SRCGAN). SRCGAN includes a generator network that generates super-resolution (SR) images and a discriminator network that is trained to distinguish the SR images from ground-truth high-resolution (HR) ones. Specifically, the discriminator network uses the ground-truth HR image as a conditional variable, which guides the network to distinguish the real images from the SR images, facilitating training a more stable generator model than GAN without this guidance. Furthermore, a residual-learning module is introduced into the generator network to solve the issue of detail information loss in SR images. Finally, the network is trained in an end-to-end manner by optimizing a perceptual loss function. Extensive evaluations on four benchmark datasets including Set5, Set14, BSD100, and Urban100 demonstrate the superiority of the proposed SRCGAN over state-of-the-art methods in terms of PSNR, SSIM, and visual effect. © The Institution of Engineering and Technology 2019.","Image resolution; Optical resolving power; Adversarial networks; Benchmark datasets; Generator modeling; High frequency HF; Image super resolutions; Learning modules; State-of-the-art methods; Super resolution; Discriminators","","Article","Final","","Scopus","2-s2.0-85077458654"
"Tan D.S.; Lin J.-M.; Lai Y.-C.; Ilao J.; Hua K.-L.","Tan, Daniel Stanley (57200534789); Lin, Jun-Ming (57208438372); Lai, Yu-Chi (32367681100); Ilao, Joel (55817587400); Hua, Kai-Lung (55223901500)","57200534789; 57208438372; 32367681100; 55817587400; 55223901500","Depth map upsampling via multi-modal generative adversarial network","2019","Sensors (Switzerland)","19","7","1587","","","","10.3390/s19071587","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064819646&doi=10.3390%2fs19071587&partnerID=40&md5=9ae406d24f75f07d5d8d89998466c0b9","Autonomous robots for smart homes and smart cities mostly require depth perception in order to interact with their environments. However, depth maps are usually captured in a lower resolution as compared to RGB color images due to the inherent limitations of the sensors. Naively increasing its resolution often leads to loss of sharpness and incorrect estimates, especially in the regions with depth discontinuities or depth boundaries. In this paper, we propose a novel Generative Adversarial Network (GAN)-based framework for depth map super-resolution that is able to preserve the smooth areas, as well as the sharp edges at the boundaries of the depth map. Our proposed model is trained on two different modalities, namely color images and depth maps. However, at test time, our model only requires the depth map in order to produce a higher resolution version. We evaluated our model both quantitatively and qualitatively, and our experiments show that our method performs better than existing state-of-the-art models. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Automation; Depth perception; Intelligent buildings; Adversarial networks; Depth discontinuities; Depth map up samplings; Depth upsampling; Encoder-decoder; Higher resolution; Inherent limitations; Super resolution; Signal sampling","Depth upsampling; Encoder-decoder networks; Generative adversarial networks","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85064819646"
"Liu P.; Hong Y.; Liu Y.","Liu, Peng (57750483500); Hong, Ying (37017047800); Liu, Yan (56178872300)","57750483500; 37017047800; 56178872300","Dual Discriminator Generative Adversarial Network for Single Image Super-Resolution","2019","Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS","2018-November","","8663715","680","687","7","10.1109/ICSESS.2018.8663715","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063604945&doi=10.1109%2fICSESS.2018.8663715&partnerID=40&md5=dbeac93b686c0d2629b7de33f07e938f","Recently, several algorithms have been proposed to achieve the single image super-resolution by using deep convolutional neural networks. In this study, we present a dual discrimination generative adversarial network (D2GAN) for single image super-resolution (SISR). The proposed model has better stability to complete the reconstruction of super-resolution images for ×4 scale factor. The improved residual network and perceptual loss function are applied in the proposed algorithm which demonstrates a superior performance over state-of-the-art restoration quality. Meanwhile, the proposed reconstruction network has a faster training and convergence speed compared with other super-resolution methods. The proposed approach is evaluated on standard datasets and gets improved performance than previous works that based on deep convolutional neural networks. © 2018 IEEE.","Convolution; Deep neural networks; Neural networks; Optical resolving power; Signal to noise ratio; Software engineering; Adversarial networks; component; Convolutional neural network; Peak signal to noise ratio; Reconstruction networks; Single images; Structural similarity indices; Superresolution methods; Discriminators","component; deep neural network; peak signal-to-noise ratio; residual networks; single image super-resolution; structural similarity index","Conference paper","Final","","Scopus","2-s2.0-85063604945"
"Xing C.; Liang X.; Bao Z.","Xing, Chen (36054157400); Liang, Xi (57214781111); Bao, Zhiyan (36022181100)","36054157400; 57214781111; 36022181100","A Small Object Detection Solution by Using Super- Resolution Recovery","2019","Proceedings of IEEE 7th International Conference on Computer Science and Network Technology, ICCSNT 2019","","","8962422","313","316","3","10.1109/ICCSNT47585.2019.8962422","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079095894&doi=10.1109%2fICCSNT47585.2019.8962422&partnerID=40&md5=a1f95422e81f52c0e9af0672f4f3e74e","Small object detection is the key challenge to aerial image object detection. Small objects in aerial images are usually too fuzzy to detect. This paper proposes a solution by using Generative Adversarial Network to recover small objects' low-resolution image to high-resolution to get better detection performance. Experiment result shows this solution achieves mAP 68.38% and missing rate 14.23%. © 2019 IEEE.","Aircraft detection; Antennas; Computer networks; Computer system recovery; Object recognition; Optical resolving power; Recovery; Unmanned aerial vehicles (UAV); Adversarial networks; Aerial images; Detection performance; High resolution; Low resolution images; Small object detection; Small objects; Super resolution; Object detection","aerial image; small object detection; super-resolution recovery; UAV","Conference paper","Final","","Scopus","2-s2.0-85079095894"
"Zheng C.; Jiang X.; Zhang Y.; Liu X.; Yuan B.; Li Z.","Zheng, Ce (57672921100); Jiang, Xue (55724182500); Zhang, Ye (57214253643); Liu, Xingzhao (57770107000); Yuan, Bin (36109487100); Li, Zhixin (57241049300)","57672921100; 55724182500; 57214253643; 57770107000; 36109487100; 57241049300","Self-normalizing generative adversarial network for super-resolution reconstruction of SAR images","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","2019-July","","8900084","1911","1914","3","10.1109/IGARSS.2019.8900084","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104026398&doi=10.1109%2fIGARSS.2019.8900084&partnerID=40&md5=d8052b974df78aea9e33f718d307e7e5","High-resolution images with abundant detailed information are necessary elements for various applications of synthetic aperture radar (SAR). In this paper, a novel super-resolution image reconstruction method based on self-normalizing generative adversarial network (SNGAN) is proposed. Compared with other published GAN-based super-resolution algorithms, the proposed method reflects its superiority in two aspects. First, the scaled exponential linear units (SeLU) is introduced as the activation function of generator to give the GAN system self-normalization ability and make it more suitable for SAR images. Second, the batch normalization layers after convolution are canceled to reduce the computational requirement and model oscillation. Experiment results on the images of TerraSAR and MSTAR dataset demonstrate that the proposed method acquires satisfactory performance on the resolution enhancement and target recognition of SAR images. ©2019 IEEE","Image enhancement; Image reconstruction; Optical resolving power; Radar target recognition; Remote sensing; Synthetic aperture radar; Activation functions; Adversarial networks; Computational requirements; High resolution image; Resolution enhancement; Super resolution algorithms; Super resolution reconstruction; Super-resolution image reconstruction; Radar imaging","Generative adversarial network (GAN); Super-resolution image reconstruction; Target recognition","Conference paper","Final","","Scopus","2-s2.0-85104026398"
"Li X.; Chebiyyam V.; Kirchhoff K.","Li, Xinyu (57189333287); Chebiyyam, Venkata (56940873400); Kirchhoff, Katrin (7006780729)","57189333287; 56940873400; 7006780729","Speech audio super-resolution for speech recognition","2019","Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH","2019-September","","","3416","3420","4","10.21437/Interspeech.2019-3043","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074701898&doi=10.21437%2fInterspeech.2019-3043&partnerID=40&md5=d51cbe78213b96b7ced2a7a4eda761b6","Automatic bandwidth extension (restoring high-frequency information from low sample rate audio) has a number of applications in speech processing. We introduce an end-to-end deep learning based system for speech bandwidth extension for use in a downstream automatic speech recognition (ASR) system. Specifically we propose a conditional generative adversarial network enriched with ASR-specific loss functions designed to upsample the speech audio while maintaining good ASR performance. Evaluations on the speech commands dataset and the LibriSpeech corpus show that our approach outperforms a number of traditional bandwidth extension methods with respect to word error rate. Copyright © 2019 ISCA","","Bandwidth extension; Generative adversarial network; Speech recognition; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85074701898"
"Prokopenko D.; Stadelmann J.V.; Schulz H.; Renisch S.; Dylov D.V.","Prokopenko, Denis (57212005731); Stadelmann, Joël Valentin (57212754317); Schulz, Heinrich (7403266987); Renisch, Steffen (6506665689); Dylov, Dmitry V. (23977461000)","57212005731; 57212754317; 7403266987; 6506665689; 23977461000","Unpaired synthetic image generation in radiology using GANs","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11850 LNCS","","","94","101","7","10.1007/978-3-030-32486-5_12","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075660783&doi=10.1007%2f978-3-030-32486-5_12&partnerID=40&md5=46ed6c55af36990931b9dbc03969c54a","In this work, we investigate approaches to generating synthetic Computed Tomography (CT) images from the real Magnetic Resonance Imaging (MRI) data. Generating the radiological scans has grown in popularity in the recent years due to its promise to enable single-modality radiotherapy planning in clinical oncology, where the co-registration of the radiological modalities is cumbersome. We rely on the Generative Adversarial Network (GAN) models with cycle consistency which permit unpaired image-to-image translation between the modalities. We also introduce the perceptual loss function term and the coordinate convolutional layer to further enhance the quality of translated images. The Unsharp masking and the Super-Resolution GAN (SRGAN) were considered to improve the quality of synthetic images. The proposed architectures were trained on the unpaired MRI-CT data and then evaluated on the paired brain dataset. The resulting CT scans were generated with the mean absolute error (MAE), the peak signal-to-noise ratio (PSNR) and the structural similarity (SSIM) scores of 60.83 HU, 17.21 dB, and 0.8, respectively. DualGAN with perceptual loss function term and coordinate convolutional layer proved to perform best. The MRI-CT translation approach holds potential to eliminate the need for the patients to undergo both examinations and to be clinically accepted as a new tool for radiotherapy planning. © Springer Nature Switzerland AG 2019.","Artificial intelligence; Convolution; Deep learning; Image enhancement; Magnetic resonance imaging; Medical imaging; Radiotherapy; Signal to noise ratio; Adversarial networks; Image translation; Mean absolute error; Peak signal to noise ratio; Proposed architectures; Radiotherapy planning; Structural similarity; Synthetic image generation; Computerized tomography","Deep learning; Image translation; Radiotherapy","Conference paper","Final","","Scopus","2-s2.0-85075660783"
"Li B.; Huang C.; Li X.; Zheng S.; Hong J.","Li, Baotong (57210474970); Huang, Congjia (57208185516); Li, Xin (57201432287); Zheng, Shuai (55971859300); Hong, Jun (7404118243)","57210474970; 57208185516; 57201432287; 55971859300; 7404118243","Non-iterative structural topology optimization using deep learning","2019","CAD Computer Aided Design","115","","","172","180","8","10.1016/j.cad.2019.05.038","45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066935460&doi=10.1016%2fj.cad.2019.05.038&partnerID=40&md5=9b207b262512edcfa5dffde9344bc3ab","This paper presents a non-iterative topology optimizer for conductive heat transfer structures with the help of deep learning. An artificial neural network is trained to deal with the black-and-white pixel images and generate near-optimal structures. Our design is a two-stage hierarchical prediction–refinement pipeline consisting of two coupled neural networks: a generative adversarial network (GAN) for predicting a low resolution near-optimal structure and a super-resolution generative adversarial network (SRGAN) for predicting the refined structure in high resolution. Training datasets with given boundary conditions and the optimized pixel image structures are obtained after simulating a big amount of topology optimization procedures. For more effective training and inference, these datasets are generated with two different resolutions. Experiments demonstrated that our learning based optimizer can provide accurate estimation of the conductive heat transfer topology using negligible computational time. This effective incorporation of deep learning into topology optimization could enable promising applications in large-scale engineering structure design. © 2019 Elsevier Ltd","Computer aided engineering; Forecasting; Heat conduction; Neural networks; Pixels; Shape optimization; Structural optimization; Topology; Adversarial networks; Black and white pixels; Conductive heat transfer; Coupled neural networks; Hierarchical refinement; Large scale engineering structures; Optimization procedures; Structural topology optimization; Deep learning","Deep learning; Generative adversarial network; Heat conduction; Hierarchical refinement; Topology optimization","Article","Final","","Scopus","2-s2.0-85066935460"
"Chen B.-X.; Liu T.-J.; Liu K.-H.; Liu H.-H.; Pei S.-C.","Chen, Bo-Xun (57212479862); Liu, Tsung-Jung (36625954800); Liu, Kuan-Hsien (36626017900); Liu, Hsin-Hua (36015616400); Pei, Soo-Chang (7202463605)","57212479862; 36625954800; 36626017900; 36015616400; 7202463605","Image Super-Resolution Using Complex Dense Block on Generative Adversarial Networks","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8803711","2866","2870","4","10.1109/ICIP.2019.8803711","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076798259&doi=10.1109%2fICIP.2019.8803711&partnerID=40&md5=f6cd403f107c08d1304edb94aead463b","The recent super-resolution (SR) techniques are divided into two directions. One is to improve PSNR and the other is to improve visual quality. We believe improving visual quality is more important and practical than blindly improving PSNR. In this paper we employ a generative adversarial network (GAN) and a new perceptual loss function for photo-realistic single image super-resolution (SISR). Our main contributions are as follows: we propose a new dense block which uses complex connections between each layer to build a more powerful generator. Next, to improve the perceptual quality, we found a new set of feature maps to compute the perceptual loss, which would make the output image look more real and natural. Finally, we compare our results with other methods by subjective evaluation. The subjects rank the image generated by various methods from good to bad. The final results show that our method can generate a more natural and realistic SR image than other state-of-the-art methods. © 2019 IEEE.","","dense block; generative adversarial network (GAN); Super-resolution (SR); visual quality.","Conference paper","Final","","Scopus","2-s2.0-85076798259"
"Qiu H.; Wang C.; Zhu H.; Zhu X.; Gu J.; Han X.","Qiu, H. (57211804273); Wang, C. (57214862646); Zhu, H. (57218146385); Zhu, X. (57211986375); Gu, J. (57212042330); Han, X. (55451013500)","57211804273; 57214862646; 57218146385; 57211986375; 57212042330; 55451013500","Two-phase Hair Image Synthesis by Self-Enhancing Generative Model","2019","Computer Graphics Forum","38","7","","403","412","9","10.1111/cgf.13847","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075069513&doi=10.1111%2fcgf.13847&partnerID=40&md5=22b99d2a8cc687d4b4f4faa522c14590","Generating plausible hair image given limited guidance, such as sparse sketches or low-resolution image, has been made possible with the rise of Generative Adversarial Networks (GANs). Traditional image-to-image translation networks can generate recognizable results, but finer textures are usually lost and blur artifacts commonly exist. In this paper, we propose a two-phase generative model for high-quality hair image synthesis. The two-phase pipeline first generates a coarse image by an existing image translation model, then applies a re-generating network with self-enhancing capability to the coarse image. The self-enhancing capability is achieved by a proposed differentiable layer, which extracts the structural texture and orientation maps from a hair image. Extensive experiments on two tasks, Sketch2Hair and Hair Super-Resolution, demonstrate that our approach is able to synthesize plausible hair image with finer details, and reaches the state-of-the-art. © 2019 The Author(s) Computer Graphics Forum © 2019 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.","Image enhancement; Textures; Adversarial networks; Generative model; Image translation; Low resolution images; Orientation maps; State of the art; Structural textures; Super resolution; Image texture","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075069513"
"Guan M.; Song D.; Tao L.","Guan, Mingyang (57211169482); Song, Dandan (35235108100); Tao, Linmi (7201566836)","57211169482; 35235108100; 7201566836","Deep Feature Translation Network Guided by Combined Loss for Single Image Super-Resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11672 LNAI","","","664","677","13","10.1007/978-3-030-29894-4_53","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072867685&doi=10.1007%2f978-3-030-29894-4_53&partnerID=40&md5=4f204d759cb25107af665ef7b56cf4b1","Single image super-resolution (SISR) which aims to infer a high-resolution (HR) image from a single low-resolution (LR) image has wide applications such as surveillance and medical image processing. However, existing methods which aiming at minimizing the mean squared error (MSE) always get high objective quality, i.e., peak signal-to-noise ratios (PSNR), but their results are blurry which lacks high-frequency details thus are perceptually unsatisfying. Some recently proposed Generative Adversarial Networks enhance the perceptual quality greatly, but their objective quality is very low, which means their generated texture details are not faithful to the real image. In this paper, we adopt a multi-scale HR construction process to generate HR images gradually to achieve large upscaling factors. For each level, the generation of HR difference features from LR features is taken as a feature translation process, and deep image feature translation network (DFTN) is designed. To recover finer texture details, we combine three loss functions: content loss, a novel fine-grained texture loss and adversarial loss in our model optimization. We desire that the content loss ensures the LR results faithful to the original image, and the other two losses push our model to capture the manifold of natural images. Experiments confirm that our model can achieve the state-of-the-art results in different evaluating metrics, including both objective and perceptual quality evaluations. Therefore, our method can generate HR images with fine texture details and faithful to original images. © 2019, Springer Nature Switzerland AG.","Artificial intelligence; Image enhancement; Mean square error; Medical imaging; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Construction process; Fine grained; High resolution image; Image super resolutions; Low resolution images; Objective qualities; Peak signal to noise ratio; Image texture","Combined loss; Feature translation network; Fine-grained texture loss; Image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85072867685"
"Zhang J.; Shamsolmoali P.; Zhang P.; Feng D.; Yang J.","Zhang, Junhao (56368684700); Shamsolmoali, Pourya (56350053200); Zhang, Pengpeng (56104492700); Feng, Deying (35219755900); Yang, Jie (15039078800)","56368684700; 56350053200; 56104492700; 35219755900; 15039078800","Multispectral image fusion using super-resolution conditional generative adversarial networks","2019","Journal of Applied Remote Sensing","13","2","022002","","","","10.1117/1.JRS.13.022002","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055792733&doi=10.1117%2f1.JRS.13.022002&partnerID=40&md5=88db0212e3ac0f09a92f60ac8917e082","In multispectral image fusion scenarios, deep learning has been widely applied. However, the fusion performance and image quality are still restricted by inflexible architecture and supervised learning mode. We proposed multispectral image fusion using super-resolution conditional generative adversarial networks (MS-cGANs) based on conditional cGANs, which produces the fused image through the flexible encode-and-decode procedure. In the proposed network, a least square model is extended to solve the gradients vanishing problem in cGANs. Then, to improve the fusion quality, the multiscale features are used to preserve the details. Furthermore, the image resolution is promoted by adding the perceptual loss in object function and injecting the super-resolution structure into a deconvolution procedure. In experimental results, MS-cGANs demonstrates a significant performance in fusing multispectral images and top-ranking image quality compared with the state-of-the-art methods. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Fusion reactions; Image quality; Image resolution; Least squares approximations; Optical resolving power; Remote sensing; Adversarial networks; Fusion performance; Least square model; Multi-scale features; Multi-spectral image fusions; Multispectral images; State-of-the-art methods; Super resolution; Image fusion","fusion; multispectral image; multispectral-conditional generative adversarial network; remote sensing","Article","Final","","Scopus","2-s2.0-85055792733"
"Su X.; Xu H.; Kang Y.; Hao X.; Gao G.; Zhang Y.","Su, Xiangdong (36613483100); Xu, Huali (57211167226); Kang, Ying (57215085007); Hao, Xiang (57209889041); Gao, Guanglai (7403171213); Zhang, Yue (57211170697)","36613483100; 57211167226; 57215085007; 57209889041; 7403171213; 57211170697","Improving text image resolution using a deep generative adversarial network for optical character recognition","2019","Proceedings of the International Conference on Document Analysis and Recognition, ICDAR","","","8978148","1193","1199","6","10.1109/ICDAR.2019.00193","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079895668&doi=10.1109%2fICDAR.2019.00193&partnerID=40&md5=9b49ebcdec951db75eee897e463ee07d","Optical character recognition (OCR) has been widely studied in previous work. Except for the models used, the recognition accuracy depends most on the resolution of the image to be recognized. To enhance OCR performance, this paper proposes an approach based on a generative adversarial network to improve text image resolution. Our approach uses a perceptual loss function that consists of an adversarial loss, a content loss and an L1 loss. The adversarial loss and the L1 loss are used to ensure the generated super-resolved images are closer to the ground truth high-resolution images. Meanwhile, the content loss is used to ensure the generated super-resolved images and the input low-resolution images have similar features on the basis of perceptual instead of pixel similarity. To evaluate the proposed approach, we compare the recognition accuracies before and after improving the resolution of both English and Chinese text images. The results show that the recognition accuracies on the super-resolved text images obtained with our approach are significantly higher than those on the low-resolution images without processing. © 2019 IEEE.","Image resolution; Optical character recognition; Adversarial networks; High resolution image; Loss functions; Low resolution images; Optical character recognition (OCR); Pixel similarities; Recognition accuracy; Super resolution; Image enhancement","Generative adversarial network; Optical character recognition; Perceptual loss function; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85079895668"
"Du B.; Ren X.; Chen S.; Ren J.; Cao D.","Du, Bianli (57214231267); Ren, Xiaokang (57204434909); Chen, Saijian (57216161895); Ren, Jie (57199127710); Cao, Danling (57216156764)","57214231267; 57204434909; 57216161895; 57199127710; 57216156764","Image super-resolution and deblurring using generative adversarial network","2019","ACM International Conference Proceeding Series","","","","266","271","5","10.1145/3373509.3373547","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082672603&doi=10.1145%2f3373509.3373547&partnerID=40&md5=ead94751442d92de0f1f0327711fa62c","Image super-resolution and deblurring are two highly ill-posed problems that are usually dealt separately. However, real-world images are often low-resolution and have complex blurring. This paper focuses on ordinary natural scene images and reconstructs clear high-resolution images directly from blurred low-resolution inputs. Firstly, we propose a model based on generative adversarial network to jointly process image super-resolution and non-uniform motion deblurring. Secondly, we decouple this joint problem into feature extraction module, super-resolution reconstruction module and deblurring module. The modules promote each other and reconstruct clearer high-resolution images. Finally, we use bilinearinterpolation followed by a convolutional layer to achieve upsampling instead of using the common deconvolution layer, which effectively suppresses checkerboard artifacts. The experimental results show that the proposed method is efficient and can perform better than the existing advanced algorithms in both quantitative and qualitative performance. © 2019 Association for Computing Machinery.","Image reconstruction; Optical resolving power; Pattern recognition; Adversarial networks; Deblurring; High resolution image; Image super resolutions; Natural scene images; Residual learning; Super resolution; Super resolution reconstruction; Image enhancement","Deblurring; Generative adversarial network (GAN); Residual learning.; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85082672603"
"","","","6th International Conference on Information Management and Big Data, SIMBig 2019","2020","Communications in Computer and Information Science","1070 CCIS","","","","","339","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084862166&partnerID=40&md5=f68f119b1048f8893aa8366b7bfdbe7e","The proceedings contain 32 papers. The special focus in this conference is on Information Management and Big Data. The topics include: Sparse Non-negative Matrix Factorization for Retrieving Genomes Across Metagenomes; collect Ethically: Reduce Bias in Twitter Datasets; big Data Recommender System for Encouraging Purchases in New Places Taking into Account Demographics; privacy Preservation and Inference with Minimal Mobility Information; development of a Hand Gesture Based Control Interface Using Deep Learning; a Progressive Formalization of Tacit Knowledge to Improve Semantic Expressiveness of Biodiversity Data; peruvian Sign Language Recognition Using a Hybrid Deep Neural Network; chronic Pain Estimation Through Deep Facial Descriptors Analysis; detection of Non-small Cell Lung Cancer Adenocarcinoma Using Supervised Learning Algorithms Applied to Metabolomic Profiles; SCUT Sampling and Classification Algorithms to Identify Levels of Child Malnutrition; characterization of Salinity Impact on Synthetic Floc Strength via Nonlinear Component Analysis; spanish Sentiment Analysis Using Universal Language Model Fine-Tuning: A Detailed Case of Study; comparing Predictive Machine Learning Algorithms in Fit for Work Occupational Health Assessments; recognition of the Image of a Person, Based on Viola-Jones; a Place to Go: Locating Damaged Regions After Natural Disasters Through Mobile Phone Data; come with Me Now: New Potential Consumers Identification from Competitors; global Brand Perception Based on Social Prestige, Credibility and Social Responsibility: A Clustering Approach; using Embeddings to Predict Changes in Large Semantic Graphs; super Resolution Approach Using Generative Adversarial Network Models for Improving Satellite Image Resolution; computer-Assisted Learning for Chinese Based on Character Families; recurrence Plot Representation for Multivariate Time-Series Analysis.","","","Conference review","Final","","Scopus","2-s2.0-85084862166"
"Lazaridis G.; Lorenzi M.; Ourselin S.; Garway-Heath D.","Lazaridis, Georgios (57220383547); Lorenzi, Marco (26654433200); Ourselin, Sebastien (6602233595); Garway-Heath, David (57204866107)","57220383547; 26654433200; 6602233595; 57204866107","Enhancing OCT signal by fusion of GANs: Improving statistical power of glaucoma clinical trials","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11764 LNCS","","","3","11","8","10.1007/978-3-030-32239-7_1","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075639106&doi=10.1007%2f978-3-030-32239-7_1&partnerID=40&md5=d32caad0cc0ae60f0a57efe6db0fb424","Accurately monitoring the efficacy of disease-modifying drugs in glaucoma therapy is of critical importance. Albeit high resolution spectral-domain optical coherence tomography (SDOCT) is now in widespread clinical use, past landmark glaucoma clinical trials have used time-domain optical coherence tomography (TDOCT), which leads, however, to poor statistical power due to low signal-to-noise characteristics. Here, we propose a probabilistic ensemble model for improving the statistical power of imaging-based clinical trials. TDOCT are converted to synthesized SDOCT images and segmented via Bayesian fusion of an ensemble of generative adversarial networks (GANs). The proposed model integrates super resolution (SR) and multi-atlas segmentation (MAS) in a principled way. Experiments on the UK Glaucoma Treatment Study (UKGTS) show that the model successfully combines the strengths of both techniques (improved image quality of SR and effective label propagation of MAS), and produces a significantly better separation between treatment arms than conventional segmentation of TDOCT. © 2019, Springer Nature Switzerland AG.","Bayesian networks; Coherent light; Disease control; Image segmentation; Medical computing; Medical imaging; Ophthalmology; Optical tomography; Signal to noise ratio; Time domain analysis; Tomography; Adversarial networks; Glaucoma therapy; Label propagation; Probabilistic ensemble; Spectral-domain optical coherence tomography; Statistical power; Super resolution; Time domain optical coherence tomography; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075639106"
"Thawakar O.; Patil P.W.; Dudhane A.; Murala S.; Kulkarni U.","Thawakar, Omkar (57211194175); Patil, Prashant W. (57204924942); Dudhane, Akshay (57201668421); Murala, Subrahmanyam (26639647100); Kulkarni, Uday (36069371500)","57211194175; 57204924942; 57201668421; 26639647100; 36069371500","Image and video super resolution using recurrent generative adversarial network","2019","2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2019","","","8909900","","","","10.1109/AVSS.2019.8909900","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076360004&doi=10.1109%2fAVSS.2019.8909900&partnerID=40&md5=9e355f3f762a51906811e9fc2fbf777e","Recently, the convolutional neural network with residual learning models achieves high accuracy for single image super-resolution with different scale factors. With adversarial learning model, effective learning of transformation function for the low-resolution input image to a high-resolution target image can be achieved. In this paper, we propose a method for image and video super-resolution using the recurrent generative adversarial network named SR2GAN. In the proposed model (SR2GAN) we use recursive learning for video super-resolution to overcome the difficulty of learning transformation function for synthesizing realistic high-resolution images. This recursive approach helps to reduce the parameters with increasing depth of the model. An extensive evaluation is performed to examine the effectiveness of the proposed model, which shows that SR2GAN performs better in terms of peak signal to noise ratio (PSNR) and structural self-similarity index (SSIM) as compared to the state-of-the-art methods for super-resolution. For source code and supplementary material visit: https://github.com/OmkarThawakar/SR2GAN/. © 2019 IEEE.","Image segmentation; Neural networks; Optical resolving power; Security systems; Signal to noise ratio; Adversarial learning; Convolutional neural network; High resolution image; High resolution target images; Peak signal to noise ratio; State-of-the-art methods; Transformation functions; Video super-resolution; Learning systems","","Conference paper","Final","","Scopus","2-s2.0-85076360004"
"Zareapoor M.; Celebi M.E.; Yang J.","Zareapoor, Masoumeh (56349635100); Celebi, M. Emre (55667346700); Yang, Jie (15039078800)","56349635100; 55667346700; 15039078800","Diverse adversarial network for image super-resolution","2019","Signal Processing: Image Communication","74","","","191","200","9","10.1016/j.image.2019.02.008","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062656878&doi=10.1016%2fj.image.2019.02.008&partnerID=40&md5=b4cb2f02f77fe6d1d28eec92744a679e","Recently, there is a fast growth in Generative adversarial network and many works have appeared focusing not only images but also videos. Despite of remarkable success of GAN in image super resolution, it suffers from the major problem of poor perceptual quality. While employing a GAN for super resolution, it tends to generate over-smoothed images that lacks high frequency textures and do not look natural. We propose an intuitive generalization to Generative Adversarial Network and its conditional variations to address the problem of image super-resolution and improves the test quality of images. DGAN is a diverse GAN architecture incorporating multiple generators and a single discriminator. The main intuition is to employ multiple generators, instead of using a single one as in the original GAN. To enforce that multiple generators produce diverse samples, the discriminator trains a loss function to distinguish between real and fake samples by designed margins, and multiple generators alternately produce realistic samples by minimizing their losses. In fact, this paper addresses 2 main challenges; recovering realistic texture low resolution images and speed up the training process. We perform extensive experiments and compare the proposed model with other variants of GAN to demonstrate the efficiency and stability of the proposed model in both quantitative and qualitative benchmarks. © 2019 Elsevier B.V.","Deep learning; Image enhancement; Optical resolving power; Textures; Adversarial networks; Diverse GAN; High frequency HF; Image super resolutions; Low resolution images; Perceptual quality; Super resolution; Training process; Image texture","Adversarial network; Deep learning; Diverse GAN; Super-resolution","Article","Final","","Scopus","2-s2.0-85062656878"
"Bing X.; Zhang W.; Zheng L.; Zhang Y.","Bing, Xinyang (57205364795); Zhang, Wenwu (57214694116); Zheng, Liying (8339952700); Zhang, Yanbo (57216699498)","57205364795; 57214694116; 8339952700; 57216699498","Medical Image Super Resolution Using Improved Generative Adversarial Networks","2019","IEEE Access","7","","8854062","145030","145038","8","10.1109/ACCESS.2019.2944862","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078922885&doi=10.1109%2fACCESS.2019.2944862&partnerID=40&md5=5ad0239e4f9565735c9fd10bc206c066","Details of small anatomical landmarks and pathologies, such as small changes of the microvasculature and soft exudates, are critical to accurate disease analysis. However, actual medical images always suffer from limited spatial resolution, due to imaging equipment and imaging parameters (e.g. scanning time of CT images). Recently, machine learning, especially deep learning techniques, have brought revolution to image super resolution reconstruction. Motivated by these achievements, in this paper, we propose a novel super resolution method for medical images based on an improved generative adversarial networks. To obtain useful image details as much as possible while avoiding the fake information in high frequency, the original squeeze and excitation block is improved by strengthening important features while weakening non-important ones. Then, by embedding the improved squeeze and excitation block in a simplified EDSR model, we build a new image super resolution network. Finally, a new fusion loss that can further strengthen the constraints on low-level features is designed for training our model. The proposed image super resolution model has been validated on the public medical images, and the results show that visual effects of the reconstructed images by our method, especially in the case of high upscaling factors, outperform state-of-the-art deep learning-based methods such as SRGAN, EDSR, VDSR and D-DBPN. © 2019 IEEE.","Computerized tomography; Deep learning; Image reconstruction; Medical image processing; Medical imaging; Optical resolving power; Adversarial networks; Anatomical landmarks; Image super resolutions; Image super-resolution reconstruction; Learning-based methods; squeeze and excitation block; Super resolution; Superresolution methods; Image enhancement","Generative adversarial network; medical image reconstruction; squeeze and excitation block; super resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078922885"
"Ozcinar C.; Rana A.; Smolic A.","Ozcinar, Cagri (35847657400); Rana, Aakanksha (57189353104); Smolic, Aljosa (6602582385)","35847657400; 57189353104; 6602582385","Super-resolution of Omnidirectional Images Using Adversarial Learning","2019","IEEE 21st International Workshop on Multimedia Signal Processing, MMSP 2019","","","8901764","","","","10.1109/MMSP.2019.8901764","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075714295&doi=10.1109%2fMMSP.2019.8901764&partnerID=40&md5=086ff520a645d7d67022c9aee70b6edc","An omnidirectional image (ODI) enables viewers to look in every direction from a fixed point through a head-mounted display providing an immersive experience compared to that of a standard image. Designing immersive virtual reality systems with ODIs is challenging as they require high resolution content. In this paper, we study super-resolution for ODIs and propose an improved generative adversarial network based model which is optimized to handle the artifacts obtained in the spherical observational space. Specifically, we propose to use a fast PatchGAN discriminator, as it needs fewer parameters and improves the super-resolution at a fine scale. We also explore the generative models with adversarial learning by introducing a spherical-content specific loss function, called 360-SS. To train and test the performance of our proposed model we prepare a dataset of 4500 ODIs. Our results demonstrate the efficacy of the proposed method and identify new challenges in ODI super-resolution for future investigations. © 2019 IEEE.","Helmet mounted displays; Optical resolving power; Spheres; Statistical tests; Virtual reality; Adversarial learning; Adversarial networks; Generative model; Head mounted displays; Immersive virtual reality; Omnidirectional image; Specific loss function; Super resolution; Multimedia signal processing","generative adversarial network; omnidirectional image; spherical-content loss; super-resolution; virtual reality","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075714295"
"Nan J.; Bo L.","Nan, Jing (56516351300); Bo, Lei (55956811800)","56516351300; 55956811800","Image super-resolution reconstructing based on generative adversarial network","2019","Proceedings of SPIE - The International Society for Optical Engineering","11342","","113420F","","","","10.1117/12.2547435","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077823628&doi=10.1117%2f12.2547435&partnerID=40&md5=5fa365531378a43b815a86ca39443f66","Image super-resolution refers to the technique of reconstructing a high-resolution image by processing one or more complementary low-resolution images. It is widely used in medical imaging, video surveillance, remote sensing imaging and other fields. The learning-based super-resolution algorithm obtains a mapping relationship between the highresolution image and the low-resolution image by learning, and then guides the generation of the high-resolution image according to the obtained mapping relationship. The generative adversarial network (GAN) is composed of a generative network model and a discriminator network model, and the two play each other until the Nash equilibrium is reached, and the texture information and the high-frequency details of the downsampled image can be restored based on the super-resolution method of generative adversarial network. However, super-resolution algorithms based on generative adversarial network can only be used for one kind of magnification time, and the versatility is insufficient. Despite convolutional neural networks has achieved breakthroughs in accuracy and speed of traditional single-frame superresolution reconstruction, and can achieve a higher peak signal-to-noise ratio (PSNR). Most of them use Mean Square Error (MSE) as the minimum optimization objective function, so although a higher peak signal-to-noise ratio can be achieved, when the image downsampling factors is higher, the reconstructed image will be too smooth, lack highfrequency details and perceptually unsatisfy in the sense that they fail to match the fidelity expected at the higher resolution. When dealing with complex data of real scenes, the model's representation ability is not high; and the generative adversarial network training is very unstable, seriously affecting the model training process. This paper is based on generative adversarial network, improving the network structure and optimizing the training method to improve the quality of generating images. The following improvements have been made to the generator model: the multi-level structure is used to enlarge the image step by step, so that the model can simultaneously generate multiple images with a larger scale, and also ensure that the image obtained at a larger magnification has higher quality; ResNet model is improved by recursive learning and residual learning, and the batch normalization structure in the model is removed. On the basis of ensuring the image quality, the efficiency of the model is effectively improved. The recursive and residual learning methods can effectively improve the feature expression ability of the model, and thus significantly improve the quality of the generated image. The Expand-Squeeze method is proposed to generate images. The basic idea is to expand the dimension of the last layer of the convolution layer of the model. In this way, more context information is obtained, and then the image is generated by using the 1x1 convolution kernel. The Expand-Squeeze method can effectively reduce the checkerboard effect and improve the quality of the generated image to some extent. This paper improves the discriminator network loss function. Measure the similarity between generated image and real image by introducing Wasserstein distance. The loss function proposed consists of two parts: the loss function of resistance and the loss function of content. The experimental results verify that the improved generation of the generative adversarial network can effectively improve the quality of the generated image and effectively improve the stability of the model training. © 2019 SPIE.","Convolution; Discriminators; Image quality; Image reconstruction; Image segmentation; Learning systems; Mapping; Mean square error; Medical imaging; Neural networks; Optical resolving power; Photonics; Remote sensing; Security systems; Signal to noise ratio; Textures; Adversarial networks; Convolutional neural network; Image super resolutions; Learning-based super-resolution; Optimization objective function; Reconstructing; ResNet; Super-resolution reconstruction; Image enhancement","Generative adversarial network; Image super-resolution; Reconstructing; ResNet","Conference paper","Final","","Scopus","2-s2.0-85077823628"
"Khursheed M.O.; Saeed D.; Khan A.M.","Khursheed, Mohammad Omar (57216483988); Saeed, Danish (57216490639); Khan, Asad Mohammed (57213751075)","57216483988; 57216490639; 57213751075","Generative Adversarial Networks: A Survey of Techniques and Methods","2020","Lecture Notes on Data Engineering and Communications Technologies","31","","","490","498","8","10.1007/978-3-030-24643-3_58","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083640430&doi=10.1007%2f978-3-030-24643-3_58&partnerID=40&md5=04b75569c27ec34f44db802540d37077","Generative Adversarial Networks (GANs) are a class of deep learning algorithms which are based on two neural networks competing with each other. They are capable of learning representations from data that isn’t well annotated and these deep representations can be quite useful in a variety of applications, including image classification, generation, and in deblurring and creating super-resolution images. This paper is an attempt to explore a few different types of GANs, and to discuss issues and solutions in training methods. We also attempt to explore various applications of GANs and the methods we can use these learned deep representations. © Springer Nature Switzerland AG 2020.","Deep learning; Image enhancement; Adversarial networks; Deblurring; Super resolution; Training methods; Learning algorithms","Adversarial model; Generative model; Image generation; Machine learning; Neural network; Super resolution","Book chapter","Final","","Scopus","2-s2.0-85083640430"
"Deng Z.; He C.; Liu Y.; Kim K.C.","Deng, Zhiwen (57203388255); He, Chuangxin (25922136800); Liu, Yingzheng (26643397400); Kim, Kyung Chun (57207486764)","57203388255; 25922136800; 26643397400; 57207486764","Super-resolution reconstruction of turbulent velocity fields using a generative adversarial network-based artificial intelligence framework","2019","Physics of Fluids","31","12","125111","","","","10.1063/1.5127031","75","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077187922&doi=10.1063%2f1.5127031&partnerID=40&md5=1941d3e290e530991c803a7ad4bcd0e5","A general super-resolution reconstruction strategy was proposed for turbulent velocity fields using a generative adversarial network-based artificial intelligence framework. Two advanced neural networks, i.e., super-resolution generative adversarial network (SRGAN) and enhanced-SRGAN (ESRGAN), were first applied in fluid mechanics to augment the spatial resolution of turbulent flow. As a validation, the flow around a single-cylinder and a more complicated wake flow behind two side-by-side cylinders were experimentally measured using particle image velocimetry. The spatial resolution of the coarse flow field can be successfully augmented by 42 and 82 times with remarkable accuracy. The reconstruction performances of SRGAN and ESRGAN were comprehensively investigated and compared, including an analysis of the recovered instantaneous flow field, statistical flow quantities, and spatial correlations. The results convincingly demonstrated that both models can reconstruct the high-spatial-resolution flow field accurately even in an intricate flow configuration, and ESRGAN can provide a better reconstruction result than SRGAN in the mean and fluctuation flow field. © 2019 Author(s).","Artificial intelligence; Cylinders (shapes); Flow fields; Fluid mechanics; Image resolution; Turbulent flow; Velocity; Velocity measurement; Adversarial networks; Flow configurations; High spatial resolution; Particle image velocimetries; Side-by-side cylinders; Spatial correlations; Super resolution reconstruction; Turbulent velocity fields; Optical resolving power","","Article","Final","","Scopus","2-s2.0-85077187922"
"Li Q.; Luo Y.","Li, Qixin (57214915616); Luo, Yaneng (57191528235)","57214915616; 57191528235","Using GAN priors for ultrahigh resolution seismic inversion","2020","SEG International Exposition and Annual Meeting 2019","","","","2453","2457","4","10.1190/segam2019-3215520.1","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079489488&doi=10.1190%2fsegam2019-3215520.1&partnerID=40&md5=d84e64a4787d5235fca6b4b54ef96992","As one of machine learning techniques, deep learning has recently achieved the state-of-the-art performances in many areas, such as computer vision, natural language processing, to name a few. A generative model called Generative Adversarial Network (GAN) was invented in 2014. This deep network model is deemed as the most interesting idea in the last 10 years by the machine learning community and outperformed the traditional methods in many tasks like image synthesis and super-resolution. Laying the heart of the GAN is its ability to model any realistically sharp data distribution. Instead of providing a “blurry” sample, the high-resolution samples can be sampled from the GAN model, no matter it is a natural image or a well log. In this abstract, we propose a novel ultrahigh resolution seismic inversion method using GAN priors. The basic workflow is described below. Firstly, a simple GAN architecture was designed. Then, we train this GAN to model the well-log data distribution. Once the GAN is properly trained, it offers the high-resolution samples as priors to the inversion algorithm. To effectively use this prior information, we adopt the projected gradient descent algorithm to iteratively fit the seismic data and projects the “blurry” sample to the high resolution set of prior samples defined by the GAN. We further use a thin-layer model to validate the feasibility and superiority of our method. Comparing with the traditional method, our result shows a higher precision and resolution. © 2019 SEG","Deep learning; Geophysical prospecting; Gradient methods; Natural language processing systems; Seismology; Well logging; Adversarial networks; Inversion algorithm; Machine learning communities; Machine learning techniques; NAtural language processing; Projected gradient; State-of-the-art performance; Ultrahigh resolution; Learning systems","","Conference paper","Final","","Scopus","2-s2.0-85079489488"
"Zhu J.; Yang G.; Lio P.","Zhu, Jin (57202160439); Yang, Guang (57216243504); Lio, Pietro (7004223170)","57202160439; 57216243504; 7004223170","How can we make gan perform better in single medical image super-resolution? A lesion focused multi-scale approach","2019","Proceedings - International Symposium on Biomedical Imaging","2019-April","","8759517","1669","1673","4","10.1109/ISBI.2019.8759517","47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073913612&doi=10.1109%2fISBI.2019.8759517&partnerID=40&md5=2b0cfa37d743d0866a270b4c6ef11c58","Single image super-resolution (SISR) is of great importance as a low-level computer vision task. The fast development of Generative Adversarial Network (GAN) based deep learning architectures realises an efficient and effective SISR to boost the spatial resolution of natural images captured by digital cameras. However, the SISR for medical images is still a very challenging problem. This is due to (1) compared to natural images, in general, medical images have lower signal to noise ratios, (2) GAN based models pre-trained on natural images may synthesise unrealistic patterns in medical images which could affect the clinical interpretation and diagnosis, and (3) the vanilla GAN architecture may suffer from unstable training and collapse mode that can also affect the SISR results. In this paper, we propose a novel lesion focused SR (LFSR) method, which incorporates GAN to achieve perceptually realistic SISR results for brain tumour MRI images. More importantly, we test and make comparison using recently developed GAN variations, e.g., Wasserstein GAN (WGAN) and WGAN with Gradient Penalty (WGAN-GP), and propose a novel multi-scale GAN (MS-GAN), to achieve a more stabilised and efficient training and improved perceptual quality of the super-resolved results. Based on both quantitative evaluations and our designed mean opinion score, the proposed LFSR coupled with MS-GAN has performed better in terms of both perceptual quality and efficiency. © 2019 IEEE.","Deep learning; Diagnosis; Image processing; Magnetic resonance imaging; Network architecture; Optical resolving power; Quality control; Signal to noise ratio; Adversarial networks; Image super resolutions; Learning architectures; Lesion detection; Mean opinion scores; Multi-scale approaches; Quantitative evaluation; Super resolution; Medical imaging","Generative adversarial network; Image processing; Lesion detection; Medical image analysis; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85073913612"
"","","","NSENS 2019 - 2nd IEEE International Conference on Micro/Nano Sensors for Al, Healthcare, and Robotics","2019","NSENS 2019 - 2nd IEEE International Conference on Micro/Nano Sensors for Al, Healthcare, and Robotics","","","","","","145","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099204125&partnerID=40&md5=d32c611ac16eacba1c7a3e22ac1bdd86","The proceedings contain 24 papers. The topics discussed include: integrated navigation accuracy improvement algorithm based on multi-sensor fusion; quantify the effect of docetaxel with different concentration on cell mechanical properties using atomic force microscopy; design and implementation of position detection system based on 3-upu parallel mechanism; advanced generative adversarial network based on dense connection for single image super resolution; research on dynamic attitude estimation and control of tricycle based on MEMS sensing; super-resolution monitoring of react-on-demand photo-assisted electrochemical printing via microsphere nanoscopy; can biomass be measured in a fermentation process using ATR-FTIR spectroscopy bacillus subtilis as an example; and carrier dynamic attitude estimation algorithm based on MEMS inertial sensor.","","","Conference review","Final","","Scopus","2-s2.0-85099204125"
"Wang Y.; Su F.; Qian Y.","Wang, Yuyang (57204102989); Su, Feng (55725786500); Qian, Ye (57210578171)","57204102989; 55725786500; 57210578171","Text-attentional conditional generative adversarial network for super-resolution of text images","2019","Proceedings - IEEE International Conference on Multimedia and Expo","2019-July","","8784962","1024","1029","5","10.1109/ICME.2019.00180","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071005605&doi=10.1109%2fICME.2019.00180&partnerID=40&md5=ed2512b0dba3dc2e59e3e44efa7ad462","Text in natural scene images are often faced with low-resolution problem, which brings significant difficulties to many text-related tasks such as text detection and recognition. In this paper, we propose a novel text-attentional Conditional Generative Adversarial Network (cGAN) model for text image super-resolution (SR). The model enhances the original cGAN by introducing effective channel and spatial attention mechanisms based on the proposed Residual Dense Channel Attention Block and text/non-text segmentation information, which focus the model on the text regions instead of the background of the image to learn more effective representations of text and achieve better text super-resolution result. The proposed model achieves state-of-the-art performances on public text image super-resolution dataset. © 2019 IEEE.","Image enhancement; Image segmentation; Optical resolving power; Adversarial networks; Attention; CGAN; Natural scene images; Segmentation informations; State-of-the-art performance; Super resolution; Text images; Character recognition","Attention; CGAN; Segmentation; Super-resolution; Text image","Conference paper","Final","","Scopus","2-s2.0-85071005605"
"","","","33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019","2019","Advances in Neural Information Processing Systems","32","","","","","31211","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090161685&partnerID=40&md5=666c852f9b19f832e85bc2a62e1f5c9e","The proceedings contain 1427 papers. The topics discussed include: the point where reality meets fantasy: mixed adversarial generators for image splice detection; you only propagate once: accelerating adversarial training via maximal principle; imitation learning from observations by minimizing inverse dynamics disagreement; asymptotic guarantees for learning generative models with the sliced-Wasserstein distance; generalized sliced Wasserstein distances; first exit time analysis of stochastic gradient descent under heavy-tailed gradient noise; blind super-resolution kernel estimation using an internal-gan; noise-tolerant fair classification; generalization in generative adversarial networks: a novel perspective from privacy protection; joint-task self-supervised learning for temporal correspondence; learning nearest neighbor graphs from noisy distance samples; random tessellation forests; and differentiable convex optimization layers.","","","Conference review","Final","","Scopus","2-s2.0-85090161685"
"Liu P.; Li C.; Schönlieb C.-B.","Liu, Pan (57201445990); Li, Chao (57194697287); Schönlieb, Carola-Bibiane (24544878300)","57201445990; 57194697287; 24544878300","GANReDL: Medical Image Enhancement Using a Generative Adversarial Network with Real-Order Derivative Induced Loss Functions","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11766 LNCS","","","110","117","7","10.1007/978-3-030-32248-9_13","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075646560&doi=10.1007%2f978-3-030-32248-9_13&partnerID=40&md5=0552f0fcfccc85a3cb29dc3cd5cb72cb","Deep (convolutional) neural networks (DCNN) have recently gained popularity, and shown improved performance in the field of image enhancement (de-noising and super-resolution, for instance). However, the central issue of recovering finer texture details in images still remains unsolved. State-of-the-art objective functions used in DCNN mostly focus on minimizing the mean squared reconstruction error. The resulting image estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details, and are therefore error-prone with respect to fine-scale, possibly clinically relevant details. In this article, we present GANReDL, a generative adversarial network (GAN) for image enhancement equipped with a real-order derivative induced loss functions (ReDL) which we will show gives improved images, in particular wrt to the reconstruction of fine-scale details. To the best of our knowledge, this is the first framework that incorporates non-integer order derivatives in loss functions. To this aim, we propose a discriminator network that is trained to differentiate between the enhanced images and ground-truth images, and propose a new loss function motivated by real-order derivatives which is capable of also capturing global image features rather than pixel-wise features only. We show, with several numerical experiments, that GANReDL is better in reconstructing the high-frequency image details, and therefore show improved performance for image enhancement over other state-of-the-art methods. © 2019, Springer Nature Switzerland AG.","Convolutional neural networks; Image reconstruction; Medical computing; Medical imaging; Numerical methods; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Derivative operators; Image super resolutions; Numerical experiments; Objective functions; Peak signal to noise ratio; Reconstruction error; State-of-the-art methods; Image enhancement","Generative adversarial networks; Medical image super-resolution; Real order derivative operators","Conference paper","Final","","Scopus","2-s2.0-85075646560"
"Zhu X.; Cheng Y.; Peng J.; Wang R.; Le M.; Liu X.","Zhu, Xuan (55964217700); Cheng, Yue (57215543495); Peng, Jinye (9246837000); Wang, Rongzhi (57221058954); Le, Mingnan (56459627500); Liu, Xin (57769862600)","55964217700; 57215543495; 9246837000; 57221058954; 56459627500; 57769862600","Super-resolved image perceptual quality improvement via multifeature discriminators","2020","Journal of Electronic Imaging","29","1","013017","","","","10.1117/1.JEI.29.1.013017","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081115128&doi=10.1117%2f1.JEI.29.1.013017&partnerID=40&md5=b4b59187b52abd0689d6bec3576da1cc","Generative adversarial network (GAN) for image super-resolution (SR) has attracted enormous interests in recent years. However, the GAN-based SR methods only use image discriminator to distinguish SR images and high-resolution (HR) images. Image discriminator fails to discriminate images accurately since image features cannot be fully expressed. We design a GAN-based SR framework GAN-IMC, which includes generator, image discriminator, morphological component discriminator, and color discriminator. The combination of multiple feature discriminators improves the accuracy of image discrimination. Adversarial training between the generator and multifeature discriminators forces SR images to converge with HR images in terms of data and features distribution. Moreover, in some cases, feature enhancement of feature-rich region is also worth considering. GAN-IMC is further optimized by weighted content loss (GAN-IMCW), which effectively restores and enhances feature-rich regions in SR images. The effectiveness and robustness of the proposed method are confirmed by extensive experiments on public datasets. Compared with state-of-the-art methods, the proposed method not only achieves competitive perceptual index and natural image quality evaluator values but also obtains pleasant visual perception in edge, texture, color, and feature-rich regions. © 2020 SPIE and IS&T.","Discriminators; Image resolution; Optical resolving power; Textures; Adversarial networks; Feature enhancement; High resolution image; Image discrimination; Image super resolutions; Multi features; Perceptual quality; State-of-the-art methods; Image enhancement","image super-resolution; multifeature discriminators; perceptual quality; weighted content loss","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081115128"
"Mohsen M.; Moustafa M.","Mohsen, Mohamed (57211081601); Moustafa, Mohamed (7101946942)","57211081601; 7101946942","Generating large scale images using GANs","2019","Proceedings of SPIE - The International Society for Optical Engineering","11179","","111790N","","","","10.1117/12.2540489","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072623295&doi=10.1117%2f12.2540489&partnerID=40&md5=02c97dd45170536dcabd0cdffa768252","Generative Adversarial Networks (GANs) have been used for the task of image generation and has achieved impressive results. There is always a challenge to train networks that generate large scale images since they tend to be huge and training needs a lot of data. In this work, we tackle this problem by dividing it into two smaller parts. We first generate small scale images using GANs then use a super resolution network to enlarge the generated images resulting in large scale images. Using a super resolution network helps in adding more details to the image which results in a better-quality image. This technique has been tested with a small amount of data to generate 128x128 pixel images and obtained better inception scores over the baseline GAN. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Deep neural networks; Neural networks; Optical resolving power; Adversarial networks; Convolutional networks; Image generations; Pixel images; Quality image; Small scale; Super resolution; Training needs; Image processing","Convolutional networks; Deep learning; Generative adversarial networks; Image generation; Neural networks; Super-resolution networks","Conference paper","Final","","Scopus","2-s2.0-85072623295"
"Jiang Z.; Huang Y.; Hu L.","Jiang, Zetao (24512367900); Huang, Yongsong (57214838902); Hu, Lirui (55640423500)","24512367900; 57214838902; 55640423500","Single image super-resolution: Depthwise separable convolution super-resolution generative adversarial network","2020","Applied Sciences (Switzerland)","10","1","375","","","","10.3390/app10010375","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079279393&doi=10.3390%2fapp10010375&partnerID=40&md5=60d5e4ecb6c7f4217a5bd79c52556e73","The super-resolution generative adversarial network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied by unpleasant artifacts. To further enhance the visual quality, we propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The method is based on depthwise separable convolution super-resolution generative adversarial network (DSCSRGAN). A new depthwise separable convolution dense block (DSC Dense Block) was designed for the generator network, which improved the ability to represent and extract image features, while greatly reducing the total amount of parameters. For the discriminator network, the batch normalization (BN) layer was discarded, and the problem of artifacts was reduced. A frequency energy similarity loss function was designed to constrain the generator network to generate better super-resolution images. Experiments on several different datasets showed that the peak signal-to-noise ratio (PSNR) was improved by more than 3 dB, structural similarity index (SSIM) was increased by 16%, and the total parameter was reduced to 42.8% compared with the original model. Combining various objective indicators and subjective visual evaluation, the algorithm was shown to generate richer image details, clearer texture, and lower complexity. © 2020 The Author(s).","","Deep learning; Depthwise separable convolution; Generative adversarial networks; Image processing; Single image super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85079279393"
"Wang L.; Zheng M.; Du W.; Wei M.; Li L.","Wang, Longgang (57191964133); Zheng, Mana (57184783700); Du, Wenbo (57207780776); Wei, Menglin (57207773330); Li, Lianlin (8630029100)","57191964133; 57184783700; 57207780776; 57207773330; 8630029100","Super-resolution SAR Image Reconstruction via Generative Adversarial Network","2019","2018 12th International Symposium on Antennas, Propagation and EM Theory, ISAPE 2018 - Proceedings","","","8634345","","","","10.1109/ISAPE.2018.8634345","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062866878&doi=10.1109%2fISAPE.2018.8634345&partnerID=40&md5=4459a03aa53290820fbf75bda2983338","In this work, we presents a super-resolution (SR) reconstruction method for the synthetic aperture radar (SAR) images based on the generative adversarial network (GAN), SRGAN for short. In comparison with conventional SR algorithms developed in the area of image processing, the proposed SRGAN technique could make an important breakthrough in terms of reconstruction accuracy and computational efficiency for the SAR image SR. To achieve high-resolution, high fidelity and optics photo-like SAR images, SRGAN explores a perceptual loss function consisting of an adversarial loss and a content loss. Selected experimental results based on Terra-SAR datasets are provided to demonstrate the state-of-the-art performance of our proposed method. © 2018 IEEE.","Computation theory; Computational efficiency; Image reconstruction; Optical data processing; Optical resolving power; Synthetic aperture radar; Adversarial networks; High resolution; Loss functions; Reconstruction accuracy; State-of-the-art performance; Super resolution; Super resolution reconstruction; Synthetic aperture radar (SAR) images; Radar imaging","GAN; Perceptual loss; SAR; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85062866878"
"Jayanarayan A.; Sowmya V.; Soman K.P.","Jayanarayan, Abhijith (57215898504); Sowmya, V. (36096164300); Soman, K.P. (57205365723)","57215898504; 36096164300; 57205365723","Remote Sensing Image Super-Resolution Using Residual Dense Network","2020","Advances in Intelligent Systems and Computing","1118","","","721","729","8","10.1007/978-981-15-2475-2_66","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082294431&doi=10.1007%2f978-981-15-2475-2_66&partnerID=40&md5=b751e81e9d91e257762a3953bbf3cebe","Image super-resolution (SR) is a wide research topic, as it has found multiple applications in different fields. We implement image super-resolution for satellite images using a residual dense network (RDN). RDN is a CNN-based model, but unlike most CNN-based super-resolution models, it utilizes the hierarchic features from the input low resolution (LR) images and combines both the specific and general features present in the image, therefore resulting in a better performance. The novelty of our work lies in two aspects. First, we apply the residual dense network to remote sensing data to obtain higher structure similarity index metric (SSIM) and peak signal-to-noise ratio (PSNR) values than the existing models. Second, we use transfer learning due to the lack of training samples in remote sensing domain. Our RDN is first trained using an external dataset DIVerse2K (DIV2K). This model is then used to obtain high-resolution(HR) images of the remote sensing U.C Merced dataset, and the corresponding PSNR and SSIM values are computed for different scaling factors such as 2, 4 and 8. The experimental results obtained using the proposed work demonstrates the better performance of RDN for the super-resolution of remote sensing images, when compared to the existing methods like super-resolution generative adversarial network (SRGAN) and transferred generative adversarial network (TGAN). © 2020, Springer Nature Singapore Pte Ltd.","Deep learning; Optical resolving power; Signal processing; Signal to noise ratio; Soft computing; Transfer learning; High resolution image; Image; Image super resolutions; Multiple applications; Peak signal to noise ratio; Remote sensing images; Super resolution; Super-resolution models; Remote sensing","Deep learning; Image; Remote sensing; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85082294431"
"Indradi S.D.; Arifianto A.; Ramadhani K.N.","Indradi, Septian Dwi (57211269348); Arifianto, Anditya (56348676400); Ramadhani, Kurniawan Nur (57551995900)","57211269348; 56348676400; 57551995900","Face image super-resolution using inception residual network and GAN framework","2019","2019 7th International Conference on Information and Communication Technology, ICoICT 2019","","","8835253","","","","10.1109/ICoICT.2019.8835253","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073187426&doi=10.1109%2fICoICT.2019.8835253&partnerID=40&md5=e3c0fd1a440e26abe4621068c1372cb5","Single Image Super-Resolution (SISR) is an image reconstruction technique that aims to generate a high-resolution image from a low-resolution image. One of the SISR implementations is to reconstruct face images in order to gain more facial information from a low-resolution face images. In this paper, we propose a method to reconstruct face images using a Generative Adversarial Network (GAN) framework that able to generate plausible high-resolution images. Inside the GAN framework, we use inception residual network to improve the generated image quality and stabilize the training. Experimental results demonstrated that our proposed method was able to generate visually pleasant face images with the highest PSNR score of 26.615 and SSIM score of 0.8461. © 2019 IEEE.","Image reconstruction; Optical resolving power; Adversarial networks; Face images; High resolution image; Image reconstruction techniques; Low resolution images; Low-resolution face images; Single images; Image enhancement","Face image; Generative Adversarial Network; Image reconstruction; Single Image Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85073187426"
"He L.; Peng B.; Yang T.; Jiang J.","He, Lianhai (57216499693); Peng, Bo (57203991970); Yang, Tianlan (57216507370); Jiang, Jingfeng (10042758900)","57216499693; 57203991970; 57216507370; 10042758900","An Application of Super-Resolution Generative Adversary Networks for Quasi-Static Ultrasound Strain Elastography: A Feasibility Study","2020","IEEE Access","8","","9051729","65769","65779","10","10.1109/ACCESS.2020.2984733","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083741769&doi=10.1109%2fACCESS.2020.2984733&partnerID=40&md5=0079a7b76a838a5f72c410f847502d0c","In this work, a super-resolution approach based on generative adversary network (GAN) was used to interpolate (up-sample) ultrasound radio-frequency (RF) echo data along the lateral (perpendicular to the acoustic beam direction) direction before motion estimation. Our primary objective was to investigate the feasibility of using a GAN-based super-solution approach to improve lateral resolution in the RF data as a means of improving strain image quality in quasi-static ultrasound strain elastography (QUSE). Unlike natural scene photographs, axial (parallel to the acoustic beam direction) resolution is significantly higher than that of lateral resolution in ultrasound RF data. To better handle RF data, we first modified a super-resolution generative adversary network (SRGAN) model developed by the computer vision community. We named the modified SRGAN model as super-resolution radio-frequency neural network (SRRFNN) model. Our preliminary experiments showed that, compared with axial strain elastograms obtained using the original ultrasound RF data, axial strain elastograms using ultrasound RF data up-sampled by the proposed SRRFNN model were improved. Based on the Wilcoxon rank-sum tests, such improvements were statistically significant (p< 0.05) for large deformation (3-5%). Also, the proposed SRRFNN model outperformed a commonly-used method (i.e. bi-cubic interpolation used in MATLAB [Mathworks Inc., MA, USA]) in terms of improving axial strain elastograms. We concluded that applying the proposed (SRRFNN) model was feasible and good-quality strain elastography data could be obtained in in vivo tumor-bearing breast ultrasound data. © 2013 IEEE.","Acoustic waves; Frequency estimation; Image enhancement; Medical imaging; Motion estimation; Optical resolving power; Radio waves; Bicubic interpolation; Breast ultrasound; Feasibility studies; Lateral resolution; Primary objective; Radio frequencies; Vision communities; Wilcoxon rank sum test; Ultrasonics","Generative adversarial network; motion tracking; quasi-static ultrasound strain elastography; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083741769"
"Jiang K.; Wang Z.; Yi P.; Wang G.; Lu T.; Jiang J.","Jiang, Kui (57203871718); Wang, Zhongyuan (57203515592); Yi, Peng (57203880354); Wang, Guangcheng (57209891180); Lu, Tao (56406646300); Jiang, Junjun (54902306100)","57203871718; 57203515592; 57203880354; 57209891180; 56406646300; 54902306100","Edge-Enhanced GAN for Remote Sensing Image Superresolution","2019","IEEE Transactions on Geoscience and Remote Sensing","57","8","8677274","5799","5812","13","10.1109/TGRS.2019.2902431","209","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069773369&doi=10.1109%2fTGRS.2019.2902431&partnerID=40&md5=4ce590788aca7fbb4534236883443046","The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches. © 1980-2012 IEEE.","Deep learning; Image reconstruction; Optical resolving power; Remote sensing; Satellites; Adversarial learning; dense connection; Edge enhancements; Remote sensing imagery; Super resolution; algorithm; experimental design; image processing; reconstruction; remote sensing; satellite data; satellite imagery; Image enhancement","Adversarial learning; dense connection; edge enhancement; remote sensing imagery; superresolution","Article","Final","","Scopus","2-s2.0-85069773369"
"Sung M.; Kim J.; Yu S.-C.","Sung, Minsung (57192577834); Kim, Juhwan (57188867311); Yu, Son-Cheol (8712522100)","57192577834; 57188867311; 8712522100","Image-based Super Resolution of Underwater Sonar Images using Generative Adversarial Network","2019","IEEE Region 10 Annual International Conference, Proceedings/TENCON","2018-October","","8650176","457","461","4","10.1109/TENCON.2018.8650176","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063208442&doi=10.1109%2fTENCON.2018.8650176&partnerID=40&md5=603fad628bfca4e1af82aff2b2f23675","Sonar sensors are widely used for underwater observations because they can be used in a turbid stream and have a long operating range. However, images taken with sonar sensors are difficult to identify because of low resolution. In this paper, we proposed a method based on a generative adversarial network to increase the resolution of the underwater sonar image. We built the network of 16 residual blocks and eight convolutional layers. We then trained it with sonar images cropped in several ways. As a result, we could improve the resolution of sonar images from various scenes and recorded a higher peak signal-to-noise ratio than interpolation. The proposed method could help to identify the underwater object without losing the working range of sonar. © 2018 IEEE.","Optical resolving power; Signal to noise ratio; Sonar; Underwater acoustics; Underwater structures; Adversarial networks; Multibeam sonar; Peak signal to noise ratio; Single images; Sonar image; Underwater objects; Underwater observation; Underwater sonars; Image enhancement","acoustic lens-based multi-beam sonar; single image super resolution; sonar image enhancement","Conference paper","Final","","Scopus","2-s2.0-85063208442"
"Lucas A.; Lopez-Tapia S.; Molina R.; Katsaggelos A.K.","Lucas, Alice (57200297854); Lopez-Tapia, Santiago (57196006294); Molina, Rafael (34870201500); Katsaggelos, Aggelos K. (7102711302)","57200297854; 57196006294; 34870201500; 7102711302","Generative Adversarial Networks and Perceptual Losses for Video Super-Resolution","2019","IEEE Transactions on Image Processing","28","7","8629024","3312","3327","15","10.1109/TIP.2019.2895768","82","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065998195&doi=10.1109%2fTIP.2019.2895768&partnerID=40&md5=2e64490d16271986e1b9f36ac9f87451","Video super-resolution (VSR) has become one of the most critical problems in video processing. In the deep learning literature, recent works have shown the benefits of using adversarial-based and perceptual losses to improve the performance on various image restoration tasks; however, these have yet to be applied for video super-resolution. In this paper, we propose a generative adversarial network (GAN)-based formulation for VSR. We introduce a new generator network optimized for the VSR problem, named VSRResNet, along with new discriminator architecture to properly guide VSRResNet during the GAN training. We further enhance our VSR GAN formulation with two regularizers, a distance loss in feature-space and pixel-space, to obtain our final VSRResFeatGAN model. We show that pre-training our generator with the mean-squared-error loss only quantitatively surpasses the current state-of-the-art VSR models. Finally, we employ the PercepDist metric to compare the state-of-the-art VSR models. We show that this metric more accurately evaluates the perceptual quality of SR solutions obtained from neural networks, compared with the commonly used PSNR/SSIM metrics. Finally, we show that our proposed model, the VSRResFeatGAN model, outperforms the current state-of-the-art SR models, both quantitatively and qualitatively. © 1992-2012 IEEE.","Deep learning; Image enhancement; Image resolution; Mean square error; Neural networks; Optical resolving power; Video signal processing; Adversarial networks; Critical problems; Image generations; Mean squared error; Perceptual quality; State of the art; Video processing; Video super-resolution; article; videorecording; Image reconstruction","Artificial neural networks; image generation; image resolution; video signal processing","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85065998195"
"Mi J.; Gao W.; Yang S.; Hao X.; Li M.; Wang M.; Zheng L.","Mi, Jiaqi (57212380300); Gao, Wanlin (8931933300); Yang, Si (57216099524); Hao, Xia (57204558984); Li, Minzan (57204839083); Wang, Minjuan (56434879900); Zheng, Lihua (10139458500)","57212380300; 8931933300; 57216099524; 57204558984; 57204839083; 56434879900; 10139458500","A Method of Plant Root Image Restoration Based on GAN","2019","IFAC-PapersOnLine","52","30","","219","224","5","10.1016/j.ifacol.2019.12.525","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081054750&doi=10.1016%2fj.ifacol.2019.12.525&partnerID=40&md5=db6174363abc88a99dd5066b98c7df54","Root is one of the most important organs for plants to obtain water and nutrients so that its morphological research is critique for identifying plant growth conditions. Aiming at breakthrough of barriers and obtaining accurate root phenotype data based on the original plant root image, a method of Arabidopsis thaliana root image restoration based on GAN (generative adversarial network) was proposed in this paper. Firstly, a second generation Kinect camera is used to capture the matched data set for training the GAN, which includes high-resolution images of some objects and their matched fuzzy and distort images, and high-resolution images of Arabidopsis' roots and their images in the biogel. Secondly, a GAN with attention mechanism is constructed and trained. The network mainly consists of two parts: The generator and the discriminator with attention mechanism. It is multi-layer convolution network, except that the generator adopts a de-convolution structure to carry out the super-resolution reconstruction. The generator is responsible for converting a fuzzy image into high-resolution image, and the discriminator is used to distinguish whether the inputted image is derived from the prepared dataset or generated by the generator. With the progress of network training, the generator is getting better and better at generating images, the same is true for the effect of the discriminator discriminating the image, that is, the better mapping relationship between the blurred or partially missing image and the high resolution complete image is established. Finally, import the root image of the Arabidopsis planted in the biogel into the trained network and the repaired and restored root image can be obtained. Compared with the original image, the restored one has more accurate details and accordingly more accurate root morphology parameters are computed. The experiment results showed that the proposed method can be used to achieve the super-resolution reconstruction and complete the incomplete or blur Arabidopsis root images. © 2019 Elsevier B.V. All rights reserved.","Convolution; Discriminators; Network layers; Optical resolving power; Plants (botany); Restoration; Adversarial networks; Arabidopsis; High resolution image; Image; Mapping relationships; Morphological research; Root system; Super resolution reconstruction; Image reconstruction","Arabidopsis; Generative Adversarial Network (GAN); Image; Restoration; Root system","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85081054750"
"Liu T.; de Haan K.; Rivenson Y.; Wei Z.; Zeng X.; Zhang Y.; Ozcan A.","Liu, Tairan (57204707339); de Haan, Kevin (57205683866); Rivenson, Yair (24081473400); Wei, Zhensong (57202577273); Zeng, Xin (57207568384); Zhang, Yibo (55910548500); Ozcan, Aydogan (7005667692)","57204707339; 57205683866; 24081473400; 57202577273; 57207568384; 55910548500; 7005667692","Deep learning-based super-resolution in coherent imaging systems","2019","Scientific Reports","9","1","3926","","","","10.1038/s41598-019-40554-1","71","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062600782&doi=10.1038%2fs41598-019-40554-1&partnerID=40&md5=7f9cf7b313122d11daf7f57b378b1b16","We present a deep learning framework based on a generative adversarial network (GAN) to perform super-resolution in coherent imaging systems. We demonstrate that this framework can enhance the resolution of both pixel size-limited and diffraction-limited coherent imaging systems. The capabilities of this approach are experimentally validated by super-resolving complex-valued images acquired using a lensfree on-chip holographic microscope, the resolution of which was pixel size-limited. Using the same GAN-based approach, we also improved the resolution of a lens-based holographic imaging system that was limited in resolution by the numerical aperture of its objective lens. This deep learning-based super-resolution framework can be broadly applied to enhance the space-bandwidth product of coherent imaging systems using image data and convolutional neural networks, and provides a rapid, non-iterative method for solving inverse image reconstruction or enhancement problems in optics. © 2019, The Author(s).","Deep Learning; Equipment Design; Female; Holography; Humans; Image Enhancement; Lung; Microscopy; Neural Networks, Computer; Papanicolaou Test; Software; Vaginal Smears; devices; diagnostic imaging; equipment design; female; holography; human; image enhancement; lung; microscopy; Papanicolaou test; procedures; software; vagina smear","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85062600782"
"Liu T.; De Haan K.; Rivenson Y.; Wei Z.; Zeng X.; Zhang Y.; Ozcan A.","Liu, Tairan (57204707339); De Haan, Kevin (57205683866); Rivenson, Yair (24081473400); Wei, Zhensong (57202577273); Zeng, Xin (57207568384); Zhang, Yibo (55910548500); Ozcan, Aydogan (7005667692)","57204707339; 57205683866; 24081473400; 57202577273; 57207568384; 55910548500; 7005667692","Enhancing resolution in coherent microscopy using deep learning","2019","2019 Conference on Lasers and Electro-Optics, CLEO 2019 - Proceedings","","","8749690","","","","10.23919/CLEO.2019.8749690","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069148294&doi=10.23919%2fCLEO.2019.8749690&partnerID=40&md5=4180dbc3e7cdc029e285e178e5f861ef","A generative adversarial network (GAN) based super-resolution framework is presented. This deep learning-based framework is capable of enhancing the resolution of coherent imaging systems in both pixel size-limited and diffraction-limited microscopy systems. © 2019 The Author(s) 2019 OSA.","","","Conference paper","Final","","Scopus","2-s2.0-85069148294"
"Li K.; Ye L.; Yang S.; Jia J.; Huang J.; Wang X.","Li, Kai (57209419711); Ye, Liang (57211080238); Yang, Shenghao (57211082449); Jia, Jinfang (57205714785); Huang, Jianqiang (56843800400); Wang, Xiaoying (36065470400)","57209419711; 57211080238; 57211082449; 57205714785; 56843800400; 36065470400","Single image super resolution based on generative adversarial networks","2019","Proceedings of SPIE - The International Society for Optical Engineering","11179","","111790T","","","","10.1117/12.2539692","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072623260&doi=10.1117%2f12.2539692&partnerID=40&md5=141c4408eff8499304e8e2d623a0a5b3","Deep neural networks based on SRGAN single image super-resolution reconstruction can generate more realistic images than CNN-based super-resolution deep neural networks. However, when the network is deeper and more complex, unpleasant artifacts can result. Through a lot of experiments, we can use the ESRGAN model to avoid such problems. When using the ESRGAN model for super-resolution reconstruction, the perceived index of the resulting results does not reach a lower value. There are two reasons for this: (1)ESRGAN does not expand the feature maping. ESRGAN uses 128∗128 to obtain the feature information of the image by default, and can't get more image information better. (2) ESRGAN did not re-optimize the generated image. Therefore, we propose ESRGAN-Pro to optimize ESRGAN for the above two aspects, combined with a large amount of training data, and get a better perception index and texture. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep neural networks; Image processing; Optical resolving power; Textures; Adversarial networks; Feature information; Image information; Learning frameworks; Realistic images; Single-image super-resolution reconstruction; Super resolution; Super resolution reconstruction; Image reconstruction","Deep learning framework; Generative adversarial networks; Image processing; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85072623260"
"Chen Y.; Sun W.; Wang H.","Chen, Yifeng (57207472864); Sun, Wei (57207468985); Wang, Haohan (56025137300)","57207472864; 57207468985; 56025137300","Heterogeneous Hi-C Data Super-resolution with a Conditional Generative Adversarial Network","2019","Proceedings - 2018 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2018","","","8621499","2213","2220","7","10.1109/BIBM.2018.8621499","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062481645&doi=10.1109%2fBIBM.2018.8621499&partnerID=40&md5=93e5373266f32999751ccff946088f3a","The high-throughput chromosome conformation capture (Hi-C) technique is a useful technique to obtain the three-dimensional conformation and information of chromatin in the nucleus. However,the development of Hi-C is limited by data resolution. In this paper, we present a conditional generative adversarial network (GAN) model HiC-GAN to imporove the resolution of Hi-C data. The proposed model is capable to generate Hi-C interaction matrices with four-time-higher resolution, where the accuracy of reconstruction can be guaranteed at around 90%. The model can not only perform well within the same cell, but also be applied to other cells of different types and species. Besides, experiments show that the model can also be used to reconstruct data with low-sequence depth and different value of resolution. © 2018 IEEE.","Chromosomes; Adversarial networks; Data resolutions; High throughput; Higher resolution; Interaction matrices; Super resolution; Bioinformatics","","Conference paper","Final","","Scopus","2-s2.0-85062481645"
"Piaoyi Y.; Yaping Z.","Piaoyi, Yuan (57212199153); Yaping, Zhang (56415609000)","57212199153; 56415609000","Image super-resolution reconstruction method using dual discriminator based on generative adversarial networks","2019","Laser and Optoelectronics Progress","56","23","231010","","","","10.3788/LOP56.231010","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076174652&doi=10.3788%2fLOP56.231010&partnerID=40&md5=a04dc7b718eb7c889141764fd767f3ee","In this study, we propose a dual discriminator super-resolution reconstruction network (DDSRRN) that can improve the super-resolution reconstruction quality of images. By adding a discriminator based on generative adversarial networks, the DDSRRN combines the Kullback-Leibler (KL) divergence and reverse KL divergence into a unified objective function for training two discriminators. Thus, the complementary statistical properties obtained from these divergences can be exploited to effectively diversify the pre-estimated density under multiple modes. Additionally, model collapse is effectively avoided during the reconstruction process, and the model training stability is improved. The model loss function can be designed based on the Charbonnier loss function to estimate the content loss. Furthermore, the intermediate features of the network are used to design the perceptual loss and style loss. Finally, a deconvolution layer is designed to reconstruct the super-resolution images, thereby reducing the image reconstruction time. The proposed method is experimentally demonstrated to provide abundant details. Thus, the proposed method exhibits good generalization ability and obtains improved subjective visual evaluation and objective quantitative evaluation. © 2019 Universitat zu Koln. All rights reserved.","","Convolutional neural network: Kl divergence; Generative adversarial network: Image super-resolution reconstruction; Image processing","Article","Final","","Scopus","2-s2.0-85076174652"
"Upadhyay U.; Awate S.P.","Upadhyay, Uddeshya (57211269778); Awate, Suyash P. (8836988800)","57211269778; 8836988800","Robust super-resolution gan, with manifold-based and perception loss","2019","Proceedings - International Symposium on Biomedical Imaging","2019-April","","8759375","1372","1376","4","10.1109/ISBI.2019.8759375","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073898999&doi=10.1109%2fISBI.2019.8759375&partnerID=40&md5=dd10f729c70842f47626465499e8a6fe","Super-resolution using deep neural networks typically relies on highly curated training sets that are often unavailable in clinical deployment scenarios. Using loss functions that assume Gaussian-distributed residuals makes the learning sensitive to corruptions in clinical training sets. We propose novel loss functions that are robust to corruptions in training sets by modeling heavy-tailed non-Gaussian distributions on the residuals. We propose a loss based on an autoencoder-based manifold-distance between the super-resolved and high-resolution images, to reproduce realistic textural content in super-resolved images. We propose to learn to super-resolve images to match human perceptions of structure, luminance, and contrast. Results on a large clinical dataset shows the advantages of each of our contributions, where our framework improves over the state of the art. © 2019 IEEE.","Deep neural networks; Large dataset; Medical imaging; Robustness (control systems); Adversarial networks; Clinical training; Deployment scenarios; Gaussian distributed; High resolution image; Manifold distance; Non-gaussian distribution; Super resolution; Optical resolving power","Generative adversarial network; Manifold-based loss; Perceptual loss; Robustness; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85073898999"
"Yang R.; Wang W.","Yang, Rui (57715906600); Wang, Wenxi (57216695832)","57715906600; 57216695832","Comparison of Super-resolution Reconstruction Algorithms Based on Texture Feature Classification","2019","2019 IEEE 4th International Conference on Image, Vision and Computing, ICIVC 2019","","","8980981","306","310","4","10.1109/ICIVC47709.2019.8980981","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084323819&doi=10.1109%2fICIVC47709.2019.8980981&partnerID=40&md5=8f9e661e5a0ae9c1d081b339d5d843c1","Single image super-resolution (SISR) is a heated research topic in recent years. This paper innovatively proposes a network selection method based on texture feature and test index of test image set. We select the universal data set Set5, Set14, B100, and DIV2K as our test data set. We propose a method of extracting texture features of test image set by using gray-level co-occurrence matrix and then classify test image sets based on four elements (Contrast, Correlation, Energy, Inverse Differential Moment). By calculating the perceptual index (PI) and root-mean-square error (RMSE) parameters of image sets under different reconstruction models, it can be concluded that enhanced super-resolution generative adversarial network (ESRGAN) has a significant effect on improving the perceived quality of images, while enhanced deep residual networks (EDSR) has the optimal algorithm accuracy. There is an extent negative correlation between PI and RMSE. This research results are of great significance for the selection of super-resolution reconstruction networks according to different indexes. © 2019 IEEE.","Image enhancement; Mean square error; Optical resolving power; Statistical tests; Textures; Adversarial networks; Gray level co-occurrence matrix; Inverse differential moment; Negative correlation; Optimal algorithm; Root mean square errors; Super resolution reconstruction; Texture feature classification; Image texture","gray-level co-occurrence matrix; perceptual index; root-mean-square error; single image super resolution","Conference paper","Final","","Scopus","2-s2.0-85084323819"
"Li Q.; Luo Y.","Li, Qixin (57831750600); Luo, Yaneng (57191528235)","57831750600; 57191528235","Using GAN priors for ultrahigh resolution seismic inversion","2019","SEG Technical Program Expanded Abstracts","","","","2453","2457","4","10.1190/segam2019-3215520.1","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100508253&doi=10.1190%2fsegam2019-3215520.1&partnerID=40&md5=ad9566a00fc94e931bee0b3c96cb4ea0","As one of machine learning techniques, deep learning has recently achieved the state-of-the-art performances in many areas, such as computer vision, natural language processing, to name a few. A generative model called Generative Adversarial Network (GAN) was invented in 2014. This deep network model is deemed as the most interesting idea in the last 10 years by the machine learning community and outperformed the traditional methods in many tasks like image synthesis and super-resolution. Laying the heart of the GAN is its ability to model any realistically sharp data distribution. Instead of providing a “blurry” sample, the high-resolution samples can be sampled from the GAN model, no matter it is a natural image or a well log. In this abstract, we propose a novel ultrahigh resolution seismic inversion method using GAN priors. The basic workflow is described below. Firstly, a simple GAN architecture was designed. Then, we train this GAN to model the well-log data distribution. Once the GAN is properly trained, it offers the high-resolution samples as priors to the inversion algorithm. To effectively use this prior information, we adopt the projected gradient descent algorithm to iteratively fit the seismic data and projects the “blurry” sample to the high resolution set of prior samples defined by the GAN. We further use a thin-layer model to validate the feasibility and superiority of our method. Comparing with the traditional method, our result shows a higher precision and resolution. © 2019 SEG","Deep learning; Gradient methods; Learning algorithms; Natural language processing systems; Seismology; Well logging; Data distribution; Generative model; High resolution; Images synthesis; Machine learning communities; Machine learning techniques; Network models; Seismic inversion; State-of-the-art performance; Ultrahigh resolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85100508253"
"Sood R.; Topiwala B.; Choutagunta K.; Sood R.; Rusu M.","Sood, Rewa (57207113139); Topiwala, Binit (57207114071); Choutagunta, Karthik (55781061100); Sood, Rohit (57207113138); Rusu, Mirabela (25628776200)","57207113139; 57207114071; 55781061100; 57207113138; 25628776200","An Application of Generative Adversarial Networks for Super Resolution Medical Imaging","2019","Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018","","","8614080","326","331","5","10.1109/ICMLA.2018.00055","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062241805&doi=10.1109%2fICMLA.2018.00055&partnerID=40&md5=659c4a8efb468762127c36a647eeaa84","Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the patient to remain still for long periods of time, which causes patient discomfort and increases the probability of motion induced image artifacts. A possible solution is to acquire low resolution (LR) images and to process them with the Super Resolution Generative Adversarial Network (SRGAN) to create an HR version. Acquiring LR images requires a lower scan time than acquiring HR images, which allows for higher patient comfort and scanner throughput. This work applies SRGAN to MR images of the prostate to improve the in-plane resolution by factors of 4 and 8. The term 'super resolution' in the context of this paper defines the post processing enhancement of medical images as opposed to 'high resolution' which defines native image resolution acquired during the MR acquisition phase. We also compare the SRGAN to three other models: SRCNN, SRResNet, and Sparse Representation. While the SRGAN results do not have the best Peak Signal to Noise Ratio (PSNR) or Structural Similarity (SSIM) metrics, they are the visually most similar to the original HR images, as portrayed by the Mean Opinion Score (MOS) results. © 2018 IEEE.","Image acquisition; Image enhancement; Image resolution; Learning systems; Machine learning; Magnetic resonance; Magnetic resonance imaging; Optical resolving power; Signal to noise ratio; Adversarial networks; Low resolution images; Mean opinion scores; Patient comfort; Peak signal to noise ratio; Sparse representation; Structural similarity; Super resolution; Medical imaging","Generative adversarial networks; Machine learning; Medical imaging; MRI; Super resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062241805"
"Hu X.; Liu X.; Wang Z.; Li X.; Peng W.; Cheng G.","Hu, Xiaoyan (55496214500); Liu, Xiangjun (57215136288); Wang, Zechen (57215133842); Li, Xinran (57215137323); Peng, Wenqiang (57215132251); Cheng, Guang (7401862789)","55496214500; 57215136288; 57215133842; 57215137323; 57215132251; 7401862789","RTSRGAN: Real-time super-resolution generative adversarial networks","2019","Proceedings - 2019 7th International Conference on Advanced Cloud and Big Data, CBD 2019","","","8916480","321","326","5","10.1109/CBD.2019.00064","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076912062&doi=10.1109%2fCBD.2019.00064&partnerID=40&md5=462cf657554f38243e27fe25d797759a","The Enhanced Super-Resolution Generative Adver-sarial Networks (ESRGAN) is the state of the art deep learning based image Super-Resolution (SR) and has the best performance in perceptual quality. However, we find it is time-consuming, which makes it impractical for SR at clients' side during video delivery since SR usually uses clients' computing resources (the computational power at the clients' side should not be as powerful as GPU) and videos often require real-time playback. While Efficient Sub-Pixel Convolutional Neural Network (ESPCN) has the best real-time performance, it is still not capable of offering a smooth watching experience and has much lower perceptual quality. In order to simultaneously meet the demands on the real-time performance and the resulting pleasant artifacts of SR at the clients' side, we propose RTSRGAN to exploit the advantages of ESRGAN in image perceptual quality and ESPCN in real-time performance. Our experimental results indicate that RTSRGAN has the fastest reconstruction speed, on the average, 15 images per second on a single 2.3GHz CPU (only 6 images per second by ESPCN), and reconstructs images of a relatively acceptable perceptual quality, which validates that our proposed RTSRGAN can be used for SR at clients' side to enhance the real-time performance and ensure the image perceptual quality. © 2019 IEEE.","Big data; Deep learning; Neural networks; Optical resolving power; ESPCN; ESRGAN; Image super resolutions; Network structures; Real time; Image enhancement","ESPCN; ESRGAN; Image super-resolution; Network structure; Real-time","Conference paper","Final","","Scopus","2-s2.0-85076912062"
"Huang J.; Li K.; Wang X.","Huang, Jianqiang (56843800400); Li, Kail (57216952718); Wang, Xiaoying (36065470400)","56843800400; 57216952718; 36065470400","Single image super-resolution reconstruction of enhanced loss function with multi-gpu training","2019","Proceedings - 2019 IEEE Intl Conf on Parallel and Distributed Processing with Applications, Big Data and  Cloud Computing, Sustainable Computing and Communications, Social Computing and Networking, ISPA/BDCloud/SustainCom/SocialCom 2019","","","9047446","559","565","6","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00085","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085482629&doi=10.1109%2fISPA-BDCloud-SustainCom-SocialCom48970.2019.00085&partnerID=40&md5=feb83eb9a8bb7b2243b0b568264853f3","According to research on super-resolution (SR), SR image reconstruction using generated anti-networks can produce images that are more realistic than using convolutional neural networks. At present, SR technology based on convolutional neural networks ignores the impact of loss function on image reconstruction; the results lack detail and accuracy. In this paper, we use SR method and combine Generative Adversarial Networks to design a super-resolution (Lapras-GAN) model of the enhanced loss function. The proposed enhancement loss function is a Mix loss function that combines the multiscale SSIM and L1 loss functions to obtain realistic images. We performed qualitative and quantitative analysis of the performance of different loss functions and demonstrated the advantages of the Mix loss function. In addition, the neural network is accelerated by multiple GPUs of multiple nodes, which can be 3-4 times faster than a single node single GPU. Experimental results show that the proposed Lapras-GAN method can generate images consistent with images produced by human perception. Further comparisons show that our Lapras-GAN has excellent performance and test time in the PIRM2018 experimental test data set. Finally, we obtained a perception index of 1.83 and a test time of 0.031s in the PIRM2018 competition test set. © 2019 IEEE.","Big data; Cloud computing; Convolution; Convolutional neural networks; Image enhancement; Image resolution; Optical resolving power; Program processors; Social networking (online); Statistical tests; Adversarial networks; Competition tests; Experimental test; Qualitative and quantitative analysis; Realistic images; Single-image super-resolution reconstruction; Super resolution; Technology-based; Image reconstruction","Deep learning; Generative adversarial networks; Gpu; Loss function; Single image super resolution","Conference paper","Final","","Scopus","2-s2.0-85085482629"
"Chen R.; Xie Y.; Luo X.; Qu Y.; Li C.","Chen, Rong (57202222085); Xie, Yuan (55710277800); Luo, Xiaotong (57216369982); Qu, Yanyun (8509964600); Li, Cuihua (57204712944)","57202222085; 55710277800; 57216369982; 8509964600; 57204712944","Joint-attention discriminator for accurate super-resolution via adversarial training","2019","MM 2019 - Proceedings of the 27th ACM International Conference on Multimedia","","","","711","719","8","10.1145/3343031.3351008","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074867584&doi=10.1145%2f3343031.3351008&partnerID=40&md5=bc7c2d5dfa25375e421923f556ace12e","Tremendous progress has been witnessed on single image super-resolution (SR), where existing deep SR models achieve impressive performance in objective criteria, e.g., PSNR and SSIM. However, most of the SR methods are limited in visual perception, for example, they look too smooth. Generative adversarial network (GAN) favors SR visual effects over most of the deep SR models but is poor in objective criteria. In order to trade off the objective and subjective SR performance, we design a joint-attention discriminator with which GAN improves the SR performance in PSNR and SSIM, as well as maintaining the visual effect compared with non-attention GAN based SR models. The joint-attention discriminator contains dense channel-wise attention and cross-layer attention blocks. The former is applied in the shallow layers of the discriminator for channel-wise weighting combination of feature maps. The latter is employed to select feature maps in some middle and deep layers for effective discrimination. Extensive experiments are conducted on six benchmark datasets and the experimental results show that our proposed discriminator combining with different generators can achieve more realistic visual performances. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Discriminators; Economic and social effects; Adversarial networks; Cross layer; Dense Channel-wise Attention; Image super resolutions; Joint attention; Optical resolving power","Cross-layer Attention; Dense Channel-wise Attention; Generative Adversarial Network; Image Super-resolution; Joint-attention Discriminator","Conference paper","Final","","Scopus","2-s2.0-85074867584"
"Lim S.; Khan S.; Alessandro M.; McFall K.","Lim, Steffen (57208834767); Khan, Sams (57208836795); Alessandro, Matteo (57208838213); McFall, Kevin (33267815600)","57208834767; 57208836795; 57208838213; 33267815600","Spatio-temporal Super-resolution with photographic and depth data using GANs","2019","ACMSE 2019 - Proceedings of the 2019 ACM Southeast Conference","","","","262","263","1","10.1145/3299815.3314482","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065927146&doi=10.1145%2f3299815.3314482&partnerID=40&md5=49be891707b9815c686a4dc6a2ffc4d7","LiDAR technology is essential for self-driving cars, which have seen a surge in interest and investments from startups and established automotive corporations alike. However, the task of automated driving requires high resolution and significant depth-range capabilities of the sensor, keeping its cost prohibitive. Super-resolution of depth maps has been explored as a potential circumvention of these problems, with a substantial number of methods being analyzed in the past few years, yielding various levels of success. We propose a super-resolution algorithm trained for depth-map data and LiDAR compatibility using Generative Adversarial Networks (GANs). © 2019 Copyright held by the owner/author(s).","Deep learning; Learning systems; Optical radar; Adversarial networks; Automated driving; Cost prohibitive; Depth image; High resolution; Number of methods; Super resolution; Super resolution algorithms; Optical resolving power","Deep learning; Depth image; GAN; LiDAR; Machine learning; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85065927146"
"","","","Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS","2019","Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS","2018-November","","","","","1178","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063642141&partnerID=40&md5=07c3c61ef114e23b4af4fdf6386a42c1","The proceedings contain 251 papers. The topics discussed include: design and implementation of a television content expansion system; scalable source code plagiarism detection using source code vectors clustering; using NARX neural network for prediction of urban rail transit passenger flow; effective ADAM-optimized LSTM neural network for electricity price forecasting; image and text correlation judgment based on deep learning; a unique method for detecting grounds in the indoor environment; design of the security mechanism for a BPO cloud computing platform; TailorFix. An automated repair framework for assignment statements; dual discriminator generative adversarial network for single image super-resolution; a recommendation system for cloud services based on knowledge graph; vision-based positioning: related technologies, applications, and research challenges; and transfer learning on convolutional neural networks for dog identification.","","","Conference review","Final","","Scopus","2-s2.0-85063642141"
"Wang X.; Lucas A.; Lopez-Tapia S.; Wu X.; Molina R.; Katsaggelos A.K.","Wang, Xijun (57209886240); Lucas, Alice (57200297854); Lopez-Tapia, Santiago (57196006294); Wu, Xinyi (57216191150); Molina, Rafael (34870201500); Katsaggelos, Aggelos K. (7102711302)","57209886240; 57200297854; 57196006294; 57216191150; 34870201500; 7102711302","A composite discriminator for generative adversarial network based video super-resolution","2019","European Signal Processing Conference","2019-September","","","","","","10.23919/EUSIPCO.2019.8903073","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075610294&doi=10.23919%2fEUSIPCO.2019.8903073&partnerID=40&md5=30eaf94f9c1899455f45c3046f8fe78b","Generative Adversarial Networks (GANs) have been used for solving the video super-resolution problem. So far, video super-resolution GAN-based methods use the traditional GAN framework which consists of a single generator and a single discriminator that are trained against each other. In this work we propose a new framework which incorporates two collaborative discriminators whose aim is to jointly improve the quality of the reconstructed video sequence. While one discriminator concentrates on general properties of the images, the second one specializes on obtaining realistically reconstructed features, such as, edges. Experiments results demonstrate that the learned model outperforms current state of the art models and obtains super-resolved frames, with fine details, sharp edges, and fewer artifacts. © 2019,IEEE","Image reconstruction; Optical resolving power; Adversarial networks; GaN based; Sharp edges; Spatially adaptive; State of the art; Video sequences; Video super-resolution; Discriminators","Generative Adversarial Networks; Spatially Adaptive; The Composite Discriminator; Video Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85075610294"
"","","","2nd International Workshop on Machine Learning for Medical Image Reconstruction, MLMIR 2019 held in Conjunction with 22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11905 LNCS","","","","","264","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076201995&partnerID=40&md5=7756405fa8d0d20a48fd30ea90563ea3","The proceedings contain 24 papers. The special focus in this conference is on Machine Learning for Medical Image Reconstruction. The topics include: Measuring CT Reconstruction Quality with Deep Convolutional Neural Networks; deep Learning Based Metal Inpainting in the Projection Domain: Initial Results; flexible Conditional Image Generation of Missing Data with Learned Mental Maps; Spatiotemporal PET Reconstruction Using ML-EM with Learned Diffeomorphic Deformation; stain Style Transfer Using Transitive Adversarial Networks; Blind Deconvolution Microscopy Using Cycle Consistent CNN with Explicit PSF Layer; Deep Learning Based Approach to Quantification of PET Tracer Uptake in Small Tumors; Task-GAN: Improving Generative Adversarial Network for Image Reconstruction; gamma Source Location Learning from Synthetic Multi-pinhole Collimator Data; Self-supervised Recurrent Neural Network for 4D Abdominal and In-utero MR Imaging; neural Denoising of Ultra-low Dose Mammography; image Reconstruction in a Manifold of Image Patches: Application to Whole-Fetus Ultrasound Imaging; image Super Resolution via Bilinear Pooling: Application to Confocal Endomicroscopy; TPSDicyc: Improved Deformation Invariant Cross-domain Medical Image Synthesis; PredictUS: A Method to Extend the Resolution-Precision Trade-Off in Quantitative Ultrasound Image Reconstruction; fast Dynamic Perfusion and Angiography Reconstruction Using an End-to-End 3D Convolutional Neural Network; APIR-Net: Autocalibrated Parallel Imaging Reconstruction Using a Neural Network; Accelerated MRI Reconstruction with Dual-Domain Generative Adversarial Network; Deep Learning for Low-Field to High-Field MR: Image Quality Transfer with Probabilistic Decimation Simulator; joint Multi-anatomy Training of a Variational Network for Reconstruction of Accelerated Magnetic Resonance Image Acquisitions; modeling and Analysis Brain Development via Discriminative Dictionary Learning; Virtual Thin Slice: 3D Conditional GAN-based Super-Resolution for CT Slice Interval.","","","Conference review","Final","","Scopus","2-s2.0-85076201995"
"","","","22nd International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11766 LNCS","","","","","814","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075667812&partnerID=40&md5=6d152d7f952489e7c71038e7ab6c7893","The proceedings contain 444 papers. The special focus in this conference is on International Conference on Medical Image Computing and Computer-Assisted Intervention. The topics include: RinQ Fingerprinting: Recurrence-Informed Quantile Networks for Magnetic Resonance Fingerprinting; RCA-U-Net: Residual Channel Attention U-Net for Fast Tissue Quantification in Magnetic Resonance Fingerprinting; GANReDL: Medical Image Enhancement Using a Generative Adversarial Network with Real-Order Derivative Induced Loss Functions; Generation of 3D Brain MRI Using Auto-Encoding Generative Adversarial Networks; Semi-supervised VAE-GAN for Out-of-Sample Detection Applied to MRI Quality Control; disease-Image Specific Generative Adversarial Network for Brain Disease Diagnosis with Incomplete Multi-modal Neuroimages; Predicting the Evolution of White Matter Hyperintensities in Brain MRI Using Generative Adversarial Networks and Irregularity Map; CoCa-GAN: Common-Feature-Learning-Based Context-Aware Generative Adversarial Network for Glioma Grading; degenerative Adversarial NeuroImage Nets: Generating Images that Mimic Disease Progression; a Two-Stage Multi-loss Super-Resolution Network for Arterial Spin Labeling Magnetic Resonance Imaging; scribble-Based Hierarchical Weakly Supervised Learning for Brain Tumor Segmentation; 3D Dilated Multi-fiber Network for Real-Time Brain Tumor Segmentation in MRI; Refined Segmentation R-CNN: A Two-Stage Convolutional Neural Network for Punctate White Matter Lesion Segmentation in Preterm Infants; voteNet: A Deep Learning Label Fusion Method for Multi-atlas Segmentation; weakly Supervised Brain Lesion Segmentation via Attentional Representation Learning; scalable Neural Architecture Search for 3D Medical Image Segmentation; unified Attentional Generative Adversarial Network for Brain Tumor Segmentation from Multimodal Unpaired Images; high Resolution Medical Image Segmentation Using Data-Swapping Method; x-Net: Brain Stroke Lesion Segmentation Based on Depthwise Separable Convolution and Long-Range Dependencies; interactive Deep Editing Framework for Medical Image Segmentation.","","","Conference review","Final","","Scopus","2-s2.0-85075667812"
"Feng Z.; Zhang W.; Lai J.; Xie X.","Feng, Zhanxiang (57007489700); Zhang, Wenxiao (57276108300); Lai, Jianhuang (56142761700); Xie, Xiaohua (57192665603)","57007489700; 57276108300; 56142761700; 57192665603","Low Resolution Person Re-identification by an Adaptive Dual-Branch Network","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11901 LNCS","","","735","746","11","10.1007/978-3-030-34120-6_60","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076886297&doi=10.1007%2f978-3-030-34120-6_60&partnerID=40&md5=8bdd27e198072f9f3e99e587401b1320","Low-resolution person re-identification (LR-REID) refers to matching cross-view pedestrians from varying resolutions, which is common and challenging for realistic surveillance systems. The LR-REID is largely under-study and the performance of the existing methods drops dramatically in LR domain. In this paper, we propose a novel adaptive dual-branch network (ADBNet) to recover HR pedestrians and learn discriminative resolution-robust representations. The ADBNet employs a two-branch structure to obtain HR images, of which the SR-branch adopts the reconstruction loss to get the global image and the GAN-branch employs the generative loss to generate sharp details. These branches are combined into a universe model and the combination weights are adaptively learned. Furthermore, we propose a relative loss to guarantee shaper edges of the output results. Experimental results on two large-scale benchmarks prove the validity of the proposed approach. © 2019, Springer Nature Switzerland AG.","Computer science; Computers; Adversarial networks; Branch structure; Combination weights; Low resolution; Person re identifications; Super resolution; Surveillance systems; Artificial intelligence","Generative adversarial network; Low resolution; Person re-identification; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85076886297"
"Werhahn M.; Xie Y.; Chu M.; Thuerey N.","Werhahn, Maximilian (57219619923); Xie, You (57204697503); Chu, Mengyu (57196000654); Thuerey, Nils (26024848800)","57219619923; 57204697503; 57196000654; 26024848800","A multi-pass GaN for fluid flow super-resolution","2019","Proceedings of the ACM on Computer Graphics and Interactive Techniques","2","2","a10","","","","10.1145/3340251","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092777667&doi=10.1145%2f3340251&partnerID=40&md5=7615328f9fa0b92e227969c3eaabf5f7","We propose a novel method to up-sample volumetric functions with generative neural networks using several orthogonal passes. Our method decomposes generative problems on Cartesian field functions into multiple smaller sub-problems that can be learned more efficiently. Specifically, we utilize two separate generative adversarial networks: the first one up-scales slices which are parallel to the XY-plane, whereas the second one refines the whole volume along the Z−axis working on slices in the YZ-plane. In this way, we obtain full coverage for the 3D target function and can leverage spatio-temporal supervision with a set of discriminators. Additionally, we demonstrate that our method can be combined with curriculum learning and progressive growing approaches. We arrive at a first method that can up-sample volumes by a factor of eight along each dimension, i.e., increasing the number of degrees of freedom by 512. Large volumetric up-scaling factors such as this one have previously not been attainable as the required number of weights in the neural networks renders adversarial training runs prohibitively difficult. We demonstrate the generality of our trained networks with a series of comparisons to previous work, a variety of complex 3D results, and an analysis of the resulting performance. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Animation; Deep learning; Flow of fluids; Generative adversarial networks; Interactive computer graphics; Orthogonal functions; Computer animation; Fluid simulations; Fluid-flow; Generative model; Multi-pass; Neural-networks; Physic-based deep learning; Physics-based; Superresolution; Volumetrics; Degrees of freedom (mechanics)","Computer animation; Fluid simulation; Generative models; Physics-based deep learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85092777667"
"Song T.-A.; Roy Chowdhury S.; Yang F.; Dutta J.","Song, Tzu-An (57211240200); Roy Chowdhury, Samadrita (57216460098); Yang, Fan (57224324302); Dutta, Joyita (24832908700)","57211240200; 57216460098; 57224324302; 24832908700","Self Supervised Super-Resolution PET Using A Generative Adversarial Network","2019","2019 IEEE Nuclear Science Symposium and Medical Imaging Conference, NSS/MIC 2019","","","9059947","","","","10.1109/NSS/MIC42101.2019.9059947","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083551596&doi=10.1109%2fNSS%2fMIC42101.2019.9059947&partnerID=40&md5=7c43d532f220304cf2a15ae879141998","Resolution limitations pose a continuing challenge for PET quantitation. While deep learning architectures based on convolutional neural networks (CNNs) have produced unprecedented accuracy at generating super-resolution (SR) PET images, most existing approaches are based on supervised learning. The latter requires training datasets with paired (low-and high-resolution) images, which are often unavailable for clinical applications. In this paper, we present a self-supervised SR (SSSR) technique for PET based on dual generative adversarial networks (GANs), which obviate the need for paired training data, ensuring wider applicability and adoptability. Our network receives as inputs a low-resolution PET image, a high-resolution anatomical MR image, and spatial information. An imperfect SR image generated by a separately-trained auxiliary CNN serves as an additional input to the network. This CNN is trained in a supervised manner using paired simulation datasets. The loss function for training the dual GANs consists of two adversarial loss terms, a cycle consistency term, and a total variation penalty on the SR image. The method was validated on clinical data by comparing the SSSR results with those generated from a supervised approach and from deconvolution stabilized by a total variation penalty. Our results show that SSSR, while weaker than its supervised counterpart, noticeably outperforms deconvolution as indicated by the peak signal-to-noise-ratio and structural similarity index measures. © 2019 IEEE.","Convolutional neural networks; Deep learning; Magnetic resonance imaging; Optical resolving power; Signal to noise ratio; Adversarial networks; Clinical application; Learning architectures; Peak signal to noise ratio; Spatial informations; Structural similarity index measures; Super resolution; Training data sets; Medical imaging","","Conference paper","Final","","Scopus","2-s2.0-85083551596"
"Wang X.; Lucas A.; Lopez-Tapia S.; Wu X.; Molina R.; Katsaggelos A.K.","Wang, Xijun (57209886240); Lucas, Alice (57200297854); Lopez-Tapia, Santiago (57196006294); Wu, Xinyi (57216191150); Molina, Rafael (34870201500); Katsaggelos, Aggelos K. (7102711302)","57209886240; 57200297854; 57196006294; 57216191150; 34870201500; 7102711302","Spatially Adaptive Losses for Video Super-resolution with GANs","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8682742","1697","1701","4","10.1109/ICASSP.2019.8682742","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068970912&doi=10.1109%2fICASSP.2019.8682742&partnerID=40&md5=53fc6e3027b417e8346330c93141a650","Deep Learning techniques and more specifically Generative Adversarial Networks (GANs) have recently been used for solving the video super-resolution (VSR) problem. In some of the published works, feature-based perceptual losses have also been used, resulting in promising results. While there has been work in the literature incorporating temporal information into the loss function, studies which make use of the spatial activity to improve GAN models are still lacking. Towards this end, this paper aims to train a GAN guided by a spatially adaptive loss function. Experimental results demonstrate that the learned model achieves improved results with sharper images, fewer artifacts and less noise. © 2019 IEEE.","Deep learning; Image enhancement; Optical resolving power; Speech communication; Adversarial networks; Learning techniques; Loss functions; Spatial activity; Spatial adaptivity; Spatially adaptive; Temporal information; Video super-resolution; Audio signal processing","Generative Adversarial Networks; Perceptual Loss; Spatial Adaptivity; Video Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85068970912"
"Yuan P.; Zhang Y.","Yuan, Piaoyi (57214807265); Zhang, Yaping (57204508813)","57214807265; 57204508813","Dual Discriminator Generative Adversarial Network for Single Image Super-Resolution","2019","Proceedings - 2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics, CISP-BMEI 2019","","","8965727","","","","10.1109/CISP-BMEI48845.2019.8965727","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079121989&doi=10.1109%2fCISP-BMEI48845.2019.8965727&partnerID=40&md5=303b9ccf127dfe040db44238a163ac43","Single image super-resolution(SISR) is to reconstruct a high resolution(HR) image from a single low resolution(LR) image. In this paper, with generative adversarial networks(GAN) model as the basic component, we build a dual discriminator super-resolution reconstruction network(DDSRRN) to improve the quality of image super-resolution reconstruction. By adding another discriminator based on GAN, we combine the Kullback-Leibler(KL) with reverse KL divergence to make a unified objective function to train the two discriminators, and by using the complementary statistical characteristics of these two divergences, the prediction density is effectively dispersed in multi-mode, which can avoid collapse of the network model during the reconstruction process and improve the stability of model training. We build the content loss function using the Charbonnier loss and use the intermediate features information of the two discriminators to build the perceptual loss function and style loss function. The experimental results show that the proposed method has sharp edges and rich details in subjective vision, and obtains better subjective visual evaluation and objective quantitative evaluation. © 2019 IEEE.","Biomedical engineering; Convolutional neural networks; Deep learning; Deep neural networks; Function evaluation; Image enhancement; Image reconstruction; Optical resolving power; Adversarial networks; High resolution image; Image super-resolution reconstruction; KL-divergence; Quantitative evaluation; Reconstruction process; Statistical characteristics; Super resolution reconstruction; Discriminators","Convolutional Neural Network; Deep Learning; Generative Adversarial Networks; Image Super-resolution Reconstruction; KL Divergence","Conference paper","Final","","Scopus","2-s2.0-85079121989"
"You C.; Cong W.; Vannier M.W.; Saha P.K.; Hoffman E.A.; Wang G.; Li G.; Zhang Y.; Zhang X.; Shan H.; Li M.; Ju S.; Zhao Z.; Zhang Z.","You, Chenyu (57203062927); Cong, Wenxiang (7005402309); Vannier, Michael W. (24536157600); Saha, Punam K. (57203243057); Hoffman, Eric A. (58000586800); Wang, Ge (7407148134); Li, Guang (55714171200); Zhang, Yi (57203829244); Zhang, Xiaoliu (57193726068); Shan, Hongming (57191481929); Li, Mengzhou (57211524125); Ju, Shenghong (12778714800); Zhao, Zhen (57040131800); Zhang, Zhuiyang (35207264300)","57203062927; 7005402309; 24536157600; 57203243057; 58000586800; 7407148134; 55714171200; 57203829244; 57193726068; 57191481929; 57211524125; 12778714800; 57040131800; 35207264300","CT Super-Resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE)","2020","IEEE Transactions on Medical Imaging","39","1","8736838","188","203","15","10.1109/TMI.2019.2922960","195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074603019&doi=10.1109%2fTMI.2019.2922960&partnerID=40&md5=58015af6656abe03c02dcce699b3a602","In this paper, we present a semi-supervised deep learning approach to accurately recover high-resolution (HR) CT images from low-resolution (LR) counterparts. Specifically, with the generative adversarial network (GAN) as the building block, we enforce the cycle-consistency in terms of the Wasserstein distance to establish a nonlinear end-to-end mapping from noisy LR input images to denoised and deblurred HR outputs. We also include the joint constraints in the loss function to facilitate structural preservation. In this process, we incorporate deep convolutional neural network (CNN), residual learning, and network in network techniques for feature extraction and restoration. In contrast to the current trend of increasing network depth and complexity to boost the imaging performance, we apply a parallel 1×1 CNN to compress the output of the hidden layer and optimize the number of layers and the number of filters for each convolutional layer. The quantitative and qualitative evaluative results demonstrate that our proposed model is accurate, efficient and robust for super-resolution (SR) image restoration from noisy LR input images. In particular, we validate our composite SR networks on three large-scale CT datasets, and obtain promising results as compared to the other state-of-the-art methods. © 1982-2012 IEEE.","Abdomen; Aged; Aged, 80 and over; Deep Learning; Female; Humans; Image Processing, Computer-Assisted; Male; Neural Networks, Computer; Tibia; Tomography, X-Ray Computed; Convolution; Deep learning; Deep neural networks; Image enhancement; Image reconstruction; Large dataset; Neural networks; Noise abatement; Optical resolving power; Restoration; Adversarial learning; Adversarial networks; Convolutional neural network; Imaging performance; residual learning; State-of-the-art methods; Super resolution; Wasserstein distance; algorithm; Article; artificial neural network; controlled study; convolutional neural network; deep learning; feature extraction; generative adversarial network; high resolution computer tomography; human; noise reduction; supervised machine learning; tibia; abdomen; aged; diagnostic imaging; female; image processing; male; procedures; very elderly; x-ray computed tomography; Computerized tomography","adversarial learning; Computed tomography (CT); deep learning; noise reduction; residual learning; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85074603019"
"Li P.; Prieto L.; Mery D.; Flynn P.J.","Li, Pei (57193160003); Prieto, Loreto (57208221874); Mery, Domingo (7004430596); Flynn, Patrick J. (35236548000)","57193160003; 57208221874; 7004430596; 35236548000","On Low-Resolution Face Recognition in the Wild: Comparisons and New Techniques","2019","IEEE Transactions on Information Forensics and Security","14","8","8600370","2000","2012","12","10.1109/TIFS.2018.2890812","80","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065657802&doi=10.1109%2fTIFS.2018.2890812&partnerID=40&md5=bdba403a98e9f567b64e2ad50ec36f32","Although face recognition systems have achieved impressive performance in recent years, the low-resolution face recognition task remains challenging, especially when the low-resolution faces are captured under non-ideal conditions, which is widely prevalent in surveillance-based applications. Faces captured in such conditions are often contaminated by blur, non-uniform lighting, and non-frontal face pose. In this paper, we analyze the face recognition techniques using data captured under low-quality conditions in the wild. We provide a comprehensive analysis of the experimental results for two of the most important applications in real surveillance applications, and demonstrate practical approaches to handle both cases that show promising performance. The following three contributions are made: (i) we conduct experiments to evaluate super-resolution methods for low-resolution face recognition; (ii) we study face re-identification on various public face datasets, including real surveillance and low-resolution subsets of large-scale datasets, presenting a baseline result for several deep learning-based approaches, and improve them by introducing a generative adversarial network pre-training approach and fully convolutional architecture; and (iii) we explore the low-resolution face identification by employing a state-of-the-art supervised discriminative learning approach. The evaluations are conducted on challenging portions of the SCface and UCCSface datasets. © 2005-2012 IEEE.","Deep learning; Image resolution; Large dataset; Monitoring; Security systems; Discriminative learning; Face recognition systems; Face recognition technique; Learning-based approach; Low resolution face recognition; Superresolution methods; Surveillance applications; Video surveillance; Face recognition","Face recognition; image resolution; video surveillance","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065657802"
"Li H.; Zhang C.; Li H.; Song N.","Li, Haowei (56286436000); Zhang, Chunxi (7405490901); Li, Huipeng (54782999700); Song, Ningfang (55812783600)","56286436000; 7405490901; 54782999700; 55812783600","White-Light Interference Microscopy Image Super-Resolution Using Generative Adversarial Networks","2020","IEEE Access","8","","8984295","27724","27733","9","10.1109/ACCESS.2020.2971841","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081104278&doi=10.1109%2fACCESS.2020.2971841&partnerID=40&md5=d17d388b6880ad10cae66d89051e8705","To reduce external disturbances and achieve high vertical resolution, the scanning time for white-light interference microscopy is very short. Because capturing high-resolution (HR) images is time consuming, low-resolution (LR) images are acquired instead. However, HR images are more desirable because they contain more details. To ensure high vertical resolution and high image resolution, one feasible solution is to process the scanned LR images to HR images by single image super-resolution (SISR). In this paper, an interference image super-resolution (IISR) model based on a generative adversarial network (GAN) is proposed. The generator is based on the enhanced super-resolution generative adversarial network (ESRGAN) architecture. With the aim of acquiring more realistic images, the discriminator network is designed using a modified DenseNet architecture, in which the pooling layers are replaced with dilated convolutional layers. The perceptual loss is optimized, and the content loss is upgraded to a continuously differentiable piecewise function. Various microscopy images are tested, including images with and without interference fringes. The IISR model has been proven to restore LR images to HR images. The comparative experiments prove that the proposed model achieves better visual quality than other models, preserving more realistic details. © 2013 IEEE.","Fiber optic sensors; Image resolution; Light interference; Network architecture; Optical resolving power; Adversarial networks; Comparative experiments; Continuously differentiable; External disturbances; High vertical resolution; Single images; Visual qualities; White light interference microscopy; Image acquisition","Generative adversarial network; Single image super-resolution; Visual quality; White-light interference microscopy","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081104278"
"Zhao L.; Bai H.; Liang J.; Zeng B.; Wang A.; Zhao Y.","Zhao, Lijun (56508606900); Bai, Huihui (18036828600); Liang, Jie (24802395600); Zeng, Bing (57226110433); Wang, Anhong (7404620368); Zhao, Yao (35304414700)","56508606900; 18036828600; 24802395600; 57226110433; 7404620368; 35304414700","Simultaneous color-depth super-resolution with conditional generative adversarial networks","2019","Pattern Recognition","88","","","356","369","13","10.1016/j.patcog.2018.11.028","55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057566258&doi=10.1016%2fj.patcog.2018.11.028&partnerID=40&md5=e3a39775c985c389daae9fdc4ff7d4f6","In this paper, color-depth conditional generative adversarial networks (CDcGAN) are proposed to resolve the problems of simultaneous color image super-resolution and depth image super-resolution in 3D videos. Firstly, a generative network is presented to leverage the mutual information of the low-resolution color image and low-resolution depth image so that they can enhance each other considering their geometric structural similarity in the same scene. Secondly, three auxiliary losses of data loss, total variation loss, and 8-connected gradient difference loss are introduced to train this generative network to ensure that the generated images are close to the real ones in addition to the adversarial loss. Finally, we study the CDcGAN and its variants. Experimental results show that the proposed approach can produce the high-quality color image and depth image from a pair of low-quality images, and it is superior to several other leading methods. Additionally, it has also been used to resolve the problems of concurrent image smoothing and edge detection, as well as the problem of HR-color-image-guided depth super-resolution to show the effectiveness and universality of the proposed method. © 2018 Elsevier Ltd","Color; Edge detection; Optical resolving power; Adversarial networks; Depth super resolutions; High quality color; Image smoothing; Mutual informations; Structural similarity; Super resolution; Total variation; Image enhancement","Edge detection; Generative adversarial networks; Image smoothing; Super-resolution","Article","Final","","Scopus","2-s2.0-85057566258"
"Fekri M.N.; Ghosh A.M.; Grolinger K.","Fekri, Mohammad Navid (57212870474); Ghosh, Ananda Mohon (57192667884); Grolinger, Katarina (36600134200)","57212870474; 57192667884; 36600134200","Generating energy data for machine learning with recurrent generative adversarial networks","2019","Energies","13","1","130","","","","10.3390/en13010130","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077467183&doi=10.3390%2fen13010130&partnerID=40&md5=a59acd2bfdfa9bd2fa9783212b0a1b02","The smart grid employs computing and communication technologies to embed intelligence into the power grid and, consequently, make the grid more efficient. Machine learning (ML) has been applied for tasks that are important for smart grid operation including energy consumption and generation forecasting, anomaly detection, and state estimation. These ML solutions commonly require sufficient historical data; however, this data is often not readily available because of reasons such as data collection costs and concerns regarding security and privacy. This paper introduces a recurrent generative adversarial network (R-GAN) for generating realistic energy consumption data by learning from real data. Generativea adversarial networks (GANs) have been mostly used for image tasks (e.g., image generation, super-resolution), but here they are used with time series data. Convolutional neural networks (CNNs) from image GANs are replaced with recurrent neural networks (RNNs) because of RNN's ability to capture temporal dependencies. To improve training stability and increase quality of generated data, Wasserstein GANs (WGANs) and Metropolis-Hastings GAN (MH-GAN) approaches were applied. The accuracy is further improved by adding features created with ARIMA and Fourier transform. Experiments demonstrate that data generated by R-GAN can be used for training energy forecasting models. © 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).","Anomaly detection; Electric power transmission networks; Energy utilization; Forecasting; Fourier transforms; Machine learning; Smart power grids; Adversarial networks; ARIMA; Energy data; Energy forecasting; Generative model; Recurrent neural networks","ARIMA; Energy data; Energy forecasting; Fourier transform; Generative adversarial network; Generative model; Recurrent neural network","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85077467183"
"Yang S.; Wang H.; Wang Y.; Li J.; Wang Y.","Yang, Sichen (57212138469); Wang, Huafeng (36783787700); Wang, Yuehai (57194466632); Li, Jintao (8982960600); Wang, Yunhao (57211984288)","57212138469; 36783787700; 57194466632; 8982960600; 57211984288","Super-resolution reconstruction algorithm based on deep learning mechanism and wavelet fusion; [深度学习机制与小波融合的超分辨率重建算法]","2020","Beijing Hangkong Hangtian Daxue Xuebao/Journal of Beijing University of Aeronautics and Astronautics","46","1","","189","197","8","10.13700/j.bh.1001-5965.2019.0146","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078945682&doi=10.13700%2fj.bh.1001-5965.2019.0146&partnerID=40&md5=29b52504f91a3e0bf49517881106e23c","Deep learning technology has developed rapidly in the field of super-resolution reconstruction. In order to further improve the quality and visual effect of reconstructed images, this paper proposes a super-resolution reconstruction based on wavelet transform and generative adversarial networks (GAN) for the unnatural problem of texture reconstruction based on the super-resolution reconstruction algorithm of GAN. In this paper, each component of the wavelet decomposition in the GAN is trained in separate subnets to realize the prediction of wavelet coefficients by the network. Effectively reconstruct high-resolution images with rich global information and local texture details. The experimental results show that the peak signal-to-noise ratio (PSNR) and structural similarity of the objective evaluation index of the reconstructed image can be improved by at least 0.99 dB and 0.031, respectively, based on the algorithm of GAN. © 2020, Editorial Board of JBUAA. All right reserved.","Image enhancement; Image reconstruction; Image texture; Optical resolving power; Signal to noise ratio; Textures; Wavelet decomposition; Wavelet transforms; Adversarial networks; High resolution image; Objective evaluation; Peak signal to noise ratio; Structural similarity; Super resolution reconstruction; Texture reconstruction; Wavelet coefficients; Deep learning","Deep learning; Generative adversarial network (GAN); Mult-iresolution analysis; Super-resolution reconstruction; Wavelet transform","Article","Final","","Scopus","2-s2.0-85078945682"
"Chudasama V.; Upla K.","Chudasama, Vishal (57202047982); Upla, Kishor (53985429600)","57202047982; 53985429600","ISRGAN: Improved Super-Resolution Using Generative Adversarial Networks","2020","Advances in Intelligent Systems and Computing","943","","","109","127","18","10.1007/978-3-030-17795-9_9","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065502490&doi=10.1007%2f978-3-030-17795-9_9&partnerID=40&md5=b03052ff064288201872087dbf1a3ce7","In this paper, we propose an approach for single image super-resolution (SISR) using generative adversarial network (GAN). The SISR has been an attractive research topic over the last two decades and it refers to the reconstruction of a high resolution (HR) image from a single low resolution (LR) observation. Recently, SISR using convolutional neural networks (CNNs) obtained remarkable performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics. Despite this, these methods suffer with a serious drawback in terms of visualization quality of the SR images; the results look overly-smoothed. This is due to the loss function in those methods has a pixel level difference which increases the values of PSNR and SSIM metrics; however the visualization quality is degraded. The GAN has a capability to generate visually appealable solutions. It can also recover the high-frequency texture details due to the discrimination process involved in GAN. Here, we propose improved single image super-resolution using GAN (ISRGAN) with the concept of densely connected deep convolutional networks for image super-resolution. Our proposed method consists two networks: ISRNet and ISRGAN. The ISRNet is trained using MSE based loss function to achieve higher PSNR and SSIM values and ISRGAN is trained by using a combination of VGG based perceptual loss and adversarial loss in order to improve the perceptual quality of the SR images. This training step forces the SR results more towards the natural image manifold. The efficiency of the proposed method is verified by conducting experiments on the different benchmark testing datasets and it shows that the proposed method of ISRGAN outperforms in terms of perception when compared to the other state-of-the-art GAN based SISR techniques. © 2020, Springer Nature Switzerland AG.","Computer vision; Convolution; Image resolution; Neural networks; Optical resolving power; Signal to noise ratio; Textures; Visualization; Convolutional networks; Convolutional neural network; Global residual learning; High resolution image; Image super resolutions; Peak signal to noise ratio; Structural similarity; Super resolution; Image enhancement","Adversarial loss; Densely connected residual network; Global residual learning; Perceptual loss; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85065502490"
"Dong Z.; Fang Y.; Huang L.; Li J.; Qi D.","Dong, Zhekang (56224417300); Fang, Yixiao (57211241290); Huang, Ling (57211553864); Li, Jidong (57211246215); Qi, Donglian (7102278254)","56224417300; 57211241290; 57211553864; 57211246215; 7102278254","A Compact Memristor-based GAN Architecture with a Case Study on Single Image Super-Resolution","2019","Proceedings of the 31st Chinese Control and Decision Conference, CCDC 2019","","","8832460","3069","3074","5","10.1109/CCDC.2019.8832460","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073101352&doi=10.1109%2fCCDC.2019.8832460&partnerID=40&md5=8dca4817628bac2175d00d457c7ebbf3","The generative adversarial network (GAN) is a kind of unsupervised learning approach with the capacity of dealing with the challenge of limited labeled data in the supervised learning world. However, the limited data bandwidth, as well as the performance gap between processing units and memory of the conventional computing platform becomes a major obstacle in the GAN based applications. In this paper, a memristor-based convolution neural network (CNN) unit synthesized with the spintronic memristor crossbar circuit and a general sigmoid activation function circuit is designed for the implementation of convolutional calculation. Notably, multiple memristor-based CNN units can be utilized for the construction of the generator and discriminator respectively. Based on this, a compact GAN architecture composed of the generator block and discriminator block is presented. For verification, the presented memristor-based GAN is applied to the single image super-resolution (SR). The experimental results demonstrate the validity and effectiveness of the entire scheme. © 2019 IEEE.","","Convolutional computation; Generative adversarial network; Single image super-resolution; Spintronic memristor","Conference paper","Final","","Scopus","2-s2.0-85073101352"
"Nam S.H.; Kim Y.H.; Truong N.Q.; Choi J.; Park K.R.","Nam, Se Hyun (57209210189); Kim, Yu Hwan (57211990295); Truong, Noi Quang (57202250049); Choi, Jiho (57215053270); Park, Kang Ryoung (8983316300)","57209210189; 57211990295; 57202250049; 57215053270; 8983316300","Age Estimation by Super-Resolution Reconstruction Based on Adversarial Networks","2020","IEEE Access","8","","8963660","17103","17120","17","10.1109/ACCESS.2020.2967800","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081096331&doi=10.1109%2fACCESS.2020.2967800&partnerID=40&md5=9d6e66ea3c263b494873b726e92612c9","Age estimation using facial images is applicable in various fields, such as age-targeted marketing, analysis of demand and preference for goods, skin care, remote medical service, and age statistics, for describing a specific place. However, if a low-resolution camera is used to capture the images, or facial images are obtained from the subjects standing afar, the resolution of the images is degraded. In such a case, information regarding wrinkles and the texture of the face are lost, and features that are crucial for age estimation cannot be obtained. Existing studies on age estimation did not consider the degradation of resolution but used only high-resolution facial images. To overcome this limitation, this paper proposes a deep convolutional neural network (CNN)-based age estimation method that reconstructs low-resolution facial images as high-resolution images using a conditional generative adversarial network (GAN), and then uses the images as inputs. An experiment is conducted using two open databases (PAL and MORPH databases). The results demonstrate that the proposed method achieves higher accuracy in high-resolution reconstruction and age estimation than the state-of-the art methods. © 2020 IEEE.","Convolutional neural networks; Deep neural networks; Medical imaging; Optical resolving power; Textures; Age estimation; conditional GAN; High resolution image; High-resolution facial image; High-resolution reconstruction; State-of-the-art methods; Super resolution reconstruction; Super-resolution image reconstruction; Image reconstruction","Age estimation; CNN; conditional GAN; super-resolution image reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081096331"
"Lai T.K.; Abbas A.F.; Abdu A.M.; Sheikh U.U.; Mokji M.; Khalil K.","Lai, Tan Kean (57208666001); Abbas, Aymen F. (57208666484); Abdu, Aliyu M. (55646727900); Sheikh, Usman U. (34267792700); Mokji, Musa (8288686800); Khalil, Kamal (47061402100)","57208666001; 57208666484; 55646727900; 34267792700; 8288686800; 47061402100","Super Resolution of Car Plate Images Using Generative Adversarial Networks","2019","Proceedings - 2019 IEEE 15th International Colloquium on Signal Processing and its Applications, CSPA 2019","","","8696010","80","85","5","10.1109/CSPA.2019.8696010","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065486481&doi=10.1109%2fCSPA.2019.8696010&partnerID=40&md5=e9bc5191c5ff35212a8ebbbdea0c413a","Car plate recognition is used in traffic monitoring and control systems such as intelligent parking lot management, finding stolen vehicles, and automated highway toll. In practice, Low-Resolution (LR) images or videos are widely used in surveillance systems. In low resolution surveillance systems, the car plate text is often illegible. Super-Resolution (SR) techniques can be used to improve the car plate quality by processing a series of LR images into a single High-Resolution (HR) image. Recovering the HR image from a single LR is still an ill-conditioned problem for SR. Previous methods always minimize the mean square loss in order to improve the peak signal to noise ratio (PSNR). However, minimizing the mean square loss leads to overly smoothed reconstructed image. In this paper, Generative Adversarial Networks (GANs) based SR is proposed to reconstruct the LR images into HR images. Besides that, perceptual loss is proposed to solve the smoothing issue. The quality of the GAN based SR generated images is compared to existing techniques such as bicubic, nearest and Super-Resolution Convolution Neural Network (SRCNN). The results show that the reconstructed images using GANs based SR achieve better results in term of perceptual quality compared to previous methods. © 2019 IEEE.","Highway traffic control; Image reconstruction; Optical resolving power; Security systems; Signal to noise ratio; Adversarial networks; Convolution neural network; High resolution image; Ill conditioned problems; Low resolution images; Peak signal to noise ratio; Super resolution; Surveillance systems; Image enhancement","Car plate; Generative adversarial networks; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85065486481"
"Wang L.; Chen W.; Yang W.; Bi F.; Yu F.R.","Wang, Lei (57196333972); Chen, Wei (57171121800); Yang, Wenjia (57216500474); Bi, Fangming (23479256400); Yu, Fei Richard (57213980384)","57196333972; 57171121800; 57216500474; 23479256400; 57213980384","A State-of-the-Art Review on Image Synthesis with Generative Adversarial Networks","2020","IEEE Access","8","","9043519","63514","63537","23","10.1109/ACCESS.2020.2982224","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083740108&doi=10.1109%2fACCESS.2020.2982224&partnerID=40&md5=0dc500e541f68a8d9a8ddefa9556e84f","Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios. © 2013 IEEE.","Semantics; Three dimensional computer graphics; 3D face reconstruction; Adversarial networks; Future research directions; Image generations; Image super resolutions; Image translation; Recent researches; State-of-the art reviews; Image processing","cartoon generation; Generative adversarial networks; image editing; image synthesis; image-to-image translation","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083740108"
"Luo H.; Liao G.; Hou X.; Liu B.; Zhou F.; Qiu G.","Luo, Hongming (57214241961); Liao, Guangsen (57214246177); Hou, Xianxu (57194454615); Liu, Bozhi (56828269600); Zhou, Fei (56446120900); Qiu, Guoping (7103292111)","57214241961; 57214246177; 57194454615; 56828269600; 56446120900; 7103292111","VHS to HDTV video translation using multi-task adversarial learning","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11961 LNCS","","","77","86","9","10.1007/978-3-030-37731-1_7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078518391&doi=10.1007%2f978-3-030-37731-1_7&partnerID=40&md5=11b1f8b3968d4e37a3e4ef0335cff0c6","There are large amount of valuable video archives in Video Home System (VHS) format. However, due to the analog nature, their quality is often poor. Compared to High-definition television (HDTV), VHS video not only has a dull color appearance but also has a lower resolution and often appears blurry. In this paper, we focus on the problem of translating VHS video to HDTV video and have developed a solution based on a novel unsupervised multi-task adversarial learning model. Inspired by the success of generative adversarial network (GAN) and CycleGAN, we employ cycle consistency loss, adversarial loss and perceptual loss together to learn a translation model. An important innovation of our work is the incorporation of super-resolution model and color transfer model that can solve unsupervised multi-task problem. To our knowledge, this is the first work that dedicated to the study of the relation between VHS and HDTV and the first computational solution to translate VHS to HDTV. We present experimental results to demonstrate the effectiveness of our solution qualitatively and quantitatively. © 2020, Springer Nature Switzerland AG.","Digital television; High definition television; Adversarial learning; Adversarial networks; Color appearance; Computational solutions; Multitask learning; Super-resolution models; Translation models; Unsupervised; Learning systems","GAN; HDTV; Multi-task learning; Unsupervised; VHS; Video translation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078518391"
"Huang H.; He R.; Sun Z.; Tan T.","Huang, Huaibo (57200614611); He, Ran (35764463900); Sun, Zhenan (8081773300); Tan, Tieniu (7402022125)","57200614611; 35764463900; 8081773300; 7402022125","Wavelet Domain Generative Adversarial Network for Multi-scale Face Hallucination","2019","International Journal of Computer Vision","127","6-7","","763","784","21","10.1007/s11263-019-01154-8","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061478119&doi=10.1007%2fs11263-019-01154-8&partnerID=40&md5=37bc97ec4fd5e774eb515653d9d66840","Most modern face hallucination methods resort to convolutional neural networks (CNN) to infer high-resolution (HR) face images. However, when dealing with very low-resolution (LR) images, these CNN based methods tend to produce over-smoothed outputs. To address this challenge, this paper proposes a wavelet-domain generative adversarial method that can ultra-resolve a very low-resolution (like 16 × 16 or even 8 × 8) face image to its larger version of multiple upscaling factors (2 × to 16 ×) in a unified framework. Different from the most existing studies that hallucinate faces in image pixel domain, our method firstly learns to predict the wavelet information of HR face images from its corresponding LR inputs before image-level super-resolution. To capture both global topology information and local texture details of human faces, a flexible and extensible generative adversarial network is designed with three types of losses: (1) wavelet reconstruction loss aims to push wavelets closer with the ground-truth; (2) wavelet adversarial loss aims to generate realistic wavelets; (3) identity preserving loss aims to help identity information recovery. Extensive experiments demonstrate that the presented approach not only achieves more appealing results both quantitatively and qualitatively than state-of-the-art face hallucination methods, but also can significantly improve identification accuracy for low-resolution face images captured in the wild. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Image coding; Image enhancement; Neural networks; Optical resolving power; Textures; Wavelet transforms; Adversarial networks; Convolutional neural network; Face hallucination; Identification accuracy; Low resolution images; Low-resolution face images; Super resolution; Wavelet reconstruction; Face recognition","Face hallucination; Face recognition; Generative adversarial network; Super-resolution; Wavelet transform","Article","Final","","Scopus","2-s2.0-85061478119"
"Hsu C.-C.; Lin C.-W.; Su W.-T.; Cheung G.","Hsu, Chih-Chung (57212268908); Lin, Chia-Wen (57210925364); Su, Weng-Tai (57189600197); Cheung, Gene (7005809540)","57212268908; 57210925364; 57189600197; 7005809540","SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination","2019","IEEE Transactions on Image Processing","28","12","8751141","6225","6236","11","10.1109/TIP.2019.2924554","52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071898150&doi=10.1109%2fTIP.2019.2924554&partnerID=40&md5=b4fbccdd964b42cfab5fb05923351b16","Though generative adversarial networks (GANs) can hallucinate high-quality high-resolution (HR) faces from low-resolution (LR) faces, they cannot ensure identity preservation during face hallucination, making the HR faces difficult to recognize. To address this problem, we propose a Siamese GAN (SiGAN) to reconstruct HR faces that visually resemble their corresponding identities. On top of a Siamese network, the proposed SiGAN consists of a pair of two identical generators and one discriminator. We incorporate reconstruction error and identity label information in the loss function of SiGAN in a pairwise manner. By iteratively optimizing the loss functions of the generator pair and the discriminator of SiGAN, we not only achieve visually-pleasing face reconstruction but also ensure that the reconstructed information is useful for identity recognition. Experimental results demonstrate that SiGAN significantly outperforms existing face hallucination GANs in objective face verification performance while achieving promising visual-quality reconstruction. Moreover, for input LR faces with unseen identities that are not part of the training dataset, SiGAN can still achieve reasonable performance. © 1992-2012 IEEE.","Image processing; Mathematical models; Adversarial networks; Convolutional neural network; Face hallucination; Generative model; Super resolution; Neural networks","convolutional neural networks; Face hallucination; generative adversarial networks; generative model; super-resolution","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85071898150"
"de Haan K.; Ballard Z.S.; Rivenson Y.; Wu Y.; Ozcan A.","de Haan, Kevin (57205683866); Ballard, Zachary S. (56074697400); Rivenson, Yair (24081473400); Wu, Yichen (57189661297); Ozcan, Aydogan (7005667692)","57205683866; 56074697400; 24081473400; 57189661297; 7005667692","Resolution enhancement in scanning electron microscopy using deep learning","2019","Scientific Reports","9","1","12050","","","","10.1038/s41598-019-48444-2","44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070813036&doi=10.1038%2fs41598-019-48444-2&partnerID=40&md5=077e901a31d80dcddade4fba66fc218f","We report resolution enhancement in scanning electron microscopy (SEM) images using a generative adversarial network. We demonstrate the veracity of this deep learning-based super-resolution technique by inferring unresolved features in low-resolution SEM images and comparing them with the accurately co-registered high-resolution SEM images of the same samples. Through spatial frequency analysis, we also report that our method generates images with frequency spectra matching higher resolution SEM images of the same fields-of-view. By using this technique, higher resolution SEM images can be taken faster, while also reducing both electron charging and damage to the samples. © 2019, The Author(s).","article; controlled study; deep learning; frequency analysis; high resolution scanning electron microscopy; human tissue","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85070813036"
"Gu Y.; Chen Z.; Chen C.; Ye D.","Gu, Yang (57211376663); Chen, Zhaojiong (7409488099); Chen, Can (57211373989); Ye, Dongyi (8950190100)","57211376663; 7409488099; 57211373989; 8950190100","Layout Adjustable Simulated Generation Method for Chinese Landscape Paintings Based on CGAN; [基于CGAN的中国山水画布局可调的仿真生成方法]","2019","Moshi Shibie yu Rengong Zhineng/Pattern Recognition and Artificial Intelligence","32","9","","844","854","10","10.16451/j.cnki.issn1003-6059.201909009","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073683527&doi=10.16451%2fj.cnki.issn1003-6059.201909009&partnerID=40&md5=57b457e56b7930977913479868db4ab3","Creating a complete landscape painting via computer simulation is difficult without studying from global layout viewpoint. To address this issue, a layout-guided Chinese landscape painting simulation method for a complete painting generation is proposed. The characteristics of landscape paintings are taken into account in the design of feasible structures of layout label maps. Composition forms and elements of landscape paintings can be depicted using those structures. On the basis of condition generative adversarial network (CGAN) approach, a multi-scale feature fusion CGAN (MSFF-CGAN) is designed based on layouts and touches of landscape paintings. The proposed network is trained to accomplish heterogeneous transfer from a layout label map to a simulated landscape painting. To deal with rare availability of layout label maps for network training, a color pixel clustering algorithm with semantic correlation is used. In order to enhance the artistic reality of the generated landscape painting, a super resolution network named MemNet is incorporated to refine the texture details. Experimental results show that the proposed method is superior to existing methods in both integrity and artistic reality. Moreover, the proposed method can be used to handle simple graffiti sketches and modify simulated landscape paintings by editing label maps. © 2019, Science Press. All right reserved.","Clustering algorithms; Semantics; Textures; Adversarial networks; Generation method; Label maps; Layout Adjustable; Multi-scale features; Network training; Painting simulation; Super resolution; Painting","Chinese Landscape Painting Simulation; Condition Generative Adversarial Network(CGAN); Layout Adjustable; Layout Label Map; MemNet Network","Article","Final","","Scopus","2-s2.0-85073683527"
"Duan R.; Zhou D.-W.; Zhao L.-J.; Chai X.-L.","Duan, Ran (57210374006); Zhou, Deng-Wen (7403394720); Zhao, Li-Juan (57203573048); Chai, Xiao-Liang (57210378186)","57210374006; 7403394720; 57203573048; 57210378186","Image super-resolution reconstruction based on multi-scale feature mapping network; [基于多尺度特征映射网络的图像超分辨率重建]","2019","Zhejiang Daxue Xuebao (Gongxue Ban)/Journal of Zhejiang University (Engineering Science)","53","7","","1331","1339","8","10.3785/j.issn.1008-973X.2019.07.012","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071921496&doi=10.3785%2fj.issn.1008-973X.2019.07.012&partnerID=40&md5=8ec6813cebe8387a4fcfb02b7a2a75f9","An image super-resolution reconstruction method based on multi-scale feature mapping network was proposed for the problems of shallow network, low utilization rate of features and fuzzy reconstructed images, which existed in the super-resolution convolutional neural network (SRCNN). Multi-scale low-resolution (LR) features were mapped into high-resolution (HR) feature space by learning the mapping relation between LR features and HR features, and the utilization rate of features in the reconstruction process was improved by using feature concatenation. A joint loss function consisting of the pixel-wise loss, the perceptual loss and the adversarial loss was defined, which performed well in restoring the low-frequency content, the sharp edges and the high-frequency textures of the reconstructed images. The experimental results of datasets Set5, Set14 and BSD100 for the upscaling factor 4 were compared with those of state-of-the-art methods. The proposed method performs well in improving the perceptual quality of the reconstructed images in order to achieve clearer edges and textures, and has better scores in the objective evaluation. © 2019, Zhejiang University Press. All right reserved.","Convolution; Deep learning; Deep neural networks; Fuzzy neural networks; Image enhancement; Image texture; Mapping; Optical resolving power; Petroleum reservoir evaluation; Textures; Adversarial networks; Convolutional neural network; Image super-resolution reconstruction; Multi-scale features; Objective evaluation; Reconstruction process; State-of-the-art methods; Super resolution reconstruction; Image reconstruction","Convolutional neural network; Deep learning; Generative adversarial network; Perceptual loss; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85071921496"
"Chen Z.; Hu J.; Zhang X.; Li X.","Chen, Zhiyong (57215550511); Hu, Jing (57145048100); Zhang, Xuyang (57218445291); Li, Xiangjun (57215549882)","57215550511; 57145048100; 57218445291; 57215549882","Single image super-resolution based on enhanced deep residual GAN","2020","Proceedings of SPIE - The International Society for Optical Engineering","11430","","114301W","","","","10.1117/12.2541900","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081132756&doi=10.1117%2f12.2541900&partnerID=40&md5=642b4e8c0f791f341d9cc8b3e07a7365","With the application of image more and more widely, People put forward higher requirements on the image quality of small objects and details in the image. In recent years, with the development of deep learning, it achieved good results in the research of image super-resolution. In this paper, we proposed EDSRGAN, a single image super-resolution(SISR) algorithm, based on enhanced residual network and the adversarial network. Compared with SRGAN, which is also based on the adversarial network, EDSRGAN can greatly reduce the high-frequency noise contained in the super-resolution(SR) image, and it also leads SRGAN in terms of peak signal to noise ratio and structural similarity evaluation indicators. Although EDSRGAN lagged behind EDSR in terms of peak signal to noise ratio and structural similarity, the SR images generated by EDSRGAN were sharper than EDSR in the object edges and targets details. EDSRGAN could achieve good results in image super-resolution on small targets. © SPIE. Downloading of the abstract is permitted for personal use only.","Computer vision; Convolutional neural networks; Deep learning; Image quality; Image resolution; Optical resolving power; Signal to noise ratio; Adversarial networks; High-frequency noise; Image super resolutions; Peak signal to noise ratio; Single images; Small targets; Structural similarity; Super resolution; Image enhancement","Convolutional neural networks; Generative adversarial networks; Residual network; Single image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85081132756"
"Han J.; Wang C.","Han, Jun (57209633331); Wang, Chaoli (57203797020)","57209633331; 57203797020","Tsr-Tvd: Temporal super-resolution for time-varying data analysis and visualization","2020","IEEE Transactions on Visualization and Computer Graphics","26","1","8802285","205","215","10","10.1109/TVCG.2019.2934255","34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075637808&doi=10.1109%2fTVCG.2019.2934255&partnerID=40&md5=ea07bcf527e88fbfa638cf30c67978f6","We present TSR-TVD, a novel deep learning framework that generates temporal super-resolution (TSR) of time-varying data (TVD) using adversarial learning. TSR-TVD is the first work that applies the recurrent generative network (RGN), a combination of the recurrent neural network (RNN) and generative adversarial network (GAN), to generate temporal high-resolution volume sequences from low-resolution ones. The design of TSR-TVD includes a generator and a discriminator. The generator takes a pair of volumes as input and outputs the synthesized intermediate volume sequence through forward and backward predictions. The discriminator takes the synthesized intermediate volumes as input and produces a score indicating the realness of the volumes. Our method handles multivariate data as well where the trained network from one variable is applied to generate TSR for another variable. To demonstrate the effectiveness of TSR-TVD, we show quantitative and qualitative results with several time-varying multivariate data sets and compare our method against standard linear interpolation and solutions solely based on RNN or CNN. © 1995-2012 IEEE.","Data visualization; Deep learning; Optical resolving power; Visualization; Adversarial networks; Forward-and-backward; Linear Interpolation; Multivariate data sets; Recurrent neural network (RNN); Super resolution; Temporal super resolution; Time-varying data visualization; article; data analysis; deep learning; prediction; quantitative analysis; recurrent neural network; Recurrent neural networks","deep learning; recurrent generative network; super-resolution; Time-varying data visualization","Article","Final","","Scopus","2-s2.0-85075637808"
"Zhu X.; Zhang L.; Zhang L.; Liu X.; Shen Y.; Zhao S.; Perez-Cisneros M.","Zhu, Xining (57207256346); Zhang, Lin (54796088600); Zhang, Lijun (39863815100); Liu, Xiao (57195942480); Shen, Ying (56146148600); Zhao, Shengjie (56328424800); Perez-Cisneros, Marco (57516895400)","57207256346; 54796088600; 39863815100; 57195942480; 56146148600; 56328424800; 57516895400","GAN-Based Image Super-Resolution with a Novel Quality Loss","2020","Mathematical Problems in Engineering","2020","","5217429","","","","10.1155/2020/5217429","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081169345&doi=10.1155%2f2020%2f5217429&partnerID=40&md5=4e0494c9689f61c34e32efdebb58a176","Single image super-resolution (SISR) has been a very attractive research topic in recent years. Breakthroughs in SISR have been achieved due to deep learning and generative adversarial networks (GANs). However, the generated image still suffers from undesired artifacts. In this paper, we propose a new method named GMGAN for SISR tasks. In this method, to generate images more in line with human vision system (HVS), we design a quality loss by integrating an image quality assessment (IQA) metric named gradient magnitude similarity deviation (GMSD). To our knowledge, it is the first time to truly integrate an IQA metric into SISR. Moreover, to overcome the instability of the original GAN, we use a variant of GANs named improved training of Wasserstein GANs (WGAN-GP). Besides GMGAN, we highlight the importance of training datasets. Experiments show that GMGAN with quality loss and WGAN-GP can generate visually appealing results and set a new state of the art. In addition, large quantity of high-quality training images with rich textures can benefit the results. © 2020 Xining Zhu et al.","Deep learning; Image quality; Optical resolving power; Textures; Adversarial networks; Gradient magnitude; Human vision systems; Image quality assessment (IQA); Image super resolutions; Research topics; State of the art; Training data sets; Quality management","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081169345"
"Xu Y.; Li X.","Xu, Yanjie (57215119289); Li, Xin (56193364000)","57215119289; 56193364000","Research on face super resolution reconstruction based on DBGAN","2019","Proceedings - 2019 4th International Conference on Mechanical, Control and Computer Engineering, ICMCCE 2019","","","8969526","807","812","5","10.1109/ICMCCE48743.2019.00185","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079290128&doi=10.1109%2fICMCCE48743.2019.00185&partnerID=40&md5=ca6ff06ec1669d351951dcf88f4a0507","Super-resolution reconstruction technology is an important research topic in many fields such as image processing and computer vision. This technology can be used widely for security monitoring, old image reconstruction, image compression transmission and other fields. In this paper, super-resolution image reconstruction is performed on a low-resolution image of four times magnification. We propose the dense convolutional networks used as a generator instead of residual networks, and set perceptual loss as the optimization goal. We use the VGG network feature map as the loss function instead of Mean Square Error, which combines the perceptual loss with the adversarial loss and is beneficial for compensating the shortcomings of previous methods that lack high frequency detail. Experimental results show that our method can produce clearer face images than the traditional methods. These reconstructed images have higher resolution and Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM) than the images generated by the deep residual networks. © 2019 IEEE.","Convolution; Image compression; Mean square error; Optical resolving power; Signal to noise ratio; Adversarial networks; Convolutional networks; Image processing and computer vision; Low resolution images; Peak signal to noise ratio; Structural similarity indices (SSIM); Super resolution reconstruction; Super-resolution image reconstruction; Image reconstruction","Dense convolutional network; Generative adversarial network; Perceptual loss; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85079290128"
"Ghosh S.S.; Hua Y.; Mukherjee S.S.; Robertson N.","Ghosh, Soumya Shubhra (57213865162); Hua, Yang (57196115171); Mukherjee, Sankha Subhra (57192180712); Robertson, Neil (7102941208)","57213865162; 57196115171; 57192180712; 7102941208","IEGAN: Multi-purpose perceptual quality image enhancement using generative adversarial network","2019","Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019","","","8658804","11","20","9","10.1109/WACV.2019.00009","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063589044&doi=10.1109%2fWACV.2019.00009&partnerID=40&md5=9d8fcbf4091b80596a282409ced73fc2","Despite the breakthroughs in quality of image enhancement, an end-to-end solution for simultaneous recovery of the finer texture details and sharpness for degraded images with low resolution is still unsolved. Some existing approaches focus on minimizing the pixel-wise reconstruction error which results in a high peak signal-to-noise ratio. The enhanced images fail to provide high-frequency details and are perceptually unsatisfying, i.e., they fail to match the quality expected in a photo-realistic image. In this paper, we present Image Enhancement Generative Adversarial Network (IEGAN), a versatile framework capable of inferring photo-realistic natural images for both artifact removal and super-resolution simultaneously. Moreover, we propose a new loss function consisting of a combination of reconstruction loss, feature loss and an edge loss counterpart. The feature loss helps to push the output image to the natural image manifold and the edge loss preserves the sharpness of the output image. The reconstruction loss provides low-level semantic information to the generator regarding the quality of the generated images compared to the original. Our approach has been experimentally proven to recover photo-realistic textures from heavily compressed low-resolution images on public benchmarks and our proposed high-resolution World100 dataset. © 2019 IEEE","Computer vision; Image texture; Semantics; Signal to noise ratio; Textures; Adversarial networks; End-to-end solutions; Low resolution images; Peak signal to noise ratio; Perceptual quality; Photorealistic images; Reconstruction error; Semantic information; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85063589044"
"Yu H.; Sa H.; Zou D.; Mao J.; Sheng W.","Yu, Hui (57214397235); Sa, Haitao (57211272418); Zou, Dafang (57211265463); Mao, Jiafa (36864218000); Sheng, Weiguo (56122524000)","57214397235; 57211272418; 57211265463; 36864218000; 56122524000","A Super-Resolution Generative Adversarial Network with Simplified Gradient Penalty and Relativistic Discriminator","2019","Proceedings of the International Joint Conference on Neural Networks","2019-July","","8852251","","","","10.1109/IJCNN.2019.8852251","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073248707&doi=10.1109%2fIJCNN.2019.8852251&partnerID=40&md5=a1f3917872918c4160d0949705c76db5","Generative Adversarial Network (GAN) has been employed for single image super-resolution (SISR). However, unregularized GAN is difficult for training. This is due gradient descent based GAN optimization is not easy to convergence, thus limiting its performance for image super-resolution. In this paper, a relativistic super-resolution GAN with a simplified gradient penalty (RSRGAN-GP) is proposed for single image super-resolution. In the proposed method, a compact residual network optimized by removing Batch-Normalization layers is employed as the generator to estimate photo-realistic images of 4× upscaling. Further, we introduce a residual network, which also has no Batch-Normalization layers as the conditional discriminator and adopt a simplified gradient regularization to penalize it for stabilizing the super-resolution GAN training, thus guaranteeing high-quality image reconstruction. Additionally, the super-resolution GAN is enhanced with a relativistic discriminator, which produces sharp and rich-detail images at no extra computational cost. The results on benchmark datasets show that our proposed method can effectively improve the visual quality of super-resolved images and achieves competitive performance compared with related works. © 2019 IEEE.","Benchmarking; Gradient methods; Image enhancement; Image reconstruction; Optical resolving power; Optimization; Adversarial networks; Benchmark datasets; Competitive performance; Computational costs; Image super resolutions; Photorealistic images; Super resolution; Visual qualities; Discriminators","Generative Adversarial Network; Gradient Penalty; Relativistic Discriminator; Residual Network; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85073248707"
"Zong L.; Chen L.","Zong, Lujie (57218104033); Chen, Lianna (56106457000)","57218104033; 56106457000","Single image super-resolution based on self-attention","2019","2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence, ICUSAI 2019","","","9124791","56","60","4","10.1109/ICUSAI47366.2019.9124791","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087913019&doi=10.1109%2fICUSAI47366.2019.9124791&partnerID=40&md5=66013007730521cf11ce3c629690e82f","Single image super-resolution (SISR) is a challenging task and has collected extensive attention in both industry and academia. The most challenging problem in super-resolution is how to recover the finer texture details. And we find the generated images usually have low-scale contrast. In this paper, we present a novel super-resolution method based on generative adversarial networks (GAN). Our model is based on SRGAN, instead, we remove most of the batch normalization layers in generator to get higher-scale contrast images and accelerate training speed. Because batch normalization layers can get rid of range flexibility from networks, this causes the generated images have lower-scale contrast compared to the origin images. We also add the self-attention module into generator to get more global dependencies when convolution operations can capture enough local dependencies but little global dependencies. We take advantage of both local dependencies and global dependencies to improve super-resolved texture details and structural, we call our model SASRGAN. The images generated by our model have higher Structural Similarity Index Measure (SSIM), it proves that our model has available performance. © 2019 IEEE.","Optical resolving power; Textures; Adversarial networks; Single images; Structural similarity index measures (SSIM); Super resolution; Superresolution methods; Training speed; Artificial intelligence","Batch normalization; Generative adversarial networks; Self-attention; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85087913019"
"Hosangadi R.; Adiga D.; Vyeth V.","Hosangadi, Raunak (57194612114); Adiga, Darshan (57212340740); Vyeth, Viveka (57212341467)","57194612114; 57212340740; 57212341467","OCR-friendly image synthesis using generative adversarial networks","2019","ACM International Conference Proceeding Series","","","","226","234","8","10.1145/3373509.3373580","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082718556&doi=10.1145%2f3373509.3373580&partnerID=40&md5=828121f6a0a3481f3dd02a09a014ab4b","With quality being a deciding factor in the accuracy of OCR predictions for a given source image, there comes the need of preprocessing methods to improve the quality of an image before undergoing this process. Towards that, we present a GAN-based method targeted at improving the quality of source image in the fields of image resolution, blur and noise. The model uses an encoder trained to generate latent image representations for a lowquality image, the representations corresponding to blur and noise types present in the image. These representations act as inputs to the constructed conditional GAN. Besides these labels, the generator takes a low-quality image as input and is trained to generate a high-quality image as represented by the target images. The discriminator makes use of a standard OCR score to measure the performance of the generator in order to achieve images that are more OCR-friendly than the given target image. The model uses perceptual losses for training to produce clear images and improve convergence. Improving of image resolution is made by the use of super-resolution by means of sub-pixel convolution. Finally, the paper describes a series of experiments run on the model, as well as the results to show improvements in OCR performance using a single mode; for images suffering a variety of distortions. The results are quantified based on multiple performance measures and the final analysis is presented. © 2019 Association for Computing Machinery.","Convolution; Image resolution; Optical character recognition; Pixels; Adversarial networks; Autoencoders; High quality images; Image synthesis; Multiple performance measures; Pre-processing method; Sub pixels; Super resolution; Image enhancement","Autoencoders; Generative adversarial networks; Optical character recognition; Sub-pixel convolution.","Conference paper","Final","","Scopus","2-s2.0-85082718556"
"","","","2nd Chinese Conference on Pattern Recognition and Computer Vision, PRCV 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11858 LNCS","","","","","805","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076949633&partnerID=40&md5=e23f9b962c8fcb68a9f8f8772a575288","The proceedings contain 68 papers. The special focus in this conference is on Pattern Recognition and Computer Vision. The topics include: Functional brain network estimation based on weighted BOLD signals for MCI identification; ESNet: An efficient symmetric network for real-time semantic segmentation; assignment problem based deep embedding; auto data augmentation for testing set; dense activation network for image denoising; the optimal graph regularized sparse coding with application to image representation; robust embedding regression for face recognition; deep feature-preserving based face hallucination: Feature discrimination versus pixels approximation; Lung parenchymal segmentation algorithm based on improved marker watershed for lung CT images; Fine grain lung nodule diagnosis based on CT using 3D convolutional neural network; segmentation guided regression network for breast cancer cellularity; automatic inspection of yarn locations by utilizing histogram segmentation and monotone hypothesis; membranous nephropathy identification using hyperspectral microscopic images; a level set method combined with gaussian mixture model for image segmentation; nonstandard periodic gait energy image for gait recognition and data augmentation; a temporal attentive approach for video-based pedestrian attribute recognition; An effective network with convLSTM for low-light image enhancement; self-calibrating scene understanding based on motifnet; BDGAN: Image blind denoising using generative adversarial networks; single image reflection removal based on deep residual learning; an automated method with attention network for cervical cancer scanning; graph-based scale-aware network for human parsing; semi-supervised lesion detection with reliable label propagation and missing label mining; image aesthetic assessment based on perception consistency; Image de-noising by an effective SURE-based weighted bilateral filtering; face super-resolution via discriminative-attributes.","","","Conference review","Final","","Scopus","2-s2.0-85076949633"
"Xi Y.; Zheng J.; Jia W.; He X.; Li H.; Ren Z.; Lam K.-M.","Xi, Yue (57193703373); Zheng, Jiangbin (7403975791); Jia, Wenjing (7202412193); He, Xiangjian (7404409118); Li, Hanhui (56431577600); Ren, Zhuqiang (57216130092); Lam, Kin-Man (55839377100)","57193703373; 7403975791; 7202412193; 7404409118; 56431577600; 57216130092; 55839377100","See Clearly in the Distance: Representation Learning GAN for Low Resolution Object Recognition","2020","IEEE Access","8","","9026982","53203","53214","11","10.1109/ACCESS.2020.2978980","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082617893&doi=10.1109%2fACCESS.2020.2978980&partnerID=40&md5=efcbc5fd47f6a15c587e5778925bada7","Identifying tiny objects with extremely low resolution is generally considered a very challenging task even for human vision, due to limited information presented inside the object areas. There have been very limited attempts in recent years to deal with low-resolution recognition. The existing solutions rely on either generating super-resolution images or learning multi-scale features. However, their performance improvement becomes very limited, especially when the resolution becomes very low. In this paper, we propose a Representation Learning Generative Adversarial Network (RL-GAN) to generate super image representation that is optimized for recognition. Our solution deals with the classical vision task of object recognition in the distance. We evaluate our idea on the challenging task of low-resolution object recognition. Comparison of experimental results conducted on public and our newly created WIDER-SHIP datasets demonstrate the effectiveness of our RL-GAN, which improves the classification results significantly, with 10-15% gain on average, compared with benchmark solutions. © 2013 IEEE.","Classification (of information); Convolutional neural networks; Adversarial networks; Benchmark solutions; Classification results; Image representations; Limited information; Low resolution; Multi-scale features; representation learning; Object recognition","Convolutional neural networks; generative adversarial networks; low resolution object recognition; representation learning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85082617893"
"Zhang N.; Wang Y.; Zhang X.; Xu D.; Wang X.","Zhang, Ning (57188816117); Wang, Yongcheng (56437944700); Zhang, Xin (57774426900); Xu, Dongdong (56299205100); Wang, Xiaodong (57208088951)","57188816117; 56437944700; 57774426900; 56299205100; 57208088951","An Unsupervised Remote Sensing Single-Image Super-Resolution Method Based on Generative Adversarial Network","2020","IEEE Access","8","","8986554","29027","29039","12","10.1109/ACCESS.2020.2972300","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079744090&doi=10.1109%2fACCESS.2020.2972300&partnerID=40&md5=e9a0ab049e0b5d66745dacf6396fc44f","Image super-resolution (SR) technique can improve the spatial resolution of images without upgrading the imaging system. As a result, SR promotes the development of high resolution (HR) remote sensing image applications. Many remote sensing image SR algorithms based on deep learning have been proposed recently, which can effectively improve the spatial resolution under the constraints of HR images. However, images acquired by remote sensing imaging devices typically have lower resolution. Hence, an insufficient number of HR remote sensing images are available for training deep neural networks. In view of this problem, we propose an unsupervised SR method that does not require HR remote sensing images. The proposed method introduces a generative adversarial network (GAN) that obtains SR images through the generator; then, the SR images are downsampled to train the discriminator with low resolution (LR) images. Our method outperformed several methods in terms of the quality of the obtained SR images as measured by 6 evaluation metrics, which proves the satisfactory performance of the proposed unsupervised method for improving the spatial resolution of remote sensing images. © 2013 IEEE.","Deep learning; Deep neural networks; Image resolution; Optical resolving power; Remote sensing; Unsupervised learning; Adversarial networks; Evaluation metrics; Image super resolutions; Low resolution images; Remote sensing images; Remote sensing imaging; Spatial resolution; Unsupervised method; Image enhancement","generative adversarial network; Image super-resolution; remote sensing; unsupervised learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85079744090"
"Deng X.; Yang R.; Xu M.; Dragotti P.L.","Deng, Xin (56071493400); Yang, Ren (57191155373); Xu, Mai (55703599800); Dragotti, Pier Luigi (6701786340)","56071493400; 57191155373; 55703599800; 6701786340","Wavelet domain style transfer for an effective perception-distortion tradeoff in single image super-resolution","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9008402","3076","3085","9","10.1109/ICCV.2019.00317","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081931338&doi=10.1109%2fICCV.2019.00317&partnerID=40&md5=dc3d02af2808b912175278fbe4ee11eb","In single image super-resolution (SISR), given a low-resolution (LR) image, one wishes to find a high-resolution (HR) version of it which is both accurate and photorealistic. Recently, it has been shown that there exists a fundamental tradeoff between low distortion and high perceptual quality, and the generative adversarial network (GAN) is demonstrated to approach the perception-distortion (PD) bound effectively. In this paper, we propose a novel method based on wavelet domain style transfer (WDST), which achieves a better PD tradeoff than the GAN based methods. Specifically, we propose to use 2D stationary wavelet transform (SWT) to decompose one image into low-frequency and high-frequency sub-bands. For the low-frequency sub-band, we improve its objective quality through an enhancement network. For the high-frequency sub-band, we propose to use WDST to effectively improve its perceptual quality. By feat of the perfect reconstruction property of wavelets, these sub-bands can be re-combined to obtain an image which has simultaneously high objective and perceptual quality. The numerical results on various datasets show that our method achieves the best trade-off between the distortion and perceptual quality among the existing state-of-the-art SISR methods. © 2019 IEEE.","Computer vision; Economic and social effects; Numerical methods; Optical resolving power; Adversarial networks; High frequency HF; Low resolution images; Numerical results; Objective qualities; Perceptual quality; Perfect reconstruction; Stationary wavelet transforms; Wavelet transforms","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081931338"
"Kudo A.; Kitamura Y.; Li Y.; Iizuka S.; Simo-Serra E.","Kudo, Akira (57211795680); Kitamura, Yoshiro (55223413500); Li, Yuanzhong (23005518900); Iizuka, Satoshi (57197724901); Simo-Serra, Edgar (55303749400)","57211795680; 55223413500; 23005518900; 57197724901; 55303749400","Virtual Thin Slice: 3D Conditional GAN-based Super-Resolution for CT Slice Interval","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11905 LNCS","","","91","100","9","10.1007/978-3-030-33843-5_9","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076209725&doi=10.1007%2f978-3-030-33843-5_9&partnerID=40&md5=fe4a9147d940d222d5190a21445084cc","Many CT slice images are stored with large slice intervals to reduce storage size in clinical practice. This leads to low resolution perpendicular to the slice images (i.e., z-axis), which is insufficient for 3D visualization or image analysis. In this paper, we present a novel architecture based on conditional Generative Adversarial Networks (cGANs) with the goal of generating high resolution images of main body parts including head, chest, abdomen and legs. However, GANs are known to have a difficulty with generating a diversity of patterns due to a phenomena known as mode collapse. To overcome the lack of generated pattern variety, we propose to condition the discriminator on the different body parts. Furthermore, our generator networks are extended to be three dimensional fully convolutional neural networks, allowing for the generation of high resolution images from arbitrary fields of view. In our verification tests, we show that the proposed method obtains the best scores by PSNR/SSIM metrics and Visual Turing Test, allowing for accurate reproduction of the principle anatomy in high resolution. We expect that the proposed method contribute to effective utilization of the existing vast amounts of thick CT images stored in hospitals. © Springer Nature Switzerland AG 2019.","Cell proliferation; Computer aided instruction; Computer vision; Computerized tomography; Deep learning; Machine learning; Medical image processing; Neural networks; Optical resolving power; Three dimensional computer graphics; 3D Visualization; Adversarial networks; Clinical practices; Convolutional neural network; High resolution image; Novel architecture; Super resolution; Verification tests; Image reconstruction","Computed tomography; Computer vision; Deep learning; Generative Adversarial Network; Super resolution","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85076209725"
"Sood R.; Rusu M.","Sood, Rewa (57207113139); Rusu, Mirabela (25628776200)","57207113139; 25628776200","Anisotropic super resolution in prostate mri using super resolution generative adversarial networks","2019","Proceedings - International Symposium on Biomedical Imaging","2019-April","","8759237","1688","1691","3","10.1109/ISBI.2019.8759237","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073901067&doi=10.1109%2fISBI.2019.8759237&partnerID=40&md5=8f1c4b5802c39a7e71cf93ad949623d8","Acquiring High Resolution (HR) Magnetic Resonance (MR) images requires the patient to remain still for long periods of time, which causes patient discomfort and increases the probability of motion induced image artifacts. A possible solution is to acquire low resolution (LR) images and to process them with the Super Resolution Generative Adversarial Network (SRGAN) to create a super-resolved version. This work applies SRGAN to MR images of the prostate and performs three experiments. The first experiment explores improving the in-plane MR image resolution by factors of 4 and 8, and shows that, while the PSNR and SSIM (Structural SIMilarity) metrics are lower than the isotropic bicubic interpolation baseline, the SRGAN is able to create images that have high edge fidelity. The second experiment explores anisotropic super-resolution via synthetic images, in that the input images to the network are anisotropically downsampled versions of HR images. This experiment demonstrates the ability of the modified SRGAN to perform anisotropic super-resolution, with quantitative image metrics that are comparable to those of the anisotropic bicubic interpolation baseline. Finally, the third experiment applies a modified version of the SRGAN to super-resolve anisotropic images obtained from the through-plane slices of the volumetric MR data. The output super-resolved images contain a significant amount of high frequency information that make them visually close to their HR counterparts. Overall, the promising results from each experiment show that super-resolution for MR images is a successful technique and that producing isotropic MR image volumes from anisotropic slices is an achievable goal. © 2019 IEEE.","Anisotropy; Image resolution; Interpolation; Learning systems; Magnetic resonance imaging; Medical imaging; Optical resolving power; Urology; Adversarial networks; Anisotropic; Bicubic interpolation; High-frequency informations; Low resolution images; Quantitative images; Structural similarity; Super resolution; Image enhancement","Anisotropic; Generative networks; Machine learning; Magnetic resonance imaging; Super resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85073901067"
"Zhang H.; Fang C.; Xie X.; Yang Y.; Mei W.; Jin D.; Fei P.","Zhang, Hao (57193765753); Fang, Chunyu (57193764423); Xie, Xinlin (57203185207); Yang, Yicong (57208147002); Mei, Wei (35810798800); Jin, Di (57192958536); Fei, Peng (57008844400)","57193765753; 57193764423; 57203185207; 57208147002; 35810798800; 57192958536; 57008844400","High-throughput, high-resolution deep learning microscopy based on registration-free generative adversarial network","2019","Biomedical Optics Express","10","3","#342882","1044","1063","19","10.1364/BOE.10.001044","60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063940154&doi=10.1364%2fBOE.10.001044&partnerID=40&md5=1d764ebcbf4feb28dd80907b2970352a","                             We combine a generative adversarial network (GAN) with light microscopy to achieve deep learning super-resolution under a large field of view (FOV). By appropriately adopting prior microscopy data in an adversarial training, the neural network can recover a high-resolution, accurate image of new specimen from its single low-resolution measurement. Its capacity has been broadly demonstrated via imaging various types of samples, such as USAF resolution target, human pathological slides, fluorescence-labelled fibroblast cells, and deep tissues in transgenic mouse brain, by both wide-field and light-sheet microscopes. The gigapixel, multi-color reconstruction of these samples verifies a successful GAN-based single image super-resolution procedure. We also propose an image degrading model to generate low resolution images for training, making our approach free from the complex image registration during training data set preparation. After a well-trained network has been created, this deep learning-based imaging approach is capable of recovering a large FOV (~95 mm                             2                             ) enhanced resolution of ~1.7 μm at high speed (within 1 second), while not necessarily introducing any changes to the setup of existing microscopes.                          © 2019 Optical Society of America under the terms of the OSA Open Access Publishing Agreement.","Cell culture; Optical resolving power; Adversarial networks; Enhanced resolutions; Fibroblast cells; High resolution; Large field of views; Low resolution images; Super resolution; Training data sets; animal cell; animal tissue; Article; BPAEC cell line; cell counting; deep learning; fluorescence imaging; fluorescence microscopy; generative adversarial network; histology; human; human tissue; image analysis; image processing; image quality; image reconstruction; image segmentation; microscopy; nerve cell network; neuroanatomy; nonhuman; phototoxicity; prostate cancer; refraction index; signal noise ratio; three dimensional imaging; training; tumor cell; tumor invasion; Deep learning","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85063940154"
"Zhang T.","Zhang, Tianji (57210340438)","57210340438","Research and Improvement of Single Image Super-Resolution Based on Generative Adversarial Network","2019","Journal of Physics: Conference Series","1237","3","032046","","","","10.1088/1742-6596/1237/3/032046","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070350281&doi=10.1088%2f1742-6596%2f1237%2f3%2f032046&partnerID=40&md5=b26ac7d5383680d17614bf42ef7aa523","This paper proposes a new image super-resolution method based on Generative Adversarial Network (GAN). Firstly, the algorithm model includes generating model and discriminant model, generating model to generate high-resolution image, discriminating model to distinguish the image true or false, the original image is true, and the generated image is false. Using alternate training method, the generated model and discriminant model achieve Nash equilibrium, and finally generate high-quality image. Compared with previous super-resolution method based on generative adversarial network (SRGAN), the following changes have been made: modifying the network structure, removing the unnecessary batch normalization layer in the standard residual block, deepening the network layer number and improving the loss function. The experimental results show that compared with the traditional bicubic interpolation method and compared with SRGAN, the proposed algorithm improves the actual image effect, peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) in varying degrees. © 2019 IOP Publishing Ltd. All rights reserved.","Intelligent computing; Network layers; Optical resolving power; Signal to noise ratio; Adversarial networks; Bicubic interpolation; Discriminant models; High resolution image; Image super resolutions; Peak signal to noise ratio; Structural similarity indices (SSIM); Superresolution methods; Image enhancement","","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85070350281"
"Wang Y.D.; Armstrong R.T.; Mostaghimi P.","Wang, Ying Da (57207002693); Armstrong, Ryan T. (37025484700); Mostaghimi, Peyman (35759133600)","57207002693; 37025484700; 35759133600","Boosting Resolution and Recovering Texture of 2D and 3D Micro-CT Images with Deep Learning","2020","Water Resources Research","56","1","e2019WR026052","","","","10.1029/2019WR026052","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078485951&doi=10.1029%2f2019WR026052&partnerID=40&md5=028574711bfb0e2191a07e6e2fd5938e","Simulation of flow directly at the pore scale depends on high-quality digital rock images but is constrained by detector hardware. A trade-off between the image field of view (FOV) and image resolution is made. This can be compensated for with superresolution (SR) techniques that take a wide FOV, low-resolution (LR) image, and superresolve a high resolution (HR). The Enhanced Deep Super Resolution Generative Adversarial Network (EDSRGAN) is trained on the DeepRock-SR, a diverse compilation of raw and processed micro-computed tomography ((μCT) images in 2D and 3D. The 2D and 3D trained networks show comparable performance of 50% to 70% reduction in relative error over bicubic interpolation with minimal computational cost during usage. Texture regeneration with EDSRGAN shows superior visual similarity versus Super Resolution Convolutional Neural Network (SRCNN) and other methods. Difference maps show SRCNN recovers large-scale edge features while EDSRGAN regenerates perceptually indistinguishable high-frequency texture. Physical accuracy is measured by permeability and phase topology on consistently segmented images, showing EDSRGAN results achieving the closest match. Performance is generalized with augmentation, showing high adaptability to noise and blur. HR images are fed into the network, generating HR-SR images to extrapolate network performance to subresolution features present in the HR images themselves. Underresolution features are regenerated despite operating outside of trained specifications. Comparison with scanning electron microscopy (SEM) images shows details are consistent with the underlying geometry. Images that are normally constrained by the mineralogy of the rock, by fast transient imaging, or by the energy of the source can be superresolved accurately for further analysis. ©2019. American Geophysical Union. All Rights Reserved.","Deep learning; Economic and social effects; Image enhancement; Image resolution; Minerals; Neural networks; Optical resolving power; Scanning electron microscopy; Textures; Adversarial networks; Bicubic interpolation; Convolutional neural network; Low resolution images; Microcomputed tomography; Scanning electron microscopy image; Subresolution features; Superresolution technique; accuracy assessment; field of view; image resolution; interpolation; performance assessment; scanning electron microscopy; three-dimensional modeling; tomography; topology; two-dimensional modeling; Computerized tomography","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078485951"
"Ferdous S.N.; Mostofa M.; Nasrabadi N.M.","Ferdous, Syeda Nyma (57211070813); Mostofa, Moktari (57211073809); Nasrabadi, Nasser M. (7006312852)","57211070813; 57211073809; 7006312852","Super resolution-Assisted deep aerial vehicle detection","2019","Proceedings of SPIE - The International Society for Optical Engineering","11006","","1100617","","","","10.1117/12.2519045","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072575189&doi=10.1117%2f12.2519045&partnerID=40&md5=3bf858847ea71f792183b3f48dec03d1","Vehicle detection in aerial imagery has become tremendously a challenging task due to the low resolution characteristics of the aerial images. Super-Resolution; a technique which recovers high-resolution image from a single low-resolution image can be an effective approach to resolve this shortcoming. Hence, our prime focus is to design a framework for detecting vehicles in super resolved aerial images. Our proposed system can be represented as a combination of two deep sub-networks. The first sub-network aims to use a Generative Adversarial Network (GAN) for getting super resolved images. A GAN consists of two networks: A generator network and a discriminator network. It ensures recovery of photo-realistic images from down-sampled images. The second sub-network consists of a deep neural network (DNN)-based object detector for detecting vehicles in super resolved images. In our architecture, the Single Shot Multi Box Detector (SSD) is used for vehicle detection. The SSD generates fixed-size bounding boxes with predicting scores for different object class instances in those boxes. It also employs a non-maximum suppression step to produce final detections. In our algorithm, our deep SSD detector is trained with the predicted super resolved images and its performance is then compared with an SSD detector that is trained only on the low-resolution images. Finally, we compare the performance of our proposed pre-Trained SSD detector on super-resolved images with an SSD that is trained only on the original high resolution images. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Aerial photography; Antennas; Deep neural networks; Machine learning; Object detection; Optical resolving power; Vehicles; Adversarial networks; Aerial vehicle; High resolution image; Low resolution images; Non-maximum suppression; Photorealistic images; Single shots; Super resolution; Aircraft detection","Aerial vehicle detection; Single Shot Detector (SSD); SRGAN (SR with Generative Adversarial Network); Super Resolution (SR)","Conference paper","Final","","Scopus","2-s2.0-85072575189"
"Chen Y.; Li J.; Niu Y.; He J.","Chen, Yiting (57213552005); Li, Jie (55885602000); Niu, Yifeng (56222112100); He, Jingbo (57219624929)","57213552005; 55885602000; 56222112100; 57219624929","Small Object Detection Networks Based on Classification-Oriented Super-Resolution GAN for UAV Aerial Imagery","2019","Proceedings of the 31st Chinese Control and Decision Conference, CCDC 2019","","","8832735","4610","4615","5","10.1109/CCDC.2019.8832735","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073117284&doi=10.1109%2fCCDC.2019.8832735&partnerID=40&md5=ccec2d2a895ee070ffed20dfd1d6b3e9","Small object detection is challenging for lack of discriminative information compared to medium, large objects. In this paper, we proposed small object detection networks based on classification-oriented super-resolution generative adversarial networks (CSRGAN) and validate them on public UAV aerial imagery benchmark. Through appending classification branch and introducing classification loss to typical SRGAN, generator of CSRGAN is trained to reconstruct realistic super-resolved (SR) images with classification-oriented discriminative features from low resolution images while discriminator is trained to predict true categories and distinguish generated SR images from original ones. Besides, VGG19 based feature-level content loss is applied to recover clearer and sharper contours in SR images, which is critical to object classification. Experiment results prove the classification-oriented enhancing effect of CSRGAN and the positive function of VGG19 based feature-level content loss. © 2019 IEEE.","","Generative Adversarial Networks; Object Classification; Object Detection; Super-resolution; UAV Aerial Imagery","Conference paper","Final","","Scopus","2-s2.0-85073117284"
"Ma W.; Pan Z.; Yuan F.; Lei B.","Ma, Wen (57207877267); Pan, Zongxu (54788169800); Yuan, Feng (57214435272); Lei, Bin (14063767500)","57207877267; 54788169800; 57214435272; 14063767500","Super-resolution of remote sensing images via a dense residual generative adversarial network","2019","Remote Sensing","11","21","2578","","","","10.3390/rs11212578","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074641655&doi=10.3390%2frs11212578&partnerID=40&md5=bca7c02fd281744f4a4761f26f63a6d7","Single image super-resolution (SISR) has been widely studied in recent years as a crucial technique for remote sensing applications. In this paper, a dense residual generative adversarial network (DRGAN)-based SISR method is proposed to promote the resolution of remote sensing images. Different from previous super-resolution (SR) approaches based on generative adversarial networks (GANs), the novelty of our method mainly lies in the following factors. First, we made a breakthrough in terms of network architecture to improve performance. We designed a dense residual network as the generative network in GAN, which can make full use of the hierarchical features from low-resolution (LR) images. We also introduced a contiguous memory mechanism into the network to take advantage of the dense residual block. Second, we modified the loss function and altered the model of the discriminative network according to theWasserstein GAN with a gradient penalty (WGAN-GP) for stable training. Extensive experiments were performed using the NWPU-RESISC45 dataset, and the results demonstrated that the proposed method outperforms state-of-the-art methods in terms of both objective evaluation and subjective perspective. © 2019 by the authors.","Network architecture; Optical resolving power; Adversarial networks; Dense residual network (DRN); Remote sensing images; Single images; Wasserstein GAN with gradient penalty (WGAN-GP); Remote sensing","Dense residual network (DRN); Generative adversarial network (GAN); Remote sensing images; Single image super-resolution (SISR); Wasserstein GAN with gradient penalty (WGAN-GP)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85074641655"
"Han J.; Zhang Z.; Mao A.; Zhou Y.","Han, Jian (57210809852); Zhang, Zijie (57210803277); Mao, Ailing (57202466601); Zhou, Yuan (57191652248)","57210809852; 57210803277; 57202466601; 57191652248","Semantics Images Synthesis and Resolution Refinement Using Generative Adversarial Networks","2020","Lecture Notes in Electrical Engineering","516","","","612","620","8","10.1007/978-981-13-6504-1_74","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071466315&doi=10.1007%2f978-981-13-6504-1_74&partnerID=40&md5=ee8f6d27ca06c29bb670fe2a2c1ddabb","In this paper, we proposed a method to synthesizing a super-resolution image with the given image and text descriptions. Our work contains two parts. Wasserstein GAN is used to generate low-level resolution image under the guidance of a novel loss function. Then, a convolution net is followed to refine the resolution. This is an end-to-end network architecture. We have validated our model on Caltech-200 bird dataset, Oxford-102 flower dataset, and BSD300 dataset. The experiments show that the generated images not only match the given descriptions well but also maintain detailed features of original images with a higher resolution. © 2020, Springer Nature Singapore Pte Ltd.","Network architecture; Semantic Web; Semantics; Adversarial networks; Caltech; Higher resolution; Images synthesis; Loss functions; Original images; Resolution images; Super resolution; Image processing","Generative Adversarial Networks (GANs); Resolution refinement; Semantics images synthesis","Conference paper","Final","","Scopus","2-s2.0-85071466315"
"","","","4th International Workshop on Simulation and Synthesis in Medical Imaging, SASHIMI 2019, held in conjunction with the 22nd International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11827 LNCS","","","","","160","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075685543&partnerID=40&md5=d02e23c98fb2596fdfe145b0997971e3","The proceedings contain 16 papers. The special focus in this conference is on International Workshop on Simulation and Synthesis in Medical Imaging, held in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention. The topics include: Physics-informed brain MRI segmentation; 3D medical image synthesis by factorised representation and deformable model learning; cycle-consistent training for reducing negative jacobian determinant in deep registration networks; ISMORE: An iterative self super-resolution algorithm; an optical model of whole blood for detecting platelets in lens-free images; Evaluation of the realism of an MRI simulator for stroke lesion prediction using convolutional neural network; Improved MR to CT synthesis for pet/mr attenuation correction using imitation learning; Unpaired multi-contrast MR image synthesis using generative adversarial networks; unsupervised retina image synthesis via disentangled representation learning; Pseudo-normal PET synthesis with generative adversarial networks for localising hypometabolism in epilepsies; breast mass detection in mammograms via blending adversarial learning; Tunable CT lung nodule synthesis conditioned on background image and semantic features; mask2Lesion: Mask-constrained adversarial skin lesion image synthesis; towards annotation-free segmentation of fluorescently labeled cell membranes in confocal microscopy images.","","","Conference review","Final","","Scopus","2-s2.0-85075685543"
"Chen S.; Li S.; Zhu C.","Chen, Sheng (57203904413); Li, Sumei (55741035700); Zhu, Chengcheng (57221460307)","57203904413; 55741035700; 57221460307","Advanced Generative Adversarial Network Based on Dense Connection for Single Image Super Resolution","2019","NSENS 2019 - 2nd IEEE International Conference on Micro/Nano Sensors for Al, Healthcare, and Robotics","","","9293953","68","71","3","10.1109/NSENS49395.2019.9293953","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099237968&doi=10.1109%2fNSENS49395.2019.9293953&partnerID=40&md5=a3f6ff302ae4f031b710c9886edad778","The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating more realistic texture in semantics and style during single image super-resolution. However, Since the loss function adopts L2 norm based on pixel points, the hallucinated details are often accompanied with unpleasant artifacts even false pixels. Our model adjusts generative loss to L1 norm, and perceptual loss is still based on L2 norm. L1 cost function can reduce the coefficients of some features to zero, thus indirectly realizing the selection of features according to the perceptual loss, and obtaining more real texture features. The combination of these two loss functions ensures that the reconstructed results of the model are very close to the target image in terms of spatial features, high-level abstract features and semantic features, overall sensory and image quality. The generating network of our model is based on dense residual structure, and the dense connection of residual-in-residual is used to implement fast and accurate learning of high frequency features of images. The adversarial network is based on the structure of discriminators in DCGAN and WGAN. Experimental results show that subjective quality we reconstructed is much higher than SRGAN. © 2019 IEEE.","Agricultural robots; Cost functions; Health care; Optical resolving power; Pixels; Robotics; Semantics; Textures; Adversarial networks; High frequency HF; Residual structure; Semantic features; Spatial features; Subjective quality; Super resolution; Texture features; Microrobots","convolution neural network; generating adversarial network; loss function; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85099237968"
"Tao Y.; Muller J.-P.","Tao, Y. (56539197700); Muller, J.-P. (7404871794)","56539197700; 7404871794","Super-resolution restoration of spaceborne HD videos using the UCL MAGiGAN system","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","1115508","","","","10.1117/12.2532889","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078198191&doi=10.1117%2f12.2532889&partnerID=40&md5=828817e6907aad33d8482ac8726058cf","We developed a novel SRR system, called Multi-Angle Gotcha image restoration with Generative Adversarial Network (MAGiGAN), to produce resolution enhancement of 3-5 times from multi-pass EO images. The MAGiGAN SRR system uses a combination of photogrammetric and machine vision approaches including image segmentation and shadow labelling, feature matching and densification, estimation of an image degradation model, and deep learning approaches, to retrieve image information from distorted features and training networks. We have tested the MAGiGAN SRR using the NVIDIA® Jetson TX-2 GPU card for onboard processing within a smart-satellite capturing high definition satellite videos, which will enable many innovative remote-sensing applications to be implemented in the future. In this paper, we show SRR processing results from a Planet® SkySat HD 70cm spaceborne video using a GPU version of the MAGiGAN system. Image quality and effective resolution enhancement are measured and discussed. © 2019 SPIE.","Deep learning; Digital television; Earth (planet); Graphics processing unit; Image enhancement; Image segmentation; Optical resolving power; Remote sensing; Restoration; Video signal processing; Adversarial networks; Earth observations; HD videos; MAGiGAN; Multi angle; Super-resolution restoration; Image reconstruction","Earth Observation; Generative Adversarial Network; MAGiGAN; Multi-angle; Planet® SkySat® HD Video; Super-Resolution Restoration","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078198191"
"","","","Chinese Intelligent Systems Conference, CISC 2019","2020","Lecture Notes in Electrical Engineering","593","","","","","2246","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072960505&partnerID=40&md5=939784cba7c0120a2e3db2c4c0349f7b","The proceedings contain 240 papers. The special focus in this conference is on Chinese Intelligent Systems. The topics include: Atmospheric Turbulence Rejection Control for a Geared Turbofan Engine; kalman-Filter-Group Based Aero-Engine Sensors Fault Diagnosis and Verification; hypersonic Periodic Cruise Trajectory Optimization Based on Flexible Use of Pseudo-spectral Method; research on Global Stability Control Strategy of Under-Actuated Crane System Based on Equivalent Input Interference; truck Size Measurement System Based on Computer Vision; Dual Space Vector Control Study of Two Stage Matrix Converter Fed PMSM; visual Pose Estimation Based on the DenseNet Network; Kinematics Numerical Inverse Solution Algorithm for Arm Type AFP Machine Based on D-H Modeling; key Methods of Recognizing Container Number Automatically Using Video Stream in Intelligent Tally; General Correntropy Based DOA Estimation for Coherent Source with Impulsive Noise; Design and Research of 100KV DC Power Supply Based on SiC; Design and Simulation Analysis of NA-PMSM Based on ANSYS; A Novel Hybrid Model of Wind Speed Forecasting Based on EWT, BiLSTM, SVR Optimized by BOA in Inner Mongolia, China; A Novel Bearing Intelligent Fault Diagnosis Method Based on Modified VMD and 1-D CNN; the Design and Implementation of Simple Corexy Structure Writing Robot; the Research and Design of Intelligent Monitoring and Management Platform for River and Lake Water Environment; a Parametric Mesh Generation Method for Aero-Engine Compressor Blades; centrifuge Fault Diagnosis Method Based on Random Forests Algorithm; medical Image Super-Resolution Based on the Generative Adversarial Network; stability of Single Species Dynamics Chained-Form System with Stage Structure via Finite-Time Control.","","","Conference review","Final","","Scopus","2-s2.0-85072960505"
"Zhang Y.; Lin H.; Guan Y.; Liu C.","Zhang, Yangyi (57213175059); Lin, Hong (57198047109); Guan, Yuhua (57213175434); Liu, Chun (57200871924)","57213175059; 57198047109; 57213175434; 57200871924","GAN image super-resolution reconstruction model with improved residual block and adversarial loss; [改进残差块和对抗损失的GAN图像超分辨率重建]","2019","Harbin Gongye Daxue Xuebao/Journal of Harbin Institute of Technology","51","11","","128","137","9","10.11918/j.issn.0367-6234.201812115","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077646110&doi=10.11918%2fj.issn.0367-6234.201812115&partnerID=40&md5=bfa1934695310d3bb70587f2839a5df1","Image super-resolution (SR) reconstruction is an important image processing technology to improve the resolution of image and video in computer vision. Image reconstruction model based on deep learning has not been satisfactory due to the too many layers involved, the excessively long training time resulting from difficult gradient transmission, and the unsatisfactory reconstructed image. This paper proposes a generative adversarial networks (GAN) image SR reconstruction model with improved residual block and adversarial loss. Firstly, on the model structure, the residual blocks of the excess batch normalization were designed and combined into a generative model, and the deep convolution network was used as the discriminant model to control the training direction of the reconstructed image to reduce the model's calculation amount. Then, in the loss function, the Earth-Mover distance was designed to alleviate model gradient disappearance. The L1 distance was used as the measure of the degree of similarity between the reconstructed image and the high-resolution image to guide the model weight update to improve the reconstructed visual effect. Experimental results from the DIV2K, Set5, and Set14 datasets demonstrate that compared with the model before improvement, the training time of the proposed model was reduced by about 14% and the image reconstruction effect was effectively improved. For the loss function combined with Earth-Mover distance and L1 distance, gradient disappearance was effectively alleviated. Therefore, the proposed model significantly improved the SR reconstruction efficiency and visual effect of low-resolution images compared with Bicubic, SRCNN, VDSR, and DSRN model. © 2019, Editorial Board of Journal of Harbin Institute of Technology. All right reserved.","Deep learning; Image enhancement; Optical resolving power; Adversarial networks; Earth mover distances; Image processing technology; Image super resolutions; Image super-resolution reconstruction; Low resolution images; Reconstruction efficiency; Super resolution; Image reconstruction","Adversarial loss; Deep learning; Earth-Mover distance; Generative Adversarial Networks (GAN); Super-resolution construction","Article","Final","","Scopus","2-s2.0-85077646110"
"Wang Z.; Li J.; Enoh M.","Wang, Zheng (57192240243); Li, Jianwu (36163299400); Enoh, Mogendi (57205457387)","57192240243; 36163299400; 57205457387","Removing ring artifacts in CBCT images via generative adversarial networks with unidirectional relative total variation loss","2019","Neural Computing and Applications","31","9","","5147","5158","11","10.1007/s00521-018-04007-6","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060094053&doi=10.1007%2fs00521-018-04007-6&partnerID=40&md5=f49a0cf5c12b6c09b0c865a88f8dee79","Cone beam computed tomography (CBCT) is an important tool for clinical diagnosis and many industrial applications. However, ring artifacts usually appear in CBCT images, due to device responding inconsistence. This paper designs a generative adversarial network (GAN) to remove ring artifacts and meanwhile to retain important texture details in CBCT images. This method firstly transforms ring artifacts in Cartesian coordinates to stripe artifacts in polar coordinates, which is very helpful for removing ring artifacts. Then, we design a new loss function for GAN, including three parts: unidirectional relative total variation loss, perceptual loss and adversarial loss. Further, inspired by super-resolution generative adversarial networks, we use very deep residual networks for both generator and discriminator. Experimental results show that the proposed method is more effective for ring artifacts removal, compared to our baseline and some traditional methods. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Diagnosis; Optical resolving power; Adversarial networks; Cartesian coordinate; Clinical diagnosis; Cone-beam computed tomography; Polar coordinate; Ring artifacts; Super resolution; Total variation; Computerized tomography","Cone beam computed tomography; Generative adversarial networks; Ring artifacts; Super-resolution generative adversarial networks","Article","Final","","Scopus","2-s2.0-85060094053"
"Zhou D.; Duan R.; Zhao L.; Chai X.","Zhou, Dengwen (7403394720); Duan, Ran (57210374006); Zhao, Lijuan (57203573048); Chai, Xiaoliang (57210378186)","7403394720; 57210374006; 57203573048; 57210378186","Single image super-resolution reconstruction based on multi-scale feature mapping adversarial network","2020","Signal Processing","166","","107251","","","","10.1016/j.sigpro.2019.107251","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070891999&doi=10.1016%2fj.sigpro.2019.107251&partnerID=40&md5=c932aa95610b1f123c4161ac39ac50b1","Single image super-resolution (SISR) aims to reconstruct a high-resolution image from a degraded low-resolution image. In recent years, the super-resolution methods based on convolutional neural network (CNN) have achieved promising performance on SISR task, indicating that CNN is a viable approach to image super-resolution reconstruction. The one limitation of the current SISR methods is that many methods use the pixel-wise loss. It is well known that the pixel-wise loss cannot well recover high-frequency details even if the high peak signal-to-noise ratio (PSNR) can be obtained. Some other methods purely focus on restoring more details, which resulted in poor PSNR score and high-frequency noise. In this paper, we proposed a multi-component loss function based on pixel-wise loss, perceptual loss and adversarial loss for a multi-scale feature mapping generator network for SISR image reconstruction model. We evaluated our method on commonly used benchmarks and compared it with other SISR methods. The results showed that our method could achieve the better balance between the high-frequency detail and stable spatial structure generation. © 2019","Deep learning; Mapping; Neural networks; Optical resolving power; Pixels; Signal to noise ratio; Adversarial networks; Convolutional neural network; Image super-resolution reconstruction; Low resolution images; Peak signal to noise ratio; Single-image super-resolution reconstruction; Super resolution; Superresolution methods; Image reconstruction","Deep learning; Generative Adversarial Network; Image restoration; Perceptual loss; Super-resolution","Article","Final","","Scopus","2-s2.0-85070891999"
"Raj A.; Li Y.; Bresler Y.","Raj, Ankit (57215770491); Li, Yuqi (57215782203); Bresler, Yoram (7006091153)","57215770491; 57215782203; 7006091153","GAN-based projector for faster recovery with convergence guarantees in linear inverse problems","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9009825","5601","5610","9","10.1109/ICCV.2019.00570","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081898761&doi=10.1109%2fICCV.2019.00570&partnerID=40&md5=829b63d6eebd3f318d3e789a90fe46bb","A Generative Adversarial Network (GAN) with generator G trained to model the prior of images has been shown to perform better than sparsity-based regularizers in ill-posed inverse problems. Here, we propose a new method of deploying a GAN-based prior to solve linear inverse problems using projected gradient descent (PGD). Our method learns a network-based projector for use in the PGD algorithm, eliminating expensive computation of the Jacobian of G. Experiments show that our approach provides a speed-up of 60-80x over earlier GAN-based recovery methods along with better accuracy in compressed sensing. Our main theoretical result is that if the measurement matrix is moderately conditioned on the manifold range(G) and the projector is delta-approximate, then the algorithm is guaranteed to reach O(delta) reconstruction error in O(log(1/delta)) steps in the low noise regime. Additionally, we propose a fast method to design such measurement matrices for a given G. Extensive experiments demonstrate the efficacy of this method by requiring 5-10x fewer measurements than random Gaussian measurement matrices for comparable recovery performance. Because the learning of the GAN and projector is decoupled from the measurement operator, our GAN-based projector and recovery algorithm are applicable without retraining to all linear inverse problems in which the measurement operator is moderately conditioned for range(G), as confirmed by experiments on compressed sensing, super-resolution, and inpainting. © 2019 IEEE.","Compressed sensing; Computer system recovery; Computer vision; Differential equations; Gradient methods; Jacobian matrices; Learning algorithms; Recovery; Adversarial networks; Gaussian measurements; ILL-posed inverse problem; Linear inverse problems; Measurement matrix; Reconstruction error; Recovery algorithms; Recovery performance; Inverse problems","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081898761"
"Ma D.; Afonso M.; Zhang F.; Bull D.R.","Ma, Di (57203993409); Afonso, Mariana (57194234663); Zhang, Fan (55495274600); Bull, David R. (57203057734)","57203993409; 57194234663; 55495274600; 57203057734","Perceptually-inspired super-resolution of compressed videos","2019","Proceedings of SPIE - The International Society for Optical Engineering","11137","","1113717","","","","10.1117/12.2530688","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077302971&doi=10.1117%2f12.2530688&partnerID=40&md5=b1aa5702c906913c5bd597d19a1e9ee1","Spatial resolution adaptation is a technique which has often been employed in video compression to enhance coding efficiency. This approach encodes a lower resolution version of the input video and reconstructs the original resolution during decoding. Instead of using conventional up-sampling filters, recent work has employed advanced super-resolution methods based on convolutional neural networks (CNNs) to further improve reconstruction quality. These approaches are usually trained to minimise pixel-based losses such as Mean-Squared Error (MSE), despite the fact that this type of loss metric does not correlate well with subjective opinions. In this paper, a perceptually-inspired super-resolution approach (M-SRGAN) is proposed for spatial up-sampling of compressed video using a modified CNN model, which has been trained using a generative adversarial network (GAN) on compressed content with perceptual loss functions. The proposed method was integrated with HEVC HM 16.20, and has been evaluated on the JVET Common Test Conditions (UHD test sequences) using the Random Access configuration. The results show evident perceptual quality improvement over the original HM 16.20, with an average bitrate saving of 35.6% (Bjøntegaard Delta measurement) based on a perceptual quality metric, VMAF. © 2019 SPIE.","Image resolution; Mean square error; Neural networks; Optical resolving power; Signal sampling; Video signal processing; Adversarial networks; Convolutional neural network; HEVC; Reconstruction quality; Spatial resolution; Super resolution; Superresolution methods; Up-sampling filter; Image compression","Generative adversarial networks; HEVC; Perceptual superresolution; Spatial resolution adaptation; Video compression","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077302971"
"Liu Q.; Lv H.; Jiang R.","Liu, Qiao (57193611768); Lv, Hairong (14037821600); Jiang, Rui (57200775668)","57193611768; 14037821600; 57200775668","HicGAN infers super resolution Hi-C data with generative adversarial networks","2019","Bioinformatics","35","14","btz317","i99","i107","8","10.1093/bioinformatics/btz317","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068931350&doi=10.1093%2fbioinformatics%2fbtz317&partnerID=40&md5=a0d4d002caa89875bb084f10327284aa","Motivation: Hi-C is a genome-wide technology for investigating 3D chromatin conformation by measuring physical contacts between pairs of genomic regions. The resolution of Hi-C data directly impacts the effectiveness and accuracy of downstream analysis such as identifying topologically associating domains (TADs) and meaningful chromatin loops. High resolution Hi-C data are valuable resources which implicate the relationship between 3D genome conformation and function, especially linking distal regulatory elements to their target genes. However, high resolution Hi-C data across various tissues and cell types are not always available due to the high sequencing cost. It is therefore indispensable to develop computational approaches for enhancing the resolution of Hi-C data. Results: We proposed hicGAN, an open-sourced framework, for inferring high resolution Hi-C data from low resolution Hi-C data with generative adversarial networks (GANs). To the best of our knowledge, this is the first study to apply GANs to 3D genome analysis. We demonstrate that hicGAN effectively enhances the resolution of low resolution Hi-C data by generating matrices that are highly consistent with the original high resolution Hi-C matrices. A typical scenario of usage for our approach is to enhance low resolution Hi-C data in new cell types, especially where the high resolution Hi-C data are not available. Our study not only presents a novel approach for enhancing Hi-C data resolution, but also provides fascinating insights into disclosing complex mechanism underlying the formation of chromatin contacts. © 2019 The Author(s) 2019. Published by Oxford University Press.","Chromatin; Genome; Genomics; Molecular Conformation; Software; chromatin; conference paper; genome analysis; chromatin; conformation; genome; genomics; software","","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85068931350"
"Ray A.; Sharma M.; Upadhyay A.; Makwana M.; Chaudhury S.; Trivedi A.; Singh A.; Saini A.","Ray, Anupama (55511748995); Sharma, Manoj (57217742709); Upadhyay, Avinash (57205628981); Makwana, Megh (57212453914); Chaudhury, Santanu (7005182122); Trivedi, Akkshita (57215092025); Singh, Ajay (57209902495); Saini, Anil (26221927300)","55511748995; 57217742709; 57205628981; 57212453914; 7005182122; 57215092025; 57209902495; 26221927300","An end-to-end trainable framework for joint optimization of document enhancement and recognition","2019","Proceedings of the International Conference on Document Analysis and Recognition, ICDAR","","","8978188","59","64","5","10.1109/ICDAR.2019.00019","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079858638&doi=10.1109%2fICDAR.2019.00019&partnerID=40&md5=ac421ef84d8067d1b8b5961bb2225f30","Recognizing text from degraded and low-resolution document images is still an open challenge in the vision community. Existing text recognition systems require a certain resolution and fails if the document is of low-resolution or heavily degraded or noisy. This paper presents an end-to-end trainable deep-learning based framework for joint optimization of document enhancement and recognition. We are using a generative adversarial network (GAN) based framework to perform image denoising followed by deep back projection network (DBPN) for super-resolution and use these super-resolved features to train a bidirectional long short term memory (BLSTM) with Connectionist Temporal Classification (CTC) for recognition of textual sequences. The entire network is end-to-end trainable and we obtain improved results than state-of-the-art for both the image enhancement and document recognition tasks. We demonstrate results on both printed and handwritten degraded document datasets to show the generalization capability of our proposed robust framework. © 2019 IEEE.","Deep learning; Image denoising; Image enhancement; Optical character recognition; Optical resolving power; Adversarial networks; Degraded documents; Document images; Document recognition; End to end; Generalization capability; Temporal classification; Vision communities; Character recognition","Document enhancement; Document image super-resolution; End-to-End training; GAN; OCR","Conference paper","Final","","Scopus","2-s2.0-85079858638"
"Cai J.; Hu H.; Shan S.; Chen X.","Cai, Jiancheng (57210356235); Hu, Han (30267595700); Shan, Shiguang (22235341500); Chen, Xilin (8284171300)","57210356235; 30267595700; 22235341500; 8284171300","FCSR-GAN: End-to-end learning for joint face completion and super-resolution","2019","Proceedings - 14th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2019","","","8756607","","","","10.1109/FG.2019.8756607","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070469922&doi=10.1109%2fFG.2019.8756607&partnerID=40&md5=9c0df21bef21e8347e8357618fdc8b03","Combined variations such as low-resolution and occlusion often present in face images in the wild, e.g., under the scenario of video surveillance. While most of the existing face enhancement approaches only handle one type of variation per model, in this paper, we propose a deep generative adversarial network (FCSR-GAN) for joint face completion and face superresolution via one model. The generator of FCSR-GAN aims to recover a high-resolution face image without occlusion given an input low-resolution face image with partial occlusions. The discriminator of FCSR-GAN consists of two adversarial losses, a perceptual loss, and a face parsing loss, which assure the high quality of the recovered face images. Experimental results on several public-domain databases (CelebA and Helen) show that the proposed approach outperforms the state-of-the-art methods in jointly doing face super-resolution (up to 4×) and face completion from low-resolution face images with occlusions. © 2019 IEEE.","Optical resolving power; Security systems; Adversarial networks; Face super-resolution; High resolution; Low-resolution face images; Partial occlusions; State-of-the-art methods; Super resolution; Video surveillance; Gesture recognition","","Conference paper","Final","","Scopus","2-s2.0-85070469922"
"Jiang W.; Luo X.","Jiang, Wenjie (57210322716); Luo, Xiaoshu (7402870658)","57210322716; 7402870658","Research on super-resolution reconstruction algorithm of remote sensing image based on generative adversarial networks","2019","Proceedings of 2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering, AUTEEE 2019","","","9033352","438","441","3","10.1109/AUTEEE48671.2019.9033352","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083255066&doi=10.1109%2fAUTEEE48671.2019.9033352&partnerID=40&md5=41de8d3169fbce26074b67d30d347923","Due to natural conditions and hardware manufacturing processes, the resolution of remote sensing images is generally low. Obtaining high-definition remote sensing images by simply improving hardware and manufacturing processes is not only costly and technically challenging but also cannot be deployed on a large scale. Aiming at the limitations of the traditional methods, this paper studies the image super-resolution reconstruction method for improving the generated anti-network. Firstly, the generator network is optimized, and an RRDB (Residual-in-Residual Dense without BN (Batch Normalization) is used. Block) module; secondly, the related idea of relativistic GAN (relativistic generative adversarial network) is introduced, the relative value of the discriminator is not the absolute value; finally, the sensation loss is improved, and the feature is used before the function is activated. The test results show that the proposed algorithm is better than SRGAN (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network), SRCNN (Super-Resolution Convolutional Neural Network) and FSRCNN (Fast Super-Resolution Convolutional Neural Network). The clarity of the reconstructed image is improved, and the reconstructed image quality is significantly improved. © 2019 IEEE.","Convolution; Convolutional neural networks; Image enhancement; Manufacture; Optical resolving power; Remote sensing; Adversarial networks; Image super-resolution reconstruction; Manufacturing process; Natural conditions; Reconstructed image; Remote sensing images; Super resolution; Super resolution reconstruction; Image reconstruction","Aerial image; Generative Adversarial Networks; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85083255066"
"Huang Q.; Li W.; Hu T.; Tao R.","Huang, Qian (57210124335); Li, Wei (56215159000); Hu, Ting (57212948752); Tao, Ran (7102857080)","57210124335; 56215159000; 57212948752; 7102857080","Hyperspectral Image Super-resolution Using Generative Adversarial Network and Residual Learning","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8683893","3012","3016","4","10.1109/ICASSP.2019.8683893","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069455619&doi=10.1109%2fICASSP.2019.8683893&partnerID=40&md5=85cc31b6adcef332903c828fbe7093aa","Due to the limitation of image acquisition, hyperspectral remote sensing imagery is hard to reflect in both high spatial and spectral resolutions. Super-resolution (SR) is a technique which can improve the spatial resolution. Inspired by recent achievements in deep convolutional neural network (CNN) and generative adversarial network (GAN), a GAN based framework is proposed for hyperspectral image super-resolution. In the proposed method, residual learning is used to obtain a high metrics and spectral fidelity, and a shorter connection is set between the input layer and output layer. The gradient features from low-resolution (LR) image to high-resolution (HR) are utilized as auxiliary information to assist deep CNN to carry out counter training with discriminator. Experimental results demonstrate that the proposed SR algorithm achieves superior performance in spectral fidelity and spatial resolution compared with baseline methods. © 2019 IEEE.","Deep neural networks; Image resolution; Neural networks; Optical resolving power; Remote sensing; Spectroscopy; Speech communication; Adversarial networks; Auxiliary information; Convolutional neural network; Hyper-spectral imageries; Hyperspectral remote sensing; Image super resolutions; Low resolution images; Super resolution; Audio signal processing","Generative Adversarial Network; Hyperspectral Imagery; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85069455619"
"Eskimez S.E.; Koishida K.; Duan Z.","Eskimez, Sefik Emre (57189600042); Koishida, Kazuhito (6601990788); Duan, Zhiyao (24450312900)","57189600042; 6601990788; 24450312900","Adversarial Training for Speech Super-Resolution","2019","IEEE Journal on Selected Topics in Signal Processing","13","2","8681126","347","358","11","10.1109/JSTSP.2019.2909077","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065997911&doi=10.1109%2fJSTSP.2019.2909077&partnerID=40&md5=c1d3ec4ce85dc7eadc59f9c87dfdf635","Speech super-resolution or speech bandwidth expansion aims to upsample a given speech signal by generating the missing high-frequency content. In this paper, we propose a deep neural network approach exploiting the adversarial training ideas that have been shown effective in image super-resolution. Specifically, our proposed network follows the generative adversarial networks setup, where the generator network uses a convolutional autoencoder architecture with one-dimensional convolution kernels to generate high-frequency log-power spectra from the low-frequency log-power spectra of the input speech. We propose to use both the reconstruction loss and the adversarial loss for training, and we employ a recent regularization method that penalizes the gradient norms of the discriminator to stabilize the training. We compare our proposed approach with two state-of-the-art neural network baselines and evaluate these methods with both objective speech quality measures and subjective perceptual and intelligibility tests. Results show that our proposed method outperforms both baselines in terms of both objective and subjective evaluations. To gain insights of the network architecture, we analyze key parameters of the proposed network including the number of layers, the number of convolution kernels, and the relative weight of the reconstruction and adversarial losses. Besides, we analyze the computational complexity of our method and the baselines and discuss ways for phase estimation. We further develop a noise-resilient version of the proposed approach by training the network with noisy speech inputs. Objective evaluation validates the noise-resilient property on unseen noise types. © 2007-2012 IEEE.","Bandwidth; Convolution; Deep neural networks; Neural networks; One dimensional; Optical resolving power; Power spectrum; Quality control; Speech intelligibility; Speech processing; Adversarial networks; Bandwidth expansions; Convolutional neural network; Image super resolutions; Objective and subjective evaluations; Objective speech quality measures; Regularization methods; Super resolution; Network architecture","1D convolutional neural networks; artificial bandwidth expansion; generative adversarial networks; speech processing; Speech super-resolution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85065997911"
"Ding Z.; Liu X.-Y.; Yin M.; Kong L.","Ding, Zihan (57203112741); Liu, Xiao-Yang (44361326100); Yin, Miao (57650324700); Kong, Linghe (35345242100)","57203112741; 44361326100; 57650324700; 35345242100","Tensor Super-Resolution with Generative Adversarial Nets: A Large Image Generation Approach","2019","Communications in Computer and Information Science","1072","","","209","223","14","10.1007/978-981-15-1398-5_15","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076971878&doi=10.1007%2f978-981-15-1398-5_15&partnerID=40&md5=e84602c017da2e287d31de89aa9e20e6","Deep generative models have been successfully applied to many applications. However, existing methods experience limitations when generating large images (the literature usually generates small images, e.g., 32\32 or 128\128). In this paper, we propose a novel scheme using tensor super-resolution with adversarial generative nets (TSRGAN), to generate large high-quality images by exploring tensor structures. Essentially, the super resolution process of TSRGAN is based on tensor representation. First, we impose tensor structures for concise image representation, which is superior in capturing the pixel proximity information and the spatial patterns of elementary objects in images, over the vectorization preprocess in existing works. Secondly, we propose TSRGAN that integrates deep convolutional generative adversarial networks and tensor super-resolution in a cascading manner, to generate high-quality images from random distributions. More specifically, we design a tensor super-resolution process that consists of tensor dictionary learning and tensor coefficients learning. Finally, on three datasets, the proposed TSRGAN generates images with more realistic textures, compared with state-of-the-art adversarial autoencoders and super-resolution methods. The size of the generated images is increased by over 8.5 times, namely in PASCAL2. © 2019, Springer Nature Singapore Pte Ltd.","Artificial intelligence; Brain; Optical resolving power; Textures; Adversarial networks; Generative model; Image representations; Random distribution; Sparse coding; Super resolution; Superresolution methods; Tensor representation; Tensors","GAN; Generative model; Super-resolution; Tensor representation; Tensor sparse coding","Conference paper","Final","","Scopus","2-s2.0-85076971878"
"Liu Z.; Gao F.; Wang Y.","Liu, Zhibo (57208718322); Gao, Feng (55842372100); Wang, Yizhou (57194437986)","57208718322; 55842372100; 57194437986","A Generative Adversarial Network for AI-Aided Chair Design","2019","Proceedings - 2nd International Conference on Multimedia Information Processing and Retrieval, MIPR 2019","","","8695313","486","490","4","10.1109/MIPR.2019.00098","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065627800&doi=10.1109%2fMIPR.2019.00098&partnerID=40&md5=b14ca9c4f38785c6fbe86b3e16ad910b","We present a method for improving human design of chairs. The goal of the method is generating enormous chair candidates in order to facilitate human designer by creating sketches and 3d models accordingly based on the generated chair design. It consists of an image synthesis module, which learns the underlying distribution of training dataset, a super-resolution module, which improve quality of generated image and human involvements. Finally, we manually pick one of the generated candidates to create a real life chair for illustration. © 2019 IEEE.","Deep learning; Design; Image enhancement; Adversarial networks; Aided designs; Human design; Image synthesis; Super resolution; Training dataset; Underlying distribution; Seats","AI-aided Design; deep learning; GAN","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065627800"
"Huang X.; Zhang Q.; Wang G.; Guo X.; Li Z.","Huang, Xu (36816004500); Zhang, Qianyao (57199113071); Wang, Guoli (35231538600); Guo, Xuemei (25822445600); Li, Zhonghua (56193311000)","36816004500; 57199113071; 35231538600; 25822445600; 56193311000","Medical Image Super-Resolution Based on the Generative Adversarial Network","2020","Lecture Notes in Electrical Engineering","593","","","243","253","10","10.1007/978-981-32-9686-2_29","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072954532&doi=10.1007%2f978-981-32-9686-2_29&partnerID=40&md5=1801cd46ec0e07c8c4e09cd8d646e888","In order to assist doctors to read medical pathological images with low resolution, this paper proposes a medical image super-resolution (SR) reconstruction method based on generative adversarial network (GAN). Considering that the pathological image has large non-organized regions, we design a medical pathological image preprocessing system to extract tissue area image patches. And, we improve discriminator with small batch relative discrimination to enhance the quality of reconstructed images by learning more prior information. We use Huber loss instead of the original MSE which can keep the network training stable. We find the feature similarity (FSIM) is suitable as an image quality evaluation way for medical image reconstruction research. And, the experimental results show the advantages of our method in the restoration of color and intercellular texture details. © 2020, Springer Nature Singapore Pte Ltd.","Image enhancement; Image reconstruction; Intelligent systems; Medical imaging; Optical resolving power; Textures; Adversarial networks; Feature similarities; Feature similarity(FSIM); Image quality evaluation; Image super resolutions; Pathological images; Quality of reconstructed images; Super resolution; Medical image processing","Feature similarity; Generative adversarial network; Huber loss; Medical image; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85072954532"
"Armanious K.; Abdulatif S.; Aziz F.; Schneider U.; Yang B.","Armanious, Karim (57208782510); Abdulatif, Sherif (57194649144); Aziz, Fady (57194650513); Schneider, Urs (55968038100); Yang, Bin (55584795030)","57208782510; 57194649144; 57194650513; 55968038100; 55584795030","An adversarial super-resolution remedy for radar design trade-offs","2019","European Signal Processing Conference","2019-September","","","","","","10.23919/EUSIPCO.2019.8902510","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075601097&doi=10.23919%2fEUSIPCO.2019.8902510&partnerID=40&md5=fb1669ec6d341145d913d5713255968b","Radar is of vital importance in many fields, such as autonomous driving, safety and surveillance applications. However, it suffers from stringent constraints on its design parametrization leading to multiple trade-offs. For example, the bandwidth in FMCW radars is inversely proportional with both the maximum unambiguous range and range resolution. In this work, we introduce a new method for circumventing radar design trade-offs. We propose the use of recent advances in computer vision, more specifically generative adversarial networks (GANs), to enhance low-resolution radar acquisitions into higher resolution counterparts while maintaining the advantages of the low-resolution parametrization. The capability of the proposed method was evaluated on the velocity resolution and range-azimuth trade-offs in micro-Doppler signatures and FMCW uniform linear array (ULA) radars, respectively. © 2019 IEEE","MIMO radar; MIMO systems; Neural networks; Optical resolving power; Radar; Radar signal processing; Remote sensing; Adversarial networks; Convolutional neural network; Micro-Doppler; Range Azimuth; Super resolution; Economic and social effects","CNN; Convolutional neural network; GAN; Generative adversarial networks; Micro-Doppler; MIMO; Radar; Range-azimuth; Remote sensing; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075601097"
"Zhang Q.; Wang H.; Du T.; Yang S.; Wang Y.; Xing Z.; Bai W.; Yi Y.","Zhang, Qi (57212800057); Wang, Huafeng (36783787700); Du, Tao (56038798600); Yang, Sichen (57212138469); Wang, Yuehai (57194466632); Xing, Zhiqiang (36832316200); Bai, Wenle (16548808600); Yi, Yang (57212149988)","57212800057; 36783787700; 56038798600; 57212138469; 57194466632; 36832316200; 16548808600; 57212149988","Super-resolution reconstruction algorithms based on fusion of deep learning mechanism and wavelet","2019","ACM International Conference Proceeding Series","","","","102","107","5","10.1145/3357254.3358600","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076009305&doi=10.1145%2f3357254.3358600&partnerID=40&md5=d9950831e29011996c436064dead3789","In this paper, we consider the problem of super-resolution reconstruction. This is a hot topic because super-resolution reconstruction has a wide range of applications in the medical field, remote sensing monitoring, and criminal investigation. Compared with traditional algorithms, the current super-resolution reconstruction algorithm based on deep learning greatly improves the clarity of reconstructed pictures. Existing work like Super-Resolution Using a Generative Adversarial Network (SRGAN) can effectively restore the texture details of the image. However, experimentally verified that the texture details of the image recovered by the SRGAN are not robust. In order to get super-resolution reconstructed images with richer high-frequency details, we improve the network structure and propose a super-resolution reconstruction algorithm combining wavelet transform and Generative Adversarial Network. The proposed algorithm can efficiently reconstruct high-resolution images with rich global information and local texture details. We have trained our model by Py Torch framework and VOC2012 dataset, and tested it by Set5, Set14, BSD100 and Urban100 test datasets. © 2019 Association for Computing Machinery.","Image enhancement; Image reconstruction; Image texture; Learning algorithms; Optical resolving power; Pattern recognition; Remote sensing; Statistical tests; Textures; Wavelet transforms; Adversarial networks; Criminal investigation; Global informations; High resolution image; Learning mechanism; Reconstructed image; Remote sensing monitoring; Super resolution reconstruction; Deep learning","Deep learning; Generative Adversarial Network; Super-resolution reconstruction; Wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85076009305"
"Damer N.; Boutros F.; Saladie A.M.; Kirchbuchner F.; Kuijper A.","Damer, Naser (50861109400); Boutros, Fadi (57205379838); Saladie, Alexandra Mosegui (57208645670); Kirchbuchner, Florian (57031859600); Kuijper, Arjan (56131137100)","50861109400; 57205379838; 57208645670; 57031859600; 56131137100","Realistic Dreams: Cascaded Enhancement of GAN-generated Images with an Example in Face Morphing Attacks","2019","2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems, BTAS 2019","","","9185994","","","","10.1109/BTAS46853.2019.9185994","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082479519&doi=10.1109%2fBTAS46853.2019.9185994&partnerID=40&md5=1953eec46aefaea13cfc2791aa51c454","The quality of images produced by generative adversarial networks (GAN) is commonly a trade-off between the model size, its training data needs, and the generation resolution. This trad-off is clear when applying GANs to issues like generating face morphing attacks, where the latent vector used by the generator is manipulated. In this paper, we propose an image enhancement solution designed to increase the quality and resolution of GAN-generated images. The solution is designed to require limited training data and be extendable to higher resolutions. We successfully apply our solution on GAN-based face morphing attacks. Beside the face recognition vulnerability and attack detectability analysis, we prove that the images enhanced by our solution are of higher visual and quantitative quality in comparison to unprocessed attacks and attack images enhanced by state-of-the-art super-resolution approaches. © 2019 IEEE.","Biometrics; Economic and social effects; Face recognition; Adversarial networks; Detectability; Face Morphing; Higher resolution; Latent vectors; Limited training data; State of the art; Super resolution; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85082479519"
"Turhan C.G.; Bilge H.Ş.","Turhan, Ceren Guzel (56246662400); Bilge, Hasan Şakir (23134504900)","56246662400; 23134504900","Scalable image generation and super resolution using generative adversarial networks; [Çekişmeli üretici ağ ile ölçeklenebilir görüntü oluşturma ve süper çözünürlük]","2020","Journal of the Faculty of Engineering and Architecture of Gazi University","35","2","","953","966","13","10.17341/gazimmfd.587010","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082076681&doi=10.17341%2fgazimmfd.587010&partnerID=40&md5=12207a96751f06b83a21e8c97015c1e8","Generative adversarial training has been one of the most active research topics and many researchers have conducted their studies on Generative Adversarial Network (GAN) shortly after it is claimed to be one of the most promising research area of the last decade by pioneers of the deep learning community. On the other hand, the idea behind generators has also reemerged autoencoder models such as Variational Autoencoder (VAE). Therefore, autoencoder models have gained their popularity back. Some restrictions of GAN models such as lack of inference mechanism, GAN and VAE based hybrid models have proposed addressing image generation. With the effect of these notions and studies, we have also considered VAE and GAN hybrid models. For obtaining synthetic but at the same time high-resolution handwritten-looking images without any training, Compositional Pattern Producing Network (CPPN) is adapted from previous studies for combining with VAE and adversarial training. For improving generation capabilities, an objective from a previous VAE/GAN model is also adapted for our VAE/CPGAN hybrid model. For analyzing the proposed model performance, baseline models such as GAN, VAE and VAE/GAN are also evaluated for comparisons. In this paper. it is clearly seen the proposed model is capable of the generating realistic and scalable super resolution synthetic images on a common dataset. © 2020 Gazi Universitesi Muhendislik-Mimarlik. All rights reserved.","Learning systems; Optical resolving power; Adversarial networks; Auto encoders; Compositional pattern-producing networks; Image generations; Inference mechanism; Learning community; Model performance; Super resolution; Deep learning","Deep learning; Generative adversarial network; Image generation; Scalable super resolution; Variational autoencoder","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85082076681"
"Lore K.G.; Reddy K.K.; Giering M.; Bernal E.A.","Lore, Kin Gwn (57190128995); Reddy, Kishore K. (55443153000); Giering, Michael (56464245300); Bernal, Edgar A. (12800366000)","57190128995; 55443153000; 56464245300; 12800366000","Generative adversarial networks for spectral super-resolution and bidirectional rgb-to-multispectral mapping","2019","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2019-June","","9025450","926","933","7","10.1109/CVPRW.2019.00122","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071519076&doi=10.1109%2fCVPRW.2019.00122&partnerID=40&md5=a0acb08f88953b4bf73f88bc6b5ad48a","Acquisition of multi-and hyperspectral imagery imposes significant requirements on the hardware capabilities of the sensors involved. In order to keep costs manageable, and due to limitations in the sensing technology, tradeoffs between the spectral and the spatial resolution of hyperspectral images are usually made. Such tradeoffs are usually not necessary when considering acquisition of traditional RGB imagery. We investigate the use of statistical learning, and in particular, of conditional generative adversarial networks (cGANs) to estimate mappings from three-channel RGB to 31-band multispectral imagery. We demonstrate the application of the proposed approach to (i) RGB-to-multispectral image mapping, (ii) spectral super-resolution of image data, and (iii) recovery of RGB imagery from multispectral data. © 2019 IEEE.","Computer vision; Optical resolving power; Remote sensing; Spectroscopy; Adversarial networks; Hyper-spectral imageries; Multi-spectral data; Multi-spectral imagery; Multispectral images; Sensing technology; Spatial resolution; Statistical learning; Photomapping","","Conference paper","Final","","Scopus","2-s2.0-85071519076"
"Yang S.; Shi X.; Zhou F.","Yang, Shuang (57221158149); Shi, Xiaoran (56502661900); Zhou, Feng (56421055500)","57221158149; 56502661900; 56421055500","Automatic Target Recognition for Low-Resolution SAR Images Based on Super-Resolution Network","2019","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar, APSAR 2019","","","9048251","","","","10.1109/APSAR46974.2019.9048251","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083498375&doi=10.1109%2fAPSAR46974.2019.9048251&partnerID=40&md5=04b803d305f4bcd954ac6136b0251674","Synthetic aperture radar (SAR) automatic target recognition (ATR) is one of the hottest issue in current research because of its wide application value. However, the low-resolution SAR images will decline the recognition accuracy of targets due to its obscure characteristic, and meanwhile it is difficult to acquire a great number of high-resolution SAR images for extracting clear characteristic. To solve these problems, this paper proposes a method of ATR for low-resolution SAR images based on super-resolution network. Super-resolution generative adversarial network (SRGAN) and deep convolutional neural network (DCNN) are utilized for extracting characteristic and classification, respectively. The segmented low-resolution SAR images are enhanced through SRGAN to improve the visual resolution and the feature characterization ability of target in SAR image; Then the enhanced SAR images are classified automatically by DCNN. Finally, the effectiveness and the efficiency are verified on the open data set, moving and stationary target acquisition and recognition (MSTAR). © 2019 IEEE.","Automatic target recognition; Convolutional neural networks; Deep neural networks; Image enhancement; Open Data; Radar target recognition; Spectral resolution; Synthetic aperture radar; Adversarial networks; Feature characterization; High-resolution SAR; Low resolution; Recognition accuracy; Stationary targets; Super resolution; Visual resolutions; Radar imaging","automatic target recognition (ATR); deep convolutional neural network (DCNN); super-resolution generative adversarial network (SRGAN); synthetic aperture radar (SAR)","Conference paper","Final","","Scopus","2-s2.0-85083498375"
"Ferreira R.S.; Zabihi Naeini E.; Vital Brazil E.","Ferreira, R.S. (23088477000); Zabihi Naeini, E. (16043138100); Vital Brazil, E. (36646892000)","23088477000; 16043138100; 36646892000","Stabilized super resolution deep generative networks for seismic data","2019","81st EAGE Conference and Exhibition 2019 Workshop Programme","","","","","","","10.3997/2214-4609.201901971","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084020411&doi=10.3997%2f2214-4609.201901971&partnerID=40&md5=f5a8f8364e6444ce84ff750891777e0f","High-resolution seismic data enable us to characterize the reservoirs with higher accuracy and/or detect smaller targets. Enhancing the seismic bandwidth can be achieved with broadband acquisition, various processing algorithms or a combination of both. In contrast to classic spectral matching type processes, we propose to take a different approach by using deep Generative Adversarial Networks (GANs). In theory, they can reconstruct the seismic data both temporally and spatially. This is inherent by design given the convolutional architecture of the GANs. That means GANs allow recovering the frequency content or the missing traces in seismic data. We propose amplitude encoding and histogram equalization to stabilize the performance of GANs on seismic data and show promising preliminary results for typical seismic processing and interpretation applications. © 81st EAGE Conference and Exhibition 2019 Workshop Programme. All rights reserved.","Seismic response; Seismic waves; Adversarial networks; Frequency contents; High resolution seismic data; Histogram equalizations; Processing algorithms; Seismic processing; Spectral matchings; Super resolution; Geophysical prospecting","","Conference paper","Final","","Scopus","2-s2.0-85084020411"
"Bode M.; Gauding M.; Kleinheinz K.; Pitsch H.","Bode, Mathis (56135121000); Gauding, Michael (26221578700); Kleinheinz, Konstantin (55770386500); Pitsch, Heinz (7003265812)","56135121000; 26221578700; 55770386500; 7003265812","Deep Learning at Scale for Subgrid Modeling in Turbulent Flows: Regression and Reconstruction","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11887 LNCS","","","541","560","19","10.1007/978-3-030-34356-9_41","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076828600&doi=10.1007%2f978-3-030-34356-9_41&partnerID=40&md5=67a2995aa5b857e670c40e8e548d0b2a","Modeling of turbulent flows is still challenging. One way to deal with the large scale separation due to turbulence is to simulate only the large scales and model the unresolved contributions as done in large-eddy simulation (LES). This paper focuses on two deep learning (DL) strategies, regression and reconstruction, which are data-driven and promising alternatives to classical modeling concepts. Using three-dimensional (3-D) forced turbulence direct numerical simulation (DNS) data, subgrid models are evaluated, which predict the unresolved part of quantities based on the resolved solution. For regression, it is shown that feedforward artificial neural networks (ANNs) are able to predict the fully-resolved scalar dissipation rate using filtered input data. It was found that a combination of a large-scale quantity, such as the filtered passive scalar itself, and a small-scale quantity, such as the filtered energy dissipation rate, gives the best agreement with the actual DNS data. Furthermore, a DL network motivated by enhanced super-resolution generative adversarial networks (ESRGANs) was used to reconstruct fully-resolved 3-D velocity fields from filtered velocity fields. The energy spectrum shows very good agreement. As size of scientific data is often in the order of terabytes or more, DL needs to be combined with high performance computing (HPC). Necessary code improvements for HPC-DL are discussed with respect to the supercomputer JURECA. After optimizing the training code, 396.2 TFLOPS were achieved. © 2019, The Author(s).","Codes (symbols); Direct numerical simulation; Energy dissipation; Feedforward neural networks; Internet protocols; Large eddy simulation; Numerical models; Regression analysis; Supercomputers; Turbulence; Turbulent flow; Velocity; Adversarial networks; Energy dissipation rate; Feed-forward artificial neural networks; High performance computing; High performance computing (HPC); Large-scale separation; Scalar dissipation rate; Threedimensional (3-d); Deep learning","Deep learning; Direct numerical simulation; High performance computing; Large-eddy simulation; Turbulence","Conference paper","Final","","Scopus","2-s2.0-85076828600"
"Sun X.; Zhao Z.; Zhang S.; Liu J.; Yang X.; Zhou C.","Sun, Xudong (55727887000); Zhao, Zhenxi (57215092424); Zhang, Song (57205684448); Liu, Jintao (57214318692); Yang, Xinting (17343047800); Zhou, Chao (36669814800)","55727887000; 57215092424; 57205684448; 57214318692; 17343047800; 36669814800","Image Super-Resolution Reconstruction Using Generative Adversarial Networks Based on Wide-Channel Activation","2020","IEEE Access","8","","9001139","33838","33854","16","10.1109/ACCESS.2020.2974759","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081179492&doi=10.1109%2fACCESS.2020.2974759&partnerID=40&md5=07a2e5479a8700bb66109650c41f661e","In recent years, residual learning has shown excellent performance on convolutional neural network (CNN)-based single-image super-resolution (SISR) tasks. However, CNN-based SISR approaches have focused mainly on the design of deep architectures, and the rectified linear units (ReLUs) used in these networks hinder shallow-to-deep information transfer. As a result, these methods are unable to utilize some shallow information, and improving model performance is difficult. To solve the above issues, this paper proposes an image SR reconstruction method based on a generative adversarial network with a residual dense architecture. First, before ReLU activation, the number of feature channels is expanded by a factor of 69 using a \times 1$ convolutional layer, which improves the utilization of shallow information. Next, the original discriminator is replaced with a relativistic average discriminator, thereby improving the authenticity of the discriminative network. Finally, preactivation features are used to improve the perceptual loss, thus providing stronger monitoring for brightness consistency and texture restoration. Experimental results show that the proposed algorithm improves the utilization of shallow information in a deep network. Structural similarity (SSIM) index evaluations show that the overall utilization of shallow information is increased by 105.52%. In addition, the average runtime is 0.42 sec/frame, nearly 3.6 times faster than those of traditional methods. Moreover, the recovered images have an average natural image quality evaluator value of 3.4 and high perceptual quality, showing that the proposed method is suitable for image reconstruction applications in fields such as agriculture and medicine. © 2013 IEEE.","Chemical activation; Convolution; Convolutional neural networks; Discriminators; Network architecture; Optical resolving power; Quality control; Textures; Adversarial networks; Discriminative networks; Image super-resolution reconstruction; Information transfers; Perceived quality; residual block; Structural similarity indices (SSIM); Super resolution; Image reconstruction","generative adversarial network; perceived quality; relativistic average discriminator; residual block; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85081179492"
"Spratley S.; Beck D.; Cohn T.","Spratley, Steven (57209881744); Beck, Daniel (56368397100); Cohn, Trevor (15043872200)","57209881744; 56368397100; 15043872200","A Unified Neural Architecture for Instrumental Audio Tasks","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8682765","461","465","4","10.1109/ICASSP.2019.8682765","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068972725&doi=10.1109%2fICASSP.2019.8682765&partnerID=40&md5=e7f6a417dff8602780b9f1c976329d14","Within Music Information Retrieval (MIR), prominent tasks - including pitch-tracking, source-separation, super-resolution, and synthesis - typically call for specialised methods, despite their similarities. Conditional Generative Adversarial Networks (cGANs) have been shown to be highly versatile in learning general image-to-image translations, but have not yet been adapted across MIR. In this work, we present an end-to-end supervisable architecture to perform all aforementioned audio tasks, consisting of a WaveNet synthesiser conditioned on the output of a jointly-trained cGAN spectrogram translator. In doing so, we demonstrate the potential of such flexible techniques to unify MIR tasks, promote efficient transfer learning, and converge research to the improvement of powerful, general methods. Finally, to the best of our knowledge, we present the first application of GANs to guided instrument synthesis. © 2019 IEEE.","Audio acoustics; Computer music; Information retrieval; Network architecture; Source separation; Speech communication; Synthesis (chemical); Adversarial networks; General method; Image translation; Music information retrieval; Neural architectures; Pitch-tracking; Super resolution; Transfer learning; Audio signal processing","audio modelling; generative adversarial network; music information retrieval; synthesis","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068972725"
"Bayramli B.; Ali U.; Qi T.; Lu H.","Bayramli, Bayram (57213001529); Ali, Usman (57214298634); Qi, Te (57207981638); Lu, Hongtao (8943716200)","57213001529; 57214298634; 57207981638; 8943716200","FH-GAN: Face hallucination and recognition using generative adversarial network","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11953 LNCS","","","3","15","12","10.1007/978-3-030-36708-4_1","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077504917&doi=10.1007%2f978-3-030-36708-4_1&partnerID=40&md5=dae82531a140c316478eca535a2abdca","There are many factors affecting visual face recognition, such as low resolution images, aging, illumination and pose variance, etc. One of the most important problem is low resolution face images which can result in bad performance on face recognition. The modern face hallucination models demonstrate reasonable performance to reconstruct high-resolution images from its corresponding low resolution images. However, they do not consider identity level information during hallucination which directly affects results of the recognition of low resolution faces. To address this issue, we propose a Face Hallucination Generative Adversarial Network (FH-GAN) which improves the quality of low resolution face images and accurately recognize those low quality images. Concretely, we make the following contributions: (1) we propose FH-GAN network, an end-to-end system, that improves both face hallucination and face recognition simultaneously. The novelty of this proposed network depends on incorporating identity information in a GAN-based face hallucination algorithm via combining a face recognition network for identity preserving. (2) We also propose a new face hallucination network, namely Dense Sparse Network (DSNet), which improves upon the state-of-art in face hallucination. (3) We demonstrate benefits of training the face recognition and GAN-based DSNet jointly by reporting good result on face hallucination and recognition. © Springer Nature Switzerland AG 2019.","Arts computing; Image enhancement; Neural networks; Adversarial networks; Convolutional neural network; High resolution image; Identity information; Low resolution images; Low-level vision; Low-resolution face images; Super resolution; Face recognition","Convolutional neural networks; Low level vision; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077504917"
"Yin R.","Yin, Ruihao (57212479843)","57212479843","Multi-Resolution Generative Adversarial Networks for Tiny-Scale Pedestrian Detection","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8803030","1665","1669","4","10.1109/ICIP.2019.8803030","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076798594&doi=10.1109%2fICIP.2019.8803030&partnerID=40&md5=265990fcf0170386ec041036b4fef684","Although pedestrian detection techniques have achieved great success recently, the accurate tiny-scale pedestrian detection remains challenging with low resolution. The existing CNN based detection methods are limited in terms of in-line mechanism when encountering tiny-scale pedestrian detection, because the embedded convolution and pooling operations tend to weaken the features' representation of tiny scales objects. To ameliorate, we propose a newly-designed Multi-Resolution Generative Adversarial Network (MRGAN) to simultaneously conduct multi-resolution pedestrian detection by directly generating a high-resolution pedestrian image from low-resolution image. The key idea is to explore the intrinsic relations between high-resolution pedestrians and low-resolution pedestrians to enhance the representation of low-resolution pedestrians. The classification loss will be used to conduct the training process of super-resolution generative adversarial network, so that the generated super-resolution images can benefit the tiny-scale pedestrian detection. Besides, we also define a segmentation-based perceptual loss by incorporating a pre-trained image segmentation sub-network to refine the detail information. Extensive experiments and comprehensive evaluations on public challenging benchmarks confirm that our method outperforms the state-of-the-art methods. © 2019 IEEE.","","Generative adversarial network; Pedestrian detection; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85076798594"
"Jiao L.; Zhao J.","Jiao, Licheng (7102491544); Zhao, Jin (57049887700)","7102491544; 57049887700","A Survey on the New Generation of Deep Learning in Image Processing","2019","IEEE Access","7","","8917633","172231","172263","32","10.1109/ACCESS.2019.2956508","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078010097&doi=10.1109%2fACCESS.2019.2956508&partnerID=40&md5=101f2abf65ea274f4722624d183ac46b","During the past decade, deep learning is one of the essential breakthroughs made in artificial intelligence. In particular, it has achieved great success in image processing. Correspondingly, various applications related to image processing are also promoting the rapid development of deep learning in all aspects of network structure, layer designing, and training tricks. However, the deeper structure makes the back-propagation algorithm more difficult. At the same time, the scale of training images without labels is also rapidly increasing, and class imbalance severely affects the performance of deep learning, these urgently require more novelty deep models and new parallel computing system to more effectively interpret the content of the image and form a suitable analysis mechanism. In this context, this survey provides four deep learning model series, which includes CNN series, GAN series, ELM-RVFL series, and other series, for comprehensive understanding towards the analytical techniques of image processing field, clarify the most important advancements and shed some light on future studies. By further studying the relationship between deep learning and image processing tasks, which can not only help us understand the reasons for the success of deep learning but also inspires new deep models and training methods. More importantly, this survey aims to improve or arouse other researchers to catch a glimpse of the state-of-the-art deep learning methods in the field of image processing and facilitate the applications of these deep learning technologies in their research tasks. Besides, we discuss the open issues and the promising directions of future research in image processing using the new generation of deep learning. © 2013 IEEE.","Backpropagation algorithms; Deep learning; Deep neural networks; Image classification; Image processing; Network layers; Neural networks; Object detection; Surveys; ADMM-Net; Adversarial networks; Convolutional neural network; deep forest; Extreme learning machine; style transfer; Super resolution; Image enhancement","ADMM-Net; capsule networks; convolutional neural network; deep forest; deep learning; extreme learning machine; generative adversarial network; image classification; Image processing; object detection; style transfer; super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078010097"
"Zhou R.; Susstrunk S.","Zhou, Ruofan (57195942513); Susstrunk, Sabine (6603829965)","57195942513; 6603829965","Kernel modeling super-resolution on real low-resolution images","2019","Proceedings of the IEEE International Conference on Computer Vision","2019-October","","9010978","2433","2443","10","10.1109/ICCV.2019.00252","78","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081663623&doi=10.1109%2fICCV.2019.00252&partnerID=40&md5=b6dc927c82cf78e951e825e0f4e43aac","Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super-resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their training data: Low-resolution (LR) images are obtained by bicubic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images. To improve generalization and robustness of deep super-resolution CNNs on real photographs, we present a kernel modeling super-resolution network (KMSR) that incorporates blur-kernel modeling in the training. Our proposed KMSR consists of two stages: We first build a pool of realistic blur-kernels with a generative adversarial network (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental validations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels. © 2019 IEEE.","Cameras; Computer vision; Convolution; Convolutional neural networks; Deep neural networks; Optical resolving power; Photography; Adversarial networks; Bicubic interpolation; Convolution kernel; Experimental validations; High resolution image; Low resolution images; State-of-the-art performance; Super resolution; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85081663623"
"Li W.; He Y.; Kong W.; Gao F.; Wang J.; Shi G.","Li, Wanyue (57211198897); He, Yi (56447293500); Kong, Wen (57075709700); Gao, Feng (57288731400); Wang, Jing (57212603848); Shi, Guohua (23986377600)","57211198897; 56447293500; 57075709700; 57288731400; 57212603848; 23986377600","Enhancement of retinal image from line-scanning ophthalmoscope using generative adversarial networks","2019","IEEE Access","7","","8768363","99830","99841","11","10.1109/ACCESS.2019.2930329","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078227388&doi=10.1109%2fACCESS.2019.2930329&partnerID=40&md5=76016a607e384d79c9130cad07468018","A line-scanning ophthalmoscope (LSO) is a retinal imaging technique that has the characteristics of high imaging resolution, wide field of view, and high imaging speed. However, the high-speed imaging with rather short exposure time inevitably reduces the signal intensity, and many factors, such as speckle noise and intraocular scatter, further degrade the signal-to-noise ratio (SNR) of retinal images. To effectively improve the image quality without increasing the LSO system’s complexity, the post-processing method of image super-resolution (SR) is adopted. In this paper, we propose a learning-based multi-frame retinal image SR method that directly learns an end-to-end mapping from low-resolution (LR) image sequences to high-resolution (HR) images. This network was validated on down-sampled and real LSO image sequences. We evaluated the method on a down-sampled dataset with the metrics of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and perceptual distance. Moreover, the power spectra and full width at half maximum (FWHM) were used as the no-reference image quality assessment (NR-IQA) algorithms to evaluate the reconstruction results of the real LSO image sequences. The experimental results indicate that the proposed method can significantly enhance the SNR of LSO images and efficiently improve the resolution of LSO retinal images, which has great practical significance for clinical diagnosis and analysis. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","Diagnosis; Image enhancement; Image quality; Imaging techniques; Ophthalmology; Quality control; Quality of service; Adversarial networks; High resolution image; Low resolution images; No-reference image quality assessments; Peak signal to noise ratio; Perceptual distance; Postprocessing methods; Structural similarity; Signal to noise ratio","Learning-based; Line-scanning ophthalmoscope; Multi-frame image super-resolution; Retinal images","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078227388"
"Pham C.-H.; Tor-Diez C.; Meunier H.; Bednarek N.; Fablet R.; Passat N.; Rousseau F.","Pham, C.-H. (57194836445); Tor-Diez, C. (57204032876); Meunier, H. (57209603238); Bednarek, N. (6602112368); Fablet, R. (6603508732); Passat, N. (57207528590); Rousseau, F. (57188780886)","57194836445; 57204032876; 57209603238; 6602112368; 6603508732; 57207528590; 57188780886","Simultaneous super-resolution and segmentation using a generative adversarial network: Application to neonatal brain MRI","2019","Proceedings - International Symposium on Biomedical Imaging","2019-April","","8759255","991","994","3","10.1109/ISBI.2019.8759255","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071690567&doi=10.1109%2fISBI.2019.8759255&partnerID=40&md5=37c35d35746a488e090cfe2f74faa4d5","Brest, France The analysis of clinical neonatal brain MRI remains challenging due to low anisotropic resolution of the data. In most pipelines, images are first re-sampled using interpolation or single image super-resolution techniques and then segmented using (semi-)automated approaches. Image reconstruction and segmentation are then performed separately. In this paper, we propose an end-to-end generative adversarial network for simultaneous high-resolution reconstruction and segmentation of brain MRI data. This joint approach is first assessed on the simulated low-resolution images of the high-resolution neonatal dHCP dataset. Then, the learned model is used to enhance and segment real clinical low-resolution images. Results demonstrate the potential of our proposed method with respect to practical medical applications. © 2019 IEEE.","Image enhancement; Image reconstruction; Magnetic resonance imaging; Medical applications; Medical imaging; Optical resolving power; Adversarial networks; Automated approach; Brain MRI; High resolution; High-resolution reconstruction; Low resolution images; Single images; Super resolution; Image segmentation","3D generative adversarial networks; Neonatal brain MRI; Segmentation; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85071690567"
"Zhong X.; Qu X.; Chen C.","Zhong, Xinru (57207735707); Qu, Xiujie (7202660844); Chen, Chen (57196286315)","57207735707; 7202660844; 57196286315","High-quality face image super-resolution based on Generative Adversarial Networks","2019","Proceedings of 2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2019","","","8998075","1178","1182","4","10.1109/IAEAC47372.2019.8998075","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081172504&doi=10.1109%2fIAEAC47372.2019.8998075&partnerID=40&md5=18942f66e238f50cc83fe453464d6025","Face image super-resolution has received increasing attention. However, since the face has a lot of fine textures, it is very difficult to rebuild for large upscaling factors. We propose a new method for face image SR, using residul dense block(RDB) as the basic unit and the Inception architecture is combined in the low layers. We use the relativistic GAN and the improved perceptual loss defined by the features before activation.For the large scaling factors, our GAN is progressive both in architecture and training. The network proposed achieves excellent performance in the reconstruction of low-resolution face images, especially under large scaling factors such as 4x and 8x. © 2019 IEEE.","Optical resolving power; Textures; Adversarial networks; Basic units; Face images; High quality; Low-resolution face images; progressive trainging; Scaling factors; Upscaling; Network architecture","face image super-resolution; progressive trainging; relativistic Generative Adversarial Networks","Conference paper","Final","","Scopus","2-s2.0-85081172504"
"Kasem H.M.; Hung K.-W.; Jiang J.","Kasem, Hossam M. (53663632700); Hung, Kwok-Wai (36990606600); Jiang, Jianmin (57193403110)","53663632700; 36990606600; 57193403110","Spatial Transformer Generative Adversarial Network for Robust Image Super-Resolution","2019","IEEE Access","7","","8933156","182993","183009","16","10.1109/ACCESS.2019.2959940","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078251578&doi=10.1109%2fACCESS.2019.2959940&partnerID=40&md5=cfd7222eab4a731e038bd9ccc0b45b9a","Recently, there have been significant advances in image super-resolution based on generative adversarial networks (GANs) to achieve breakthroughs in generating more images with high subjective quality. However, there are remaining challenges needs to be met, such as simultaneously recovering the finer texture details for large upscaling factors and mitigating the geometric transformation effects. In this paper, we propose a novel robust super-resolution GAN (i.e. namely RSR-GAN) which can simultaneously perform both the geometric transformation and recovering the finer texture details. Specifically, since the performance of the generator depends on the discreminator, we propose a novel discriminator design by incorporating the spatial transformer module with residual learning to improve the discrimination of fake and true images through removing the geometric noise, in order to enhance the super-resolution of geometric corrected images. Finally, to further improve the perceptual quality, we introduce an additional DCT loss term into the existing loss function. Extensive experiments, measured by both PSNR and SSIM measurements, show that our proposed method achieves a high level of robustness against a number of geometric transformations, including rotation, translation, a combination of rotation and scaling effects, and a cobmination of rotaion, transalation and scaling effects. Benchmarked by the existing state-of-The-Arts SR methods, our proposed delivers superior performances on a wide range of datasets which are publicly available and widely adopted across research communities. © 2013 IEEE.","Arts computing; Geometry; Mathematical transformations; Optical resolving power; Textures; Adversarial networks; Geometric transformations; Image super resolutions; Perceptual quality; Research communities; State of the art; Subjective quality; Super resolution; Image enhancement","generative adversarial networks; robust generative adversarial network; robust image super-resolution; spatial transformer network; Super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078251578"
"Sharma A.; Jindal N.","Sharma, Akanksha (57188975329); Jindal, Neeru (8571819900)","57188975329; 8571819900","CBNWI-50: A deep learning bird dataset for image translation and resolution improvement using generative adversarial network","2019","International Journal of Innovative Technology and Exploring Engineering","8","9 Special Issue","","91","102","11","10.35940/ijitee.I1015.0789S19","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073322956&doi=10.35940%2fijitee.I1015.0789S19&partnerID=40&md5=992461ce073e33f5d2c68ce1b7e79c4a","Generative Adversarial Networks have gained prominence in a short span of time as they can synthesize images from latent noise by minimizing the adversarial cost function. New variants of GANs have been developed to perform specific tasks using state-of-the-art GAN models, like image translation, single image super resolution, segmentation, classification, style transfer etc. However, a combination of two GANs to perform two different applications in one model has been sparsely explored. Hence, this paper concatenates two GANs and aims to perform Image Translation using Cycle GAN model on bird images and improve their resolution using SRGAN. During the extensive survey, it is observed that most of the deep learning databases on Aves were built using the new world species (i.e. species found in North America). Hence, to bridge this gap, a new Ave database, 'Common Birds of North-Western India' (CBNWI-50), is also proposed in this work. © BEIESP.","","Bird Dataset; Generative Adversarial Networks; Image Translation; Indian-Subcontinent; Single Image Super Resolution","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85073322956"
"Wang H.; Wu W.; Su Y.; Duan Y.; Wang P.","Wang, Han (57202273043); Wu, Wei (57209634017); Su, Yang (57212318188); Duan, Yongsheng (57210595208); Wang, Pengze (57210605841)","57202273043; 57209634017; 57212318188; 57210595208; 57210605841","Image super-resolution using a improved generative adversarial network","2019","ICEIEC 2019 - Proceedings of 2019 IEEE 9th International Conference on Electronics Information and Emergency Communication","","","8784610","312","315","3","10.1109/ICEIEC.2019.8784610","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071072253&doi=10.1109%2fICEIEC.2019.8784610&partnerID=40&md5=cf1f5a0aef4d6c09573a2cbcecc9f81c","In recent years, there have been a variety of learning methods applied to single image super-resolution problems (SISR). Generative adversarial network (GAN) for image super-resolution which can infer photo-realistic natural images for 4× upscaling factors has been proposed. Images generated from SRGAN have sharper details, but some texture will be distorted and deformed. In this situation, the image looks unsatisfactory. In order to generate clearer, more eye-catching picture, we improved the SRGAN network. We proposed a encoder block in the generator to extract more crucial features. In this condition, we can generate more clear and natural image. © 2019 IEEE.","Learning systems; Optical resolving power; Signal encoding; Textures; Adversarial networks; component; encoder; Image super resolutions; Learning methods; Natural images; Photo-realistic; Super resolution; Image enhancement","component; encoder; GAN; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85071072253"
"Kim Y.J.; Hazra D.; Byun Y.; Ahn K.-J.","Kim, Yong Jun (57217096542); Hazra, Debapriya (57191375984); Byun, Yungcheol (8897891700); Ahn, Khi-Jung (7202870298)","57217096542; 57191375984; 8897891700; 7202870298","Old Document Restoration using Super Resolution GAN and Semantic Image Inpainting","2019","ACM International Conference Proceeding Series","","","","34","38","4","10.1145/3397453.3397459","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086182282&doi=10.1145%2f3397453.3397459&partnerID=40&md5=7dc7e787b462ce60460a31cf7767223e","Restoration of damaged images is a fundamental problem that has been attempted before the advent of digital image processing technology. In this paper, one of the deep neural network technologies (GAN), we propose an image restoration network using Generative Adversarial Network. The proposed system is the image generation network, the generation result plateIt consists of a star network. Old documents not only contain information, but also we can learn about historical people's thought and consciousness from the past. Old document restoration is referred to as the restoration of documents that are usually made of parchment which are damaged either naturally or artificially. Missing regions in old documents are filled based on the current visual data which is a hard task in image inpainting. In this paper, we present Super Resolution Generative Adversarial Network (SRGAN) and semantic image inpainting for restoring the old documents so that they can be reused. © 2019 ACM.","Artificial intelligence; Deep neural networks; Optical resolving power; Restoration; Semantics; Adversarial networks; Digital image processing technologies; Image generations; Image Inpainting; Network technologies; Restoration network; Semantic images; Super resolution; Image reconstruction","Old document restoration; Semantic Image Inpainting; SRGAN","Conference paper","Final","","Scopus","2-s2.0-85086182282"
"Yibin W.; Rongyue Z.; Hong X.; Hao W.; Pan Y.; Zhou Y.","Yibin, Wan (57211684346); Rongyue, Zhang (56031306300); Hong, Xiao (56532922200); Hao, Wang (57204720186); Pan, Yihao (57211680230); Zhou, Yubin (57205499988)","57211684346; 56031306300; 56532922200; 57204720186; 57211680230; 57205499988","Terahertz image super-resolution reconstruction of passive safety inspection based on generative adversarial network","2019","Proceedings - 2019 IEEE International Congress on Cybermatics: 12th IEEE International Conference on Internet of Things, 15th IEEE International Conference on Green Computing and Communications, 12th IEEE International Conference on Cyber, Physical and Social Computing and 5th IEEE International Conference on Smart Data, iThings/GreenCom/CPSCom/SmartData 2019","","","8875391","22","27","5","10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00027","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074852818&doi=10.1109%2fiThings%2fGreenCom%2fCPSCom%2fSmartData.2019.00027&partnerID=40&md5=9e925055bd3b41e6fd27f9e48d7fce9b","When passive terahertz imaging technology is used in Safety inspection, it has low image resolution and is susceptible to environment. As a result, it is difficult to support the demand for high-resolution terahertz images in related projects. This paper proposes a passive terahertz image reconstruction method based on improved Generative Adversarial Networks, and compares it with other existing methods. Experimental results show that the improved SRGAN method has higher peak signal-To-noise ratio (PSNR) and structural similarity (SSIM). © 2019 IEEE.","Green computing; Image enhancement; Image resolution; Imaging techniques; Internet of things; Optical resolving power; Safety engineering; Signal to noise ratio; Adversarial networks; Image reconstruction methods; Image super-resolution reconstruction; Peak signal to noise ratio; Safety inspections; Structural similarity; Tera Hertz; Terahertz imaging; Image reconstruction","(GAN) Generative Adversarial Networks; image super-resolution reconstruction; passive terahertz image","Conference paper","Final","","Scopus","2-s2.0-85074852818"
"Pathak H.N.; Li X.; Minaee S.; Cowan B.","Pathak, Harsh Nilesh (57207578682); Li, Xinxin (57207583008); Minaee, Shervin (56102393500); Cowan, Brooke (57185138400)","57207578682; 57207583008; 56102393500; 57185138400","Efficient Super Resolution for Large-Scale Images Using Attentional GAN","2019","Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018","","","8622477","1777","1786","9","10.1109/BigData.2018.8622477","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062589619&doi=10.1109%2fBigData.2018.8622477&partnerID=40&md5=d7ab7b45038947c3258e9b65a7c8ec1f","Single Image Super Resolution (SISR) is a well-researched problem with broad commercial relevance. However, most of the SISR literature focuses on small-size images under 500px, whereas business needs can mandate the generation of very high resolution images. At Expedia Group, we were tasked with generating images of at least 2000px for display on the website - four times greater than the sizes typically reported in the literature. This requirement poses a challenge that state-of-the-art models, validated on small images, have not been proven to handle. In this paper, we investigate solutions to the problem of generating high-quality images for large-scale1 super resolution in a commercial setting. We find that training a generative adversarial network (GAN) with attention from scratch using a large-scale lodging image data set generates images with high PSNR and SSIM scores. We describe a novel attentional SISR model for large-scale images, A-SRGAN, that uses a Flexible Self Attention layer to enable processing of large-scale images. We also describe a distributed algorithm which speeds up training by around a factor of five. © 2018 IEEE.","Big data; Deep learning; Adversarial networks; attention; Business needs; Commercial settings; High quality images; State of the art; Super resolution; Very high resolution (VHR) image; Optical resolving power","attention; deep learning; distributed training; generative adversarial networks; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062589619"
"Lee D.; Lee S.; Lee H.; Lee K.; Lee H.-J.","Lee, Donghyeon (56683212000); Lee, Sangheon (57211724557); Lee, Hoseong (57210361563); Lee, Kyujoong (50661645000); Lee, Hyuk-Jae (8161276300)","56683212000; 57211724557; 57210361563; 50661645000; 8161276300","Resolution-preserving generative adversarial networks for image enhancement","2019","IEEE Access","7","","2934320","110344","110357","13","10.1109/ACCESS.2019.2934320","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076875685&doi=10.1109%2fACCESS.2019.2934320&partnerID=40&md5=2c303c83fcda6ae63ea38c953c0423bd","Generative adversarial networks (GANs) are used for image enhancement such as single image super-resolution (SISR) and deblurring. The conventional GANs-based image enhancement suffers from two drawbacks that cause a quality degradation due to a loss of detailed information. First, the conventional discriminator network adopts strided convolution layers which cause a reduction in the resolution of the feature map, and thereby resulting in a loss of detailed information. Second, the previous GANs for image enhancement use the feature map of the visual geometry group (VGG) network for generating a content loss, which also causes visual artifacts because the maxpooling layers in the VGG network result in a loss of detailed information. To overcome these two drawbacks, this paper presents a proposal of a new resolution-preserving discriminator network architecture which removes the strided convolution layers, and a new content loss generated from the VGG network without maxpooling layers. The proposed discriminator network is applied to the super-resolution generative adversarial network (SRGAN), which is called a resolution-preserving SRGAN (RPSRGAN). Experimental results show that RPSRGAN generates more realistic super-resolution images than SRGAN does, and consequently, RPSRGAN with the new content loss improves the average peak signal-to-noise ratio (PSNR) by 0.75 dB and 0.32 dB for super-resolution images with the scale factors of 2 and 4, respectively. For deblurring, the visual appearance is also significantly improved, and the average PSNR is increased by 1.54 dB when the proposed discriminator and content loss are applied to the deblurring adversarial network.  © 2019 IEEE.","Convolution; Discriminators; Network architecture; Network layers; Optical resolving power; Signal to noise ratio; Adversarial networks; Peak signal to noise ratio; Quality degradation; Scale Factor; Single images; Super resolution; Visual appearance; Visual artifacts; Image enhancement","Deblurring; Generative adversarial networks; Image enhancement; Single image super-resolution","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85076875685"
"Chen S.; Li S.; Zhu C.","Chen, Sheng (57203904413); Li, Sumei (55741035700); Zhu, Chengcheng (57221460307)","57203904413; 55741035700; 57221460307","Guided Super-Resolution Restoration of Single Image Based on Image Quality Evaluation Network","2019","NSENS 2019 - 2nd IEEE International Conference on Micro/Nano Sensors for Al, Healthcare, and Robotics","","","9293988","72","77","5","10.1109/NSENS49395.2019.9293988","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099179029&doi=10.1109%2fNSENS49395.2019.9293988&partnerID=40&md5=f64518cd9b7fa315600c1187467b78aa","SISR (Single image super-resolution) has always been a key problem in image processing field. In recent years, deep learning has been successfully used to SISR reconstruction. However, most of the previous deep learning methods use L2 norm based on pixel pairs as loss function, which results in a high peak signal-to-noise ratio (PSNR) value, but the perception quality has not been improved. When using Generative Adversarial Network (GAN), although it has good perception quality, PSNR is lower. So we'll generate realistic results when both of them are used well. The image quality evaluation (IQA) network is to evaluate the image quality, so as to obtain good PSNR value and perception quality. In this paper, we use image quality assessment network to guide the SISR reconstruction network. Besides that, our proposed Super-resolution reconstruction of single image method is composed of several our given cross-attention units (CA) and is trained iteratively. Experimental results demonstrate that our method in qualitative and quantitative is better than others. © 2019 IEEE.","Agricultural robots; Deep learning; Health care; Image reconstruction; Iterative methods; Learning systems; Microrobots; Optical resolving power; Quality control; Robotics; Signal to noise ratio; Adversarial networks; Image quality assessment; Image quality evaluation; Learning methods; Peak signal to noise ratio; Reconstruction networks; Super resolution reconstruction; Super-resolution restoration; Image quality","cross-attention; Generative Adversarial Network; image quality assessment; Single image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85099179029"
"Li M.; Sun Y.; Zhang Z.; Xie H.; Yu J.","Li, Mengyan (57205562397); Sun, Yuechuan (57192994787); Zhang, Zhaoyu (57205560270); Xie, Haonian (57210578236); Yu, Jun (56260965400)","57205562397; 57192994787; 57205560270; 57210578236; 56260965400","Deep learning face hallucination via attributes transfer and enhancement","2019","Proceedings - IEEE International Conference on Multimedia and Expo","2019-July","","8785029","604","609","5","10.1109/ICME.2019.00110","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071010125&doi=10.1109%2fICME.2019.00110&partnerID=40&md5=a6b651f13982d6e97518caf9857ecb88","Face hallucination technique aims to generate high-resolution (HR) face images from low-resolution (LR) inputs. Even though existing face hallucination methods have achieved great performance on the global region evaluation, most of them cannot reasonably restore local attributes, especially when ultra-resolving tiny LR face image (16×16 pixels) to its larger version (8×upscaling factor). In this paper, we propose a novel attribute-guided face transfer and enhancement network for face hallucination. Specifically, we first construct a face transfer network, which upsamples LR face images to HR feature maps, and then fuses facial attributes and the upsampled features to generate HR face images with rational attributes. Finally, a face enhancement network is developed based on generative adversarial network (GAN) to improve visual quality by exploiting a composite loss that combines image color, texture and content. Extensive experiments demonstrate that our method achieves superior face hallucination results and outperforms the state-of-the-art. © 2019 IEEE.","Image enhancement; Textures; Adversarial networks; Face attribute transfer; Face hallucination; Face super-resolution; Facial attributes; State of the art; Transfer network; Visual qualities; Deep learning","Face attribute transfer; Face enhancement; Face hallucination; Face super-resolution; Facial attributes","Conference paper","Final","","Scopus","2-s2.0-85071010125"
"Hsu C.-C.; Lin C.-H.","Hsu, Chih-Chung (57212268908); Lin, Chia-Hsiang (55967027800)","57212268908; 55967027800","Dual reconstruction with densely connected residual network for single image super-resolution","2019","Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019","","","9022331","3643","3650","7","10.1109/ICCVW.2019.00449","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082451563&doi=10.1109%2fICCVW.2019.00449&partnerID=40&md5=874a1b9092234c37c5d6696c5c7b5a59","Deep learning-based single image super-resolution enables very fast and high-visual-quality reconstruction. Recently, an enhanced super-resolution based on generative adversarial network (ESRGAN) has achieved excellent performance in terms of both qualitative and quantitative quality of the reconstructed high-resolution image. In this paper, we propose to add one more shortcut between two dense-blocks, as well as add shortcut between two convolution layers inside a dense-block. With this simple strategy of adding more shortcuts in the proposed network, it enables a faster learning process as the gradient information can be back-propagated more easily. Based on the improved ESRGAN, the dual reconstruction is proposed to learn different aspects of the super-resolved image for judiciously enhancing the quality of the reconstructed image. In practice, the super-resolution model is pre-trained solely based on pixel distance, followed by fine-tuning the parameters in the model based on adversarial loss and perceptual loss. Finally, we fuse two different models by weighted-summing their parameters to obtain the final super-resolution model. Experimental results demonstrated that the proposed method achieves excellent performance in the real-world image super-resolution challenge. We have also verified that the proposed dual reconstruction does further improve the quality of the reconstructed image in terms of both PSNR and SSIM. © 2019 IEEE.","Computer vision; Deep learning; Image enhancement; Optical resolving power; Adversarial networks; ESRGAN; Generative adversarial nets; Gradient informations; High resolution image; Reconstructed image; Super resolution; Super-resolution models; Image reconstruction","Deep learning; Dual reconstruction; ESRGAN; Generative adversarial nets; Super resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85082451563"
"Lee W.-Y.; Chuang P.-Y.; Wang Y.-C.F.","Lee, Wei-Yu (57209878733); Chuang, Po-Yu (57209890402); Wang, Yu-Chiang Frank (35216822800)","57209878733; 57209890402; 35216822800","Perceptual Quality Preserving Image Super-resolution via Channel Attention","2019","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2019-May","","8683507","1737","1741","4","10.1109/ICASSP.2019.8683507","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068981101&doi=10.1109%2fICASSP.2019.8683507&partnerID=40&md5=e19eac7ab2e5880d60b37feb0a062e14","Generative Adversarial Network (GAN) has been widely applied on Single Image Super-Resolution (SISR) problems. However, there can be quite a variability in the results from the GAN-based methods. In some cases, the GAN-based methods might cause structure distortion, which can be easily distinguished by human beings, especially for artificial structures, because the methods only focus on the perceptual quality of the whole image. On the other hand, PSNR-oriented methods can prevent structure distortion but with overly smoothed context. To overcome these problems, we propose a deep neural net refiner for SISR methods, not only improving perceptual quality but also preserving context structures. In the experiments, our model qualitatively and quantitatively performs favorably against the state-of-the-art SISR methods. © 2019 IEEE.","Deep neural networks; Optical resolving power; Speech communication; Adversarial networks; Artificial structures; Channel Attention; Image super resolutions; Perceptual quality; State of the art; Structure distortions; Super resolution; Audio signal processing","Channel Attention; Generative Adversarial Networks; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85068981101"
"Liu H.; Wang F.; Liu L.","Liu, Han (56177336900); Wang, Fan (57202218151); Liu, Lijun (57212123011)","56177336900; 57202218151; 57212123011","Image super-resolution reconstruction based on an improved generative adversarial network","2019","1st International Conference on Industrial Artificial Intelligence, IAI 2019","","","8850808","","","","10.1109/ICIAI.2019.8850808","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073499190&doi=10.1109%2fICIAI.2019.8850808&partnerID=40&md5=43b3235b47f37089b4856e61d2f0f665","To solve the problem that images reconstructed by traditional super-resolution reconstruction (SR) techniques are smooth and lack good details, in this paper, we have presented an improved generative adversarial network for image super-resolution. The improved method was based on deep neural networks whose generative model contained a multi-layer convolution module and multi-layer deconvolution module, in which a layer hopping connection and a loss function was added to the perceptual loss. The discriminant model was made up of a multi-layer neural network whose loss function was based on the discriminant model loss function that was generated from the generative adversarial network. Finally, we selected PSNR and SSIM as the indicator in the experiments. In the experiments, the PSNR value of 2x, 3x and 4x magnification factor are improved on average by 1.125, 2.175 and 2.075 respectively and the SSIM value of 2x, 3x and 4x magnification factor are basically improved. Compared with the existing image super-resolution reconstruction methods, the effectiveness of the method that we proposed in image super-resolution reconstruction was proven. © 2019 IEEE.","Convolution; Deep neural networks; Image enhancement; Image reconstruction; Network layers; Optical resolving power; Adversarial networks; Convolutional networks; Discriminant models; Generative model; Image super resolutions; Image super-resolution reconstruction; Magnification factors; Super resolution reconstruction; Multilayer neural networks","Convolutional Network; Generative Adversarial Network; Image Super-resolution; Residual Network","Conference paper","Final","","Scopus","2-s2.0-85073499190"
"Jiang R.; Li X.; Mei S.; Li L.; Yue S.; Zhang L.","Jiang, Ruituo (57212484996); Li, Xu (55273488000); Mei, Shaohui (25822578400); Li, Lixin (55730828600); Yue, Shigang (7102296450); Zhang, Lei (57207389437)","57212484996; 55273488000; 25822578400; 55730828600; 7102296450; 57207389437","Learning Spatial and Spectral Features VIA 2D-1D Generative Adversarial Network for Hyperspectral Image Super-Resolution","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8803200","2149","2153","4","10.1109/ICIP.2019.8803200","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076810874&doi=10.1109%2fICIP.2019.8803200&partnerID=40&md5=3e51725f1faafe9c0f076b9188596395","Three-dimensional (3D) convolutional networks have been proven to be able to explore spatial context and spectral information simultaneously for super-resolution (SR). However, such kind of network can't be practically designed very 'deep' due to the long training time and GPU memory limitations involved in 3D convolution. Instead, in this paper, spatial context and spectral information in hyperspectral images (HSIs) are explored using Two-dimensional (2D) and One-dimenional (1D) convolution, separately. Therefore, a novel 2D-1D generative adversarial network architecture (2D-1D-HSRGAN) is proposed for SR of HSIs. Specifically, the generator network consists of a spatial network and a spectral network, in which spatial network is trained with the least absolute deviations loss function to explore spatial context by 2D convolution and spectral network is trained with the spectral angle mapper (SAM) loss function to extract spectral information by 1D convolution. Experimental results over two real HSIs demonstrate that the proposed 2D-1D-HSRGAN clearly outperforms several state-of-the-art algorithms. © 2019 IEEE.","","generative adversarial network; Hyperspectral images; super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076810874"
"Upadhyay U.; Awate S.P.","Upadhyay, Uddeshya (57211269778); Awate, Suyash P. (8836988800)","57211269778; 8836988800","A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11768 LNCS","","","556","564","8","10.1007/978-3-030-32254-0_62","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075646744&doi=10.1007%2f978-3-030-32254-0_62&partnerID=40&md5=e7bcdeb162a814c14500f18faaaf2afd","Deep neural networks for image quality enhancement typically need large quantities of highly-curated training data comprising pairs of low-quality images and their corresponding high-quality images. While high-quality image acquisition is typically expensive and time-consuming, medium-quality images are faster to acquire, at lower equipment costs, and available in larger quantities. Thus, we propose a novel generative adversarial network (GAN) that can leverage training data at multiple levels of quality (e.g., high and medium quality) to improve performance while limiting costs of data curation. We apply our mixed-supervision GAN to (i) super-resolve histopathology images and (ii) enhance laparoscopy images by combining super-resolution and surgical smoke removal. Results on large clinical and pre-clinical datasets show the benefits of our mixed-supervision GAN over the state of the art. © 2019, Springer Nature Switzerland AG.","Data curation; Deep neural networks; Image acquisition; Image quality; Large dataset; Medical computing; Medical imaging; Optical resolving power; Quality control; Smoke; Surgery; Adversarial networks; Image quality enhancements; Mixed-supervision; Smoke removal; Super resolution; Image enhancement","Generative adversarial network (GAN); Image quality enhancement; Mixed-supervision; Super-resolution; Surgical smoke removal","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85075646744"
"Wu X.; Lucas A.; Lopez-Tapia S.; Wang X.; Kim Y.H.; Molina R.; Katsaggelos A.K.","Wu, Xinyi (57216191150); Lucas, Alice (57200297854); Lopez-Tapia, Santiago (57196006294); Wang, Xijun (57209886240); Kim, Yul Hee (57211990325); Molina, Rafael (34870201500); Katsaggelos, Aggelos K. (7102711302)","57216191150; 57200297854; 57196006294; 57209886240; 57211990325; 34870201500; 7102711302","Semantic prior based generative adversarial network for video super-resolution","2019","European Signal Processing Conference","2019-September","","","","","","10.23919/EUSIPCO.2019.8902987","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075599127&doi=10.23919%2fEUSIPCO.2019.8902987&partnerID=40&md5=28be2f66cb6d78b960704b6acd467e90","Semantic information is widely used in the deep learning literature to improve the performance of visual media processing. In this work, we propose a semantic prior based Generative Adversarial Network (GAN) model for video super-resolution. The model fully utilizes various texture styles from different semantic categories of video-frame patches, contributing to more accurate and efficient learning for the generator. Based on the GAN framework, we introduce the semantic prior by making use of the spatial feature transform during the learning process of the generator. The patch-wise semantic prior is extracted on the whole video frame by a semantic segmentation network. A hybrid loss function is designed to guide the learning performance. Experimental results show that our proposed model is advantageous in sharpening video frames, reducing noise and artifacts, and recovering realistic textures. © 2019 IEEE","Deep learning; Optical resolving power; Semantic Web; Signal processing; Textures; Adversarial networks; Loss functions; Semantic segmentation; Spatial features; Video super-resolution; Semantics","Generative Adversarial Networks; Hybrid loss function; Semantic Segmentation; Spatial Feature Transform; Video Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85075599127"
"Lin K.; Liu Y.; Li T.H.; Liu S.; Li G.","Lin, Kai (57200647276); Liu, Yubao (57216376114); Li, Thomas H. (57200647563); Liu, Shan (55600731300); Li, Ge (57208803757)","57200647276; 57216376114; 57200647563; 55600731300; 57208803757","Text image super-resolution by image matting and text label supervision","2019","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2019-June","","9025615","1722","1727","5","10.1109/CVPRW.2019.00222","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083341889&doi=10.1109%2fCVPRW.2019.00222&partnerID=40&md5=6c322239d83b8bc9927e46c55a5967e8","These days, many methods have been proposed to deal with nature image super-resolution (SR) and get impressive performance. However, these methods don't do well in text images SR due to their ignorance of the difference between nature images and text images. In this paper, we propose a matting-based dual generative adversarial network (mdGAN) for text image SR. Firstly, the input image is decomposed into text, foreground and background layers using deep image matting. Then two parallel branches are constructed to recover text boundary information and color information respectively. Furthermore, in order to improve the restoration accuracy of characters in output image, we use the input image's corresponding ground truth text label as extra supervise information to refine the two-branch networks during training. Experiments on real text images demonstrate that our method outperforms several state-of-the-art methods quantitatively and qualitatively. © 2019 IEEE.","Computer vision; Optical resolving power; Adversarial networks; Boundary information; Color information; Ground truth; Image matting; Image super resolutions; Parallel branches; State-of-the-art methods; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85083341889"
"Lopez-Tapia S.; Lucas A.; Molina R.; Katsaggelos A.K.","Lopez-Tapia, Santiago (57196006294); Lucas, Alice (57200297854); Molina, Rafael (34870201500); Katsaggelos, Aggelos K. (7102711302)","57196006294; 57200297854; 34870201500; 7102711302","Gan-Based Video Super-Resolution with Direct Regularized Inversion of the Low-Resolution Formation Model","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8803709","2886","2890","4","10.1109/ICIP.2019.8803709","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076817145&doi=10.1109%2fICIP.2019.8803709&partnerID=40&md5=310de51863272464532cdd7093035a97","While high and ultra high definition displays are becoming popular, most of the available content has been acquired at much lower resolutions. In this work we propose to pseudo-invert with regularization the image formation model using GANs and perceptual losses. Our model, which does not require the use of motion compensation, utilizes explicitly the low resolution image formation model and additionally introduces two feature losses which are used to obtain perceptually improved high resolution images. The experimental validation shows that our approach outperforms current video super resolution learning based models. © 2019 IEEE.","","Convolutional Neuronal Networks; Generative Adversarial Networks; Perceptual Loss Functions; Super-resolution; Video","Conference paper","Final","","Scopus","2-s2.0-85076817145"
"Mobini M.; Ghaderi F.","Mobini, Majid (57195069895); Ghaderi, Foad (55882794000)","57195069895; 55882794000","StarGAN Based Facial Expression Transfer for Anime Characters","2020","2020 25th International Computer Conference, Computer Society of Iran, CSICC 2020","","","9050061","","","","10.1109/CSICC49403.2020.9050061","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083363659&doi=10.1109%2fCSICC49403.2020.9050061&partnerID=40&md5=5b5c76fe40e47ee60b1c9bb6e8847887","Human facial expression transfer has been well explored using Generative Adversarial Networks. Also, in case of anime style images, several successful attempts have been made to generate high-quality anime face images using GAN approach. However, the task of anime facial expression transfer is not well studied yet due to the lack of a clean labeled anime dataset. We address this issue from both data and model perspectives, by providing a clean labeled anime dataset and leveraging the use of the StarGAN image-to-image translation framework. Our collected dataset consists of about 5k high-quality anime face images including five major emotions collected from online image boards. We preprocessed our dataset by CARN super-resolution technique to improve quality of the images, and applied tuned StarGAN model to learn the mapping of an input anime image with arbitrary expression to the target expression. We evaluate our work by visually comparing the output translated results with the baseline model. Moreover, we provide a quantitative analysis of our proposed approach by computing the confusion matrix of expression transfer accuracy. © 2020 IEEE.","Labeled data; Social computing; Adversarial networks; Baseline models; Confusion matrices; Expression transfers; Facial Expressions; Human facial expressions; Image translation; Super resolution; Image enhancement","Anime Generation; Facial Expression Transfer; Generative Adversarial Network; Unpaired Image Translation","Conference paper","Final","","Scopus","2-s2.0-85083363659"
"Ngxande M.; Tapamo J.-R.; Burke M.","Ngxande, Mkhuseli (57202813365); Tapamo, Jules-Raymond (55916165800); Burke, Michael (36099859700)","57202813365; 55916165800; 36099859700","DepthwiseGANs: Fast Training Generative Adversarial Networks for Realistic Image Synthesis","2019","Proceedings - 2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa, SAUPEC/RobMech/PRASA 2019","","","8704766","111","116","5","10.1109/RoboMech.2019.8704766","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065815116&doi=10.1109%2fRoboMech.2019.8704766&partnerID=40&md5=082f0d0256b2a79c22a766db249fdfb8","Recent work has shown significant progress in the direction of synthetic data generation using Generative Adversarial Networks (GANs). GANs have been applied in many fields of computer vision including text-to-image conversion, domain transfer, super-resolution, and image-to-video applications. In computer vision, traditional GANs are based on deep convolutional neural networks. However, deep convolutional neural networks can require extensive computational resources because they are based on multiple operations performed by convolutional layers, which can consist of millions of trainable parameters. Training a GAN model can be difficult and it takes a significant amount of time to reach an equilibrium point In this paper, we investigate the use of depthwise separable convolutions to reduce training time while maintaining data generation performance. Our results show that a DepthwiseGAN architecture can generate realistic images in shorter training periods when compared to a StarGan architecture, but that model capacity still plays a significant role in generative modelling. In addition, we show that depthwise separable convolutions perform best when only applied to the generator. For quality evaluation of generated images, we use the Fréchet Inception Distance (FID), which compares the similarity between the generated image distribution and that of the training dataset. © 2019 IEEE.","Computer vision; Convolution; Deep neural networks; Neural networks; Adversarial networks; Computational resources; Convolutional neural network; GANs; Multiple operations; Realistic image synthesis; Synthetic data; Synthetic data generations; Network architecture","Depthwise Separable Convolution; FID; GANs; Synthetic Data","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065815116"
"Liu H.; Zheng X.; Han J.; Chu Y.; Tao T.","Liu, Heng (57022065500); Zheng, Xiaoyu (57212537068); Han, Jungong (14522692900); Chu, Yuezhong (34978302900); Tao, Tao (55420313200)","57022065500; 57212537068; 14522692900; 34978302900; 55420313200","Survey on GAN-based face hallucination with its model development","2019","IET Image Processing","13","14","","2662","2672","10","10.1049/iet-ipr.2018.6545","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077438473&doi=10.1049%2fiet-ipr.2018.6545&partnerID=40&md5=bb3e16565e51cc13d666fbe0e48a5f5e","Face hallucination aims to produce a high-resolution face image from an input low-resolution face image, which is of great importance for many practical face applications, such as face recognition and face verification. Since the structure of the face image is complex and sensitive, obtaining a super-resolved face image is more difficult than generic image super-resolution. Recently, with great success in the high-level face recognition task, deep learning methods, especially generative adversarial networks (GANs), have also been applied to the low-level vision task – face hallucination. This work is to provide a model evolvement survey on GAN-based face hallucination. The principles of image resolution degradation and GAN-based learning are presented firstly. Then, a comprehensive review of the state-of-art GAN-based face hallucination methods is provided. Finally, the comparisons of these GAN-based face hallucination methods and the discussions of the related issues for future research direction are also provided. © The Institution of Engineering and Technology 2019.","Deep learning; Image resolution; Surveys; Adversarial networks; Face hallucination; Face Verification; Future research directions; Learning methods; Low-level vision; Low-resolution face images; Model development; Face recognition","","Article","Final","","Scopus","2-s2.0-85077438473"
"Ui Hoque M.R.; Burks R.; Kwan C.; Li J.","Ui Hoque, Md Reshad (57215223328); Burks, Roland (57215221542); Kwan, Chiman (7201421216); Li, Jiang (56226550100)","57215223328; 57215221542; 7201421216; 56226550100","Deep Learning for Remote Sensing Image Super-Resolution","2019","2019 IEEE 10th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2019","","","8993047","0286","0292","6","10.1109/UEMCON47517.2019.8993047","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080145619&doi=10.1109%2fUEMCON47517.2019.8993047&partnerID=40&md5=736354c0ac5bc10838c078f6109ef3a9","The aim of image super-Resolution (SR) is to enhance image resolution while still retain the integrity of the original image. There are many ongoing types of research on image super-resolution for natural images, but any a few on remote sensing images. In this paper, we proposed deep learning-based image super-resolution techniques, including convolutional neural network (CNN) and generative adversarial network (GAN) to enhance the resolution of remote sensing images by a factor 4. In CNN, it learns an end to end mapping from low-resolution image to high-resolution image whereas, in GAN, the model learns the mapping guided by the GAN loss and gives the sharper appearance in high-resolution images. Our experimental results show that visually GAN models perform well but are inferior to other models in terms of image quality metrics, whereas quantitatively CNN models outperform other super-resolution models. © 2019 IEEE.","Convolutional neural networks; Image enhancement; Image resolution; Learning systems; Mapping; Mobile telecommunication systems; Optical resolving power; Remote sensing; Ubiquitous computing; Adversarial networks; High resolution image; Image quality metrics; Image super resolutions; Low resolution images; Remote sensing images; Super resolution; Super-resolution models; Deep learning","CNN; Deep Learning; GAN; Machine Learning; Remote sensing image; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85080145619"
"Majdabadi M.M.; Ko S.-B.","Majdabadi, Mahdiyar Molahasani (57216440558); Ko, Seok-Bum (7403326100)","57216440558; 7403326100","MSG-CapsGAN: Multi-Scale Gradient Capsule GAN for Face Super Resolution","2020","2020 International Conference on Electronics, Information, and Communication, ICEIC 2020","","","9051244","","","","10.1109/ICEIC49074.2020.9051244","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083483222&doi=10.1109%2fICEIC49074.2020.9051244&partnerID=40&md5=9a72e3e8b032c58f32b050d50f23ca60","One of the most useful sub-fields of Super-Resolution (SR) is face SR. Given a Low-Resolution (LR) image of a face, the High-Resolution (HR) counterpart is demanded. However, performing SR task on extremely low resolution images is very challenging due to the image distortion in the HR results. Many deep learning-based SR approaches have intended to solve this issue by using attribute domain information. However, they require more complex data and even additional networks. To simplify this process and yet preserve the precision, a novel Multi-Scale Gradient GAN with Capsule Network as its discriminator is proposed in this paper. MSG-CapsGAN surpassed the state-of-the-art face SR networks in terms of PSNR. This network is a step towards a precise pose invariant SR system. © 2020 IEEE.","Deep learning; Domain informations; Face super-resolution; High resolution; Image distortions; Low resolution images; Pose invariant; State of the art; Super resolution; Optical resolving power","Capsule Network; Generative Adversarial Network (GAN); Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85083483222"
"Zheng X.; Liu H.; Han J.; Hou S.","Zheng, Xiaoyu (57212537068); Liu, Heng (57022065500); Han, Jungong (14522692900); Hou, Shudong (36508758900)","57212537068; 57022065500; 14522692900; 36508758900","Deep feature-preserving based face hallucination: Feature discrimination versus pixels approximation","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11858 LNCS","","","114","125","11","10.1007/978-3-030-31723-2_10","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076972392&doi=10.1007%2f978-3-030-31723-2_10&partnerID=40&md5=ca364940a919e36314240f1077fb7feb","Face hallucination aims to produce a high-resolution (HR) face image from an input low-resolution (LR) face image, which is of great importance for many practical face applications, such as face recognition and face verification. Since the structure features of face image is complex and sensitive, obtaining a super-resolved face image is more difficult than generic image super-resolution (SR). To address these limitations, we present a novel GAN (Generative adversarial network) based feature-preserving face hallucination approach for very low resolution (16 × 16 pixels) faces and large scale upsampling (8×). Specifically, we design a new residual structure based face generator and adopt two different discriminators-an image discriminator and a feature discriminator, to encourage the model to acquire more realistic face features rather than artifacts. The evaluations based on both PSNR and visual result reveal that the proposed model is superior to the state-of-the-art methods. © Springer Nature Switzerland AG 2019.","Computer vision; Pixels; Adversarial networks; Face hallucination; Face Verification; Feature discrimination; Feature preserving; Residual structure; State-of-the-art methods; Structure features; Face recognition","Face hallucination; Feature discrimination; GAN","Conference paper","Final","","Scopus","2-s2.0-85076972392"
"Wang Y.; Perazzi F.; McWilliams B.; Sorkine-Hornung A.; Sorkine-Hornung O.; Schroers C.","Wang, Yifan (57205634907); Perazzi, Federico (55365483100); McWilliams, Brian (36194109800); Sorkine-Hornung, Alexander (55681538300); Sorkine-Hornung, Olga (55693591100); Schroers, Christopher (55351985500)","57205634907; 55365483100; 36194109800; 55681538300; 55693591100; 55351985500","A fully progressive approach to single-image super-resolution","2018","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June","","8575284","977","986","9","10.1109/CVPRW.2018.00131","138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060885279&doi=10.1109%2fCVPRW.2018.00131&partnerID=40&md5=3e8659ccc98eca7a76587836df5b3ed4","Recent deep learning approaches to single image superresolution have achieved impressive results in terms of traditional error measures and perceptual quality. However, in each case it remains challenging to achieve high quality results for large upsampling factors. To this end, we propose a method (ProSR) that is progressive both in architecture and training: the network upsamples an image in intermediate steps, while the learning process is organized from easy to hard, as is done in curriculum learning. To obtain more photorealistic results, we design a generative adversarial network (GAN), named ProGanSR, that follows the same progressive multi-scale design principle. This not only allows to scale well to high upsampling factors (e.g., 8×) but constitutes a principled multi-scale approach that increases the reconstruction quality for all upsampling factors simultaneously. In particular ProSR ranks 2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge [35]. Compared to the top-ranking team, our model is marginally lower, but runs 5 times faster. © 2018 IEEE.","Computer vision; Optical resolving power; Signal sampling; Adversarial networks; Learning approach; Learning process; Multi-scale approaches; Multi-scale design; Perceptual quality; Photo-realistic; Reconstruction quality; Deep learning","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85060885279"
"Tian B.; Yan W.; Wang W.; Su Q.; Liu Y.; Liu G.; Wang W.","Tian, Bing (57211813016); Yan, Wentao (57202206405); Wang, Wei (57206569650); Su, Qi (57220832362); Liu, Yin (57203137818); Liu, Guangxiu (57203135142); Wang, Wanguo (55491210500)","57211813016; 57202206405; 57206569650; 57220832362; 57203137818; 57203135142; 55491210500","Super-resolution deblurring algorithm for generative adversarial networks","2018","Proceedings - 2017 2nd International Conference on Mechanical, Control and Computer Engineering, ICMCCE 2017","2018-January","","","135","140","5","10.1109/ICMCCE.2017.56","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050621277&doi=10.1109%2fICMCCE.2017.56&partnerID=40&md5=146cb28e3b97995334b026c5d7a2dd11","Image quality improvement has a significant impact on target detection and recognition. Generative Adversarial Nets are inspired by two-person zero-sum game in game theory. It can learn to automatically generate images, which can be conditional learning, a guide to the image generation. In this paper, we analyze the characteristics of motion blur, and propose a method to add the defocused fuzzy kernel and multi-direction motion fuzzy kernel to the training samples, and use the super-resolution anti-network method to exercise blur and carry out the fuzzy image data recorded by the UAV experiment analysis. © 2017 IEEE.","Game theory; Optical resolving power; Adversarial networks; Deblurring algorithms; Experiment analysis; Fuzzy kernel; Image quality improvements; Super resolution; Target detection and recognition; Two-person zero-sum game; Image enhancement","defocused fuzzy kernel; generative adversarial networks; motion fuzzy kernel; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85050621277"
"","","","5th International Workshop on Computational Methods for Molecular Imaging, CMMI 2017, 2nd International Workshop on Reconstruction and Analysis of Moving Body Organs, RAMBO 2017 and 1st International Stroke Workshop on Imaging and Treatment Challenges, SWITCH 2017 held in Conjunction with 20th International Conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2017","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10555 LNCS","","","1","184","183","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029587411&partnerID=40&md5=1d4c48afc05846114e1ac688f0d8817f","The proceedings contain 18 papers. The special focus in this conference is on Computational Methods for Molecular Imaging. The topics include: Individual analysis of molecular brain imaging data through automatic identification of abnormality patterns; 3D alpha matting based co-segmentation of tumors on PET-CT images; dynamic respiratory motion estimation using patch-based kernel-PCA priors for lung cancer radiotherapy; mass transportation for deformable image registration with application to lung ct; motion-robust spatially constrained parameter estimation in renal diffusion weighted MRI by 3D motion tracking and correction of sequential slices; semi-automatic cardiac and respiratory gated MRI for cardiac assessment during exercise; freehand ultrasound image simulation with spatially-conditioned generative adversarial networks; context-sensitive super-resolution for fast fetal magnetic resonance imaging; reconstruction of 3D cardiac MR images from 2D slices using directional total variation; automated ventricular system segmentation in CT images of deformed brains due to ischemic and subarachnoid hemorrhagic stroke; towards automatic collateral circulation score evaluation in ischemic stroke using image decompositions and support vector machines; the effect of non-contrast ct slice thickness on thrombus density and perviousness assessment.","","","Conference review","Final","","Scopus","2-s2.0-85029587411"
"Kim D.-W.; Chung J.-R.; Kim J.; Lee D.Y.; Jeong S.Y.; Jung S.-W.","Kim, Dong-Wook (57205529276); Chung, Jae-Ryun (57208125681); Kim, Jongho (57203325061); Lee, Dae Yeol (57748527400); Jeong, Se Yoon (7402425053); Jung, Seung-Won (25223220400)","57205529276; 57208125681; 57203325061; 57748527400; 7402425053; 25223220400","Constrained adversarial loss for generative adversarial network-based faithful image restoration","2019","ETRI Journal","41","4","","415","425","10","10.4218/etrij.2018-0473","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065667452&doi=10.4218%2fetrij.2018-0473&partnerID=40&md5=af1fbe44aa2c9aadd1c4c6f6710c5a7c","Generative adversarial networks (GAN) have been successfully used in many image restoration tasks, including image denoising, super-resolution, and compression artifact reduction. By fully exploiting its characteristics, state-of-the-art image restoration techniques can be used to generate images with photorealistic details. However, there are many applications that require faithful rather than visually appealing image reconstruction, such as medical imaging, surveillance, and video coding. We found that previous GAN-training methods that used a loss function in the form of a weighted sum of fidelity and adversarial loss fails to reduce fidelity loss. This results in non-negligible degradation of the objective image quality, including peak signal-to-noise ratio. Our approach is to alternate between fidelity and adversarial loss in a way that the minimization of adversarial loss does not deteriorate the fidelity. Experimental results on compression-artifact reduction and super-resolution tasks show that the proposed method can perform faithful and photorealistic image restoration. © 2019 ETRI","Deep learning; Image coding; Image denoising; Image quality; Medical imaging; Optical resolving power; Restoration; Security systems; Signal to noise ratio; Video signal processing; Adversarial networks; Compression artifacts; Objective image quality; Peak signal to noise ratio; Photorealistic images; State of the art; Super resolution; Training methods; Image reconstruction","compression artifact reduction; deep learning; generative adversarial network; image restoration","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85065667452"
"Chen L.; Dan W.; Cao L.; Wang C.; Li J.","Chen, Li (57192670726); Dan, Wen (57205365148); Cao, Liujuan (35749499000); Wang, Cheng (36990982800); Li, Jonathan (57235557700)","57192670726; 57205365148; 35749499000; 36990982800; 57235557700","Joint Denoising and Super-Resolution via Generative Adversarial Training","2018","Proceedings - International Conference on Pattern Recognition","2018-August","","8546286","2753","2758","5","10.1109/ICPR.2018.8546286","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059752322&doi=10.1109%2fICPR.2018.8546286&partnerID=40&md5=514b3f5c7d5c809884d0a4046fe778eb","Single image denoising and super-resolution are sitting in the core of various image processing and pattern recognition applications. Typically, these two tasks are handled separately, without regarding to joint reinforcement and learning. The former deals with equal-size pixel-to-pixel translation, while the latter deals with scaling up amount of input pixels. In this paper, we propose a Generative Adversarial Network(GAN) towards joint learning of single image denoising and super-resolution. In principle, our design allows both tasks to share several common building blocks, with the linking between both outputs to reinforce each other. Such a reinforcement is accomplished via designing a novel generative network through optimizing a novel loss function to achieve both denoising and super-resolution. Quantitatively comparing to a set of alternative approaches and baselines, the experiment demonstrated superior performance our method in denoising and super-resolution with high upscaling factors. © 2018 IEEE.","Optical resolving power; Pattern recognition; Pixels; Reinforcement; Adversarial networks; Building blockes; De-noising; Equal sizes; Joint learning; Joint reinforcement; Loss functions; Super resolution; Image denoising","Denoising; GAN; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85059752322"
"Bao J.; Chen D.; Wen F.; Li H.; Hua G.","Bao, Jianmin (57200622052); Chen, Dong (57937101800); Wen, Fang (36549728200); Li, Houqiang (35956273100); Hua, Gang (57215375265)","57200622052; 57937101800; 36549728200; 35956273100; 57215375265","CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training","2017","Proceedings of the IEEE International Conference on Computer Vision","2017-October","","8237561","2764","2773","9","10.1109/ICCV.2017.299","260","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041929941&doi=10.1109%2fICCV.2017.299&partnerID=40&md5=bc20eea182cdac9a394de59bc858c1c4","We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models. © 2017 IEEE.","Computer vision; Face recognition; Signal encoding; Adversarial networks; Asymmetric loss function; Attribute vectors; Data augmentation; Feature matching; Image generations; Probabilistic modeling; Recognition models; Image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85041929941"
"Ge W.; Gong B.; Yu Y.","Ge, Weifeng (56462106200); Gong, Bingchen (56963430600); Yu, Yizhou (8554163500)","56462106200; 56963430600; 8554163500","Image super-resolution via deterministic-stochastic synthesis and local statistical rectification","2018","ACM Transactions on Graphics","37","6","260","","","","10.1145/3272127.3275060","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064826382&doi=10.1145%2f3272127.3275060&partnerID=40&md5=16ccfb8509f68478f38b1024b16f4ed7","Single image superresolution has been a popular research topic in the last two decades and has recently received a new wave of interest due to deep neural networks. In this paper, we approach this problem from a different perspective. With respect to a downsampled low resolution image, we model a high resolution image as a combination of two components, a deterministic component and a stochastic component. The deterministic component can be recovered from the low-frequency signals in the downsampled image. The stochastic component, on the other hand, contains the signals that have little correlation with the low resolution image. We adopt two complementary methods for generating these two components. While generative adversarial networks are used for the stochastic component, deterministic component reconstruction is formulated as a regression problem solved using deep neural networks. Since the deterministic component exhibits clearer local orientations, we design novel loss functions tailored for such properties for training the deep regression network. These two methods are first applied to the entire input image to produce two distinct high-resolution images. Afterwards, these two images are fused together using another deep neural network that also performs local statistical rectification, which tries to make the local statistics of the fused image match the same local statistics of the groundtruth image. Quantitative results and a user study indicate that the proposed method outperforms existing state-of-the-art algorithms with a clear margin. © 2018 Association for Computing Machinery.","Deep learning; Deep neural networks; Optical resolving power; Stochastic models; Deterministic component; Gram matrices; Image super-resolution; Local correlations; Stochastic component; Stochastic systems","Deep learning; Deterministic component; Image superresolution; Local correlation matrix; Local gram matrix; Stochastic component","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85064826382"
"Lat A.; Jawahar C.V.","Lat, Ankit (57205368023); Jawahar, C.V. (6701503911)","57205368023; 6701503911","Enhancing OCR Accuracy with Super Resolution","2018","Proceedings - International Conference on Pattern Recognition","2018-August","","8545609","3162","3167","5","10.1109/ICPR.2018.8545609","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059734337&doi=10.1109%2fICPR.2018.8545609&partnerID=40&md5=6e3d031f7ad3e0a1f391169e51ef4fc2","Accuracy of OCR is often marred by the poor quality of the input document images. Generally this performance degradation is attributed to the resolution and quality of scanning. This calls for special efforts to improve the quality of document images before passing it to the OCR engine. One compelling option is to super-resolve these low resolution document images before passing them to the OCR engine. In this work we address this problem by super-resolving document images using Generative Adversarial Network (GAN). We propose a super resolution based preprocessing step that can enhance the accuracies of the OCRs (including the commercial ones). Our method is specially suited for printed document images. We validate the utility in wide variety of document images (where fonts, styles, and languages vary) without any pre-processing step to adapt across situations. Our experiments show an improvement upto 21% in accuracy OCR on test images scanned at low resolution. One immediate application of this can be in enhancing the recognition of historic documents which have been scanned at low resolutions. © 2018 IEEE.","Engines; Optical resolving power; Pattern recognition; Adversarial networks; Document images; Low resolution; Performance degradation; Pre-processing step; Printed documents; Super resolution; Test images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85059734337"
"Sønderby C.K.; Caballero J.; Theis L.; Shi W.; Huszár F.","Sønderby, Casper Kaae (56955507900); Caballero, Jose (56410493800); Theis, Lucas (54883612000); Shi, Wenzhe (36631090300); Huszár, Ferenc (55233726100)","56955507900; 56410493800; 54883612000; 36631090300; 55233726100","Amortised map inference for image super-resolution","2017","5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings","","","","","","","","89","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081923574&partnerID=40&md5=e0ab726b13bac899c5a6e92f32eb7330","Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss. However, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is nontrivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for amortised MAP inference whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. Using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e. g. variational autoencoders. © ICLR 2019 - Conference Track Proceedings. All rights reserved.","Maximum likelihood estimation; Mean square error; Network architecture; Neural networks; Optical resolving power; Adversarial networks; Convolutional neural network; High resolution image; High-resolution output; Image super resolutions; Novel neural network; Optimisation problems; Variational inference; Inverse problems","","Conference paper","Final","","Scopus","2-s2.0-85081923574"
"","","","19th Pacific-Rim Conference on Multimedia, PCM 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11164 LNCS","","","","","2541","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057246470&partnerID=40&md5=64e6c86a5145909e8190f3f5dcb1b542","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.","","","Conference review","Final","","Scopus","2-s2.0-85057246470"
"Chin J.; Mehmood A.","Chin, Jonathan (57211081108); Mehmood, Asif (36133731600)","57211081108; 36133731600","Generative adversarial networks based super resolution of satellite aircraft imagery","2019","Proceedings of SPIE - The International Society for Optical Engineering","10995","","109950W","","","","10.1117/12.2524720","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070258802&doi=10.1117%2f12.2524720&partnerID=40&md5=75e4bc319f16b44aeac3d4bc6d3c1527","Generative Adversarial Networks (GANs) are one of the most popular Machine Learning algorithms developed in recent times, and are a class of neural networks that are used in unsupervised machine learning. The advantage of unsupervised machine learning approaches such as GANs is that they do not need a large amount of labeled data, which is costly and time consuming. GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. In this work, GANs are utilized to solve the single image super-resolution problem. This approach in literature is referred to as super resolution GANs (SRGAN), and employs a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes the solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and the original photo-realistic images, and the content loss is motivated by the perceptual similarity and not the similarity in the pixel space. This paper presents implementation of SRGAN using Deep convolution network applied to both the aerial and satellite imagery of the aircrafts. The results thus obtained are compared with traditional super resolution methods. The resulting estimates of SRGAN are compared against the traditional methods using peak signal to noise ratio (PSNR) and structure similarity index metric (SSIM). The PSNR and SSIM of SRGAN estimates are similar to traditional method such as Bicubic interpolation but traditional methods are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. © 2019 SPIE. Downloading of the abstract is permitted for personal use only.","Aircraft; Antennas; Convolution; Deep learning; Deep neural networks; Learning algorithms; Neural networks; Optical resolving power; Pattern recognition; Satellite imagery; Semantics; Signal to noise ratio; Convolutional neural network; Image super resolutions; Peak signal to noise ratio; Perceptual similarity; Single images; Super resolution; Superresolution methods; Unsupervised machine learning; Machine learning","Convolutional Neural Network; Deep Learning; Satellite Imagery; Single Image Super Resolution; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85070258802"
"Liu T.; De Haan K.; Rivenson Y.; Wei Z.; Zeng X.; Zhang Y.; Ozcan A.","Liu, Tairan (57204707339); De Haan, Kevin (57205683866); Rivenson, Yair (24081473400); Wei, Zhensong (57202577273); Zeng, Xin (57207568384); Zhang, Yibo (55910548500); Ozcan, Aydogan (7005667692)","57204707339; 57205683866; 24081473400; 57202577273; 57207568384; 55910548500; 7005667692","Enhancing resolution in coherent microscopy using deep learning","2019","Optics InfoBase Conference Papers","Part F127-CLEO_AT 2019","","","","","","10.1364/CLEO_AT.2019.AM2I.6","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068116455&doi=10.1364%2fCLEO_AT.2019.AM2I.6&partnerID=40&md5=752f2a954a19cf7e8a516fbe6e5d2cf7","A generative adversarial network (GAN) based super-resolution framework is presented. This deep learning-based framework is capable of enhancing the resolution of coherent imaging systems in both pixel size-limited and diffraction-limited microscopy systems. © 2019 The Author(s)","Image resolution; Adversarial networks; Coherent imaging systems; Coherent microscopy; Diffraction limited; Microscopy systems; Pixel size; Super resolution; Deep learning","","Conference paper","Final","","Scopus","2-s2.0-85068116455"
"Chen S.; Shi D.; Sadiq M.; Zhu M.","Chen, Songkui (57204110188); Shi, Daming (56299801000); Sadiq, Muhammad (57213764623); Zhu, Meilu (57200920263)","57204110188; 56299801000; 57213764623; 57200920263","Image denoising via generative adversarial networks with detail loss","2019","ACM International Conference Proceeding Series","Part F148384","","","261","265","4","10.1145/3322645.3322656","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066928486&doi=10.1145%2f3322645.3322656&partnerID=40&md5=6ad861a4b765bda248072b5cebba364c","Image denoising is a challenging task which aims to remove additional noise and preserve all useful information. Many existing image denoising algorithms focus on improving the typical object measure, peak signal-to-noise ratio (PSNR), and take the mean square error (MSE) as their loss function to train their networks. Although these algorithms can effectively improve the PSNR on the benchmark dataset, their denoised images often lose some important details or become over-smooth in some texture-rich regions. In order to solve this problem, we introduce Generative Adversarial Networks (GAN) and perceptual loss from single image super-resolution (SISR) field into our image denoising work. The GAN and perceptual loss can help our network to better focus on the recovering of details during denoising. To understand easily, we use the term Detail Loss to represent the whole loss which includes the MSE and the perceptual loss. Besides, we propose a new convolutional neural network which achieves state-of-the-art result on PSNR. Our experimental results show that our method outperforms the current state-of-the-art methods on preserving the details during denoising. Compared with the current state-of-the-art methods, the denoised images by our method are clearer, sharper and more realistic on details. © 2019 Association for Computing Machinery.","Convolutional neural networks; Image enhancement; Mean square error; Signal to noise ratio; Textures; Adversarial networks; Benchmark datasets; Image denoising algorithm; Loss functions; Peak signal to noise ratio; Single images; State of the art; State-of-the-art methods; Image denoising","Detail Loss; Generative Adversarial Networks; Image Denoising","Conference paper","Final","","Scopus","2-s2.0-85066928486"
"Wang X.; Yu K.; Wu S.; Gu J.; Liu Y.; Dong C.; Qiao Y.; Loy C.C.","Wang, Xintao (57195942631); Yu, Ke (57195941109); Wu, Shixiang (57195942432); Gu, Jinjin (57212042330); Liu, Yihao (57206486742); Dong, Chao (56335662200); Qiao, Yu (36086392600); Loy, Chen Change (25522308800)","57195942631; 57195941109; 57195942432; 57212042330; 57206486742; 56335662200; 36086392600; 25522308800","ESRGAN: Enhanced super-resolution generative adversarial networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS","","","63","79","16","10.1007/978-3-030-11021-5_5","333","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061732156&doi=10.1007%2f978-3-030-11021-5_5&partnerID=40&md5=cedd63b57a86c69ee214bfd9a0c151e2","The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN. © Springer Nature Switzerland AG 2019.","Computer vision; Network architecture; Textures; Absolute values; Adversarial networks; Building units; Natural textures; Single images; Super resolution; Visual qualities; Optical resolving power","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85061732156"
"Albay E.; Demir U.; Unal G.","Albay, Enes (57188923094); Demir, Ugur (57810490700); Unal, Gozde (57220534209)","57188923094; 57810490700; 57220534209","Diffusion MRI Spatial Super-Resolution Using Generative Adversarial Networks","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11121 LNCS","","","155","163","8","10.1007/978-3-030-00320-3_19","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066140785&doi=10.1007%2f978-3-030-00320-3_19&partnerID=40&md5=d4b70f300fe0794f026110a5a8633f6c","Spatial resolution is one of the main constraints in diffusion Magnetic Resonance Imaging (dMRI). Increasing resolution leads to a decrease in SNR of the diffusion images. Acquiring high resolution images without reducing SNRs requires larger magnetic fields and long scan times which are typically not applicable in the clinical settings. Currently feasible voxel size is around 1 mm3 for a diffusion image. In this paper, we present a deep neural network based post-processing method to increase the spatial resolution in diffusion MRI. We utilize Generative Adversarial Networks (GANs) to obtain a higher resolution diffusion MR image in the spatial dimension from lower resolution diffusion images. The obtained real data results demonstrate a first time proof of concept that GANs can be useful in super-resolution problem of diffusion MRI for upscaling in the spatial dimension. © Springer Nature Switzerland AG 2018.","Deep neural networks; Diffusion; Generative adversarial networks; Image resolution; Medical imaging; Diffusion magnetic resonance imaging; Generative adversarial network; High-resolution images; Magnetic resonance imaging; Magnetic-field; Scan time; Spatial dimension; Spatial resolution; Superresolution; Magnetic resonance imaging","Diffusion MRI (dMRI); Generative adversarial networks (GANs); Magnetic resonance imaging (MRI); Super resolution","Conference paper","Final","","Scopus","2-s2.0-85066140785"
"Mahapatra D.; Bozorgtabar B.; Hewavitharanage S.; Garnavi R.","Mahapatra, Dwarikanath (25422481200); Bozorgtabar, Behzad (37109488100); Hewavitharanage, Sajini (57039081800); Garnavi, Rahil (15845331200)","25422481200; 37109488100; 57039081800; 15845331200","Image super resolution using generative adversarial networks and local saliency maps for retinal image analysis","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10435 LNCS","","","382","390","8","10.1007/978-3-319-66179-7_44","59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029496528&doi=10.1007%2f978-3-319-66179-7_44&partnerID=40&md5=c7045bb3fe4a3d042e7c1d6fe1604bd1","We propose an image super resolution (ISR) method using generative adversarial networks (GANs) that takes a low resolution input fundus image and generates a high resolution super resolved (SR) image upto scaling factor of 16. This facilitates more accurate automated image analysis, especially for small or blurred landmarks and pathologies. Local saliency maps, which define each pixel’s importance, are used to define a novel saliency loss in the GAN cost function. Experimental results show the resulting SR images have perceptual quality very close to the original images and perform better than competing methods that do not weigh pixels according to their importance. When used for retinal vasculature segmentation, our SR images result in accuracy levels close to those obtained when using the original images. © Springer International Publishing AG 2017.","Cost functions; Image resolution; Image segmentation; Medical computing; Medical imaging; Ophthalmology; Optical resolving power; Pixels; Adversarial networks; Automated image analysis; High resolution; Image super resolutions; Original images; Perceptual quality; Retinal image analysis; Retinal vasculature; Image analysis","","Conference paper","Final","","Scopus","2-s2.0-85029496528"
"Bulat A.; Yang J.; Tzimiropoulos G.","Bulat, Adrian (57191432700); Yang, Jing (56610891700); Tzimiropoulos, Georgios (18042589100)","57191432700; 56610891700; 18042589100","To learn image super-resolution, use a GAN to learn how to do image degradation first","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11210 LNCS","","","187","202","15","10.1007/978-3-030-01231-1_12","62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055081834&doi=10.1007%2f978-3-030-01231-1_12&partnerID=40&md5=233ad276fb77e8cd817cb4de4a78df14","This paper is on image and face super-resolution. The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling). We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images. To circumvent this problem, we propose a two-stage process which firstly trains a High-to-Low Generative Adversarial Network (GAN) to learn how to degrade and downsample high-resolution images requiring, during training, only unpaired high and low-resolution images. Once this is achieved, the output of this network is used to train a Low-to-High GAN for image super-resolution using this time paired low- and high-resolution images. Our main result is that this network can be now used to effectively increase the quality of real-world low-resolution images. We have applied the proposed pipeline for the problem of face super-resolution where we report large improvement over baselines and prior work although the proposed method is potentially applicable to other object categories. © Springer Nature Switzerland AG 2018.","Computer vision; Signal sampling; Adversarial networks; Face super-resolution; GANs; High resolution image; Image degradation; Image super resolutions; Low resolution images; Two-stage process; Optical resolving power","GANs; Generative Adversarial Networks; Image and face super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055081834"
"Xu X.; Sun D.; Pan J.; Zhang Y.; Pfister H.; Yang M.-H.","Xu, Xiangyu (57195934971); Sun, Deqing (52264559600); Pan, Jinshan (55258321100); Zhang, Yujin (56154269900); Pfister, Hanspeter (23028620700); Yang, Ming-Hsuan (7404927015)","57195934971; 52264559600; 55258321100; 56154269900; 23028620700; 7404927015","Learning to Super-Resolve Blurry Face and Text Images","2017","Proceedings of the IEEE International Conference on Computer Vision","2017-October","","8237298","251","260","9","10.1109/ICCV.2017.36","156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041915021&doi=10.1109%2fICCV.2017.36&partnerID=40&md5=a9523c50132fe2e97baf99a369634766","We present an algorithm to directly restore a clear highresolution image from a blurry low-resolution input. This problem is highly ill-posed and the basic assumptions for existing super-resolution methods (requiring clear input) and deblurring methods (requiring high-resolution input) no longer hold. We focus on face and text images and adopt a generative adversarial network (GAN) to learn a category-specific prior to solve this problem. However, the basic GAN formulation does not generate realistic highresolution images. In this work, we introduce novel training losses that help recover fine details. We also present a multi-class GAN that can process multi-class image restoration tasks, i.e., face and text images, using a single generator network. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art methods on both synthetic and real-world images at a lower computational cost. © 2017 IEEE.","Computer vision; Restoration; Adversarial networks; Category specifics; Computational costs; High resolution; High resolution image; Real-world image; State-of-the-art methods; Superresolution methods; Image reconstruction","","Conference paper","Final","","Scopus","2-s2.0-85041915021"
"Zhang D.; Shao J.; Hu G.; Gao L.","Zhang, Dongyang (57197844592); Shao, Jie (57002035900); Hu, Gang (55460970200); Gao, Lianli (56611089900)","57197844592; 57002035900; 55460970200; 56611089900","Sharp and Real Image Super-Resolution Using Generative Adversarial Network","2017","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10636 LNCS","","","217","226","9","10.1007/978-3-319-70090-8_23","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035233925&doi=10.1007%2f978-3-319-70090-8_23&partnerID=40&md5=8e18d65ebba7da9d81eaa2303163df2e","Recent studies have achieved great progress on accuracy and speed of single image super-resolution (SISR) based on neural networks. Most current SISR methods use mean squared error (MSE) loss as objective function. As a result, they can get high peak signal-to-noise ratios (PSNR) which are however not in full agreement with the visual qualities by experiments, and thus the output from these methods could be prone to blurry and over-smoothed. Especially at large upscaling factors, the output images are perceptually unsatisfactory in general. In this paper, we firstly propose a novel residual network architecture based on generative adversarial network (GAN) for image super-resolution (SR), which is capable of inferring photo-realistic images for 4 upscaling factors. Perceptual loss is applied as the objective function to make output image sharper and more real. In addition, we adopt some tricks to preprocess the input dataset and use improved techniques to train the generator and discriminator separately, which are proved to be effective for the result. We validate our GAN-based approach on CelebA dataset with mean opinion score (MOS) as performance measure. The results demonstrate that the proposed approach performs better than previous methods. © 2017, Springer International Publishing AG.","Mean square error; Network architecture; Optical resolving power; Adversarial networks; Image super resolutions; Improved techniques; Mean opinion scores; Objective functions; Peak signal to noise ratio; Photorealistic images; Super resolution; Signal to noise ratio","Generative adversarial network; Residual network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85035233925"
"Tao Y.; Muller J.-P.","Tao, Y. (56539197700); Muller, J.-P. (7404871794)","56539197700; 7404871794","Repeat multiview panchromatic super-resolution restoration using the UCL MAGiGAN system","2018","Proceedings of SPIE - The International Society for Optical Engineering","10789","","1078903","","","","10.1117/12.2500196","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059024601&doi=10.1117%2f12.2500196&partnerID=40&md5=eb5c8dab554c5cc6473c3417b6e63921","High spatial resolution imaging data is always considered desirable in the field of remote sensing, particularly Earth observation. However, given the physical constraints of the imaging instruments themselves, one needs to be able to trade-off spatial resolution against launch mass as well as telecommunications bandwidth for transmitting data back to the Earth. In this paper, we present a newly developed super-resolution restoration system, called MAGiGAN, based on our original GPT-SRR system combined with deep learning image networks to be able to restore up to 4x higher resolution enhancement using multi-angle repeat images as input. © 2018 SPIE.","Deep learning; Economic and social effects; Image enhancement; Image resolution; Observatories; Optical resolving power; Restoration; Adversarial networks; Earth observations; MAGiGAN; Multi angle; Super-resolution restoration; Remote sensing","Deep learning; Earth observation; Generative adversarial network; MAGiGAN; Multi-angle; Super-resolution restoration","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85059024601"
"Alwon S.","Alwon, Stephen (56669792600)","56669792600","Generative adversarial networks in seismic data processing","2018","SEG Technical Program Expanded Abstracts","","","","1991","1995","4","10.1190/segam2018-2996002.1","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097849275&doi=10.1190%2fsegam2018-2996002.1&partnerID=40&md5=aeb5f3400bc44998b71c31ee310198ea","Generative adversarial networks (GANs) are a class of machine learning techniques that involve two networks trained simultaneously to generate a desired outcome. These schemes have had success in many traditional image processing tasks, such as style transfer and super-resolution, but are relatively unexplored in geophysics. We outline the underlying theory behind GANs and present networks that can perform traditional seismic processing tasks such as noise attenuation and trace interpolation. © 2018 SEG","Data handling; Geophysical prospecting; Image processing; Seismology; Images processing; Machine learning techniques; Noise attenuation; Seismic data processing; Seismic processing; Superresolution; Generative adversarial networks","","Conference paper","Final","","Scopus","2-s2.0-85097849275"
"","","","14th Asian Conference on Computer Vision, ACCV 2018","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11363 LNCS","","","","","4380","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067240912&partnerID=40&md5=c9ff463e7ef36999fb318e7a10f821fc","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Towards Multi-class Object Detection in Unconstrained Remote Sensing Imagery; panorama from Representative Frames of Unconstrained Videos Using DiffeoMeshes; robust and Efficient Ellipse Fitting Using Tangent Chord Distance; knowledge Distillation with Feature Maps for Image Classification; bidirectional Conditional Generative Adversarial Networks; cross-Resolution Person Re-identification with Deep Antithetical Learning; a Temporally-Aware Interpolation Network for Video Frame Inpainting; linear Solution to the Minimal Absolute Pose Rolling Shutter Problem; scale Estimation of Monocular SfM for a Multi-modal Stereo Camera; geometry Meets Semantics for Semi-supervised Monocular Depth Estimation; zero-Shot Facial Expression Recognition with Multi-label Label Propagation; deep Manifold Alignment for Mid-Grain Sketch Based Image Retrieval; Visual Graphs from Motion (VGfM): Scene Understanding with Object Geometry Reasoning; deep Semantic Matching with Foreground Detection and Cycle-Consistency; hidden Two-Stream Convolutional Networks for Action Recognition; a Multi-purpose Convolutional Neural Network for Simultaneous Super-Resolution and High Dynamic Range Image Reconstruction; ITM-CNN: Learning the Inverse Tone Mapping from Low Dynamic Range Video to High Dynamic Range Displays Using Convolutional Neural Networks; Structure Aware SLAM Using Quadrics and Planes; maintaining Natural Image Statistics with the Contextual Loss; U-DADA: Unsupervised Deep Action Domain Adaptation; artistic Object Recognition by Unsupervised Style Adaptation; believe It or Not, We Know What You Are Looking At!; iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects; multi-Attribute Probabilistic Linear Discriminant Analysis for 3D Facial Shapes; combination of Two Fully Convolutional Neural Networks for Robust Binarization; deep Depth from Focus.","","","Conference review","Final","","Scopus","2-s2.0-85067240912"
"Li Y.-Y.; Zhang Y.-D.; Zhou X.-W.; Xu W.","Li, Ying-Yue (57205504414); Zhang, Yun-Dong (57218713389); Zhou, Xue-Wu (57205507572); Xu, Wei (57207174679)","57205504414; 57218713389; 57205507572; 57207174679","EESR: Edge Enhanced Super-Resolution","2018","2018 14th IEEE International Conference on Solid-State and Integrated Circuit Technology, ICSICT 2018 - Proceedings","","","8564942","","","","10.1109/ICSICT.2018.8564942","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060285243&doi=10.1109%2fICSICT.2018.8564942&partnerID=40&md5=26f1bac4ba2545df8c0e24bedd93e5db","In this paper, an edge enhanced super-resolution network (EESR) is proposed for a better generation of high-frequency structures in blind super resolution. In EESR, an edge detection network is used to extract high-frequency pixels. A generative adversarial network (GAN) is used to fine-tune the EESR network. And a more applicative criterion perceptive texture score (PTS) is defined and used to evaluate the quality of generated high-resolution image. Experiments show that the new EESR is able to recover textures with 4 times up-sampling, and gained PTS of 0.6210 on DIV2K test set, which is much better than the state-of-the-art methods. © 2018 IEEE.","Edge detection; Integrated circuits; Adversarial networks; Blind Super-resolution; Detection networks; High frequency HF; High resolution image; State-of-the-art methods; Super resolution; Up sampling; Optical resolving power","","Conference paper","Final","","Scopus","2-s2.0-85060285243"
"Diederich B.; Then P.; Jügler A.; Förster R.; Heintzmann R.","Diederich, Benedict (57201024522); Then, Patrick (24492593000); Jügler, Alexander (57205281888); Förster, Ronny (56342473200); Heintzmann, Rainer (55926434700)","57201024522; 24492593000; 57205281888; 56342473200; 55926434700","CellSTORM—Cost-effective super-resolution on a cellphone using dSTORM","2019","PLoS ONE","14","1","e0209827","","","","10.1371/journal.pone.0209827","30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059798839&doi=10.1371%2fjournal.pone.0209827&partnerID=40&md5=00c167f4dbbf96590b7b5bb8c2d82f20","High optical resolution in microscopy usually goes along with costly hardware components, such as lenses, mechanical setups and cameras. Several studies proved that Single Molecular Localization Microscopy can be made affordable, relying on off-the-shelf optical components and industry grade CMOS cameras. Recent technological advantages have yielded consumer-grade camera devices with surprisingly good performance. The camera sensors of smartphones have benefited of this development. Combined with computing power smartphones provide a fantastic opportunity for “imaging on a budget”. Here we show that a consumer cellphone is capable of optical super-resolution imaging by (direct) Stochastic Optical Reconstruction Microscopy (dSTORM), achieving optical resolution better than 80 nm. In addition to the use of standard reconstruction algorithms, we used a trained image-to-image generative adversarial network (GAN) to reconstruct video sequences under conditions where traditional algorithms provide sub-optimal localization performance directly on the smartphone. We believe that “cellSTORM” paves the way to make super-resolution microscopy not only affordable but available due to the ubiquity of cellphone cameras. © 2019 Diederich et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Algorithms; Image Processing, Computer-Assisted; Machine Learning; Microscopy, Fluorescence; Optical Imaging; Smartphone; algorithm; Article; cost effectiveness analysis; image processing; image quality; image reconstruction; imaging and display; machine learning; stochastic optical reconstruction microscopy; devices; fluorescence imaging; fluorescence microscopy; smartphone","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85059798839"
"He W.; Yokoya N.","He, Wei (55908554500); Yokoya, Naoto (36440631200)","55908554500; 36440631200","Multi-temporal sentinel-1 and -2 data fusion for optical Image Simulation","2018","ISPRS International Journal of Geo-Information","7","10","389","","","","10.3390/ijgi7100389","59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056671515&doi=10.3390%2fijgi7100389&partnerID=40&md5=af751b20fb7f715d36e2ed504c44d7ed","In this paper, we present the optical image simulation from synthetic aperture radar (SAR) data using deep learning based methods. Two models, i.e., optical image simulation directly from the SAR data and from multi-temporal SAR-optical data, are proposed to testify the possibilities. The deep learning based methods that we chose to achieve the models are a convolutional neural network (CNN) with a residual architecture and a conditional generative adversarial network (cGAN). We validate our models using the Sentinel-1 and -2 datasets. The experiments demonstrate that the model with multi-temporal SAR-optical data can successfully simulate the optical image; meanwhile, the state-of-the-art model with simple SAR data as input failed. The optical image simulation results indicate the possibility of SAR-optical information blending for the subsequent applications such as large-scale cloud removal, and optical data temporal super-resolution. We also investigate the sensitivity of the proposed models against the training samples, and reveal possible future directions. © 2018 by the authors.","","Convolutional neural network; Data simulation; Generative adversarial network; Optical; Sentinel; Synthetic aperture radar","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85056671515"
"Han L.; Yin Z.","Han, Liang (57195672263); Yin, Zhaozheng (36351279000)","57195672263; 36351279000","A cascaded refinement GAN for phase contrast microscopy image super resolution","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11071 LNCS","","","347","355","8","10.1007/978-3-030-00934-2_39","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054087273&doi=10.1007%2f978-3-030-00934-2_39&partnerID=40&md5=4edbf792312c34bbbafaf8f1a6694763","Phase contrast microscopy is a widely-used non-invasive technique for monitoring live cells over time. High-throughput biological experiments expect a wide-view (i.e., a low microscope magnification) to monitor the entire cell population and a high magnification on individual cell’s details, which is hard to achieve simultaneously. In this paper, we propose a cascaded refinement Generative Adversarial Network (GAN) for phase contrast microscopy image super-resolution. Our algorithm uses an optic-related data enhancement and super-resolves a phase contrast microscopy image in a coarse-to-fine fashion, with a new loss function consisting of a content loss and an adversarial loss. The proposed algorithm is both qualitatively and quantitatively evaluated on a dataset of 500 phase contrast microscopy images, showing its superior performance for super-resolving phase contrast microscopy images. The proposed algorithm provides a computational solution on achieving a high magnification on individual cell’s details and a wide-view on cell populations at the same time, which will benefit the microscopy community. © Springer Nature Switzerland AG 2018.","Cell culture; Cell proliferation; Cells; Medical computing; Medical imaging; Optical resolving power; Adversarial networks; Biological experiments; Computational solutions; High magnifications; Microscope magnification; Noninvasive technique; Phase-contrast microscopy; Phase-contrast microscopy images; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85054087273"
"","","","14th Asian Conference on Computer Vision, ACCV 2018","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11361 LNCS","","","","","4380","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066792511&partnerID=40&md5=4ab817b9c207bcf3a7258de551a1d13e","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Minutiae-Based Gender Estimation for Full and Partial Fingerprints of Arbitrary Size and Shape; simultaneous Face Detection and Head Pose Estimation: A Fast and Unified Framework; progressive Feature Fusion Network for Realistic Image Dehazing; semi-supervised Learning for Face Sketch Synthesis in the Wild; evolvement Constrained Adversarial Learning for Video Style Transfer; An Unsupervised Deep Learning Framework via Integrated Optimization of Representation Learning and GMM-Based Modeling; recovering Affine Features from Orientation- and Scale-Invariant Ones; totally Looks Like - How Humans Compare, Compared to Machines; generation of Virtual Dual Energy Images from Standard Single-Shot Radiographs Using Multi-scale and Conditional Adversarial Network; pioneer Networks: Progressively Growing Generative Autoencoder; GD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds; Multi-level Sequence GAN for Group Activity Recognition; spatio-Temporal Fusion Networks for Action Recognition; image2Mesh: A Learning Framework for Single Image 3D Reconstruction; face Completion with Semantic Knowledge and Collaborative Adversarial Learning; water-Filling: An Efficient Algorithm for Digitized Document Shadow Removal; matchable Image Retrieval by Learning from Surface Reconstruction; thinking Outside the Box: Generation of Unconstrained 3D Room Layouts; VIENA2: A Driving Anticipation Dataset; multilevel Collaborative Attention Network for Person Search; editable Generative Adversarial Networks: Generating and Editing Faces Simultaneously; enhancing Perceptual Attributes with Bayesian Style Generation; Deep Convolutional Compressed Sensing for LiDAR Depth Completion; Learning for Video Super-Resolution Through HR Optical Flow Estimation; deep Multiple Instance Learning for Zero-Shot Image Tagging.","","","Conference review","Final","","Scopus","2-s2.0-85066792511"
"Yang M.; Li G.; Shu C.; Zhao P.; Han H.","Yang, Maoke (57207759399); Li, Guoqing (57219364182); Shu, Chang (47761369300); Zhao, Pan (57199407178); Han, Hua (35190240200)","57207759399; 57219364182; 47761369300; 57199407178; 35190240200","GAN based sample simulation for SEM-image super resolution","2017","Communications in Computer and Information Science","772","","","383","393","10","10.1007/978-981-10-7302-1_32","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037983720&doi=10.1007%2f978-981-10-7302-1_32&partnerID=40&md5=ea36f5cf73c956361cb414d969580abf","We propose to employ image super resolution to accelerate collection speed of scanning electric microscopes (SEM). This process can be done by collecting images in lower resolution, and then upscale the collected images with image super-resolution algorithms. However, because of physical factors, SEM-images collected in different resolution changed not only in their scale, but also with noise level and physical distortion. Consequently, it is hard to obtain training dataset. In order to solve this problem, we designed a generative adversarial network (GAN) to fit the noise of SEM images, and then generate realistic training samples from high resolution SEM data. Finally, a fully convolutional network have been designed to perform image super-resolution and image denoise at the same time. This pipeline works well on our SEM-image dataset. © Springer Nature Singapore Pte Ltd. 2017.","Computer vision; Optical resolving power; Adversarial networks; Convolutional networks; Different resolutions; Image super resolutions; Lower resolution; Physical factors; Sample simulations; Training dataset; Image denoising","Generative adversarial network; Image super resolution; Scanning electric microscope","Conference paper","Final","","Scopus","2-s2.0-85037983720"
"","","","19th Pacific-Rim Conference on Multimedia, PCM 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11165 LNCS","","","","","2541","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057222002&partnerID=40&md5=b43bc9db0a6c6dd8ce8e69bb3979fe1a","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.","","","Conference review","Final","","Scopus","2-s2.0-85057222002"
"Ge W.; Gong B.; Yu Y.","Ge, Weifeng (56462106200); Gong, Bingchen (56963430600); Yu, Yizhou (8554163500)","56462106200; 56963430600; 8554163500","Image super-resolution via deterministic-stochastic synthesis and local statistical rectification","2018","SIGGRAPH Asia 2018 Technical Papers, SIGGRAPH Asia 2018","","","260","","","","10.1145/3272127.3275060","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066070009&doi=10.1145%2f3272127.3275060&partnerID=40&md5=8f727182a3eadd14cbc54465c915d826","Single image superresolution has been a popular research topic in the last two decades and has recently received a new wave of interest due to deep neural networks. In this paper, we approach this problem from a different perspective. With respect to a downsampled low resolution image, we model a high resolution image as a combination of two components, a deterministic component and a stochastic component. The deterministic component can be recovered from the low-frequency signals in the downsampled image. The stochastic component, on the other hand, contains the signals that have little correlation with the low resolution image. We adopt two complementary methods for generating these two components. While generative adversarial networks are used for the stochastic component, deterministic component reconstruction is formulated as a regression problem solved using deep neural networks. Since the deterministic component exhibits clearer local orientations, we design novel loss functions tailored for such properties for training the deep regression network. These two methods are first applied to the entire input image to produce two distinct high-resolution images. Afterwards, these two images are fused together using another deep neural network that also performs local statistical rectification, which tries to make the local statistics of the fused image match the same local statistics of the groundtruth image. Quantitative results and a user study indicate that the proposed method outperforms existing state-of-the-art algorithms with a clear margin. © 2018 Association for Computing Machinery.","Deep learning; Deep neural networks; Interactive computer graphics; Optical resolving power; Stochastic models; Deterministic component; Gram matrices; Image super-resolution; Local correlations; Stochastic component; Stochastic systems","Deep Learning; Deterministic Component; Image Superresolution; Local Correlation Matrix; Local Gram Matrix; Stochastic Component","Conference paper","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85066070009"
"Vasu S.; Thekke Madam N.; Rajagopalan A.N.","Vasu, Subeesh (55253806000); Thekke Madam, Nimisha (57206472424); Rajagopalan, A.N. (16647440100)","55253806000; 57206472424; 16647440100","Analyzing perception-distortion tradeoff using enhanced perceptual super-resolution network","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS","","","114","131","17","10.1007/978-3-030-11021-5_8","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061754577&doi=10.1007%2f978-3-030-11021-5_8&partnerID=40&md5=ad8da0a6555b63d8067fd14bd81007ae","Convolutional neural network (CNN) based methods have recently achieved great success for image super-resolution (SR). However, most deep CNN based SR models attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while resulting in poor quantified perceptual quality (e.g. human opinion score, no-reference quality measures such as NIQE). Few works have attempted to improve the perceptual quality at the cost of performance reduction in distortion measures. A very recent study has revealed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. Often the restoration algorithms that are superior in terms of perceptual quality, are inferior in terms of distortion measures. Our work attempts to analyze the trade-off between distortion and perceptual quality for the problem of single image SR. To this end, we use the well-known SR architecture- enhanced deep super-resolution (EDSR) network and show that it can be adapted to achieve better perceptual quality for a specific range of the distortion measure. While the original network of EDSR was trained to minimize the error defined based on per-pixel accuracy alone, we train our network using a generative adversarial network framework with EDSR as the generator module. Our proposed network, called enhanced perceptual super-resolution network (EPSR), is trained with a combination of mean squared error loss, perceptual loss, and adversarial loss. Our experiments reveal that EPSR achieves the state-of-the-art trade-off between distortion and perceptual quality while the existing methods perform well in either of these measures alone. © Springer Nature Switzerland AG 2019.","Computer vision; Deep learning; Mean square error; Neural networks; Optical resolving power; Adversarial networks; Convolutional neural network; Distortion measures; Image super resolutions; Mean squared error; Perceptual quality; Restoration algorithm; Super resolution; Economic and social effects","Deep learning; GAN; Perceptual quality; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85061754577"
"","","","Computational Optical Sensing and Imaging, COSI 2018","2018","Optics InfoBase Conference Papers","Part F99-COSI 2018","","","","","105","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051247040&partnerID=40&md5=592bc145e4770885fa672c5161d89849","This proceedings contains 53 papers. COSI encompasses the latest advances in computational imaging research. Representative topics include compressive sensing, tomographic imaging, light-field sensing, digital holography, SAR, phase retrieval, computational spectroscopy, blind deconvolution and phase diversity, pointspread function engineering and digital/optical super resolution. The conference topics include: Non-Line-of-Sight Imaging using Superheterodyne Interferometry; Resolving Non Line-of-Sight (NLoS) motion using Speckle; Indirect Imaging Using Correlography; Micro Resolution Time-of-Flight Imaging; Passive Non-line-of-sight Source Classification from Coherence Measurements; Diffuse Time-of-flight Imaging with a Single-Photon Camera; Imaging with Phasor Fields for Non-Line-of Sight Applications; Indirect Imaging Using Virtualized Pattern Projection; Aparna Viswanath, Muralidhar M. Balaji, Prasanna Rangarajan, Duncan MacFarlane, and Marc P Christensen; Multi-layered Born scattering model for 3D phase imaging with multiple scattering objects; Depth-resolved Lensless Imaging; 3D Fluorescence Microscopy with DiffuserCam; Double-Cubic Point Spread Function for 3D Extended-Depth Localization Microscopy; Depth Sensitivity Improvement of Region-of-Interest Diffuse Optical Tomography from Superficial Signal Regression; Manob Jyoti Saikia, Rakesh Manjappa, Kunal Mankodiya, and Rajan Kanhirodan; On Block-Reference Coherent Diffraction Imaging; Deep Learning Enhances Mobile Microscopy; A Novel Optical Structure to Implement One-dimensional Fourier Transform with Spherical Lenses; Seeing through Multimode Fibers with Deep Learning; Plenoptic imaging from intensity correlations; A new method for designing highly efficient metasurface devices: Local Phase Method; Novel Optimizations for Phase Retrieval; Phase Retrieval Based on Wave Modulation; Enhanced Phase Retrieval using Quantum Illumination; Temporal Super-resolution Full Waveform LiDAR; Super-Resolution Imaging Based on Spectral Dimensional Information; Remote Sensing of Photoplethysmogram using Multi Spot Illumination; Enlarged Field of View Scattering Imaging Using Speckle Autocorrelation; Mitigating metalens aberrations via computational imaging; Shane Colburn and Arka Majumdar; Binarization threshold optimization of ghost imaging; Ghost Imaging With Gram-Schmidt Orthogonalization; Comparison between ghost imaging and traditional active optical imaging; Demonstration of computational temporal ghost imaging: detecting fast signals beyond bandwidth of detectors; Imaging the Joint Probability Distribution of Spatially Entangled Photon Pairs with a Camera; Optimization of light field fluctuation patterns in ghost imaging by mutual coherence minimization based on dictionary learning; Characterizing the optical memory effect using quantum illumination; Compressive Ultrafast Single Pixel Camera; Encrypted Single Pixel Imaging with Basis Illumination Patterns; Correlation Matrix Estimation from Compressed Measurements in a Pattern Recognition System; Exploiting Inter Voxel Correlation in Compressed Computational Imaging; Naren Viswanathan, Suresh Venkatesh, and David Schurig; Double-threshold Denoising for Single-pixel Camera; Multi-object Recognition in Turbid Water Using Compressive Sensing; Covariance Matrix Estimation from Multiple Subsets in Compressive Spectral Imaging; Compressive Spectral Polarization Imaging Using a Single Pixel Detector; Subsampling Schemes for the 2D Nuclear Magnetic Resonance Spectroscopy; Compressive coded LED and coded aperture spectral video system; Spatial Super-resolution reconstruction via SSCSI Compressive Spectral Imagers; Snapshot Compressive Spectral+Depth Imaging with Color-Coded Apertures; Spectral zooming in SSCSI Compressive Spectral Imagers; Compressive Photon-Sieve Spectral Imaging; Field-varying aberration recovery in EUV microscopy using mask roughness; Computational Cannula Microscopy: Utilizing a Simple Glass Needle for Imaging; Integral Refractive Index Imaging of Flowing Cell Nuclei; Compressive hyperspectral imaging for snapshot multi-channel fluorescence microscopy; Cell imaging by phase extraction neural network (PhENN); Quantitative Phase Maps of Live Cells Classified By Transfer Learning and Generative Adversarial Network (GAN); A Neuro-Inspired Model for Image Motion Processing; Optical Sensing and Control Based on Machine Learning; Deep Learned Phase Mask for Single Image Depth Estimation and 3D scanning; Neural Network classification for intensity imaging through multimode optical fibres; Phase Unwrapping Using Residual Neural Networks; Bending-Independent Imaging through Glass-Air Disordered Fiber Based on Deep Learning; Speckle suppression using the convolutional neural network with an exponential linear unit. The key terms of this proceedings include Compressive Sensing, Computational microscopy, Depth-resolved and turbid imaging, Imaging through aberrations, Structured illumination  and super resolution, Indirect and non-line-of-sight imaging, Machine Learning in Computational Sensing and Imaging, Phase Retrieval, Postdeadline Papers - COSI, Quantum Computational Imaging.","","","Conference review","Final","","Scopus","2-s2.0-85051247040"
"Hu H.; Cui M.; Hu W.","Hu, Hui (57706262900); Cui, Miao (57205127313); Hu, Wenwen (57194410880)","57706262900; 57205127313; 57194410880","Generative adversarial networks- and ResNets-based framework for image translation with super-resolution","2018","Journal of Electronic Imaging","27","6","063018","","","","10.1117/1.JEI.27.6.063018","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058691896&doi=10.1117%2f1.JEI.27.6.063018&partnerID=40&md5=03c1981c5d14e6bdf5020900501f2ec8","                             Recent research on image translation has great progress with the development of generative adversarial networks (GANs) techniques. Generating high-resolution images with unsupervised architecture is one of the most challenging tasks for image translation. To this end, we propose an enhanced super-resolution generative adversarial network for image translation. First, for unlabeled datasets, we employ reconstructed consistency loss and mutual dual GANs, which contains two generators:G                             A → B                             , G                             B → A                              and two discriminators: D                             B                             , D                             A                              to develop an unsupervised learning framework. As reconstructed consistency loss is added between generators of G                             A → B                              and G                             B → A                             , our designed overall architecture can learn mapping function of different domains even without unpaired samples. In addition, the generator network includes encoder network, decoder network, and residual network with skip connections to generate high-resolution images with realistic details. Meanwhile, a stable normalization is proposed to stabilize the training of our discriminator networks. Finally, experimental results are carried out on six different datasets, demonstrating that our algorithms outperform the state-of-the-art methods in terms of the image quality and image resolution.                          © 2018 SPIE and IS&T.","Image enhancement; Image resolution; Optical resolving power; Adversarial networks; Different domains; High resolution image; Image translation; Mapping functions; Recent researches; State-of-the-art methods; Super resolution; Network architecture","generative adversarial networks; image translation; residual networks; super-resolution","Article","Final","","Scopus","2-s2.0-85058691896"
"Wang H.; Rivenson Y.; Jin Y.; Wei Z.; Gao R.; Günaydın H.; Bentolila L.A.; Kural C.; Ozcan A.","Wang, Hongda (57192201317); Rivenson, Yair (24081473400); Jin, Yiyin (57205154017); Wei, Zhensong (57202577273); Gao, Ronald (57205152760); Günaydın, Harun (57197829021); Bentolila, Laurent A. (6603255631); Kural, Comert (9236116800); Ozcan, Aydogan (7005667692)","57192201317; 24081473400; 57205154017; 57202577273; 57205152760; 57197829021; 6603255631; 9236116800; 7005667692","Deep learning enables cross-modality super-resolution in fluorescence microscopy","2019","Nature Methods","16","1","","103","110","7","10.1038/s41592-018-0239-0","369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058835841&doi=10.1038%2fs41592-018-0239-0&partnerID=40&md5=12f21e4ccf94a0dd4c139a06dff85902","We present deep-learning-enabled super-resolution across different fluorescence microscopy modalities. This data-driven approach does not require numerical modeling of the imaging process or the estimation of a point-spread-function, and is based on training a generative adversarial network (GAN) to transform diffraction-limited input images into super-resolved ones. Using this framework, we improve the resolution of wide-field images acquired with low-numerical-aperture objectives, matching the resolution that is acquired using high-numerical-aperture objectives. We also demonstrate cross-modality super-resolution, transforming confocal microscopy images to match the resolution acquired with a stimulated emission depletion (STED) microscope. We further demonstrate that total internal reflection fluorescence (TIRF) microscopy images of subcellular structures within cells and tissues can be transformed to match the results obtained with a TIRF-based structured illumination microscope. The deep network rapidly outputs these super-resolved images, without any iterations or parameter search, and could serve to democratize super-resolution imaging. © 2018, The Author(s), under exclusive licence to Springer Nature America, Inc.","Animals; Cattle; Deep Learning; Endothelial Cells; HeLa Cells; Humans; Microscopy, Confocal; Microscopy, Fluorescence; Pulmonary Artery; Subcellular Fractions; animal; bovine; cell fractionation; confocal microscopy; cytology; endothelium cell; fluorescence microscopy; HeLa cell line; human; procedures; pulmonary artery; ultrastructure","","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85058835841"
"Ledig C.; Theis L.; Huszár F.; Caballero J.; Cunningham A.; Acosta A.; Aitken A.; Tejani A.; Totz J.; Wang Z.; Shi W.","Ledig, Christian (55332432900); Theis, Lucas (54883612000); Huszár, Ferenc (55233726100); Caballero, Jose (56410493800); Cunningham, Andrew (57201315696); Acosta, Alejandro (36543646900); Aitken, Andrew (57191077810); Tejani, Alykhan (56333410500); Totz, Johannes (55283307200); Wang, Zehan (55627808600); Shi, Wenzhe (36631090300)","55332432900; 54883612000; 55233726100; 56410493800; 57201315696; 36543646900; 57191077810; 56333410500; 55283307200; 55627808600; 36631090300","Photo-realistic single image super-resolution using a generative adversarial network","2017","Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017","2017-January","","","105","114","9","10.1109/CVPR.2017.19","5130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035231525&doi=10.1109%2fCVPR.2017.19&partnerID=40&md5=cd63e231f367cc290b6de95208dac87f","Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image superresolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method. © 2017 IEEE.","Computer vision; Discrete cosine transforms; Neural networks; Optical resolving power; Pattern recognition; Signal to noise ratio; Convolutional neural network; High resolution image; Image super-resolution; Peak signal to noise ratio; Perceptual similarity; Photorealistic images; State-of-the-art methods; Superresolution methods; Image texture","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85035231525"
"","","","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","","","9519","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062861149&partnerID=40&md5=e44fe934b37bf888619f0c158311b8d7","The proceedings contain 979 papers. The topics discussed include: embodied question answering; learning by asking questions; finding tiny faces in the wild with generative adversarial network; paired CycleGAN: asymmetric style transfer for applying and removing makeup; learning pose specific representations by predicting different views; weakly and semi supervised human body part parsing via pose-guided knowledge transfer; person transfer GAN to bridge domain gap for person re-identification; cross-modal deep variational hand pose estimation; disentangled person image generation; super-fan: integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs; multistage adversarial losses for pose-based human image synthesis; and a certifiably globally optimal solution to the non-minimal relative pose problem.","","","Conference review","Final","","Scopus","2-s2.0-85062861149"
"Shao W.-Z.; Xu J.-J.; Chen L.; Ge Q.; Wang L.-Q.; Bao B.-K.; Li H.-B.","Shao, Wen-Ze (55959121000); Xu, Jing-Jing (57208769839); Chen, Long (57208005338); Ge, Qi (55226556800); Wang, Li-Qian (36895613500); Bao, Bing-Kun (24922856800); Li, Hai-Bo (56375326400)","55959121000; 57208769839; 57208005338; 55226556800; 36895613500; 24922856800; 56375326400","Tiny face hallucination via boundary equilibrium generative adversarial networks","2019","Proceedings of SPIE - The International Society for Optical Engineering","11069","","110693M","","","","10.1117/12.2524361","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065760563&doi=10.1117%2f12.2524361&partnerID=40&md5=1bd51b4646dd89e89882b1e4c2a8c9a1","It is known that actual performance of most previous face hallucination approaches will drop dramatically as a very low-resolution tiny face is provided. Inspired by the latest progress in deep unsupervised learning, this paper works on tiny faces of size 16×16 pixels and magnifies them into their 8× upsampling ones by exploiting the boundary equilibrium generative adverarial networks (BEGAN). Besides imposing a pixel-wise L2 regularization term to the generative model, it is found that our targeted auto-encoding generator with residual blocks and skip connections is a key component for BEGAN achieving state-of-the-art hallucination performance. The cropped CelebA face dataset is preliminarily used in our experiments. The results demonstrate that the proposed approach is not only of fast and stable convergence, but also robust to pose, expression, illuminance and occluded variations. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Deep learning; Encoding (symbols); Signal encoding; Adversarial networks; Boundary equilibrium; Face hallucination; GANs; Regularization terms; Stable convergence; Super resolution; Tiny faces; Pixels","Adversarial training; Auto-encoding; GANs; Skip connections; Super-resolution; Tiny faces","Conference paper","Final","","Scopus","2-s2.0-85065760563"
"","","","2nd Chinese Conference on Computer Vision, CCCV 2017","2017","Communications in Computer and Information Science","771","","","","","1377","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037854791&partnerID=40&md5=18e5108e37cb9ea3346e936251542663","The proceedings contain 113 papers. The special focus in this conference is on Computer Vision. The topics include: Visual saliency fusion based multi-feature for semantic image retrieval; unsupervised multi-view subspace learning via maximizing dependence; structured multi-view supervised feature selection algorithm research; an automatic shoeprint retrieval method using neural codes for commercial shoeprint scanners; uncovering the effect of visual saliency on image retrieval; shape-color differential moment invariants under affine transforms; a novel layer based image fusion approach via transfer learning and coupled dictionary; local saliency extraction for fusion of visible and infrared images; distributed compressive sensing for light field reconstruction using structured random matrix; improved face verification with simple weighted feature combination; stereoscopic image quality assessment based on binocular adding and subtracting; quality assessment of palm vein image using natural scene statistics; an error-activation-guided blind metric for stitched panoramic image quality assessment; image aesthetic quality evaluation using convolution neural network embedded fine-tune; high capacity reversible data hiding with contrast enhancement; rank learning for dehazed image quality assessment; a low-rank total-variation regularized tensor completion algorithm; nighttime haze removal with fusion atmospheric light and improved entropy; light field super-resolution using cross-resolution input based on patchmatch and learning method; a new image sparse reconstruction method for mixed Gaussian-poisson noise with multiple constraints; pore-scale facial features matching under 3D morphable model constraint; face video super-resolution with identity guided generative adversarial networks; exemplar-based pixel by pixel inpainting based on patch shift; GAN based sample simulation for SEM-image super resolution; PSO-based single image defogging.","","","Conference review","Final","","Scopus","2-s2.0-85037854791"
"Choi J.; Kang D.","Choi, Junmyung (57195283257); Kang, Dongjoong (55455030300)","57195283257; 55455030300","Deep super-resolution method via generative adversarial networks for license place image enhancement","2017","Journal of Institute of Control, Robotics and Systems","23","8","","635","643","8","10.5302/J.ICROS.2017.17.0087","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026740440&doi=10.5302%2fJ.ICROS.2017.17.0087&partnerID=40&md5=1bc18e430f34516a411d0a3ebf7a2e67","Convolutional neural networks and generative adversarial neural networks have recently shown outstanding performance in single-image super-resolution. In this paper, we propose a deep super-resolution method based on generative adversarial networks to reconstruct a high-resolution license plate image from a low-resolution license plate image to improve the accuracy of lowresolution license plate recognition. To achieve this, we create a super-resolution model using deep residual blocks. In addition, our model uses the benefits of adversarial loss, pixel-wise loss, and perceptual loss. Adversarial loss plays a role in enabling the discriminator network to distinguish between the high-resolution image and the fake high-resolution image generated by the generator network well, while it facilitates the generator network to generate a realistic high-resolution image with the goal of fooling the discriminator network. Pixel-wise loss and perceptual loss help the generator network to reconstruct a better high-resolution image. We compare the proposed method and other state-of-the-art methods with the performance of the license plate image recognizer. The experimental results demonstrate that our proposed method performs well by showing that the accuracy of the lowresolution image is improved from 38.45 to 73.65% by the proposed method. © ICROS 2017.","Deep learning; Image recognition; Image reconstruction; License plates (automobile); Neural networks; Optical character recognition; Optical resolving power; Pixels; Adversarial networks; Convolutional neural network; License plate recognition; Low resolution images; State-of-the-art methods; Super resolution; Super-resolution models; Superresolution methods; Image processing","Deep learning; Generative adversarial network; Image recognition; License plate; Super resolution","Article","Final","","Scopus","2-s2.0-85026740440"
"Bruna J.; Sprechmann P.; LeCun Y.","Bruna, Joan (50461069300); Sprechmann, Pablo (23010370200); LeCun, Yann (55666793600)","50461069300; 23010370200; 55666793600","Super-resolution with deep convolutional sufficient statistics","2016","4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings","","","","","","","","99","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083954075&partnerID=40&md5=38877744e10ec3493c0104fb2cf439ec","Inverse problems in image and audio, and super-resolution in particular, can be seen as high-dimensional structured prediction problems, where the goal is to characterize the conditional distribution of a high-resolution output given its low-resolution corrupted observation. When the scaling ratio is small, point estimates achieve impressive performance, but soon they suffer from the regression-to-the-mean problem, result of their inability to capture the multi-modality of this conditional distribution. Modeling high-dimensional image and audio distributions is a hard task, requiring both the ability to model complex geometrical structures and textured regions. In this paper, we propose to use as conditional model a Gibbs distribution, where its sufficient statistics are given by deep Convolutional Neural Networks (CNN). The features computed by the network are stable to local deformation, and have reduced variance when the input is a stationary texture. These properties imply that the resulting sufficient statistics minimize the uncertainty of the target signals given the degraded observations, while being highly informative. The filters of the CNN are initialized by multiscale complex wavelets, and then we propose an algorithm to fine-tune them by estimating the gradient of the conditional log-likelihood, which bears some similarities with Generative Adversarial Networks. We evaluate experimentally the proposed framework in the image super-resolution task, but the approach is general and could be used in other challenging ill-posed problems such as audio bandwidth extension. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.","Complex networks; Convolution; Deep neural networks; Neural networks; Optical resolving power; Textures; Conditional distribution; Convolutional neural network; High-dimensional images; High-resolution output; Image super resolutions; Regression to the means; Structured prediction; Sufficient statistics; Inverse problems","","Conference paper","Final","","Scopus","2-s2.0-85083954075"
"Bosch M.; Gifford C.M.; Rodriguez P.A.","Bosch, Marc (24398475400); Gifford, Christopher M. (24467858000); Rodriguez, Pedro A. (7202921325)","24398475400; 24467858000; 7202921325","Super-Resolution for Overhead Imagery Using DenseNets and Adversarial Learning","2018","Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018","2018-January","","8354263","1414","1422","8","10.1109/WACV.2018.00159","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051034458&doi=10.1109%2fWACV.2018.00159&partnerID=40&md5=eed44b52a0fb48f4f208858a38b94c41","Recent advances in Generative Adversarial Learning allow for new modalities of image super-resolution by learning low to high resolution mappings. In this paper we present our work using Generative Adversarial Networks (GANs) with applications to overhead and satellite imagery. We have experimented with several state-of-the-art architectures. We propose a GAN-based architecture using densely connected convolutional neural networks (DenseNets) to be able to super-resolve overhead imagery with a factor of up to 8x. We have also investigated resolution limits of these networks. We report results on several publicly available datasets, including SpaceNet data and IARPA Multi-View Stereo Challenge, and compare performance with other state-of-the-art architectures. © 2018 IEEE.","Computer vision; Network architecture; Neural networks; Optical resolving power; Satellite imagery; Adversarial learning; Adversarial networks; Convolutional neural network; Image super resolutions; Multi-view stereo; Resolution limits; State of the art; Super resolution; Stereo image processing","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85051034458"
"Matsumura N.; Tokura H.; Kuroda Y.; Ito Y.; Nakano K.","Matsumura, Naoki (57205753391); Tokura, Hiroki (57193613981); Kuroda, Yuki (57203037914); Ito, Yasuaki (7410025690); Nakano, Koji (37013723100)","57205753391; 57193613981; 57203037914; 7410025690; 37013723100","Tile art image generation using conditional generative adversarial networks","2018","Proceedings - 2018 6th International Symposium on Computing and Networking Workshops, CANDARW 2018","","","8590901","209","215","6","10.1109/CANDARW.2018.00047","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061427192&doi=10.1109%2fCANDARW.2018.00047&partnerID=40&md5=7bb60fa1d475dd1e1fcd62588db45dbf","Image-to-image translation is a task of mapping an image in one domain to a corresponding image in another domain. The task includes various types of problems such as super-resolution, colorization, and artistic style transfer. In recent years, with the advent of deep learning, the technology has been rapidly advanced. The main purpose of this paper is to propose a tile art image generation method using machine learning approach based on conditional generative adversarial networks. To make the training data set of tile art images, we adopted a square-pointillism image generation method using the greedy approach. After training, the proposed network can generate tile art images that have the structure of tiles and reproduce the original images well. As regards generating time, the greedy approach takes 1322 seconds to generate tile art image of size 4096x3072, while the proposed machine learning approach takes 0.593 seconds. © 2018 IEEE.","Deep learning; Learning systems; Machine learning; Adversarial networks; Artistic style transfer; Conditional GAN; Greedy approaches; Image generations; Image translation; Machine learning approaches; Training data sets; Arts computing","Conditional GAN; Image-to-image translation; Machine learning; Tile Art","Conference paper","Final","","Scopus","2-s2.0-85061427192"
"Gu J.; Li Z.; Wang Y.; Yang H.; Qiao Z.; Yu J.","Gu, Jiaqi (57213420794); Li, Zeju (57192593081); Wang, Yuanyuan (35241434100); Yang, Haowei (56044153900); Qiao, Zhongwei (16444536400); Yu, Jinhua (35325887300)","57213420794; 57192593081; 35241434100; 56044153900; 16444536400; 35325887300","Deep Generative Adversarial Networks for Thin-Section Infant MR Image Reconstruction","2019","IEEE Access","7","","8721701","68290","68304","14","10.1109/ACCESS.2019.2918926","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067254024&doi=10.1109%2fACCESS.2019.2918926&partnerID=40&md5=0e5bbe85a0a8a9a4923c88c96d91200c","Due to their high spatial resolution, thin-section magnetic resonance (MR) images serve as ideal medical images for brain structure investigation and brain surgery navigation. However, compared with the clinically widely used thick-section MR images, thin-section MR images are less available due to the imaging cost. Thin-section MR images of infants are even scarcer but are quite valuable for the study of human brain development. Therefore, we propose a method for the reconstruction of thin-section MR images from thick-section images. A two-stage reconstruction framework based on generative adversarial networks (GANs) and a convolutional neural network (CNN) is proposed to reconstruct thin-section MR images from thick-section images in the axial and sagittal planes. A 3D-Y-Net-GAN is first proposed to fuse MR images from the axial and sagittal planes and to achieve the first-stage thin-section reconstruction. A 3D-DenseU-Net followed by a stack of enhanced residual blocks is then proposed to provide further detail recalibrations and structural corrections in the sagittal plane. In this method, a comprehensive loss function is also proposed to help the networks capture more structural details. The reconstruction performance of the proposed method is compared with bicubic interpolation, sparse representation, and 3D-SRU-Net. Cross-validation based on 35 cases and independent testing based on two datasets with totally 114 cases reveal that, compared with the other three methods, the proposed method provides an average 23.5% improvement in peak signal-to-noise ratio (PSNR), 90.5% improvement in structural similarity (SSIM), and 21.5% improvement in normalized mutual information (NMI). The quantitative evaluation and visual inspection demonstrate that our proposed method outperforms those methods by reconstructing more realistic results with better structural details. © 2013 IEEE.","Deep learning; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Neural networks; Signal to noise ratio; Convolutional neural network; infant magnetic resonance (MR) images; Normalized mutual information; Peak signal to noise ratio; Reconstruction frameworks; Super resolution reconstruction; thick-section; Thin section; Image reconstruction","Deep learning; infant magnetic resonance (MR) images; super-resolution reconstruction; thick-section; thin-section","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85067254024"
"Gopan K.; Kumar G.S.","Gopan, Karthika (57188862044); Kumar, G.S. (56405054400)","57188862044; 56405054400","Video Super Resolution with Generative Adversarial Network","2018","Proceedings of the 2nd International Conference on Trends in Electronics and Informatics, ICOEI 2018","","","8553719","1489","1493","4","10.1109/ICOEI.2018.8553719","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059978866&doi=10.1109%2fICOEI.2018.8553719&partnerID=40&md5=e4dafa2c40cfc3607b265b7d38db27da","This paper exhibits a novel implementation of video super resolution through Generative adversarial network. Generative adversarial networks (GANs) are a class of artificial intelligence algorithm implemented by a system of two neural networks provide a unique way to learn deep representations. The results of GAN shows slightly lower PSNR compared to traditional methods, since the frames produced by GAN is more appealing when viewed by human. GAN can learn from large datasets and automatically add high-frequency details and features to frames while traditional methods can't. The generator discriminator architecture in GAN pushes it to generate more realistic and appealing frames. © 2018 IEEE.","Electronics engineering; Electronics industry; Adversarial networks; Artificial intelligence algorithms; High frequency HF; Large datasets; Video super-resolution; Optical resolving power","Generative adversarial network; Video super resolution","Conference paper","Final","","Scopus","2-s2.0-85059978866"
"","","","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2017","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2017-January","","","","","7495","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063189674&partnerID=40&md5=a54b557625f62c94a358895ed6ae3562","The proceedings contain 781 papers. The topics discussed include: exclusivity-consistency regularized multi-view subspace clustering; borrowing treasures from the wealthy: deep transfer learning through selective joint fine-tuning; the more you know: using knowledge graphs for image classification; dynamic edge-conditioned filters in convolutional neural networks on graphs; convolutional neural network architecture for geometric matching; deep affordance-grounded sensorimotor object recognition; on compressing deep models by low rank and sparse decomposition; unsupervised pixel-level domain adaptation with generative adversarial networks; photo-realistic single image super-resolution using a generative adversarial network; a practical method for fully automatic intrinsic camera calibration using directionally encoded light; elastic shape-from-template with spatially sparse deforming forces; and distinguishing the indistinguishable: exploring structural ambiguities via geodesic context.","","","Conference review","Final","","Scopus","2-s2.0-85063189674"
"Lian S.; Zhou H.; Sun Y.","Lian, Shuailong (57219594056); Zhou, Hejian (57209746521); Sun, Yi (55535529200)","57219594056; 57209746521; 55535529200","FG-SRGAN: A Feature-Guided Super-Resolution Generative Adversarial Network for Unpaired Image Super-Resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11554 LNCS","","","151","161","10","10.1007/978-3-030-22796-8_17","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068614644&doi=10.1007%2f978-3-030-22796-8_17&partnerID=40&md5=e26504b6932f63414664a7deecd0fe76","Recently, the performance of single image super-resolution has been significantly improved by convolution neural networks (CNN). However, most of these networks are trained with paired images and take the bicubic-downsampled images as inputs. It’s impractical if we want to super-resolve low-resolution images in the real world, since there is no ground truth high-resolution images corresponding to the low-resolution images. To tackle this challenge, a Feature-Guided Super-Resolution Generative Adversarial Network (FG-SRGAN) for unpaired image super-resolution is proposed in this paper. A guidance module is introduced in FG-SRGAN, which is utilized to reduce the space of possible mapping functions and help to learn the correct mapping function from low-resolution domain to high-resolution domain. Furthermore, we treat the outputs of guidance module as fake examples, which can be leveraged using another adversarial loss. This is beneficial for the main task as it forces FG-SRGAN to learn valid representations for super-resolution. When applied to super-resolve low-resolution face images in the real world, FG-SRGAN is able to achieve satisfactory performance both qualitatively and quantitatively. © 2019, Springer Nature Switzerland AG.","Mapping; Optical resolving power; Unsupervised learning; Adversarial networks; Convolution neural network; High resolution image; Image super resolutions; Low resolution images; Low-resolution face images; Mapping functions; Super resolution; Image enhancement","GAN; Image super-resolution; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85068614644"
"Li L.; Hu M.","Li, Liangfu (56094006900); Hu, Min (57208783130)","56094006900; 57208783130","Method for small-bridge-crack segmentation rased on generative adversarial network","2019","Laser and Optoelectronics Progress","56","9","","","","","10.3788/LOP56.101004","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065756530&doi=10.3788%2fLOP56.101004&partnerID=40&md5=8406b10ad9def290a7ed8dd4b6bb5a33","For cracks in small bridges, a segmentation method is proposed based on a generative adversarial network. This method introduces a segmental branch into the discriminator structure and combines the generative confrontation network with the semantic segmentation network. In addition, the method is capable of super-resolution image reconstruction and segmentation. To solve the problem of small-bridge-crack segmentation, this method transforms low-resolution small-bridge-crack images into super-resolution coarse-bridge-crack images, which are then segmented. The experimental results show that the proposed method facilitates the identification of small-bridge-crack and its segmentation is accurate. Compared with the traditional segmentation method, the recall rate and mean intersection over union of this method are improved by 6% and 10%, respectively. © 2019 Universitat zu Koln. All rights reserved.","","Bridge pavement crack; Deep learning; Generative confrontation network; Image processing; Semantic segmentation; Super-resolution image","Article","Final","","Scopus","2-s2.0-85065756530"
"Corley I.A.; Huang Y.","Corley, Isaac A. (57203182771); Huang, Yufei (35558675700)","57203182771; 35558675700","Deep EEG super-resolution: Upsampling EEG spatial resolution with Generative Adversarial Networks","2018","2018 IEEE EMBS International Conference on Biomedical and Health Informatics, BHI 2018","2018-January","","","100","103","3","10.1109/BHI.2018.8333379","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050887027&doi=10.1109%2fBHI.2018.8333379&partnerID=40&md5=d6e94d70df3b6b94df99b59493d0ecc9","Electroencephalography (EEG) activity contains a wealth of information about what is happening within the human brain. Recording more of this data has the potential to unlock endless future applications. However, the cost of EEG hardware is increasingly expensive based upon the number of EEG channels being recorded simultaneously. We combat this problem in this paper by proposing a novel deep EEG superresolution (SR) approach based on Generative Adversarial Networks (GANs). This approach can produce high spatial resolution EEG data from low resolution samples, by generating channel-wise upsampled data to effectively interpolate numerous missing channels, thus reducing the need for expensive EEG equipment. We tested the performance using an EEG dataset from a mental imagery task. Our proposed GAN model provided ∼104 fold and ∼102 fold reduction in mean-squared error (MSE) and mean-absolute error (MAE), respectively, over the baseline bicubic interpolation method. We further validate our method by training a classifier on the original classification task, which displayed minimal loss in accuracy while using the super-resolved data. The proposed SR EEG by GAN is a promising approach to improve the spatial resolution of low density EEG headset. © 2018 IEEE.","Brain; Electrophysiology; Image resolution; Mean square error; Optical resolving power; Adversarial networks; Bicubic interpolation; Classification tasks; Future applications; High spatial resolution; Mean absolute error; Spatial resolution; Wealth of information; Electroencephalography","","Conference paper","Final","","Scopus","2-s2.0-85050887027"
"Lin G.; Wu Q.; Chen L.; Qiu L.; Wang X.; Liu T.; Chen X.","Lin, Guimin (23985230700); Wu, Qingxiang (7404602640); Chen, Liang (57059516200); Qiu, Lida (56489442200); Wang, Xuan (56071892500); Liu, Tianjian (56131860400); Chen, Xiyao (8509505000)","23985230700; 7404602640; 57059516200; 56489442200; 56071892500; 56131860400; 8509505000","Deep unsupervised learning for image super-resolution with generative adversarial network","2018","Signal Processing: Image Communication","68","","","88","100","12","10.1016/j.image.2018.07.003","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050349799&doi=10.1016%2fj.image.2018.07.003&partnerID=40&md5=3af8cf42e87ea1b1e13ce45fb3a51025","The aim of Image super-resolution (SR) is to recover high-resolution images from low-resolution ones. By virtue of the great success in numerous computer vision tasks achieved by the convolutional neural networks (CNNs), it is a nice direction to tackle the SR problem using CNNs. Despite progress in accuracy of SR using deeper CNNs, those models are almost trained base upon supervised way. In this paper, we propose a deep unsupervised learning approach for SR with a Generative Adversarial Network (GAN) framework, which is composed of a deep convolutional generator network with dense connections and a discriminator. A sub-pixel convolutional layer is operated on the top of the generator to upscale the inputs, and the standard convolutions are all implemented in the LR space, which leads to a fast restoration. The generator is trained to directly recover the high-resolution image from the low-resolution image. Strided convolution and ReLU activations are employed in the discriminator to distinguish the HR images from the produced HR images. The generator model is optimized with a combination of a data error, a regular term and an adversarial loss, which ensures local–global contents consistency and pixel faithfulness. Note that no labeled training data is employed during the training. Comparisons with several state-of-the-art supervised learn-based methods, experimental results demonstrate that the proposed model achieves a comparable result in terms of both quantitative and qualitative measurements, and it also implies the feasibility and effectiveness of the proposed unsupervised learning-based single-image super-resolution algorithm. © 2018 Elsevier B.V.","Convolution; Neural networks; Optical resolving power; Pixels; Unsupervised learning; Adversarial networks; Convolutional neural network; High resolution image; Image super resolutions; Qualitative measurements; Regularizer; Sub pixels; Super resolution; Deep learning","Deep unsupervised learning; Generative adversarial network; Regularizer; Sub-pixel convolution; Super-resolution","Article","Final","","Scopus","2-s2.0-85050349799"
"Yang X.; Lu T.; Wang J.; Zhang Y.; Wu Y.; Wang Z.; Xiong Z.","Yang, Xi (57204804832); Lu, Tao (56406646300); Wang, Jiaming (57206676342); Zhang, Yanduo (55993581700); Wu, Yuntao (55993578900); Wang, Zhongyuan (57203515592); Xiong, Zixiang (7202954235)","57204804832; 56406646300; 57206676342; 55993581700; 55993578900; 57203515592; 7202954235","Enhanced discriminative generative adversarial network for face super-resolution","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11165 LNCS","","","441","452","11","10.1007/978-3-030-00767-6_41","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057217631&doi=10.1007%2f978-3-030-00767-6_41&partnerID=40&md5=8abed336c6c559f9f67f76079bbe4fb6","Recently, some generative adversarial network (GAN)-based super-resolution (SR) methods have progressed to the point where they can produce photo-realistic natural images by using a generator (G) and discriminator (D) adversarial scheme. However, vanilla GAN-based SR methods cannot achieve good reconstruction and perceptual fidelity on real-world facial images at the same time. Because of D loss, them are hard to converge stably, which may cause the model collapse. In this paper, we present an Enhanced Discriminative Generative Adversarial Network (EDGAN) for SR facial recognition to achieve better reconstruction and perceptual fidelities. First, we discover that a versatile D boosts the adversarial framework to a preferable Nash equilibrium. Then, we design the D via dense connections, which brings more stable adversarial loss. Furthermore, a novel perceptual loss function, by reusing the intermediate features of D, is used to eliminate the gradient vanishing problem of Gs. To our knowledge, this is the first framework to focus on improving the performance of the D. Quantitatively, experimental results show the advantages of EDGAN on two widely used facial image databases against the state-of-the-art methods with different terms. EDGAN performs sharper and realistic results on real-world facial images with large pose and illumination variations than its competitors. © Springer Nature Switzerland AG 2018.","Face recognition; Optical resolving power; Adversarial networks; Densely connect; Face super-resolution; Facial recognition; Feature reuse; Perceptual fidelity; Pose and illumination variations; State-of-the-art methods; Image enhancement","Densely connect; Face super-resolution; Feature reuse; Generative adversarial network","Conference paper","Final","","Scopus","2-s2.0-85057217631"
"Wu X.; Li X.; He J.; Wu X.; Mumtaz I.","Wu, Xianyu (57205394227); Li, Xiaojie (56066646100); He, Jia (36165510700); Wu, Xi (57221065403); Mumtaz, Imran (57212166474)","57205394227; 56066646100; 36165510700; 57221065403; 57212166474","Generative Adversarial Networks with Enhanced Symmetric Residual Units for Single Image Super-Resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11295 LNCS","","","483","494","11","10.1007/978-3-030-05710-7_40","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059854201&doi=10.1007%2f978-3-030-05710-7_40&partnerID=40&md5=b26acee3776181b9a3aa6dfe9669f2d6","In this paper, we propose a new generative adversarial network (GAN) with enhanced symmetric residual units for single image super-resolution (ERGAN). ERGAN consists of a generator network and a discriminator network. The former can maximally reconstruct a super-resolution image similar to the original image. This lead to the discriminator network cannot distinguish the image from the training data or the generated sample. Combining residual units used in the generator network, ERGAN can retain the high-frequency features and alleviate the difficulty training in deep networks. Moreover, we constructed the symmetric skip-connections in residual units. This reused features generated from the low-level, and learned more high-frequency content. Moreover, ERGAN reconstructed the super-resolution image by four times the length and width of the original image and exhibited better visual characteristics. Experimental results on extensive benchmark evaluation showed that ERGAN significantly outperformed state-of-the-art approaches in terms of accuracy and vision. © 2019, Springer Nature Switzerland AG.","Discriminators; Image reconstruction; Optical resolving power; Adversarial networks; Benchmark evaluation; High frequency HF; Original images; Residual units; State-of-the-art approach; Super resolution; Symmetric skip-connection; Image enhancement","GAN; Residual units; Super-resolution; Symmetric skip-connection","Conference paper","Final","","Scopus","2-s2.0-85059854201"
"Deng X.","Deng, Xin (56071493400)","56071493400","Enhancing Image Quality via Style Transfer for Single Image Super-Resolution","2018","IEEE Signal Processing Letters","25","4","","571","575","4","10.1109/LSP.2018.2805809","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042133817&doi=10.1109%2fLSP.2018.2805809&partnerID=40&md5=b41cab9289ea6f8507a4e33fbf6fb87d","Recently, by feat of the Generative Adversarial Network (GAN), single image super-resolution (SISR) has achieved great breakthroughs in enhancing the perceptual image quality. However, since the network is trained by minimizing the perceptual loss, the GAN based SISR method (SRGAN) [1] results in images with very low objective quality, i.e., peak signal-to-noise ratio (PSNR). In this letter, we aim to solve this problem in an image style transfer way, to generate an image with similar perceptual quality as SRGAN, but with much higher objective quality. Moreover, we propose a threshold-based method to automatically alter the objective and perceptual quality of the reconstructed image through adjusting only one parameter. Experimental results show that our method can achieve more than 1.6 dB PSNR improvement over SRGAN with similar Mean Opinion Score value. Also, with the same objective quality, our method can provide significantly better perceptual results than other state-of-the-art SISR methods. © 1994-2012 IEEE.","Image processing; Image quality; Image reconstruction; Image resolution; Neural networks; Optical resolving power; Signal processing; Signal to noise ratio; Adversarial networks; Peak signal to noise ratio; Perceptual image quality; Reconstructed image; Signal processing algorithms; Signal resolution; Single images; style transfer; Image enhancement","Image quality; single image super-resolution (SISR); style transfer","Article","Final","","Scopus","2-s2.0-85042133817"
"Vu T.; Luu T.M.; Yoo C.D.","Vu, Thang (57206482454); Luu, Tung M. (57206486235); Yoo, Chang D. (7201746384)","57206482454; 57206486235; 7201746384","Perception-enhanced image super-resolution via relativistic generative adversarial networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS","","","98","113","15","10.1007/978-3-030-11021-5_7","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061724416&doi=10.1007%2f978-3-030-11021-5_7&partnerID=40&md5=156cf19d09e515bb8f78f9f0376cde3e","This paper considers a deep Generative Adversarial Networks (GAN) based method referred to as the Perception-Enhanced Super-Resolution (PESR) for Single Image Super Resolution (SISR) that enhances the perceptual quality of the reconstructed images by considering the following three issues: (1) ease GAN training by replacing an absolute with a relativistic discriminator, (2) include in the loss function a mechanism to emphasize difficult training samples which are generally rich in texture and (3) provide a flexible quality control scheme at test time to trade-off between perception and fidelity. Based on extensive experiments on six benchmark datasets, PESR outperforms recent state-of-the-art SISR methods in terms of perceptual quality. The code is available at https://github.com/thangvubk/PESR. © Springer Nature Switzerland AG 2019.","Benchmarking; Computer vision; Economic and social effects; Optical resolving power; Textures; Adversarial networks; Benchmark datasets; Image super resolutions; Loss functions; Perceptual quality; Reconstructed image; Super resolution; Training sample; Image enhancement","Perceptual quality; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85061724416"
"Creswell A.; White T.; Dumoulin V.; Arulkumaran K.; Sengupta B.; Bharath A.A.","Creswell, Antonia (57191416280); White, Tom (57219522961); Dumoulin, Vincent (55237034500); Arulkumaran, Kai (57191492242); Sengupta, Biswa (35422801900); Bharath, Anil A. (6701719807)","57191416280; 57219522961; 55237034500; 57191492242; 35422801900; 6701719807","Generative Adversarial Networks: An Overview","2018","IEEE Signal Processing Magazine","35","1","8253599","53","65","12","10.1109/MSP.2017.2765202","1061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040689867&doi=10.1109%2fMSP.2017.2765202&partnerID=40&md5=7056f331445310374e5fd7451d199fe8","Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application. © 1991-2012 IEEE.","Semantics; Adversarial networks; Annotated training data; Image super-resolution; Image synthesis; Semantic images; Signal processing","","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85040689867"
"Bosch M.; Munoz Abujder R.R.R.; Gifford C.","Bosch, Marc (24398475400); Munoz Abujder, Rodrigo Rene Rai (57211071520); Gifford, Christopher (24467858000)","24398475400; 57211071520; 24467858000","Towards image and video super-resolution for improved analytics from overhead imagery","2019","Proceedings of SPIE - The International Society for Optical Engineering","10992","","1099203","","","","10.1117/12.2518179","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072569349&doi=10.1117%2f12.2518179&partnerID=40&md5=0532d54f73676e98e6b8379d5ea61274","In this work, we address the problem of losing details in the overhead remote sensing image acquisition and generation process due to sensor resolution and distance to target by leveraging state-of-the-art deep neural network architectures. The goal is to recover such details by super-resolving the images acquired by overhead imaging sensors in order for human analysts to interpret data more accurately, and consequentially, for automated visual exploitation algorithms to be applied more effectively. We have developed a super-resolution framework operating on overhead full motion video (FMV) and still imagery (e.g. satellite images). Our framework consists of a neural network capable of learning the mapping between low and high resolution images in order to produce plausible details about the scene. Our framework combines Generative Adversarial Networks (GANs) and Recurrent Neural Networks (RNNs) to process low resolution signals both spatially and, in the case of FMV, temporally. We have applied the output of our system to several visual perception tasks, including object detection, object tracking, and semantic segmentation. We have also applied our methods to data from different geographical areas, sensors, and even modalities to demonstrate broad and generalized applicability. Copyright © 2019 SPIE.","Computer vision; Deep neural networks; Image acquisition; Network architecture; Object detection; Object recognition; Optical resolving power; Recurrent neural networks; Remote sensing; Satellite imagery; Semantics; Adversarial networks; Full motion video; High resolution image; Recurrent neural network (RNNs); Remote sensing images; Semantic segmentation; Super resolution; Video super-resolution; Image enhancement","computer vision; full motion video; generative adversarial networks (GANs); object detection; satellite imagery; semantic segmentation; super-resolution","Conference paper","Final","","Scopus","2-s2.0-85072569349"
"Zhou F.; Li X.; Li Z.","Zhou, Fuqiang (7401723117); Li, Xiaojie (57200757790); Li, Zuoxin (57200758241)","7401723117; 57200757790; 57200758241","High-frequency details enhancing DenseNet for super-resolution","2018","Neurocomputing","290","","","34","42","8","10.1016/j.neucom.2018.02.027","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042377231&doi=10.1016%2fj.neucom.2018.02.027&partnerID=40&md5=0b0e47e39257b410c46ee3f6f877d60a","Convolutional neural networks based models have made impressive advances for single-image super-resolution task. To advance the reconstruction quality of high-frequency details of the images, which are difficult to recover in super-resolution task, this paper proposes a super-resolution method using a high-frequency information enhancing densely connected convolutional neural network (SRDN) which can make the network pay more attention to high-frequency regions’ reconstruction like edges and textures during training. Our method applies relatively higher weights on the gradient descent values of these high-frequency regions’ pixels before they are propagated backward to update the parameters of the network during training. After that, we use a Generative Adversarial Network to finetune the trained model for finer texture details and more photo-realistic results. Experiments show that our approach can achieve a significant boost in the reconstruction quality of high-frequency details at high magnification ratios. We also design a novel measurement to evaluate the high-frequency details’ difference (HFD) between the ground truth image and the generated image. © 2018 Elsevier B.V.","Convolution; Neural networks; Optical resolving power; Adversarial networks; Convolutional neural network; Details enhancing; High-frequency informations; High-Frequency regions; Reconstruction quality; Super resolution; Superresolution methods; Article; artificial neural network; controlled study; densely connected convolutional neural network; image analysis; image enhancement; image quality; image reconstruction; priority journal; signal noise ratio; single image super resolution; Image enhancement","CNN; Details enhancing; GAN; Super-resolution","Article","Final","","Scopus","2-s2.0-85042377231"
"","","","14th Asian Conference on Computer Vision, ACCV 2018","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11366 LNCS","","","","","4380","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066961321&partnerID=40&md5=89a7ea56dbc42cc329f3149a4ca244a8","The proceedings contain 269 papers. The special focus in this conference is on Computer Vision. The topics include: Style Transfer with Adversarial Learning for Cross-Dataset Person Re-identification; automatic Graphics Program Generation Using Attention-Based Hierarchical Decoder; occlusion Aware Stereo Matching via Cooperative Unsupervised Learning; background Subtraction Based on Fusion of Color and Local Patterns; a Coded Aperture for Watermark Extraction from Defocused Images; towards Locally Consistent Object Counting with Constrained Multi-stage Convolutional Neural Networks; an Improved Learning Framework for Covariant Local Feature Detection; better Guider Predicts Future Better: Difference Guided Generative Adversarial Networks; guided Feature Selection for Deep Visual Odometry; common Self-polar Triangle of Concentric Conics for Light Field Camera Calibration; appearance-Based Gaze Estimation Using Dilated-Convolutions; deep Clustering and Block Hashing Network for Face Image Retrieval; learning Energy Based Inpainting for Optical Flow; learning Background Subtraction by Video Synthesis and Multi-scale Recurrent Networks; universal Bounding Box Regression and Its Applications; continuous-Time Stereo Visual Odometry Based on Dynamics Model; a2A: Attention to Attention Reasoning for Movie Question Answering; TraMNet - Transition Matrix Network for Efficient Action Tube Proposals; shape-Conditioned Image Generation by Learning Latent Appearance Representation from Unpaired Data; dense In Dense: Training Segmentation from Scratch; CubemapSLAM: A Piecewise-Pinhole Monocular Fisheye SLAM System; Single Image Super-Resolution Using Lightweight CNN with Maxout Units; AVID: Adversarial Visual Irregularity Detection; localization-Aware Active Learning for Object Detection; deep Inverse Halftoning via Progressively Residual Learning; dynamic Random Walk for Superpixel Segmentation; BAN: Focusing on Boundary Context for Object Detection; rethinking Planar Homography Estimation Using Perspective Fields.","","","Conference review","Final","","Scopus","2-s2.0-85066961321"
"Zhu C.; Xu L.; Liu X.-Y.; Qian F.","Zhu, Chenxiao (57203368174); Xu, Lingqing (57203373718); Liu, Xiao-Yang (44361326100); Qian, Feng (57203399882)","57203368174; 57203373718; 44361326100; 57203399882","Tensor-Generative Adversarial Network with Two-Dimensional Sparse Coding: Application to Real-Time Indoor Localization","2018","IEEE International Conference on Communications","2018-May","","8423008","","","","10.1109/ICC.2018.8423008","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051422966&doi=10.1109%2fICC.2018.8423008&partnerID=40&md5=5d2107e491b07d832dba1ce57d1e7042","Localization technology is important for the development of indoor location-based services (LBS). Global Positioning System (GPS) becomes invalid in indoor environments due to the non-line-of-sight issue, so it is urgent to develop a real-time high-accuracy localization approach for smartphones. However, accurate localization is challenging due to issues such as real-time response requirements, limited fingerprint samples and mobile device storage. To address these problems, we propose a novel deep learning architecture: Tensor-Generative Adversarial Network (TGAN). We first introduce a transform-based 3D tensor to model fingerprint samples. Instead of those passive methods that construct a fingerprint database as a prior, our model applies artificial neural network with deep learning to train network classifiers and then gives out estimations. Then we propose a novel tensorbased super-resolution scheme using the generative adversarial network (GAN) that adopts sparse coding as the generator network and a residual learning network as the discriminator. Further, we analyze the performance of TGAN and implement a trace-based localization experiment, which achieves better performance. Compared to existing methods for smartphones indoor positioning, that are energy- consuming and high demands on devices, TGAN can give out an improved solution in localization accuracy, response time and implementation complexity. © 2018 IEEE.","Classification (of information); Codes (symbols); Deep learning; Global positioning system; Indoor positioning systems; Mobile devices; Network coding; Neural networks; Smartphones; Telecommunication services; Tensors; Adversarial networks; Implementation complexity; Indoor localization; Learning architectures; Localization accuracy; Localization technologies; RF fingerprints; Sparse coding; Location based services","Indoor localization; RF fingerprint; Smartphones; Tensor-based generative adversarial network; Two-dimensional sparse coding","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85051422966"
"Ruan L.; Chen B.; Miu-Ling L.A.M.","Ruan, Lingyan (57205468831); Chen, Bin (56742504100); Miu-Ling, L.A.M. (7202630301)","57205468831; 56742504100; 7202630301","Light field synthesis from a single image using improved wasserstein generative adversarial network","2018","European Association for Computer Graphics - 39th Annual Conference, EUROGRAPHICS 2018 - Posters","","","","19","20","1","10.2312/egp.20181017","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078263765&doi=10.2312%2fegp.20181017&partnerID=40&md5=c8b87d71a504f9aa4c4f0b9c558c038a","We present a deep learning-based method to synthesize a 4D light field from a single 2D RGB image. We consider the light field synthesis problem equivalent to image super-resolution, and solve it by using the improved Wasserstein Generative Adversarial Network with gradient penalty (WGAN-GP). Experimental results demonstrate that our algorithm can predict complex occlusions and relative depths in challenging scenes. The light fields synthesized by our method has much higher signal-to-noise ratio and structural similarity than the state-of-the-art approach. Eurographics Proceedings © 2018 The Eurographics Association.","Computer graphics; Deep learning; Signal to noise ratio; Adversarial networks; Image super resolutions; Learning-based methods; Light fields; RGB images; Single images; State-of-the-art approach; Structural similarity; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85078263765"
"","","","ACM International Conference Proceeding Series","2018","ACM International Conference Proceeding Series","","","","","","242","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055678302&partnerID=40&md5=cba4f10f8c35449e0a392e5107e4a56f","The proceedings contain 46 papers. The topics discussed include: a framework with a multi-task CNN model joint with a re-ranking method for vehicle re-identification; deep hashing with multilevel similarity learning for multimedia similarity search; a novel framework for semantic segmentation with generative adversarial network; deep click feature based query merging for robust image recognition; the comparison of different graph convolutional neural networks for image recognition; an adaptive blind watermarking algorithm based on DCT and its application in multimedia data processing; salient object detection based on HDCT and the prior boundary; stabilizing video facial landmark detection and tracking via global and local filtering; automatic makeup based on generative adversarial nets; enhancement of underwater images by super-resolution generative adversarial networks; a convolutional sequential model for network load forecasting; prediction of RNA-protein interactions with distributed feature representations and a hybrid deep model; an image stabilization algorithm on camera trajectory estimation and feature block matching; learning semantic topics for domain-adapted textual knowledge transfer; automatic image cropping with a single fully convolutional network; domain separation network for cross-modal retrieval; entropy-based spammer detection; and video question answering via multi-granularity temporal attention network learning.","","","Conference review","Final","","Scopus","2-s2.0-85055678302"
"Wang Y.; Ding W.; Su F.","Wang, Yuyang (57204102989); Ding, Wenjun (57195941496); Su, Feng (55725786500)","57204102989; 57195941496; 55725786500","Super-resolution of text image based on conditional generative adversarial network","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11166 LNCS","","","270","281","11","10.1007/978-3-030-00764-5_25","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054521154&doi=10.1007%2f978-3-030-00764-5_25&partnerID=40&md5=8211f58ae5dfcd749a788bdc69601540","To generate high-resolution text images from available low-resolution ones is of great value to many text-related applications, especially text recognition. In this paper, we propose an effective super-resolution method for text images based on Conditional Generative Adversarial Network (cGAN). Specifically, we improve the cGAN model by removing the Batch Normalization layers and introducing the Inception structure to make it more suited to the text image super-resolution task, which contribute to the overall enhanced performances of the proposed method relative to the original cGAN model. Experiment results on public dataset demonstrate the effectiveness of the proposed method. © Springer Nature Switzerland AG 2018.","Character recognition; Optical resolving power; Adversarial networks; Batch normalization; Inception; Super resolution; Text images; Image enhancement","Batch normalization; Conditional generative adversarial network; Inception; Super-resolution; Text image","Conference paper","Final","","Scopus","2-s2.0-85054521154"
"Wang A.; Wang Y.; Song X.; Iwahori Y.","Wang, Aili (55483869300); Wang, Ying (57207001725); Song, Xiaoying (57210435647); Iwahori, Yuji (7003339167)","55483869300; 57207001725; 57210435647; 7003339167","Remote Sensing Image Super-Resolution Reconstruction based on Generative Adversarial Network","2019","International Journal of Performability Engineering","15","7","","1783","1791","8","10.23940/ijpe.19.07.p4.17831791","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070709753&doi=10.23940%2fijpe.19.07.p4.17831791&partnerID=40&md5=0c36fb174d47d236fbcf8f8086646e1b","The super-resolution reconstruction algorithm based on generative adversarial network (GAN) can generate realistic texture in the super-resolution process of a single remote sensing image. In order to further improve the visual quality of the reconstructed image, this paper will improve the generation network, discrimination network, and perceptual loss of the generated confrontation network. Firstly, the batch normalization layer is removed and dense connections are used in the residual blocks, which effectively improves the performance of the generated network. Then, we use the relative discriminant network to learn more detailed texture. Finally, we obtain the perception loss before the activation function to maintain the consistency of brightness. In addition, transfer learning is used to solve the problem of insufficient remote sensing data. The experimental results show that the proposed algorithm has superiority in the super-resolution reconstruction of remote sensing images and can obtain better subjective visual effects. © 2019 Totem Publisher, Inc. All rights reserved.","Image enhancement; Optical resolving power; Remote sensing; Textures; Activation functions; Adversarial networks; Reconstructed image; Remote sensing data; Remote sensing images; Residual dense block; Super resolution reconstruction; Transfer learning; Image reconstruction","Generative adversarial network; Remote sensing image super-resolution reconstruction; Residual dense block; Transfer learning","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85070709753"
"Liu L.; Wang S.; Wan L.","Liu, Lu (57206979171); Wang, Shenghui (14053344600); Wan, Lili (35281234600)","57206979171; 14053344600; 35281234600","Component Semantic Prior Guided Generative Adversarial Network for Face Super-Resolution","2019","IEEE Access","7","","8734147","77027","77036","9","10.1109/ACCESS.2019.2921859","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068253298&doi=10.1109%2fACCESS.2019.2921859&partnerID=40&md5=46964fd23d92396aad1824d517a3b59f","Face super-resolved (SR) images aid human perception. The state-of-the-art face SR methods leverage the spatial location of facial components as prior knowledge. However, it remains a great challenge to generate natural textures. In this paper, we propose a component semantic prior guided generative adversarial network (CSPGAN) to synthesize faces. Specifically, semantic probability maps of facial components are exploited to modulate features in the CSPGAN through affine transformation. To compensate for the overly smooth performance of the generative network, a gradient loss is proposed to recover the high-frequency details. Meanwhile, the discriminative network is designed to perform multiple tasks which predict semantic category and distinguish authenticity simultaneously. The extensive experimental results demonstrate the superiority of the CSPGAN in reconstructing photorealistic textures. © 2013 IEEE.","Knowledge management; Optical resolving power; Semantic Web; Textures; Adversarial networks; Affine transformations; Discriminative networks; Face super-resolution; Facial components; High frequency HF; Multiple tasks; Semantic category; Semantics","face super-resolution; Facial component; generative adversarial networks; multiple task; semantic prior","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85068253298"
"Zhang C.; Ouyang X.; Patras P.","Zhang, Chaoyun (57200217527); Ouyang, Xi (57188922178); Patras, Paul (25824029800)","57200217527; 57188922178; 25824029800","ZipNet-GAN: Inferring fine-grained mobile traffic patterns via a generative adversarial neural network","2017","CoNEXT 2017 - Proceedings of the 2017 13th International Conference on emerging Networking EXperiments and Technologies","","","","363","375","12","10.1145/3143361.3143393","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040225802&doi=10.1145%2f3143361.3143393&partnerID=40&md5=bfaa0dbac6c131e458e03aacd6316dd4","Large-scale mobile traffic analytics is becoming essential to digital infrastructure provisioning, public transportation, events planning, and other domains. Monitoring city-wide mobile traffic is however a complex and costly process that relies on dedicated probes. Some of these probes have limited precision or coverage, others gather tens of gigabytes of logs daily, which independently offer limited insights. Extracting fine-grained patterns involves expensive spatial aggregation of measurements, storage, and post-processing. In this paper, we propose a mobile traffic super-resolution technique that overcomes these problems by inferring narrowly localised traffic consumption from coarse measurements. We draw inspiration from image processing and design a deep-learning architecture tailored to mobile networking, which combines Zipper Network (ZipNet) and Generative Adversarial neural Network(GAN) models. This enables to uniquely capture spatio-temporal relations between traffic volume snapshots routinely monitored over broad coverage areas ('low-resolution') and the corresponding consumption at 0.05 km2 level ('high-resolution') usually obtained after intensive computation. Experiments we conduct with a real-world data set demonstrate that the proposed ZipNet(-GAN) infers traffic consumption with remarkable accuracy and up to 100X higher granularity as compared to standard probing, while outperforming existing data interpolation techniques. To our knowledge, this is the first time super-resolution concepts are applied to large-scale mobile traffic analysis and our solution is the first to infer fine-grained urban traffic patterns from coarse aggregates. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Deep learning; Digital storage; Image processing; Probes; Adversarial networks; Digital infrastructures; Learning architectures; Mobile traffic; Public transportation; Spatial aggregation; Spatio-temporal relations; Super resolution; Optical resolving power","Deep learning; Generative adversarial networks; Mobile traffic; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85040225802"
"Xie Y.; Franz E.; Chu M.; Thuerey N.","Xie, You (57204697503); Franz, Erik (57204695884); Chu, Mengyu (57196000654); Thuerey, Nils (26024848800)","57204697503; 57204695884; 57196000654; 26024848800","TempoGAN: A temporally coherent, volumetric GAN for super-resolution fluid flow","2018","ACM Transactions on Graphics","37","4","A56","","","","10.1145/3197517.3201304","170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056713844&doi=10.1145%2f3197517.3201304&partnerID=40&md5=8ecd465b1b2d05ceb7ca7e9d8627250a","We propose a temporally coherent generative model addressing the superresolution problem for fluid flows. Our work represents a first approach to synthesize four-dimensional physics fields with neural networks. Based on a conditional generative adversarial network that is designed for the inference of three-dimensional volumetric data, our model generates consistent and detailed results by using a novel temporal discriminator, in addition to the commonly used spatial one. Our experiments show that the generator is able to infer more realistic high-resolution details by using additional physical quantities, such as low-resolution velocities or vorticities. Besides improvements in the training process and in the generated outputs, these inputs offer means for artistic control as well. We additionally employ a physics-aware data augmentation step, which is crucial to avoid overfitting and to reduce memory requirements. In this way, our network learns to generate advected quantities with highly detailed, realistic, and temporally coherent features. Our method works instantaneously, using only a single time-step of low-resolution fluid data. We demonstrate the abilities of our method using a variety of complex inputs and applications in two and three dimensions. © 2018 Copyright held by the owner/author(s).","Animation; Deep learning; Optical resolving power; Volumetric analysis; Adversarial networks; Computer animation; Fluid simulations; Generative model; High-Resolution Details; Memory requirements; Physical quantities; Physics-based; Flow of fluids","computer animation; fluid simulation; generative models; physics-based deep learning","Article","Final","","Scopus","2-s2.0-85056713844"
"Li W.; Li G.; Yue W.; Xu H.","Li, Wanlan (57205124499); Li, Guangya (57198457388); Yue, Weidong (57203688614); Xu, Hao (57206577540)","57205124499; 57198457388; 57203688614; 57206577540","Realistic single-image super-resolution using autoencoding adversarial networks","2018","Journal of Electronic Imaging","27","6","063020","","","","10.1117/1.JEI.27.6.063020","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058689839&doi=10.1117%2f1.JEI.27.6.063020&partnerID=40&md5=e32bf2162464d990842234a02ac59121","The accuracy and efficiency of single-image super-resolution (SR) using techniques based on convolutional neural networks have recently shown much improvement. However, most of the existing algorithms aim at improving the peak signal-to-noise ratio by minimizing the mean squared error between the ground-truth images and the generated SR images. This leads to a lack of high-frequency information and nonconformance with the perception of human eyes. To reconstruct realistic natural images in SR with large up-sampling factors, we combine the benefits of some recent approaches and propose a method based on autoencoding adversarial networks. The proposed architecture includes a generator, which is a symmetric encode-decode network used to extract feature maps and recover high-resolution images, and a conditional discriminator, which is used to determine whether the generated image is from the real distribution or not. In addition, we extract highlevel features from a pretrained network to optimize the perceptual loss and make the output more precise. Compared with several state-of-the-art methods, our proposed method shows outstanding performance in recovering fine texture details. The mean opinion score shows that our method yields results that are more satisfactory to human perception than the other methods under comparison. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Generative adversarial networks; Image enhancement; Mean square error; Neural networks; Optical resolving power; Signal to noise ratio; Textures; Adversarial networks; Convolutional neural network; Ground truth; Image super resolutions; Images reconstruction; Mean squared error; Peak signal to noise ratio; Single images; Superresolution; Visual perception; Image reconstruction","generative adversarial networks; image reconstruction; super-resolution; visual perception","Article","Final","","Scopus","2-s2.0-85058689839"
"Kong J.; Wang C.","Kong, Jie (57201572233); Wang, Congying (57205222998)","57201572233; 57205222998","Resolution Enhancement for Low-resolution Text Images Using Generative Adversarial Network","2018","MATEC Web of Conferences","246","","03040","","","","10.1051/matecconf/201824603040","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059181682&doi=10.1051%2fmatecconf%2f201824603040&partnerID=40&md5=274e33c8e85100ab62ee6ea223159393","In recent years, although Optical Character Recognition (OCR) has made considerable progress, low-resolution text images commonly appearing in many scenarios may still cause errors in recognition. For this problem, the technique of Generative Adversarial Network in super-resolution processing is applied to enhance the resolution of low-quality text images in this study. The principle and the implementation in TensorFlow of this technique are introduced. On this basis, a system is proposed to perform the resolution enhancement and OCR for low-resolution text images. The experimental results indicate that this technique could significantly improve the accuracy, reduce the error rate and false rejection rate of low-resolution text images identification. © The Authors, published by EDP Sciences, 2018.","Optical character recognition; Waterworks; Adversarial networks; False rejection rate; Low qualities; Low resolution; Optical character recognition (OCR); Resolution enhancement; Super resolution; Text images; Image enhancement","","Conference paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85059181682"
"Kim S.; Suh D.Y.","Kim, San (57208597883); Suh, Doug Young (7202639547)","57208597883; 7202639547","Recursive Conditional Generative Adversarial Networks for Video Transformation","2019","IEEE Access","7","","8673567","37807","37821","14","10.1109/ACCESS.2019.2906472","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065253451&doi=10.1109%2fACCESS.2019.2906472&partnerID=40&md5=2f1e8326c28cfcffab2cf4a5d8c4016b","Conditional generative adversarial networks (cGANs) are used in various transformation applications, such as super-resolution, colorization, image denoising, and image inpainting. So far, cGANs have been applied to the transformation of still images, but their use could be extended to the transformation of video contents, which has a much larger market. This paper considers problems with the cGAN-based transformation of video contents. The major problem is flickering caused by the discontinuity between adjacent image frames. Several postprocessing algorithms have been proposed to reduce that effect after transformation. We propose a recursive cGAN in which the previous output frame is used as an input in addition to the current input frame to reduce the flickering effect without losing the objective quality of each image. Compared with previous postprocessing algorithms, our approach performed better in terms of various evaluation metrics for video contents. © 2013 IEEE.","Flickering; Video recording; Adversarial networks; Evaluation metrics; Image Inpainting; Image transformations; Objective qualities; Postprocessing algorithms; Super resolution; video transformation; Image denoising","generative adversarial network; Image-to-image transformation; reducing flicker; video transformation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85065253451"
"Lucas A.; Katsaggelos A.K.; Lopez-Tapuia S.; Molina R.","Lucas, Alice (57200297854); Katsaggelos, Aggelos K. (7102711302); Lopez-Tapuia, Santiago (57207792592); Molina, Rafael (34870201500)","57200297854; 7102711302; 57207792592; 34870201500","Generative adversarial networks and perceptual losses for video super-resolution","2018","Proceedings - International Conference on Image Processing, ICIP","","","8451714","51","55","4","10.1109/ICIP.2018.8451714","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062918260&doi=10.1109%2fICIP.2018.8451714&partnerID=40&md5=129ec852a4b86722d63a73b1d8fe2b03","Recent research on image super-resolution (SR) has shown that the use of perceptual losses such as feature-space loss functions and adversarial training can greatly improve the perceptual quality of the resulting SR output. In this paper, we extend the use of these perceptual-focused approaches for image SR to that of video SR. We design a 15-block residual neural network, VSRResNet, which is pre-trained on a the traditional mean -squared -error (MSE) loss and later fine-tuned with a feature-space loss function in an adversarial setting. We show that our proposed system, VSRRes-FeatGAN, produces super-resolved frames of much higher perceptual quality than those provided by the MSE-based model. © 2018 IEEE.","Mean square error; Neurons; Optical resolving power; Adversarial networks; Loss functions; Neuronal networks; Super resolution; Video; Image enhancement","Convolutional Neuronal Networks; Generative Adversarial Networks; Perceptual Loss Functions; Superresolution; Video","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062918260"
"Cheon M.; Kim J.-H.; Choi J.-H.; Lee J.-S.","Cheon, Manri (55938054900); Kim, Jun-Hyuk (57191686104); Choi, Jun-Ho (55722482400); Lee, Jong-Seok (36062379400)","55938054900; 57191686104; 55722482400; 36062379400","Generative adversarial network-based image super-resolution using perceptual content losses","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS","","","51","62","11","10.1007/978-3-030-11021-5_4","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061709563&doi=10.1007%2f978-3-030-11021-5_4&partnerID=40&md5=f26d19f4aa9127ab657585a34835749f","In this paper, we propose a deep generative adversarial network for super-resolution considering the trade-off between perception and distortion. Based on good performance of a recently developed model for super-resolution, i.e., deep residual network using enhanced upscale modules (EUSR) [9], the proposed model is trained to improve perceptual performance with only slight increase of distortion. For this purpose, together with the conventional content loss, i.e., reconstruction loss such as L1 or L2, we consider additional losses in the training phase, which are the discrete cosine transform coefficients loss and differential content loss. These consider perceptual part in the content loss, i.e., consideration of proper high frequency components is helpful for the trade-off problem in super-resolution. The experimental results show that our proposed model has good performance for both perception and distortion, and is effective in perceptual super-resolution applications. © Springer Nature Switzerland AG 2019.","Computer vision; Deep learning; Discrete cosine transforms; Distortion (waves); Economic and social effects; Image coding; Sensory perception; Adversarial networks; Developed model; Discrete cosine transform coefficients; High frequency components; Image super resolutions; Super resolution; Trade-off problem; Training phase; Optical resolving power","Deep learning; Distortion; Perception; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85061709563"
"Guei A.-C.; Akhloufi M.A.","Guei, Axel-Christian (57202650067); Akhloufi, Moulay A. (23979609400)","57202650067; 23979609400","Deep learning for face recognition at a distance","2018","Proceedings of SPIE - The International Society for Optical Engineering","10652","","106520T","","","","10.1117/12.2304896","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049161393&doi=10.1117%2f12.2304896&partnerID=40&md5=1b31899d442b359c0e227a98ffb385f6","Face recognition is a research area that has been widely studied by the computer vision community in the past years. Most of the work deals with close frontal images of the face where facial structures can be easily distinguished. Little work deals with recognizing faces at a distance, where faces are at a very low resolution and barely distinguishable. In this work, we present a deep learning architecture that can be used to enhance lower resolution facial images captured at a distance. The proposed framework uses Deep Convolutional Generative Adversarial Networks (DCGAN). The proposed architecture works well even in the presence of a small number of images for learning. The new enhanced images are then sent to a face recognition algorithm for classification. The proposed framework outperforms classical enhancement techniques and leads to an increase in the face recognition performance. © 2018 SPIE.","Biometrics; Deep learning; Image enhancement; Network architecture; Adversarial networks; Face recognition algorithms; Face recognition performance; Learning architectures; Lower resolution; Proposed architectures; Super resolution; Vision communities; Face recognition","Biometrics; Deep learning; Face recognition; Generative Adversarial Networks; Super Resolution","Conference paper","Final","","Scopus","2-s2.0-85049161393"
"Huang B.; Chen W.; Wu X.; Lin C.-L.; Suganthan P.N.","Huang, Bin (57207403504); Chen, Weihai (35776127700); Wu, Xingming (8976009300); Lin, Chun-Liang (24923215500); Suganthan, Ponnuthurai Nagaratnam (7003996538)","57207403504; 35776127700; 8976009300; 24923215500; 7003996538","High-quality face image generated with conditional boundary equilibrium generative adversarial networks","2018","Pattern Recognition Letters","111","","","72","79","7","10.1016/j.patrec.2018.04.028","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046414882&doi=10.1016%2fj.patrec.2018.04.028&partnerID=40&md5=6ccca5b101a456f6bf9963bfa0d32493","We propose a novel single face image super-resolution method, which is named Face Conditional Generative Adversarial Network (FCGAN), based on boundary equilibrium generative adversarial networks. Without taking any prior facial information, our approach combines the pixel-wise L1 loss and GAN loss to optimize our super-resolution model and to generate a high-quality face image from a low-resolution one robustly (with upscaling factor 4 ×). Additionally, Compared with existing peer researches, both training and testing phases of FCGAN are end-to-end pipeline without pre/post-processing. To enhance the convergence speed and strengthen feature propagation, the Generator and Discriminator networks are designed with a skip-connection architecture, and both using an auto-encoder structure. Quantitative experiments demonstrate that our model achieves competitive performance compared with the state-of-the-art models based on both visual quality and quantitative criterions. We believe this high-quality face image generated method can impact many applications in face identification and intelligent monitor. © 2018 Elsevier B.V.","Neural networks; Adversarial networks; Competitive performance; Convolutional Neural Networks (CNN); Face hallucination; Quantitative experiments; Super resolution; Super-resolution models; Training and testing; Optical resolving power","Convolutional Neural Network (CNN); Face hallucination; Generative Adversarial Network (GAN); Super resolution","Article","Final","","Scopus","2-s2.0-85046414882"
"Purohit K.; Mandal S.; Rajagopalan A.N.","Purohit, Kuldeep (57192539878); Mandal, Srimanta (56103894000); Rajagopalan, A.N. (16647440100)","57192539878; 56103894000; 16647440100","Scale-recurrent multi-residual dense network for image super-resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS","","","132","149","17","10.1007/978-3-030-11021-5_9","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061732237&doi=10.1007%2f978-3-030-11021-5_9&partnerID=40&md5=cd7361d4d87c6c3c99b54076808ba7c4","Recent advances in the design of convolutional neural network (CNN) have yielded significant improvements in the performance of image super-resolution (SR). The boost in performance can be attributed to the presence of residual or dense connections within the intermediate layers of these networks. The efficient combination of such connections can reduce the number of parameters drastically while maintaining the restoration quality. In this paper, we propose a scale recurrent SR architecture built upon units containing series of dense connections within a residual block (Residual Dense Blocks (RDBs)) that allow extraction of abundant local features from the image. Our scale recurrent design delivers competitive performance for higher scale factors while being parametrically more efficient as compared to current state-of-the-art approaches. To further improve the performance of our network, we employ multiple residual connections in intermediate layers (referred to as Multi-Residual Dense Blocks), which improves gradient propagation in existing layers. Recent works have discovered that conventional loss functions can guide a network to produce results which have high PSNRs but are perceptually inferior. We mitigate this issue by utilizing a Generative Adversarial Network (GAN) based framework and deep feature (VGG) losses to train our network. We experimentally demonstrate that different weighted combinations of the VGG loss and the adversarial loss enable our network outputs to traverse along the perception-distortion curve. The proposed networks perform favorably against existing methods, both perceptually and objectively (PSNR-based) with fewer parameters. © Springer Nature Switzerland AG 2019.","Computer vision; Deep learning; Image enhancement; Neural networks; Optical resolving power; Adversarial networks; Competitive performance; Convolutional neural network; Image super resolutions; Intermediate layers; Restoration quality; State-of-the-art approach; Super resolution; Network layers","Deep learning; Dense connections; Residual networks; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85061732237"
"Tao Y.; Muller J.-P.","Tao, Yu (56539197700); Muller, Jan-Peter (7404871794)","56539197700; 7404871794","Super-resolution restoration of MISR images using the UCL MAGiGAN system","2019","Remote Sensing","11","1","52","","","","10.3390/rs11010052","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059950995&doi=10.3390%2frs11010052&partnerID=40&md5=a20a39691361ba100c7146d05da7ba55","High spatial resolution Earth observation imagery is considered desirable for many scientific and commercial applications. Given repeat multi-angle imagery, an imaging instrument with a specified spatial resolution, we can use image processing and deep learning techniques to enhance the spatial resolution. In this paper, we introduce the University College London (UCL) MAGiGAN super-resolution restoration (SRR) system based on multi-angle feature restoration and deep SRR networks. We explore the application of MAGiGAN SRR to a set of 9 MISR red band images (275 m) to produce up to a factor of 3.75 times resolution enhancement. We show SRR results over four different test sites containing different types of image content including urban and rural targets, sea ice and a cloud field. Different image metrics are introduced to assess the overall SRR performance, and these are employed to compare the SRR results with the original MISR input images and higher resolution Landsat images, where available. Significant resolution improvement over various types of image content is demonstrated and the potential of SRR for different scientific application is discussed. © 2019 by the authors.","Deep learning; Image reconstruction; Image resolution; Optical resolving power; Restoration; Sea ice; Urban growth; Adversarial networks; Feature matching; Gotcha; MISR; Super-resolution restoration; Image enhancement","Deep learning; Feature matching; GAN; Generative adversarial network; Gotcha; GPT; MISR; SRR; Super-resolution restoration","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85059950995"
"Wei X.; Feng S.; Zhu J.; Su H.","Wei, Xingxing (55669197900); Feng, Sitong (57204974221); Zhu, Jun (56734692500); Su, Hang (37017428500)","55669197900; 57204974221; 56734692500; 37017428500","Video-to-video translation with global temporal consistency","2018","MM 2018 - Proceedings of the 2018 ACM Multimedia Conference","","","","18","25","7","10.1145/3240508.3240708","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058235852&doi=10.1145%2f3240508.3240708&partnerID=40&md5=e97d5ce95232b22b2f9adce84176bd56","Although image-to-image translation has been widely studied, the video-to-video translation is rarely mentioned. In this paper, we propose an unified video-to-video translation framework to accomplish different tasks, like video super-resolution, video colourization, and video segmentation, etc. A consequent question within video-to-video translation lies in the flickering appearance along with the varying frames. To overcome this issue, a usual method is to incorporate the temporal loss between adjacent frames in the optimization, which is a kind of local frame-wise temporal consistency. We instead present a residual error based mechanism to ensure the video-level consistency of the same location in different frames (called дlobal temporal consistency). The global and local consistency are simultaneously integrated into our video-to-video framework to achieve more stable videos. Our method is based on the GAN framework, where we present a two-channel discriminator. One channel is to encode the video RGB space, and another is to encode the residual error of the video as a whole to meet the global consistency. Extensive experiments conducted on different video-to-video translation tasks verify the effectiveness and flexibleness of the proposed method. © 2018 Association for Computing Machinery.","Encoding (symbols); Large scale systems; Adversarial networks; Global consistency; Image translation; Local consistency; Temporal consistency; Video segmentation; Video super-resolution; Video to videos; Translation (languages)","Generative Adversarial Network; Temporal Consistency; Video-to-Video Translation","Conference paper","Final","","Scopus","2-s2.0-85058235852"
"Bai Y.; Zhang Y.; Ding M.; Ghanem B.","Bai, Yancheng (57206596265); Zhang, Yongqiang (57190277288); Ding, Mingli (8925275900); Ghanem, Bernard (24331436200)","57206596265; 57190277288; 8925275900; 24331436200","SOD-MTGAN: Small object detection via multi-task generative adversarial network","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11217 LNCS","","","210","226","16","10.1007/978-3-030-01261-8_13","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055587142&doi=10.1007%2f978-3-030-01261-8_13&partnerID=40&md5=73efa96858692294a647469f5f24522d","Object detection is a fundamental and important problem in computer vision. Although impressive results have been achieved on large/medium sized objects in large-scale detection benchmarks (e.g. the COCO dataset), the performance on small objects is far from satisfactory. The reason is that small objects lack sufficient detailed appearance information, which can distinguish them from the background or similar objects. To deal with the small object detection problem, we propose an end-to-end multi-task generative adversarial network (MTGAN). In the MTGAN, the generator is a super-resolution network, which can up-sample small blurred images into fine-scale ones and recover detailed information for more accurate detection. The discriminator is a multi-task network, which describes each super-resolved image patch with a real/fake score, object category scores, and bounding box regression offsets. Furthermore, to make the generator recover more details for easier detection, the classification and regression losses in the discriminator are back-propagated into the generator during training. Extensive experiments on the challenging COCO dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods. © Springer Nature Switzerland AG 2018.","Benchmarking; Computer vision; Image enhancement; Object recognition; Optical resolving power; Adversarial networks; COCO; Detection performance; Image patches; Object categories; Small object detection; State-of-the-art methods; Super resolution; Object detection","COCO; Generative adversarial network; Multi-task; Small object detection; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85055587142"
"Luo X.; Chen R.; Xie Y.; Qu Y.; Li C.","Luo, Xiaotong (57216369982); Chen, Rong (57202222085); Xie, Yuan (55710277800); Qu, Yanyun (8509964600); Li, Cuihua (57204712944)","57216369982; 57202222085; 55710277800; 8509964600; 57204712944","Bi-GANs-ST for perceptual image super-resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11133 LNCS","","","20","34","14","10.1007/978-3-030-11021-5_2","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061695865&doi=10.1007%2f978-3-030-11021-5_2&partnerID=40&md5=a5e3ae075ff0e4ed43982c4cc9d359c2","Image quality measurement is a critical problem for image super-resolution (SR) algorithms. Usually, they are evaluated by some well-known objective metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results in accordance with the perception of human being. Recently, a more reasonable perception measurement has been proposed in [1], which is also adopted by the PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a high-quality SR result which balances between the two indices, i.e., the perception index and root-mean-square error (RMSE). To do so, we design a new deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary generative adversarial networks (GAN) branches. One is memory residual SRGAN (MR-SRGAN), which emphasizes on improving the objective performance, such as reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which obtains the result that favors better subjective perception via a two-stage adversarial training mechanism. Then, to produce final result with excellent perception scores and RMSE, we use soft-thresholding method to merge the results generated by the two GANs. Our method performs well on the perceptual image super-resolution task of the PIRM 2018 challenge. Experimental results on five benchmarks show that our proposal achieves highly competent performance compared with other state-of-the-art methods. © Springer Nature Switzerland AG 2019.","Benchmarking; Computer vision; Mean square error; Adversarial networks; Image quality measurement; Image super resolutions; Perceptual image; Root mean square errors; Soft thresholding; State-of-the-art methods; Subjective perceptions; Optical resolving power","GAN; Image super-resolution; Perceptual image; Soft-thresholding","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85061695865"
"","","","2nd Chinese Conference on Computer Vision, CCCV 2017","2017","Communications in Computer and Information Science","773","","","","","1377","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038086371&partnerID=40&md5=b6622d05247c58921045dc68f6d2e997","The proceedings contain 113 papers. The special focus in this conference is on Computer Vision. The topics include: Visual saliency fusion based multi-feature for semantic image retrieval; unsupervised multi-view subspace learning via maximizing dependence; structured multi-view supervised feature selection algorithm research; an automatic shoeprint retrieval method using neural codes for commercial shoeprint scanners; uncovering the effect of visual saliency on image retrieval; shape-color differential moment invariants under affine transforms; a novel layer based image fusion approach via transfer learning and coupled dictionary; local saliency extraction for fusion of visible and infrared images; distributed compressive sensing for light field reconstruction using structured random matrix; improved face verification with simple weighted feature combination; stereoscopic image quality assessment based on binocular adding and subtracting; quality assessment of palm vein image using natural scene statistics; an error-activation-guided blind metric for stitched panoramic image quality assessment; image aesthetic quality evaluation using convolution neural network embedded fine-tune; high capacity reversible data hiding with contrast enhancement; rank learning for dehazed image quality assessment; a low-rank total-variation regularized tensor completion algorithm; nighttime haze removal with fusion atmospheric light and improved entropy; light field super-resolution using cross-resolution input based on patchmatch and learning method; a new image sparse reconstruction method for mixed Gaussian-poisson noise with multiple constraints; pore-scale facial features matching under 3D morphable model constraint; face video super-resolution with identity guided generative adversarial networks; exemplar-based pixel by pixel inpainting based on patch shift; GAN based sample simulation for SEM-image super resolution; PSO-based single image defogging.","","","Conference review","Final","","Scopus","2-s2.0-85038086371"
"Alwon S.","Alwon, Stephen (56669792600)","56669792600","Generative adversarial networks in seismic data processing","2019","2018 SEG International Exposition and Annual Meeting, SEG 2018","","","","1991","1995","4","10.1190/segam2018-2996002.1","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059371887&doi=10.1190%2fsegam2018-2996002.1&partnerID=40&md5=889c5f0cc81a5da2ac4f6a55f3b2efe6","Generative adversarial networks (GANs) are a class of machine learning techniques that involve two networks trained simultaneously to generate a desired outcome. These schemes have had success in many traditional image processing tasks, such as style transfer and super-resolution, but are relatively unexplored in geophysics. We outline the underlying theory behind GANs and present networks that can perform traditional seismic processing tasks such as noise attenuation and trace interpolation. © 2018 SEG.","Geophysical prospecting; Image processing; Learning systems; Seismology; Adversarial networks; Machine learning techniques; Noise attenuation; Seismic data processing; Seismic processing; Super resolution; Data handling","","Conference paper","Final","","Scopus","2-s2.0-85059371887"
"Song Y.-Z.; Liu W.-Y.; Chen J.-C.; Lin K.W.","Song, Yi-Zhen (57208343677); Liu, Wen-Yen (57208345047); Chen, Ju-Chin (57223719987); Lin, Kawuu W. (8890111700)","57208343677; 57208345047; 57223719987; 8890111700","Single Image Super-Resolution with Vision Loss Function","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11432 LNAI","","","173","179","6","10.1007/978-3-030-14802-7_15","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064531885&doi=10.1007%2f978-3-030-14802-7_15&partnerID=40&md5=9da18f986632eda9df8efc62f0ffcb3c","Super-resolution is the use of low-resolution images to reconstruct corresponding high-resolution images. This technology is used in many places such as medical fields and monitor systems. The traditional method is to interpolate to fill in the information lost when the image is enlarged. The initial use of deep learning is SRCNN, which is divided into three steps, extracting image block features, feature nonlinear mapping and reconstruction. Both PSNR and SSIM have significant progress compared with traditional methods, but there are still some details in detail restoration. defect. SRGAN will generate anti-network applications to SR problems. The method is to improve the image magnification by more than 4 times, which is easy to produce too smooth. In this study, we hope to improve the EnhanceNet by training with different loss functions and different types of images to achieve better reconstruction results. © 2019, Springer Nature Switzerland AG.","Database systems; Deep learning; Image reconstruction; Optical resolving power; Adversarial networks; High resolution image; Image block feature; Image magnification; Low resolution images; Network applications; Nonlinear mappings; Super resolution; Image enhancement","Deep learning; Generative adversarial network; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85064531885"
"","","","Advances in Neural Information Processing Systems","2018","Advances in Neural Information Processing Systems","2018-December","","","","","11075","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064824053&partnerID=40&md5=1c33a37d53ebf2970bc3d937dfac191e","The proceedings contain 1007 papers. The topics discussed include: efficient algorithms for non-convex isotonic regression through submodular optimization; structure-aware convolutional neural networks; kalman normalization: normalizing internal representations; text-adaptive generative adversarial networks: manipulating images with natural language; adapted deep embeddings: a synthesis of methods for k-shot inductive transfer learning; generalized inverse optimization through online learning; an off-policy policy gradient theorem using emphatic weightings; supervised autoencoders: improving generalization performance with unsupervised regularizers; understanding weight normalized deep neural networks with rectified linear units; joint sub-bands learning with clique structures for wavelet domain super-resolution; an efficient pruning algorithm for robust isotonic regression; see and think: disentangling semantic scene completion; and deep non-blind deconvolution via generalized low-rank approximation.","","","Conference review","Final","","Scopus","2-s2.0-85064824053"
"Chen Z.; Zhao T.; Cheng N.; Sun X.; Fu X.","Chen, Zhengyu (57204467883); Zhao, Tongtong (57192707963); Cheng, Na (57204472780); Sun, Xundong (57205505353); Fu, Xianping (7402204912)","57204467883; 57192707963; 57204472780; 57205505353; 7402204912","Towards underwater object recognition based on supervised learning","2018","2018 OCEANS - MTS/IEEE Kobe Techno-Oceans, OCEANS - Kobe 2018","","","8559050","","","","10.1109/OCEANSKOBE.2018.8559050","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060316809&doi=10.1109%2fOCEANSKOBE.2018.8559050&partnerID=40&md5=39548b2dcd76be996746512c0ccc631f","Underwater robots play a significant role in exploring the underwater world. In recent years, underwater robots still can’t recognize the underwater objects accurately. In order to find a solution to the problem of underwater robot recognition, we put forward a framework. There are three parts in our framework. First, a color correction algorithm is used to compensate color casts and produce natural color corrected images. Second, we employ Super-Resolution Generative Adversarial Network to enhance the underwater images. There are two parts in Super-Resolution Generative Adversarial Network. One is the modified generate network G, and the other is the discriminator network D. We modify the generate network G on basis of ResNet. Third, we employ object recognition algorithm to process the enhanced images for detecting and recognizing the underwater object. The experimental results show that the proposed framework can achieve good results in underwater object recognition. © 2018 IEEE","Color; Color image processing; Image enhancement; Oceanography; Optical resolving power; Robots; Underwater imaging; Adversarial networks; Color correction; Corrected image; Natural colors; Object recognition algorithm; Super resolution; Underwater objects; Underwater robots; Object recognition","Color correction; Generative Adversarial Network; Image enhancement; Object recognition; Underwater image","Conference paper","Final","","Scopus","2-s2.0-85060316809"
"Guei A.-C.; Akhloufi M.A.","Guei, Axel-Christian (57202650067); Akhloufi, Moulay A. (23979609400)","57202650067; 23979609400","Deep generative adversarial networks for infrared image enhancement","2018","Proceedings of SPIE - The International Society for Optical Engineering","10661","","106610B","","","","10.1117/12.2304875","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050799519&doi=10.1117%2f12.2304875&partnerID=40&md5=0e005d1a31559ad2a6e474a473ae9534","Extracting face images at a distance, in the crowd, or with a lower resolution infrared camera leads to a poorquality face image that is barely distinguishable. In this work, we present a Deep Convolutional Generative Adversarial Networks (DCGAN) for infrared face image enhancement. The proposed algorithm is used to build a super-resolution face image from its lower resolution counterpart. The resulting images are evaluated in term of qualitative and quantitative metrics on infrared face datasets (NIR and LWIR). The proposed algorithm performs well and preserves important details of the face. The analysis of the resulting images show that the proposed framework is promising and can help improve the performance of image super-resolution generation and enhancement in the infrared spectrum. © 2018 SPIE.","Infrared devices; Infrared imaging; Infrared radiation; Optical resolving power; Thermography (imaging); Adversarial networks; Image super resolutions; Infra-red cameras; Infrared face images; Infrared spectrum; Lower resolution; Quantitative metrics; Super resolution; Image enhancement","Deep generative adversarial networks; Image enhancement; Infrared faces; Infrared imaging; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85050799519"
"Turhan C.G.; Bilge H.S.","Turhan, Ceren Guzel (56246662400); Bilge, Hasan Sakir (23134504900)","56246662400; 23134504900","Single image super resolution using deep convolutional generative neural networks; [Derin konvolüsyonel generatif aǧlar ile süper çözünürlük]","2018","26th IEEE Signal Processing and Communications Applications Conference, SIU 2018","","","","1","4","3","10.1109/SIU.2018.8404829","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050802741&doi=10.1109%2fSIU.2018.8404829&partnerID=40&md5=fc7c035fbf1245ac9878c00ee9913864","Nowadays, deep convolutional networks have been focused on single image super-resolution problem due to their impressive performance on generating high-resolution images like as other computer vision tasks. It is clearly seen that among best known super-resolution models deep learning-based methods give the-state-of-the-art results. In this study, FSRGAN, based on a popular deep convolutional network (FSRCNN) due to its efficiency in spite of its simple architecture, is presented with generative adversarial training approach combining a discriminative network to the generator. The performance of the presented model is demonstrated by comparing to its baseline model, which is used as a generative network of our FSRGAN, the interpolation methods on well-known data sets based on PSNR metric. © 2018 IEEE.","Convolution; Deep neural networks; Neural networks; Optical resolving power; Adversarial networks; Convolutional networks; Convolutional neural network; Discriminative networks; High resolution image; Learning-based methods; Single images; Super-resolution models; Signal processing","Convolutional neural networks; Discriminative networks; Generative adversarial networks; Single image super resolution","Conference paper","Final","","Scopus","2-s2.0-85050802741"
"Galbusera F.; Bassani T.; Casaroli G.; Gitto S.; Zanchetta E.; Costa F.; Sconfienza L.M.","Galbusera, Fabio (15047667100); Bassani, Tito (23388372700); Casaroli, Gloria (57189334141); Gitto, Salvatore (57188980176); Zanchetta, Edoardo (57191269722); Costa, Francesco (26436305900); Sconfienza, Luca Maria (24448438200)","15047667100; 23388372700; 57189334141; 57188980176; 57191269722; 26436305900; 24448438200","Generative models: an upcoming innovation in musculoskeletal radiology? A preliminary test in spine imaging","2018","European Radiology Experimental","2","1","29","","","","10.1186/s41747-018-0060-7","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066820541&doi=10.1186%2fs41747-018-0060-7&partnerID=40&md5=e759319af7844ef11296632ee3a3fa65","Background: Deep learning is a ground-breaking technology that is revolutionising many research and industrial fields. Generative models are recently gaining interest. Here, we investigate their potential, namely conditional generative adversarial networks, in the field of magnetic resonance imaging (MRI) of the spine, by performing clinically relevant benchmark cases. Methods: First, the enhancement of the resolution of T2-weighted (T2W) images (super-resolution) was tested. Then, automated image-to-image translation was tested in the following tasks: (1) from T1-weighted to T2W images of the lumbar spine and (2) vice versa; (3) from T2W to short time inversion-recovery (STIR) images; (4) from T2W to turbo inversion recovery magnitude (TIRM) images; (5) from sagittal standing x-ray projections to T2W images. Clinical and quantitative assessments of the outputs by means of image quality metrics were performed. The training of the models was performed on MRI and x-ray images from 989 patients. Results: The performance of the models was generally positive and promising, but with several limitations. The number of disc protrusions or herniations showed good concordance (κ = 0.691) between native and super-resolution images. Moderate-to-excellent concordance was found when translating T2W to STIR and TIRM images (κ ≥ 0.842 regarding disc degeneration), while the agreement was poor when translating x-ray to T2W images. Conclusions: Conditional generative adversarial networks are able to generate perceptually convincing synthetic images of the spine in super-resolution and image-to-image translation tasks. Taking into account the limitations of the study, deep learning-based generative methods showed the potential to be an upcoming innovation in musculoskeletal radiology. © 2018, The Author(s).","","Lumbar vertebrae; Machine learning (deep learning); Magnetic resonance imaging; Neural network (computer); X-rays","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85066820541"
"","","","19th Pacific-Rim Conference on Multimedia, PCM 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11166 LNCS","","","","","2541","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054544658&partnerID=40&md5=78422fb46096c60049b42c102cf40f2e","The proceedings contain 229 papers. The special focus in this conference is on Multimedia. The topics include: Video clip growth: A general algorithm for multi-view video summarization; cross-media retrieval via deep semantic canonical correlation analysis and logistic regression; 3D global trajectory and multi-view local motion combined player action recognition in volleyball analysis; underwater image enhancement by the combination of dehazing and color correction; a novel no-reference QoE assessment model for frame freezing of mobile video; saliency detection based on deep learning and graph cut; rethinking fusion baselines for multi-modal human action recognition; A DCT-JND profile for disorderly concealment effect; breast ultrasound image classification and segmentation using convolutional neural networks; mixup-based acoustic scene classification using multi-channel convolutional neural network; intra-image region context for image captioning; viewpoint quality evaluation for augmented virtual environment; A flower classification framework based on ensemble of CNNs; Image translation between high-resolution remote sensing optical and SAR data using conditional GAN; A combined strategy of hand tracking for desktop VR; super-resolution of text image based on conditional generative adversarial network; latitude-based visual attention in 360-degree video display; branched convolutional neural networks for face alignment; a robust approach for scene text detection and tracking in video; improving intra block copy with low-rank based rectification for urban building scenes; multimodal fusion for traditional chinese painting generation; assembly-based 3D modeling using graph convolutional neural networks; blur measurement for partially blurred images with saliency constrained global refinement; SCAN: Spatial and channel attention network for vehicle re-identification; Cross-modal retrieval with discriminative dual-path CNN.","","","Conference review","Final","","Scopus","2-s2.0-85054544658"
"Lee Y.; Jun J.; Hong Y.; Jeon M.","Lee, Younkwan (57201723975); Jun, Jiwon (57209643351); Hong, Yoojin (57201728427); Jeon, Moongu (23987776700)","57201723975; 57209643351; 57201728427; 23987776700","Practical license plate recognition in unconstrained surveillance systems with adversarial super-resolution","2019","VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications","5","","","68","76","8","10.5220/0007378300680076","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068333555&doi=10.5220%2f0007378300680076&partnerID=40&md5=4e6a053cf3bb7186d77373a534fc00a7","Although most current license plate (LP) recognition applications have been significantly advanced, they are still limited to ideal environments where training data are carefully annotated with constrained scenes. In this paper, we propose a novel license plate recognition method to handle unconstrained real world traffic scenes. To overcome these difficulties, we use adversarial super-resolution (SR), and one-stage character segmentation and recognition. Combined with a deep convolutional network based on VGG-net, our method provides simple but reasonable training procedure. Moreover, we introduce GIST-LP, a challenging LP dataset where image samples are effectively collected from unconstrained surveillance scenes. Experimental results on AOLP and GIST-LP dataset illustrate that our method, without any scene-specific adaptation, outperforms current LP recognition approaches in accuracy and provides visual enhancement in our SR results that are easier to understand than original data. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.","Computer graphics; Computer vision; Intelligent systems; License plates (automobile); Monitoring; Optical resolving power; Adversarial networks; Intelligent transportation systems; License plate recognition; Super resolution; Visual surveillance; Optical character recognition","Generative Adversarial Networks; Intelligent Transportation Systems; License Plate Recognition; Super-Resolution; Visual Surveillance","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85068333555"
"Chen Y.; Tai Y.; Liu X.; Shen C.; Yang J.","Chen, Yu (56046881400); Tai, Ying (56437886200); Liu, Xiaoming (35793096800); Shen, Chunhua (7402859952); Yang, Jian (56999514900)","56046881400; 56437886200; 35793096800; 7402859952; 56999514900","FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578362","2492","2501","9","10.1109/CVPR.2018.00264","288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055695688&doi=10.1109%2fCVPR.2018.00264&partnerID=40&md5=c3db836189aeba21cd5ffe832fb46722","Face Super-Resolution (SR) is a domain-specific superresolution problem. The facial prior knowledge can be leveraged to better super-resolve face images. We present a novel deep end-to-end trainable Face Super-Resolution Network (FSRNet), which makes use of the geometry prior, i.e., facial landmark heatmaps and parsing maps, to super-resolve very low-resolution (LR) face images without well-aligned requirement. Specifically, we first construct a coarse SR network to recover a coarse high-resolution (HR) image. Then, the coarse HR image is sent to two branches: a fine SR encoder and a prior information estimation network, which extracts the image features, and estimates landmark heatmaps/parsing maps respectively. Both image features and prior information are sent to a fine SR decoder to recover the HR image. To generate realistic faces, we also propose the Face Super-Resolution Generative Adversarial Network (FSRGAN) to incorporate the adversarial loss into FSRNet. Further, we introduce two related tasks, face alignment and parsing, as the new evaluation metrics for face SR, which address the inconsistency of classic metrics w.r.t. visual perception. Extensive experiments show that FSRNet and FSRGAN significantly outperforms state of the arts for very LR face SR, both quantitatively and qualitatively. © 2018 IEEE.","Computer vision; Knowledge management; Adversarial networks; Evaluation metrics; Face super-resolution; High resolution image; Prior information; State of the art; Super resolution; Visual perception; Optical resolving power","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85055695688"
"Chen Y.; Shi F.; Christodoulou A.G.; Xie Y.; Zhou Z.; Li D.","Chen, Yuhua (57190346094); Shi, Feng (56347687400); Christodoulou, Anthony G. (36813085800); Xie, Yibin (56019417700); Zhou, Zhengwei (57215806908); Li, Debiao (35262251100)","57190346094; 56347687400; 36813085800; 56019417700; 57215806908; 35262251100","Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multi-level densely connected network","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11070 LNCS","","","91","99","8","10.1007/978-3-030-00928-1_11","151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054058069&doi=10.1007%2f978-3-030-00928-1_11&partnerID=40&md5=7e5dbc4e13e3fd3c273e77e3e0f7f803","High-resolution (HR) magnetic resonance images (MRI) provide detailed anatomical information important for clinical application and quantitative image analysis. However, HR MRI conventionally comes at the cost of longer scan time, smaller spatial coverage, and lower signal-to-noise ratio (SNR). Recent studies have shown that single image super-resolution (SISR), a technique to recover HR details from one single low-resolution (LR) input image, could provide high quality image details with the help of advanced deep convolutional neural networks (CNN). However, deep neural networks consume memory heavily and run slowly, especially in 3D settings. In this paper, we propose a novel 3D neural network design, namely a multi-level densely connected super-resolution network (mDCSRN) with generative adversarial network (GAN)–guided training. The mDCSRN trains and inferences quickly, and the GAN promotes realistic output hardly distinguishable from original HR images. Our results from experiments on a dataset with 1,113 subjects shows that our new architecture outperforms other popular deep learning methods in recovering 4x resolution-downgraded images and runs 6x faster. © Springer Nature Switzerland AG 2018.","Deep learning; Deep neural networks; Image enhancement; Magnetic resonance; Medical computing; Medical imaging; Neural networks; Optical resolving power; Signal to noise ratio; Adversarial networks; Anatomical information; Deep convolutional neural networks; Densely connected networks; Magnetic resonance images (MRI); Neural network designs; Quantitative image analysis; Super resolution; Magnetic resonance imaging","Deep learning; Image enhancement; MRI; Super-Resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85054058069"
"","","","6th National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics, NCVPRIPG 2017","2018","Communications in Computer and Information Science","841","","","","","559","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046432044&partnerID=40&md5=abe17c7470c6015bb4c975b5d56c10c1","The proceedings contain 48 papers. The special focus in this conference is on Computer Vision, Pattern Recognition, Image Processing and Graphics. The topics include: Parametric reshaping of humans in videos incorporating motion retargeting; enhanced aggregated channel features detector for pedestrian detection using parameter optimisation and deep features; unsupervised segmentation of speech signals using Kernel-Gram matrices; Design of biorthogonal wavelet filters of DTCWT using factorization of halfband polynomials; single noisy image super resolution by minimizing nuclear norm in virtual sparse domain; near real-time correction of specular reflections in flash images using no-flash image prior; A method for detecting JPEG anti-forensics; an end-to-end deep learning framework for super-resolution based inpainting; saliency map improvement using edge-aware filtering; classification of human actions using 3-D convolutional neural networks: A hierarchical approach; A generative adversarial network for tone mapping HDR images; efficient clustering-based noise covariance estimation for maximum noise fraction; GMM based single depth image super-resolution; patch similarity in transform domain for intensity/range image denoising with edge preservation; multi-modal image analysis for plant stress phenotyping; source classification using document images from smartphones and flatbed scanners; Homomorphic incremental directional averaging for noise suppression in SAR images; An EEG-based image annotation system; multimodal registration of retinal images; Dynamic class learning approach for smart CBIR; SmartTennisTV: Automatic indexing of tennis videos; exploring memory and time efficient neural networks for image captioning; dataset augmentation with synthetic images improves semantic segmentation; deep neural network for foreground object segmentation: An unsupervised approach; document image segmentation using deep features; flow-free video object segmentation.","","","Conference review","Final","","Scopus","2-s2.0-85046432044"
"","","","31st Australasian Joint Conference on Artificial Intelligence, AI 2018","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11320 LNAI","","","","","854","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059010660&partnerID=40&md5=40d8a308c1839a3598e72285543895bb","The proceedings contain 76 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Uncovering discriminative knowledge-guided medical concepts for classifying coronary artery disease notes; a multi-tree genetic programming representation for melanoma detection using local and global features; impacts of climate change and socioeconomic development on electric load in California; Implementing propositional networks on FPGA; a genetic programming hyper-heuristic approach for online resource allocation in container-based clouds; a computational framework for autonomous self-repair systems; investigation of unsupervised models for biodiversity assessment; field-regularised factorization machines for mining the maintenance logs of equipment; real-time collusive shill bidding detection in online auctions; hierarchical population-based learning for optimal large-scale coalition structure generation in smart grids; OHC: Uncovering overlapping heterogeneous communities; a genetic programming approach for constructing foreground and background saliency features for salient object detection; Distinction between ships and icebergs in SAR images using ensemble loss trained convolutional neural networks; Shark detection from aerial imagery using region-based CNN, a study; a hybrid differential evolution approach to designing deep convolutional neural networks for image classification; a gaussian filter-based feature learning approach using genetic programming to image classification; image up-sampling for super resolution with generative adversarial network; a model for learning representations of 3D objects through tactile exploration: Effects of object asymmetries and landmarks; cyclist trajectory prediction using bidirectional recurrent neural networks; diversified late acceptance search; empowerment-driven single agent exploration for locating multiple wireless transmitters; hyper-heuristic based local search for combinatorial optimisation problems; constraint-guided local search for single mixed-operation runway.","","","Conference review","Final","","Scopus","2-s2.0-85059010660"
"Li F.; He X.; Wei Z.; He J.; He D.","Li, Fangbiao (57194278744); He, Xin (56177901200); Wei, Zhonghui (14620016400); He, Jiawei (55622400800); He, Dinglong (56482629300)","57194278744; 56177901200; 14620016400; 55622400800; 56482629300","Multiframe infrared image super-resolution reconstruction using generative adversarial networks","2018","Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering","47","2","0203003","","","","10.3788/IRLA201847.0203003","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046955316&doi=10.3788%2fIRLA201847.0203003&partnerID=40&md5=1758008e0d32f5272decc863e5f00855","Generative adversarial networks had shown promising potential in conditional image generation. It seemed that the GANs were particularly suitable for use in image super-resolution reconstruction. However, there was a shortcoming of excessive smoothness and lack of high frequency detail information for the reconstructed SR images by using GANs. Aiming at resolving the problem that the method of single image super-resolution reconstruction ignored the spatio-temporal relationship between image frames, a method of multiframe infrared image super-resolution reconstruction based on generative adversarial networks (M-GANs) was proposed in this paper. Firstly, motion compensation was proposed for registration low resolution image frames; Secondly, a weight representation convolutional layer was performed to calculate the weight transfer; Finally, the generative adversarial network was used to reconstruct the high resolution image. Experimental results demonstrate that the proposed method surpass current state-of-the-art performance of both subjective and objective evaluation. © 2018, Editorial Board of Journal of Infrared and Laser Engineering. All right reserved.","Deep learning; Image resolution; Infrared imaging; Motion compensation; Optical resolving power; Thermography (imaging); Adversarial networks; High resolution image; Image super-resolution reconstruction; Single-image super-resolution reconstruction; Spatio-temporal relationships; State-of-the-art performance; Subjective and objective evaluations; Super resolution reconstruction; Image reconstruction","Deep learning; Generative adversarial networks; Infrared imaging; Super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85046955316"
"Shi X.; Zhou F.; Yang S.; Zhang Z.; Su T.","Shi, Xiaoran (56502661900); Zhou, Feng (56421055500); Yang, Shuang (57221158149); Zhang, Zijing (55624622900); Su, Tao (57210523273)","56502661900; 56421055500; 57221158149; 55624622900; 57210523273","Automatic target recognition for synthetic aperture radar images based on super-resolution generative adversarial network and deep convolutional neural network","2019","Remote Sensing","11","2","135","","","","10.3390/rs11020135","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060727566&doi=10.3390%2frs11020135&partnerID=40&md5=e8ce4ef98870e5df209201de961715a6","Aiming at the problem of the difficulty of high-resolution synthetic aperture radar (SAR) image acquisition and poor feature characterization ability of low-resolution SAR image, this paper proposes a method of an automatic target recognition method for SAR images based on a super-resolution generative adversarial network (SRGAN) and deep convolutional neural network (DCNN). First, the threshold segmentation is utilized to eliminate the SAR image background clutter and speckle noise and accurately extract target area of interest. Second, the low-resolution SAR image is enhanced through SRGAN to improve the visual resolution and the feature characterization ability of target in the SAR image. Third, the automatic classification and recognition for SAR image is realized by using DCNN with good generalization performance. Finally, the open data set, moving and stationary target acquisition and recognition, is utilized and good recognition results are obtained under standard operating condition and extended operating conditions, which verify the effectiveness, robustness, and good generalization performance of the proposed method. © 2019 by the authors.","Automatic target recognition; Convolution; Deep neural networks; Image enhancement; Image segmentation; Neural networks; Open Data; Radar target recognition; Spectral resolution; Synthetic aperture radar; Adversarial networks; Automatic classification; Convolutional neural network; Extended operating conditions; Feature characterization; Generalization performance; High resolution synthetic aperture radar images; Standard operating conditions; Radar imaging","Automatic target recognition (ATR); Deep convolutional neural network (DCNN); Image segmentation; Super-resolution generative adversarial network (SRGAN); Synthetic aperture radar (SAR)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85060727566"
"","","","Workshop on Bildverarbeitung fur die Medizin, 2019","2019","Informatik aktuell","","","","","","350","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065098977&partnerID=40&md5=f9489525576c809944ba01a28d2bebb2","The proceedings contain 75 papers. The special focus in this conference is on Bildverarbeitung fur die Medizin. The topics include: Segmentation of Vertebral Metastases in MRI Using an U-Net like Convolutional Neural Network; synthetic Training with Generative Adversarial Networks for Segmentation of Microscopies; Gradient-Based Expanding Spherical Appearance Models for Femoral Model Initialization in MRI; Deep segmentation refinement with result-dependent learning: A double U-net for hip joint segmentation in MRI; automatic Estimation of Cochlear Duct Length and Volume Size; interactive Neural Network Robot User Investigation for Medical Image Segmentation; Tracing of Nerve Fibers Through Brain Regions of Fiber Crossings in Reconstructed 3D-PLI Volumes; Dilated deeply supervised networks for hippocampus segmentation in MRI; Automatic Detection and Segmentation of the Acute Vessel Thrombus in Cerebral CT; automatic Detection of Blood Vessels in Optical Coherence Tomography Scans; Sparsely Connected Convolutional Layers in CNNs for Liver Segmentation in CT; smooth Ride: Low-Pass Filtering of Manual Segmentations Improves Consensus; user Loss A Forced-Choice-Inspired Approach to Train Neural Networks Directly by User Interaction; sodium Image Denoising Based on a Convolutional Denoising Autoencoder; improved X-Ray Bone Segmentation by Normalization and Augmentation Strategies; multi-Modal Super-Resolution with Deep Guided Filtering; semi-Automatic Cell Correspondence Analysis Using Iterative Point Cloud Registration; pediatric Patient Surface Model Atlas Generation and X-Ray Skin Dose Estimation; Blind Rigid Motion Estimation for Arbitrary MRI Sampling Trajectories; maximum Likelihood Estimation of Head Motion Using Epipolar Consistency; Prediction of Liver Function Based on DCE-CT; Retrospective Blind MR Image Recovery with Parametrized Motion Models; phase-Sensitive Region-of-Interest Computed Tomography.","","","Conference review","Final","","Scopus","2-s2.0-85065098977"
"Adate A.; Tripathy B.K.","Adate, Amit (57200514157); Tripathy, B.K. (7006285374)","57200514157; 7006285374","Understanding single image super-resolution techniques with generative adversarial networks","2019","Advances in Intelligent Systems and Computing","816","","","833","840","7","10.1007/978-981-13-1592-3_66","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058997582&doi=10.1007%2f978-981-13-1592-3_66&partnerID=40&md5=9b0a57ea9a3c7b5400255cd3edaa1d68","Single Image Super-Resolution techniques have the function of retrieving a high resolution image from a single low resolution input. They implement deep learning heuristics which perform the techniques to form pixel-accourate reproductions. In this paper we have experimented upon various neural architectures with unique approaches towards the task of super-resolution. We have especially elaborated upon adversarial training networks which are yielding progressive results in both conditional and quantifiable benchmarks. © Springer Nature Singapore Pte Ltd. 2019.","Deep learning; Soft computing; Adversarial networks; Handwritten digit; High resolution image; Low resolution; Neural architectures; Single images; Super resolution; Training network; Optical resolving power","Adversarial training; Generative adversarial networks; Handwritten digits; Single image super-resolution","Conference paper","Final","","Scopus","2-s2.0-85058997582"
"Guei A.-C.; Akhloufi M.","Guei, Axel-Christian (57202650067); Akhloufi, Moulay (23979609400)","57202650067; 23979609400","Deep learning enhancement of infrared face images using generative adversarial networks","2018","Applied Optics","57","18","","D98","D107","9","10.1364/AO.57.000D98","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048976871&doi=10.1364%2fAO.57.000D98&partnerID=40&md5=7faaff084361988d856b545aec675939","This work presents a deep learning framework based on the use of deep convolutional generative adversarial networks (DCGAN) for infrared face image super-resolution. We use DCGAN for upscaling the images by a factor of 4 × 4, starting at a size of 16 × 16 and obtaining a 64 × 64 face image. Tests are conducted using different infrared face datasets operating in the near-infrared (NIR) and the long-wave infrared (LWIR) spectrum. We can see that the proposed framework performs well and preserves important details of the face. This kind of approach can be very useful in security applications where we can scan faces in the crowd or detect faces at a distance and upscale them for further recognition through an infrared or a multispectral face recognition system. © 2018 Optical Society of America.","Algorithms; Databases as Topic; Face; Female; Humans; Image Processing, Computer-Assisted; Infrared Rays; Machine Learning; Male; Neural Networks (Computer); Signal-To-Noise Ratio; Face recognition; Image enhancement; Infrared devices; Infrared radiation; Adversarial networks; Face recognition systems; Infrared face images; Learning enhancements; Learning frameworks; Longwave infrared; Multi-spectral; Security application; algorithm; anatomy and histology; artificial neural network; data base; face; female; human; image processing; infrared radiation; machine learning; male; signal noise ratio; Deep learning","","Article","Final","","Scopus","2-s2.0-85048976871"
"","","","16th International Symposium on Neural Networks, ISNN 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11554 LNCS","","","","","1087","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068614605&partnerID=40&md5=d074fedaa0cd2c40f315b81c7b97180d","The proceedings contain 110 papers. The special focus in this conference is on Neural Networks. The topics include: Graph-FCN for Image Semantic Segmentation; architecture Search for Image Inpainting; an Improved Capsule Network Based on Newly Reconstructed Network and the Method of Sharing Parameters; bidirectional Gated Recurrent Unit Networks for Relation Classification with Multiple Attentions and Semantic Information; functional and Structural Features of Recurrent Neural Networks with Controlled Elements; A GAN-Based Data Augmentation Method for Multimodal Emotion Recognition; FG-SRGAN: A Feature-Guided Super-Resolution Generative Adversarial Network for Unpaired Image Super-Resolution; artificial Neural Networks for Realized Volatility Prediction in Cryptocurrency Time Series; pattern Matching in Sequential Data Using Reservoir Projections; Bus Arrival Time Prediction with LSTM Neural Network; multi Step Prediction of Landslide Displacement Time Series Based on Extended Kalman Filter and Back Propagation Trough Time; noise Filtering in Cellular Neural Networks; learning Agents with Prioritization and Parameter Noise in Continuous State and Action Space; demand Forecasting Techniques for Build-to-Order Lean Manufacturing Supply Chains; imputation Using a Correlation-Enhanced Auto-Associative Neural Network with Dynamic Processing of Missing Values; broad Learning for Optimal Short-Term Traffic Flow Prediction; A Novel QGA-UKF Algorithm for Dynamic State Estimation of Power System; an Improved Result on H∞ Performance State Estimation of Delayed Static Neural Networks; uncertainty Estimation via Stochastic Batch Normalization; a Hybrid Neurodynamic Algorithm to Multi-objective Operation Management in Microgrid; from Differential Equations to Multilayer Neural Network Models; simulation of a Chaos-Like Irregular Neural Firing Pattern Based on Improved Deterministic Chay Model; maxEntropy Pursuit Variational Inference.","","","Conference review","Final","","Scopus","2-s2.0-85068614605"
"Cheng N.; Zhao T.; Chen Z.; Fu X.","Cheng, Na (57204472780); Zhao, Tongtong (57192707963); Chen, Zhengyu (57204467883); Fu, Xianping (7402204912)","57204472780; 57192707963; 57204467883; 7402204912","Enhancement of underwater images by super-resolution generative adversarial networks","2018","ACM International Conference Proceeding Series","","","3240881","","","","10.1145/3240876.3240881","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055701907&doi=10.1145%2f3240876.3240881&partnerID=40&md5=25fb93c791f03799ed0f56b3253392ba","Underwater image enhancement plays an important role in oceanic engineering, while the research is far from enough. The problems like color casts, low contrast brought out by the properties of water and its impurities, make it a challenging task. This paper proposes a novel framework for enhancing underwater image. It includes two parts, that is , pre-processing and de-blurring with improved Super-resolution Generative Adversarial Networks. First, in the process of pre-processing, we use the color correction approach and the contrast enhancement method to compensate color casts and produce natural color images. Second, an improved Super-resolution Generative Adversarial Networks is applied to pre-processed images in order to remove blurring and boost detail. Based on the network, the loss net is modified, so that the pre-processed images will be de-blurred and sharpened. The experimental results show that the proposed strategy improves the quality of underwater images efficiently. © 2018 ACM.","Color; Color image processing; Optical resolving power; Underwater imaging; Adversarial networks; Color correction; Contrast Enhancement; Natural color images; Oceanic engineering; Pre-processing; Processed images; Super resolution; Image enhancement","Color correction; Contrast enhancement; Super-resolution Generative Adversarial Networks; Underwater image","Conference paper","Final","","Scopus","2-s2.0-85055701907"
"Tsunekawa S.; Inoue K.; Yoshioka M.","Tsunekawa, Shohei (57190061976); Inoue, Katsufumi (56723194900); Yoshioka, Michifumi (7402480508)","57190061976; 56723194900; 7402480508","Image up-sampling for super resolution with generative adversarial network","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11320 LNAI","","","258","270","12","10.1007/978-3-030-03991-2_26","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058989561&doi=10.1007%2f978-3-030-03991-2_26&partnerID=40&md5=70cb07c4e01477fa33f019cf6d25fd39","In case, we carry out single image Super Resolution (SR) utilizing deep learning, we utilize bicubic interpolation for up-sampling of low resolution images before input them into SR methods. In the preprocessing, these basic interpolation methods cause blur and noise effects for after processed images. These noise images may affect the SR results. In this research, by focusing on this point, we propose a new image up-sampling method utilizing Generative Adversarial Network (GAN). In this work, we improve an image evaluation criterion in generator part of GAN by combining Multi-Scale Structural Similarity (MS-SSIM) and L1 norm. From experiments, we have confirmed that our method allows us to create more qualitatively up-sampling images. As the quantitative results, our proposed method have achieved 0.90 [dB] of average PSNR, 3.35 [%] of average SSIM, and 1.28 [%] of average MS-SSIM improvement using Set 5 and Set 14 dataset compared with bicubic interpolation. © Springer Nature Switzerland AG 2018.","Artificial intelligence; Deep learning; Image enhancement; Interpolation; Optical resolving power; Signal receivers; Adversarial networks; Bicubic interpolation; Image up-sampling; Interpolation method; Low resolution images; Quantitative result; Structural similarity; Super resolution; Image sampling","GAN; Image up-sampling; Super resolution","Conference paper","Final","","Scopus","2-s2.0-85058989561"
"Li D.; Wang Z.","Li, Dingyi (57191923614); Wang, Zengfu (35276043300)","57191923614; 35276043300","Face video super-resolution with identity guided generative adversarial networks","2017","Communications in Computer and Information Science","772","","","357","369","12","10.1007/978-981-10-7302-1_30","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038021309&doi=10.1007%2f978-981-10-7302-1_30&partnerID=40&md5=8cdb083c3779f7afdd08964d1b290015","Faces are of particular concerns in video surveillance systems. It is challenging to reconstruct clear faces from low-resolution (LR) videos. In this paper, we propose a new method for face video super-resolution (SR) based on identity guided generative adversarial networks (GANs). We establish a two-stage convolutional neural network (CNN) for face video SR, and employ identity guided GANs to recover high-resolution (HR) facial details. Extensive experiments validate the effectiveness of our proposed method from the following aspects: fidelity, visual quality and robustness to pose, expression and illuminance variations. © Springer Nature Singapore Pte Ltd. 2017.","Computer vision; Neural networks; Optical resolving power; Adversarial networks; Convolutional neural network; Face hallucination; High resolution; Super resolution; Video super-resolution; Video surveillance systems; Visual qualities; Security systems","Face hallucination; Generative adversarial networks (GANs); Identity guidance; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85038021309"
"Mahapatra D.; Bozorgtabar B.; Garnavi R.","Mahapatra, Dwarikanath (25422481200); Bozorgtabar, Behzad (37109488100); Garnavi, Rahil (15845331200)","25422481200; 37109488100; 15845331200","Image super-resolution using progressive generative adversarial networks for medical image analysis","2019","Computerized Medical Imaging and Graphics","71","","","30","39","9","10.1016/j.compmedimag.2018.10.005","110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056921498&doi=10.1016%2fj.compmedimag.2018.10.005&partnerID=40&md5=a45f3e27af796a8f55e45dd17aff9079","Anatomical landmark segmentation and pathology localisation are important steps in automated analysis of medical images. They are particularly challenging when the anatomy or pathology is small, as in retinal images (e.g. vasculature branches or microaneurysm lesions) and cardiac MRI, or when the image is of low quality due to device acquisition parameters as in magnetic resonance (MR) scanners. We propose an image super-resolution method using progressive generative adversarial networks (P-GANs) that can take as input a low-resolution image and generate a high resolution image of desired scaling factor. The super resolved images can be used for more accurate detection of landmarks and pathologies. Our primary contribution is in proposing a multi-stage model where the output image quality of one stage is progressively improved in the next stage by using a triplet loss function. The triplet loss enables stepwise image quality improvement by using the output of the previous stage as the baseline. This facilitates generation of super resolved images of high scaling factor while maintaining good image quality. Experimental results for image super-resolution show that our proposed multi stage P-GAN outperforms competing methods and baseline GANs. The super resolved images when used for landmark and pathology detection result in accuracy levels close to those obtained when using the original high resolution images. We also demonstrate our methods effectiveness on magnetic resonance (MR) images, thus establishing its broader applicability © 2018 Elsevier Ltd","Algorithms; Anatomic Landmarks; Fundus Oculi; Heart Ventricles; Humans; Image Enhancement; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Retinal Vessels; Image analysis; Image quality; Image segmentation; Magnetic resonance; Magnetic resonance imaging; Medical imaging; Ophthalmology; Optical resolving power; Pathology; Acquisition parameters; Adversarial networks; Generative model; Image quality improvements; Image super resolutions; Low resolution images; Magnetic resonance scanners; Retinal fundus; article; eye fundus; image analysis; image quality; loss of function mutation; nuclear magnetic resonance imaging; algorithm; anatomic landmark; diagnostic imaging; eye fundus; heart ventricle; human; image enhancement; image processing; nuclear magnetic resonance imaging; procedures; retina blood vessel; Image enhancement","Adversarial networks; Image super-resolution; MRI; Pathology; Progressive generative models; Retinal fundus","Article","Final","","Scopus","2-s2.0-85056921498"
"Tan W.; Yan B.; Bare B.","Tan, Weimin (57188575769); Yan, Bo (36764113400); Bare, Bahetiyaer (56433101200)","57188575769; 36764113400; 56433101200","Feature Super-Resolution: Make Machine See More Clearly","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578518","3994","4002","8","10.1109/CVPR.2018.00420","38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062827369&doi=10.1109%2fCVPR.2018.00420&partnerID=40&md5=d0841266cb36badf1f13da8a1c4fad50","Identifying small size images or small objects is a notoriously challenging problem, as discriminative representations are difficult to learn from the limited information contained in them with poor-quality appearance and unclear object structure. Existing research works usually increase the resolution of low-resolution image in the pixel space in order to provide better visual quality for human viewing. However, the improved performance of such methods is usually limited or even trivial in the case of very small image size (we will show it in this paper explicitly). In this paper, different from image super-resolution (ISR), we propose a novel super-resolution technique called feature super-resolution (FSR), which aims at enhancing the discriminatory power of small size image in order to provide high recognition precision for machine. To achieve this goal, we propose a new Feature Super-Resolution Generative Adversarial Network (FSR-GAN) model that transforms the raw poor features of small size images to highly discriminative ones by performing super-resolution in the feature space. Our FSR-GAN consists of two subnetworks: a feature generator network G and a feature discriminator network D. By training the G and the D networks in an alternative manner, we encourage the G network to discover the latent distribution correlations between small size and large size images and then use G to improve the representations of small images. Extensive experiment results on Oxford5K, Paris, Holidays, and Flick100k datasets demonstrate that the proposed FSR approach can effectively enhance the discriminatory ability of features. Even when the resolution of query images is reduced greatly, e.g., 1/64 original size, the query feature enhanced by our FSR approach achieves surprisingly high retrieval performance at different image resolutions and increases the retrieval precision by 25% compared to the raw query feature. © 2018 IEEE.","Computer vision; Image resolution; Optical resolving power; Adversarial networks; Discriminatory power; Image super resolutions; Limited information; Low resolution images; Object structure; Retrieval performance; Visual qualities; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85062827369"
"","","","2nd Chinese Conference on Computer Vision, CCCV 2017","2017","Communications in Computer and Information Science","772","","","","","1377","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038019215&partnerID=40&md5=63cbdb532ddc908d7392462faa549b91","The proceedings contain 113 papers. The special focus in this conference is on Computer Vision. The topics include: Visual saliency fusion based multi-feature for semantic image retrieval; unsupervised multi-view subspace learning via maximizing dependence; structured multi-view supervised feature selection algorithm research; an automatic shoeprint retrieval method using neural codes for commercial shoeprint scanners; uncovering the effect of visual saliency on image retrieval; shape-color differential moment invariants under affine transforms; a novel layer based image fusion approach via transfer learning and coupled dictionary; local saliency extraction for fusion of visible and infrared images; distributed compressive sensing for light field reconstruction using structured random matrix; improved face verification with simple weighted feature combination; stereoscopic image quality assessment based on binocular adding and subtracting; quality assessment of palm vein image using natural scene statistics; an error-activation-guided blind metric for stitched panoramic image quality assessment; image aesthetic quality evaluation using convolution neural network embedded fine-tune; high capacity reversible data hiding with contrast enhancement; rank learning for dehazed image quality assessment; a low-rank total-variation regularized tensor completion algorithm; nighttime haze removal with fusion atmospheric light and improved entropy; light field super-resolution using cross-resolution input based on patchmatch and learning method; a new image sparse reconstruction method for mixed Gaussian-poisson noise with multiple constraints; pore-scale facial features matching under 3D morphable model constraint; face video super-resolution with identity guided generative adversarial networks; exemplar-based pixel by pixel inpainting based on patch shift; GAN based sample simulation for SEM-image super resolution; PSO-based single image defogging.","","","Conference review","Final","","Scopus","2-s2.0-85038019215"
"Qu X.; Wang X.; Wang Z.; Wang L.; Zhang L.","Qu, Xuexin (57204646691); Wang, Xin (56623215300); Wang, Zihan (57209886882); Wang, Lei (57070597700); Zhang, Lingchen (54390558200)","57204646691; 56623215300; 57209886882; 57070597700; 54390558200","Perceptual-DualGAN: Perceptual Losses for Image to Image Translation with Generative Adversarial Nets","2018","Proceedings of the International Joint Conference on Neural Networks","2018-July","","8489108","","","","10.1109/IJCNN.2018.8489108","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056549139&doi=10.1109%2fIJCNN.2018.8489108&partnerID=40&md5=83ac796488d528f36f38ebd249f3a0ff","Thinkingabout cross-domain image-to-image translation problems, where an input image belonging to domain U is transformed into an output image belonging to another domain V. A series of typical tasks, such as style transformation, colorization, super-resolution, can be seen as cross-domain image-to-image translation tasks. Recent methods such as Conditional Generative Adversarial Networks (cGANs) make big progress in this field, but they require paired image data, which is hard to obtain. The DualGAN (Unsupervised Dual Learning for Image-to-Image Translation) architecture was proposed to solve the issue of lack of paired data. But the pixel-level reconstruction losses of DualGAN are simple. In this paper, we replace the pixel-level reconstruction losses with the perceptual reconstruction losses, and propose a more advanced framework for cross-domain image-to-image translation named perceptual-DualGAN. The perceptual reconstruction losses consist of feature reconstruction losses and style reconstruction losses, both of them are computed from pretrained loss networks. Experiments on multiple image translation tasks show that our framework almost performs superior to other methods. And the results of experiments illustrate that our framework can generate more realistic and more natural photos. © 2018 IEEE.","Pixels; Adversarial networks; Cross-domain; Feature reconstruction; Generative model; Image translation; Loss networks; Multiple image; Super resolution; Image processing","cross-domain image-to-image translation; GAN; generative model; perceptual losses","Conference paper","Final","","Scopus","2-s2.0-85056549139"
"Ma W.; Pan Z.; Guo J.; Lei B.","Ma, Wen (57207877267); Pan, Zongxu (54788169800); Guo, Jiayi (57194143247); Lei, Bin (14063767500)","57207877267; 54788169800; 57194143247; 14063767500","Super-resolution of remote sensing images based on transferred generative adversarial network","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8517442","1148","1151","3","10.1109/IGARSS.2018.8517442","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063138028&doi=10.1109%2fIGARSS.2018.8517442&partnerID=40&md5=39673883be5d097f6a3bd4bf2fe8f3fc","Single image super-resolution (SR) has been widely studied in recent years as a crucial technique for remote sensing applications. This paper proposes a SR method for remote sensing images based on a transferred generative adversarial network (TGAN). Different from the previous GAN-based SR approaches, the novelty of our method mainly reflects from two aspects. First, the batch normalization layers are removed to reduce the memory consumption and the computational burden, as well as raising the accuracy. Second, our model is trained in a transfer-learning fashion to cope with the insufficiency of training data, which is the crux of applying deep learning methods to remote sensing applications. The model is firstly trained on an external dataset DIV2K and further fine-tuned with the remote sensing dataset. Our experimental results demonstrate that the proposed method is superior to SRCNN and SRGAN in terms of both the objective evaluation and the subjective perspective. © 2018 IEEE","","Generative adversarial network; Remote sensing images; Super-resolution; Transfer learning","Conference paper","Final","","Scopus","2-s2.0-85063138028"
"Turhan C.G.; Bilge H.S.","Turhan, Caren Guzel (56246662400); Bilge, Hasan Sakir (23134504900)","56246662400; 23134504900","Variational Autoencoded Compositional Pattern Generative Adversarial Network for Handwritten Super Resolution Image Generation","2018","UBMK 2018 - 3rd International Conference on Computer Science and Engineering","","","8566539","564","568","4","10.1109/UBMK.2018.8566539","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060602882&doi=10.1109%2fUBMK.2018.8566539&partnerID=40&md5=19ce2415ac3f8a1c2c8fce3bc51d7dab","Since generative adversarial training has been decleared as one of the most exciting topics of the last 10 years by the pioneers, many researchers have focused on the Generative Adversarial Network (GAN) in their studies. On the otherhand, Variational Autoencoders (VAE) had gain autoencoders' popularity back. Due to some restrictions of GAN models and their lack of inference mechanism, hybrid models of GAN and VAE have emerged for image generation problem in nowadays. With the influence of these views and improvements, we have focused on addressing not only generating synthetic handwritten images but also their high-resolution version. For these tasks, Compositional Pattern Producing Networks (CPPN), VAE and GAN models are combined inspired by an existing model with some modification of its objective function. With this model, the idea behind the inspired study for generating high-resolution images are combined with the feature-wise reconstruction objective of a VAE/GAN hybrid model instead of pixel-like reconstruction approach of traditional VAE. For evaluating the model efficiency, our VAE/CPGAN model is compared with its basis models (GAN, VAE and VAE/GAN) and inspired model accoording to inception score. In this study, it is clearly seen that the proposed model is able to converge much faster than compared models for modeling the underlying distribution of handwritten image data. © 2018 IEEE.","Learning systems; Autoencoders; Generative model; Handwritten images; High resolution image; Image generations; Image enhancement","adversarial training; generative models; high-resolution images; image generation; synthetic handwritten images; variational autoencoders","Conference paper","Final","","Scopus","2-s2.0-85060602882"
"Li Y.; Yang Z.; Mao X.; Wang Y.; Li Q.; Liu W.; Wang Y.","Li, Yong (57211570964); Yang, Zhenguo (56828347100); Mao, Xudong (54883408900); Wang, Yong (56084254400); Li, Qing (56190105400); Liu, Wenyin (22433292200); Wang, Ying (56084250600)","57211570964; 56828347100; 54883408900; 56084254400; 56190105400; 22433292200; 56084250600","GAN with Pixel and Perceptual Regularizations for Photo-Realistic Joint Deblurring and Super-Resolution","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11542 LNCS","","","395","401","6","10.1007/978-3-030-22514-8_36","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067658475&doi=10.1007%2f978-3-030-22514-8_36&partnerID=40&md5=9b50b3107257bc09922756f750cc66b1","In this paper, we propose a Generative Adversarial Network with Pixel and Perceptual regularizations, denoted as P2GAN, to restore single motion blurry and low-resolution images jointly into clear and high-resolution images. It is an end-to-end neural network consisting of deblurring module and super-resolution module, which repairs degraded pixels in the motion-blur images firstly, and then outputs the deblurred images and deblurred features for further reconstruction. More specifically, the proposed P2GAN integrates pixel-wise loss in pixel-level, contextual loss and adversarial loss in perceptual level simultaneously, in order to guide on deblurring and super-resolution reconstruction of the raw images that are blurry and in low-resolution, which help obtaining realistic images. Extensive experiments conducted on a real-world dataset manifest the effectiveness of the proposed approaches, outperforming the state-of-the-art models. © Springer Nature Switzerland AG 2019.","Computer graphics; Image reconstruction; Optical resolving power; Pixels; Adversarial networks; GANs; High resolution image; Image deblurring; Low resolution images; State of the art; Super resolution; Super resolution reconstruction; Image enhancement","Contextual loss; GANs; Image deblurring; Pixel loss; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85067658475"
"Zhang M.; Liu W.; Ma H.","Zhang, Minghui (57204048322); Liu, Wu (55145372400); Ma, Huadong (7403096223)","57204048322; 55145372400; 7403096223","Joint License Plate Super-Resolution and Recognition in One Multi-Task Gan Framework","2018","ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2018-April","","8462282","1443","1447","4","10.1109/ICASSP.2018.8462282","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054289426&doi=10.1109%2fICASSP.2018.8462282&partnerID=40&md5=b6352c2c6da0559e455295ab1d0c30e8","License plate recognition (LPR) plays an important role in intelligent transport systems. The existed LPR systems are mostly based on hand-crafted methods for detection, segmentation, and recognition, which cannot accurately recognize the license plate in unconstrained surveillance environments. In this paper, we propose a Multi-Task Generative Adversarial Network (MTGAN) based LPR system, which combines the license plate super-resolution and recognition in one end-to-end framework. In the proposed MTGAN, we design a Fully Connected Network (FCN) as generative network (GN), which can combine knowledge from data distribution and domain prior knowledge of license plate to generate the spatial corresponding and high-resolution plate images in the synthesis pipeline. More important, a multi-task discriminative network is designed in MTGAN to combine the super-resolution and recognition in an adversarial manner to enhance each other. The experiments on the built real-world license plate dataset show that the proposed LPR system can generate high-resolution license plates as well as recognize them with higher accuracy than state-of-the-art LPR systems. © 2018 IEEE.","","Generative Adversarial Network; License Plate Recognition; Multi-Task; Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85054289426"
"Park S.-J.; Son H.; Cho S.; Hong K.-S.; Lee S.","Park, Seong-Jin (56201822200); Son, Hyeongseok (57195110392); Cho, Sunghyun (55487932900); Hong, Ki-Sang (7402515496); Lee, Seungyong (55943831500)","56201822200; 57195110392; 55487932900; 7402515496; 55943831500","SRFeat: Single Image Super-Resolution with Feature Discrimination","2018","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11220 LNCS","","","455","471","16","10.1007/978-3-030-01270-0_27","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055093688&doi=10.1007%2f978-3-030-01270-0_27&partnerID=40&md5=28d752166ba08ddd414fcb3205287a87","Generative adversarial networks (GANs) have recently been adopted to single image super-resolution (SISR) and showed impressive results with realistically synthesized high-frequency textures. However, the results of such GAN-based approaches tend to include less meaningful high-frequency noise that is irrelevant to the input image. In this paper, we propose a novel GAN-based SISR method that overcomes the limitation and produces more realistic results by attaching an additional discriminator that works in the feature domain. Our additional discriminator encourages the generator to produce structural high-frequency features rather than noisy artifacts as it distinguishes synthetic and real images in terms of features. We also design a new generator that utilizes long-range skip connections so that information between distant layers can be transferred more effectively. Experiments show that our method achieves the state-of-the-art performance in terms of both PSNR and perceptual quality compared to recent GAN-based methods. © 2018, Springer Nature Switzerland AG.","Optical resolving power; Adversarial networks; Feature discrimination; Feature domain; High frequency HF; High-frequency noise; Perceptual quality; State-of-the-art performance; Super resolution; Computer vision","Adversarial network; High frequency features; Perceptual quality; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85055093688"
"Lee Y.; Yun J.; Hong Y.; Lee J.; Jeon M.","Lee, Younkwan (57201723975); Yun, Jaewoong (57205435119); Hong, Yoojin (57201728427); Lee, Juhyun (57205448394); Jeon, Moongu (23987776700)","57201723975; 57205435119; 57201728427; 57205448394; 23987776700","Accurate License Plate Recognition and Super-Resolution Using a Generative Adversarial Networks on Traffic Surveillance Video","2018","2018 IEEE International Conference on Consumer Electronics - Asia, ICCE-Asia 2018","","","8552121","","","","10.1109/ICCE-ASIA.2018.8552121","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060063774&doi=10.1109%2fICCE-ASIA.2018.8552121&partnerID=40&md5=7a6b66b104ed8adcd266d8d3af8aeb9f","Automatic License Plate Recognition (ALPR) is one of the most important methods of intelligent traffic surveillance applications. Some existing ALPR systems are developed for near-frontal plate images in a single lane. However, most surveillance cameras have a challenging environment: small size object, poor resolution and blurred image. We propose a new method that can be applied in the ALPR challenged environments by using super-resolution (SR) module based on Generative Adversarial Networks (GAN). We also used the state-of-the-art and real-time object detection method, You Only Look Once (YOLO), for license plate detection and character recognition. We collected a challenging dataset at low resolution and small object less than 60∗60 size and evaluate our approach on it. The achieved mean accuracy of recognition of license plate is above 2% better than other methods in our dataset. Our implementation demonstrate the superiority over the state-of-the-art. © 2018 IEEE.","License plates (automobile); Monitoring; Object detection; Object recognition; Optical character recognition; Optical resolving power; Security systems; Adversarial networks; Automatic license plate recognition; License plate detection; License plate recognition; Object detection method; Super resolution; Traffic surveillance; Visual surveillance; Automatic vehicle identification","Generative Adversarial Networks; License Plate Recognition; Super-Resolution; Visual Surveillance","Conference paper","Final","","Scopus","2-s2.0-85060063774"
"Ju C.; Su X.; Yang H.; Ning H.","Ju, Chunwu (57204643973); Su, Xiuqin (18438569800); Yang, Haoyuan (57213132805); Ning, Hailong (57207357175)","57204643973; 18438569800; 57213132805; 57207357175","Single-image super-resolution reconstruction via generative adversarial network","2019","Proceedings of SPIE - The International Society for Optical Engineering","10843","","108430J","","","","10.1117/12.2505809","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062445635&doi=10.1117%2f12.2505809&partnerID=40&md5=265e30072d441ece1eb60240053d2c8d","Single-image super-resolution (SISR) reconstruction is important for image processing, and lots of algorithms based on deep convolutional neural network (CNN) have been proposed in recent years. Although these algorithms have better accuracy and recovery results than traditional methods without CNN, they ignore finer texture details when super-resolving at a large upscaling factor. To solve this problem, in this paper we propose an algorithm based on generative adversarial network for single-image super-resolution restoration at 4x upscaling factors. We decode a restored high-resolution image by the generative network and make the generator output results finer, more realistic texture details by the adversarial network. We performed experiments on the DIV2K dataset and proved that our method has better performance in single image super-resolution reconstruction. The image quality of this reconstruction method is improved at the peak signal-to-noise ratio and structural similarity index and the results have a good visual effect. © 2019 SPIE.","Deep neural networks; Image enhancement; Image quality; Manufacture; Neural networks; Optical data processing; Optical resolving power; Optoelectronic devices; Restoration; Signal to noise ratio; Textures; Adversarial networks; Convolutional neural network; High resolution image; Peak signal to noise ratio; Reconstruction method; Single-image super-resolution reconstruction; Structural similarity indices; Super resolution reconstruction; Image reconstruction","Adversarial network; Generative network; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85062445635"
"Yuan Y.; Liu S.; Zhang J.; Zhang Y.; Dong C.; Lin L.","Yuan, Yuan (57220641312); Liu, Siyuan (55578796600); Zhang, Jiawei (57194062920); Zhang, Yongbing (7601315649); Dong, Chao (56335662200); Lin, Liang (15061363400)","57220641312; 55578796600; 57194062920; 7601315649; 56335662200; 15061363400","Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks","2018","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2018-June","","8575264","814","823","9","10.1109/CVPRW.2018.00113","262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050346519&doi=10.1109%2fCVPRW.2018.00113&partnerID=40&md5=bba1ebcef8f0cf5de25044ad698d0c07","We consider the single image super-resolution problem in a more general case that the low-/high-resolution pairs and the down-sampling process are unavailable. Different from traditional super-resolution formulation, the low-resolution input is further degraded by noises and blurring. This complicated setting makes supervised learning and accurate kernel estimation impossible. To solve this problem, we resort to unsupervised learning without paired data, inspired by the recent successful image-to-image translation applications. With generative adversarial networks (GAN) as the basic component, we propose a Cycle-in-Cycle network structure to tackle the problem within three steps. First, the noisy and blurry input is mapped to a noise-free low-resolution space. Then the intermediate image is up-sampled with a pre-trained deep model. Finally, we fine-tune the two modules in an end-to-end manner to get the high-resolution output. Experiments on NTIRE2018 datasets demonstrate that the proposed unsupervised method achieves comparable results as the state-of-the-art supervised models. © 2018 IEEE.","Computer vision; Machine learning; Optical resolving power; Adversarial networks; High-resolution output; Image super resolutions; Image translation; Intermediate image; Kernel estimation; Network structures; Unsupervised method; Problem solving","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85050346519"
"Zhu J.; Yang G.; Lio P.","Zhu, Jin (57202160439); Yang, Guang (57216243504); Lio, Pietro (7004223170)","57202160439; 57216243504; 7004223170","Lesion focused super-resolution","2019","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","10949","","109491L","","","","10.1117/12.2512576","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068319048&doi=10.1117%2f12.2512576&partnerID=40&md5=ce1549e1024ce9811e847a75cfba41ca","Super-resolution (SR) for image enhancement has great importance in medical image applications. Broadly speaking, there are two types of SR, one requires multiple low resolution (LR) images from different views of the same object to be reconstructed to the high resolution (HR) output, and the other one relies on the learning from a large amount of training datasets, i.e., LR-HR pairs. In real clinical environment, acquiring images from multi-views is expensive and sometimes infeasible. In this paper, we present a novel Generative Adversarial Networks (GAN) based learning framework to achieve SR from its LR version. By performing simulation based studies on the Multimodal Brain Tumor Segmentation Challenge (BraTS) datasets, we demonstrate the efficacy of our method in application of brain tumor MRI enhancement. Compared to bilinear interpolation and other state-of-the-art SR methods, our model is lesion focused, which has not only resulted in better perceptual image quality without blurring, but also been more efficient and directly benefit for the following clinical tasks, e.g., lesion detection and abnormality enhancement. Therefore, we can envisage the application of our SR method to boost image spatial resolution while maintaining crucial diagnostic information for further clinical tasks. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Brain; Diagnosis; Image enhancement; Image processing; Large dataset; Medical imaging; Optical resolving power; Tumors; Bilinear interpolation; Brain tumor segmentation; Image spatial resolution; Lesion detection; Low resolution images; Medical image applications; Perceptual image quality; Super resolution; Medical image processing","image processing; lesion detection; medical image analysis; Super-resolution","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85068319048"
