"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Park N.-W.; Park M.-G.; Kwak G.-H.; Hong S.","Park, No-Wook (7202111787); Park, Min-Gyu (57215432360); Kwak, Geun-Ho (57206203736); Hong, Sungwook (55817600100)","7202111787; 57215432360; 57206203736; 55817600100","Deep Learning-Based Virtual Optical Image Generation and Its Application to Early Crop Mapping","2023","Applied Sciences (Switzerland)","13","3","1766","","","","10.3390/app13031766","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147917899&doi=10.3390%2fapp13031766&partnerID=40&md5=665bb799b92337d2b903139947863e3e","This paper investigates the potential of cloud-free virtual optical imagery generated using synthetic-aperture radar (SAR) images and conditional generative adversarial networks (CGANs) for early crop mapping, which requires cloud-free optical imagery at the optimal date for classification. A two-stage CGAN approach, including representation and generation stages, is presented to generate virtual Sentinel-2 spectral bands using all available information from Sentinel-1 SAR and Sentinel-2 optical images. The dual-polarization-based radar vegetation index and all available multi-spectral bands of Sentinel-2 imagery are particularly considered for feature extraction in the representation stage. A crop classification experiment using Sentinel-1 and -2 images in Illinois, USA, demonstrated that the use of all available scattering and spectral features achieved the best prediction performance for all spectral bands, including visible, near-infrared, red-edge, and shortwave infrared bands, compared with the cases that only used dual-polarization backscattering coefficients and partial input spectral bands. Early crop mapping with an image time series, including the virtual Sentinel-2 image, yielded satisfactory classification accuracy comparable to the case of using an actual time-series image set, regardless of the different combinations of spectral bands. Therefore, the generation of virtual optical images using the proposed model can be effectively applied to early crop mapping when the availability of cloud-free optical images is limited. © 2023 by the authors.","","crop classification; deep learning; generative adversarial networks; virtual image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85147917899"
"Ghozatlou O.; Datcu M.","Ghozatlou, Omid (57216394996); Datcu, Mihai (7004523124)","57216394996; 7004523124","HYBRID GAN AND SPECTRAL ANGULAR DISTANCE FOR CLOUD REMOVAL","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","","","","2695","2698","3","10.1109/IGARSS47720.2021.9554891","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129811525&doi=10.1109%2fIGARSS47720.2021.9554891&partnerID=40&md5=864effecbf986acda7e68016b9eaf883","This paper aims to present a new algorithm to remove thin clouds and retain information in corrupted images without the use of auxiliary data. By injecting physical properties into the cycle consistent generative adversarial network (GAN), we were able to convert a cloudy multispectral image to a cloudless image. To recover information beneath clouds and shadows we create a synthetic multispectral space to obtain illumination invariant features. Multispectral vectors were transformed from Cartesian coordinates to Polar coordinates to obtain spectral angular distance (SAD) then we employed them as input to train the deep neural network (DNN). Afterward, the outputs of DNN were transformed to Cartesian coordinates to obtain shadow and cloud-free multispectral images. The proposed method, Hybrid GAN-SAD yields trustworthy reconstructed results because of exploiting transparent information from certain multispectral bands to recover uncorrupted images.  © 2021 IEEE.","Computer vision; Deep neural networks; Gallium nitride; Angular distance; Auxiliary data; Cartesian coordinate; Cloud removal; Corrupted images; Generative adversarial network; Multi-spectral; Multispectral images; Multispectral satellite image; Polar coordinate; Generative adversarial networks","Cloud Removal; Generative Adversarial Networks (GANs); Multispectral Satellite Images; Polar Coordinates","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129811525"
"Zhang J.; Shamsolmoali P.; Zhang P.; Feng D.; Yang J.","Zhang, Junhao (56368684700); Shamsolmoali, Pourya (56350053200); Zhang, Pengpeng (56104492700); Feng, Deying (35219755900); Yang, Jie (15039078800)","56368684700; 56350053200; 56104492700; 35219755900; 15039078800","Multispectral image fusion using super-resolution conditional generative adversarial networks","2019","Journal of Applied Remote Sensing","13","2","022002","","","","10.1117/1.JRS.13.022002","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055792733&doi=10.1117%2f1.JRS.13.022002&partnerID=40&md5=88db0212e3ac0f09a92f60ac8917e082","In multispectral image fusion scenarios, deep learning has been widely applied. However, the fusion performance and image quality are still restricted by inflexible architecture and supervised learning mode. We proposed multispectral image fusion using super-resolution conditional generative adversarial networks (MS-cGANs) based on conditional cGANs, which produces the fused image through the flexible encode-and-decode procedure. In the proposed network, a least square model is extended to solve the gradients vanishing problem in cGANs. Then, to improve the fusion quality, the multiscale features are used to preserve the details. Furthermore, the image resolution is promoted by adding the perceptual loss in object function and injecting the super-resolution structure into a deconvolution procedure. In experimental results, MS-cGANs demonstrates a significant performance in fusing multispectral images and top-ranking image quality compared with the state-of-the-art methods. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Fusion reactions; Image quality; Image resolution; Least squares approximations; Optical resolving power; Remote sensing; Adversarial networks; Fusion performance; Least square model; Multi-scale features; Multi-spectral image fusions; Multispectral images; State-of-the-art methods; Super resolution; Image fusion","fusion; multispectral image; multispectral-conditional generative adversarial network; remote sensing","Article","Final","","Scopus","2-s2.0-85055792733"
"Zhang J.; Wang X.; Liu J.; Zhang D.; Lu Y.; Zhou Y.; Sun L.; Hou S.; Fan X.; Shen S.; Zhao J.","Zhang, Jun (57476905100); Wang, Xinxin (58078049400); Liu, Jingyan (57211953226); Zhang, Dongfang (57482555600); Lu, Yin (58078804200); Zhou, Yuhong (57927467400); Sun, Lei (57193565866); Hou, Shenglin (58078804300); Fan, Xiaofei (57212384880); Shen, Shuxing (57672098800); Zhao, Jianjun (57821019400)","57476905100; 58078049400; 57211953226; 57482555600; 58078804200; 57927467400; 57193565866; 58078804300; 57212384880; 57672098800; 57821019400","Multispectral Drone Imagery and SRGAN for Rapid Phenotypic Mapping of Individual Chinese Cabbage Plants","2022","Plant Phenomics","2022","","0007","","","","10.34133/plantphenomics.0007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146912072&doi=10.34133%2fplantphenomics.0007&partnerID=40&md5=55237c8471277c0f6451f89809067f39","The phenotypic parameters of crop plants can be evaluated accurately and quickly using an unmanned aerial vehicle (UAV) equipped with imaging equipment. In this study, hundreds of images of Chinese cabbage (Brassica rapa L. ssp. pekinensis) germplasm resources were collected with a low-cost UAV system and used to estimate cabbage width, length, and relative chlorophyll content (soil plant analysis development [SPAD] value). The super-resolution generative adversarial network (SRGAN) was used to improve the resolution of the original image, and the semantic segmentation network Unity Networking (UNet) was used to process images for the segmentation of each individual Chinese cabbage. Finally, the actual length and width were calculated on the basis of the pixel value of the individual cabbage and the ground sampling distance. The SPAD value of Chinese cabbage was also analyzed on the basis of an RGB image of a single cabbage after background removal. After comparison of various models, the model in which visible images were enhanced with SRGAN showed the best performance. With the validation set and the UNet model, the segmentation accuracy was 94.43%. For Chinese cabbage dimensions, the model was better at estimating length than width. The R2 of the visible-band model with images enhanced using SRGAN was greater than 0.84. For SPAD prediction, the R2 of the model with images enhanced with SRGAN was greater than 0.78. The root mean square errors of the 3 semantic segmentation network models were all less than 2.18. The results showed that the width, length, and SPAD value of Chinese cabbage predicted using UAV imaging were comparable to those obtained from manual measurements in the field. Overall, this research demonstrates not only that UAVs are useful for acquiring quantitative phenotypic data on Chinese cabbage but also that a regression model can provide reliable SPAD predictions. This approach offers a reliable and convenient phenotyping tool for the investigation of Chinese cabbage breeding traits.  Copyright © 2022 Jun Zhang et al.","Agricultural robots; Antennas; Crops; Drones; Image enhancement; Mean square error; Regression analysis; Semantic Segmentation; Aerial vehicle; Brassica rapa L; Chinese cabbage; Crop plants; Germplasms; Imaging equipment; Multi-spectral; Plant analysis; Semantic segmentation; Superresolution; Semantics","","Article","Final","","Scopus","2-s2.0-85146912072"
"Liu X.; Wang Y.; Liu Q.","Liu, Xiangyu (57192693924); Wang, Yunhong (34870959400); Liu, Qingjie (55534263100)","57192693924; 34870959400; 55534263100","Psgan: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","2018","Proceedings - International Conference on Image Processing, ICIP","","","8451049","873","877","4","10.1109/ICIP.2018.8451049","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062900060&doi=10.1109%2fICIP.2018.8451049&partnerID=40&md5=1849950b74eeded4a0073d53039a6838","Remote sensing image fusion (also known as pan-sharpening) aims to generate a high resolution multi -spectral image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral (MS) image. In this paper, we propose PSGAN, a generative adversarial network (GAN) for remote sensing image pansharpening. To the best of our knowledge, this is the first attempt at producing high quality pan-sharpened images with GANs. The PSGAN consists of two parts. Firstly, a two-stream fusion architecture is designed to generate the desired high resolution multi -spectral images, then a fully convolutional network serving as a discriminator is applied to distinct 'real' or 'pan-sharpened' MS images. Experiments on images acquired by Quickbird and GaoFen-1 satellites demonstrate that the proposed PSGAN can fuse PAN and MS images effectively and significantly improve the results over the state of the art traditional and CNN based pan-sharpening methods. © 2018 IEEE.","Deep learning; Image fusion; Image resolution; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Fusion architecture; High spatial resolution; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Image enhancement","Deep learning; GAN; Image fusion; Pan-sharpening; Remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062900060"
"Kerdegari H.; Razaak M.; Argyriou V.; Remagnino P.","Kerdegari, Hamideh (55438018000); Razaak, Manzoor (55639468300); Argyriou, Vasileios (13806485100); Remagnino, Paolo (6602806859)","55438018000; 55639468300; 13806485100; 6602806859","Smart Monitoring of Crops Using Generative Adversarial Networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11678 LNCS","","","554","563","9","10.1007/978-3-030-29888-3_45","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072861446&doi=10.1007%2f978-3-030-29888-3_45&partnerID=40&md5=8816c1824a3e9d2d2a6848b73e57ac31","Unmanned aerial vehicles (UAV) are used in precision agriculture (PA) to enable aerial monitoring of farmlands. Intelligent methods are required to pinpoint weed infestations and make optimal choice of pesticide. UAV can fly a multispectral camera and collect data. However, the classification of multispectral images using supervised machine learning algorithms such as convolutional neural networks (CNN) requires a large amount of training data. This is a common drawback in deep learning. Our method makes use of a semi-supervised generative adversarial networks (GAN), providing a pixel-wise classification for all the acquired multispectral images. It consists of a generator network to provide photo-realistic images as extra training data to a multi-class classifier acting as a discriminator and trained on small amounts of labeled data. The performance of the proposed semi-supervised GAN is evaluated on the weedNet dataset consisting of multispectral crop and weed images collected by a micro aerial vehicle (MAV). Results indicate high classification accuracy can be achieved and show the potential of GAN-based methods for the challenging task of multispectral image classification. © 2019, Springer Nature Switzerland AG.","Antennas; Crops; Deep learning; Image analysis; Image classification; Learning algorithms; Machine learning; Micro air vehicle (MAV); Neural networks; Supervised learning; Unmanned aerial vehicles (UAV); Adversarial networks; Classification accuracy; Convolutional neural network; Multi-spectral cameras; Multispectral image classification; Multispectral images; Semi-supervised; Supervised machine learning; Classification (of information)","Classification; Generative adversarial networks (GAN); Multispectral images; Semi-supervised GAN; Unmanned aerial vehicles (UAV)","Conference paper","Final","","Scopus","2-s2.0-85072861446"
"Latif H.; Ghuffar S.; Ahmad H.M.","Latif, Hasan (57971676700); Ghuffar, Sajid (14630367300); Ahmad, Hafiz Mughees (57208205069)","57971676700; 14630367300; 57208205069","Super-resolution of Sentinel-2 images using Wasserstein GAN","2022","Remote Sensing Letters","13","12","","1194","1202","8","10.1080/2150704X.2022.2136019","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142262873&doi=10.1080%2f2150704X.2022.2136019&partnerID=40&md5=6ed46ccc6359ea33ed4a46f5cec66add","The Sentinel-2 satellites deliver 13 band multi-spectral imagery with bands having 10 m, 20 m or 60 m spatial resolution. The low-resolution bands can be upsampled to match the high resolution bands to extract valuable information at higher spatial resolution. This paper presents a Wasserstein Generative Adversarial Network (WGAN) based approach named as DSen2-WGAN to super-resolve the low-resolution (i.e., 20 m and 60 m) bands of Sentinel-2 images to a spatial resolution of 10 m. A proposed generator is trained in an adversarial manner using the min-max game to super-resolve the low-resolution bands with the guidance of available high-resolution bands in an image. The performance evaluated using metrics such as Signal Reconstruction Error (SRE) and Root Mean Squared Error (RMSE) shows the effectiveness of the proposed approach as compared to the state-of-the-art method, DSen2 as the DSen2-WGAN reduced RMSE by 14.68% and 7%, while SRE improved by almost 4% and 1.6% for 6 (Formula presented.) and 2 (Formula presented.) super-resolution. Lastly, for further evaluation, we have used trained DSen2-WGAN model to super-resolve the bands of EuroSAT dataset, a satellite image classification dataset based on Sentinel-2 images. The per band classification accuracy of low-resolution bands shows significant improvement after super-resolution using our proposed approach. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Air navigation; Classification (of information); Image resolution; Mean square error; Signal reconstruction; Spectroscopy; High resolution; High spatial resolution; Lower resolution; Multispectral imagery; Network-based approach; Reconstruction error; Root mean squared errors; Signals reconstruction; Spatial resolution; Superresolution; data set; image classification; image resolution; satellite data; Sentinel; spatial resolution; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85142262873"
"Jozdani S.; Chen D.; Pouliot D.; Alan Johnson B.","Jozdani, Shahab (57188989068); Chen, Dongmei (57203235632); Pouliot, Darren (8212745300); Alan Johnson, Brian (57493655200)","57188989068; 57203235632; 8212745300; 57493655200","A review and meta-analysis of generative adversarial networks and their applications in remote sensing","2022","International Journal of Applied Earth Observation and Geoinformation","108","","102734","","","","10.1016/j.jag.2022.102734","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126636731&doi=10.1016%2fj.jag.2022.102734&partnerID=40&md5=477a218f5f179f495a2631341aad3e8a","Generative Adversarial Networks (GANs) are one of the most creative advances in Deep Learning (DL) in recent years. The Remote Sensing (RS) community has adopted GANs quickly, and reported successful use in a wide variety of applications. Given a sharp increase in research on GANs in the field of RS, there is a need for an in-depth review of the major technological/methodological advances and new applications. In this regard, we conducted a comprehensive review and meta-analysis of GAN-related RS papers, with the goals of familiarizing the RS community with the potential of GANs and helping researchers further explore RS applications of GANs by untangling challenges common in this field. Our review is based on 231 journal papers that were retrieved and selected through the Web of Science (WoS) database. We reviewed the theories, applications, and challenges of GANs, and highlighted the gaps to explore in future studies. Through the meta-analysis conducted in this study, we observed that image classification (especially urban mapping) has been the most popular application of GANs, potentially due to the wide availability of benchmark datasets. One the other hand, we found that relatively few studies have explored the potential of GANs for analyzing medium spatial-resolution multi-spectral images (e.g., Landsat or Sentinel-2), even though such images are often freely available and useful for a wide range of applications (e.g., urban expansion analysis, vegetation mapping, etc.). In spite of the applications of GANs for different RS processing tasks, there are still several gaps/questions in this field such as: 1) which GAN models/configurations are more suitable for different applications?) 2) to what degree can GANs replace real RS data in different applications? Such gaps/questions can be appropriately addressed by, for example, conducting experimental studies on evaluating different GAN models for various RS applications to provide better insights into how/which GAN models can be best deployed. The meta-analysis results presented in this study could be helpful for RS researchers to know the opportunities of using GANs and understand how GANs contribute to the current challenges in different RS applications. © 2022 The Authors","database; image analysis; machine learning; meta-analysis; remote sensing; spatial resolution","Deep learning; GANs; Generative adversarial networks; Remote sensing","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126636731"
"Dai H.; Liu X.; Qiao Y.; Zheng K.; Xiao X.; Cai Z.","Dai, Haoran (57573734600); Liu, Xiaobo (36647829600); Qiao, Yulin (57215855806); Zheng, Kexin (57573734500); Xiao, Xiao (57573926200); Cai, Zhihua (56424630500)","57573734600; 36647829600; 57215855806; 57573734500; 57573926200; 56424630500","UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","2021","Proceeding - 2021 China Automation Congress, CAC 2021","","","","1803","1808","5","10.1109/CAC53003.2021.9727490","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128081131&doi=10.1109%2fCAC53003.2021.9727490&partnerID=40&md5=d58e630e4449514acdb2991dd153cc28","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFN-GAN for remote sensing image fusion. In our proposed UFN-GAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets. © 2021 IEEE","Decoding; Deep learning; Edge detection; Image enhancement; Image fusion; Remote sensing; Signal encoding; Spectral resolution; Unsupervised learning; Deep learning; Feature map; Fusion image; High spectral resolution; Multispectral images; Original images; Remote sensing images; Spatial informations; Spatial resolution; Spectral information; Generative adversarial networks","Deep learning; Generative adversarial network; Image fusion; Remote sensing images; Unsupervised learning; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85128081131"
"Sarrut D.; Etxebeste A.; Muñoz E.; Krah N.; Létang J.M.","Sarrut, David (6603306037); Etxebeste, Ane (55357049700); Muñoz, Enrique (57118964900); Krah, Nils (56940595300); Létang, Jean Michel (6601992807)","6603306037; 55357049700; 57118964900; 56940595300; 6601992807","Artificial Intelligence for Monte Carlo Simulation in Medical Physics","2021","Frontiers in Physics","9","","738112","","","","10.3389/fphy.2021.738112","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119057226&doi=10.3389%2ffphy.2021.738112&partnerID=40&md5=fa22e4e493fbea9f2c351dad87050f7d","Monte Carlo simulation of particle tracking in matter is the reference simulation method in the field of medical physics. It is heavily used in various applications such as 1) patient dose distribution estimation in different therapy modalities (radiotherapy, protontherapy or ion therapy) or for radio-protection investigations of ionizing radiation-based imaging systems (CT, nuclear imaging), 2) development of numerous imaging detectors, in X-ray imaging (conventional CT, dual-energy, multi-spectral, phase contrast …), nuclear imaging (PET, SPECT, Compton Camera) or even advanced specific imaging methods such as proton/ion imaging, or prompt-gamma emission distribution estimation in hadrontherapy monitoring. Monte Carlo simulation is a key tool both in academic research labs as well as industrial research and development services. Because of the very nature of the Monte Carlo method, involving iterative and stochastic estimation of numerous probability density functions, the computation time is high. Despite the continuous and significant progress on computer hardware and the (relative) easiness of using code parallelisms, the computation time is still an issue for highly demanding and complex simulations. Hence, since decades, Variance Reduction Techniques have been proposed to accelerate the processes in a specific configuration. In this article, we review the recent use of Artificial Intelligence methods for Monte Carlo simulation in medical physics and their main associated challenges. In the first section, the main principles of some neural networks architectures such as Convolutional Neural Networks or Generative Adversarial Network are briefly described together with a literature review of their applications in the domain of medical physics Monte Carlo simulations. In particular, we will focus on dose estimation with convolutional neural networks, dose denoising from low statistics Monte Carlo simulations, detector modelling and event selection with neural networks, generative networks for source and phase space modelling. The expected interests of those approaches are discussed. In the second section, we focus on the current challenges that still arise in this promising field. Copyright © 2021 Sarrut, Etxebeste, Muñoz, Krah and Létang.","","AI; deep learning; GAN; medical physics; Monte Carlo simulation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119057226"
"Qian X.; Li J.; Cheng G.; Yao X.; Zhao S.; Chen Y.; Jiang L.","Qian, Xiaoliang (36465575400); Li, Jia (57206965588); Cheng, Gong (36801169800); Yao, Xiwen (55813585000); Zhao, Suna (57020366900); Chen, Yibin (57204183485); Jiang, Liying (57192317855)","36465575400; 57206965588; 36801169800; 55813585000; 57020366900; 57204183485; 57192317855","Evaluation of the effect of feature extraction strategy on the performance of high-resolution remote sensing image scene classification; [特征提取策略对高分辨率遥感图像场景分类性能影响的评估]","2018","Yaogan Xuebao/Journal of Remote Sensing","22","5","","758","776","18","10.11834/jrs.20188015","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054837402&doi=10.11834%2fjrs.20188015&partnerID=40&md5=2c311a1ca35c45fd86d52158784b9a88","Remote sensing image scene classification aims to tag remote sensing images with semantic categories according to the content of the image and is important in disaster monitoring, environmental detection, and urban planning. Scene classification results can provide valuable information about object recognition and image retrieval and can effectively improve the performance of image interpretation. The general process of remote sensing image scene classification mainly consists of feature extraction and scene classification based on image features. Given that the design of classifiers is relatively mature, this work focuses on feature extraction strategy. The influence of various strategies on the performance of scene classification is short of unified evaluation, which limits its development. The effect of various feature extraction strategies on the performance of high-resolution remote sensing image scene classification is evaluated in this study. In the second section of this paper, existing feature extraction strategies are divided into two categories: (1) hand-designed and (2) data-driven feature extraction. Hand-designed features, such as Color Histograms (CH) and Scale Invariant Feature Transform (SIFT), provide the primary description of images and are presented in the early period. Further abstract description of the images is introduced by coding of hand-designed features, such as Bag of Visual Words (BoVW) and has higher classification accuracy than hand-designed features. However, these feature extraction strategies generally suffer from poor generalization capability due to specific requirements for designing. Furthermore, hand-designed features require significant domain knowledge. By contrast, data-driven features can directly learn powerful features from a large number of sample images and are generally divided into shallow and deep learning features. Shallow learning feature extraction mainly involves Principal Component Analysis (PCA), Independent Component Analysis (ICA), and sparse coding algorithms. Typical deep learning feature extraction strategies include stacked autoencoder (SAE), Deep Belief Network (DBN), and Convolutional Neural Network (CNN). Compared with deep learning models, shallow learning models can be regarded as a neural network with a single hidden layer and thus cannot capture high-level semantic features. The superiority of deep learning features is obvious when dealing with complex scene classification. Furthermore, CNN-based features exhibit improved performance compared with SAE- and DBN-based features because the one-dimensional structure of SAE and DBN destroys the spatial information of images. In the third section of this paper, 29 feature descriptors are quantitatively compared in UC Merced, AID, and NWPU RESISC-45 datasets and eight combinations of feature descriptors are quantitatively compared in the NWPU RESISC-45 dataset. The effect of different feature extraction strategies on the performance of scene classification and the complexity of each dataset are evaluated through quantitative comparison. The experimental results are as follows. (1) The classification accuracy and stability of hand-designed features is poor, however the efficiency of most features is satisfactory and can attain better performance by combining with other types of features. (2) Among all feature extraction strategies, the coding of hand-designed features possesses moderate levels of classification accuracy, efficiency, and stability. (3) The classification accuracy and stability of data-driven features are best, but most of them have low efficiency. (4) AlexNet, a deep learning model with few layers, exhibits the best comprehensive performance and is suitable for occasions that require high classification accuracy, efficiency, and stability. (5) Some scene classes belonging to land use type are easy to be confused because of similar landmark buildings or sites. Moreover, some scene classes belonging to land cover type are easy to be confused because of their similar geomorphologic features. (6) The recently proposed NWPU RESISC-45 dataset is more complex than the other datasets and is more challenging for scene classification algorithms. Finally, the summary and conclusion of this paper are presented, and the discussion of future development is provided. On the one hand, combining prior knowledge introduced by hand-designed features with the CNN model may be one of the future development directions. On the other hand, introducing Generative Adversarial Networks (GAN) into CNN training may be a research hotspot in the future. In addition, remote sensing parameters, such as NDVI and NDWI, and multi-spectral information can be integrated with current feature extraction strategies for practical applications. © 2018, Science Press. All right reserved.","Abstracting; Codes (symbols); Complex networks; Data mining; Deep learning; Efficiency; Extraction; Feature extraction; Image classification; Image coding; Image enhancement; Image retrieval; Independent component analysis; Land use; Neural networks; Object recognition; Principal component analysis; Remote sensing; Semantics; Stability; Convolutional Neural Networks (CNN); Data driven; Hand-designed features; High resolution; High resolution remote sensing images; Independent component analyses (ICA); Scale invariant feature transforms; Scene classification; Classification (of information)","Data driven features; Deep learning; Feature extraction strategy; Hand-designed features; High-resolution; Scene classification","Article","Final","","Scopus","2-s2.0-85054837402"
"Li G.; Li J.; Fan H.","Li, Guihui (57201908517); Li, Jinjiang (25638923500); Fan, Hui (36647626600)","57201908517; 25638923500; 36647626600","Edge-guided multispectral image fusion algorithm","2020","Journal of Applied Remote Sensing","14","4","046515","","","","10.1117/1.JRS.14.046515","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098642871&doi=10.1117%2f1.JRS.14.046515&partnerID=40&md5=1cfd168ef028c7b969d79143cb468b45","Most existing multispectral fusion algorithms often suffer from spectral or spatial information distortion. Driven by this motivation, we propose an edge-guided multispectral (MS) image fusion algorithm. In particular, it combines the advantages of generative adversarial networks and improved fusion frameworks, so the merged image can better preserve the spectral information of the original multispectral image while injecting spatial detail information. Specifically, first, an MS image with more image detail is generated using the generated confrontation network for preliminary reconstruction. The panchromatic image edge information and the antagonistic learning strategy are introduced for the robust multispectral image reconstruction. Then, using the reconstructed MS image and the general component substitution image fusion framework, the whole fusion system of this paper is constructed. An enhancement operator is introduced to inject spatial details. Our extensive dataset evaluations show that our approach performs better in terms of high objective quality and human visual perception than several of the most advanced fusion methods.  © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Edge detection; Image enhancement; Image reconstruction; Adversarial networks; Component substitution; Human visual perception; Multi-spectral image fusions; Multispectral fusion; Multispectral images; Spatial informations; Spectral information; Image fusion","edge enhancement; generative adversarial network; multispectral image fusion; super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85098642871"
"Li J.; Wu Z.; Sheng Q.; Wang B.; Hu Z.; Zheng S.; Camps-Valls G.; Molinier M.","Li, Jun (57202722259); Wu, Zhaocong (15023850900); Sheng, Qinghong (36562635800); Wang, Bo (57190495006); Hu, Zhongwen (55630272400); Zheng, Shaobo (57852261300); Camps-Valls, Gustau (6603888005); Molinier, Matthieu (22234853700)","57202722259; 15023850900; 36562635800; 57190495006; 55630272400; 57852261300; 6603888005; 22234853700","A hybrid generative adversarial network for weakly-supervised cloud detection in multispectral images","2022","Remote Sensing of Environment","280","","113197","","","","10.1016/j.rse.2022.113197","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136277692&doi=10.1016%2fj.rse.2022.113197&partnerID=40&md5=58da582698f1afcd1e7983df21b2c735","Cloud detection is a crucial step in the optical satellite image processing pipeline for Earth observation. Clouds in optical remote sensing images seriously affect the visibility of the background and greatly reduce the usability of images for land applications. Traditional methods based on thresholding, multi-temporal or multi-spectral information are often specific to a particular satellite sensor. Convolutional Neural Networks for cloud detection often require labeled cloud masks for training that are very time-consuming and expensive to obtain. To overcome these challenges, this paper presents a hybrid cloud detection method based on the synergistic combination of generative adversarial networks (GAN) and a physics-based cloud distortion model (CDM). The proposed weakly-supervised GAN-CDM method (available online https://github.com/Neooolee/GANCDM) only requires patch-level labels for training, and can produce cloud masks at pixel-level in both training and testing stages. GAN-CDM is trained on a new globally distributed Landsat 8 dataset (WHUL8-CDb, available online doi:https://doi.org/10.5281/zenodo.6420027) including image blocks and corresponding block-level labels. Experimental results show that the proposed GAN-CDM method trained on Landsat 8 image blocks achieves much higher cloud detection accuracy than baseline deep learning-based methods, not only in Landsat 8 images (L8 Biome dataset, 90.20% versus 72.09%) but also in Sentinel-2 images (“S2 Cloud Mask Catalogue” dataset, 92.54% versus 77.00%). This suggests that the proposed method provides accurate cloud detection in Landsat images, has good transferability to Sentinel-2 images, and can quickly be adapted for different optical satellite sensors. © 2022 The Authors","Deep learning; HTTP; Image processing; Landsat; Learning systems; Optical data processing; Optical remote sensing; Cloud detection; Cloud distortion model; Cloud masks; Deep learning; Distortion model; Generative adversarial network; LANDSAT; Model method; Remote-sensing; Satellite sensors; artificial neural network; detection method; image processing; Landsat; remote sensing; satellite imagery; Sentinel; spectral analysis; supervised learning; Generative adversarial networks","Cloud detection; Cloud distortion model; Deep learning; Generative adversarial networks (GAN); Remote sensing","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85136277692"
"Fawakherji M.; Potena C.; Pretto A.; Bloisi D.D.; Nardi D.","Fawakherji, Mulham (57208207588); Potena, Ciro (56993817600); Pretto, Alberto (23393749700); Bloisi, Domenico D. (57189023230); Nardi, Daniele (7006582655)","57208207588; 56993817600; 23393749700; 57189023230; 7006582655","Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming","2021","Robotics and Autonomous Systems","146","","103861","","","","10.1016/j.robot.2021.103861","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114121190&doi=10.1016%2fj.robot.2021.103861&partnerID=40&md5=126f68cc8b0efc0703fcf9eb669ff3a7","An effective perception system is a fundamental component for farming robots, as it enables them to properly perceive the surrounding environment and to carry out targeted operations. The most recent methods make use of state-of-the-art machine learning techniques to learn a valid model for the target task. However, those techniques need a large amount of labeled data for training. A recent approach to deal with this issue is data augmentation through Generative Adversarial Networks (GANs), where entire synthetic scenes are added to the training data, thus enlarging and diversifying their informative content. In this work, we propose an alternative solution with respect to the common data augmentation methods, applying it to the fundamental problem of crop/weed segmentation in precision farming. Starting from real images, we create semi-artificial samples by replacing the most relevant object classes (i.e., crop and weeds) with their synthesized counterparts. To do that, we employ a conditional GAN (cGAN), where the generative model is trained by conditioning the shape of the generated object. Moreover, in addition to RGB data, we take into account also near-infrared (NIR) information, generating four channel multi-spectral synthetic images. Quantitative experiments, carried out on three publicly available datasets, show that (i) our model is capable of generating realistic multi-spectral images of plants and (ii) the usage of such synthetic images in the training process improves the segmentation performance of state-of-the-art semantic segmentation convolutional networks. © 2021 Elsevier B.V.","Agricultural robots; Convolutional neural networks; Crops; Image enhancement; Infrared devices; Learning systems; Semantics; Spectroscopy; Alternative solutions; Convolutional networks; Fundamental component; Machine learning techniques; Quantitative experiments; Segmentation performance; Semantic segmentation; Surrounding environment; Image segmentation","Agricultural robotics; cGANs; Crop/weed detection; Semantic segmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85114121190"
"Zhou Y.; Zhong F.; Hu S.","Zhou, Yifeng (57223890878); Zhong, Fenghe (57216790612); Hu, Song (55278122800)","57223890878; 57216790612; 55278122800","Temporal and spectral unmixing of photoacoustic signals by deep learning","2021","Optics Letters","46","11","","2690","2693","3","10.1364/OL.426678","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106668121&doi=10.1364%2fOL.426678&partnerID=40&md5=e7c9d026a1755b011d44e66e64c9509f","Improving the imaging speed of multi-parametric photoacoustic microscopy (PAM) is essential to leveraging its impact in biomedicine.However, to avoid temporal overlap, the A-line rate is limited by the acoustic speed in biological tissues to a few megahertz.Moreover, to achieve high-speed PAMof the oxygen saturation of hemoglobin, the stimulated Raman scattering effect in optical fibers has been widely used to generate 558 nm from a commercial 532 nm laser for dual-wavelength excitation. However, the fiber length for effective wavelength conversion is typically short, corresponding to a small time delay that leads to a significant overlap of the A-lines acquired at the two wavelengths. Increasing the fiber length extends the time interval but limits the pulse energy at 558 nm. In this Letter, we report a conditional generative adversarial network-based approach that enables temporal unmixing of photoacoustic A-line signals with an interval as short as ~38 ns, breaking the physical limit on the A-line rate. Moreover, this deep learning approach allows the use of multi-spectral laser pulses for PAM excitation, addressing the insufficient energy of monochromatic laser pulses. This technique lays the foundation for ultrahigh-speed multi-parametric PAM. © 2021 Optical Society of America.","Deep Learning; Photoacoustic Techniques; Hemoglobin oxygen saturation; Laser excitation; Laser pulses; Optical fibers; Photoacoustic microscopy; Adversarial networks; Biological tissues; Learning approach; Monochromatic lasers; Oxygen saturation of hemoglobin; Photoacoustic signals; Spectral unmixing; Ultra high speed; photoacoustics; Deep learning","","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85106668121"
"Mizginov V.A.; Kniaz V.V.; Fomin N.A.","Mizginov, V.A. (57192159517); Kniaz, V.V. (56540022100); Fomin, N.A. (57202441244)","57192159517; 56540022100; 57202441244","A method for synthesizing thermal images using gan multi-layered approach","2021","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","54","2/W1","","155","162","7","","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108460695&partnerID=40&md5=8c3432a628f1295e75932e92243dd1a8","The active development of neural network technologies and optoelectronic systems has led to the introduction of computer vision technologies in various fields of science and technology. Deep learning made it possible to solve complex problems that a person had not been able to solve before. The use of multi-spectral optical systems has significantly expanded the field of application of video systems. Tasks such as image recognition, object re-identification, video surveillance require high accuracy, speed and reliability. These qualities are provided by algorithms based on deep convolutional neural networks. However, they require to have large databases of multi-spectral images of various objects to achieve state-of-the-art results. While large and various databases of color images of different objects are widely available in public domain, then similar databases of thermal images are either not available, or they represent a small number of types of objects. The quality of three-dimensional modeling for the thermal imaging spectral range remains at an insufficient level for solving a number of important tasks, which require high precision and reliability. The realistic synthesis of thermal images is especially important due to the complexity and high cost of obtaining real data. This paper is focused on the development of a method for synthesizing thermal imaging images based on generative adversarial neural networks. We developed an algorithm for a multi-spectral image-to-image translation. We have changed to the original GAN architecture and converted the loss function. We presented a new learning approach. For this, we prepared a special training dataset including about 2000 image tensors. The evaluation of the results obtained showed that the proposed method can be used to expand the available databases of thermal images. © Authors 2021. CC BY 4.0 License.","Complex networks; Computer vision; Convolutional neural networks; Database systems; Deep neural networks; Image recognition; Infrared imaging; Optoelectronic devices; Security systems; Spectroscopy; Computer vision technology; Multi-layered approach; Multispectral images; Network technologies; Optoelectronic systems; Re identifications; Science and Technology; Three-dimensional model; Deep learning","Generative adversarial networks; Image synthesis; Infrared image; Object recognition","Conference paper","Final","","Scopus","2-s2.0-85108460695"
"Mokalla S.R.; Bourlai T.","Mokalla, Suha Reddy (57215273795); Bourlai, Thirimachos (8850031300)","57215273795; 8850031300","Effects of Demographics and Photometric Normalization on Image Translation GANs for Cross-Spectral Face Recognition","2021","Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021","","","","2109","2118","9","10.1109/BigData52589.2021.9671292","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125329605&doi=10.1109%2fBigData52589.2021.9671292&partnerID=40&md5=6b1f82c587aaecb66d7dbdc69d99cdec","This paper focuses on thermal-to-visible face matching through image synthesis. Most of the legacy face image datasets are composed of visible band data. Thermal band as well as dual band, i.e. visible and thermal face datasets, are limited. Operating in the thermal band and therefore working on visible thermal face recognition (FR) systems can be beneficial in various scenarios. The challenge is cross-spectral matching, i.e. matching gallery, visible band, face images against thermal ones. To address this problem, we train and test two of the most popular image-to-image translation Generative Adversarial Networks (GANs). These are Pix2pix and StarGAN2. In this work, the two aforementioned GAN trained models are tested, and the visible face images generated are matched against the ground truth visible faces using one of the most powerful visible-to-visible face matching algorithms, namely Facenet. We also perform an ablation study where the original thermal and visible images are photometrically normalized before training the image synthesis-specific models. The main outcomes of our study are that the FR accuracy from the pix2pix model did not vary significantly; when using StarGAN2, the original face images yield much higher accuracy compared to the photometrically normalized ones; finally, we observe that, when using the pix2pix model for image synthesis, bearded and non-Caucasian generated face images suffer the most from different noise factors. Specifically, the FR accuracy when using pix2pix after image synthesis yields a face verification area under curve (AUC) of 58.3%, while the same models when tested on data excluding bearded and non-Caucasian faces yields an accuracy of 68.6%. © 2021 IEEE.","Computer vision; Generative adversarial networks; Photometry; Cross-spectral; Images synthesis; Multi-spectral; Photometric normalization; Pix2pix; Stargan2; Thermal; Thermal spectra; Thermal-to-visible; Visible spectrums; Face recognition","Cross-spectral; Face recognition; Image synthesis; Multi-spectral; Photometric normalization; Pix2pix; StarGAN2; Thermal spectrum; Thermal-to-visible; Visible spectrum","Conference paper","Final","","Scopus","2-s2.0-85125329605"
"Gong M.; Yang Y.; Zhan T.; Niu X.; Li S.","Gong, Maoguo (8933846400); Yang, Yuelei (57204044678); Zhan, Tao (57052462500); Niu, Xudong (57196705445); Li, Shuwei (57210320838)","8933846400; 57204044678; 57052462500; 57196705445; 57210320838","A Generative Discriminatory Classified Network for Change Detection in Multispectral Imagery","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","1","8600384","321","333","12","10.1109/JSTARS.2018.2887108","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059449220&doi=10.1109%2fJSTARS.2018.2887108&partnerID=40&md5=fff6fc5380de5e6c6f863588252413a5","Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods. © 2008-2012 IEEE.","Deep learning; Image classification; Remote sensing; Adversarial networks; Change detection; Competitive performance; Large amounts; Multi-spectral imagery; Multispectral images; Training sample; Unlabeled data; data set; detection method; image analysis; image classification; machine learning; spatiotemporal analysis; spectral analysis; Discriminators","Change detection; deep learning; generative adversarial networks (GANs); multispectral imagery","Article","Final","","Scopus","2-s2.0-85059449220"
"Arun P.V.; Sadeh R.; Avneri A.; Tubul Y.; Camino C.; Buddhiraju K.M.; Porwal A.; Lati R.N.; Zarco-Tejada P.J.; Peleg Z.; Herrmann I.","Arun, P.V. (57202034266); Sadeh, R. (57209321842); Avneri, A. (57207958385); Tubul, Y. (57266839800); Camino, C. (55387868700); Buddhiraju, K.M. (36815713200); Porwal, A. (9738526000); Lati, R.N. (37088792100); Zarco-Tejada, P.J. (6701731228); Peleg, Z. (15136927700); Herrmann, I. (36070624100)","57202034266; 57209321842; 57207958385; 57266839800; 55387868700; 36815713200; 9738526000; 37088792100; 6701731228; 15136927700; 36070624100","Multimodal Earth observation data fusion: Graph-based approach in shared latent space","2022","Information Fusion","78","","","20","39","19","10.1016/j.inffus.2021.09.004","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115401406&doi=10.1016%2fj.inffus.2021.09.004&partnerID=40&md5=180247e9a56560637a79ea9eed3cfb89","Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few. © 2021 Elsevier B.V.","Antennas; Convolutional neural networks; Generative adversarial networks; Graphic methods; Image enhancement; Image fusion; Learning systems; Metadata; Observatories; Satellite imagery; Signal encoding; Unmanned aerial vehicles (UAV); Convolutional neural network; Cross-domain; Cross-modal; Earth observation data; Ground measured spectrum; HyperSpectral; Multi-modal; Multi-spectral; Multispectral unmanned aerial vehicle; Spectra's; Convolution","Convolutional neural networks; Fusion; Ground measured spectra; Hyperspectral; Multispectral UAV","Article","Final","","Scopus","2-s2.0-85115401406"
"Kerdegari H.; Razaak M.; Argyriou V.; Remagnino P.","Kerdegari, Hamideh (55438018000); Razaak, Manzoor (55639468300); Argyriou, Vasileios (13806485100); Remagnino, Paolo (6602806859)","55438018000; 55639468300; 13806485100; 6602806859","Urban scene segmentation using semi-supervised GAN","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","111551H","","","","10.1117/12.2533055","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078136549&doi=10.1117%2f12.2533055&partnerID=40&md5=300b8ecab1c88e4fe9f5c879afc2c092","Semantic segmentation of remote sensing data such as multispectral imagery has been boosted recently using deep convolutional neural networks (CNN). However, segmentation of multispectral images using supervised machine learning algorithms such as CNN requires a significant number of pixel-level annotated data, often unavailable, making the task extremely challenging. To address this, this paper puts forward a semi-supervised framework, based on generative adversarial networks (GAN). The proposed solution consists of a generator network to provide photo-realistic images as extra training data to a multi-class classifier acting as a discriminator and trained on a small annotated dataset. Performance of the proposed semi-supervised GAN is evaluated on two benchmarks multispectral semantic segmentation datasets collected from urban scenes of Vaihingen and Potsdam. Results indicate that the proposed framework achieves competitive performance compared to state-of-the-art semantic segmentation methods and show the potential of GAN-based methods for the challenging task of multispectral image segmentation. © 2019 SPIE.","Benchmarking; Classification (of information); Deep neural networks; Learning algorithms; Machine learning; Neural networks; Remote sensing; Semantics; Supervised learning; Adversarial networks; Competitive performance; Convolutional neural network; Multi-class classifier; Multi-spectral imagery; Semantic segmentation; Semi-supervised; Supervised machine learning; Image segmentation","Convolu-tional neural network; Generative adversarial networks (GAN); Multispectral imagery; Semantic segmentation; Semi-supervised GAN","Conference paper","Final","","Scopus","2-s2.0-85078136549"
"Dai X.; Yuan X.; Wei X.","Dai, Xuerui (57193803463); Yuan, Xue (7402202691); Wei, Xueye (8847047900)","57193803463; 7402202691; 8847047900","Data augmentation for thermal infrared object detection with cascade pyramid generative adversarial network","2022","Applied Intelligence","52","1","","967","981","14","10.1007/s10489-021-02445-9","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105923141&doi=10.1007%2fs10489-021-02445-9&partnerID=40&md5=67e086fae0ec29a19096da0dea3b93d5","Object detection based on convolutional neural network (CNN) should be trained effectively with much data. Data augmentation techniques devote to generate more data, which can enhance the generalization ability and robustness of detection network. For object detection in thermal infrared (TIR) images, objects are difficult to label because of the heavy noise and low resolution. So, it is highly recommended for us to do data augmentation. However, traditional data augmentation strategies (such as image flipping, random color jittering) only produce limited training samples. In order to generate images with high resolution, and ensure they are subject to the distribution of real samples, generative adversarial network (GAN) is introduced. To generate high-resolution samples, image pyramids are input into different branches, then these cascade features are fused to gradually improve the resolution. For the sake of improving the discriminant capability of discriminator, the feature matching loss is calculated when training. And the generated images with different resolutions are discriminated in multiple stages. The data augmentation algorithm proposed in this paper is called cascade pyramid generative adversarial network (CPGAN). No matter on the KAIST Multispectral data set or OSU thermal-color data set, with our CPGAN, the detection accuracy of classical detection algorithms is greatly improved. In addition, the detection speed remains entirely unaffected because CPGAN only exists in the training phase. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Image enhancement; Infrared radiation; Object recognition; Adversarial networks; Data augmentation algorithms; Detection algorithm; Detection networks; Different resolutions; Generalization ability; Multi-spectral data; Thermal infrared images; Object detection","Data augmentation; GAN; Object detection; Thermal infrared images","Article","Final","","Scopus","2-s2.0-85105923141"
"Wang L.; Gao C.; Zhao Y.; Song T.; Feng Q.","Wang, Lan (57191097124); Gao, Chenqiang (12753046300); Zhao, Yue (57203762157); Song, Tiecheng (57203522510); Feng, Qi (57129725400)","57191097124; 12753046300; 57203762157; 57203522510; 57129725400","Infrared and Visible Image Registration Using Transformer Adversarial Network","2018","Proceedings - International Conference on Image Processing, ICIP","","","8451370","1248","1252","4","10.1109/ICIP.2018.8451370","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062916597&doi=10.1109%2fICIP.2018.8451370&partnerID=40&md5=6184460aa52ca5b80bd09e0acb4bef23","In this paper we address the task of infrared and visible image registration in complex scenes. Due to the difference of infrared and visible images, it is neither easy to reliably find features nor suitable for directly training in deep learning architecture. Thus, we propose a two-stage adversarial network, which first conducts a multi-spectral image transfer to obtain a mapped image. And then the proposed network incorporate a transformer module into the conditional adversarial network architecture to get the refined warped image. Our method can back propagate the multi-spectral registration loss and achieve end-to-end training. Experiments on our multi -spectral dataset demonstrate that this approach is effective and robust, which outperforms other state-of-the-art methods. © 2018 IEEE.","Deep learning; Image registration; Infrared imaging; Spectroscopy; Adversarial networks; Complex scenes; Infrared and visible image; Learning architectures; Multi-spectral; Multi-spectrum; Multispectral images; State-of-the-art methods; Network architecture","Generative Adversarial Networks; Image Registration; Infrared Image; Multi-spectrum","Conference paper","Final","","Scopus","2-s2.0-85062916597"
"Gandikota R.; Radha Krishna K.; Sharma A.; ManjuSarma M.; Bothale V.M.","Gandikota, Rohit (57212350460); Radha Krishna, K. (57222241774); Sharma, Anupama (55605769875); ManjuSarma, M. (57222244883); Bothale, Vinod M (6505542077)","57212350460; 57222241774; 55605769875; 57222244883; 6505542077","RTC-GAN: REAL-TIME CLASSIFICATION of SATELLITE IMAGERY USING DEEP GENERATIVE ADVERSARIAL NETWORKS with INFUSED SPECTRAL INFORMATION","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323363","6993","6996","3","10.1109/IGARSS39084.2020.9323363","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101979819&doi=10.1109%2fIGARSS39084.2020.9323363&partnerID=40&md5=6e47f541ca713a2956765bbf3703c248","This paper implements a deep learning-based Convolutional Neural Network (CNN) with adversarial training and infused pixel information to classify multi-spectral data into 4 LULC classes and cloud. The network is capable of classifying the image on a real-time basis at acquisition time in pixel level by considering the various spectral band values at the pixel and a spatial region around the pixel to collect the spatial features. This way, both spatial information, and spectral information are considered to classify the image. This novel GAN architecture named RTC-GAN is generalized over all the satellites that have their sensors in and around standard NIR, R and G spectral bands while being able to classify the images in realtime. This network is realized and tested on data obtained from satellites Landsat 8 Sentinel2 and Indian Remote Sensing (IRS) satellites like Cartosat-2S, Resourcesat-2/2A. The dataset is not open-sourced and hence very minimal information is provided regarding the IRS data. © 2020 IEEE.","Convolutional neural networks; Deep learning; Geology; Image classification; Pixels; Remote sensing; Satellite imagery; Acquisition time; Adversarial networks; Minimal information; Multi-spectral data; Pixel information; Spatial features; Spatial informations; Spectral information; Classification (of information)","Cartosat; Generative Adversarial Networks; Landsat8; LULC; Real-Time Analysis; Satellite Imagery","Conference paper","Final","","Scopus","2-s2.0-85101979819"
"","","","5th International Conference on Machine Vision and Information Technology, CMVIT 2021","2021","Journal of Physics: Conference Series","1880","1","","","","286","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105483234&partnerID=40&md5=c8e90ba1c150e8d31ff95138411efca2","The proceedings contain 37 papers. The topics discussed include: a brief analysis of intelligent voice technology for China's mainstream media content production and distribution implications - take the application of IFLYTEK series products in mainstream media as an example; time-frequency bandpass filter with nonstationary signal decomposition application; design and implementation of wireless identification and sensing platform in structure monitoring; dynamic scene deblurring of multi-scale progressive attention network; an occlusion handling evaluation criterion for deep learning object segmentation; image inpainting of multi-spectral image with laser lines based on generative adversarial network; research on intelligent damage assessment system for time-sharing rental vehicles based on image recognition; and dynamic degradation quantification of wind turbine high speed shaft bearing based on oscillation based sparsity indices.","","","Conference review","Final","","Scopus","2-s2.0-85105483234"
"Lore K.G.; Reddy K.K.; Giering M.; Bernal E.A.","Lore, Kin Gwn (57190128995); Reddy, Kishore K. (55443153000); Giering, Michael (56464245300); Bernal, Edgar A. (12800366000)","57190128995; 55443153000; 56464245300; 12800366000","Generative adversarial networks for spectral super-resolution and bidirectional rgb-to-multispectral mapping","2019","IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","2019-June","","9025450","926","933","7","10.1109/CVPRW.2019.00122","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071519076&doi=10.1109%2fCVPRW.2019.00122&partnerID=40&md5=a0acb08f88953b4bf73f88bc6b5ad48a","Acquisition of multi-and hyperspectral imagery imposes significant requirements on the hardware capabilities of the sensors involved. In order to keep costs manageable, and due to limitations in the sensing technology, tradeoffs between the spectral and the spatial resolution of hyperspectral images are usually made. Such tradeoffs are usually not necessary when considering acquisition of traditional RGB imagery. We investigate the use of statistical learning, and in particular, of conditional generative adversarial networks (cGANs) to estimate mappings from three-channel RGB to 31-band multispectral imagery. We demonstrate the application of the proposed approach to (i) RGB-to-multispectral image mapping, (ii) spectral super-resolution of image data, and (iii) recovery of RGB imagery from multispectral data. © 2019 IEEE.","Computer vision; Optical resolving power; Remote sensing; Spectroscopy; Adversarial networks; Hyper-spectral imageries; Multi-spectral data; Multi-spectral imagery; Multispectral images; Sensing technology; Spatial resolution; Statistical learning; Photomapping","","Conference paper","Final","","Scopus","2-s2.0-85071519076"
"Jamali A.; Mahdianpari M.","Jamali, Ali (56909712300); Mahdianpari, Masoud (57190371939)","56909712300; 57190371939","A cloud-based framework for large-scale monitoring of ocean plastics using multi-spectral satellite imagery and generative adversarial network","2021","Water (Switzerland)","13","18","2553","","","","10.3390/w13182553","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115435905&doi=10.3390%2fw13182553&partnerID=40&md5=7aa145df5785a2f6a098be414387695c","Marine debris is considered a threat to the inhabitants, as well as the marine environments. Accumulation of marine debris, besides climate change factors, including warming water, sea-level rise, and changes in oceans’ chemistry, are causing the potential collapse of the marine environment’s health. Due to the increase of marine debris, including plastics in coastlines, ocean and sea surfaces, and even in deep ocean layers, there is a need for developing new advanced technology for the detection of large-sized marine pollution (with sizes larger than 1 m) using state-of-the-art remote sensing and machine learning tools. Therefore, we developed a cloud-based framework for large-scale marine pollution detection with the integration of Sentinel-2 satellite imagery and advanced machine learning tools on the Sentinel Hub cloud application programming interface (API). Moreover, we evaluated the performance of two shallow machine learning algorithms of random forest (RF) and support vector machine (SVM), as well as the deep learning method of the generative adversarial network-random forest (GAN-RF) for the detection of ocean plastics in the pilot site of Mytilene Island, Greece. Based on the obtained results, the shallow algorithms of RF and SVM achieved an overall accuracy of 88% and 84%, respectively, with available training data of plastic debris. The GAN-RF classifier improved the detection of ocean plastics of the RF method by 8%, achieving an overall accuracy of 96% by generating several synthetic ocean plastic samples. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Aegean Islands; Greece; Lesbos; Northern Aegean; Application programming interfaces (API); Classification (of information); Climate change; Debris; Decision trees; Deep learning; Elastomers; Learning algorithms; Marine pollution; Oil spills; Radiometers; Remote sensing; Satellite imagery; Sea level; Support vector machines; Cloud-based; Large-scale monitoring; Marine debris; Marine environment; Multi-spectral; Ocean plastic; Overall accuracies; Random forests; Sentinel hub; Support vectors machine; accuracy assessment; advanced technology; climate change; debris flow; deep water; marine environment; marine pollution; monitoring; monitoring system; plastic waste; satellite imagery; sea surface; Sentinel; spectral analysis; support vector machine; warming; Generative adversarial networks","Generative adversarial network; Marine debris; Marine pollution; Ocean plastics; Random forest; Sentinel Hub; Support vector machine","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85115435905"
"Wang L.; Xu X.; Yu Y.; Yang R.; Gui R.; Xu Z.; Pu F.","Wang, Lei (57211488504); Xu, Xin (56294598500); Yu, Yue (57214104632); Yang, Rui (57208294306); Gui, Rong (57211231417); Xu, Zhaozhuo (57171068000); Pu, Fangling (13408173100)","57211488504; 56294598500; 57214104632; 57208294306; 57211231417; 57171068000; 13408173100","SAR-to-optical image translation using supervised cycle-consistent adversarial networks","2019","IEEE Access","7","","8825802","129136","129149","13","10.1109/ACCESS.2019.2939649","70","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078034428&doi=10.1109%2fACCESS.2019.2939649&partnerID=40&md5=a6086960f8b2c0f3cb16077d491dcb0b","Optical remote sensing (RS) data suffer from the limitation of bad weather and cloud contamination, whereas synthetic aperture radar (SAR) can work under all weather conditions and overcome this disadvantage of optical RS data. However, due to the imaging mechanism of SAR and the speckle noise, untrained people are difficult to recognize the land cover types visually from SAR images. Inspired by the excellent image-to-image translation performance of Generative Adversarial Networks (GANs), a supervised Cycle-Consistent Adversarial Network (S-CycleGAN) was proposed to generate large optical images from the SAR images. When the optical RS data are unavailable or partly unavailable, the generated optical images can be alternative data that aid in land cover visual recognition for untrained people. The main steps of SAR-to-optical image translation were as follows. First, the large SAR image was split to small patches. Then S-CycleGAN was used to translate the SAR patches to optical image patches. Finally, the optical image patches were stitched to generate the large optical image. A paired SAR-optical image dataset which covered 32 Chinese cities was published to evaluate the proposed method. The dataset was generated from Sentinel-1 (SEN-1) SAR images and Sentinel-2 (SEN-2) multi-spectral images. S-CycleGAN was applied to two experiments, which were SAR-to-optical image translation and cloud removal, and the results showed that S-CycleGAN could keep both the land cover and structure information well, and its performance was superior to some famous image-to-image translation models. © 2013 IEEE.","Flow visualization; Geometrical optics; Remote sensing; Spectroscopy; Synthetic aperture radar; Adversarial networks; Cloud contamination; Cloud removal; Multispectral images; Optical image; Optical remote sensing; Sentinel; Structure information; Radar imaging","cloud removal; GAN; SAR-to-optical image translation; Sentinel; visualization","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078034428"
"Lu L.; Zhu H.; Dong J.; Ju Y.; Zhou H.","Lu, Liang (57201005874); Zhu, Hongbao (57208780202); Dong, Junyu (22634069200); Ju, Yakun (57197800254); Zhou, Huiyu (23062556900)","57201005874; 57208780202; 22634069200; 57197800254; 23062556900","Three-dimensional reconstruction with a laser line based on image in-painting and multi-spectral photometric stereo","2021","Sensors","21","6","2131","1","18","17","10.3390/s21062131","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102573052&doi=10.3390%2fs21062131&partnerID=40&md5=29954801d13c9eb86120377265f81f36","This paper presents a multi-spectral photometric stereo (MPS) method based on image in-painting, which can reconstruct the shape using a multi-spectral image with a laser line. One of the difficulties in multi-spectral photometric stereo is to extract the laser line because the required illumination for MPS, e.g., red, green, and blue light, may pollute the laser color. Unlike previous methods, through the improvement of the network proposed by Isola, a Generative Adversarial Network based on image in-painting was proposed, to separate a multi-spectral image with a laser line into a clean laser image and an uncorrupted multi-spectral image without the laser line. Then these results were substituted into the method proposed by Fan to obtain high-precision 3D reconstruction results. To make the proposed method applicable to real-world objects, a rendered image dataset obtained using the rendering models in ShapeNet has been used for training the network. Evaluation using the rendered images and real-world images shows the superiority of the proposed approach over several previous methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Image enhancement; Image reconstruction; Photometry; Rendering (computer graphics); Spectroscopy; 3D reconstruction; Adversarial networks; Multispectral images; Photometric stereo; Real-world image; Real-world objects; Rendered images; Three-dimensional reconstruction; Stereo image processing","And laser extraction; Generative adversarial network; Image in-painting; Multi-spectral photometric stereo","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102573052"
"Romero L.S.; Marcello J.; Vilaplana V.","Romero, Luis Salgueiro (57218455911); Marcello, Javier (6602158797); Vilaplana, Verónica (23394280500)","57218455911; 6602158797; 23394280500","Super-resolution of Sentinel-2 imagery using generative adversarial networks","2020","Remote Sensing","12","15","2424","","","","10.3390/RS12152424","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089853089&doi=10.3390%2fRS12152424&partnerID=40&md5=6bb6fc362a861fc8952daf3ca71d5b36","Sentinel-2 satellites provide multi-spectral optical remote sensing images with four bands at 10 m of spatial resolution. These images, due to the open data distribution policy, are becoming an important resource for several applications. However, for small scale studies, the spatial detail of these images might not be sufficient. On the other hand, WorldView commercial satellites offer multi-spectral images with a very high spatial resolution, typically less than 2 m, but their use can be impractical for large areas or multi-temporal analysis due to their high cost. To exploit the free availability of Sentinel imagery, it is worth considering deep learning techniques for single-image super-resolution tasks, allowing the spatial enhancement of low-resolution (LR) images by recovering high-frequency details to produce high-resolution (HR) super-resolved images. In this work, we implement and train a model based on the Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) with pairs of WorldView-Sentinel images to generate a super-resolved multispectral Sentinel-2 output with a scaling factor of 5. Our model, named RS-ESRGAN, removes the upsampling layers of the network to make it feasible to train with co-registered remote sensing images. Results obtained outperform state-of-the-art models using standard metrics like PSNR, SSIM, ERGAS, SAM and CC. Moreover, qualitative visual analysis shows spatial improvements as well as the preservation of the spectral information, allowing the super-resolved Sentinel-2 imagery to be used in studies requiring very high spatial resolution. © 2020 by the authors.","Deep learning; Image analysis; Image resolution; Network layers; Open Data; Optical resolving power; Remote sensing; Spectroscopy; Commercial satellites; Data distribution policies; Low resolution images; Multi-temporal analysis; Optical remote sensing; Remote sensing images; Spectral information; Very high spatial resolutions; Image enhancement","Deep learning; Generative adversarial network; Sentinel-2; Super-resolution; WorldView","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85089853089"
"Ren C.X.; Ziemann A.; Theiler J.; Durieux A.M.S.","Ren, Christopher X. (57004276700); Ziemann, Amanda (36134071500); Theiler, James (7004449154); Durieux, Alice M.S. (57212026338)","57004276700; 36134071500; 7004449154; 57212026338","Cycle-Consistent Adversarial Networks for Realistic Pervasive Change Generation in Remote Sensing Imagery","2020","Proceedings of the IEEE Southwest Symposium on Image Analysis and Interpretation","2020-March","","9094603","42","45","3","10.1109/SSIAI49293.2020.9094603","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085490462&doi=10.1109%2fSSIAI49293.2020.9094603&partnerID=40&md5=17ae73c5ca32f7d9f596595252ae0d13","This paper introduces a new method of generating realistic pervasive changes in the context of evaluating the effectiveness of change detection algorithms in controlled settings. The method-a cycle-consistent adversarial network (CycleGAN)-requires low quantities of training data to generate realistic changes. Here we show an application of CycleGAN in creating realistic snow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how these images can be used as a test bed for anomalous change detection algorithms. © 2020 IEEE.","Remote sensing; Signal detection; Adversarial networks; Change detection algorithms; Multi-spectral; Remote sensing imagery; Training data; Image analysis","change detection; deep learning; generative adversarial networks; image analysis; multispectral; remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085490462"
"Wang Y.; Xie Y.; Wu Y.; Liang K.; Qiao J.","Wang, Yajie (55734159700); Xie, Yanyan (57553191200); Wu, Yanyan (55977823200); Liang, Kai (57552266200); Qiao, Jilin (57552266300)","55734159700; 57553191200; 55977823200; 57552266200; 57552266300","An Unsupervised Multi-scale Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13142 LNCS","","","356","368","12","10.1007/978-3-030-98355-0_30","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127094685&doi=10.1007%2f978-3-030-98355-0_30&partnerID=40&md5=83048658bff66110025738d16f575ad4","Pan-sharpening of remote sensing images is an effective method to get high spatial resolution multi-spectral (HRMS) images by fusing low spatial resolution multi-spectral (LRMS) images and high spatial resolution panchromatic (PAN) images. Recently, many remote sensing images pan-sharpening methods based on convolutional neural networks (CNN) have been proposed and achieved excellent performance. However, two drawbacks still exist. On the one hand, since there are no ideal HRMS images as targets for learning, most existing methods require an extra effort to produce the simulated data for training. On the other hand, these methods ignore the local features of the original images. To address these issues, we propose an unsupervised multi-scale generative adversarial network method, which can train directly on the full-resolution images without down-sampling. Firstly, a multi-scale dense generator network is proposed to extract features from the original images to generate HRMS images. Secondly, two discriminators are used to protect the spectral information of LRMS images and spatial information of PAN images, respectively. Finally, to improve the quality of the fused image and implement training under the unsupervised setting, a new loss function is proposed. Experimental results based on QuickBird and GaoFen-2 data sets demonstrate that the proposed method can obtain much better fusion results for the full-resolution images. © 2022, Springer Nature Switzerland AG.","Convolutional neural networks; Image enhancement; Image resolution; Remote sensing; Full resolution images; High spatial resolution; Multi-scales; Multispectral images; Original images; Pan-sharpening; Remote sensing images; Remote-sensing; Spatial resolution; Unsupervised; Generative adversarial networks","Full-resolution images; Generative adversarial network; Pan-sharpening; Remote sensing; Unsupervised","Conference paper","Final","","Scopus","2-s2.0-85127094685"
"Girla I.-A.; Neagoe V.-E.","Girla, Ionut-Alexandru (57271086400); Neagoe, Victor-Emil (56005805800)","57271086400; 56005805800","A Weakly-Supervised Change Detection for Multispectral Earth Observation Imagery using a Long Short-Term Memory Classifier with a Virtual Training Data Neural Generator","2022","14th International Conference on Communications, COMM 2022 - Proceedings","","","","","","","10.1109/COMM54429.2022.9817344","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135194582&doi=10.1109%2fCOMM54429.2022.9817344&partnerID=40&md5=6396e10ae09bd132aeb5e1d1a2c45f31","This paper proposes a novel approach to improve accuracy of weakly-supervised change detection for multispectral Earth Observation (EO) imagery. The method is based on the idea to use an initial small-size EO labeled dataset to generate a larger set of virtual data. We have considered two variants of virtual data-generators based on the general architecture called Generative Adversarial Network (GAN): MLPGAN and LSGAN. The resulting virtual dataset is used to train a simple Long Short-Term Memory (LSTM) classifier. The proposed method is evaluated using the Mexico dataset acquired by the Thematic Mapper (TM) sensor of the Landsat 5 satellite. For each acquisition date, two spectral bands are considered (B4, B5). The two images have been acquired in April 2000 and May 2002, respectively. We have evaluated the change detection performances (OA, Kappa, MAR, and FAR) using two virtual data generators corresponding to considered GAN architectures. As a benchmark method, we have considered the case when the LSTM classifier is trained with the original small-size dataset without synthetic data generation.  © 2022 IEEE.","Brain; Change detection; Classification (of information); Generative adversarial networks; Image enhancement; Memory architecture; Network architecture; Data generation; Earth observations; Generative adversarial network; Long short term memory  classifier; Multi-spectral; Supervised change detection; Synthetic training data; Synthetic training data generation; Virtual data; Weakly-supervised change detection; Long short-term memory","Generative Adversarial Network (GAN); Long Short Term Memory (LSTM) classifier; synthetic training data generation (STDG); Weakly-supervised change detection","Conference paper","Final","","Scopus","2-s2.0-85135194582"
"Chen Y.; Xu Y.-M.; Di Y.-J.; Cui X.-N.; Zhang J.; Zhou X.-D.; Xiao C.-Y.; Li S.-H.","Chen, Ying (55796483000); Xu, Yang-Mei (57207828557); Di, Yuan-Jian (57193879680); Cui, Xing-Ning (57194236469); Zhang, Jie (57216516975); Zhou, Xin-De (57213830709); Xiao, Chun-Yan (24765782900); Li, Shao-Hua (57208221648)","55796483000; 57207828557; 57193879680; 57194236469; 57216516975; 57213830709; 24765782900; 57208221648","COD Concentration Prediction Model Based on Multi-Spectral Data Fusion and GANs Algorithm; [多光谱数据融合和GANs算法的COD浓度预测]","2021","Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis","41","1","","188","193","5","10.3964/j.issn.1000-0593(2021)01-0188-06","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099633503&doi=10.3964%2fj.issn.1000-0593%282021%2901-0188-06&partnerID=40&md5=e9ae16922f3392eaf43b04767a11ff03","Excessive concentration of organic pollutants in water is harmful, which causes not only serious environmental pollution but also endangers human health. Chemical oxygen demand (COD) can be used to characterize the pollution degree of organic pollutants in water. A quantitative prediction model of COD concentration based on generative adversarial networks (GANs) algorithm is proposed, which combines ultraviolet (UV) and Near Infrared (NIR) spectra with data-level fusion (DLDF) and feature level data fusion (FLDF). In this paper, firstly, COD standard samples are prepared according to a certain concentration gradient, and the ultraviolet spectrum (190~310 nm) and near-infrared spectrum (830~2 100 nm) of the standard sample are collected respectively. The first derivative and Savitzky-Golay (S-G) smoothing pretreatment of the obtained ultraviolet and near-infrared spectrum data are carried out to eliminate the baseline drift of the spectrum and the interference noise. Then, the data fusion of data level and featural level are carried out directly basing on the pretreated ultraviolet and near-infrared spectra, and the COD concentration prediction model is constructed by GANs algorithm. The model is evaluated by using the square of the correlation coefficient of the evaluation parameters (R2), the mean square root error of the predicted value and the real concentration value (RMSEP) and the prediction deviation. The results show that neither FLDF model nor DLDF model is not ideal. The analysis shows that the model contribution of the ultraviolet spectrum is concealed in the near-infrared band due to the unbalanced data in the ultraviolet and near-infrared bands, which makes the spectral fusion meaningless. In order to avoid the problem of fusion failure, the normalizat-ion method is proposed to deal with the mixed spectrum in the text. The effects of standard normal variation (SNV), maximum and minimum normalization (MMN) and vector normalization (VN) on the modeling are discussed. Then the normalized ultraviolet and near-infrared spectral data are fused again under the given sub-interval number, the input X of GAN model is taken as the input X, and the real measured COD value is taken as the output Y. The prediction models of COD concentration are established after different normalization methods. The modeling results show that different normalization methods have a great influence on the hybrid spectral data fusion model, and the prediction accuracy of the data-level fusion model and the feature-level fusion model is significantly improved before it is normalized, among which the prediction model with the maximum and minimum normalization is the most obvious. Finally, in order to verify the accuracy of the multi-spectral data fusion GANs Prediction model, the GANs prediction model of the full wavelength ultraviolet band of a single spectral source and the GANs prediction model of the full wavelength near-infrared band of a single spectral source are established. The experimental results show that the correlation coefficient of the characteristic level spectral fusion model basing on the ultraviolet and near-infrared spectra is 0.994 7, the prediction mean square root error is 0.976, the prediction model error is reduced by 52.9% comparing with the data level fusion, and the predicted recovery rate is 98.4%~103.1%, which is much better than the other groups. The generalization ability of the model is strong and the prediction accuracy is high. Compared with the monitoring model of single spectral source, the data fusion of mixed spectra can reflect more the chemical information of water samples, and reveals the pollutant degree of a water body more comprehensively, reflects the difference of pollutants in a water body from different levels, provides some technical support for on-line monitoring of COD concentration in water. © 2021, Peking University Press. All right reserved.","Chemical oxygen demand; Data fusion; Errors; Forecasting; Infrared devices; Near infrared spectroscopy; Organic pollutants; Ultraviolet spectroscopy; Water pollution; Concentration gradients; Concentration prediction; Correlation coefficient; Environmental pollutions; Mean square root error; Near infrared spectral; Quantitative prediction; Ultra-violet spectrums; Predictive analytics","COD concentration prediction; Data fusion; GANs model; NIR spectrum; UV spectrum","Article","Final","","Scopus","2-s2.0-85099633503"
"Li F.; Ma L.; Cai J.","Li, Feimo (57190281893); Ma, Lei (27171956800); Cai, Jian (57207874020)","57190281893; 27171956800; 57207874020","Multi-discriminator generative adversarial network for high resolution gray-scale satellite image colorization","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8517930","3489","3492","3","10.1109/IGARSS.2018.8517930","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063144299&doi=10.1109%2fIGARSS.2018.8517930&partnerID=40&md5=5ed32be21b7dbc7734245b58ace7c2f0","Automatic colorization for grayscale satellite images can help with eliminating lighting differences between multi-spectral captures, and provides strong prior information for ground type classification and object detection. In this paper, we introduced a novel generative adversarial network with multiple discriminators for colorizing gray-scale satellite images with pseudo-natural appearances. Although being powerful, deep generative model in its common form with a single discriminator could be unstable for achieving spatial consistency on local textured regions, especially highly textured ones. To address this issue, the generator in our proposed structure produces a group of colored outputs from feature maps at different scale levels of the network, each being supervised by an independent discriminator to fit the original colored training input in discrete Lab color space. The final colored output is a cascaded ensemble of these preceding by-products via summation, thus the fitting errors are reduced by a geometric series form. Quantitative and qualitative comparisons with the sole-discriminator version have been performed on high-resolution satellite images in experiments, where significant reductions in prediction errors have been observed. © 2018 IEEE","","Generative adversarial network; Gray-scale satellite images; Multiple discriminators; Pseudo-natural colorization","Conference paper","Final","","Scopus","2-s2.0-85063144299"
"Hasan C.; Horne R.; Mauw S.; Mizera A.","Hasan, Cengis (34881811700); Horne, Ross (55249833700); Mauw, Sjouke (6601991482); Mizera, Andrzej (35410075000)","34881811700; 55249833700; 6601991482; 35410075000","Cloud removal from satellite imagery using multispectral edge-filtered conditional generative adversarial networks","2022","International Journal of Remote Sensing","43","5","","1881","1893","12","10.1080/01431161.2022.2048915","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127289358&doi=10.1080%2f01431161.2022.2048915&partnerID=40&md5=a7c093b8e4eb27bce3040f97716bfa67","We propose a Generative Adversarial Network (GAN) based architecture for removing clouds from satellite imagery. Data used for training comprises of visible light RGB and near-infrared (NIR) band images. The novelty lies in the structure of the discriminator in the GAN architecture, which compares generated and target cloud-free RGB images concatenated with their edge-filtered versions. Experimental results show that our approach to removing clouds outperforms both visually and according to metrics, a benchmark solution that does not take edge filtering into account, and that improvements are robust when varying both training dataset size and NIR cloud penetrability. © 2022 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Infrared devices; Network architecture; Satellite imagery; Benchmark solutions; Cloud removal; Edge filtering; Multi-spectral; Near infrared band; Network-based architectures; Removing cloud; RGB images; Training dataset; Visible light; comparative study; image analysis; image classification; image resolution; satellite data; satellite imagery; Generative adversarial networks","","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127289358"
"Jiang F.; Gong M.; Zhan T.; Fan X.","Jiang, Fenlong (57210730944); Gong, Maoguo (8933846400); Zhan, Tao (57052462500); Fan, Xiaolong (57209827950)","57210730944; 8933846400; 57052462500; 57209827950","A Semisupervised GAN-Based Multiple Change Detection Framework in Multi-Spectral Images","2020","IEEE Geoscience and Remote Sensing Letters","17","7","8854295","1223","1227","4","10.1109/LGRS.2019.2941318","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090386350&doi=10.1109%2fLGRS.2019.2941318&partnerID=40&md5=9996c3aeb4484448b04822ecaf9650ad","Effectively highlighting multiple changes in the earth surface from multi-temporal remote sensing images is a meaningful but challenging task. In order to reduce costs and ensure the performance, it is advisable to employ a semisupervised strategy to achieve this goal. As a discriminative joint classification task, semisupervised change detection aims to extract useful and discriminative features from a large amount of unlabeled data in addition to limited labeled samples. The discriminator of a well-trained generative adversarial network (GAN) is just right for this. Therefore, in this letter, we proposed a semisupervised GAN-based multiple change detection framework for multi-spectral images. First, the GAN is trained by all data without any prior information. Then, we combine two identical trained discriminators to construct a dual-pipeline joint classifier. Finally, the classifier is fine-tuned by a very small amount of labeled data to detect multiple changes. The superior performance of the proposed model over both real multi-spectral data sets demonstrates its robustness and effectiveness.  © 2004-2012 IEEE.","Pipelines; Spectroscopy; Adversarial networks; Change detection; Classification tasks; Discriminative features; Multi-spectral data; Multi-temporal remote sensing; Multispectral images; Prior information; algorithm; detection method; image analysis; spectral analysis; supervised learning; Remote sensing","Generative adversarial network (GAN); multi-spectral images; multiple change detection; semisupervised","Article","Final","","Scopus","2-s2.0-85090386350"
"Zhao Z.; Zhang J.; Xu S.; Sun K.; Huang L.; Liu J.; Zhang C.","Zhao, Zixiang (57218542866); Zhang, Jiangshe (9737712100); Xu, Shuang (56367405500); Sun, Kai (57193093191); Huang, Lu (57222016088); Liu, Junmin (42761838200); Zhang, Chunxia (55703936800)","57218542866; 9737712100; 56367405500; 57193093191; 57222016088; 42761838200; 55703936800","FGF-GAN: A LIGHTWEIGHT GENERATIVE ADVERSARIAL NETWORK FOR PANSHARPENING VIA FAST GUIDED FILTER","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428272","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126434284&doi=10.1109%2fICME51207.2021.9428272&partnerID=40&md5=8d20df2cce1d516406276ae77807356d","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters. © 2021 IEEE Computer Society. All rights reserved.","Deep learning; Image enhancement; Image fusion; Remote sensing; Spectroscopy; Channel dimension; Fast guided filter; Guided filters; High resolution; Input image; Lower resolution; Multispectral images; Pan-sharpening; Remote-sensing; Single channels; Generative adversarial networks","Fast guided filter; Generative adversarial network; Image fusion; Pansharpening","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126434284"
"He L.; Zhang W.; Shi J.; Li F.","He, Lijun (57887558200); Zhang, Wanyue (57219787617); Shi, Jiankang (57887558300); Li, Fan (57888468700)","57887558200; 57219787617; 57887558300; 57888468700","Cross-Domain Association Mining Based Generative Adversarial Network for Pansharpening","2022","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15","","","7770","7783","13","10.1109/JSTARS.2022.3204824","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137849856&doi=10.1109%2fJSTARS.2022.3204824&partnerID=40&md5=461cb4f06f8186ace43da392250266df","Multispectral (MS) pansharpening can improve the spatial resolution of MS images, which plays an increasingly important role in agriculture and environmental monitoring. Existing neural network-based methods tend to focus on global features of images, without considering the inherent relationships between similar substances in MS images. However, there is a high probability that different substances at the junction mix with each other, which leads to spectral distortion in the final pansharpened image. In this article, we propose a cross-domain association mining-based generative adversarial network for pansharpening, which consists of a spectral fidelity generator and dual discriminators. In our spectral fidelity generator, the cross-region similarity attention module is designed to establish dependencies between similar substances at different positions in the image, thereby leveraging the similar spectral features to generate pansharpened images with better spectral preservation. To mine the potential relationship between the MS image domain and the panchromatic image domain, we pretrain a spatial information extraction network. The network is then transferred to the dual-discriminator architecture to obtain the spatial information of the pansharpened images more accurately and prevent the loss of spatial details. The experimental results show that our method outperforms several state-of-the-art pansharpening methods in both quantitative and qualitative evaluations. © 2008-2012 IEEE.","Deep learning; Discriminators; Image enhancement; Image resolution; Deep learning; Dual discriminator; Features extraction; Generator; Image association; Junction; Multi-spectral; Multispectral  pansharpening; Pan-sharpening; Spatial resolution; Superresolution; image classification; machine learning; network analysis; qualitative analysis; quantitative analysis; remote sensing; satellite data; satellite imagery; Generative adversarial networks","Deep learning; dual discriminators; image association; multispectral (MS) pansharpening","Article","Final","","Scopus","2-s2.0-85137849856"
"Gao M.; Zhou Y.; Zhai W.; Zeng S.; Li Q.","Gao, Mingliang (26634962800); Zhou, Yi’nan (58078506800); Zhai, Wenzhe (57252998400); Zeng, Shuai (58079513100); Li, Qilei (57202858223)","26634962800; 58078506800; 57252998400; 58079513100; 57202858223","SaReGAN: a salient regional generative adversarial network for visible and infrared image fusion","2023","Multimedia Tools and Applications","","","","","","","10.1007/s11042-023-14393-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146862705&doi=10.1007%2fs11042-023-14393-2&partnerID=40&md5=2051842f336d33d8f066a8e91d48f344","Multispectral image fusion plays a crucial role in smart city environment safety. In the domain of visible and infrared image fusion, object vanishment after fusion is a key problem which restricts the fusion performance. To address this problem, a novel Salient Regional Generative Adversarial Network GAN (SaReGAN) is presented for infrared and VIS image fusion. The SaReGAN consists of three parts. In the first part, the salient regions of infrared image are extracted by visual saliency map and the information of these regions is preserved. In the second part, the VIS image, infrared image and salient information are merged thoroughly in the generator to gain a pre-fused image. In the third part, the discriminator attempts to differentiate the pre-fused image and VIS image, in order to learn details from VIS image based on the adversarial mechanism. Experimental results verify that the SaReGAN outperforms other state-of-the-art methods in quantitative and qualitative evaluations. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image fusion; Infrared imaging; Smart city; Fused images; Fusion performance; Image-based; Infrared image fusions; Learn+; Multi-spectral image fusions; Saliency map; Salient regions; Visible and infrared image; Visual saliency; Generative adversarial networks","Generative adversarial network; Image fusion; Salient region; Smart city; Visible and infrared image","Article","Article in press","","Scopus","2-s2.0-85146862705"
"Polewski P.; Shelton J.; Yao W.; Heurich M.","Polewski, P. (25825460700); Shelton, J. (38362455600); Yao, W. (57220597141); Heurich, M. (23568273000)","25825460700; 38362455600; 57220597141; 23568273000","SEGMENTATION of SINGLE STANDING DEAD TREES in HIGH-RESOLUTION AERIAL IMAGERY with GENERATIVE ADVERSARIAL NETWORK-BASED SHAPE PRIORS","2020","International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","43","B2","","717","723","6","10.5194/isprs-archives-XLIII-B2-2020-717-2020","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091078721&doi=10.5194%2fisprs-archives-XLIII-B2-2020-717-2020&partnerID=40&md5=39fd596dce8137a0a0e56b6de1e929d2","The use of multispectral imagery for monitoring biodiversity in ecosystems is becoming widespread. A key parameter of forest ecosystems is the distribution of dead wood. This work addresses the segmentation of individual dead tree crowns in nadir-view aerial infrared imagery. While dead vegetation produces a distinct spectral response in the near infrared band, separating adjacent trees within large swaths of dead stands remains a challenge. We tackle this problem by casting the segmentation task within the active contour framework, a mathematical formulation combining learned models of the object's shape and appearance as prior information. We explore the use of a deep convolutional generative adversarial network (DCGAN) in the role of the shape model, replacing the original linear mixture-of-eigenshapes formulation. Also, we rely on probabilities obtained from a deep fully convolutional network (FCN) as the appearance prior. Experiments conducted on manually labeled reference polygons show that the DCGAN is able to learn a low-dimensional manifold of tree crown shapes, outperforming the eigenshape model with respect to the similarity of the reproduced and referenced shapes on about 45&thinsp;% of the test samples. The DCGAN is successful mostly for less convex shapes, whereas the baseline remains superior for more regular tree crown polygons. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.","Aerial photography; Antennas; Biodiversity; Convolution; Convolutional neural networks; Ecosystems; Infrared devices; Adversarial networks; Convolutional networks; High resolution aerial imagery; Low-dimensional manifolds; Mathematical formulation; Multi-spectral imagery; Near infrared band; Standing dead trees; Forestry","active contour; CNN; color infrared; forest health monitoring; GAN","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091078721"
"Guei A.-C.; Akhloufi M.","Guei, Axel-Christian (57202650067); Akhloufi, Moulay (23979609400)","57202650067; 23979609400","Deep learning enhancement of infrared face images using generative adversarial networks","2018","Applied Optics","57","18","","D98","D107","9","10.1364/AO.57.000D98","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048976871&doi=10.1364%2fAO.57.000D98&partnerID=40&md5=7faaff084361988d856b545aec675939","This work presents a deep learning framework based on the use of deep convolutional generative adversarial networks (DCGAN) for infrared face image super-resolution. We use DCGAN for upscaling the images by a factor of 4 × 4, starting at a size of 16 × 16 and obtaining a 64 × 64 face image. Tests are conducted using different infrared face datasets operating in the near-infrared (NIR) and the long-wave infrared (LWIR) spectrum. We can see that the proposed framework performs well and preserves important details of the face. This kind of approach can be very useful in security applications where we can scan faces in the crowd or detect faces at a distance and upscale them for further recognition through an infrared or a multispectral face recognition system. © 2018 Optical Society of America.","Algorithms; Databases as Topic; Face; Female; Humans; Image Processing, Computer-Assisted; Infrared Rays; Machine Learning; Male; Neural Networks (Computer); Signal-To-Noise Ratio; Face recognition; Image enhancement; Infrared devices; Infrared radiation; Adversarial networks; Face recognition systems; Infrared face images; Learning enhancements; Learning frameworks; Longwave infrared; Multi-spectral; Security application; algorithm; anatomy and histology; artificial neural network; data base; face; female; human; image processing; infrared radiation; machine learning; male; signal noise ratio; Deep learning","","Article","Final","","Scopus","2-s2.0-85048976871"
"Kaza N.; Ojaghi A.; Costa P.C.; Robles F.E.","Kaza, Nischita (57216785493); Ojaghi, Ashkan (57211079053); Costa, Paloma Casteleiro (57222515991); Robles, Francisco E. (26530538400)","57216785493; 57211079053; 57222515991; 26530538400","Deep learning based virtual staining of label-free ultraviolet (UV) microscopy images for hematological analysis","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11655","","116550C","","","","10.1117/12.2576429","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107953172&doi=10.1117%2f12.2576429&partnerID=40&md5=696bc9336f9c61a979a0fa295c349e9d","Deep ultraviolet microscopy (UV) enables high-resolution, label-free imaging of biological samples and yields diagnostically relevant quantitative molecular and structural information. We recently demonstrated that deep UV microscopy can serve as a simple, fast, and low-cost alternative to modern hematology analyzers that assess variations in the morphological, molecular, and cytogenetic properties of blood cells to monitor and diagnose blood disorders. We also introduced a pseudocolorization scheme that uses multi-spectral UV images (acquired at three different wavelengths) to generate images whose colors accurately recapitulate those produced by conventional Giemsa staining, and can thus be used for visual hematological analysis. Here, we present a deep-learning framework to virtually stain single-channel UV images acquired at 260 nm, providing a factor of three improvement in imaging speed without sacrificing accuracy. We train a generative adversarial network (GAN) using image pairs consisting of single-channel UV images of blood smears and their corresponding pseudocolorized images to generate realistic, virtually stained images. The virtual stained images are post-processed to improve contrast and yield consistent background colors. We quantify the performance of our framework in terms of the structural similarity index (SSIM) for each color channel. Our virtual staining scheme is the first step towards a completely automated hematological analysis pipeline that includes segmentation and classification of different blood cell types to compute metrics of diagnostic value. Our method eliminates the need to acquire images at different wavelengths and could potentially lead to the development of a faster and more compact label-free, point-of-care hematology analyzer. © 2021 SPIE.","Blood; Cells; Color; Deep learning; E-learning; Image acquisition; Image analysis; Medical imaging; Uranium metallography; Vanadium metallography; Adversarial networks; Biological samples; Cytogenetic properties; Label-free imaging; Learning frameworks; Microscopy images; Structural information; Structural similarity indices (SSIM); Image enhancement","Deep learning; Hematology analysis; Label-free imaging; Ultraviolet microscopy; Virtual staining","Conference paper","Final","","Scopus","2-s2.0-85107953172"
"Requena-Mesa C.; Reichstein M.; Mahecha M.; Kraft B.; Denzler J.","Requena-Mesa, Christian (57208244647); Reichstein, Markus (57206534330); Mahecha, Miguel (16444433900); Kraft, Basil (57207884535); Denzler, Joachim (6701534437)","57208244647; 57206534330; 16444433900; 57207884535; 6701534437","Predicting Landscapes from Environmental Conditions Using Generative Networks","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11824 LNCS","","","203","217","14","10.1007/978-3-030-33676-9_14","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076136395&doi=10.1007%2f978-3-030-33676-9_14&partnerID=40&md5=9551024c6898e2bfe7c456c97c0667f6","Landscapes are meaningful ecological units that strongly depend on the environmental conditions. Such dependencies between landscapes and the environment have been noted since the beginning of Earth sciences and cast into conceptual models describing the interdependencies of climate, geology, vegetation and geomorphology. Here, we ask whether landscapes, as seen from space, can be statistically predicted from pertinent environmental conditions. To this end we adapted a deep learning generative model in order to establish the relationship between the environmental conditions and the view of landscapes from the Sentinel-2 satellite. We trained a conditional generative adversarial network to generate multispectral imagery given a set of climatic, terrain and anthropogenic predictors. The generated imagery of the landscapes share many characteristics with the real one. Results based on landscape patch metrics, indicative of landscape composition and structure, show that the proposed generative model creates landscapes that are more similar to the targets than the baseline models while overall reflectance and vegetation cover are predicted better. We demonstrate that for many purposes the generated landscapes behave as real with immediate application for global change studies. We envision the application of machine learning as a tool to forecast the effects of climate change on the spatial features of landscapes, while we assess its limitations and breaking points. © Springer Nature Switzerland AG 2019.","Climate change; Deep learning; Earth (planet); Pattern recognition; Vegetation; Adversarial networks; Conceptual model; Ecological units; Environmental conditions; Generative model; Landscape composition; Multi-spectral imagery; Spatial features; Climate models","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85076136395"
"Ma J.; Yu W.; Chen C.; Liang P.; Guo X.; Jiang J.","Ma, Jiayi (26638975600); Yu, Wei (56479633000); Chen, Chen (57192217138); Liang, Pengwei (57201500677); Guo, Xiaojie (36607970100); Jiang, Junjun (54902306100)","26638975600; 56479633000; 57192217138; 57201500677; 36607970100; 54902306100","Pan-GAN: An unsupervised pan-sharpening method for remote sensing image fusion","2020","Information Fusion","62","","","110","120","10","10.1016/j.inffus.2020.04.006","183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084634225&doi=10.1016%2fj.inffus.2020.04.006&partnerID=40&md5=dedd4ce965c307d547a20af2658b9785","Pan-sharpening in remote sensing image fusion refers to obtaining multi-spectral images of high-resolution by fusing panchromatic images and multi-spectral images of low-resolution. Recently, convolution neural network (CNN)-based pan-sharpening methods have achieved the state-of-the-art performance. Even though, two problems still remain. On the one hand, the existing CNN-based strategies require supervision, where the low-resolution multi-spectral image is obtained by simply blurring and down-sampling the high-resolution one. On the other hand, they typically ignore rich spatial information of panchromatic images. To address these issues, we propose a novel unsupervised framework for pan-sharpening based on a generative adversarial network, termed as Pan-GAN, which does not rely on the so-called ground-truth during network training. In our method, the generator separately establishes the adversarial games with the spectral discriminator and the spatial discriminator, so as to preserve the rich spectral information of multi-spectral images and the spatial information of panchromatic images. Extensive experiments are conducted to demonstrate the effectiveness of the proposed Pan-GAN compared with other state-of-the-art pan-sharpening approaches. Our Pan-GAN has shown promising performance in terms of qualitative visual effects and quantitative evaluation metrics. © 2020 Elsevier B.V.","Remote sensing; Spectroscopy; Adversarial networks; Convolution neural network; Multispectral images; Quantitative evaluation; Remote sensing images; Spatial informations; Spectral information; State-of-the-art performance; Image fusion","Deep learning; Generative adversarial network; Image fusion; Pan-sharpening; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85084634225"
"Rodríguez-Suárez B.; Quesada-Barriuso P.; Argüello F.","Rodríguez-Suárez, Brais (57450951800); Quesada-Barriuso, Pablo (55953738200); Argüello, Francisco (55932997900)","57450951800; 55953738200; 55932997900","Design of CGAN Models for Multispectral Reconstruction in Remote Sensing","2022","Remote Sensing","14","4","816","","","","10.3390/rs14040816","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124543215&doi=10.3390%2frs14040816&partnerID=40&md5=435c9c4747a8d04ac324e272f9139144","Multispectral imaging methods typically require cameras with dedicated sensors that make them expensive. In some cases, these sensors are not available or existing images are RGB, so the advantages of multispectral processing cannot be exploited. To solve this drawback, several techniques have been proposed to reconstruct the spectral reflectance of a scene from a single RGB image captured by a camera. Deep learning methods can already solve this problem with good spectral accuracy. Recently, a new type of deep learning network, the Conditional Generative Adversarial Network (CGAN), has been proposed. It is a deep learning architecture that simultaneously trains two networks (generator and discriminator) with the additional feature that both networks are conditioned on some sort of auxiliary information. This paper focuses the use of CGANs to achieve the reconstruction of multispectral images from RGB images. Different regression network models (convolutional neuronal networks, U-Net, and ResNet) have been adapted and integrated as generators in the CGAN, and compared in performance for multispectral reconstruction. Experiments with the BigEarthNet database show that CGAN with ResNet as a generator provides better results than other deep learning networks with a root mean square error of 316 measured over a range from 0 to 16,384. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Cameras; Deep learning; Image reconstruction; Mean square error; Neurons; Remote sensing; Conditional generative adversarial network; Deep learning; Learning network; Multi-spectral; Multispectral images; Multispectral imaging; Network models; Remote-sensing; RGB images; Spectral reconstruction; Generative adversarial networks","CGAN; Deep learning; Multispectral image; Spectral reconstruction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124543215"
"Karaca A.C.; Kara O.; Güllü M.K.","Karaca, Ali Can (55292760600); Kara, Ozan (57221815490); Güllü, Mehmet Kemal (55666247200)","55292760600; 57221815490; 55666247200","MultiTempGAN: Multitemporal multispectral image compression framework using generative adversarial networks","2021","Journal of Visual Communication and Image Representation","81","","103385","","","","10.1016/j.jvcir.2021.103385","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119176940&doi=10.1016%2fj.jvcir.2021.103385&partnerID=40&md5=0dbc36c8a377487bfb873208d19d64b7","Multispectral satellites that measure the reflected energy from the different regions on the Earth generate the multispectral (MS) images continuously. The following MS image for the same region can be acquired with respect to the satellite revisit period. The images captured at different times over the same region are called multitemporal images. Traditional compression methods generally benefit from spectral and spatial correlation within the MS image. However, there is also a temporal correlation between multitemporal images. To this end, we propose a novel generative adversarial network (GAN) based prediction method called MultiTempGAN for compression of multitemporal MS images. The proposed method defines a lightweight GAN-based model that learns to transform the reference image to the target image. Here, the generator parameters of MultiTempGAN are saved for the reconstruction purpose in the receiver system. Due to MultiTempGAN has a low number of parameters, it provides efficiency in multitemporal MS image compression. Experiments were carried out on three Sentinel-2 MS image pairs belonging to different geographical regions. We compared the proposed method with JPEG2000-based conventional compression methods and three deep learning methods in terms of signal-to-noise ratio, mean spectral angle, mean spectral correlation, and laplacian mean square error metrics. Additionally, we have also evaluated the change detection performances and visual maps of the methods. Experimental results demonstrate that MultiTempGAN not only achieves the best metric values among the other methods at high compression ratios but also presents convincing performances in change detection applications. © 2021 Elsevier Inc.","Big data; Deep learning; Generative adversarial networks; Geographical regions; Image compression; Mean square error; Signal to noise ratio; Change detection; Compression methods; Multi-spectral; Multi-temporal; Multi-temporal image; Multispectral images; Multispectral-image compression; Reflected energy; Remote-sensing; Spectral correlation; Remote sensing","Big data; Generative adversarial networks; Multispectral image compression; Multitemporal images; Remote sensing","Article","Final","","Scopus","2-s2.0-85119176940"
"Munir F.; Azam S.; Rafique M.A.; Sheri A.M.; Jeon M.; Pedrycz W.","Munir, Farzeen (57195518458); Azam, Shoaib (57193140704); Rafique, Muhammd Aasim (57190753891); Sheri, Ahmad Muqeem (26422488100); Jeon, Moongu (23987776700); Pedrycz, Witold (57210775415)","57195518458; 57193140704; 57190753891; 26422488100; 23987776700; 57210775415","Exploring thermal images for object detection in underexposure regions for autonomous driving","2022","Applied Soft Computing","121","","108793","","","","10.1016/j.asoc.2022.108793","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127734668&doi=10.1016%2fj.asoc.2022.108793&partnerID=40&md5=066fd82bd5f998a49289e3a03b7eeaed","Underexposure regions are vital in constructing a complete perception of the surrounding environment for safe autonomous driving. The availability of thermal cameras has provided an essential alternative to explore regions where other optical sensors lack in capturing interpretable signals. A thermal camera captures an image using the heat difference emitted by objects in the infrared spectrum, and object detection in thermal images becomes effective for autonomous driving in challenging conditions. Although object detection in the visible spectrum domain has matured, thermal object detection lacks effectiveness. A significant challenge is the scarcity of labeled data for the thermal domain, which is essential for SOTA artificial intelligence techniques. This work proposes a domain adaptation framework that employs a style transfer technique for transfer learning from visible spectrum images to thermal images. The framework uses a generative adversarial network (GAN) to transfer the low-level features from the visible spectrum domain to the thermal domain through style consistency. The efficacy of the proposed object detection method in thermal images is evident from the improved results when using styled images from publicly available thermal image datasets (FLIR ADAS and KAIST Multi-Spectral). © 2022 Elsevier B.V.","Autonomous vehicles; Cameras; Generative adversarial networks; Image enhancement; Infrared devices; Object recognition; Autonomous driving; Domain adaptation; Style transfer; Surrounding environment; Thermal; Thermal camera; Thermal domains; Thermal images; Thermal object detection; Visible spectrums; Object detection","Domain adaptation; Style transfer; Thermal Object detection","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85127734668"
"Zhang L.; Li W.; Zhang C.; Lei D.","Zhang, Liping (57200044819); Li, Weisheng (36067507500); Zhang, Ce (57216935063); Lei, Dajiang (36627356400)","57200044819; 36067507500; 57216935063; 36627356400","A generative adversarial network with structural enhancement and spectral supplement for pan-sharpening","2020","Neural Computing and Applications","32","24","","18347","18359","12","10.1007/s00521-020-04973-w","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085396047&doi=10.1007%2fs00521-020-04973-w&partnerID=40&md5=ac288a7b77ea458da25f625788cbb545","Pan-sharpening aims to obtain high-resolution multi-spectral images by fusing panchromatic images and low-resolution multi-spectral images though reasonable rules. This paper proposed a novel generative adversarial network for pan-sharpening, which utilizes the supplemented spectral information from low-resolution multi-spectral images and the enhanced structural information from panchromatic images to generate high-resolution multi-spectral images. Firstly, the forward differential operator is used to extract the spatial structural information of the panchromatic image both in the horizontal and vertical directions. Secondly, an architecture of generative adversarial network is designed. The enhanced structural information generated by the accumulation of the structural information of the two directions is added to the image fusion process in generator and the discriminating process in discriminator, and a new optimization objective is designed accordingly. What is more, the low-resolution multi-spectral image is added to the convolution process in the generator as a supplement to the spectral information. Finally, in order to obtain better image generation effect, a special objective function of the generator is designed, which adds a unique relationship to reduce the loss of spatial structural information and spectral information of fused images. Experiments on QuickBird and WorldView-3 satellites datasets show that the proposed method can generate high quality fused images and is better than most advanced methods in both objective indicators and intuitive observations. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Image fusion; Mathematical operators; Spectroscopy; Adversarial networks; Differential operators; Multispectral images; Objective functions; Spatial structural information; Spectral information; Structural enhancements; Structural information; Image enhancement","Enhanced structural information; Generative adversarial network; Pan-sharpening; Spectral supplement","Article","Final","","Scopus","2-s2.0-85085396047"
"Wu Y.; Bai Z.; Miao Q.; Ma W.; Yang Y.; Gong M.","Wu, Yue (56215531900); Bai, Zhuangfei (57215535197); Miao, Qiguang (9133503300); Ma, Wenping (57205878746); Yang, Yuelei (57204044678); Gong, Maoguo (8933846400)","56215531900; 57215535197; 9133503300; 57205878746; 57204044678; 8933846400","A classified adversarial network for multi-spectral remote sensing image change detection","2020","Remote Sensing","12","13","2098","","","","10.3390/rs12132098","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087546161&doi=10.3390%2frs12132098&partnerID=40&md5=d6e7fa657f548ae8c96252ad77cb39e0","Adversarial training has demonstrated advanced capabilities for generating image models. In this paper, we propose a deep neural network, named a classified adversarial network (CAN), for multi-spectral image change detection. This network is based on generative adversarial networks (GANs). The generator captures the distribution of the bitemporal multi-spectral image data and transforms it into change detection results, and these change detection results (as the fake data) are input into the discriminator to train the discriminator. The results obtained by pre-classification are also input into the discriminator as the real data. The adversarial training can facilitate the generator learning the transformation from a bitemporal image to a change map. When the generator is trained well, the generator has the ability to generate the final result. The bitemporal multi-spectral images are input into the generator, and then the final change detection results are obtained from the generator. The proposed method is completely unsupervised, and we only need to input the preprocessed data that were obtained from the pre-classification and training sample selection. Through adversarial training, the generator can better learn the relationship between the bitemporal multi-spectral image data and the corresponding labels. Finally, the well-trained generator can be applied to process the raw bitemporal multi-spectral images to obtain the final change map (CM). The effectiveness and robustness of the proposed method were verified by the experimental results on the real high-resolution multi-spectral image data sets. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Deep neural networks; Image classification; Spectroscopy; Adversarial networks; Change detection; High resolution; Multi-spectral image data; Multispectral images; Pre-processed data; Remote sensing images; Training sample selection; Remote sensing","Change detection; Generative adversarial networks (GANs); Multi-spectral remote sensing image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85087546161"
"Ren C.X.; Ziemann A.; Theiler J.; Moore J.","Ren, Christopher X. (57004276700); Ziemann, Amanda (36134071500); Theiler, James (7004449154); Moore, Juston (25723538500)","57004276700; 36134071500; 7004449154; 25723538500","Deepfaking it: Experiments in generative, adversarial multispectral remote sensing","2021","Proceedings of SPIE - The International Society for Optical Engineering","11727","","117270M","","","","10.1117/12.2587753","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108698773&doi=10.1117%2f12.2587753&partnerID=40&md5=1be0ef3d95265a89a9420c6ed640e76e","In this work we utilize generative adversarial networks (GANs) to synthesize realistic transformations for remote sensing imagery in the multispectral domain. Despite the apparent perceptual realism of the transformed images at a first glance, we show that a deep learning classifier can very easily be trained to differentiate between real and GAN-generated images, likely due to subtle but pervasive artifacts introduced by the GAN during the synthesis process. We also show that a very low-amplitude adversarial attack can easily fool the aforementioned deep learning classifier, although these types of attacks can be partially mitigated via adversarial training. Finally, we explore the features utilized by the classifier to differentiate real images from GAN-generated ones, and how adversarial training causes the classifier to focus on different, lower-frequency features. © 2021 SPIE.","Clustering algorithms; Deep learning; Hyperspectral imaging; Spectroscopy; Adversarial networks; Learning classifiers; Low-amplitude; Lower frequencies; Multi-spectral; Multispectral remote sensing; Remote sensing imagery; Synthesis process; Remote sensing","Deep learning; Generative Adversarial Networks; Machine learning; Multispectral imagery; Remote Sensing","Conference paper","Final","","Scopus","2-s2.0-85108698773"
"Huang B.; Li Z.; Yang C.; Sun F.; Song Y.","Huang, Binghui (57216946822); Li, Zhi (57208551292); Yang, Chao (57195032153); Sun, Fuchun (57204699218); Song, Yixu (15124457200)","57216946822; 57208551292; 57195032153; 57204699218; 15124457200","Single satellite optical imagery dehazing using SAR image prior based on conditional generative adversarial networks","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093471","1795","1802","7","10.1109/WACV45572.2020.9093471","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085521974&doi=10.1109%2fWACV45572.2020.9093471&partnerID=40&md5=4ebab6919c32b079b180d91c1193b85f","Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models. © 2020 IEEE.","Computer vision; Demulsification; Image fusion; Remote sensing; Satellite imagery; Sensor data fusion; Space-based radar; Synthetic aperture radar; Adversarial networks; Atmospheric conditions; Auxiliary information; Cloudy conditions; Multi-sensor data; Multisensor data fusion; Remote sensing images; Satellite optical imagery; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85085521974"
"Tao Y.; Conway S.J.; Mulle J.-P.; Putri A.R.D.; Thomas N.; Cremonese G.","Tao, Yu (56539197700); Conway, Susan J. (56468905300); Mulle, Jan-Peter (57223337753); Putri, Alfiah R. D. (56348927300); Thomas, Nicolas (7401830482); Cremonese, Gabriele (56240953900)","56539197700; 56468905300; 57223337753; 56348927300; 7401830482; 56240953900","Single image super-resolution restoration of tgo cassis colour images: Demonstration with perseverance rover landing site and mars science targets","2021","Remote Sensing","13","9","1777","","","","10.3390/rs13091777","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105613199&doi=10.3390%2frs13091777&partnerID=40&md5=3f0d040f86a11fa63f4bf131fb23cbaf","The ExoMars Trace Gas Orbiter (TGO)’s Colour and Stereo Surface Imaging System (CaSSIS) provides multi-spectral optical imagery at 4-5m/pixel spatial resolution. Improving the spatial resolution of CaSSIS images would allow greater amounts of scientific information to be extracted. In this work, we propose a novel Multi-scale Adaptive weighted Residual Super-resolution Generative Adversarial Network (MARSGAN) for single-image super-resolution restoration of TGO CaSSIS images, and demonstrate how this provides an effective resolution enhancement factor of about 3 times. We demonstrate with qualitative and quantitative assessments of CaSSIS SRR results over the Mars2020 Perseverance rover’s landing site. We also show examples of similar SRR performance over 8 science test sites mainly selected for being covered by HiRISE at higher resolution for comparison, which include many features unique to the Martian surface. Application of MARSGAN will allow high resolution colour imagery from CaSSIS to be obtained over extensive areas of Mars beyond what has been possible to obtain to date from HiRISE. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Calcium compounds; Color; Image enhancement; Image reconstruction; Image resolution; Optical resolving power; Restoration; Scales (weighing instruments); Adversarial networks; Effective resolutions; Higher resolution; Qualitative and quantitative assessments; Scientific information; Spatial resolution; Super resolution; Weighted residuals; Stereo image processing","CaSSIS; Frost; GAN; Generative Adversarial Network; Gullies; Jezero Crater; Mars2020; Perseverance; RSL; Slope streaks; SRR; Super-resolution restoration; TGO","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85105613199"
"Requena-Mesa C.; Reichstein M.; Mahecha M.; Kraft B.; Denzler J.","Requena-Mesa, C. (57208244647); Reichstein, M. (57206534330); Mahecha, M. (16444433900); Kraft, B. (57207884535); Denzler, J. (6701534437)","57208244647; 57206534330; 16444433900; 57207884535; 6701534437","Predicting landscapes as seen from space from environmental conditions","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519427","1768","1771","3","10.1109/IGARSS.2018.8519427","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061616822&doi=10.1109%2fIGARSS.2018.8519427&partnerID=40&md5=451e8f72ed938b70ac10d3cf76deb8ee","Satellite images are information rich snapshots of ecosystems and landscapes. In consequence, the features in the images strongly depend on the environmental conditions. Such dependency between climate and landscapes has been regarded since the beginning of earth sciences; however, it has never been taken as literally as in the present study. We adapted a deep learning generative model as a first demonstration of the potential behind deep learning for spatial pattern generation in geoscience. The purpose is to build a conditional Generative Adversarial Network (cGAN) useful to establish the relationship between two loosely linked set of variables that show multitude of complex spatial features such as climate conditions to aerial image. We trained a custom cGAN to generate Sentinel-2 multispectral imagery given a set of climatic and terrain predictors. Results show that the generated imagery shares many characteristics with the real one. In some cases, the quality of the generated imagery is high enough to deceive humans. We envision that such use of deep learning for geoscience could become an important tool to test the effects of climate on landscapes and ecosystems. © 2018 IEEE.","Antennas; Earth (planet); Ecosystems; Geology; Remote sensing; Satellite imagery; Space optics; Adversarial networks; Climate; Climate condition; Environmental conditions; Generative model; Landscape ecology; Multi-spectral imagery; Sentinel 2; Deep learning","Climate; Deep learning; GAN; Landscape ecology; Satellite imagery; Sentinel 2","Conference paper","Final","","Scopus","2-s2.0-85061616822"
"","","","2018 IEEE 4th International Conference on Identity, Security, and Behavior Analysis, ISBA 2018","2018","2018 IEEE 4th International Conference on Identity, Security, and Behavior Analysis, ISBA 2018","2018-January","","","","","160","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050243826&partnerID=40&md5=c2ba625983c5e5e05283e4f7be916097","The proceedings contain 25 papers. The topics discussed include: robust gender classification using extended multi-spectral imaging by exploring the spectral angle mapper; a novel set of pixel difference-based features for pedestrian detection; fusion analysis of soft biometrics for recognition at a distance; DyGazePass: a gaze gesture-based dynamic authentication system to counter shoulder surfing and video analysis attacks; a gender-specific behavioral analysis of mobile device usage data; analysis of head and torso movements for authentication; cloud-ID-screen: secure fingerprint data in the cloud; normalized face image generation with perceptron generative adversarial networks; multi-view gait recognition using 2D-EGEI and NMF; facial biometric presentation attack detection using temporal texture cooccurrence; secure smart metering based on LoRa technology; a longitudinal study of iris recognition in children; privacy preserving IP traceback; GHCLNet: a generalized hierarchically tuned contact lens detection network; biometric presentation attack detection using gaze alignment; on the use of convolutional neural networks for speech presentation attack detection; improved sequential fusion of heart-signal and fingerprint for anti-spoofing; adversarial domain adaptive subspace clustering; and prediction of human error using eye movements patterns for unintentional insider threat detection.","","","Conference review","Final","","Scopus","2-s2.0-85050243826"
"Grohnfeldt C.; Schmitt M.; Zhu X.","Grohnfeldt, Claas (55946211600); Schmitt, Michael (7401931279); Zhu, Xiaoxiang (55696622200)","55946211600; 7401931279; 55696622200","A conditional generative adversarial network to fuse SAR and multispectral optical data for cloud removal from Sentinel-2 images","2018","International Geoscience and Remote Sensing Symposium (IGARSS)","2018-July","","8519215","1726","1729","3","10.1109/IGARSS.2018.8519215","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056354296&doi=10.1109%2fIGARSS.2018.8519215&partnerID=40&md5=86a96bdfdcfcff984120e3d9c682d5d1","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and hazefree MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-1 SAR data confirm that our extended SAR-OptcGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input. © 2018 IEEE.","Data fusion; Deep learning; Geology; Network architecture; Remote sensing; Synthetic aperture radar; Adversarial networks; Cloudremoval; Equivalent model; Multi-spectral; Optical data; Optical remote sensing; Sentinel-1; Single sensor; Radar imaging","Cloudremoval; Data fusion; Deep learning; Generative adversarial network (GAN); Optical remote sensing; SAR","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85056354296"
"Lu L.; Zhong G.; Dong J.","Lu, L. (57201005874); Zhong, G. (56393836400); Dong, J. (22634069200)","57201005874; 56393836400; 22634069200","Image Inpainting of Multi-Spectral Image with Laser Lines Based on Generative Adversarial Network","2021","Journal of Physics: Conference Series","1880","1","012011","","","","10.1088/1742-6596/1880/1/012011","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105444086&doi=10.1088%2f1742-6596%2f1880%2f1%2f012011&partnerID=40&md5=6326052320436bfea515455f00a9a3bf","This paper presents a Generative Adversarial Network based on image in-painting, which can reconstruct the shape using a multi-spectral image with a laser line. One of the difficulties in multi-spectral photometric stereo is to extract the laser line, because the required illumination for multi-spectral photometric stereo, e.g. the red, green, and blue lights, may pollute the colour of the laser line. In this paper, we presents a method, which uses the Generative Adversarial Network based on image in-painting, to separate a multi-spectral image with a laser line into a clean laser image and an uncorrupted multi-spectral image without the laser line, to reconstruct the shape using a multi-spectral image with a laser line. To make the proposed method applicable to real-world objects, a rendered image dataset obtained using the rendering models in ShapeNet has been used for training the network, and the evaluation shows the superiority of the proposed approach over several previous methods, on both rendered images and real-world images.  © Published under licence by IOP Publishing Ltd.","Computer vision; Image reconstruction; Photometry; Rendering (computer graphics); Spectroscopy; Adversarial networks; Image Inpainting; Multi-spectral; Multispectral images; Photometric stereo; Real-world image; Real-world objects; Rendered images; Stereo image processing","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85105444086"
"Ren K.; Sun W.; Zhou J.; Meng X.; Yang G.; Peng J.","Ren, Kai (57211514962); Sun, Weiwei (55726567900); Zhou, Jun (57803409000); Meng, Xiangchao (56158755000); Yang, Gang (57192178476); Peng, Jiangtao (24833160700)","57211514962; 55726567900; 57803409000; 56158755000; 57192178476; 24833160700","A Temporal-Spectral Generative Adversarial Fusion Network for Improving Satellite Hyperspectral Temporal Resolution","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","899","902","3","10.1109/IGARSS46834.2022.9884778","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141897443&doi=10.1109%2fIGARSS46834.2022.9884778&partnerID=40&md5=2faec60991f7f52ed7cfb14ae26510d0","The improvement of temporal resolution of hyperspectral (HS) data is a fundamental and challenging problem. In this paper, we propose a Temporal-Spectral fusion method based on Generative Adversarial Network (TSF-GAN). First, the generator is used to train the nonlinear relationship between multispectral (MS) and HS data pairs at time T1 and T3, and we map the relationship to the MS data at T2 to obtain the HS data. Second, the discriminator is used to identify whether the differential image of HS data at different times is consistent with that of MS data, and whether the HS data at time T2 after spectral down-sampling is consistent with that of MS data at time T2. Preliminary experimental results demonstrate that the proposed TSF-GAN achieves comparative fidelity and has strong practicability. © 2022 IEEE.","Computer vision; Data pairs; Fusion methods; HyperSpectral; Hyperspectral Data; Multi-spectral; Multi-spectral data; Non-linear relationships; Temporal resolution; Temporal-spectral fusion; Temporal-spectral fusion method based on generative adversarial network; Generative adversarial networks","hyperspectral; multispectral; Temporal-Spectral fusion; TSF-GAN","Conference paper","Final","","Scopus","2-s2.0-85141897443"
"Diao W.; Zhang F.; Sun J.; Xing Y.; Zhang K.; Bruzzone L.","Diao, Wenxiu (57406352000); Zhang, Feng (57207771718); Sun, Jiande (12645161300); Xing, Yinghui (57195357426); Zhang, Kai (56451954400); Bruzzone, Lorenzo (7006892410)","57406352000; 57207771718; 12645161300; 57195357426; 56451954400; 7006892410","ZeRGAN: Zero-Reference GAN for Fusion of Multispectral and Panchromatic Images","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2021.3137373","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122586976&doi=10.1109%2fTNNLS.2021.3137373&partnerID=40&md5=3812d8c2e0d267127499fd308b68d534","In this article, we present a new pansharpening method, a zero-reference generative adversarial network (ZeRGAN), which fuses low spatial resolution multispectral (LR MS) and high spatial resolution panchromatic (PAN) images. In the proposed method, zero-reference indicates that it does not require paired reduced-scale images or unpaired full-scale images for training. To obtain accurate fusion results, we establish an adversarial game between a set of multiscale generators and their corresponding discriminators. Through multiscale generators, the fused high spatial resolution MS (HR MS) images are progressively produced from LR MS and PAN images, while the discriminators aim to distinguish the differences of spatial information between the HR MS images and the PAN images. In other words, the HR MS images are generated from LR MS and PAN images after the optimization of ZeRGAN. Furthermore, we construct a nonreference loss function, including an adversarial loss, spatial and spectral reconstruction losses, a spatial enhancement loss, and an average constancy loss. Through the minimization of the total loss, the spatial details in the HR MS images can be enhanced efficiently. Extensive experiments are implemented on datasets acquired by different satellites. The results demonstrate that the effectiveness of the proposed method compared with the state-of-the-art methods. The source code is publicly available at https://github.com/RSMagneto/ZeRGAN. IEEE","Generative adversarial networks; Image enhancement; Image resolution; Generative adversarial network; Generator; High spatial resolution; Multi-spectral; Multispectral images; Pan-sharpening; Panchromatic  image; Spatial resolution; Training data; Zero-reference training.; Image fusion","Generative adversarial network (GAN); Generative adversarial networks; Generators; image fusion; multispectral image; panchromatic (PAN) image; Pansharpening; Satellites; Spatial resolution; Training; Training data; zero-reference training.","Article","Article in press","","Scopus","2-s2.0-85122586976"
"Soni A.; Loui A.; Brown S.; Salvaggio C.","Soni, Ayush (57219745167); Loui, Alexander (7003882796); Brown, Scott (55712929800); Salvaggio, Carl (57203299445)","57219745167; 7003882796; 55712929800; 57203299445","High-quality multispectral image generation using conditional GANs","2020","IS and T International Symposium on Electronic Imaging Science and Technology","2020","8","","","","","10.2352/ISSN.2470-1173.2020.8.IMAWM-086","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095130809&doi=10.2352%2fISSN.2470-1173.2020.8.IMAWM-086&partnerID=40&md5=fbe01d08b86b7ffc06fa1e8778919ee5","In this paper, we demonstrate the use of a Conditional Generative Adversarial Networks (cGAN) framework for producing high-fidelity, multispectral aerial imagery using low-fidelity imagery of the same kind as input. The motivation behind is that it is easier, faster, and often less costly to produce low-fidelity images than high-fidelity images using the various available techniques, such as physics-driven synthetic image generation models. Once the cGAN network is trained and tuned in a supervised manner on a data set of paired low- and high-quality aerial images, it can then be used to enhance new, lower-quality baseline images of similar type to produce more realistic, high-fidelity multispectral image data. This approach can potentially save significant time and effort compared to traditional approaches of producing multispectral images. © 2020 Society for Imaging Science and Technology. All rights reserved.","Aerial photography; Antennas; Global system for mobile communications; Adversarial networks; Aerial imagery; Baseline images; High-fidelity images; Multi-spectral image data; Multispectral images; Synthetic image generation; Traditional approaches; Image enhancement","","Conference paper","Final","","Scopus","2-s2.0-85095130809"
"Huang X.; Xu D.; Li Z.; Wang C.","Huang, Xiao (57201292422); Xu, Dong (57194030432); Li, Zhenlong (55809947500); Wang, Cuizhen (57195968532)","57201292422; 57194030432; 55809947500; 57195968532","Translating Multispectral Imagery to Nighttime Imagery via Conditional Generative Adversarial Networks","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9323669","6758","6761","3","10.1109/IGARSS39084.2020.9323669","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101976290&doi=10.1109%2fIGARSS39084.2020.9323669&partnerID=40&md5=bbb2015f039c5435d5d9d9bc6cac9b01","Nighttime satellite imagery has been applied in a wide range of fields. However, our limited understanding of how observed light intensity is formed and whether it can be simulated greatly hinders its further application. This study explores the potential of conditional Generative Adversarial Networks (cGAN) in translating multispectral imagery to nighttime imagery. A popular cGAN framework, pix2pix, was adopted and modified to facilitate this translation using gridded training image pairs derived from Landsat 8 and Visible Infrared Imaging Radiometer Suite (VIIRS). The results of this study prove the possibility of multispectral-to-nighttime translation and further indicate that, with the additional social media data, the generated nighttime imagery can be very similar to the ground-truth imagery. This study fills the gap in understanding the composition of satellite observed nighttime light and provides new paradigms to solve the emerging problems in nighttime remote sensing fields, including nighttime series construction, light desaturation, and multi-sensor calibration. © 2020 IEEE.","Geology; Satellite imagery; Thermography (imaging); Adversarial networks; Light intensity; Multi-spectral; Multi-spectral imagery; Night-time lights; Social media datum; Training image; Visible infrared imaging radiometer suites; Remote sensing","generative adversarial network; image translation; nighttime imagery","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101976290"
"Mohamed El Mahdi B.; Abdelkrim N.; Abdenour A.; Zohir I.; Wassim B.; Fethi D.","Mohamed El Mahdi, Bouchenafa (58081636100); Abdelkrim, Nemra (25960280700); Abdenour, Amamra (58081121800); Zohir, Irki (24528634100); Wassim, Boubertakh (58082337200); Fethi, Demim (57191986238)","58081636100; 25960280700; 58081121800; 24528634100; 58082337200; 57191986238","A Novel Multispectral Maritime Target classification based on ThermalGAN (RGB-to-Thermal Image Translation)","2023","Journal of Experimental and Theoretical Artificial Intelligence","","","","","","","10.1080/0952813X.2023.2165723","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146983236&doi=10.1080%2f0952813X.2023.2165723&partnerID=40&md5=c2aff6df70634f5822ecb5dee00f8f7f","Convolutional Neural Networks (CNN) for ship classification in multi-spectral images (RGB, IR, etc.) is proposed in this paper. Recent developments in deep learning have significantly advanced the field of ship recognition. However, since maritime light intensity is frequently disturbed, multispectral imaging is considered a more robust substitute for RGB imaging. The proposed architectures were fine-tuned after being trained from scratch on the publicly available dataset VAIS (RGB-IR pairs). Unfortunately, the classification results plateaued at 59.74% accuracy, which is unsatisfactory for most real-life applications. Such an accuracy wall was due to the small number of training images. In order to overcome the scarcity of IR ship images, we proposed a novel image data augmentation strategy that translates RGB images to IR. A Pix2Pix model and a Generative Adversarial Network (GAN) network were modified to carry out the generation process as an RGB to IR translator. The KAIST general-purpose RGB-IR image pairs dataset was used to train our RGB-to-IR image translator, whereas the VAIS dataset was held aside for validation purposes. Our proposed network improved the accuracy of the native network by 8% (from 59.74% to 67.74%), which is fairly satisfactory in the field of ship recognition. © 2023 Informa UK Limited, trading as Taylor & Francis Group.","Convolutional neural networks; Deep learning; Image classification; Infrared imaging; Ships; Image translation; IR images; KAIST; Maritime targets; Multi-spectral; Ship recognition; Target Classification; Thermal images; Thermalgan; VAIS; Generative adversarial networks","Classification; KAIST; Multi-spectral; ThermalGAN; VAIS","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85146983236"
"Xing Y.; Yang S.; Zhang Y.; Zhang Y.","Xing, Yinghui (57195357426); Yang, Shuyuan (8159166000); Zhang, Yan (57218471311); Zhang, Yanning (56075029000)","57195357426; 8159166000; 57218471311; 56075029000","Learning Spectral Cues for Multispectral and Panchromatic Image Fusion","2022","IEEE Transactions on Image Processing","31","","","6964","6975","11","10.1109/TIP.2022.3215906","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141570373&doi=10.1109%2fTIP.2022.3215906&partnerID=40&md5=65dddd0edccfef56fc1ba324ca59f176","Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.  © 1992-2012 IEEE.","Deep learning; Error compensation; Extraction; Generative adversarial networks; Image fusion; Linear transformations; Mean square error; Remote sensing; Features extraction; Multi-spectral; Multispectral images; Pan-sharpening; Remote-sensing; Spatial resolution; Spectral cue; Spectral loss; Task analysis; article; clinical assessment; learning; Image enhancement","generative adversarial networks; Image fusion; pansharpening; spectral cues; spectral loss","Article","Final","","Scopus","2-s2.0-85141570373"
"Tang L.; Zhang H.; Xu H.; Ma J.","Tang, Linfeng (57223158028); Zhang, Hao (57215014270); Xu, Han (57201056465); Ma, Jiayi (26638975600)","57223158028; 57215014270; 57201056465; 26638975600","Deep learning-based image fusion: a survey; [基于深度学习的图像融合方法综述]","2023","Journal of Image and Graphics","28","1","","3","36","33","10.11834/jig.220422","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147204739&doi=10.11834%2fjig.220422&partnerID=40&md5=ef8fb9eb40bb95e22d8f40f5b432bba4","Image fusion aims to integrate complementary information from multi-source images into a single fused image to characterize the imaging scene and facilitate the subsequent vision tasks further. In recent years, it has been a concern in the field of image processing, especially in artificial intelligence-related industries like intelligent medical service, autonomous driving, smart photography, security surveillance, and military monitoring. Moreover, the growth of deep learning has been promoting deep learning-based image fusion algorithms. In particular, the emergence of advanced techniques, such as the auto-encoder, generative adversarial network, and Transformer, has led to a qualitative leap in image fusion performance. However, a comprehensive review and analysis of state-of-the-art deep learning-based image fusion algorithms for different fusion scenarios are required to be realized. Thus, we develop a systematic and critical review to explore the developments of image fusion in recent years. First, a comprehensive and systematic introduction of the image fusion field is presented from the following three aspects: 1) the development of image fusion technology, 2) the prevailing datasets, and 3) the common evaluation metrics. Then, more extensive qualitative experiments, quantitative experiments, and running efficiency evaluations of representative image fusion methods are conducted on the public datasets to compare their performance. Finally, the summary and challenges in the image fusion community are highlighted. In particular, some prospects are recommended further in the field of image fusion. First of all, from the perspective of fusion scenarios, the existing image fusion methods can be divided into three categories, i. e., multi-modal image fusion, digital photography image fusion, and remote sensing image fusion. Specifically, multi-modal image fusion is composed of infrared and visible image fusion as well as medical image fusion, and digital photography image fusion consists of multi-exposure image fusion as well as multi-focus image fusion. Multi-spectral and panchromatic image fusion can be as one of the key aspects for remote sensing image fusion. In addition, the domain of deep learning-based image fusion algorithms can be classified into the auto-encoder based (AE-based) fusion framework, convolutional neural network based (CNN-based) fusion framework, and generative adversarial network based (GAN-based) fusion framework from the aspect of network architectures. The AE-based fusion framework achieves the feature extraction and image reconstruction by a pre-trained auto-encoder, and accomplishes deep feature fusion via manual fusion strategies. To clarify feature extraction, fusion, and image reconstruction, the CNN-based fusion framework is originated from detailed network structures and loss functions. The GAN-based framework defines the image fusion problem as an adversarial game between the generators and discriminators. From the perspective of the supervision paradigm, the deep learning fusion methods can also be categorized into three classes, i. e., unsupervised, self-supervised, and supervised. The supervised methods leverage ground truth value to guide the training processes, and the unsupervised approaches construct loss function via constraining the similarity between the fusion result and source images. The self-supervised algorithms are associated with the AE-based framework in common. Our critical review is focused on the main concepts and discussions of the characteristics of each method for different fusion scenarios from the perspectives of the network architecture and supervision paradigm. Especially, we summarize the limitations of different fusion algorithms and provide some recommendations for further research. Secondly, we briefly introduce the popular public datasets and provide the interfaces-related to download them for each specific image fusion scenario. Then, we present the common evaluation metrics in the image fusion field from two aspects: regular-based evaluation metrics and specific-based metrics designed for pan-sharpening. The generic metrics can be utilized to evaluate multi-modal and digital photography image fusion algorithms of those are entropy-based, correlation-based, image feature-based, image structure-based, and human visual perception-based metrics in total. Some of the generic metrics, such as peak signal-to-noise ratio (PSNR), correlation coefficient (CC), structural similarity index measure (SSIM), and visual information fidelity (VIF), are also used for the quantitative assessment of pan-sharpening. The specific metrics designed for pan-sharpening consist of no-reference metrics and full-reference metrics that employ the full-resolution image as the reference image, i. e., ground truth. Thirdly, we present the qualitative/quantitative results, and average running times of representative alternatives for various fusion missions. Finally, this review has critically analyzed the conclusion, highlights the challenges in the image fusion community, and carried out forecasting analysis, such as non-registered image fusion, high-level vision task-driven image fusion, cross-resolution image fusion, real-time image fusion, color image fusion, image fusion based on physical imaging principles, image fusion under extreme conditions, and comprehensive evaluation metrics, etc. The methods, datasets, and evaluation metrics mentioned are linked at: https://github.com/Linfeng-Tang/Image-Fusion. © 2023 SAE-China. All rights reserved.","","deep learning; digital photography; image fusion; multi-modal; remote sensing imagery","Review","Final","","Scopus","2-s2.0-85147204739"
"Wei Z.; Li C.-L.; Shen Y.-A.; Liu Y.-F.; Zhou P.-C.","Wei, Zhe (55900977300); Li, Cong-Li (56404814600); Shen, Yan-An (57194940455); Liu, Yong-Feng (57219556153); Zhou, Pu-Cheng (7401848718)","55900977300; 56404814600; 57194940455; 57219556153; 7401848718","Thick Cloud Region Content Generation of UAV Image Based on Two-Stage Model; [基于两阶段模型的无人机图像厚云区域内容生成]","2021","Jisuanji Xuebao/Chinese Journal of Computers","44","11","","2233","2247","14","10.11897/SP.J.1016.2021.02233","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118560876&doi=10.11897%2fSP.J.1016.2021.02233&partnerID=40&md5=473e68e02950d6da1fb590a3ba2c4860","Cloud covering often causes the loss of information on the underlying surface of the image during UAV flight. However, existing cloud region estimating methods based on multi-spectral and multi-temporal mainly orient to satellite remote sensing images, and cannot be applied directly to UAV images. How to use the available information to reasonably infer the content covered by the thick cloud, so as to improve the image availability, remains an urgent problem to be solved. With image inpainting theory, which regards the covered regions as the missing or damaged parts of the image and devotes to reconstruct their consistency, a two-stage thick cloud region content generating method based on DCGAN is proposed for the characteristics of single spectrum, short flight time and random flight path of UAV imaging. The two-stage model consists of a first stage DCGAN, an image retrieval module, an affine transformation net and a second stage DCGAN from front to back sequentially. The first stage DCGAN takes the masked image in, and generates preliminary completion result. In order to make the most of the homogeneous samples in dataset, an image retrieval module and an affine transformation is added. BoW retrieval algorithm is used to search for top N homogeneous samples of the image completed in the first stage, and the affine transform network is designed to align them with attention mechanisms for the second stage. The second stage DCGAN, which has the same structure as the first, takes the preliminary completion result and the output of the affine transform network, and generates the refined result in the end. The 4 parts constitute a complete forward form. This model makes image generation easier to utilize the information of the known image with identical distribution, and solves the difficulty of feature extraction with multiple distributions in UAV images, and addresses the limitation that existing inpainting methods rely heavily on single image. This paper also improves the structure of classical DCGAN, and designs a new joint loss function, combining local and global adversarial loss with the perceptual loss and the total variation loss, which not only prevent blurry result, but also generate pixels that approximate the true semantic distribution with less noise. In the training phase, image retrieval module is trained firstly to get the whole bags of visual word and clusters. Then affine transform network is trained by affined samples with manual random setting. 2-stage DCGANs are trained end-to-end using Adam optimization alternately with samples generated by the first 2 module. In the testing phase, these modules are cascaded and worked at fixed parameters. Simulation experiments with masks and real cloud-containing image are carried out respectively. On the central 1/4 mask of the simulation experiments, PSNR and SSIM are improved by 0.3214~3.6793 and 0.0005~0.0543, and average pixel L1 loss, NIQE and BLIINDS are decreased by 0.0171~4.1120, 0.0565~4.7440 and 0.8841~4.2586, compared with other classical methods, respectively. In the real cloud-containing image experiments, NIQE and BLIINDS indexes are decreased by 0.1062~1.8992 and 1.0903~5.6495. Visual effects under the same conditions are shown and analyzed. The subjective and objective experimental results show that compared with the classical method, the proposed method has certain advantages in semantic rationality, information accuracy and visual naturalness, and provides a better solution for single spectral image pixel value prediction against thick cloud covering. © 2021, Science Press. All right reserved.","Affine transforms; Antennas; Deep learning; Feature extraction; Image enhancement; Image retrieval; Remote sensing; Semantics; Unmanned aerial vehicles (UAV); Affine transformations; Classical methods; Deep convolutional generative adversarial net; Deep learning; Homogeneous samples; Image generations; Image-based; Two stage model; Unmanned aerial vehicle image; Vehicle images; Generative adversarial networks","Deep convolutional generative adversarial net; Deep learning; Image generation; Two-stage model; Unmanned aerial vehicle image","Article","Final","","Scopus","2-s2.0-85118560876"
